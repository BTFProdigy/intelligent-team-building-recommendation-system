Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 141?150, Prague, June 2007. c?2007 Association for Computational Linguistics
Structured Prediction Models via the Matrix-Tree Theorem
Terry Koo, Amir Globerson, Xavier Carreras and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,gamir,carreras,mcollins}@csail.mit.edu
Abstract
This paper provides an algorithmic frame-
work for learning statistical models involv-
ing directed spanning trees, or equivalently
non-projective dependency structures. We
show how partition functions and marginals
for directed spanning trees can be computed
by an adaptation of Kirchhoff?s Matrix-Tree
Theorem. To demonstrate an application of
the method, we perform experiments which
use the algorithm in training both log-linear
and max-margin dependency parsers. The
new training methods give improvements in
accuracy over perceptron-trained models.
1 Introduction
Learning with structured data typically involves
searching or summing over a set with an exponen-
tial number of structured elements, for example the
set of all parse trees for a given sentence. Meth-
ods for summing over such structures include the
inside-outside algorithm for probabilistic context-
free grammars (Baker, 1979), the forward-backward
algorithm for hidden Markov models (Baum et
al., 1970), and the belief-propagation algorithm for
graphical models (Pearl, 1988). These algorithms
compute marginal probabilities and partition func-
tions, quantities which are central to many meth-
ods for the statistical modeling of complex struc-
tures (e.g., the EM algorithm (Baker, 1979; Baum
et al, 1970), contrastive estimation (Smith and Eis-
ner, 2005), training algorithms for CRFs (Lafferty et
al., 2001), and training algorithms for max-margin
models (Bartlett et al, 2004; Taskar et al, 2004a)).
This paper describes inside-outside-style algo-
rithms for the case of directed spanning trees. These
structures are equivalent to non-projective depen-
dency parses (McDonald et al, 2005b), and more
generally could be relevant to any task that involves
learning a mapping from a graph to an underlying
spanning tree. Unlike the case for projective depen-
dency structures, partition functions and marginals
for non-projective trees cannot be computed using
dynamic-programming methods such as the inside-
outside algorithm. In this paper we describe how
these quantities can be computed by adapting a well-
known result in graph theory: Kirchhoff?s Matrix-
Tree Theorem (Tutte, 1984). A na??ve application of
the theorem yields O(n4) and O(n6) algorithms for
computation of the partition function and marginals,
respectively. However, our adaptation finds the par-
tition function and marginals in O(n3) time using
simple matrix determinant and inversion operations.
We demonstrate an application of the new infer-
ence algorithm to non-projective dependency pars-
ing. Specifically, we show how to implement
two popular supervised learning approaches for this
task: globally-normalized log-linear models and
max-margin models. Log-linear estimation criti-
cally depends on the calculation of partition func-
tions and marginals, which can be computed by
our algorithms. For max-margin models, Bartlett
et al (2004) have provided a simple training al-
gorithm, based on exponentiated-gradient (EG) up-
dates, that requires computation of marginals and
can thus be implemented within our framework.
Both of these methods explicitly minimize the loss
incurred when parsing the entire training set. This
contrasts with the online learning algorithms used in
previous work with spanning-tree models (McDon-
ald et al, 2005b).
We applied the above two marginal-based train-
ing algorithms to six languages with varying de-
grees of non-projectivity, using datasets obtained
from the CoNLL-X shared task (Buchholz and
Marsi, 2006). Our experimental framework com-
pared three training approaches: log-linear models,
max-margin models, and the averaged perceptron.
Each of these was applied to both projective and
non-projective parsing. Our results demonstrate that
marginal-based training yields models which out-
141
perform those trained using the averaged perceptron.
In summary, the contributions of this paper are:
1. We introduce algorithms for inside-outside-
style calculations for directed spanning trees, or
equivalently non-projective dependency struc-
tures. These algorithms should have wide
applicability in learning problems involving
spanning-tree structures.
2. We illustrate the utility of these algorithms in
log-linear training of dependency parsing mod-
els, and show improvements in accuracy when
compared to averaged-perceptron training.
3. We also train max-margin models for depen-
dency parsing via an EG algorithm (Bartlett
et al, 2004). The experiments presented here
constitute the first application of this algorithm
to a large-scale problem. We again show im-
proved performance over the perceptron.
The goal of our experiments is to give a rigorous
comparative study of the marginal-based training al-
gorithms and a highly-competitive baseline, the av-
eraged perceptron, using the same feature sets for
all approaches. We stress, however, that the purpose
of this work is not to give competitive performance
on the CoNLL data sets; this would require further
engineering of the approach.
Similar adaptations of the Matrix-Tree Theorem
have been developed independently and simultane-
ously by Smith and Smith (2007) andMcDonald and
Satta (2007); see Section 5 for more discussion.
2 Background
2.1 Discriminative Dependency Parsing
Dependency parsing is the task of mapping a sen-
tence x to a dependency structure y. Given a sen-
tence x with n words, a dependency for that sen-
tence is a tuple (h,m) where h ? [0 . . . n] is the
index of the head word in the sentence, and m ?
[1 . . . n] is the index of a modifier word. The value
h = 0 is a special root-symbol that may only ap-
pear as the head of a dependency. We use D(x) to
refer to all possible dependencies for a sentence x:
D(x) = {(h,m) : h ? [0 . . . n],m ? [1 . . . n]}.
A dependency parse is a set of dependencies
that forms a directed tree, with the sentence?s root-
symbol as its root. We will consider both projective
Projective Non-projective
Single
Root 1 30 2Heroot saw her 1 30 2Heroot saw her
Multi
Root 1 30 2Heroot saw her 1 30 2Heroot saw her
Figure 1: Examples of the four types of dependency struc-
tures. We draw dependency arcs from head to modifier.
trees, where dependencies are not allowed to cross,
and non-projective trees, where crossing dependen-
cies are allowed. Dependency annotations for some
languages, for example Czech, can exhibit a signifi-
cant number of crossing dependencies. In addition,
we consider both single-root and multi-root trees. In
a single-root tree y, the root-symbol has exactly one
child, while in a multi-root tree, the root-symbol has
one or more children. This distinction is relevant
as our training sets include both single-root corpora
(in which all trees are single-root structures) and
multi-root corpora (in which some trees are multi-
root structures).
The two distinctions described above are orthog-
onal, yielding four classes of dependency structures;
see Figure 1 for examples of each kind of structure.
We use T sp (x) to denote the set of all possible pro-
jective single-root dependency structures for a sen-
tence x, and T snp(x) to denote the set of single-root
non-projective structures for x. The sets T mp (x) and
T mnp (x) are defined analogously for multi-root struc-
tures. In contexts where any class of dependency
structures may be used, we use the notation T (x) as
a placeholder that may be defined as T sp (x), T
s
np(x),
T mp (x) or T
m
np (x).
Following McDonald et al (2005a), we use a dis-
criminative model for dependency parsing. Fea-
tures in the model are defined through a function
f(x, h,m) which maps a sentence x together with
a dependency (h,m) to a feature vector in Rd. A
feature vector can be sensitive to any properties of
the triple (x, h,m). Given a parameter vector w,
the optimal dependency structure for a sentence x is
y?(x;w) = argmax
y?T (x)
?
(h,m)?y
w ? f(x, h,m) (1)
where the set T (x) can be defined as T sp (x), T
s
np(x),
T mp (x) or T
m
np (x), depending on the type of parsing.
142
The parameters w will be learned from a train-
ing set {(xi, yi)}Ni=1 where each xi is a sentence and
each yi is a dependency structure. Much of the pre-
vious work on learningw has focused on training lo-
cal models (see Section 5). McDonald et al (2005a;
2005b) trained global models using online algo-
rithms such as the perceptron algorithm or MIRA.
In this paper we consider training algorithms based
on work in conditional random fields (CRFs) (Laf-
ferty et al, 2001) and max-margin methods (Taskar
et al, 2004a).
2.2 Three Inference Problems
This section highlights three inference problems
which arise in training and decoding discriminative
dependency parsers, and which are central to the ap-
proaches described in this paper.
Assume that we have a vector ? with values
?h,m ? R for all (h,m) ? D(x); these values cor-
respond to weights on the different dependencies in
D(x). Define a conditional distribution over all de-
pendency structures y ? T (x) as follows:
P (y |x;?) =
exp
{?
(h,m)?y ?h,m
}
Z(x;?)
(2)
Z(x;?) =
?
y?T (x)
exp
?
?
?
?
(h,m)?y
?h,m
?
?
?
(3)
The function Z(x;?) is commonly referred to as the
partition function.
Given the distribution P (y |x;?), we can define
the marginal probability of a dependency (h,m) as
?h,m(x;?) =
?
y?T (x) : (h,m)?y
P (y |x;?)
The inference problems are then as follows:
Problem 1: Decoding:
Find argmaxy?T (x)
?
(h,m)?y ?h,m
Problem 2: Computation of the Partition Func-
tion: Calculate Z(x;?).
Problem 3: Computation of the Marginals:
For all (h,m) ? D(x), calculate ?h,m(x;?).
Note that all three problems require a maximiza-
tion or summation over the set T (x), which is ex-
ponential in size. There is a clear motivation for
being able to solve Problem 1: by setting ?h,m =
w ? f(x, h,m), the optimal dependency structure
y?(x;w) (see Eq. 1) can be computed. In this paper
the motivation for solving Problems 2 and 3 arises
from training algorithms for discriminative models.
As we will describe in Section 4, both log-linear and
max-margin models can be trained via methods that
make direct use of algorithms for Problems 2 and 3.
In the case of projective dependency structures
(i.e., T (x) defined as T sp (x) or T
m
p (x)), there are
well-known algorithms for all three inference prob-
lems. Decoding can be carried out using Viterbi-
style dynamic-programming algorithms, for exam-
ple the O(n3) algorithm of Eisner (1996). Com-
putation of the marginals and partition function can
also be achieved in O(n3) time, using a variant of
the inside-outside algorithm (Baker, 1979) applied
to the Eisner (1996) data structures (Paskin, 2001).
In the non-projective case (i.e., T (x) defined as
T snp(x) or T
m
np (x)), McDonald et al (2005b) de-
scribe how the CLE algorithm (Chu and Liu, 1965;
Edmonds, 1967) can be used for decoding. How-
ever, it is not possible to compute the marginals
and partition function using the inside-outside algo-
rithm. We next describe a method for computing
these quantities in O(n3) time using matrix inverse
and determinant operations.
3 Spanning-tree inference using the
Matrix-Tree Theorem
In this section we present algorithms for computing
the partition function and marginals, as defined in
Section 2.2, for non-projective parsing. We first re-
iterate the observation of McDonald et al (2005a)
that non-projective parses correspond to directed
spanning trees on a complete directed graph of n
nodes, where n is the length of the sentence. The
above inference problems thus involve summation
over the set of all directed spanning trees. Note that
this set is exponentially large, and there is no obvi-
ous method for decomposing the sum into dynamic-
programming-like subproblems. This section de-
scribes how a variant of Kirchhoff?s Matrix-Tree
Theorem (Tutte, 1984) can be used to evaluate the
partition function and marginals efficiently.
In what follows, we consider the single-root set-
ting (i.e., T (x) = T snp(x)), leaving the multi-root
143
case (i.e., T (x) = T mnp (x)) to Section 3.3. For a
sentence x with n words, define a complete directed
graph G on n nodes, where each node corresponds
to a word in x, and each edge corresponds to a de-
pendency between two words in x. Note thatG does
not include the root-symbol h = 0, nor does it ac-
count for any dependencies (0,m) headed by the
root-symbol. We assign non-negative weights to the
edges of this graph, yielding the following weighted
adjacency matrix A(?) ? Rn?n, for h,m = 1 . . . n:
Ah,m(?) =
{
0, if h = m
exp {?h,m} , otherwise
To account for the dependencies (0,m) headed by
the root-symbol, we define a vector of root-selection
scores r(?) ? Rn, form = 1 . . . n:
rm(?) = exp {?0,m}
Let the weight of a dependency structure y ? T snp(x)
be defined as:
?(y;?) = rroot(y)(?)
?
(h,m)?y : h 6=0
Ah,m(?)
Here, root(y) = m : (0,m) ? y is the child of the
root-symbol; there is exactly one such child, since
y ? T snp(x). Eq. 2 and 3 can be rephrased as:
P (y |x;?) =
?(y;?)
Z(x;?)
(4)
Z(x;?) =
?
y?T snp(x)
?(y;?) (5)
In the remainder of this section, we drop the nota-
tional dependence on x for brevity.
The original Matrix-Tree Theorem addressed the
problem of counting the number of undirected span-
ning trees in an undirected graph. For the models
we study here, we require a sum of weighted and
directed spanning trees. Tutte (1984) extended the
Matrix-Tree Theorem to this case. We briefly sum-
marize his method below.
First, define the Laplacian matrix L(?) ? Rn?n
of G, for h,m = 1 . . . n:
Lh,m(?) =
{ ?n
h?=1Ah?,m(?) if h = m
?Ah,m(?) otherwise
Second, for a matrix X , let X(h,m) be the minor of
X with respect to row h and column m; i.e., the
determinant of the matrix formed by deleting row h
and columnm fromX . Finally, define the weight of
any directed spanning tree of G to be the product of
the weights Ah,m(?) for the edges in that tree.
Theorem 1 (Tutte, 1984, p. 140). Let L(?) be the
Laplacian matrix of G. Then L(m,m)(?) is equal to
the sum of the weights of all directed spanning trees
of G which are rooted at m. Furthermore, the mi-
nors vary only in sign when traversing the columns
of the Laplacian (Tutte, 1984, p. 150):
?h,m : (?1)h+mL(h,m)(?) = L(m,m)(?) (6)
3.1 Partition functions via matrix determinants
From Theorem 1, it directly follows that
L(m,m)(?) =
?
y?U(m)
?
(h,m)?y : h 6=0
Ah,m(?)
where U(m) = {y ? T snp : root(y) = m}. A
na??ve method for computing the partition function is
therefore to evaluate
Z(?) =
n?
m=1
rm(?)L(m,m)(?)
The above would require calculating n determinants,
resulting in O(n4) complexity. However, as we
show below Z(?) may be obtained in O(n3) time
using a single determinant evaluation.
Define a newmatrix L?(?) to beL(?) with the first
row replaced by the root-selection scores:
L?h,m(?) =
{
rm(?) h = 1
Lh,m(?) h > 1
This matrix allows direct computation of the parti-
tion function, as the following proposition shows.
Proposition 1 The partition function in Eq. 5 is
given by Z(?) = |L?(?)|.
Proof: Consider the row expansion of |L?(?)| with
respect to row 1:
|L?(?)| =
n?
m=1
(?1)1+mL?1,m(?)L?(1,m)(?)
=
n?
m=1
(?1)1+mrm(?)L(1,m)(?)
=
n?
m=1
rm(?)L(m,m)(?) = Z(?)
The second line follows from the construction of
L?(?), and the third line follows from Eq. 6.
144
3.2 Marginals via matrix inversion
The marginals we require are given by
?h,m(?) =
1
Z(?)
?
y?T snp : (h,m)?y
?(y;?)
To calculate these marginals efficiently for all values
of (h,m) we use a well-known identity relating the
log partition-function to marginals
?h,m(?) =
? logZ(?)
??h,m
Since the partition function in this case has a closed-
form expression (i.e., the determinant of a matrix
constructed from ?), the marginals can also obtained
in closed form. Using the chain rule, the derivative
of the log partition-function in Proposition 1 is
?h,m(?) =
? log |L?(?)|
??h,m
=
n?
h?=1
n?
m?=1
? log |L?(?)|
?L?h?,m?(?)
?L?h?,m?(?)
??h,m
To perform the derivative, we use the identity
? log |X|
?X
=
(
X?1
)T
and the fact that ?L?h?,m?(?)/??h,m is nonzero for
only a few h?,m?. Specifically, when h = 0, the
marginals are given by
?0,m(?) = rm(?)
[
L??1(?)
]
m,1
and for h > 0, the marginals are given by
?h,m(?) = (1 ? ?1,m)Ah,m(?)
[
L??1(?)
]
m,m
?
(1 ? ?h,1)Ah,m(?)
[
L??1(?)
]
m,h
where ?h,m is the Kronecker delta. Thus, the com-
plexity of evaluating all the relevant marginals is
dominated by the matrix inversion, and the total
complexity is therefore O(n3).
3.3 Multiple Roots
In the case of multiple roots, we can still compute
the partition function and marginals efficiently. In
fact, the derivation of this case is simpler than for
single-root structures. Create an extended graph G?
which augmentsG with a dummy root node that has
edges pointing to all of the existing nodes, weighted
by the appropriate root-selection scores. Note that
there is a bijection between directed spanning trees
ofG? rooted at the dummy root and multi-root struc-
tures y ? T mnp (x). Thus, Theorem 1 can be used to
compute the partition function directly: construct a
Laplacian matrix L(?) for G? and compute the mi-
nor L(0,0)(?). Since this minor is also a determi-
nant, the marginals can be obtained analogously to
the single-root case. More concretely, this technique
corresponds to defining the matrix L?(?) as
L?(?) = L(?) + diag(r(?))
where diag(v) is the diagonal matrix with the vector
v on its diagonal.
3.4 Labeled Trees
The techniques above extend easily to the case
where dependencies are labeled. For a model with
L different labels, it suffices to define the edge
and root scores as Ah,m(?) =
?L
`=1 exp {?h,m,`}
and rm(?) =
?L
`=1 exp {?0,m,`}. The partition
function over labeled trees is obtained by operat-
ing on these values as described previously, and
the marginals are given by an application of the
chain rule. Both inference problems are solvable in
O(n3 + Ln2) time.
4 Training Algorithms
This section describes two methods for parameter
estimation that rely explicitly on the computation of
the partition function and marginals.
4.1 Log-Linear Estimation
In conditional log-linear models (Johnson et al,
1999; Lafferty et al, 2001), a distribution over parse
trees for a sentence x is defined as follows:
P (y |x;w) =
exp
{?
(h,m)?y w ? f(x, h,m)
}
Z(x;w)
(7)
where Z(x;w) is the partition function, a sum over
T sp (x), T
s
np(x), T
m
p (x) or T
m
np (x).
We train the model using the approach described
by Sha and Pereira (2003). Assume that we have a
training set {(xi, yi)}Ni=1. The optimal parameters
145
are taken to be w? = argminw L(w) where
L(w) = ?C
N?
i=1
logP (yi |xi;w) +
1
2
||w||2
The parameterC > 0 is a constant dictating the level
of regularization in the model.
Since L(w) is a convex function, gradient de-
scent methods can be used to search for the global
minimum. Such methods typically involve repeated
computation of the loss L(w) and gradient ?L(w)?w ,
requiring efficient implementations of both func-
tions. Note that the log-probability of a parse is
logP (y |x;w) =
?
(h,m)?y
w ? f(x, h,m)? logZ(x;w)
so that the main issue in calculating the loss func-
tion L(w) is the evaluation of the partition functions
Z(xi;w). The gradient of the loss is given by
?L(w)
?w
= w ? C
N?
i=1
?
(h,m)?yi
f(xi, h,m)
+ C
N?
i=1
?
(h,m)?D(xi)
?h,m(xi;w)f(xi, h,m)
where
?h,m(x;w) =
?
y?T (x) : (h,m)?y
P (y |x;w)
is the marginal probability of a dependency (h,m).
Thus, the main issue in the evaluation of the gradient
is the computation of the marginals ?h,m(xi;w).
Note that Eq. 7 forms a special case of the log-
linear distribution defined in Eq. 2 in Section 2.2.
If we set ?h,m = w ? f(x, h,m) then we have
P (y |x;w) = P (y |x;?), Z(x;w) = Z(x;?), and
?h,m(x;w) = ?h,m(x;?). Thus in the projective
case the inside-outside algorithm can be used to cal-
culate the partition function and marginals, thereby
enabling training of a log-linear model; in the non-
projective case the algorithms in Section 3 can be
used for this purpose.
4.2 Max-Margin Estimation
The second learning algorithm we consider is
the large-margin approach for structured prediction
(Taskar et al, 2004a; Taskar et al, 2004b). Learning
in this framework again involves minimization of a
convex function L(w). Let the margin for parse tree
y on the i?th training example be defined as
mi,y(w) =
?
(h,m)?yi
w?f(xi, h,m) ?
?
(h,m)?y
w?f(xi, h,m)
The loss function is then defined as
L(w) = C
N?
i=1
max
y?T (xi)
(Ei,y ?mi,y(w)) +
1
2
||w||2
where Ei,y is a measure of the loss?or number of
errors?for parse y on the i?th training sentence. In
this paper we take Ei,y to be the number of incorrect
dependencies in the parse tree y when compared to
the gold-standard parse tree yi.
The definition of L(w) makes use of the expres-
sion maxy?T (xi) (Ei,y ?mi,y(w)) for the i?th train-
ing example, which is commonly referred to as the
hinge loss. Note that Ei,yi = 0, and also that
mi,yi(w) = 0, so that the hinge loss is always non-
negative. In addition, the hinge loss is 0 if and only
ifmi,y(w) ? Ei,y for all y ? T (xi). Thus the hinge
loss directly penalizes margins mi,y(w) which are
less than their corresponding losses Ei,y.
Figure 2 shows an algorithm for minimizing
L(w) that is based on the exponentiated-gradient al-
gorithm for large-margin optimization described by
Bartlett et al (2004). The algorithm maintains a set
of weights ?i,h,m for i = 1 . . . N, (h,m) ? D(xi),
which are updated example-by-example. The algo-
rithm relies on the repeated computation of marginal
values ?i,h,m, which are defined as follows:1
?i,h,m =
?
y?T (xi) : (h,m)?y
P (y |xi) (8)
P (y |xi) =
exp
{?
(h,m)?y ?i,h,m
}
?
y??T (xi) exp
{?
(h,m)?y? ?i,h,m
}
A similar definition is used to derive marginal val-
ues ??i,h,m from the values ?
?
i,h,m. Computation of
the ? and ?? values is again inference of the form
described in Problem 3 in Section 2.2, and can be
1Bartlett et al (2004) write P (y |xi) as ?i,y . The ?i,y vari-
ables are dual variables that appear in the dual objective func-
tion, i.e., the convex dual of L(w). Analysis of the algorithm
shows that as the ?i,h,m variables are updated, the dual vari-
ables converge to the optimal point of the dual objective, and
the parameters w converge to the minimum of L(w).
146
Inputs: Training examples {(xi, yi)}Ni=1.
Parameters: Regularization constant C, starting point ?,
number of passes over training set T .
Data Structures: Real values ?i,h,m and li,h,m for i =
1 . . . N, (h,m) ? D(xi). Learning rate ?.
Initialization: Set learning rate ? = 1C . Set ?i,h,m = ? for
(h,m) ? yi, and ?i,h,m = 0 for (h,m) /? yi. Set li,h,m = 0
for (h,m) ? yi, and li,h,m = 1 for (h,m) /? yi. Calculate
initial parameters as
w = C
?
i
?
(h,m)?D(xi)
?i,h,mf(xi, h,m)
where ?i,h,m = (1? li,h,m ??i,h,m) and the ?i,h,m values
are calculated from the ?i,h,m values as described in Eq. 8.
Algorithm: Repeat T passes over the training set, where
each pass is as follows:
Set obj = 0
For i = 1 . . . N
? For all (h,m) ? D(xi):
??i,h,m = ?i,h,m + ?C (li,h,m +w ? f(xi, h,m))
? For example i, calculate marginals ?i,h,m
from ?i,h,m values, and marginals ??i,h,m
from ??i,h,m values (see Eq. 8).
? Update the parameters:
w = w + C
?
(h,m)?D(xi)
?i,h,mf(xi, h,m)
where ?i,h,m = ?i,h,m ? ??i,h,m,
? For all (h,m) ? D(xi), set ?i,h,m = ??i,h,m
? Set obj = obj + C
?
(h,m)?D(xi)
li,h,m??i,h,m
Set obj = obj ? ||w||
2
2 . If obj has decreased
compared to last iteration, set ? = ?2 .
Output: Parameter values w.
Figure 2: The EG Algorithm for Max-Margin Estimation.
The learning rate ? is halved each time the dual objective func-
tion (see (Bartlett et al, 2004)) fails to increase. In our experi-
ments we chose ? = 9, which was found to work well during
development of the algorithm.
achieved using the inside-outside algorithm for pro-
jective structures, and the algorithms described in
Section 3 for non-projective structures.
5 Related Work
Global log-linear training has been used in the con-
text of PCFG parsing (Johnson, 2001). Riezler et al
(2004) explore a similar application of log-linear
models to LFG parsing. Max-margin learning
has been applied to PCFG parsing by Taskar et al
(2004b). They show that this problem has a QP
dual of polynomial size, where the dual variables
correspond to marginal probabilities of CFG rules.
A similar QP dual may be obtained for max-margin
projective dependency parsing. However, for non-
projective parsing, the dual QP would require an ex-
ponential number of constraints on the dependency
marginals (Chopra, 1989). Nevertheless, alternative
optimization methods like that of Tsochantaridis et
al. (2004), or the EGmethod presented here, can still
be applied.
The majority of previous work on dependency
parsing has focused on local (i.e., classification of
individual edges) discriminative training methods
(Yamada and Matsumoto, 2003; Nivre et al, 2004;
Y. Cheng, 2005). Non-local (i.e., classification of
entire trees) training methods were used by McDon-
ald et al (2005a), who employed online learning.
Dependency parsing accuracy can be improved
by allowing second-order features, which consider
more than one dependency simultaneously. McDon-
ald and Pereira (2006) define a second-order depen-
dency parsing model in which interactions between
adjacent siblings are allowed, and Carreras (2007)
defines a second-order model that allows grandpar-
ent and sibling interactions. Both authors give poly-
time algorithms for exact projective parsing. By
adapting the inside-outside algorithm to these mod-
els, partition functions and marginals can be com-
puted for second-order projective structures, allow-
ing log-linear and max-margin training to be ap-
plied via the framework developed in this paper.
For higher-order non-projective parsing, however,
computational complexity results (McDonald and
Pereira, 2006; McDonald and Satta, 2007) indicate
that exact solutions to the three inference problems
of Section 2.2 will be intractable. Exploration of ap-
proximate second-order non-projective inference is
a natural avenue for future research.
Two other groups of authors have independently
and simultaneously proposed adaptations of the
Matrix-Tree Theorem for structured inference on di-
rected spanning trees (McDonald and Satta, 2007;
Smith and Smith, 2007). There are some algorithmic
differences between these papers and ours. First, we
define both multi-root and single-root algorithms,
whereas the other papers only consider multi-root
147
parsing. This distinction can be important as one
often expects a dependency structure to have ex-
actly one child attached to the root-symbol, as is the
case in a single-root structure. Second, McDonald
and Satta (2007) propose an O(n5) algorithm for
computing the marginals, as opposed to the O(n3)
matrix-inversion approach used by Smith and Smith
(2007) and ourselves.
In addition to the algorithmic differences, both
groups of authors consider applications of the
Matrix-Tree Theorem which we have not discussed.
For example, both papers propose minimum-risk
decoding, and McDonald and Satta (2007) dis-
cuss unsupervised learning and language model-
ing, while Smith and Smith (2007) define hidden-
variable models based on spanning trees.
In this paper we used EG training methods only
for max-margin models (Bartlett et al, 2004). How-
ever, Globerson et al (2007) have recently shown
how EG updates can be applied to efficient training
of log-linear models.
6 Experiments on Dependency Parsing
In this section, we present experimental results
applying our inference algorithms for dependency
parsing models. Our primary purpose is to estab-
lish comparisons along two relevant dimensions:
projective training vs. non-projective training, and
marginal-based training algorithms vs. the averaged
perceptron. The feature representation and other rel-
evant dimensions are kept fixed in the experiments.
6.1 Data Sets and Features
We used data from the CoNLL-X shared task
on multilingual dependency parsing (Buchholz and
Marsi, 2006). In our experiments, we used a subset
consisting of six languages; Table 1 gives details of
the data sets used.2 For each language we created
a validation set that was a subset of the CoNLL-X
2Our subset includes the two languages with the lowest ac-
curacy in the CoNLL-X evaluations (Turkish and Arabic), the
language with the highest accuracy (Japanese), the most non-
projective language (Dutch), a moderately non-projective lan-
guage (Slovene), and a highly projective language (Spanish).
All languages but Spanish have multi-root parses in their data.
We are grateful to the providers of the treebanks that constituted
the data of our experiments (Hajic? et al, 2004; van der Beek et
al., 2002; Kawata and Bartels, 2000; Dz?eroski et al, 2006; Civit
and Mart??, 2002; Oflazer et al, 2003).
language %cd train val. test
Arabic 0.34 49,064 5,315 5,373
Dutch 4.93 178,861 16,208 5,585
Japanese 0.70 141,966 9,495 5,711
Slovene 1.59 22,949 5,801 6,390
Spanish 0.06 78,310 11,024 5,694
Turkish 1.26 51,827 5,683 7,547
Table 1: Information for the languages in our experiments.
The 2nd column (%cd) is the percentage of crossing dependen-
cies in the training and validation sets. The last three columns
report the size in tokens of the training, validation and test sets.
training set for that language. The remainder of each
training set was used to train the models for the dif-
ferent languages. The validation sets were used to
tune the meta-parameters (e.g., the value of the reg-
ularization constantC) of the different training algo-
rithms. We used the official test sets and evaluation
script from the CoNLL-X task. All of the results that
we report are for unlabeled dependency parsing.3
The non-projective models were trained on the
CoNLL-X data in its original form. Since the pro-
jective models assume that the dependencies in the
data are non-crossing, we created a second train-
ing set for each language where non-projective de-
pendency structures were automatically transformed
into projective structures. All projective models
were trained on these new training sets.4 Our feature
space is based on that of McDonald et al (2005a).5
6.2 Results
We performed experiments using three training al-
gorithms: the averaged perceptron (Collins, 2002),
log-linear training (via conjugate gradient descent),
and max-margin training (via the EG algorithm).
Each of these algorithms was trained using pro-
jective and non-projective methods, yielding six
training settings per language. The different
training algorithms have various meta-parameters,
which we optimized on the validation set for
each language/training-setting combination. The
3Our algorithms also support labeled parsing (see Section
3.4). Initial experiments with labeled models showed the same
trend that we report here for unlabeled parsing, so for simplicity
we conducted extensive experiments only for unlabeled parsing.
4The transformations were performed by running the pro-
jective parser with score +1 on correct dependencies and -1 oth-
erwise: the resulting trees are guaranteed to be projective and to
have a minimum loss with respect to the correct tree. Note that
only the training sets were transformed.
5It should be noted that McDonald et al (2006) use a richer
feature set that is incomparable to our features.
148
Perceptron Max-Margin Log-Linear
p np p np p np
Ara 71.74 71.84 71.74 72.99 73.11 73.67
Dut 77.17 78.83 76.53 79.69 76.23 79.55
Jap 91.90 91.78 92.10 92.18 91.68 91.49
Slo 78.02 78.66 79.78 80.10 78.24 79.66
Spa 81.19 80.02 81.71 81.93 81.75 81.57
Tur 71.22 71.70 72.83 72.02 72.26 72.62
Table 2: Test data results. The p and np columns show results
with projective and non-projective training respectively.
Ara Dut Jap Slo Spa Tur AV
P 71.74 78.83 91.78 78.66 81.19 71.70 79.05
E 72.99 79.69 92.18 80.10 81.93 72.02 79.82
L 73.67 79.55 91.49 79.66 81.57 72.26 79.71
Table 3: Results for the three training algorithms on the differ-
ent languages (P = perceptron, E = EG, L = log-linear models).
AV is an average across the results for the different languages.
averaged perceptron has a single meta-parameter,
namely the number of iterations over the training set.
The log-linear models have two meta-parameters:
the regularization constant C and the number of
gradient steps T taken by the conjugate-gradient
optimizer. The EG approach also has two meta-
parameters: the regularization constant C and the
number of iterations, T .6 For models trained using
non-projective algorithms, both projective and non-
projective parsing was tested on the validation set,
and the highest scoring of these two approaches was
then used to decode test data sentences.
Table 2 reports test results for the six training sce-
narios. These results show that for Dutch, which is
the language in our data that has the highest num-
ber of crossing dependencies, non-projective train-
ing gives significant gains over projective training
for all three training methods. For the other lan-
guages, non-projective training gives similar or even
improved performance over projective training.
Table 3 gives an additional set of results, which
were calculated as follows. For each of the three
training methods, we used the validation set results
to choose between projective and non-projective
training. This allows us to make a direct com-
parison of the three training algorithms. Table 3
6We trained the perceptron for 100 iterations, and chose the
iteration which led to the best score on the validation set. Note
that in all of our experiments, the best perceptron results were
actually obtained with 30 or fewer iterations. For the log-linear
and EG algorithms we tested a number of values for C, and for
each value of C ran 100 gradient steps or EG iterations, finally
choosing the best combination of C and T found in validation.
shows the results of this comparison.7 The results
show that log-linear and max-margin models both
give a higher average accuracy than the perceptron.
For some languages (e.g., Japanese), the differences
from the perceptron are small; however for other
languages (e.g., Arabic, Dutch or Slovene) the im-
provements seen are quite substantial.
7 Conclusions
This paper describes inference algorithms for
spanning-tree distributions, focusing on the funda-
mental problems of computing partition functions
and marginals. Although we concentrate on log-
linear and max-margin estimation, the inference al-
gorithms we present can serve as black-boxes in
many other statistical modeling techniques.
Our experiments suggest that marginal-based
training produces more accurate models than per-
ceptron learning. Notably, this is the first large-scale
application of the EG algorithm, and shows that it is
a promising approach for structured learning.
In line with McDonald et al (2005b), we confirm
that spanning-tree models are well-suited to depen-
dency parsing, especially for highly non-projective
languages such as Dutch. Moreover, spanning-tree
models should be useful for a variety of other prob-
lems involving structured data.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their constructive comments. In addi-
tion, the authors gratefully acknowledge the follow-
ing sources of support. Terry Koo was funded by
a grant from the NSF (DMS-0434222) and a grant
from NTT, Agmt. Dtd. 6/21/1998. Amir Glober-
son was supported by a fellowship from the Roth-
schild Foundation - Yad Hanadiv. Xavier Carreras
was supported by the Catalan Ministry of Innova-
tion, Universities and Enterprise, and a grant from
NTT, Agmt. Dtd. 6/21/1998. Michael Collins was
funded by NSF grants 0347631 and DMS-0434222.
7We ran the sign test at the sentence level to measure the
statistical significance of the results aggregated across the six
languages. Out of 2,472 sentences total, log-linear models gave
improved parses over the perceptron on 448 sentences, and
worse parses on 343 sentences. The max-margin method gave
improved/worse parses for 500/383 sentences. Both results are
significant with p ? 0.001.
149
References
J. Baker. 1979. Trainable grammars for speech recognition. In
97th meeting of the Acoustical Society of America.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester. 2004. Ex-
ponentiated gradient algorithms for large?margin structured
classification. In NIPS.
L.E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970. A max-
imization technique occurring in the statistical analysis of
probabilistic functions of markov chains. Annals of Mathe-
matical Statistics, 41:164?171.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In Proc. CoNLL-X.
X. Carreras. 2007. Experiments with a higher-order projective
dependency parser. In Proc. EMNLP-CoNLL.
S. Chopra. 1989. On the spanning tree polyhedron. Oper. Res.
Lett., pages 25?29.
Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a
directed graph. Science Sinica, 14:1396?1400.
M. Civit and Ma A. Mart??. 2002. Design principles for a Span-
ish treebank. In Proc. of the First Workshop on Treebanks
and Linguistic Theories (TLT).
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In Proc. EMNLP.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Z?abokrtsky, and
A. Z?ele. 2006. Towards a Slovene dependency treebank. In
Proc. of the Fifth Intern. Conf. on Language Resources and
Evaluation (LREC).
J. Edmonds. 1967. Optimum branchings. Journal of Research
of the National Bureau of Standards, 71B:233?240.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. COLING.
A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007. Ex-
ponentiated gradient algorithms for log-linear structured pre-
diction. In Proc. ICML.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic
Language Resources and Tools, pages 110?117.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999.
Estimators for stochastic unification-based grammars. In
Proc. ACL.
M. Johnson. 2001. Joint and conditional estimation of tagging
and parsing models. In Proc. ACL.
Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese
treebank in VERBMOBIL. Verbmobil-Report 240, Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditonal ran-
dom fields: Probabilistic models for segmenting and labeling
sequence data. In Proc. ICML.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. EACL.
R. McDonald and G. Satta. 2007. On the complexity of non-
projective data-driven dependency parsing. In Proc. IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. HLT-EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual
dependency parsing with a two-stage discriminative parser.
In Proc. CoNLL-X.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. CoNLL.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In A. Abeille?, editor, Tree-
banks: Building and Using Parsed Corpora, chapter 15.
Kluwer Academic Publishers.
M.A. Paskin. 2001. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical Report
UCB/CSD-01-1148, University of California, Berkeley.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference (2nd edition). Mor-
gan Kaufmann Publishers.
S. Riezler, R. Kaplan, T. King, J. Maxwell, A. Vasserman, and
R. Crouch. 2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proc. HLT-NAACL.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL.
N.A. Smith and J. Eisner. 2005. Contrastive estimation: Train-
ing log-linear models on unlabeled data. In Proc. ACL.
D.A. Smith and N.A. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In Proc. EMNLP-CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2004a. Max-margin
markov networks. In NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004b. Max-margin parsing. In Proc. EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdependent
and structured output spaces. In Proc. ICML.
W. Tutte. 1984. Graph Theory. Addison-Wesley.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
Y. Matsumoto Y. Cheng, M. Asahara. 2005. Machine learning-
based dependency analyzer for chinese. In Proc. ICCC.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. IWPT.
150
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 957?961,
Prague, June 2007. c?2007 Association for Computational Linguistics
Experiments with a Higher-Order Projective Dependency Parser
Xavier Carreras
Massachusetts Institute of Technology (MIT)
Computer Science and Artificial Intelligence Laboratory (CSAIL)
32 Vassar St., Cambridge, MA 02139
carreras@csail.mit.edu
Abstract
We present experiments with a dependency
parsing model defined on rich factors. Our
model represents dependency trees with fac-
tors that include three types of relations be-
tween the tokens of a dependency and their
children. We extend the projective pars-
ing algorithm of Eisner (1996) for our case,
and train models using the averaged percep-
tron. Our experiments show that consider-
ing higher-order information yields signifi-
cant improvements in parsing accuracy, but
comes at a high cost in terms of both time
and memory consumption. In the multi-
lingual exercise of the CoNLL-2007 shared
task (Nivre et al, 2007), our system obtains
the best accuracy for English, and the second
best accuracies for Basque and Czech.
1 Introduction
Structured prediction problems usually involve
models that work with factored representations of
structures. The information included in the factors
determines the type of features that the model can
exploit. However, richer representations translate
into higher complexity of the inference algorithms
associated with the model.
In dependency parsing, the basic first-order model
is defined by a decomposition of a tree into head-
modifier dependencies. Previous work extended this
basic model to include second-order relations?i.e.
dependencies that are adjacent to the main depen-
dency of the factor. Specifically, these approaches
considered sibling relations of the modifier token
(Eisner, 1996; McDonald and Pereira, 2006). In this
paper we extend the parsing model with other types
of second-order relations. In particular, we incorpo-
rate relations between the head and modifier tokens
and the children of the modifier.
One paradigmatic case where the relations we
consider are relevant is PP-attachment. For example,
in ?They sold 1,210 cars in the U.S.?, the ambigu-
ity problem is to determine whether the preposition
?in? (which governs ?the U.S.?) is modifying ?sold?
or ?cars?, the former being correct in this case. It is
generally accepted that to solve the attachment deci-
sion it is necessary to look at the head noun within
the prepositional phrase (i.e., ?U.S.? in the exam-
ple), which has a grand-parental relation with the
two candidate tokens that the phrase may attach?
see e.g. (Ratnaparkhi et al, 1994). Other ambigu-
ities in language may also require consideration of
grand-parental relations in the dependency structure.
We present experiments with higher-order models
trained with averaged perceptron. The second-order
relations that we incorporate in the model yield sig-
nificant improvements in accuracy. However, the in-
ference algorithms for our factorization are very ex-
pensive in terms of time and memory consumption,
and become impractical when dealing with many la-
bels or long sentences.
2 Higher-Order Projective Models
A dependency parser receives a sentence x of n to-
kens, and outputs a labeled dependency tree y. In
the tree, a labeled dependency is a triple ?h, m, l?,
where h ? [0 . . . n] is the index of the head token,
957
lh m ccch mi mo
Figure 1: A factor in the higher-order parsing model.
m ? [1 . . . n] is the index of the modifier token, and
l ? [1 . . . L] is the label of the dependency. The
value h = 0 is used for dependencies where the
head is a special root-symbol of the sentence. We
denote by T (x) the set of all possible dependency
structures for a sentence x. In this paper, we restrict
to projective dependency trees. The dependency tree
computed by the parser for a given sentence is:
y?(x) = arg max
y?T (x)
?
f?y
score(w,x, f)
The parsing model represents a structure y as a set of
factors, f ? y, and scores each factor using param-
eters w. In a first-order model a factor corresponds
to a single labeled dependency, i.e. f = ?h, m, l?.
The features of the model are defined through a fea-
ture function ?1(x, h, m) which maps a sentence to-
gether with an unlabeled dependency to a feature
vector in Rd1 . The parameters of the model are a
collection of vectors wl1 ? Rd1 , one for each pos-
sible label. The first-order model scores a factor as
score1(w,x, ?h, m, l?) = ?1(x, h, m) ? wl1.
The higher-order model defined in this paper de-
composes a dependency structure into factors that
include children of the head and the modifier. In
particular, a factor in our model is represented by
the signature f = ?h, m, l, ch, cmi, cmo? where, as
in the first-order model, h, m and l are respectively
the head, modifier and label of the main dependency
of the factor; ch is the child of h in [h . . .m] that
is closest to m; cmi is child of m inside [h . . .m]
that is furthest from m; cmo is the child of m out-
side [h . . . m] that is furthest from m. Figure 1 de-
picts a factor of the higher-order model, and Table 1
lists the factors of an example sentence. Note that a
factor involves a main labeled dependency and three
adjacent unlabeled dependencies that attach to chil-
dren of h and m. Special values are used when either
of these children are null.
The higher-order model defines additional
m h ch cmi cmo
They 1 2 - - -
sold 2 0 - 1 5
1,200 3 4 - - -
cars 4 2 - 3 -
in 5 2 4 - 7
the 6 7 - - -
U.S. 7 5 - 6 -
Table 1: Higher-order factors for an example sentence. For
simplicity, labels of the factors have been omitted. A first-order
model considers only ?h, m?. The second-order model of Mc-
Donald and Pereira (2006) considers ?h, m, ch?. For the PP-
attachment decision (factor in row 5), the higher-order model
allows us to define features that relate the verb (?sold?) with the
content word of the prepositional phrase (?U.S.?).
second-order features through a function
?2(x, h, m, c) which maps a head, a modifier
and a child in a feature vector in Rd2 . The param-
eters of the model are a collection of four vectors
for each dependency label: wl1 ? Rd1 as in the
first-order model; and wlh,wlmi and wlmo, all three
in Rd2 and each associated to one of the adjacent
dependencies in the factor. The score of a factor is:
score2(w,x, ?h, m, l, ch, cmi, cmo?) =
?1(x, h, m) ? wl1 + ?2(x, h, m, ch) ? wlh +
?2(x, h, m, cmi) ? wlmi + ?2(x, h, m, cmo) ? wlmo
Note that the model uses a common feature func-
tion for second-order relations, but features could
be defined specifically for each type of relation.
Note also that while the higher-order factors include
four dependencies, our modelling choice only ex-
ploits relations between the main dependency and
secondary dependencies. Considering relations be-
tween secondary dependencies would greatly in-
crease the cost of the associated algorithms.
2.1 Parsing Algorithm
In this section we sketch an extension of the pro-
jective dynamic programming algorithm of Eis-
ner (1996; 2000) for the higher-order model de-
fined above. The time complexity of the algo-
rithm is O(n4L), and the memory requirements are
O(n2L + n3). As in the Eisner approach, our algo-
rithm visits sentence spans in a bottom up fashion,
and constructs a chart with two types of dynamic
programming structures, namely open and closed
structures?see Figure 2 for a diagram. The dy-
namic programming structures are:
958
h m h m ecmo
l
micr+1rhc
l
Figure 2: Dynamic programming structures used in the pars-
ing algorithm. The variables in boldface constitute the index of
the chart entry for a structure; the other variables constitute the
back-pointer stored in the chart entry. Left: an open structure
for the chart entry [h, m, l]O ; the algorithm looks for the r, ch
and cmi that yield the optimal score for this structure. Right:
a closed structure for the chart entry [h, e, m]C ; the algorithm
looks for the l and cmo that yield the optimal score.
? Open structures: For each span from s to e and
each label l, the algorithm maintains a chart
entry [s, e, l]O associated to the dependency
?s, e, l?. For each entry, the algorithm looks
for the optimal splitting point r, sibling ch and
grand-child cmi using parameters wl1, wlh and
wlmi. This can be done in O(n2) because our
features do not consider interactions between
ch and cmi. Similar entries [e, s, l]O are main-
tained for dependencies headed at e.
? Closed structures: For each span from s to e
and each token m ? [s . . . e], the algorithm
maintains an entry [s, e, m]C associated to a
partial dependency tree rooted at s in which m
is the last modifier of s. The algorithm chooses
the optimal dependency label l and grand-child
cmo in O(nL), using parameters wlmo. Similar
entries [e, s, m]C are maintained for dependen-
cies headed at e.
We implemented two variants of the algorithm.
The first forces the root token to participate in ex-
actly one dependency. The second allows many de-
pendencies involving the root token. For the single-
root case, it is necessary to treat the root token dif-
ferently than other tokens. In the experiments, we
used the single-root variant if sentences in the train-
ing set satisfy this property. Otherwise we used the
multi-root variant.
2.2 Features
The first-order features ?1(x, h, m) are the exact
same implementation as in previous CoNLL sys-
tem (Carreras et al, 2006). In turn, those features
were inspired by successful previous work in first-
order dependency parsing (McDonald et al, 2005).
The most basic feature patterns consider the sur-
face form, part-of-speech, lemma and other morpho-
syntactic attributes of the head or the modifier of a
dependency. The representation also considers com-
plex features that exploit a variety of conjunctions
of the forms and part-of-speech tags of the follow-
ing items: the head and modifier; the head, modifier,
and any token in between them; the head, modifier,
and the two tokens following or preceding them.
As for the second-order features, we again
base our features with those of McDonald and
Pereira (2006), who reported successful experiments
with second-order models. We add some patterns to
their features. Let dir be ?right? if h < m, and
?left? otherwise; let form(xi) and cpos(xi) return
the surface form and coarse part-of-speech of token
xi, respectively. The definition of ?2(x, h, m, c) is:
? dir ? cpos(xh) ? cpos(xm) ? cpos(xc)
? dir ? cpos(xh) ? cpos(xc)
? dir ? cpos(xm) ? cpos(xc)
? dir ? form(xh) ? form(xc)
? dir ? form(xm) ? form(xc)
? dir ? cpos(xh) ? form(xc)
? dir ? cpos(xm) ? form(xc)
? dir ? form(xh) ? cpos(xc)
? dir ? form(xm) ? cpos(xc)
3 Experiments and Results
We report experiments with higher-order models for
the ten languages in the multilingual track of the
CoNLL-2007 shared task (Nivre et al, 2007).1
In all experiments, we trained our models us-
ing the averaged perceptron (Freund and Schapire,
1999), following the extension of Collins (2002) for
structured prediction problems. To train models, we
used ?projectivized? versions of the training depen-
dency trees.2
1We are grateful to the providers of the treebanks that con-
stituted the data for the shared task (Hajic? et al, 2004; Aduriz
et al, 2003; Mart?? et al, 2007; Chen et al, 2003; Bo?hmova? et
al., 2003; Marcus et al, 1993; Johansson and Nugues, 2007;
Prokopidis et al, 2005; Csendes et al, 2005; Montemagni et
al., 2003; Oflazer et al, 2003).
2We obtained projective trees for training sentences by run-
ning the projective parser with an oracle model (that assigns a
score of +1 to correct dependencies and -1 otherwise).
959
Catalan Czech English
First-Order, no averaging 82.07 68.98 83.75
First-Order 86.15 75.96 87.54
Higher-Order, ch 87.50 77.15 88.70
Higher-Order, ch cmo 87.68 77.62 89.28
Higher-Order, ch cmi cmo 88.04 78.09 89.59
Table 2: Labeled attachment scores on validation data
(?10,000 tokens per language), for different models that ex-
ploit increasing orders of factorizations.
3.1 Impact of Higher-Order Factorization
Our first set of experiments looks at the performance
of different factorizations. We selected three lan-
guages with a large number of training sentences,
namely Catalan, Czech and English. To evaluate
models, we held out the training sentences that cover
the first 10,000 tokens; the rest was used for training.
We compared four models at increasing orders of
factorizations. The first is a first-order model. The
second model is similar to that of McDonald and
Pereira (2006): a factor consists of a main labeled
dependency and the head child closest to the mod-
ifier (ch). The third model incorporates the modi-
fier child outside the main dependency in the fac-
torization (cmo). Finally, the last model incorpo-
rates the modifier child inside the dependency span
(cmi), thus corresponding to the complete higher-
order model presented in the previous section.
Table 2 shows the accuracies of the models on
validation data. Each model was trained for up to
10 epochs, and evaluated at the end of each epoch;
we report the best accuracy of these evaluations.
Clearly, the accuracy increases as the factors in-
clude richer information in terms of second-order
relations. The richest model obtains the best accu-
racy in the three languages, being much better than
that of the first-order model. The table also reports
the accuracy of an unaveraged first-order model, il-
lustrating the benefits of parameter averaging.
3.2 Results on the Multilingual Track
We trained a higher-order model for each language,
using the averaged perceptron. In the experiments
presented above we observed that the algorithm
does not over-fit, and that after two or three train-
ing epochs only small variations in accuracy occur.
Based on this fact, we designed a criterion to train
models: we ran the training algorithm for up to three
training test
sent./min. mem. UAS LAS
Arabic 1.21 1.8GB 81.48 70.20
Basque 33.15 1.2GB 81.08 75.73
Catalan 5.50 1.7GB 92.46 87.60
Chinese 1461.66 60MB 86.20 80.86
Czech 18.19 1.8GB 85.16 78.60
English 15.57 1.0GB 90.63 89.61
Greek 8.10 250MB 81.37 73.56
Hungarian 5.65 1.6GB 79.92 75.42
Italian 12.44 900MB 87.19 83.46
Turkish 116.55 600MB 82.41 75.85
Average - - 84.79 79.09
Table 3: Performance of the higher-order projective models
on the multilingual track of the CoNLL-2007 task. The first two
columns report the speed (in sentences per minute) and mem-
ory requirements of the training algorithm?these evaluations
were made on the first 1,000 training sentences with a Dual-
Core AMD OpteronTM Processor 256 at 1.8GHz with 4GB of
memory. The last two columns report unlabelled (UAS) and
labelled (LAS) attachment scores on test data.
days of computation, or a maximum of 15 epochs.
For Basque, Chinese and Turkish we could complete
the 15 epochs. For Arabic and Catalan, we could
only complete 2 epochs. Table 3 reports the perfor-
mance of the higher-order projective models on the
ten languages of the multilingual track.
4 Conclusion
We have presented dependency parsing models that
exploit higher-order factorizations of trees. Such
factorizations allow the definition of second-order
features associated with sibling and grand-parental
relations. For some languages, our models obtain
state-of-the-art results.
One drawback of our approach is that the infer-
ence algorithms for higher-order models are very ex-
pensive. For languages with many dependency la-
bels or long sentences, training and parsing becomes
impractical for current machines. Thus, a promising
line of research is the investigation of methods to
efficiently incorporate higher-order relations in dis-
criminative parsing.
Acknowledgments
I am grateful to Terry Koo, Amir Globerson and Michael
Collins for their helpful comments relating this work, and to the
anonymous reviewers for their suggestions. A significant part
of the system and the code was based on my previous system in
the CoNLL-X task, developed with Mihai Surdeanu and Llu??s
Ma`rquez at the UPC. The author was supported by the Catalan
Ministry of Innovation, Universities and Enterprise.
960
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. Diaz
de Ilarraza, A. Garmendia, and M. Oronoz. 2003. Con-
struction of a Basque dependency treebank. In Proc. of the
2nd Workshop on Treebanks and Linguistic Theories (TLT),
pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The
PDT: a 3-level annotation scenario. In Abeille? (Abeille?,
2003), chapter 7, pages 103?127.
X. Carreras, M. Surdeanu, and L. Ma`rquez. 2006. Projective
dependency parsing with perceptron. In Proc. CoNLL-X.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and
Z. Gao. 2003. Sinica treebank: Design criteria, representa-
tional issues and implementation. In Abeille? (Abeille?, 2003),
chapter 13, pages 231?248.
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In Proc. of EMNLP-2002.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005. The
Szeged Treebank. Springer.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. COLING.
J. Eisner. 2000. Bilexical grammars and their cubic-time pars-
ing algorithms. In H. C. Bunt and A. Nijholt, editors, New
Developments in Natural Language Parsing, pages 29?62.
Kluwer Academic Publishers.
Y. Freund and R. E. Schapire. 1999. Large margin classifi-
cation using the perceptron algorithm. Machine Learning,
37(3):277?296.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic
Language Resources and Tools, pages 110?117.
R. Johansson and P. Nugues. 2007. Extended constituent-to-
dependency conversion for English. In Proc. of the 16th
Nordic Conference on Computational Linguistics (NODAL-
IDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. of EACL-
2006.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-
margin training of dependency parsers. In Proc. ACL.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, O. Coraz-
zari, A. Lenci, A. Zampolli, F. Fanciulli, M. Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino, F. Zan-
zotto, N. Nana, F. Pianesi, and R. Delmonte. 2003. Build-
ing the Italian Syntactic-Semantic Treebank. In Abeille?
(Abeille?, 2003), chapter 11, pages 189?210.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of EMNLP-CoNLL.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In Abeille? (Abeille?, 2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papageor-
giou, and S. Piperidis. 2005. Theoretical and practical is-
sues in the construction of a Greek dependency treebank. In
Proc. of the 4th Workshop on Treebanks and Linguistic The-
ories (TLT), pages 149?160.
A. Ratnaparkhi, J. Reinar, and S. Roukos. 1994. A maximum
entropy model for prepositional phrase attachment. In Proc.
of the ARPA Workshop on Human Language Technology.
961
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 200?209,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Non-Projective Parsing for Statistical Machine Translation
Xavier Carreras Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{carreras,mcollins}@csail.mit.edu
Abstract
We describe a novel approach for syntax-
based statistical MT, which builds on a
variant of tree adjoining grammar (TAG).
Inspired by work in discriminative depen-
dency parsing, the key idea in our ap-
proach is to allow highly flexible reorder-
ing operations during parsing, in combina-
tion with a discriminative model that can
condition on rich features of the source-
language string. Experiments on trans-
lation from German to English show im-
provements over phrase-based systems,
both in terms of BLEU scores and in hu-
man evaluations.
1 Introduction
Syntax-based models for statistical machine trans-
lation (SMT) have recently shown impressive re-
sults; many such approaches are based on ei-
ther synchronous grammars (e.g., (Chiang, 2005)),
or tree transducers (e.g., (Marcu et al, 2006)).
This paper describes an alternative approach for
syntax-based SMT, which directly leverages meth-
ods from non-projective dependency parsing. The
key idea in our approach is to allow highly flexible
reordering operations, in combination with a dis-
criminative model that can condition on rich fea-
tures of the source-language input string.
Our approach builds on a variant of tree adjoin-
ing grammar (TAG; (Joshi and Schabes, 1997))
(specifically, the formalism of (Carreras et al,
2008)). The models we describe make use of
phrasal entries augmented with subtrees that pro-
vide syntactic information in the target language.
As one example, when translating the sentence
wir mu?ssen auch diese kritik ernst nehmen from
German into English, the following sequence of
syntactic phrasal entries might be used (we show
each English syntactic fragment above its associ-
ated German sub-string):
S
NP
we
VP
must ADVP
also
NP
these criticisms
ADVP
seriously
VP
take
wir mu?ssen auch diese kritik ernst nehmen
TAG parsing operations are then used to combine
these fragments into a full parse tree, giving the
final English translation we must also take these
criticisms seriously.
Some key aspects of our approach are as fol-
lows:
? We impose no constraints on entries in the
phrasal lexicon. The method thereby retains the
full set of lexical entries of phrase-based systems
(e.g., (Koehn et al, 2003)).1
? The model allows a straightforward integra-
tion of lexicalized syntactic language models?for
example the models of (Charniak, 2001)?in addi-
tion to a surface language model.
? The operations used to combine tree frag-
ments into a complete parse tree are signifi-
cant generalizations of standard parsing operations
found in TAG; specifically, they are modified to be
highly flexible, potentially allowing any possible
permutation (reordering) of the initial fragments.
As one example of the type of parsing opera-
tions that we will consider, we might allow the
tree fragments shown above for these criticisms
and take to be combined to form a new structure
with the sub-string take these criticisms. This step
in the derivation is necessary to achieve the correct
English word order, and is novel in a couple of re-
spects: first, these criticisms is initially seen to the
left of take, but after the adjunction this order is
reversed; second, and more unusually, the treelet
for seriously has been skipped over, with the re-
sult that the German words translated at this point
(diese, kritik, and nehmen) form a non-contiguous
sequence. More generally, we will allow any two
1Note that in the above example each English phrase con-
sists of a completely connected syntactic structure; this is not,
however, a required constraint, see section 3.2 for discussion.
200
tree fragments to be combined during the transla-
tion process, irrespective of the reorderings which
are introduced, or the non-projectivity of the pars-
ing operations that are required.
The use of flexible parsing operations raises two
challenges that will be a major focus of this paper.
First, these operations will allow the model to cap-
ture complex reordering phenomena, but will in
addition introduce many spurious possibilities. In-
spired by work in discriminative dependency pars-
ing (e.g., (McDonald et al, 2005)), we add proba-
bilistic constraints to the model through a discrim-
inative model that links lexical dependencies in the
target language to features of the source language
string. We also investigate hard constraints on the
dependency structures that are created during pars-
ing. Second, there is a need to develop efficient
decoding algorithms for the models. We describe
approximate search methods that involve a signif-
icant extension of decoding algorithms originally
developed for phrase-based translation systems.
Experiments on translation from German to En-
glish show a 0.5% improvement in BLEU score
over a phrase-based system. Human evaluations
show that the syntax-based system gives a sig-
nificant improvement over the phrase-based sys-
tem. The discriminative dependency model gives
a 1.5% BLEU point improvement over a basic
model that does not condition on the source lan-
guage string; the hard constraints on dependency
structures give a 0.8% BLEU improvement.
2 Relationship to Previous Work
A number of syntax-based translation systems
have framed translation as a parsing problem,
where search for the most probable translation is
achieved using algorithms that are generalizations
of conventional parsing methods. Early examples
of this work include (Alshawi, 1996; Wu, 1997);
more recent models include (Yamada and Knight,
2001; Eisner, 2003; Melamed, 2004; Zhang and
Gildea, 2005; Chiang, 2005; Quirk et al, 2005;
Marcu et al, 2006; Zollmann and Venugopal,
2006; Nesson et al, 2006; Cherry, 2008; Mi et
al., 2008; Shen et al, 2008). The majority of
these methods make use of synchronous gram-
mars, or tree transducers, which operate over parse
trees in the source and/or target languages. Re-
ordering rules are typically specified through rota-
tions or transductions stated at the level of context-
free rules, or larger fragments, within parse trees.
These rules can be learned automatically from cor-
pora.
A critical difference in our work is to allow
arbitrary reorderings of the source language sen-
tence (as in phrase-based systems), through the
use of flexible parsing operations. Rather than
stating reordering rules at the level of source or
target language parse trees, we capture reorder-
ing phenomena using a discriminative dependency
model. Other factors that distinguish us from pre-
vious work are the use of all phrases proposed by a
phrase-based system, and the use of a dependency
language model that also incorporates constituent
information (although see (Charniak et al, 2003;
Shen et al, 2008) for related approaches).
3 A Syntactic Translation Model
3.1 Background
Our work builds on the variant of tree adjoin-
ing grammar (TAG) introduced by (Carreras et
al., 2008). In this formalism the basic units
in the grammar are spines, which associate tree
fragments with lexical items. These spines can
be combined using a sister-adjunction operation
(Rambow et al, 1995), to form larger pieces of
structure.2 For example, we might have the fol-
lowing operation:
NP
there
S
VP
is
? S
NP
there
VP
is
In this case the spine for there has sister-adjoined
into the S node in the spine for is; we re-
fer to the spine for there as being the modifier
spine, and the spine for is being the head spine.
There are close connections to dependency for-
malisms: in particular in this operation we see
a lexical dependency between the modifier word
there and the head word is. It is possible to de-
fine syntactic language models, similar to (Char-
niak, 2001), which associate probabilities with
these dependencies, roughly speaking of the form
P (w
m
, s
m
|w
h
, s
h
, pos, ?), where w
m
and s
m
are
the identities of the modifier word and spine, w
h
and s
h
are the identities of the head word and
spine, pos is the position in the head spine that is
being adjoined into, and ? is some additional state
(e.g., state that tracks previous modifiers that have
adjoined into the same spine).
2We also make use of the r-adjunction operation defined in
(Carreras et al, 2008), which, together with sister-adjunction,
allows us to model the full range of structures found in the
Penn treebank.
201
SNP
there
VP
is NP
NPB
no hierarchy
PP
of NP
discrimination
es gibt keine hierarchie der diskriminierung
Figure 1: A training example consisting of an English (tar-
get language) tree and a German (source language) sentence.
In this paper we will also consider treelets,
which are a generalization of spines, and which
allow lexical entries that include more than one
word. These treelets can again be combined us-
ing a sister-adjunction operation. As an example,
consider the following operation:
VP
be ADJP
able
SG
to VP
respond
? VP
be ADJP
able SG
to VP
respond
In this case the treelet for to respond sister-adjoins
into the treelet for be able. This operation intro-
duces a bi-lexical dependency between the modi-
fier word to and the head word able.
3.2 S-phrases
This section describes how phrase entries from
phrase-based translation systems can be modified
to include associated English syntactic structures.
These syntactic phrase-entries (from here on re-
ferred to as ?s-phrases?) will form the basis of the
translation models that we describe.
We extract s-phrases from training examples
consisting of a source-language string paired with
a target-language parse tree. For example, con-
sider the training example in figure 1. We as-
sume some method that enumerates a set of pos-
sible phrase entries for each training example:
each phrase entry is a pair ?(i, j), (k, l)? speci-
fying that source-language words f
i
. . . f
j
corre-
spond to target-language words e
k
. . . e
l
in the ex-
ample. For example, one phrase entry for the ex-
ample might be ?(1, 2), (1, 2)?, representing the
pair ?es gibt ? there is?. In our experiments
we use standard methods in phrase-based systems
(Koehn et al, 2003) to define the set of phrase en-
tries for each sentence in training data.
es gibt keine hierarchie der
S
NP
there
VP
is
DT
no
NP
NPB
hierarchy
PP
of
Figure 2: Example syntactic phrase entries. We show Ger-
man sub-strings above their associated sequence of treelets.4
For each phrase entry, we add syntactic infor-
mation to the English string. To continue our ex-
ample, the resulting entry would be as follows:
es gibt ? S
NP
there
VP
is
To give a more formal description of how syn-
tactic structures are derived for phrases, first note
that each parse tree t is mapped to a TAG deriva-
tion using the method described in (Carreras et al,
2008). This procedure uses the head finding rules
of (Collins, 1997). The resulting derivation con-
sists of a TAG spine for each word seen in the sen-
tence, together with a set of adjunction operations
which each involve a modifier spine and a head
spine. Given an English string e = e
1
. . . e
n
, with
an associated parse tree t, the syntactic structure
associated with a substring e
k
. . . e
l
(e.g., there is)
is then defined as follows:
? For each word in the English sub-string, in-
clude its associated TAG spine in t.
? In addition, include any adjunction operations
in t where both the head and modifier word are in
the sub-string e
j
. . . e
k
.
In the above example, the resulting structure
(i.e., the structure for there is) is a single treelet.
In other cases, however, we may get a sequence of
treelets, which are disconnected from each other.
For example, another likely phrase-entry for this
training example is ?es gibt keine ? there is no?
resulting in the first lexical entry in figure 2, which
has two treelets. Allowing s-phrases with multiple
treelets ensures that all phrases used by phrase-
based systems can be used within our approach.
As a final step, we add additional align-
ment information to each s-phrase. Con-
sider an s-phrase which contains source-language
words f
1
. . . f
n
paired with target-language words
e
1
. . . e
m
. The alignment information is a vec-
tor ?(a
1
, b
1
) . . . (a
m
, b
m
)? that specifies for each
word e
i
its alignment to words f
a
i
. . . f
b
i
in the
source language. For example, for the phrase en-
202
try ?es gibt ? there is? a correct alignment would
be ?(1, 1), (2, 2)?, specifying that there is aligned
to es, and is is aligned to gibt (note that in many,
but not all, cases a
i
= b
i
, i.e., a target language
word is aligned to a single source language word).
The alignment information in s-phrases will
be useful in tying syntactic dependencies cre-
ated in the target language to positions in the
source language string. In particular, we will con-
sider discriminative models (analogous to models
for dependency parsing, e.g., see (McDonald et
al., 2005)) that estimate the probability of target-
language dependencies conditioned on properties
of the source-language string. Alignments may be
derived in a number of ways; in our method we
directly use phrase entries proposed by a phrase-
based system. Specifically, for each target word e
i
in a phrase entry ?f
1
. . . f
n
, e
1
. . . e
m
? for a train-
ing example, we find the smallest5 phrase entry
in the same training example that includes e
i
on
the target side, and is a subset of f
1
. . . f
n
on the
source side; the word e
i
is then aligned to the sub-
set of source language words in this ?minimal?
phrase.
In conclusion, s-phrases are defined as follows:
Definition 1 An s-phrase is a 4-tuple ?f, e, t, a?
where: f is a sequence of foreign words; e is
a sequence of English words; t is a sequence of
treelets specifying a TAG spine for each English
word, and potentially some adjunctions between
these spines; and a is an alignment. For an s-
phrase q we will sometimes refer to the 4 elements
of q as f(q), e(q), t(q) and a(q).
3.3 The Model
We now introduce a model that makes use of s-
phrases, and which is flexible in the reorderings
that it allows. To provide some intuition, and some
motivation for the use of reordering operations,
figure 3 gives several examples of German strings
which have different word orders from English.
The crucial idea will be to use TAG adjunction
operations to combine treelets to form a complete
parse tree, but with a complete relaxation on the
order in which the treelets are combined. For ex-
ample, consider again the example given in the
introduction to this paper. In the first step of a
derivation that builds on these treelets, the treelet
5The ?size? of a phrase entry is defined to be n
s
+ n
t
where n
s
is the number of source language words in the
phrase, n
t
is the number of target language words.
1(a) [die verwaltung] [muss] [ku?nftig] [schneller] [reagieren]
[ko?nnen] 1(b) the administration must be able to respond
more quickly in future
1(c) NP
the
admin. . .
S
VP
must
PP
in future
ADVP
more
quickly
SG
to VP
respond
VP
be ADJP
able
2(a) [meiner ansicht nach] [darf] [der erweiterungsprozess]
[nicht] [unno?tig] [verzo?gert] [werden] 2(b) in my opinion the
expansion process should not be delayed unnecessarily
2(c) PP
in my
opinion
S
VP
should
NP
the . . . process
RB
not
ADVP
unnecessarily
VP
delayed
VP
be
Figure 3: Examples of translations. In each example (a)
is the original German string, with a possible segmentation
marked with ?[? and ?]?; (b) is a translation for (a); and (c)
is a sequence of phrase entries, including syntactic structures,
for the segmentation given in (a).
for these criticisms might adjoin into the treelet for
take, giving the following new sequence:
S
NP
we
VP
must ADVP
also
ADVP
seriously
VP
V
take
NP
these criticisms
In the next derivation step seriously is adjoined to
the right of take, giving the following treelets:
S
NP
we
VP
must ADVP
also
VP
V
take
NP
these criticisms
ADVP
seriously
In the final step the second treelet adjoins into the
VP above must, giving a parse tree for the string
we must also take these criticisms seriously, and
completing the translation.
Formally, given an input sentence f , a derivation
d is a pair ?q, pi? where:
? q = q
1
. . . q
n
is a sequence of s-phrases such
that f = f(q
1
)?f(q
2
)? . . .?f(q
n
) (where u?v
denotes the concatenation of strings u and v).
? pi is a set of adjunction operations that
connects the sequence of treelets contained in
?t(q
1
), t(q
2
), . . . , t(q
n
)? into a parse tree in the
target language. The operations allow a com-
plete relaxation of word order, potentially allow-
ing any of the n! possible orderings of the n s-
phrases. We make use of both sister-adjunction
and r-adjunction operations, as defined in (Car-
reras et al, 2008).6
6In principle we allow any treelet to adjoin into any other
treelet?for example there are no hard, grammar-based con-
straints ruling out the combination of certain pairs of non-
terminals. Note however that in some cases operations will
have probability 0 under the syntactic language model intro-
duced later in this section.
203
DT
no
NP
NPB
hierarchy
PP
of
NP
discrimination
? NP
NPB
hierarchy
PP
of
NP
DT
no
discrimination
Figure 4: A spurious derivation step. The treelets arise
from [keine] [hierarchie der] [diskriminierung].
Given a derivation d = ?q, pi?, we define e(d)
to be the target-language string defined by the
derivation, and t(d) to be the complete target-
language parse tree created by the derivation. The
most likely derivation for a foreign sentence f
is argmax
d?G(f)
score(d), where G(f) is the set
of possible derivations for f , and the score for a
derivation is defined as7
score(d) = score
LM
(e(d)) + score
SY N
(t(d))
+ score
R
(d) +
n
?
j=1
score
P
(q
j
) (1)
The components of the model are as follows:
? score
LM
(e(d)) is the log probability of the
English string under a trigram language model.
? score
SY N
(t(d)) is the log probability of the
English parse tree under a syntactic language
model, similar to (Charniak, 2001), that associates
probabilities with lexical dependencies.
? score
R
(d) will be used to score the pars-
ing operations in pi, based on the source-language
string and the alignments in the s-phrases. This
part of the model is described extensively in sec-
tion 4.1 of this paper.
? score
P
(q) is the score for an s-phrase q.
This score is a log-linear combination of var-
ious features, including features that are com-
monly found in phrase-based systems: for exam-
ple logP (f(q)|e(q)), log P (e(q)|f(q)), and lex-
ical translation probabilities. In addition, we in-
clude a feature logP (t(q)|f(q), e(q)), which cap-
tures the probability of the phrase in question hav-
ing the syntactic structure t(q).
Note that a model that includes the terms
score
LM
(e(d)) and
?
n
j=1
score
P
(q
j
) alone
would essentially be a basic phrase-based
model (with no distortion terms). The terms
score
SY N
(t(d)) and score
R
(d) add syntactic
information to this basic model.
A key motivation for this model is the flexibility
of the reordering operations that it allows. How-
ever, the approach raises two major challenges:
7In practice, MERT training (Och, 2003) will be used to
train relative weights for the different model components.
Constraints on reorderings. Relaxing the op-
erations in the parsing model will allow complex
reorderings to be captured, but will also introduce
many spurious possibilities. As one example, con-
sider the derivation step shown in figure 4. This
step may receive a high probability from a syntac-
tic or surface language model?no discrimination
is a quite plausible NP in English?but it should
be ruled out for other reasons, for example be-
cause it does not respect the dependencies in the
original German (i.e., keine/no is not a modifier
to diskriminierung/discrimination in the German
string). The challenge will be to develop either
hard constraints which rule out spurious derivation
steps such as these, or soft constraints, encapsu-
lated in score
R
(d), which penalize them.
Efficient search. Exact search for the derivation
which maximizes the score in Eq. 1 cannot be
accomplished efficiently using dynamic program-
ming (as in phrase-based systems, it is easy to
show that the decoding problem is NP-complete).
Approximate search methods will be needed.
The next two sections of this paper describe so-
lutions to these two challenges.
4 Constraints on Reorderings
4.1 A Discriminative Dependency Model
We now describe the model score
R
introduced in
the previous section. Recall that pi specifies k ad-
junction operations that are used to build a full
parse tree, where k ? n is the number of treelets
within the sequence of s-phrases q = ?q
1
. . . q
n
?.
Each of the k adjunction operations creates a
dependency between a modifier word w
m
within
a phrase q
m
, and a head word w
h
within a phrase
q
h
. For example, in the example in section 3.3
where these criticisms was combined with take,
the modifier word is criticisms and the head word
is take. The modifier and head words have TAG
spines s
m
and s
h
respectively. In addition we can
define (a
m
, b
m
) to be the start and end indices of
the words in the foreign string to which the word
w
m
is aligned; this information can be recovered
because the s-phrase q
m
contains alignment infor-
mation for all target words in the phrase, includ-
ing w
m
. Similarly, we can define (a
h
, b
h
) to be
alignment information for the head word w
h
. Fi-
nally, we can define ? to be a binary flag speci-
fying whether or not the adjunction operation in-
volves reordering (in the take criticism example,
this flag is set to true, because the order in En-
204
VP
DT N
NP
N
criticismsthese take
nehmenernstwir mu?ssen auch diese kritik
Figure 5: An adjunction operation that involves the mod-
ifier criticisms and the head take. The phrases involved are
underlined; the dotted lines show alignments within s-phrases
between English words and positions in the German string.
The ?-dependency in this case includes the head and modi-
fier words, together with their spines, and their alignments to
positions in the German string (kritik and nehmen).
glish is reversed from that in German). This leads
to the following definition:
Definition 2 Given a derivation d = ?q, pi?, we
define ?(d) to be the set of ?-dependencies
in d. Each ?-dependency is a tuple
?w
m
, s
m
, a
m
, b
m
, w
h
, s
h
, a
h
, b
h
, ?? of elements as
described above.
Figure 5 gives an illustration of how an adjunction
creates one such ?-dependency.
The model is then defined as
score
R
(d) =
?
???(d)
score
r
(?, f)
where score
r
(?, f) is a score associated with the
?-dependency ?. This score can potentially be
sensitive to any information in ? or the source-
language string f ; in particular, note that the align-
ment indices (a
m
, b
m
) and (a
h
, b
h
) essentially
anchor the target-language dependency to posi-
tions in the source-language string, allowing the
score for the dependency to be based on features
that have been widely used in discriminative de-
pendency parsing, for example features based on
the proximity of the two positions in the source-
language string, the part-of-speech tags in the sur-
rounding context, and so on. These features have
been shown to be powerful in the context of regu-
lar dependency parsing, and our intent is to lever-
age them in the translation problem.
In our model, we define score
r
as follows. We
estimate a model P (y|?, f) where y ? {?1,+1},
and y = +1 indicates that a dependency does exist
between w
m
and w
h
, and y = ?1 indicates that a
dependency does not exist. We then define
score
r
(?, f) = logP (+1|?, f)
To estimate P (y|?, f), we first extract a set of la-
beled training examples of the form ?y
i
, ?
i
, f
i
? for
i = 1 . . . N from our training data as follows:
for each pair of target-language words (w
m
, w
h
)
seen in the training data, we can extract associ-
ated spines (s
m
, s
h
) from the relevant parse tree,
and also extract a label y indicating whether or not
a head-modifier dependency is seen between the
two words in the parse tree. Given an s-phrase in
the training example that includes w
m
, we can ex-
tract alignment information (a
m
, b
m
) from the s-
phrase; we can extract similar information (a
h
, b
h
)
for w
h
. The end result is a training example of the
form ?y, ?, f?.8 We then estimate P (y|?, f) using
a simple backed-off model that takes into account
the identity of the two spines, the value for the flag
r, the distance between (a
m
, b
m
) and (a
h
, b
h
), and
part-of-speech information in the source language.
4.2 Contiguity of pi-Constituents
We now describe a second type of constraint,
which limits the amount of non-projectivity in
derivations. Consider again the k adjunction op-
erations in pi, which are used to connect treelets
into a full parse tree. Each adjunction operation
involves a head treelet that dominates a modifier
treelet. Thus for any treelet t, we can consider its
descendants, that is, the entire set of treelets that
are directly or indirectly dominated by t. We de-
fine a pi-constituent for treelet t to be the subset
of source-language words dominated by t and its
descendants. We then introduce the following con-
straint on pi-constituents:
Definition 3 (pi-constituent constraint.) A pi-
constituent is contiguous iff it consists of a con-
tiguous sequence of words in the source language.
A derivation pi satisfies the pi-constituent con-
straint iff all pi-constituents that it contains are
contiguous.
In this paper we constrain all derivations to sat-
isfy the pi-constituent constraint (future work may
consider probabilistic versions of the constraint).
The intuition behind the constraint deserves
more discussion. The constraint specifies that the
modifiers to each treelet can appear in any or-
der around the treelet, with arbitrary reorderings
or non-projective operations. However, once a
treelet has taken all its modifiers, the resulting pi-
constituent must form a contiguous sub-sequence
8To be precise, there may be multiple (or even zero) s-
phrases which include w
m
or w
h
, and these s-phrases may
include conflicting alignment information. Given n
m
differ-
ent alignments seen for w
m
, and n
h
different alignments seen
for w
h
, we create n
m
?n
h
training examples, which include
all possible combinations of alignments.
205
of the source-language string. As one set of exam-
ples, consider the translations in figure 3, and the
example given in the introduction. These exam-
ples involve reordering of arguments and adjuncts
within clauses, a very common case of reordering
in translation from German to English. The re-
orderings in these translations are quite flexible,
but in all cases satisfy the pi-constituent constraint.
As an illustration of a derivation that violates
the constraint, consider again the derivation step
shown in figure 4. This step has formed a par-
tial hypothesis, no discrimination, which corre-
sponds to the German words keine and diskrim-
inierung, which do not form a contiguous sub-
string in the German. Consider now a complete
derivation, which derives the string there is hier-
archy of no discrimination, and which includes the
pi-constituent no discrimination shown in the fig-
ure (i.e., where the treelet discrimination takes no
as its only modifier). This derivation will violate
the pi-constituent constraint.9
5 Decoding
We now describe decoding algorithms for the syn-
tactic models: we first describe inference rules
that are used to combine pieces of structure, and
then describe heuristic search algorithms that use
these inference rules. Throughout this section,
for brevity and simplicity, we describe algorithms
that apply under the assumption that each s-phrase
has a single associated treelet. The generalization
to the case where an s-phrase may have multiple
treelets is discussed in section 5.3.
5.1 Inference Rules
Parsing operations for the TAG grammars de-
scribed in (Carreras et al, 2008) are based on
the dynamic programming algorithms in (Eisner,
2000). A critical idea in dynamic programming al-
gorithms such as these is to associate constituents
in a chart with spans of the input sentence, and
to introduce inference rules that combine con-
stituents into larger pieces of structure. The crucial
step in generalizing these algorithms to the non-
projective case, and to translation, will be to make
use of bit-strings that keep track of which words in
the German have already been translated in a chart
entry. To return to the example from the intro-
duction, again assume that the selected s-phrases
9Note, however, that the derivation step show in figure 4
will be considered in the search, because if discrimination
takes additional modifiers, and thereby forms a pi-constituent
that dominates a contiguous sub-string in the German, then
the resulting derivation will be valid.
0. Data structures: Q
i
for i = 1 . . . n is a set of hypotheses
for each length i, S is a set of chart entries
1. S ? ?
2. Initialize Q
1
. . .Q
n
with basic chart entries derived
from phrase entries
3. For i = 1 . . . n
4. For any A ? BEAM(Q
i
)
5. If S contains a chart entry with the same signature
as A, and which has a higher inside score,
6. continue
7. Else
8. Add A to S
9. For any chart entry C that can be derived from
A together with another chart entry B ? S ,
add C to the set Q
j
where j = length(C)
10. ReturnQ
n
, a set of items of length n
Figure 6: A beam search algorithm. A dynamic-
programming signature consists of the regular dynamic-
programming state for the parsing algorithm, together with
the span (bit-string) associated with a constituent.
segment the German input into [wir mu?ssen auch]
[diese kritik] [ernst] [nehmen], and the treelets are
as shown in the introduction. Each of these treelets
will form a basic entry in the chart, and will have
an associated bit-string indicating which German
words have been translated by that entry.
These basic chart entries can then be combined
to form larger pieces of structure. For example,
the following inferential step is possible:
NP/0001100
these criticisms
VP/0000001
V
take
? VP/0001101
V
take
NP
these criticisms
We have shown the bit-string representation for
each consituent: for example, the new constituent
has the bit-string 0001101 representing the fact
that the non-contiguous sub-strings diese kritik
and nehmen have been translated at this point. Any
two constituents can be combined, providing that
the logical AND of their bit-strings is all 0?s.
Inference steps such as that shown above will
have an associated score corresponding to the
TAG adjunction that is involved: in our mod-
els, both score
SY N
and score
R
will contribute to
this score. In addition, we add state?specifically,
word bigrams at the start and end of constituents?
that allows trigram language model scores to be
calculated as constituents are combined.
5.2 Approximate Search
There are 2n possible bit-strings for a sentence of
length n, hence the search space is of exponen-
tial size; approximate algorithms are therefore re-
quired in search for the highest scoring derivation.
Figure 6 shows a beam search algorithm which
makes use of the inference rules described in the
206
previous section. The algorithm stores sets Q
i
for i = 1 . . . n, where n is the source-language
sentence length; each set Q
i
stores hypotheses of
length i (i.e., hypotheses with an associated bit-
string with i ones). These sets are initialized with
basic entries derived from s-phrases.
The function BEAM(Q
i
) returns all items
within Q
i
that have a high enough score to fall
within a beam (more details for BEAM are given
below). At each iteration (step 4), each item in
turn is taken from BEAM(Q
i
) and added to a
chart; the inference rules described in the previ-
ous section are used to derive new items which are
added to the appropriate set Q
j
, where j > i.
We have found the definition of BEAM(Q
i
) to
be critical to the success of the method. As a first
step, each item in Q
i
receives a score that is a sum
of an inside score (the cost of all derivation steps
used to create the item) and a future score (an esti-
mate of the cost to complete the translation). The
future score is based on the source-language words
that are still to be translated?this can be directly
inferred from the item?s bit-string?this is similar
to the use of future scores in Pharoah (Koehn et al,
2003), and in fact we use Pharoah?s future scores
in our model. We then give the following defini-
tion, where N is a parameter (the beam size):
Definition 4 (BEAM) Given Q
i
, define Q
i,j
for
j = 1 . . . n to be the subset of items in Q
i
which
have their j?th bit equal to one (i.e., have the j?th
source language word translated). Define Q?
i,j
to
be the N highest scoring elements in Q
i,j
. Then
BEAM(Q
i
) = ?
n
j=1
Q
?
i,j
.
To motivate this definition, note that a naive
method would simply define BEAM(Q
i
) to be
the N highest scoring elements of Q
i
. This def-
inition, however, assumes that constituents which
form translations of different parts of a sentence
have scores that can be compared?an assumption
that would be true if the future scores were highly
accurate, but which quickly breaks down when fu-
ture scores are inaccurate. In contrast, the defi-
nition above ensures that the top N analyses for
each of the n source language words are stored at
each stage, and hence that all parts of the source
sentence are well represented. In experiments, the
naive approach was essentially a failure, with pars-
ing of some sentences either failing or being hope-
lessly inefficient, depending on the choice of N .
In contrast, definition 4 gives good results.
System BLEU score
Syntax-based 25.2
Syntax (no Score
R
) 23.7 (-1.5)
Syntax (no pi-c constraint) 24.4 (-0.8)
Table 1: Development set results showing the effect of re-
moving Score
R
or the pi-constituent constraint.
5.3 Allowing Multiple Treelets per s-Phrase
The decoding algorithms that we have described
apply in the case where each s-phrase has a sin-
gle treelet. The extension of these algorithms
to the case where a phrase may have multiple
treelets (e.g., see figure 2) is straightforward, but
for brevity the details are omitted. The basic idea
is to extend bit-string representations with a record
of ?pending? treelets which have not yet been in-
cluded in a derivation. It is also possible to enforce
the pi-constituent constraint during decoding, as
well as a constraint that ensures that reordering op-
erations do not ?break apart? English sub-strings
within s-phrases that have multiple treelets (for ex-
ample, for the s-phrase in figure 2, we ensure that
there is no remains as a contiguous sequence of
words in any translation using this s-phrase).
6 Experiments
We trained the syntax-based system on 751,088
German-English translations from the Europarl
corpus (Koehn, 2005). A syntactic language
model was also trained on the English sentences
in the training data. We used Pharoah (Koehn et
al., 2003) as a baseline system for comparison; the
s-phrases used in our system include all phrases,
with the same scores, as those used by Pharoah,
allowing a direct comparison. For efficiency rea-
sons we report results on sentences of length 30
words or less.10 The syntax-based method gives
a BLEU (Papineni et al, 2002) score of 25.04,
a 0.46 BLEU point gain over Pharoah. This re-
sult was found to be significant (p = 0.021) under
the paired bootstrap resampling method of Koehn
(2004), and is close to significant (p = 0.058) un-
der the sign test of Collins et al (2005).
Table 1 shows results for the full syntax-based
system, and also results for the system with the
discriminative dependency scores (see section 4.1)
and the pi-contituent constraint removed from the
system. In both cases we see a clear impact of
these components of the model, with 1.5 and 0.8
BLEU point decrements respectively.
10Both Pharoah and our system have weights trained using
MERT (Och, 2003) on sentences of length 30 words or less,
to ensure that training and test conditions are matched.
207
R: in our eyes , the opportunity created by this directive of introducing longer buses on international routes is efficient .
S: the opportunity now presented by this directive is effective in our opinion , to use long buses on international routes .
P: the need for this directive now possibility of longer buses on international routes to is in our opinion , efficiently .
R: europe and asia must work together to intensify the battle against drug trafficking , money laundering , international
crime , terrorism and the sexual exploitation of minors .
S: europe and asia must work together in order to strengthen the fight against drug trafficking , money laundering , against
international crime , terrorism and the sexual exploitation of minors .
P: europe and asia must cooperate in the fight against drug trafficking , money laundering , against international crime ,
terrorism and the sexual exploitation of minors strengthened .
R: equally important for the future of europe - at biarritz and later at nice - will be the debate on the charter of fundamental
rights .
S: it is equally important for the future of europe to speak on the charter of fundamental rights in biarritz , and then in nice .
P: just as important for the future of europe , it will be in biarritz and then in nice on the charter of fundamental rights to
speak .
R: the convention was thus a muddled system , generating irresponsibility , and not particularly favourable to well-ordered
democracy .
S: therefore , the convention has led to a system of a promoter of irresponsibility of the lack of clarity and hardly coincided
with the rules of a proper democracy .
P: the convention therefore led to a system of full of lack of clarity and hardly a promoter of the irresponsibility of the rules
of orderly was a democracy .
Figure 7: Examples where both annotators judged the syntactic system to give an improved translation when compared to
the baseline system. 51 out of 200 translations fall into this category. These examples were chosen at random from these 51
examples. R is the human (reference) translation; S is the translation from the syntax-based system; P is the output from the
baseline (phrase-based) system.
Syntax PB = Total
Syntax 51 3 7 61
PB 1 25 11 37
= 21 14 67 102
Total 73 42 85 200
Table 2: Human annotator judgements. Rows show re-
sults for annotator 1, and columns for annotator 2. Syntax
and PB show the number of cases where an annotator re-
spectively preferred/dispreferred the syntax-based system. =
gives counts of translations judged to be equal in quality.
In addition, we obtained human evaluations on
200 sentences chosen at random from the test data,
using two annotators. For each example, the ref-
erence translation was presented to the annota-
tor, followed by translations from the syntax-based
and phrase-based systems (in a random order). For
each example, each annotator could either decide
that the two translations were of equal quality, or
that one translation was better than the other. Ta-
ble 2 shows results of this evaluation. Both an-
notators show a clear preference for the syntax-
based system: for annotator 1, 73 translations are
judged to be better for the syntax-based system,
with 42 translations being worse; for annotator 2,
61 translations are improved with 37 being worse;
both annotators? results are statistically significant
with p < 0.05 under the sign test. Figure 7 shows
some translation examples where the syntax-based
system was judged to give an improvement.
7 Conclusions and Future Work
We have described a translation model that makes
use of flexible parsing operations, critical ideas
being the definition of s-phrases, ?-dependencies,
the pi-constituent constraint, and an approximate
search algorithm. A key area for future work
will be further development of the discriminative
dependency model (section 4.1). The model of
score
r
(?, f) that we have described in this paper is
relatively simple; in general, however, there is the
potential for score
r
to link target language depen-
dencies to arbitrary properties of the source lan-
guage string f (recall that ? contains a head and
modifier spine in the target language, along with
positions in the source-language string to which
these spines are aligned). For example, we might
introduce features that: a) condition dependencies
created in the target language on dependency re-
lations between their aligned words in the source
language; b) condition target-language dependen-
cies on whether they are aligned to words that
are in the same clause or segment in the source
language string; or, c) condition the grammatical
roles of nouns in the target language on grammat-
ical roles of aligned words in the source language.
These features should improve translation qual-
ity by giving a tighter link between syntax in the
source and target languages, and would be easily
incorporated in the approach we have described.
Acknowledgments We would like to thank Ryan Mc-
Donald for conversations that were influential in this work,
and Meg Aycinena Lippow and Ben Snyder for translation
judgments. This work was supported under the GALE pro-
gram of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022.
208
References
H. Alshawi. 1996. Head automata and bilingual tiling:
Translation with minimal representations. In Pro-
ceedings of ACL, pages 167?176.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming and the perceptron for efficient,
feature-rich parsing. In Proc. of CoNLL.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for machine transla-
tion. In Proceedings of MT Summit IX.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proceedings of ACL 2001.
C. Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of
ACL-08: HLT, pages 72?80, Columbus, Ohio, June.
Association for Computational Linguistics.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of ACL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics, pages 16?23, Madrid, Spain,
July. Association for Computational Linguistics.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In H. C. Bunt and A. Ni-
jholt, editors, New Developments in Natural Lan-
guage Parsing, pages 29?62. Kluwer Academic
Publishers.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of
ACL.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and K. Salomaa, ed-
itors, Handbook of Formal Languages, volume 3,
pages 169?124. Springer.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
HLT/NAACL.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of MT
Summit.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
Spmt: Statistical machine translation with syntac-
tified target language phrases. In Proceedings of
EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
D. Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based
translation. In Proceedings of ACL-08: HLT, pages
192?199. Association for Computational Linguis-
tics.
R. Nesson, S.M. Shieber, and A. Rush. 2006. In-
duction of probabilistic synchronous tree-insertion
grammars for machine translation. In Proceedings
of the 7th AMTA.
F.J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings of ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318.
Association for Computational Linguistics.
C. Quirk, A. Menezes, and Colin Cherry. 2005. De-
pendency tree translation: Syntactically informed
phrasal smt. In Proceedings of ACL.
O. Rambow, K. Vijay-Shanker, and D. Weir. 1995.
D-tree grammars. In Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 151?158, Cambridge, Mas-
sachusetts, USA, June. Association for Computa-
tional Linguistics.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of ACL.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized
inversion transduction grammar for alignment. In
Proceedings of ACL, pages 473?482.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proceedings of NAACL 2006 Workshop on Statisti-
cal Machine Translation.
209
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551?560,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
An Empirical Study of Semi-supervised Structured Conditional Models
for Dependency Parsing
Jun Suzuki, Hideki Isozaki
NTT CS Lab., NTT Corp.
Kyoto, 619-0237, Japan
jun@cslab.kecl.ntt.co.jp
isozaki@cslab.kecl.ntt.co.jp
Xavier Carreras, and Michael Collins
MIT CSAIL
Cambridge, MA 02139, USA
carreras@csail.mit.edu
mcollins@csail.mit.edu
Abstract
This paper describes an empirical study
of high-performance dependency parsers
based on a semi-supervised learning ap-
proach. We describe an extension of semi-
supervised structured conditional models
(SS-SCMs) to the dependency parsing
problem, whose framework is originally
proposed in (Suzuki and Isozaki, 2008).
Moreover, we introduce two extensions re-
lated to dependency parsing: The first ex-
tension is to combine SS-SCMs with an-
other semi-supervised approach, described
in (Koo et al, 2008). The second exten-
sion is to apply the approach to second-
order parsing models, such as those de-
scribed in (Carreras, 2007), using a two-
stage semi-supervised learning approach.
We demonstrate the effectiveness of our
proposed methods on dependency parsing
experiments using two widely used test
collections: the Penn Treebank for En-
glish, and the Prague Dependency Tree-
bank for Czech. Our best results on
test data in the above datasets achieve
93.79% parent-prediction accuracy for En-
glish, and 88.05% for Czech.
1 Introduction
Recent work has successfully developed depen-
dency parsing models for many languages us-
ing supervised learning algorithms (Buchholz and
Marsi, 2006; Nivre et al, 2007). Semi-supervised
learning methods, which make use of unlabeled
data in addition to labeled examples, have the po-
tential to give improved performance over purely
supervised methods for dependency parsing. It
is often straightforward to obtain large amounts
of unlabeled data, making semi-supervised ap-
proaches appealing; previous work on semi-
supervised methods for dependency parsing in-
cludes (Smith and Eisner, 2007; Koo et al, 2008;
Wang et al, 2008).
In particular, Koo et al (2008) describe a
semi-supervised approach that makes use of clus-
ter features induced from unlabeled data, and gives
state-of-the-art results on the widely used depen-
dency parsing test collections: the Penn Tree-
bank (PTB) for English and the Prague Depen-
dency Treebank (PDT) for Czech. This is a very
simple approach, but provided significant perfor-
mance improvements comparing with the state-
of-the-art supervised dependency parsers such as
(McDonald and Pereira, 2006).
This paper introduces an alternative method for
semi-supervised learning for dependency parsing.
Our approach basically follows a framework pro-
posed in (Suzuki and Isozaki, 2008). We extend it
for dependency parsing, which we will refer to as
a Semi-supervised Structured Conditional Model
(SS-SCM). In this framework, a structured condi-
tional model is constructed by incorporating a se-
ries of generative models, whose parameters are
estimated from unlabeled data. This paper de-
scribes a basic method for learning within this ap-
proach, and in addition describes two extensions.
The first extension is to combine our method with
the cluster-based semi-supervised method of (Koo
et al, 2008). The second extension is to apply the
approach to second-order parsing models, more
specifically the model of (Carreras, 2007), using
a two-stage semi-supervised learning approach.
We conduct experiments on dependency parsing
of English (on Penn Treebank data) and Czech (on
the Prague Dependency Treebank). Our experi-
ments investigate the effectiveness of: 1) the basic
SS-SCM for dependency parsing; 2) a combina-
tion of the SS-SCM with Koo et al (2008)?s semi-
supervised approach (even in the case we used the
same unlabeled data for both methods); 3) the two-
stage semi-supervised learning approach that in-
551
corporates a second-order parsing model. In ad-
dition, we evaluate the SS-SCM for English de-
pendency parsing with large amounts (up to 3.72
billion tokens) of unlabeled data .
2 Semi-supervised Structured
Conditional Models for Dependency
Parsing
Suzuki et al (2008) describe a semi-supervised
learning method for conditional random fields
(CRFs) (Lafferty et al, 2001). In this paper we
extend this method to the dependency parsing
problem. We will refer to this extended method
as Semi-supervised Structured Conditional Mod-
els (SS-SCMs). The remainder of this section de-
scribes our approach.
2.1 The Basic Model
Throughout this paper we will use x to denote an
input sentence, and y to denote a labeled depen-
dency structure. Given a sentence x with n words,
a labeled dependency structure y is a set of n de-
pendencies of the form (h,m, l), where h is the
index of the head-word in the dependency, m is
the index of the modifier word, and l is the label
of the dependency. We use h = 0 for the root of
the sentence. We assume access to a set of labeled
training examples, {x
i
,y
i
}
N
i=1
, and in addition a
set of unlabeled examples, {x
?
i
}
M
i=1
.
In conditional log-linear models for dependency
parsing (which are closely related to conditional
random fields (Lafferty et al, 2001)), a distribu-
tion over dependency structures for a sentence x
is defined as follows:
p(y|x) =
1
Z(x)
exp{g(x,y)}, (1)
where Z(x) is the partition function, w is a pa-
rameter vector, and
g(x,y) =
?
(h,m,l)?y
w ? f(x, h,m, l)
Here f(x, h,m, l) is a feature vector represent-
ing the dependency (h,m, l) in the context of the
sentence x (see for example (McDonald et al,
2005a)).
In this paper we extend the definition of g(x,y)
to include features that are induced from unlabeled
data. Specifically, we define
g(x,y) =
?
(h,m,l)?y
w ? f(x, h,m, l)
+
?
(h,m,l)?y
k
?
j=1
v
j
q
j
(x, h,m, l). (2)
In this model v
1
, . . . , v
k
are scalar parameters that
may be positive or negative; q
1
. . . q
k
are func-
tions (in fact, generative models), that are trained
on unlabeled data. The v
j
parameters will dictate
the relative strengths of the functions q
1
. . . q
k
, and
will be trained on labeled data.
For convenience, we will use v to refer to the
vector of parameters v
1
. . . v
k
, and q to refer to the
set of generative models q
1
. . . q
k
. The full model
is specified by values for w,v, and q. We will
write p(y|x;w,v,q) to refer to the conditional
distribution under parameter values w,v,q.
We will describe a three-step parameter estima-
tion method that: 1) initializes the q functions
(generative models) to be uniform distributions,
and estimates parameter values w and v from la-
beled data; 2) induces new functions q
1
. . . q
k
from
unlabeled data, based on the distribution defined
by the w,v,q values from step (1); 3) re-estimates
w and v on the labeled examples, keeping the
q
1
. . . q
k
from step (2) fixed. The end result is a
model that combines supervised training with gen-
erative models induced from unlabeled data.
2.2 The Generative Models
We now describe how the generative models
q
1
. . . q
k
are defined, and how they are induced
from unlabeled data. These models make direct
use of the feature-vector definition f(x,y) used in
the original, fully supervised, dependency parser.
The first step is to partition the d fea-
tures in f(x,y) into k separate feature vectors,
r
1
(x,y) . . . r
k
(x,y) (with the result that f is the
concatenation of the k feature vectors r
1
. . . r
k
). In
our experiments on dependency parsing, we parti-
tioned f into up to over 140 separate feature vec-
tors corresponding to different feature types. For
example, one feature vector r
j
might include only
those features corresponding to word bigrams in-
volved in dependencies (i.e., indicator functions
tied to the word bigram (x
m
, x
h
) involved in a de-
pendency (x, h,m, l)).
We then define a generative model that assigns
a probability
q
?
j
(x, h,m, l) =
d
j
?
a=1
?
r
j,a
(x,h,m,l)
j,a
(3)
to the d
j
-dimensional feature vector r
j
(x, h,m, l).
The parameters of this model are ?
j,1
. . . ?
j,d
j
;
552
they form a multinomial distribution, with the con-
straints that ?
j,a
? 0, and
?
a
?
j,a
= 1. This
model can be viewed as a very simple (naive-
Bayes) model that defines a distribution over fea-
ture vectors r
j
? R
d
j
. The next section describes
how the parameters ?
j,a
are trained on unlabeled
data.
Given parameters ?
j,a
, we can simply define the
functions q
1
. . . q
k
to be log probabilities under the
generative model:
q
j
(x, h,m, l) = log q
?
j
(x, h,m, l)
=
d
j
?
a=1
r
j,a
(x, h,m, l) log ?
j,a
.
We modify this definition slightly, be introducing
scaling factors c
j,a
> 0, and defining
q
j
(x, h,m, l) =
d
j
?
a=1
r
j,a
(x, h,m, l) log
?
j,a
c
j,a
(4)
In our experiments, c
j,a
is simply a count of the
number of times the feature indexed by (j, a) ap-
pears in unlabeled data. Thus more frequent fea-
tures have their contribution down-weighted in the
model. We have found this modification to be ben-
eficial.
2.3 Estimating the Parameters of the
Generative Models
We now describe the method for estimating the
parameters ?
j,a
of the generative models. We
assume initial parameters w,v,q, which define
a distribution p(y|x
?
i
;w,v,q) over dependency
structures for each unlabeled example x
?
i
. We will
re-estimate the generative models q, based on un-
labeled examples. The likelihood function on un-
labeled data is defined as
M
?
i=1
?
y
p(y|x
?
i
;w,v,q)
?
(h,m,l)?y
log q
?
j
(x
?
i
, h,m, l),
(5)
where q
?
j
is as defined in Eq. 3. This function re-
sembles the Q function used in the EM algorithm,
where the hidden labels (in our case, dependency
structures), are filled in using the conditional dis-
tribution p(y|x
?
i
;w,v,q).
It is simple to show that the estimates ?
j,a
that
maximize the function in Eq. 5 can be defined as
follows. First, define a vector of expected counts
based on w,v,q as
?
r
j
=
M
?
i=1
?
y
p(y|x
?
i
;w,v,q)
?
(h,m,l)?y
r
j
(x
?
i
, h,m, l).
Note that it is straightforward to calculate these ex-
pected counts using a variant of the inside-outside
algorithm (Baker, 1979) applied to the (Eisner,
1996) dependency-parsing data structures (Paskin,
2001) for projective dependency structures, or the
matrix-tree theorem (Koo et al, 2007; Smith and
Smith, 2007; McDonald and Satta, 2007) for non-
projective dependency structures.
The estimates that maximize Eq. 5 are then
?
j,a
=
r?
j,a
?
d
j
a=1
r?
j,a
.
In a slight modification, we employ the follow-
ing estimates in our model, where ? > 1 is a pa-
rameter of the model:
?
j,a
=
(? ? 1) + r?
j,a
d
j
? (? ? 1) +
?
d
j
a=1
r?
j,a
. (6)
This corresponds to a MAP estimate under a
Dirichlet prior over the ?
j,a
parameters.
2.4 The Complete Parameter-Estimation
Method
This section describes the full parameter estima-
tion method. The input to the algorithm is a set
of labeled examples {x
i
,y
i
}
N
i=1
, a set of unla-
beled examples {x
?
i
}
M
i=1
, a feature-vector defini-
tion f(x,y), and a partition of f into k feature vec-
tors r
1
. . . r
k
which underly the generative mod-
els. The output from the algorithm is a parameter
vector w, a set of generative models q
1
. . . q
k
, and
parameters v
1
. . . v
k
, which define a probabilistic
dependency parsing model through Eqs. 1 and 2.
The learning algorithm proceeds in three steps:
Step 1: Estimation of a Fully Supervised
Model. We choose the initial value q
0
of the
generative models to be the uniform distribution,
i.e., we set ?
j,a
= 1/d
j
for all j, a. We then de-
fine the regularized log-likelihood function for the
labeled examples, with the generative model fixed
at q
0
, to be:
L(w,v;q
0
) =
n
?
i=1
log p(y
i
|x
i
;w,v,q
0
)
?
C
2
(
||w||
2
+ ||v||
2
)
553
This is a conventional regularized log-likelihood
function, as commonly used in CRF models. The
parameter C > 0 dictates the level of regular-
ization in the model. We define the initial pa-
rameters (w
0
,v
0
) = argmax
w,v
L(w,v;q
0
).
These parameters can be found using conventional
methods for estimating the parameters of regu-
larized log-likelihood functions (in our case we
use LBFGS (Liu and Nocedal, 1989)). Note that
the gradient of the log-likelihood function can be
calculated using the inside-outside algorithm ap-
plied to projective dependency parse structures, or
the matrix-tree theorem applied to non-projective
structures.
Step 2: Estimation of the Generative Mod-
els. In this step, expected count vectors
?
r
1
. . .
?
r
k
are first calculated, based on the distribution
p(y|x;w
0
,v
0
,q
0
). Generative model parameters
?
j,a
are calculated through the definition in Eq. 6;
these estimates define updated generative models
q
1
j
for j = 1 . . . k through Eq. 4. We refer to the
new values for the generative models as q
1
.
Step 3: Re-estimation of w and v. In
the final step, w
1
and v
1
are estimated as
argmax
w,v
L(w,v;q
1
) where L(w,v;q
1
) is de-
fined in an analogous way to L(w,v;q
0
). Thus w
and v are re-estimated to optimize log-likelihood
of the labeled examples, with the generative mod-
els q
1
estimated in step 2.
The final output from the algorithm is the set of
parameters (w
1
,v
1
,q
1
). Note that it is possible to
iterate the method?steps 2 and 3 can be repeated
multiple times (Suzuki and Isozaki, 2008)?but
in our experiments we only performed these steps
once.
3 Extensions
3.1 Incorporating Cluster-Based Features
Koo et al (2008) describe a semi-supervised
approach that incorporates cluster-based features,
and that gives competitive results on dependency
parsing benchmarks. The method is a two-stage
approach. First, hierarchical word clusters are de-
rived from unlabeled data using the Brown et al
clustering algorithm (Brown et al, 1992). Sec-
ond, a new feature set is constructed by represent-
ing words by bit-strings of various lengths, corre-
sponding to clusters at different levels of the hier-
archy. These features are combined with conven-
tional features based on words and part-of-speech
tags. The new feature set is then used within a
conventional discriminative, supervised approach,
such as the averaged perceptron algorithm.
The important point is that their approach uses
unlabeled data only for the construction of a new
feature set, and never affects to learning algo-
rithms. It is straightforward to incorporate cluster-
based features within the SS-SCM approach de-
scribed in this paper. We simply use the cluster-
based feature-vector representation f(x,y) intro-
duced by (Koo et al, 2008) as the basis of our ap-
proach.
3.2 Second-order Parsing Models
Previous work (McDonald and Pereira, 2006; Car-
reras, 2007) has shown that second-order parsing
models, which include information from ?sibling?
or ?grandparent? relationships between dependen-
cies, can give significant improvements in accu-
racy over first-order parsing models. In principle
it would be straightforward to extend the SS-SCM
approach that we have described to second-order
parsing models. In practice, however, a bottle-
neck for the method would be the estimation of
the generative models on unlabeled data. This
step requires calculation of marginals on unlabeled
data. Second-order parsing models generally re-
quire more costly inference methods for the cal-
culation of marginals, and this increased cost may
be prohibitive when large quantities of unlabeled
data are employed.
We instead make use of a simple ?two-stage? ap-
proach for extending the SS-SCM approach to the
second-order parsing model of (Carreras, 2007).
In the first stage, we use a first-order parsing
model to estimate generative models q
1
. . . q
k
from
unlabeled data. In the second stage, we incorpo-
rate these generative models as features within a
second-order parsing model. More precisely, in
our approach, we first train a first-order parsing
model by Step 1 and 2, exactly as described in
Section 2.4, to estimate w
0
, v
0
and q
1
. Then,
we substitute Step 3 as a supervised learning such
as MIRA with a second-order parsing model (Mc-
Donald et al, 2005a), which incorporates q
1
as a
real-values features. We refer this two-stage ap-
proach to as two-stage SS-SCM.
In our experiments we use the 1-best MIRA
algorithm (McDonald and Pereira, 2006)
1
as a
1
We used a slightly modified version of 1-best MIRA,
whose difference can be found in the third line in Eq. 7,
namely, including L(y
i
,y).
554
(a) English dependency parsing
Data set (WSJ Sec. IDs) # of sentences # of tokens
Training (02?21) 39,832 950,028
Development (22) 1,700 40,117
Test (23) 2,012 47,377
Unlabeled 1,796,379 43,380,315
(b) Czech dependency parsing
Data set # of sentences # of tokens
Training 73,088 1,255,590
Development 7,507 126,030
Test 7,319 125,713
Unlabeled 2,349,224 39,336,570
Table 1: Details of training, development, test data
(labeled data sets) and unlabeled data used in our
experiments
parameter-estimation method for the second-order
parsing model. In particular, we perform the fol-
lowing optimizations on each update t = 1, ..., T
for re-estimating w and v:
min ||w
(t+1)
?w
(t)
||+ ||v
(t+1)
? v
(t)
||
s.t. S(x
i
,y
i
)? S(x
i
,
?
y) ? L(y
i
,
?
y)
?
y = argmax
y
S(x
i
,y) + L(y
i
,y),
(7)
where L(y
i
,y) represents the loss between correct
output of i?th sample y
i
and y. Then, the scoring
function S for each y can be defined as follows:
S(x,y) =w ? (f
1
(x,y) + f
2
(x,y))
+B
k
?
j=1
v
j
q
j
(x,y),
(8)
where B represents a tunable scaling factor, and
f
1
and f
2
represent the feature vectors of first and
second-order parsing parts, respectively.
4 Experiments
We now describe experiments investigating the ef-
fectiveness of the SS-SCM approach for depen-
dency parsing. The experiments test basic, first-
order parsing models, as well as the extensions
to cluster-based features and second-order parsing
models described in the previous section.
4.1 Data Sets
We conducted experiments on both English and
Czech data. We used the Wall Street Journal
sections of the Penn Treebank (PTB) III (Mar-
cus et al, 1994) as a source of labeled data for
English, and the Prague Dependency Treebank
(PDT) 1.0 (Haji?c, 1998) for Czech. To facili-
tate comparisons with previous work, we used ex-
actly the same training, development and test sets
Corpus article name (mm/yy) # of sent. # of tokens
BLLIP wsj 00/87?00/89 1,796,379 43,380,315
Tipster wsj 04/90?03/92 1,550,026 36,583,547
North wsj 07/94?12/96 2,748,803 62,937,557
American reu 04/94?07/96 4,773,701 110,001,109
Reuters reu 09/96?08/97 12,969,056 214,708,766
English afp 05/94?12/06 21,231,470 513,139,928
Gigaword apw 11/94?12/06 46,978,725 960,733,303
ltw 04/94?12/06 10,524,545 230,370,454
nyt 07/94?12/06 60,752,363 1,266,531,274
xin 01/95?12/06 12,624,835 283,579,330
total 175,949,903 3,721,965,583
Table 2: Details of the larger unlabeled data set
used in English dependency parsing: sentences ex-
ceeding 128 tokens in length were excluded for
computational reasons.
as those described in (McDonald et al, 2005a;
McDonald et al, 2005b; McDonald and Pereira,
2006; Koo et al, 2008). The English dependency-
parsing data sets were constructed using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to dependency tree repre-
sentations. We split the data into three parts:
sections 02-21 for training, section 22 for de-
velopment and section 23 for test. The Czech
data sets were obtained from the predefined train-
ing/development/test partition in the PDT. The un-
labeled data for English was derived from the
Brown Laboratory for Linguistic Information Pro-
cessing (BLLIP) Corpus (LDC2000T43)
2
, giving
a total of 1,796,379 sentences and 43,380,315
tokens. The raw text section of the PDT was
used for Czech, giving 2,349,224 sentences and
39,336,570 tokens. These data sets are identical
to the unlabeled data used in (Koo et al, 2008),
and are disjoint from the training, development
and test sets. The datasets used in our experiments
are summarized in Table 1.
In addition, we will describe experiments that
make use of much larger amounts of unlabeled
data. Unfortunately, we have no data available
other than PDT for Czech, this is done only for
English dependency parsing. Table 2 shows the
detail of the larger unlabeled data set used in our
experiments, where we eliminated sentences that
have more than 128 tokens for computational rea-
sons. Note that the total size of the unlabeled data
reaches 3.72G (billion) tokens, which is approxi-
2
We ensured that the sentences used in the PTB were
excluded from the unlabeled data, since sentences used in
BLLIP corpus are a super-set of the PTB.
555
mately 4,000 times larger than the size of labeled
training data.
4.2 Features
4.2.1 Baseline Features
In general we will assume that the input sentences
include both words and part-of-speech (POS) tags.
Our baseline features (?baseline?) are very simi-
lar to those described in (McDonald et al, 2005a;
Koo et al, 2008): these features track word and
POS bigrams, contextual features surrounding de-
pendencies, distance features, and so on. En-
glish POS tags were assigned by MXPOST (Rat-
naparkhi, 1996), which was trained on the train-
ing data described in Section 4.1. Czech POS tags
were obtained by the following two steps: First,
we used ?feature-based tagger? included with the
PDT
3
, and then, we used the method described in
(Collins et al, 1999) to convert the assigned rich
POS tags into simplified POS tags.
4.2.2 Cluster-based Features
In a second set of experiments, we make use of the
feature set used in the semi-supervised approach
of (Koo et al, 2008). We will refer to this as the
?cluster-based feature set? (CL). The BLLIP (43M
tokens) and PDT (39M tokens) unlabeled data sets
shown in Table 1 were used to construct the hierar-
chical clusterings used within the approach. Note
that when this feature set is used within the SS-
SCM approach, the same set of unlabeled data is
used to both induce the clusters, and to estimate
the generative models within the SS-SCM model.
4.2.3 Constructing the Generative Models
As described in section 2.2, the generative mod-
els in the SS-SCM approach are defined through
a partition of the original feature vector f(x,y)
into k feature vectors r
1
(x,y) . . . r
k
(x,y). We
follow a similar approach to that of (Suzuki and
Isozaki, 2008) in partitioning f(x,y), where the
k different feature vectors correspond to different
feature types or feature templates. Note that, in
general, we are not necessary to do as above, this
is one systematic way of a feature design for this
approach.
4.3 Other Experimental Settings
All results presented in our experiments are given
in terms of parent-prediction accuracy on unla-
3
Training, development, and test data in PDT already con-
tains POS tags assigned by the ?feature-based tagger?.
beled dependency parsing. We ignore the parent-
predictions of punctuation tokens for English,
while we retain all the punctuation tokens for
Czech. These settings match the evaluation setting
in previous work such as (McDonald et al, 2005a;
Koo et al, 2008).
We used the method proposed by (Carreras,
2007) for our second-order parsing model. Since
this method only considers projective dependency
structures, we ?projectivized? the PDT training
data in the same way as (Koo et al, 2008). We
used a non-projective model, trained using an ap-
plication of the matrix-tree theorem (Koo et al,
2007; Smith and Smith, 2007; McDonald and
Satta, 2007) for the first-order Czech models, and
projective parsers for all other models.
As shown in Section 2, SS-SCMs with 1st-order
parsing models have two tunable parameters, C
and ?, corresponding to the regularization con-
stant, and the Dirichlet prior for the generative
models. We selected a fixed value ? = 2, which
was found to work well in preliminary experi-
ments.
4
The value of C was chosen to optimize
performance on development data. Note that C
for supervised SCMs were also tuned on develop-
ment data. For the two-stage SS-SCM for incor-
porating second-order parsing model, we have ad-
ditional one tunable parameter B shown in Eq. 8.
This was also chosen by the value that provided
the best performance on development data.
In addition to providing results for models
trained on the full training sets, we also performed
experiments with smaller labeled training sets.
These training sets were either created through
random sampling or by using a predefined subset
of document IDs from the labeled training data.
5 Results and Discussion
Table 3 gives results for the SS-SCM method un-
der various configurations: for first and second-
order parsing models, with and without the clus-
ter features of (Koo et al, 2008), and for varying
amounts of labeled data. The remainder of this
section discusses these results in more detail.
5.1 Effects of the Quantity of Labeled Data
We can see from the results in Table 3 that our
semi-supervised approach consistently gives gains
4
An intuitive meaning of ? = 2 is that this adds one
pseudo expected count to every feature when estimating new
parameter values.
556
(a) English dependency parsing: w/ 43M token unlabeled data (BLLIP)
WSJ sec. IDs wsj 21 random selection random selection wsj 15?18 wsj 02-21(all)
# of sentences / tokens 1,671 / 40,039 2,000 / 48,577 8,000 / 190,958 8,936 / 211,727 39,832 / 950,028
feature type baseline CL baseline CL baseline CL baseline CL baseline CL
Supervised SCM (1od) 85.63 86.80 87.02 88.05 89.23 90.45 89.43 90.85 91.21 92.53
SS-SCM (1od) 87.16 88.40 88.07 89.55 90.06 91.45 90.23 91.63 91.72 93.01
(gain over Sup. SCM) (+1.53) (+1.60) (+1.05) (+1.50) (+0.83) (+1.00) (+0.80) (+0.78) (+0.51) (+0.48)
Supervised MIRA (2od) 87.99 89.05 89.20 90.06 91.20 91.75 91.50 92.14 93.02 93.54
2-stage SS-SCM(+MIRA) (2od) 88.88 89.94 90.03 90.90 91.73 92.51 91.95 92.73 93.45 94.13
(gain over Sup. MIRA) (+0.89) (+0.89) (+0.83) (+0.84) (+0.53) (+0.76) (+0.45) (+0.59) (+0.43) (+0.59)
(b) Czech dependency parsing: w/ 39M token unlabeled data (PDT)
PDT Doc. IDs random selection c[0-9]* random selection l[a-i]* (all)
# of sentences / tokens 2,000 / 34,722 3,526 / 53,982 8,000 / 140,423 14,891 / 261,545 73,008 /1,225,590
feature type baseline CL baseline CL baseline CL baseline CL baseline CL
Supervised SCM (1od) 75.67 77.82 76.88 79.24 80.61 82.85 81.94 84.47 84.43 86.72
SS-SCM (1od) 76.47 78.96 77.61 80.28 81.30 83.49 82.74 84.91 85.00 87.03
(gain over Sup. SCM) (+0.80) (+1.14) (+0.73) (+1.04) (+0.69) (+0.64) (+0.80) (+0.44) (+0.57) (+0.31)
Supervised MIRA (2od) 78.19 79.60 79.58 80.77 83.15 84.39 84.27 85.75 86.82 87.76
2-stage SS-SCM(+MIRA) (2od) 78.71 80.09 80.37 81.40 83.61 84.87 84.95 86.00 87.03 88.03
(gain over Sup. MIRA) (+0.52) (+0.49) (+0.79) (+0.63) (+0.46) (+0.48) (+0.68) (+0.25) (+0.21) (+0.27)
Table 3: Dependency parsing results for the SS-SCM method with different amounts of labeled training
data. Supervised SCM (1od) and Supervised MIRA (2od) are the baseline first and second-order ap-
proaches; SS-SCM (1od) and 2-stage SS-SCM(+MIRA) (2od) are the first and second-order approaches
described in this paper. ?Baseline? refers to models without cluster-based features, ?CL? refers to models
which make use of cluster-based features.
in performance under various sizes of labeled data.
Note that the baseline methods that we have used
in these experiments are strong baselines. It is
clear that the gains from our method are larger for
smaller labeled data sizes, a tendency that was also
observed in (Koo et al, 2008).
5.2 Impact of Combining SS-SCM with
Cluster Features
One important observation from the results in Ta-
ble 3 is that SS-SCMs can successfully improve
the performance over a baseline method that uses
the cluster-based feature set (CL). This is in spite
of the fact that the generative models within the
SS-SCM approach were trained on the same un-
labeled data used to induce the cluster-based fea-
tures.
5.3 Impact of the Two-stage Approach
Table 3 also shows the effectiveness of the two-
stage approach (described in Section 3.2) that inte-
grates the SS-SCM method within a second-order
parser. This suggests that the SS-SCM method
can be effective in providing features (generative
models) used within a separate learning algorithm,
providing that this algorithm can make use of real-
valued features.
91.5
92.0
92.5
93.0
93.5
10 100 1,000 10,000
CL
baseline
43.4M 143M
468M 1.38G
3.72G
(Mega tokens)
Unlabeled data size: [Log-scale]
P
a
r
e
n
t
-
p
r
e
d
ic
t
io
n
 
A
c
c
u
r
a
c
y
(BLLIP)
Figure 1: Impact of unlabeled data size for the SS-
SCM on development data of English dependency
parsing.
5.4 Impact of the Amount of Unlabeled Data
Figure 1 shows the dependency parsing accuracy
on English as a function of the amount of unla-
beled data used within the SS-SCM approach. (As
described in Section 4.1, we have no unlabeled
data other than PDT for Czech, hence this section
only considers English dependency parsing.) We
can see that performance does improve as more
unlabeled data is added; this trend is seen both
with and without cluster-based features. In addi-
tion, Table 4 shows the performance of our pro-
posed method using 3.72 billion tokens of unla-
557
feature type baseline CL
SS-SCM (1st-order) 92.23 93.23
(gain over Sup. SCM) (+1.02) (+0.70)
2-stage SS-SCM(+MIRA) (2nd-order) 93.68 94.26
(gain over Sup. MIRA) (+0.66) (+0.72)
Table 4: Parent-prediction accuracies on develop-
ment data with 3.72G tokens unlabeled data for
English dependency parsing.
beled data. Note, however, that the gain in perfor-
mance as unlabeled data is added is not as sharp
as might be hoped, with a relatively modest dif-
ference in performance for 43.4 million tokens vs.
3.72 billion tokens of unlabeled data.
5.5 Computational Efficiency
The main computational challenge in our ap-
proach is the estimation of the generative mod-
els q = ?q
1
. . . q
k
? from unlabeled data, partic-
ularly when the amount of unlabeled data used
is large. In our implementation, on the 43M to-
ken BLLIP corpus, using baseline features, it takes
about 5 hours to compute the expected counts re-
quired to estimate the parameters of the generative
models on a single 2.93GHz Xeon processor. It
takes roughly 18 days of computation to estimate
the generative models from the larger (3.72 billion
word) corpus. Fortunately it is simple to paral-
lelize this step; our method takes a few hours on
the larger data set when parallelized across around
300 separate processes.
Note that once the generative models have been
estimated, decoding with the model, or train-
ing the model on labeled data, is relatively in-
expensive, essentially taking the same amount of
computation as standard dependency-parsing ap-
proaches.
5.6 Results on Test Data
Finally, Table 5 displays the final results on test
data. There results are obtained using the best
setting in terms of the development data perfor-
mance. Note that the English dependency pars-
ing results shown in the table were achieved us-
ing 3.72 billion tokens of unlabeled data. The im-
provements on test data are similar to those ob-
served on the development data. To determine
statistical significance, we tested the difference of
parent-prediction error-rates at the sentence level
using a paired Wilcoxon signed rank test. All eight
comparisons shown in Table 5 are significant with
(a) English dependency parsing: w/ 3.72G token ULD
feature set baseline CL
SS-SCM (1st-order) 91.89 92.70
(gain over Sup. SCM) (+0.92) (+0.58)
2-stage SS-SCM(+MIRA) (2nd-order) 93.41 93.79
(gain over Sup. MIRA) (+0.65) (+0.48)
(b) Czech dependency parsing: w/ 39M token ULD (PDT)
feature set baseline CL
SS-SCM (1st-order) 84.98 87.14
(gain over Sup. SCM) (+0.58) (+0.39)
2-stage SS-SCM(+MIRA) (2nd-order) 86.90 88.05
(gain over Sup. MIRA) (+0.15) (+0.36)
Table 5: Parent-prediction accuracies on test data
using the best setting in terms of development data
performances in each condition.
(a) English dependency parsers on PTB
dependency parser test description
(McDonald et al, 2005a) 90.9 1od
(McDonald and Pereira, 2006) 91.5 2od
(Koo et al, 2008) 92.23 1od, 43M ULD
SS-SCM (w/ CL) 92.70 1od, 3.72G ULD
(Koo et al, 2008) 93.16 2od, 43M ULD
2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD
(b) Czech dependency parsers on PDT
dependency parser test description
(McDonald et al, 2005b) 84.4 1od
(McDonald and Pereira, 2006) 85.2 2od
(Koo et al, 2008) 86.07 1od, 39M ULD
(Koo et al, 2008) 87.13 2od, 39M ULD
SS-SCM (w/ CL) 87.14 1od, 39M ULD
2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD
Table 6: Comparisons with the previous top sys-
tems: (1od, 2od: 1st- and 2nd-order parsing
model, ULD: unlabeled data).
p < 0.01.
6 Comparison with Previous Methods
Table 6 shows the performance of a number of
state-of-the-art approaches on the English and
Czech data sets. For both languages our ap-
proach gives the best reported figures on these
datasets. Our results yield relative error reduc-
tions of roughly 27% (English) and 20% (Czech)
over McDonald and Pereira (2006)?s second-order
supervised dependency parsers, and roughly 9%
(English) and 7% (Czech) over the previous best
results provided by Koo et. al. (2008)?s second-
order semi-supervised dependency parsers.
Note that there are some similarities between
our two-stage semi-supervised learning approach
and the semi-supervised learning method intro-
duced by (Blitzer et al, 2006), which is an exten-
sion of the method described by (Ando and Zhang,
558
2005). In particular, both methods use a two-stage
approach; They first train generative models or
auxiliary problems from unlabeled data, and then,
they incorporate these trained models into a super-
vised learning algorithm as real valued features.
Moreover, both methods make direct use of exist-
ing feature-vector definitions f(x,y) in inducing
representations from unlabeled data.
7 Conclusion
This paper has described an extension of the
semi-supervised learning approach of (Suzuki and
Isozaki, 2008) to the dependency parsing problem.
In addition, we have described extensions that in-
corporate the cluster-based features of Koo et al
(2008), and that allow the use of second-order
parsing models. We have described experiments
that show that the approach gives significant im-
provements over state-of-the-art methods for de-
pendency parsing; performance improves when
the amount of unlabeled data is increased from
43.8 million tokens to 3.72 billion tokens. The ap-
proach should be relatively easily applied to lan-
guages other than English or Czech.
We stress that the SS-SCM approach requires
relatively little hand-engineering: it makes di-
rect use of the existing feature-vector representa-
tion f(x,y) used in a discriminative model, and
does not require the design of new features. The
main choice in the approach is the partitioning
of f(x,y) into components r
1
(x,y) . . . r
k
(x,y),
which in our experience is straightforward.
References
R. Kubota Ando and T. Zhang. 2005. A Framework for
Learning Predictive Structures from Multiple Tasks
and Unlabeled Data. Journal of Machine Learning
Research, 6:1817?1853.
J. K. Baker. 1979. Trainable Grammars for Speech
Recognition. In Speech Communication Papers for
the 97th Meeting of the Acoustical Society of Amer-
ica, pages 547?550.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
Adaptation with Structural Correspondence Learn-
ing. In Proc. of EMNLP-2006, pages 120?128.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram
Models of Natural Language. Computational Lin-
guistics, 18(4):467?479.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
of CoNLL-X, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of EMNLP-
CoNLL, pages 957?961.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann.
1999. A Statistical Parser for Czech. In Proc. of
ACL, pages 505?512.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proc. of
COLING-96, pages 340?345.
Jan Haji?c. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning. Studies in Honor of
Jarmila Panevov?a, pages 12?19. Prague Karolinum,
Charles University Press.
T. Koo, A. Globerson, X. Carreras, and M. Collins.
2007. Structured Prediction Models via the Matrix-
Tree Theorem. In Proc. of EMNLP-CoNLL, pages
141?150.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
Semi-supervised Dependency Parsing. In Proc. of
ACL-08: HLT, pages 595?603.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 282?289.
D. C. Liu and J. Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Math. Programming, Ser. B, 45(3):503?528.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. McDonald and F. Pereira. 2006. Online Learning of
Approximate Dependency Parsing Algorithms. In
Proc. of EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the Com-
plexity of Non-Projective Data-Driven Dependency
Parsing. In Proc. of IWPT, pages 121?132.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line Large-margin Training of Dependency Parsers.
In Proc. of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Haji?c.
2005b. Non-projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proc. of HLT-
EMNLP, pages 523?530.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proc. of
EMNLP-CoNLL, pages 915?932.
Mark A. Paskin. 2001. Cubic-time Parsing and Learn-
ing Algorithms for Grammatical Bigram. Technical
report, University of California at Berkeley, Berke-
ley, CA, USA.
559
A. Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proc. of EMNLP,
pages 133?142.
D. A. Smith and J. Eisner. 2007. Bootstrapping
Feature-Rich Dependency Parsers with Entropic Pri-
ors. In Proc. of EMNLP-CoNLL, pages 667?677.
D. A. Smith and N. A. Smith. 2007. Probabilis-
tic Models of Nonprojective Dependency Trees. In
Proc. of EMNLP-CoNLL, pages 132?140.
J. Suzuki and H. Isozaki. 2008. Semi-supervised
Sequential Labeling and Segmentation Using Giga-
Word Scale Unlabeled Data. In Proc. of ACL-08:
HLT, pages 665?673.
Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi-
supervised Convex Training for Dependency Pars-
ing. In Proc. of ACL-08: HLT, pages 532?540.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
560
Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Simple Semi-supervised Dependency Parsing
Terry Koo, Xavier Carreras, and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,carreras,mcollins}@csail.mit.edu
Abstract
We present a simple and effective semi-
supervised method for training dependency
parsers. We focus on the problem of lex-
ical representation, introducing features that
incorporate word clusters derived from a large
unannotated corpus. We demonstrate the ef-
fectiveness of the approach in a series of de-
pendency parsing experiments on the Penn
Treebank and Prague Dependency Treebank,
and we show that the cluster-based features
yield substantial gains in performance across
a wide range of conditions. For example, in
the case of English unlabeled second-order
parsing, we improve from a baseline accu-
racy of 92.02% to 93.16%, and in the case
of Czech unlabeled second-order parsing, we
improve from a baseline accuracy of 86.13%
to 87.13%. In addition, we demonstrate that
our method also improves performance when
small amounts of training data are available,
and can roughly halve the amount of super-
vised data required to reach a desired level of
performance.
1 Introduction
In natural language parsing, lexical information is
seen as crucial to resolving ambiguous relationships,
yet lexicalized statistics are sparse and difficult to es-
timate directly. It is therefore attractive to consider
intermediate entities which exist at a coarser level
than the words themselves, yet capture the informa-
tion necessary to resolve the relevant ambiguities.
In this paper, we introduce lexical intermediaries
via a simple two-stage semi-supervised approach.
First, we use a large unannotated corpus to define
word clusters, and then we use that clustering to
construct a new cluster-based feature mapping for
a discriminative learner. We are thus relying on the
ability of discriminative learning methods to identify
and exploit informative features while remaining ag-
nostic as to the origin of such features. To demon-
strate the effectiveness of our approach, we conduct
experiments in dependency parsing, which has been
the focus of much recent research?e.g., see work
in the CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007).
The idea of combining word clusters with dis-
criminative learning has been previously explored
by Miller et al (2004), in the context of named-
entity recognition, and their work directly inspired
our research. However, our target task of depen-
dency parsing involves more complex structured re-
lationships than named-entity tagging; moreover, it
is not at all clear that word clusters should have any
relevance to syntactic structure. Nevertheless, our
experiments demonstrate that word clusters can be
quite effective in dependency parsing applications.
In general, semi-supervised learning can be mo-
tivated by two concerns: first, given a fixed amount
of supervised data, we might wish to leverage ad-
ditional unlabeled data to facilitate the utilization of
the supervised corpus, increasing the performance of
the model in absolute terms. Second, given a fixed
target performance level, we might wish to use un-
labeled data to reduce the amount of annotated data
necessary to reach this target.
We show that our semi-supervised approach
yields improvements for fixed datasets by perform-
ing parsing experiments on the Penn Treebank (Mar-
cus et al, 1993) and Prague Dependency Treebank
(Hajic?, 1998; Hajic? et al, 2001) (see Sections 4.1
and 4.3). By conducting experiments on datasets of
varying sizes, we demonstrate that for fixed levels of
performance, the cluster-based approach can reduce
the need for supervised data by roughly half, which
is a substantial savings in data-annotation costs (see
Sections 4.2 and 4.4).
The remainder of this paper is divided as follows:
595
Ms. Haag plays Elianti .*
obj
proot
nmod sbj
Figure 1: An example of a labeled dependency tree. The
tree contains a special token ?*? which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
Section 2 gives background on dependency parsing
and clustering, Section 3 describes the cluster-based
features, Section 4 presents our experimental results,
Section 5 discusses related work, and Section 6 con-
cludes with ideas for future research.
2 Background
2.1 Dependency parsing
Recent work (Buchholz and Marsi, 2006; Nivre
et al, 2007) has focused on dependency parsing.
Dependency syntax represents syntactic informa-
tion as a network of head-modifier dependency arcs,
typically restricted to be a directed tree (see Fig-
ure 1 for an example). Dependency parsing depends
critically on predicting head-modifier relationships,
which can be difficult due to the statistical sparsity
of these word-to-word interactions. Bilexical depen-
dencies are thus ideal candidates for the application
of coarse word proxies such as word clusters.
In this paper, we take a part-factored structured
classification approach to dependency parsing. For a
given sentence x, let Y(x) denote the set of possible
dependency structures spanning x, where each y ?
Y(x) decomposes into a set of ?parts? r ? y. In the
simplest case, these parts are the dependency arcs
themselves, yielding a first-order or ?edge-factored?
dependency parsing model. In higher-order parsing
models, the parts can consist of interactions between
more than two words. For example, the parser of
McDonald and Pereira (2006) defines parts for sib-
ling interactions, such as the trio ?plays?, ?Elianti?,
and ?.? in Figure 1. The Carreras (2007) parser
has parts for both sibling interactions and grandpar-
ent interactions, such as the trio ?*?, ?plays?, and
?Haag? in Figure 1. These kinds of higher-order
factorizations allow dependency parsers to obtain a
limited form of context-sensitivity.
Given a factorization of dependency structures
into parts, we restate dependency parsing as the fol-
apple pear Apple IBM bought run of in
01
100 101 110 111000 001 010 011
00
0
10
1
11
Figure 2: An example of a Brown word-cluster hierarchy.
Each node in the tree is labeled with a bit-string indicat-
ing the path from the root node to that node, where 0
indicates a left branch and 1 indicates a right branch.
lowing maximization:
PARSE(x;w) = argmax
y?Y(x)
?
r?y
w ? f(x, r)
Above, we have assumed that each part is scored
by a linear model with parameters w and feature-
mapping f(?). For many different part factoriza-
tions and structure domains Y(?), it is possible to
solve the above maximization efficiently, and several
recent efforts have concentrated on designing new
maximization algorithms with increased context-
sensitivity (Eisner, 2000; McDonald et al, 2005b;
McDonald and Pereira, 2006; Carreras, 2007).
2.2 Brown clustering algorithm
In order to provide word clusters for our exper-
iments, we used the Brown clustering algorithm
(Brown et al, 1992). We chose to work with the
Brown algorithm due to its simplicity and prior suc-
cess in other NLP applications (Miller et al, 2004;
Liang, 2005). However, we expect that our approach
can function with other clustering algorithms (as in,
e.g., Li and McCallum (2005)). We briefly describe
the Brown algorithm below.
The input to the algorithm is a vocabulary of
words to be clustered and a corpus of text containing
these words. Initially, each word in the vocabulary
is considered to be in its own distinct cluster. The al-
gorithm then repeatedly merges the pair of clusters
which causes the smallest decrease in the likelihood
of the text corpus, according to a class-based bigram
language model defined on the word clusters. By
tracing the pairwise merge operations, one obtains
a hierarchical clustering of the words, which can be
represented as a binary tree as in Figure 2.
Within this tree, each word is uniquely identified
by its path from the root, and this path can be com-
pactly represented with a bit string, as in Figure 2.
In order to obtain a clustering of the words, we se-
lect all nodes at a certain depth from the root of the
596
hierarchy. For example, in Figure 2 we might select
the four nodes at depth 2 from the root, yielding the
clusters {apple,pear}, {Apple,IBM}, {bought,run},
and {of,in}. Note that the same clustering can be ob-
tained by truncating each word?s bit-string to a 2-bit
prefix. By using prefixes of various lengths, we can
produce clusterings of different granularities (Miller
et al, 2004).
For all of the experiments in this paper, we used
the Liang (2005) implementation of the Brown algo-
rithm to obtain the necessary word clusters.
3 Feature design
Key to the success of our approach is the use of fea-
tures which allow word-cluster-based information to
assist the parser. The feature sets we used are simi-
lar to other feature sets in the literature (McDonald
et al, 2005a; Carreras, 2007), so we will not attempt
to give a exhaustive description of the features in
this section. Rather, we describe our features at a
high level and concentrate on our methodology and
motivations. In our experiments, we employed two
different feature sets: a baseline feature set which
draws upon ?normal? information sources such as
word forms and parts of speech, and a cluster-based
feature set that also uses information derived from
the Brown cluster hierarchy.
3.1 Baseline features
Our first-order baseline feature set is similar to the
feature set of McDonald et al (2005a), and consists
of indicator functions for combinations of words and
parts of speech for the head and modifier of each
dependency, as well as certain contextual tokens.1
Our second-order baseline features are the same as
those of Carreras (2007) and include indicators for
triples of part of speech tags for sibling interactions
and grandparent interactions, as well as additional
bigram features based on pairs of words involved
these higher-order interactions. Examples of base-
line features are provided in Table 1.
1We augment the McDonald et al (2005a) feature set with
backed-off versions of the ?Surrounding Word POS Features?
that include only one neighboring POS tag. We also add binned
distance features which indicate whether the number of tokens
between the head and modifier of a dependency is greater than
2, 5, 10, 20, 30, or 40 tokens.
Baseline Cluster-based
ht,mt hc4,mc4
hw,mw hc6,mc6
hw,ht,mt hc*,mc*
hw,ht,mw hc4,mt
ht,mw,mt ht,mc4
hw,mw,mt hc6,mt
hw,ht,mw,mt ht,mc6
? ? ? hc4,mw
hw,mc4
? ? ?
ht,mt,st hc4,mc4,sc4
ht,mt,gt hc6,mc6,sc6
? ? ? ht,mc4,sc4
hc4,mc4,gc4
? ? ?
Table 1: Examples of baseline and cluster-based feature
templates. Each entry represents a class of indicators for
tuples of information. For example, ?ht,mt? represents
a class of indicator features with one feature for each pos-
sible combination of head POS-tag and modifier POS-
tag. Abbreviations: ht = head POS, hw = head word,
hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head,
hc* = full bit string of head; mt,mw,mc4,mc6,mc* =
likewise for modifier; st,gt,sc4,gc4,. . . = likewise
for sibling and grandchild.
3.2 Cluster-based features
The first- and second-order cluster-based feature sets
are supersets of the baseline feature sets: they in-
clude all of the baseline feature templates, and add
an additional layer of features that incorporate word
clusters. Following Miller et al (2004), we use pre-
fixes of the Brown cluster hierarchy to produce clus-
terings of varying granularity. We found that it was
nontrivial to select the proper prefix lengths for the
dependency parsing task; in particular, the prefix
lengths used in the Miller et al (2004) work (be-
tween 12 and 20 bits) performed poorly in depen-
dency parsing.2 After experimenting with many dif-
ferent feature configurations, we eventually settled
on a simple but effective methodology.
First, we found that it was helpful to employ two
different types of word clusters:
1. Short bit-string prefixes (e.g., 4?6 bits), which
we used as replacements for parts of speech.
2One possible explanation is that the kinds of distinctions
required in a named-entity recognition task (e.g., ?Alice? versus
?Intel?) are much finer-grained than the kinds of distinctions
relevant to syntax (e.g., ?apple? versus ?eat?).
597
2. Full bit strings,3 which we used as substitutes
for word forms.
Using these two types of clusters, we generated new
features by mimicking the template structure of the
original baseline features. For example, the baseline
feature set includes indicators for word-to-word and
tag-to-tag interactions between the head and mod-
ifier of a dependency. In the cluster-based feature
set, we correspondingly introduce new indicators for
interactions between pairs of short bit-string pre-
fixes and pairs of full bit strings. Some examples
of cluster-based features are given in Table 1.
Second, we found it useful to concentrate on
?hybrid? features involving, e.g., one bit-string and
one part of speech. In our initial attempts, we fo-
cused on features that used cluster information ex-
clusively. While these cluster-only features provided
some benefit, we found that adding hybrid features
resulted in even greater improvements. One possible
explanation is that the clusterings generated by the
Brown algorithm can be noisy or only weakly rele-
vant to syntax; thus, the clusters are best exploited
when ?anchored? to words or parts of speech.
Finally, we found it useful to impose a form of
vocabulary restriction on the cluster-based features.
Specifically, for any feature that is predicated on a
word form, we eliminate this feature if the word
in question is not one of the top-N most frequent
words in the corpus. When N is between roughly
100 and 1,000, there is little effect on the perfor-
mance of the cluster-based feature sets.4 In addition,
the vocabulary restriction reduces the size of the fea-
ture sets to managable proportions.
4 Experiments
In order to evaluate the effectiveness of the cluster-
based feature sets, we conducted dependency pars-
ing experiments in English and Czech. We test the
features in a wide range of parsing configurations,
including first-order and second-order parsers, and
labeled and unlabeled parsers.5
3As in Brown et al (1992), we limit the clustering algorithm
so that it recovers at most 1,000 distinct bit-strings; thus full bit
strings are not equivalent to word forms.
4We used N = 800 for all experiments in this paper.
5In an ?unlabeled? parser, we simply ignore dependency la-
bel information, which is a common simplification.
The English experiments were performed on the
Penn Treebank (Marcus et al, 1993), using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to a dependency tree represen-
tation.6 We split the Treebank into a training set
(Sections 2?21), a development set (Section 22), and
several test sets (Sections 0,7 1, 23, and 24). The
data partition and head rules were chosen to match
previous work (Yamada and Matsumoto, 2003; Mc-
Donald et al, 2005a; McDonald and Pereira, 2006).
The part of speech tags for the development and test
data were automatically assigned by MXPOST (Rat-
naparkhi, 1996), where the tagger was trained on
the entire training corpus; to generate part of speech
tags for the training data, we used 10-way jackknif-
ing.8 English word clusters were derived from the
BLLIP corpus (Charniak et al, 2000), which con-
tains roughly 43 million words of Wall Street Jour-
nal text.9
The Czech experiments were performed on the
Prague Dependency Treebank 1.0 (Hajic?, 1998;
Hajic? et al, 2001), which is directly annotated
with dependency structures. To facilitate compar-
isons with previous work (McDonald et al, 2005b;
McDonald and Pereira, 2006), we used the train-
ing/development/test partition defined in the corpus
and we also used the automatically-assigned part of
speech tags provided in the corpus.10 Czech word
clusters were derived from the raw text section of
the PDT 1.0, which contains about 39 million words
of newswire text.11
We trained the parsers using the averaged percep-
tron (Freund and Schapire, 1999; Collins, 2002),
which represents a balance between strong perfor-
mance and fast training times. To select the number
6We used Joakim Nivre?s ?Penn2Malt? conversion tool
(http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html). Depen-
dency labels were obtained via the ?Malt? hard-coded setting.
7For computational reasons, we removed a single 249-word
sentence from Section 0.
8That is, we tagged each fold with the tagger trained on the
other 9 folds.
9We ensured that the sentences of the Penn Treebank were
excluded from the text used for the clustering.
10Following Collins et al (1999), we used a coarsened ver-
sion of the Czech part of speech tags; this choice also matches
the conditions of previous work (McDonald et al, 2005b; Mc-
Donald and Pereira, 2006).
11This text was disjoint from the training and test corpora.
598
Sec dep1 dep1c MD1 dep2 dep2c MD2 dep1-L dep1c-L dep2-L dep2c-L
00 90.48 91.57 (+1.09) ? 91.76 92.77 (+1.01) ? 90.29 91.03 (+0.74) 91.33 92.09 (+0.76)
01 91.31 92.43 (+1.12) ? 92.46 93.34 (+0.88) ? 90.84 91.73 (+0.89) 91.94 92.65 (+0.71)
23 90.84 92.23 (+1.39) 90.9 92.02 93.16 (+1.14) 91.5 90.32 91.24 (+0.92) 91.38 92.14 (+0.76)
24 89.67 91.30 (+1.63) ? 90.92 91.85 (+0.93) ? 89.55 90.06 (+0.51) 90.42 91.18 (+0.76)
Table 2: Parent-prediction accuracies on Sections 0, 1, 23, and 24. Abbreviations: dep1/dep1c = first-order parser with
baseline/cluster-based features; dep2/dep2c = second-order parser with baseline/cluster-based features; MD1 = Mc-
Donald et al (2005a); MD2 = McDonald and Pereira (2006); suffix -L = labeled parser. Unlabeled parsers are scored
using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. Improvements of
cluster-based features over baseline features are shown in parentheses.
of iterations of perceptron training, we performed up
to 30 iterations and chose the iteration which opti-
mized accuracy on the development set. Our feature
mappings are quite high-dimensional, so we elimi-
nated all features which occur only once in the train-
ing data. The resulting models still had very high
dimensionality, ranging from tens of millions to as
many as a billion features.12
All results presented in this section are given
in terms of parent-prediction accuracy, which mea-
sures the percentage of tokens that are attached to
the correct head token. For labeled dependency
structures, both the head token and dependency label
must be correctly predicted. In addition, in English
parsing we ignore the parent-predictions of punc-
tuation tokens,13 and in Czech parsing we retain
the punctuation tokens; this matches previous work
(Yamada and Matsumoto, 2003; McDonald et al,
2005a; McDonald and Pereira, 2006).
4.1 English main results
In our English experiments, we tested eight differ-
ent parsing configurations, representing all possi-
ble choices between baseline or cluster-based fea-
ture sets, first-order (Eisner, 2000) or second-order
(Carreras, 2007) factorizations, and labeled or unla-
beled parsing.
Table 2 compiles our final test results and also
includes two results from previous work by Mc-
Donald et al (2005a) and McDonald and Pereira
(2006), for the purposes of comparison. We note
a few small differences between our parsers and the
12Due to the sparsity of the perceptron updates, however,
only a small fraction of the possible features were active in our
trained models.
13A punctuation token is any token whose gold-standard part
of speech tag is one of {?? ?? : , .}.
parsers evaluated in this previous work. First, the
MD1 and MD2 parsers were trained via the MIRA
algorithm (Crammer and Singer, 2003; Crammer et
al., 2004), while we use the averaged perceptron. In
addition, the MD2 model uses only sibling interac-
tions, whereas the dep2/dep2c parsers include both
sibling and grandparent interactions.
There are some clear trends in the results of Ta-
ble 2. First, performance increases with the order of
the parser: edge-factored models (dep1 and MD1)
have the lowest performance, adding sibling rela-
tionships (MD2) increases performance, and adding
grandparent relationships (dep2) yields even better
accuracies. Similar observations regarding the ef-
fect of model order have also been made by Carreras
(2007).
Second, note that the parsers using cluster-based
feature sets consistently outperform the models us-
ing the baseline features, regardless of model order
or label usage. Some of these improvements can be
quite large; for example, a first-order model using
cluster-based features generally performs as well as
a second-order model using baseline features. More-
over, the benefits of cluster-based feature sets com-
bine additively with the gains of increasing model
order. For example, consider the unlabeled parsers
in Table 2: on Section 23, increasing the model or-
der from dep1 to dep2 results in a relative reduction
in error of roughly 13%, while introducing cluster-
based features from dep2 to dep2c yields an addi-
tional relative error reduction of roughly 14%. As a
final note, all 16 comparisons between cluster-based
features and baseline features shown in Table 2 are
statistically significant.14
14We used the sign test at the sentence level. The comparison
between dep1-L and dep1c-L is significant at p < 0.05, and all
other comparisons are significant at p < 0.0005.
599
Tagger always trained on full Treebank Tagger trained on reduced dataset
Size dep1 dep1c ? dep2 dep2c ?
1k 84.54 85.90 1.36 86.29 87.47 1.18
2k 86.20 87.65 1.45 87.67 88.88 1.21
4k 87.79 89.15 1.36 89.22 90.46 1.24
8k 88.92 90.22 1.30 90.62 91.55 0.93
16k 90.00 91.27 1.27 91.27 92.39 1.12
32k 90.74 92.18 1.44 92.05 93.36 1.31
All 90.89 92.33 1.44 92.42 93.30 0.88
Size dep1 dep1c ? dep2 dep2c ?
1k 80.49 84.06 3.57 81.95 85.33 3.38
2k 83.47 86.04 2.57 85.02 87.54 2.52
4k 86.53 88.39 1.86 87.88 89.67 1.79
8k 88.25 89.94 1.69 89.71 91.37 1.66
16k 89.66 91.03 1.37 91.14 92.22 1.08
32k 90.78 92.12 1.34 92.09 93.21 1.12
All 90.89 92.33 1.44 92.42 93.30 0.88
Table 3: Parent-prediction accuracies of unlabeled English parsers on Section 22. Abbreviations: Size = #sentences in
training corpus; ? = difference between cluster-based and baseline features; other abbreviations are as in Table 2.
4.2 English learning curves
We performed additional experiments to evaluate the
effect of the cluster-based features as the amount
of training data is varied. Note that the depen-
dency parsers we use require the input to be tagged
with parts of speech; thus the quality of the part-of-
speech tagger can have a strong effect on the per-
formance of the parser. In these experiments, we
consider two possible scenarios:
1. The tagger has a large training corpus, while
the parser has a smaller training corpus. This
scenario can arise when tagged data is cheaper
to obtain than syntactically-annotated data.
2. The same amount of labeled data is available
for training both tagger and parser.
Table 3 displays the accuracy of first- and second-
order models when trained on smaller portions of
the Treebank, in both scenarios described above.
Note that the cluster-based features obtain consistent
gains regardless of the size of the training set. When
the tagger is trained on the reduced-size datasets,
the gains of cluster-based features are more pro-
nounced, but substantial improvements are obtained
even when the tagger is accurate.
It is interesting to consider the amount by which
cluster-based features reduce the need for supervised
data, given a desired level of accuracy. Based on
Table 3, we can extrapolate that cluster-based fea-
tures reduce the need for supervised data by roughly
a factor of 2. For example, the performance of the
dep1c and dep2c models trained on 1k sentences is
roughly the same as the performance of the dep1
and dep2 models, respectively, trained on 2k sen-
tences. This approximate data-halving effect can be
observed throughout the results in Table 3.
When combining the effects of model order and
cluster-based features, the reductions in the amount
of supervised data required are even larger. For ex-
ample, in scenario 1 the dep2c model trained on 1k
sentences is close in performance to the dep1 model
trained on 4k sentences, and the dep2c model trained
on 4k sentences is close to the dep1 model trained on
the entire training set (roughly 40k sentences).
4.3 Czech main results
In our Czech experiments, we considered only unla-
beled parsing,15 leaving four different parsing con-
figurations: baseline or cluster-based features and
first-order or second-order parsing. Note that our
feature sets were originally tuned for English pars-
ing, and except for the use of Czech clusters, we
made no attempt to retune our features for Czech.
Czech dependency structures may contain non-
projective edges, so we employ a maximum directed
spanning tree algorithm (Chu and Liu, 1965; Ed-
monds, 1967; McDonald et al, 2005b) as our first-
order parser for Czech. For the second-order pars-
ing experiments, we used the Carreras (2007) parser.
Since this parser only considers projective depen-
dency structures, we ?projectivized? the PDT 1.0
training set by finding, for each sentence, the pro-
jective tree which retains the most correct dependen-
cies; our second-order parsers were then trained with
respect to these projective trees. The development
and test sets were not projectivized, so our second-
order parser is guaranteed to make errors in test sen-
tences containing non-projective dependencies. To
overcome this, McDonald and Pereira (2006) use a
15We leave labeled parsing experiments to future work.
600
dep1 dep1c dep2 dep2c
84.49 86.07 (+1.58) 86.13 87.13 (+1.00)
Table 4: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 test set, for baseline features and
cluster-based features. Abbreviations are as in Table 2.
Parser Accuracy
Nivre and Nilsson (2005) 80.1
McDonald et al (2005b) 84.4
Hall and Nova?k (2005) 85.1
McDonald and Pereira (2006) 85.2
dep1c 86.07
dep2c 87.13
Table 5: Unlabeled parent-prediction accuracies of Czech
parsers on the PDT 1.0 test set, for our models and for
previous work.
Size dep1 dep1c ? dep2 dep2c ?
1k 72.79 73.66 0.87 74.35 74.63 0.28
2k 74.92 76.23 1.31 76.63 77.60 0.97
4k 76.87 78.14 1.27 78.34 79.34 1.00
8k 78.17 79.83 1.66 79.82 80.98 1.16
16k 80.60 82.44 1.84 82.53 83.69 1.16
32k 82.85 84.65 1.80 84.66 85.81 1.15
64k 84.20 85.98 1.78 86.01 87.11 1.10
All 84.36 86.09 1.73 86.09 87.26 1.17
Table 6: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 development set. Abbreviations
are as in Table 3.
two-stage approximate decoding process in which
the output of their second-order parser is ?deprojec-
tivized? via greedy search. For simplicity, we did
not implement a deprojectivization stage on top of
our second-order parser, but we conjecture that such
techniques may yield some additional performance
gains; we leave this to future work.
Table 4 gives accuracy results on the PDT 1.0
test set for our unlabeled parsers. As in the En-
glish experiments, there are clear trends in the re-
sults: parsers using cluster-based features outper-
form parsers using baseline features, and second-
order parsers outperform first-order parsers. Both of
the comparisons between cluster-based and baseline
features in Table 4 are statistically significant.16 Ta-
ble 5 compares accuracy results on the PDT 1.0 test
set for our parsers and several other recent papers.
16We used the sign test at the sentence level; both compar-
isons are significant at p < 0.0005.
N dep1 dep1c dep2 dep2c
100 89.19 92.25 90.61 93.14
200 90.03 92.26 91.35 93.18
400 90.31 92.32 91.72 93.20
800 90.62 92.33 91.89 93.30
1600 90.87 ? 92.20 ?
All 90.89 ? 92.42 ?
Table 7: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: N = thresh-
old value; other abbreviations are as in Table 2. We
did not train cluster-based parsers using threshold values
larger than 800 due to computational limitations.
dep1-P dep1c-P dep1 dep2-P dep2c-P dep2
77.19 90.69 90.89 86.73 91.84 92.42
Table 8: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: suffix -P =
model without POS; other abbreviations are as in Table 2.
4.4 Czech learning curves
As in our English experiments, we performed addi-
tional experiments on reduced sections of the PDT;
the results are shown in Table 6. For simplicity, we
did not retrain a tagger for each reduced dataset,
so we always use the (automatically-assigned) part
of speech tags provided in the corpus. Note that
the cluster-based features obtain improvements at all
training set sizes, with data-reduction factors simi-
lar to those observed in English. For example, the
dep1c model trained on 4k sentences is roughly as
good as the dep1 model trained on 8k sentences.
4.5 Additional results
Here, we present two additional results which fur-
ther explore the behavior of the cluster-based fea-
ture sets. In Table 7, we show the development-set
performance of second-order parsers as the thresh-
old for lexical feature elimination (see Section 3.2)
is varied. Note that the performance of cluster-based
features is fairly insensitive to the threshold value,
whereas the performance of baseline features clearly
degrades as the vocabulary size is reduced.
In Table 8, we show the development-set perfor-
mance of the first- and second-order parsers when
features containing part-of-speech-based informa-
tion are eliminated. Note that the performance ob-
tained by using clusters without parts of speech is
close to the performance of the baseline features.
601
5 Related Work
As mentioned earlier, our approach was inspired by
the success of Miller et al (2004), who demon-
strated the effectiveness of using word clusters as
features in a discriminative learning approach. Our
research, however, applies this technique to depen-
dency parsing rather than named-entity recognition.
In this paper, we have focused on developing new
representations for lexical information. Previous re-
search in this area includes several models which in-
corporate hidden variables (Matsuzaki et al, 2005;
Koo and Collins, 2005; Petrov et al, 2006; Titov
and Henderson, 2007). These approaches have the
advantage that the model is able to learn different
usages for the hidden variables, depending on the
target problem at hand. Crucially, however, these
methods do not exploit unlabeled data when learn-
ing their representations.
Wang et al (2005) used distributional similarity
scores to smooth a generative probability model for
dependency parsing and obtained improvements in
a Chinese parsing task. Our approach is similar to
theirs in that the Brown algorithm produces clusters
based on distributional similarity, and the cluster-
based features can be viewed as being a kind of
?backed-off? version of the baseline features. How-
ever, our work is focused on discriminative learning
as opposed to generative models.
Semi-supervised phrase structure parsing has
been previously explored by McClosky et al (2006),
who applied a reranked parser to a large unsuper-
vised corpus in order to obtain additional train-
ing data for the parser; this self-training appraoch
was shown to be quite effective in practice. How-
ever, their approach depends on the usage of a
high-quality parse reranker, whereas the method de-
scribed here simply augments the features of an ex-
isting parser. Note that our two approaches are com-
patible in that we could also design a reranker and
apply self-training techniques on top of the cluster-
based features.
6 Conclusions
In this paper, we have presented a simple but effec-
tive semi-supervised learning approach and demon-
strated that it achieves substantial improvement over
a competitive baseline in two broad-coverage depen-
dency parsing tasks. Despite this success, there are
several ways in which our approach might be im-
proved.
To begin, recall that the Brown clustering algo-
rithm is based on a bigram language model. Intu-
itively, there is a ?mismatch? between the kind of
lexical information that is captured by the Brown
clusters and the kind of lexical information that is
modeled in dependency parsing. A natural avenue
for further research would be the development of
clustering algorithms that reflect the syntactic be-
havior of words; e.g., an algorithm that attempts to
maximize the likelihood of a treebank, according to
a probabilistic dependency model. Alternately, one
could design clustering algorithms that cluster entire
head-modifier arcs rather than individual words.
Another idea would be to integrate the cluster-
ing algorithm into the training algorithm in a limited
fashion. For example, after training an initial parser,
one could parse a large amount of unlabeled text and
use those parses to improve the quality of the clus-
ters. These improved clusters can then be used to
retrain an improved parser, resulting in an overall
algorithm similar to that of McClosky et al (2006).
Setting aside the development of new clustering
algorithms, a final area for future work is the exten-
sion of our method to new domains, such as con-
versational text or other languages, and new NLP
problems, such as machine translation.
Acknowledgments
The authors thank the anonymous reviewers for
their insightful comments. Many thanks also to
Percy Liang for providing his implementation of
the Brown algorithm, and Ryan McDonald for his
assistance with the experimental setup. The au-
thors gratefully acknowledge the following sources
of support. Terry Koo was funded by NSF grant
DMS-0434222 and a grant from NTT, Agmt. Dtd.
6/21/1998. Xavier Carreras was supported by the
Catalan Ministry of Innovation, Universities and
Enterprise, and a grant from NTT, Agmt. Dtd.
6/21/1998. Michael Collins was funded by NSF
grants 0347631 and DMS-0434222.
602
References
P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-Based n-gram Mod-
els of Natural Language. Computational Linguistics,
18(4):467?479.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task
on Multilingual Dependency Parsing. In Proceedings
of CoNLL, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proceedings of
EMNLP-CoNLL, pages 957?961.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987?89 WSJ Corpus Release 1, LDC
No. LDC2000T43. Linguistic Data Consortium.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A Statistical Parser for Czech. In Proceedings of ACL,
pages 505?512.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP, pages 1?8.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2004. Online Passive-Aggressive Algorithms. In
S. Thrun, L. Saul, and B. Scho?lkopf, editors, NIPS 16,
pages 1229?1236.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner. 2000. Bilexical Grammars and Their Cubic-
Time Parsing Algorithms. In H. Bunt and A. Nijholt,
editors, Advances in Probabilistic and Other Parsing
Technologies, pages 29?62. Kluwer Academic Pub-
lishers.
Y. Freund and R. Schapire. 1999. Large Margin Clas-
sification Using the Perceptron Algorithm. Machine
Learning, 37(3):277?296.
J. Hajic?, E. Hajic?ova?, P. Pajas, J. Panevova, and P. Sgall.
2001. The Prague Dependency Treebank 1.0, LDC
No. LDC2001T10. Linguistics Data Consortium.
J. Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In
E. Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
K. Hall and V. Nova?k. 2005. Corrective Modeling for
Non-Projective Dependency Parsing. In Proceedings
of IWPT, pages 42?52.
T. Koo and M. Collins. 2005. Hidden-Variable Models
for Discriminative Reranking. In Proceedings of HLT-
EMNLP, pages 507?514.
W. Li and A. McCallum. 2005. Semi-Supervised Se-
quence Modeling with Syntactic Topic Models. In
Proceedings of AAAI, pages 813?818.
P. Liang. 2005. Semi-Supervised Learning for Natural
Language. Master?s thesis, Massachusetts Institute of
Technology.
M.P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with Latent Annotations. In Proceedings of
ACL, pages 75?82.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective Self-Training for Parsing. In Proceedings of
HLT-NAACL, pages 152?159.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line Large-Margin Training of Dependency Parsers. In
Proceedings of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-Projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
Tagging with Word Clusters and Discriminative Train-
ing. In Proceedings of HLT-NAACL, pages 337?342.
J. Nivre and J. Nilsson. 2005. Pseudo-Projective Depen-
dency Parsing. In Proceedings of ACL, pages 99?106.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proceedings
of EMNLP-CoNLL 2007, pages 915?932.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of COLING-ACL, pages
433?440.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-Of-Speech Tagging. In Proceedings of EMNLP,
pages 133?142.
I. Titov and J. Henderson. 2007. Constituent Parsing
with Incremental Sigmoid Belief Networks. In Pro-
ceedings of ACL, pages 632?639.
Q.I. Wang, D. Schuurmans, and D. Lin. 2005. Strictly
Lexical Dependency Parsing. In Proceedings of IWPT,
pages 152?159.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis With Support Vector Machines. In
Proceedings of IWPT, pages 195?206.
603
43
44
45
46
47
48
49
50
Semantic Role Labeling: An Introduction to
the Special Issue
Llu??s Ma`rquez?
Universitat Polite`cnica de Catalunya
Xavier Carreras??
Massachusetts Institute of Technology
Kenneth C. Litkowski?
CL Research
Suzanne Stevenson?
University of Toronto
Semantic role labeling, the computational identification and labeling of arguments in text,
has become a leading task in computational linguistics today. Although the issues for this
task have been studied for decades, the availability of large resources and the development of
statistical machine learning methods have heightened the amount of effort in this field. This
special issue presents selected and representative work in the field. This overview describes
linguistic background of the problem, the movement from linguistic theories to computational
practice, the major resources that are being used, an overview of steps taken in computational
systems, and a description of the key issues and results in semantic role labeling (as revealed in
several international evaluations). We assess weaknesses in semantic role labeling and identify
important challenges facing the field. Overall, the opportunities and the potential for useful
further research in semantic role labeling are considerable.
1. Introduction
The sentence-level semantic analysis of text is concerned with the characterization of
events, such as determining ?who? did ?what? to ?whom,? ?where,? ?when,? and
?how.? The predicate of a clause (typically a verb) establishes ?what? took place,
and other sentence constituents express the participants in the event (such as ?who? and
?where?), as well as further event properties (such as ?when? and ?how?). The primary
task of semantic role labeling (SRL) is to indicate exactly what semantic relations hold
among a predicate and its associated participants and properties, with these relations
? Departament de Llenguatges i Sistemes Informa`tics, Universitat Polite`cnica de Catalunya, Jordi Girona
Salgado 1?3, 08034 Barcelona, Spain. E-mail: lluism@lsi.upc.edu.
?? Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT, 32 Vassar St., Cambridge, MA
02139, USA. E-mail: carreras@csail.mit.edu.
? CL Research, 9208 Gue Road, Damascus, MD 20872 USA. E-mail: ken@clres.com.
? Department of Computer Science, 6 King?s College Road, Toronto, ON M5S 3G4, Canada.
E-mail: suzanne@cs.toronto.edu.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
drawn from a pre-specified list of possible semantic roles for that predicate (or class of
predicates). In order to accomplish this, the role-bearing constituents in a clause must
be identified and their correct semantic role labels assigned, as in:
[The girl on the swing]Agent [whispered]Pred to [the boy beside her]Recipient
Typical roles used in SRL are labels such as Agent, Patient, and Location for the entities
participating in an event, and Temporal and Manner for the characterization of other
aspects of the event or participant relations. This type of role labeling thus yields a first-
level semantic representation of the text that indicates the basic event properties and
relations among relevant entities that are expressed in the sentence.
Research has proceeded for decades on manually created lexicons, grammars, and
other semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000)
in support of deep semantic analysis of language input, but such approaches have been
labor-intensive and often restricted to narrow domains. The 1990s saw a growth in
the development of statistical machine learning methods across the field of computa-
tional linguistics, enabling systems to learn complex linguistic knowledge rather than
requiring manual encoding. These methods were shown to be effective in acquiring
knowledge necessary for semantic interpretation, such as the properties of predicates
and the relations to their arguments?for example, learning subcategorization frames
(Briscoe and Carroll 1997) or classifying verbs according to argument structure prop-
erties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large
corpora have been manually annotated with semantic roles in FrameNet (Fillmore,
Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and
NomBank (Meyers et al 2004), enabling the development of statistical approaches
specifically for SRL.
With the advent of supporting resources, SRL has become a well-defined task with
a substantial body of work and comparative evaluation (see, among others, Gildea and
Jurafsky [2002], Surdeanu et al [2003], Xue and Palmer [2004], Pradhan et al [2005a],
the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The
identification of event frames may potentially benefit many natural language processing
(NLP) applications, such as information extraction (Surdeanu et al 2003), question
answering (Narayanan and Harabagiu 2004), summarization (Melli et al 2005), and
machine translation (Boas 2002). Related work on classifying the semantic relations in
noun phrases has also been encouraging for NLP tasks (Moldovan et al 2004; Rosario
and Hearst 2004).
Although the use of SRL systems in real-world applications has thus far been
limited, the outlook is promising for extending this type of analysis to many appli-
cations requiring some level of semantic interpretation. SRL represents an excellent
framework with which to perform research on computational techniques for acquiring
and exploiting semantic relations among the different components of a text.
This special issue of Computational Linguistics presents several articles represent-
ing the state-of-the-art in SRL, and this overview is intended to provide a broader
context for that work. First, we briefly discuss some of the linguistic views on se-
mantic roles that have had the most influence on computational approaches to SRL
and related NLP tasks. Next, we show how the linguistic notions have influenced
the development of resources that support SRL. We then provide an overview of
SRL methods and describe the state-of-the-art as well as current open problems in the
field.
146
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
2. Semantic Roles in Linguistics
Since the foundational work of Fillmore (1968), considerable linguistic research has been
devoted to the nature of semantic roles. Although there is substantial agreement on
major semantic roles, such as Agent and Theme, there is no consensus on a definitive
list of semantic roles, or even whether such a list exists. Proposed lists range from a
large set of situation-specific roles, such as Suspect, Authorities, and Offense (Fillmore,
Ruppenhofer, and Baker 2004), to a relatively small set of general roles, such as Agent,
Theme, Location, and Goal (typically referred to as thematic roles, as in Jackendoff
[1990]), to the set of two core roles, Proto-Agent and Proto-Theme, whose entailments
determine the precise relation expressed (Dowty 1991). This uncertainty within linguis-
tic theory carries over into computational work on SRL, where there is much variability
on the roles assumed in different resources.
A major focus of work in the linguistics community is on the mapping between the
predicate?argument structure that determines the roles, and the syntactic realization of
the recipients of those roles (Grimshaw 1990; Levin 1993; Levin and Rappaport Hovav
2005). Semantic role lists are generally viewed as inadequate for explaining the mor-
phosyntactic behavior of argument expression, with argument realization dependent
on a deeper lexical semantic representation of the components of the event that the
predicate describes. Although much of the mapping from argument structure to syntax
is predictable, this mapping is not completely regular, nor entirely understood. An
important question for SRL, therefore, is the extent to which performance is degraded
by the irregularities noted in linguistic studies of semantic roles.
Nonetheless, sufficient regularity exists to provide the foundation for meaningful
generalizations. Much research has focused on explaining the varied expression of verb
arguments within syntactic positions (Levin 1993). A major conclusion of that work is
that the patterns of syntactic alternation exhibit regularity that reflects an underlying
semantic similarity among verbs, forming the basis for verb classes. Such classes, and
the argument structure specifications for them, have proven useful in a number of NLP
tasks (Habash, Dorr, and Traum 2003; Shi and Mihalcea 2005), including SRL (Swier and
Stevenson 2004), and have provided the foundation for the computational verb lexicon
VerbNet (Kipper, Dang, and Palmer 2000).
This approach to argument realization focuses on the relation of morphosyntactic
behavior to argument semantics, and typically leads to a general conceptualization of
semantic roles. In frame semantics (Fillmore 1976), on the other hand, a word activates
a frame of semantic knowledge that relates linguistic semantics to encyclopedic knowl-
edge. This effort has tended to focus on the delineation of situation-specific frames (e.g.,
an Arrest frame) and correspondingly more specific semantic roles (e.g., Suspect and
Authorities) that codify the conceptual structure associated with lexical items (Fillmore,
Ruppenhofer, and Baker 2004). With a recognition that many lexical items could activate
any such frame, this approach leads to lexical classes of a somewhat different nature
than those of Levin (1993). Whereas lexical items in a Levin class are syntactically
homogeneous and share coarse semantic properties, items in a frame may syntactically
vary somewhat but share fine-grained, real-world semantic properties.
A further difference in these perspectives is the view of the roles themselves. In
defining verb classes that capture argument structure similarities, Levin (1993) does not
explicitly draw on the notion of semantic role, instead basing the classes on behavior
that is hypothesized to reflect the properties of those roles. Other work also eschews
the notion of a simple list of roles, instead postulating underlying semantic structure
that captures the relevant properties (Levin and Rappaport Hovav 1998). Interestingly,
147
Computational Linguistics Volume 34, Number 2
as described in Fillmore, Ruppenhofer, and Baker (2004), frame semantics also avoids a
predefined list of roles, but for different reasons. The set of semantic roles, called frame
elements, are chosen for each frame, rather than being selected from a predefined list
that may not capture the relevant distinctions in that particular situation. Clearly, to
the extent that disagreement persists on semantic role lists and the nature of the roles
themselves, SRL may be working on a shifting target.
These approaches also differ in the broad characterization of event participants
(and their roles) as more or less essential to the predicate. In the more syntactic-oriented
approaches, roles are typically divided into two categories: arguments, which cap-
ture a core relation, and adjuncts, which are less central. In frame semantics, the roles
are divided into core frame elements (e.g., Suspect, Authorities, Offense) and periph-
eral or extra-thematic elements (e.g., Manner, Time, Place). These distinctions carry
over into SRL, where we see that systems generally perform better on the more central
arguments.
Finally, although predicates are typically expressed as verbs, and thus much work
in both linguistics and SRL focuses on them, some nouns and adjectives may be used
predicatively, assigning their own roles to entities (as in the adjective phrase proud that
we finished the paper, where the subordinate clause is a Theme argument of the adjective
proud). Frame semantics tends to include in a frame relevant non-verb lexical items,
due to the emphasis on a common situation semantics. In contrast, the morphosyntactic
approaches have focused on defining classes of verbs only, because they depend on
common syntactic behavior that may not be apparent across syntactic categories.
Interestingly, prepositions have a somewhat dual status with regard to role labeling.
In languages like English, prepositions serve an important function in signaling the rela-
tion of a participant to a verb. For example, it is widely accepted that to in give the book to
Mary serves as a grammatical indicator of the Recipient role assigned by the verb, rather
than as a role assigner itself. In other situations, however, a preposition can be viewed
as a role-assigning predicate in its own right. Although some work in computational
linguistics is tackling the issue of the appropriate characterization of prepositions and
their contribution to semantic role assignment (as we see subsequently), much work
remains in order to fully integrate linguistic theories of prepositional function and
semantics into SRL.
3. From Linguistic Theory to Computational Resources
The linguistic approaches to semantic roles discussed previously have greatly influ-
enced current work on SRL, leading to the creation of significant computational lexicons
capturing the foundational properties of predicate?argument relations.
In the FrameNet project (Fillmore, Ruppenhofer, and Baker 2004), lexicographers
define a frame to capture some semantic situation (e.g., Arrest), identify lexical items
as belonging to the frame (e.g., apprehend and bust), and devise appropriate roles for
the frame (e.g., Suspect, Authorities, Offense). They then select and annotate example
sentences from the British National Corpus and other sources to illustrate the range of
possible assignments of roles to sentence constituents for each lexical item (at present,
over 141,000 sentences have been annotated).
FrameNet thus consists of both a computational lexicon and a role-annotated cor-
pus. The existence of such a corpus enabled Gildea and Jurafsky (2002) to develop the
first statistical machine learning approach to SRL, using various lexical and syntactic
features such as phrase type and grammatical function calculated over the annotated
constituents. Although this research spurred the current wave of SRL work that has
148
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
refined and extended Gildea and Jurafsky?s approach, the FrameNet data has not been
used extensively. One issue is that the corpus is not a representative sample of the
language, but rather consists of sentences chosen manually to illustrate the possible
role assignments for a given lexical item. Another issue is that the semantic roles are
situation-specific, rather than general roles like Agent, Theme, and Location that can be
used across many situations and genres.
The computational verb lexicon, VerbNet (Kipper, Dang, and Palmer 2000), instead
builds on Levin?s (1993) work on defining verb classes according to shared argument re-
alization patterns. VerbNet regularizes and extends the original Levin classes; moreover,
each class is explicitly associated with argument realization specifications that state the
constituents that a verb can occur with and the role assigned to each. The roles are
mostly drawn from a small set (around 25) of general roles widely used in linguistic
theory. This lexicon has been an important resource in computational linguistics, but
because of the lack of an associated role-annotated corpus, it has only been used directly
in SRL in an unsupervised setting (Swier and Stevenson 2004).
Research on VerbNet inspired the development of the Proposition Bank (PropBank;
Palmer, Gildea, and Kingsbury 2005), which has emerged as a primary resource for
research in SRL (and used in four of the articles in this special issue). PropBank ad-
dresses some of the issues for SRL posed by the FrameNet data. First, the PropBank
project has annotated the semantic roles for all verbs in the Penn Treebank corpus (the
Wall Street Journal [WSJ] news corpus). This provides a representative sample of text
with role-annotations, in contrast to FrameNet?s reliance on manually selected, illus-
trative sentences. Importantly, PropBank?s composition allows for consideration of the
statistical patterns across natural text. Although there is some concern about the limited
genre of its newspaper text, this aspect has the advantage of allowing SRL systems to
benefit from the state-of-the-art syntactic parsers and other resources developed with
the WSJ TreeBank data. Moreover, current work is extending the PropBank annotation
to balanced corpora such as the Brown corpus.
The lexical information associated with verbs in PropBank also differs significantly
from the situation-specific roles of FrameNet. At the same time, the PropBank designers
recognize the difficulty of providing a small, predefined list of semantic roles that is suf-
ficient for all verbs and predicate?argument relations, as in VerbNet. PropBank instead
takes a ?theory-neutral? approach to the designation of core semantic roles. Each verb
has a frameset listing its allowed role labelings in which the arguments are designated
by number (starting from 0). Each numbered argument is provided with an English-
language description specific to that verb. Participants typically considered as adjuncts
are given named argument roles, because there is more general agreement on such
modifiers as Temporal or Manner applying consistently across verbs. Different senses
for a polysemous verb have different framesets; however, syntactic alternations which
preserve meaning (as identified in Levin [1993]) are considered to be a single frameset.
While the designations of Arg0 and Arg1 are intended to indicate the general roles of
Agent and Theme/Patient across verbs, other argument numbers do not consistently
correspond to general (non-verb-specific) semantic roles.
Given the variability in the sets of roles used across the computational resources,
an important issue is the extent to which different role sets affect the SRL task, as well
as subsequent use of the output in other NLP applications. Gildea and Jurafsky (2002)
initiated this type of investigation by exploring whether their results were dependent
on the set of semantic roles they used. To this end, they mapped the FrameNet frame
elements into a set of abstract thematic roles (i.e., more general roles such as Agent,
Theme, Location), and concluded that their system could use these thematic roles
149
Computational Linguistics Volume 34, Number 2
without degradation. Similar questions must be investigated in the context of PropBank,
where the framesets for the verbs may have significant domain-specific meanings and
arguments due to the dependence of the project on WSJ data. Given the uncertainty in
the linguistic status of semantic role lists, and the lack of evidence about which types
of roles would be most useful in various NLP tasks, an important ongoing focus of
attention is the value of mapping between the role sets of the different resources (Swier
and Stevenson 2005; Loper, Yi, and Palmer 2007; Yi, Loper, and Palmer 2007).
We noted previously the somewhat special part that prepositions play in marking
semantic relations, in some sense mediating the role assignment of a verb to an argu-
ment. The resources noted earlier differ in their treatment of prepositions. In VerbNet,
for example, prepositions are listed explicitly as part of the syntactic context in which
a role is assigned (e.g., Agent V Prep(for) Recipient), but it is the NP object of the prep-
osition that receives the semantic role. In FrameNet and PropBank, on the other hand,
the full prepositional phrase is considered as the frame element (the constituent re-
ceiving the role). Clearly, further work needs to proceed on how to best capture the in-
teraction between verbs and prepositions in SRL. This is especially complex given
the high polysemy of prepositions, and work has proceeded on relating preposition
disambiguation to role assignment (e.g., O?Hara and Wiebe 2003). For such approaches
to make meaningful progress, resources are needed that elaborate the senses of prepo-
sitions and relate those senses to semantic roles. In The Preposition Project (TPP;
Litkowski and Hargraves 2005), a comprehensive, hierarchical characterization of the
semantic roles for all preposition senses in English is being developed. TPP has sense-
tagged more than 25,000 preposition instances in FrameNet sentences, allowing for
comprehensive investigation of the linking between preposition sense and semantic role
assignment.
4. Approaches to Automatic SRL
The work on SRL has included a broad spectrum of probabilistic and machine-learning
approaches to the task. We focus here on supervised systems, because most SRL research
takes an approach requiring training on role-annotated data. We briefly survey the main
approaches to automatic SRL, and the types of learning features used.
4.1 SRL Step by Step
Given a sentence and a designated verb, the SRL task consists of identifying the bound-
aries of the arguments of the verb predicate (argument identification) and labeling
them with semantic roles (argument classification). The most common architecture for
automatic SRL consists of the following steps to achieve these subtasks.
The first step in SRL typically consists of filtering (or pruning) the set of argu-
ment candidates for a given predicate. Because arguments may be a continuous or
discontinuous sequence of words, any subsequence of words in the sentence is an
argument candidate. Exhaustive exploration of this space of candidates is not feasible,
because it is both very large and imbalanced (i.e., the vast majority of candidates are
not actual arguments of the verb). The simple heuristic rules of Xue and Palmer (2004)
are commonly used to perform filtering because they greatly reduce the set of candidate
arguments, while maintaining a very high recall.
The second step consists of a local scoring of argument candidates by means of
a function that outputs probabilities (or confidence scores) for each of the possible
150
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
role labels, plus an extra ?no-argument? label indicating that the candidate should
not be considered an argument in the solution. In this step, candidates are usually
treated independently of each other. A crucial aspect in local scoring (see Section 4.2)
is the representation of candidates with features, rather than the particular choice of
classification algorithm.
Argument identification and classification may be treated jointly or separately in
the local scoring step. In the latter case, a pipeline of two subprocesses is typically
applied, first scoring between ?argument? and ?no-argument? labels, and then scoring
the particular argument labels. Because argument identification is closely related to
syntax and argument classification is more a semantic issue, useful features for the two
subtasks may be very different?that is, a good feature for addressing recognition may
hurt classification and vice versa (Pradhan et al 2005a).
The third step in SRL is to apply a joint scoring (or global scoring) in order to
combine the predictions of local scorers to produce a good structure of labeled argu-
ments for the predicate. In this step, dependencies among several arguments of the same
predicate can be exploited. For instance, Punyakanok, Roth, and Yih (this issue) ensure
that a labeling satisfies a set of structural and SRL-dependent constraints (arguments
do not overlap, core arguments do not repeat, etc.). Also in this issue, Toutanova,
Haghighi, and Manning apply re-ranking to select the best among a set of candidate
complete solutions produced by a base SRL system. Finally, probabilistic models have
also been applied to produce the structured output, for example, generative models
(Thompson, Levy, and Manning 2003), sequence tagging with classifiers (Ma`rquez et al
2005; Pradhan et al 2005b), and Conditional Random Fields on tree structures (Cohn
and Blunsom 2005). These approaches at a global level may demand considerable extra
computation, but current optimization techniques help solve them quite efficiently.
Some variations in the three-step architecture are found. Systems may bypass one
of the steps, by doing only local scoring, or skipping directly to joint scoring. A fourth
step may consist of fixing common errors or enforcing coherence in the final solution.
This postprocess usually consists of a set of hand-developed heuristic rules that are
dependent on a particular architecture and corpus of application.
An important consideration within this general SRL architecture is the combination
of systems and input annotations. Most SRL systems include some kind of combi-
nation to increase robustness, gain coverage, and reduce effects of parse errors. One
may combine: (1) the output of several independent SRL basic systems (Surdeanu
et al 2007; Pradhan et al 2005b), or (2) several outputs from the same SRL system
obtained by changing input annotations or other internal parameters (Koomen et al
2005; Toutanova, Haghighi, and Manning 2005). The combination can be as simple as
selecting the best among the set of complete candidate solutions, but usually consists of
combining fragments of alternative solutions to construct the final output. Finally, the
combination component may involve machine learning or not. The gain in performance
from the combination step is consistently between two and three F1 points. However, a
combination approach increases system complexity and penalizes efficiency.
Several exceptions to this described architecture for SRL can be found in the lit-
erature. One approach entails joint labeling of all predicates of the sentence, instead
of proceeding one by one. This opens the possibility of exploiting dependencies among
the different verbs in the sentence. However, the complexity may grow significantly, and
results so far are inconclusive (Carreras, Ma`rquez, and Chrupa?a 2004; Surdeanu et al
2007). Other promising approaches draw on dependency parsing rather than traditional
phrase structure parsing (Johansson and Nugues 2007), or combine parsing and SRL
into a single step of semantic parsing (Musillo and Merlo 2006).
151
Computational Linguistics Volume 34, Number 2
4.2 Feature Engineering
As previously noted, devising the features with which to encode candidate arguments
is crucial for obtaining good results in the SRL task. Given a verb and a candidate argu-
ment (a syntactic phrase) to be classified in the local scoring step, three types of features
are typically used: (1) features that characterize the candidate argument and its context;
(2) features that characterize the verb predicate and its context; and (3) features that cap-
ture the relation (either syntactic or semantic) between the candidate and the predicate.
Gildea and Jurafsky (2002) presented a compact set of features across these three
types, which has served as the core of most of the subsequent SRL work: (1) the phrase
type, headword, and governing category of the constituent; (2) the lemma, voice, and
subcategorization pattern of the verb; and (3) the left/right position of the constituent
with respect to the verb, and the category path between them. Extensions to these fea-
tures have been proposed in various directions. Exploiting the ability of some machine
learning algorithms to work with very large feature spaces, some authors have largely
extended the representation of the constituent and its context, including among others:
first and last words (and part-of-speech) in the constituent, bag-of-words, n-grams of
part of speech, and sequence of top syntactic elements in the constituent. Parent and
sibling constituents in the tree may also be codified with all the previous structural and
lexical features (Pradhan et al 2005a; Surdeanu et al 2007). Other authors have designed
new features with specific linguistic motivations. For instance, Surdeanu et al (2003)
generalized the concept of headword with the content word feature. They also used
named entity labels as features. Xue and Palmer (2004) presented the syntactic frame
feature, which captures the overall sentence structure using the verb predicate and the
constituent as pivots. All these features resulted in a significant increase in performance.
Finally, regarding the relation between the constituent and the predicate, several
variants of Gildea and Jurafsky?s syntactic path have been proposed in the literature
(e.g., generalizations to avoid sparsity, and adaptations to partial parsing). Also, some
attempts have been made at characterizing the semantic relation between the predicate
and the constituent. In Zapirain, Agirre, and Ma`rquez (2007) and Erk (2007), selectional
preferences between predicate and headword of the constituent are explored to generate
semantic compatibility features. Using conjunctions of several of the basic features is
also common practice. This may be very relevant when the machine learning method
used is linear in the space of features.
Joint scoring and combination components open the door to richer types of fea-
tures, which may take into account global properties of the candidate solution plus de-
pendencies among the different arguments. The most remarkable work in this direction
is the reranking approach by Toutanova, Haghighi, and Manning in this issue. When
training the ranker to select the best candidate solution they codify pattern features as
strings containing the whole argument structure of the candidate. Several variations
of this type of feature (with different degrees of generalization to avoid sparseness)
allow them to significantly increase the performance of the base system. Also related,
Pradhan et al (2005b) and Surdeanu et al (2007) convert the confidence scores of several
base SRL systems into features for training a final machine learning?based combination
system. Surdeanu et al (2007) develop a broad spectrum of features, with sentence-
based information, describing the role played by the candidate argument in every
solution proposed by the different base SRL systems.
A completely different approach to feature engineering is the use of kernel meth-
ods to implicitly exploit all kinds of substructures in the syntactic representation of
the candidates. This knowledge poor approach intends to take advantage of a massive
152
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
quantity of features without the need for manual engineering of specialized features.
This motivation might be relevant for fast system development and porting, especially
when specialized linguistic knowledge of the language of application is not available.
The most studied approach consists of using some variants of the ?all subtrees kernel?
applied to the sentence parse trees. The work by Moschitti, Pighin, and Basili in this
issue is the main representative of this family.
5. Empirical Evaluations of SRL Systems
Many experimental studies have been conducted since the work of Gildea and Jurafsky
(2002), including seven international evaluation tasks in ACL-related conferences and
workshops: the SIGNLL CoNLL shared tasks in 2004 and 2005 (Carreras and Ma`rquez
2004, 2005), the SIGLEX Senseval-3 in 2004 (Litkowski 2004), and four tasks in the
SIGLEX SemEval in 2007 (Pradhan et al 2007; Ma`rquez et al 2007; Baker, Ellsworth, and
Erk 2007; Litkowski and Hargraves 2007). In the subsequent sections, we summarize
their main features, results, and conclusions, although note that the scores are not
directly comparable across different exercises, due to differences in scoring and in the
experimental methodologies.
5.1 Task Definition and Evaluation Metrics
The standard experiment in automatic SRL can be defined as follows: Given a sentence
and a target predicate appearing in it, find the arguments of the predicate and label
them with semantic roles. A system is evaluated in terms of precision, recall, and F1 of
the labeled arguments. In evaluating a system, an argument is considered correct when
both its boundaries and the semantic role label match a gold standard. Performance
can be divided into two components: (1) the precision, recall, and F1 of unlabeled
arguments, measuring the accuracy of the system at segmenting the sentence; and (2)
the classification accuracy of assigning semantic roles to the arguments that have been
correctly identified. In calculating the metrics, the de facto standard is to give credit only
when a proposed argument perfectly matches an argument in the reference solution;
nonetheless, variants that give some credit for partial matching also exist.
5.2 Shared Task Experiments Using FrameNet, PropBank, and VerbNet
To date, most experimental work has made use of English data annotated either with
PropBank or FrameNet semantic roles.
The CoNLL shared tasks in 2004 and 2005 were based on PropBank (Carreras and
Ma`rquez 2004, 2005), which is the largest evaluation benchmark available today, and
also the most used by researchers?all articles in this special issue dealing with English
use this benchmark. In the evaluation, the best systems obtained an F1 score of ?80%,
and have achieved only minimal improvements since then. The articles in this issue by
Punyakanok, Roth, and Yih; Toutanova, Haghighi, and Manning; and Pradhan, Ward,
and Martin describe such efforts. An analysis of the outputs in CoNLL-2005 showed
that argument identification accounts for most of the errors: a system will recall ?81%
of the correct unlabeled arguments, and ?95% of those will be assigned the correct
semantic role. The analysis also showed that systems recognized core arguments better
than adjuncts (with F1 scores from the high 60s to the high 80s for the former, but below
60% for the latter). Finally, it was also observed that, although systems performed better
153
Computational Linguistics Volume 34, Number 2
on verbs appearing frequently in training, the best systems could recognize arguments
of unseen verbs with an F1 in the low 70s, not far from the overall performance.
1
SemEval-2007 included a task on semantic evaluation for English, combining word
sense disambiguation and SRL based on PropBank (Pradhan et al 2007). Unlike the
CoNLL tasks, this task concentrated on 50 selected verbs. Interestingly, the data was
annotated using verb-independent roles using the PropBank/VerbNet mapping from
Yi, Loper, and Palmer (2007). The two participating systems could predict VerbNet roles
as accurately as PropBank verb-dependent roles.
Experiments based on FrameNet usually concentrate on a selected list of frames.
In Senseval-3, 40 frames were selected for an SRL task with the goal of replicating
Gildea and Jurafsky (2002) and improving on them (Litkowski 2004). Participants were
evaluated on assigning semantic roles to given arguments, with best F1 of 92%, and on
the task of segmenting and labeling arguments, with best F1 of 83%.
SemEval-2007 also included an SRL task based on FrameNet (Baker, Ellsworth, and
Erk 2007). It was much more complete, realistic, and difficult than its predecessor in
Senseval-3. The goal was to perform complete analysis of semantic roles on unseen
texts, first determining the appropriate frames of predicates, and then determining their
arguments labeled with semantic roles. It also involved creating a graph of the sentence
representing part of its semantics, by means of frames and labeled arguments. The
test data of this task consisted of novel manually-annotated documents, containing a
number of frames and roles not in the FrameNet lexicon. Three teams submitted results,
with precision percentages in the 60s, but recall percentages only in the 30s.
To our knowledge, there is no evidence to date on the relative difficulty of assigning
FrameNet or PropBank roles.
5.3 Impact of Syntactic Processing in SRL
Semantic roles are closely related to syntax, and, therefore, automatic SRL heavily relies
on the syntactic structure of the sentence. In PropBank, over 95% of the arguments
match with a single constituent of the parse tree. If the output produced by a statistical
parser is used (e.g., Collins?s or Charniak?s) the exact matching is still over 90%. More-
over, some simple rules can be used to join constituents and fix a considerable portion
of the mismatches (Toutanova, Haghighi, and Manning 2005). Thus, it has become a
common practice to use full parse trees as the main source for solving SRL.
The joint model presented in this issue by Toutanova, Haghighi, and Manning
obtains an F1 at ?90% on the WSJ test of the CoNLL-2005 evaluation when using gold-
standard trees; but with automatic syntactic analysis, its best result falls to ?80%. This
and other work consistently show that the drop in performance occurs in identifying
argument boundaries; when arguments are identified correctly with predicted parses,
the accuracy of assigning semantic roles is similar to that with correct parses.
A relevant question that has been addressed in experimental work concerns the
use of a partial parser instead of a parser that produces full WSJ trees. In the CoNLL-
2004 task, systems were restricted to the use of base syntactic phrases (i.e., chunks)
and clauses, and the best results that could be obtained were just below 70%. But the
training set in that evaluation was about five times smaller than that of the 2005 task.
Punyakanok, Roth, and Yih (this issue) and Surdeanu et al (2007) have shown that, in
1 The analysis summarized here was presented in the oral session at CoNLL-2005. The slides of the session,
containing the results supporting this analysis, are available in the CoNLL-2005 shared task Web site.
154
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
fact, a system working with partial parsing can do almost as well as a system working
with full parses, with differences in F1 of only ?2?3 points.
Currently, the top-performing systems on the CoNLL data make use of several
outputs of syntactic parsers, as discussed in Section 4. It is clear that many errors in
SRL are caused by having incorrect syntactic constituents, as reported by Punyakanok,
Roth, and Yih in this issue. By using many parses, the recognition of semantic roles is
more robust to parsing errors. Yet, it remains unanswered what is the most appropriate
level of syntactic analysis needed in SRL.
5.4 Generalization of SRL Systems to New Domains
Porting a system to a new domain, different than the domain used to develop and train
the system, is a challenging question in NLP. SRL is no exception, with the particular
difficulty that a predicate in a new domain may exhibit a behavior not contemplated
in the dictionary of frames at training time. This difficulty was identified as a major
challenge in the FrameNet-based task in SemEval-2007 (Baker, Ellsworth, and Erk 2007).
In the CoNLL-2005 task, WSJ-trained systems were tested on three sections of
the Brown corpus annotated by the PropBank team. The performance of all systems
dropped dramatically: The best systems scored F1 below 70%, as opposed to figures at
?80% when testing on WSJ data. This is perhaps not surprising, taking into account that
the pre-processing systems involved in the analysis (tagger and parser) also experienced
a significant drop in performance. The article in this issue by Pradhan, Ward, and
Martin further investigates the robustness across text genres when porting a system
from WSJ to Brown. Importantly, the authors claim that the loss in accuracy takes place
in assigning the semantic roles, rather than in the identification of argument boundaries.
5.5 SRL on Languages Other Than English
SemEval-2007 featured the first evaluation exercise of SRL systems for languages other
than English, namely for Spanish and Catalan (Ma`rquez et al 2007). The data was part
of the CESS-ECE corpus, consisting of ?100K tokens for each language. The semantic
role annotations are similar to PropBank, in that role labels are specific to each verb,
but also include a verb-independent thematic role label similar to the scheme proposed
in VerbNet. The task consisted of assigning semantic class labels to target verbs, and
identifying and labeling arguments of such verbs, in both cases using gold-standard
syntax. Only two teams participated, with best results at ?86% for disambiguating
predicates, and at ?83% for labeling arguments.
The work by Xue in this issue studies semantic role labeling for Chinese, using the
Chinese PropBank and NomBank corpora. Apart from working also with nominalized
predicates, this work constitutes the first comprehensive study on SRL for a language
different from English.
5.6 SRL with Other Parts-of-Speech
The SemEval-2007 task on disambiguating prepositions (Litkowski and Hargraves 2007)
used FrameNet sentences as the training and test data, with over 25,000 sentences for
the 34 most common English prepositions. Although not overtly defined as semantic
role labeling, each instance was characterized with a semantic role name and also had
an associated FrameNet frame element. Almost 80% of the prepositional phrases in the
instances were identified as core frame elements, and are likely to be closely associated
155
Computational Linguistics Volume 34, Number 2
with arguments of the words to which they are attached. The three participants used a
variety of methods, with the top performing team using machine learning techniques
similar to those in other semantic role labeling tasks.
6. Final Remarks
To date, SRL systems have been shown to perform reasonably well in some controlled
experiments, with F1 measures in the low 80s on standard test collections for English.
Still, a number of important challenges exist for future research on SRL. It remains
unclear what is the appropriate level of syntax needed to support robust analysis of
semantic roles, and to what degree improved performance in SRL is constrained by the
state-of-the-art in tagging and parsing. Beyond syntax, the relation of semantic roles to
other semantic knowledge (such as WordNet, named entities, or even a catalogue of
frames) has scarcely been addressed in the design of current SRL models. A deeper
understanding of these questions could help in developing methods that yield im-
proved generalization, and that are less dependent on large quantities of role-annotated
training data.
Indeed, the requirement of most SRL approaches for such training data, which is
both difficult and highly expensive to produce, is the major obstacle to the widespread
application of SRL across different genres and different languages. Given the degrada-
tion of performance when a supervised system is faced with unseen events or a testing
corpus different from training, this is a major impediment to increasing the application
of SRL even within English, a language for which two major annotated corpora are
available. It is critical for the future of SRL that research broadens to include wider
investigation of unsupervised and minimally supervised learning methods.
In addition to these open research problems, there are also methodological issues
that need to be addressed regarding how research is conducted and evaluated. Shared
task frameworks have been crucial in SRL development by supporting explicit compar-
isons of approaches, but such benchmark testing can also overly focus research efforts
on small improvements in particular evaluation measures. Improving the entire SRL
approach in a significant way may require more open-ended investigation and more
qualitative analysis.
Acknowledgments
We are grateful for the insightful comments
of two anonymous reviewers whose input
helped us to improve the article. This work
was supported by the Spanish Ministry of
Education and Science (Ma`rquez); the
Catalan Ministry of Innovation, Universities
and Enterprise; and a grant from NTT, Agmt.
Dtd. 6/21/1998 (Carreras); and NSERC of
Canada (Stevenson).
References
Baker, C., M. Ellsworth, and K. Erk. 2007.
SemEval-2007 Task 19: Frame semantic
structure extraction. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007), pages 99?104,
Prague, Czech Republic.
Boas, H. C. 2002. Bilingual framenet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1364?1371,
Las Palmas de Gran Canaria, Spain.
Briscoe, T. and J. Carroll. 1997. Automatic
extraction of subcategorization from
corpora. In Proceedings of the 5th ACL
Conference on Applied Natural Language
Processing (ANLP), pages 356?363,
Washington, DC.
Carreras, X. and L. Ma`rquez. 2004.
Introduction to the CoNLL-2004 Shared
Task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning (CoNLL-2004),
pages 89?97, Boston, MA.
Carreras, X. and L. Ma`rquez. 2005.
Introduction to the CoNLL-2005 Shared
156
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
Task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 152?164, Ann Arbor, MI.
Carreras, X., L. Ma`rquez, and G. Chrupa?a.
2004. Hierarchical recognition of
propositional arguments with perceptrons.
In Proceedings of the Eighth Conference on
Computational Natural Language Learning
(CoNLL-2004), pages 106?109, Boston, MA.
Cohn, T. and P. Blunsom. 2005. Semantic role
labelling with tree conditional random
fields. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 169?172,
Ann Arbor, MI.
Copestake, A. and D. Flickinger. 2000. An
open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings
of the Second International Conference on
Language Resources and Evaluation (LREC),
pages 591?600, Athens, Greece.
Dowty, D. 1991. Thematic proto-roles and
argument selection. Language, 67:547?619.
Erk, K. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics,
pages 216?223, Prague, Czech Republic.
Fillmore, C. 1968. The case for case. In
E. Bach and R. T. Harms, editors, Universals
in Linguistic Theory. Holt, Rinehart &
Winston, New York, pages 1?88.
Fillmore, C. J. 1976. Frame semantics and the
nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin
and Development of Language and Speech,
280:20?32.
Fillmore, C. J., J. Ruppenhofer, and C. F.
Baker. 2004. Framenet and representing
the link between semantic and syntactic
relations. In Churen Huang and Winfried
Lenders, editors, Frontiers in Linguistics,
volume I of Language and Linguistics
Monograph Series B. Institute of Linguistics,
Academia Sinica, Taipei, pages 19?59.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Grimshaw, J. 1990. Argument Structure. MIT
Press, Cambridge, MA.
Habash, N., B. J. Dorr, and D. Traum. 2003.
Hybrid natural language generation from
lexical conceptual structures. Machine
Translation, 18(2):81?128.
Hirst, G. 1987. Semantic Interpretation and
the Resolution of Ambiguity. Cambridge
University Press, Cambridge.
Jackendoff, R. 1990. Semantic Structures. MIT
Press, Cambridge, MA.
Johansson, R. and P. Nugues. 2007. LTH:
Semantic structure extraction using
nonprojective dependency trees. In
Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 227?230, Prague,
Czech Republic.
Kipper, K., H. T. Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon.
In Proceedings of the 17th National Conference
on Artificial Intelligence (AAAI-2000),
Austin, TX.
Koomen, P., V. Punyakanok, D. Roth, and
W. Yih. 2005. Generalized inference
with multiple semantic role labeling
systems. In Proceedings of the Ninth
Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 181?184, Ann Arbor, MI.
Levin, B. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
The University of Chicago Press,
Chicago, IL.
Levin, B. and M. Rappaport Hovav. 1998.
Building verb meanings. In M. Butt and
W. Geuder, editors, The Projection of
Arguments: Lexical and Compositional
Factors. CSLI Publications, Stanford, CA,
pages 97?134.
Levin, B. and M. Rappaport Hovav. 2005.
Argument Realization. Cambridge
University Press, Cambridge.
Litkowski, K. C. 2004. Senseval-3 task:
Automatic labeling of semantic roles. In
Proceedings of the 3rd International Workshop
on the Evaluation of Systems for the Semantic
Analysis of Text (Senseval-3), pages 9?12,
Barcelona, Spain.
Litkowski, K. C. and O. Hargraves. 2005.
The preposition project. In Proceedings
of the ACL-SIGSEM Workshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistic
Formalisms and Applications, pages 171?179,
Colchester, UK.
Litkowski, K. C. and O. Hargraves. 2007.
SemEval-2007 Task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007),
pages 24?29, Prague, Czech Republic.
Loper, E., S. Yi, and M. Palmer. 2007.
Combining lexical resources: Mapping
between PropBank and VerbNet. In
Proceedings of the 7th International Workshop
on Computational Semantics, pages 118?128,
Tilburg, The Netherlands.
157
Computational Linguistics Volume 34, Number 2
Ma`rquez, L., P. R. Comas, J. Gime?nez, and
N. Catala`. 2005. Semantic role labeling
as sequential tagging. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 193?196, Ann Arbor, MI.
Ma`rquez, L., L. Villarejo, M.A. Mart??, and
M. Taule?. 2007. SemEval-2007 Task 09:
Multilevel semantic annotation of Catalan
and Spanish. In Proceedings of the 4th
International Workshop on Semantic
Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani,
Z. Shi, B. Gu, A. Sarkar, and F. Popowich.
2005. Description of SQUASH, the SFU
question answering summary handler
for the DUC-2005 Summarization Task.
In Proceedings of the HLT/EMNLP
Document Understanding Workshop (DUC),
Vancouver, Canada, available at
http://duc.nist.gov/pubs/2005papers/
simonfraseru.sarkar.pdf.
Merlo, P. and S. Stevenson. 2001. Automatic
verb classification based on statistical
distributions of argument structure.
Computational Linguistics, 27(3):373?408.
Meyers, A., R. Reeves, C. Macleod,
R. Szekely, V. Zielinska, B. Young, and
R. Grishman. 2004. The NomBank Project:
An interim report. In Proceedings of
the HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31,
Boston, MA.
Moldovan, D., A. Badulescu, M. Tatu,
D. Antohe, and R. Girju. 2004. Models for
the semantic classification of noun
phrases. In Proceedings of the HLT-NAACL
2004 Workshop on Computational Lexical
Semantics, pages 60?67, Boston, MA.
Musillo, G. and P. Merlo. 2006. Accurate
parsing of the proposition bank. In
Proceedings of the Human Language
Technology Conference of the NAACL,
pages 101?104, New York, NY.
Narayanan, S. and S. Harabagiu. 2004.
Question answering based on semantic
structures. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva, Switzerland.
O?Hara, T. and J. Wiebe. 2003. Preposition
semantic classification via Penn Treebank
and FrameNet. In Proceedings of the
Seventh Conference on Computational
Natural Language Learning (CoNLL-2003),
pages 79?86, Edmonton, Canada.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The Proposition Bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pradhan, S., K. Hacioglu, V. Krugler,
W. Ward, J. Martin, and D. Jurafsky. 2005a.
Support vector learning for semantic
argument classification. Machine Learning,
60(1):11?39.
Pradhan, S., K. Hacioglu, W. Ward, J. H.
Martin, and D. Jurafsky. 2005b. Semantic
role chunking combining complementary
syntactic views. In Proceedings of the
Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 217?220, Ann Arbor, MI.
Pradhan, S., E. Loper, D. Dligach, and
M. Palmer. 2007. SemEval-2007 Task 17:
English lexical sample, SRL and all words.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague,
Czech Republic.
Pustejovsky, J. 1995. The Generative Lexicon.
MIT Press, Cambridge, MA.
Rosario, B. and M. Hearst. 2004. Classifying
semantic relations in bioscience text. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 430?437, Barcelona, Spain.
Schulte im Walde, S. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32(2):159?194.
Shi, L. and R. Mihalcea. 2005. Putting pieces
together: Combining FrameNet, VerbNet
and WordNet for robust semantic parsing.
In Computational Linguistics and Intelligent
Text Processing; Sixth International
Conference, CICLing 2005, Proceedings,
LNCS, vol 3406, pages 100?111, Mexico
City, Mexico.
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using
predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 8?15,
Sapporo, Japan.
Surdeanu, M., L. Ma`rquez, X. Carreras, and
P. R. Comas. 2007. Combination strategies
for semantic role labeling. Journal of
Artificial Intelligence Research (JAIR),
29:105?151.
Swier, R. and S. Stevenson. 2004.
Unsupervised semantic role labelling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 95?102, Barcelona, Spain.
Swier, R. and S. Stevenson. 2005. Exploiting
a verb lexicon in automatic semantic role
158
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
labelling. In Proceedings of the Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMLNP), pages
883?890, Vancouver, B.C., Canada.
Thompson, C. A., R. Levy, and C. Manning.
2003. A generative model for semantic role
labeling. In Proceedings of the 14th European
Conference on Machine Learning (ECML),
pages 397?408, Dubrovnik, Croatia.
Toutanova, K., A. Haghighi, and C. Manning.
2005. Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 589?596, Ann Arbor, MI.
Xue, N. and M. Palmer. 2004. Calibrating
features for semantic role labeling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 88?94, Barcelona, Spain.
Yi, S., E. Loper, and M. Palmer. 2007. Can
semantic roles generalize across corpora?
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 548?555, Rochester, NY.
Zapirain, B., E. Agirre, and L. Ma`rquez. 2007.
UBC-UPC: Sequential SRL using
selectional preferences: an approach with
maximum entropy Markov models. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007),
pages 354?357, Prague, Czech Republic.
159

 	
	ff 
		ffA Simple Named Entity Extractor using AdaBoost
Xavier Carreras and Llu??s Ma`rquez and Llu??s Padro?
TALP Research Center
Departament de Llenguatges i Sistemes Informa`tics
Universitat Polite`cnica de Catalunya
{carreras,lluism,padro}@lsi.upc.es
1 Introduction
This paper presents a Named Entity Extraction (NEE)
system for the CoNLL-2003 shared task competition. As
in the past year edition (Carreras et al, 2002a), we have
approached the task by treating the two main sub?tasks of
the problem, recognition (NER) and classification (NEC),
sequentially and independently with separate modules.
Both modules are machine learning based systems, which
make use of binary and multiclass AdaBoost classifiers.
Named Entity recognition is performed as a greedy se-
quence tagging procedure under the well?known BIO la-
belling scheme. This tagging process makes use of three
binary classifiers trained to be experts on the recognition
of B, I, and O labels, respectively. Named Entity classifi-
cation is viewed as a 4?class classification problem (with
LOC, PER, ORG, and MISC class labels), which is straight-
forwardly addressed by the use of a multiclass learning
algorithm.
The system presented here consists of a replication,
with some minor changes, of the system that obtained the
best results in the CoNLL-2002 NEE task. Therefore, it
can be considered as a benchmark of the state?of?the?
art technology for the current edition, and will allow also
to make comparisons about the training corpora of both
editions.
2 Learning the Decisions
We use AdaBoost with confidence rated predictions as
learning algorithm for the classifiers involved in the sys-
tem. More particularly, the basic binary version has been
used to learn the I, O, and B classifiers for the NER
module, and the multiclass multilabel extension (namely
AdaBoost.MH) has been used to perform entity classifi-
cation.
The idea of these algorithms is to learn an accurate
strong classifier by linearly combining, in a weighted vot-
ing scheme, many simple and moderately?accurate base
classifiers or rules. Each base rule is learned sequen-
tially by presenting the base learning algorithm a weight-
ing over the examples, which is dynamically adjusted de-
pending on the behavior of the previously learned rules.
AdaBoost has been applied, with significant success, to
a number of problems in different areas, including NLP
tasks (Schapire, 2002). We refer the reader to (Schapire
and Singer, 1999) for details about the general algorithms
(for both the binary and multiclass variants), and (Car-
reras and Ma`rquez, 2001; Carreras et al, 2002b) for par-
ticular applications to NLP domains.
In our setting, the boosting algorithm combines sev-
eral small fixed?depth decision trees, as base rules. Each
branch of a tree is, in fact, a conjunction of binary fea-
tures, allowing the strong boosting classifier to work with
complex and expressive rules.
3 Feature Representation
A window W anchored in a word w represents the local
context of w used by a classifier to make a decision on
that word. In the window, each word around w is cod-
ified with a set of primitive features, together with its
relative position to w. Each primitive feature with each
relative position and each possible value forms a final bi-
nary feature for the classifier (e.g., ?the word form at
position(-2) is street?). The kind of information coded
in those features may be grouped in the following kinds:
? Lexical: Word forms and their position in the win-
dow (e.g., W (3)=?bank?). When available, word
lemmas and their position in the window.
? Syntactic: Part-of-Speech tags and Chunk tags.
? Orthographic: Word properties with regard to how
is it capitalized (initial-caps, all-caps), the kind
of characters that form the word (contains-digits,
all-digits, alphanumeric, roman-number), the pres-
ence of punctuation marks (contains-dots, contains-
hyphen, acronym), single character patterns (lonely-
initial, punctuation-mark, single-char), or the mem-
bership of the word to a predefined class (functional-
word1), or pattern (URL).
? Affixes: The prefixes and suffixes of the word (up to
4 characters).
? Word Type Patterns: Type pattern of consecutive
words in the context. The type of a word is ei-
ther functional (f), capitalized (C), lowercased (l),
punctuation mark (.), quote (?) or other (x). For
instance, the word type pattern for the phrase ?John
Smith payed 3 euros? would be CClxl.
? Left Predictions: The {B,I,O} tags being predicted
in the current classification (at recognition stage), or
the predicted category for entities in left context (at
classification stage).
? Bag-of-Words: Form of the words in the window,
without considering positions (e.g., ?bank?? W ).
? Trigger Words: Triggering properties of window
words. An external list is used to determine whether
a word may trigger a certain Named Entity (NE)
class (e.g., ?president? may trigger class PER).
? Gazetteer Features: Gazetteer information for win-
dow words. An external gazetteer is used to deter-
mine possible classes for each word.
4 The NER Module
The Named Entity recognition task is performed as a
combination of local classifiers which test simple deci-
sions on each word in the text.
According to a BIO labelling scheme, each word is
tagged as either the beginning of a NE (B tag), a word
inside a NE (I tag), or a word outside a NE (O tag).
We use three binary classifiers for the tagging, one cor-
responding to each tag. All the words in the train set are
used as training examples, applying a one-vs-all binariza-
tion. When tagging, the sentence is processed from left to
right, greedily selecting for each word the tag with maxi-
mum confidence that is coherent with the current solution
(e.g., O tags cannot be followed by I tags). Despite its
simplicity, the greedy BIO tagging performed very well
for the NER task. Other more sophisticated represen-
tations and tagging schemes, studied in the past edition
(Carreras et al, 2002a), did not improve the performance
at all.
The three classifiers use the same information to codify
examples. According to the information types introduced
in section 3, all the following features are considered for
each target word: lexical, syntactic, orthographic, and
affixes in a {-3,+3} window; left predictions in a {-3,-1}
1Functional words are determiners and prepositions which
typically appear inside NEs.
window; and all the word type patterns that cover the 0
position in a {-3,+3} window.
The semantic information represented by the rest
of features, namely bag-of-words, trigger words, and
gazetteer features, did not help the recognition of
Named Entities, and therefore was not used.
5 The NEC Module
NEC is regarded as a classification task, consisting of as-
signing the NE type to each already recognized NE. In
contrast to the last year system, the problem has not been
binarized and treated in an ECOC (error correcting out-
put codes) combination scheme. Instead, the multiclass
multilabel AdaBoost.MH algorithm has been used. The
reason is that although ECOC provides slightly better re-
sults, its computational cost is also much higher than the
required for AdaBoost.MH.
The algorithm has been employed with different pa-
rameterizations, by modeling NEC either as a three-class
classification problem (in which MISC is selected only
when the entity is negatively classified as PER, ORG and
LOC) or as a four-class problem, in which MISC is just
one more class. The latter turned out to be the best choice
(with very significant differences).
The window information described in section 3 is
used in the NEC module computing all features for a
{-3,+3} window around the NE being classified, ex-
cept for the bag-of-words group, for which a {-5,+5}
window is used. Information relative to orthographic,
left predictions, and bag-of-words features is straight-
forwardly coded as described above, but other requires
further detail:
? Lexical features: Apart from word form and lemma
for each window position, two additional binary fea-
tures are used: One is satisfied when the focus NE
form and lemma coincide exactly, and the other
when they coincide after turning both of them into
lowercase.
? Syntactic features: Part-of-Speech (PoS) and
Chunk tags of window words (e.g., W (3).PoS=NN).
PoS and Chunk pattern of the NE (e.g.,
NNPS POS JJ for the NE ?People ?s Daily?)
? Affix features: Prefixes and suffixes of all window
words. Prefixes and suffixes of the NE being classi-
fied and of its internal components (e.g., considering
the entity ?People ?s Daily?, ?ly? is taken as a suf-
fix of the NE, ?ple? is taken as a suffix of the first
internal word, etc.).
? Trigger Words: Triggering properties of window
words (e.g., W (3).trig=PER). Triggering properties
of components of the NE being classified (e.g., for
the entity ?Bank of England? we could have a fea-
ture NE(1).trig=ORG). Context patterns to the left
of the NE, where each word is marked with its trig-
gering properties, or with a functional?word tag if
appropriate (e.g., the phrase ?the president of United
States?, would produce the pattern f ORG f for the
NE ?United States?, assuming that the word ?presi-
dent? is listed as a possible trigger for ORG).
? Gazetteer Features: Gazetteer information for
the NE being classified and for its components
(e.g., for the entity ?Bank of England?, features
NE(3).gaz=LOC and NE.gaz=ORG would be acti-
vated if ?England? is found in the gazetteer as LOC
and ?Bank of England? as ORG, respectively.
? Additionally, binary features encoding the length in
words of the NE being classified are also used.
6 Experimental Setting
The list of functional words for the task has been automat-
ically constructed using the training set. The lowercased
words inside a NE that appeared more than 3 times were
selected as functional words for the language.
Similarly, a gazetteer was constructed with the NEs in
the training set. When training, only a random 40% of the
entries in the gazetteer were considered. Moreover, we
used external knowledge in the form of a list of trigger
words for NEs and an external gazetteer. These knowl-
edge sources are the same that we used in the last year
competition for Spanish NEE. The entries of the trigger?
word list were linked to the Spanish WordNet, so they
have been directly translated by picking the correspond-
ing synsets of the English WordNet. The gazetteer has
been left unchanged, assuming interlinguality of most of
the entries. The gazetteer provided by the CoNLL-2003
organization has not been used in the work reported in
this paper.
In all cases, a preprocess of attribute filtering was per-
formed in order to avoid overfitting and to speed?up
learning. All features that occur less than 3 times in the
training corpus were discarded.
For each classification problem we trained the corre-
sponding AdaBoost classifiers, learning up to 4,000 base
decision trees per classifier, with depths ranging from 1
(decision stumps) to 4. The depth of the base rules and
the number of rounds were directly optimized on the de-
velopment set. The set of unlabelled examples provided
by the organization was not used in this work.
7 Results
The described system has been applied to both languages
in the shared task, though German and English environ-
ments are not identical: The German corpus enables the
use of lemma features while English does not. Also, the
used trigger word list is available for English but not for
German.
The results of the BIO model for the NER task on
the development and test sets for English and German
are presented in table 1. As will be seen later for the
whole task, the results are systematically better for En-
glish than for German. As it can be observed, the be-
haviour on the development and test English sets is quite
different. While in the development set the NER mod-
ule achieves a very good balance between precision and
recall, in the test set the precision drops almost 4 points,
being the F1 results much worse. On the contrary, de-
velopment and test sets for German are much more sim-
ilar. In this case, recall levels obtained for the language
are much lower compared to precision ones. This fact is
indicating the difficulty for reliably detecting the begin-
nings of the Named Entities in German (all common and
proper nouns are capitalized). Probably, a non?greedy
tagging procedure would have the chance to improve the
recognition results.
Precision Recall F?=1
English devel. 95.65% 95.51% 95.58
English test 91.93% 94.02% 92.96
German devel. 88.15% 71.55% 78.99
German test 85.87% 72.61% 78.68
Table 1: Results of the BIO recognizer for the NER task
Regarding NEC task, optimal feature selection is dif-
ferent for each language: Chunk information is almost
useless in English (or even harmful, when combined with
PoS features), but useful in German. On the contrary, al-
though the use of left predictions for NEC is useful for
English, the lower accuracy of the German system ren-
ders those features harmful (they are very useful when
assuming perfect left predictions). Table 2 presents NEC
accuracy results assuming perfect recognition of entities.
English German
features accuracy features accuracy
basic 91.47% basic 79.02%
basic+P 92.14% basic+P 79.29%
basic+C 91.60% basic+C 79.04%
basic+PC 92.12% basic+PC 79.91%
basic+Pg 93.86% basic+PCg 81.54%
basic+PG 95.05% basic+PCG 85.12%
basic+PGT 95.14%
Table 2: NEC accuracy on the development set assuming
a perfect recognition of named entities
The basic feature set includes all lexical, orthographic,
affix and bag?of?words information. P stands for Part-of-
Speech features, C for chunking?related information, T
for trigger?words features and g/G for gazetteer?related
information2. In general, more complex features sets
yield better results, except for the C case in English, as
commented above.
Table 4 presents the results on the NEE task obtained
by pipelining the NER and NEC modules. The NEC
module used both knowledge extracted from the training
set as well as external sources such as the gazetteer or
trigger word lists.
Almost the same conclusions extracted from the NER
results apply to the complete task, although here the re-
sults are lower due to the cascade of errors introduced by
the two modules: 1) Results on English are definitely bet-
ter than on German; 2) Development and test sets present
a regular behaviour in German, while for English they are
significantly different. We find the latter particularly dis-
appointing because it is indicating that no reliable con-
clusions can be extracted about the generalization error
of the NEE system constructed, by testing it on a 3,000
sentence corpus. This may be caused by the fact that the
training set is no representative enough, or by a too biased
learning of the NEE system towards the development set.
Regarding particular categories, we can see that for En-
glish the results are not extremely dissimilar (F1 values
fall in a range of 10 points for each set), being LOC and
PER the most easy to identify and ORG and MISC the most
difficult. Comparatively, in the German case bigger dif-
ferences are observed (F1 ranges from 52.58 to 80.79 in
the test set), e.g., recognition of MISC entities is far worse
than all the rest. Another slight difference against English
is that the easiest category is PER instead of LOC.
In order to allow fair comparison with other systems,
table 3 presents the results achieved on the development
set without using external knowledge. The features used
correspond to the basic model plus Part-of-Speech infor-
mation (plus Chunks for German), plus a gazetteer build
with the entities appearing in the training corpus.
Precision Recall F?=1
English devel. 90.34% 90.21% 90.27
English test 83.19% 85.07% 84.12
German devel. 74.87% 60.77% 67.09
German test 74.69% 63.16% 68.45
Table 3: Overall results using no external knowledge
Acknowledgments
This research has been partially funded by the European
Commission (Meaning, IST-2001-34460) and the Span-
ish Research Dept. (Hermes, TIC2000-0335-C03-02; Pe-
2g refers to a gazetteer containing only entities appearing in
the training set while G includes also external knowledge
English devel. Precision Recall F?=1
LOC 95.33% 94.39% 94.86
MISC 89.94% 83.41% 86.55
ORG 86.98% 88.14% 87.56
PER 91.79% 94.68% 93.21
Overall 91.51% 91.37% 91.44
English test Precision Recall F?=1
LOC 88.14% 90.41% 89.26
MISC 82.02% 75.36% 78.54
ORG 78.40% 80.43% 79.41
PER 86.36% 91.65% 88.93
Overall 84.05% 85.96% 85.00
German devel. Precision Recall F?=1
LOC 75.72% 73.67% 74.68
MISC 72.34% 42.48% 53.52
ORG 76.89% 63.82% 69.75
PER 83.84% 68.88% 75.63
Overall 77.90% 63.23% 69.80
German test Precision Recall F?=1
LOC 70.31% 70.92% 70.61
MISC 64.91% 44.18% 52.58
ORG 71.70% 54.08% 61.65
PER 87.59% 74.98% 80.79
Overall 75.47% 63.82% 69.15
Table 4: Final results for English and German
tra - TIC2000-1735-C02-02). Xavier Carreras holds a
grant by the Catalan Government Research Department.
References
X. Carreras and L. Ma`rquez. 2001. Boosting Trees for
Clause Splitting. In Proceedings of the 5th CoNLL,
Tolouse, France.
X. Carreras, L. Ma`rquez, and L. Padro?. 2002a. Named
Entity Extraction using AdaBoost. In Proceedings of
the 6th CoNLL, Taipei, Taiwan.
X. Carreras, L. Ma`rquez, V. Punyakanok, and D. Roth.
2002b. Learning and Inference for Clause Identifica-
tion. In Proceedings of the 14th European Conference
on Machine Learning, ECML, Helsinki, Finland.
R. E. Schapire and Y. Singer. 1999. Improved Boosting
Algorithms Using Confidence-rated Predictions. Ma-
chine Learning, 37(3).
R. E. Schapire. 2002. The Boosting Approach to Ma-
chine Learning. An Overview. In Proceedings of the
MSRI Workshop on Nonlinear Estimation and Classi-
fication, Berkeley, CA.
Learning a Perceptron-Based Named Entity Chunker
via Online Recognition Feedback
Xavier Carreras and Llu??s Ma`rquez and Llu??s Padro?
TALP Research Center
Departament de Llenguatges i Sistemes Informa`tics
Universitat Polite`cnica de Catalunya
{carreras,lluism,padro}@lsi.upc.es
1 Introduction
We present a novel approach for the problem of Named
Entity Recognition and Classification (NERC), in the
context of the CoNLL-2003 Shared Task.
Our work is framed into the learning and inference
paradigm for recognizing structures in Natural Language
(Punyakanok and Roth, 2001; Carreras et al, 2002). We
make use of several learned functions which, applied
at local contexts, discriminatively select optimal partial
structures. On the top of this local recognition, an infer-
ence layer explores the partial structures and builds the
optimal global structure for the problem.
For the NERC problem, the structures to be recognized
are the named entity phrases (NE) of a sentence. First, we
apply learning at word level to identify NE candidates by
means of a Begin-Inside classification. Then, we make
use of functions learned at phrase level ?one for each
NE category? to discriminate among competing NEs.
We propose a simple online learning algorithm for
training all the involved functions together. Each function
is modeled as a voted perceptron (Freund and Schapire,
1999). The learning strategy works online at sentence
level. When visiting a sentence, the functions being
learned are first used to recognize the NE phrases, and
then updated according to the correctness of their solu-
tion. We analyze the dependencies among the involved
perceptrons and a global solution in order to design a
global update rule based on the recognition of named-
entities, which reflects to each individual perceptron its
committed errors from a global perspective.
The learning approach presented here is closely re-
lated to ?and inspired by? some recent works in the area
of NLP and Machine Learning. Collins (2002) adapted
the perceptron learning algorithm to tagging tasks, via
sentence-based global feedback. Crammer and Singer
(2003) presented an online topic-ranking algorithm in-
volving several perceptrons and ranking-based update
rules for training them.
2 Named-Entity Phrase Chunking
In this section we describe our NERC approach as a
phrase chunking problem. First we formalize the prob-
lem of NERC, then we propose a NE-Chunker.
2.1 Problem Formalization
Let x be a sentence belonging to the sentence space X ,
formed by n words xi with i ranging from 0 to n?1. Let
K be the set of NE categories, which in the CoNLL-2003
setting is K = {LOC, PER, ORG, MISC}.
A NE phrase, denoted as (s, e)k, is a phrase spanning
from word xs to word xe, having s ? e, with category
k ? K. Let NE be the set of all potential NE phrases,
expressed as NE = {(s, e)k | 0 ? s ? e, k ? K} .
We say that two different NE phrases ne1 = (s1, e1)k1
and ne2 = (s2, e2)k2 overlap, denoted as ne1?ne2 iff
e1 ? s2 ? e2 ? s1. A solution for the NERC problem
is a set y formed by NE phrases that do not overlap, also
known as a chunking. We define the set Y as the set of all
possible chunkings. Formally, it can be expressed as:
Y = {y ? NE | ?ne1, ne2 ? y ne16?ne2}
The goal of the NE extraction problem is to identify
the correct solution y ? Y for a given sentence x.
2.2 NE-Chunker
The NE-Chunker is a function which given a sentence
x ? X identifies the set of NE phrases y ? Y:
NEch : X ? Y
The NE-Chunker recognizes NE phrases in two lay-
ers of processing. In the first layer, a set of NE can-
didates for a sentence is identified, out of all the po-
tential phrases in NE . To do so, we apply learning at
word level in order to perform a Begin-Inside classifica-
tion. That is, we assume a function hB(w) which de-
cides whether a word w begins a NE phrase or not, and a
function hI(w) which decides whether a word is inside a
NE phrase or not. Furthermore, we define the predicate
BI?, which tests whether a certain phrase is formed by
a starting begin word and subsequent inside words. For-
mally, BI?((s, e)k) = (hB(s) ? ?i : s < i ? e : hI(i)).
The recognition will only consider solutions formed by
phases in NE which satisfy the BI? predicate. Thus, this
layer is used to filter out candidates from NE and conse-
quently reduce the size of the solution space Y . Formally,
the solution space that is explored can be expressed as
YBI? = {y ? Y | ?ne?y BI?(ne)}.
The second layer selects the best coherent set of NE
phrases by applying learning at phrase level. We assume
a number of scoring functions, which given a NE phrase
produce a real-valued score indicating the plausibility of
the phrase. In particular, for each category k ? K we as-
sume a function scorek which produces a positive score if
the phrase is likely to belong to category k, and a negative
score otherwise.
Given this, the NE-Chunker is a function which
searches a NE chunking for a sentence x according to
the following optimality criterion:
NEch(x) = arg max
y?YBI?
?
(s,e)k?y
scorek(s, e)
That is, among the considered chunkings of the sen-
tence, the optimal one is defined to be the one whose
NE phrases maximize the summation of phrase scores.
Practically, there is no need to explicitly enumerate each
possible chunking in YBI? . Instead, by using dynamic
programming the optimal chunking can be found in
quadratic time over the sentence length, performing a
Viterby-style exploration from left to right (Punyakanok
and Roth, 2001).
Summarizing, the NE-Chunker recognizes the set of
NE phrases of a sentence as follows: First, NE candidates
are identified in linear time, applying a linear number of
decisions. Then, the optimal coherent set of NE phrases
is selected in quadratic time, applying a quadratic number
of decisions.
3 Learning via Recognition Feedback
We now present an online learning strategy for training
the learning components of the NE-Chunker, namely the
functions hB and hI and the functions scorek, for k ? K.
Each function is implemented using a perceptron1 and
a representation function.
A perceptron is a linear discriminant function hw? :
Rn ? R parametrized by a weight vector w? in Rn.
Given an instance x? ? Rn, a perceptron outputs as
prediction the inner product between vectors x? and w?,
hw?(x) = w? ? x?.
1Actually, we use a variant of the model called the voted
perceptron, explained below.
The representation function ? : X ? Rn codifies an
instance x belonging to some space X into a vector inRn
with which the perceptron can operate.
The functions hB and hI predict whether a word begins
or is inside a NE phrase, respectively. Each one consists
of a perceptron weight vector, w?B and w?I, and a shared
representation function ?w, explained in section 4. Each
function is computed as hl = w?l ? ?w(x), for l ? {B, I},
and the sign is taken as the binary classification.
The functions scorek, for k ? K, compute a score for
a phrase (s, e) being a NE phrase of category k. For each
function there is a vector w?k, and a shared representation
function ?p, also explained in section 4. The score is
given by the expression scorek(s, e) = w?k ? ?p(s, e).
3.1 Learning Algorithm
We propose a mistake-driven online learning algorithm
for training the parameter vectors w? of each perceptron all
in one go. The algorithm starts with all vectors initialized
to 0?, and then runs repeatedly in a number of epochs T
through all the sentences in the training set. Given a sen-
tence, it predicts its optimal chunking as specified above
using the current vectors. If the predicted chunking is not
perfect the vectors which are responsible of the incorrect
predictions are updated additively.
The sentence-based learning algorithm is as follows:
? Input: {(x1, y1), . . . , (xm, ym)}.
? Define: W = {w?B, w?I} ? {w?k|k ? K}.
? Initialize: ?w? ? W w? = 0?;
? for t = 1 . . . T , for i = 1 . . .m :
1. y? = NEchW (xi)
2. learning feedback(W,xi, yi, y?)
? Output: the vectors in W .
We now describe the learning feedback. Let y? be the
gold set of NE phrases for a sentence x, and y? the set pre-
dicted by the NE-Chunker. Let goldB(i) and goldI(i) be
respectively the perfect indicator functions for the begin
and inside classifications, that is, they return 1 if word
xi begins or is inside some phrase in y? and 0 otherwise.
We differentiate three kinds of phrases in order to give
feedback to the functions being learned:
? Phrases correctly identified: ?(s, e)k ? y? ? y?:
? Do nothing, since they are correct.
? Missed phrases: ?(s, e)k ? y? \ y?:
1. Update begin word, if misclassified:
if (w?B ? ?w(xs) ? 0) then
w?B = w?B + ?w(xs)
2. Update misclassified inside words:
?i : s < i ? e : such that (w?I ? ?w(xi) ? 0)
w?I = w?I + ?w(xi)
3. Update score function, if it has been applied:
if (w?B ? ?w(xs) > 0 ?
?i : s < i ? e : w?I ? ?w(xi) > 0) then
w?k = w?k + ?p(s, e)
? Over-predicted phrases: ?(s, e)k ? y? \ y?:
1. Update score function:
w?k = w?k ? ?p(s, e)
2. Update begin word, if misclassified :
if (goldB(s) = 0) then
w?B = w?B ? ?w(xs)
3. Update misclassified inside words :
?i : s < i ? e : such that (goldI(i) = 0)
w?I = w?I ? ?w(xi)
This feedback models the interaction between the two
layers of the recognition process. The Begin-Inside iden-
tification filters out phrase candidates for the scoring
layer. Thus, misclassifying words of a correct phrase
blocks the generation of the candidate and produces a
missed phrase. Therefore, we move the begin or end
prediction vectors toward the misclassified words of a
missed phrase. When an incorrect phrase is predicted,
we move away the prediction vectors of the begin and in-
side words, provided that they are not in the beginning or
inside a phrase in the gold chunking. Note that we delib-
erately do not care about false positives begin or inside
words which do not finally over-produce a phrase.
Regarding the scoring layer, each category prediction
vector is moved toward missed phrases and moved away
from over-predicted phrases.
3.2 Voted Perceptron and Kernelization
Although the analysis above concerns the perceptron al-
gorithm, we use a modified version, the voted perceptron
algorithm, introduced in (Freund and Schapire, 1999).
The key point of the voted version is that, while train-
ing, it stores information in order to make better predic-
tions on test data. Specifically, all the prediction vec-
tors w?j generated after every mistake are stored, together
with a weight cj , which corresponds to the number of
decisions the vector w?j survives until the next mistake.
Let J be the number of vector that a perceptron accumu-
lates. The final hypothesis is an averaged vote over the
predictions of each vector, computed with the expression
hw?(x?) =
?J
j=1 c
j(w?j ? x?) .
Moreover, we work with the dual formulation of the
vectors, which allows the use of kernel functions. It is
shown in (Freund and Schapire, 1999) that a vector w can
be expressed as the sum of instances xj that were added
(sxj = +1) or subtracted (sxj = ?1) in order to create it,
as w =
?J
j=1 sxjx
j
. Given a kernel function K(x, x?),
the final expression of a dual voted perceptron becomes:
hw?(x?) =
J?
j=1
cj
j?
l=1
sxlK(x?
l, x?)
In this paper we work with polynomial kernels
K(x, x?) = (x ? x? + 1)d, where d is the degree of the
kernel.
4 Feature-Vector Representation
In this section we describe the representation functions
?w and ?p, which respectively map a word or a phrase
and their local context into a feature vector in Rn, partic-
ularly, {0, 1}n. First, we define a set of predicates which
are computed on words and return one or more values:
? Form(w), PoS(w): The form and PoS of word w.
? Orthographic(w): Binary flags of word w with re-
gard to how is it capitalized (initial-caps, all-caps),
the kind of characters that form the word (contains-
digits, all-digits, alphanumeric, Roman-number),
the presence of punctuation marks (contains-
dots, contains-hyphen, acronym), single character
patterns (lonely-initial, punctuation-mark, single-
char), or the membership of the word to a predefined
class (functional-word2), or pattern (URL).
? Affixes(w): The prefixes and suffixes of the word w
(up to 4 characters).
? Word Type Patterns(ws . . . we): Type pattern of
consecutive words ws . . . we. The type of a word
is either functional (f), capitalized (C), lowercased
(l), punctuation mark (.), quote (?) or other (x).
For instance, the word type pattern for the phrase
?John Smith payed 3 euros? would be CClxl.
For the function ?w(xi) we compute the predicates in
a window of words around xi, that is, words xi+l with
l ? [?Lw,+Lw]. Each predicate label, together with
each relative position l and each returned value forms a
final binary indicator feature. The word type patterns are
evaluated in all sequences within the window which in-
clude the central word i.
For the function ?p(s, e) we represent the context of
the phrase by evaluating a [?Lp, 0] window of predicates
at the s word and a separate [0,+Lp] window at the e
word. At the s window, we also codify the named enti-
ties already recognized at the left context, capturing their
category and relative position. Furthermore, we represent
the (s, e) phrase by evaluating the predicates without cap-
turing the relative position in the features. In particular,
2Functional words are determiners and prepositions which
typically appear inside NEs.
for the words within (s, e) we evaluate the form, affixes
and type patterns of sizes 2, 3 and 4. We also evaluate the
complete concatenated form of the phrase and the word
type pattern spanning the whole phrase. Finally, we make
use of a gazetteer to capture possible NE categories of the
whole NE form and each single word within it.
5 Experiments and Results
A list of functional words was automatically extracted
from each language training set, selecting those lower-
cased words within NEs appearing 3 times or more. For
each language, we also constructed a gazetteer with the
NEs in the training set. When training, only a random
40% of the entries was considered.
We performed parameter tuning on the English lan-
guage. Concerning the features, we set the window sizes
(Lw and Lp) to 3 (we tested 2 and 3) , and we did not con-
sidered features occurring less than 5 times in the data.
When moving to German, we found better to work with
lemmas instead of word forms.
Concerning the learning algorithm, we evaluated ker-
nel degrees from 1 to 5. Degrees 2 and 3 performed some-
what better than others, and we chose degree 2. We then
ran the algorithm through the English training set for up
to five epochs, and through the German training set for up
to 3 epochs. 3 On both languages, the performance was
still slightly increasing while visiting more training sen-
tences. Unfortunately, we were not able to run the algo-
rithm until performance was stable. Table 1 summarizes
the obtained results on all sets. Clearly, the NERC task
on English is much easier than on German. Figures indi-
cate that the moderate performance on German is mainly
caused by the low recall, specially for ORG and MISC en-
tities. It is interesting to note that while in English the
performance is much better on the development set, in
German we achieve better results on the test set. This
seems to indicate that the difference in performance be-
tween development and test sets is due to irregularities
in the NEs that appear in each set, rather than overfitting
problems of our learning strategy.
The general performance of phrase recognition system
we present is fairly good, and we think it is competitive
with state-of-the-art named entity extraction systems.
Acknowledgments
This research has been partially funded by the European
Commission (Meaning, IST-2001-34460) and the Span-
ish Research Dept. (Hermes, TIC2000-0335-C03-02; Pe-
tra - TIC2000-1735-C02-02). Xavier Carreras holds a
grant by the Catalan Government Research Department.
3Implemented in PERL and run on a Pentium IV (Linux,
2.5GHz, 512Mb) it took about 120 hours for English and 70
hours for German.
English devel. Precision Recall F?=1
LOC 90.77% 93.63% 92.18
MISC 91.98% 80.80% 86.03
ORG 86.02% 83.52% 84.75
PER 91.37% 90.77% 91.07
Overall 90.06% 88.47% 89.26
English test Precision Recall F?=1
LOC 86.66% 89.15% 87.88
MISC 84.90% 72.08% 77.97
ORG 82.73% 77.60% 80.09
PER 88.25% 86.39% 87.31
Overall 85.81% 82.84% 84.30
German devel. Precision Recall F?=1
LOC 75.21% 67.32% 71.05
MISC 76.90% 42.18% 54.48
ORG 76.80% 47.22% 58.48
PER 76.87% 60.96% 67.99
Overall 76.36% 55.06% 63.98
German test Precision Recall F?=1
LOC 72.89% 65.22% 68.84
MISC 67.14% 42.09% 51.74
ORG 77.67% 42.30% 54.77
PER 87.23% 70.88% 78.21
Overall 77.83% 58.02% 66.48
Table 1: Results obtained for the development and the
test data sets for the English and German languages.
References
X. Carreras, L. Ma`rquez, V. Punyakanok, and D. Roth.
2002. Learning and Inference for Clause Identifica-
tion. In Proceedings of the 14th European Conference
on Machine Learning, ECML, Helsinki, Finland.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments Perceptron Algorithms. In Proceedings of the
EMNLP?02.
K. Crammer and Y. Singer. 2003. A Family of Additive
Online Algorithms for Category Ranking. Journal of
Machine Learning Research, 3:1025?1058.
Y. Freund and R. E. Schapire. 1999. Large Margin Clas-
sification Using the Perceptron Algorithm. Machine
Learning, 37(3):277?296.
V. Punyakanok and D. Roth. 2001. The Use of Clas-
sifiers in Sequential Inference. In Proceedings of the
NIPS-13.
Low?cost Named Entity Classification for Catalan: Exploiting
Multilingual Resources and Unlabeled Data
Llu??s Ma`rquez, Adria` de Gispert, Xavier Carreras, and Llu??s Padro?
TALP Research Center
Universitat Polite`cnica de Catalunya
Jordi Girona, 1?3, E-08034, Barcelona
 
lluism,agispert,carreras,padro  @talp.upc.es
Abstract
This work studies Named Entity Classi-
fication (NEC) for Catalan without mak-
ing use of large annotated resources of
this language. Two views are explored
and compared, namely exploiting solely
the Catalan resources, and a direct training
of bilingual classification models (Span-
ish and Catalan), given that a large col-
lection of annotated examples is available
for Spanish. The empirical results ob-
tained on real data point out that multi-
lingual models clearly outperform mono-
lingual ones, and that the resulting Cata-
lan NEC models are easier to improve by
bootstrapping on unlabelled data.
1 Introduction
There is a wide consensus about that Named Entity
Recognition and Classification (NERC) are Natural
Language Processing tasks which may improve the
performance of many applications, such as Informa-
tion Extraction, Machine Translation, Question An-
swering, Topic Detection and Tracking, etc. Thus,
interest on detecting and classifying those units in a
text has kept on growing during the last years.
Previous work in this topic is mainly framed in the
Message Understanding Conferences (MUC), de-
voted to Information Extraction, which included a
NERC competition task. More recent approaches
can be found in the proceedings of the shared task
at the 2002 and 2003 editions of the Conference
on Natural Language Learning (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003),
where several machine?learning (ML) systems were
compared at the NERC task for several languages.
One remarkable aspect of most widely used ML
algorithms is that they are supervised, that is, they
require a set of labelled data to be trained on. This
may cause a severe bottleneck when such data is not
available or is expensive to obtain, which is usu-
ally the case for minority languages with few pre?
existing linguistic resources and/or limited funding
possibilities. This is one of the main causes for
the recent growing interest on developing language?
independent NERC systems, which may be trained
from small training sets by taking advantage of un-
labelled examples (Collins and Singer, 1999; Abney,
2002), and which are easy to adapt to changing do-
mains (being all these aspects closely related).
This work focuses on exploring the construc-
tion of a low?cost Named Entity classification
(NEC) module for Catalan without making use of
large/expensive resources of the language. In doing
so, the paper first explores the training of classifi-
cation models by using only Catalan resources and
then proposes a training scheme, in which a Cata-
lan/Spanish bilingual classifier is trained directly
from a training set including examples of the two
languages. In both cases, the bootstrapping of the
resulting classifiers is also explored by using a large
unannotated Catalan corpus. The strategy used for
training the bilingual NE classification models has
been also applied with good results to NE recogni-
tion in (Carreras et al, 2003), a work that can be
considered complementary to this one.
When considering the training of bilingual mod-
els, we take advantage of the facts that Spanish
and Catalan are two Romance languages with sim-
ilar syntactic structure, and that ?since Spanish
and Catalan social and cultural environments greatly
overlap? many Named Entities appear in both lan-
guages corpora. Relying on this structural and con-
tent similarity, we will build our Catalan NE classi-
fier on the following assumptions: (a) Named Enti-
ties appear in the same contexts in both languages,
and (b) Named Entities are composed by similar pat-
terns in both languages.
The paper presents an extensive experimental
evaluation, giving strong evidence about the advan-
tage of using multilingual models for training on a
language with scarce resources. Additionally, the
Catalan NEC models resulting from the bilingual
training are easier to improve by bootstrapping on
unlabelled data.
The paper is organized as follows. Section 2
describes the Catalan and Spanish resources avail-
able and the feature codification of examples. Sec-
tion 3 briefly describes the learning algorithms used
to train the classifiers. Section 4 is devoted to the
learning of NEC modules using only Catalan re-
sources, while section 5 presents and evaluates the
bilingual approach. Finally, the main conclusions of
the work are summarized in section 6.
2 Setting
2.1 Corpus and data resources
The experimentation of this work has been carried
on two corpora, one for each language. Both corpora
consist of sentences extracted from news articles of
the year 2,000. The Catalan data, extracted from the
Catalan edition of the daily newspaper El Perio?dico
de Catalunya, has been randomly divided into three
sets: a training set (to train a system) and a test set
(to perform evaluation) for manual annotation, and
a remaining set left as unlabelled. The Spanish data
corresponds to the CoNLL 2002 Shared Task Span-
ish data, the original source being the EFE Spanish
Newswire Agency. The training set has been used
to improve classification for Catalan, whereas the
test set has been used to evaluate the bilingual classi-
fier. The original development set has not been used.
Table 1 shows the number of sentences, words and
lang. set #sent. #words #NEs
es train. 8,322 264,715 18,797
es test 1,516 51,533 3,558
ca train. 817 23,177 1,232
ca test 844 23,595 1,338
ca unlab. 83,725 2,201,712 75,038 
Table 1: Sizes of Spanish and Catalan data sets
Named Entities in each set. Although a large amount
of Catalan unlabelled NEs is available, it must be ob-
served that these are automatically recognised with a
91.5% accurate NER module, introducing a certain
error that might undermine bootstrapping results.
Considered classes include MUC categories PER
LOC and ORG, plus a fourth category MIS, includ-
ing named entities such as documents, measures and
taxes, sport competitions, titles of art works and oth-
ers. For Catalan, we find 33.0% of PER, 17.1% of
LOC, 43.5% of ORG and 6.4% of MIS out of the
2,570 manually annotated NEs, whereas for Span-
ish, out of the 22,355 labelled NEs, 22.6% are PER,
26.8% are LOC, 39.4% are ORG and the remaining
11.2% are MIS.
Additionally, we used a Spanish 7,427 trigger?
word list typically accompanying persons, organiza-
tions, locations, etc., and an 11,951 entry gazetteer
containing geographical and person names. These
lists have been semi-automatically extracted from
lexical resources and manually enriched afterwards.
They have been used in some previous works allow-
ing significant improvements for the Spanish NERC
task (Carreras et al, 2002; Carreras et al, 2003).
Trigger?words are annotated with the correspond-
ing Spanish synsets in the EuroWordNet lexical
knowledge base. Since there are translation links
among Spanish and Catalan (and other languages)
for the majority of these words, an equivalent ver-
sion of the trigger?word list for Catalan has been
automatically derived. In this work, we consider
the gazetteer as a language independent resource and
is indistinctly used for training Catalan and Spanish
models.
2.2 Feature codification
The features that characterise the NE examples are
defined in a window  anchored at a word  , repre-
senting its local context used by a classifier to make
a decision. In the window, each word around  is
codified with a set of primitive features, requiring no
linguistic pre?processing, together with its relative
position to  . Each primitive feature with each rela-
tive position and each possible value forms a final bi-
nary feature for the classifier (e.g., ?the word form
at position(-2) is street?). The kind of information
coded in these features may be grouped in the fol-
lowing kinds:
 Lexical: Word forms and their position in the
window (e.g., 	
 =?bank?), as well as word
forms appearing in the named entity under con-
sideration, independent from their position.
 Orthographic: Word properties regarding
how it is capitalised (initial-caps, all-caps),
the kind of characters that form the word
(contains-digits, all-digits, alphanumeric,
roman-number), the presence of punctua-
tion marks (contains-dots, contains-hyphen,
acronym), single character patterns (lonely-
initial, punctuation-mark, single-char), or the
membership of the word to a predefined class
(functional-word1) or pattern (URL).
 Affixes: The prefixes and suffixes up to 4 char-
acters of the NE being classified and its internal
components.
 Word Type Patterns: Type pattern of consec-
utive words in the context. The type of a word
is either functional (f), capitalised (C), lower-
cased (l), punctuation mark (.), quote (?) or
other (x).
 Bag-of-Words: Form of the words in the
window, without considering positions (e.g.,
?bank?  ).
 Trigger Words: Triggering properties of win-
dow words, using an external list to deter-
mine whether a word may trigger a certain
Named Entity (NE) class (e.g., ?president? may
trigger class PER). Also context patterns to
the left of the NE are considered, where each
word is marked with its triggering properties,
or with a functional?word tag, if appropriate
(e.g., the phrase ?the president of United Na-
tions? produces pattern f ORG f for the NE
1Functional words are determiners and prepositions which
typically appear inside NEs.
?United Nations?, assuming that ?president? is
listed as a possible trigger for ORG).
 Gazetteer Features: Gazetteer information for
window words. A gazetteer entry consists of a
set of possible NE categories.
 Additionally, binary features encoding the
length in words of the NE being classified.
All features are computed for a  -3,+3  window
around the NE being classified, except for the Bag-
of-Words, for which a  -5,+5  window is used.
3 Learning Algorithms
As previously said, we compare two learning ap-
proaches when learning from Catalan examples: su-
pervised (using the AdaBoost algorithm), and unsu-
pervised (using the Greedy Agreement Algorithm).
Both of them are briefly described below.
3.1 Supervised Learning
We use the multilabel multiclass AdaBoost.MH
algorithm (with confidence?rated predictions) for
learning the classification models. The idea of this
algorithm is to learn an accurate strong classifier by
linearly combining, in a weighted voting scheme,
many simple and moderately?accurate base classi-
fiers or rules. Each base rule is sequentially learned
by presenting the base learning algorithm a weight-
ing over the examples (denoting importance of ex-
amples), which is dynamically adjusted depending
on the behaviour of the previously learned rules. We
refer the reader to (Schapire and Singer, 1999) for
details about the general algorithm, and to (Schapire,
2002) for successful applications to many areas, in-
cluding several NLP tasks. Additionally, a NERC
system based on the AdaBoost algorithm obtained
the best results in the CoNLL?02 Shared Task com-
petition (Carreras et al, 2002).
In our setting, the boosting algorithm combines
several small fixed?depth decision trees. Each
branch of a tree is, in fact, a conjunction of binary
features, allowing the strong boosting classifier to
work with complex and expressive rules.
3.2 Unsupervised Learning
We have implemented the Greedy Agreement Algo-
rithm (Abney, 2002) which, based on two indepen-
dent views of the data, is able to learn two binary
classifiers from a set of hand-typed seed rules. Each
classifier is a majority vote of several atomic rules,
which abstains when the voting ends in a tie. The
atomic rules are just mappings of a single feature
into a class (e.g., if suffix ?lez? then PER). When
learning, the atomic rule that maximally reduces the
disagreement on unlabelled data between both clas-
sifiers is added to one of the classifiers, and the
process is repeated alternating the classifiers. See
(Abney, 2002) for a formal proof that this algo-
rithm tends to gradually reduce the classification er-
ror given the adequate seed rules.
For its extreme simplicity and potentially good re-
sults, this algorithm is very appealing for the NEC
task. In fact, results are reported to be competitive
against more sophisticated methods (Co-DL, Co-
Boost, etc.) for this specific task in (Abney, 2002).
Three important questions arise from the algo-
rithm. First, what features compose each view. Sec-
ond, how seed rules should be selected or whether
this selection strongly affects the final classifiers.
Third, how the algorithm, presented in (Abney,
2002) for binary classification, can be extended to
a multiclass problem.
In order to answer these questions and gain some
knowledge on how the algorithm works empirically,
we performed initial experiments on the big labelled
portion of the Spanish data.
When it comes to view selection, we tried two
alternatives. The first, suggested in (Collins and
Singer, 1999; Abney, 2002), divides into one view
capturing internal features of the NE, and the other
capturing features of its left-right contexts (here-
after referred to as Greedy Agreement pure, or GA  ).
Since the contextual view turned out to be quite lim-
ited in performance, we interchanged some feature
groups between the views. Specifically, we moved
the Lexical features independent of their position to
the contextual view, and the the Bag-of-Words fea-
tures to the internal one (we will refer to this divi-
sion as Greedy Agreement mixed, or GA  ). The lat-
ter, containing redundant and conditionally depen-
dent features, yielded slightly better results in terms
of precision?coverage trade?off.
As for seed rules selection, we have tried two dif-
ferent strategies. On the one hand, blindly choos-
ing as many atomic rules as possible that decide at
least in 98% of the cases for a class in a small vali-
dation set of labelled data, and on the other, manu-
ally selecting from these atomic rules only those that
might be valid still for a bigger data set. This second
approach proved empirically better, as it provided a
much higher starting point in the test set (in terms
of precision), whereas a just slightly lower coverage
value, presenting a better learning curve.
Finally, we have approached the multiclass set-
ting by a one?vs?all binarization, that is, divid-
ing the classification problem into four binary de-
cisions (one per class), and combining the resul-
tant rules. Several techniques to combine them have
been tested, from making a prediction only when
one classifier assigns positive for the given instance
and all other classifiers assign negative (very high
precision, low coverage), to much unrestrictive ap-
proaches, such as combining all votes from each
classifier (lower precision, higher coverage). Re-
sults proved that the best approach is to sum all votes
from all non-abstaining binary classifiers, where a
vote of a concrete classifier for the negative class is
converted to one vote for each of the other classes.
The best results obtained in terms of cover-
age/precision and evaluated over the whole set of
training data (and thus more significant than over a
small test set) are 80.7/84.9. These results are com-
parable to the ones presented in (Abney, 2002), tak-
ing into account, apart from the language change,
that we have introduced a fourth class to be treated
the same as the other three. Results when using
Catalan data are presented in section 4.
4 Using only Catalan resources
This section describes the results obtained by using
only the Catalan resources and comparing the fully
unsupervised Greedy Agreement algorithm with the
AdaBoost supervised learning algorithm.
4.1 Unsupervised vs. supervised learning
In this experiment, we used the Catalan training set
for extracting seed rules of the GA algorithm and to
train an AdaBoost classifier. The whole unlabelled
Catalan corpus was used for bootstrapping the GA
algorithm. All the results were computed over the
Catalan test set.
Figure 1 shows a precision?coverage plot of
AdaBoost (noted as CA, for CAtalan training) and
20
30
40
50
60
70
80
90
100
20 30 40 50 60 70 80 90 100
Pr
ec
is
io
n
Coverage
CA
GA(p)
GA(m)
Figure 1: Precision?coverage plot of GA  , GA  , and
CA models trained on Catalan resources
the Greedy Agreement algorithm for the two views
selections (noted GA  and GA  , respectively). The
curve for CA has been computed by varying a confi-
dence threshold: CA abstains when the highest pre-
diction of AdaBoost is lower than this threshold.
On the one hand, it can be seen that GA  is more
precise than GA  for low values of coverage but their
asymptotic behaviour is quite similar. By stopping
at the best point in the validation set, the Greedy
Agreement algorithm (GA  ) achieves a precision of
76.53% with a coverage of 83.62% on the test set.
On the other hand, the AdaBoost classifier clearly
outperforms both GA models at all levels of cover-
age, indicating that the supervised training is prefer-
able even when using really small training sets (an
accuracy around 70% is obtained by training Ad-
aBoost only with the 20% of the learning examples,
i.e., 270 examples).
The first three rows of table 2 contain the accu-
racy of these systems (i.e., precision when coverage
is 100%), detailed at the NE type level (best results
printed in boldface)2. The fourth row (BTS) corre-
sponds to the best results obtained when additional
unlabelled Catalan examples are taken into account,
as explained below.
It can be observed that the GA models are highly
biased towards the most frequent NE types (ORG and
PER) and that the accuracy achieved on the less rep-
2In order to obtain a 100% coverage with the GA models we
have introduced a naive algorithm for breaking ties in favour of
the most frequent categories, in the cases in which the algorithm
abstains.
LOC ORG PER MIS avg.
GA  14.66 83.64 93.88 0.00 66.66
GA  20.67 95.30 76.94 4.00 68.28
CA 61.65 86.84 91.67 40.00 79.83
BTS 65.41 87.22 91.94 37.33 80.63
Table 2: Accuracy results of all models trained on
Catalan resources
resented categories is very low for LOC and negli-
gible for MIS. The MIS category is rather difficult
to learn (also for the supervised algorithm), proba-
bly because it does not account for any concrete NE
type and does not show many regularities. Consid-
ering this fact, we learned the models using only the
LOC, ORG, and PER categories and treated the MIS
as a default value (assigned whenever the classifier
does not have enough evidence for any of the cate-
gories). The results obtained were even worse.
4.2 Bootstrapping AdaBoost models using
unlabelled examples
Ideally, the supervised approach can be boosted by
using the unlabelled Catalan examples in a kind of
iterative bootstrapping procedure. We have tested
a quite simple strategy for bootstrapping. The
unlabelled data in Catalan has been randomly di-
vided into a number of equal?size disjoint subsets

. . .

, containing 1,000 sentences each. Given
the initial training set for Catalan, noted as  , the
process is as follows:
1. Learn the ffIntroduction to the CoNLL-2004 Shared Task:
Semantic Role Labeling
Xavier Carreras and Llu??s Ma`rquez
TALP Research Centre
Technical University of Catalonia (UPC)
{carreras,lluism}@lsi.upc.es
Abstract
In this paper we describe the CoNLL-2004
shared task: semantic role labeling. We intro-
duce the specification and goal of the task, de-
scribe the data sets and evaluation methods, and
present a general overview of the systems that
have contributed to the task, providing compar-
ative description.
1 Introduction
In recent years there has been an increasing interest in
semantic parsing of natural language, which is becoming
a key issue in Information Extraction, Question Answer-
ing, Summarization, and, in general, in all NLP applica-
tions requiring some kind of semantic interpretation.
The shared task of CoNLL-2004 1 concerns the recog-
nition of semantic roles, for the English language. We
will refer to it as Semantic Role Labeling (SRL). Given a
sentence, the task consists of analyzing the propositions
expressed by some target verbs of the sentence. In par-
ticular, for each target verb all the constituents in the sen-
tence which fill a semantic role of the verb have to be
extracted (see Figure 1 for a detailed example). Typical
semantic arguments include Agent, Patient, Instrument,
etc. and also adjuncts such as Locative, Temporal, Man-
ner, Cause, etc.
Most existing systems for automatic semantic role la-
beling make use of a full syntactic parse of the sentence
in order to define argument boundaries and to extract rel-
evant information for training classifiers to disambiguate
between role labels. Thus, the task has been usually ap-
proached as a two phase procedure consisting of recogni-
tion and labeling of arguments.
1CoNLL-2004 Shared Task web page ?with
data, software and systems? outputs available? at
http://cnts.uia.ac.be/conll2004/roles .
Regarding the learning component of the systems,
we find pure probabilistic models (Gildea and Juraf-
sky, 2002; Gildea and Palmer, 2002; Gildea and Hock-
enmaier, 2003), Maximum Entropy (Fleischman et al,
2003), generative models (Thompson et al, 2003), De-
cision Trees (Surdeanu et al, 2003; Chen and Ram-
bow, 2003), and Support Vector Machines (Hacioglu and
Ward, 2003; Pradhan et al, 2003a; Pradhan et al, 2003b).
There have also been some attempts at relaxing the ne-
cessity of using syntactic information derived from full
parse trees. For instance, in (Pradhan et al, 2003a; Ha-
cioglu and Ward, 2003), a SVM-based SRL system is
devised which performs an IOB sequence tagging using
only shallow syntactic information at the level of phrase
chunks.
Nowadays, there exist two main English corpora with
semantic annotations from which to train SRL systems:
PropBank (Palmer et al, 2004) and FrameNet (Fillmore
et al, 2001). In the CoNLL-2004 shared task we concen-
trate on the PropBank corpus, which is the Penn Treebank
corpus enriched with predicate?argument structures. It
addresses predicates expressed by verbs and labels core
arguments with consecutive numbers (A0 to A5), try-
ing to maintain coherence along different predicates. A
number of adjuncts, derived from the Treebank functional
tags, are also included in PropBank annotations.
To date, the best results reported on the PropBank cor-
respond to a F1 measure slightly over 83, when using
the gold standard parse trees from Penn Treebank as the
main source of information (Pradhan et al, 2003b). This
performance drops to 77 when a real parser is used in-
stead. Comparatively, the best SRL system based solely
on shallow syntactic information (Pradhan et al, 2003a)
performs more than 15 points below. Although these re-
sults are not directly comparable to the ones obtained in
the CoNLL-2004 shared task (different datasets, differ-
ent version of PropBank, etc.) they give an idea about the
state-of-the art results on the task.
The challenge for CoNLL-2004 shared task is to come
up with machine learning strategies which address the
SRL problem on the basis of only partial syntactic in-
formation, avoiding the use of full parsers and external
lexico-semantic knowledge bases. The annotations pro-
vided for the development of systems include, apart from
the argument boundaries and role labels, the levels of pro-
cessing treated in the previous editions of the CoNLL
shared task, i.e., words, PoS tags, base chunks, clauses,
and named entities.
The rest of the paper is organized as follows. Section
2 describes the general setting of the task. Section 3 pro-
vides a detailed description of training, development and
test data. Participant systems are described and compared
in section 4. In particular, information about learning
techniques, SRL strategies, and feature development is
provided, together with performance results on the devel-
opment and test sets. Finally, section 5 concludes.
2 Task Description
The goal of the task is to develop a machine learning sys-
tem to recognize arguments of verbs in a sentence, and
label them with their semantic role. A verb and its set of
arguments form a proposition in the sentence, and typi-
cally, a sentence will contain a number of propositions.
There are two properties that characterize the structure
of the arguments in a proposition. First, arguments do not
overlap, and are organized sequentially. Second, an argu-
ment may appear split into a number of non-contiguous
phrases. For instance, in the sentence ?[A1 The apple],
said John, [C?A1 is on the table]?, the utterance argument
(labeled with type A1) appears split into two phrases.
Thus, there is a set of non-overlapping arguments la-
beled with semantic roles associated with each proposi-
tion. The set of arguments of a proposition can be seen as
a chunking of the sentence, in which chunks are parts of
the semantic roles of the proposition predicate.
In practice, number of target verbs are marked in a sen-
tence, each governing one proposition. A system has to
recognize and label the arguments of each target verb.
2.1 Methodological Setting
Training and development data are provided to build the
learning system. Apart from the correct output, both data
sets contain the correct input, as well as predictions of the
input made by state-of-the-art processors. The training
set is used for training systems, whereas the development
set is used to tune parameters of the learning systems and
select the best model.
Systems have to be developed strictly with the data
provided, which consists of input and output data and the
official external resources (described below). Since the
correct annotations for the input data are provided, a sys-
tem is allowed either to be trained to predict the input
part, or to make use of an external tool developed strictly
within this setting, such as previous CoNLL shared task
systems.
2.2 Evaluation
Evaluation is performed on a separate test set, which in-
cludes only predicted input data. A system is evaluated
with respect to precision, recall and the F1 measure. Pre-
cision (p) is the proportion of arguments predicted by a
system which are correct. Recall (r) is the proportion of
correct arguments which are predicted by a system. Fi-
nally, the F1 measure computes the harmonic mean of
precision and recall, and is the final measure to com-
pare the performance of systems. It is formulated as:
F?=1 = 2pr/(p + r).
For an argument to be correctly recognized, the words
spanning the argument as well as its semantic role have
to be correct. 2
As an exceptional case, the verb argument of each
proposition is excluded from the evaluation. This argu-
ment is the lexicalization of the predicate of the proposi-
tion. Most of the time, the verb corresponds to the target
verb of the proposition, which is provided as input, and
only in few cases the verb participant spans more words
than the target verb.
Except for non-trivial cases, this situation makes the
verb fairly easy to identify and, since there is one verb
with each proposition, evaluating its recognition over-
estimates the overall performance of a system. For this
reason, the verb argument is excluded from evaluation.
3 Data
The data consists of six sections of the Wall Street Jour-
nal part of the Penn Treebank (Marcus et al, 1993), and
follows the setting of past editions of the CoNLL shared
task: training set (sections 15-18), development set (sec-
tion 20) and test set (section 21). We first describe anno-
tations related to argument structure. Then, we describe
the preprocessing of input data. Finally, we describe the
format of the data sets.
3.1 PropBank
The Proposition Bank (PropBank) (Palmer et al, 2004)
annotates the Penn Treebank with verb argument struc-
ture. The semantic roles covered by PropBank are the
following:
? Numbered arguments (A0?A5, AA): Arguments
defining verb-specific roles. Their semantics de-
pends on the verb and the verb usage in a sentence,
or verb sense. In general, A0 stands for the agent
2The srl-eval.pl program is the official program to
evaluate the performance of a system. It is available at the
Shared Task web page.
and A1 corresponds to the patient or theme of the
proposition, and these two are the most frequent
roles. However, no consistent generalization can be
made across different verbs or different senses of the
same verb. PropBank takes the definition of verb
senses from VerbNet, and for each verb and each
sense defines the set of possible roles for that verb
usage, called the roleset. The definition of rolesets
is provided in the PropBank Frames files, which is
made available for the shared task as an official re-
source to develop systems.
? Adjuncts (AM-): General arguments that any verb
may take optionally. There are 13 types of adjuncts:
AM-ADV : general-purpose AM-MOD : modal verb
AM-CAU : cause AM-NEG : negation marker
AM-DIR : direction AM-PNC : purpose
AM-DIS : discourse marker AM-PRD : predication
AM-EXT : extent AM-REC : reciprocal
AM-LOC : location AM-TMP : temporal
AM-MNR : manner
? References (R-): Arguments representing argu-
ments realized in other parts of the sentence. The
role of a reference is the same as the role of the ref-
erenced argument. The label is an R- tag prefixed to
the label of the referent, e.g. R-A1.
? Verbs (V): Participant realizing the verb of the
proposition, with exactly one verb for each one.
We used the February 2004 release of PropBank. Most
predicative verbs were annotated, although not all of
them (for example, most of the occurrences of the verb
?to have? and ?to be? were not annotated). We applied
procedures to check consistency of propositions, looking
for overlapping arguments, and incorrect semantic role
labels. Also, co-referenced arguments were annotated as
a single item in PropBank, and we automatically distin-
guished between the referent and the reference with sim-
ple rules matching pronominal expressions, which were
tagged as R arguments. A total number of 68 proposi-
tions were not compliant with our procedures, and were
filtered out from the CoNLL data sets. The predicate-
argument annotations, thus, are not necessarily complete
in a sentence. Table 1 provides counts of the number of
sentences, annotated propositions, distinct verbs and ar-
guments in the three data sets.
3.2 Preprocessing
In this section we describe the pipeline of processors to
compute the annotations which form the input part of
the data: part-of-speech (PoS) tags, chunks, clauses and
named entities. The preprocessors correspond to the fol-
lowing state-of-the-art systems for each level of annota-
tion:
Training Devel. Test
Sentences 8,936 2,012 1,671
Tokens 211,727 47,377 40,039
Propositions 19,098 4,305 3,627
Distinct Verbs 1,838 978 855
All Arguments 50,182 11,121 9,598
A0 12,709 2,875 2,579
A1 18,046 4,064 3,429
A2 4,223 954 714
A3 784 149 150
A4 626 147 50
A5 14 4 2
AA 5 0 0
AM-ADV 1,727 352 307
AM-CAU 283 53 49
AM-DIR 231 60 50
AM-DIS 1,077 204 213
AM-EXT 152 49 14
AM-LOC 1,279 230 228
AM-MNR 1,337 334 255
AM-MOD 1,753 389 337
AM-NEG 687 131 127
AM-PNC 446 100 85
AM-PRD 10 3 3
AM-REC 2 1 0
AM-TMP 3,567 759 747
R-A0 738 162 159
R-A1 360 74 70
R-A2 49 17 9
R-A3 8 0 1
R-AA 1 0 0
R-AM-ADV 1 0 0
R-AM-LOC 27 4 4
R-AM-MNR 4 0 1
R-AM-PNC 1 0 1
R-AM-TMP 35 6 14
Table 1: Counts on the three data sets.
? PoS tagger: (Gime?nez and Ma`rquez, 2003), based
on Support Vector Machines, and trained on Penn
Treebank sections 0?18.
? Chunker and Clause Recognizer: (Carreras and
Ma`rquez, 2003), based on Voted Perceptrons, and
following the CoNLL settings of 2000 and 2001
tasks (Tjong Kim Sang and Buchholz, 2000; Tjong
Kim Sang and De?jean, 2001). These two processors
form a coherent partial syntax of a sentence, that is,
chunks and clauses form a tree.
? Named entities with (Chieu and Ng, 2003), based
on Maximum-Entropy classifiers, and following the
CoNLL-2003 task setting (Tjong Kim Sang and
De Meulder, 2003).
Precision Recall F1/Acc.
PoS Dev. (acc.) ? ? 96.88
PoS Test (acc.) ? ? 96.70
Chunking Dev. 94.28% 93.65% 93.96
Chunking Test 93.80% 92.93% 93.36
Clauses Dev. 90.51% 86.12% 88.26
Clauses Test 88.73% 82.92% 85.73
Named Entities 88.12% 88.51% 88.31
Table 2: Results of the preprocessing modules on the de-
velopment and test sets. Named Entity figures are based
on the CoNLL-2003 test set.
Such processors were ran in a pipeline, from PoS tags,
to chunks, clauses and finally named entities. Table 2
summarizes the performance of the processors on the de-
velopment and test sections. These figures differ from the
original results in the original due to a better quality of the
input information in our runs. The figures of the named
entity extractor are based on the corpus of the CoNLL-
2003 shared task, since gold annotations of named enti-
ties were not available for the current corpus.
3.3 Format
Figure 1 shows an example of a fully-annotated sentence.
Annotations of a sentence are given using a flat represen-
tation in columns, separated by spaces. Each column en-
codes an annotation by associating a tag with every word.
For each sentence, the following columns are provided:
1. Words.
2. Part of Speech tags.
3. Chunks in IOB2 format.
4. Clauses in Start-End format.
5. Named Entities in IOB2 format.
6. Target verbs, marking n predicative verbs. This
column, provided as input, specifies the governing
verbs of the propositions to be analyzed. Each target
verb is in the base form. Occasionally this column
does not mark any verb (i.e., n may be 0).
7. For each of the n target verbs, a column in Start-End
format specifying the arguments of the proposition.
These columns are the output of a system, that is,
the ones to be predicted, and are not available for
the test set.
IOB2 format. Represents chunks which do not overlap
nor embed. Words outside a chunk receive the tag O. For
words forming a chunk of type k, the first word receives
the B-k tag (Begin), and the remaining words receive the
tag I-k (Inside).
Start-End format. Represents non-overlapping
phrases (clauses or arguments) which may be embed-
ded3 inside one another. Each tag indicates whether
a clause starts or ends at that word and is of the form
START*END. The START part is a concatenation of (k
parentheses, each representing that a phrase of type k
starts at that word. The END part is a concatenation of
k) parentheses, each representing that a phrase of type
k ends at that word. For example, the * tag represents
a word with no starts and ends; the (A0*A0) tag
represents a word constituting an A0 argument; and the
(S(S*S) tag represents a word which constitutes a
base clause (labeled S) and starts another higher-level
clause. Finally, the concatenation of all tags constitutes
a well-formed bracketing. For the particular case of split
arguments, of type k, the first part appears as a phrase
with label k, and the remaining as phrases with label
C-k (continuation prefix). See examples of annotations
at columns 4th, 7th and 8th of Figure 1.
4 Participating Systems
Ten systems have participated in the CoNLL-2004 shared
task. They approached the task in several ways, using dif-
ferent learning components and labeling strategies. The
following subsections briefly summarize the most impor-
tant properties of each system and provide a qualitative
comparison between them, together with a quantitative
evaluation on the development and test sets.
4.1 Learning techniques
Up to six different learning algorithms have been ap-
plied in the CoNLL-2004 shared task. None of them
is new with respect to the past editions. Two teams
used the Maximum Entropy (ME) statistical framework
(Baldewein et al, 2004; Lim et al, 2004). Two teams
used Brill?s Transformation-based Error-driven Learning
(TBL) (Higgins, 2004; Williams et al, 2004). Two other
groups applied Memory-Based Learning (MBL) (van den
Bosch et al, 2004; Kouchnir, 2004). The remaining four
teams employed vector-based linear classifiers of differ-
ent types: Hacioglu et al (2004) and Park et al (2004)
used Support Vector Machines (SVM) with polyno-
mial kernels, Carreras et al (2004) used Voted Percep-
trons (VP) also with polynomial kernels, and finally,
Punyakanok et al (2004) used SNoW, a Winnow-based
network of linear separators. Additionally, the team of
Baldewein et al (2004) used a EM?based clustering al-
gorithm for feature development (see section 4.3).
As a main difference with respect to past editions, less
effort has been put into combining different learning al-
gorithms and outputs. Instead, the main effort of partici-
pants went into developing useful SRL strategies and into
the development of features (see sections 4.2 and 4.3).
As an exception, van den Bosch et al (2004) applied a
3Arguments in data do not embed, though format allows so.
The DT B-NP (S* O - (A0* *
San NNP I-NP * B-ORG - * *
Francisco NNP I-NP * I-ORG - * *
Examiner NNP I-NP * I-ORG - *A0) *
issued VBD B-VP * O issue (V*V) *
a DT B-NP * O - (A1* (A1*
special JJ I-NP * O - * *
edition NN I-NP * O - *A1) *A1)
around IN B-PP * O - (AM-TMP* *
noon NN B-NP * O - *AM-TMP) *
yesterday NN B-NP * O - (AM-TMP*AM-TMP) *
that WDT B-NP (S* O - (C-A1* (R-A1*R-A1)
was VBD B-VP (S* O - * *
filled VBN I-VP * O fill * (V*V)
entirely RB B-ADVP * O - * (AM-MNR*AM-MNR)
with IN B-PP * O - * *
earthquake NN B-NP * O - * (A2*
news NN I-NP * O - * *
and CC I-NP * O - * *
information NN I-NP *S)S) O - *C-A1) *A2)
. . O *S) O - * *
Figure 1: An example of an annotated sentence, in columns. Input consists of words (1st), PoS tags (2nd), base chunks
(3rd), clauses (4th) and named entities (5th). The 6th column marks target verbs, and their propositions are found in
remaining columns. According to the PropBank Frames, for issue (7th), the A0 annotates the issuer, and the A1 the
thing issued, which appears split into two parts. For fill (8th), A1 is the the destination, and A2 the theme.
voting strategy to derive the final sequence tagging as
a voted combination of three overlapping n-gram output
sequences. The same team also applied a meta-learning
step, by using iterative classifier stacking, for correcting
systematic errors committed by the low?level classifiers.
This work is also worth mentioning because of the exten-
sive work done on parameter tuning and feature selection.
4.2 SRL approaches
SRL is a complex task which has to be decomposed into
a number of simpler decisions and tagging schemes in
order to be addressed by learning techniques.
One first issue is the annotation of the different propo-
sitions of a sentence. Most of the groups treated the
annotation of semantic roles for each verb predicate as
an independent problem. An exception is the system of
Carreras et al (2004), which performs the annotation of
all propositions simultaneously. As a consequence, the
former teams treat the problem as the recognition of se-
quential structures (a.k.a. chunking), while the latter di-
rectly derives a hierarchical structure formed by the argu-
ments of all propositions. Table 3 summarizes the main
properties of each system regarding the SRL strategy im-
plemented. This property corresponds to the first column.
Regarding the labeling strategy, we can distinguish at
least three different strategies. The first one consists of
performing role identification directly by a IOB-type se-
quence tagging. The second approach consists of divid-
ing the problem into two independent phases: recogni-
tion, in which the arguments are recognized, and label-
ing, in which the already recognized arguments are as-
signed role labels. The third approach also proceeds in
two phases: filtering, in which a set of argument can-
didates are decided and labeling, in which the set of
optimal arguments is derived from the proposed can-
didates. As a variant of the first two-phase strategy,
van den Bosch et al (2004) first perform a direct classi-
fication of chunks into argument labels, and then decide
the actual arguments in a post-process by joining previ-
ously classified argument fragments. All this information
is summarized in the second column of Table 3.
An implication of implementing the two-phase strat-
egy is the ability to work with argument candidates in
the second phase, allowing to develop feature patterns for
complete arguments. Regarding the first phase, the recog-
nition of candidate arguments is performed by means
of a IOB or open?close tagging using classifiers, either
argument?independent, or specialized by argument type.
It is also worth noting that all participant systems per-
formed learning of predicate-independent classifiers in-
stead of specializing by the verb predicate. Information
about verb predicates is captured through features and
some global restrictions.
Another important issue is the granularity at which
the sentence elements are processed. It has become very
clear that a good election for this problem is phrase-by-
phrase processing (P-by-P, using the notation introduced
by Hacioglu et al (2004)) instead of word-by-word (W-
by-W). The motivation is twofold: (1) phrase boundaries
are almost always consistent with argument boundaries;
(2) P-by-P processing is computationally less expensive
and allows to explore a relatively larger context. Most of
the groups performed a P-by-P processing, but admitting
a processing by words within the target verb chunks. The
system by Baldewein et al (2004) works with a bit more
general elements called ?chunk sequences?, extracted in
a preprocess using heuristic rules. This information is
presented in the third column of Table 3.
Information regarding clauses has proven to be very
useful, as can be seen in section 4.3. All systems captured
some kind of clause information through feature codifica-
tion. However, some of the systems restrict the search for
arguments only to the immediate clause (Park et al, 2004;
Williams et al, 2004) and others use the clause hierarchy
to guide the exploration of the sentence (Lim et al, 2004;
Carreras et al, 2004).
Very relevant to the SRL strategy is the availability of
global sentential information when decisions are taken.
Almost all of the systems try to capture some global level
information by collecting features describing the target
predicate and its context, the ?syntactic path? from the
element under consideration to the predicate, etc. (see
section 4.3). But only some of them include a global
optimization procedure at sentence level in the labeling
strategy. The systems working with Maximum Entropy
Models (Baldewein et al, 2004; Lim et al, 2004) use
beam search to find taggings that maximize the prob-
ability of the output sequence. Carreras et al (2004)
and Punyakanok et al (2004) also define a global scor-
ing function to maximize. At this point, the system of
Punyakanok et al (2004) deserves special consideration,
since it formally implements a set of structural and lin-
guistic constraints directly in the global cost function to
maximize. These constraints act as a filter for valid out-
put sequences and ensure coherence of the output. Au-
thors refer to this part of the system as the inference
layer and they implement it using integer linear program-
ming. The iterative classifier stacking mechanism used
by van den Bosch et al (2004) also tries to alleviate the
problem of locality of the low-level classifiers. This in-
formation is found in the fourth column of Table 3.
Finally, some systems use some kind of postprocess-
ing to ensure coherence of the final labeling, correct some
systematic errors, or to treat some types of adjunctive ar-
guments. In most of the cases, this postprocess is per-
formed on the basis of simple ad-hoc rules. This infor-
mation is included in the last column of Table 3.
4.3 Features
With a very few exceptions all the participant systems
have used all levels of linguistic information provided in
the training data sets, that is, words, PoS and chunk la-
bels, clauses, and named entities.
It is worth mentioning that the general type of features
prop. lab. gran. glob. post
hacioglu s t P-by-P no no
punyakanok s fl W-by-W yes no
carreras j fl P-by-P yes no
lim s t P-by-P yes no
park s rc P-by-P no yes
higgins s t W-by-W no yes
van den bosch s cj P-by-P part. yes
kouchnir s rc P-by-P no yes
baldewein s rc P-by-P yes no
williams s t mixed no no
Table 3: Main properties of the SRL strategies imple-
mented by the ten participant teams (sorted by perfor-
mance on the test set). ?prop.? stands for the treatment of
all propositions of a sentence; possible values are: s (sep-
arate) and j (joint). ?lab.? stands for labeling strategy;
possible values are: t (one step tagging), rc (recognition
+ classification), fl (filtering + labeling), cj (classifica-
tion + joining). ?gran.? stands for granularity; ?glob.?
stands for global optimization. ?post? stands for post-
processing.
derived from the basic information are strongly inspired
by previous works on the SRL task (Gildea and Jurafsky,
2002; Surdeanu et al, 2003; Pradhan et al, 2003a). Many
systems used the same kind of ideas but implemented
in different ways, since the particular learning strategies
used (see section 4.2) impose different constraints on the
type of information available or the way of expressing it.
As a general idea, we can divide the features into four
types: (1) basic features, evaluating some kind of local
information on the context of the word or constituent be-
ing treated; (2) Features characterizing the internal struc-
ture of a candidate argument; (3) Features describing
properties of the target verb predicate; (4) Features that
capture the relations between the verb predicate and the
constituent under consideration.
All systems used some kind of basic features. Roughly
speaking, they consist of words, PoS tags, chunks, clause
labels, and named entities extracted from a window-
based context. These values can be considered with
or without the relative position with respect to the el-
ement under consideration, and some n-grams of them
can also be computed. If the granularity of the sys-
tem is at phrase level then typically a representative
head word of the phrase is used as lexical information.
As an exception to the general approach, the system of
Williams et al (2004) does not make use of word forms.
The rest of the features are more interesting since they
are task dependent, and deserve special attention. Table
4 summarizes the type of features exploited by systems.
To represent an argument itself, few attributes are of
general usage. Some systems count the length of it,
with different granularities. Others make use of heuris-
tics to derive its syntactic type. There are systems that
extract a structured representation of the argument, ei-
ther homogeneous (capturing different sequences of head
words, PoS tags, chunks or clauses), or heterogeneous
(combining all elements, based on the syntactic hierar-
chy). A few systems have captured the existence of
neighboring arguments, previously identified in the pro-
cess. Interestingly, the system of Lim et al (2004) rep-
resents the context of an argument relative to the syntac-
tic hierarchy by means of relative constituent sequences
and syntactic levels. Concerning lexicalization of the
argument, most of the techniques rely on head word
rules based on Collins?, or content word rules as in
Surdeanu et al (2003). Only Carreras et al (2004) de-
cide to use a bag-of-words model, apart from heuristic-
based lexicalization.
Regarding the target verb, the voice feature of the verb
is generally used, in addition to basic features capturing
the form and PoS tag of the verb. Some systems captured
statistics on frequent argument patterns for each predi-
cate. Also, systems represented the elements in the prox-
imity of the target verb, inspired by local subcategoriza-
tion patterns of a predicate.
As for features related to a constituent-predicate pair,
all systems use the simple feature describing the relative
position between them, and to a lesser degree, the dis-
tance and the difference in clausal levels. Again, there is
a general tendency to describe the structured path from
the argument to the verb. Its design goes from sim-
ple homogeneous sequences of head words or chunks, to
more sophisticated paths combining chunks and clauses,
and capturing hierarchical properties. The system of
Park et al (2004) also tracks the number of different syn-
tactic elements found between the pair. Remarkably, the
system of Baldewein et al (2004) uses an EM clustering
technique to derive features representing the affinity of an
argument and a predicate.
On top of basic feature extraction, all teams work-
ing with SVM and VP used polynomial kernels of de-
gree 2. Similar in expressiveness, the system designed
by Punyakanok et al (2004) expanded the feature space
with all pairs of basic features.
4.4 Evaluation
A baseline rate was computed for the task. It was pro-
duced by a system developed by Erik Tjong Kim Sang,
from the University of Antwerp, Belgium. The base-
line processor finds semantic roles based on the following
seven rules:
? Tag target verb and successive particles as V.
? Tag not and n?t in target verb chunk as AM-NEG.
? Tag modal verbs in target verb chunk as AM-MOD.
? Tag first NP before target verb as A0.
? Tag first NP after target verb as A1.
? Tag that, which and who before target verb as
R-A0.
? Switch A0 and A1, and R-A0 and R-A1 if the target
verb is part of a passive VP chunk. A VP chunk is
considered in passive voice if it contains a form of
to be and the verb does not end in ing.
Table 5 presents the overall results obtained by the
ten participating systems, on the development and test
sets. The best performance was obtained by the SVM-
based IOB tagger of (Hacioglu et al, 2004), which al-
most reached the performance of 70 in F1 on the test.
The seven best systems obtained F1 scores in the range
of 60-70, and only three systems scored below that.
Comparing the results across development and test cor-
pora, most systems experienced a decrease in perfor-
mance between 1.5 and 3 points. As in previous editions
of the shared task, we attribute this behavior to a greater
difficulty of the test set instead of an overfitting effect.
Interestingly, the three systems performing below 60 in
the development set did not experienced this decrease. In
fact (Williams et al, 2004) and (Baldewein et al, 2004)
even improved the results on the test set.
Table 6 details the performance of systems for the A0-
A4 arguments, on the test set. Consistently, the best per-
forming system of the task also outperforms all other sys-
tems on these semantic roles.
5 Conclusion
We have described the CoNLL-2004 shared task on se-
mantic role labeling. The task was based on the Prop-
Bank corpus, and the challenge was to come up with ma-
chine learning techniques to recognize and label semantic
roles on the basis of partial syntactic structure. Ten sys-
tems have participated to the task, contributing with a va-
riety of standard or novel learning architectures. The best
system, presented by the most experienced group on the
task (Hacioglu et al, 2004), achieved a moderate perfor-
mance of 69.49 at the F1 measure. It is based on a SVM
tagging system, performing IOB decisions on the chunks
of the sentence, and exploiting a wide variety of features
based on partial syntax.
Most of the systems advance the state-of-the-art on se-
mantic role labeling on the basis of partial syntax. How-
ever, state-of-the-art systems working with full syntax
still perform substantially better, although far from a de-
sired behavior for real-task application. Two questions
remain open: which syntactic structures are needed as in-
put for the task, and what other sources of information are
required to obtain a real-world, accurate performance.
As a future line, a more thorough experimental eval-
uation is required to see which are the components that
sy ne al at as aw an vv vs vf vc rp di pa ex
hacioglu + + + ? ? + ? + + ? + + + + +
punyakanok + + + + + + ? + ? + + + ? + +
carreras + ? ? ? + + ? + ? ? ? + ? + +
lim + ? ? ? ? + + + ? ? ? + ? + ?
park + ? ? ? ? ? ? + ? ? + + + + +
higgins + + ? ? ? ? + + ? ? ? + + + ?
van den bosch + + ? ? ? ? ? + + ? ? + + ? ?
kouchnir + ? + ? + + ? + ? + ? + + ? ?
baldewein + + + + + + ? + + ? ? + + ? ?
williams + + ? ? ? ? ? ? ? ? ? + ? ? ?
Table 4: Main feature types used by the 10 participating systems in the CoNLL-2004 shared task, sorted by perfor-
mance on the test set. ?sy?: use of partial syntax (all levels); ?ne?: use of named entities; ?al?: argument length; ?at?:
argument type; ?as?: argument internal structure; ?aw?: head-word lexicalization of arguments; ?an?: neighboring
arguments; ?vv?: verb voice; ?vs?: verb statistics; ?vf?: verb features derived from PropBank frames; ?vc?: verb local
context; ?rp?: relative position; ?di?: distance (horizontal or in the hierarchy); ?pa?: path; ?ex?: feature expansion.
most contributed to the performance of systems.
Acknowledgements
Authors would like to thank the following people and
institutions. The PropBank team, and specially Martha
Palmer and Scott Cotton, for making the corpus available.
The CoNLL-2004 board for fruitful discussions and sug-
gestions. In particular, Erik Tjong Kim Sang for useful
comments from his valuable experience, and for making
the baseline SRL processor available. Llu??s Padro? and
Mihai Surdeanu, Grzegorz Chrupa?a, and Hwee Tou Ng
for helping us in the reviewing process and the prepara-
tion of this document. Finally, the teams contributing to
shared task, for their great interest in participating.
This work has been partially funded by the European
Commission (Meaning, IST-2001-34460) and the Span-
ish Research Department (Aliado, TIC2002-04447-C02).
Xavier Carreras is supported by a pre-doctoral grant from
the Catalan Research Department.
References
Ulrike Baldewein, Katrin Erk, Sebastian Pado?, and Detlef
Prescher. 2004. Semantic role labeling with chunk
sequences. In Proceedings of CoNLL-2004.
Xavier Carreras and Llu??s Ma`rquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003, Borovets, Bulgaria.
Xavier Carreras, Llu??s Ma`rquez, and Grzegorz Chrupa?a.
2004. Hierarchical recognition of propositional argu-
ments with perceptrons. In Proceedings of CoNLL-
2004.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003,
Sapporo, Japan.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of CoNLL-2003, Edmonton, Canada.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proceedings of the Pa-
cific Asian Conference on Language, Informa tion and
Computation, Hong Kong, China.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for framenet clas-
sification. In Proceedings of EMNLP-2003, Sapporo,
Japan.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of EMNLP-2003, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of ACL 2002, Philadelphia, USA.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2003. Fast and accu-
rate part-of-speech tagging: The svm approach revis-
ited. In Proceedings of RANLP-2003, Borovets, Bul-
garia.
Kadri Hacioglu and Wayne Ward. 2003. Target word de-
tection and semantic role chunking using support vec-
tor machines. In Proceedings of HLT-NAACL 2003,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2004. Semantic role la-
beling by tagging syntactic chunks. In Proceedings of
CoNLL-2004.
Derrick Higgins. 2004. A transformation-based ap-
proach to argument labeling. In Proceedings of
CoNLL-2004.
development Precision Recall F1
hacioglu 74.18% 69.43% 71.72
punyakanok 71.96% 64.93% 68.26
carreras 73.40% 63.70% 68.21
lim 69.78% 62.57% 65.97
park 67.27% 64.36% 65.78
higgins 65.59% 60.16% 62.76
van den bosch 69.06% 57.84% 62.95
kouchnir 44.93% 63.12% 52.50
baldewein 64.90% 41.61% 50.71
williams 53.37% 32.43% 40.35
baseline 50.63% 30.30% 37.91
test Precision Recall F1
hacioglu 72.43% 66.77% 69.49
punyakanok 70.07% 63.07% 66.39
carreras 71.81% 61.11% 66.03
lim 68.42% 61.47% 64.76
park 65.63% 62.43% 63.99
higgins 64.17% 57.52% 60.66
van den bosch 67.12% 54.46% 60.13
kouchnir 56.86% 49.95% 53.18
baldewein 65.73% 42.60% 51.70
williams 58.08% 34.75% 43.48
baseline 54.60% 31.39% 39.87
Table 5: Overall precision, recall and F1 rates obtained by
the ten participating systems in the CoNLL-2004 shared
task on the development and test sets.
Beata Kouchnir. 2004. A memory-based approach for
semantic role labeling. In Proceedings of CoNLL-
2004.
Joon-Ho Lim, Young-Sook Hwang, So-Young Park, and
Hae-Chang Rim. 2004. Semantic role labeling using
maximum entropy model. In Proceedings of CoNLL-
2004.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2004. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics. Submit-
ted.
Kyung-Mi Park, Young-Sook Hwang, and Hae-Chang
Rim. 2004. Two-phase semantic role labeling
based on support vector machines. In Proceedings of
CoNLL-2004.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2003a. Support vector learning for semantic argument
classification. Technical Report TR-CSLR-2003-03,
Center for Spoken Language Research, University of
Colorado.
A0 A1 A2 A3 A4
hacioglu 81.37 71.63 49.33 51.11 66.67
punyakanok 79.38 68.16 46.69 34.04 65.22
carreras 79.05 66.96 43.28 31.22 62.07
lim 77.42 66.00 49.07 41.77 54.55
park 76.38 66.14 46.57 42.32 51.76
higgins 70.67 62.72 45.52 40.00 39.64
van den bosch 74.95 60.83 40.41 37.44 62.37
kouchnir 65.49 54.48 30.95 19.71 36.07
baldewein 66.76 53.37 37.60 22.89 27.69
williams 56.24 49.05 00.00 00.00 00.00
baseline 57.65 34.19 00.00 00.00 00.00
Table 6: F1 scores on the most frequent core argument
types obtained by the ten participating systems in the
CoNLL-2004 shared task on the test set. Systems sorted
by overall performance on the test set.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003b. Semantic role
parsing: Adding semantic structure to unstructured
text. In Proceedings of the International Conference
on Data Mining (ICDM-2003), Melbourne, USA.
Vasin Punyakanok, Dan Roth, Wen-Tau Yih, Dav Zimak,
and Yuancheng Tu. 2004. Semantic role labeling via
generalized inference over classifiers. In Proceedings
of CoNLL-2004.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic
role labeling. In Proceedings of ECML?03, Dubrovnik,
Croatia.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the CoNLL-2000 shared task: Chunking.
In Proceedings of the 4th Conference on Natural Lan-
guage Learning, CoNLL-2000.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003.
Erik F. Tjong Kim Sang and Herve? De?jean. 2001. Intro-
duction to the CoNLL-2001 shared task: Clause identi-
fication. In Proceedings of the 5th Conference on Nat-
ural Language Learning, CoNLL-2001.
Antal van den Bosch, Sander Canisius, Walter Daele-
mans, Iris Hendrickx, and Erik Tjong Kim Sang.
2004. Memory-based semantic role labeling: Optimiz-
ing features, algorithm, and output. In Proceedings of
CoNLL-2004.
Ken Williams, Christopher Dozier, and Andrew McCul-
loh. 2004. Learning transformation rules for semantic
role labeling. In Proceedings of CoNLL-2004.
Hierarchical Recognition of Propositional Arguments with Perceptrons
Xavier Carreras and Llu??s Ma`rquez
TALP Research Centre
Technical University of Catalonia (UPC)
{carreras,lluism}@lsi.upc.es
Grzegorz Chrupa?a
GRIAL Research Group
University of Barcelona (UB)
grzegorz@pithekos.net
1 Introduction
We describe a system for the CoNLL-2004 Shared Task
on Semantic Role Labeling (Carreras and Ma`rquez,
2004a). The system implements a two-layer learning ar-
chitecture to recognize arguments in a sentence and pre-
dict the role they play in the propositions. The explo-
ration strategy visits possible arguments bottom-up, navi-
gating through the clause hierarchy. The learning compo-
nents in the architecture are implemented as Perceptrons,
and are trained simultaneously online, adapting their be-
havior to the global target of the system. The learn-
ing algorithm follows the global strategy introduced in
(Collins, 2002) and adapted in (Carreras and Ma`rquez,
2004b) for partial parsing tasks.
2 Semantic Role Labeling Strategy
The strategy for recognizing propositional arguments in
sentences is based on two main observations about argu-
ment structure in the data. The first observation is the
relation of the arguments of a proposition with the chunk
and clause hierarchy: a proposition places its arguments
in the clause directly containing the verb (local clause),
or in one of the ancestor clauses. Given a clause, we de-
fine the sequence of top-most syntactic elements as the
words, chunks or clauses which are directly rooted at the
clause. Then, arguments are formed as subsequences of
top-most elements of a clause. Finally, for local clauses
arguments are found strictly to the left or to the right of
the target verb, whereas for ancestor clauses arguments
are usually to the left of the verb. This observation holds
for most of the arguments in the data. A general excep-
tion are arguments of type V, which are found only in the
local clause, starting at the position of the target verb.
The second observation is that the arguments of all
propositions of a sentence do not cross their boundaries,
and that arguments of a particular proposition are usually
found strictly within an argument of a higher level propo-
sition. Thus, the problem can be thought of as finding a
hierarchy of arguments in which arguments are embed-
ded inside others, and each argument is related to a num-
ber of propositions of a sentence in a particular role. If an
argument is related to a certain verb, no other argument
linking to the same verb can be found within it.
The system presented in this paper translates these ob-
servations into constraints which are enforced to hold in
a solution, and guide the recognition strategy. A limita-
tion of the system is that it makes no attempt to recognize
arguments which are split in many phrases.
In what follows, x is a sentence, and xi is the i-th word
of the sentence. We assume a mechanism to access the
input information of x (PoS tags, chunks and clauses),
as well as the set of target verbs V , represented by their
position. A solution y ? Y for a sentence x is a set of
arguments of the form (s, e)kv , where (s, e) represents an
argument spanning from word xs to word xe, playing a
semantic role k ? K with a verb v ? V . Finally, [S,E]
denotes a clause spanning from word xS to word sE .
The SRL(x) function, predicting semantic roles of a
sentence x, implements the following strategy:
1. Initialize set of arguments, A, to empty.
2. Define the level of each clause as its distance to the
root clause.
3. Explore clauses bottom-up, i.e. from deeper levels
to the root clause. For a clause [S,E]:
A := A ? arg search(x, [S,E])
4. Return A
2.1 Building Argument Hierarchies
Here we describe the function arg search, which builds a
set of arguments organized hierarchically, within a clause
[S,E] of a sentence x. The function makes use of two
learning-based components, defined here and described
below. First, a filtering function F, which, given a can-
didate argument, determines its plausible categories, or
rejects it when no evidence for it being an argument
is found. Second, a set of k-score functions, for each
k ? K, which, given an argument, predict a score of plau-
sibility for it being of role type k of a certain proposition.
The function arg search searches for the argument hi-
erarchy which optimizes a global score on the hierarchy.
As in earlier works, we define the global score (?) as the
summation of scores of each argument in the hierarchy.
The function explores all possible arguments in the clause
formed by contiguous top-most elements, and selects the
subset which optimizes the global score function, forcing
a hierarchy in which the arguments linked to the same
verb do not embed.
Using dynamic programming, the function can be
computed in cubic time. It considers fragments of top-
most elements, which are visited bottom-up, incremen-
tally in length, until the whole clause is explored. While
exploring, it maintains a two-dimensional matrix A of
partial solutions: each position [s, e] contains the optimal
argument hierarchy for the fragment from s to e. Finally,
the solution is found at A[S,E]. For a fragment from s to
e the algorithm is as follows:
1. A := A[s, r] ? A[r+1, e] where
r := arg maxs?r<e ?
(
A[s, r]
)
+ ?
(
A[r+1, e]
)
2. For each prop v ? V :
(a) K := F((s, e), v)
(b) Compute k? such that
k? := arg maxk?K k-score((s, e), v, x)
Set ? to the score of category k?.
(c) Set Av as the arguments in A linked to v.
(d) If (?(Av) < ?
)
then A := A\Av ?{(s, e)k
?
v }
3. A[s, e] := A
Note that an argument is visited once, and that its score
can be stored to efficiently compute the ? global score.
2.2 Start-End Filtering
The function F determines which categories in K are
plausible for an argument (s, e) to relate to a verb v.
This is done via start-end filters (FkS and FkE), one for
each type in K1. They operate on words, independently
of verbs, deciding whether a word is likely to start or end
some argument of role type k.
The selection of categories is conditional to the relative
level of the verb and the clause, and to the relative posi-
tion of the verb and the argument. The conditions are:
? v is local to the clause, and (v=s) and FVE(xe):
K := {V}
? v is local, and (e<v ? v<s):
K := {k ? K | FkS(xs) ? FkE(xe)}
1Actually, we share start-end filters for A0-A5 arguments.
? v is at deeper level, and (e<v):
K := {k ? K | k 6?K(v) ? FkS(xs) ? FkE(xe)}
where K(v) is the set of categories already assigned
to the verb in deeper clauses.
? Otherwise, K is set to empty.
Note that setting K to empty has the effect of filter-
ing out the argument for the proposition. Note also that
Start-End classifications do not depend on the verb, thus
they can be performed once per candidate word, before
entering the exploration of clauses. Then, when visiting
a clause, the Start-End filtering can be performed with
stored predictions.
3 Learning with Perceptrons
In this section we describe the learning components of
the system, namely start, end and score functions, and the
Perceptron-based algorithm to train them together online.
Each function is implemented using a linear separator,
hw : Rn ? R, operating in a feature space defined by
a feature extraction function, ? : X ? Rn, for some
instance space X . The start-end functions (FkS and FkE)
are formed by a prediction vector for each type, noted as
wkS or w
k
E, and a shared representation function ?w which
maps a word in context to a feature vector. A prediction
is computed as FkS(x) = wkS ??w(x), and similarly for the
FkE, and the sign is taken as the binary classification.
The score functions compute real-valued scores for
arguments (s, e)v . We implement these functions with
a prediction vector wk for each type k ? K, and
a shared representation function ?a which maps an
argument-verb pair to a feature vector. The score pre-
diction for a type k is then given by the expression:
k-score((s, e), v, x) = wk ? ?a((s, e), v, x).
3.1 Perceptron Learning Algorithm
We describe a mistake-driven online algorithm to train
prediction vectors together. The algorithm is essentially
the same as the one introduced in (Collins, 2002). Let W
be the set of prediction vectors:
? Initialize: ?w?W w := 0
? For each epoch t := 1 . . . T ,
for each sentence-solution pair (x, y) in training:
1. y? = SRLW (x)
2. learning feedback(W,x, y, y?)
? Return W
3.2 Learning Feedback for Filtering-Ranking
We now describe the learning feedback rule, introduced
in earlier works (Carreras and Ma`rquez, 2004b). We dif-
ferentiate two kinds of global errors in order to give feed-
back to the functions being learned: missed arguments
and over-predicted arguments. In each case, we identify
the prediction vectors responsible for producing the in-
correct argument and update them additively: vectors are
moved towards instances predicted too low, and moved
away from instances predicted too high.
Let y? be the gold set of arguments for a sentence
x, and y? those predicted by the SRL function. Let
goldS(xi, k) and goldE(xi, k) be, respectively, the per-
fect indicator functions for start and end boundaries of
arguments of type k. That is, they return 1 if word xi
starts/ends some k-argument in y? and -1 otherwise. The
feedback is as follows:
? Missed arguments: ?(s, e)kv ? y?\y?:
1. Update misclassified boundary words:
if (wkS ? ?w(xs) ? 0) then wkS = wkS + ?w(xs)
if (wkE ??w(xe) ? 0) then wkE = wkE +?w(xe)
2. Update score function, if applied:
if (k ? F ((s, e), v) then
wk = wk + ?a((s, e), v, x)
? Over-predicted arguments: ?(s, e)kp ? y?\y?:
1. Update score function:
wk = wk ? ?a((s, e), v, x)
2. Update words misclassified as S or E:
if (goldS(xs, k)=?1) then wkS = wkS??w(xs)
if (goldE(xe, k)=?1) then wkE =wkE??w(xe)
3.3 Kernel Perceptrons with Averaged Predictions
Our final architecture makes use of Voted Perceptrons
(Freund and Schapire, 1999), which compute a predic-
tion as an average of all vectors generated during train-
ing. Roughly, each vector contributes to the average pro-
portionally to the number of correct positive training pre-
dictions the vector has made. Furthermore, a prediction
vector can be expressed in dual form as a combination of
training instances, which allows the use of kernel func-
tions. We use standard polynomial kernels of degree 2.
4 Features
The features of the system are extracted from three types
of elements: words, target verbs, and arguments. They
are formed making use of PoS tags, chunks and clauses
of the sentence. The functions ?w and ?a are defined
in terms of a collection of feature extraction patterns,
which are binarized in the functions: each extracted pat-
tern forms a binary dimension indicating the existence of
the pattern in a learning instance.
Extraction on Words. The list of features extracted
from a word xi is the following:
? PoS tag.
? Form, if the PoS tag does not match with the Perl
regexp /?(CD|FW|J|LS|N|POS|SYM|V)/.
? Chunk type, of the chunk containing the word.
? Binary-valued flags: (a) Its chunk is one-word or
multi-word; (b) Starts and/or ends, or is strictly
within a chunk (3 flags); (c) Starts and/or ends
clauses (2 flags); (d) Aligned with a target verb; and
(e) First and/or last word of the sentence (2 flags).
Given a word xi, the ?w function implements a ?3
window, that is, it returns the features of the words xi+r,
with ?3?r?+3, each with its relative position r.
Extraction on Target Verbs. Given a target verb v, we
extract the following features from the word xv:
? Form, PoS tag, and target verb infinitive form.
? Voice : passive, if xv has PoS tag VBN, and either its
chunk is not VP or xv is preceded by a form of ?to
be? or ?to get? within its chunk; otherwise active.
? Chunk type.
? Binary-valued flags: (a) Its chunk is multi-word or
not; and (b) Starts and/or ends clauses (2 flags).
Extraction on Arguments. The ?a function performs
the following feature extraction for an argument (s, e)
linked to a verb v:
? Target verb features, of verb v.
? Word features, of words s?1, s, e, and e+1, each
anchored with its relative position.
? Distance of v to s and to e: for both pairs, a flag
indicating if distance is {0, 1,?1, >1, <1}.
? PoS Sequence, of PoS tags from s to e: (a) n-grams
of size 2, 3 and 4; and (b) the complete PoS pattern,
if it is less than 5 tags long.
? TOP sequence: tags of the top-most elements found
strictly from s to e. The tag of a word is its PoS. The
tag of a chunk is its type. The tag of a clause is its
type (S) enriched as follows: if the PoS tag of the
first word matches /?(IN|W|TO)/ the tag is en-
riched with the form of that word (e.g. S-to); if
that word is a verb, the tag is enriched with its PoS
(e.g. S-VBG); otherwise, it is just S. The follow-
ing features are extracted: (a) n-grams of sizes 2, 3
and 4; (b) The complete pattern, if it is less than 5
tags long; and (c) Anchored tags of the first, second,
penultimate and last elements.
? PATH sequence: tags of elements found between
the argument and the verb. It is formed by a con-
catenation of horizontal tags and vertical tags. The
horizontal tags correspond to the TOP sequence of
elements at the same level of the argument, from it to
the phrase containing the verb, both excluded. The
vertical part is the list of tags of the phrases which
contain the verb, from the phrase at the level of the
argument to the verb. The tags of the PATH se-
quence are extracted as in the TOP sequence, with
an additional mark indicating whether an element is
horizontal to the left or to the right of the argument,
or vertical. The following features are extracted: (a)
n-grams of sizes 4 and 5; and (b) The complete pat-
tern, if it is less than 5 tags long.
? Bag of Words: we consider the top-most elements
of the argument which are not clauses, and extract
all nouns, adjectives and adverbs. We then form a
separate bag for each category.
? Lexicalization: we extract the form of the head of
the first top-most element of the argument, via com-
mon head word rules; if the first element is a PP
chunk, we also extract the head of the first NP found.
5 Experiments and Results
We have build a system which implements the presented
architecture for recognizing arguments and their semantic
roles. The configuration of learning functions, related to
the roles in the CoNLL-2004 data, is set as follows :
? Five score functions for the A0?A4 types, and two
shared filtering functions FANS and FANE .
? For each of the 13 adjunct types (AM-*), a score
function and a pair of filtering functions.
? Three score functions for the R0?R2 types, and two
filtering functions FRS and FRE shared among them.
? For verbs, a score function and an end filter.
We ran the learning algorithm on the training set (with
predicted input syntax) with a polynomial kernel of de-
gree 2, for up to 8 epochs. Table 1 presents the ob-
tained results on the development set, either artificial or
real. The second and third rows provide, respectively, the
loss suffered because of errors in the filtering and scor-
ing layer. The filtering layer performs reasonably well,
since 89.44% recall can be achieved on the top of it.
However, the scoring functions clearly moderate the per-
formance, since working with perfect start-end functions
only achieve an F1 at 75.60. Finally, table 2 presents final
detailed results on the test set.
Precision Recall F?=1
g?FS, g?FE, g-score 99.92% 94.73% 97.26
FS, FE, g?score 99.90% 89.44% 94.38
g?FS, g?FE, score 85.12% 67.99% 75.60
FS, FE, score 73.40% 63.70% 68.21
Table 1: Overall results on the development set. Func-
tions with prefix g are gold functions, providing bounds
of our performance. The top row is the upper bound per-
formance of our architecture. The bottom row is the real
performance.
Precision Recall F?=1
Overall 71.81% 61.11% 66.03
A0 81.83% 76.46% 79.05
A1 68.73% 65.27% 66.96
A2 59.41% 34.03% 43.28
A3 58.18% 21.33% 31.22
A4 72.97% 54.00% 62.07
A5 0.00% 0.00% 0.00
AM-ADV 54.50% 35.50% 43.00
AM-CAU 58.33% 28.57% 38.36
AM-DIR 64.71% 22.00% 32.84
AM-DIS 64.06% 57.75% 60.74
AM-EXT 100.00% 50.00% 66.67
AM-LOC 35.62% 22.81% 27.81
AM-MNR 50.89% 22.35% 31.06
AM-MOD 97.57% 95.25% 96.40
AM-NEG 90.23% 94.49% 92.31
AM-PNC 36.11% 15.29% 21.49
AM-PRD 0.00% 0.00% 0.00
AM-TMP 61.86% 48.86% 54.60
R-A0 78.85% 77.36% 78.10
R-A1 64.29% 51.43% 57.14
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 98.32% 98.24% 98.28
Table 2: Results on the test set
Acknowledgements
This research is supported by the European Commission
(Meaning, IST-2001-34460) and the Spanish Research Depart-
ment (Aliado, TIC2002-04447-C02). Xavier Carreras is sup-
ported by a grant from the Catalan Research Department.
References
Xavier Carreras and Llu?is Ma`rquez. 2004a. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic Role
Labeling. In Proceedings of CoNLL-2004.
Xavier Carreras and Llu?is Ma`rquez. 2004b. Online
learning via global feedback for phrase recognition.
In Advances in Neural Information Processing Systems
16. MIT Press.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proceedings of the
EMNLP?02.
Y. Freund and R. E. Schapire. 1999. Large Margin Clas-
sification Using the Perceptron Algorithm. Machine
Learning, 37(3):277?296.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 152?164, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Introduction to the CoNLL-2005 Shared Task:
Semantic Role Labeling
Xavier Carreras and Llu??s Ma`rquez
TALP Research Centre
Technical University of Catalonia (UPC)
{carreras,lluism}@lsi.upc.edu
Abstract
In this paper we describe the CoNLL-
2005 shared task on Semantic Role La-
beling. We introduce the specification and
goals of the task, describe the data sets and
evaluation methods, and present a general
overview of the 19 systems that have con-
tributed to the task, providing a compara-
tive description and results.
1 Introduction
In the few last years there has been an increasing
interest in shallow semantic parsing of natural lan-
guage, which is becoming an important component
in all kind of NLP applications. As a particular case,
Semantic Role Labeling (SRL) is currently a well-
defined task with a substantial body of work and
comparative evaluation. Given a sentence, the task
consists of analyzing the propositions expressed by
some target verbs of the sentence. In particular, for
each target verb all the constituents in the sentence
which fill a semantic role of the verb have to be rec-
ognized. Typical semantic arguments include Agent,
Patient, Instrument, etc. and also adjuncts such as
Locative, Temporal, Manner, Cause, etc.
Last year, the CoNLL-2004 shared task aimed
at evaluating machine learning SRL systems based
only on partial syntactic information. In (Carreras
and Ma`rquez, 2004) one may find a detailed review
of the task and also a brief state-of-the-art on SRL
previous to 2004. Ten systems contributed to the
task, which was evaluated using the PropBank cor-
pus (Palmer et al, 2005). The best results were
around 70 in F1 measure. Though not directly com-
parable, these figures are substantially lower than the
best results published up to date using full parsing
as input information (F1 slightly over 79). In addi-
tion to the CoNLL-2004 shared task, another evalua-
tion exercise was conducted in the Senseval-3 work-
shop (Litkowski, 2004). Eight systems relying on
full parsing information were evaluated in that event
using the FrameNet corpus (Fillmore et al, 2001).
From the point of view of learning architectures and
study of feature relevance, it is also worth mention-
ing the following recent works (Punyakanok et al,
2004; Moschitti, 2004; Xue and Palmer, 2004; Prad-
han et al, 2005a).
Following last year?s initiative, the CoNLL-2005
shared task1 will concern again the recognition of
semantic roles for the English language. Compared
to the shared task of CoNLL-2004, the novelties in-
troduced in the 2005 edition are:
? Aiming at evaluating the contribution of full
parsing in SRL, the complete syntactic trees
given by two alternative parsers have been pro-
vided as input information for the task. The
rest of input information does not vary and cor-
responds to the levels of processing treated in
the previous editions of the CoNLL shared task,
i.e., words, PoS tags, base chunks, clauses, and
named entities.
? The training corpus has been substantially en-
larged. This allows to test the scalability of
1The official CoNLL-2005 shared task web page, in-
cluding data, software and systems? outputs, is available at
http://www.lsi.upc.edu/?srlconll.
152
learning-based SRL systems to big datasets and
to compute learning curves to see how much
data is necessary to train. Again, we concen-
trate on the PropBank corpus (Palmer et al,
2005), which is the Wall Street Journal part
of the Penn TreeBank corpus enriched with
predicate?argument structures.
? In order to test the robustness of the pre-
sented systems, a cross-corpora evaluation is
performed using a fresh test set from the Brown
corpus.
Regarding evaluation, two different settings were
devised depending if the systems use the informa-
tion strictly contained in the training data (closed
challenge) or they make use of external sources
of information and/or tools (open challenge). The
closed setting allows to compare systems under
strict conditions, while the open setting aimed at ex-
ploring the contributions of other sources of infor-
mation and the limits of the current learning-based
systems on the SRL task. At the end, all 19 systems
took part in the closed challenge and none of them
in the open challenge.
The rest of the paper is organized as follows. Sec-
tion 2 describes the general setting of the task. Sec-
tion 3 provides a detailed description of training,
development and test data. Participant systems are
described and compared in section 4. In particular,
information about learning techniques, SRL strate-
gies, and feature development is provided, together
with performance results on the development and
test sets. Finally, section 5 concludes.
2 Task Description
As in the 2004 edition, the goal of the task was to
develop a machine learning system to recognize ar-
guments of verbs in a sentence, and label them with
their semantic role. A verb and its set of arguments
form a proposition in the sentence, and typically, a
sentence contains a number of propositions.
There are two properties that characterize the
structure of the arguments in a proposition. First, ar-
guments do not overlap, and are organized sequen-
tially. Second, an argument may appear split into
a number of non-contiguous phrases. For instance,
in the sentence ?[A1 The apple], said John, [C?A1
is on the table]?, the utterance argument (labeled
with type A1) appears split into two phrases. Thus,
there is a set of non-overlapping arguments labeled
with semantic roles associated with each proposi-
tion. The set of arguments of a proposition can be
seen as a chunking of the sentence, in which chunks
are parts of the semantic roles of the proposition
predicate.
In practice, number of target verbs are marked
in a sentence, each governing one proposition. A
system has to recognize and label the arguments of
each target verb. To support the role labeling task,
sentences contain input annotations, that consist of
syntactic information and named entities. Section 3
describes in more detail the annotations of the data.
2.1 Evaluation
Evaluation is performed on a collection of unseen
test sentences, that are marked with target verbs and
contain only predicted input annotations.
A system is evaluated with respect to precision,
recall and the F1 measure of the predicted argu-
ments. Precision (p) is the proportion of arguments
predicted by a system which are correct. Recall (r)
is the proportion of correct arguments which are pre-
dicted by a system. Finally, the F1 measure com-
putes the harmonic mean of precision and recall, and
is the final measure to compare the performance of
systems. It is formulated as: F?=1 = 2pr/(p + r).
For an argument to be correctly recognized, the
words spanning the argument as well as its semantic
role have to be correct. 2
As an exceptional case, the verb argument of each
proposition is excluded from the evaluation. This ar-
gument is the lexicalization of the predicate of the
proposition. Most of the time, the verb corresponds
to the target verb of the proposition, which is pro-
vided as input, and only in few cases the verb par-
ticipant spans more words than the target verb. Ex-
cept for non-trivial cases, this situation makes the
verb fairly easy to identify and, since there is one
verb with each proposition, evaluating its recogni-
tion over-estimates the overall performance of a sys-
tem. For this reason, the verb argument is excluded
from evaluation.
2The srl-eval.pl program is the official program to
evaluate the performance of a system. It is available at the
Shared Task web page.
153
And CC * (S* (S* * - (AM-DIS*) (AM-DIS*)
to TO (VP* (S* (S(VP* * - * (AM-PNC*
attract VB *) * (VP* * attract (V*) *
younger JJR (NP* * (NP* * - (A1* *
listeners NNS *) *) *)))) * - *) *)
, , * * * * - * *
Radio NNP (NP* * (NP* (ORG* - (A0* (A0*
Free NNP * * * * - * *
Europe NNP *) * *) *) - *) *)
intersperses VBZ (VP*) * (VP* * intersperse * (V*)
the DT (NP* * (NP(NP* * - * (A1*
latest JJS *) * *) * - * *
in IN (PP*) * (PP* * - * *
Western JJ (NP* * (NP* (MISC*) - * *
rock NN * * * * - * *
groups NNS *) * *)))) * - * *)
. . * *) *) * - * *
Figure 1: An example of an annotated sentence, in columns. Input consists of words (1st column), PoS
tags (2nd), base chunks (3rd), clauses (4th), full syntactic tree (5th) and named entities (6th). The 7th
column marks target verbs, and their propositions are found in remaining columns. According to the
PropBank Frames, for attract (8th), the A0 annotates the attractor, and the A1 the thing attracted; for
intersperse (9th), A0 is the arranger, and A1 the entity interspersed.
2.2 Closed Challenge Setting
The organization provided training, development
and test sets derived from the standard sections of
the Penn TreeBank (Marcus et al, 1993) and Prop-
Bank (Palmer et al, 2005) corpora.
In the closed challenge, systems have to be built
strictly with information contained in the training
sections of the TreeBank and PropBank. Since this
collection contains the gold reference annotations
of both syntactic and predicate-argument structures,
the closed challenge allows: (1) to make use of any
preprocessing system strictly developed within this
setting, and (2) to learn from scratch any annotation
that is contained in the data. To support the former,
the organization provided the output of state-of-the-
art syntactic preprocessors, described in Section 3.
The development set is used to tune the parame-
ters of a system. The gold reference annotations are
also available in this set, but only to evaluate the per-
formance of different parametrizations of a system,
and select the optimal one. Finally, the test set is
used to evaluate the performance of a system. It is
only allowed to use predicted annotations in this set.
Since all systems in this setting have had access to
the same training and development data, the evalua-
tion results on the test obtained by different systems
are comparable in a fair manner.
3 Data
The data consists of sections of the Wall Street Jour-
nal part of the Penn TreeBank (Marcus et al, 1993),
with information on predicate-argument structures
extracted from the PropBank corpus (Palmer et al,
2005). In this edition of the CoNLL shared task,
we followed the standard partition used in syntactic
parsing: sections 02-21 for training, section 24 for
development, and section 23 for test. In addition, the
test set of the shared task includes three sections of
the Brown corpus (namely, ck01-03). The predicate-
argument annotations of the latter test material were
kindly provided by the PropBank team, and are very
valuable, as they allow to evaluate learning systems
on a portion of data that comes from a different
source than training.
We first describe the annotations related to argu-
ment structures. Then, we describe the preprocess-
ing systems that have been selected to predict the
input part of the data. Figure 1 shows an example of
a fully-annotated sentence.
3.1 PropBank
The Proposition Bank (PropBank) (Palmer et al,
2005) annotates the Penn TreeBank with verb argu-
ment structure. The semantic roles covered by Prop-
Bank are the following:
154
? Numbered arguments (A0?A5, AA): Argu-
ments defining verb-specific roles. Their se-
mantics depends on the verb and the verb us-
age in a sentence, or verb sense. The most
frequent roles are A0 and A1 and, commonly,
A0 stands for the agent and A1 corresponds to
the patient or theme of the proposition. How-
ever, no consistent generalization can be made
across different verbs or different senses of the
same verb. PropBank takes the definition of
verb senses from VerbNet, and for each verb
and each sense defines the set of possible roles
for that verb usage, called the roleset. The def-
inition of rolesets is provided in the PropBank
Frames files, which is made available for the
shared task as an official resource to develop
systems.
? Adjuncts (AM-): General arguments that any
verb may take optionally. There are 13 types of
adjuncts:
AM-ADV : general-purpose AM-MOD : modal verb
AM-CAU : cause AM-NEG : negation marker
AM-DIR : direction AM-PNC : purpose
AM-DIS : discourse marker AM-PRD : predication
AM-EXT : extent AM-REC : reciprocal
AM-LOC : location AM-TMP : temporal
AM-MNR : manner
? References (R-): Arguments representing ar-
guments realized in other parts of the sentence.
The role of a reference is the same as the role of
the referenced argument. The label is an R- tag
prefixed to the label of the referent, e.g. R-A1.
? Verbs (V): Argument corresponding to the verb
of the proposition. Each proposition has exa-
clty one verb argument.
We used PropBank-1.0. Most predicative verbs
were annotated, although not all of them (for exam-
ple, most of the occurrences of the verb ?to have?
and ?to be? were not annotated). We applied proce-
dures to check consistency of propositions, looking
for overlapping arguments, and incorrect semantic
role labels. Also, co-referenced arguments were an-
notated as a single item in PropBank, and we au-
tomatically distinguished between the referent and
the reference with simple rules matching pronomi-
nal expressions, which were tagged as R arguments.
Train. Devel. tWSJ tBrown
Sentences 39,832 1,346 2,416 426
Tokens 950,028 32,853 56,684 7,159
Propositions 90,750 3,248 5,267 804
Verbs 3,101 860 982 351
Arguments 239,858 8,346 14,077 2,177
A0 61,440 2,081 3,563 566
A1 84,917 2,994 4,927 676
A2 19,926 673 1,110 147
A3 3,389 114 173 12
A4 2,703 65 102 15
A5 68 2 5 0
AA 14 1 0 0
AM 7 0 0 0
AM-ADV 8,210 279 506 143
AM-CAU 1,208 45 73 8
AM-DIR 1,144 36 85 53
AM-DIS 4,890 202 320 22
AM-EXT 628 28 32 5
AM-LOC 5,907 194 363 85
AM-MNR 6,358 242 344 110
AM-MOD 9,181 317 551 91
AM-NEG 3,225 104 230 50
AM-PNC 2,289 81 115 17
AM-PRD 66 3 5 1
AM-REC 14 0 2 0
AM-TMP 16,346 601 1,087 112
R-A0 4,112 146 224 25
R-A1 2,349 83 156 21
R-A2 291 5 16 0
R-A3 28 0 1 0
R-A4 7 0 1 0
R-AA 2 0 0 0
R-AM-ADV 5 0 2 0
R-AM-CAU 41 3 4 2
R-AM-DIR 1 0 0 0
R-AM-EXT 4 1 1 0
R-AM-LOC 214 9 21 4
R-AM-MNR 143 6 6 2
R-AM-PNC 12 0 0 0
R-AM-TMP 719 31 52 10
Table 1: Counts on the data sets.
A total number of 80 propositions were not compli-
ant with our procedures (one in the Brown files, the
rest in WSJ) and were filtered out from the CoNLL
data sets.
Table 1 provides counts of the number of sen-
tences, tokens, annotated propositions, distinct
verbs, and arguments in the four data sets.
3.2 Preprocessing Systems
In this section we describe the selected processors
that computed input annotations for the SRL sys-
tems. The annotations are: part-of-speech (PoS)
tags, chunks, clauses, full syntactic trees and named
entities. As it has been noted, participants were also
155
allowed to use any processor developed within the
same WSJ partition.
The preprocessors correspond to the following
state-of-the-art systems:
? UPC processors, consisting of:
? PoS tagger: (Gime?nez and Ma`rquez,
2003), based on Support Vector Machines,
and trained on WSJ sections 02-21.
? Base Chunker and Clause Recognizer:
(Carreras and Ma`rquez, 2003), based on
Voted Perceptrons, trained on WSJ sec-
tions 02-21. These two processors form a
coherent partial syntax of a sentence, that
is, chunks and clauses form a partial syn-
tactic tree.
? Full parser of Collins (1999), with ?model 2?.
Predicts WSJ full parses, with information of
the lexical head for each syntactic constituent.
The PoS tags (required by the parser) have been
computed with (Gime?nez and Ma`rquez, 2003).
? Full parser of Charniak (2000). Jointly predicts
PoS tags and full parses.
? Named Entities predicted with the Maximum-
Entropy based tagger of Chieu and Ng (2003).
The tagger follows the CoNLL-2003 task set-
ting (Tjong Kim Sang and De Meulder, 2003),
and thus is not developed with WSJ data. How-
ever, we allowed its use because there is no
available named entity recognizer developed
with WSJ data. The reported performance on
the CoNLL-2003 test is F1 = 88.31, with
Prec/Rec. at 88.12/88.51.
Tables 2 and 3 summarize the performance of
the syntactic processors on the development and test
sets. The performance of full parsers on the WSJ
test is lower than that reported in the correspond-
ing papers. The reason is that our evaluation fig-
ures have been computed in a strict manner with re-
spect to punctuation tokens, while the full parsing
community usually does not penalize for punctua-
tion wrongly placed in the tree.3 As it can be ob-
3Before evaluating Collins?, we raised punctuation to the
highest point in the tree, using a script that is available at the
shared task webpage. Otherwise, the performance would have
Prec./Recall figures below 37.
Dev. tWSJ tBrown
UPC PoS-tagger 97.13 97.36 94.73
Charniak (2000) 92.01 92.29 87.89
Table 2: Accuracy (%) of PoS taggers.
served, the performance of all syntactic processors
suffers a substantial loss in the Brown test set. No-
ticeably, the parser of Collins (1999) seems to be the
more robust when moving from WSJ to Brown.
4 A Review of Participant Systems
Nineteen systems participated in the CoNLL-2005
shared task. They approached the task in several
ways, using different learning components and la-
beling strategies. The following subsections briefly
summarize the most important properties of each
system and provide a qualitative comparison be-
tween them, together with a quantitative evaluation
on the development and test sets.
4.1 Learning techniques
Up to 8 different learning algorithms have been ap-
plied to train the learning components of partici-
pant systems. See the ?ML-method? column of ta-
ble 4 for a summary of the following information.
Log?linear models and vector-based linear classi-
fiers dominated over the rest. Probably, this is due to
the versatility of the approaches and the availability
of very good software toolkits.
In particular, 8 teams used the Maximum En-
tropy (ME) statistical framework (Che et al, 2005;
Haghighi et al, 2005; Park and Rim, 2005; Tjong
Kim Sang et al, 2005; Sutton and McCallum, 2005;
Tsai et al, 2005; Yi and Palmer, 2005; Venkatapathy
et al, 2005). Support Vector Machines (SVM) were
used by 6 teams. Four of them with the standard
polynomial kernels (Mitsumori et al, 2005; Tjong
Kim Sang et al, 2005; Tsai et al, 2005; Pradhan et
al., 2005b), another one using Gaussian kernels (Oz-
gencil and McCracken, 2005), and a last group using
tree-based kernels specifically designed for the task
(Moschitti et al, 2005). Another team used also a re-
lated learning approach, SNoW, which is a Winnow-
based network of linear separators (Punyakanok et
al., 2005).
Decision Tree learning (DT) was also represented
156
Devel. Test WSJ Test Brown
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1
UPC Chunker 94.66 93.17 93.91 95.26 94.52 94.89 92.64 90.85 91.73
UPC Clauser 90.38 84.73 87.46 90.93 85.94 88.36 84.21 74.32 78.95
Collins (1999) 85.02 83.55 84.28 85.63 85.20 85.41 82.68 81.33 82.00
Charniak (2000) 87.60 87.38 87.49 88.20 88.30 88.25 80.54 81.15 80.84
Table 3: Results of the syntactic parsers on the development, and WSJ and Brown test sets. Unlike in full
parsing, the figures have been computed on a strict evaluation basis with respect to punctuation.
by Ponzetto and Strube (2005), who used C4.5.
Ensembles of decision trees learned through the
AdaBoost algorithm (AB) were applied by Ma`rquez
et al (2005) and Surdeanu and Turmo (2005). Tjong
Kim Sang et al (2005) applied, among others,
Memory-Based Learning (MBL).
Regarding novel learning paradigms not applied
in previous shared tasks, we find Relevant Vector
Machine (RVM), which is a kernel?based linear dis-
criminant inside the framework of Sparse Bayesian
Learning (Johansson and Nugues, 2005) and Tree
Conditional Random Fields (T-CRF) (Cohn and
Blunsom, 2005), that extend the sequential CRF
model to tree structures. Finally, Lin and Smith
(2005) presented a proposal radically different from
the rest, with very light learning components. Their
approach (Consensus in Pattern Matching, CPM)
contains some elements of Memory-based Learning
and ensemble classification.
From the Machine Learning perspective, system
combination is another interesting component ob-
served in many of the proposals. This fact, which is
a difference from last year shared task, is explained
as an attempt of increasing the robustness and cover-
age of the systems, which are quite dependent on in-
put parsing errors. The different outputs to combine
are obtained by varying input information, chang-
ing learning algorithm, or considering n-best solu-
tion lists. The combination schemes presented in-
clude very simple voting-like combination heuris-
tics, stacking of classifiers, and a global constraint
satisfaction framework modeled with Integer Linear
Programming. Global models trained to re-rank al-
ternative outputs represent a very interesting alter-
native that has been proposed by two systems. All
these issues are reviewed in detail in section 4.2.
4.2 SRL approaches
SRL is a complex task, which may be decomposed
into a number of simpler decisions and annotating
schemes in order to be addressed by learning tech-
niques. Table 4 contains a summary of the main
properties of the 19 systems presented. In this sec-
tion we will explain the contents of that table by
columns (from left-to-right).
One first issue to consider is the input structure
to navigate in order to extract the constituents that
will form labeled arguments. The majority of sys-
tems perform parse tree node labeling, searching
for a one?to?one map between arguments and parse
constituents. This information is summarized in the
?synt? column of Table 4. ?col?, ?cha?, ?upc? stand
for the syntactic parse trees (the latter is partial) pro-
vided as input by the organization. Additionally,
some teams used lists of n-best parsings generated
by available tools (?n-cha? by Charniak parser; ?n-
bikel? by Bikel?s implementation of Collins parser).
Interestingly, Yi and Palmer (2005) retrained Rat-
naparkhi?s parser using the WSJ training sections
enriched with semantic information coming from
PropBank annotations. These are referred to as AN
and AM parses. As it can be seen, Charniak parses
were used by most of the systems. Collins parses
were used also in some of the best performing sys-
tems based on combination.
The exceptions to the hierarchical processing are
the systems by Pradhan et al (2005b) and Mitsumori
et al (2005), which perform a chunking-based se-
quential tokenization. As for the former, the system
is the same than the one presented in the 2004 edi-
tion. The system by Ma`rquez et al (2005) explores
hierarchical syntactic structures but selects, in a pre-
process, a sequence of tokens to perform a sequen-
tial tagging afterwards.
157
ML-method synt pre label embed glob post comb type
punyakanok SNoW n-cha,col x&p i+c defer yes no n-cha+col ac-ILP
haghighi ME n-cha ? i+c dp-prob yes no n-cha re-rank
marquez AB cha,upc seq bio !need no no cha+upc s-join
pradhan SVM cha,col/chunk ? c/bio ? no no cha+col?chunk stack
surdeanu AB cha prun c g-top no yes no ?
tsai ME,SVM cha x&p c defer yes no ME+SVM ac-ILP
che ME cha no c g-score no yes no ?
moschitti SVM cha prun i+c !need no no no ?
tjongkimsang ME,SVM,TBL cha prun i+c !need no yes ME+SVM+TBL s-join
yi ME cha,AN,AM x&p i+c defer no no cha+AN+AM ac-join
ozgencil SVM cha prun i+c g-score no no no ?
johansson RVM cha softp i+c ? no no no ?
cohn T-CRF col x&p c g-top yes no no ?
park ME cha prun i+c ? no no no ?
mitsumori SVM chunk no bio !need no no no ?
venkatapathy ME col prun i+c frames yes no no ?
ponzetto DT col prun c g-top no yes no ?
lin CPM cha gt-para i+c !need no no no ?
sutton ME n-bikel x&p i+c dp-prob yes no n-bikel re-rank
Table 4: Main properties of the SRL strategies implemented by the participant teams, sorted by F1 per-
formance on the WSJ+Brown test set. synt stands for the syntactic structure explored; pre stands for
pre-processing steps; label stands for the labeling strategy; embed stands for the technique to ensure non-
embedding of arguments; glob stands for global optimization; post stands for post-processing; comb stands
for system output combination, and type stands for the type of combination. Concrete values appearing in
the table are explained in section 4.1. The symbol ??? stands for unknown values not reported by the system
description papers.
In general, the presented systems addressed the
SRL problem by applying different chained pro-
cesses. In Table 4 the column ?pre? summarizes pre-
processing. In most of the cases this corresponds to
a pruning procedure to filter out constituents that are
not likely to be arguments. As in feature develop-
ment, the related bibliography has been followed for
pruning. For instance, many systems used the prun-
ing strategy described in (Xue and Palmer, 2004)
(?x&p?) and other systems used the soft pruning
rules described in (Pradhan et al, 2005a) (?softp?).
Remarkably, Park and Rim (2005) parametrize the
pruning procedure and then study the effect of be-
ing more or less aggressive at filtering constituents.
In the case of Ma`rquez et al (2005), pre-processing
corresponds to a sequentialization of syntactic hier-
archical structures. As a special case, Lin and Smith
(2005) used the GT-PARA analyzer for converting
parse trees into a flat representation of all predicates
including argument boundaries.
The second stage, reflected in column ?label? of
Table 4, is the proper labeling of selected candi-
dates. Most of the systems used a two-step proce-
dure consisting of first identifying arguments (e.g.,
with a binary ?null? vs. ?non-null? classifier) and
then classifying them. This is referred to as ?i+c? in
the table. Some systems address this phase in a sin-
gle classification step by adding a ?null? category
to the multiclass problem (referred to as ?c?). The
methods performing a sequential tagging use a BIO
tagging scheme (?bio?). As a special case, Mos-
chitti et al (2005) subdivide the ?i+c? strategy into
four phases: after identification, heuristics are ap-
plied to assure compatibility of identified arguments;
and, before classifying arguments into roles, a pre-
classification into core vs. adjunct arguments is per-
formed. Venkatapathy et al (2005) use three labels
instead of two in the identification phase : ?null?,
?mandatory?, and ?optional?.
Since arguments in a solution do not embed and
most systems identify arguments as nodes in a hier-
archical structure, non-embedding constraints must
be resolved in order to generate a coherent argu-
ment labeling. The ?embed? column of Table 4 ac-
counts for this issue. The majority of systems ap-
plied specific greedy procedures that select a subset
of consistent arguments. The families of heuristics
to do that selection include prioritizing better scored
158
constituents (?g-score?), or selecting the arguments
that are first reached in a top-down exploration (?g-
top?). Some probabilistic systems include the non-
embedding constraints within the dynamic program-
ming inference component, and thus calculate the
most probable coherent labeling (?dp-prob?). The
?defer? value means that this is a combination sys-
tem and that coherence of the individual system pre-
dictions is not forced, but deferred to the later com-
bination step. As a particular case, Venkatapathy et
al. (2005) use PropBank subcategorization frames to
force a coherent solution. Note that tagging-based
systems do not need to check non-embedding con-
straints (?!need? value).
The ?glob? column of Table 4 accounts for the lo-
cality/globality of the process used to calculate the
output solution given the argument prediction candi-
dates. Systems with a ?yes? value in that column de-
fine some kind of scoring function (possibly proba-
bilistic) that applies to complete candidate solutions,
and then calculate the solution that maximizes the
scoring using an optimization algorithm.
Some systems use some kind of postprocessing to
improve the final output of the system by correct-
ing some systematic errors, or treating some types
of simple adjunct arguments. This information is in-
cluded in the ?post? column of Table 4. In most of
the cases, this postprocess is performed on the basis
of simple ad-hoc rules. However, it is worth men-
tioning the work of Tjong Kim Sang et al (2005)
in which spelling error correction techniques are
adapted for improving the resulting role labeling. In
that system, postprocessing is applied before system
combination.
Most of the best performing systems included a
combination of different base subsystems to increase
robustness of the approach and to gain coverage and
independence from parse errors. Last 2 columns of
Table 4 present this information. In the ?comb? col-
umn the source of the combination is reported. Basi-
cally, the alternative outputs to combine can be gen-
erated by different input syntactic structures or n-
best parse candidates, or by applying different learn-
ing algorithms to the same input information.
The type of combination is reported in the last col-
umn. Ma`rquez et al (2005) and Tjong Kim Sang
et al (2005) performed a greedy merging of the ar-
guments of base complete solutions (?s-join?). Yi
and Palmer (2005) did also a greedy merging of ar-
guments but taking into account not complete so-
lutions but all candidate arguments labeled by base
systems (?ac-join?). In a more sophisticated way,
Punyakanok et al (2005) and Tsai et al (2005) per-
formed global inference as constraint satisfaction
using Integer Linear Programming, also taking into
account all candidate arguments (?ac-ILP?). It is
worth noting that the generalized inference applied
in those papers allows to include, jointly with the
combination of outputs, a number of linguistically-
motivated constraints to obtain a coherent solution.
Pradhan et al (2005b) followed a stacking ap-
proach by learning a chunk-based SRL system in-
cluding as features the outputs of two syntax-based
systems. Finally, Haghighi et al (2005) and Sut-
ton and McCallum (2005) performed a different ap-
proach by learning a re-ranking function as a global
model on top of the base SRL models. Actually,
Haghighi et al (2005) performed a double selection
step: an inner re-ranking of n-best solutions coming
from the base system on a single tree; and an outer
selection of the final solution among the candidate
solutions coming from n-best parse trees. The re-
ranking approach allows to define global complex
features applying to complete candidate solutions to
train the rankers.
4.3 Features
Looking at the description of the different systems, it
becomes clear that the general type of features used
in this edition is strongly based on previous work on
the SRL task (Gildea and Jurafsky, 2002; Surdeanu
et al, 2003; Pradhan et al, 2005a; Xue and Palmer,
2004). With no exception, all systems have made
intensive use of syntax to extract features. While
most systems work only on the output of a parser
?Charniak?s being the most preferred? some sys-
tems depend on many syntactic parsers. In the latter
situation, either a system is a combination of many
individual systems (each working with a different
parser), or a system extracts features from many dif-
ferent parse trees while exploring the nodes of only
one parse tree. Most systems have also considered
named entities for extracting features.
The main types of features seen in this SRL edi-
tion can be divided into four general categories: (1)
Features characterizing the structure of a candidate
159
sources argument verb arg?verb p
synt ne at aw ab ac ai pp sd v sc rp di ps pv pi sf as
punyakanok cha,col,upc + + h + t + + ? + + + c + ? + + ?
haghighi cha ? + h + p,s ? + + + + + t + + ? ? +
marquez cha,upc + + h + t + ? + + + + w,c + + ? + ?
pradhan cha,col,upc + + h,c + p,s,t + + ? + + + c,t + + + + ?
surdeanu cha + + h,c + p,s + ? + + + + w,t + + + ? ?
tsai cha,upc + + h + p,s,t ? ? ? + + + w + ? ? ? ?
che cha + + h + ? ? + ? + + + t + + ? ? ?
moschitti cha ? + h + p + + ? + + + t + + ? + ?
tjongkimsang cha + + ? + p,t ? + ? + + + w,t + + + ? ?
yi cha,an,am ? + h,c ? p,s ? + ? + + + w + ? ? + ?
ozgencil cha ? + h ? p ? ? ? + + + ? + + ? ? ?
johansson cha,upc + + h ? ? ? ? ? + + + ? + + ? ? ?
cohn col ? + h + p,s ? + ? + + + w + ? + + ?
park cha ? + h,c ? p ? ? ? + + + ? + ? + ? ?
mitsumori upc,cha + + ? + t ? ? + + ? + c,t ? + ? ? ?
venkatapathy col + + h + ? ? ? ? + ? + ? + ? ? ? ?
ponzetto col,upc + + h + ? + ? ? + ? ? w,c,t ? ? + ? ?
lin cha ? + h + ? ? ? ? + ? + w ? ? ? ? ?
sutton bik ? + h + p,s ? ? ? + ? + ? + ? ? ? +
Table 5: Main feature types used by the 19 participating systems in the CoNLL-2005 shared task, sorted by
performance on the WSJ+Brown test set. Sources: synt: use of parsers, namely Charniak (cha), Collins
(col), UPC partial parsers (upc), Bikel?s Collins model (bik) and/or argument-enriched parsers (an,am); ne:
use of named entities. On the argument: at: argument type; aw: argument words, namely the head (h)
and/or content words (c); ab: argument boundaries, i.e. form and PoS of first and/or last argument words; ac:
argument context, capturing features of the parent (p) and/or left/right siblings (s), or the tokens surrounding
the argument (t); ai: indicators of the structure of the argument (e,g., on internal constituents, surround-
ing/boundary punctuation, governing category, etc.); pp: specific features for prepositional phrases; sd:
semantic dictionaries. On the verb: v: standard verb features (voice, word/lemma, PoS); sc: subcatego-
rization. On the arg-verb relation: rp: relative position; di: distance, based on words (w), chunks (c) or
the syntactic tree (t); ps: standard path; pv: path variations; pi: scalar indicator variables on the path (of
chunks, clauses, or other phrase types), common ancestor, etc.; sf: syntactic frame (Xue and Palmer, 2004);
On the complete proposition: as: sequence of arguments of a proposition.
argument; (2) Features describing properties of the
target verb predicate; (3) Features that capture the
relation between the verb predicate and the con-
stituent under consideration; and (4) Global features
describing the complete argument labeling of a pred-
icate. The rest of the section describes the most com-
mon feature types in each category. Table 5 summa-
rizes the type of features exploited by systems.
To represent an argument itself, all systems make
use of the syntactic type of the argument. Almost
all teams used the heuristics of Collins (1999) to ex-
tract the head word of the argument, and used fea-
tures that capture the form, lemma and PoS tag of
the head. In the same line, some systems also use
features of the content words of the argument, using
the heuristics of Surdeanu et al (2003). Very gen-
erally also, many systems extract features from the
first and last words of the argument. Regarding the
syntactic elements surrounding the argument, many
systems working on full trees have considered the
parent and siblings of the argument, capturing their
syntactic type and head word. Differently, other
systems have captured features from the left/right
tokens surrounding the argument, which are typi-
cally words, but can be chunks or general phrases in
systems that sequentialize the task (Ma`rquez et al,
2005; Pradhan et al, 2005b; Mitsumori et al, 2005).
Many systems use a variety of indicator features that
capture properties of the argument structure and its
local syntactic annotations. For example, indicators
of the immediate syntactic types that form the argu-
ment, flags raised by punctuation tokens in or nearby
the argument, or the governing category feature of
Gildea and Jurafsky (2002). It is also somewhat gen-
160
eral the use of specific features that apply when the
constituent is a prepositional phrase, such as look-
ing for the head word of the noun phrase within it.
A few systems have also built semantic dictionaries
from training data, that collect words appearing fre-
quently in temporal, locative or other arguments.
To represent the predicate, all systems have used
features codifying the form, lemma, PoS tag and
voice of the verb. It is also of general use the subcat-
egorization feature, capturing the syntactic rule that
expands the parent of the predicate. Some systems
captured statistics related to the frequency of a verb
in training data (not in Table 5).
Regarding features related to an argument-verb
pair, almost all systems use the simple feature de-
scribing the relative position between them. To
a lesser degree, systems have computed distances
from one to the other, based on the number of words
or chunks between them, or based on the syntactic
tree. Not surprisingly, all systems have extracted the
path from the argument to the verb. While almost
all systems use the standard path of (Gildea and Ju-
rafsky, 2002), many have explored variations of it.
A common one consists of the path from the argu-
ment to the lowest common ancestor of the verb and
the argument. Another variation is the partial path,
that is built of chunks and clauses only. Indicator
features that capture scalar values of the path are
also common, and concentrate mainly on looking
at the common ancestor, capturing the difference of
clausal levels, or looking for punctuation and other
linguistic elements in the path. In this category, it is
also noticeable the use of the syntactic frame feature,
proposed by Xue and Palmer (2004).
Finally, in this edition two systems apply learn-
ing at a global context (Haghighi et al, 2005; Sut-
ton and McCallum, 2005) and, consequently, they
are able to extract features from a complete labeling
of a predicate. Basically, the central feature in this
context extracts the sequential pattern of predicate
arguments. Then, this pattern can be enriched with
syntactic categories, broken down into role-specific
indicator variables, or conjoined with the predicate
lemma.
Apart from basic feature extraction, combination
of features has also been explored in this edition.
Many of the combinations depart from the manually
selected conjunctions of Xue and Palmer (2004).
4.4 Evaluation
A baseline rate was computed for the task. It
was produced using a system developed in the past
shared task edition by Erik Tjong Kim Sang, from
the University of Amsterdam, The Netherlands. The
baseline processor finds semantic roles based on the
following seven rules:
? Tag target verb and successive particles as V.
? Tag not and n?t in target verb chunk as
AM-NEG.
? Tag modal verbs in target verb chunk as
AM-MOD.
? Tag first NP before target verb as A0.
? Tag first NP after target verb as A1.
? Tag that, which and who before target verb
as R-A0.
? Switch A0 and A1, and R-A0 and R-A1 if the
target verb is part of a passive VP chunk. A
VP chunk is considered in passive voice if it
contains a form of to be and the verb does
not end in ing.
Table 6 presents the overall results obtained by
the nineteen systems plus the baseline, on the de-
velopment and test sets (i.e., Development, Test
WSJ, Test Brown, and Test WSJ+Brown). The sys-
tems are sorted by the performance on the combined
WSJ+Brown test set.
As it can be observed, all systems clearly outper-
formed the baseline. There are seven systems with a
final F1 performance in the 75-78 range, seven more
with performances in the 70-75 range, and five with
a performance between 65 and 70. The best perfor-
mance was obtained by Punyakanok et al (2005),
which almost reached an F1 at 80 in the WSJ test
set and almost 78 in the combined test. Their results
on the WSJ test equal the best results published so
far on this task and datasets (Pradhan et al, 2005a),
though they are not directly comparable due to a
different setting in defining arguments not perfectly
matching the predicted parse constituents. Since the
evaluation in the shared task setting is more strict,
we believe that the best results obtained in the shared
task represent a new breakthrough in the SRL task.
It is also quite clear that the systems using com-
bination are better than the individuals. It is worth
noting that the first 4 systems are combined. The
161
Development Test WSJ Test Brown Test WSJ+Brown
P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1
punyakanok 80.05 74.83 77.35 82.28 76.78 79.44 73.38 62.93 67.75 81.18 74.92 77.92
haghighi 77.66 75.72 76.68 79.54 77.39 78.45 70.24 65.37 67.71 78.34 75.78 77.04
marquez 78.39 75.53 76.93 79.55 76.45 77.97 70.79 64.35 67.42 78.44 74.83 76.59
pradhan 80.90 75.38 78.04 81.97 73.27 77.37 73.73 61.51 67.07 80.93 71.69 76.03
surdeanu 79.14 71.57 75.17 80.32 72.95 76.46 72.41 59.67 65.42 79.35 71.17 75.04
tsai 81.13 72.42 76.53 82.77 70.90 76.38 73.21 59.49 65.64 81.55 69.37 74.97
che 79.65 71.34 75.27 80.48 72.79 76.44 71.13 59.99 65.09 79.30 71.08 74.97
moschitti 74.95 73.10 74.01 76.55 75.24 75.89 65.92 61.83 63.81 75.19 73.45 74.31
tjongkimsang 76.79 70.01 73.24 79.03 72.03 75.37 70.45 60.13 64.88 77.94 70.44 74.00
yi 75.70 69.99 72.73 77.51 72.97 75.17 67.88 59.03 63.14 76.31 71.10 73.61
ozgencil 73.57 71.87 72.71 74.66 74.21 74.44 65.52 62.93 64.20 73.48 72.70 73.09
johansson 73.40 70.85 72.10 75.46 73.18 74.30 65.17 60.59 62.79 74.13 71.50 72.79
cohn 73.51 68.98 71.17 75.81 70.58 73.10 67.63 60.08 63.63 74.76 69.17 71.86
park 72.68 69.16 70.87 74.69 70.78 72.68 64.58 60.31 62.38 73.35 69.37 71.31
mitsumori 71.68 64.93 68.14 74.15 68.25 71.08 63.24 54.20 58.37 72.77 66.37 69.43
venkatapathy 71.88 64.76 68.14 73.76 65.52 69.40 65.25 55.72 60.11 72.66 64.21 68.17
ponzetto 71.82 61.60 66.32 75.05 64.81 69.56 66.69 52.14 58.52 74.02 63.12 68.13
lin 70.11 61.96 65.78 71.49 64.67 67.91 65.75 52.82 58.58 70.80 63.09 66.72
sutton 64.43 63.11 63.76 68.57 64.99 66.73 62.91 54.85 58.60 67.86 63.63 65.68
baseline 50.00 28.98 36.70 51.13 29.16 37.14 62.66 33.07 43.30 52.58 29.69 37.95
Table 6: Overall precision, recall and F1 rates obtained by the 19 participating systems in the CoNLL-2005
shared task on the development and test sets. Systems sorted by F1 score on the WSJ+Brown test set.
best individual system on the task is that of Sur-
deanu and Turmo (2005), which obtained F1=75.04
on the combined test set, about 3 points below than
the best performing combined system. On the de-
velopment set, that system achieved a performace
of 75.17 (slightly below than the 75.27 reported by
Che et al (2005) on the same dataset). Accord-
ing to the description papers, we find that other
individual systems, from which the combined sys-
tems are constructed, performed also very well. For
instance, Tsai et al (2005) report F1=75.76 for a
base system on the development set, Ma`rquez et al
(2005) report F1=75.75, Punyakanok et al (2005)
report F1=74.76, and Haghighi et al (2005) report
F1=74.52.
The best results in the CoNLL-2005 shared task
are 10 points better than those of last year edition.
This increase in performance should be attributed to
a combination of the following factors: 1) training
sets have been substantially enlarged; 2) predicted
parse trees are available as input information; and 3)
more sophisticated combination schemes have been
implemented. In order to have a more clear idea of
the impact of enriching the syntactic information,
we refer to (Ma`rquez et al, 2005), who developed
an individual system based only on partial parsing
(?upc? input information). That system performed
F1=73.57 on the development set, which is 2.18
points below the F1=75.75 obtained by the same ar-
chitecture using full parsing, and 4.47 points below
the best performing combined system on the devel-
opment set (Pradhan et al, 2005b).
Comparing the results across development and
WSJ test corpora, we find that, with two exceptions,
all systems experienced a significant increase in per-
formance (normally between 1 and 2 F1 points).
This fact may be attributed to the different levels of
difficulty found across WSJ sections. The linguistic
processors and parsers perform slightly worse in the
development set. As a consequence, the matching
between parse nodes and actual arguments is lower.
Regarding the evaluation using the Brown test
set, all systems experienced a severe drop in perfor-
mance (about 10 F1 points), even though the base-
line on the Brown test set is higher than that of
the WSJ test set. As already said in previous sec-
tions, all the linguistic processors, from PoS tag-
ging to full parsing, showed a much lower perfor-
mance than in the WSJ test set, evincing that their
performance cannot be extrapolated across corpora.
Presumably, this fact is the main responsible of the
performace drop, though we do not discard an ad-
ditional overfitting effect due to the design of spe-
cific features that do not generalize well. More im-
162
portantly, this results impose (again) a severe criti-
cism on the current pipelined architecture for Natu-
ral Language Processing. Error propagation and am-
plification through the chained modules make the fi-
nal output generalize very badly when changing the
domain of application.
5 Conclusion
We have described the CoNLL-2005 shared task
on semantic role labeling. Contrasting with the
CoNLL-2004 edition, the current edition has in-
corporated the use of full syntax as input to the
SRL systems, much larger training sets, and cross-
corpora evaluation. The first two novelties have
most likely contributed to an improvement of re-
sults. The latter has evinced a major drawback of
natural language pipelined architectures.
Nineteen teams have participated to the task, con-
tributing with a variety of learning algorithms, la-
beling strategies, feature design and experimenta-
tion. While, broadly, all systems make use of the
same basic techniques described in existing SRL
literature, some novel aspects have also been ex-
plored. A remarkable aspect, common in the four
top-performing systems and many other, is that
of combining many individual SRL systems, each
working on different syntactic structures. Combin-
ing systems improves robustness, and overcomes
the limitations in coverage that working with a sin-
gle, non-correct syntactic structure imposes. The
best system, presented by Punyakanok et al (2005),
achieves an F1 at 79.44 on the WSJ test. This per-
formance, of the same order than the best reported
in literature, is still far from the desired behavior of
a natural language analyzer. Furthermore, the per-
formance of such SRL module in a real application
will be about ten points lower, as demonstrated in
the evaluation on the sentences from Brown.
We conclude with two open questions. First, what
semantic knowledge is needed to improve the qual-
ity and performance of SRL systems. Second, be-
yond pipelines, what type of architectures and lan-
guage learning methodology ensures a robust per-
formance of processors.
Acknowledgements
Authors would like to thank the following people and institu-
tions. The PropBank team, and specially Martha Palmer and
Benjamin Snyder, for making available PropBank-1.0 and the
prop-banked Brown files. The Linguistic Data Consortium, for
issuing a free evaluation license for the shared task to use the
TreeBank. Hai Leong Chieu and Hwee Tou Ng, for running
their Named Entity tagger on the task data. Finally, the teams
contributing to the shared task, for their great enthusiasm.
This work has been partially funded by the European Com-
munity (Chil - IP506909; PASCAL - IST-2002-506778) and
the Spanish Ministry of Science and Technology (Aliado,
TIC2002-04447-C02).
References
Xavier Carreras and Llu??s Ma`rquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003, Borovets, Bulgaria.
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL-2004.
Eugene Charniak. 2000. A maximum-entropy inspired
parser. In Proceedings of NAACL-2000.
Wanxiang Che, Ting Liu, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings of
CoNLL-2005.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of CoNLL-2003, Edmonton, Canada.
Trevor Cohn and Philip Blunsom. 2005. Semantic role
labelling with tree conditional random fields. In Pro-
ceedings of CoNLL-2005.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proceedings of the Pa-
cific Asian Conference on Language, Informa tion and
Computation, Hong Kong, China.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2003. Fast and accu-
rate part-of-speech tagging: The svm approach revis-
ited. In Proceedings of RANLP-2003, Borovets, Bul-
garia.
163
Aria Haghighi, Kristina Toutanova, and Christopher
Manning. 2005. A joint model for semantic role la-
beling. In Proceedings of CoNLL-2005.
Richard Johansson and Pierre Nugues. 2005. Sparse
bayesian classification of predicate arguments. In Pro-
ceedings of CoNLL-2005.
Chi-San Lin and Tony C. Smith. 2005. Semantic role
labeling via consensus in pattern-matching. In Pro-
ceedings of CoNLL-2005.
Ken Litkowski. 2004. Senseval-3 task: Automatic label-
ing of semantic roles. In Proceedings of the Senseval-3
ACL-SIGLEX Workshop.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19.
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez, and Neus
Catala`. 2005. Semantic role labeling as sequential
tagging. In Proceedings of CoNLL-2005.
Tomohiro Mitsumori, Masaki Murata, Yasushi Fukuda,
Kouichi Doi, and Hirohumi Doi. 2005. Semantic role
labeling using support vector machines. In Proceed-
ings of CoNLL-2005.
Alessandro Moschitti, Ana-Maria Giuglea, Bonaventura
Coppola, and Roberto Basili. 2005. Hierarchical se-
mantic role labeling. In Proceedings of CoNLL-2005.
Alessandro Moschitti. 2004. A study on convolution
kernel for shallow semantic parsing. In Proceedings
of the 42nd Annual Conference of the Association for
Computational Linguistics (ACL-2004).
Necati Ercan Ozgencil and Nancy McCracken. 2005.
Semantic role labeling using libSVM. In Proceedings
of CoNLL-2005.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1).
Kyung-Mi Park and Hae-Chang Rim. 2005. Maximum
entropy based semantic role labeling. In Proceedings
of CoNLL-2005.
Simone Paolo Ponzetto and Michael Strube. 2005. Se-
mantic role labeling using lexical statistical informa-
tion. In Proceedings of CoNLL-2005.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Daniel Jurafsky.
2005a. Support vector learning for semantic argu-
ment classification. Machine Learning. Special issue
on Speech and Natural Language Processing. To ap-
pear.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2005b. Semantic role
chunking combining complementary syntactic views.
In Proceedings of CoNLL-2005.
Vasin Punyakanok, Dan Roth, Wen-Tau Yih, and Dav Zi-
mak. 2004. Semantic role labeling via integer lin-
ear programming inference. In Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING).
Vasin Punyakanok, Peter Koomen, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multi-
ple semantic role labeling systems. In Proceedings of
CoNLL-2005.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Pro-
ceedings of CoNLL-2005.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Charles Sutton and Andrew McCallum. 2005. Joint
parsing and semantic role labeling. In Proceedings of
CoNLL-2005.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003.
Erik Tjong Kim Sang, Sander Canisius, Antal van den
Bosch, and Toine Bogers. 2005. Applying spelling er-
ror correction techniques for improving semantic role
labelling. In Proceedings of CoNLL-2005.
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and Wen-
Lian Hsu. 2005. Exploiting full parsing information
to label semantic roles using an ensemble of me and
svm via integer linear programming. In Proceedings
of CoNLL-2005.
Sriram Venkatapathy, Akshar Bharati, and Prashanth
Reddy. 2005. Inferring semantic roles using sub-
categorization frames and maximum entropy model.
In Proceedings of CoNLL-2005.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Szu-ting Yi and Martha Palmer. 2005. The integration of
syntactic parsing and semantic role labeling. In Pro-
ceedings of CoNLL-2005.
164
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 181?185, New York City, June 2006. c?2006 Association for Computational Linguistics
Projective Dependency Parsing with Perceptron
Xavier Carreras, Mihai Surdeanu and Llu??s Ma`rquez
TALP Research Centre ? Software Department (LSI)
Technical University of Catalonia (UPC)
Campus Nord - Edifici Omega, Jordi Girona Salgado 1?3, E-08034 Barcelona
{carreras,surdeanu,lluism}@lsi.upc.edu
Abstract
We describe an online learning depen-
dency parser for the CoNLL-X Shared
Task, based on the bottom-up projective
algorithm of Eisner (2000). We experi-
ment with a large feature set that mod-
els: the tokens involved in dependencies
and their immediate context, the surface-
text distance between tokens, and the syn-
tactic context dominated by each depen-
dency. In experiments, the treatment of
multilingual information was totally blind.
1 Introduction
We describe a learning system for the CoNLL-X
Shared Task on multilingual dependency parsing
(Buchholz et al, 2006), for 13 different languages.
Our system is a bottom-up projective dependency
parser, based on the cubic-time algorithm by Eisner
(1996; 2000). The parser uses a learning function
that scores all possible labeled dependencies. This
function is trained globally with online Perceptron,
by parsing training sentences and correcting its pa-
rameters based on the parsing mistakes. The features
used to score, while based on the previous work in
dependency parsing (McDonald et al, 2005), intro-
duce some novel concepts such as better codification
of context and surface distances, and runtime infor-
mation from dependencies previously parsed.
Regarding experimentation, the treatment of mul-
tilingual data has been totally blind, with no spe-
cial processing or features that depend on the lan-
guage. Considering its simplicity, our system
achieves moderate but encouraging results, with an
overall labeled attachment accuracy of 74.72% on
the CoNLL-X test set.
2 Parsing and Learning Algorithms
This section describes the three main components of
the dependency parsing: the parsing model, the pars-
ing algorithm, and the learning algorithm.
2.1 Model
Let 1, . . . , L be the dependency labels, defined be-
forehand. Let x be a sentence of n words, x1 . . . xn.
Finally, let Y(x) be the space of well-formed depen-
dency trees for x. A dependency tree y ? Y(x) is a
set of n dependencies of the form [h,m, l], where
h is the index of the head word (0 ? h ? n,
where 0 means root), m is the index of the modi-
fier word (1 ? m ? n), and l is the dependency
label (1 ? l ? L). Each word of x participates as a
modifier in exactly one dependency of y.
Our dependency parser, dp, returns the maximum
scored dependency tree for a sentence x:
dp(x,w) = argmax
y?Y(x)
?
[h,m,l]?y
sco([h,m, l], x, y,w)
In the formula, w is the weight vector of the
parser, that is, the set of parameters used to score de-
pendencies during the parsing process. It is formed
by a concatenation of L weight vectors, one for each
dependency label, w = (w1, . . . ,wl, . . . ,wL). We
assume a feature extraction function, ?, that repre-
sents an unlabeled dependency [h,m] in a vector of
D features. Each of the wl has D parameters or
dimensions, one for each feature. Thus, the global
181
weight vector w maintains L ? D parameters. The
scoring function is defined as follows:
sco([h,m, l], x, y,w) = ?(h,m, x, y) ? wl
Note that the scoring of a dependency makes use
of y, the tree that contains the dependency. As de-
scribed next, at scoring time y just contains the de-
pendencies found between h and m.
2.2 Parsing Algorithm
We use the cubic-time algorithm for dependency
parsing proposed by Eisner (1996; 2000). This pars-
ing algorithm assumes that trees are projective, that
is, dependencies never cross in a tree. While this as-
sumption clearly does not hold in the CoNLL-X data
(only Chinese trees are actually 100% projective),
we chose this algorithm for simplicity. As it will be
shown, the percentage of non-projective dependen-
cies is not very high, and clearly the error rates we
obtain are caused by other major factors.
The parser is a bottom-up dynamic programming
algorithm that visits sentence spans of increasing
length. In a given span, from word s to word e, it
completes two partial dependency trees that cover
all words within the span: one rooted at s and the
other rooted at e. This is done in two steps. First, the
optimal dependency structure internal to the span is
chosen, by combining partial solutions from inter-
nal spans. This structure is completed with a depen-
dency covering the whole span, in two ways: from
s to e, and from e to s. In each case, the scoring
function is used to select the dependency label that
maximizes the score.
We take advantage of this two-step processing to
introduce features for the scoring function that rep-
resent some of the internal dependencies of the span
(see Section 3 for details). It has to be noted that
the parsing algorithm we use does not score depen-
dencies on top of every possible internal structure.
Thus, by conditioning on features extracted from y
we are making the search approximative.
2.3 Perceptron Learning
As learning algorithm, we use Perceptron tailored
for structured scenarios, proposed by Collins (2002).
In recent years, Perceptron has been used in a num-
ber of Natural Language Learning works, such as in
w = 0
for t = 1 to T
foreach training example (x, y) do
y? = dp(x,w)
foreach [h,m, l] ? y\y? do
wl = wl + ?(h,m, x, y?)
foreach [h,m, l] ? y?\y do
wl = wl ? ?(h,m, x, y?)
return w
Figure 1: Pseudocode of the Perceptron Algorithm. T is a
parameter that indicates the number of epochs that the algorithm
cycles the training set.
partial parsing (Carreras et al, 2005) or even depen-
dency parsing (McDonald et al, 2005).
Perceptron is an online learning algorithm that
learns by correcting mistakes made by the parser
when visiting training sentences. The algorithm is
extremely simple, and its cost in time and memory
is independent from the size of the training corpora.
In terms of efficiency, though, the parsing algorithm
must be run at every training sentence.
Our system uses the regular Perceptron working
in primal form. Figure 1 sketches the code. Given
the number of languages and dependency types in
the CoNLL-X exercise, we found prohibitive to
work with a dual version of Perceptron, that would
allow the use of a kernel function to expand features.
3 Features
The feature extraction function, ?(h,m, x, y), rep-
resents in a feature vector a dependency from word
positions m to h, in the context of a sentence x and a
dependency tree y. As usual in discriminative learn-
ing, we work with binary indicator features: if a cer-
tain feature is observed in an instance, the value of
that feature is 1; otherwise, the value is 0. For con-
venience, we describe ? as a composition of several
base feature extraction functions. Each extracts a
number of disjoint features. The feature extraction
function ?(h,m, x, y) is calculated as:
?token(x, h, ?head?) + ?tctx(x, h, ?head?) +
?token(x,m, ?mod?) + ?tctx(x,m, ?mod?) +
?dep(x,mmdh,m) + ?dctx(x,mmdh,m) +
?dist(x,mmdh,m) + ?runtime(x, y, h,m, dh,m)
where ?token extracts context-independent token
features, ?tctx computes context-based token fea-
tures, ?dep computes context-independent depen-
182
?token(x, i, type)
type ? w(xi)
type ? l(xi)
type ? cp(xi)
type ? fp(xi)
foreach(ms): type ?ms(xi)
type ? w(xi) ? cp(xi)
foreach(ms): type ? w(xi) ?ms(xi)
?tctx(x, i, type)
?token(x, i? 1, type ? string(i? 1))
?token(x, i? 2, type ? string(i? 2))
?token(x, i+ 1, type ? string(i+ 1))
?token(x, i+ 2, type ? string(i+ 2))
type ? cp(xi) ? cp(xi?1)
type ? cp(xi) ? cp(xi?1) ? cp(xi?2)
type ? cp(xi) ? cp(xi+1)
type ? cp(xi) ? cp(xi+1) ? cp(xi+2)
Table 1: Token features, both context-independent (?token)
and context-based (?tctx). type - token type, i.e. ?head? or
?mod?, w - token word, l - token lemma, cp - token coarse part-
of-speech (POS) tag, fp - token fine-grained POS tag, ms -
token morpho-syntactic feature. The ? operator stands for string
concatenation.
?dep(x, i, j,dir)
dir ? w(xi) ? cp(xi) ? w(xj) ? cp(xj)
dir ? cp(xi) ? w(xj) ? cp(xj)
dir ? w(xi) ? w(xj) ? cp(xj)
dir ? w(xi) ? cp(xi) ? cp(xj)
dir ? w(xi) ? cp(xi) ? w(xj)
dir ? w(xi) ? w(xj)
dir ? cp(xi) ? cp(xj)
?dctx(x, i, j,dir)
dir ? cp(xi) ? cp(xi+1) ? cp(xj?1) ? cp(xj)
dir ? cp(xi?1) ? cp(xi) ? cp(xj?1) ? cp(xj)
dir ? cp(xi) ? cp(xi+1) ? cp(xj) ? cp(xj+1)
dir ? cp(xi?1) ? cp(xi) ? cp(xj) ? cp(xj+1)
Table 2: Dependency features, both context-independent
(?dep) and context-based (?dctx), between two points i and j,
i < j. dir - dependency direction: left to right or right to left.
dency features, ?dctx extracts contextual depen-
dency features, ?dist calculates surface-distance fea-
tures between the two tokens, and finally, ?runtime
computes dynamic features at runtime based on the
dependencies previously built for the given interval
during the bottom-up parsing. mmdh,m is a short-
hand for a triple of numbers: min(h,m), max(h,m)
and dh,m (a sign indicating the direction, i.e., +1 if
m < h, and ?1 otherwise).
We detail the token features in Table 1, the depen-
dency features in Table 2, and the surface-distance
features in Table 3. Most of these features are in-
spired by previous work in dependency parsing (Mc-
Donald et al, 2005; Collins, 1999). What is impor-
?dist(x, i, j,dir)
foreach(k ? (i, j)): dir ? cp(xi) ? cp(xk) ? cp(xj)
number of tokens between i and j
number of verbs between i and j
number of coordinations between i and j
number of punctuations signs between i and j
Table 3: Surface distance features between points i and j. Nu-
meric features are discretized using ?binning? to a small number
of intervals.
?runtime(x,y,h,m,dir)
let l1, . . . , lS be the labels of dependencies
in y that attach to h and are found from m to h.
foreach i, 1? i?S : dir ? cp(xh) ? cp(xm) ? li
if S?1 , dir ? cp(xh) ? cp(xm) ? l1
if S?2 , dir ? cp(xh) ? cp(xm) ? l1 ? l2
if S?3 , dir ? cp(xh) ? cp(xm) ? l1 ? l2 ? l3
if S?4 , dir ? cp(xh) ? cp(xm) ? l1 ? l2 ? l3 ? l4
if S=0 , dir ? cp(xh) ? cp(xm) ? null
if 0<S?4 , dir ? cp(xh) ? cp(xm) ? regular
if S>4 , dir ? cp(xh) ? cp(xm) ? big
Table 4: Runtime features of y between m and h.
tant for the work presented here is that we construct
explicit feature combinations (see above tables) be-
cause we configured our linear predictors in primal
form, in order to keep training times reasonable.
While the features presented in Tables 1, 2, and 3
are straightforward exploitations of the training data,
the runtime features (?runtime) take a different, and
to our knowledge novel in the proposed framework,
approach: for a dependency from m to h, they rep-
resent the dependencies found between m and h
that attach also to h. They are described in detail
in Table 4. As we have noted above, these fea-
tures are possible because of the parsing scheme,
which scores a dependency only after all dependen-
cies spanned by it are scored.
4 Experiments and Results
We experimented on the 13 languages proposed
in the CoNLL-X Shared Task (Hajic? et al, 2004;
Simov et al, 2005; Simov and Osenova, 2003; Chen
et al, 2003; Bo?hmova? et al, 2003; Kromann, 2003;
van der Beek et al, 2002; Brants et al, 2002;
Kawata and Bartels, 2000; Afonso et al, 2002;
Dz?eroski et al, 2006; Civit and Mart??, 2002; Nilsson
et al, 2005; Oflazer et al, 2003; Atalay et al, 2003).
Our approach to deal with many different languages
was totally blind: we did not inspect the data to mo-
tivate language-specific features or processes.
183
We did feature filtering based on frequency
counts. Our feature extraction patterns, that ex-
ploit both lexicalization and combination, gener-
ate millions of feature dimensions, even with small
datasets. Our criterion was to use at most 500,000
different dimensions in each label weight vector. For
each language, we generated all possible features,
and then filtered out most of them according to the
counts. Depending on the number of training sen-
tences, our counts cut-offs vary from 3 to 15.
For each language, we held out from training data
a portion of sentences (300, 500 or 1000 depend-
ing on the total number of sentences) and trained a
model for up to 20 epochs in the rest of the data. We
evaluated each model on the held out data for differ-
ent number of training epochs, and selected the op-
timum point. Then, we retrained each model on the
whole training set for the selected number of epochs.
Table 5 shows the attachment scores obtained
by our system, both unlabeled (UAS) and labeled
(LAS). The first column (GOLD) presents the LAS
obtained with a perfect scoring function: the loss in
accuracy is related to the projectivity assumption of
our parsing algorithm. Dutch turns out to be the
most non-projective language, with a loss in accu-
racy of 5.44%. In our opinion, the loss in other lan-
guages is relatively small, and is not a major limita-
tion to achieve a high performance in the task. Our
system achieves an overall LAS of 74.72%, with
substantial variation from one language to another.
Turkish, Arabic, Dutch, Slovene and Czech turn out
to be the most difficult languages for our system,
with accuracies below 70%. The easiest language
is clearly Japanese, with a LAS of 88.13%, followed
by Chinese, Portuguese, Bulgarian and German, all
with LAS above 80%.
Table 6 shows the contribution of base feature ex-
traction functions. For four languages, we trained
models that increasingly incorporate base functions.
It can be shown that all functions contribute to a bet-
ter score. Contextual features (?3) bring the system
to the final order of performance, while distance (?4)
and runtime (?) features still yield substantial im-
provements.
5 Analysis and Conclusions
It is difficult to explain the difference in performance
across languages. Nevertheless, we have identified
GOLD UAS LAS
Bulgarian 99.56 88.81 83.30
Arabic 99.76 72.65 60.94
Chinese 100.0 88.65 83.68
Czech 97.78 77.44 68.82
Danish 99.18 85.67 79.74
Dutch 94.56 71.39 67.25
German 98.84 85.90 82.41
Japanese 99.16 90.79 88.13
Portuguese 98.54 87.76 83.37
Slovene 98.38 77.72 68.43
Spanish 99.96 80.77 77.16
Swedish 99.64 85.54 78.65
Turkish 98.41 70.05 58.06
Overall 98.68 81.19 74.72
Table 5: Results of the system on test data. GOLD: labeled
attachment score using gold scoring functions; the loss in ac-
curacy is caused by the projectivity assumption made by the
parser. UAS : unlabeled attachment score. LAS : labeled at-
tachment score, the measure to compare systems in CoNLL-X.
Bulgarian is excluded from overall scores.
?1 ?2 ?3 ?4 ?
Turkish 33.02 48.00 55.33 57.16 58.06
Spanish 12.80 53.80 68.18 74.27 77.16
Portuguese 47.10 64.74 80.89 82.89 83.37
Japanese 38.78 78.13 86.87 88.27 88.13
Table 6: Labeled attachment scores at increasing feature con-
figurations. ?1 uses only ?token at the head and modifier. ?2
extends ?1 with ?dep. ?3 incorporates context features, namely
?tctx at the head and modifier, and ?dctx. ?4 extends ?3 with
?dist. Finally, the final feature extraction function ? increases
?4 with ?runtime.
four generic factors that we believe caused the most
errors across all languages:
Size of training sets: the relation between the
amount of training data and performance is strongly
supported in learning theory. We saw the same re-
lation in this evaluation: for Turkish, Arabic, and
Slovene, languages with limited number of train-
ing sentences, our system obtains accuracies below
70%. However, one can not argue that the training
size is the only cause of errors: Czech has the largest
training set, and our accuracy is also below 70%.
Modeling large distance dependencies: even
though we include features to model the distance
between two dependency words (?dist), our analy-
sis indicates that these features fail to capture all the
intricacies that exist in large-distance dependencies.
Table 7 shows that, for the two languages analyzed,
the system performance decreases sharply as the dis-
tance between dependency tokens increases.
184
to root 1 2 3 ? 6 >= 7
Spanish 83.04 93.44 86.46 69.97 61.48
Portuguese 90.81 96.49 90.79 74.76 69.01
Table 7: F?=1 score related to dependency token distance.
Modeling context: many attachment decisions, e.g.
prepositional attachment, depend on additional con-
text outside of the two dependency tokens. To ad-
dress this issue, we have included in our model fea-
tures to capture context, both static (?dctx and ?tctx)
and dynamic (?runtime). Nevertheless, our error
analysis indicates that our model is not rich enough
to capture the context required to address complex
dependencies. All the top 5 focus words with the
majority of errors for Spanish and Portuguese ? ?y?,
?de?, ?a?, ?en?, and ?que? for Spanish, and ?em?,
?de?, ?a?, ?e?, and ?para? for Portuguese ? indicate
complex dependencies such as prepositional attach-
ments or coordinations.
Projectivity assumption: Dutch is the language
with most crossing dependencies in this evaluation,
and the accuracy we obtain is below 70%.
On the Degree of Lexicalization We conclude the
error analysis of our model with a look at the de-
gree of lexicalization in our model. A quick analy-
sis of our model on the test data indicates that only
34.80% of the dependencies for Spanish and 42.94%
of the dependencies for Portuguese are fully lexical-
ized, i.e. both the head and modifier words appear
in the model feature set (see Table 8). There are
two reasons that cause our model to be largely un-
lexicalized: (a) in order to keep training times rea-
sonable we performed heavy filtering of all features
based on their frequency, which eliminates many
lexicalized features from the final model, and (b)
due to the small size of most of the training cor-
pora, most lexicalized features simply do not ap-
pear in the testing section. Considering these re-
sults, a reasonable question to ask is: how much
are we losing because of this lack of lexical infor-
mation? We give an approximate answer by ana-
lyzing the percentage of fully-lexicalized dependen-
cies that are correctly parsed by our model. As-
suming that our model scales well, the accuracy on
fully-lexicalized dependencies is an indication for
the gain (or loss) to be had from lexicalization. Our
model parses fully-lexicalized dependencies with an
Fully One token Fully
lexicalized unlexicalized unlexicalized
Spanish 34.80% 54.77% 10.43%
Portuguese 42.94% 49.26% 7.80%
Table 8: Degree of dependency lexicalization.
accuracy of 74.81% LAS for Spanish (2.35% lower
than the overall score) and of 83.77% LAS for Por-
tuguese (0.40% higher than the overall score). This
analysis indicates that our model has limited gains
(if any) from lexicalization.
In order to improve the quality of our dependency
parser we will focus on previously reported issues
that can be addressed by a parsing model: large-
distance dependencies, better modeling of context,
and non-projective parsing algorithms.
Acknowledgements
This work was partially funded by the European Union Com-
mission (PASCAL - IST-2002-506778) and Spanish Ministry
of Science and Technology (TRANGRAM - TIN2004-07925-
C03-02). Mihai Surdeanu was supported by a Ramon y Cajal
fellowship of the latter institution.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006.
CoNLL-X shared task on multilingual dependency parsing.
In Proc. of the Tenth Conf. on Computational Natural Lan-
guage Learning (CoNLL-X). SIGNLL.
X. Carreras, Llu??s Ma`rquez, and J. Castro. 2005. Filtering-
ranking perceptron learning for partial parsing. Machine
Learning, 1?3(60):41?71.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In Proc. of EMNLP-2002.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. of the 16th Intern.
Conf. on Computational Linguistics (COLING).
J. Eisner. 2000. Bilexical grammars and their cubic-time pars-
ing algorithms. In H. C. Bunt and A. Nijholt, editors, New
Developments in Natural Language Parsing, pages 29?62.
Kluwer Academic Publishers.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-
margin training of dependency parsers. In Proc. of the 43rd
Annual Meeting of the ACL.
185
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 161?171, Dublin, Ireland, August 23-29 2014.
Learning Task-specific Bilexical Embeddings
Pranava Swaroop Madhyastha Xavier Carreras Ariadna Quattoni
TALP Research Center
Universitat Polit`ecnica de Catalunya
Campus Nord UPC, Barcelona
pranava,carreras,aquattoni@lsi.upc.edu
Abstract
We present a method that learns bilexical operators over distributional representations of words
and leverages supervised data for a linguistic relation. The learning algorithm exploits low-
rank bilinear forms and induces low-dimensional embeddings of the lexical space tailored for
the target linguistic relation. An advantage of imposing low-rank constraints is that prediction
is expressed as the inner-product between low-dimensional embeddings, which can have great
computational benefits. In experiments with multiple linguistic bilexical relations we show that
our method effectively learns using embeddings of a few dimensions.
1 Introduction
We address the task of learning functions that compute compatibility scores between pairs of lexical
items under some linguistic relation. We refer to these functions as bilexical operators. As an instance of
this problem, consider learning a model that predicts the probability that an adjective modifies a noun in
a sentence. In this case, we would like the bilexical operator to capture the fact that some adjectives are
more compatible with some nouns than others. For example, a bilexical operator should predict that the
adjective electronic has high probability of modifying the noun device but little probability of modifying
the noun case.
Bilexical operators can be useful for multiple NLP applications. For example, they can be used to
reduce ambiguity in a parsing task. Consider the following sentence extracted from a weblog: Vynil
can be applied to electronic devices and cases, wooden doors and furniture and walls. If we want to
predict the dependency structure of this sentence we need to make several decisions. In particular, the
parser would need to decide (1) Does electronic modify devices? (2) Does electronic modify cases? (3)
Does wooden modify doors? (4) Does wooden modify furniture? Now imagine that in the corpus used to
train the parser none of these nouns have been observed, then it is unlikely that these attachments can be
resolved correctly. However, if an accurate noun-adjective bilexical operator were available most of the
uncertainty could be resolved. This is because a good bilinear operator would give high probability to the
pairs electronic-device, wooden-door, wooden-furniture and low probability to the pair electronic-case.
The simplest way of inducing a bilexical operator is to learn it from a training corpus. That is, assuming
that we are given some data annotated with a linguistic relation between a modifier and a head (e.g.
adjective and noun) we can simply build a maximum likelihood estimator for Pr(m | h) by counting the
occurrences of modifiers and heads under the target relation. For example, we could consider learning
bilexical operators from sentences annotated with dependency structures. Clearly, this model can not
generalize to head words not present in the training data.
To mitigate this we could consider bilexical operators that can exploit lexical embeddings, such as
a distributional vector-space representation of words. In this case, we assume that for every word we
can compute an n-dimensional vector space representation ?(w) ? R
n
. This representation typically
captures distributional features of the context in which the lexical item can occur. The key point is that
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
161
we do not need a supervised corpus to compute the representation. All we need is a large textual corpus
to compute the relevant statistics. Once we have the representation we can exploit operations in the
induced vector space to define lexical compatibility operators. For example we could define a bilexical
operator as:
Pr(m | h) =
exp {??(m), ?(h)?}
?
m
?
exp {??(m
?
), ?(h)?}
(1)
where ??(x), ?(y)? denotes the inner-product. Alternatively, given an initial high-dimensional distribu-
tional representation computed from a large textual corpus we could first induce a projection to a lower
k dimensional space by performing truncated singular value decomposition. The idea is that the lower
dimensional representation will be more efficient and it will better capture the relevant dimensions of the
distributional representation. The bilexical operator would then take the form of:
Pr(m|h) =
exp {?U?(m), U?(h)?}
?
m
?
exp {?U?(m
?
), U?(h)?}
(2)
where U ? R
k?n
is the projection matrix obtained via SVD. The advantage of this approach is that as
long as we can estimate the distribution of contexts of words we can compute the value of the bilexical
operator. However, this approach has a clear limitation: to design a bilinear operator for a target linguistic
relation we must design the appropriate distributional representation. Moreover, there is no clear way of
exploiting a supervised training corpus.
In this paper we combine both the supervised and distributional approaches and present a learning
algorithm for inducing bilexical operators from a combination of supervised and unsupervised training
data. The main idea is to define bilexical operators using bilinear forms over distributional representa-
tions: ?(x)
>
W?(y), where W ? R
n?n
is a matrix of parameters. We can then train our model on the
supervised training corpus via conditional maximum-likelihood estimation. To induce a low-dimensional
representation, we first observe that the implicit dimensionality of the bilinear form is given by the rank
ofW . In practice controlling the rank ofW can result in important computational savings in cases where
one evaluates a target word x against a large number of candidate words y: this is because we can project
the representations ?(x) and ?(y) down to the low-dimensional space where evaluating the function is
simply an inner-product. This setting is in fact usual, for example for lexical retrieval applications (e.g.
given a noun, sort all adjectives in the vocabulary according to their compatibility), or for parsing (where
one typically evaluates the compatibility between all pairs of words in a sentence).
Consequently with these ideas, we propose to regularize the maximum-likelihood estimation using
a nuclear norm regularizer that serves as a convex relaxation to the rank function. To minimize the
regularized objective we make use of an efficient iterative proximal method that involves computing the
gradient of the function and performing singular value decompositions.
We test the proposed algorithm on several linguistic relations and show that it can predict modifiers
for unknown words more accurately than the unsupervised approach. Furthermore, we compare different
types of regularizers for the bilexical operatorW , and observe that indeed the low-rank regularizer results
in the most efficient technique at prediction time.
In summary, the main contributions of this paper are:
? We propose a supervised framework for learning bilexical operators over distributional representa-
tions, based on learning bilinear forms W .
? We show that we can obtain low-dimensional compressions of the distributional representation by
imposing low-rank constraints to the bilinear form. Combined with supervision, this results in
lexical embeddings tailored for a specific bilexical task.
? In experiments, we show that our models generalize well to unseen word pairs, using only a few
dimensions, and outperforming standard unsupervised distributional approaches. We also present
an application to prepositional phrase attachment.
162
2 Bilinear Models for Bilexical Predictions
2.1 Definitions
Let V be a vocabulary, and let x ? V denote a word. Let H ? V be a set of head words, andM? V be
a set of modifier words. In the noun-adjective relation example, H is the set of nouns andM is the set
of adjectives.
The task is as follows. We are given a training set of l tuples D = {(m,h)
1
, . . . , (m,h)
l
}, where
m ? M and h ? H and we want to learn a model of the conditional distribution Pr(m | h). We want
this model to perform well on all head-modifier pairs. In particular we will test the performance of the
model on heads that do not appear in D.
We assume that we are given access to a distributional representation function ? : V ? R
n
, where
?(x) is the n-dimensional representation of x. Typically, this function is computed from an unsupervised
corpus. We use ?(x)
[i]
to refer to the i-th coordinate of the vector.
2.2 Bilinear Model
Our model makes use of the bilinear form W : R
n
? R
n
? R, where W ? R
n?n
, and evaluates as
?(m)
>
W?(h). We define the bilexical operator as:
Pr(m | h) =
exp
{
?(m)
>
W?(h)
}
?
m
?
?M
exp {?(m
?
)
>
W?(h)}
(3)
Note that the above model is nothing more than a conditional log-linear model defined over n
2
fea-
tures f
i,j
(m,h) = ?(m)
[i]
?(h)
[j]
(this can be seen clearly when we write the bilinear form as
?
n
i=1
?
n
j=1
f
i,j
(m,h)W
i,j
. The reason why it is useful to regard W as a matrix will become evident in
the next section.
Before moving to the next section, let us note that the unsupervised SVD model in Eq. (2) is also a
bilinear model as defined here. This can be seen if we set W = UU
>
, which is a bilinear form of rank
k. The key difference is in the way W is learned using supervision.
3 Learning Low-rank Bilexical Operators
3.1 Low-rank Optimization
Given a training set D and a feature function ?(x) we can do standard conditional max-likelihood opti-
mization and minimize the negative of the log-likelihood function, log Pr(D):
?
(m,h)?D
?(m)
>
W?(h)? log
?
m
?
?M
exp
{
?(m
?
)
>
W?(h)
}
(4)
We would like to control the complexity of the learned model by including some regularization penalty.
Moreover, like in the low-dimensional unsupervised approach we want our model to induce a low-
dimensional representation of the lexical space. The first observation is that the bilinear form computes
a weighted inner product in some space. Consider the singular value decomposition: W = U?V . We
can write the bilinear form as: [?(m)
>
U ] ? [V ?(h)], thus we can regard m? = ?(m)
>
U as a projection
of m and
?
h = V ?(h) as a projection of h. Then the bilinear form can be written as:
?
n
i=1
?
[i,i]
m?
[i]
?
h
[i]
.
The rank of W defines the dimensionality of the induced space. It is easy to see that if W has rank k it
can be factorized as U?V where U ? R
n?k
and V ? R
k?n
.
Since the rank of W determines the dimensionality of the induced space, it would be reasonable to
add a rank minimization penalty in the objective in (4). Unfortunately this would lead to a non-convex
regularized objective. Instead, we propose to use as a regularizer a convex relaxation of the rank function,
the nuclear norm ?W?
?
(the `
1
norm of the singular values of W ). Putting it all together, our learning
algorithm minimizes:
?
(m,h)?D
? log Pr(m | h)) + ??W?
?
(5)
163
Here ? is a constant that controls the trade-off between fitting the data and the complexity of the model.
This objective is clearly convex since both the objective and the regularizer are convex. To minimize it
we use the a proximal gradient algorithm which is described next.
3.2 A Proximal Algorithm for Bilexical Operators
We now describe the learning algorithm that we use to induce the bilexical operators from training data.
We are interested in minimizing the objective (5), or in fact a more general version where we can replace
the regularizer ?W?
?
by standard `
1
or `
2
penalties. For any convex regularizer r(W ) (namely `
1
, `
2
or
the nuclear norm) the objective in (5) is convex. Our learning algorithm is based on a simple optimization
scheme known as forward-backward splitting (FOBOS) (Duchi and Singer, 2009).
This algorithm has convergence rates in the order of 1/
2
, which we found sufficiently fast for our
application. Many other optimization approaches are possible, for example one could express the regu-
larizer as a convex constraint and utilize a projected gradient method which has a similar convergence
rate. Proximal methods are slightly more simple to implement and we chose the proximal approach.
The FOBOS algorithm works as follows. In a series of iterations t = 1 . . . T compute parameter
matrices W
t
as follows:
1. Compute the gradient of the negative log-likelihood, and update the parameters
W
t+0.5
= W
t
? ?
t
g(W
t
)
where ?
t
=
c
?
t
is a step size and g(W
t
) is the gradient of the loss at W
t
.
2. Update W
t+0.5
to take into account the regularization penalty r(W ), by solving
W
t+1
= argmin
W
||W
t+0.5
?W ||
2
2
+ ?
t
?r(W )
For the regularizers we consider, this step is solved using the proximal operator associated with the
regularizer. Specifically:
? For `
1
it is a simple thresholding:
W
t+1
(i, j) = sign(W
t+0.5
(i, j)) ?max(W
t+0.5
(i, j)? ?
t
?, 0)
? For `
2
it is a simple scaling:
W
t+1
=
1
1 + ?
t
?
W
t+0.5
? For nuclear-norm, perform SVD thresholding. Compute the SVD to write W
t+0.5
= USV
>
with S a diagonal matrix and U, V orthogonal matrices. Denote by ?
i
the i-th element on the
diagonal of S. Define a new matrix
?
S with diagonal elements ??
i
= max(?
i
? ?
t
?, 0). Then
set
W
t+1
= U
?
SV
>
Optimizing a bilinear model using nuclear-norm regularization involves the extra cost of performing
SVD of W at each iteration. In our experiments the dimension of W was 2, 000? 2, 000 and computing
SVD was fast, much faster than computing the gradient, which dominates the cost of the algorithm. The
optimization parameters of the method are the regularization constant ?, the step size constant c and the
number of iterations T . In our experiments we ran a range of ? and c values for 200 iterations, and used
a validation set to pick the best configuration.
164
4 Related Work
Research in learning representations for natural language processing can be broadly classified into
two different paradigms based on the learning setting: unsupervised representation learning and semi-
supervised representation learning. Unsupervised representation learning does not require any supervised
training data, while semi-supervised representation learning requires the presence of supervised training
data with the potential advantage that it can adapt the representation to the task at hand.
Unsupervised approaches to learning representations mainly involve representations that are learned
not for a specific task, rather a variety of tasks. These representations rely more on the property of
abstractness and generalization. Further, unsupervised approaches can be roughly categorized into (a)
clustering-based approaches that make use of clusters induced using a notion of distributed similarity,
such as the method by Brown et al. (1992); (b) neural-network-based representations that focus on learn-
ing multilayer neural network in a way to extract features from the data (Morin and Bengio, 2005; Mnih
and Hinton, 2007; Bengio and S?en?ecal, 2008; Mnih and Hinton, 2009); (c) pure distributional approaches
that principally follow the distributional assumption that the words which share a set of contexts are sim-
ilar (Sahlgren, 2006; Turney and Pantel, 2010; Dumais et al., 1988; Landauer et al., 1998; Lund et al.,
1995; V?ayrynen et al., 2007).
We also induce lexical embeddings, but in our case we employ supervision. That is, we follow a
semi-supervised paradigm for learning representations. Semi-supervised approaches initially learn rep-
resentations typically in an unsupervised setting and then induce a representation that is jointly learned
for the task with a labeled corpus. A high-dimensional representation is extracted from unlabeled data,
while the supervised step compresses the representation to be low-dimensional in a way that favors the
the task at hand.
Collobert and Weston (2008) present a neural network language model, where given a sentence, it
performs a set of language processing tasks (from part of speech tagging, chunking, extracting named
entity, extracting semantic roles and decisions on the correctness of the sentence) by using the learned
representations. The representation itself is extracted from unlabeled corpora, while all the other tasks
are jointly trained on labeled corpus.
Socher et al. (2011) present a model based on recursive neural networks that learns vector space rep-
resentations for words, multi-word phrases and sentences. Given a sentence with its syntactic structure,
their model assings vector representations to each of the lexical tokens of the sentence, and then traverses
the syntactic tree bottom-up, such that at each node a vector representation of the corresponding phrase
is obtained by composing the vectors associated with the children.
Bai et al. (2010) use a technique similar to ours, using bilinear forms with low-rank constraints. In
their case, they explicitly look for a low-rank factorization of the matrix, making their optimization
non-convex. As far as we know, ours is the first convex formulation, where we employ a relaxation
of the rank (i.e. the nuclear norm) to make the objective convex. They apply the method to document
ranking, and thus optimize a max-margin ranking loss. In our application to bilexical models, we perform
conditional max-likelihood estimation. Hutchinson et al. (2013) propose an explicitly sparse and low-
rank maximum-entropy language model. The sparse plus low rank setting is learned in such a way that
the low rank component learns the regularities in the training data and the sparse component learns the
exceptions like multiword expressions etc.
Chechik et al. (2010) also learned bilinear operators using max-margin techniques, with pairwise
similarity as supervision, but they did not consider low-rank constraints.
One related area where bilinear operators are used to induce embeddings is distance metric learning.
Weinberger and Saul (2009) used large-margin nearest neighbor methods to learn a non-sparse embed-
ding, but these are computationally intensive and might not be suitable for large-scale tasks in NLP.
5 Experiments on Syntactic Relations
We conducted a set of experiments to test the ability of our algorithm to learn bilexical operators for
several linguistic relations. As supervised training data we use the gold standard dependencies of the
WSJ training section of the Penn Treebank (Marcus et al., 1993). We consider the following relations:
165
 50
 55
 60
 65
 70
 75
 80
 85
 90
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
Adjectives given Noun
unsupervised
NN
L1
L2
 66
 68
 70
 72
 74
 76
 78
 80
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
Nouns given Adjective
unsupervised
NN
L1
L2
 46
 48
 50
 52
 54
 56
 58
 60
 62
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
Objects given Verb
unsupervised
NN
L1
L2
 60
 65
 70
 75
 80
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
Verbs given Object
unsupervised
NN
L1
L2
Figure 1: Pairwise accuracy with respect to the number of double operations required to compute the
distribution over modifiers for a head word. Plots for noun-adjective and verb-object relations, in both
directions.
? Noun-Adjective: we model the distribution of adjectives given a noun; and a separate distribution
of nouns given an adjective.
? Verb-Object: we model the distribution of object nouns given a verb; and a separate distribution of
verbs given an object.
? Prepositions: in this case we consider bilexical operators associated with a preposition, which model
the probability of a head noun or verb above the preposition given the noun below the preposition.
We present results for prepositional relations given by ?with?, ?for?, ?in? and ?on?.
The distributional representation ?(x) was computed using the BLLIP corpus (Charniak et al., 2000).
We compute a bag-of-words representation for the context of each lexical item, that is ?(w)
[i]
corre-
sponds to the frequency of word i appearning in the context of w. We use a context window of size 10
and restrict our bag-of-words vocabulary to contain only the 2,000 most frequent words present in the
corpus. Vectors were normalized.
166
 50
 55
 60
 65
 70
 75
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
With
unsupervised
NN
L1
L2
 54
 56
 58
 60
 62
 64
 66
 68
 70
 72
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
For
unsupervised
NN
L1
L2
 46
 48
 50
 52
 54
 56
 58
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
In
unsupervised
NN
L1
L2
 60
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
1e3 1e4 1e5 1e6 1e7 1e8
pa
irw
ise
 a
cc
ur
ac
y
number of operations
On
unsupervised
NN
L1
L2
Figure 2: Pairwise accuracy with respect to the number of double operations required to compute the
distribution over modifiers for a head word. Plots for four prepositional relations: with, for, in, on. The
distributions are of verbs and objects above the preposition given the noun below the preposition.
To test the performance of our algorithm for each relation we partition the set of heads into a training
and a test set, 60% of the heads are use for training, 10% of the heads are used for validation and 30% of
the heads are used for testing. Then, we consider all observed modifiers in the data to form a vocabulary
of modifier words. The goal of this task is to learn conditional distribution over all these modifers given
a head word without context. In our experiments, the number of modifiers per relation ranges from 2,500
to 7,500 words. For each head word, we create a list of compatible modifiers from the annotated data, by
taking all modifiers that occur at least once with the head. Hence, for each head the set of all modifiers
is partitioned into compatible and non-compatible. For testing, we measure a pairwise accuracy, the
percentage of compatible/non-compatible pairs of modifiers where the former obtains higher probability.
Let us stress that none of the test head words has been observed in training, while the list of modifiers is
the same for training, validation and testing.
We compare the performance of the bilexical model trained with nuclear norm regularization (NN)
with other regularization penalties (L1 and L2). We also compare these supervised methods with an
167
Noun Predicted Adjectives
president executive, senior, chief, frank, former, international, marketing,
assistant, annual, financial
wife former, executive, new, financial, own, senior, old, other, deputy,
major
shares annual, due, net, convertible, average, new, high-yield, initial,
tax-exempt, subordinated
mortgages annualized, annual, three-month, one-year, average, six-month,
conventional, short-term, higher, lower
month last, next, fiscal, first, past, latest, early, previous, new, current
problem new, good, major, tough, bad, big, first, financial, long, federal
holiday new, major, special, fourth-quarter, joint, quarterly, third-quarter,
small, strong, own
Table 1: 10 most likely adjectives for some test nouns.
unsupervised model: a low-dimensional SVD model as in Eq. (2), which corresponds to an inner product
as in Eq. (1) when all dimensions are considered.
To report performance, we measure pairwise accuracy with respect to the capacity of the model in
terms of number of active parameters. To measure the capacity of a model we consider the number of
double operations that are needed to compute, given a head, the scores for all modifiers in the vocabulary
(we exclude the exponentiations and normalization needed to compute the distribution of modifiers given
a head, since this is a constant cost for all the models we compare, and is not needed if we only want to
rank modifiers). Recall that the dimension of ?(x) is n, and assume that there are m total modifiers in
the vocabulary. In our experiments n = 2, 000 and m ranges from 2, 500 to 7, 500. The correspondances
with operations are:
? Assume that the L1 and L2 models have k non-zero weights in W . Then the number of operations
to compute a distribution is km.
? Assume that the NN and the unsupervised models have rank k. We assume that the modifier vectors
are alredy projected down to k dimensions. For a new head, one needs to project it and perform m
inner products, hence the number of operations is kn+ km.
Figure 1 shows the performance of models for noun-adjective and verb-object relations, while Figure 2
shows plots for prepositional relations.
1
The first observation is that supervised approaches outperform
the unsupervised approach. In cases such as noun-adjetive relations the unsupervised approach performs
close to the supervised approaches, suggesting that the pure distributional approach can sometimes work.
But in most relations the improvement obtained by using supervision is very large. When comparing the
type of regularizer, we see that if the capacity of the model is unrestricted (right part of the curves), all
models tend to perform similarly. However, when restricting the size, the nuclear-norm model performs
much better. Roughly, 20 hidden dimensions are enough to obtain the most accurate performances
(which result in? 140, 000 operations for initial representaions of 2, 000 dimensions and 5, 000 modifier
candidates). As an example of the type of predictions, Table 1 shows the most likely adjectives for some
test nouns.
6 Experiments on PP Attachment
We now switch to a standard classification task, prepositional phrase attachment, that we frame as a
bilexical prediction task. We start from the formulation of the task as a binary classification problem by
Ratnaparkhi et al. (1994): given a tuple x = ?v, o, p, n? consisting of a verb v, noun object o, preposition
1
To obtain curves for each model type with respect to a range of number of operations, we first obtained the best model on
validation data and then forced it to have at most k non-zero features or rank k by projecting, for a range of k values.
168
 55
 60
 65
 70
 75
 80
for from with
att
ac
hm
en
t a
cc
ura
cy
bilinear L1
bilinear L2
bilinear NN
linear
interpolated L1
interpolated L2
interpolated NN
Figure 3: Attachment accuracies of linear, bilinear and interpolated models for three prepositions.
p and noun n, decide if the prepositional phrase p-n attaches to v (y = V) or to o (y = O). For example,
in ? meet,demand,for,products? the correct attachment is O.
Ratnaparkhi et al. (1994) define a linear maximum likelihood model of the form
Pr(y | x) = exp{?w, f(x, y)?} ? Z(x)
?1
, where f(x, y) is a vector of d features, w is a parame-
ter vector in R
d
, and Z(x) is the normalizer summing over y = {V, O}. Here we define a bilexical
model of the form that uses a distributional representation ?:
Pr(V|?v, o, p, n?) =
exp{?(v)
>
W
p
V
?(n)}
Z(x)
Pr(O|?v, o, p, n?) =
exp{?(o)
>
W
p
O
?(n)}
Z(x)
(6)
The bilinear model is parameterized by two matricesW
V
andW
O
per preposition, each of which captures
the compatibility between nouns below a certain preposition and heads of V or O prepositional relations,
respectively. Again Z(x) is the normalizer summing over y = {V, O}, but now using the bilinear form.
It is straighforward to modify the learning algorithm in Section 3 such that the loss is a negative log-
likelihood for binary classification, and the regularizer considers the sum of norms of the model matrices.
We ran experiments using the data by Ratnaparkhi et al. (1994). We trained separate models for
different prepositions, focusing on the prepositions that are more ambiguous: for, from, with.
We compare to a linear ?maxent? model following Ratnaparkhi et al. (1994) that uses the same feature
set. Figure 3 shows the test results for the linear model, and bilinear models trained with L1, L2, NN
regularization penalties. The results of the bilinear models are significantly below the accuracy of the
linear model, suggesting that some of the non-lexical features of the linear model (such as prior weighting
of the two classes) might be difficult to capture by the bilinear model over lexical representations. To
check if the bilinear model might complement the linear model or just be worse than it, we tested simple
combinations based on linear interpolations. For a constant ? ? [0, 1] we define:
Pr(y | x) = ? Pr
L
(y | x) + (1? ?) Pr
B
(y | x) . (7)
We search for the best ? on the validation set, and report results of combining the linear model with
each of the three bilinear models. Results are shown also in Figure 3. Interpolation models improve over
linear models, though only the improvement for for is significant (2.6%). Future work should exploit
finer combinations between standard linear features and distributional bilinear forms.
7 Conclusions
We have presented a model for learning bilexical operators that can leverage both supervised and unsu-
pervised data. The model is based on exploiting bilinear forms over distributional representations. The
169
learning algorithm induces a low-dimensional representation of the lexical space by imposing low-rank
constraints on the parameters of the bilinear form. By means of supervision, our model induces two
low-dimensional lexical embeddings, one on each side of the bilexical linguistic relation, and compu-
tations can be expressed as an inner-product between the two embeddings. This factorized form of the
model can have great computational advantages: in many applications one needs to evaluate the function
multiple times for a fixed set of lexical items, for example in dependency parsing. Hence, one can first
project the lexical items to their embeddings, and then compute all pairwise scores as inner-products. In
experiments, we have shown that the embeddings we obtain in a number of linguistic relations can be
modeled with a few hidden dimensions.
As future work, we would like to apply the low-rank approach to other model forms that can employ
lexical embeddings, specially when supervision is available. For example, dependency parsing models,
or models of predicate-argument structures representing semantic roles, exploit bilexical relations. In
these applications, being able to generalize to word pairs that are not observed during training is essential.
We would also like to study how to combine low-rank bilexical operators, which in essence induce
a task-specific representation of words, with other forms of features that capture class or contextual
information. One desires that such combinations can preserve the computational advantages behind
low-rank embeddings.
Acknowledgements
We thank the reviewers for their helpful comments. This work was supported by projects XLike (FP7-
288342), ERA-Net CHISTERA VISEN and TACARDI (TIN2012-38523-C02-00). Xavier Carreras was
supported by the Ram?on y Cajal program of the Spanish Government (RYC-2008-02223).
References
Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, and
Kilian Weinberger. 2010. Learning to rank with (a lot of) word features. Information Retrieval, 13(3):291?314,
June.
Yoshua Bengio and Jean-S?ebastien S?en?ecal. 2008. Adaptive importance sampling to accelerate training of a neural
probabilistic language model. IEEE Transactions on Neural Networks, 19(4):713?722.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18:467?479.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, and Mark Johnson. 2000. BLLIP 1987?89 WSJ Corpus
Release 1, LDC No. LDC2000T43. Linguistic Data Consortium.
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio. 2010. Large scale online learning of image similarity
through ranking. Journal of Machine Learning Research, pages 1109?1135.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning,
ICML ?08, pages 160?167, New York, NY, USA. ACM.
John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. Journal
of Machine Learning Research, 10:2899?2934.
Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Scott Deerwester, and Richard Harshman. 1988. Using
latent semantic analysis to improve access to textual information. In SIGCHI Conference on Human Factors in
Computing Systems, pages 281?285. ACM.
Brian Hutchinson, Mari Ostendorf, and Maryam Fazel. 2013. Exceptions in language as learned by the multi-
factor sparse plus low-rank language model. In ICASSP, pages 8580?8584.
Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis.
Discourse Processes, 25:259?284.
Kevin Lund, Curt Burgess, and Ruth A. Atchley. 1995. Semantic and associative priming in high-dimensional
semantic space. In Cognitive Science Proceedings, LEA, pages 660?665.
170
Mitchell P. Marcus, Beatrice Santorini, and Mary A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of
English: The Penn Treebank. Computational Linguistics, 19(2):313?330.
Andriy Mnih and Geoffrey E. Hinton. 2007. Three new graphical models for statistical language modelling. In
Proceedings of the 24th International Conference on Machine Learning, pages 641?648.
Andriy Mnih and Geoffrey E. Hinton. 2009. A scalable hierarchical distributed language model. In Advances in
Neural Information Processing Systems, pages 1081?1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In AIS-
TATS05, pages 246?252.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 1994. A maximum entropy model for prepositional phrase
attachment. In Proceedings of the workshop on Human Language Technology, HLT ?94, pages 250?255,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and
paradigmatic relations between words in high-dimensional vector spaces. Ph.D. thesis, Stockholm University.
Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-
supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing, pages 151?161. Association for Computational Linguis-
tics.
Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37(1):141?188, January.
Jaakko J. V?ayrynen, Timo Honkela, and Lasse Lindqvist. 2007. Towards explicit semantic features using indepen-
dent component analysis. In Proceedings of the Workshop Semantic Content Acquisition and Representation
(SCAR), Stockholm, Sweden. Swedish Institute of Computer Science. SICS Technical Report T2007-06.
Kilian Q. Weinberger and Lawrence K. Saul. 2009. Distance metric learning for large margin nearest neighbor
classification. Journal of Machine Learning Research, 10:207?244, June.
171
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 624?635,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Unsupervised Spectral Learning of WCFG as Low-rank Matrix Completion
Raphae?l Bailly? Xavier Carreras? Franco M. Luque? Ariadna Quattoni?
? Universitat Polite`cnica de Catalunya
Barcelona, 08034
rbailly,carreras,aquattoni@lsi.upc.edu
? Universidad Nacional de Co?rdoba and CONICET
X500HUA Co?rdoba, Argentina
francolq@famaf.unc.edu.ar
Abstract
We derive a spectral method for unsupervised
learning of Weighted Context Free Grammars.
We frame WCFG induction as finding a Han-
kel matrix that has low rank and is linearly
constrained to represent a function computed
by inside-outside recursions. The proposed al-
gorithm picks the grammar that agrees with a
sample and is the simplest with respect to the
nuclear norm of the Hankel matrix.
1 Introduction
Weighted Context Free Grammars (WCFG) define
an important class of languages. Their expressivity
makes them good candidates for modeling a wide
range of natural language phenomena. This expres-
sivity comes at a cost: unsupervised learning of
WCFG seems to be a particularly hard task. And
while it is a well-studied problem, it is still to a great
extent unsolved.
Several methods for unsupervised learning of
WCFG have been proposed. Some rely on heuristics
that are used to build incrementally an approxima-
tion of the unknown grammar (Adriaans et al, 2000;
Van Zaanen, 2000; Tu and Honavar, 2008). Other
methods are based on maximum likelihood estima-
tion, searching for the grammar that has the largest
posterior given the training corpus (Baker, 1979;
Lari and Young, 1990; Pereira and Schabes, 1992;
Klein and Manning, 2002). Several Bayesian in-
ference approaches have also been proposed (Chen,
1995; Kurihara and Sato, 2006; Liang et al, 2007;
Cohen et al, 2010). These approaches perform pa-
rameter estimation by exploiting Markov sampling
techniques.
Recently, for the related problem of unsupervised
dependency parsing, Gormley and Eisner (2013)
proposed a new way of framing the max-likelihood
estimation. In their formulation the problem is ex-
pressed as an integer quadratic program subject to
non-linear constraints. They exploit techniques from
mathematical programming to solve the resulting
optimization.
In spirit, the work by Clark (2001; 2007) is prob-
ably the most similar to our approach since both ap-
proaches share an algebraic view of the problem. In
his case the key idea is to work with an algebraic
representation of a WCFG. The problem of recover-
ing the constituents of the grammar is reduced to the
problem of identifying its syntactic congruence.
In the last years, multiple spectral learning algo-
rithms have been proposed for a wide range of mod-
els (Hsu et al, 2009; Bailly et al, 2009; Bailly et al,
2010; Balle et al, 2011; Luque et al, 2012; Cohen
et al, 2012). Since the spectral approach provides a
good thinking tool to reason about distributions over
??, the question of whether they can be used for un-
supervised learning of WCFG seems natural. Still,
while spectral algorithms for unsupervised learning
of languages can learn regular languages, tree lan-
guages and simple dependency grammars, the fron-
tier to WCFG seems hard to reach.
In fact, the most recent theoretical results on spec-
tral learning of WCFG do not seem to be very en-
couraging. Recently, Hsu et al (2012) showed that
the problem of recovering the joint distribution over
PCFG derivations and their yields is not identifiable.
624
Although, for some simple grammar subclasses (e.g.
independent left and right children), identification in
the weaker sense (over the yields of the grammar)
implies strong identification (e.g. over joint distri-
bution of yields and derivations). In their paper, they
propose a spectral algorithm based on a generaliza-
tion of the method of moments for these restricted
subclasses.
Thus one open direction for spectral research con-
sists on defining subclasses of context free lan-
guages that can be learned (in the strong sense) from
observations of yields. Yet, an alternative research
direction is to consider learnability in the weaker
sense. In this paper we take the second road, and
focus on the problem of approximating the distribu-
tion over yields generated by a WCFG.
Our main contribution is to present a spectral al-
gorithm for unsupervised learning of WCFG. Fol-
lowing ideas from Balle et al (2012), the algo-
rithm is framed as a convex optimization where we
search for a low-rank matrix satisfying two types
of constraints: (1) Constraints derived from observ-
able statistics over yields; and (2) Constraints de-
rived from certain recurrence relations satisfied by a
WCFG. Our derivations of the learning algorithm il-
lustrate the main ingredients behind the spectral ap-
proach to learning functions over ?? which are: (1)
to exploit the recurrence relations satisfied by the
target family of functions and (2) provide algebraic
formulations of these relations.
We alert the reader that although we are able to
frame the problem as a convex optimization, the
number of variables involved is quite large and pro-
hibits a practical implementation of the method on
a realistic scenario. The experiments we present
should be regarded as examples designed to illus-
trate the behavior of the method. More research
is needed to make the optimization more efficient,
and we are optimistic that such improvements can
be achieved by exploiting problem-specific proper-
ties of the optimization. Regardless of this, ours is
a novel way of framing the grammatical inference
problem.
The rest of the paper is organized as follows. Sec-
tion 2 gives preliminaries on WCFG and the type of
functions we will learn. Section 3 establishes that
spectral methods can learn a WCFG from a Han-
kel matrix containing statistics about context-free
cuts. Section 4 presents the unsupervised algorithm,
where we formulate grammar induction as a low-
rank optimization. Section 5 presents experiments,
and finally we conclude the paper.
Notation Let ? be an alphabet. We use ? to de-
note an arbitrary symbol in ?. The set of all fi-
nite strings over ? is denoted by ??, where we
write ? for the empty string. We also use the set
?+ = ?? \ {?}.
We use bold letters to represent column vectors
v and matrices M . We use In to denote the n-
dimensional identity matrix. We use M+ to de-
note the Moore-Penrose pseudoinverse of some ma-
trixM . M?M ? is the Kronecker product between
matricesM ? Rm?n andM ? ? Rp?q resulting in a
matrix in Rmp?nq. The rest of notation will be given
as needed.
2 Weighted Context Free Grammars
In this section we define Weighted Context Free
Grammars (WCFG). We start with a classic defini-
tion and then describe an algebraic form of WCFG
that will be used throughout the paper. We also de-
scribe the fundamental recursions in WCFG.
2.1 WCFG in Classic Form
A WCFG over ? is a tuple G? =
?V,R, T, w?, wT , wR? where
? V is the set of non-terminal symbols. We as-
sume that V = {1, . . . , n} for some natural
number n, and that V ? ? = ?.
? R is a set of binary rules of the form i ? j k
where i, j, k ? V .
? T is a set of unary rules of the form i ? ?
where i ? V and ? ? ?.
? w? : V ? R, with w?(i) being the weight of
starting a derivation with non-terminal i.
? wT : V ? ? ? R, with wT (i ? ?) being the
weight of rule rewriting i into ?.
? wR : V ? V ? V ? R, with wR(i ? j k)
being the weight of rewriting i into j k.
A WCFG G? computes a function gG? : ?
+ ? R
defined as
gG?(x) =
?
i?V
w?(i)??G?(i
?
=? x) , (1)
625
where we define the inside function ??G? : V ??
+ ?
R recursively:
??G?(i
?
=? ?) = wT (i? ?) (2)
??G?(i
?
=? x) =
?
j,k?V
x1,x2??+
s.t. x=x1x2
wR(i? j k) (3)
??G?(j
?
=? x1)??G?(k
?
=? x2) ,
where in the second case we assume |x| > 1. The
inside function ??G?(i
?
=? x) exploits the fundamen-
tal inside recursion in WCFG (Baker, 1979; Lari and
Young, 1990). We will find useful to define the out-
side function ??G? : ?
??V ??? ? R defined recur-
sively as:
??G?(?; i;?) = w?(i) (4)
??G?(x; i; y) =
?
j,k?V
x1???,x2??+
s.t. x=x1x2
wR(j ? k i)? (5)
??G?(x1; j; y) ? ??G?(k
?
=? x2)
+
?
j,k?V
y1??+,y2???
s.t. y=y1y2
wR(j ? i k)?
??G?(x; j; y2) ? ??G?(k
?
=? y1) ,
where in the second case we assume that either
x 6= ? or y 6= ?.
For x, z ? ?? and y ? ?+ we have that
?
i?V
??G?(x; i; z) ? ??G?(i
?
=? y) (6)
is the weight that the grammar G? assigns to a string
xyz that has a cut or bracketing around y. Techni-
cally, it corresponds to the sum of the weights of all
derivations that have a constituent spanning y. In
particular we have that
gG?(x) =
?
i
??G?(?; i;?) ? ??G?(i
?
=? x) .
If x is a string of lengthm, and x[t:t?] is the substring
of x from positions t to t?, it also happens that
gG?(x) =
?
i
??G?(x[1:t?1]; i;x[t+1:m])???G?(i
?
=? x[t]))
for any t between 1 and m.
In this paper we will make frequent use of inside
and outside quantities. Notationally, for outsides the
semi-colon between two strings, i.e. x; z, will sim-
bolize a cut where we can insert an inside string y.
Finally, we note that Probabilistic Context Free
Grammars (PCFG) are a special case of WCFG
where: w?(i) is the probability to start a derivation
with non-terminal i; wR(i ? j k) is the condi-
tional probability of rewriting nonterminal i into j
and k; wT (i ? ?) is the probability of rewriting i
into symbol ?;
?
iw?(i) = 1; and for each i ? V ,?
j,k wR(i ? j k) +
?
? wT (i ? ?) = 1. Un-
der these conditions the function gG? is a probability
distibution over ?+.
2.2 WCFG in Algebraic Form
We now define a WCFG in algebraic form. A
Weighted Context Free Grammar (WCFG) over ?
with n states is a tuple G = ???, {??},A? with:
? An initial vector ?? ? Rn.
? Terminal vectors ?? ? R
n for ? ? ?.
? A bilinear operatorA ? Rn?n
2
.
A WCFG G computes a function gG : ?? ? R
defined as
gG(x) = ?
>
? ?G(x) (7)
where the inside function ?G : ?+ ? Rn is
?G(?) = ?? (8)
?G(x) =
?
x1,x2??+
x=x1x2
A(?G(x1)? ?G(x2)) (9)
We will define the outside function ?G : ?? ?
?? ? Rn as:
?G(?;?) = ?? (10)
?G(x; z)
> =
?
x1???,x2??+
x=x1x2
?G(x1; z)
>A(?G(x2)? In)
+
?
z1??+,z2???
z=z1z2
?G(x; z2)
>A(In ? ?G(z1)) (11)
For x, z ? ?? and y ? ?+ we have that
?G(x; z)
>?G(y) (12)
is the weight that the grammar assigns to the string
xyz with a cut around y. In particular, gG(x) =
?G(?;?)>?G(x).
626
Let us make clear that a WCFG is the same
device in classic or algebraic forms. If G? =
?V,R, T, w?, wT , wR? and G = ???, {??},A?, the
mapping is:
w?(i) = ??(i) (13)
wT (i? ?) = ??[i] (14)
wR(i? j k) = A[i, j, k] (15)
??G?(i
?
=? x) = ?G(x)[i] (16)
??G?(x; i; z) = ?G(x; z)[i] (17)
See Section A.1 for a proof of Eq. 16 and 17.
3 WCFG and Hankel Matrices
In this section we describe Hankel matrices for
WCFG. These matrices explicitly capture inside-
outside recursions employed by WCFG functions,
and are key to a derivation of a spectral learning al-
gorithm that learns a grammar G using statistics of
a training sample.
Let us define some sets. We say that I1 = ?+
is the set of inside strings. The set of composed in-
side strings I2 is the set of elements (x, x?), where
x, x? ? ?+. Intuitively (x, x?) represents two adja-
cent spans with an operation, i.e., it keeps the trace
of the operation that composes x with x? and yields
xx?. We will use the set I = I1 ? I2.
The set of outside contextsO is the set containing
elements ?x; z?, where x, z ? ??. Intuitively, ?x; z?
represents a context where we can insert an inside
element y in between x and z, yielding xyz.
Consider a function f : O ? I ? R. The Hankel
matrix of f is the bi-infinite matrix Hf ? RO?I
such thatHf (o, i) = f(o, i).
In practice we will work with finite sub-blocks of
Hf . To this end we will employ the notion of basis
B = (P,S), where {??, ??} ? P ? O is a set
of outside contexts and ? ? S ? I1 is a set of
inside strings. We will use p = |P| and s = |S|.
Furthermore, we define the inside completion of S
as the set S? = {(x, x?) | x, x? ? S}. Note that
S? ? I2. We say that B? = (P,S?) is the inside
completion of B.
The sub-block of Hf defined by B is the p ? s
matrix HB ? RP?S with HB(o, i) = Hf (o, i) =
f(o, i). In addition toHB, we are interested in these
additional finite vectors and matrices:
? h? ? RS is the s-dimensional vector with co-
ordinates h?(x) = f(??, ??, x).
? h? ? RP is the p-dimensional vector with co-
ordinates h?(o) = f(o, ?).
? HA ? RP?S
?
with HA(o, (x1, x2)) =
f(o, (x1, x2)).
3.1 Hankel Factorizations
If f is computed by a WCFG G, then Hf has rank
n factorization. To see this, consider the follow-
ing matrices. First a matrix S ? Rn?I
1
of inside
vectors for all strings, with column x taking value
Sx = ?G(x). Then a matrix P ? RO?n of out-
side vectors for all contexts, with row ?x; z? tak-
ing value P ?x;z? = ?G(x; z). It is easy to see that
Hf = PS, since Hf (?x; z?, y) = P ?x;z?Sy =
?G(x; z)>?G(y). ThereforeHf has rank n.
The same happens for sub-blocks. If HB is the
sub-block associated with basis B = (P,S), then
the sub-blocks P B ? RP?n and SB ? Rn?S of P
and S also accomplish that HB = P BSB . It also
happens that
h>? = ?
>
? SB (18)
h? = P B?? (19)
HA = P BA(SB ? SB) . (20)
We say that a basis B is complete for f if
rank(HB) = rank(Hf ). The following is a key
result for spectral methods.
Lemma 1. Let B = (P,S) be a complete basis of
dimension n for a function f and let HB ? RP?S
be the Hankel sub-block of f for B. Let h?, h? and
HA be the additional matrices for B. IfHB = PS
is a rank n factorization, then the WCFG G =
???, {??},A? with
?>? = h
>
? S
+ (21)
?? = P
+h? (22)
A = P+HA(S ? S)
+ (23)
computes f .
See proof in Section A.2.
627
3.2 Supervised Spectral Learning of WCFG
The spectral learning method directly exploits
Lemma 1. In a nutshell, the spectral method is:
1. Choose a complete basis B = (P,S) and a di-
mension n.
2. Use training data to compute estimates of the
necessary Hankel matrices: HB, h?, h?,HA.
3. Compute the SVD ofHB,HB = U?V >.
4. Create a truncated rank n factorization of HB
asP nSn, havingP n = Un?n andSn = V >n ,
where we only consider the top n singular val-
ues/vectors of ?,U ,V .
5. Use Lemma 1 to computeG, usingP n and Sn.
Because of Lemma 1, if B is complete and we
have access to the trueHB, h?, h?,HA of a WCFG
target function g?, then the algorithm will compute
a G that exactly computes g?. In practice, we only
have access to empirical estimates of the Hankel ma-
trices. In this case, there exist PAC-style sample
complexity bounds that state that gG will be a close
approximation to g? (Hsu et al, 2009; Bailly et al,
2009; Bailly et al, 2010).
The parameters of the algorithm are the basis and
the dimension of the grammar n. One typically em-
ploys some validation strategy using held-out data.
Empirically, the performance of these methods has
been shown to be good, and similar to that of EM
(Luque et al, 2012; Cohen et al, 2013). It is also
important to mention that in the case that the target
g? is a probability distribution, the function gG will
be close to g?, but it will only define a distribution in
the limit: in practice it will not sum to one, and for
some inputs it might return negative values. This is a
practical difficulty of spectral methods, for example
to apply evaluation metrics like perplexity which are
only defined for distributions.
4 Unsupervised Learning of WCFG
In the previous section we have exposed that if we
have access to estimates of a Hankel matrix of a
WCFG G, we can recover G. However, the statis-
tics in the Hankel require access to strings that have
information about context-free cuts. We will assume
that we only have access to statistics about plain
strings of a distribution, i.e. p(x), which we call
observations. In this scenario, one natural idea is
to search for a Hankel matrix that agrees with the
observations. The method we present in this sec-
tion frames this problem as a low-rank matrix op-
timization problem. We first characterize the space
of solutions to our problem, i.e. Hankel matrices
associated with WCFG that agree with observable
statistics. Then we present the method.
4.1 Characterization of a WCFG Hankel
In this section we describe valid WCFG Hankel ma-
trices using linear constraints.
We first describe an inside-outside basis that is
an extension of the one in the previous section. In-
side elements are the same, namely I = I1 ? I2,
where I1 are strings (x) and I2 are composed
strings (x, x?). The set of outside contexts O1 is
the set containing elements ?x; z?, defined as be-
fore. The set of composed outside contexts has el-
ements ?x, x?; z?, and ?x; z?, z?, where x, z ? ??
and x?, z? ? ?+. These outside contexts keep an
operation open in one of the sides. For example, if
we consider ?x; z?, z? and insert a string y, we obtain
x(y, z?)z, where we use (y, z?) to explicitly denote a
composed inside string. We will use O = O1 ? O2.
In this section, we will assume that I and O are
finite and closed. By closed, we mean that:
? (x) ? I ? (x1, x2) ? I for x = x1x2
? (x1, x2) ? I ? x1 ? I, x2 ? I
? ?x; z? ? O ? ?x1, x2; z? ? O for x = x1x2
? ?x; z? ? O ? ?x; z1, z2? ? O for z = z1z2
? ?x1, x2; z? ? O ? (x2) ? I
? ?x; z1, z2? ? O ? (z1) ? I
We will consider a Hankel matrix H ? RO?I .
Some entries of this matrix will correspond to ob-
servable quantities. Specifically, for any string x ?
I1 for which we know p(x) we can define the fol-
lowing observable constraint:
p(x) = H(??;??, (x)) (24)
The rest of entries of H correspond to a string
with an inside-outside cut, and these are not ob-
servable. Our method will infer the values of these
entries. The following constraints will ensure that
the matrix H is a well defined Hankel matrix for
WCFG:
628
? Hankel constraints: ? ?x; z? ? O, (y1, y2) ? I
H(?x; z?, (y1, y2)) = H(?x, y1; z?, (y2))
= H(?x; y2, z?, (y1)) (25)
? Inside constraints: ? o ? O, (x) ? I
H(o, (x)) =
?
x=x1x2
H(o, (x1, x2)) (26)
? Outside constraints: ? ?x; z? ? O, i ? I
H(?x; z?, i) =
?
x=x1x2
H(?x1, x2; z?, i)
+
?
z=z1z2
H(?x; z1, z2?, i) (27)
Constraint (25) states that composition operations
that result in the same structure should have the same
value. Constraints (26) and (27) ensure that the val-
ues in the Hankel follow the inside-outside recur-
sions that define the computations of a WCFG func-
tion. The following lemma formalizes this concept.
LetH? be the sub-block ofH restricted toO1?I1,
i.e. without compositions.
Lemma 2. If H satisfies constraints (25),(26) and
(27), and if rank(H) = rank(H?) then there exists
a WCFG that generatesH?.
See proof in Section A.3.
4.2 Convex Optimization
We now present the core optimization program be-
hind our method. Let vec(H) be a vector in R|O|?|I|
corresponding to all coefficients of H in column
vector form. Let O be a matrix such that O ?
vec(H) = z represents the observation constraints.
For example, if i-th row of O corresponds to the
Hankel coefficientH(??;??, (x)) then z(i) = p(x).
Let K be a matrix such that K ? vec(H) = 0 rep-
resents the constraints (25), (26) and (27).
The optimization problem is:
minimize
H
rank(H)
subject to ?O ? vec(H)? z?2 ? ?
K ? vec(H) = 0
?H?2 ? 1.
(28)
Intuitively, we look for H that agrees with the ob-
servable statistics and satisfies the inside-outside
constraints. ? is a parameter of the method that con-
trols the degree of error in fitting the observables z.
The ?H?2 ? 1 is satisfied by any Hankel matrix
derived from a true distribution, and is used to avoid
incoherent solutions.
The above optimization problem, however, is
computationally hard because of the rank objective.
We employ a common relaxation of the rank objec-
tive, based on the nuclear norm as in (Balle et al,
2012). The optimization is:
minimize
H
?H??
subject to ?O ? vec(H)? z?2 ? ?
K ? vec(H) = 0
?H?2 ? 1.
(29)
To optimize (29) we employ a projected gradient
strategy, similar to the FISTA scheme proposed by
Beck and Teboulle (2009). The method alternates
between separate projections for the observable con-
straints, the `2 norm, the inside-outside constraints,
and the nuclear norm. Of these, the latter two are the
most expensive.
Elsewhere, we develop theoretical properties of
the optimization (28) applied to finite-state transduc-
tions (Bailly et al, 2013). One can prove that there is
theoretical identifiability of the rank and the param-
eters of an FST distribution, using a rank minimiza-
tion formulation. However, this problem is NP-hard,
and it remains open whether there exists a polyno-
mial method with identifiability results. These re-
sults should generalize to WCFG.
5 Experiments
In this section we describe some experiments with
the learning algorithms for WCFG. Our goal is
to verify that the algorithms can learn some basic
context-free languages, and to study the possibility
of using them on real data.
5.1 Synthetic Experiments
We performed experiments on synthetic data, ob-
tained by choosing a PCFG with random parameters
(? [0, 1]), with a normalization step in order to get
a probability distribution. We built the Hankel ma-
trix from the inside basis {(x)}x?? and outside basis
629
 1e-06
 1e-05
 0.0001
 0.001
 0.01
 0.1
 1
 100  1000  10000  100000  1e+06
KL
 div
erg
enc
e
Sample size
Unsupervised SpectralSupervised SpectralUnsupervised EMSupervised EM
Figure 1: KL divergence for spectral and EM methods,
unsupervised and supervised, for different sizes of learn-
ing sample, on log-log scales. Results are averages over
50 random target PCFG with 2 states and 2 symbols.
{??;??} ? {?x;??, ??;x?}x??. The composed in-
sides for the operator matrix are thus {(x, y)}x,y??.
The matrix in the optimizer has the following struc-
ture
H =
?
?
?
?
?
(y) ? ? ? (y, z)
??;?? (?; y;?) ? ? ? (?; y, z;?)
?x;?? (x; y;?) ? ? ? (x; y, z;?)
??;x? (?; y;x) ? ? ? (?; y, z;x)
... ? ? ? ? ? ? ? ? ?
?
?
?
?
?
The constraints we use are:
K ={H((x; y;?)) = H((?;x; y))}x,y???
{H((?;x; y)) = H((?;x, y;?))}x,y???
{H((x; y;?)) = H((?;x, y;?))}x,y??
and
O ={H((?;x;?)) = pS(x)}x?? ?
{H((?;x; y)) = pS(xy)}x,y?? ?
{H((x; y, z;?)) +H((?;x, y; z)) = pS(xyz)}x,y,z??
We use pS to denote the empirical distribution.
Those are simplified versions of the Hankel, inside,
outside and observation constraints. The set O is
built from the following remarks: (1) (xy) = (x, y)
and (2) (xyz) = (xy, z)+(x, yz). The method uses
statistics for sequences up to length 3.
The algorithm we use for the unsupervised spec-
tral method is a simplified version: we use alter-
natively a hard projection on the constraints (by
 1e-08
 1e-07
 1e-06
 1e-05
 0.0001
 0.001
 0.01
 0.1
 1000  10000  100000  1e+06  1e+07  1e+08  1e+09
KL
 div
erg
enc
e
Sample size
Unsupervised SpectralSupervised Spectral
Figure 2: KL divergence for unsupervised and supervised
spectral methods, for different sizes of learning sample,
on log-log scales. Results are averages over 50 random
target PCFG with 3 states and 6 symbols.
projecting iteratively on each constraint), and a
thresholding-shrinkage operation for the target di-
mension. We use the same trick as FISTA for the
update. We finally use the regular spectral method
on this matrix to get our model.
We compare this method with an unsupervised
EM, and also with supervised versions of spectral
method and EM. We compare the accuracy of the
different models in terms of KL-divergence for se-
quences up to length 10. We run 50 optimization
steps for the unsupervised spectral method, and 200
iterations for the EM methods. Figure 1 shows the
results, corresponding the the geometric mean over
50 experiments on random targets of 2 symbols and
2 states.
For sample size greater than 105, the unsupervised
spectral method seems to provide better solutions
than both EM and supervised EM. The solution, in
terms of KL-divergence, is comparable to the one
obtained with the supervised spectral method. The
computation time of unsupervised spectral method
is almost constant w.r.t. the sample size, around
1.67s, while computation time of unsupervised EM
(resp. supervised EM) is 6.103s (resp. 2.104s) for
sample size 106.
Figure 2 presents learnings curve for random tar-
gets with 3 states and 6 symbols. One can see that,
for big sample sizes (109), the unsupervised spectral
method is losing accuracy compared to the super-
vised method. This is due to a lack of information,
630
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 0  0.01  0.02  0.03  0.04  0.05
L1
 dis
tan
ce
Basis factor
Spectral WFAUnsupervised SpectralSupervised Spectral
Figure 3: Learning errors for different models in terms of
the size of the basis.
and could be overcome by considering a greater ba-
sis (e.g. inside sequences up to length 2 or 3).
5.2 Dyck Languages
We now present experiments using the following
PCFG:
S ? S S (0.2) | aS b (0.4) | a b (0.4)
This PCFG generates a probabilistic version of the
well-known Dyck language or balanced parenthesis
language, an archetypical context-free language.
We do experiments with the following models and
algorithms:
? WFA: a Weighted Finite Automata learned us-
ing spectral methods as described in (Luque et
al., 2012). Parameters: number of states and
size of basis.
? Supervised Spectral: a WCFG learned from
structured strings using the algorithm of sec-
tion 3.2. We choose as basis the most frequent
insides and outsides observed in the training
data. The size of the basis is determined by a
parameter f called the basis factor, that deter-
mines the proportion of total insides and out-
sides that will be in the basis.
? Unsupervised Spectral: a WCFG learned from
strings using the algorithm of Section 4. The
basis is like in the supervised case, but since
context-free cuts in the strings are not observed,
basis size ofH obs. i/o ctr.
1 ? 11 39 ? 159 34 162
6 ? 14 1,163 ? 764 146 6,360
12 ? 18 4,462 ? 2,239 322 25,374
18 ? 22 9,124 ? 4,149 479 52,524
24 ? 26 15,755 ? 6,858 657 89,718
30 ? 29 19,801 ? 8,545 769 112,374
36 ? 34 27,989 ? 11,682 916 156,690
42 ? 37 3,638 ? 15,026 1,035 200,346
48 ? 41 45,192 ? 18,235 1,157 244,398
54 ? 45 53,741 ? 21,196 1,281 284,466
60 ? 48 60,844 ? 23,890 1,382 318,354
Table 1: Problem sizes for the WSJ10 training corpus.
basis / n 5 10 15 20
1 ? 11 1.265 10?3
6 ? 14 7.06 10?4 6.92 10?4
12 ? 18 7.30 10?4 6.28 10?4 6.01 10?4
18 ? 22 7.31 10?4 6.29 10?4 5.84 10?4 5.59 10?4
24 ? 26 7.35 10?4 6.39 10?4 5.88 10?4 5.31 10?4
30 ? 29 7.34 10?4 6.41 10?4 5.86 10?4 5.30 10?4
Table 2: Experiments with the unsupervised spectral
method on the WSJ10 corpus. Results are in terms of
expected L1 on the training set, for different basis and
numbers of states.
all possible inside and outsides of the sample
(i.e. all possible substrings and contexts) are
considered.
We generate a training set by sampling 4,000
strings from the target PCFG and counting the rel-
ative frequency of each. For the supervised model,
we generate strings paired with their context-free
derivation. To measure the quality of the learned
models, we use the L1 distance to the target distri-
bution over a fixed set of strings ??n, for n = 7.1
Figure 3 shows the results for the different mod-
els and for different basis sizes (in terms of the basis
factor f ). Here we can clearly see that the WCFG
models, even the unsupervised one, outperform the
WFA in reproducing the target distribution.
5.3 Natural Language Experiments
Now we present some preliminar tests using natural
language data. For these tests, we used the WSJ10
subset of the Penn Treebank, as Klein and Manning
(2002). This dataset consists of the sentences of
length ? 10 after filtering punctuation and currency.
We removed lexical items and mapped the POS tags
1Given two functions f1 and f2 over strings, the L1 distance
is the sum of the absolute difference over all strings in a set:
?
x |f1(x)? f2(x)|.
631
to the Universal Part-of-Speech Tagset (Petrov et al,
2012), reducing the alphabet to a set of 11 symbols.
Table 1 shows the size of the problem for differ-
ent basis sizes. As described in the previous sub-
section for the unsupervised case, we obtain the ba-
sis by taking the most frequent observed substrings
and contexts. We then compute all yields that can
be generated with this basis, and close the basis to
include all possible insides and outsides with oper-
ations completions, such that we create a Hankel as
described in Section 4.1. Table 1 shows, for each
base, the size of H we induce, the number of ob-
servable constraints (i.e. sentences we train from),
and the number of inside-outside constraints.
With the current implementation of the optimizer
we were only able to run the unsupervised learning
for small basis sizes. Table 2 shows the expected L1
on training data. For a fixed basis, as we increase
the number of states we see that the error decreases,
showing that the method is inducing a Hankel matrix
that explains the observable statistics.
6 Conclusions
We have presented a novel approach for unsuper-
vised learning of WCFG. Our method combines in-
gredients of spectral learning with low-rank convex
optimization methods.
Our method optimizes over a matrix that, even if it
grows polynomially with respect to the size of train-
ing, results in a large problem. To scale the method
to learn languages of the complexity of natural lan-
guages we would need to identify optimization algo-
rithms specially suited for this problem.
A Proofs
A.1 Proof of Inside-Outside Eq. 16 and 17
For the inside function, the base case is trivial. By
induction:
?G(x)[i] =
?
x=x1x2
A(?G(x1)? ?G(x2))[i]
=
?
j,k?V
x=x1x2
A[i, j, k] ? ?G(x1)[j] ? ?G(x2)[k]
=
?
j,k?V
x=x1x2
wR(i? j k) ? ??G?(j
?
=? x1) ? ??G?(k
?
=? x2)
= ??G?(i
?
=? x)
For the outside function, let ei be an n-
dimensional vector with coordinate i to 1 and the
rest to 0. We reformulate the mapping as:
?G(x; z)
>ei = ??G?(x; i; z) (30)
The base case is trivial by definitions. We use the
property of Kronecker products that (v ? In)v? =
(v? v?) and (In? v)v? = (v?? v) for v,v? ? Rn.
We first look at one of the terms of ?G(x; z)>ei:
?G(x1; z)
>A(?G(x2)? In)ei
= ?G(x1; z)
>A(?G(x2)? ei)
=
?
j,k?V
(?G(x1; z)
>ej) ?A[j, k, i] ? ?G(x2)[k]
=
?
j,k?V
??G?(x1; j; z) ? wR(j ? k i) ? ??G?(k
?
=? x2)
Applying the distributive property in ?G(x; z)>ei it
is easy to see that all terms are mapped to the corre-
sponding term in ??G?(x; i; z).
A.2 Proof of Lemma 1
Let G? = ????, {?
?
?},A
?? be a WCFG for f that in-
duces a rank factorizationH = P ?S?. We first show
that there exists an invertible matrixM that changes
the basis of the operators of G into those of G?.
Define M = S?S+ and note that P+P ?S?S+ =
P+HS+ = I implies that M is invertible with
M?1 = P+P ?. We now check that the operators
of G correspond to the operators of G? under this
change of basis. First we see that
A = P+HA(S ? S)
+
= P+P ?A?(S? ? S?)(S ? S)+
= M?1A?(S?S+ ? S?S+)
= M?1A?(M ?M) .
Now, since h? = ??>? S
? and h? = P ???? , it follows
that ?>? = ?
?
?
>M and ?? = M
?1???.
Finally we check that G and G? compute the
same function, namely f(o, i) = ?G(o)>?G(i) =
?G?(o)>?G?(i). We first see that ?G(x) =
632
M?1?G?(x):
?G(?) = ?? = M
?1??? (31)
?G(x) =
?
x=x1x2
A(?G(x1)? ?G(x2)) (32)
=
?
x=x1x2
M?1A?(M ?M)(?G(x1)? ?G(x2))
= M?1
?
x=x1x2
A?(M?G(x1)?M?G(x2))
= M?1
?
x=x1x2
A?(?G?(x1)? ?G?(x2))
It can also be shown that ?G(x; z)> =
?G?(x; z)>M . One must see that in any term:
?G(x1; z)
>A(?G(x2)? In) (33)
= ?G(x1; z)
>M?1A?(M ?M)(?G(x2)? In)
= ?G?(x1; z)
>A?(M?G(x2)?MIn)
= ?G?(x1; z)
>A?(?G?(x2)? In)M
and the relation follows. Finally:
?G(x; z)
>?G(y) (34)
= ?G?(x; z)
>MM?1?G?(y)
= ?G?(x; z)
>?G?(y)
A.3 Proof of Lemma 2
We will use the following sub-blocks ofH:
? H? is the sub-block restricted to O1 ? I1, i.e.
without compositions.
? HA is the sub-block restricted to O1 ? I2, i.e.
inside compositions.
? H ?A is the sub-block restricted to O
2 ? I1, i.e.
outside compositions.
? h>? ? R
I1 is the row ofH? for ??;??.
? h(x) ? RO
1
is the column ofH? for (x).
? h(x1,x2) ? R
O1 is the column of HA for
(x1, x2).
? h??x;z? ? R
I1 is the row of h? for ?x; z?.
? h??x1,x2;z? and h
?
?x;z1,z2? be the rows in R
I1 of
h?A for ?x1, x2; z? and ?x; z1, z2?).
One supposes that rank(H?) = rank(H). We de-
fine G as
?>? = h
>
?H
+
? , ?a = h(a),A = HA(H
+
? ?H
+
? )
Lemma 3. One has that ?G(x) = h(x), and
?G(x1, x2) = h(x1,x2).
Proof. By induction. For sequences of size 1, one
has ?G(x) = ?x = h(x). For the recursive case,
let e(x) be a vector in RI
1
with 1 in the coordinate
of (x) in H?. Let e(x,y) be a vector in RI
2
with 1
in the coordinate of (x, y) in HA. For ?G(x, y),
one has H+? ?G(x) = e(x), and H
+
? ?G(y) =
e(y), thus H
+
? ?G(x) ? H
+
? ?G(y) = e(x,y) and
HA(H+? ?G(x) ? H
+
? ?G(y)) = h(x,y). Finally,
one has that ?G(x) =
?
x=x1x2 ?G(x1, x2) =?
x=x1x2 h(x1,x2) = h(x1x2x3) by the equation
(26).
One has a symmetric result for outside vectors. We
define G? as
?>? = h
>
? , ?a = H
+
? h(a),A = H
+
?HA
Lemma 4. One has that ?G?(?x; z?)> =
h??x;z?, ?G?(?x1, x2; z?)
> = h??x1,x2;z? and
?G?(?x; z1, z2?)> = h
?
?x;z1,z2?.
Proof. (Sketch) Equation (31) is used in the same
way than (27) before. Equation (25) is used to en-
sure a link betweenH ?A andHA.
Let g be the mapping computed by G and
G?. One has that g(o, i) = ?G?(o)>?G?(i) =
?G(o)>?G(i) = ?G?(o)>H+? ?G(i) = H?(o, i).
Acknowledgements We are grateful to Borja Balle
and the anonymous reviewers for providing us with help-
ful comments. This work was supported by a Google
Research Award, and by projects XLike (FP7-288342),
ERA-Net CHISTERA VISEN, TACARDI (TIN2012-
38523-C02-02), BASMATI (TIN2011-27479-C04-03),
SGR-GPLN (2009-SGR-1082) and SGR-LARCA (2009-
SGR-1428). Xavier Carreras was supported by the
Ramo?n y Cajal program of the Spanish Government
(RYC-2008-02223). Franco M. Luque was supported by
the National University of Co?rdoba and by a Postdoc-
toral fellowship of CONICET, Argentinian Ministry of
Science, Technology and Productive Innovation.
633
References
Pieter Adriaans, Marten Trautwein, and Marco Vervoort.
2000. Towards high speed grammar induction on large
text corpora. In SOFSEM 2000: Theory and Practice
of Informatics, pages 173?186. Springer.
Raphae?l Bailly, Franois Denis, and Liva Ralaivola. 2009.
Grammatical inference as a principal component anal-
ysis problem. In Le?on Bottou and Michael Littman,
editors, Proceedings of the 26th International Confer-
ence on Machine Learning, pages 33?40, Montreal,
June. Omnipress.
Raphae?l Bailly, Amaury Habrard, and Franc?ois Denis.
2010. A spectral approach for probabilistic grammat-
ical inference on trees. In Proceedings of the 21st
International Conference Algorithmic Learning The-
ory, Lecture Notes in Computer Science, pages 74?88.
Springer.
Raphae?l Bailly, Xavier Carreras, and Ariadna Quattoni.
2013. Unsupervised spectral learning of finite-state
transducers. In Advances in Neural Information Pro-
cessing Systems 26.
James K. Baker. 1979. Trainable grammars for speech
recognition. In D. H. Klatt and J. J. Wolf, editors,
Speech Communication Papers for the 97th Meeting
of the Acoustical Society of America, pages 547?550.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2011. A spectral learning algorithm for finite state
transducers. In Proceedings of ECML PKDD, pages
156?171.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2012. Local loss optimization in operator models: A
new insight into spectral learning. In John Langford
and Joelle Pineau, editors, Proceedings of the 29th In-
ternational Conference on Machine Learning (ICML-
2012), ICML ?12, pages 1879?1886, New York, NY,
USA, July. Omnipress.
Amir Beck and Marc Teboulle. 2009. A fast iter-
ative shrinkage-thresholding algorithm for linear in-
verse problems. SIAM J. Img. Sci., 2(1):183?202,
March.
Stanley F Chen. 1995. Bayesian grammar induction for
language modeling. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 228?235. Association for Computational Lin-
guistics.
Alexander Clark. 2001. Unsupervised induction of
stochastic context-free grammars using distributional
clustering. In Proceedings of the 2001 workshop on
Computational Natural Language Learning-Volume 7,
page 13. Association for Computational Linguistics.
Alexander Clark. 2007. Learning deterministic context
free grammars: The omphalos competition. Machine
Learning, 66(1):93?110.
Shay B. Cohen, David M. Blei, and Noah A. Smith.
2010. Variational inference for adaptor grammars.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 564?
572, Los Angeles, California, June. Association for
Computational Linguistics.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2012. Spectral learning of
latent-variable pcfgs. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 223?231,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with spec-
tral learning of latent-variable pcfgs. In Proceedings
of the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 148?157, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Matthew Gormley and Jason Eisner. 2013. Nonconvex
global optimization for latent-variable models. In Pro-
ceedings of the 51st Annual Meeting of the Association
for Computational Linguistics (ACL), Sofia, Bulgaria,
August. 11 pages.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009. A
spectral algorithm for learning hidden markov models.
In Proceedings of the Annual Conference on Compu-
tational Learning Theory (COLT).
Daniel Hsu, Sham Kakade, and Percy Liang. 2012.
Identifiability and unmixing of latent parse trees. In
P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou,
and K.Q. Weinberger, editors, Advances in Neural In-
formation Processing Systems 25, pages 1520?1528.
Dan Klein and Christopher D Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
128?135. Association for Computational Linguistics.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language. In
Grammatical Inference: Algorithms and Applications,
pages 84?96. Springer.
Karim Lari and Steve J Young. 1990. The estimation
of stochastic context-free grammars using the inside-
outside algorithm. Computer speech & language,
4(1):35?56.
Percy Liang, Slav Petrov, Michael I Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
dirichlet processes. In EMNLP-CoNLL, pages 688?
697.
634
Franco M. Luque, Ariadna Quattoni, Borja Balle, and
Xavier Carreras. 2012. Spectral learning for non-
deterministic dependency parsing. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics, pages
409?419, Avignon, France, April. Association for
Computational Linguistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the As-
sociation for Computational Linguistics, pages 128?
135, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC, May.
Kewei Tu and Vasant Honavar. 2008. Unsupervised
learning of probabilistic context-free grammar using
iterative biclustering. In Grammatical Inference: Al-
gorithms and Applications, pages 224?237. Springer.
Menno Van Zaanen. 2000. Abl: Alignment-based learn-
ing. In Proceedings of the 18th conference on Compu-
tational linguistics-Volume 2, pages 961?967. Associ-
ation for Computational Linguistics.
635
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 430?435,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Shortest-path Method for Arc-factored Semantic Role Labeling
Xavier Llu??s
TALP Research Center
Universitat Polit`ecnica de
Catalunya
xlluis@cs.upc.edu
Xavier Carreras
Xerox Research Centre
Europe
xavier.carreras@xrce.xerox.com
Llu??s M
`
arquez
ALT Research Group
Qatar Computing Research
Institute
lmarquez@qf.org.qa
Abstract
We introduce a Semantic Role Labeling
(SRL) parser that finds semantic roles for a
predicate together with the syntactic paths
linking predicates and arguments. Our
main contribution is to formulate SRL in
terms of shortest-path inference, on the as-
sumption that the SRL model is restricted
to arc-factored features of the syntactic
paths behind semantic roles. Overall, our
method for SRL is a novel way to ex-
ploit larger variability in the syntactic re-
alizations of predicate-argument relations,
moving away from pipeline architectures.
Experiments show that our approach im-
proves the robustness of the predictions,
producing arc-factored models that per-
form closely to methods using unrestricted
features from the syntax.
1 Introduction
Semantic role labeling (SRL) consists of finding
the arguments of a predicate and labeling them
with semantic roles (Gildea and Jurafsky, 2002;
M`arquez et al., 2008). The arguments fill roles that
answer questions of the type ?who? did ?what? to
?whom?, ?how?, and ?why? for a given sentence
predicate. Most approaches to SRL are based on
a pipeline strategy, first parsing the sentence to
obtain a syntactic tree and then identifying and
classifying arguments (Gildea and Jurafsky, 2002;
Carreras and M`arquez, 2005).
SRL methods critically depend on features of
the syntactic structure, and consequently parsing
mistakes can harm the quality of semantic role
predictions (Gildea and Palmer, 2002). To allevi-
ate this dependence, previous work has explored
k-best parsers (Johansson and Nugues, 2008),
combination systems (Surdeanu et al., 2007) or
joint syntactic-semantic models (Johansson, 2009;
Henderson et al., 2008; Llu??s et al., 2013).
In this paper we take a different approach. In
our scenario SRL is the end goal, and we as-
sume that syntactic parsing is only an intermedi-
ate step to extract features to support SRL predic-
tions. In this setting we define a model that, given
a predicate, identifies each of the semantic roles
together with the syntactic path that links the pred-
icate with the argument. Thus, following previous
work (Moschitti, 2004; Johansson, 2009), we take
the syntactic path as the main source of syntac-
tic features, but instead of just conditioning on it,
we predict it together with the semantic role. The
main contribution of this paper is a formulation of
SRL parsing in terms of efficient shortest-path in-
ference, under the assumption that the SRL model
is restricted to arc-factored features of the syntac-
tic path linking the argument with the predicate.
Our assumption ?that features of an SRL
model should factor over dependency arcs? is
supported by some empirical frequencies. Table 1
shows the most frequent path patterns on CoNLL-
2009 (Haji?c et al., 2009) data for several lan-
guages, where a path pattern is a sequence of as-
cending arcs from the predicate to some ancestor,
followed by descending arcs to the argument. For
English the distribution of path patterns is rather
simple: the majority of paths consists of a num-
ber of ascending arcs followed by zero or one de-
scending arc. Thus a common strategy in SRL sys-
tems, formulated by Xue and Palmer (2004), is to
look for arguments in the ancestors of the pred-
icate and their direct descendants. However, in
Czech and Japanese data we observe a large por-
tion of paths with two or more descending arcs,
which makes it difficult to characterize the syn-
tactic scope in which arguments are found. Also,
in the datasets for German, Czech and Chinese the
three most frequent patterns cover over the 90% of
all arguments. In contrast, Japanese exhibits much
more variability and a long tail of infrequent types
430
English German Czech Chinese Japanese
?
% % path
?
% % path
?
% % path
?
% % path
?
% % path
63.63 63.6298 ? 77.22 77.2202 ? 63.90 63.8956 ? 78.09 78.0949 ? 37.20 37.1977 ??
73.97 10.3429 ?? 93.51 16.2854 ?? 86.26 22.3613 ?? 85.36 7.26962 ?? 51.52 14.3230 ?
80.63 6.65915 ? 97.43 3.92111 ??? 90.24 3.98078 ?? 91.27 5.90333 ??? 60.79 9.27270 ???
85.97 5.33352 ? 98.19 0.76147 ?? 93.95 3.71713 ??? 95.93 4.66039 ?? 70.03 9.23857 ?
90.78 4.81104 ??? 98.70 0.51640 ???? 95.48 1.52168 ??? 97.53 1.60392 ? 74.17 4.13359 ????
93.10 2.31928 ???? 99.17 0.46096 ? 96.92 1.44091 ? 98.28 0.75086 ???? 76.76 2.59117 ??
95.19 2.09043 ?? 99.43 0.26841 ??? 97.68 0.76714 ??? 98.77 0.48734 ?? 78.82 2.06111 ????
96.26 1.07468 ????? 99.56 0.12837 ???? 98.28 0.59684 ???? 99.13 0.36270 ??? 80.85 2.03381 ?????
97.19 0.92482 ?? 99.67 0.10503 ????? 98.60 0.31759 ???? 99.45 0.31699 ????? 82.66 1.80631 ???
97.93 0.74041 ??? 99.77 0.10503 ?? 98.88 0.28227 ???? 99.72 0.27041 ???? 83.71 1.05558 ???
98.41 0.48565 ?????? 99.82 0.04960 ??? 99.15 0.26721 ???? 99.82 0.10049 ??? 84.74 1.02828 ?????
98.71 0.29769 ???? 99.87 0.04960 ??? 99.27 0.12430 ????? 99.86 0.03623 ??? 85.68 0.93500 ?????
98.94 0.22733 ??????? 99.90 0.02626 ? 99.37 0.10103 ????? 99.89 0.02890 ???? 86.61 0.93273 ??????
99.11 0.17805 ??? 99.92 0.02042 ????? 99.47 0.09747 ?? 99.92 0.02890 ?????? 87.29 0.68249 ??????
99.27 0.15316 ??? 99.94 0.02042 ?????? 99.56 0.08515 ????? 99.94 0.02846 ? 87.90 0.60969 ????
99.39 0.12065 ????? 99.95 0.01459 ????? 99.63 0.07419 ????? 99.96 0.02070 ????? 88.47 0.56646 ??????
99.50 0.11024 ???? 99.96 0.01167 ???? 99.69 0.05667 ????? 99.97 0.00992 ????? 89.01 0.53689 ???????
99.60 0.09931 ???????? 99.97 0.00875 ???? 99.73 0.04216 ?????? 99.98 0.00733 ??????? 89.49 0.48684 ??????
99.65 0.05283 ???? 99.98 0.00875 ??????? 99.76 0.02875 ?????? 99.99 0.00431 ?????? 89.94 0.45044 ????
Table 1: Summary of the most frequent paths on the CoNLL-2009 Shared Task datasets. ? indicates that we traverse a syntactic
dependency upwards from a modifier to a head. ? is for dependencies following a descending head to modifier edge. The
symbol ? represents that the argument is the predicate itself. We exclude from this table Catalan and Spanish as predicates and
arguments are always trivially related by a single syntactic dependency that descends.
of patterns. In general it is not feasible to capture
path patterns manually, and it is not desirable that
a statistical system depends on rather sparse non-
factored path features. For this reason in this paper
we explore arc-factored models for SRL.
Our method might be specially useful in appli-
cations were we are interested in some target se-
mantic role, i.e. retrieving agent relations for some
verb, since it processes semantic roles indepen-
dently of each other. Our method might also be
generalizable to other kinds of semantic relations
which strongly depend on syntactic patterns such
as relation extraction in information extraction or
discourse parsing.
2 Arc-factored SRL
We define an SRL parsing model that re-
trieves predicate-argument relations based on arc-
factored syntactic representations of paths con-
necting predicates with their arguments. Through-
out the paper we assume a fixed sentence x =
x
1
, . . . , x
n
and a fixed predicate index p. The
SRL output is an indicator vector z, where
z
r,a
= 1 indicates that token a is filling role
r for predicate p. Our SRL parser performs
argmax
z?Z(x,p)
s(x, p, z), where Z(x, p) defines
the set of valid argument structures for p, and
s(x, p, z) computes a plausibility score for z given
x and p. Our first assumption is that the score
function factors over role-argument pairs:
s(x, p, z) =
?
z
r,a
=1
s(x, p, r, a) . (1)
Then we assume two components in the model,
one that scores the role-argument pair alone, and
another that considers the best (max) syntactic de-
pendency pathpi that connects the predicate pwith
the argument a:
s(x, p, r, a) = s
0
(x, p, r, a) +
max
pi
s
syn
(x, p, r, a,pi) . (2)
The model does not assume access to the syntac-
tic structure of x, hence in Eq. (2) we locally re-
trieve the maximum-scoring path for an argument-
role pair. A path pi is a sequence of dependencies
?h,m, l? where h is the head, m the modifier and l
the syntactic label. We further assume that the syn-
tactic component factors over the dependencies in
the path:
s
syn
(x, p, r, a,pi)=
?
?h,m,l??pi
s
syn
(x, p, r, a, ?h,m, l?) .
(3)
This will allow to employ efficient shortest-path
inference, which is the main contribution of this
paper and is described in the next section. Note
that since paths are locally retrieved per role-
argument pair, there is no guarantee that the set
of paths across roles forms a (sub)tree.
As a final note, in this paper we follow Llu??s
et al. (2013) and consider a constrained space of
valid argument structures Z(x, p): (a) each role is
realized at most once, and (b) each token fills at
most one role. As shown by Llu??s et al. (2013),
this can be efficiently solved as a linear assign-
431
Figure 1: Graph representing all possible syntactic paths
from a single predicate to their arguments. We find in this
graph the best SRL using a shortest-path algorithm. Note that
many edges are omitted for clarity reasons. We labeled the
nodes and arcs as follows: p is the predicate and source ver-
tex; u
1
, . . . , u
n
are tokens reachable by an ascending path;
v
1
, . . . , v
n
are tokens reachable by a ascending path (possi-
bly empty) followed by a descending path (possibly empty);
a
i?j
is an edge related to an ascending dependency from
node u
i
to node u
j
; d
i?j
is a descending dependency from
node v
i
to node v
j
; 0
i?i
is a 0-weighted arc that connects the
ascending portion of the path ending at u
i
with the descend-
ing portion of the path starting at v
i
.
ment problem as long as the SRL model factors
over role-argument pairs, as in Eq. (1).
3 SRL as a Shortest-path Problem
We now focus on solving the maximization over
syntactic paths in Eq. (2). We will turn it into a
minimization problem which can be solved with a
polynomial-cost algorithm, in our case a shortest-
path method. Assume a fixed argument and role,
and define ?
?h,m,l?
to be a non-negative penalty for
the syntactic dependency ?h,m, l? to appear in the
predicate-argument path. We describe a shortest-
path method that finds the path of arcs with the
smaller penalty:
min
pi
?
?h,m,l??pi
?
?h,m,l?
. (4)
We find these paths by appropriately constructing
a weighted graph G = (V,E) that represents the
problem. Later we show how to adapt the arc-
factored model scores to be non-negative penal-
ties, such that the solution to Eq. (4) will be the
negative of the maximizer of Eq. (2).
It remains only to define the graph construc-
tion where paths correspond to arc-factored edges
weighted by ? penalties. We start by noting that
any path from a predicate p to an argument v
i
is
formed by a number of ascending syntactic arcs
followed by a number of descending arcs. The as-
cending segment connects p to some ancestor q (q
might be p itself, which implies an empty ascend-
ing segment); the descending segment connects q
with v
i
(which again might be empty). To com-
pactly represent all these possible paths we define
the graph as follows (see Figure 1):
1. Add node p as the source node of the graph.
2. Add nodes u
1
, . . . , u
n
for every token of the
sentence except p.
3. Link every pair of these nodes u
i
, u
j
with a
directed edge a
i?j
weighted by the corre-
sponding ascending arc, namely min
l
?
?j,i,l?
.
Also add ascending edges from p to any u
i
weighted by min
l
?
?i,p,l?
. So far we have
a connected component representing all as-
cending path segments.
4. Add nodes v
1
, . . . , v
n
for every token of the
sentence except p, and add edges d
i?j
be-
tween them weighted by descending arcs,
namely min
l
?
?i,j,l?
. This adds a second
strongly-connected component representing
descending path segments.
5. For each i, add an edge from u
i
to v
i
with
weight 0. This ensures that ascending and
descending path segments are connected con-
sistently.
6. Add direct descending edges from p to all the
v
i
nodes to allow for only-descending paths,
weighted by min
l
?
?p,i,l?
.
Dijkstra?s algorithm (Dijkstra, 1959) will find
the optimal path from predicate p to all tokens in
time O(V
2
) (see Cormen et al. (2009) for an in-
depth description). Thus, our method runs this
algorithm for each possible role of the predicate,
obtaining the best paths to all arguments at each
run.
4 Adapting and Training Model Scores
The shortest-path problem is undefined if a nega-
tive cycle is found in the graph as we may indefi-
nitely decrease the cost of a path by looping over
this cycle. Furthermore, Dijkstra?s algorithm re-
quires all arc scores to be non-negative penalties.
However, the model in Eq. (3) computes plausibil-
ity scores for dependencies, not penalties. And, if
we set this model to be a standard feature-based
linear predictor, it will predict unrestricted real-
valued scores.
One approach to map plausibility scores to
penalties is to assume a log-linear form for our
432
model. Let us denote by x? the tuple ?x, p, r, a?,
which we assume fixed in this section. The log-
linear model predicts:
Pr(?h,m, l? | x?) =
exp{w ? f(x?, ?h,m, l?)}
Z(x?)
,
(5)
where f(x?, ?h,m, l?) is a feature vector for an
arc in the path, w are the parameters, and Z(x?)
is the normalizer. We can turn predictions into
non-negative penalties by setting ?
?h,m,l?
to be
the negative log-probability of ?h,m, l?; namely
?
?h,m,l?
= ?w ? f(x?, ?h,m, l?) + logZ(x?). Note
that logZ(x?) shifts all values to the non-negative
side.
However, log-linear estimation of w is typically
expensive since it requires to repeatedly com-
pute feature expectations. Furthermore, our model
as defined in Eq. (2) combines arc-factored path
scores with path-independent scores, and it is de-
sirable to train these two components jointly. We
opt for a mistake-driven training strategy based
on the Structured Averaged Perceptron (Collins,
2002), which directly employs shortest-path infer-
ence as part of the training process.
To do so we predict plausibility scores for a de-
pendency directly as w ? f(x?, ?h,m, l?). To map
scores to penalties, we define
?
0
= max
?h,m,l?
w ? f(x?, ?h,m, l?)
and we set
?
?h,m,l?
= ?w ? f(x?, ?h,m, l?) + ?
0
.
Thus, ?
0
has a similar purpose as the log-
normalizer Z(x?) in a log-linear model, i.e., it
shifts the negated scores to the positive side; but
in our version the normalizer is based on the max
value, not the sum of exponentiated predictions as
in log-linear models. If we set our model function
to be
s
syn
(x?, ?h,m, l?) = w ? f(x?, ?h,m, l?)? ?
0
then the shortest-path method is exact.
5 Experiments
We present experiments using the CoNLL-2009
Shared Task datasets (Haji?c et al., 2009), for the
verbal predicates of English. Evaluation is based
on precision, recall and F
1
over correct predicate-
argument relations
1
. Our system uses the fea-
ture set of the state-of-the-art system by Johansson
(2009), but ignoring the features that do not factor
over single arcs in the path.
The focus of these experiments is to see the per-
formance of the shortest-path method with respect
to the syntactic variability. Rather than running
the method with the full set of possible depen-
dency arcs in a sentence, i.e. O(n
2
), we only con-
sider a fraction of the most likely dependencies.
To do so employ a probabilistic dependency-based
model, following Koo et al. (2007), that computes
the distribution over head-label pairs for a given
modifier, Pr(h, l | x,m). Specifically, for each
modifier token we only consider the dependencies
or heads whose probability is above a factor ? of
the most likely dependency for the given modi-
fier. Thus, ? = 1 selects only the most likely de-
pendency (similar to a pipeline system, but with-
out enforcing tree constraints), and as ? decreases
more dependencies are considered, to the point
where ? = 0 would select all possible dependen-
cies. Table 2 shows the ratio of dependencies in-
cluded with respect to a pipeline system for the de-
velopment set. As an example, if we set ? = 0.5,
for a given modifier we consider the most likely
dependency and also the dependencies with proba-
bility larger than 1/2 of the probability of the most
likely one. In this case the total number of depen-
dencies is 10.3% larger than only considering the
most likely one.
Table 3 shows results of the method on develop-
ment data, when training and testing with different
? values. The general trend is that testing with the
most restricted syntactic graph results in the best
performance. However, we observe that as we al-
low for more syntactic variability during training,
the results largely improve. Setting ? = 1 for both
training and testing gives a semantic F
1
of 75.9.
This configuration is similar to a pipeline approach
but considering only factored features. If we allow
to train with ? = 0.1 and we test with ? = 1 the
results improve by 1.96 points to a semantic F
1
of 77.8 points. When syntactic variability is too
large, e.g., ? = 0.01, no improvements are ob-
served.
Finally, table 4 shows results on the verbal En-
glish WSJ test set using our best configuration
1
Unlike in the official CoNLL-2009 evaluation, in this
work we exclude the predicate sense from the features and
the evaluation.
433
Threshold ? 1 0.9 0.5 0.1 0.01
Ratio 1 1.014 1.103 1.500 2.843
Table 2: Ratio of additional dependencies in the graphs with
respect to a single-tree pipeline model (? = 1) on develop-
ment data.
Threshold prec (%) rec (%) F
1
training ? = 1
1 77.91 73.97 75.89
0.9 77.23 74.17 75.67
0.5 73.30 75.03 74.16
0.1 58.22 68.75 63.05
0.01 32.83 53.69 40.74
training ? = 0.5
1 81.17 73.57 77.18
0.9 80.74 73.78 77.10
0.5 78.40 74.79 76.55
0.1 65.76 71.61 68.56
0.01 42.95 57.68 49.24
training ? = 0.1
1 84.03 72.52 77.85
0.9 83.76 72.66 77.82
0.5 82.75 73.33 77.75
0.1 77.25 72.20 74.64
0.01 63.90 65.98 64.92
training ? = 0.01
1 81.62 69.06 74.82
0.9 81.45 69.19 74.82
0.5 80.80 69.80 74.90
0.1 77.92 68.94 73.16
0.01 74.12 65.92 69.78
Table 3: Results of our shortest-path system for different
number of allowed dependencies showing precision, recall
and F
1
on development set for the verbal predicates of the
English language.
from the development set. We compare to the
state-of-the art system by Zhao et al. (2009) that
was the top-performing system for the English lan-
guage in SRL at the CoNLL-2009 Shared Task.
We also show the results for a shortest-path system
trained and tested with ? = 1. In addition we in-
clude an equivalent pipeline system using all fea-
tures, both factored and non-factored, as defined
in Johansson (2009). We observe that by not be-
ing able to capture non-factored features the final
performance drops by 1.6 F
1
points.
6 Conclusions
We have formulated SRL in terms of shortest-
path inference. Our model predicts semantic roles
together with associated syntactic paths, and as-
sumes an arc-factored representation of the path.
This property allows for efficient shortest-path al-
System prec(%) rec(%) F
1
Zhao et al. 2009 86.91 81.22 83.97
Non-factored 86.96 75.92 81.06
Factored ? = 1 79.88 76.12 77.96
Factored best 85.26 74.41 79.46
Table 4: Test set results for verbal predicates of the in-domain
English dataset. The configurations are labeled as follows.
Factored ? = 1: our shortest-path system trained and tested
with ? = 1, similar to a pipeline system but without en-
forcing tree constraints and restricted to arc-factored features.
Factored best: our shortest-path system with the best results
from table 3. Non-factored: an equivalent pipeline system
that includes both factored and non-factored features.
gorithms that, given a predicate and a role, retrieve
the most likely argument and its path.
In the experimental section we prove the fea-
sibility of the approach. We observe that arc-
factored models are in fact more restricted, with a
drop in accuracy with respect to unrestricted mod-
els. However, we also observe that our method
largely improves the robustness of the arc-factored
method when training with a degree of syntac-
tic variability. Overall, ours is a simple strategy
to bring arc-factored models close to the perfor-
mance of unrestricted models. Future work should
explore further approaches to parse partial syntac-
tic structure specific to some target semantic rela-
tions.
Acknowledgments
This work was financed by the European Com-
mission for the XLike project (FP7-288342); and
by the Spanish Government for projects Tacardi
(TIN2012-38523-C02-00) and Skater (TIN2012-
38584-C06-01). For a large part of this work
Xavier Carreras was at the Universitat Polit`ecnica
de Catalunya under a Ram?on y Cajal contract
(RYC-2008-02223).
References
Xavier Carreras and Llu??s M`arquez. 2005. Intro-
duction to the CoNLL-2005 shared task: Semantic
role labeling. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 152?164, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
434
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2009. Introduction to
Algorithms. The MIT Press.
Edsger W. Dijkstra. 1959. A note on two problems
in connexion with graphs. Numerische Mathematik,
1(1):269?271.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288, September.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 239?246,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of CoNLL-2008 Shared
Task.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis with
propbank and nombank. In CoNLL 2008: Pro-
ceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 183?187,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
Richard Johansson. 2009. Statistical bistratal depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 561?569, Singapore, August. As-
sociation for Computational Linguistics.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 141?150, Prague, Czech Republic,
June. Association for Computational Linguistics.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint Arc-factored Parsing of Syntactic and
Semantic Dependencies. Transactions of the As-
sociation for Computational Linguistics (TACL),
1(1):219?230, May.
Llu??s M`arquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Semantic
Role Labeling: An Introduction to the Special Issue.
Computational Linguistics, 34(2):145?159, June.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
335?342, Barcelona, Spain, July.
Mihai Surdeanu, Llu??s M`arquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal of Artificial Intelli-
gence Research.
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing features for semantic role labeling. In Dekang
Lin and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 88?94, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multi-
lingual dependency learning: Exploiting rich fea-
tures for tagging syntactic and semantic dependen-
cies. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task, pages 61?66, Boulder,
Colorado, June. Association for Computational Lin-
guistics.
435
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 409?419,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Spectral Learning for Non-Deterministic Dependency Parsing
Franco M. Luque
Universidad Nacional de Co?rdoba
and CONICET
Co?rdoba X5000HUA, Argentina
francolq@famaf.unc.edu.ar
Ariadna Quattoni and Borja Balle and Xavier Carreras
Universitat Polite`cnica de Catalunya
Barcelona E-08034
{aquattoni,bballe,carreras}@lsi.upc.edu
Abstract
In this paper we study spectral learning
methods for non-deterministic split head-
automata grammars, a powerful hidden-
state formalism for dependency parsing.
We present a learning algorithm that, like
other spectral methods, is efficient and non-
susceptible to local minima. We show
how this algorithm can be formulated as
a technique for inducing hidden structure
from distributions computed by forward-
backward recursions. Furthermore, we
also present an inside-outside algorithm
for the parsing model that runs in cubic
time, hence maintaining the standard pars-
ing costs for context-free grammars.
1 Introduction
Dependency structures of natural language sen-
tences exhibit a significant amount of non-local
phenomena. Historically, there have been two
main approaches to model non-locality: (1) in-
creasing the order of the factors of a dependency
model (e.g. with sibling and grandparent relations
(Eisner, 2000; McDonald and Pereira, 2006; Car-
reras, 2007; Martins et al 2009; Koo and Collins,
2010)), and (2) using hidden states to pass in-
formation across factors (Matsuzaki et al 2005;
Petrov et al 2006; Musillo and Merlo, 2008).
Higher-order models have the advantage that
they are relatively easy to train, because estimat-
ing the parameters of the model can be expressed
as a convex optimization. However, they have
two main drawbacks. (1) The number of param-
eters grows significantly with the size of the fac-
tors, leading to potential data-sparsity problems.
A solution to address the data-sparsity problem
is to explicitly tell the model what properties of
higher-order factors need to be remembered. This
can be achieved by means of feature engineering,
but compressing such information into a state of
bounded size will typically be labor intensive, and
will not generalize across languages. (2) Increas-
ing the size of the factors generally results in poly-
nomial increases in the parsing cost.
In principle, hidden variable models could
solve some of the problems of feature engineering
in higher-order factorizations, since they could
automatically induce the information in a deriva-
tion history that should be passed across factors.
Potentially, they would require less feature engi-
neering since they can learn from an annotated
corpus an optimal way to compress derivations
into hidden states. For example, one line of work
has added hidden annotations to the non-terminals
of a phrase-structure grammar (Matsuzaki et al
2005; Petrov et al 2006; Musillo and Merlo,
2008), resulting in compact grammars that ob-
tain parsing accuracies comparable to lexicalized
grammars. A second line of work has modeled
hidden sequential structure, like in our case, but
using PDFA (Infante-Lopez and de Rijke, 2004).
Finally, a third line of work has induced hidden
structure from the history of actions of a parser
(Titov and Henderson, 2007).
However, the main drawback of the hidden
variable approach to parsing is that, to the best
of our knowledge, there has not been any convex
formulation of the learning problem. As a result,
training a hidden-variable model is both expen-
sive and prone to local minima issues.
In this paper we present a learning algorithm
for hidden-state split head-automata grammars
(SHAG) (Eisner and Satta, 1999). In this for-
409
malism, head-modifier sequences are generated
by a collection of finite-state automata. In our
case, the underlying machines are probabilistic
non-deterministic finite state automata (PNFA),
which we parameterize using the operator model
representation. This representation allows the use
of simple spectral algorithms for estimating the
model parameters from data (Hsu et al 2009;
Bailly, 2011; Balle et al 2012). In all previous
work, the algorithms used to induce hidden struc-
ture require running repeated inference on train-
ing data?e.g. Expectation-Maximization (Demp-
ster et al 1977), or split-merge algorithms. In
contrast, spectral methods are simple and very ef-
ficient ?parameter estimation is reduced to com-
puting some data statistics, performing SVD, and
inverting matrices.
The main contributions of this paper are:
? We present a spectral learning algorithm for
inducing PNFA with applications to head-
automata dependency grammars. Our for-
mulation is based on thinking about the dis-
tribution generated by a PNFA in terms of
the forward-backward recursions.
? Spectral learning algorithms in previous
work only use statistics of prefixes of se-
quences. In contrast, our algorithm is able
to learn from substring statistics.
? We derive an inside-outside algorithm for
non-deterministic SHAG that runs in cubic
time, keeping the costs of CFG parsing.
? In experiments we show that adding non-
determinism improves the accuracy of sev-
eral baselines. When we compare our algo-
rithm to EM we observe a reduction of two
orders of magnitude in training time.
The paper is organized as follows. Next section
describes the necessary background on SHAG
and operator models. Section 3 introduces Op-
erator SHAG for parsing, and presents a spectral
learning algorithm. Section 4 presents a parsing
algorithm. Section 5 presents experiments and
analysis of results, and section 6 concludes.
2 Preliminaries
2.1 Head-Automata Dependency Grammars
In this work we use split head-automata gram-
mars (SHAG) (Eisner and Satta, 1999; Eis-
ner, 2000), a context-free grammatical formal-
ism whose derivations are projective dependency
trees. We will use xi:j = xixi+1 ? ? ?xj to de-
note a sequence of symbols xt with i ? t ? j.
A SHAG generates sentences s0:N , where sym-
bols st ? X with 1 ? t ? N are regular words
and s0 = ? 6? X is a special root symbol. Let
X? = X ? {?}. A derivation y, i.e. a depen-
dency tree, is a collection of head-modifier se-
quences ?h, d, x1:T ?, where h ? X? is a word,
d ? {LEFT, RIGHT} is a direction, and x1:T is
a sequence of T words, where each xt ? X is
a modifier of h in direction d. We say that h is
the head of each xt. Modifier sequences x1:T are
ordered head-outwards, i.e. among x1:T , x1 is the
word closest to h in the derived sentence, and xT
is the furthest. A derivation y of a sentence s0:N
consists of a LEFT and a RIGHT head-modifier se-
quence for each st. As special cases, the LEFT se-
quence of the root symbol is always empty, while
the RIGHT one consists of a single word corre-
sponding to the head of the sentence. We denote
by Y the set of all valid derivations.
Assume a derivation y contains ?h, LEFT, x1:T ?
and ?h, RIGHT, x?1:T ??. Let L(y, h) be the derived
sentence headed by h, which can be expressed as
L(y, xT ) ? ? ? L(y, x1) h L(y, x?1) ? ? ? L(y, x
?
T ?).
1
The language generated by a SHAG are the
strings L(y, ?) for any y ? Y .
In this paper we use probabilistic versions of
SHAG where probabilities of head-modifier se-
quences in a derivation are independent of each
other:
P(y) =
?
?h,d,x1:T ??y
P(x1:T |h, d) . (1)
In the literature, standard arc-factored models fur-
ther assume that
P(x1:T |h, d) =
T+1?
t=1
P(xt|h, d, ?t) , (2)
where xT+1 is always a special STOP word, and ?t
is the state of a deterministic automaton generat-
ing x1:T+1. For example, setting ?1 = FIRST and
?t>1 = REST corresponds to first-order models,
while setting ?1 = NULL and ?t>1 = xt?1 corre-
sponds to sibling models (Eisner, 2000; McDon-
ald et al 2005; McDonald and Pereira, 2006).
1Throughout the paper we assume we can distinguish the
words in a derivation, irrespective of whether two words at
different positions correspond to the same symbol.
410
2.2 Operator Models
An operator model A with n states is a tuple
??1, ?>?, {Aa}a?X ?, where Aa ? R
n?n is an op-
erator matrix and ?1, ?? ? Rn are vectors. A
computes a function f : X ? ? R as follows:
f(x1:T ) = ?
>
? AxT ? ? ? Ax1 ?1 . (3)
One intuitive way of understanding operator
models is to consider the case where f computes
a probability distribution over strings. Such a dis-
tribution can be described in two equivalent ways:
by making some independence assumptions and
providing the corresponding parameters, or by ex-
plaining the process used to compute f . This is
akin to describing the distribution defined by an
HMM in terms of a factorization and its corre-
sponding transition and emission parameters, or
using the inductive equations of the forward al-
gorithm. The operator model representation takes
the latter approach.
Operator models have had numerous applica-
tions. For example, they can be used as an alter-
native parameterization of the function computed
by an HMM (Hsu et al 2009). Consider an HMM
with n hidden states and initial-state probabilities
pi ? Rn, transition probabilities T ? Rn?n, and
observation probabilities Oa ? Rn?n for each
a ? X , with the following meaning:
? pi(i) is the probability of starting at state i,
? T (i, j) is the probability of transitioning
from state j to state i,
? Oa is a diagonal matrix, such that Oa(i, i) is
the probability of generating symbol a from
state i.
Given an HMM, an equivalent operator model
can be defined by setting ?1 = pi, Aa = TOa
and ?? = ~1. To see this, let us show that the for-
ward algorithm computes the expression in equa-
tion (3). Let ?t denote the state of the HMM
at time t. Consider a state-distribution vector
?t ? Rn, where ?t(i) = P(x1:t?1, ?t = i). Ini-
tially ?1 = pi. At each step in the chain of prod-
ucts (3), ?t+1 = Axt ?t updates the state dis-
tribution from positions t to t + 1 by applying
the appropriate operator, i.e. by emitting symbol
xt and transitioning to the new state distribution.
The probability of x1:T is given by
?
i ?T+1(i).
Hence, Aa(i, j) is the probability of generating
symbol a and moving to state i given that we are
at state j.
HMM are only one example of distributions
that can be parameterized by operator models.
In general, operator models can parameterize any
PNFA, where the parameters of the model corre-
spond to probabilities of emitting a symbol from
a state and moving to the next state.
The advantage of working with operator mod-
els is that, under certain mild assumptions on the
operator parameters, there exist algorithms that
can estimate the operators from observable statis-
tics of the input sequences. These algorithms are
extremely efficient and are not susceptible to local
minima issues. See (Hsu et al 2009) for theoret-
ical proofs of the learnability of HMM under the
operator model representation.
In the following, we write x = xi:j ? X ? to
denote sequences of symbols, and use Axi:j as a
shorthand for Axj ? ? ?Axi . Also, for convenience
we assume X = {1, . . . , l}, so that we can index
vectors and matrices by symbols in X .
3 Learning Operator SHAG
We will define a SHAG using a collection of op-
erator models to compute probabilities. Assume
that for each possible head h in the vocabulary X?
and each direction d ? {LEFT, RIGHT} we have
an operator model that computes probabilities of
modifier sequences as follows:
P(x1:T |h, d) = (?h,d? )
> Ah,dxT ? ? ? A
h,d
x1 ?
h,d
1 .
Then, this collection of operator models defines
an operator SHAG that assigns a probability to
each y ? Y according to (1). To learn the model
parameters, namely ??h,d1 , ?
h,d
? , {A
h,d
a }a?X ? for
h ? X? and d ? {LEFT, RIGHT}, we use spec-
tral learning methods based on the works of Hsu
et al(2009), Bailly (2011) and Balle et al(2012).
The main challenge of learning an operator
model is to infer a hidden-state space from ob-
servable quantities, i.e. quantities that can be com-
puted from the distribution of sequences that we
observe. As it turns out, we cannot recover the
actual hidden-state space used by the operators
we wish to learn. The key insight of the spectral
learning method is that we can recover a hidden-
state space that corresponds to a projection of the
original hidden space. Such projected space is
equivalent to the original one in the sense that we
411
can find operators in the projected space that pa-
rameterize the same probability distribution over
sequences.
In the rest of this section we describe an algo-
rithm for learning an operator model. We will as-
sume a fixed head word and direction, and drop h
and d from all terms. Hence, our goal is to learn
the following distribution, parameterized by oper-
ators ?1, {Aa}a?X , and ??:
P(x1:T ) = ?>? AxT ? ? ? Ax1 ?1 . (4)
Our algorithm shares many features with the
previous spectral algorithms of Hsu et al(2009)
and Bailly (2011), though the derivation given
here is based upon the general formulation of
Balle et al(2012). The main difference is that
our algorithm is able to learn operator models
from substring statistics, while algorithms in pre-
vious works were restricted to statistics on pre-
fixes. In principle, our algorithm should extract
much more information from a sample.
3.1 Preliminary Definitions
The spectral learning algorithm will use statistics
estimated from samples of the target distribution.
More specifically, consider the function that com-
putes the expected number of occurrences of a
substring x in a random string x? drawn from P:
f(x) = E(x v] x?)
=
?
x??X ?
(x v] x
?)P(x?)
=
?
p,s?X ?
P(pxs) , (5)
where x v] x? denotes the number of times x ap-
pears in x?. Here we assume that the true values
of f(x) for bigrams are known, though in practice
the algorithm will work with empirical estimates
of these.
The information about f known by the algo-
rithm is organized in matrix form as follows. Let
P ? Rl?l be a matrix containing the value of f(x)
for all strings of length two, i.e. bigrams.2. That
is, each entry in P ? Rl?l contains the expected
number of occurrences of a given bigram:
P (b, a) = E(ab v] x) . (6)
2In fact, while we restrict ourselves to strings of length
two, an analogous algorithm can be derived that considers
longer strings to define P . See (Balle et al 2012) for details.
Furthermore, for each b ? X let Pb ? Rl?l denote
the matrix whose entries are given by
Pb(c, a) = E(abc v] x) , (7)
the expected number of occurrences of trigrams.
Finally, we define vectors p1 ? Rl and p? ? Rl
as follows: p1(a) =
?
s?X ? P(as), the probabil-
ity that a string begins with a particular symbol;
and p?(a) =
?
p?X ? P(pa), the probability that
a string ends with a particular symbol.
Now we show a particularly useful way to ex-
press the quantities defined above in terms of the
operators ??1, ?>?, {Aa}a?X ? of P. First, note
that each entry of P can be written in this form:
P (b, a) =
?
p,s?X ?
P(pabs) (8)
=
?
p,s?X ?
?>? As Ab Aa Ap ?1
= (?>?
?
s?X ?
As) Ab Aa(
?
p?X ?
Ap ?1) .
It is not hard to see that, since P is a probability
distribution over X ?, actually ?>?
?
s?X ? As =
~1>. Furthermore, since
?
p?X ? Ap =?
k?0(
?
a?X Aa)
k = (I ?
?
a?X Aa)
?1,
we write ??1 = (I ?
?
a?X Aa)
?1?1. From (8) it
is natural to define a forward matrix F ? Rn?l
whose ath column contains the sum of all hidden-
state vectors obtained after generating all prefixes
ended in a:
F (:, a) = Aa
?
p?X ?
Ap ?1 = Aa ??1 . (9)
Conversely, we also define a backward matrix
B ? Rl?n whose ath row contains the probability
of generating a from any possible state:
B(a, :) = ?>?
?
s?X ?
AsAa = ~1
>Aa . (10)
By plugging the forward and backward matri-
ces into (8) one obtains the factorization P =
BF . With similar arguments it is easy to see
that one also has Pb = BAbF , p1 = B ?1, and
p>? = ?
>
? F . Hence, ifB and F were known, one
could in principle invert these expressions in order
to recover the operators of the model from em-
pirical estimations computed from a sample. In
the next section we show that in fact one does not
need to know B and F to learn an operator model
for P, but rather that having a ?good? factorization
of P is enough.
412
3.2 Inducing a Hidden-State Space
We have shown that an operator model A com-
puting P induces a factorization of the matrix P ,
namely P = BF . More generally, it turns out that
when the rank of P equals the minimal number of
states of an operator model that computes P, then
one can prove a duality relation between opera-
tors and factorizations of P . In particular, one can
show that, for any rank factorization P = QR, the
operators given by ??1 = Q+p1, ??>? = p
>
?R
+,
and A?a = Q+PaR+, yield an operator model for
P. A key fact in proving this result is that the func-
tion P is invariant to the basis chosen to represent
operator matrices. See (Balle et al 2012) for fur-
ther details.
Thus, we can recover an operator model for P
from any rank factorization of P , provided a rank
assumption on P holds (which hereafter we as-
sume to be the case). Since we only have access
to an approximation of P , it seems reasonable to
choose a factorization which is robust to estima-
tion errors. A natural such choice is the thin SVD
decomposition of P (i.e. using top n singular vec-
tors), given by: P = U(?V >) = U(U>P ).
Intuitively, we can think of U and U>P as pro-
jected backward and forward matrices. Now that
we have a factorization of P we can construct an
operator model for P as follows: 3
??1 = U
>p1 , (11)
??>? = p
>
?(U
>P )+ , (12)
A?a = U
>Pa(U
>P )+ . (13)
Algorithm 1 presents pseudo-code for an algo-
rithm learning operators of a SHAG from train-
ing head-modifier sequences using this spectral
method. Note that each operator model in the
3To see that equations (11-13) define a model for P, one
must first see that the matrix M = F (?V >)+ is invertible
with inverse M?1 = U>B. Using this and recalling that
p1 = B?1, Pa = BAaF , p>? = ?
>
?F , one obtains that:
??1 = U
>B?1 = M
?1?1 ,
??>? = ?
>
?F (U
>BF )+ = ?>?M ,
A?a = U
>BAaF (U
>BF )+ = M?1AaM .
Finally:
P(x1:T ) = ?
>
? AxT ? ? ?Ax1 ?1
= ?>?MM
?1AxTM ? ? ?M
?1Ax1MM
?1?1
= ??>?A?xT ? ? ? A?x1 ??1
Algorithm 1 Learn Operator SHAG
inputs:
? An alphabet X
? A training set TRAIN = {?hi, di, xi1:T ?}
M
i=1
? The number of hidden states n
1: for each h ? X? and d ? {LEFT, RIGHT} do
2: Compute an empirical estimate from TRAIN of
statistics matrices p?1, p??, P? , and {P?a}a?X
3: Compute the SVD of P? and let U? be the matrix
of top n left singular vectors of P?
4: Compute the observable operators for h and d:
5: ??h,d1 = U?
>p?1
6: (??h,d? )
> = p?>?(U?
>P? )+
7: A?h,da = U?
>P?a(U?>P? )+ for each a ? X
8: end for
9: return Operators ???h,d1 , ??
h,d
? , A?
h,d
a ?
for each h ? X? , d ? {LEFT, RIGHT}, a ? X
SHAG is learned separately. The running time
of the algorithm is dominated by two computa-
tions. First, a pass over the training sequences to
compute statistics over unigrams, bigrams and tri-
grams. Second, SVD and matrix operations for
computing the operators, which run in time cubic
in the number of symbols l. However, note that
when dealing with sparse matrices many of these
operations can be performed more efficiently.
4 Parsing Algorithms
Given a sentence s0:N we would like
to find its most likely derivation, y? =
argmaxy?Y(s0:N ) P(y). This problem, known as
MAP inference, is known to be intractable for
hidden-state structure prediction models, as it
involves finding the most likely tree structure
while summing out over hidden states. We use
a common approximation to MAP based on first
computing posterior marginals of tree edges (i.e.
dependencies) and then maximizing over the
tree structure (see (Park and Darwiche, 2004)
for complexity of general MAP inference and
approximations). For parsing, this strategy is
sometimes known as MBR decoding; previous
work has shown that empirically it gives good
performance (Goodman, 1996; Clark and Cur-
ran, 2004; Titov and Henderson, 2006; Petrov
and Klein, 2007). In our case, we use the
non-deterministic SHAG to compute posterior
marginals of dependencies. We first explain the
general strategy of MBR decoding, and then
present an algorithm to compute marginals.
413
Let (si, sj) denote a dependency between head
word i and modifier word j. The posterior
or marginal probability of a dependency (si, sj)
given a sentence s0:N is defined as
?i,j = P((si, sj) | s0:N ) =
?
y?Y(s0:N ) : (si,sj)?y
P(y) .
To compute marginals, the sum over derivations
can be decomposed into a product of inside and
outside quantities (Baker, 1979). Below we de-
scribe an inside-outside algorithm for our gram-
mars. Given a sentence s0:N and marginal scores
?i,j , we compute the parse tree for s0:N as
y? = argmax
y?Y(s0:N )
?
(si,sj)?y
log?i,j (14)
using the standard projective parsing algorithm
for arc-factored models (Eisner, 2000). Overall
we use a two-pass parsing process, first to com-
pute marginals and then to compute the best tree.
4.1 An Inside-Outside Algorithm
In this section we sketch an algorithm to com-
pute marginal probabilities of dependencies. Our
algorithm is an adaptation of the parsing algo-
rithm for SHAG by Eisner and Satta (1999) to
the case of non-deterministic head-automata, and
has a runtime cost of O(n2N3), where n is the
number of states of the model, and N is the
length of the input sentence. Hence the algorithm
maintains the standard cubic cost on the sentence
length, while the quadratic cost on n is inher-
ent to the computations defined by our model in
Eq. (3). The main insight behind our extension
is that, because the computations of our model in-
volve state-distribution vectors, we need to extend
the standard inside/outside quantities to be in the
form of such state-distribution quantities.4
Throughout this section we assume a fixed sen-
tence s0:N . Let Y(xi:j) be the set of derivations
that yield a subsequence xi:j . For a derivation y,
we use root(y) to indicate the root word of it,
and use (xi, xj) ? y to refer a dependency in y
from head xi to modifier xj . Following Eisner
4Technically, when working with the projected operators
the state-distribution vectors will not be distributions in the
formal sense. However, they correspond to a projection of a
state distribution, for some projection that we do not recover
from data (namely M?1 in footnote 3). This projection has
no effect on the computations because it cancels out.
and Satta (1999), we use decoding structures re-
lated to complete half-constituents (or ?triangles?,
denoted C) and incomplete half-constituents (or
?trapezoids?, denoted I), each decorated with a di-
rection (denoted L and R). We assume familiarity
with their algorithm.
We define ?I,Ri,j ? R
n as the inside score-vector
of a right trapezoid dominated by dependency
(si, sj),
?I,Ri,j =
?
y?Y(si:j) : (si,sj)?y ,
y={?si,R,x1:t?} ? y? , xt=sj
P(y?)?si,R(x1:t) . (15)
The term P(y?) is the probability of head-modifier
sequences in the range si:j that do not involve
si. The term ?si,R(x1:t) is a forward state-
distribution vector ?the qth coordinate of the
vector is the probability that si generates right
modifiers x1:t and remains at state q. Similarly,
we define ?I,Ri,j ? R
n as the outside score-vector
of a right trapezoid, as
?I,Ri,j =
?
y?Y(s0:isj:n) : root(y)=s0,
y={?si,R,xt:T ?} ? y? , xt=sj
P(y?)?si,R(xt+1:T ) , (16)
where ?si,R(xt+1:T ) ? Rn is a backward state-
distribution vector ?the qth coordinate is the
probability of being at state q of the right au-
tomaton of si and generating xt+1:T . Analogous
inside-outside expressions can be defined for the
rest of structures (left/right triangles and trape-
zoids). With these quantities, we can compute
marginals as
?i,j =
{
(?I,Ri,j )
> ?I,Ri,j Z
?1 if i < j ,
(?I,Li,j)
> ?I,Li,j Z
?1 if j < i ,
(17)
where Z=
?
y?Y(s0:N)
P(y) = (??,R? )> ?
C,R
0,N .
Finally, we sketch the equations for computing
inside scores in O(N3) time. The outside equa-
tions can be derived analogously (see (Paskin,
2001)). For 0 ? i < j ? N :
?C,Ri,i = ?
si,R
1 (18)
?C,Ri,j =
j?
k=i+1
?I,Ri,k
(
(?sk,R? )
> ?C,Rk,j
)
(19)
?I,Ri,j =
j?
k=i
Asi,Rsj ?
C,R
i,k
(
(?
sj ,L
? )> ?
C,L
k+1,j
)
(20)
414
5 Experiments
The goal of our experiments is to show that in-
corporating hidden states in a SHAG using oper-
ator models can consistently improve parsing ac-
curacy. A second goal is to compare the spec-
tral learning algorithm to EM, a standard learning
method that also induces hidden states.
The first set of experiments involve fully unlex-
icalized models, i.e. parsing part-of-speech tag se-
quences. While this setting falls behind the state-
of-the-art, it is nonetheless valid to analyze empir-
ically the effect of incorporating hidden states via
operator models, which results in large improve-
ments. In a second set of experiments, we com-
bine the unlexicalized hidden-state models with
simple lexicalized models. Finally, we present
some analysis of the automaton learned by the
spectral algorithm to see the information that is
captured in the hidden state space.
5.1 Fully Unlexicalized Grammars
We trained fully unlexicalized dependency gram-
mars from dependency treebanks, that is, X are
PoS tags and we parse PoS tag sequences. In
all cases, our modifier sequences include special
START and STOP symbols at the boundaries. 5 6
We compare the following SHAG models:
? DET: a baseline deterministic grammar with
a single state.
? DET+F: a deterministic grammar with two
states, one emitting the first modifier of a
sequence, and another emitting the rest (see
(Eisner and Smith, 2010) for a similar deter-
ministic baseline).
? SPECTRAL: a non-deterministic grammar
with n hidden states trained with the spectral
algorithm. n is a parameter of the model.
? EM: a non-deterministic grammar with n
states trained with EM. Here, we estimate
operators ???1, ???, A?
h,d
a ? using forward-
backward for the E step. To initialize, we
mimicked an HMM initialization: (1) we set
??1 and ??? randomly; (2) we created a ran-
dom transition matrix T ? Rn?n; (3) we
5Even though the operators ?1 and ?? of a PNFA ac-
count for start and stop probabilities, in preliminary experi-
ments we found that having explicit START and STOP sym-
bols results in more accurate models.
6Note that, for parsing, the operators for the START and
STOP symbols can be packed into ?1 and ?? respectively.
One just defines ??1 = ASTART ?1 and ?
?>
? = ?
>
? ASTOP.
 68
 70
 72
 74
 76
 78
 80
 82
 2  4  6  8  10  12  14
un
lab
ele
d a
ttac
hm
ent
 sc
ore
number of states
DetDet+FSpectralEM (5)EM (10)EM (25)EM (100)
Figure 1: Accuracy curve on English development set
for fully unlexicalized models.
created a diagonal matrix Oh,da ? Rn?n,
where Oh,da (i, i) is the probability of gener-
ating symbol a from h and d (estimated from
training); (4) we set A?h,da = TO
h,d
a .
We trained SHAG models using the standard
WSJ sections of the English Penn Treebank (Mar-
cus et al 1994). Figure 1 shows the Unlabeled
Attachment Score (UAS) curve on the develop-
ment set, in terms of the number of hidden states
for the spectral and EM models. We can see
that DET+F largely outperforms DET7, while the
hidden-state models obtain much larger improve-
ments. For the EM model, we show the accuracy
curve after 5, 10, 25 and 100 iterations.8
In terms of peak accuracies, EM gives a slightly
better result than the spectral method (80.51% for
EM with 15 states versus 79.75% for the spectral
method with 9 states). However, the spectral al-
gorithm is much faster to train. With our Matlab
implementation, it took about 30 seconds, while
each iteration of EM took from 2 to 3 minutes,
depending on the number of states. To give a con-
crete example, to reach an accuracy close to 80%,
there is a factor of 150 between the training times
of the spectral method and EM (where we com-
pare the peak performance of the spectral method
versus EM at 25 iterations with 13 states).
7For parsing with deterministic SHAG we employ MBR
inference, even though Viterbi inference can be performed
exactly. In experiments on development data DET improved
from 62.65% using Viterbi to 68.52% using MBR, and
DET+F improved from 72.72% to 74.80%.
8We ran EM 10 times under different initial conditions
and selected the run that gave the best absolute accuracy after
100 iterations. We did not observe significant differences
between the runs.
415
DET DET+F SPECTRAL EM
WSJ 69.45% 75.91% 80.44% 81.68%
Table 1: Unlabeled Attachment Score of fully unlexi-
calized models on the WSJ test set.
Table 1 shows results on WSJ test data, se-
lecting the models that obtain peak performances
in development. We observe the same behavior:
hidden-states largely improve over deterministic
baselines, and EM obtains a slight improvement
over the spectral algorithm. Comparing to previ-
ous work on parsing WSJ PoS sequences, Eisner
and Smith (2010) obtained an accuracy of 75.6%
using a deterministic SHAG that uses informa-
tion about dependency lengths. However, they
used Viterbi inference, which we found to per-
form worse than MBR inference (see footnote 7).
5.2 Experiments with Lexicalized
Grammars
We now turn to combining lexicalized determinis-
tic grammars with the unlexicalized grammars ob-
tained in the previous experiment using the spec-
tral algorithm. The goal behind this experiment
is to show that the information captured in hidden
states is complimentary to head-modifier lexical
preferences.
In this case X consists of lexical items, and we
assume access to the PoS tag of each lexical item.
We will denote as ta and wa the PoS tag and word
of a symbol a ? X? . We will estimate condi-
tional distributions P(a | h, d, ?), where a ? X
is a modifier, h ? X? is a head, d is a direction,
and ? is a deterministic state. Following Collins
(1999), we use three configurations of determin-
istic states:
? LEX: a single state.
? LEX+F: two distinct states for first modifier
and rest of modifiers.
? LEX+FCP: four distinct states, encoding:
first modifier, previous modifier was a coor-
dination, previous modifier was punctuation,
and previous modifier was some other word.
To estimate P we use a back-off strategy:
P(a|h, d, ?) = PA(ta|h, d, ?)PB(wa|ta, h, d, ?)
To estimate PA we use two back-off levels,
the fine level conditions on {wh, d, ?} and the
 72
 74
 76
 78
 80
 82
 84
 86
 2  3  4  5  6  7  8  9  10
un
lab
ele
d a
ttac
hm
ent
 sc
ore
number of states
LexLex+FLex+FCPLex + SpectralLex+F + SpectralLex+FCP + Spectral
Figure 2: Accuracy curve on English development set
for lexicalized models.
coarse level conditions on {th, d, ?}. For PB we
use three levels, which from fine to coarse are
{ta, wh, d, ?}, {ta, th, d, ?} and {ta}. We follow
Collins (1999) to estimate PA and PB from a tree-
bank using a back-off strategy.
We use a simple approach to combine lexical
models with the unlexical hidden-state models we
obtained in the previous experiment. Namely, we
use a log-linear model that computes scores for
head-modifier sequences as
s(?h, d, x1:T ?) = logPsp(x1:T |h, d) (21)
+ logPdet(x1:T |h, d) ,
where Psp and Pdet are respectively spectral and
deterministic probabilistic models. We tested
combinations of each deterministic model with
the spectral unlexicalized model using different
number of states. Figure 2 shows the accuracies of
single deterministic models, together with combi-
nations using different number of states. In all
cases, the combinations largely improve over the
purely deterministic lexical counterparts, suggest-
ing that the information encoded in hidden states
is complementary to lexical preferences.
5.3 Results Analysis
We conclude the experiments by analyzing the
state space learned by the spectral algorithm.
Consider the space Rn where the forward-state
vectors lie. Generating a modifier sequence corre-
sponds to a path through the n-dimensional state
space. We clustered sets of forward-state vectors
in order to create a DFA that we can use to visu-
alize the phenomena captured by the state space.
416
 cc
jj dt nnp
prp$ vbg jjs
rb vbn posjj in dt cd
1 5
7
I
2
0
3
 cc
nns
   cd
 ,
$ nnp
 cd nns
STOP
,
 prp$ rb posjj dt nnp
9
$ nnjjr nnp
 STOP
STOP
  cc
 nn
STOP
  cc
 nn
,
prp$ nn pos
Figure 3: DFA approximation for the generation of NN
left modifier sequences.
To build a DFA, we computed the forward vec-
tors corresponding to frequent prefixes of modi-
fier sequences of the development set. Then, we
clustered these vectors using a Group Average
Agglomerative algorithm using the cosine simi-
larity measure (Manning et al 2008). This simi-
larity measure is appropriate because it compares
the angle between vectors, and is not affected by
their magnitude (the magnitude of forward vec-
tors decreases with the number of modifiers gen-
erated). Each cluster i defines a state in the DFA,
and we say that a sequence x1:t is in state i if its
corresponding forward vector at time t is in clus-
ter i. Then, transitions in the DFA are defined us-
ing a procedure that looks at how sequences tra-
verse the states. If a sequence x1:t is at state i at
time t ? 1, and goes to state j at time t, then we
define a transition from state i to state j with la-
bel xt. This procedure may require merging states
to give a consistent DFA, because different se-
quences may define different transitions for the
same states and modifiers. After doing a merge,
new merges may be required, so the procedure
must be repeated until a DFA is obtained.
For this analysis, we took the spectral model
with 9 states, and built DFA from the non-
deterministic automata corresponding to heads
and directions where we saw largest improve-
ments in accuracy with respect to the baselines.
A DFA for the automaton (NN, LEFT) is shown
in Figure 3. The vectors were originally divided
in ten clusters, but the DFA construction required
two state mergings, leading to a eight state au-
tomaton. The state named I is the initial state.
Clearly, we can see that there are special states
for punctuation (state 9) and coordination (states
1 and 5). States 0 and 2 are harder to interpret.
To understand them better, we computed an esti-
mation of the probabilities of the transitions, by
counting the number of times each of them is
used. We found that our estimation of generating
STOP from state 0 is 0.67, and from state 2 it is
0.15. Interestingly, state 2 can transition to state 0
generating prp$, POS or DT, that are usual end-
ings of modifier sequences for nouns (recall that
modifiers are generated head-outwards, so for a
left automaton the final modifier is the left-most
modifier in the sentence).
6 Conclusion
Our main contribution is a basic tool for inducing
sequential hidden structure in dependency gram-
mars. Most of the recent work in dependency
parsing has explored explicit feature engineering.
In part, this may be attributed to the high cost of
using tools such as EM to induce representations.
Our experiments have shown that adding hidden-
structure improves parsing accuracy, and that our
spectral algorithm is highly scalable.
Our methods may be used to enrich the rep-
resentational power of more sophisticated depen-
dency models. For example, future work should
consider enhancing lexicalized dependency gram-
mars with hidden states that summarize lexical
dependencies. Another line for future research
should extend the learning algorithm to be able
to capture vertical hidden relations in the depen-
dency tree, in addition to sequential relations.
Acknowledgements We are grateful to Gabriele
Musillo and the anonymous reviewers for providing us
with helpful comments. This work was supported by
a Google Research Award and by the European Com-
mission (PASCAL2 NoE FP7-216886, XLike STREP
FP7-288342). Borja Balle was supported by an FPU
fellowship (AP2008-02064) of the Spanish Ministry
of Education. The Spanish Ministry of Science and
Innovation supported Ariadna Quattoni (JCI-2009-
04240) and Xavier Carreras (RYC-2008-02223 and
?KNOW2? TIN2009-14715-C04-04).
417
References
Raphael Bailly. 2011. Quadratic weighted automata:
Spectral algorithm and likelihood maximization.
JMLR Workshop and Conference Proceedings ?
ACML.
James K. Baker. 1979. Trainable grammars for speech
recognition. In D. H. Klatt and J. J. Wolf, editors,
Speech Communication Papers for the 97th Meeting
of the Acoustical Society of America, pages 547?
550.
Borja Balle, Ariadna Quattoni, and Xavier Carreras.
2012. Local loss optimization in operator models:
A new insight into spectral learning. Technical Re-
port LSI-12-5-R, Departament de Llenguatges i Sis-
temes Informa`tics (LSI), Universitat Polite`cnica de
Catalunya (UPC).
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 957?961, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Stephen Clark and James R. Curran. 2004. Parsing
the wsj using ccg and log-linear models. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 103?110, Barcelona, Spain, July.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the em algorithm. Journal of the royal sta-
tistical society, Series B, 39(1):1?38.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 457?464, Univer-
sity of Maryland, June.
Jason Eisner and Noah A. Smith. 2010. Favor
short dependencies: Parsing with soft and hard con-
straints on dependency length. In Harry Bunt, Paola
Merlo, and Joakim Nivre, editors, Trends in Parsing
Technology: Dependency Parsing, Domain Adapta-
tion, and Deep Parsing, chapter 8, pages 121?150.
Springer.
Jason Eisner. 2000. Bilexical grammars and their
cubic-time parsing algorithms. In Harry Bunt and
Anton Nijholt, editors, Advances in Probabilis-
tic and Other Parsing Technologies, pages 29?62.
Kluwer Academic Publishers, October.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the 34th Annual Meeting
of the Association for Computational Linguistics,
pages 177?183, Santa Cruz, California, USA, June.
Association for Computational Linguistics.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009.
A spectral algorithm for learning hidden markov
models. In COLT 2009 - The 22nd Conference on
Learning Theory.
Gabriel Infante-Lopez and Maarten de Rijke. 2004.
Alternative approaches for generating bodies of
grammar rules. In Proceedings of the 42nd Meet-
ing of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 454?461,
Barcelona, Spain, July.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
first edition, July.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19.
Andre Martins, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 342?
350, Suntec, Singapore, August. Association for
Computational Linguistics.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?05),
pages 75?82, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 81?88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523?530, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Gabriele Antonio Musillo and Paola Merlo. 2008. Un-
lexicalised hidden variable models of split depen-
dency grammars. In Proceedings of ACL-08: HLT,
Short Papers, pages 213?216, Columbus, Ohio,
June. Association for Computational Linguistics.
James D. Park and Adnan Darwiche. 2004. Com-
plexity results and approximation strategies for map
418
explanations. Journal of Artificial Intelligence Re-
search, 21:101?133.
Mark Paskin. 2001. Cubic-time parsing and learning
algorithms for grammatical bigram models. Techni-
cal Report UCB/CSD-01-1148, University of Cali-
fornia, Berkeley.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404?411, Rochester, New York, April.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440, Sydney, Australia, July. Association for Com-
putational Linguistics.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
2006 Conference on Empirical Methods in Natu-
ral Language Processing, pages 560?567, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Ivan Titov and James Henderson. 2007. A latent vari-
able model for generative dependency parsing. In
Proceedings of the Tenth International Conference
on Parsing Technologies, pages 144?155, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
419
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 9?12,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
XLike Project Language Analysis Services
Xavier Carreras
?
, Llu??s Padr
?
o
?
, Lei Zhang
?
, Achim Rettinger
?
, Zhixing Li
1
,
Esteban Garc??a-Cuesta

,
?
Zeljko Agi
?
c
?
, Bo?zo Bekavac
/
, Blaz Fortuna
?
, Tadej
?
Stajner
?
? Universitat Polit`ecnica de Catalunya, Barcelona, Spain.  iSOCO S.A. Madrid, Spain.
/ University of Zagreb, Zagreb, Croatia. ? University of Potsdam, Germany.
? Jo?zef Stefan Institute, Ljubljana, Slovenia. 1 Tsinghua University, Beijing, China.
? Karlsruhe Institute of Technology, Karlsruhe, Germany.
Abstract
This paper presents the linguistic analysis
infrastructure developed within the XLike
project. The main goal of the imple-
mented tools is to provide a set of func-
tionalities supporting the XLike main ob-
jectives: Enabling cross-lingual services
for publishers, media monitoring or de-
veloping new business intelligence appli-
cations. The services cover seven major
and minor languages: English, German,
Spanish, Chinese, Catalan, Slovenian, and
Croatian. These analyzers are provided
as web services following a lightweigth
SOA architecture approach, and they are
publically accessible and shared through
META-SHARE.
1
1 Introduction
Project XLike
2
goal is to develop technology able
to gather documents in a variety of languages and
genres (news, blogs, tweets, etc.) and to extract
language-independent knowledge from them, in
order to provide new and better services to pub-
lishers, media monitoring, and business intelli-
gence. Thus, project use cases are provided by
STA (Slovenian Press Agency) and Bloomberg, as
well as New York Times as an associated partner.
Research partners in the project are Jo?zef Ste-
fan Institute (JSI), Karlsruhe Institute of Technol-
ogy (KIT), Universitat Polit`ecnica de Catalunya
(UPC), University of Zagreb (UZG), and Tsinghua
University (THU). The Spanish company iSOCO
is in charge of integration of all components de-
veloped in the project.
This paper deals with the language technology
developed within the project XLike to convert in-
1
accessible and shared here means that the services are
publicly callable, not that the code is open-source.
http://www.meta-share.eu
2
http://www.xlike.org
put documents into a language-independent rep-
resentation that afterwards enables knowledge ag-
gregation.
To achieve this goal, a bench of linguistic pro-
cessing pipelines is devised as the first step in the
document processing flow. Then, a cross-lingual
semantic annotation method, based on Wikipedia
and Linked Open Data (LOD), is applied. The
semantic annotation stage enriches the linguistic
anaylsis with links to knowledge bases for differ-
ent languages, or links to language independent
representations.
2 Linguistic Analyzers
Apart from basic state-of-the-art tokenizers, lem-
matizers, PoS/MSD taggers, and NE recogniz-
ers, each pipeline requires deeper processors able
to build the target language-independent seman-
tic representantion. For that, we rely on three
steps: dependency parsing, semantic role label-
ing and word sense disambiguation. These three
processes, combined with multilingual ontologi-
cal resouces such as different WordNets and Pred-
icateMatrix (L?opez de la Calle et al., 2014), a
lexical semantics resource combining WordNet,
FrameNet, and VerbNet, are the key to the con-
struction of our semantic representation.
2.1 Dependency Parsing
We use graph-based methods for dependency
parsing, namely, MSTParser
3
(McDonald et al.,
2005) is used for Chinese and Croatian, and
Treeler
4
is used for the other languages. Treeler is
a library developed by the UPC team that imple-
ments several statistical methods for tagging and
parsing.
We use these tools in order to train dependency
parsers for all XLike languages using standard
available treebanks.
3
http://sourceforge.net/projects/mstparser
4
http://treeler.lsi.upc.edu
9
2.2 Semantic Role Labeling
As with syntactic parsing, we are developing SRL
methods with the Treeler library. In order to train
models, we will use the treebanks made available
by the CoNLL-2009 shared task, which provided
data annotated with predicate-argument relations
for English, Spanish, Catalan, German and Chi-
nese. No treebank annotated with semantic roles
exists for Slovene or Croatian. A prototype of
SRL has been integrated in all pipelines (except
the Slovene and Croatian pipelines). The method
implemented follows a pipeline architecture de-
scribed in (Llu??s et al., 2013).
2.3 Word Sense Disambiguation
Word sense disambiguation is performed for all
languages with a publicly available WordNet. This
includes all languages in the project except Chi-
nese. The goal of WSD is to map specific lan-
guages to a common semantic space, in this case,
WN synsets. Thanks to existing connections be-
tween WN and other resources, SUMO and Open-
CYC sense codes are also output when available.
Thanks to PredicateMatrix, the obtained con-
cepts can be projected to FrameNet, achieving a
normalization of the semantic roles produced by
the SRL (which are treebank-dependent, and thus,
not the same for all languages). The used WSD
engine is the UKB (Agirre and Soroa, 2009) im-
plementation provided by FreeLing (Padr?o and
Stanilovsky, 2012).
2.4 Frame Extraction
The final step is to convert all the gathered linguis-
tic information into a semantic representation. Our
method is based on the notion of frame: a seman-
tic frame is a schematic representation of a situ-
ation involving various participants. In a frame,
each participant plays a role. There is a direct cor-
respondence between roles in a frame and seman-
tic roles; namely, frames correspond to predicates,
and participants correspond to the arguments of
the predicate. We distinguish three types of par-
ticipants: entities, words, and frames.
Entities are nodes in the graph connected to
real-world entities as described in Section 3.
Words are common words or concepts, linked to
general ontologies such as WordNet. Frames cor-
respond to events or predicates described in the
document. Figure 1 shows an example sentence,
the extracted frames and their arguments.
It is important to note that frames are a more
general representation than SVO-triples. While
SVO-triples represent a binary relation between
two participants, frames can represent n-ary rela-
tions (e.g. predicates with more than two argu-
ments, or with adjuncts). Frames also allow repre-
senting the sentences where one of the arguments
is in turn a frame (as is the case with plan to make
in the example).
Finally, although frames are extracted at sen-
tence level, the resulting graphs are aggregated
in a single semantic graph representing the whole
document via a very simple coreference resolution
based on detecting named entity aliases and repe-
titions of common nouns. Future improvements
include using an state-of-the-art coreference reso-
lution module for languages where it is available.
3 Cross-lingual Semantic Annotation
This step adds further semantic annotations on top
of the results obtained by linguistic processing.
All XLike languages are covered. The goal is
to map word phrases in different languages into
the same semantic interlingua, which consists of
resources specified in knowledge bases such as
Wikipedia and Linked Open Data (LOD) sources.
Cross-lingual semantic annotation is performed in
two stages: (1) first, candidate concepts in the
knowledge base are linked to the linguistic re-
sources based on a newly developed cross-lingual
linked data lexica, called xLiD-Lexica, (2) next
the candidate concepts get disambiguated based
on the personalized PageRank algorithm by utiliz-
ing the structure of information contained in the
knowledge base.
The xLiD-Lexica is stored in RDF format and
contains about 300 million triples of cross-lingual
groundings. It is extracted from Wikipedia dumps
of July 2013 in English, German, Spanish, Cata-
lan, Slovenian and Chinese, and based on the
canonicalized datasets of DBpedia 3.8 contain-
ing triples extracted from the respective Wikipedia
whose subject and object resource have an equiv-
alent English article.
4 Web Service Architecture Approach
The different language functionalities are imple-
mented following the service oriented architec-
ture (SOA) approach defined in the project XLike.
Therefore all the pipelines (one for each language)
have been implemented as web services and may
10
Figure 1: Graphical representation of frames in the sentence Acme, based in New York, now plans to
make computer and electronic products.
be requested to produce different levels of analy-
sis (e.g. tokenization, lemmatization, NERC, pars-
ing, relation extraction). This approach is very ap-
pealing due to the fact that it allows to treat ev-
ery language independently and execute the whole
language analysis process at different threads or
computers allowing an easier parallelization (e.g.
using external high perfomance platforms such as
Amazon Elastic Compute Cloud EC2
5
) as needed.
Furthermore it also provides independent develop-
ment lifecycles for each language which is crucial
in this type of research projects. Recall that these
web services can be deployed locally or remotely,
maintaining the option of using them in a stand-
alone configuration.
The main structure for each one of the pipelines
is described below:
? Spanish, English, and Catalan: all mod-
ules are based on FreeLing (Padr?o and
Stanilovsky, 2012) and Treeler.
? German: German shallow processing is
based on OpenNLP
6
, Stanford POS tagger
and NE extractor (Toutanova et al., 2003;
Finkel et al., 2005). Dependency parsing,
semantic role labeling, word sense disam-
biguation, and SRL-based frame extraction
are based on FreeLing and Treeler.
? Slovene: Slovene shallow processing is pro-
vided by JSI Enrycher
7
(
?
Stajner et al., 2010),
which consists of the Obeliks morphosyntac-
tic analysis library (Gr?car et al., 2012), the
LemmaGen lemmatizer (Jur?si?c et al., 2010)
and a CRF-based entity extractor (
?
Stajner et
al., 2012). Dependency parsing, word sense
5
http://aws.amazon.com/ec2/
6
http://opennlp.apache.org
7
http://enrycher.ijs.si
disambiguation are based on FreeLing and
Treeler. Frame extraction is rule-based since
no SRL corpus is available for Slovene.
? Croatian: Croatian shallow processing is
based on proprietary tokenizer, POS/MSD-
tagging and lemmatisaton system (Agi?c et
al., 2008), NERC system (Bekavac and
Tadi?c, 2007) and dependency parser (Agi?c,
2012). Word sense disambiguation is based
on FreeLing. Frame extraction is rule-based
since no SRL corpus is available for Croatian.
? Chinese: Chinese shallow and deep process-
ing is based on a word segmentation compo-
nent ICTCLAS
8
and a semantic dependency
parser trained on CSDN corpus. Then, rule-
based frame extraction is performed (no SRL
corpus nor WordNet are available for Chi-
nese).
Each language analysis service is able to pro-
cess thousands of words per second when per-
forming shallow analysis (up to NE recognition),
and hundreds of words per second when produc-
ing the semantic representation based on full anal-
ysis. Moreover, the web service architecture en-
ables the same server to run a different thread for
each client, thus taking advantage of multiproces-
sor capabilities.
The components of the cross-lingual semantic
annotation stage are:
? xLiD-Lexica: The cross-lingual groundings
in xLiD-Lexica are translated into RDF data
and are accessible through a SPARQL end-
point, based on OpenLink Virtuoso
9
as the
back-end database engine.
8
http://ictclas.org/
9
http://virtuoso.openlinksw.com/
11
? Semantic Annotation: The cross-lingual se-
mantic annotation service is based on the
xLiD-Lexica for entity mention recognition
and the JUNG Framework
10
for graph-based
disambiguation.
5 Conclusion
We presented the web service based architecture
used in XLike FP7 project to linguistically ana-
lyze large amounts of documents in seven differ-
ent languages. The analysis pipelines perform ba-
sic processing as tokenization, PoS-tagging, and
named entity extraction, as well as deeper analy-
sis such as dependency parsing, word sense disam-
biguation, and semantic role labelling. The result
of these linguistic analyzers is a semantic graph
capturing the main events described in the docu-
ment and their core participants.
On top of that, the cross-lingual semantic an-
notation component links the resulting linguistic
resources in one language to resources in a knowl-
edge bases in any other language or to language
independent representations. This semantic repre-
sentation is later used in XLike for document min-
ing purposes such as enabling cross-lingual ser-
vices for publishers, media monitoring or devel-
oping new business intelligence applications.
The described analysis services are currently
available via META-SHARE as callable RESTful
services.
Acknowledgments
This work was funded by the European Union
through project XLike (FP7-ICT-2011-288342).
References
?
Zeljko Agi?c, Marko Tadi?c, and Zdravko Dovedan.
2008. Improving part-of-speech tagging accuracy
for Croatian by morphological analysis. Informat-
ica, 32(4):445?451.
?
Zeljko Agi?c. 2012. K-best spanning tree dependency
parsing with verb valency lexicon reranking. In Pro-
ceedings of COLING 2012: Posters, pages 1?12,
Mumbai, India, December. The COLING 2012 Or-
ganizing Committee.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th conference of the European
chapter of the Association for Computational Lin-
guistics (EACL-2009), Athens, Greece.
10
Java Universal Network/Graph Framework
http://jung.sourceforge.net/
Bo?zo Bekavac and Marko Tadi?c. 2007. Implementa-
tion of Croatian NERC system. In Proceedings of
the Workshop on Balto-Slavonic Natural Language
Processing (BSNLP2007), Special Theme: Informa-
tion Extraction and Enabling Technologies, pages
11?18. Association for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL?05), pages 363?370.
Miha Gr?car, Simon Krek, and Kaja Dobrovoljc. 2012.
Obeliks: statisti?cni oblikoskladenjski ozna?cevalnik
in lematizator za slovenski jezik. In Zbornik Osme
konference Jezikovne tehnologije, Ljubljana, Slove-
nia.
Matjaz Jur?si?c, Igor Mozeti?c, Tomaz Erjavec, and Nada
Lavra?c. 2010. Lemmagen: Multilingual lemmati-
sation with induced ripple-down rules. Journal of
Universal Computer Science, 16(9):1190?1214.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics, 1:219?230.
Maddalen L?opez de la Calle, Egoitz Laparra, and Ger-
man Rigau. 2014. First steps towards a predicate
matrix. In Proceedings of the Global WordNet Con-
ference (GWC 2014), Tartu, Estonia, January. GWA.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 91?98, Ann Ar-
bor, Michigan, June.
Llu??s Padr?o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Tadej
?
Stajner, Delia Rusu, Lorand Dali, Bla?z Fortuna,
Dunja Mladeni?c, and Marko Grobelnik. 2010. A
service oriented framework for natural language text
enrichment. Informatica, 34(3):307?313.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Lin- guistics on Human Language Technology
(NAACL?03).
Tadej
?
Stajner, Toma?z Erjavec, and Simon Krek.
2012. Razpoznavanje imenskih entitet v slovenskem
besedilu. In In Proceedings of 15th Internation
Multiconference on Information Society - Jezikovne
Tehnologije, Ljubljana, Slovenia.
12
Transactions of the Association for Computational Linguistics, 1 (2013) 219?230. Action Editor: Brian Roark.
Submitted 1/2013; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Joint Arc-factored Parsing of Syntactic and Semantic Dependencies
Xavier Llu??s and Xavier Carreras and Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
Jordi Girona 1?3, 08034 Barcelona
{xlluis,carreras,lluism}@lsi.upc.edu
Abstract
In this paper we introduce a joint arc-factored
model for syntactic and semantic dependency
parsing. The semantic role labeler predicts
the full syntactic paths that connect predicates
with their arguments. This process is framed
as a linear assignment task, which allows
to control some well-formedness constraints.
For the syntactic part, we define a standard
arc-factored dependency model that predicts
the full syntactic tree. Finally, we employ dual
decomposition techniques to produce consis-
tent syntactic and predicate-argument struc-
tures while searching over a large space of
syntactic configurations. In experiments on
the CoNLL-2009 English benchmark we ob-
serve very competitive results.
1 Introduction
Semantic role labeling (SRL) is the task of identi-
fying the arguments of lexical predicates in a sen-
tence and labeling them with semantic roles (Gildea
and Jurafsky, 2002; Ma`rquez et al, 2008). SRL is
an important shallow semantic task in NLP since
predicate-argument relations directly represent se-
mantic properties of the type ?who? did ?what? to
?whom?, ?how?, and ?why? for events expressed by
predicates (typically verbs and nouns).
Predicate-argument relations are strongly related
to the syntactic structure of the sentence: the ma-
jority of predicate arguments correspond to some
syntactic constituent, and the syntactic structure that
connects an argument with the predicate is a strong
indicator of its semantic role. Actually, semantic
roles represent an abstraction of the syntactic form
of a predicative event. While syntactic functions of
arguments change with the form of the event (e.g.,
active vs. passive forms), the semantic roles of argu-
ments remain invariant to their syntactic realization.
Consequently, since the first works, SRL systems
have assumed access to the syntactic structure of the
sentence (Gildea and Jurafsky, 2002; Carreras and
Ma`rquez, 2005). A simple approach is to obtain
the parse trees as a pre-process to the SRL system,
which allows the use of unrestricted features of the
syntax. However, as in other pipeline approaches in
NLP, it has been shown that the errors of the syn-
tactic parser severely degrade the predictions of the
SRL model (Gildea and Palmer, 2002). A common
approach to alleviate this problem is to work with
multiple alternative syntactic trees and let the SRL
system optimize over any input tree or part of it
(Toutanova et al, 2008; Punyakanok et al, 2008).
As a step further, more recent work has proposed
parsing models that predict syntactic structure aug-
mented with semantic predicate-argument relations
(Surdeanu et al, 2008; Hajic? et al, 2009; Johansson,
2009; Titov et al, 2009; Llu??s et al, 2009), which is
the focus of this paper. These joint models should
favor the syntactic structure that is most consistent
with the semantic predicate-argument structures of
a sentence. In principle, these models can exploit
syntactic and semantic features simultaneously, and
could potentially improve the accuracy for both syn-
tactic and semantic relations.
One difficulty in the design of joint syntactic-
semantic parsing models is that there exist impor-
tant structural divergences between the two layers.
219
Mary loves to play guitar .
?
SBJ OPRD IM OBJ
P
ARG0 ARG1
ARG0
ARG1
Figure 1: An example . . .
Das et al (2012) . . .
Riedel and McCallum (2011) . . .
3 A Syntactic-Semantic Dependency
Model
We will describe structures of syntactic and seman-
tic dependencies with vectors of binary variables.
We will denote by yh,m,l a syntactic dependency
from head token h to dependant token m labeled
with syntactic function l. Similarly we will denote
by zp,a,r a semantic dependency between predicate
token p and argument token a labeled with seman-
tic role r. We will use y and z to denote vectors of
binary variables indexed by syntactic and semantic
dependencies, respectively.
A joint model for syntactic and semantic depen-
dency parsing could be defined as:
argmax
y,z
s syn(x,y) + s srl(x, z,y) .
In the equation, s syn(x,y) gives a score for the
syntactic tree y. In the literature, it is standard to
use arc-factored models defined as
s syn(x,y) =
?
yh,m,l=1
s syn(x, h,m, l) ,
where we overload s syn to be a function that
computes scores for individual labeled syntactic
dependencies. In discriminative models one has
s syn(x, h,m, l) = wsyn ? fsyn(x, h,m, l), where
fsyn is a feature vector for the syntactic dependency
and wsyn is a vector of parameters (McDonald et al,
2005).
The other term, s srl(x, z,y), gives a score for
a semantic dependency structure using the syntactic
structure y as features. Previous work has empiri-
cally proved the importance of exploiting syntactic
features in the semantic component (Gildea and Ju-
rafsky, 2002; Xue and Palmer, 2004; Punyakanok et
al., 2008). However, without further assumptions,
this property makes the optimization problem com-
putationally hard. One simple approximation is to
use a pipeline model: first compute the optimal syn-
tactic tree, and then optimize for the best semantic
structure given the syntactic tree. In the rest of the
paper we describe a method that searches over syn-
tactic and semantic dependency structures jointly.
We first impose the assumption that syntactic fea-
tures of the semantic component are restricted to the
syntactic path between a predicate and an argument,
following previous work (Johansson, 2009). For-
mally, for a predicate p, argument a and role r we
will define a vector of dependency indicators ?p,a,r
similar to the ones above: ?p,a,rh,m,l indicates if a de-
pendency ?h,m, l? is part of the syntactic path that
links predicate p with token a. Figure 1 gives an ex-
ample of one such paths. Given full syntactic and
semantic structures y and z it is trivial to construct a
vector ? that is a concatenation of vectors ?p,a,r for
all ?p, a, r? in z. We can now define a linear seman-
tic model as
s srl(x, z,?) =
?
zp,a,r=1
s srl(x, p, a, r,?p,a,r) ,
(1)
where s srl computes a score for a semantic de-
pendency ?p, a, r? together with its syntactic path
?p,a,r. As in the syntactic component, this function
is typically defined as a linear function over a set of
features of the semantic dependency and its path.
With this joint model, the inference problem can
be formulated as:
argmax
y,z,?
s syn(x,y) + s srl(x, z,?) (2)
subject to
cTree : y is a valid dependency tree
cRole : ?p, r :
?
a
zp,a,r ? 1
cArg : ?p, a :
?
r
zp,a,r ? 1
cPath : ?p, a, r : if zp,a,r = 1 then
?p,a,r is a path from p to a
otherwise ?p,a,r = 0
cSubtree : ?p, r, a : ?p,r,a is a subtree of y
Figure 1: A sentenc with synt ctic dependencies (top)
and semantic dependencies for the predicates ?loves? and
?play? (bottom). The thick arcs illustrate a structural di-
vergence where the argument ?Mary? is linked to ?play?
with a path involving three syntactic dependencies.
This is cl arly seen in dependency-based representa-
tions of syntax and semantic roles (Surdeanu et al,
2008), such as in the example in Figure 1: the con-
struct ?loves to? causes the argument ?Mary? to be
syntactically distant fro the predicate ?play?. Lin-
guistic phenomena such as auxiliary verbs, control
and raising, typically result in syntactic structures
where semantic arguments are not among the direct
depend ts of their predicate ?e.g., abou 25% of
arguments are distant in the English development set
of the CoNLL-2009 shared task. Besides, standard
models for dependency parsing crucially depend on
arc factorizations of the dependency structure (Mc-
Donald et al, 2005; Nivre and Nilsson, 2005), other-
wise their computational properties break. Hence, it
is challenging to define efficient methods for syntac-
tic and semantic dependency parsing that can exploit
features of both layers simultaneously. In this paper
we propose a method for this joint task.
In our method we define predicate-centric seman-
tic models that, rather than predicting just the ar-
gument that realizes each semantic role, they pre-
dict the full syntactic path that connects the predi-
cate with the argument. We show how efficient pre-
dictions with these models can be made using as-
signment algorithms in bipartite graphs. Simulta-
neously, we use a standard arc-factored dependency
model that predicts the full syntactic tree of the sen-
tence. Finally, we employ dual decomposition tech-
niques (Koo et al, 2010; Rush et al, 2010; Sontag
et al, 2010) to find agreement between the full de-
pendency tree and the partial syntactic trees linking
each predicate with its arguments. In summary, the
main contributions of this paper are:
? We frame SRL as a weighted assignment prob-
lem in a bipartite graph. Under this framework
we can control assignment constraints between
roles and arguments. Key to our method, we
c n efficiently search over a large space of syn-
actic realizations of se antic argument .
? We solve joint inference of syntactic and se-
mantic dependencies with a dual decomposi-
tion method, similar to that of Koo et al (2010).
Our system produces consistent syntactic and
predicate-argument structures while searching
over a large space of syntactic configurations.
In the experimental section we compare joint
and pipeline models. The final results of our joint
syntactic-semantic system are competitive with the
state-of-the-art and improve over the best results
published by a joint method on the CoNLL-2009
English dataset.
2 A Syntactic-S mantic Dependency
Model
We first describe how we represent structures of syn-
tactic and semantic dependencies like the one in Fig-
ure 1. Throughout the paper, we will assume a fixed
input sentence x with n tokens where lexical predi-
cates are marked. We will also assume fixed sets of
syntactic functions Rsyn and semantic roles Rsem.
We will represent depencency structures using vec-
tors of binary variables. A variable yh,m,l will in-
dicate the presence of a syntactic dependency from
head token h to dependant tokenm labeled with syn-
tactic function l. Then, a syntactic tree will be de-
noted as a vector y of variables indexed by syntactic
dependencies. Similarly, a variable zp,a,r will indi-
cate the presence of a semantic dependency between
predicate token p and argument token a labeled with
semantic role r. We will represent a semantic role
structure as a vector z indexed by semantic depen-
dencies. Whenever we enumerate syntactic depen-
dencies ?h,m, l? we will assume that they are in the
valid range for x, i.e. 0 ? h ? n, 1 ? m ? n,
h 6= m and l ? Rsyn where h = 0 stands for a
special root token. Similarly, for semantic depen-
dencies ?p, a, r? we will assume that p points to a
predicate of x, 1 ? a ? n and r ? Rsem.
220
A joint model for syntactic and semantic depen-
dency parsing could be defined as:
argmax
y,z
s syn(x,y) + s srl(x, z,y) . (1)
In the equation, s syn(x,y) gives a score for the
syntactic tree y. In the literature, it is standard to
use arc-factored models defined as
s syn(x,y) =
?
yh,m,l=1
s syn(x, h,m, l) , (2)
where we overload s syn to be a function that
computes scores for individual syntactic depen-
dencies. In linear discriminative models one has
s syn(x, h,m, l) = wsyn ? fsyn(x, h,m, l), where
fsyn is a feature vector for a syntactic dependency
and wsyn is a vector of parameters (McDonald et
al., 2005). In Section 6 we describe how we trained
score functions with discriminative methods.
The other term in Eq. 1, s srl(x, z,y), gives a
score for a semantic dependency structure z using
features of the syntactic structure y. Previous work
has empirically proved the importance of exploit-
ing syntactic features in the semantic component
(Gildea and Jurafsky, 2002; Xue and Palmer, 2004;
Punyakanok et al, 2008). However, without further
assumptions, this property makes the optimization
problem computationally hard. One simple approx-
imation is to use a pipeline model: first compute the
optimal syntactic tree y, and then optimize for the
best semantic structure z given y. In the rest of the
paper we describe a method that searches over syn-
tactic and semantic dependency structures jointly.
We first note that for a fixed semantic dependency,
the semantic component will typically restrict the
syntactic features representing the dependency to a
specific subtree of y. For example, previous work
has restricted such features to the syntactic path that
links a predicate with an argument (Moschitti, 2004;
Johansson, 2009), and in this paper we employ this
restriction. Figure 1 gives an example of a sub-
tree, where we highlight the syntactic path that con-
nects the semantic dependency between ?play? and
?Mary? with role ARG0.
Formally, for a predicate p, argument a and role
r we define a local syntactic subtree pip,a,r repre-
sented as a vector: pip,a,rh,m,l indicates if a dependency
?h,m, l? is part of the syntactic path that links pred-
icate p with token a and role r.1 Given full syntactic
and semantic structures y and z it is trivial to con-
struct a vector pi that concatenates vectors pip,a,r for
all ?p, a, r? in z. The semantic model becomes
s srl(x, z,pi) =
?
zp,a,r=1
s srl(x, p, a, r,pip,a,r) ,
(3)
where s srl computes a score for a semantic de-
pendency ?p, a, r? together with its syntactic path
pip,a,r. As in the syntactic component, this function
is typically defined as a linear function over a set of
features of the semantic dependency and its path.
The inference problem of our joint model is:
argmax
y,z,pi
s syn(x,y) + s srl(x, z,pi) (4)
subject to
cTree : y is a valid dependency tree
cRole : ?p, r : ?
a
zp,a,r ? 1
cArg : ?p, a : ?
r
zp,a,r ? 1
cPath : ?p, a, r : if zp,a,r = 1 then
pip,a,r is a path from p to a,
otherwise pip,a,r = 0
cSubtree : ?p, a, r : pip,a,r is a subtree of y
Constraint cTree dictates that y is a valid depen-
dency tree; see (Martins et al, 2009) for a detailed
specification. The next two sets of constraints con-
cern the semantic structure only. cRole imposes that
each semantic role is realized at most once.2 Con-
versely, cArg dictates that an argument can realize
at most one semantic role in a predicate. The final
two sets of constraints model the syntactic-semantic
interdependencies. cPath imposes that each pip,a,r
represents a syntactic path between p and a when-
ever there exists a semantic relation. Finally, cSub-
tree imposes that the paths in pi are consistent with
the full syntactic structure, i.e. they are subtrees.
1In this paper we say that structures pip,a,r are paths from
predicates to arguments, but they could be more general sub-
trees. The condition to build a joint system is that these subtrees
must be parseable in the way we describe in Section 3.1.
2In general a semantic role can be realized with more than
one argument, though it is rare. It is not hard to modify our
framework to allow for a maximum number of occurrences of a
semantic role.
221
In Section 3 we define a process that optimizes
the semantic structure ignoring constraint cSubtree.
Then in Section 4 we describe a dual decomposition
method that uses the first process repeatedly to solve
the joint problem.
3 SRL as Assignment
In this section we frame the problem of finding se-
mantic dependencies as a linear assignment task.
The problem we optimize is:
argmax
z,pi
s srl(x, z,pi) (5)
subject to cRole, cArg, cPath
In this case we dropped the full syntactic structure
y from the optimization in Eq. 4, as well as the
corresponding constraints cTree and cSubtree. As
a consequence, we note that the syntactic paths pi
are not tied to any consistency constraint other than
each of the paths being a well-formed sequence of
dependencies linking the predicate to the argument.
In other words, the optimal solution in this case does
not guarantee that the set of paths from a predicate to
all of its arguments satisfies tree constraints. We first
describe how these paths can be optimized locally.
Then we show how to find a solution z satisfying
cRole and cArg using an assignment algorithm.
3.1 Local Optimization of Syntactic Paths
Let z? and p?i be the optimal values of Eq. 5. For any
?p, a, r?, let
p?ip,a,r = argmax
pip,a,r
s srl(x, p, a, r,pip,a,r) . (6)
For any ?p, a, r? such that z?p,a,r = 1 it has to be that
p?ip,a,r = p?ip,a,r. If this was not true, replacing p?ip,a,r
with p?ip,a,r would improve the objective of Eq. 5
without violating the constraints, thus contradicting
the hypothesis about optimality of p?i. Therefore, for
each ?p, a, r? we can optimize its best syntactic path
locally as defined in Eq. 6.
In this paper, we will assume access to a list of
likely syntactic paths for each predicate p and argu-
ment candidate a, such that the optimization in Eq. 6
can be solved explicitly by looping over each path in
the list. The main advantage of this method is that,
since paths are precomputed, our model can make
unrestricted use of syntactic path features.
(1)
Mary
(2)
plays
(3)
guitar (4)NULL (5)NULL (6)NULL
(1)
ARG0
(2)
ARG1
(3)
ARG2 (4)NULL (5)NULL (6)NULL
W1,1
W4,2W2,3
W3,4
W5,5 W6,6
Figure 2: Illustration of the assignment graph for the sen-
tence ?Mary plays guitar?, where the predicate ?plays?
can have up to three roles: ARG0 (agent), ARG1 (theme)
and ARG2 (benefactor). Nodes labeled NULL represent
a null role or token. Highlighted edges are the correct
assignment.
It is simple to employ a probabilistic syntactic de-
pendency model to create the list of likely paths for
each predicate-argument pair. In the experiments we
explore this approach and show that with an average
of 44 paths per predicate we can recover 86.2% of
the correct paths.
We leave for future work the development of ef-
ficient methods to recover the most likely syntactic
structure linking an argument with its predicate.
3.2 The Assignment Algorithm
Coming back to solving Eq. 5, it is easy to see
that an optimal solution satisfying constraints cRole
and cArg can be found with a linear assignment
algorithm. The process we describe determines
the predicate-argument relations separately for each
predicate. Assume a bipartite graph of size N with
role nodes r1 . . . rN on one side and argument nodes
a1 . . . aN on the other side. Assume also a matrix of
non-negative scoresWi,j corresponding to assigning
argument aj to role ri. A linear assignment algo-
rithm finds a bijection f : i? j from roles to argu-
ments that maximizes?Ni=1Wi,f(i). The Hungarian
algorithm finds the exact solution to this problem in
O(N3) time (Kuhn, 1955; Burkard et al, 2009).
All that is left is to construct a bipartite graph rep-
resenting predicate roles and sentence tokens, such
that some roles and tokens can be left unassigned,
which is a common setting for assignment tasks. Al-
gorithm 1 describes a procedure for constructing a
weighted bipartite graph for SRL, and Figure 2 il-
lustrates an example of a bipartite graph. We then
222
Algorithm 1 Construction of an Assignment Graph
for Semantic Role Labeling
Let p be a predicate with k possible roles. Let n be the
number of argument candidates in the sentence. This al-
gorithm creates a bipartite graph withN = n+k vertices
on each side.
1. Create role vertices ri for i = 1 . . . N , where
? for 1 ? i ? k, ri is the i-th role,
? for 1 ? i ? n, rk+i is a special NULL role.
2. Create argument vertices aj for j = 1 . . . N , where
? for 1 ? j ? n, aj is the j-th argument candidate,
? for 1 ? j ? k, an+j is a special NULL argument.
3. Define a matrix of model scores S ? R(k+1)?n:
(a) Optimization of syntactic paths:
For 1 ? i ? k, 1 ? j ? n
Si,j = max
pip,aj,ri
s srl(x, p, aj , ri,pip,aj ,ri)
(b) Scores of NULL assignments3:
For 1 ? j ? n
Sk+1,j = 0
4. Let S0 = mini,j Si,j , the minimum of any score
in S. Define a matrix of non-negative scores W ?
RN?N as follows:
(a) for 1 ? i ? k, 1 ? j ? n
Wi,j = Si,j ? S0
(b) for k < i ? N, 1 ? j ? n
Wi,j = Sk+1,j ? S0
(c) for 1 < i ? N, n < j ? N
Wi,j = 0
run the Hungarian algorithm on the weighted graph
and obtain a bijection f : ri ? aj , from which it
is trivial to recover the optimal solution of Eq. 5.
Finally, we note that it is simple to allow for multi-
ple instances of a semantic role by adding more role
nodes in step 1; it would be straightforward to add
penalties in step 3 for multiple instances of roles.
4 A Dual Decomposition Algorithm
We now present a dual decomposition method to op-
timize Eq. 4, that uses the assignment algorithm pre-
sented above as a subroutine. Our method is sim-
ilar to that of Koo et al (2010), in the sense that
3In our model we fix the score of null assignments to 0. It is
straightforward to compute a discriminative score instead.
our joint optimization can be decomposed into two
sub-problems that need to agree on the syntactic
dependencies they predict. For a detailed descrip-
tion of dual decomposition methods applied to NLP
see (Sontag et al, 2010; Rush et al, 2010).
We note that in Eq. 4 the constraint cSubtree ties
the syntactic and semantic structures, imposing that
any path pip,a,r that links a predicate p with an argu-
ment a must be a subtree of the full syntactic struc-
ture y. Formally the set of constraints is:
yh,m,l ? pip,a,rh,m,l ? p, a, r, h,m, l .
These constraints can be compactly written as
c ? yh,m,l ?
?
p,a,r
pip,a,rh,m,l ? h,m, l ,
where c is a constant equal to the number of dis-
tinct semantic dependencies ?p, a, r?. In addition,
we can introduce a vector non-negative slack vari-
ables ? with a component for each syntactic depen-
dency ?h,m,l, turning the constraints into:
c ? yh,m,l ?
?
p,a,r
pip,a,rh,m,l ? ?h,m,l = 0 ? h,m, l
We can now rewrite Eq. 4 as:
argmax
y,z,pi,??0
s syn(x,y) + s srl(x, z,pi) (7)
subject to
cTree, cRole, cArg, cPath
?h,m, l : c ? yh,m,l ?
?
p,a,r
pip,a,rh,m,l ? ?h,m,l = 0
As in Koo et al (2010), we will relax subtree cons-
traints by introducing a vector of Lagrange multipli-
ers ? indexed by syntactic dependencies, i.e. each
coordinate ?h,m,l is a Lagrange multiplier for the
constraint associated with ?h,m, l?. The Lagrangian
of the problem is:
L(y, z,pi, ?,?)= s syn(x,y) + s srl(x, z,pi)
+ ? ?
(
c ? y ?
?
p,a,r
pip,a,r ? ?
)
(8)
We can now formulate Eq. 7 as:
max
y,z,pi,??0
s.t. cTree,cRole,cArg,cPath
c?y??p,a,r pip,a,r??=0
L(y, z,pi, ?,?) (9)
223
This optimization problem has the property that its
optimum value is the same as the optimum of Eq. 7
for any value of ?. This is because whenever the
constraints are satisfied, the terms in the Lagrangian
involving ? are zero. If we remove the subtree con-
straints from Eq. 9 we obtain the dual objective:
D(?) = max
y,z,pi,??0
s.t. cTree,cRole,cArg,cPath
L(y, z,pi, ?,?) (10)
= max
y s.t. cTree
(
s syn(x,y) + c ? y ? ?
)
+ maxz,pi
s.t. cRole,cArg,cPath
(
s srl(x, z,pi)? ? ?
?
p,a,r
pip,a,r
)
+ max
??0
(?? ? ?) (11)
The dual objective is an upper bound to the opti-
mal value of primal objective of Eq. 7. Thus, we
are interested in finding the minimum of the dual in
order to tighten the upper-bound. We will solve
min
?
D(?) (12)
using a subgradient method. Algorithm 2 presents
pseudo-code. The algorithm takes advantage of the
decomposed form of the dual in Eq. 11, where
we have rewritten the Lagrangian such that syntac-
tic and semantic structures appear in separate terms.
This will allow to compute subgradients efficiently.
In particular, the subgradient of D at a point ? is:
?(?) = c ? y? ?
?
p,a,r
p?ip,a,r ? ?? (13)
where
y? = argmax
y s.t. cTree
(
s syn(x,y) + c ? y ? ?
) (14)
z?, p?i = argmax
z,pi s.t.
cRole,cArg,cPath
s srl(x, z,pi)? ??
?
p,a,r
pip,a,r (15)
?? = argmax
??0
?? ? ? (16)
Whenever p?i is consistent with y? the subgradient
will be zero and the method will converge. When
paths p?i contain a dependency ?h,m, l? that is in-
consistent with y?, the associated dual ?h,m,l will in-
crease, hence lowering the score of all paths that use
?h,m, l? at the next iteration; at same time, the to-
tal score for that dependency will increase, favoring
syntactic dependency structures alternative to y?. As
Algorithm 2 A dual-decomposition algorithm for
syntactic-semantic dependency parsing
Input: x, a sentence; T , number of iterations;
Output: syntactic and semantic structures y? and z?
Notation: we use cSem= cRole ? cArg ? cPath
1: ?1 = 0 # initialize dual variables
2: c =number of distinct ?h,m, l? in x
3: for t = 1 . . . T do
4: y? = argmaxy s.t. cTree
(
s syn(x,y) + c ? ?t ? y
)
5: z?, p?i = argmax z,pi
s.t. cSem
(
s srl(x, z,pi)
??t ??p,a,r pip,a,r
)
6: ?t+1 = ?t # dual variables for the next iteration
7: Set ?t, the step size of the current iteration
8: for each ?h,m, l? do
9: q = ?p,a,r ?p,a,rh,m,l # num. paths using ?h,m, l?
10: if q > 0 and y?h,m,l = 0 then
11: ?t+1h,m,l = ?t+1h,m,l + ?tq
12: break if ?t+1 = ?t # convergence
13: return y?, z?
in previous work, in the algorithm a parameter ?t
controls the size of subgradient steps at iteration t.
The key point of the method is that solutions to
Eq. 14 and 15 can be computed efficiently using sep-
arate processes. In particular, Eq. 14 corresponds
to a standard dependency parsing problem, where
for each dependency ?h,m, l? we have an additional
score term c ??h,m,l ?in our experiments we use the
projected dependency parsing algorithm by (Eisner,
2000). To calculate Eq. 15 we use the assignment
method described in Section 3, where it is straight-
forward to introduce additional score terms ??h,m,l
to every factor pip,a,rh,m,l. It can be shown that wheneverthe subgradient method converges, the solutions y?
and z? are the optimal solutions to our original prob-
lem in Eq. 4 (see (Koo et al, 2010) for a justifi-
cation). In practice we run the subgradient method
for a maximum number of iterations, and return the
solutions of the last iteration if it does not converge.
5 Related Work
Recently, there have been a number of approaches
to joint parsing of syntactic and semantic dependen-
cies, partly because of the availability of treebanks in
this format popularized by the CoNLL shared tasks
(Surdeanu et al, 2008; Hajic? et al, 2009).
Like in our method, Johansson (2009) defined a
model that exploits features of a semantic depen-
224
dency together with the syntactic path connecting
the predicate and the argument. That method uses an
approximate parsing algorithm that employs k-best
inference and beam search. Similarly, Llu??s et al
(2009) defined a joint model that forces the predi-
cate structure to be represented in the syntactic de-
pendency tree, by enriching arcs with semantic in-
formation. The semantic component uses features of
pre-computed syntactic structures that may diverge
from the joint structure. In contrast, our joint pars-
ing method is exact whenever the dual decomposi-
tion algorithm converges.
Titov et al (2009) augmented a transition-based
dependency parser with operations that produce
synchronous derivations of syntactic and semantic
structures. Instead of explicitly representing seman-
tic dependencies together with a syntactic path, they
induce latent representations of the interactions be-
tween syntactic and semantic layers.
In all works mentioned the model has no con-
trol of assignment constraints that disallow label-
ing multiple arguments with the same semantic role.
Punyakanok et al (2008) first introduced a system
that explicitly controls these constraints, as well as
other constraints that look at pairwise assignments
which we can not model. They solve SRL using
general-purpose Integer Linear Programming (ILP)
methods. In similar spirit, Riedel and McCallum
(2011) presented a model for extracting structured
events that controls interactions between predicate-
argument assignments. They take into account pair-
wise assignments and solve the optimization prob-
lem with dual decomposition. More recently, Das
et al (2012) proposed a dual decomposition method
that deals with several assignment constraints for
predicate-argument relations. Their method is an
alternative to general ILP methods. To our knowl-
edge, our work is the first that frames SRL as a linear
assignment task, for which simple and exact algo-
rithms exist. We should note that these works model
predicate-argument relations with assignment con-
straints, but none of them predicts the underlying
syntactic structure.
Our dual decomposition method follows from that
of Koo et al (2010). In both cases two separate pro-
cesses predict syntactic dependency structures, and
the dual decomposition algorithm seeks agreement
at the level of individual dependencies. One dif-
ference is that our semantic process predicts partial
syntax (restricted to syntactic paths connecting pred-
icates and arguments), while in their case each of the
two processes predicts the full set of dependencies.
6 Experiments
We present experiments using our syntactic-
semantic parser on the CoNLL-2009 Shared Task
English benchmark (Hajic? et al, 2009). It consists
of the usual WSJ training/development/test sections
mapped to dependency trees, augmented with se-
mantic predicate-argument relations from PropBank
(Palmer et al, 2005) and NomBank (Meyers et al,
2004) also represented as dependencies. It also con-
tains a PropBanked portion of the Brown corpus as
an out-of-domain test set.
Our goal was to evaluate the contributions of pars-
ing algorithms in the following configurations:
Base Pipeline Runs a syntactic parser and then runs
an SRL parser constrained to paths of the best
syntactic tree. In the SRL it only enforces con-
straint cArg, by simply classifying the candi-
date argument in each path into one of the pos-
sible semantic roles or as NULL.
Pipeline with Assignment Runs the assignment al-
gorithm for SRL, enforcing constraints cRole
and cArg, but constrained to paths of the best
syntactic tree.
Forest Runs the assignment algorithm for SRL on
a large set of precomputed syntactic paths, de-
scribed below. This configuration corresponds
to running Dual Decomposition for a single it-
eration, and is not guaranteed to predict consis-
tent syntactic and semantic structures.
Dual Decomposition (DD) Runs dual decomposi-
tion using the assignment algorithm on the set
of precomputed paths. Syntactic and semantic
structures are consistent when it reaches con-
vergence.
All four systems used the same type of discrimina-
tive scorers and features. Next we provide details
about these systems. Then we present the results.
6.1 Implementation
Syntactic model We used two discriminative arc-
factored models for labeled dependency parsing: a
225
first-order model, and a second-order model with
grandchildren interactions, both reimplementations
of the parsers by McDonald et al (2005) and Car-
reras (2007) respectively. In both cases we used
projective dependency parsing algorithms based on
(Eisner, 2000).4 To learn the models, we used a
log-linear loss function following Koo et al (2007),
which trains probabilistic discriminative parsers.
At test time, we used the probabilistic parsers to
compute marginal probabilities p(h,m, l | x), us-
ing inside-outside algorithms for first/second-order
models. Hence, for either of the parsing models, we
always obtain a table of first-order marginal scores,
with one score per labeled dependency. Then we
run first-order inference with these marginals to ob-
tain the best tree. We found that the higher-order
parser performed equally well on development us-
ing this method as using second-order inference to
predict trees: since we run the parser multiple times
within Dual Decomposition, our strategy results in
faster parsing times.
Precomputed Paths Both Forest and Dual De-
composition run assignment on a set of precomputed
paths, and here we explain how we build it. We first
observed that 98.4% of the correct arguments in de-
velopment data are either direct descendants of the
predicate, direct descendants of an ancestor of the
predicate, or an ancestor of the predicate.5 All meth-
ods we test are restricted to this syntactic scope. To
generate a list of paths, we did as follows:
? Calculate marginals of unlabeled dependencies
using the first-order parser: p(h,m | x) =?
l p(h,m, l | x). Note that for each m, the
probabilities p(h,m|x) for all h form a distri-
bution (i.e. they sum to one). Then, for eachm,
keep the most-likely dependencies that cover at
least 90% of the mass, and prune the rest.
? Starting from a predicate p, generate a path
by taking any number of dependencies that as-
cend, and optionally adding one dependency
that descends. We constrained paths to be pro-
jective, and to have a maximum number of 6
4Our method allows to use non-projective dependency pars-
ing methods seamlessly.
5This is specific to CoNLL-2009 data for English. In gen-
eral, for other languages the coverage of these rules may be
lower. We leave this question to future work.
ascendant dependencies.
? Label each unlabeled edge ?h,m? in the paths
with l = argmaxl p(h,m, l | x).
On development data, this procedure generated an
average of 43.8 paths per predicate that cover 86.2%
of the correct paths. In contrast, enumerating paths
of the single-best tree covers 79.4% of correct paths
for the first-order parser, and 82.2% for the second-
order parser.6
SRL model We used a discriminative model with
similar features to those in the system of Johansson
(2009). In addition, we included the following:
? Unigram/bigram/trigram path features. For
all n-grams in the syntactic path, patterns
of words and POS tags (e.g., mary+loves+to,
mary+VB+to).
? Voice features. The predicate voice together
with the word/POS of the argument (e.g., pas-
sive+mary).
? Path continuity. Count of non-consecutive to-
kens in a predicate-argument path.
To train SRL models we used the averaged per-
ceptron (Collins, 2002). For the base pipeline we
trained standard SRL classifiers. For the rest of
models we used the structured Perceptron running
the assignment algorithm as inference routine. In
this case, we generated a large set of syntactic paths
for training using the procedure described above,
and we set the loss function to penalize mistakes in
predicting the semantic role of arguments and their
syntactic path.
Dual Decomposition We added a parameter ?
weighting the syntactic and semantic components of
the model as follows:
(1? ?) s syn(x,y) + ? s srl(x, z,pi) .
As syntactic scores we used normalized marginal
probabilities of dependencies, either from the first
or the higher-order parser. The scores of all factors
of the SRL model were normalized at every sen-
tence to be between -1 and 1. The rest of details
6One can evaluate the maximum recall on correct arguments
that can be obtained, irrespective of whether the syntactic path
is correct: for the set of paths it is 98.3%, while for single-best
trees it is 91.9% and 92.7% for first and second-order models.
226
o LAS UAS semp semr semF1 sempp
Pipeline 1 85.32 88.86 86.23 67.67 75.83 45.64
w. Assig. 1 85.32 88.86 84.08 71.82 77.47 51.17
Forest - - - 80.67 73.60 76.97 51.33
Pipeline 2 87.77 90.96 87.07 68.65 76.77 47.07
w. Assig. 2 87.77 90.96 85.21 73.41 78.87 53.80
Table 1: Results on development for the baseline and as-
signment pipelines, running first and second-order syn-
tactic parsers, and the Forest method. o indicates the or-
der of syntactic inference.
of the method were implemented following Koo et
al. (2010), including the strategy for decreasing the
step size ?t. We ran the algorithm for up to 500 it-
erations, with initial step size of 0.001.
6.2 Results
To evaluate syntactic dependencies we use unla-
beled attachment score (UAS), i.e., the percentage
of words with the correct head, and labeled attach-
ment scores (LAS), i.e., the percentage of words
with the correct head and syntactic label. Semantic
predicate-argument relations are evaluated with pre-
cision (semp), recall (semr) and F1 measure (semF1)
at the level of labeled semantic dependencies. In ad-
dition, we measure the percentage of perfectly pre-
dicted predicate structures (sempp).7
Table 1 shows the results on the development set
for our three first methods. We can see that the
pipeline methods running assignment improve over
the baseline pipelines in semantic F1 by about 2
points, due to the application of the cRole constraint.
The Forest method also shows an improvement in
recall of semantic roles with respect to the pipeline
methods. Presumably, the set of paths available in
the Forest model allows to recognize a higher num-
ber of arguments at an expense of a lower preci-
sion. Regarding the percentage of perfect predicate-
argument structures there is a remarkable improve-
ment in the systems that apply the full set of con-
7Our evaluation metrics differ slightly from the official met-
ric at CoNLL-2009. That metric considers predicate senses
as special semantic dependencies and, thus, it includes them
in the calculation of the evaluation metrics. In this paper, we
are not addressing predicate sense disambiguation and, conse-
quently, we ignore predicate senses when presenting evaluation
results. When we report the performance of CoNLL systems,
their scores will be noticeably lower than the scores reported at
the shared task. This is because predicate disambiguation is a
reasonably simple task with a very high baseline around 90%.
o ? LAS UAS semp semr semF1 sempp %conv
1 0.1 85.32 88.86 84.09 71.84 77.48 51.77 100
1 0.4 85.36 88.91 84.07 71.94 77.53 51.85 100
1 0.5 85.38 88.93 84.08 72.03 77.59 51.96 100
1 0.6 85.41 88.95 84.05 72.19 77.67 52.03 99.8
1 0.7 85.44 89.00 84.10 72.42 77.82 52.24 99.7
1 0.8 85.48 89.02 83.99 72.69 77.94 52.57 99.5
1 0.9 85.39 88.93 83.68 72.82 77.88 52.49 99.8
2 0.1 87.78 90.96 85.20 73.11 78.69 53.74 100
2 0.4 87.78 90.96 85.21 73.12 78.70 53.74 100
2 0.5 87.78 90.96 85.19 73.12 78.70 53.72 100
2 0.6 87.78 90.96 85.20 73.13 78.70 53.72 99.9
2 0.7 87.78 90.96 85.19 73.13 78.70 53.72 99.8
2 0.8 87.80 90.98 85.20 73.18 78.74 53.77 99.8
2 0.9 87.84 91.02 85.20 73.23 78.76 53.82 100
Table 2: Results of the dual decomposition method on
development data, for different values of the ? parame-
ter. o is the order of the syntactic parser. %conv is the
percentage of examples that converged.
straints using the assignment algorithm. We believe
that the cRole constraint that ensures no repeated
roles for a given predicate is a key factor to predict
the full set of arguments of a predicate.
The Forest configuration is the starting point to
run the dual decomposition algorithm. We ran ex-
periments for various values of the ? parameter. Ta-
ble 2 shows the results. We see that as we increase
?, the SRL component has more relative weight, and
the syntactic structure changes. The DD methods are
always able to improve over the Forest methods, and
find convergence in more than 99.5% of sentences.
Compared to the pipeline running assignment, DD
improves semantic F1 for first-order inference, but
not for higher-order inference, suggesting that 2nd
order predictions of paths are quite accurate. We
also observe slight benefits in syntactic accuracy.
Table 3 presents results of our system on the
test sets, where we run Pipeline with Assignment
and Dual Decomposition with our best configura-
tion (? = 0.8/0.9 for 1st/2nd order syntax). For
comparison, the table also reports the results of
the best CoNLL?2009 joint system, Merlo09 (Ges-
mundo et al, 2009), which proved to be very com-
petitive ranking third in the closed challenge. We
also include Llu??s09 (Llu??s et al, 2009), which is an-
other joint syntactic-semantic system from CoNLL?
2009.8 In the WSJ test DD obtains the best syntactic
accuracies, while the Pipeline obtains the best se-
8Another system to compare to is the joint system by Jo-
hansson (2009). Unfortunately, a direct comparison is not possi-
ble because it is evaluated on the CoNLL-2008 datasets, which
227
WSJ LAS UAS semp semr semF1 sempp
Llu??s09 87.48 89.91 73.87 67.40 70.49 39.68
Merlo09 88.79 91.26 81.00 76.45 78.66 54.80
Pipe-Assig 1st 86.85 89.68 85.12 73.78 79.05 54.12
DD 1st 87.04 89.89 85.03 74.56 79.45 54.92
Pipe-Assig 2nd 89.19 91.62 86.11 75.16 80.26 55.96
DD 2nd 89.21 91.64 86.01 74.84 80.04 55.73
Brown LAS UAS semp semr semF1 sempp
Llu??s09 80.92 85.96 62.29 59.22 60.71 29.79
Merlo09 80.84 86.32 68.97 63.06 65.89 38.92
Pipe-Assig 1st 80.96 86.58 72.91 60.16 65.93 38.44
DD 1st 81.18 86.86 72.53 60.76 66.12 38.13
Pipe-Assig 2nd 82.56 87.98 73.94 61.63 67.23 38.99
DD 2nd 82.61 88.04 74.12 61.59 67.28 38.92
Table 3: Comparative results on the CoNLL?2009 En-
glish test sets, namely the WSJ test (top table) and the
out of domain test from the Brown corpus (bottom table).
mantic F1. The bottom part of Table 3 presents re-
sults on the out-of-domain Brown test corpus. In this
case, DD obtains slightly better results than the rest,
both in terms of syntactic accuracy and semantic F1.
Table 4 shows statistical significance tests for the
syntactic LAS and semantic F1 scores of Table 3.
We have applied the sign test (Wackerly et al, 2007)
and approximate randomization tests (Yeh, 2000)
to all pairs of systems outputs. The differences be-
tween systems in the WSJ test can be considered
significant in almost all cases with p = 0.05. In
the Brown test set, results are more unstable and dif-
ferences are not significant in general, probably be-
cause of the relatively small size of that test.
Regarding running times, our implementation of
the baseline pipeline with 2nd order inference parses
the development set (1,334 sentences) in less than
7 minutes. Running assignment in the pipeline in-
creases parsing time by ?8% due to the overhead
from the assignment algorithm. The Forest method,
with an average of 61.3 paths per predicate, is?13%
slower than the pipeline due to the exploration of the
space of precomputed paths. Finally, Dual Decom-
position with 2nd order inference converges in 36.6
iterations per sentence on average. The first itera-
tion of DD has to perform roughly the same work
as Forest, while subsequent iterations only need to
re-parse the sentence with respect to the dual up-
are slightly different. However, note that Merlo09 is an applica-
tion of the system by Titov et al (2009). In that paper authors
report results on the CoNLL-2008 datasets, and they are com-
parable to Johansson?s.
WSJ Brown
ME PA1 DD1 PA2 DD2 ME PA1 DD1 PA2 DD2
LL ?? ?? ? ?? ??    ?? ??
ME ?? ?? ?? ?? ? ?
PA1 ?? ?? ?? ? ?? ??
DD1 ?? ?? ?? ??
PA2 ?
Table 4: Statistical tests of significance for LAS and
semF1 differences between pairs of systems from Table 3.
?/? = LAS difference is significant by the sign/ approxi-
mate randomization tests at 0.05 level. / = same mean-
ing for semF1 . The legend for systems is: LL: Llu??s09,
ME: Merlo09, PA1/2: Pipeline with Assignment, 1st/2nd
order, DD1/2: Dual Decomposition, 1st/2nd order.
dates, which are extremely sparse. Our current im-
plementation did not take advantage of the sparsity
of updates, and overall, DD was on average 13 times
slower than the pipeline running assignment and 15
times slower than the baseline pipeline.
7 Conclusion
We have introduced efficient methods to parse
syntactic dependency structures augmented with
predicate-argument relations, with two key ideas.
One is to predict the local syntactic structure that
links a predicate with its arguments, and seek agree-
ment with the full syntactic structure using dual
decomposition techniques. The second is to con-
trol linear assignment constraints in the predicate-
argument structure.
In experiments we observe large improvements
resulting from the assignment constraints. As for
the dual decomposition technique for joint parsing,
it does improve over the pipelines when we use a
first order parser. This means that in this configu-
ration the explicit semantic features help to find a
solution that is better in both layers. To some ex-
tent, this empirically validates the research objec-
tive of joint models. However, when we move to
second-order parsers the differences with respect to
the pipeline are insignificant. It is to be expected
that as syntactic parsers improve, the need of joint
methods is less critical. It remains an open question
to validate if large improvements can be achieved
by integrating syntactic-semantic features. To study
this question, it is necessary to have efficient pars-
ing algorithms for joint dependency structures. This
paper contributes with a method that has optimality
228
guarantees whenever it converges.
Our method can incorporate richer families of fea-
tures. It is straightforward to incorporate better se-
mantic representations of predicates and arguments
than just plain words, e.g. by exploiting WordNet or
distributional representations as in (Zapirain et al,
2013). Potentially, this could result in larger im-
provements in the performance of syntactic and se-
mantic parsing.
It is also necessary to experiment with differ-
ent languages, where the performance of syntactic
parsers is lower than in English, and hence there is
potential for improvement. Our treatment of local
syntactic structure that links predicates with argu-
ments, based on explicit enumeration of likely paths,
was simplistic. Future work should explore meth-
ods that model the syntactic structure linking predi-
cates with arguments: whenever this structure can be
parsed efficiently, our dual decomposition algorithm
can be employed to define an efficient joint system.
Acknowledgments
We thank the editor and the anonymous reviewers for
their valuable feedback. This work was financed by
the European Commission for the XLike project (FP7-
288342); and by the Spanish Government for project
OpenMT-2 (TIN2009-14675-C03-01), project Skater
(TIN2012-38584-C06-01), and a Ramo?n y Cajal contract
for Xavier Carreras (RYC-2008-02223).
References
Rainer Burkard, Mario Dell?Amico, and Silvano
Martello. 2009. Assignment Problems. Society for
Industrial and Applied Mathematics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 152?164, Ann Arbor, Michigan, June.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957?961, Prague, Czech Republic, June.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with Perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing, pages 1?8, July.
Dipanjan Das, Andre? F. T. Martins, and Noah A. Smith.
2012. An exact dual decomposition algorithm for
shallow semantic parsing with constraints. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics - Volume 1: Proceedings of
the main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop on
Semantic Evaluation, SemEval ?12, pages 209?217,
Stroudsburg, PA, USA.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and Other
Parsing Technologies, pages 29?62. Kluwer Academic
Publishers, October.
Andrea Gesmundo, James Henderson, Paola Merlo, and
Ivan Titov. 2009. A latent variable model of syn-
chronous syntactic-semantic parsing for multiple lan-
guages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task, pages 37?42, Boulder,
Colorado, June.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288, September.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of 40th Annual Meeting of the Association for
Computational Linguistics, pages 239?246, Philadel-
phia, Pennsylvania, USA, July.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning (CoNLL-2009): Shared Task, pages 1?18,
Boulder, Colorado, USA, June.
Richard Johansson. 2009. Statistical bistratal depen-
dency parsing. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, pages 561?569, Singapore, August.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured prediction mod-
els via the matrix-tree theorem. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 141?
150, Prague, Czech Republic, June.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1288?1298, Cambridge, MA, October.
229
Harold W. Kuhn. 1955. The Hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2(1-2):83?97.
Xavier Llu??s, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of
the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL 2009): Shared Task,
pages 79?84, Boulder, Colorado, June.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,
and Suzanne Stevenson. 2008. Semantic Role Label-
ing: An Introduction to the Special Issue. Computa-
tional Linguistics, 34(2):145?159, June.
Andre? Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 342?350, Sun-
tec, Singapore, August.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 91?98, Ann Arbor, Michigan, June.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank Project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages 335?
342, Barcelona, Spain, July.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 99?106, Ann Ar-
bor, Michigan, June.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106, March.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(3):257?287, June.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 1?12,
Edinburgh, Scotland, UK., July.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11, Cambridge, MA, October.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2010. Introduction to dual decomposition for infer-
ence. In S. Sra, S. Nowozin, and S. J. Wright, editors,
Optimization for Machine Learning. MIT Press.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The conll
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August.
Ivan Titov, James Henderson, Paola Merlo, and Gabriele
Musillo. 2009. Online graph planarisation for syn-
chronous parsing of semantic and syntactic dependen-
cies. In Proceedings of the 21st international jont
conference on Artifical intelligence, IJCAI?09, pages
1562?1567.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191, June.
Dennis D. Wackerly, William Mendenhall, and
Richard L. Scheaffer, 2007. Mathematical Statis-
tics with Applications, chapter 15: Nonparametric
statistics. Duxbury Press.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 88?94, Barcelona, Spain, July.
Alexander S. Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th conference on Computational linguis-
tics, pages 947?953.
Ben?at Zapirain, Eneko Agirre, Llu??s Ma`rquez, and Mihai
Surdeanu. 2013. Selectional preferences for semantic
role classification. Computational Linguistics, 39(3).
230
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 9?16
Manchester, August 2008
TAG, Dynamic Programming, and the Perceptron
for Efficient, Feature-rich Parsing
Xavier Carreras Michael Collins Terry Koo
MIT CSAIL, Cambridge, MA 02139, USA
{carreras,mcollins,maestro}@csail.mit.edu
Abstract
We describe a parsing approach that makes use
of the perceptron algorithm, in conjunction with
dynamic programming methods, to recover full
constituent-based parse trees. The formalism allows
a rich set of parse-tree features, including PCFG-
based features, bigram and trigram dependency fea-
tures, and surface features. A severe challenge in
applying such an approach to full syntactic pars-
ing is the efficiency of the parsing algorithms in-
volved. We show that efficient training is feasi-
ble, using a Tree Adjoining Grammar (TAG) based
parsing formalism. A lower-order dependency pars-
ing model is used to restrict the search space of the
full model, thereby making it efficient. Experiments
on the Penn WSJ treebank show that the model
achieves state-of-the-art performance, for both con-
stituent and dependency accuracy.
1 Introduction
In global linear models (GLMs) for structured pre-
diction, (e.g., (Johnson et al, 1999; Lafferty et al,
2001; Collins, 2002; Altun et al, 2003; Taskar et
al., 2004)), the optimal label y? for an input x is
y
?
= arg max
y?Y(x)
w ? f(x, y) (1)
where Y(x) is the set of possible labels for the in-
put x; f(x, y) ? Rd is a feature vector that rep-
resents the pair (x, y); and w is a parameter vec-
tor. This paper describes a GLM for natural lan-
guage parsing, trained using the averaged percep-
tron. The parser we describe recovers full syntac-
tic representations, similar to those derived by a
probabilistic context-free grammar (PCFG). A key
motivation for the use of GLMs in parsing is that
they allow a great deal of flexibility in the features
which can be included in the definition of f(x, y).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
A critical problem when training a GLM for
parsing is the computational complexity of the
inference problem. The averaged perceptron re-
quires the training set to be repeatedly decoded
under the model; under even a simple PCFG rep-
resentation, finding the argmax in Eq. 1 requires
O(n
3
G) time, where n is the length of the sen-
tence, and G is a grammar constant. The average
sentence length in the data set we use (the Penn
WSJ treebank) is over 23 words; the grammar con-
stant G can easily take a value of 1000 or greater.
These factors make exact inference algorithms vir-
tually intractable for training or decoding GLMs
for full syntactic parsing.
As a result, in spite of the potential advantages
of these methods, there has been very little previ-
ous work on applying GLMs for full parsing with-
out the use of fairly severe restrictions or approxi-
mations. For example, the model in (Taskar et al,
2004) is trained on only sentences of 15 words or
less; reranking models (Collins, 2000; Charniak
and Johnson, 2005) restrict Y(x) to be a small set
of parses from a first-pass parser; see section 1.1
for discussion of other related work.
The following ideas are central to our approach:
(1) A TAG-based, splittable grammar. We
describe a novel, TAG-based parsing formalism
that allows full constituent-based trees to be recov-
ered. A driving motivation for our approach comes
from the flexibility of the feature-vector represen-
tations f(x, y) that can be used in the model. The
formalism that we describe allows the incorpora-
tion of: (1) basic PCFG-style features; (2) the
use of features that are sensitive to bigram depen-
dencies between pairs of words; and (3) features
that are sensitive to trigram dependencies. Any
of these feature types can be combined with sur-
face features of the sentence x, in a similar way
9
to the use of surface features in conditional ran-
dom fields (Lafferty et al, 2001). Crucially, in
spite of these relatively rich representations, the
formalism can be parsed efficiently (in O(n4G)
time) using dynamic-programming algorithms de-
scribed by Eisner (2000) (unlike many other TAG-
related approaches, our formalism is ?splittable?
in the sense described by Eisner, leading to more
efficient parsing algorithms).
(2) Use of a lower-order model for pruning.
The O(n4G) running time of the TAG parser is
still too expensive for efficient training with the
perceptron. We describe a method that leverages
a simple, first-order dependency parser to restrict
the search space of the TAG parser in training and
testing. The lower-order parser runs in O(n3H)
time where H ? G; experiments show that it is
remarkably effective in pruning the search space
of the full TAG parser.
Experiments on the Penn WSJ treebank show
that the model recovers constituent structures with
higher accuracy than the approaches of (Charniak,
2000; Collins, 2000; Petrov and Klein, 2007),
and with a similar level of performance to the
reranking parser of (Charniak and Johnson, 2005).
The model also recovers dependencies with sig-
nificantly higher accuracy than state-of-the-art de-
pendency parsers such as (Koo et al, 2008; Mc-
Donald and Pereira, 2006).
1.1 Related Work
Previous work has made use of various restrictions
or approximations that allow efficient training of
GLMs for parsing. This section describes the rela-
tionship between our work and this previous work.
In reranking approaches, a first-pass parser
is used to enumerate a small set of candidate
parses for an input sentence; the reranking model,
which is a GLM, is used to select between these
parses (e.g., (Ratnaparkhi et al, 1994; Johnson et
al., 1999; Collins, 2000; Charniak and Johnson,
2005)). A crucial advantage of our approach is that
it considers a very large set of alternatives in Y(x),
and can thereby avoid search errors that may be
made in the first-pass parser.1
Another approach that allows efficient training
of GLMs is to use simpler syntactic representa-
tions, in particular dependency structures (McDon-
1Some features used within reranking approaches may be
difficult to incorporate within dynamic programming, but it
is nevertheless useful to make use of GLMs in the dynamic-
programming stage of parsing. Our parser could, of course,
be used as the first-stage parser in a reranking approach.
ald et al, 2005). Dependency parsing can be
implemented in O(n3) time using the algorithms
of Eisner (2000). In this case there is no gram-
mar constant, and parsing is therefore efficient. A
disadvantage of these approaches is that they do
not recover full, constituent-based syntactic struc-
tures; the increased linguistic detail in full syntac-
tic structures may be useful in NLP applications,
or may improve dependency parsing accuracy, as
is the case in our experiments.2
There has been some previous work on GLM
approaches for full syntactic parsing that make use
of dynamic programming. Taskar et al (2004)
describe a max-margin approach; however, in this
work training sentences were limited to be of 15
words or less. Clark and Curran (2004) describe
a log-linear GLM for CCG parsing, trained on the
Penn treebank. This method makes use of paral-
lelization across an 18 node cluster, together with
up to 25GB of memory used for storage of dy-
namic programming structures for training data.
Clark and Curran (2007) describe a perceptron-
based approach for CCG parsing which is consid-
erably more efficient, and makes use of a super-
tagging model to prune the search space of the full
parsing model. Recent work (Petrov et al, 2007;
Finkel et al, 2008) describes log-linear GLMs ap-
plied to PCFG representations, but does not make
use of dependency features.
2 The TAG-Based Parsing Model
2.1 Derivations
This section describes the idea of derivations in
our parsing formalism. As in context-free gram-
mars or TAGs, a derivation in our approach is a
data structure that specifies the sequence of opera-
tions used in combining basic (elementary) struc-
tures in a grammar, to form a full parse tree. The
parsing formalism we use is related to the tree ad-
joining grammar (TAG) formalisms described in
(Chiang, 2003; Shen and Joshi, 2005). However,
an important difference of our work from this pre-
vious work is that our formalism is defined to be
?splittable?, allowing use of the efficient parsing
algorithms of Eisner (2000).
A derivation in our model is a pair ?E,D? where
E is a set of spines, and D is a set of dependencies
2Note however that the lower-order parser that we use to
restrict the search space of the TAG-based parser is based on
the work of McDonald et al (2005). See also (Sagae et al,
2007) for a method that uses a dependency parser to restrict
the search space of a more complex HPSG parser.
10
(a) S
VP
VBD
ate
NP
NN
cake
(b) S
VP
VP
VBD
ate
NP
NN
cake
Figure 1: Two example trees.
specifying how the spines are combined to form
a parse tree. The spines are similar to elementary
trees in TAG. Some examples are as follows:
NP
NNP
John
S
VP
VBD
ate
NP
NN
cake
ADVP
RB
quickly
ADVP
RB
luckily
These structures do not have substitution nodes, as
is common in TAGs.3 Instead, the spines consist
of a lexical anchor together with a series of unary
projections, which usually correspond to different
X-bar levels associated with the anchor.
The operations used to combine spines are sim-
ilar to the TAG operations of adjunction and sis-
ter adjunction. We will call these operations regu-
lar adjunction (r-adjunction) and sister adjunction
(s-adjunction). As one example, the cake spine
shown above can be s-adjoined into the VP node of
the ate spine, to form the tree shown in figure 1(a).
In contrast, if we use the r-adjunction operation to
adjoin the cake tree into the VP node, we get a dif-
ferent structure, which has an additional VP level
created by the r-adjunction operation: the resulting
tree is shown in figure 1(b). The r-adjunction op-
eration is similar to the usual adjunction operation
in TAGs, but has some differences that allow our
grammars to be splittable; see section 2.3 for more
discussion.
We now give formal definitions of the sets E and
D. Take x to be a sentence consisting of n + 1
words, x
0
. . . x
n
, where x
0
is a special root sym-
bol, which we will denote as ?. A derivation for the
input sentence x consists of a pair ?E,D?, where:
? E is a set of (n + 1) tuples of the form ?i, ??,
where i ? {0 . . . n} is an index of a word in the
sentence, and ? is the spine associated with the
word x
i
. The set E specifies one spine for each
of the (n + 1) words in the sentence. Where it is
3It would be straightforward to extend the approach to in-
clude substitution nodes, and a substitution operation.
clear from context, we will use ?
i
to refer to the
spine in E corresponding to the i?th word.
? D is a set of n dependencies. Each depen-
dency is a tuple ?h,m, l?. Here h is the index of
the head-word of the dependency, corresponding
to the spine ?
h
which contains a node that is being
adjoined into. m is the index of the modifier-word
of the dependency, corresponding to the spine ?
m
which is being adjoined into ?
h
. l is a label.
The label l is a tuple ?POS, A, ?
h
, ?
m
, L?. ?
h
and
?
m
are the head and modifier spines that are be-
ing combined. POS specifies which node in ?
h
is
being adjoined into. A is a binary flag specifying
whether the combination operation being used is s-
adjunction or r-adjunction. L is a binary flag spec-
ifying whether or not any ?previous? modifier has
been r-adjoined into the position POS in ?
h
. By a
previous modifier, we mean a modifier m? that was
adjoined from the same direction as m (i.e., such
that h < m? < m or m < m? < h).
It would be sufficient to define l to be the pair
?POS, A??the inclusion of ?
h
, ?
m
and L adds re-
dundant information that can be recovered from
the set E, and other dependencies in D?but it
will be convenient to include this information in
the label. In particular, it is important that given
this definition of l, it is possible to define a func-
tion GRM(l) that maps a label l to a triple of non-
terminals that represents the grammatical relation
between m and h in the dependency structure. For
example, in the tree shown in figure 1(a), the gram-
matical relation between cake and ate is the triple
GRM(l) = ?VP VBD NP?. In the tree shown in
figure 1(b), the grammatical relation between cake
and ate is the triple GRM(l) = ?VP VP NP?.
The conditions under which a pair ?E,D? forms
a valid derivation for a sentence x are similar to
those in conventional LTAGs. Each ?i, ?? ? E
must be such that ? is an elementary tree whose
anchor is the word x
i
. The dependencies D must
form a directed, projective tree spanning words
0 . . . n, with ? at the root of this tree, as is also
the case in previous work on discriminative ap-
proches to dependency parsing (McDonald et al,
2005). We allow any modifier tree ?
m
to adjoin
into any position in any head tree ?
h
, but the de-
pendencies D must nevertheless be coherent?for
example they must be consistent with the spines in
E, and they must be nested correctly.4 We will al-
4For example, closer modifiers to a particular head must
adjoin in at the same or a lower spine position than modifiers
11
low multiple modifier spines to s-adjoin or r-adjoin
into the same node in a head spine; see section 2.3
for more details.
2.2 A Global Linear Model
The model used for parsing with this approach is
a global linear model. For a given sentence x, we
define Y(x) to be the set of valid derivations for x,
where each y ? Y(x) is a pair ?E,D? as described
in the previous section. A function f maps (x, y)
pairs to feature-vectors f(x, y) ? Rd. The param-
eter vector w is also a vector in Rd. Given these
definitions, the optimal derivation for an input sen-
tence x is y? = argmax
y?Y(x)
w ? f(x, y).
We now come to how the feature-vector f(x, y)
is defined in our approach. A simple ?first-order?
model would define
f(x, y) =
?
?i,???E(y)
e(x, ?i, ??) +
?
?h,m,l??D(y)
d(x, ?h,m, l?) (2)
Here we use E(y) and D(y) to respectively refer
to the set of spines and dependencies in y. The
function e maps a sentence x paired with a spine
?i, ?? to a feature vector. The function d maps de-
pendencies within y to feature vectors. This de-
composition is similar to the first-order model of
McDonald et al (2005), but with the addition of
the e features.
We will extend our model to include higher-
order features, in particular features based on sib-
ling dependencies (McDonald and Pereira, 2006),
and grandparent dependencies, as in (Carreras,
2007). If y = ?E,D? is a derivation, then:
? S(y) is a set of sibling dependencies. Each
sibling dependency is a tuple ?h,m, l, s?. For each
?h,m, l, s? ? S the tuple ?h,m, l? is an element of
D; there is one member of S for each member of
D. The index s is the index of the word that was
adjoined to the spine for h immediately before m
(or the NULL symbol if no previous adjunction has
taken place).
? G(y) is a set of grandparent dependencies of
type 1. Each type 1 grandparent dependency is a
tuple ?h,m, l, g?. There is one member of G for
every member of D. The additional information,
the index g, is the index of the word that is the first
modifier to the right of the spine for m.
that are further from the head.
(a) S
VP
VP
VP
VBD
ate
NP
NN
cake
NP
NN
today
ADVP
RB
quickly
(b) S
NP
NNP
John
VP
ADVP
RB
luckily
VP
VBD
ate
Figure 2: Two Example Trees
S
NP
NNP
John
VP
VP
ADVP
RB
luckily
VP
VBD
ate
NP
NN
cake
NP
NN
today
ADVP
RB
quickly
Figure 3: An example tree, formed by a combina-
tion of the two structures in figure 2.
? Q(y) is an additional set of grandparent de-
pendencies, of type 2. Each of these dependencies
is a tuple ?h,m, l, q?. Again, there is one member
of Q for every member of D. The additional infor-
mation, the index q, is the index of the word that is
the first modifier to the left of the spine for m.
The feature-vector definition then becomes:
f(x, y) =
X
?i,???E(y)
e(x, ?i, ??) +
X
?h,m,l??D(y)
d(x, ?h,m, l?) +
X
?h,m,l,s??S(y)
s(x, ?h,m, l, s?) +
X
?h,m,l,g??G(y)
g(x, ?h,m, l, g?) +
X
?h,m,l,q??Q(y)
q(x, ?h,m, l, q?)
(3)
where s, g and q are feature vectors corresponding
to the new, higher-order elements.5
2.3 Recovering Parse Trees from Derivations
As in TAG approaches, there is a mapping from
derivations ?E,D? to parse trees (i.e., the type of
trees generated by a context-free grammar). In our
case, we map a spine and its dependencies to a con-
stituent structure by first handling the dependen-
5We also added constituent-boundary features to the
model, which is a simple change that led to small improve-
ments on validation data; for brevity we omit the details.
12
cies on each side separately and then combining
the left and right sides.
First, it is straightforward to build the con-
stituent structure resulting from multiple adjunc-
tions on the same side of a spine. As one exam-
ple, the structure in figure 2(a) is formed by first
s-adjoining the spine with anchor cake into the VP
node of the spine for ate, then r-adjoining spines
anchored by today and quickly into the same node,
where all three modifier words are to the right of
the head word. Notice that each r-adjunction op-
eration creates a new VP level in the tree, whereas
s-adjunctions do not create a new level. Now con-
sider a tree formed by first r-adjoining a spine for
luckily into the VP node for ate, followed by s-
adjoining the spine for John into the S node, in
both cases where the modifiers are to the left of
the head. In this case the structure that would be
formed is shown in figure 2(b).
Next, consider combining the left and right
structures of a spine. The main issue is how to
handle multiple r-adjunctions or s-adjunctions on
both sides of a node in a spine, because our deriva-
tions do not specify how adjunctions from different
sides embed with each other. In our approach, the
combination operation preserves the height of the
different modifiers from the left and right direc-
tions. To illustrate this, figure 3 shows the result
of combining the two structures in figure 2. The
combination of the left and right modifier struc-
tures has led to flat structures, for example the rule
VP? ADVP VP NP in the above tree.
Note that our r-adjunction operation is different
from the usual adjunction operation in TAGs, in
that ?wrapping? adjunctions are not possible, and
r-adjunctions from the left and right directions are
independent from each other; because of this our
grammars are splittable.
3 Parsing Algorithms
3.1 Use of Eisner?s Algorithms
This section describes the algorithm for finding
y
?
= argmax
y?Y(x)
w ? f(x, y) where f(x, y) is
defined through either the first-order model (Eq. 2)
or the second-order model (Eq. 3).
For the first-order model, the methods described
in (Eisner, 2000) can be used for the parsing algo-
rithm. In Eisner?s algorithms for dependency pars-
ing each word in the input has left and right finite-
state (weighted) automata, which generate the left
and right modifiers of the word in question. We
make use of this idea of automata, and also make
direct use of the method described in section 4.2 of
(Eisner, 2000) that allows a set of possible senses
for each word in the input string. In our use of
the algorithm, each possible sense for a word cor-
responds to a different possible spine that can be
associated with that word. The left and right au-
tomata are used to keep track of the last position
in the spine that was adjoined into on the left/right
of the head respectively. We can make use of sep-
arate left and right automata?i.e., the grammar is
splittable?because left and right modifiers are ad-
joined independently of each other in the tree. The
extension of Eisner?s algorithm to the second-order
model is similar to the algorithm described in (Car-
reras, 2007), but again with explicit use of word
senses and left/right automata. The resulting algo-
rithms run in O(Gn3) and O(Hn4) time for the
first-order and second-order models respectively,
where G and H are grammar constants.
3.2 Efficient Parsing
The efficiency of the parsing algorithm is impor-
tant in applying the parsing model to test sen-
tences, and also when training the model using dis-
criminative methods. The grammar constants G
and H introduced in the previous section are poly-
nomial in factors such as the number of possible
spines in the model, and the number of possible
states in the finite-state automata implicit in the
parsing algorithm. These constants are large, mak-
ing exhaustive parsing very expensive.
To deal with this problem, we use a simple ini-
tial model to prune the search space of the more
complex model. The first-stage model we use
is a first-order dependency model, with labeled
dependencies, as described in (McDonald et al,
2005). As described shortly, we will use this model
to compute marginal scores for dependencies in
both training and test sentences. A marginal score
?(x, h,m, l) is a value between 0 and 1 that re-
flects the plausibility of a dependency for sentence
x with head-word x
h
, modifier word x
m
, and la-
bel l. In the first-stage pruning model the labels l
are triples of non-terminals representing grammat-
ical relations, as described in section 2.1 of this
paper?for example, one possible label would be
?VP VBD NP?, and in general any triple of non-
terminals is possible.
Given a sentence x, and an index m of a word
in that sentence, we define DMAX(x,m) to be the
13
highest scoring dependency with m as a modifier:
DMAX(x,m) = max
h,l
?(x, h,m, l)
For a sentence x, we then define the set of allow-
able dependencies to be
pi(x) = {?h,m, l? : ?(x, h,m, l) ? ?DMAX(x,m)}
where ? is a constant dictating the beam size that
is used (in our experiments we used ? = 10?6).
The set pi(x) is used to restrict the set of pos-
sible parses under the full TAG-based model. In
section 2.1 we described how the TAG model has
dependency labels of the form ?POS, A, ?
h
, ?
m
, L?,
and that there is a function GRM that maps labels
of this form to triples of non-terminals. The ba-
sic idea of the pruned search is to only allow de-
pendencies of the form ?h,m, ?POS, A, ?
h
, ?
m
, L??
if the tuple ?h,m, GRM(?POS, A, ?
h
, ?
m
, L?)? is a
member of pi(x), thus reducing the search space
for the parser.
We now turn to how the marginals ?(x, h,m, l)
are defined and computed. A simple approach
would be to use a conditional log-linear model
(Lafferty et al, 2001), with features as defined by
McDonald et al (2005), to define a distribution
P (y|x) where the parse structures y are depen-
dency structures with labels that are triples of non-
terminals. In this case we could define
?(x, h,m, l) =
?
y:(h,m,l)?y
P (y|x)
which can be computed with inside-outside style
algorithms, applied to the data structures from
(Eisner, 2000). The complexity of training and ap-
plying such a model is again O(Gn3), where G is
the number of possible labels, and the number of
possible labels (triples of non-terminals) is around
G = 1000 in the case of treebank parsing; this
value for G is still too large for the method to be ef-
ficient. Instead, we train three separate models ?
1
,
?
2
, and ?
3
for the three different positions in the
non-terminal triples. We then take ?(x, h,m, l) to
be a product of these three models, for example we
would calculate
?(x, h,m, ?VP VBD NP?) =
?
1
(x, h,m, ?VP?)? ?
2
(x, h,m, ?VBD?)
??
3
(x, h,m, ?NP?)
Training the three models, and calculating the
marginals, now has a grammar constant equal
to the number of non-terminals in the grammar,
which is far more manageable. We use the algo-
rithm described in (Globerson et al, 2007) to train
the conditional log-linear model; this method was
found to converge to a good model after 10 itera-
tions over the training data.
4 Implementation Details
4.1 Features
Section 2.2 described the use of feature vectors
associated with spines used in a derivation, to-
gether with first-order, sibling, and grandparent
dependencies. The dependency features used in
our experiments are closely related to the features
described in (Carreras, 2007), which are an ex-
tension of the McDonald and Pereira (2006) fea-
tures to cover grandparent dependencies in addi-
tion to first-order and sibling dependencies. The
features take into account the identity of the la-
bels l used in the derivations. The features could
potentially look at any information in the la-
bels, which are of the form ?POS, A, ?
h
, ?
m
, L?,
but in our experiments, we map labels to a pair
(GRM(?POS, A, ?
h
, ?
m
, L?), A). Thus the label fea-
tures are sensitive only to the triple of non-
terminals corresponding to the grammatical rela-
tion involved in an adjunction, and a binary flag
specifiying whether the operation is s-adjunction
or r-adjunction.
For the spine features e(x, ?i, ??), we use fea-
ture templates that are sensitive to the identity of
the spine ?, together with contextual features of
the string x. These features consider the iden-
tity of the words and part-of-speech tags in a win-
dow that is centered on x
i
and spans the range
x
(i?2)
. . . x
(i+2)
.
4.2 Extracting Derivations from Parse Trees
In the experiments in this paper, the following
three-step process was used: (1) derivations were
extracted from a training set drawn from the Penn
WSJ treebank, and then used to train a parsing
model; (2) the test data was parsed using the re-
sulting model, giving a derivation for each test
data sentence; (3) the resulting test-data deriva-
tions were mapped back to Penn-treebank style
trees, using the method described in section 2.1.
To achieve step (1), we first apply a set of head-
finding rules which are similar to those described
in (Collins, 1997). Once the head-finding rules
have been applied, it is straightforward to extract
14
precision recall F
1
PPK07 ? ? 88.3
FKM08 88.2 87.8 88.0
CH2000 89.5 89.6 89.6
CO2000 89.9 89.6 89.8
PK07 90.2 89.9 90.1
this paper 91.4 90.7 91.1
CJ05 ? ? 91.4
H08 ? ? 91.7
CO2000(s24) 89.6 88.6 89.1
this paper (s24) 91.1 89.9 90.5
Table 1: Results for different methods. PPK07, FKM08,
CH2000, CO2000, PK07, CJ05 and H08 are results on section
23 of the Penn WSJ treebank, for the models of Petrov et al
(2007), Finkel et al (2008), Charniak (2000), Collins (2000),
Petrov and Klein (2007), Charniak and Johnson (2005), and
Huang (2008). (CJ05 is the performance of an updated
model at http://www.cog.brown.edu/mj/software.htm.) ?s24?
denotes results on section 24 of the treebank.
s23 s24
KCC08 unlabeled 92.0 91.0
KCC08 labeled 92.5 91.7
this paper 93.5 92.5
Table 2: Table showing unlabeled dependency accuracy for
sections 23 and 24 of the treebank, using the method of (Ya-
mada and Matsumoto, 2003) to extract dependencies from
parse trees from our model. KCC08 unlabeled is from (Koo
et al, 2008), a model that has previously been shown to have
higher accuracy than (McDonald and Pereira, 2006). KCC08
labeled is the labeled dependency parser from (Koo et al,
2008); here we only evaluate the unlabeled accuracy.
derivations from the Penn treebank trees.
Note that the mapping from parse trees to
derivations is many-to-one: for example, the ex-
ample trees in section 2.3 have structures that are
as ?flat? (have as few levels) as is possible, given
the set D that is involved. Other similar trees,
but with more VP levels, will give the same set
D. However, this issue appears to be benign in the
Penn WSJ treebank. For example, on section 22 of
the treebank, if derivations are first extracted using
the method described in this section, then mapped
back to parse trees using the method described in
section 2.3, the resulting parse trees score 100%
precision and 99.81% recall in labeled constituent
accuracy, indicating that very little information is
lost in this process.
4.3 Part-of-Speech Tags, and Spines
Sentences in training, test, and development data
are assumed to have part-of-speech (POS) tags.
POS tags are used for two purposes: (1) in the
features described above; and (2) to limit the set
of allowable spines for each word during parsing.
Specifically, for each POS tag we create a separate
1st stage 2nd stage
? active coverage oracle F
1
speed F
1
10
?4 0.07 97.7 97.0 5:15 91.1
10
?5 0.16 98.5 97.9 11:45 91.6
10
?6 0.34 99.0 98.5 21:50 92.0
Table 3: Effect of the beam size, controlled by ?, on the
performance of the parser on the development set (1,699 sen-
tences). In each case ? refers to the beam size used in both
training and testing the model. ?active?: percentage of de-
pendencies that remain in the beam out of the total number of
labeled dependencies (1,000 triple labels times 1,138,167 un-
labeled dependencies); ?coverage?: percentage of correct de-
pendencies in the beam out of the total number of correct de-
pendencies. ?oracle F
1
?: maximum achievable score of con-
stituents, given the beam. ?speed?: parsing time in min:sec
for the TAG-based model (this figure does not include the time
taken to calculate the marginals using the lower-order model);
?F
1
?: score of predicted constituents.
dictionary listing the spines that have been seen
with this POS tag in training data; during parsing
we only allow spines that are compatible with this
dictionary. (For test or development data, we used
the part-of-speech tags generated by the parser of
(Collins, 1997). Future work should consider in-
corporating the tagging step within the model; it is
not challenging to extend the model in this way.)
5 Experiments
Sections 2-21 of the Penn Wall Street Journal tree-
bank were used as training data in our experiments,
and section 22 was used as a development set. Sec-
tions 23 and 24 were used as test sets. The model
was trained for 20 epochs with the averaged per-
ceptron algorithm, with the development data per-
formance being used to choose the best epoch. Ta-
ble 1 shows the results for the method.
Our experiments show an improvement in per-
formance over the results in (Collins, 2000; Char-
niak, 2000). We would argue that the Collins
(2000) method is considerably more complex than
ours, requiring a first-stage generative model, to-
gether with a reranking approach. The Char-
niak (2000) model is also arguably more com-
plex, again using a carefully constructed genera-
tive model. The accuracy of our approach also
shows some improvement over results in (Petrov
and Klein, 2007). This work makes use of a
PCFG with latent variables that is trained using
a split/merge procedure together with the EM al-
gorithm. This work is in many ways comple-
mentary to ours?for example, it does not make
use of GLMs, dependency features, or of repre-
sentations that go beyond PCFG productions?and
15
some combination of the two methods may give
further gains.
Charniak and Johnson (2005), and Huang
(2008), describe approaches that make use of non-
local features in conjunction with the Charniak
(2000) model; future work may consider extend-
ing our approach to include non-local features.
Finally, other recent work (Petrov et al, 2007;
Finkel et al, 2008) has had a similar goal of scal-
ing GLMs to full syntactic parsing. These mod-
els make use of PCFG representations, but do not
explicitly model bigram or trigram dependencies.
The results in this work (88.3%/88.0% F
1
) are
lower than our F
1
score of 91.1%; this is evidence
of the benefits of the richer representations enabled
by our approach.
Table 2 shows the accuracy of the model in
recovering unlabeled dependencies. The method
shows improvements over the method described
in (Koo et al, 2008), which is a state-of-the-art
second-order dependency parser similar to that of
(McDonald and Pereira, 2006), suggesting that the
incorporation of constituent structure can improve
dependency accuracy.
Table 3 shows the effect of the beam-size on the
accuracy and speed of the parser on the develop-
ment set. With the beam setting used in our exper-
iments (? = 10?6), only 0.34% of possible depen-
dencies are considered by the TAG-based model,
but 99% of all correct dependencies are included.
At this beam size the best possible F
1
constituent
score is 98.5. Tighter beams lead to faster parsing
times, with slight drops in accuracy.
6 Conclusions
We have described an efficient and accurate parser
for constituent parsing. A key to the approach has
been to use a splittable grammar that allows effi-
cient dynamic programming algorithms, in com-
bination with pruning using a lower-order model.
The method allows relatively easy incorporation of
features; future work should leverage this in pro-
ducing more accurate parsers, and in applying the
parser to different languages or domains.
Acknowledgments X. Carreras was supported by the
Catalan Ministry of Innovation, Universities and Enterprise,
by the GALE program of DARPA, Contract No. HR0011-06-
C-0022, and by a grant from NTT, Agmt. Dtd. 6/21/1998.
T. Koo was funded by NSF grant IIS-0415030. M. Collins
was funded by NSF grant IIS-0347631 and DARPA contract
No. HR0011-06-C-0022. Thanks to Jenny Rose Finkel for
suggesting that we evaluate dependency parsing accuracies.
References
Altun, Y., I. Tsochantaridis, and T. Hofmann. 2003. Hidden
markov support vector machines. In ICML.
Carreras, X. 2007. Experiments with a higher-order projec-
tive dependency parser. In Proc. EMNLP-CoNLL Shared
Task.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
ACL.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proc. NAACL.
Chiang, D. 2003. Statistical parsing with an automatically
extracted tree adjoining grammar. In Bod, R., R. Scha, and
K. Sima?an, editors, Data Oriented Parsing, pages 299?
316. CSLI Publications.
Clark, S. and J. R. Curran. 2004. Parsing the wsj using ccg
and log-linear models. In Proc. ACL.
Clark, Stephen and James R. Curran. 2007. Perceptron train-
ing for a wide-coverage lexicalized-grammar parser. In
Proc. ACL Workshop on Deep Linguistic Processing.
Collins, M. 1997. Three generative, lexicalised models for
statistical parsing. In Proc. ACL.
Collins, M. 2000. Discriminative reranking for natural lan-
guage parsing. In Proc. ICML.
Collins, M. 2002. Discriminative training methods for hid-
den markov models: Theory and experiments with percep-
tron algorithms. In Proc. EMNLP.
Eisner, J. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Bunt, H. C. and A. Nijholt, editors,
New Developments in Natural Language Parsing, pages
29?62. Kluwer Academic Publishers.
Finkel, J. R., A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing. In
Proc. ACL/HLT.
Globerson, A., T. Koo, X. Carreras, and M. Collins. 2007.
Exponentiated gradient algorithms for log-linear struc-
tured prediction. In Proc. ICML.
Huang, L. 2008. Forest reranking: Discriminative parsing
with non-local features. In Proc. ACL/HLT.
Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic unification-based gram-
mars. In Proc. ACL.
Koo, Terry, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL/HLT.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditonal
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML.
McDonald, R. and F. Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In Proc. EACL.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In Proc.
ACL.
Petrov, S. and D. Klein. 2007. Improved inference for unlex-
icalized parsing. In Proc. of HLT-NAACL.
Petrov, S., A. Pauls, and D. Klein. 2007. Discriminative log-
linear grammars with latent variables. In Proc. NIPS.
Ratnaparkhi, A., S. Roukos, and R. Ward. 1994. A maximum
entropy model for parsing. In Proc. ICSLP.
Sagae, Kenji, Yusuke Miyao, and Jun?ichi Tsujii. 2007. Hpsg
parsing with shallow dependency constraints. In Proc.
ACL, pages 624?631.
Shen, L. and A.K. Joshi. 2005. Incremental ltag parsing. In
Proc HLT-EMNLP.
Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of the
EMNLP-2004.
Yamada, H. and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. IWPT.
16
