Answer  Ext rac t ion  
Steven Abney  Michae l  Co l l ins  Ami t  S ingha l  
AT&T Shannon Laboratory  
180 Park  Ave. 
F lo rharn  Park ,  N J  07932 
{abney,  mco l l ins , s ingha l}@research .a t t . corn  
Abst ract  
Information retrieval systems have typically concen- 
trated on retrieving a set of documents which are rel- 
evant to a user's query. This paper describes a sys- 
tem that attempts to retrieve a much smaller section 
of text, namely, a direct answer to a user's question. 
The SMART IR system is used to extract a ranked 
set of passages that are relevant o the query. En- 
tities are extracted from these passages as potential 
answers to the question, and ranked for plausibility 
according to how well their type matches the query, 
and according to their frequency and position in the 
passages. The system was evaluated at the TREC-8 
question answering track: we give results and error 
analysis on these queries. 
1 Introduction 
In this paper, we describe and evaluate a question- 
answering system based on passage retrieval and 
entity-extraction technology. 
There has long been a concensus in the Informa- 
tion Retrieval (IR) community that natural anguage 
processing has little to offer for retrieval systems. 
Plausibly, this is creditable to the preeminence of ad 
hoc document retrieval as the task of interest in IR. 
However, there is a growing recognition of the lim- 
itations of ad hoc retrieval, both in the sense that 
current systems have reached the limit of achievable 
performance, and in the sense that users' informa- 
tion needs are often not well characterized by docu- 
ment retrieval. 
In many cases, a user has a question with a spe- 
cific answer, such as What city is it where the Euro- 
pean Parliament meets? or Who discovered Pluto? 
In such cases, ranked answers with links to support- 
ing documentation are much more useful than the 
ranked list of documents that standard retrieval en- 
gines produce. 
The ability to answer specific questions also pro- 
vides a foundation for addressing quantitative in- 
quiries such as How many times has the Fed raised 
interest rates this year? which can be interpreted 
as the cardinality of the set of answers to a specific 
question that happens to have multiple correct an- 
swers, like On what date did the Fed raise interest 
rates this year? 
We describe a system that extracts specific an- 
swers from a document collection. The system's per- 
formance was evaluated in the question-answering 
track that has been introduced this year at the 
TREC information-retrieval conference. The major 
points of interest are the following. 
? Comparison of the system's performance to a 
system that uses the same passage retrieval 
component, but no natural language process- 
ing, shows that NLP provides ignificant perfor- 
mance improvements on the question-answering 
task. 
? The system is designed to build on the strengths 
of both IR and NLP technologies. This makes 
for much more robustness than a pure NLP sys- 
tem would have, while affording much greater 
precision than a pure IR system would have. 
? The task is broken into subtasks that admit of 
independent development and evaluation. Pas- 
sage retrieval and entity extraction are both re- 
cognized independent tasks. Other subtasks are 
entity classification and query classification-- 
both being classification tasks that use features 
obtained by parsing--and entity ranking. 
In the following section, we describe the question- 
answering system, and in section 3, we quantify its 
performance and give an error analysis. 
2 The  Quest ion -Answer ing  System 
The system takes a natural-language query as input 
and produces a list of answers ranked in order of 
confidence. The top five answers were submitted to 
the TREC evaluation. 
Queries are processed in two stages. In the infor- 
mation retrieval stage, the most promising passages 
of the most promising documents are retrieved. In 
the linguistic processing stage, potential answers are 
extracted from these passages and ranked. 
The system can be divided into five main compo- 
nents. The information retrieval stage consists of a 
296 
single component, passage retrieval, and the linguis- 
tic processing stage circumscribes four components: 
entity extraction, entity classification, query classi- 
fication, and entity ranking. 
Passage Ret r ieva l  Identify relevant documents, 
and within relevant documents, identify the 
passages most likely to contain the answer to 
the question. 
Ent i ty  Ext ract ion  Extract a candidate set of pos- 
sible answers from the passages. 
Ent i ty  Classification The candidate set is a list of 
entities falling into a number of categories, in- 
cluding people, locations, organizations, quan- 
tities, dates, and linear measures. In some cases 
(dates, quantities, linear measures), entity clas- 
sification is a side effect of entity extraction, 
but in other cases (proper nouns, which may 
be people, locations, or organizations), there is 
a separate classification step after extraction. 
Query  Classi f icat ion Determine what category of 
entity the question is asking for. For example, 
if the query is 
Who is the author of the book, The 
Iron Lady: A Biography of Margaret 
Thatcher? 
the answer should be an entity of type Person. 
Ent i ty  Ranking Assign scores to entities, repre- 
senting roughly belief that the entity is the cor- 
rect answer. There are two components of the 
score. The most-significant bit is whether or 
not the category of the entity (as determined 
by entity classification) matches the category 
that the question is seeking (as determined by 
query classification). A finer-grained ranking is 
imposed on entities with the correct category, 
through the use of frequency and other infor- 
mation. 
The following sections describe these five compo- 
nents in detail. 
2.1 Passage Retrieval 
The first step is to find passages likely to contain the 
answer to the query. We use a modified version of 
the SMART information retrieval system (Buckley 
and Lewit, 1985; Salton, 1971) to recover a set of 
documents which are relevant o the question. We 
define passages as overlapping sets consisting of a 
sentence and its two immediate neighbors. (Pas- 
sages are in one-one correspondence with with sen- 
tences, and adjacent passages have two sentences in 
common.) The score for passage i was calculated as 
1 ?Si-z + ?Si + ~'S,+1 (1) 
where Sj, the score for sentence j, is the sum of IDF 
weights of non-stop terms that it shares with the 
query, plus an additional bonus for pairs of words 
(bigrams) that the sentence and query have in com- 
mon. 
The top 50 passages are passed on as input to 
linguistic processing. 
2.2 Ent i ty  Ext ract ion  
Entity extraction is done using the Cass partial pars- 
er (Abney, 1996). From the Cass output, we take 
dates, durations, linear measures, and quantities. 
In addition, we constructed specialized code for 
extracting proper names. The proper-name extrac- 
tor essentially classifies capitalized words as intrinsi- 
cally capitalized or not, where the alternatives to in- 
trinsic capitalization are sentence-initial capitaliza- 
tion or capitalization in titles and headings. The 
extractor uses various heuristics, including whether 
the words under consideration appear unambiguous- 
ly capitalized elsewhere in the document. 
2.3 Ent i ty  Classif ication 
The following types of entities were extracted as po- 
tential answers to queries. 
Person, Locat ion,  Organization, Other 
Proper names were classified into these cate- 
gories using a classifier built using the method 
described in (Collins and Singer, 1999). 1 This 
is the only place where entity classification was 
actually done as a separate step from entity 
extraction. 
Dates Four-digit numbers starting with 1 . . .  or 
20. .  were taken to be years. Cass was used to 
extract more complex date expressions ( uch as 
Saturday, January 1st, 2000). 
Quant i t ies  Quantities include bare numbers and 
numeric expressions' like The Three Stooges, 4 
1//2 quarts, 27~o. The head word of complex nu- 
meric expressions was identified (stooges, quarts 
or percent); these entities could then be later 
identified as good answers to How many ques- 
tions such as How many stooges were there ?
Durat ions,  Linear Measures  Durations and lin- 
ear measures are essentially special cases of 
quantities, in which the head word is a time 
unit or a unit of linear measure. Examples of 
durations are three years, 6 1/2 hours. Exam- 
ples of linear measures are 140 million miles, 
about 12 feet. 
We should note that this list does not exhaust he 
space of useful categories. Monetary amounts (e.g., 
~The classifier makes a three way distinction between 
Person, Location and Organization; names where the classi- 
fier makes no decision were classified as Other Named E~tity. 
297 
$25 million) were added to the system shortly after 
the Trec run, but other gaps in coverage remain. We 
discuss this further in section 3. 
2.4 Query  Classif ication 
This step involves processing the query to identify 
the category of answer the user is seeking. We parse 
the query, then use the following rules to determine 
the category of the desired answer: 
? Who, Whom -+ Person. 
? Where, Whence, Whither--+ Locat ion.  
? When -+ Date. 
? How few, great, little, many, much -+ 
Quemtity. We also extract the head word of 
the How expression (e.g., stooges in how many 
stooges) for later comparison to the head word 
of candidate answers. 
? How long --+ Duration or Linear Measure. 
How tall, wide, high, big, far --+ Linear 
Measure. 
? The wh-words Which or What typically appear 
with a head noun that describes the category 
of entity involved. These questions fall into two 
formats: What  X where X is the noun involved, 
and What  is the ... X. Here are a couple of 
examples: 
What  company is the largest Japanese 
ship builder? 
What  is the largest city in Germany? 
For these queries the head noun (e.g., compa- 
ny or city) is extracted, and a lexicon map- 
ping nouns to categories is used to identify the 
category of the query. The lexicon was partly 
hand-built (including some common cases such 
as number --+ Quant i ty  or year --~ Date). A 
large list of nouns indicating Person, Locat ion  
or Organ izat ion  categories was automatical- 
ly taken from the contextual (appositive) cues 
learned in the named entity classifier described 
in (Collins and Singer, 1999). 
? In queries containing no wh-word (e.g., Name 
the largest city in Germany), the first noun 
phrase that is an immediate constituent of the 
matrix sentence is extracted, and its head is 
used to determine query category, as for What 
X questions. 
? Otherwise, the category is the wildcard Any. 
2.5 Ent i ty  Rank ing  
Entity scores have two components. The first, most- 
significant, component is whether or not the entity's 
category matches the query's category. (If the query 
category is Any, all entities match it.) 
In most cases, the matching is boolean: either an 
entity has the correct category or not. However, 
there are a couple of special cases where finer distinc- 
tions are made. If a question is of the Date type, and 
the query contains one of the words day or month, 
then "full" dates are ranked above years. Converse- 
ly, if the query contains the word year, then years are 
ranked above full dates. In How many X questions 
(where X is a noun), quantified phrases whose head 
noun is also X are ranked above bare numbers or 
other quantified phrases: for example, in the query 
How many lives were lost in the Lockerbie air crash, 
entities such as 270 lives or almost 300 lives would 
be ranked above entities such as 200 pumpkins or 
150. 2 
The second component of the entity score is based 
on the frequency and position of occurrences of a 
given entity within the retrieved passages. Each oc- 
currence of an entity in a top-ranked passage counts 
10 points, and each occurrence of an entity in any 
other passage counts 1 point. ("Top-ranked pas- 
sage" means the passage or passages that received 
the maximal score from the passage retrieval compo- 
nent.) This score component is used as a secondary 
sort key, to impose a ranking on entities that are not 
distinguished by the first score component. 
In counting occurrences of entities, it is necessary 
to decide whether or not two occurrences are to- 
kens of the same entity or different entities. To this 
end, we do some normalization of entities. Dates 
are mapped to the format year-month-day: that is, 
last Tuesday, November 9, 1999 and 11/9/99 are 
both mapped to the normal form 1999 Nov 9 before 
frequencies are counted. Person names axe aliased 
based on the final word they contain. For example, 
Jackson and Michael Jackson are both mapped to 
the normal form Jackson. a 
3 Eva luat ion  
3.1 Resul ts  on the TREC-8  Evaluat ion 
The system was evaluated in the TREC-8 question- 
answering track. TREC provided 198 questions as a 
blind test set: systems were required to provide five 
potential answers for each question, ranked in or- 
der of plausibility. The output from each system 
was then scored by hand by evaluators at NIST, 
each answer being marked as either correct or in- 
correct. The system's core on a particular question 
is a function of whether it got a correct answer in the 
five ranked answers, with higher scores for the an- 
swer appearing higher in the ranking. The system 
receives a score of 1, 1/2, 1/3, 1/4, 1/5, or 0, re- 
2perhaps less desirably, people would not be recognized 
as a synonym of lives in this example: 200 people would be 
indistinguishable from 200 pumpkins. 
3This does introduce occasional errors, when two people 
with the same last name appear in retrieved passages. 
298 
System Mean Answer Mean 
Ans Len in Top 5 Score 
Entity 10.5 B 46% 0.356 
Passage 50 50 B 38.9% 0.261 
Passage 250 250 B 68% 0.545 
Figure 1: Results on the TREC-8 Evaluation 
spectively, according as the correct answer is ranked 
1st, 2nd, 3rd, 4th, 5th, or lower in the system out- 
put. The final score for a system is calculated as its 
mean score on the 198 questions. 
The TREC evaluation considered two question- 
answering scenarios: one where answers were lim- 
ited to be less than 250 bytes in length, the other 
where the limit was 50 bytes. The output from the 
passage retrieval component (section 2.1), with some 
trimming of passages to ensure they were less than 
250 bytes, was submitted to the 250 byte scenario. 
The output of the full entity-based system was sub- 
mitted to the 50 byte track. For comparison, we also 
submitted the output of a 50-byte system based on 
IR techniques alone. In this system single-sentence 
passages were retrieved as potential answers, their 
score being calculated using conventional IR meth- 
ods. Some trimming of sentences so that they were 
less than 50 bytes in length was performed. 
Figure 1 shows results on the TREC-8 evaluation. 
The 250-byte passage-based system found a correct 
answer somewhere in the top five answers on 68% of 
the questions, with a final score of 0.545. The 50- 
byte passage-based system found a correct answer 
on 38.9% of all questions, with an average score of 
0.261. The reduction in accuracy when moving from 
the 250-byte limit to the 50-byte limit is expected, 
because much higher precision is required; the 50- 
byte limit allows much less extraneous material to 
be included with the answer. The benefit of the 
including less extraneous material is that the user 
can interpret the output with much less effort. 
Our entity-based system found a correct answer in 
the top five answers on 46% of the questions, with 
a final score of 0.356. The performance is not as 
good as that of the 250-byte passage-based system. 
But when less extraneous material is permitted, the 
entity-based system outperforms the passage-based 
approach. The accuracy of the entity-based sys- 
tem is significantly better than that of the 50-byte 
passage-based system, and it returns virtually no ex- 
traneous material, as reflected in the average answer 
length of only 10.5 bytes. The implication is that 
NLP techniques become increasingly useful when 
short answers are required. 
3.2 Error Analysis of the Ent i ty-Based 
System 
3.2.1 Ranking of Answers 
As a first point, we looked at the performance ofthe 
entity-based system, considering the queries where 
the correct answer was found somewhere in the top 
5 answers (46% of the 198 questions). We found that 
on these questions, the percentage ofanswers ranked 
1, 2, 3, 4, and 5 was 66%, 14%, 11%, 4%, and 4% 
respectively. This distribution is by no means uni- 
form; it is clear that when the answer is somewhere 
in the top five, it is very likely to be ranked 1st or 
2nd. The system's performance is quite bimodah 
it either completely fails to get the answer, or else 
recovers it with a high ranking. 
3.2.2 Accuracy on Different Categories 
Figure 2 shows the distribution of question types 
in the TREC-8 test set ("Percentage of Q's"), and 
the performance ofthe entity-based system by ques- 
tion type ("System Accuracy"). We categorized the 
questions by hand, using the eight categories de- 
scribed in section 2.3, plus two categories that es- 
sentially represent types that were not handled by 
the system at the time of the TREC competition: 
Monetary Amount and Miscellaneous. 
"System Accuracy" means the percentage ofques- 
tions for which the correct answer was in the top five 
returned by the system. There is a sharp division in 
the performance on different question types. The 
categories Person, Location, Date and Quantity 
are handled fairly well, with the correct answer ap- 
pearing in the top five 60% of the time. These four 
categories make up 67% of all questions. In contrast, 
the other question types, accounting for 33% of the 
questions, are handled with only 15% accuracy. 
Unsurprisingly, the Miscellaneous and Other 
Named Ent i ty  categories are problematic; unfortu- 
nately, they are also rather frequent. Figure 3 shows 
some examples of these queries. They include a large 
tail of questions eeking other entity types (moun- 
tain ranges, growth rates, films, etc.) and questions 
whose answer is not even an entity (e.g., "Why did 
David Koresh ask the FBI for a word processor?") 
For reference, figure 4 gives an impression of the 
sorts of questions that the system does well on (cor- 
rect answer in top five). 
3.2.3 Errors by Component 
Finally, we performed an analysis to gauge which 
components represent performance bottlenecks in 
the current system. We examined system logs for 
a 50-question sample, and made a judgment of what 
caused the error, when there was an error. Figure 5 
gives the breakdown. Each question was assigned to 
exactly one line of the table. 
The largest body of errors, accounting for 18% of 
the questions, are those that are due to unhandled 
299 
Question I Rank I Output from System 
Who is the author of the book, The Iron Lady: A Biography of 2 
Margaret Thatcher? 
What is the name of the managing director of Apricot Computer? i 
What country is the biggest producer of tungsten? 
Who was the first Taiwanese President? 
When did Nixon visit China? 
How many calories are there in a Big Mac? 4 
What is the acronym for the rating system for air conditioner effi- 1 
ciency? 
Hugo Young 
Dr Peter Horne 
China 
Taiwanese President Li 
Teng hui 
1972 
562 calories 
EER 
Figure 4: A few TREC questions answered correctly by the system. 
Type Percent 
of Q's 
System 
Accuracy 
Person 28 62.5 
Location 18.5 67.6 
Date 11 45.5 
Quantity 9.5 52.7 
TOTAL 67 60 
Other Named Ent 
Miscellaneous 
Linear Measure 
Monetary Amt 
Organization 
Duration 
14.5 
8.5 
3.5 
3 
2 
1.5 
33 TOTAL 
31 
5.9 
0 
0 
0 
0 
15 
Errors 
Passage retrieval failed 
Answer is not an entity 
Answer of unhandled type: money 
Answer of unhandled type: misc 
Entity extraction failed 
Entity classification failed 
Query classification failed 
Entity ranking failed 
16% 
4% 
10% 
8% 
2% 
4% 
4% 
4% 
Successes 
Answer at Rank 2-5 I 16% 
Answer at Rank 1 I 32% 
TOTAL 
Figure 2: Performance ofthe entity-based system on 
different question types. "System Accuracy" means 
percent of questions for which the correct answer 
was in the top five returned by the system. "Good" 
types are in the upper block, "Bad" types are in the 
lower block. 
What does the Peugeot company manufacture? 
Why did David Koresh ask the FBI for a word 
processor? 
What are the Valdez Principles? 
What was the target rate for M3 growth in 1992? 
What does El Nino mean in spanish? 
Figure 5: Breakdown of questions by error type, in 
particular, by component responsible. Numbers are 
percent of questions in a 50-question sample. 
five, but not at rank one, are almost all due to fail- 
ures of entity ranking) Various factors contributing 
to misrankings are the heavy weighting assigned to 
answers in the top-ranked passage, the failure to ad- 
just frequencies by "complexity" (e.g., it is signifi- 
cant if 22.5 million occurs everal times, but not if 3 
occurs several times), and the failure of the system 
to consider the linguistic context in which entities 
appear. 
Figure 3: Examples of "Other Named Entity" and 
Miscellaneous questions. 
types, of which half are monetary amounts. (Ques- 
tions with non-entity answers account for another 
4%.) Another large block (16%) is due to the pas- 
sage retrieval component: the correct answer was 
not present in the retrieved passages. The linguistic 
components ogether account for the remaining 14% 
of error, spread evenly among them. 
The cases in which the correct answer is in the top 
4 Conc lus ions  and  Future  Work  
We have described a system that handles arbi- 
trary questions, producing a candidate list of an- 
swers ranked by their plausibility. Evaluation on 
the TREC question-answering track showed that the 
correct answer to queries appeared in the top five an- 
swers 46% of the time, with a mean score of 0.356. 
The average length of answers produced by the sys- 
tem was 10.5 bytes. 
4The sole exception was a query misclassification caused 
by a parse failure---miraculously, the correct answer made it 
to rank five despite being of the "wrong" type. 
300 
There are several possible areas for future work. 
There may be potential for improved performance 
through more sophisticated use of NLP techniques. 
In particular, the syntactic ontext in which a par- 
ticular entity appears may provide important infor- 
mation, but it is not currently used by the system. 
Another area of future work is to extend the 
entity-extraction component of the system to han- 
dle arbitrary types (mountain ranges, films etc.). 
The error analysis in section 3.2.2 showed that these 
question types cause particular difficulties for the 
system. 
The system is largely hand-built. It is likely that 
as more features are added a trainable statistical or 
machine learning approach to the problem will be- 
come increasingly desirable. This entails developing 
a training set of question-answer pairs, raising the 
question of how a relatively large corpus of questions 
can be gathered and annotated. 
Re ferences  
Steven Abney. 1996. Partial parsing via finite- 
state cascades. J Natural Language Engineering, 
2(4):337-344, December. 
C. Buckley and A.F. Lewit. 1985. Optimization of 
inverted vector searches. In Proe. Eighth Interna- 
tional ACM SIGIR Conference, pages 97-110. 
Michael Collins and Yoram Singer. 1999. Unsuper- 
vised models for named entity classification. In 
EMNLP. 
G. Salton, editor. 1971. The Smart Retrieval Sys- 
tem - Experiments in Automatic Document Pro- 
cessing. Prentice-Hall, Inc., Englewood Cliffs, NJ. 
301 
New Ranking Algorithms for Parsing and Tagging:
Kernels over Discrete Structures, and the Voted Perceptron
Michael Collins
AT&T Labs-Research,
Florham Park,
New Jersey.
mcollins@research.att.com
Nigel Duffy
iKuni Inc.,
3400 Hillview Ave., Building 5,
Palo Alto, CA 94304.
nigeduff@cs.ucsc.edu
Abstract
This paper introduces new learning al-
gorithms for natural language processing
based on the perceptron algorithm. We
show how the algorithms can be efficiently
applied to exponential sized representa-
tions of parse trees, such as the ?all sub-
trees? (DOP) representation described by
(Bod 1998), or a representation tracking
all sub-fragments of a tagged sentence.
We give experimental results showing sig-
nificant improvements on two tasks: pars-
ing Wall Street Journal text, and named-
entity extraction from web data.
1 Introduction
The perceptron algorithm is one of the oldest algo-
rithms in machine learning, going back to (Rosen-
blatt 1958). It is an incredibly simple algorithm to
implement, and yet it has been shown to be com-
petitive with more recent learning methods such as
support vector machines ? see (Freund & Schapire
1999) for its application to image classification, for
example.
This paper describes how the perceptron and
voted perceptron algorithms can be used for pars-
ing and tagging problems. Crucially, the algorithms
can be efficiently applied to exponential sized repre-
sentations of parse trees, such as the ?all subtrees?
(DOP) representation described by (Bod 1998), or a
representation tracking all sub-fragments of a tagged
sentence. It might seem paradoxical to be able to ef-
ficiently learn and apply a model with an exponential
number of features.1 The key to our algorithms is the
1Although see (Goodman 1996) for an efficient algorithm
for the DOP model, which we discuss in section 7 of this paper.
?kernel? trick ((Cristianini and Shawe-Taylor 2000)
discuss kernel methods at length). We describe how
the inner product between feature vectors in these
representations can be calculated efficiently using
dynamic programming algorithms. This leads to
polynomial time2 algorithms for training and apply-
ing the perceptron. The kernels we describe are re-
lated to the kernels over discrete structures in (Haus-
sler 1999; Lodhi et al 2001).
A previous paper (Collins and Duffy 2001)
showed improvements over a PCFG in parsing the
ATIS task. In this paper we show that the method
scales to far more complex domains. In parsing Wall
Street Journal text, the method gives a 5.1% relative
reduction in error rate over the model of (Collins
1999). In the second domain, detecting named-
entity boundaries in web data, we show a 15.6% rel-
ative error reduction (an improvement in F-measure
from 85.3% to 87.6%) over a state-of-the-art model,
a maximum-entropy tagger. This result is derived
using a new kernel, for tagged sequences, described
in this paper. Both results rely on a new approach
that incorporates the log-probability from a baseline
model, in addition to the ?all-fragments? features.
2 Feature?Vector Representations of Parse
Trees and Tagged Sequences
This paper focuses on the task of choosing the cor-
rect parse or tag sequence for a sentence from a
group of ?candidates? for that sentence. The candi-
dates might be enumerated by a number of methods.
The experiments in this paper use the top   candi-
dates from a baseline probabilistic model: the model
of (Collins 1999) for parsing, and a maximum-
entropy tagger for named-entity recognition.
2i.e., polynomial in the number of training examples, and
the size of trees or sentences in training and test data.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 263-270.
                         Proceedings of the 40th Annual Meeting of the Association for
The choice of representation is central: what fea-
tures should be used as evidence in choosing be-
tween candidates? We will use a function
	


to denote a  -dimensional feature vector that rep-
resents a tree or tagged sequence

. There are many
possibilities for

. An obvious example for parse
trees is to have one component of

for each
rule in a context-free grammar that underlies the
trees. This is the representation used by Stochastic
Context-Free Grammars. The feature vector tracks
the counts of rules in the tree

, thus encoding the
sufficient statistics for the SCFG.
Given a representation, and two structures

and

, the inner product between the structures can be
defined as





Ranking Algorithms for Named?Entity Extraction:
Boosting and the Voted Perceptron
Michael Collins
AT&T Labs-Research, Florham Park, New Jersey.
mcollins@research.att.com
Abstract
This paper describes algorithms which
rerank the top N hypotheses from a
maximum-entropy tagger, the applica-
tion being the recovery of named-entity
boundaries in a corpus of web data. The
first approach uses a boosting algorithm
for ranking problems. The second ap-
proach uses the voted perceptron algo-
rithm. Both algorithms give compara-
ble, significant improvements over the
maximum-entropy baseline. The voted
perceptron algorithm can be considerably
more efficient to train, at some cost in
computation on test examples.
1 Introduction
Recent work in statistical approaches to parsing and
tagging has begun to consider methods which in-
corporate global features of candidate structures.
Examples of such techniques are Markov Random
Fields (Abney 1997; Della Pietra et al 1997; John-
son et al 1999), and boosting algorithms (Freund et
al. 1998; Collins 2000; Walker et al 2001). One
appeal of these methods is their flexibility in incor-
porating features into a model: essentially any fea-
tures which might be useful in discriminating good
from bad structures can be included. A second ap-
peal of these methods is that their training criterion
is often discriminative, attempting to explicitly push
the score or probability of the correct structure for
each training sentence above the score of competing
structures. This discriminative property is shared by
the methods of (Johnson et al 1999; Collins 2000),
and also the Conditional Random Field methods of
(Lafferty et al 2001).
In a previous paper (Collins 2000), a boosting al-
gorithm was used to rerank the output from an ex-
isting statistical parser, giving significant improve-
ments in parsing accuracy on Wall Street Journal
data. Similar boosting algorithms have been applied
to natural language generation, with good results, in
(Walker et al 2001). In this paper we apply rerank-
ing methods to named-entity extraction. A state-of-
the-art (maximum-entropy) tagger is used to gener-
ate 20 possible segmentations for each input sen-
tence, along with their probabilities. We describe
a number of additional global features of these can-
didate segmentations. These additional features are
used as evidence in reranking the hypotheses from
the max-ent tagger. We describe two learning algo-
rithms: the boosting method of (Collins 2000), and a
variant of the voted perceptron algorithm, which was
initially described in (Freund & Schapire 1999). We
applied the methods to a corpus of over one million
words of tagged web data. The methods give signif-
icant improvements over the maximum-entropy tag-
ger (a 17.7% relative reduction in error-rate for the
voted perceptron, and a 15.6% relative improvement
for the boosting method).
One contribution of this paper is to show that ex-
isting reranking methods are useful for a new do-
main, named-entity tagging, and to suggest global
features which give improvements on this task. We
should stress that another contribution is to show
that a new algorithm, the voted perceptron, gives
very credible results on a natural language task. It is
an extremely simple algorithm to implement, and is
very fast to train (the testing phase is slower, but by
no means sluggish). It should be a viable alternative
to methods such as the boosting or Markov Random
Field algorithms described in previous work.
2 Background
2.1 The data
Over a period of a year or so we have had over one
million words of named-entity data annotated. The
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 489-496.
                         Proceedings of the 40th Annual Meeting of the Association for
data is drawn from web pages, the aim being to sup-
port a question-answering system over web data. A
number of categories are annotated: the usual peo-
ple, organization and location categories, as well as
less frequent categories such as brand-names, scien-
tific terms, event titles (such as concerts) and so on.
From this data we created a training set of 53,609
sentences (1,047,491 words), and a test set of 14,717
sentences (291,898 words).
The task we consider is to recover named-entity
boundaries. We leave the recovery of the categories
of entities to a separate stage of processing.1 We
evaluate different methods on the task through pre-
cision and recall. If a method proposes   entities on
the test set, and  of these are correct (i.e., an entity is
marked by the annotator with exactly the same span
as that proposed) then the precision of a method is
	


 
. Similarly, if  is the total number of en-
tities in the human annotated version of the test set,
then the recall is  
 .
2.2 The baseline tagger
The problem can be framed as a tagging task ? to
tag each word as being either the start of an entity,
a continuation of an entity, or not to be part of an
entity at all (we will use the tags S, C and N respec-
tively for these three cases). As a baseline model
we used a maximum entropy tagger, very similar to
the ones described in (Ratnaparkhi 1996; Borthwick
et. al 1998; McCallum et al 2000). Max-ent tag-
gers have been shown to be highly competitive on a
number of tagging tasks, such as part-of-speech tag-
ging (Ratnaparkhi 1996), named-entity recognition
(Borthwick et. al 1998), and information extraction
tasks (McCallum et al 2000). Thus the maximum-
entropy tagger we used represents a serious baseline
for the task. We used the following features (sev-
eral of the features were inspired by the approach
of (Bikel et. al 1999), an HMM model which gives
excellent results on named entity extraction):
 The word being tagged, the previous word, and
the next word.
 The previous tag, and the previous two tags (bi-
gram and trigram features).
1In initial experiments, we found that forcing the tagger to
recover categories as well as the segmentation, by exploding the
number of tags, reduced performance on the segmentation task,
presumably due to sparse data problems.
 A compound feature of three fields: (a) Is the
word at the start of a sentence?; (b) does the word
occur in a list of words which occur more frequently
as lower case rather than upper case words in a large
corpus of text? (c) the type of the first letter  of
the word, where     is defined as ?A? if  is a
capitalized letter, ?a? if  is a lower-case letter, ?0?
if  is a digit, and  otherwise. For example, if the
word Animal is seen at the start of a sentence, and
it occurs in the list of frequent lower-cased words,
then it would be mapped to the feature 1-1-A.
 The word with each character mapped to its

 
. For example, G.M. would be mapped to
A.A., and Animal would be mapped to Aaaaaa.
 The word with each character mapped to its
type, but repeated consecutive character types are
not repeated in the mapped string. For example, An-
imal would be mapped to Aa, G.M. would again be
mapped to A.A..
The tagger was applied and trained in the same
way as described in (Ratnaparkhi 1996). The feature
templates described above are used to create a set of
Discriminative Training Methods for Hidden Markov Models:
Theory and Experiments with Perceptron Algorithms
Michael Collins
AT&T Labs-Research, Florham Park, New Jersey.
mcollins@research.att.com
Abstract
We describe new algorithms for train-
ing tagging models, as an alternative
to maximum-entropy models or condi-
tional random elds (CRFs). The al-
gorithms rely on Viterbi decoding of
training examples, combined with sim-
ple additive updates. We describe the-
ory justifying the algorithms through
a modication of the proof of conver-
gence of the perceptron algorithm for
classication problems. We give exper-
imental results on part-of-speech tag-
ging and base noun phrase chunking, in
both cases showing improvements over
results for a maximum-entropy tagger.
1 Introduction
Maximum-entropy (ME) models are justiably
a very popular choice for tagging problems in
Natural Language Processing: for example see
(Ratnaparkhi 96) for their use on part-of-speech
tagging, and (McCallum et al 2000) for their
use on a FAQ segmentation task. ME models
have the advantage of being quite exible in the
features that can be incorporated in the model.
However, recent theoretical and experimental re-
sults in (Laerty et al 2001) have highlighted
problems with the parameter estimation method
for ME models. In response to these problems,
they describe alternative parameter estimation
methods based on Conditional Markov Random
Fields (CRFs). (Laerty et al 2001) give exper-
imental results suggesting that CRFs can per-
form signicantly better than ME models.
In this paper we describe parameter estima-
tion algorithms which are natural alternatives to
CRFs. The algorithms are based on the percep-
tron algorithm (Rosenblatt 58), and the voted
or averaged versions of the perceptron described
in (Freund & Schapire 99). These algorithms
have been shown by (Freund & Schapire 99) to
be competitive with modern learning algorithms
such as support vector machines; however, they
have previously been applied mainly to classi-
cation tasks, and it is not entirely clear how the
algorithms can be carried across to NLP tasks
such as tagging or parsing.
This paper describes variants of the percep-
tron algorithm for tagging problems. The al-
gorithms rely on Viterbi decoding of training
examples, combined with simple additive up-
dates. We describe theory justifying the algo-
rithm through a modication of the proof of con-
vergence of the perceptron algorithm for classi-
cation problems. We give experimental results
on part-of-speech tagging and base noun phrase
chunking, in both cases showing improvements
over results for a maximum-entropy tagger (a
11.9% relative reduction in error for POS tag-
ging, a 5.1% relative reduction in error for NP
chunking). Although we concentrate on tagging
problems in this paper, the theoretical frame-
work and algorithm described in section 3 of
this paper should be applicable to a wide va-
riety of models where Viterbi-style algorithms
can be used for decoding: examples are Proba-
bilistic Context-Free Grammars, or ME models
for parsing. See (Collins and Duy 2001; Collins
and Duy 2002; Collins 2002) for other applica-
tions of the voted perceptron to NLP problems.
1
2 Parameter Estimation
2.1 HMM Taggers
In this section, as a motivating example, we de-
scribe a special case of the algorithm in this
paper: the algorithm applied to a trigram tag-
ger. In a trigram HMM tagger, each trigram
1
The theorems in section 3, and the proofs in sec-
tion 5, apply directly to the work in these other papers.
                                            Association for Computational Linguistics.
                        Language Processing (EMNLP), Philadelphia, July 2002, pp. 1-8.
                         Proceedings of the Conference on Empirical Methods in Natural
of tags and each tag/word pair have associated
parameters. We write the parameter associated
with a trigram hx; y; zi as 
x;y;z
, and the param-
eter associated with a tag/word pair (t; w) as

t;w
. A common approach is to take the param-
eters to be estimates of conditional probabilities:

x;y;z
= logP (z j x; y), 
t;w
= logP (w j t).
For convenience we will use w
[1:n]
as short-
hand for a sequence of words [w
1
; w
2
: : : w
n
],
and t
[1:n]
as shorthand for a taq sequence
[t
1
; t
2
: : : t
n
]. In a trigram tagger the score for
a tagged sequence t
[1:n]
paired with a word se-
quence w
[1:n]
is
2
P
n
i=1

t
i 2
;t
i 1
;t
i
+
P
n
i=1

t
i
;w
i
.
When the parameters are conditional probabil-
ities as above this \score" is an estimate of the
log of the joint probability P (w
[1:n]
; t
[1:n]
). The
Viterbi algorithm can be used to nd the highest
scoring tagged sequence under this score.
As an alternative to maximum{likelihood pa-
rameter estimates, this paper will propose the
following estimation algorithm. Say the train-
ing set consists of n tagged sentences, the i'th
sentence being of length n
i
. We will write these
examples as (w
i
[1:n
i
]
; t
i
[1:n
i
]
) for i = 1 : : : n. Then
the training algorithm is as follows:
 Choose a parameter T dening the number
of iterations over the training set.
3
 Initially set al parameters 
x;y;z
and 
t;w
to be zero.
 For t = 1 : : : T; i = 1 : : : n: Use the Viterbi
algorithm to nd the best tagged sequence for
sentence w
i
[1:n
i
]
under the current parameter
settings: we call this tagged sequence z
[1:n
i
]
.
For every tag trigram hx; y; zi seen c
1
times in
t
i
[1:n
i
]
and c
2
times in z
[1:n
i
]
where c
1
6= c
2
set

x;y;z
= 
x;y;z
+ c
1
  c
2
. For every tag/word
pair ht; wi seen c
1
times in (w
i
[1:n
i
]
; t
i
[1:n
i
]
) and
c
2
times in (w
i
[1:n
i
]
; z
[1:n
i
]
) where c
1
6= c
2
set

t;w
= 
t;w
+ c
1
  c
2
.
As an example, say the i'th tagged sentence
(w
i
[1:n
i
]
; t
i
[1:n
i
]
) in training data is
the/D man/N saw/V the/D dog/N
and under the current parameter settings the
highest scoring tag sequence (w
i
[1:n
i
]
; z
[1:n
i
]
) is
2
We take t
 1
and t
 2
to be special NULL tag symbols.
3
T is usually chosen by tuning on a development set.
the/D man/N saw/N the/D dog/N
Then the parameter update will add 1 to the
parameters 
D;N;V
, 
N;V;D
, 
V;D;N
, 
V;saw
and
subtract 1 from the parameters 
D;N;N
, 
N;N;D
,

N;D;N
, 
N;saw
. Intuitively this has the ef-
fect of increasing the parameter values for fea-
tures which were \missing" from the proposed
sequence z
[1:n
i
]
, and downweighting parameter
values for \incorrect" features in the sequence
z
[1:n
i
]
. Note that if z
[1:n
i
]
= t
i
[1:n
i
]
| i.e., the
proposed tag sequence is correct | no changes
are made to the parameter values.
2.2 Local and Global Feature Vectors
We now describe how to generalize the algorithm
to more general representations of tagged se-
quences. In this section we describe the feature-
vector representations which are commonly used
in maximum-entropy models for tagging, and
which are also used in this paper.
In maximum-entropy taggers (Ratnaparkhi
96; McCallum et al 2000), the tagging prob-
lem is decomposed into sequence of decisions in
tagging the problem in left-to-right fashion. At
each point there is a \history" { the context in
which a tagging decision is made { and the task
is to predict the tag given the history. Formally,
a history is a 4-tuple ht
 1
; t
 2
; w
[1:n]
; ii where
t
 1
; t
 2
are the previous two tags, w
[1:n]
is an ar-
ray specifying the n words in the input sentence,
and i is the index of the word being tagged. We
use H to denote the set of all possible histories.
Maximum-entropy models represent the tag-
ging task through a feature-vector representation
of history-tag pairs. A feature vector representa-
tion  : HT ! R
d
is a function  that maps a
history{tag pair to a d-dimensional feature vec-
tor. Each component 
s
(h; t) for s = 1 : : : d
could be an arbitrary function of (h; t). It is
common (e.g., see (Ratnaparkhi 96)) for each
feature 
s
to be an indicator function. For ex-
ample, one such feature might be

1000
(h; t) =
8
>
<
>
:
1 if current word w
i
is the
and t = DT
0 otherwise
Similar features might be dened for every
word/tag pair seen in training data. Another
feature type might track trigrams of tags, for ex-
ample 
1001
(h; t) = 1 if ht
 2
; t
 1
; ti = hD, N, Vi
and 0 otherwise. Similar features would be de-
ned for all trigrams of tags seen in training. A
real advantage of these models comes from the
freedom in dening these features: for example,
(Ratnaparkhi 96; McCallum et al 2000) both
describe feature sets which would be di?cult to
incorporate in a generative model.
In addition to feature vector representations
of history/tag pairs, we will nd it convenient
to dene feature vectors of (w
[1:n]
; t
[1:n]
) pairs
where w
[1:n]
is a sequence of n words, and t
[1:n]
is an entire tag sequence. We use  to de-
note a function from (w
[1:n]
; t
[1:n]
) pairs to d-
dimensional feature vectors. We will often refer
to  as a \global" representation, in contrast
to  as a \local" representation. The particular
global representations considered in this paper
are simple functions of local representations:

s
(w
[1:n]
; t
[1:n]
) =
n
X
i=1

s
(h
i
; t
i
) (1)
where h
i
= ht
i 1
; t
i 2
; w
[1:n]
; ii. Each global
feature 
s
(w
[1:n]
; t
[1:n]
) is simply the value for
the local representation 
s
summed over all his-
tory/tag pairs in (w
[1:n]
; t
[1:n]
). If the local fea-
tures are indicator functions, then the global fea-
tures will typically be \counts". For example,
with 
1000
dened as above, 
1000
(w
[1:n]
; t
[1:n]
)
is the number of times the is seen tagged as DT
in the pair of sequences (w
[1:n]
; t
[1:n]
).
2.3 Maximum-Entropy Taggers
In maximum-entropy taggers the feature vectors
 together with a parameter vector  2 R
d
are
used to dene a conditional probability distri-
bution over tags given a history as
P (t j h; ) =
e
P
s

s

s
(h;t)
Z(h; )
where Z(h; ) =
P
l2T
e
P
s

s

s
(h;l)
. The log of
this probability has the form log p(t j h; ) =
P
d
s=1

s

s
(h; t)  logZ(h; ), and hence the log
probability for a (w
[1:n]
; t
[1:n]
) pair will be
X
i
d
X
s=1

s

s
(h
i
; t
i
) 
X
i
logZ(h
i
; ) (2)
where h
i
= ht
i 1
; t
i 2
; w
[1:n]
; ii. Given parame-
ter values , and an input sentence w
[1:n]
, the
highest probability tagged sequence under the
formula in Eq. 2 can be found e?ciently using
the Viterbi algorithm.
The parameter vector  is estimated from a
training set of sentence/tagged-sequence pairs.
Maximum-likelihood parameter values can be
estimated using Generalized Iterative Scaling
(Ratnaparkhi 96), or gradient descent methods.
In some cases it may be preferable to apply a
bayesian approach which includes a prior over
parameter values.
2.4 A New Estimation Method
We now describe an alternative method for es-
timating parameters of the model. Given a se-
quence of words w
[1:n]
and a sequence of part of
speech tags, t
[1:n]
, we will take the \score" of a
tagged sequence to be
n
X
i=1
d
X
s=1

s

s
(h
i
; t
i
) =
d
X
s=1

s

s
(w
[1:n]
; t
[1:n]
) :
where h
i
is again ht
i 1
; t
i 2
; w
[1:n]
; ii. Note that
this is almost identical to Eq. 2, but without the
local normalization terms logZ(h
i
; ). Under
this method for assigning scores to tagged se-
quences, the highest scoring sequence of tags for
an input sentence can be found using the Viterbi
algorithm. (We can use an almost identical de-
coding algorithm to that for maximum-entropy
taggers, the dierence being that local normal-
ization terms do not need to be calculated.)
We then propose the training algorithm in g-
ure 1. The algorithm takes T passes over the
training sample. All parameters are initially set
to be zero. Each sentence in turn is decoded us-
ing the current parameter settings. If the high-
est scoring sequence under the current model is
not correct, the parameters 
s
are updated in a
simple additive fashion.
Note that if the local features 
s
are indica-
tor functions, then the global features 
s
will be
counts. In this case the update will add c
s
  d
s
to each parameter 
s
, where c
s
is the number
of times the s'th feature occurred in the cor-
rect tag sequence, and d
s
is the number of times
Inputs: A training set of tagged sentences,
(w
i
[1:n
i
]
; t
i
[1:n
i
]
) for i = 1 : : : n. A parameter T
specifying number of iterations over the training set. A
\local representation"  which is a function that maps
history/tag pairs to d-dimensional feature vectors. The
global representation  is dened through  as in Eq. 1.
Initialization: Set parameter vector  = 0.
Algorithm:
For t = 1 : : : T; i = 1 : : : n
 Use the Viterbi algorithm to nd the output of the
model on the i'th training sentence with the current pa-
rameter settings, i.e.,
z
[1:n
i
]
= argmax
u
[1:n
i
]
2T
n
i
P
s

s

s
(w
i
[1:n
i
]
; u
[1:n
i
]
)
where T
n
i
is the set of all tag sequences of length n
i
.
 If z
[1:n
i
]
6= t
i
[1:n
i
]
then update the parameters

s
= 
s
+
s
(w
i
[1:n
i
]
; t
i
[1:n
i
]
)   
s
(w
i
[1:n
i
]
; z
[1:n
i
]
)
Output: Parameter vector .
Figure 1: The training algorithm for tagging.
it occurs in highest scoring sequence under the
current model. For example, if the features 
s
are indicator functions tracking all trigrams and
word/tag pairs, then the training algorithm is
identical to that given in section 2.1.
2.5 Averaging Parameters
There is a simple renement to the algorithm
in gure 1, called the \averaged parameters"
method. Dene 
t;i
s
to be the value for the s'th
parameter after the i'th training example has
been processed in pass t over the training data.
Then the \averaged parameters" are dened as

s
=
P
t=1:::T;i=1:::n

t;i
s
=nT for all s = 1 : : : d.
It is simple to modify the algorithm to store
this additional set of parameters. Experiments
in section 4 show that the averaged parameters
perform signicantly better than the nal pa-
rameters 
T;n
s
. The theory in the next section
gives justication for the averaging method.
3 Theory Justifying the Algorithm
In this section we give a general algorithm for
problems such as tagging and parsing, and give
theorems justifying the algorithm. We also show
how the tagging algorithm in gure 1 is a spe-
cial case of this algorithm. Convergence theo-
rems for the perceptron applied to classication
problems appear in (Freund & Schapire 99) {
the results in this section, and the proofs in sec-
tion 5, show how the classication results can be
Inputs: Training examples (x
i
; y
i
)
Initialization: Set  = 0
Algorithm:
For t = 1 : : : T , i = 1 : : : n
Calculate z
i
= argmax
z2GEN(x
i
)
(x
i
; z)  
If(z
i
6= y
i
) then  =  +(x
i
; y
i
)   (x
i
; z
i
)
Output: Parameters 
Figure 2: A variant of the perceptron algorithm.
carried over to problems such as tagging.
The task is to learn a mapping from inputs
x 2 X to outputs y 2 Y. For example, X might
be a set of sentences, with Y being a set of pos-
sible tag sequences. We assume:
 Training examples (x
i
; y
i
) for i = 1 : : : n.
 A function GEN which enumerates a set of
candidates GEN(x) for an input x.
 A representation  mapping each (x; y) 2
X  Y to a feature vector (x; y) 2 R
d
.
 A parameter vector  2 R
d
.
The componentsGEN; and  dene a map-
ping from an input x to an output F (x) through
F (x) = arg max
y2GEN(x)
(x; y)  
where (x; y)   is the inner product
P
s

s

s
(x; y). The learning task is to set the
parameter values  using the training examples
as evidence.
The tagging problem in section 2 can be
mapped to this setting as follows:
 The training examples are sentence/tagged-
sequence pairs: x
i
= w
i
[1:n
i
]
and y
i
= t
i
[1:n
i
]
for i = 1 : : : n.
 Given a set of possible tags T , we dene
GEN(w
[1:n]
) = T
n
, i.e., the function GEN
maps an input sentence w
[1:n]
to the set of
all tag sequences of length n.
 The representation (x; y) =
(w
[1:n]
; t
[1:n]
) is dened through local
feature vectors (h; t) where (h; t) is a
history/tag pair. (See Eq. 1.)
Figure 2 shows an algorithm for setting the
weights . It can be veried that the training
algorithm for taggers in gure 1 is a special case
of this algorithm, if we dene (x
i
; y
i
);GEN and
 as just described.
We will now give a rst theorem regarding
the convergence of this algorithm. This theorem
therefore also describes conditions under which
the algorithm in gure 1 converges. First, we
need the following denition:
Denition 1 Let GEN(x
i
) = GEN(x
i
)   fy
i
g. In
other words GEN(x
i
) is the set of incorrect candidates
for an example x
i
. We will say that a training sequence
(x
i
; y
i
) for i = 1 : : : n is separable with margin ? > 0
if there exists some vector U with jjUjj = 1 such that
8i; 8z 2 GEN(x
i
); U (x
i
; y
i
) U (x
i
; z)  ? (3)
(jjUjj is the 2-norm of U, i.e., jjUjj =
p
P
s
U
2
s
.)
We can then state the following theorem (see
section 5 for a proof):
Theorem 1 For any training sequence (x
i
; y
i
) which is
separable with margin ?, then for the perceptron algorithm
in gure 2
Number of mistakes 
R
2
?
2
where R is a constant such that 8i; 8z 2
GEN(x
i
) jj(x
i
; y
i
)   (x
i
; z)jj  R.
This theorem implies that if there is a param-
eter vector U which makes zero errors on the
training set, then after a nite number of itera-
tions the training algorithm will have converged
to parameter values with zero training error. A
crucial point is that the number of mistakes is in-
dependent of the number of candidates for each
example (i.e. the size of GEN(x
i
) for each i),
depending only on the separation of the training
data, where separation is dened above. This
is important because in many NLP problems
GEN(x) can be exponential in the size of the
inputs. All of the convergence and generaliza-
tion results in this paper depend on notions of
separability rather than the size of GEN.
Two questions come to mind. First, are there
guarantees for the algorithm if the training data
is not separable? Second, performance on a
training sample is all very well, but what does
this guarantee about how well the algorithm
generalizes to newly drawn test examples? (Fre-
und & Schapire 99) discuss how the theory can
be extended to deal with both of these questions.
The next sections describe how these results can
be applied to the algorithms in this paper.
3.1 Theory for inseparable data
In this section we give bounds which apply when
the data is not separable. First, we need the
following denition:
Denition 2 Given a sequence (x
i
; y
i
), for a U, ? pair
dene m
i
= U (x
i
; y
i
) max
z2GEN(x
i
)
U (x
i
; z) and

i
= maxf0; ?  m
i
g. Finally, dene D
U;?
=
p
P
n
i=1

2
i
.
The value D
U;?
is a measure of how close U
is to separating the training data with margin ?.
D
U;?
is 0 if the vector U separates the data with
at least margin ?. If U separates almost all of
the examples with margin ?, but a few examples
are incorrectly tagged or have margin less than
?, then D
U;?
will take a relatively small value.
The following theorem then applies (see sec-
tion 5 for a proof):
Theorem 2 For any training sequence (x
i
; y
i
), for the
rst pass over the training set of the perceptron algorithm
in gure 2,
Number of mistakes  min
U;?
(R+D
U;?
)
2
?
2
where R is a constant such that 8i; 8z 2
GEN(x
i
) jj(x
i
; y
i
)   (x
i
; z)jj  R, and the
min is taken over ? > 0, jjUjj = 1.
This theorem implies that if the training data
is \close" to being separable with margin ? {
i.e., there exists some U such that D
U;?
is rela-
tively small { then the algorithm will again make
a small number of mistakes. Thus theorem 2
shows that the perceptron algorithm can be ro-
bust to some training data examples being dif-
cult or impossible to tag correctly.
3.2 Generalization results
Theorems 1 and 2 give results bounding the
number of errors on training samples, but the
question we are really interested in concerns
guarantees of how well the method generalizes
to new test examples. Fortunately, there are
several theoretical results suggesting that if the
perceptron algorithm makes a relatively small
number of mistakes on a training sample then it
is likely to generalize well to new examples. This
section describes some of these results, which
originally appeared in (Freund & Schapire 99),
and are derived directly from results in (Helm-
bold and Warmuth 95).
First we dene a modication of the percep-
tron algorithm, the voted perceptron. We can
consider the rst pass of the perceptron algo-
rithm to build a sequence of parameter set-
tings 
1;i
for i = 1 : : : n. For a given test ex-
ample x, each of these will dene an output
v
i
= argmax
z2GEN(x)

1;i
 (x; z). The voted
perceptron takes the most frequently occurring
output in the set fv
1
: : : v
n
g. Thus the voted
perceptron is a method where each of the pa-
rameter settings 
1;i
for i = 1 : : : n get a sin-
gle vote for the output, and the majority wins.
The averaged algorithm in section 2.5 can be
considered to be an approximation of the voted
method, with the advantage that a single decod-
ing with the averaged parameters can be per-
formed, rather than n decodings with each of
the n parameter settings.
In analyzing the voted perceptron the one as-
sumption we will make is that there is some
unknown distribution P (x; y) over the set X 
Y, and that both training and test examples
are drawn independently, identically distributed
(i.i.d.) from this distribution. Corollary 1 of
(Freund & Schapire 99) then states:
Theorem 3 (Freund & Schapire 99) Assume all ex-
amples are generated i.i.d. at random. Let
h(x
1
; y
1
)i : : : (x
n
; y
n
)i be a sequence of training examples
and let (x
n+1
; y
n+1
) be a test example. Then the prob-
ability (over the choice of all n + 1 examples) that the
voted-perceptron algorithm does not predict y
n+1
on in-
put x
n+1
is at most
2
n + 1
E
n+1

min
U;?
(R +D
U;?
)
2
?
2

where E
n+1
[] is an expected value taken over n + 1 ex-
amples, R and D
U;?
are as dened above, and the min is
taken over ? > 0, jjUjj = 1.
4 Experiments
4.1 Data Sets
We ran experiments on two data sets: part-of-
speech tagging on the Penn Wall Street Journal
treebank (Marcus et al 93), and base noun-
phrase recognition on the data sets originally in-
troduced by (Ramshaw and Marcus 95). In each
case we had a training, development and test set.
For part-of-speech tagging the training set was
sections 0{18 of the treebank, the development
set was sections 19{21 and the nal test set was
sections 22-24. In NP chunking the training set
Current word w
i
& t
i
Previous word w
i 1
& t
i
Word two back w
i 2
& t
i
Next word w
i+1
& t
i
Word two ahead w
i+2
& t
i
Bigram features w
i 2
; w
i 1
& t
i
w
i 1
; w
i
& t
i
w
i
; w
i+1
& t
i
w
i+1
; w
i+2
& t
i
Current tag p
i
& t
i
Previous tag p
i 1
& t
i
Tag two back p
i 2
& t
i
Next tag p
i+1
& t
i
Tag two ahead p
i+2
& t
i
Bigram tag features p
i 2
; p
i 1
& t
i
p
i 1
; p
i
& t
i
p
i
; p
i+1
& t
i
p
i+1
; p
i+2
& t
i
Trigram tag features p
i 2
; p
i 1
; p
i
& t
i
p
i 1
; p
i
; p
i+1
& t
i
p
i
; p
i+1
; p
i+2
& t
i
Figure 3: Feature templates used in the NP chunking
experiments. w
i
is the current word, and w
1
: : : w
n
is the
entire sentence. p
i
is POS tag for the current word, and
p
1
: : : p
n
is the POS sequence for the sentence. t
i
is the
chunking tag assigned to the i'th word.
was taken from section 15{18, the development
set was section 21, and the test set was section
20. For POS tagging we report the percentage
of correct tags on a test set. For chunking we
report F-measure in recovering bracketings cor-
responding to base NP chunks.
4.2 Features
For POS tagging we used identical features to
those of (Ratnaparkhi 96), the only dierence
being that we did not make the rare word dis-
tinction in table 1 of (Ratnaparkhi 96) (i.e.,
spelling features were included for all words in
training data, and the word itself was used as a
feature regardless of whether the word was rare).
The feature set takes into account the previous
tag and previous pairs of tags in the history, as
well as the word being tagged, spelling features
of the words being tagged, and various features
of the words surrounding the word being tagged.
In the chunking experiments the input \sen-
tences" included words as well as parts-of-speech
for those words from the tagger in (Brill 95). Ta-
ble 3 shows the features used in the experiments.
The chunking problem is represented as a three-
tag task, where the tags are B, I, O for words
beginning a chunk, continuing a chunk, and be-
ing outside a chunk respectively. All chunks be-
gin with a B symbol, regardless of whether the
previous word is tagged O or I.
NP Chunking Results
Method F-Measure Numits
Perc, avg, cc=0 93.53 13
Perc, noavg, cc=0 93.04 35
Perc, avg, cc=5 93.33 9
Perc, noavg, cc=5 91.88 39
ME, cc=0 92.34 900
ME, cc=5 92.65 200
POS Tagging Results
Method Error rate/% Numits
Perc, avg, cc=0 2.93 10
Perc, noavg, cc=0 3.68 20
Perc, avg, cc=5 3.03 6
Perc, noavg, cc=5 4.04 17
ME, cc=0 3.4 100
ME, cc=5 3.28 200
Figure 4: Results for various methods on the part-of-
speech tagging and chunking tasks on development data.
All scores are error percentages. Numits is the number
of training iterations at which the best score is achieved.
Perc is the perceptron algorithm, ME is the maximum
entropy method. Avg/noavg is the perceptron with or
without averaged parameter vectors. cc=5 means only
features occurring 5 times or more in training are in-
cluded, cc=0 means all features in training are included.
4.3 Results
We applied both maximum-entropy models and
the perceptron algorithm to the two tagging
problems. We tested several variants for each
algorithm on the development set, to gain some
understanding of how the algorithms' perfor-
mance varied with various parameter settings,
and to allow optimization of free parameters so
that the comparison on the nal test set is a fair
one. For both methods, we tried the algorithms
with feature count cut-os set at 0 and 5 (i.e.,
we ran experiments with all features in training
data included, or with all features occurring 5
times or more included { (Ratnaparkhi 96) uses
a count cut-o of 5). In the perceptron algo-
rithm, the number of iterations T over the train-
ing set was varied, and the method was tested
with both averaged and unaveraged parameter
vectors (i.e., with 
T;n
s
and 
T;n
s
, as dened in
section 2.5, for a variety of values for T ). In
the maximum entropy model the number of it-
erations of training using Generalized Iterative
Scaling was varied.
Figure 4 shows results on development data
on the two tasks. The trends are fairly clear:
averaging improves results signicantly for the
perceptron method, as does including all fea-
tures rather than imposing a count cut-o of 5.
In contrast, the ME models' performance suers
when all features are included. The best percep-
tron conguration gives improvements over the
maximum-entropy models in both cases: an im-
provement in F-measure from 92:65% to 93:53%
in chunking, and a reduction from 3:28% to
2:93% error rate in POS tagging. In looking
at the results for dierent numbers of iterations
on development data we found that averaging
not only improves the best result, but also gives
much greater stability of the tagger (the non-
averaged variant has much greater variance in
its scores).
As a nal test, the perceptron and ME tag-
gers were applied to the test sets, with the op-
timal parameter settings on development data.
On POS tagging the perceptron algorithm gave
2.89% error compared to 3.28% error for the
maximum-entropy model (a 11.9% relative re-
duction in error). In NP chunking the percep-
tron algorithm achieves an F-measure of 93.63%,
in contrast to an F-measure of 93.29% for the
ME model (a 5.1% relative reduction in error).
5 Proofs of the Theorems
This section gives proofs of theorems 1 and 2.
The proofs are adapted from proofs for the clas-
sication case in (Freund & Schapire 99).
Proof of Theorem 1: Let 
k
be the weights
before the k'th mistake is made. It follows that

1
= 0. Suppose the k'th mistake is made at
the i'th example. Take z to the output proposed
at this example, z = argmax
y2GEN(x
i
)
(x
i
; y) 

k
. It follows from the algorithm updates that

k+1
= 
k
+(x
i
; y
i
) (x
i
; z). We take inner
products of both sides with the vector U:
U  
k+1
= U  
k
+U  (x
i
; y
i
)  U  (x
i
; z)
 U  
k
+ ?
where the inequality follows because of the prop-
erty of U assumed in Eq. 3. Because 
1
= 0,
and therefore U  
1
= 0, it follows by induc-
tion on k that for all k, U  
k+1
 k?. Be-
cause U  
k+1
 jjUjj jj
k+1
jj, it follows that
jj
k+1
jj  k?.
We also derive an upper bound for jj
k+1
jj
2
:
jj
k+1
jj
2
= jj
k
jj
2
+ jj(x
i
; y
i
)   (x
i
; z)jj
2
+2
k
 ((x
i
; y
i
)   (x
i
; z))
 jj
k
jj
2
+R
2
where the inequality follows because
jj(x
i
; y
i
)  (x
i
; z)jj
2
 R
2
by assump-
tion, and 
k
 ((x
i
; y
i
)  (x
i
; z))  0 because
z is the highest scoring candidate for x
i
under
the parameters 
k
. It follows by induction that
jj
k+1
jj
2
 kR
2
.
Combining the bounds jj
k+1
jj  k? and
jj
k+1
jj
2
 kR
2
gives the result for all k that
k
2
?
2
 jj
k+1
jj
2
 kR
2
) k  R
2
=?
2
Proof of Theorem 2: We transform the rep-
resentation (x; y) 2 R
d
to a new representation

(x; y) 2 R
d+n
as follows. For i = 1 : : : d de-
ne


i
(x; y) = 
i
(x; y). For j = 1 : : : n dene


d+j
(x; y) =  if (x; y) = (x
j
; y
j
), 0 otherwise,
where  is a parameter which is greater than 0.
Similary, say we are given a U; ? pair, and cor-
responding values for 
i
as dened above. We
dene a modied parameter vector

U 2 R
d+n
with

U
i
= U
i
for i = 1 : : : d and

U
d+j
= 
j
=
for j = 1 : : : n. Under these denitions it can be
veried that
8i; 8z 2 GEN(x
i
);

U 

(x
i
; y
i
)  

U 

(x
i
; z)  ?
8i; 8z 2 GEN(x
i
); jj

(x
i
; y
i
)  

(x
i
; z)jj
2
 R
2
+
2
jj

Ujj
2
= jjUjj
2
+
P
i

2
i
=
2
= 1 +D
2
U;?
=
2
It can be seen that the vector

U=jj

Ujj separates
the data with margin ?=
q
1 + D
2
U;?
=
2
. By the-
orem 1, this means that the rst pass of the per-
ceptron algorithm with representation

 makes
at most k
max
() =
1
?
2
(R
2
+
2
)(1 +
D
2
U;?

2
) mis-
takes. But the rst pass of the original algo-
rithm with representation  is identical to the
rst pass of the algorithm with representation

, because the parameter weights for the addi-
tional features


d+j
for j = 1 : : : n each aect a
single example of training data, and do not aect
the classication of test data examples. Thus
the original perceptron algorithm also makes at
most k
max
() mistakes on its rst pass over the
training data. Finally, we can minimize k
max
()
with respect to , giving  =
p
RD
U;?
, and
k
max
(
p
RD
U;?
) = (R
2
+D
2
U;?
)=?
2
, implying the
bound in the theorem.
6 Conclusions
We have described new algorithms for tagging,
whose performance guarantees depend on a no-
tion of \separability" of training data exam-
ples. The generic algorithm in gure 2, and
the theorems describing its convergence prop-
erties, could be applied to several other models
in the NLP literature. For example, a weighted
context-free grammar can also be conceptual-
ized as a way of dening GEN,  and , so the
weights for generative models such as PCFGs
could be trained using this method.
Acknowledgements
Thanks to Nigel Duy, Rob Schapire and Yoram
Singer for many useful discussions regarding
the algorithms in this paper, and to Fernando
Pereira for pointers to the NP chunking data
set, and for suggestions regarding the features
used in the experiments.
References
Brill, E. (1995). Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part of Speech Tagging. Computational Lin-
guistics.
Collins, M., and Duy, N. (2001). Convolution Kernels
for Natural Language. In Proceedings of Neural Infor-
mation Processing Systems (NIPS 14).
Collins, M., and Duy, N. (2002). New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Discrete
Structures, and the Voted Perceptron. In Proceedings
of ACL 2002.
Collins, M. (2002). Ranking Algorithms for Named{
Entity Extraction: Boosting and the Voted Percep-
tron. In Proceedings of ACL 2002.
Freund, Y. & Schapire, R. (1999). Large Margin Classi-
cation using the Perceptron Algorithm. In Machine
Learning, 37(3):277{296.
Helmbold, D., and Warmuth, M. On weak learning. Jour-
nal of Computer and System Sciences, 50(3):551-573,
June 1995.
Laerty, J., McCallum, A., and Pereira, F. (2001). Con-
ditional random elds: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML 2001.
McCallum, A., Freitag, D., and Pereira, F. (2000) Max-
imum entropy markov models for information extrac-
tion and segmentation. In Proceedings of ICML 2000.
Marcus, M., Santorini, B., & Marcinkiewicz, M. (1993).
Building a large annotated corpus of english: The
Penn treebank. Computational Linguistics, 19.
Ramshaw, L., and Marcus, M. P. (1995). Text Chunking
Using Transformation-Based Learning. In Proceedings
of the Third ACL Workshop on Very Large Corpora,
Association for Computational Linguistics, 1995.
Ratnaparkhi, A. (1996). A maximum entropy part-of-
speech tagger. In Proceedings of the empirical methods
in natural language processing conference.
Rosenblatt, F. 1958. The Perceptron: A Probabilistic
Model for Information Storage and Organization in the
Brain. Psychological Review, 65, 386{408. (Reprinted
in Neurocomputing (MIT Press, 1998).)
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 141?150, Prague, June 2007. c?2007 Association for Computational Linguistics
Structured Prediction Models via the Matrix-Tree Theorem
Terry Koo, Amir Globerson, Xavier Carreras and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,gamir,carreras,mcollins}@csail.mit.edu
Abstract
This paper provides an algorithmic frame-
work for learning statistical models involv-
ing directed spanning trees, or equivalently
non-projective dependency structures. We
show how partition functions and marginals
for directed spanning trees can be computed
by an adaptation of Kirchhoff?s Matrix-Tree
Theorem. To demonstrate an application of
the method, we perform experiments which
use the algorithm in training both log-linear
and max-margin dependency parsers. The
new training methods give improvements in
accuracy over perceptron-trained models.
1 Introduction
Learning with structured data typically involves
searching or summing over a set with an exponen-
tial number of structured elements, for example the
set of all parse trees for a given sentence. Meth-
ods for summing over such structures include the
inside-outside algorithm for probabilistic context-
free grammars (Baker, 1979), the forward-backward
algorithm for hidden Markov models (Baum et
al., 1970), and the belief-propagation algorithm for
graphical models (Pearl, 1988). These algorithms
compute marginal probabilities and partition func-
tions, quantities which are central to many meth-
ods for the statistical modeling of complex struc-
tures (e.g., the EM algorithm (Baker, 1979; Baum
et al, 1970), contrastive estimation (Smith and Eis-
ner, 2005), training algorithms for CRFs (Lafferty et
al., 2001), and training algorithms for max-margin
models (Bartlett et al, 2004; Taskar et al, 2004a)).
This paper describes inside-outside-style algo-
rithms for the case of directed spanning trees. These
structures are equivalent to non-projective depen-
dency parses (McDonald et al, 2005b), and more
generally could be relevant to any task that involves
learning a mapping from a graph to an underlying
spanning tree. Unlike the case for projective depen-
dency structures, partition functions and marginals
for non-projective trees cannot be computed using
dynamic-programming methods such as the inside-
outside algorithm. In this paper we describe how
these quantities can be computed by adapting a well-
known result in graph theory: Kirchhoff?s Matrix-
Tree Theorem (Tutte, 1984). A na??ve application of
the theorem yields O(n4) and O(n6) algorithms for
computation of the partition function and marginals,
respectively. However, our adaptation finds the par-
tition function and marginals in O(n3) time using
simple matrix determinant and inversion operations.
We demonstrate an application of the new infer-
ence algorithm to non-projective dependency pars-
ing. Specifically, we show how to implement
two popular supervised learning approaches for this
task: globally-normalized log-linear models and
max-margin models. Log-linear estimation criti-
cally depends on the calculation of partition func-
tions and marginals, which can be computed by
our algorithms. For max-margin models, Bartlett
et al (2004) have provided a simple training al-
gorithm, based on exponentiated-gradient (EG) up-
dates, that requires computation of marginals and
can thus be implemented within our framework.
Both of these methods explicitly minimize the loss
incurred when parsing the entire training set. This
contrasts with the online learning algorithms used in
previous work with spanning-tree models (McDon-
ald et al, 2005b).
We applied the above two marginal-based train-
ing algorithms to six languages with varying de-
grees of non-projectivity, using datasets obtained
from the CoNLL-X shared task (Buchholz and
Marsi, 2006). Our experimental framework com-
pared three training approaches: log-linear models,
max-margin models, and the averaged perceptron.
Each of these was applied to both projective and
non-projective parsing. Our results demonstrate that
marginal-based training yields models which out-
141
perform those trained using the averaged perceptron.
In summary, the contributions of this paper are:
1. We introduce algorithms for inside-outside-
style calculations for directed spanning trees, or
equivalently non-projective dependency struc-
tures. These algorithms should have wide
applicability in learning problems involving
spanning-tree structures.
2. We illustrate the utility of these algorithms in
log-linear training of dependency parsing mod-
els, and show improvements in accuracy when
compared to averaged-perceptron training.
3. We also train max-margin models for depen-
dency parsing via an EG algorithm (Bartlett
et al, 2004). The experiments presented here
constitute the first application of this algorithm
to a large-scale problem. We again show im-
proved performance over the perceptron.
The goal of our experiments is to give a rigorous
comparative study of the marginal-based training al-
gorithms and a highly-competitive baseline, the av-
eraged perceptron, using the same feature sets for
all approaches. We stress, however, that the purpose
of this work is not to give competitive performance
on the CoNLL data sets; this would require further
engineering of the approach.
Similar adaptations of the Matrix-Tree Theorem
have been developed independently and simultane-
ously by Smith and Smith (2007) andMcDonald and
Satta (2007); see Section 5 for more discussion.
2 Background
2.1 Discriminative Dependency Parsing
Dependency parsing is the task of mapping a sen-
tence x to a dependency structure y. Given a sen-
tence x with n words, a dependency for that sen-
tence is a tuple (h,m) where h ? [0 . . . n] is the
index of the head word in the sentence, and m ?
[1 . . . n] is the index of a modifier word. The value
h = 0 is a special root-symbol that may only ap-
pear as the head of a dependency. We use D(x) to
refer to all possible dependencies for a sentence x:
D(x) = {(h,m) : h ? [0 . . . n],m ? [1 . . . n]}.
A dependency parse is a set of dependencies
that forms a directed tree, with the sentence?s root-
symbol as its root. We will consider both projective
Projective Non-projective
Single
Root 1 30 2Heroot saw her 1 30 2Heroot saw her
Multi
Root 1 30 2Heroot saw her 1 30 2Heroot saw her
Figure 1: Examples of the four types of dependency struc-
tures. We draw dependency arcs from head to modifier.
trees, where dependencies are not allowed to cross,
and non-projective trees, where crossing dependen-
cies are allowed. Dependency annotations for some
languages, for example Czech, can exhibit a signifi-
cant number of crossing dependencies. In addition,
we consider both single-root and multi-root trees. In
a single-root tree y, the root-symbol has exactly one
child, while in a multi-root tree, the root-symbol has
one or more children. This distinction is relevant
as our training sets include both single-root corpora
(in which all trees are single-root structures) and
multi-root corpora (in which some trees are multi-
root structures).
The two distinctions described above are orthog-
onal, yielding four classes of dependency structures;
see Figure 1 for examples of each kind of structure.
We use T sp (x) to denote the set of all possible pro-
jective single-root dependency structures for a sen-
tence x, and T snp(x) to denote the set of single-root
non-projective structures for x. The sets T mp (x) and
T mnp (x) are defined analogously for multi-root struc-
tures. In contexts where any class of dependency
structures may be used, we use the notation T (x) as
a placeholder that may be defined as T sp (x), T
s
np(x),
T mp (x) or T
m
np (x).
Following McDonald et al (2005a), we use a dis-
criminative model for dependency parsing. Fea-
tures in the model are defined through a function
f(x, h,m) which maps a sentence x together with
a dependency (h,m) to a feature vector in Rd. A
feature vector can be sensitive to any properties of
the triple (x, h,m). Given a parameter vector w,
the optimal dependency structure for a sentence x is
y?(x;w) = argmax
y?T (x)
?
(h,m)?y
w ? f(x, h,m) (1)
where the set T (x) can be defined as T sp (x), T
s
np(x),
T mp (x) or T
m
np (x), depending on the type of parsing.
142
The parameters w will be learned from a train-
ing set {(xi, yi)}Ni=1 where each xi is a sentence and
each yi is a dependency structure. Much of the pre-
vious work on learningw has focused on training lo-
cal models (see Section 5). McDonald et al (2005a;
2005b) trained global models using online algo-
rithms such as the perceptron algorithm or MIRA.
In this paper we consider training algorithms based
on work in conditional random fields (CRFs) (Laf-
ferty et al, 2001) and max-margin methods (Taskar
et al, 2004a).
2.2 Three Inference Problems
This section highlights three inference problems
which arise in training and decoding discriminative
dependency parsers, and which are central to the ap-
proaches described in this paper.
Assume that we have a vector ? with values
?h,m ? R for all (h,m) ? D(x); these values cor-
respond to weights on the different dependencies in
D(x). Define a conditional distribution over all de-
pendency structures y ? T (x) as follows:
P (y |x;?) =
exp
{?
(h,m)?y ?h,m
}
Z(x;?)
(2)
Z(x;?) =
?
y?T (x)
exp
?
?
?
?
(h,m)?y
?h,m
?
?
?
(3)
The function Z(x;?) is commonly referred to as the
partition function.
Given the distribution P (y |x;?), we can define
the marginal probability of a dependency (h,m) as
?h,m(x;?) =
?
y?T (x) : (h,m)?y
P (y |x;?)
The inference problems are then as follows:
Problem 1: Decoding:
Find argmaxy?T (x)
?
(h,m)?y ?h,m
Problem 2: Computation of the Partition Func-
tion: Calculate Z(x;?).
Problem 3: Computation of the Marginals:
For all (h,m) ? D(x), calculate ?h,m(x;?).
Note that all three problems require a maximiza-
tion or summation over the set T (x), which is ex-
ponential in size. There is a clear motivation for
being able to solve Problem 1: by setting ?h,m =
w ? f(x, h,m), the optimal dependency structure
y?(x;w) (see Eq. 1) can be computed. In this paper
the motivation for solving Problems 2 and 3 arises
from training algorithms for discriminative models.
As we will describe in Section 4, both log-linear and
max-margin models can be trained via methods that
make direct use of algorithms for Problems 2 and 3.
In the case of projective dependency structures
(i.e., T (x) defined as T sp (x) or T
m
p (x)), there are
well-known algorithms for all three inference prob-
lems. Decoding can be carried out using Viterbi-
style dynamic-programming algorithms, for exam-
ple the O(n3) algorithm of Eisner (1996). Com-
putation of the marginals and partition function can
also be achieved in O(n3) time, using a variant of
the inside-outside algorithm (Baker, 1979) applied
to the Eisner (1996) data structures (Paskin, 2001).
In the non-projective case (i.e., T (x) defined as
T snp(x) or T
m
np (x)), McDonald et al (2005b) de-
scribe how the CLE algorithm (Chu and Liu, 1965;
Edmonds, 1967) can be used for decoding. How-
ever, it is not possible to compute the marginals
and partition function using the inside-outside algo-
rithm. We next describe a method for computing
these quantities in O(n3) time using matrix inverse
and determinant operations.
3 Spanning-tree inference using the
Matrix-Tree Theorem
In this section we present algorithms for computing
the partition function and marginals, as defined in
Section 2.2, for non-projective parsing. We first re-
iterate the observation of McDonald et al (2005a)
that non-projective parses correspond to directed
spanning trees on a complete directed graph of n
nodes, where n is the length of the sentence. The
above inference problems thus involve summation
over the set of all directed spanning trees. Note that
this set is exponentially large, and there is no obvi-
ous method for decomposing the sum into dynamic-
programming-like subproblems. This section de-
scribes how a variant of Kirchhoff?s Matrix-Tree
Theorem (Tutte, 1984) can be used to evaluate the
partition function and marginals efficiently.
In what follows, we consider the single-root set-
ting (i.e., T (x) = T snp(x)), leaving the multi-root
143
case (i.e., T (x) = T mnp (x)) to Section 3.3. For a
sentence x with n words, define a complete directed
graph G on n nodes, where each node corresponds
to a word in x, and each edge corresponds to a de-
pendency between two words in x. Note thatG does
not include the root-symbol h = 0, nor does it ac-
count for any dependencies (0,m) headed by the
root-symbol. We assign non-negative weights to the
edges of this graph, yielding the following weighted
adjacency matrix A(?) ? Rn?n, for h,m = 1 . . . n:
Ah,m(?) =
{
0, if h = m
exp {?h,m} , otherwise
To account for the dependencies (0,m) headed by
the root-symbol, we define a vector of root-selection
scores r(?) ? Rn, form = 1 . . . n:
rm(?) = exp {?0,m}
Let the weight of a dependency structure y ? T snp(x)
be defined as:
?(y;?) = rroot(y)(?)
?
(h,m)?y : h 6=0
Ah,m(?)
Here, root(y) = m : (0,m) ? y is the child of the
root-symbol; there is exactly one such child, since
y ? T snp(x). Eq. 2 and 3 can be rephrased as:
P (y |x;?) =
?(y;?)
Z(x;?)
(4)
Z(x;?) =
?
y?T snp(x)
?(y;?) (5)
In the remainder of this section, we drop the nota-
tional dependence on x for brevity.
The original Matrix-Tree Theorem addressed the
problem of counting the number of undirected span-
ning trees in an undirected graph. For the models
we study here, we require a sum of weighted and
directed spanning trees. Tutte (1984) extended the
Matrix-Tree Theorem to this case. We briefly sum-
marize his method below.
First, define the Laplacian matrix L(?) ? Rn?n
of G, for h,m = 1 . . . n:
Lh,m(?) =
{ ?n
h?=1Ah?,m(?) if h = m
?Ah,m(?) otherwise
Second, for a matrix X , let X(h,m) be the minor of
X with respect to row h and column m; i.e., the
determinant of the matrix formed by deleting row h
and columnm fromX . Finally, define the weight of
any directed spanning tree of G to be the product of
the weights Ah,m(?) for the edges in that tree.
Theorem 1 (Tutte, 1984, p. 140). Let L(?) be the
Laplacian matrix of G. Then L(m,m)(?) is equal to
the sum of the weights of all directed spanning trees
of G which are rooted at m. Furthermore, the mi-
nors vary only in sign when traversing the columns
of the Laplacian (Tutte, 1984, p. 150):
?h,m : (?1)h+mL(h,m)(?) = L(m,m)(?) (6)
3.1 Partition functions via matrix determinants
From Theorem 1, it directly follows that
L(m,m)(?) =
?
y?U(m)
?
(h,m)?y : h 6=0
Ah,m(?)
where U(m) = {y ? T snp : root(y) = m}. A
na??ve method for computing the partition function is
therefore to evaluate
Z(?) =
n?
m=1
rm(?)L(m,m)(?)
The above would require calculating n determinants,
resulting in O(n4) complexity. However, as we
show below Z(?) may be obtained in O(n3) time
using a single determinant evaluation.
Define a newmatrix L?(?) to beL(?) with the first
row replaced by the root-selection scores:
L?h,m(?) =
{
rm(?) h = 1
Lh,m(?) h > 1
This matrix allows direct computation of the parti-
tion function, as the following proposition shows.
Proposition 1 The partition function in Eq. 5 is
given by Z(?) = |L?(?)|.
Proof: Consider the row expansion of |L?(?)| with
respect to row 1:
|L?(?)| =
n?
m=1
(?1)1+mL?1,m(?)L?(1,m)(?)
=
n?
m=1
(?1)1+mrm(?)L(1,m)(?)
=
n?
m=1
rm(?)L(m,m)(?) = Z(?)
The second line follows from the construction of
L?(?), and the third line follows from Eq. 6.
144
3.2 Marginals via matrix inversion
The marginals we require are given by
?h,m(?) =
1
Z(?)
?
y?T snp : (h,m)?y
?(y;?)
To calculate these marginals efficiently for all values
of (h,m) we use a well-known identity relating the
log partition-function to marginals
?h,m(?) =
? logZ(?)
??h,m
Since the partition function in this case has a closed-
form expression (i.e., the determinant of a matrix
constructed from ?), the marginals can also obtained
in closed form. Using the chain rule, the derivative
of the log partition-function in Proposition 1 is
?h,m(?) =
? log |L?(?)|
??h,m
=
n?
h?=1
n?
m?=1
? log |L?(?)|
?L?h?,m?(?)
?L?h?,m?(?)
??h,m
To perform the derivative, we use the identity
? log |X|
?X
=
(
X?1
)T
and the fact that ?L?h?,m?(?)/??h,m is nonzero for
only a few h?,m?. Specifically, when h = 0, the
marginals are given by
?0,m(?) = rm(?)
[
L??1(?)
]
m,1
and for h > 0, the marginals are given by
?h,m(?) = (1 ? ?1,m)Ah,m(?)
[
L??1(?)
]
m,m
?
(1 ? ?h,1)Ah,m(?)
[
L??1(?)
]
m,h
where ?h,m is the Kronecker delta. Thus, the com-
plexity of evaluating all the relevant marginals is
dominated by the matrix inversion, and the total
complexity is therefore O(n3).
3.3 Multiple Roots
In the case of multiple roots, we can still compute
the partition function and marginals efficiently. In
fact, the derivation of this case is simpler than for
single-root structures. Create an extended graph G?
which augmentsG with a dummy root node that has
edges pointing to all of the existing nodes, weighted
by the appropriate root-selection scores. Note that
there is a bijection between directed spanning trees
ofG? rooted at the dummy root and multi-root struc-
tures y ? T mnp (x). Thus, Theorem 1 can be used to
compute the partition function directly: construct a
Laplacian matrix L(?) for G? and compute the mi-
nor L(0,0)(?). Since this minor is also a determi-
nant, the marginals can be obtained analogously to
the single-root case. More concretely, this technique
corresponds to defining the matrix L?(?) as
L?(?) = L(?) + diag(r(?))
where diag(v) is the diagonal matrix with the vector
v on its diagonal.
3.4 Labeled Trees
The techniques above extend easily to the case
where dependencies are labeled. For a model with
L different labels, it suffices to define the edge
and root scores as Ah,m(?) =
?L
`=1 exp {?h,m,`}
and rm(?) =
?L
`=1 exp {?0,m,`}. The partition
function over labeled trees is obtained by operat-
ing on these values as described previously, and
the marginals are given by an application of the
chain rule. Both inference problems are solvable in
O(n3 + Ln2) time.
4 Training Algorithms
This section describes two methods for parameter
estimation that rely explicitly on the computation of
the partition function and marginals.
4.1 Log-Linear Estimation
In conditional log-linear models (Johnson et al,
1999; Lafferty et al, 2001), a distribution over parse
trees for a sentence x is defined as follows:
P (y |x;w) =
exp
{?
(h,m)?y w ? f(x, h,m)
}
Z(x;w)
(7)
where Z(x;w) is the partition function, a sum over
T sp (x), T
s
np(x), T
m
p (x) or T
m
np (x).
We train the model using the approach described
by Sha and Pereira (2003). Assume that we have a
training set {(xi, yi)}Ni=1. The optimal parameters
145
are taken to be w? = argminw L(w) where
L(w) = ?C
N?
i=1
logP (yi |xi;w) +
1
2
||w||2
The parameterC > 0 is a constant dictating the level
of regularization in the model.
Since L(w) is a convex function, gradient de-
scent methods can be used to search for the global
minimum. Such methods typically involve repeated
computation of the loss L(w) and gradient ?L(w)?w ,
requiring efficient implementations of both func-
tions. Note that the log-probability of a parse is
logP (y |x;w) =
?
(h,m)?y
w ? f(x, h,m)? logZ(x;w)
so that the main issue in calculating the loss func-
tion L(w) is the evaluation of the partition functions
Z(xi;w). The gradient of the loss is given by
?L(w)
?w
= w ? C
N?
i=1
?
(h,m)?yi
f(xi, h,m)
+ C
N?
i=1
?
(h,m)?D(xi)
?h,m(xi;w)f(xi, h,m)
where
?h,m(x;w) =
?
y?T (x) : (h,m)?y
P (y |x;w)
is the marginal probability of a dependency (h,m).
Thus, the main issue in the evaluation of the gradient
is the computation of the marginals ?h,m(xi;w).
Note that Eq. 7 forms a special case of the log-
linear distribution defined in Eq. 2 in Section 2.2.
If we set ?h,m = w ? f(x, h,m) then we have
P (y |x;w) = P (y |x;?), Z(x;w) = Z(x;?), and
?h,m(x;w) = ?h,m(x;?). Thus in the projective
case the inside-outside algorithm can be used to cal-
culate the partition function and marginals, thereby
enabling training of a log-linear model; in the non-
projective case the algorithms in Section 3 can be
used for this purpose.
4.2 Max-Margin Estimation
The second learning algorithm we consider is
the large-margin approach for structured prediction
(Taskar et al, 2004a; Taskar et al, 2004b). Learning
in this framework again involves minimization of a
convex function L(w). Let the margin for parse tree
y on the i?th training example be defined as
mi,y(w) =
?
(h,m)?yi
w?f(xi, h,m) ?
?
(h,m)?y
w?f(xi, h,m)
The loss function is then defined as
L(w) = C
N?
i=1
max
y?T (xi)
(Ei,y ?mi,y(w)) +
1
2
||w||2
where Ei,y is a measure of the loss?or number of
errors?for parse y on the i?th training sentence. In
this paper we take Ei,y to be the number of incorrect
dependencies in the parse tree y when compared to
the gold-standard parse tree yi.
The definition of L(w) makes use of the expres-
sion maxy?T (xi) (Ei,y ?mi,y(w)) for the i?th train-
ing example, which is commonly referred to as the
hinge loss. Note that Ei,yi = 0, and also that
mi,yi(w) = 0, so that the hinge loss is always non-
negative. In addition, the hinge loss is 0 if and only
ifmi,y(w) ? Ei,y for all y ? T (xi). Thus the hinge
loss directly penalizes margins mi,y(w) which are
less than their corresponding losses Ei,y.
Figure 2 shows an algorithm for minimizing
L(w) that is based on the exponentiated-gradient al-
gorithm for large-margin optimization described by
Bartlett et al (2004). The algorithm maintains a set
of weights ?i,h,m for i = 1 . . . N, (h,m) ? D(xi),
which are updated example-by-example. The algo-
rithm relies on the repeated computation of marginal
values ?i,h,m, which are defined as follows:1
?i,h,m =
?
y?T (xi) : (h,m)?y
P (y |xi) (8)
P (y |xi) =
exp
{?
(h,m)?y ?i,h,m
}
?
y??T (xi) exp
{?
(h,m)?y? ?i,h,m
}
A similar definition is used to derive marginal val-
ues ??i,h,m from the values ?
?
i,h,m. Computation of
the ? and ?? values is again inference of the form
described in Problem 3 in Section 2.2, and can be
1Bartlett et al (2004) write P (y |xi) as ?i,y . The ?i,y vari-
ables are dual variables that appear in the dual objective func-
tion, i.e., the convex dual of L(w). Analysis of the algorithm
shows that as the ?i,h,m variables are updated, the dual vari-
ables converge to the optimal point of the dual objective, and
the parameters w converge to the minimum of L(w).
146
Inputs: Training examples {(xi, yi)}Ni=1.
Parameters: Regularization constant C, starting point ?,
number of passes over training set T .
Data Structures: Real values ?i,h,m and li,h,m for i =
1 . . . N, (h,m) ? D(xi). Learning rate ?.
Initialization: Set learning rate ? = 1C . Set ?i,h,m = ? for
(h,m) ? yi, and ?i,h,m = 0 for (h,m) /? yi. Set li,h,m = 0
for (h,m) ? yi, and li,h,m = 1 for (h,m) /? yi. Calculate
initial parameters as
w = C
?
i
?
(h,m)?D(xi)
?i,h,mf(xi, h,m)
where ?i,h,m = (1? li,h,m ??i,h,m) and the ?i,h,m values
are calculated from the ?i,h,m values as described in Eq. 8.
Algorithm: Repeat T passes over the training set, where
each pass is as follows:
Set obj = 0
For i = 1 . . . N
? For all (h,m) ? D(xi):
??i,h,m = ?i,h,m + ?C (li,h,m +w ? f(xi, h,m))
? For example i, calculate marginals ?i,h,m
from ?i,h,m values, and marginals ??i,h,m
from ??i,h,m values (see Eq. 8).
? Update the parameters:
w = w + C
?
(h,m)?D(xi)
?i,h,mf(xi, h,m)
where ?i,h,m = ?i,h,m ? ??i,h,m,
? For all (h,m) ? D(xi), set ?i,h,m = ??i,h,m
? Set obj = obj + C
?
(h,m)?D(xi)
li,h,m??i,h,m
Set obj = obj ? ||w||
2
2 . If obj has decreased
compared to last iteration, set ? = ?2 .
Output: Parameter values w.
Figure 2: The EG Algorithm for Max-Margin Estimation.
The learning rate ? is halved each time the dual objective func-
tion (see (Bartlett et al, 2004)) fails to increase. In our experi-
ments we chose ? = 9, which was found to work well during
development of the algorithm.
achieved using the inside-outside algorithm for pro-
jective structures, and the algorithms described in
Section 3 for non-projective structures.
5 Related Work
Global log-linear training has been used in the con-
text of PCFG parsing (Johnson, 2001). Riezler et al
(2004) explore a similar application of log-linear
models to LFG parsing. Max-margin learning
has been applied to PCFG parsing by Taskar et al
(2004b). They show that this problem has a QP
dual of polynomial size, where the dual variables
correspond to marginal probabilities of CFG rules.
A similar QP dual may be obtained for max-margin
projective dependency parsing. However, for non-
projective parsing, the dual QP would require an ex-
ponential number of constraints on the dependency
marginals (Chopra, 1989). Nevertheless, alternative
optimization methods like that of Tsochantaridis et
al. (2004), or the EGmethod presented here, can still
be applied.
The majority of previous work on dependency
parsing has focused on local (i.e., classification of
individual edges) discriminative training methods
(Yamada and Matsumoto, 2003; Nivre et al, 2004;
Y. Cheng, 2005). Non-local (i.e., classification of
entire trees) training methods were used by McDon-
ald et al (2005a), who employed online learning.
Dependency parsing accuracy can be improved
by allowing second-order features, which consider
more than one dependency simultaneously. McDon-
ald and Pereira (2006) define a second-order depen-
dency parsing model in which interactions between
adjacent siblings are allowed, and Carreras (2007)
defines a second-order model that allows grandpar-
ent and sibling interactions. Both authors give poly-
time algorithms for exact projective parsing. By
adapting the inside-outside algorithm to these mod-
els, partition functions and marginals can be com-
puted for second-order projective structures, allow-
ing log-linear and max-margin training to be ap-
plied via the framework developed in this paper.
For higher-order non-projective parsing, however,
computational complexity results (McDonald and
Pereira, 2006; McDonald and Satta, 2007) indicate
that exact solutions to the three inference problems
of Section 2.2 will be intractable. Exploration of ap-
proximate second-order non-projective inference is
a natural avenue for future research.
Two other groups of authors have independently
and simultaneously proposed adaptations of the
Matrix-Tree Theorem for structured inference on di-
rected spanning trees (McDonald and Satta, 2007;
Smith and Smith, 2007). There are some algorithmic
differences between these papers and ours. First, we
define both multi-root and single-root algorithms,
whereas the other papers only consider multi-root
147
parsing. This distinction can be important as one
often expects a dependency structure to have ex-
actly one child attached to the root-symbol, as is the
case in a single-root structure. Second, McDonald
and Satta (2007) propose an O(n5) algorithm for
computing the marginals, as opposed to the O(n3)
matrix-inversion approach used by Smith and Smith
(2007) and ourselves.
In addition to the algorithmic differences, both
groups of authors consider applications of the
Matrix-Tree Theorem which we have not discussed.
For example, both papers propose minimum-risk
decoding, and McDonald and Satta (2007) dis-
cuss unsupervised learning and language model-
ing, while Smith and Smith (2007) define hidden-
variable models based on spanning trees.
In this paper we used EG training methods only
for max-margin models (Bartlett et al, 2004). How-
ever, Globerson et al (2007) have recently shown
how EG updates can be applied to efficient training
of log-linear models.
6 Experiments on Dependency Parsing
In this section, we present experimental results
applying our inference algorithms for dependency
parsing models. Our primary purpose is to estab-
lish comparisons along two relevant dimensions:
projective training vs. non-projective training, and
marginal-based training algorithms vs. the averaged
perceptron. The feature representation and other rel-
evant dimensions are kept fixed in the experiments.
6.1 Data Sets and Features
We used data from the CoNLL-X shared task
on multilingual dependency parsing (Buchholz and
Marsi, 2006). In our experiments, we used a subset
consisting of six languages; Table 1 gives details of
the data sets used.2 For each language we created
a validation set that was a subset of the CoNLL-X
2Our subset includes the two languages with the lowest ac-
curacy in the CoNLL-X evaluations (Turkish and Arabic), the
language with the highest accuracy (Japanese), the most non-
projective language (Dutch), a moderately non-projective lan-
guage (Slovene), and a highly projective language (Spanish).
All languages but Spanish have multi-root parses in their data.
We are grateful to the providers of the treebanks that constituted
the data of our experiments (Hajic? et al, 2004; van der Beek et
al., 2002; Kawata and Bartels, 2000; Dz?eroski et al, 2006; Civit
and Mart??, 2002; Oflazer et al, 2003).
language %cd train val. test
Arabic 0.34 49,064 5,315 5,373
Dutch 4.93 178,861 16,208 5,585
Japanese 0.70 141,966 9,495 5,711
Slovene 1.59 22,949 5,801 6,390
Spanish 0.06 78,310 11,024 5,694
Turkish 1.26 51,827 5,683 7,547
Table 1: Information for the languages in our experiments.
The 2nd column (%cd) is the percentage of crossing dependen-
cies in the training and validation sets. The last three columns
report the size in tokens of the training, validation and test sets.
training set for that language. The remainder of each
training set was used to train the models for the dif-
ferent languages. The validation sets were used to
tune the meta-parameters (e.g., the value of the reg-
ularization constantC) of the different training algo-
rithms. We used the official test sets and evaluation
script from the CoNLL-X task. All of the results that
we report are for unlabeled dependency parsing.3
The non-projective models were trained on the
CoNLL-X data in its original form. Since the pro-
jective models assume that the dependencies in the
data are non-crossing, we created a second train-
ing set for each language where non-projective de-
pendency structures were automatically transformed
into projective structures. All projective models
were trained on these new training sets.4 Our feature
space is based on that of McDonald et al (2005a).5
6.2 Results
We performed experiments using three training al-
gorithms: the averaged perceptron (Collins, 2002),
log-linear training (via conjugate gradient descent),
and max-margin training (via the EG algorithm).
Each of these algorithms was trained using pro-
jective and non-projective methods, yielding six
training settings per language. The different
training algorithms have various meta-parameters,
which we optimized on the validation set for
each language/training-setting combination. The
3Our algorithms also support labeled parsing (see Section
3.4). Initial experiments with labeled models showed the same
trend that we report here for unlabeled parsing, so for simplicity
we conducted extensive experiments only for unlabeled parsing.
4The transformations were performed by running the pro-
jective parser with score +1 on correct dependencies and -1 oth-
erwise: the resulting trees are guaranteed to be projective and to
have a minimum loss with respect to the correct tree. Note that
only the training sets were transformed.
5It should be noted that McDonald et al (2006) use a richer
feature set that is incomparable to our features.
148
Perceptron Max-Margin Log-Linear
p np p np p np
Ara 71.74 71.84 71.74 72.99 73.11 73.67
Dut 77.17 78.83 76.53 79.69 76.23 79.55
Jap 91.90 91.78 92.10 92.18 91.68 91.49
Slo 78.02 78.66 79.78 80.10 78.24 79.66
Spa 81.19 80.02 81.71 81.93 81.75 81.57
Tur 71.22 71.70 72.83 72.02 72.26 72.62
Table 2: Test data results. The p and np columns show results
with projective and non-projective training respectively.
Ara Dut Jap Slo Spa Tur AV
P 71.74 78.83 91.78 78.66 81.19 71.70 79.05
E 72.99 79.69 92.18 80.10 81.93 72.02 79.82
L 73.67 79.55 91.49 79.66 81.57 72.26 79.71
Table 3: Results for the three training algorithms on the differ-
ent languages (P = perceptron, E = EG, L = log-linear models).
AV is an average across the results for the different languages.
averaged perceptron has a single meta-parameter,
namely the number of iterations over the training set.
The log-linear models have two meta-parameters:
the regularization constant C and the number of
gradient steps T taken by the conjugate-gradient
optimizer. The EG approach also has two meta-
parameters: the regularization constant C and the
number of iterations, T .6 For models trained using
non-projective algorithms, both projective and non-
projective parsing was tested on the validation set,
and the highest scoring of these two approaches was
then used to decode test data sentences.
Table 2 reports test results for the six training sce-
narios. These results show that for Dutch, which is
the language in our data that has the highest num-
ber of crossing dependencies, non-projective train-
ing gives significant gains over projective training
for all three training methods. For the other lan-
guages, non-projective training gives similar or even
improved performance over projective training.
Table 3 gives an additional set of results, which
were calculated as follows. For each of the three
training methods, we used the validation set results
to choose between projective and non-projective
training. This allows us to make a direct com-
parison of the three training algorithms. Table 3
6We trained the perceptron for 100 iterations, and chose the
iteration which led to the best score on the validation set. Note
that in all of our experiments, the best perceptron results were
actually obtained with 30 or fewer iterations. For the log-linear
and EG algorithms we tested a number of values for C, and for
each value of C ran 100 gradient steps or EG iterations, finally
choosing the best combination of C and T found in validation.
shows the results of this comparison.7 The results
show that log-linear and max-margin models both
give a higher average accuracy than the perceptron.
For some languages (e.g., Japanese), the differences
from the perceptron are small; however for other
languages (e.g., Arabic, Dutch or Slovene) the im-
provements seen are quite substantial.
7 Conclusions
This paper describes inference algorithms for
spanning-tree distributions, focusing on the funda-
mental problems of computing partition functions
and marginals. Although we concentrate on log-
linear and max-margin estimation, the inference al-
gorithms we present can serve as black-boxes in
many other statistical modeling techniques.
Our experiments suggest that marginal-based
training produces more accurate models than per-
ceptron learning. Notably, this is the first large-scale
application of the EG algorithm, and shows that it is
a promising approach for structured learning.
In line with McDonald et al (2005b), we confirm
that spanning-tree models are well-suited to depen-
dency parsing, especially for highly non-projective
languages such as Dutch. Moreover, spanning-tree
models should be useful for a variety of other prob-
lems involving structured data.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their constructive comments. In addi-
tion, the authors gratefully acknowledge the follow-
ing sources of support. Terry Koo was funded by
a grant from the NSF (DMS-0434222) and a grant
from NTT, Agmt. Dtd. 6/21/1998. Amir Glober-
son was supported by a fellowship from the Roth-
schild Foundation - Yad Hanadiv. Xavier Carreras
was supported by the Catalan Ministry of Innova-
tion, Universities and Enterprise, and a grant from
NTT, Agmt. Dtd. 6/21/1998. Michael Collins was
funded by NSF grants 0347631 and DMS-0434222.
7We ran the sign test at the sentence level to measure the
statistical significance of the results aggregated across the six
languages. Out of 2,472 sentences total, log-linear models gave
improved parses over the perceptron on 448 sentences, and
worse parses on 343 sentences. The max-margin method gave
improved/worse parses for 500/383 sentences. Both results are
significant with p ? 0.001.
149
References
J. Baker. 1979. Trainable grammars for speech recognition. In
97th meeting of the Acoustical Society of America.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester. 2004. Ex-
ponentiated gradient algorithms for large?margin structured
classification. In NIPS.
L.E. Baum, T. Petrie, G. Soules, and N. Weiss. 1970. A max-
imization technique occurring in the statistical analysis of
probabilistic functions of markov chains. Annals of Mathe-
matical Statistics, 41:164?171.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on
multilingual dependency parsing. In Proc. CoNLL-X.
X. Carreras. 2007. Experiments with a higher-order projective
dependency parser. In Proc. EMNLP-CoNLL.
S. Chopra. 1989. On the spanning tree polyhedron. Oper. Res.
Lett., pages 25?29.
Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a
directed graph. Science Sinica, 14:1396?1400.
M. Civit and Ma A. Mart??. 2002. Design principles for a Span-
ish treebank. In Proc. of the First Workshop on Treebanks
and Linguistic Theories (TLT).
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In Proc. EMNLP.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Z?abokrtsky, and
A. Z?ele. 2006. Towards a Slovene dependency treebank. In
Proc. of the Fifth Intern. Conf. on Language Resources and
Evaluation (LREC).
J. Edmonds. 1967. Optimum branchings. Journal of Research
of the National Bureau of Standards, 71B:233?240.
J. Eisner. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proc. COLING.
A. Globerson, T. Koo, X. Carreras, and M. Collins. 2007. Ex-
ponentiated gradient algorithms for log-linear structured pre-
diction. In Proc. ICML.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic
Language Resources and Tools, pages 110?117.
M. Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler. 1999.
Estimators for stochastic unification-based grammars. In
Proc. ACL.
M. Johnson. 2001. Joint and conditional estimation of tagging
and parsing models. In Proc. ACL.
Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese
treebank in VERBMOBIL. Verbmobil-Report 240, Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditonal ran-
dom fields: Probabilistic models for segmenting and labeling
sequence data. In Proc. ICML.
R. McDonald and F. Pereira. 2006. Online learning of approx-
imate dependency parsing algorithms. In Proc. EACL.
R. McDonald and G. Satta. 2007. On the complexity of non-
projective data-driven dependency parsing. In Proc. IWPT.
R. McDonald, K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proc. HLT-EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual
dependency parsing with a two-stage discriminative parser.
In Proc. CoNLL-X.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. CoNLL.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In A. Abeille?, editor, Tree-
banks: Building and Using Parsed Corpora, chapter 15.
Kluwer Academic Publishers.
M.A. Paskin. 2001. Cubic-time parsing and learning algo-
rithms for grammatical bigram models. Technical Report
UCB/CSD-01-1148, University of California, Berkeley.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent Sys-
tems: Networks of Plausible Inference (2nd edition). Mor-
gan Kaufmann Publishers.
S. Riezler, R. Kaplan, T. King, J. Maxwell, A. Vasserman, and
R. Crouch. 2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proc. HLT-NAACL.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL.
N.A. Smith and J. Eisner. 2005. Contrastive estimation: Train-
ing log-linear models on unlabeled data. In Proc. ACL.
D.A. Smith and N.A. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In Proc. EMNLP-CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2004a. Max-margin
markov networks. In NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.
2004b. Max-margin parsing. In Proc. EMNLP.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdependent
and structured output spaces. In Proc. ICML.
W. Tutte. 1984. Graph Theory. Addison-Wesley.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
Y. Matsumoto Y. Cheng, M. Asahara. 2005. Machine learning-
based dependency analyzer for chinese. In Proc. ICCC.
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. IWPT.
150
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 678?687, Prague, June 2007. c?2007 Association for Computational Linguistics
Online Learning of Relaxed CCG Grammars for Parsing to Logical Form
Luke S. Zettlemoyer and Michael Collins
MIT CSAIL
lsz@csail.mit.edu,mcollins@csail.mit.edu
Abstract
We consider the problem of learning to
parse sentences to lambda-calculus repre-
sentations of their underlying semantics and
present an algorithm that learns a weighted
combinatory categorial grammar (CCG). A
key idea is to introduce non-standard CCG
combinators that relax certain parts of the
grammar?for example allowing flexible
word order, or insertion of lexical items?
with learned costs. We also present a new,
online algorithm for inducing a weighted
CCG. Results for the approach on ATIS
data show 86% F-measure in recovering
fully correct semantic analyses and 95.9%
F-measure by a partial-match criterion, a
more than 5% improvement over the 90.3%
partial-match figure reported by He and
Young (2006).
1 Introduction
Recent work (Mooney, 2007; He and Young, 2006;
Zettlemoyer and Collins, 2005) has developed learn-
ing algorithms for the problem of mapping sentences
to underlying semantic representations. In one such
approach (Zettlemoyer and Collins, 2005) (ZC05),
the input to the learning algorithm is a training set
consisting of sentences paired with lambda-calculus
expressions. For instance, the training data might
contain the following example:
Sentence: list flights to boston
Logical Form: ?x.flight(x) ? to(x, boston)
In this case the lambda-calculus expression denotes
the set of all flights that land in Boston. In ZC05
it is assumed that training examples do not include
additional information, for example parse trees or
a) on may four atlanta to denver delta flight 257
?x.month(x,may) ? day number(x, fourth)?
from(x, atlanta) ? to(x, denver)?
airline(x, delta air lines) ? flight(x)?
flight number(x, 257)
b) show me information on american airlines from fort worth
texas to philadelphia
?x.airline(x, american airlines)?
from(x, fort worth) ? to(x, philadelphia)
c) okay that one?s great too now we?re going to go on april
twenty second dallas to washington the latest nighttime
departure one way
argmax(?x.flight(x) ? from(x, dallas)?
to(x,washington) ? month(x, april)?
day number(x, 22) ? during(x, night)?
one way(x), ?y.depart time(y))
Figure 1: Three sentences from the ATIS domain.
other derivations. The output from the learning algo-
rithm is a combinatory categorial grammar (CCG),
together with parameters that define a log-linear dis-
tribution over parses under the grammar. Experi-
ments show that the approach gives high accuracy on
two database-query problems, introduced by Zelle
and Mooney (1996) and Tang and Mooney (2000).
The use of a detailed grammatical formalism such
as CCG has the advantage that it allows a system to
handle quite complex semantic effects, such as co-
ordination or scoping phenomena. In particular, it
allows us to leverage the considerable body of work
on semantics within these formalisms, for example
see Carpenter (1997). However, a grammar based
on a formalism such as CCG can be somewhat rigid,
and this can cause problems when a system is faced
with spontaneous, unedited natural language input,
as is commonly seen in natural language interface
applications. For example, consider the sentences
shown in figure 1, which were taken from the ATIS
travel-planning domain (Dahl et al, 1994). These
sentences exhibit characteristics which present sig-
nificant challenges to the approach of ZC05. For ex-
678
ample, the sentences have quite flexible word order,
and include telegraphic language where some words
are effectively omitted.
In this paper we describe a learning algorithm that
retains the advantages of using a detailed grammar,
but is highly effective in dealing with phenomena
seen in spontaneous natural language, as exempli-
fied by the ATIS domain. A key idea is to extend
the approach of ZC05 by allowing additional non-
standard CCG combinators. These combinators re-
lax certain parts of the grammar?for example al-
lowing flexible word order, or insertion of lexical
items?with learned costs for the new operations.
This approach has the advantage that it can be seam-
lessly integrated into CCG learning algorithms such
as the algorithm described in ZC05.
A second contribution of the work is a new, on-
line algorithm for CCG learning. The approach in-
volves perceptron training of a model with hidden
variables. In this sense it is related to the algorithm
of Liang et al (2006). However it has the addi-
tional twist of also performing grammar induction
(lexical learning) in an online manner. In our exper-
iments, we show that the new algorithm is consid-
erably more efficient than the ZC05 algorithm; this
is important when training on large training sets, for
example the ATIS data used in this paper.
Results for the approach on ATIS data show 86%
F-measure accuracy in recovering fully correct se-
mantic analyses, and 95.9% F-measure by a partial-
match criterion described by He and Young (2006).
The latter figure contrasts with a figure of 90.3% for
the approach reported by He and Young (2006).1
Results on the Geo880 domain also show an im-
provement in accuracy, with 88.9% F-measure for
the new approach, compared to 87.0% F-measure
for the method in ZC05.
2 Background
2.1 Semantics
Training examples in our approach consist of sen-
tences paired with lambda-calculus expressions. We
use a version of the lambda calculus that is closely
related to the one presented by Carpenter (1997).
There are three basic types: t, the type of truth val-
1He and Young (2006) do not give results for recovering
fully correct parses.
ues; e, the type for entities; and r, the type for real
numbers. Functional types are defined by specify-
ing their input and output types, for example ?e, t?
is the type of a function from entities to truth val-
ues. In general, declarative sentences have a logical
form of type t. Question sentences generally have
functional types.2 Each expression is constructed
from constants, logical connectors, quantifiers and
lambda functions.
2.2 Combinatory Categorial Grammars
Combinatory categorial grammar (CCG) is a syn-
tactic theory that models a wide range of linguistic
phenomena (Steedman, 1996; Steedman, 2000).
The core of a CCG grammar is a lexicon ?. For
example, consider the lexicon
flights := N : ?x.flight(x)
to := (N\N)/NP : ?y.?f.?x.f(x) ? to(x, y)
boston := NP : boston
Each entry in the lexicon is a pair consisting of a
word and an associated category. The category con-
tains both syntactic and semantic information. For
example, the first entry states that the word flights
can have the category N : ?x.flight(x). This cat-
egory consists of a syntactic type N , together with
the semantics ?x.flight(x). In general, the seman-
tic entries for words in the lexicon can consist of any
lambda-calculus expression. Syntactic types can ei-
ther be simple types such as N , NP , or S, or can be
more complex types that make use of slash notation,
for example (N\N)/NP .
CCG makes use of a set of combinators which
are used to combine categories to form larger pieces
of syntactic and semantic structure. The simplest
such rules are the functional application rules:
A/B : f B : g ? A : f(g) (>)
B : g A\B : f ? A : f(g) (<)
The first rule states that a category with syntactic
type A/B can be combined with a category to the
right of syntactic type B to create a new category
of type A. It also states that the new semantics
will be formed by applying the function f to
the expression g. The second rule handles argu-
ments to the left. Using these rules, we can parse the
2For example, many question sentences have semantics of
type ?e, t?, as in ?x.flight(x) ? to(x, boston).
679
following phrase to create a new category of typeN :
flights to boston
N (N\N)/NP NP
?x.flight(x) ?y.?f.?x.f(x) ? to(x, y) boston
>
(N\N)
?f.?x.f(x) ? to(x, boston)
<
N
?x.flight(x) ? to(x, boston)
The top-most parse operations pair each word with a
corresponding category from the lexicon. The later
steps are labeled ?> (for each instance of forward
application) or ?< (for backward application).
A second set of combinators in CCG grammars
are the rules of functional composition:
A/B : f B/C : g ? A/C : ?x.f(g(x)) (> B)
B\C : g A\B : f ? A\C : ?x.f(g(x)) (< B)
These rules allow for an unrestricted notion of con-
stituency that is useful for modeling coordination
and other linguistic phenomena. As we will see, they
also turn out to be useful when modeling construc-
tions with relaxed word order, as seen frequently in
domains such as ATIS.
In addition to the application and composition
rules, we will also make use of type raising and co-
ordination combinators. A full description of these
combinators goes beyond the scope of this paper.
Steedman (1996; 2000) presents a detailed descrip-
tion of CCG.
2.3 Log-Linear CCGs
We can generalize CCGs to weighted, or probabilis-
tic, models as follows. Our models are similar to
several other approaches (Ratnaparkhi et al, 1994;
Johnson et al, 1999; Lafferty et al, 2001; Collins,
2004; Taskar et al, 2004). We will write x to de-
note a sentence, and y to denote a CCG parse for a
sentence. We use GEN(x; ?) to refer to all possi-
ble CCG parses for x under some CCG lexicon ?.
We will define f(x, y) ? Rd to be a d-dimensional
feature?vector that represents a parse tree y paired
with an input sentence x. In principle, f could in-
clude features that are sensitive to arbitrary sub-
structures within the pair (x, y). We will define
w ? Rd to be a parameter vector. The optimal parse
for a sentence x under parameters w and lexicon ?
is then defined as
y?(x) = arg max
y?GEN(x;?)
w ? f(x, y) .
Assuming sufficiently local features3 in f , search for
y? can be achieved using dynamic-programming-
style algorithms, typically with some form of beam
search.4 Training a model of this form involves
learning the parameters w and potentially also the
lexicon ?. This paper focuses on a method for learn-
ing a (w,?) pair from a training set of sentences
paired with lambda-calculus expressions.
2.4 Zettlemoyer and Collins 2005
We now give a description of the approach of Zettle-
moyer and Collins (2005). This method will form
the basis for our approach, and will be one of the
baseline models for the experimental comparisons.
The input to the ZC05 algorithm is a set of train-
ing examples (xi, zi) for i = 1 . . . n. Each xi is
a sentence, and each zi is a corresponding lambda-
expression. The output from the algorithm is a pair
(w,?) specifying a set of parameter values, and a
CCG lexicon. Note that for a given training example
(xi, zi), there may be many possible parses y which
lead to the correct semantics zi.5 For this reason
the training problem is a hidden-variable problem,
where the training examples contain only partial in-
formation, and the CCG lexicon and parse deriva-
tions must be learned without direct supervision.
A central part of the ZC05 approach is a function
GENLEX(x, z) which maps a sentence x together
with semantics z to a set of potential lexical entries.
The function GENLEX is defined through a set of
rules?see figure 2?that consider the expression z,
and generate a set of categories that may help in
building the target semantics z. An exhaustive set
of lexical entries is then generated by taking all cat-
egories generated by the GENLEX rules, and pair-
ing themwith all possible sub-strings of the sentence
x. Note that our lexicon can contain multi-word en-
tries, where a multi-word string such as New York
can be paired with a CCG category. The final out-
3For example, features which count the number of lexical
entries of a particular type, or features that count the number of
applications of a particular CCG combinator.
4In our experiments we use a parsing algorithm that is simi-
lar to a CKY-style parser with dynamic programming. Dynamic
programming is used but each entry in the chart maintains a full
semantic expression, preventing a polynomial-time algorithm;
beam search is used to make the approach tractable.
5This problem is compounded by the fact that the lexicon
is unknown, so that many of the possible hidden derivations
involve completely spurious lexical entries.
680
Rules Example categories produced from the logical form
Input Trigger Output Category argmax(?x.flight(x) ? from(x, boston), ?x.cost(x))
constant c NP : c NP : boston
arity one predicate p N : ?x.p(x) N : ?x.flight(x)
arity one predicate p S\NP : ?x.p(x) S\NP : ?x.flight(x)
arity two predicate p2 (S\NP )/NP : ?x.?y.p2(y, x) (S\NP )/NP : ?x.?y.from(y, x)
arity two predicate p2 (S\NP )/NP : ?x.?y.p2(x, y) (S\NP )/NP : ?x.?y.from(x, y)
arity one predicate p1 N/N : ?g.?x.p1(x) ? g(x) N/N : ?g.?x.flight(x) ? g(x)
literal with arity two predicate p2
and constant second argument c N/N : ?g.?x.p2(x, c) ? g(x) N/N : ?g.?x.from(x, boston) ? g(x)
arity two predicate p2 (N\N)/NP : ?y.?g.?x.p2(x, y) ? g(x) (N\N)/NP : ?y.?g.?x.from(x, y) ? g(x)
an argmax /min with second
argument arity one function f NP/N : ?g. argmax /min(g, ?x.f(x)) NP/N : ?g. argmax(g, ?x.cost(x))
arity one function f S/NP : ?x.f(x) S/NP : ?x.cost(x)
arity one function f (N\N)/NP : ?y.?f.?x.g(x) ? f(x) >/< y (N\N)/NP : ?y.?f.?x.g(x) ? cost(x) > y
no trigger S/NP : ?x.x, S/N : ?f.?x.f(x) S/NP : ?x.x, S/N : ?f.?x.f(x)
Figure 2: Rules used in GENLEX. Each row represents a rule. The first column lists the triggers that identify some sub-structure
within a logical form. The second column lists the category that is created. The third column lists categories that are created when
the rule is applied to the logical form at the top of this column. We use the 10 rules described in ZC05 and add two new rules,
listed in the last two rows above. This first new rule is instantiated for greater than (>) and less than (<) comparisions. The second
new rule has no trigger; it is always applied. It generates categories that are used to learn lexical entries for semantically vacuous
sentence prefixes such as the phrase show me information on in the example in figure 1(b).
put from GENLEX(x, z) is a large set of potential
lexical entries, with the vast majority of those en-
tries being spurious. The algorithm in ZC05 embeds
GENLEX within an overall learning approach that
simultaneously selects a small subset of all entries
generated by GENLEX and estimates parameter val-
uesw. Zettlemoyer and Collins (2005) present more
complete details. In section 4.2 we describe a new,
online algorithm that uses GENLEX.
3 Parsing Extensions: Combinators
This section describes a set of CCG combinators
which we add to the conventional CCG combinators
described in section 2.2. These additional combi-
nators are natural extensions of the forward appli-
cation, forward composition, and type-raising rules
seen in CCG. We first describe a set of combina-
tors that allow the parser to significantly relax con-
straints on word order. We then describe a set of
type-raising rules which allow the parser to cope
with telegraphic input (in particular, missing func-
tion words). In both cases these additional rules
lead to significantly more parses for any sentence
x given a lexicon ?. Many of these parses will be
suspect from a linguistic perspective; broadening the
set of CCG combinators in this way might be con-
sidered a dangerous move. However, the learning
algorithm in our approach can learn weights for the
new rules, effectively allowing the model to learn to
use them only in appropriate contexts; in the exper-
iments we show that the rules are highly effective
additions when used within a weighted CCG.
3.1 Application and Composition Rules
The first new combinators we consider are the
relaxed functional application rules:
A\B : f B : g ? A : f(g) (&)
B : g A/B : f ? A : f(g) (.)
These are variants of the original application
rules, where the slash direction on the principal cat-
egories (A/B or A\B) is reversed.6 These rules al-
low simple reversing of regular word order, for ex-
ample
flights one way
N N/N
?x.flight(x) ?f.?x.f(x) ? one way(x)
.
N
?x.flight(x) ? one way(x)
Note that we can recover the correct analysis for this
fragment, with the same lexical entries as those used
for the conventional word order, one-way flights.
A second set of new combinators are the relaxed
functional composition rules:
A\B : f B/C : g ? A/C : ?x.f(g(x)) (& B)
B\C : g A/B : f ? A\C : ?x.f(g(x)) (. B)
These rules are variantions of the standard func-
tional composition rules, where the slashes of the
principal categories are reversed.
6Rules of this type are non-standard in the sense that they
violate Steedman?s Principle of Consistency (2000); this princi-
ple states that rules must be consistent with the slash direction
of the principal category. Steedman (2000) only considers rules
that do not violate this principle?for example, crossed compo-
sition rules, which we consider later, and which Steedman also
considers, do not violate this principle.
681
An important point is that that these new compo-
sition and application rules can deal with quite flex-
ible word orders. For example, take the fragment to
washington the latest flight. In this case the parse is
to washington the latest flight
N\N NP/N N
?f.?x.f(x)? ?f. argmax(f, ?x.flight(x)
to(x,washington) ?y.depart time(y))
.B
NP\N
?f. argmax(?x.f(x)?
to(x,washington), ?y.depart time(y))
&
NP
argmax(?x.flight(x) ? to(x,washington),
?y.depart time(y))
Note that in this case the substring the latest has cat-
egory NP/N , and this prevents a naive parse where
the latest first combines with flight, and to washing-
ton then combines with the latest flight. The func-
tional composition rules effectively allow the latest
to take scope over flight and to washington, in spite
of the fact that the latest appears between the two
other sub-strings. Examples like this are quite fre-
quent in domains such as ATIS.
We add features in the model which track the oc-
currences of each of these four new combinators.
Specifically, we have four new features in the def-
inition of f; each feature tracks the number of times
one of the combinators is used in a CCG parse. The
model learns parameter values for each of these fea-
tures, allowing it to learn to penalise these rules to
the correct extent.
3.2 Additional Rules of Type-Raising
We now describe new CCG operations designed to
deal with cases where words are in some sense miss-
ing in the input. For example, in the string flights
Boston to New York, one style of analysis would
assume that the preposition from had been deleted
from the position before Boston.
The first set of rules is generated from the follow-
ing role-hypothesising type shifting rules template:
NP : c ? N\N : ?f.?x.f(x) ? p(x, c) (TR)
This rule can be applied to any NP with semantics
c, and any arity-two function p such that the second
argument of p has the same type as c. By ?any? arity-
two function, we mean any of the arity-two func-
tions seen in training data. We define features within
the feature-vector f that are sensitive to the number
of times these rules are applied in a parse; a separate
feature is defined for each value of p.
In practice, in our experiments most rules of this
form have p as the semantics of some preposition,
for example from or to. A typical example of a use
of this rule would be the following:
flights boston to new york
N NP N\N
?x.flight(x) bos ?f.?x.f(x)
?to(x, new york)
TR
N\N
?f.?x.f(x) ? from(x, bos)
<
N
?f.?x.flight(x) ? from(x, bos)
<
N
?x.flight(x) ? to(x, new york) ? from(x, bos)
The second rule we consider is the null-head type
shifting rule:
N\N : f ? N : f(?x.true) (TN)
This rule allows parses of fragments such as Amer-
ican Airlines from New York, where there is again a
word that is in some sense missing (it is straightfor-
ward to derive a parse for American Airlines flights
from New York). The analysis would be as follows:
American Airlines from New York
N/N N\N
?f.?x.f(x) ? airline(x, aa) ?f.?x.f(x) ? from(x, new york)
TN
N
?x.from(x, new york)
>
N
?x.airline(x, aa) ? from(x, new york)
The new rule effectively allows the preposi-
tional phrase from New York to type-shift to
an entry with syntactic type N and semantics
?x.from(x, new york), representing the set of all
things from New York.7
We introduce a single additional feature which
counts the number of times this rule is used.
3.3 Crossed Composition Rules
Finally, we include crossed functional composition
rules:
A/B : f B\C : g ? A\C : ?x.f(g(x)) (>B?)
B/C : g A\B : f ? A/C : ?x.f(g(x)) (<B?)
These rules are standard CCG operators but they
were not used by the parser described in ZC05.
When used in unrestricted contexts, they can sig-
nificantly relax word order. Again, we address this
7Note that we do not analyze this prepositional phrase as
having the semantics ?x.flight(x) ? from(x, new york)?
although in principle this is possible?as the flight(x) predi-
cate is not necessarily implied by this utterance.
682
dallas to washington the latest on friday
NP (N\N)/NP NP NP/N (N\N)/NP NP
dallas ?y.?f.?x.f(x) washington ?f. argmax(f, ?y.?f.?x.f(x) friday
?to(x, y) ?y.depart time(y)) ?day(x, y)
TR > >
N\N N\N N\N
?f.?x.f(x) ? from(x, dallas) ?f.?x.f(x) ? to(x,washington) ?f.?x.f(x) ? day(x, friday)
<B TN
N\N N
?f.?x.f(x) ? from(x, dallas) ? to(x,washington) ?x.day(x, friday)
.B
NP\N
?f. argmax(?x.f(x) ? from(x, dallas) ? to(x,washington), ?y.depart time(y))
&
NP
argmax(?x.day(x, friday) ? from(x, dallas) ? to(x,washington), ?y.depart time(y))
Figure 3: A parse with the flexible parser.
problem by introducing features that count the num-
ber of times they are used in a parse.8
3.4 An Example
As a final point, to see how these rules can interact
in practice, see figure 3. This example demonstrates
the use of the relaxed application and composition
rules, as well as the new type-raising rules.
4 Learning
This section describes an approach to learning in our
model. We first define the features used and then de-
scribe a new online learning algorithm for the task.
4.1 Features in the Model
Section 2.3 described the use of a function f(x, y)
which maps a sentence x together with a CCG parse
y to a feature vector. As described in section 3,
we introduce features for the new CCG combina-
tors. In addition, we follow ZC05 in defining fea-
tures which track the number of times each lexical
item in ? is used. For example, we would have one
feature tracking the number of times the lexical entry
flights := N : ?x.flights(x) is used in a parse,
and similar features for all other members of ?.
Finally, we introduce new features which directly
consider the semantics of a parse. For each predicate
f seen in training data, we introduce a feature that
counts the number of times f is conjoined with itself
at some level in the logical form. For example, the
expression ?x.flight(x) ? from(x, new york) ?
from(x, boston) would trigger the new feature for
8In general, applications of the crossed composition rules
can be lexically governed, as described in work on Multi-Modal
CCG (Baldridge, 2002). In the future we would like to incorpo-
rate more fine-grained lexical distinctions of this type.
the from predicate signaling that the logical-form
describes flights with more than one origin city. We
introduce similar features which track disjunction as
opposed to conjunction.
4.2 An Online Learning Algorithm
Figure 4 shows a learning algorithm that takes a
training set of (xi, zi) pairs as input, and returns
a weighted CCG (i.e., a pair (w,?)) as its output.
The algorithm is online, in that it visits each ex-
ample in turn, and updates both w and ? if neces-
sary. In Step 1 on each example, the input xi is
parsed. If it is parsed correctly, the algorithm im-
mediately moves to the next example. In Step 2,
the algorithm temporarily introduces all lexical en-
tries seen in GENLEX(xi, zi), and finds the highest
scoring parse that leads to the correct semantics zi.
A small subset of GENLEX(xi, zi)?namely, only
those lexical entries that are contained in the highest
scoring parse?are added to ?. In Step 3, a simple
perceptron update (Collins, 2002) is performed. The
hypothesis is parsed again with the new lexicon, and
an update to the parameters w is made if the result-
ing parse does not have the correct logical form.
This algorithm differs from the approach in ZC05
in a couple of important respects. First, the ZC05 al-
gorithm performed learning of the lexicon ? at each
iteration in a batch method, requiring a pass over the
entire training set. The new algorithm is fully online,
learning both ? and w in an example-by-example
fashion. This has important consequences for the
efficiency of the algorithm. Second, the parameter
estimation method in ZC05 was based on stochastic
gradient descent on a log-likelihood objective func-
tion. The new algorithm makes use of perceptron
683
Inputs: Training examples {(xi, zi) : i = 1 . . . n} where
each xi is a sentence, each zi is a logical form. An initial
lexicon ?0. Number of training iterations, T .
Definitions: GENLEX(x, z) takes as input a sentence x and
a logical form z and returns a set of lexical items as de-
scribed in section 2.4. GEN(x; ?) is the set of all parses
for x with lexicon ?. GEN(x, z; ?) is the set of all parses
for x with lexicon ?, which have logical form z. The
function f(x, y) represents the features described in sec-
tion 4.1. The function L(y) maps a parse tree y to its
associated logical form.
Initialization: Set parameters w to initial values described in
section 6.2. Set ? = ?0.
Algorithm:
? For t = 1 . . . T, i = 1 . . . n :
Step 1: (Check correctness)
? Let y? = argmaxy?GEN(xi;?) w ? f(xi, y) .
? If L(y?) = zi, go to the next example.
Step 2: (Lexical generation)
? Set ? = ? ? GENLEX(xi, zi) .
? Let y? = argmaxy?GEN(xi,zi;?) w ? f(xi, y) .
? Define ?i to be the set of lexical entries in y?.
? Set lexicon to ? = ? ? ?i .
Step 3: (Update parameters)
? Let y? = argmaxy?GEN(xi;?) w ? f(xi, y) .
? If L(y?) 6= zi :
? Set w = w + f(xi, y?) ? f(xi, y?) .
Output: Lexicon ? together with parameters w.
Figure 4: An online learning algorithm.
updates, which are simpler and cheaper to compute.
As in ZC05, the algorithm assumes an initial lex-
icon ?0 that contains two types of entries. First, we
compile entries such as Boston := NP : boston
for entities such as cities, times and month-names
that occur in the domain or underlying database. In
practice it is easy to compile a list of these atomic
entities. Second, the lexicon has entries for some
function words such as wh-words, and determiners.9
5 Related Work
There has been a significant amount of previ-
ous work on learning to map sentences to under-
lying semantic representations. A wide variety
9Our assumption is that these entries are likely to be domain
independent, so it is simple enough to compile a list that can
be reused in new domains. Another approach, which we may
consider in the future, would be to annotate a small subset of
the training examples with full CCG derivations, from which
these frequently occurring entries could be learned.
of techniques have been considered including ap-
proaches based on machine translation techniques
(Papineni et al, 1997; Ramaswamy and Kleindienst,
2000; Wong and Mooney, 2006), parsing techniques
(Miller et al, 1996; Ge and Mooney, 2006), tech-
niques that use inductive logic programming (Zelle
and Mooney, 1996; Thompson and Mooney, 2002;
Tang and Mooney, 2000; Kate et al, 2005), and
ideas from string kernels and support vector ma-
chines (Kate and Mooney, 2006; Nguyen et al,
2006). In our experiments we compare to He and
Young (2006) on the ATIS domain and Zettlemoyer
and Collins (2005) on the Geo880 domain, be-
cause these systems currently achieve the best per-
formance on these problems.
The approach of Zettlemoyer and Collins (2005)
was presented in section 2.4. He and Young (2005)
describe an algorithm that learns a probabilistic
push-down automaton that models hierarchical de-
pendencies but can still be trained on a data set that
does not have full treebank-style annotations. This
approach has been integrated with a speech recog-
nizer and shown to be robust to recognition errors
(He and Young, 2006).
There is also related work in the CCG litera-
ture. Clark and Curran (2003) present a method for
learning the parameters of a log-linear CCG pars-
ing model from fully annotated normal?form parse
trees. Watkinson and Manandhar (1999) present an
unsupervised approach for learning CCG lexicons
that does not represent the semantics of the train-
ing sentences. Bos et al (2004) present an al-
gorithm that learns CCG lexicons with semantics
but requires fully?specified CCG derivations in the
training data. Bozsahin (1998) presents work on us-
ing CCG to model languages with free word order.
In addition, there is related work that focuses on
modeling child language learning. Siskind (1996)
presents an algorithm that learns word-to-meaning
mappings from sentences that are paired with a set
of possible meaning representations. Villavicencio
(2001) describes an approach that learns a categorial
grammar with syntactic and semantic information.
Both of these approaches use sentences from child-
directed speech, which differ significantly from the
natural language interface queries we consider.
Finally, there is work on manually developing
parsing techniques to improve robustness (Carbonell
684
and Hayes, 1983; Seneff, 1992). In contrast, our ap-
proach is integrated into a learning framework.
6 Experiments
The main focus of our experiments is on the ATIS
travel planning domain. For development, we used
4978 sentences, split into a training set of 4500 ex-
amples, and a development set of 478 examples. For
test, we used the ATIS NOV93 test set which con-
tains 448 examples. To create the annotations, we
created a script that maps the original SQL annota-
tions provided with the data to lambda-calculus ex-
pressions.
He and Young (2006) previously reported results
on the ATIS domain, using a learning approach
which also takes sentences paired with semantic an-
notations as input. In their case, the semantic struc-
tures resemble context-free parses with semantic (as
opposed to syntactic) non-terminal labels. In our ex-
periments we have used the same split into train-
ing and test data as He and Young (2006), ensur-
ing that our results are directly comparable. He and
Young (2006) report partial match figures for their
parser, based on precision and recall in recovering
attribute-value pairs. (For example, the sentence
flights to Boston would have a single attribute-value
entry, namely destination = Boston.) It is sim-
ple for us to map from lambda-calculus expressions
to attribute-value entries of this form; for example,
the expression to(x,Boston) would be mapped to
destination = Boston. He and Young (2006) gave
us their data and annotations, so we can directly
compare results on the partial-match criterion. We
also report accuracy for exact matches of lambda-
calculus expressions, which is a stricter criterion.
In addition, we report results for the method on
the Geo880 domain. This allows us to compare
directly to the previous work of Zettlemoyer and
Collins (2005), using the same split of the data into
training and test sets of sizes 600 and 280 respec-
tively. We use cross-validation of the training set, as
opposed to a separate development set, for optimiza-
tion of parameters.
6.1 Improving Recall
The simplest approach to the task is to train the
parser and directly apply it to test sentences. In our
experiments we will see that this produces results
which have high precision, but somewhat lower re-
call, due to some test sentences failing to parse (usu-
ally due to words in the test set which were never
observed in training data). A simple strategy to alle-
viate this problem is as follows. If the sentence fails
to parse, we parse the sentence again, this time al-
lowing parse moves which can delete words at some
cost. The cost of this deletion operation is optimized
on development data. This approach can signifi-
cantly improve F-measure on the partial-match cri-
terion in particular. We report results both with and
without this second pass strategy.
6.2 Parameters in the Approach
The algorithm in figure 4 has a number of param-
eters, the set {T, ?, ?, ?}, which we now describe.
The values of these parameters were chosen to op-
timize the performance on development data. T is
the number of passes over the training set, and was
set to be 4. Each lexical entry in the initial lexicon
?0 has an associated feature which counts the num-
ber of times this entry is seen in a parse. The initial
parameter value in w for all features of this form
was chosen to be some value ?. Each of the new
CCG rules?the application, composition, crossed-
composition, and type-raising rules described in sec-
tion 3?has an associated parameter. We set al of
these parameters to the same initial value ?. Finally,
when new lexical entries are added to ? (in step 2
of the algorithm), their initial weight is set to some
value ?. In practice, optimization on development
data led to a positive value for ?, and negative val-
ues for ? and ?.
6.3 Results
Table 1 shows accuracy for the method by the exact-
match criterion on the ATIS test set. The two pass
strategy actually hurts F-measure in this case, al-
though it does improve recall of the method.
Table 2 shows results under the partial-match cri-
terion. The results for our approach are higher
than those reported by He and Young (2006) even
without the second, high-recall, strategy. With the
two-pass strategy our method has more than halved
the F-measure error rate, giving improvements from
90.3% F-measure to 95.9% F-measure.
Table 3 shows results on the Geo880 domain. The
685
Precision Recall F1
Single-Pass Parsing 90.61 81.92 86.05
Two-Pass Parsing 85.75 84.6 85.16
Table 1: Exact-match accuracy on the ATIS test set.
Precision Recall F1
Single-Pass Parsing 96.76 86.89 91.56
Two-Pass Parsing 95.11 96.71 95.9
He and Young (2006) ? ? 90.3
Table 2: Partial-credit accuracy on the ATIS test set.
new method gives improvements in performance
both with and without the two pass strategy, showing
that the new CCG combinators, and the new learn-
ing algorithm, give some improvement on even this
domain. The improved performance comes from a
slight drop in precision which is offset by a large in-
crease in recall.
Table 4 shows ablation studies on the ATIS data,
where we have selectively removed various aspects
of the approach, to measure their impact on perfor-
mance. It can be seen that accuracy is seriously de-
graded if the new CCG rules are removed, or if the
features associated with these rules (which allow the
model to penalize these rules) are removed.
Finally, we report results concerning the effi-
ciency of the new online algorithm as compared to
the ZC05 algorithm. We compared running times
for the new algorithm, and the ZC05 algorithm, on
the geography domain, with both methods making
4 passes over the training data. The new algorithm
took less than 4 hours, compared to over 12 hours
for the ZC05 algorithm. The main explanation for
this improved performance is that on many training
examples,10 in step 1 of the new algorithm a cor-
rect parse is found, and the algorithm immediately
moves on to the next example. Thus GENLEX is
not required, and in particular parsing the example
with the large set of entries generated by GENLEX
is not required.
7 Discussion
We presented a new, online algorithm for learn-
ing a combinatory categorial grammar (CCG), to-
gether with parameters that define a log-linear pars-
ing model. We showed that the use of non-standard
CCG combinators is highly effective for parsing sen-
10Measurements on the Geo880 domain showed that in the 4
iterations, 83.3% of all parses were successful at step 1.
Precision Recall F1
Single-Pass Parsing 95.49 83.2 88.93
Two-Pass Parsing 91.63 86.07 88.76
ZC05 96.25 79.29 86.95
Table 3: Exact-match accuracy on the Geo880 test set.
Precision Recall F1
Full Online Method 87.26 74.44 80.35
Without control features 70.33 42.45 52.95
Without relaxed word order 82.81 63.98 72.19
Without word insertion 77.31 56.94 65.58
Table 4: Exact-match accuracy on the ATIS development set
for the full algorithm and restricted versions of it. The sec-
ond row reports results of the approach without the features
described in section 3 that control the use of the new combi-
nators. The third row presents results without the combinators
from section 3.1 that relax word order. The fourth row reports
experiments without the type-raising combinators presented in
section 3.2.
tences with the types of phenomena seen in sponta-
neous, unedited natural language. The resulting sys-
tem achieved significant accuracy improvements in
both the ATIS and Geo880 domains.
Acknowledgements
Wewould like to thank Yulan He and Steve Young
for their help with obtaining the ATIS data set. We
also acknowledge the support for this research. Luke
Zettlemoyer was funded by a Microsoft graduate
research fellowship and Michael Collins was sup-
ported by the National Science Foundation under
grants 0347631 and DMS-0434222.
References
Jason Baldridge. 2002. Lexically Specified Derivational Con-
trol in Combinatory Categorial Grammar. Ph.D. thesis,
University of Edinburgh.
Johan Bos, Stephen Clark, Mark Steedman, James R. Curran,
and Julia Hockenmaier. 2004. Wide-coverage semantic rep-
resentations from a CCG parser. In Proceedings of the 20th
International Conference on Computational Linguistics.
Cem Bozsahin. 1998. Deriving the predicate-argument struc-
ture for a free word order language. In Proceedings of the
36th Annual Meeting of the Association for Computational
Linguistics.
Jaime G. Carbonell and Philip J. Hayes. 1983. Recovery strate-
gies for parsing extragrammatical language. American Jour-
nal of Computational Linguistics, 9.
Bob Carpenter. 1997. Type-Logical Semantics. The MIT Press.
Stephen Clark and James R. Curran. 2003. Log-linear models
for wide-coverage CCG parsing. In Proceedings of the Con-
ference on Empirical Methods in Natural Language Process-
ing.
686
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Processing.
Michael Collins. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-free
methods. In Harry Bunt, John Carroll and Giorgio Satta,
editors, New Developments in Parsing Technology. Kluwer.
Deborah A. Dahl, Madeleine Bates, Michael Brown, William
Fisher, Kate Hunicke-Smith, David Pallett, Christine Pao,
Alexander Rudnicky, and Elizabeth Shriberg. 1994. Ex-
panding the scope of the atis task: the atis-3 corpus. In ARPA
Human Language Technology Workshop.
Ruifang Ge and Raymond J. Mooney. 2006. Discriminative
reranking for semantic parsing. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions.
Yulan He and Steve Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and Lan-
guage.
Yulan He and Steve Young. 2006. Spoken language under-
standing using the hidden vector state model. Speech Com-
munication Special Issue on Spoken Language Understand-
ing for Conversational Systems.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proceedings of the Association for
Computational Linguistics.
Rohit J. Kate and Raymond J. Mooney. 2006. Using string-
kernels for learning semantic parsers. In Proceedings of the
44th Annual Meeting of the Association for Computational
Linguistics.
Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005.
Learning to transform natural to formal languages. In Pro-
ceedings of the 20th National Conference on Artificial Intel-
ligence.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of the
18th International Conference on Machine Learning.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach to ma-
chine translation. In Proceedings of the 44th Annual Meeting
of the Association for Computational Linguistics.
Scott Miller, David Stallard, Robert J. Bobrow, and Richard L.
Schwartz. 1996. A fully statistical approach to natural lan-
guage interfaces. In Proceedings of the Association for Com-
putational Linguistics.
Raymond J. Mooney. 2007. Learning for semantic parsing. In
Computational Linguistics and Intelligent Text Processing:
Proceedings of the 8th International Conference.
Le-Minh Nguyen, Akira Shimazu, and Xuan-Hieu Phan. 2006.
Semantic parsing with structured SVM ensemble classifica-
tion models. In Proceedings of the COLING/ACL 2006Main
Conference Poster Sessions.
K. A. Papineni, S. Roukos, and T. R. Ward. 1997. Feature-
based language understanding. In Proceedings of European
Conference on Speech Communication and Technology.
Ganesh N. Ramaswamy and Jan Kleindienst. 2000. Hierar-
chical feature-based translation for scalable natural language
understanding. In Proceedings of 6th International Confer-
ence on Spoken Language Processing.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994.
A maximum entropy model for parsing. In Proceedings of
the International Conference on Spoken Language Process-
ing.
Stephanie Seneff. 1992. Robust parsing for spoken language
systems. In Proceedings of the IEEE Conference on Acous-
tics, Speech, and Signal Processing.
Jeffrey M. Siskind. 1996. A computational study of cross-
situational techniques for learning word-to-meaning map-
pings. Cognition, 61(2-3).
Mark Steedman. 1996. Surface Structure and Interpretation.
The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The MIT Press.
Lappoon R. Tang and Raymond J. Mooney. 2000. Automated
construction of database interfaces: Integrating statistical
and relational learning for semantic parsing. In Joint Confer-
ence on Empirical Methods in Natural Language Processing
and Very Large Corpora.
Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and
Christopher Manning. 2004. Max-margin parsing. In Pro-
ceedings of the Conference on Empirical Methods in Natural
Language Processing.
Cynthia A. Thompson and Raymond J. Mooney. 2002. Acquir-
ing word-meaning mappings for natural language interfaces.
Journal of Artificial Intelligence Research, 18.
Aline Villavicencio. 2001. The acquisition of a unification-
based generalised categorial grammar. Ph.D. thesis, Uni-
versity of Cambridge.
Stephen Watkinson and Suresh Manandhar. 1999. Unsuper-
vised lexical learning with categorial grammars using the
LLL corpus. In Proceedings of the 1st Workshop on Learn-
ing Language in Logic.
Yuk Wah Wong and Raymond Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation. In Pro-
ceedings of the Human Language Technology Conference of
the NAACL.
John M. Zelle and Raymond J. Mooney. 1996. Learning to
parse database queries using inductive logic programming.
In Proceedings of the 14th National Conference on Artificial
Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classification
with probabilistic categorial grammars. In Proceedings of
the 21st Conference on Uncertainty in Artificial Intelligence.
687
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 737?745, Prague, June 2007. c?2007 Association for Computational Linguistics
Chinese Syntactic Reordering for Statistical Machine Translation
Chao Wang
MIT CSAIL
32 Vassar Street, Room 362
Cambridge, MA 02139, USA
wangc@csail.mit.edu
Michael Collins
MIT CSAIL
32 Vassar Street, Room G-484
Cambridge, MA 02139, USA
mcollins@csail.mit.edu
Philipp Koehn
School of Informatics
2 Buccleuch Place, 5BP 2L2
Edinburgh, EH8 9LW, UK
pkoehn@inf.ed.ac.uk
Abstract
Syntactic reordering approaches are an ef-
fective method for handling word-order dif-
ferences between source and target lan-
guages in statistical machine translation
(SMT) systems. This paper introduces a re-
ordering approach for translation from Chi-
nese to English. We describe a set of syntac-
tic reordering rules that exploit systematic
differences between Chinese and English
word order. The resulting system is used
as a preprocessor for both training and test
sentences, transforming Chinese sentences
to be much closer to English in terms of their
word order. We evaluated the reordering
approach within the MOSES phrase-based
SMT system (Koehn et al, 2007). The
reordering approach improved the BLEU
score for the MOSES system from 28.52 to
30.86 on the NIST 2006 evaluation data. We
also conducted a series of experiments to an-
alyze the accuracy and impact of different
types of reordering rules.
1 Introduction
Syntactic reordering approaches are an effective
method for handling systematic differences in word
order between source and target languages within
the context of statistical machine translation (SMT)
systems (Xia and McCord, 2004; Collins et al,
2005). In reordering approaches, sentences in the
source language are first parsed, for example using a
Treebank-trained parser. A series of transformations
is then applied to the resulting parse tree, with the
goal of transforming the source language sentence
into a word order that is closer to that of the target
language. The reordering process is used to prepro-
cess both the training and test data used within an
existing SMT system. Reordering approaches have
given significant improvements in performance for
translation from French to English (Xia and Mc-
Cord, 2004) and from German to English (Collins
et al, 2005).
This paper describes a syntactic reordering ap-
proach for translation from Chinese to English. Fig-
ure 1 gives an example illustrating some of the dif-
ferences in word order between the two languages.
The example shows a Chinese sentence whose literal
translation in English is:
this is French delegation at Winter
Olympics on achieve DEC best accom-
plishment
and where a natural translation would be
this is the best accomplishment that the
French delegation achieved at the Winter
Olympics
As exemplified by this sentence, Chinese differs
from English in several important respects: for ex-
ample, relative clauses appear before the noun being
modified; prepositional phrases often appear before
the head they modify; and so on. It can be seen that
some significant reordering of the input is required
to produce a good English translation. For this ex-
ample, application of reordering rules leads to a new
Chinese string whose word-by-word English para-
phrase is:
737
Before syntactic reordering After syntactic reordering
IP NP PN ?(this)
VP VC(is)
NP CP IP NP NR {I(French)
NN ?L?(delegation)
VP PP P 3(at)
LCP NP NN ?G
(Winter)
NR $?
(Olympics)
LC ?(on)
VP-A VV Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 200?209,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Non-Projective Parsing for Statistical Machine Translation
Xavier Carreras Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{carreras,mcollins}@csail.mit.edu
Abstract
We describe a novel approach for syntax-
based statistical MT, which builds on a
variant of tree adjoining grammar (TAG).
Inspired by work in discriminative depen-
dency parsing, the key idea in our ap-
proach is to allow highly flexible reorder-
ing operations during parsing, in combina-
tion with a discriminative model that can
condition on rich features of the source-
language string. Experiments on trans-
lation from German to English show im-
provements over phrase-based systems,
both in terms of BLEU scores and in hu-
man evaluations.
1 Introduction
Syntax-based models for statistical machine trans-
lation (SMT) have recently shown impressive re-
sults; many such approaches are based on ei-
ther synchronous grammars (e.g., (Chiang, 2005)),
or tree transducers (e.g., (Marcu et al, 2006)).
This paper describes an alternative approach for
syntax-based SMT, which directly leverages meth-
ods from non-projective dependency parsing. The
key idea in our approach is to allow highly flexible
reordering operations, in combination with a dis-
criminative model that can condition on rich fea-
tures of the source-language input string.
Our approach builds on a variant of tree adjoin-
ing grammar (TAG; (Joshi and Schabes, 1997))
(specifically, the formalism of (Carreras et al,
2008)). The models we describe make use of
phrasal entries augmented with subtrees that pro-
vide syntactic information in the target language.
As one example, when translating the sentence
wir mu?ssen auch diese kritik ernst nehmen from
German into English, the following sequence of
syntactic phrasal entries might be used (we show
each English syntactic fragment above its associ-
ated German sub-string):
S
NP
we
VP
must ADVP
also
NP
these criticisms
ADVP
seriously
VP
take
wir mu?ssen auch diese kritik ernst nehmen
TAG parsing operations are then used to combine
these fragments into a full parse tree, giving the
final English translation we must also take these
criticisms seriously.
Some key aspects of our approach are as fol-
lows:
? We impose no constraints on entries in the
phrasal lexicon. The method thereby retains the
full set of lexical entries of phrase-based systems
(e.g., (Koehn et al, 2003)).1
? The model allows a straightforward integra-
tion of lexicalized syntactic language models?for
example the models of (Charniak, 2001)?in addi-
tion to a surface language model.
? The operations used to combine tree frag-
ments into a complete parse tree are signifi-
cant generalizations of standard parsing operations
found in TAG; specifically, they are modified to be
highly flexible, potentially allowing any possible
permutation (reordering) of the initial fragments.
As one example of the type of parsing opera-
tions that we will consider, we might allow the
tree fragments shown above for these criticisms
and take to be combined to form a new structure
with the sub-string take these criticisms. This step
in the derivation is necessary to achieve the correct
English word order, and is novel in a couple of re-
spects: first, these criticisms is initially seen to the
left of take, but after the adjunction this order is
reversed; second, and more unusually, the treelet
for seriously has been skipped over, with the re-
sult that the German words translated at this point
(diese, kritik, and nehmen) form a non-contiguous
sequence. More generally, we will allow any two
1Note that in the above example each English phrase con-
sists of a completely connected syntactic structure; this is not,
however, a required constraint, see section 3.2 for discussion.
200
tree fragments to be combined during the transla-
tion process, irrespective of the reorderings which
are introduced, or the non-projectivity of the pars-
ing operations that are required.
The use of flexible parsing operations raises two
challenges that will be a major focus of this paper.
First, these operations will allow the model to cap-
ture complex reordering phenomena, but will in
addition introduce many spurious possibilities. In-
spired by work in discriminative dependency pars-
ing (e.g., (McDonald et al, 2005)), we add proba-
bilistic constraints to the model through a discrim-
inative model that links lexical dependencies in the
target language to features of the source language
string. We also investigate hard constraints on the
dependency structures that are created during pars-
ing. Second, there is a need to develop efficient
decoding algorithms for the models. We describe
approximate search methods that involve a signif-
icant extension of decoding algorithms originally
developed for phrase-based translation systems.
Experiments on translation from German to En-
glish show a 0.5% improvement in BLEU score
over a phrase-based system. Human evaluations
show that the syntax-based system gives a sig-
nificant improvement over the phrase-based sys-
tem. The discriminative dependency model gives
a 1.5% BLEU point improvement over a basic
model that does not condition on the source lan-
guage string; the hard constraints on dependency
structures give a 0.8% BLEU improvement.
2 Relationship to Previous Work
A number of syntax-based translation systems
have framed translation as a parsing problem,
where search for the most probable translation is
achieved using algorithms that are generalizations
of conventional parsing methods. Early examples
of this work include (Alshawi, 1996; Wu, 1997);
more recent models include (Yamada and Knight,
2001; Eisner, 2003; Melamed, 2004; Zhang and
Gildea, 2005; Chiang, 2005; Quirk et al, 2005;
Marcu et al, 2006; Zollmann and Venugopal,
2006; Nesson et al, 2006; Cherry, 2008; Mi et
al., 2008; Shen et al, 2008). The majority of
these methods make use of synchronous gram-
mars, or tree transducers, which operate over parse
trees in the source and/or target languages. Re-
ordering rules are typically specified through rota-
tions or transductions stated at the level of context-
free rules, or larger fragments, within parse trees.
These rules can be learned automatically from cor-
pora.
A critical difference in our work is to allow
arbitrary reorderings of the source language sen-
tence (as in phrase-based systems), through the
use of flexible parsing operations. Rather than
stating reordering rules at the level of source or
target language parse trees, we capture reorder-
ing phenomena using a discriminative dependency
model. Other factors that distinguish us from pre-
vious work are the use of all phrases proposed by a
phrase-based system, and the use of a dependency
language model that also incorporates constituent
information (although see (Charniak et al, 2003;
Shen et al, 2008) for related approaches).
3 A Syntactic Translation Model
3.1 Background
Our work builds on the variant of tree adjoin-
ing grammar (TAG) introduced by (Carreras et
al., 2008). In this formalism the basic units
in the grammar are spines, which associate tree
fragments with lexical items. These spines can
be combined using a sister-adjunction operation
(Rambow et al, 1995), to form larger pieces of
structure.2 For example, we might have the fol-
lowing operation:
NP
there
S
VP
is
? S
NP
there
VP
is
In this case the spine for there has sister-adjoined
into the S node in the spine for is; we re-
fer to the spine for there as being the modifier
spine, and the spine for is being the head spine.
There are close connections to dependency for-
malisms: in particular in this operation we see
a lexical dependency between the modifier word
there and the head word is. It is possible to de-
fine syntactic language models, similar to (Char-
niak, 2001), which associate probabilities with
these dependencies, roughly speaking of the form
P (w
m
, s
m
|w
h
, s
h
, pos, ?), where w
m
and s
m
are
the identities of the modifier word and spine, w
h
and s
h
are the identities of the head word and
spine, pos is the position in the head spine that is
being adjoined into, and ? is some additional state
(e.g., state that tracks previous modifiers that have
adjoined into the same spine).
2We also make use of the r-adjunction operation defined in
(Carreras et al, 2008), which, together with sister-adjunction,
allows us to model the full range of structures found in the
Penn treebank.
201
SNP
there
VP
is NP
NPB
no hierarchy
PP
of NP
discrimination
es gibt keine hierarchie der diskriminierung
Figure 1: A training example consisting of an English (tar-
get language) tree and a German (source language) sentence.
In this paper we will also consider treelets,
which are a generalization of spines, and which
allow lexical entries that include more than one
word. These treelets can again be combined us-
ing a sister-adjunction operation. As an example,
consider the following operation:
VP
be ADJP
able
SG
to VP
respond
? VP
be ADJP
able SG
to VP
respond
In this case the treelet for to respond sister-adjoins
into the treelet for be able. This operation intro-
duces a bi-lexical dependency between the modi-
fier word to and the head word able.
3.2 S-phrases
This section describes how phrase entries from
phrase-based translation systems can be modified
to include associated English syntactic structures.
These syntactic phrase-entries (from here on re-
ferred to as ?s-phrases?) will form the basis of the
translation models that we describe.
We extract s-phrases from training examples
consisting of a source-language string paired with
a target-language parse tree. For example, con-
sider the training example in figure 1. We as-
sume some method that enumerates a set of pos-
sible phrase entries for each training example:
each phrase entry is a pair ?(i, j), (k, l)? speci-
fying that source-language words f
i
. . . f
j
corre-
spond to target-language words e
k
. . . e
l
in the ex-
ample. For example, one phrase entry for the ex-
ample might be ?(1, 2), (1, 2)?, representing the
pair ?es gibt ? there is?. In our experiments
we use standard methods in phrase-based systems
(Koehn et al, 2003) to define the set of phrase en-
tries for each sentence in training data.
es gibt keine hierarchie der
S
NP
there
VP
is
DT
no
NP
NPB
hierarchy
PP
of
Figure 2: Example syntactic phrase entries. We show Ger-
man sub-strings above their associated sequence of treelets.4
For each phrase entry, we add syntactic infor-
mation to the English string. To continue our ex-
ample, the resulting entry would be as follows:
es gibt ? S
NP
there
VP
is
To give a more formal description of how syn-
tactic structures are derived for phrases, first note
that each parse tree t is mapped to a TAG deriva-
tion using the method described in (Carreras et al,
2008). This procedure uses the head finding rules
of (Collins, 1997). The resulting derivation con-
sists of a TAG spine for each word seen in the sen-
tence, together with a set of adjunction operations
which each involve a modifier spine and a head
spine. Given an English string e = e
1
. . . e
n
, with
an associated parse tree t, the syntactic structure
associated with a substring e
k
. . . e
l
(e.g., there is)
is then defined as follows:
? For each word in the English sub-string, in-
clude its associated TAG spine in t.
? In addition, include any adjunction operations
in t where both the head and modifier word are in
the sub-string e
j
. . . e
k
.
In the above example, the resulting structure
(i.e., the structure for there is) is a single treelet.
In other cases, however, we may get a sequence of
treelets, which are disconnected from each other.
For example, another likely phrase-entry for this
training example is ?es gibt keine ? there is no?
resulting in the first lexical entry in figure 2, which
has two treelets. Allowing s-phrases with multiple
treelets ensures that all phrases used by phrase-
based systems can be used within our approach.
As a final step, we add additional align-
ment information to each s-phrase. Con-
sider an s-phrase which contains source-language
words f
1
. . . f
n
paired with target-language words
e
1
. . . e
m
. The alignment information is a vec-
tor ?(a
1
, b
1
) . . . (a
m
, b
m
)? that specifies for each
word e
i
its alignment to words f
a
i
. . . f
b
i
in the
source language. For example, for the phrase en-
202
try ?es gibt ? there is? a correct alignment would
be ?(1, 1), (2, 2)?, specifying that there is aligned
to es, and is is aligned to gibt (note that in many,
but not all, cases a
i
= b
i
, i.e., a target language
word is aligned to a single source language word).
The alignment information in s-phrases will
be useful in tying syntactic dependencies cre-
ated in the target language to positions in the
source language string. In particular, we will con-
sider discriminative models (analogous to models
for dependency parsing, e.g., see (McDonald et
al., 2005)) that estimate the probability of target-
language dependencies conditioned on properties
of the source-language string. Alignments may be
derived in a number of ways; in our method we
directly use phrase entries proposed by a phrase-
based system. Specifically, for each target word e
i
in a phrase entry ?f
1
. . . f
n
, e
1
. . . e
m
? for a train-
ing example, we find the smallest5 phrase entry
in the same training example that includes e
i
on
the target side, and is a subset of f
1
. . . f
n
on the
source side; the word e
i
is then aligned to the sub-
set of source language words in this ?minimal?
phrase.
In conclusion, s-phrases are defined as follows:
Definition 1 An s-phrase is a 4-tuple ?f, e, t, a?
where: f is a sequence of foreign words; e is
a sequence of English words; t is a sequence of
treelets specifying a TAG spine for each English
word, and potentially some adjunctions between
these spines; and a is an alignment. For an s-
phrase q we will sometimes refer to the 4 elements
of q as f(q), e(q), t(q) and a(q).
3.3 The Model
We now introduce a model that makes use of s-
phrases, and which is flexible in the reorderings
that it allows. To provide some intuition, and some
motivation for the use of reordering operations,
figure 3 gives several examples of German strings
which have different word orders from English.
The crucial idea will be to use TAG adjunction
operations to combine treelets to form a complete
parse tree, but with a complete relaxation on the
order in which the treelets are combined. For ex-
ample, consider again the example given in the
introduction to this paper. In the first step of a
derivation that builds on these treelets, the treelet
5The ?size? of a phrase entry is defined to be n
s
+ n
t
where n
s
is the number of source language words in the
phrase, n
t
is the number of target language words.
1(a) [die verwaltung] [muss] [ku?nftig] [schneller] [reagieren]
[ko?nnen] 1(b) the administration must be able to respond
more quickly in future
1(c) NP
the
admin. . .
S
VP
must
PP
in future
ADVP
more
quickly
SG
to VP
respond
VP
be ADJP
able
2(a) [meiner ansicht nach] [darf] [der erweiterungsprozess]
[nicht] [unno?tig] [verzo?gert] [werden] 2(b) in my opinion the
expansion process should not be delayed unnecessarily
2(c) PP
in my
opinion
S
VP
should
NP
the . . . process
RB
not
ADVP
unnecessarily
VP
delayed
VP
be
Figure 3: Examples of translations. In each example (a)
is the original German string, with a possible segmentation
marked with ?[? and ?]?; (b) is a translation for (a); and (c)
is a sequence of phrase entries, including syntactic structures,
for the segmentation given in (a).
for these criticisms might adjoin into the treelet for
take, giving the following new sequence:
S
NP
we
VP
must ADVP
also
ADVP
seriously
VP
V
take
NP
these criticisms
In the next derivation step seriously is adjoined to
the right of take, giving the following treelets:
S
NP
we
VP
must ADVP
also
VP
V
take
NP
these criticisms
ADVP
seriously
In the final step the second treelet adjoins into the
VP above must, giving a parse tree for the string
we must also take these criticisms seriously, and
completing the translation.
Formally, given an input sentence f , a derivation
d is a pair ?q, pi? where:
? q = q
1
. . . q
n
is a sequence of s-phrases such
that f = f(q
1
)?f(q
2
)? . . .?f(q
n
) (where u?v
denotes the concatenation of strings u and v).
? pi is a set of adjunction operations that
connects the sequence of treelets contained in
?t(q
1
), t(q
2
), . . . , t(q
n
)? into a parse tree in the
target language. The operations allow a com-
plete relaxation of word order, potentially allow-
ing any of the n! possible orderings of the n s-
phrases. We make use of both sister-adjunction
and r-adjunction operations, as defined in (Car-
reras et al, 2008).6
6In principle we allow any treelet to adjoin into any other
treelet?for example there are no hard, grammar-based con-
straints ruling out the combination of certain pairs of non-
terminals. Note however that in some cases operations will
have probability 0 under the syntactic language model intro-
duced later in this section.
203
DT
no
NP
NPB
hierarchy
PP
of
NP
discrimination
? NP
NPB
hierarchy
PP
of
NP
DT
no
discrimination
Figure 4: A spurious derivation step. The treelets arise
from [keine] [hierarchie der] [diskriminierung].
Given a derivation d = ?q, pi?, we define e(d)
to be the target-language string defined by the
derivation, and t(d) to be the complete target-
language parse tree created by the derivation. The
most likely derivation for a foreign sentence f
is argmax
d?G(f)
score(d), where G(f) is the set
of possible derivations for f , and the score for a
derivation is defined as7
score(d) = score
LM
(e(d)) + score
SY N
(t(d))
+ score
R
(d) +
n
?
j=1
score
P
(q
j
) (1)
The components of the model are as follows:
? score
LM
(e(d)) is the log probability of the
English string under a trigram language model.
? score
SY N
(t(d)) is the log probability of the
English parse tree under a syntactic language
model, similar to (Charniak, 2001), that associates
probabilities with lexical dependencies.
? score
R
(d) will be used to score the pars-
ing operations in pi, based on the source-language
string and the alignments in the s-phrases. This
part of the model is described extensively in sec-
tion 4.1 of this paper.
? score
P
(q) is the score for an s-phrase q.
This score is a log-linear combination of var-
ious features, including features that are com-
monly found in phrase-based systems: for exam-
ple logP (f(q)|e(q)), log P (e(q)|f(q)), and lex-
ical translation probabilities. In addition, we in-
clude a feature logP (t(q)|f(q), e(q)), which cap-
tures the probability of the phrase in question hav-
ing the syntactic structure t(q).
Note that a model that includes the terms
score
LM
(e(d)) and
?
n
j=1
score
P
(q
j
) alone
would essentially be a basic phrase-based
model (with no distortion terms). The terms
score
SY N
(t(d)) and score
R
(d) add syntactic
information to this basic model.
A key motivation for this model is the flexibility
of the reordering operations that it allows. How-
ever, the approach raises two major challenges:
7In practice, MERT training (Och, 2003) will be used to
train relative weights for the different model components.
Constraints on reorderings. Relaxing the op-
erations in the parsing model will allow complex
reorderings to be captured, but will also introduce
many spurious possibilities. As one example, con-
sider the derivation step shown in figure 4. This
step may receive a high probability from a syntac-
tic or surface language model?no discrimination
is a quite plausible NP in English?but it should
be ruled out for other reasons, for example be-
cause it does not respect the dependencies in the
original German (i.e., keine/no is not a modifier
to diskriminierung/discrimination in the German
string). The challenge will be to develop either
hard constraints which rule out spurious derivation
steps such as these, or soft constraints, encapsu-
lated in score
R
(d), which penalize them.
Efficient search. Exact search for the derivation
which maximizes the score in Eq. 1 cannot be
accomplished efficiently using dynamic program-
ming (as in phrase-based systems, it is easy to
show that the decoding problem is NP-complete).
Approximate search methods will be needed.
The next two sections of this paper describe so-
lutions to these two challenges.
4 Constraints on Reorderings
4.1 A Discriminative Dependency Model
We now describe the model score
R
introduced in
the previous section. Recall that pi specifies k ad-
junction operations that are used to build a full
parse tree, where k ? n is the number of treelets
within the sequence of s-phrases q = ?q
1
. . . q
n
?.
Each of the k adjunction operations creates a
dependency between a modifier word w
m
within
a phrase q
m
, and a head word w
h
within a phrase
q
h
. For example, in the example in section 3.3
where these criticisms was combined with take,
the modifier word is criticisms and the head word
is take. The modifier and head words have TAG
spines s
m
and s
h
respectively. In addition we can
define (a
m
, b
m
) to be the start and end indices of
the words in the foreign string to which the word
w
m
is aligned; this information can be recovered
because the s-phrase q
m
contains alignment infor-
mation for all target words in the phrase, includ-
ing w
m
. Similarly, we can define (a
h
, b
h
) to be
alignment information for the head word w
h
. Fi-
nally, we can define ? to be a binary flag speci-
fying whether or not the adjunction operation in-
volves reordering (in the take criticism example,
this flag is set to true, because the order in En-
204
VP
DT N
NP
N
criticismsthese take
nehmenernstwir mu?ssen auch diese kritik
Figure 5: An adjunction operation that involves the mod-
ifier criticisms and the head take. The phrases involved are
underlined; the dotted lines show alignments within s-phrases
between English words and positions in the German string.
The ?-dependency in this case includes the head and modi-
fier words, together with their spines, and their alignments to
positions in the German string (kritik and nehmen).
glish is reversed from that in German). This leads
to the following definition:
Definition 2 Given a derivation d = ?q, pi?, we
define ?(d) to be the set of ?-dependencies
in d. Each ?-dependency is a tuple
?w
m
, s
m
, a
m
, b
m
, w
h
, s
h
, a
h
, b
h
, ?? of elements as
described above.
Figure 5 gives an illustration of how an adjunction
creates one such ?-dependency.
The model is then defined as
score
R
(d) =
?
???(d)
score
r
(?, f)
where score
r
(?, f) is a score associated with the
?-dependency ?. This score can potentially be
sensitive to any information in ? or the source-
language string f ; in particular, note that the align-
ment indices (a
m
, b
m
) and (a
h
, b
h
) essentially
anchor the target-language dependency to posi-
tions in the source-language string, allowing the
score for the dependency to be based on features
that have been widely used in discriminative de-
pendency parsing, for example features based on
the proximity of the two positions in the source-
language string, the part-of-speech tags in the sur-
rounding context, and so on. These features have
been shown to be powerful in the context of regu-
lar dependency parsing, and our intent is to lever-
age them in the translation problem.
In our model, we define score
r
as follows. We
estimate a model P (y|?, f) where y ? {?1,+1},
and y = +1 indicates that a dependency does exist
between w
m
and w
h
, and y = ?1 indicates that a
dependency does not exist. We then define
score
r
(?, f) = logP (+1|?, f)
To estimate P (y|?, f), we first extract a set of la-
beled training examples of the form ?y
i
, ?
i
, f
i
? for
i = 1 . . . N from our training data as follows:
for each pair of target-language words (w
m
, w
h
)
seen in the training data, we can extract associ-
ated spines (s
m
, s
h
) from the relevant parse tree,
and also extract a label y indicating whether or not
a head-modifier dependency is seen between the
two words in the parse tree. Given an s-phrase in
the training example that includes w
m
, we can ex-
tract alignment information (a
m
, b
m
) from the s-
phrase; we can extract similar information (a
h
, b
h
)
for w
h
. The end result is a training example of the
form ?y, ?, f?.8 We then estimate P (y|?, f) using
a simple backed-off model that takes into account
the identity of the two spines, the value for the flag
r, the distance between (a
m
, b
m
) and (a
h
, b
h
), and
part-of-speech information in the source language.
4.2 Contiguity of pi-Constituents
We now describe a second type of constraint,
which limits the amount of non-projectivity in
derivations. Consider again the k adjunction op-
erations in pi, which are used to connect treelets
into a full parse tree. Each adjunction operation
involves a head treelet that dominates a modifier
treelet. Thus for any treelet t, we can consider its
descendants, that is, the entire set of treelets that
are directly or indirectly dominated by t. We de-
fine a pi-constituent for treelet t to be the subset
of source-language words dominated by t and its
descendants. We then introduce the following con-
straint on pi-constituents:
Definition 3 (pi-constituent constraint.) A pi-
constituent is contiguous iff it consists of a con-
tiguous sequence of words in the source language.
A derivation pi satisfies the pi-constituent con-
straint iff all pi-constituents that it contains are
contiguous.
In this paper we constrain all derivations to sat-
isfy the pi-constituent constraint (future work may
consider probabilistic versions of the constraint).
The intuition behind the constraint deserves
more discussion. The constraint specifies that the
modifiers to each treelet can appear in any or-
der around the treelet, with arbitrary reorderings
or non-projective operations. However, once a
treelet has taken all its modifiers, the resulting pi-
constituent must form a contiguous sub-sequence
8To be precise, there may be multiple (or even zero) s-
phrases which include w
m
or w
h
, and these s-phrases may
include conflicting alignment information. Given n
m
differ-
ent alignments seen for w
m
, and n
h
different alignments seen
for w
h
, we create n
m
?n
h
training examples, which include
all possible combinations of alignments.
205
of the source-language string. As one set of exam-
ples, consider the translations in figure 3, and the
example given in the introduction. These exam-
ples involve reordering of arguments and adjuncts
within clauses, a very common case of reordering
in translation from German to English. The re-
orderings in these translations are quite flexible,
but in all cases satisfy the pi-constituent constraint.
As an illustration of a derivation that violates
the constraint, consider again the derivation step
shown in figure 4. This step has formed a par-
tial hypothesis, no discrimination, which corre-
sponds to the German words keine and diskrim-
inierung, which do not form a contiguous sub-
string in the German. Consider now a complete
derivation, which derives the string there is hier-
archy of no discrimination, and which includes the
pi-constituent no discrimination shown in the fig-
ure (i.e., where the treelet discrimination takes no
as its only modifier). This derivation will violate
the pi-constituent constraint.9
5 Decoding
We now describe decoding algorithms for the syn-
tactic models: we first describe inference rules
that are used to combine pieces of structure, and
then describe heuristic search algorithms that use
these inference rules. Throughout this section,
for brevity and simplicity, we describe algorithms
that apply under the assumption that each s-phrase
has a single associated treelet. The generalization
to the case where an s-phrase may have multiple
treelets is discussed in section 5.3.
5.1 Inference Rules
Parsing operations for the TAG grammars de-
scribed in (Carreras et al, 2008) are based on
the dynamic programming algorithms in (Eisner,
2000). A critical idea in dynamic programming al-
gorithms such as these is to associate constituents
in a chart with spans of the input sentence, and
to introduce inference rules that combine con-
stituents into larger pieces of structure. The crucial
step in generalizing these algorithms to the non-
projective case, and to translation, will be to make
use of bit-strings that keep track of which words in
the German have already been translated in a chart
entry. To return to the example from the intro-
duction, again assume that the selected s-phrases
9Note, however, that the derivation step show in figure 4
will be considered in the search, because if discrimination
takes additional modifiers, and thereby forms a pi-constituent
that dominates a contiguous sub-string in the German, then
the resulting derivation will be valid.
0. Data structures: Q
i
for i = 1 . . . n is a set of hypotheses
for each length i, S is a set of chart entries
1. S ? ?
2. Initialize Q
1
. . .Q
n
with basic chart entries derived
from phrase entries
3. For i = 1 . . . n
4. For any A ? BEAM(Q
i
)
5. If S contains a chart entry with the same signature
as A, and which has a higher inside score,
6. continue
7. Else
8. Add A to S
9. For any chart entry C that can be derived from
A together with another chart entry B ? S ,
add C to the set Q
j
where j = length(C)
10. ReturnQ
n
, a set of items of length n
Figure 6: A beam search algorithm. A dynamic-
programming signature consists of the regular dynamic-
programming state for the parsing algorithm, together with
the span (bit-string) associated with a constituent.
segment the German input into [wir mu?ssen auch]
[diese kritik] [ernst] [nehmen], and the treelets are
as shown in the introduction. Each of these treelets
will form a basic entry in the chart, and will have
an associated bit-string indicating which German
words have been translated by that entry.
These basic chart entries can then be combined
to form larger pieces of structure. For example,
the following inferential step is possible:
NP/0001100
these criticisms
VP/0000001
V
take
? VP/0001101
V
take
NP
these criticisms
We have shown the bit-string representation for
each consituent: for example, the new constituent
has the bit-string 0001101 representing the fact
that the non-contiguous sub-strings diese kritik
and nehmen have been translated at this point. Any
two constituents can be combined, providing that
the logical AND of their bit-strings is all 0?s.
Inference steps such as that shown above will
have an associated score corresponding to the
TAG adjunction that is involved: in our mod-
els, both score
SY N
and score
R
will contribute to
this score. In addition, we add state?specifically,
word bigrams at the start and end of constituents?
that allows trigram language model scores to be
calculated as constituents are combined.
5.2 Approximate Search
There are 2n possible bit-strings for a sentence of
length n, hence the search space is of exponen-
tial size; approximate algorithms are therefore re-
quired in search for the highest scoring derivation.
Figure 6 shows a beam search algorithm which
makes use of the inference rules described in the
206
previous section. The algorithm stores sets Q
i
for i = 1 . . . n, where n is the source-language
sentence length; each set Q
i
stores hypotheses of
length i (i.e., hypotheses with an associated bit-
string with i ones). These sets are initialized with
basic entries derived from s-phrases.
The function BEAM(Q
i
) returns all items
within Q
i
that have a high enough score to fall
within a beam (more details for BEAM are given
below). At each iteration (step 4), each item in
turn is taken from BEAM(Q
i
) and added to a
chart; the inference rules described in the previ-
ous section are used to derive new items which are
added to the appropriate set Q
j
, where j > i.
We have found the definition of BEAM(Q
i
) to
be critical to the success of the method. As a first
step, each item in Q
i
receives a score that is a sum
of an inside score (the cost of all derivation steps
used to create the item) and a future score (an esti-
mate of the cost to complete the translation). The
future score is based on the source-language words
that are still to be translated?this can be directly
inferred from the item?s bit-string?this is similar
to the use of future scores in Pharoah (Koehn et al,
2003), and in fact we use Pharoah?s future scores
in our model. We then give the following defini-
tion, where N is a parameter (the beam size):
Definition 4 (BEAM) Given Q
i
, define Q
i,j
for
j = 1 . . . n to be the subset of items in Q
i
which
have their j?th bit equal to one (i.e., have the j?th
source language word translated). Define Q?
i,j
to
be the N highest scoring elements in Q
i,j
. Then
BEAM(Q
i
) = ?
n
j=1
Q
?
i,j
.
To motivate this definition, note that a naive
method would simply define BEAM(Q
i
) to be
the N highest scoring elements of Q
i
. This def-
inition, however, assumes that constituents which
form translations of different parts of a sentence
have scores that can be compared?an assumption
that would be true if the future scores were highly
accurate, but which quickly breaks down when fu-
ture scores are inaccurate. In contrast, the defi-
nition above ensures that the top N analyses for
each of the n source language words are stored at
each stage, and hence that all parts of the source
sentence are well represented. In experiments, the
naive approach was essentially a failure, with pars-
ing of some sentences either failing or being hope-
lessly inefficient, depending on the choice of N .
In contrast, definition 4 gives good results.
System BLEU score
Syntax-based 25.2
Syntax (no Score
R
) 23.7 (-1.5)
Syntax (no pi-c constraint) 24.4 (-0.8)
Table 1: Development set results showing the effect of re-
moving Score
R
or the pi-constituent constraint.
5.3 Allowing Multiple Treelets per s-Phrase
The decoding algorithms that we have described
apply in the case where each s-phrase has a sin-
gle treelet. The extension of these algorithms
to the case where a phrase may have multiple
treelets (e.g., see figure 2) is straightforward, but
for brevity the details are omitted. The basic idea
is to extend bit-string representations with a record
of ?pending? treelets which have not yet been in-
cluded in a derivation. It is also possible to enforce
the pi-constituent constraint during decoding, as
well as a constraint that ensures that reordering op-
erations do not ?break apart? English sub-strings
within s-phrases that have multiple treelets (for ex-
ample, for the s-phrase in figure 2, we ensure that
there is no remains as a contiguous sequence of
words in any translation using this s-phrase).
6 Experiments
We trained the syntax-based system on 751,088
German-English translations from the Europarl
corpus (Koehn, 2005). A syntactic language
model was also trained on the English sentences
in the training data. We used Pharoah (Koehn et
al., 2003) as a baseline system for comparison; the
s-phrases used in our system include all phrases,
with the same scores, as those used by Pharoah,
allowing a direct comparison. For efficiency rea-
sons we report results on sentences of length 30
words or less.10 The syntax-based method gives
a BLEU (Papineni et al, 2002) score of 25.04,
a 0.46 BLEU point gain over Pharoah. This re-
sult was found to be significant (p = 0.021) under
the paired bootstrap resampling method of Koehn
(2004), and is close to significant (p = 0.058) un-
der the sign test of Collins et al (2005).
Table 1 shows results for the full syntax-based
system, and also results for the system with the
discriminative dependency scores (see section 4.1)
and the pi-contituent constraint removed from the
system. In both cases we see a clear impact of
these components of the model, with 1.5 and 0.8
BLEU point decrements respectively.
10Both Pharoah and our system have weights trained using
MERT (Och, 2003) on sentences of length 30 words or less,
to ensure that training and test conditions are matched.
207
R: in our eyes , the opportunity created by this directive of introducing longer buses on international routes is efficient .
S: the opportunity now presented by this directive is effective in our opinion , to use long buses on international routes .
P: the need for this directive now possibility of longer buses on international routes to is in our opinion , efficiently .
R: europe and asia must work together to intensify the battle against drug trafficking , money laundering , international
crime , terrorism and the sexual exploitation of minors .
S: europe and asia must work together in order to strengthen the fight against drug trafficking , money laundering , against
international crime , terrorism and the sexual exploitation of minors .
P: europe and asia must cooperate in the fight against drug trafficking , money laundering , against international crime ,
terrorism and the sexual exploitation of minors strengthened .
R: equally important for the future of europe - at biarritz and later at nice - will be the debate on the charter of fundamental
rights .
S: it is equally important for the future of europe to speak on the charter of fundamental rights in biarritz , and then in nice .
P: just as important for the future of europe , it will be in biarritz and then in nice on the charter of fundamental rights to
speak .
R: the convention was thus a muddled system , generating irresponsibility , and not particularly favourable to well-ordered
democracy .
S: therefore , the convention has led to a system of a promoter of irresponsibility of the lack of clarity and hardly coincided
with the rules of a proper democracy .
P: the convention therefore led to a system of full of lack of clarity and hardly a promoter of the irresponsibility of the rules
of orderly was a democracy .
Figure 7: Examples where both annotators judged the syntactic system to give an improved translation when compared to
the baseline system. 51 out of 200 translations fall into this category. These examples were chosen at random from these 51
examples. R is the human (reference) translation; S is the translation from the syntax-based system; P is the output from the
baseline (phrase-based) system.
Syntax PB = Total
Syntax 51 3 7 61
PB 1 25 11 37
= 21 14 67 102
Total 73 42 85 200
Table 2: Human annotator judgements. Rows show re-
sults for annotator 1, and columns for annotator 2. Syntax
and PB show the number of cases where an annotator re-
spectively preferred/dispreferred the syntax-based system. =
gives counts of translations judged to be equal in quality.
In addition, we obtained human evaluations on
200 sentences chosen at random from the test data,
using two annotators. For each example, the ref-
erence translation was presented to the annota-
tor, followed by translations from the syntax-based
and phrase-based systems (in a random order). For
each example, each annotator could either decide
that the two translations were of equal quality, or
that one translation was better than the other. Ta-
ble 2 shows results of this evaluation. Both an-
notators show a clear preference for the syntax-
based system: for annotator 1, 73 translations are
judged to be better for the syntax-based system,
with 42 translations being worse; for annotator 2,
61 translations are improved with 37 being worse;
both annotators? results are statistically significant
with p < 0.05 under the sign test. Figure 7 shows
some translation examples where the syntax-based
system was judged to give an improvement.
7 Conclusions and Future Work
We have described a translation model that makes
use of flexible parsing operations, critical ideas
being the definition of s-phrases, ?-dependencies,
the pi-constituent constraint, and an approximate
search algorithm. A key area for future work
will be further development of the discriminative
dependency model (section 4.1). The model of
score
r
(?, f) that we have described in this paper is
relatively simple; in general, however, there is the
potential for score
r
to link target language depen-
dencies to arbitrary properties of the source lan-
guage string f (recall that ? contains a head and
modifier spine in the target language, along with
positions in the source-language string to which
these spines are aligned). For example, we might
introduce features that: a) condition dependencies
created in the target language on dependency re-
lations between their aligned words in the source
language; b) condition target-language dependen-
cies on whether they are aligned to words that
are in the same clause or segment in the source
language string; or, c) condition the grammatical
roles of nouns in the target language on grammat-
ical roles of aligned words in the source language.
These features should improve translation qual-
ity by giving a tighter link between syntax in the
source and target languages, and would be easily
incorporated in the approach we have described.
Acknowledgments We would like to thank Ryan Mc-
Donald for conversations that were influential in this work,
and Meg Aycinena Lippow and Ben Snyder for translation
judgments. This work was supported under the GALE pro-
gram of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022.
208
References
H. Alshawi. 1996. Head automata and bilingual tiling:
Translation with minimal representations. In Pro-
ceedings of ACL, pages 167?176.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming and the perceptron for efficient,
feature-rich parsing. In Proc. of CoNLL.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for machine transla-
tion. In Proceedings of MT Summit IX.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In Proceedings of ACL 2001.
C. Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proceedings of
ACL-08: HLT, pages 72?80, Columbus, Ohio, June.
Association for Computational Linguistics.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of ACL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics, pages 16?23, Madrid, Spain,
July. Association for Computational Linguistics.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In H. C. Bunt and A. Ni-
jholt, editors, New Developments in Natural Lan-
guage Parsing, pages 29?62. Kluwer Academic
Publishers.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of
ACL.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and K. Salomaa, ed-
itors, Handbook of Formal Languages, volume 3,
pages 169?124. Springer.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
HLT/NAACL.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Proceedings of MT
Summit.
D. Marcu, W. Wang, A. Echihabi, and K. Knight. 2006.
Spmt: Statistical machine translation with syntac-
tified target language phrases. In Proceedings of
EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
D. Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based
translation. In Proceedings of ACL-08: HLT, pages
192?199. Association for Computational Linguis-
tics.
R. Nesson, S.M. Shieber, and A. Rush. 2006. In-
duction of probabilistic synchronous tree-insertion
grammars for machine translation. In Proceedings
of the 7th AMTA.
F.J. Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings of ACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318.
Association for Computational Linguistics.
C. Quirk, A. Menezes, and Colin Cherry. 2005. De-
pendency tree translation: Syntactically informed
phrasal smt. In Proceedings of ACL.
O. Rambow, K. Vijay-Shanker, and D. Weir. 1995.
D-tree grammars. In Proceedings of the 33rd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 151?158, Cambridge, Mas-
sachusetts, USA, June. Association for Computa-
tional Linguistics.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of ACL.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized
inversion transduction grammar for alignment. In
Proceedings of ACL, pages 473?482.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In
Proceedings of NAACL 2006 Workshop on Statisti-
cal Machine Translation.
209
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 551?560,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
An Empirical Study of Semi-supervised Structured Conditional Models
for Dependency Parsing
Jun Suzuki, Hideki Isozaki
NTT CS Lab., NTT Corp.
Kyoto, 619-0237, Japan
jun@cslab.kecl.ntt.co.jp
isozaki@cslab.kecl.ntt.co.jp
Xavier Carreras, and Michael Collins
MIT CSAIL
Cambridge, MA 02139, USA
carreras@csail.mit.edu
mcollins@csail.mit.edu
Abstract
This paper describes an empirical study
of high-performance dependency parsers
based on a semi-supervised learning ap-
proach. We describe an extension of semi-
supervised structured conditional models
(SS-SCMs) to the dependency parsing
problem, whose framework is originally
proposed in (Suzuki and Isozaki, 2008).
Moreover, we introduce two extensions re-
lated to dependency parsing: The first ex-
tension is to combine SS-SCMs with an-
other semi-supervised approach, described
in (Koo et al, 2008). The second exten-
sion is to apply the approach to second-
order parsing models, such as those de-
scribed in (Carreras, 2007), using a two-
stage semi-supervised learning approach.
We demonstrate the effectiveness of our
proposed methods on dependency parsing
experiments using two widely used test
collections: the Penn Treebank for En-
glish, and the Prague Dependency Tree-
bank for Czech. Our best results on
test data in the above datasets achieve
93.79% parent-prediction accuracy for En-
glish, and 88.05% for Czech.
1 Introduction
Recent work has successfully developed depen-
dency parsing models for many languages us-
ing supervised learning algorithms (Buchholz and
Marsi, 2006; Nivre et al, 2007). Semi-supervised
learning methods, which make use of unlabeled
data in addition to labeled examples, have the po-
tential to give improved performance over purely
supervised methods for dependency parsing. It
is often straightforward to obtain large amounts
of unlabeled data, making semi-supervised ap-
proaches appealing; previous work on semi-
supervised methods for dependency parsing in-
cludes (Smith and Eisner, 2007; Koo et al, 2008;
Wang et al, 2008).
In particular, Koo et al (2008) describe a
semi-supervised approach that makes use of clus-
ter features induced from unlabeled data, and gives
state-of-the-art results on the widely used depen-
dency parsing test collections: the Penn Tree-
bank (PTB) for English and the Prague Depen-
dency Treebank (PDT) for Czech. This is a very
simple approach, but provided significant perfor-
mance improvements comparing with the state-
of-the-art supervised dependency parsers such as
(McDonald and Pereira, 2006).
This paper introduces an alternative method for
semi-supervised learning for dependency parsing.
Our approach basically follows a framework pro-
posed in (Suzuki and Isozaki, 2008). We extend it
for dependency parsing, which we will refer to as
a Semi-supervised Structured Conditional Model
(SS-SCM). In this framework, a structured condi-
tional model is constructed by incorporating a se-
ries of generative models, whose parameters are
estimated from unlabeled data. This paper de-
scribes a basic method for learning within this ap-
proach, and in addition describes two extensions.
The first extension is to combine our method with
the cluster-based semi-supervised method of (Koo
et al, 2008). The second extension is to apply the
approach to second-order parsing models, more
specifically the model of (Carreras, 2007), using
a two-stage semi-supervised learning approach.
We conduct experiments on dependency parsing
of English (on Penn Treebank data) and Czech (on
the Prague Dependency Treebank). Our experi-
ments investigate the effectiveness of: 1) the basic
SS-SCM for dependency parsing; 2) a combina-
tion of the SS-SCM with Koo et al (2008)?s semi-
supervised approach (even in the case we used the
same unlabeled data for both methods); 3) the two-
stage semi-supervised learning approach that in-
551
corporates a second-order parsing model. In ad-
dition, we evaluate the SS-SCM for English de-
pendency parsing with large amounts (up to 3.72
billion tokens) of unlabeled data .
2 Semi-supervised Structured
Conditional Models for Dependency
Parsing
Suzuki et al (2008) describe a semi-supervised
learning method for conditional random fields
(CRFs) (Lafferty et al, 2001). In this paper we
extend this method to the dependency parsing
problem. We will refer to this extended method
as Semi-supervised Structured Conditional Mod-
els (SS-SCMs). The remainder of this section de-
scribes our approach.
2.1 The Basic Model
Throughout this paper we will use x to denote an
input sentence, and y to denote a labeled depen-
dency structure. Given a sentence x with n words,
a labeled dependency structure y is a set of n de-
pendencies of the form (h,m, l), where h is the
index of the head-word in the dependency, m is
the index of the modifier word, and l is the label
of the dependency. We use h = 0 for the root of
the sentence. We assume access to a set of labeled
training examples, {x
i
,y
i
}
N
i=1
, and in addition a
set of unlabeled examples, {x
?
i
}
M
i=1
.
In conditional log-linear models for dependency
parsing (which are closely related to conditional
random fields (Lafferty et al, 2001)), a distribu-
tion over dependency structures for a sentence x
is defined as follows:
p(y|x) =
1
Z(x)
exp{g(x,y)}, (1)
where Z(x) is the partition function, w is a pa-
rameter vector, and
g(x,y) =
?
(h,m,l)?y
w ? f(x, h,m, l)
Here f(x, h,m, l) is a feature vector represent-
ing the dependency (h,m, l) in the context of the
sentence x (see for example (McDonald et al,
2005a)).
In this paper we extend the definition of g(x,y)
to include features that are induced from unlabeled
data. Specifically, we define
g(x,y) =
?
(h,m,l)?y
w ? f(x, h,m, l)
+
?
(h,m,l)?y
k
?
j=1
v
j
q
j
(x, h,m, l). (2)
In this model v
1
, . . . , v
k
are scalar parameters that
may be positive or negative; q
1
. . . q
k
are func-
tions (in fact, generative models), that are trained
on unlabeled data. The v
j
parameters will dictate
the relative strengths of the functions q
1
. . . q
k
, and
will be trained on labeled data.
For convenience, we will use v to refer to the
vector of parameters v
1
. . . v
k
, and q to refer to the
set of generative models q
1
. . . q
k
. The full model
is specified by values for w,v, and q. We will
write p(y|x;w,v,q) to refer to the conditional
distribution under parameter values w,v,q.
We will describe a three-step parameter estima-
tion method that: 1) initializes the q functions
(generative models) to be uniform distributions,
and estimates parameter values w and v from la-
beled data; 2) induces new functions q
1
. . . q
k
from
unlabeled data, based on the distribution defined
by the w,v,q values from step (1); 3) re-estimates
w and v on the labeled examples, keeping the
q
1
. . . q
k
from step (2) fixed. The end result is a
model that combines supervised training with gen-
erative models induced from unlabeled data.
2.2 The Generative Models
We now describe how the generative models
q
1
. . . q
k
are defined, and how they are induced
from unlabeled data. These models make direct
use of the feature-vector definition f(x,y) used in
the original, fully supervised, dependency parser.
The first step is to partition the d fea-
tures in f(x,y) into k separate feature vectors,
r
1
(x,y) . . . r
k
(x,y) (with the result that f is the
concatenation of the k feature vectors r
1
. . . r
k
). In
our experiments on dependency parsing, we parti-
tioned f into up to over 140 separate feature vec-
tors corresponding to different feature types. For
example, one feature vector r
j
might include only
those features corresponding to word bigrams in-
volved in dependencies (i.e., indicator functions
tied to the word bigram (x
m
, x
h
) involved in a de-
pendency (x, h,m, l)).
We then define a generative model that assigns
a probability
q
?
j
(x, h,m, l) =
d
j
?
a=1
?
r
j,a
(x,h,m,l)
j,a
(3)
to the d
j
-dimensional feature vector r
j
(x, h,m, l).
The parameters of this model are ?
j,1
. . . ?
j,d
j
;
552
they form a multinomial distribution, with the con-
straints that ?
j,a
? 0, and
?
a
?
j,a
= 1. This
model can be viewed as a very simple (naive-
Bayes) model that defines a distribution over fea-
ture vectors r
j
? R
d
j
. The next section describes
how the parameters ?
j,a
are trained on unlabeled
data.
Given parameters ?
j,a
, we can simply define the
functions q
1
. . . q
k
to be log probabilities under the
generative model:
q
j
(x, h,m, l) = log q
?
j
(x, h,m, l)
=
d
j
?
a=1
r
j,a
(x, h,m, l) log ?
j,a
.
We modify this definition slightly, be introducing
scaling factors c
j,a
> 0, and defining
q
j
(x, h,m, l) =
d
j
?
a=1
r
j,a
(x, h,m, l) log
?
j,a
c
j,a
(4)
In our experiments, c
j,a
is simply a count of the
number of times the feature indexed by (j, a) ap-
pears in unlabeled data. Thus more frequent fea-
tures have their contribution down-weighted in the
model. We have found this modification to be ben-
eficial.
2.3 Estimating the Parameters of the
Generative Models
We now describe the method for estimating the
parameters ?
j,a
of the generative models. We
assume initial parameters w,v,q, which define
a distribution p(y|x
?
i
;w,v,q) over dependency
structures for each unlabeled example x
?
i
. We will
re-estimate the generative models q, based on un-
labeled examples. The likelihood function on un-
labeled data is defined as
M
?
i=1
?
y
p(y|x
?
i
;w,v,q)
?
(h,m,l)?y
log q
?
j
(x
?
i
, h,m, l),
(5)
where q
?
j
is as defined in Eq. 3. This function re-
sembles the Q function used in the EM algorithm,
where the hidden labels (in our case, dependency
structures), are filled in using the conditional dis-
tribution p(y|x
?
i
;w,v,q).
It is simple to show that the estimates ?
j,a
that
maximize the function in Eq. 5 can be defined as
follows. First, define a vector of expected counts
based on w,v,q as
?
r
j
=
M
?
i=1
?
y
p(y|x
?
i
;w,v,q)
?
(h,m,l)?y
r
j
(x
?
i
, h,m, l).
Note that it is straightforward to calculate these ex-
pected counts using a variant of the inside-outside
algorithm (Baker, 1979) applied to the (Eisner,
1996) dependency-parsing data structures (Paskin,
2001) for projective dependency structures, or the
matrix-tree theorem (Koo et al, 2007; Smith and
Smith, 2007; McDonald and Satta, 2007) for non-
projective dependency structures.
The estimates that maximize Eq. 5 are then
?
j,a
=
r?
j,a
?
d
j
a=1
r?
j,a
.
In a slight modification, we employ the follow-
ing estimates in our model, where ? > 1 is a pa-
rameter of the model:
?
j,a
=
(? ? 1) + r?
j,a
d
j
? (? ? 1) +
?
d
j
a=1
r?
j,a
. (6)
This corresponds to a MAP estimate under a
Dirichlet prior over the ?
j,a
parameters.
2.4 The Complete Parameter-Estimation
Method
This section describes the full parameter estima-
tion method. The input to the algorithm is a set
of labeled examples {x
i
,y
i
}
N
i=1
, a set of unla-
beled examples {x
?
i
}
M
i=1
, a feature-vector defini-
tion f(x,y), and a partition of f into k feature vec-
tors r
1
. . . r
k
which underly the generative mod-
els. The output from the algorithm is a parameter
vector w, a set of generative models q
1
. . . q
k
, and
parameters v
1
. . . v
k
, which define a probabilistic
dependency parsing model through Eqs. 1 and 2.
The learning algorithm proceeds in three steps:
Step 1: Estimation of a Fully Supervised
Model. We choose the initial value q
0
of the
generative models to be the uniform distribution,
i.e., we set ?
j,a
= 1/d
j
for all j, a. We then de-
fine the regularized log-likelihood function for the
labeled examples, with the generative model fixed
at q
0
, to be:
L(w,v;q
0
) =
n
?
i=1
log p(y
i
|x
i
;w,v,q
0
)
?
C
2
(
||w||
2
+ ||v||
2
)
553
This is a conventional regularized log-likelihood
function, as commonly used in CRF models. The
parameter C > 0 dictates the level of regular-
ization in the model. We define the initial pa-
rameters (w
0
,v
0
) = argmax
w,v
L(w,v;q
0
).
These parameters can be found using conventional
methods for estimating the parameters of regu-
larized log-likelihood functions (in our case we
use LBFGS (Liu and Nocedal, 1989)). Note that
the gradient of the log-likelihood function can be
calculated using the inside-outside algorithm ap-
plied to projective dependency parse structures, or
the matrix-tree theorem applied to non-projective
structures.
Step 2: Estimation of the Generative Mod-
els. In this step, expected count vectors
?
r
1
. . .
?
r
k
are first calculated, based on the distribution
p(y|x;w
0
,v
0
,q
0
). Generative model parameters
?
j,a
are calculated through the definition in Eq. 6;
these estimates define updated generative models
q
1
j
for j = 1 . . . k through Eq. 4. We refer to the
new values for the generative models as q
1
.
Step 3: Re-estimation of w and v. In
the final step, w
1
and v
1
are estimated as
argmax
w,v
L(w,v;q
1
) where L(w,v;q
1
) is de-
fined in an analogous way to L(w,v;q
0
). Thus w
and v are re-estimated to optimize log-likelihood
of the labeled examples, with the generative mod-
els q
1
estimated in step 2.
The final output from the algorithm is the set of
parameters (w
1
,v
1
,q
1
). Note that it is possible to
iterate the method?steps 2 and 3 can be repeated
multiple times (Suzuki and Isozaki, 2008)?but
in our experiments we only performed these steps
once.
3 Extensions
3.1 Incorporating Cluster-Based Features
Koo et al (2008) describe a semi-supervised
approach that incorporates cluster-based features,
and that gives competitive results on dependency
parsing benchmarks. The method is a two-stage
approach. First, hierarchical word clusters are de-
rived from unlabeled data using the Brown et al
clustering algorithm (Brown et al, 1992). Sec-
ond, a new feature set is constructed by represent-
ing words by bit-strings of various lengths, corre-
sponding to clusters at different levels of the hier-
archy. These features are combined with conven-
tional features based on words and part-of-speech
tags. The new feature set is then used within a
conventional discriminative, supervised approach,
such as the averaged perceptron algorithm.
The important point is that their approach uses
unlabeled data only for the construction of a new
feature set, and never affects to learning algo-
rithms. It is straightforward to incorporate cluster-
based features within the SS-SCM approach de-
scribed in this paper. We simply use the cluster-
based feature-vector representation f(x,y) intro-
duced by (Koo et al, 2008) as the basis of our ap-
proach.
3.2 Second-order Parsing Models
Previous work (McDonald and Pereira, 2006; Car-
reras, 2007) has shown that second-order parsing
models, which include information from ?sibling?
or ?grandparent? relationships between dependen-
cies, can give significant improvements in accu-
racy over first-order parsing models. In principle
it would be straightforward to extend the SS-SCM
approach that we have described to second-order
parsing models. In practice, however, a bottle-
neck for the method would be the estimation of
the generative models on unlabeled data. This
step requires calculation of marginals on unlabeled
data. Second-order parsing models generally re-
quire more costly inference methods for the cal-
culation of marginals, and this increased cost may
be prohibitive when large quantities of unlabeled
data are employed.
We instead make use of a simple ?two-stage? ap-
proach for extending the SS-SCM approach to the
second-order parsing model of (Carreras, 2007).
In the first stage, we use a first-order parsing
model to estimate generative models q
1
. . . q
k
from
unlabeled data. In the second stage, we incorpo-
rate these generative models as features within a
second-order parsing model. More precisely, in
our approach, we first train a first-order parsing
model by Step 1 and 2, exactly as described in
Section 2.4, to estimate w
0
, v
0
and q
1
. Then,
we substitute Step 3 as a supervised learning such
as MIRA with a second-order parsing model (Mc-
Donald et al, 2005a), which incorporates q
1
as a
real-values features. We refer this two-stage ap-
proach to as two-stage SS-SCM.
In our experiments we use the 1-best MIRA
algorithm (McDonald and Pereira, 2006)
1
as a
1
We used a slightly modified version of 1-best MIRA,
whose difference can be found in the third line in Eq. 7,
namely, including L(y
i
,y).
554
(a) English dependency parsing
Data set (WSJ Sec. IDs) # of sentences # of tokens
Training (02?21) 39,832 950,028
Development (22) 1,700 40,117
Test (23) 2,012 47,377
Unlabeled 1,796,379 43,380,315
(b) Czech dependency parsing
Data set # of sentences # of tokens
Training 73,088 1,255,590
Development 7,507 126,030
Test 7,319 125,713
Unlabeled 2,349,224 39,336,570
Table 1: Details of training, development, test data
(labeled data sets) and unlabeled data used in our
experiments
parameter-estimation method for the second-order
parsing model. In particular, we perform the fol-
lowing optimizations on each update t = 1, ..., T
for re-estimating w and v:
min ||w
(t+1)
?w
(t)
||+ ||v
(t+1)
? v
(t)
||
s.t. S(x
i
,y
i
)? S(x
i
,
?
y) ? L(y
i
,
?
y)
?
y = argmax
y
S(x
i
,y) + L(y
i
,y),
(7)
where L(y
i
,y) represents the loss between correct
output of i?th sample y
i
and y. Then, the scoring
function S for each y can be defined as follows:
S(x,y) =w ? (f
1
(x,y) + f
2
(x,y))
+B
k
?
j=1
v
j
q
j
(x,y),
(8)
where B represents a tunable scaling factor, and
f
1
and f
2
represent the feature vectors of first and
second-order parsing parts, respectively.
4 Experiments
We now describe experiments investigating the ef-
fectiveness of the SS-SCM approach for depen-
dency parsing. The experiments test basic, first-
order parsing models, as well as the extensions
to cluster-based features and second-order parsing
models described in the previous section.
4.1 Data Sets
We conducted experiments on both English and
Czech data. We used the Wall Street Journal
sections of the Penn Treebank (PTB) III (Mar-
cus et al, 1994) as a source of labeled data for
English, and the Prague Dependency Treebank
(PDT) 1.0 (Haji?c, 1998) for Czech. To facili-
tate comparisons with previous work, we used ex-
actly the same training, development and test sets
Corpus article name (mm/yy) # of sent. # of tokens
BLLIP wsj 00/87?00/89 1,796,379 43,380,315
Tipster wsj 04/90?03/92 1,550,026 36,583,547
North wsj 07/94?12/96 2,748,803 62,937,557
American reu 04/94?07/96 4,773,701 110,001,109
Reuters reu 09/96?08/97 12,969,056 214,708,766
English afp 05/94?12/06 21,231,470 513,139,928
Gigaword apw 11/94?12/06 46,978,725 960,733,303
ltw 04/94?12/06 10,524,545 230,370,454
nyt 07/94?12/06 60,752,363 1,266,531,274
xin 01/95?12/06 12,624,835 283,579,330
total 175,949,903 3,721,965,583
Table 2: Details of the larger unlabeled data set
used in English dependency parsing: sentences ex-
ceeding 128 tokens in length were excluded for
computational reasons.
as those described in (McDonald et al, 2005a;
McDonald et al, 2005b; McDonald and Pereira,
2006; Koo et al, 2008). The English dependency-
parsing data sets were constructed using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to dependency tree repre-
sentations. We split the data into three parts:
sections 02-21 for training, section 22 for de-
velopment and section 23 for test. The Czech
data sets were obtained from the predefined train-
ing/development/test partition in the PDT. The un-
labeled data for English was derived from the
Brown Laboratory for Linguistic Information Pro-
cessing (BLLIP) Corpus (LDC2000T43)
2
, giving
a total of 1,796,379 sentences and 43,380,315
tokens. The raw text section of the PDT was
used for Czech, giving 2,349,224 sentences and
39,336,570 tokens. These data sets are identical
to the unlabeled data used in (Koo et al, 2008),
and are disjoint from the training, development
and test sets. The datasets used in our experiments
are summarized in Table 1.
In addition, we will describe experiments that
make use of much larger amounts of unlabeled
data. Unfortunately, we have no data available
other than PDT for Czech, this is done only for
English dependency parsing. Table 2 shows the
detail of the larger unlabeled data set used in our
experiments, where we eliminated sentences that
have more than 128 tokens for computational rea-
sons. Note that the total size of the unlabeled data
reaches 3.72G (billion) tokens, which is approxi-
2
We ensured that the sentences used in the PTB were
excluded from the unlabeled data, since sentences used in
BLLIP corpus are a super-set of the PTB.
555
mately 4,000 times larger than the size of labeled
training data.
4.2 Features
4.2.1 Baseline Features
In general we will assume that the input sentences
include both words and part-of-speech (POS) tags.
Our baseline features (?baseline?) are very simi-
lar to those described in (McDonald et al, 2005a;
Koo et al, 2008): these features track word and
POS bigrams, contextual features surrounding de-
pendencies, distance features, and so on. En-
glish POS tags were assigned by MXPOST (Rat-
naparkhi, 1996), which was trained on the train-
ing data described in Section 4.1. Czech POS tags
were obtained by the following two steps: First,
we used ?feature-based tagger? included with the
PDT
3
, and then, we used the method described in
(Collins et al, 1999) to convert the assigned rich
POS tags into simplified POS tags.
4.2.2 Cluster-based Features
In a second set of experiments, we make use of the
feature set used in the semi-supervised approach
of (Koo et al, 2008). We will refer to this as the
?cluster-based feature set? (CL). The BLLIP (43M
tokens) and PDT (39M tokens) unlabeled data sets
shown in Table 1 were used to construct the hierar-
chical clusterings used within the approach. Note
that when this feature set is used within the SS-
SCM approach, the same set of unlabeled data is
used to both induce the clusters, and to estimate
the generative models within the SS-SCM model.
4.2.3 Constructing the Generative Models
As described in section 2.2, the generative mod-
els in the SS-SCM approach are defined through
a partition of the original feature vector f(x,y)
into k feature vectors r
1
(x,y) . . . r
k
(x,y). We
follow a similar approach to that of (Suzuki and
Isozaki, 2008) in partitioning f(x,y), where the
k different feature vectors correspond to different
feature types or feature templates. Note that, in
general, we are not necessary to do as above, this
is one systematic way of a feature design for this
approach.
4.3 Other Experimental Settings
All results presented in our experiments are given
in terms of parent-prediction accuracy on unla-
3
Training, development, and test data in PDT already con-
tains POS tags assigned by the ?feature-based tagger?.
beled dependency parsing. We ignore the parent-
predictions of punctuation tokens for English,
while we retain all the punctuation tokens for
Czech. These settings match the evaluation setting
in previous work such as (McDonald et al, 2005a;
Koo et al, 2008).
We used the method proposed by (Carreras,
2007) for our second-order parsing model. Since
this method only considers projective dependency
structures, we ?projectivized? the PDT training
data in the same way as (Koo et al, 2008). We
used a non-projective model, trained using an ap-
plication of the matrix-tree theorem (Koo et al,
2007; Smith and Smith, 2007; McDonald and
Satta, 2007) for the first-order Czech models, and
projective parsers for all other models.
As shown in Section 2, SS-SCMs with 1st-order
parsing models have two tunable parameters, C
and ?, corresponding to the regularization con-
stant, and the Dirichlet prior for the generative
models. We selected a fixed value ? = 2, which
was found to work well in preliminary experi-
ments.
4
The value of C was chosen to optimize
performance on development data. Note that C
for supervised SCMs were also tuned on develop-
ment data. For the two-stage SS-SCM for incor-
porating second-order parsing model, we have ad-
ditional one tunable parameter B shown in Eq. 8.
This was also chosen by the value that provided
the best performance on development data.
In addition to providing results for models
trained on the full training sets, we also performed
experiments with smaller labeled training sets.
These training sets were either created through
random sampling or by using a predefined subset
of document IDs from the labeled training data.
5 Results and Discussion
Table 3 gives results for the SS-SCM method un-
der various configurations: for first and second-
order parsing models, with and without the clus-
ter features of (Koo et al, 2008), and for varying
amounts of labeled data. The remainder of this
section discusses these results in more detail.
5.1 Effects of the Quantity of Labeled Data
We can see from the results in Table 3 that our
semi-supervised approach consistently gives gains
4
An intuitive meaning of ? = 2 is that this adds one
pseudo expected count to every feature when estimating new
parameter values.
556
(a) English dependency parsing: w/ 43M token unlabeled data (BLLIP)
WSJ sec. IDs wsj 21 random selection random selection wsj 15?18 wsj 02-21(all)
# of sentences / tokens 1,671 / 40,039 2,000 / 48,577 8,000 / 190,958 8,936 / 211,727 39,832 / 950,028
feature type baseline CL baseline CL baseline CL baseline CL baseline CL
Supervised SCM (1od) 85.63 86.80 87.02 88.05 89.23 90.45 89.43 90.85 91.21 92.53
SS-SCM (1od) 87.16 88.40 88.07 89.55 90.06 91.45 90.23 91.63 91.72 93.01
(gain over Sup. SCM) (+1.53) (+1.60) (+1.05) (+1.50) (+0.83) (+1.00) (+0.80) (+0.78) (+0.51) (+0.48)
Supervised MIRA (2od) 87.99 89.05 89.20 90.06 91.20 91.75 91.50 92.14 93.02 93.54
2-stage SS-SCM(+MIRA) (2od) 88.88 89.94 90.03 90.90 91.73 92.51 91.95 92.73 93.45 94.13
(gain over Sup. MIRA) (+0.89) (+0.89) (+0.83) (+0.84) (+0.53) (+0.76) (+0.45) (+0.59) (+0.43) (+0.59)
(b) Czech dependency parsing: w/ 39M token unlabeled data (PDT)
PDT Doc. IDs random selection c[0-9]* random selection l[a-i]* (all)
# of sentences / tokens 2,000 / 34,722 3,526 / 53,982 8,000 / 140,423 14,891 / 261,545 73,008 /1,225,590
feature type baseline CL baseline CL baseline CL baseline CL baseline CL
Supervised SCM (1od) 75.67 77.82 76.88 79.24 80.61 82.85 81.94 84.47 84.43 86.72
SS-SCM (1od) 76.47 78.96 77.61 80.28 81.30 83.49 82.74 84.91 85.00 87.03
(gain over Sup. SCM) (+0.80) (+1.14) (+0.73) (+1.04) (+0.69) (+0.64) (+0.80) (+0.44) (+0.57) (+0.31)
Supervised MIRA (2od) 78.19 79.60 79.58 80.77 83.15 84.39 84.27 85.75 86.82 87.76
2-stage SS-SCM(+MIRA) (2od) 78.71 80.09 80.37 81.40 83.61 84.87 84.95 86.00 87.03 88.03
(gain over Sup. MIRA) (+0.52) (+0.49) (+0.79) (+0.63) (+0.46) (+0.48) (+0.68) (+0.25) (+0.21) (+0.27)
Table 3: Dependency parsing results for the SS-SCM method with different amounts of labeled training
data. Supervised SCM (1od) and Supervised MIRA (2od) are the baseline first and second-order ap-
proaches; SS-SCM (1od) and 2-stage SS-SCM(+MIRA) (2od) are the first and second-order approaches
described in this paper. ?Baseline? refers to models without cluster-based features, ?CL? refers to models
which make use of cluster-based features.
in performance under various sizes of labeled data.
Note that the baseline methods that we have used
in these experiments are strong baselines. It is
clear that the gains from our method are larger for
smaller labeled data sizes, a tendency that was also
observed in (Koo et al, 2008).
5.2 Impact of Combining SS-SCM with
Cluster Features
One important observation from the results in Ta-
ble 3 is that SS-SCMs can successfully improve
the performance over a baseline method that uses
the cluster-based feature set (CL). This is in spite
of the fact that the generative models within the
SS-SCM approach were trained on the same un-
labeled data used to induce the cluster-based fea-
tures.
5.3 Impact of the Two-stage Approach
Table 3 also shows the effectiveness of the two-
stage approach (described in Section 3.2) that inte-
grates the SS-SCM method within a second-order
parser. This suggests that the SS-SCM method
can be effective in providing features (generative
models) used within a separate learning algorithm,
providing that this algorithm can make use of real-
valued features.
91.5
92.0
92.5
93.0
93.5
10 100 1,000 10,000
CL
baseline
43.4M 143M
468M 1.38G
3.72G
(Mega tokens)
Unlabeled data size: [Log-scale]
P
a
r
e
n
t
-
p
r
e
d
ic
t
io
n
 
A
c
c
u
r
a
c
y
(BLLIP)
Figure 1: Impact of unlabeled data size for the SS-
SCM on development data of English dependency
parsing.
5.4 Impact of the Amount of Unlabeled Data
Figure 1 shows the dependency parsing accuracy
on English as a function of the amount of unla-
beled data used within the SS-SCM approach. (As
described in Section 4.1, we have no unlabeled
data other than PDT for Czech, hence this section
only considers English dependency parsing.) We
can see that performance does improve as more
unlabeled data is added; this trend is seen both
with and without cluster-based features. In addi-
tion, Table 4 shows the performance of our pro-
posed method using 3.72 billion tokens of unla-
557
feature type baseline CL
SS-SCM (1st-order) 92.23 93.23
(gain over Sup. SCM) (+1.02) (+0.70)
2-stage SS-SCM(+MIRA) (2nd-order) 93.68 94.26
(gain over Sup. MIRA) (+0.66) (+0.72)
Table 4: Parent-prediction accuracies on develop-
ment data with 3.72G tokens unlabeled data for
English dependency parsing.
beled data. Note, however, that the gain in perfor-
mance as unlabeled data is added is not as sharp
as might be hoped, with a relatively modest dif-
ference in performance for 43.4 million tokens vs.
3.72 billion tokens of unlabeled data.
5.5 Computational Efficiency
The main computational challenge in our ap-
proach is the estimation of the generative mod-
els q = ?q
1
. . . q
k
? from unlabeled data, partic-
ularly when the amount of unlabeled data used
is large. In our implementation, on the 43M to-
ken BLLIP corpus, using baseline features, it takes
about 5 hours to compute the expected counts re-
quired to estimate the parameters of the generative
models on a single 2.93GHz Xeon processor. It
takes roughly 18 days of computation to estimate
the generative models from the larger (3.72 billion
word) corpus. Fortunately it is simple to paral-
lelize this step; our method takes a few hours on
the larger data set when parallelized across around
300 separate processes.
Note that once the generative models have been
estimated, decoding with the model, or train-
ing the model on labeled data, is relatively in-
expensive, essentially taking the same amount of
computation as standard dependency-parsing ap-
proaches.
5.6 Results on Test Data
Finally, Table 5 displays the final results on test
data. There results are obtained using the best
setting in terms of the development data perfor-
mance. Note that the English dependency pars-
ing results shown in the table were achieved us-
ing 3.72 billion tokens of unlabeled data. The im-
provements on test data are similar to those ob-
served on the development data. To determine
statistical significance, we tested the difference of
parent-prediction error-rates at the sentence level
using a paired Wilcoxon signed rank test. All eight
comparisons shown in Table 5 are significant with
(a) English dependency parsing: w/ 3.72G token ULD
feature set baseline CL
SS-SCM (1st-order) 91.89 92.70
(gain over Sup. SCM) (+0.92) (+0.58)
2-stage SS-SCM(+MIRA) (2nd-order) 93.41 93.79
(gain over Sup. MIRA) (+0.65) (+0.48)
(b) Czech dependency parsing: w/ 39M token ULD (PDT)
feature set baseline CL
SS-SCM (1st-order) 84.98 87.14
(gain over Sup. SCM) (+0.58) (+0.39)
2-stage SS-SCM(+MIRA) (2nd-order) 86.90 88.05
(gain over Sup. MIRA) (+0.15) (+0.36)
Table 5: Parent-prediction accuracies on test data
using the best setting in terms of development data
performances in each condition.
(a) English dependency parsers on PTB
dependency parser test description
(McDonald et al, 2005a) 90.9 1od
(McDonald and Pereira, 2006) 91.5 2od
(Koo et al, 2008) 92.23 1od, 43M ULD
SS-SCM (w/ CL) 92.70 1od, 3.72G ULD
(Koo et al, 2008) 93.16 2od, 43M ULD
2-stage SS-SCM(+MIRA, w/ CL) 93.79 2od, 3.72G ULD
(b) Czech dependency parsers on PDT
dependency parser test description
(McDonald et al, 2005b) 84.4 1od
(McDonald and Pereira, 2006) 85.2 2od
(Koo et al, 2008) 86.07 1od, 39M ULD
(Koo et al, 2008) 87.13 2od, 39M ULD
SS-SCM (w/ CL) 87.14 1od, 39M ULD
2-stage SS-SCM(+MIRA, w/ CL) 88.05 2od, 39M ULD
Table 6: Comparisons with the previous top sys-
tems: (1od, 2od: 1st- and 2nd-order parsing
model, ULD: unlabeled data).
p < 0.01.
6 Comparison with Previous Methods
Table 6 shows the performance of a number of
state-of-the-art approaches on the English and
Czech data sets. For both languages our ap-
proach gives the best reported figures on these
datasets. Our results yield relative error reduc-
tions of roughly 27% (English) and 20% (Czech)
over McDonald and Pereira (2006)?s second-order
supervised dependency parsers, and roughly 9%
(English) and 7% (Czech) over the previous best
results provided by Koo et. al. (2008)?s second-
order semi-supervised dependency parsers.
Note that there are some similarities between
our two-stage semi-supervised learning approach
and the semi-supervised learning method intro-
duced by (Blitzer et al, 2006), which is an exten-
sion of the method described by (Ando and Zhang,
558
2005). In particular, both methods use a two-stage
approach; They first train generative models or
auxiliary problems from unlabeled data, and then,
they incorporate these trained models into a super-
vised learning algorithm as real valued features.
Moreover, both methods make direct use of exist-
ing feature-vector definitions f(x,y) in inducing
representations from unlabeled data.
7 Conclusion
This paper has described an extension of the
semi-supervised learning approach of (Suzuki and
Isozaki, 2008) to the dependency parsing problem.
In addition, we have described extensions that in-
corporate the cluster-based features of Koo et al
(2008), and that allow the use of second-order
parsing models. We have described experiments
that show that the approach gives significant im-
provements over state-of-the-art methods for de-
pendency parsing; performance improves when
the amount of unlabeled data is increased from
43.8 million tokens to 3.72 billion tokens. The ap-
proach should be relatively easily applied to lan-
guages other than English or Czech.
We stress that the SS-SCM approach requires
relatively little hand-engineering: it makes di-
rect use of the existing feature-vector representa-
tion f(x,y) used in a discriminative model, and
does not require the design of new features. The
main choice in the approach is the partitioning
of f(x,y) into components r
1
(x,y) . . . r
k
(x,y),
which in our experience is straightforward.
References
R. Kubota Ando and T. Zhang. 2005. A Framework for
Learning Predictive Structures from Multiple Tasks
and Unlabeled Data. Journal of Machine Learning
Research, 6:1817?1853.
J. K. Baker. 1979. Trainable Grammars for Speech
Recognition. In Speech Communication Papers for
the 97th Meeting of the Acoustical Society of Amer-
ica, pages 547?550.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
Adaptation with Structural Correspondence Learn-
ing. In Proc. of EMNLP-2006, pages 120?128.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based n-gram
Models of Natural Language. Computational Lin-
guistics, 18(4):467?479.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
of CoNLL-X, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. of EMNLP-
CoNLL, pages 957?961.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann.
1999. A Statistical Parser for Czech. In Proc. of
ACL, pages 505?512.
J. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proc. of
COLING-96, pages 340?345.
Jan Haji?c. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning. Studies in Honor of
Jarmila Panevov?a, pages 12?19. Prague Karolinum,
Charles University Press.
T. Koo, A. Globerson, X. Carreras, and M. Collins.
2007. Structured Prediction Models via the Matrix-
Tree Theorem. In Proc. of EMNLP-CoNLL, pages
141?150.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
Semi-supervised Dependency Parsing. In Proc. of
ACL-08: HLT, pages 595?603.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 282?289.
D. C. Liu and J. Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Math. Programming, Ser. B, 45(3):503?528.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. McDonald and F. Pereira. 2006. Online Learning of
Approximate Dependency Parsing Algorithms. In
Proc. of EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the Com-
plexity of Non-Projective Data-Driven Dependency
Parsing. In Proc. of IWPT, pages 121?132.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line Large-margin Training of Dependency Parsers.
In Proc. of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Haji?c.
2005b. Non-projective Dependency Parsing us-
ing Spanning Tree Algorithms. In Proc. of HLT-
EMNLP, pages 523?530.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proc. of
EMNLP-CoNLL, pages 915?932.
Mark A. Paskin. 2001. Cubic-time Parsing and Learn-
ing Algorithms for Grammatical Bigram. Technical
report, University of California at Berkeley, Berke-
ley, CA, USA.
559
A. Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proc. of EMNLP,
pages 133?142.
D. A. Smith and J. Eisner. 2007. Bootstrapping
Feature-Rich Dependency Parsers with Entropic Pri-
ors. In Proc. of EMNLP-CoNLL, pages 667?677.
D. A. Smith and N. A. Smith. 2007. Probabilis-
tic Models of Nonprojective Dependency Trees. In
Proc. of EMNLP-CoNLL, pages 132?140.
J. Suzuki and H. Isozaki. 2008. Semi-supervised
Sequential Labeling and Segmentation Using Giga-
Word Scale Unlabeled Data. In Proc. of ACL-08:
HLT, pages 665?673.
Q. I. Wang, D. Schuurmans, and D. Lin. 2008. Semi-
supervised Convex Training for Dependency Pars-
ing. In Proc. of ACL-08: HLT, pages 532?540.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines.
In Proc. of IWPT.
560
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 507?514, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Hidden?Variable Models for Discriminative Reranking
Terry Koo
MIT CSAIL
maestro@mit.edu
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Abstract
We describe a new method for the repre-
sentation of NLP structures within rerank-
ing approaches. We make use of a condi-
tional log?linear model, with hidden vari-
ables representing the assignment of lexi-
cal items to word clusters or word senses.
The model learns to automatically make
these assignments based on a discrimina-
tive training criterion. Training and de-
coding with the model requires summing
over an exponential number of hidden?
variable assignments: the required sum-
mations can be computed efficiently and
exactly using dynamic programming. As
a case study, we apply the model to
parse reranking. The model gives an F?
measure improvement of ? 1.25% be-
yond the base parser, and an ? 0.25%
improvement beyond the Collins (2000)
reranker. Although our experiments are
focused on parsing, the techniques de-
scribed generalize naturally to NLP struc-
tures other than parse trees.
1 Introduction
A number of recent approaches in statistical NLP
have focused on reranking algorithms. In rerank-
ing methods, a baseline model is used to generate a
set of candidate output structures for each input in
training or test data. A second model, which typi-
cally makes use of more complex features than the
baseline model, is then used to rerank the candidates
proposed by the baseline. Reranking approaches
have given improvements in accuracy on a number
of NLP problems including parsing (Collins, 2000;
Charniak and Johnson, 2005), machine translation
(Och and Ney, 2002; Shen et al, 2004), informa-
tion extraction (Collins, 2002), and natural language
generation (Walker et al, 2001).
The success of reranking approaches depends
critically on the choice of representation used by the
reranking model. Typically, each candidate struc-
ture (e.g., each parse tree in the case of parsing) is
mapped to a feature?vector representation. Previous
work has generally relied on two approaches to rep-
resentation: explicitly hand?crafted features (e.g., in
Charniak and Johnson (2005)) or features defined
through kernels (e.g., see Collins and Duffy (2002)).
This paper describes a new method for the rep-
resentation of NLP structures within reranking ap-
proaches. We build on the intuition that lexical items
in natural language often fall into word clusters (for
example, president and chairman might belong to
the same cluster) or fall into distinct word senses
(e.g., bank might have two distinct senses). Our
method involves a hidden?variable model, where
the hidden variables correspond to an assignment
of words to either clusters or word?senses. Lexical
items are automatically assigned their hidden values
using unsupervised learning within a discriminative
reranking approach.
We make use of a conditional log?linear model
for our task. Formally, hidden variables within
the log?linear model consist of global assignments,
where a global assignment entails an assignment of
every word in the sentence to some hidden cluster
or sense value. The number of such global assign-
ments grows exponentially fast with the length of
the sentence being processed. Training and decod-
ing with the model requires summing over the ex-
ponential number of possible global assignments, a
major technical challenge in our model. We show
that the required summations can be computed ef-
ficiently and exactly using dynamic?programming
methods (i.e., the belief propagation algorithm for
Markov random fields (Yedidia et al, 2003)) under
certain restrictions on features in the model.
Previous work on reranking has made heavy use
of lexical statistics, but has treated lexical items as
atoms. The motivation for our method comes from
the observation that statistics based on lexical items
are critical, but that these statistics suffer consid-
erably from problems of data sparsity and word?
507
sense polysemy. Our model has the ability to allevi-
ate data sparsity issues by learning to assign words
to word clusters, and can mitigate problems with
word?sense polysemy by learning to assign lexical
items to underlying word senses based upon con-
textual information. A critical difference between
our method and previous work on unsupervised ap-
proaches to word?clustering or word?sense discov-
ery is that our model is trained using a discriminative
criterion, where the assignment of words to clusters
or senses is driven by the reranking task in question.
As a case study, in this paper we focus on syn-
tactic parse reranking. We describe three model
types that can be captured by our approach. The
first method emulates a clustering operation, where
the aim is to place similar words (e.g., president and
chairman) into the same cluster. The second method
emulates a refinement operation, where the aim is to
recover distinct senses underlying a single word (for
example, distinct senses underlying the noun bank).
The third definition makes use of an existing ontol-
ogy (i.e., WordNet (Miller et al, 1993)). In this case
the set of possible hidden values for each word cor-
responds to possible WordNet senses for the word.
In experimental results on the Penn Wall Street
Journal treebank parsing domain, the hidden?
variable model gives an F?measure improvement of
? 1.25% beyond a baseline model (the parser de-
scribed in Collins (1999)), and gives an ? 0.25%
improvement beyond the reranking approach de-
scribed in Collins (2000). Although the experiments
in this paper are focused on parsing, the techniques
we describe generalize naturally to other NLP struc-
tures such as strings or labeled sequences. We dis-
cuss this point further in Section 6.1.
2 Related Work
Various machine?learning methods have been used
within reranking tasks, including conditional log?
linear models (Ratnaparkhi et al, 1994; Johnson et
al., 1999), boosting methods (Collins, 2000), vari-
ants of the perceptron algorithm (Collins, 2002;
Shen et al, 2004), and generalizations of support?
vector machines (Shen and Joshi, 2003). There have
been several previous approaches to parsing using
log?linear models and hidden variables. Riezler
et al (2002) describe a discriminative LFG pars-
ing model that is trained on standard (syntax only)
treebank annotations by treating each tree as a full
LFG analysis with an observed c-structure and hid-
den f -structure. Clark and Curran (2004) present an
alternative CCG parsing approach that divides each
CCG parse into a dependency structure (observed)
and a derivation (hidden). More recently, Matsuzaki
et al (2005) introduce a probabilistic CFG aug-
mented with hidden information at each nontermi-
nal, which gives their model the ability to tailor it-
self to the task at hand. The form of our model is
closely related to that of Quattoni et al (2005), who
describe a hidden?variable model for object recog-
nition in computer vision.
The approaches of Riezler et al, Clark and Cur-
ran, and Matsuzaki et al are similar to our own
work in that the hidden variables are exponential
in number and must be handled with dynamic?
programming techniques. However, they differ from
our approach in the definition of the hidden variables
(the Matsuzaki et al model is the most similar). In
addition, these three approaches don?t use rerank-
ing, so their features must be restricted to local scope
in order to allow dynamic?programming approaches
to training. Finally, these approaches use Viterbi
or other approximations during decoding, something
our model can avoid (see section 6.2).
In some instantiations, our model effectively clus-
ters words into categories. Our approach differs
from standard word clustering in that the cluster-
ing criteria is directly linked to the reranking objec-
tive, whereas previous word?clustering approaches
(e.g. Brown et al (1992) or Pereira et al (1993))
have typically leveraged distributional similarity. In
other instantiations, our model establishes word?
sense distinctions. Bikel (2000) has done previous
work on incorporating the WordNet hierarchy into
a generative parsing model; however, this approach
requires data with word?sense annotations whereas
our model deals with word?sense ambiguity through
unsupervised discriminative training.
3 The Hidden?Variable Model
In this section we describe a hidden?variable model
based on conditional log?linear models. Each sen-
tence si for i = 1 . . . n in our training data has a
set of ni candidate parse trees ti,1, . . . , ti,ni , which
are the output of an N?best baseline parser. Each
candidate parse has an associated F?measure score,
508
indicating its similarity to the gold?standard parse.
Without loss of generality, we define ti,1 to be the
parse with the highest F?measure for sentence si.
Given a candidate parse tree ti,j , the hidden?
variable model assigns a domain of hidden val-
ues to each word in the tree. For example, the
hidden?value domain for the word bank could be
{bank1, bank2, bank3} or {NN1,NN2,NN3}. De-
tailed descriptions of the domains we used are given
in Section 4.1. Formally, if ti,j spans m words then
the hidden?value domains for each word are the sets
H1(ti,j), . . . ,Hm(ti,j). A global hidden?value as-
signment, which attaches a hidden value to every
word in ti,j , is written h = (h1, . . . , hm) ? H(ti,j),
where H(ti,j) = H1(ti,j)? . . .?Hm(ti,j) is the set
of all possible global assignments for ti,j .
We define a feature?based representation ? such
that ?(ti,j ,h) ? Rd is a vector of feature occur-
rence counts that describes candidate parse ti,j with
global assignment h ? H(ti,j). We write ?k for
k = 1 . . . d to denote the kth component of the vec-
tor ?. Each component of the feature vector is the
count of some substructure within (ti,j ,h). For ex-
ample, ?12 and ?101 could be defined as follows:
?12(ti,j ,h) =
Number of times the word the
occurs with hidden value the3
and part of speech tag DT in
(ti,j ,h).
?101(ti,j ,h) =
Number of times CEO1 ap-
pears as the subject of owns2
in (ti,j ,h)
(1)
We use a parameter vector ? ? Rd to define a
log?linear distribution over candidate trees together
with global hidden?value assignments:
p(ti,j ,h | si,?) =
e?(ti,j ,h)??
?
j?,h??H(ti,j? )
e?(ti,j? ,h
?)??
By marginalizing out the global assignments, we ob-
tain a distribution over the candidate parses alone:
p(ti,j | si,?) =
?
h?H(ti,j)
p(ti,j ,h | si,?) (2)
Later in this paper we will describe how to train
the parameters of the model by minimizing the fol-
lowing loss function?which is the negative log?
likelihood of the training data?with respect to ?:
L(?) = ?
?
i
log p(ti,1 | si,?)
= ?
?
i
log
?
h?H(ti,1) p(ti,1,h | si,?)
(3)
saw with
PP(with)VP(saw)
S(saw)
a telescope
NP(telescope)
the boy
NP(boy)
The man
NP(man)
saw
man
The the
withboy telescope
a
Figure 1: A sample parse tree and its dependency tree.
3.1 Local Feature Vectors
Note that the number of possible global assignments
(i.e., |H(ti,j)|) grows exponentially fast with respect
to the number of words spanned by ti,j . This poses
a problem when training the model, or when calcu-
lating the probability of a parse tree through Eq. 2.
This section describes how to address this difficulty
by restricting features to sufficiently local scope. In
Section 3.2 we show that this restriction allows effi-
cient training and decoding of the model.
The restriction to local feature?vectors makes use
of the dependency structure underlying the parse
tree ti,j . Formally, for tree ti,j , we define the cor-
responding dependency tree D(ti,j) to be a set of
edges between words in ti,j , where (u, v) ? D(ti,j)
if and only if there is a head?modifier dependency
between words u and v. See Figure 1 for an exam-
ple dependency tree. We restrict the definition of
? in the following way1. If w, u and v are word
indices, we introduce single?variable local feature
vectors ?(ti,j , w, hw) ? Rd and pairwise local fea-
ture vectors ?(ti,j , u, v, hu, hv) ? Rd. The global
feature vector ?(ti,j ,h) is then decomposed into a
sum over the local feature vectors:
?(ti,j ,h) =
?
1?w?m
?(ti,j , w, hw) +
?
(u,v)?D(ti,j)
?(ti,j , u, v, hu, hv)
(4)
Notice that the second sum, over pairwise local
feature vectors, respects the dependency structure
D(ti,j). Section 3.2 describes how this decompo-
sition of ? leads to an efficient and exact dynamic?
programming approach that, during training, allows
us to calculate the gradient ?L?? and, during testing,
allows us to find the most probable candidate parse.
In our implementation, each dimension of the lo-
cal feature vectors is an indicator function signaling
the presence of a feature, so that a sum over local
feature vectors in a tree gives the occurrence count
1Note that the restriction on local feature vectors only con-
cerns the inclusion of hidden values; features may still observe
arbitrary structure within the underlying parse tree ti,j .
509
of features in that tree. For instance, define
?12(ti,j , w, hw) =
r
hw = the3 and tree ti,j assigns word
w to part?of?speech DT
z
?101(ti,j , u, v, hu, hv) =
s
(hu, hv) = (CEO1, owns2)
and tree ti,j places (u, v) in
a subject?verb relationship
{
where the notation JPK signifies a 0/1 indicator of
predicate P . When summed over the tree, these defi-
nitions of ?12 and ?101 yield global features ?12 and
?101 as given in the previous example (see Eq. 1).
3.2 Training the Model
We now describe how the loss function in Eq. 3 can
be optimized using gradient descent. The gradient
of the loss function is given by:
?L
?? = ?
?
i
F (ti,1,?) +
?
i,j
p(ti,j | si,?)F (ti,j ,?)
where F (ti,j ,?) =
?
h?H(ti,j)
p(ti,j ,h | si,?)
p(ti,j | si,?)
?(ti,j ,h)
is the expected value of the feature vector produced
by parse tree ti,j . As we remarked earlier, |H(ti,j)|
is exponential in size so direct calculation of either
p(ti,j | si,?) or F (ti,j ,?) is impractical. However,
using the feature?vector decomposition in Eq. 4, we
can rewrite the key functions of ? as follows:
p(ti,j | si,?) =
Zi,j?
j?
Zi,j?
F (ti,j ,?) =?
1 ? w ? m
hw ? Hw(ti,j)
p(ti,j , w, hw)?(ti,j , w, hw) +
?
(u, v) ? D(ti,j)
hu ? Hu(ti,j)
hv ? Hv(ti,j)
p(ti,j , u, v, hu, hv)?(ti,j , u, v, hu, hv)
where p(ti,j , w, hw) and p(ti,j , u, v, hu, hv) are
marginalized probabilities and Zi,j is the associated
normalization constant:
Zi,j =
?
h?H(ti,j)
e?(ti,j ,h)??
p(ti,j , w, x) =
?
h |hw=x
p(ti,j ,h | si,?)
p(ti,j , u, v, x, y) =
?
h |hu=x,hv=y
p(ti,j ,h | si,?)
The three quantities above can be computed with be-
lief propagation (Yedidia et al, 2003), a dynamic?
programming technique that is efficient2 and exact
2The running time of belief propagation varies linearly with
the number of nodes in D(ti,j) and quadratically with the car-
dinality of the largest hidden?value domain.
when the graph D(ti,j) is a tree, which is the case
in our parse reranking model. Having calculated
the gradient in this way, we minimize the loss using
stochastic gradient descent3 (LeCun et al, 1998).
4 Features for Parse Reranking
The previous section described hidden?variable
models for discriminative reranking. We now de-
scribe features for the parse reranking problem. We
focus on the definition of hidden?value domains and
local feature vectors in the reranking model.
4.1 Hidden?Value Domains and Local Features
Each word in a parse tree is given a domain of pos-
sible hidden values by the hidden?variable model.
Models with widely varying behavior can be created
by changing the way these domains are defined. In
particular, in this section we will see how different
definitions of the domains give rise to the three main
model types: clustering, refinement, and mapping
into a pre?built ontology such as WordNet.
As illustration, consider a simple approach that
splits each word into a domain of three word?sense
hidden values (e.g., the word bank would yield the
domain {bank1, bank2, bank3}). In this approach,
each word receives a domain of hidden values that
is not shared with any other word. The model is
then able to distinguish several different usages for
each word, emulating a refinement operation. An
alternative approach is to split each word?s part?of?
speech tag into several sub?tags (e.g., bank would
yield {NN1,NN2,NN3}). This approach assigns the
same domain to many words; for instance, singular
nouns such as bond, market, and bank all receive the
same domain. The behavior of the model then emu-
lates a clustering operation.
Figure 2 shows the single?variable and pairwise
features used in our experiments. The features
are shown with hidden variables corresponding to
word?specific hidden values, such as shares1 or
bought3. In our experiments, we made use of fea-
tures such as those in Figure 2 in combination with
the following four definitions of the hidden?value
3We also performed some experiments using the conjugate
gradient descent algorithm (Johnson et al, 1999). However, we
did not find a significant difference between the performance of
either method. Since stochastic gradient descent was faster and
required less memory, our final experiments used the stochastic
gradient method.
510
bought yesterday1.5m shares in heavy trading
VBD(bought) PP(in) NP(yesterday)NP(shares)
S(bought)
VP(bought)
PSfrag replacements
Pairwise features generated for (shares1, bought3) =
(shares1, bought3, Dependency: VBD, NP, VP, Right, +Adj, -CC)
(shares1, bought3, Rule: VP? VBDhead, NPmod, PP, NP)
(shares1, bought3, Gpar Rule: S ? VP ? VBDhead, NPmod, PP, NP)
Single?variable features generated for (shares1) =
(shares1)
(shares1, Word: shares)
(shares1, POS: NN)
(shares1, Word: shares, POS: NN)
(shares1, Highest NT: NP)
(shares1, Word: shares, Highest NT: NP)
(shares1, POS: NN, Highest NT: NP)
(shares1, Word: shares, POS: NN, Highest NT: NP)
(shares1, Up Path: NP, VP, S)
(shares1, Word: shares, Up Path: NP, VP, S)
(shares1, Down Path: NP, NN)
(shares1, Word: shares, Down Path: NP, NN)
(shares1, Head Rule: NP ? CD, CD, NNShead)
(shares1, Word: shares, Head Rule: NP? CD, CD, NNShead)
(shares1, Mod Rule: VP ? VBDhead, NPmod, PP, NP)
(shares1, Word: shares, Mod Rule: VP? VBDhead, NPmod, PP, NP)
(shares1, Head Gpar Rule: VP ? NP? CD, CD, NNShead)
(shares1, Word: shares, Head Gpar Rule: VP? NP? CD, CD, NNShead)
(shares1, Mod Gpar Rule: S ? VP? VBDhead, NPmod, PP, NP)
(shares1, Word: shares, Mod Gpar Rule: S ? VP ? VBDhead, NPmod, PP, NP)
Figure 2: The features used in our model. We
show the single?variable features produced for hidden value
shares1 and the pairwise features produced for hidden values
(shares1, bought3), in the context of the given parse fragment.
Highest NT = highest nonterminal, Up Path = sequence of ances-
tor nonterminals, Down Path = sequence of headed nonterminals,
Head Rule = rules headed by the word, Mod Rule = rule in which
word acts as modifier, Head/Mod Gpar Rule = Head/Mod Rule plus
grandparent nonterminal.
domains (in each case we give the model type that
results from the definition?clustering, refinement,
or pre?built ontology?in parentheses):
Lexical (Refinement) Each word is split into
three sub?values. See Figure 2 for an example of
features generated for this choice of domain.
Part?of?Speech (Clustering) The part?of?
speech tag of each word is split into five sub?values.
In Figure 2, the word shares would be assigned
the domain {NNS1, . . . ,NNS5}, and the word bought
would have the domain {VBD1, . . . , VBD5}.
Highest Nonterminal (Clustering) The high-
est nonterminal to which each word propagates as
a headword is split into five sub?values. In Figure 2
the word bought yields domain {S1, . . . , S5}, while
in yields {PP1, . . . , PP5}.
Supersense (Pre?Built Ontology) We borrow
the idea of using WordNet lexicographer filenames
as broad ?supersenses? from Ciaramita and John-
son (2003). For each word, we split each of its
supersenses into three sub?supersenses. If no su-
persenses are available, we fall back to splitting
the part?of?speech into five sub?values. For ex-
ample, shares has the supersenses noun.possession,
noun.act and noun.artifact, which yield the do-
main {noun.possession1, noun.act1, noun.artifact1, . . .
noun.possession3, noun.act3, noun.artifact3}. On the
other hand, in does not have any WordNet super-
senses, so it is assigned the domain {IN1, . . . , IN5}.
4.2 The Final Feature Sets
We created eight feature sets by combining the
four hidden?value domains above with two alterna-
tive definitions of dependency structures: standard
head?modifier dependencies and ?sibling dependen-
cies.? When using sibling dependencies, connec-
tions are established between the headwords of ad-
jacent siblings. For instance, the head?modifier
dependencies produced by the tree fragment in
Figure 2 are (bought, shares), (bought, in), and
(bought, yesterday), while the corresponding sibling
dependencies are (bought, shares), (shares, in), and
(in, yesterday).
4.3 Mixed Models
The different hidden?variable models display vary-
ing strengths and weaknesses. We created mixtures
of different models using a weighted average:
log p(ti,j |si) =
M?
m=1
?m log pm(ti,j |si,?m)?Z(si)
where Z(si) is a normalization constant that can be
ignored, as it does not affect the ranking of parses.
The ?m weights are determined through optimiza-
tion of parsing scores on a development set.
5 Experimental Results
We trained and tested the model on data from the
Penn Treebank (Marcus et al, 1994). Candidate
parses were produced by an N?best version of the
Collins (1999) parser. Our training data consists of
Treebank Sections 2?21, divided into a training cor-
pus of 35,540 sentences and a development corpus
of 3,676 sentences. In later experiments, we made
use of a secondary development corpus of 1,914 sen-
tences from Section 0. Sections 22?24, containing
5,455 sentences, were held out as the test set.
For each of the eight feature sets described in
Section 4.2, we used the stochastic gradient descent
511
Section 22 Section 23 Section 24 Total
LR LP LR LP LR LP LR LP
C99 89.12 89.20 88.14 88.56 87.17 87.97 88.19 88.60
MIX 90.43 90.61 89.25 89.69 88.46 89.29 89.41 89.87
C2K 90.27 90.62 89.43 89.97 88.56 89.58 89.46 90.07
MIX+ 90.57 90.79 89.80 90.27 88.78 89.73 89.78 90.29
Table 1: The results on Sections 22?24. LR = Labeled Recall,
LP = Labeled Precision.
method to optimize the parameters of the model. We
created various mixtures of the eight models using
the weighted?average technique described in Sec-
tion 4.3, testing the accuracy of each mixture on the
secondary development set. Our final model was a
mixture of three of the eight possible models: super-
sense hidden values with sibling trees, lexical hid-
den values with sibling trees, and highest nontermi-
nal hidden values with normal head?modifier trees.
Our final tests evaluated four models. The two
baseline models are the Collins (1999) base parser,
C99, and the Collins (2000) reranker, C2K. The first
new model is the MIX model, which is a combina-
tion of the C99 base model with the three models
described above. The second new model, MIX+, is
created by augmenting MIX with features from the
method in C2K. Table 1 shows the results. The
MIX model obtains an F?measure improvement of
? 1.25% over the C99 baseline, an improvement that
is comparable to the C2K reranker. The MIX+ model
yields an improvement of ? 0.25% beyond C2K.
We tested the significance of 8 comparisons cor-
responding to the results in Table 1 using the sign
test4: we tested MIX vs. C99 on Sections 22, 23, and
24 individually, as well as on Sections 22?24 taken
as a whole; we also tested MIX+ vs. C2K on these 4
test sets. Of the 8 comparisons, all showed signif-
icant improvements at the level p ? 0.01 with the
exception of one test, MIX+ vs. C2K on Section 24.
6 Discussion
6.1 Applying the Model to Other NLP Tasks
In this section, we discuss how hidden?variable
models might be applied to other NLP problems, and
in particular to structures other than parse trees. To
4The input to the sign test is a set of sentences with judge-
ments for each sentence indicating whether model 1 gives a
better parse than model 2, model 2 gives a better parse than
model 1, or models 1 and 2 give equal quality parses. When
using the sign test, for each sentence in question we calculate
the F?measure at the sentence level for the two models being
compared, deriving the required judgement from these scores.
summarize the model, the major components of the
approach are as follows:
? We assume some set of candidate structures ti,j ,
which are to be reranked by the model. Each struc-
ture ti,j has ni,j words w1, . . . , wni,j , and each word
wk has a set Hk(ti,j) of possible hidden values.
? We assume a graph D(ti,j) for each ti,j that de-
fines possible interactions between hidden variables
in the model. We assume some definition of local
feature vectors, which consider either single hidden
variables, or pairs of hidden variables that are con-
nected by an edge in D(ti,j).
The approach can be instantiated in several ways
when applying the model to other NLP tasks. We
have already seen that by changing the definition
of the hidden?value domains Hk(ti,j), we can de-
rive models with widely varying behavior. In ad-
dition, there is no requirement that the hidden vari-
ables only be associated with words in the structure;
the hidden variables could be associated with other
units. For example, in speech recognition hidden
variables could be associated with phonemes rather
than words, and in Chinese word segmentation, hid-
den variables could be associated with individual
characters rather than words.
NLP tasks other than parsing involve structures
ti,j that are not necessarily parse trees. For example,
in speech recognition candidates are simply strings
(utterances); in tagging tasks candidates are labeled
sequences (e.g., sentences labeled with part?of?
speech tag sequences); in machine translation can-
didate structures may be source?language/target?
language pairs, along with alignment structures
specifying the correspondence between words in the
two languages. Sentences and labeled sequences are
in a sense simplifications of the parsing case, where
a natural choice for the underlying graph D(ti,j)
would be an N th order Markov structure, where each
word depends on the previous N words. Machine
translation alignments are a more interesting type of
structure, where the choice ofD(ti,j) might actually
depend on the alignment between the two sentences.
As a final note, there is some flexibility in the
choice of D(ti,j). In the case that D(ti,j) is a tree
belief propagation is exact. In the more general case
where D(ti,j) contains cycles, there are alternative
algorithms that are either exact (Cowell et al, 1999)
or approximate (Yedidia et al, 2003).
512
6.2 Packed Representations and Locality
One natural extension of our reranker is to adapt it to
candidate parses represented as a packed parse for-
est, so that it can operate on the base parser?s full
output instead of a limited N -best list. However,
as we described in Section 3.1, our features are lo-
cally scoped with respect to hidden?variable interac-
tions but unrestricted regarding information derived
from the underlying candidate parses, which poses
a problem for the use of packed representations.
For instance, the Up/Down Path features (see Figure
2) enumerate the vertical sequences of nontermi-
nals that extend above and below a given headword.
We could restrict the features to local scope on the
candidate parses, allowing dynamic?programming
to be used to train the model with a packed rep-
resentation. However, even with these restrictions,
finding argmaxt
?
h p(t,h | s,?) is NP?hard, and
the Viterbi approximation argmaxt,h p(t,h | s,?)
? or other approximations ? would have to be used
(see Matsuzaki et al (2005)).
6.3 Empirical Analysis of the Hidden Values
Our model makes no assumptions about the interpre-
tation of the hidden values assigned to words: dur-
ing training, the model simply learns a distribution
over global hidden?value assignments that is useful
in improving the log?likelihood of the training data.
Intuitively, however, we expect that the model will
learn to make hidden?value assignments that are rea-
sonable from a linguistic standpoint. In this section
we describe some empirical observations concern-
ing hidden values assigned by the model.
We established a corpus of parse trees with
hidden?value annotations, as follows. First, we find
the optimal parameters ?? on the training set. For
every sentence si in the training set, we then use
?? to find t?i , the most probable candidate parse un-
der the model. Finally, we use ?? to decode h?i ,
the most probable global assignment of hidden val-
ues, for each parse tree t?i . We created a corpus of
(t?i ,h
?
i ) pairs for the feature set defined by part?of?
speech hidden?value domains and standard depen-
dency structures. The remainder of this section de-
scribes trends for several of the most common part?
of?speech categories in the corpus.
As a first example, consider the hidden values
for the part?of?speech VB (infinitival verb). In the
majority of cases, words tagged VB either modify a
modal verb tagged MD (e.g., in the new rate will/MD
be/VB payable) or the infinitival marker to (e.g., in in
an effort to streamline/VB bureaucracy). The statis-
tics of our corpus reflect this distinction. In 11,546
cases of the VB1 hidden value, 10,652 cases mod-
ified to, and 81 cases modified modals tagged MD.
In contrast, in 11,042 cases of the VB2 value, the
numbers were 8,354 and 599 for modification of
modals and to respectively, showing the opposite
preference. This polarization of hidden values al-
lows modifiers to the VB (e.g., payable in the new
rate will be payable) to be sensitive to whether the
verb is modifying a modal or to.
In a related case, the hidden values for the part?
of?speech TO (corresponding to the word to) also
show that the model is learning useful structure.
Consider cases where to heads a clause which may
or may not have a subject (e.g., in it expects ?its sales
to remain steady? vs. a proposal ?to ease reporting
requirements?). We find that for hidden values TO1
and TO5 together, 946 out of 976 cases have a sub-
ject. In contrast, for the hidden value TO4, only 29
out of 10,148 cases have a subject. This splitting
of the TO part?of?speech allows modifiers to to, or
words modified by to, to be sensitive to the presence
or absence of a subject in the clause headed by to.
Finally, consider the hidden values for the part?
of?speech NNS (plural noun). In this case, the model
distinguishes contexts where a plural noun acting as
the head of a noun?phrase is or isn?t modified by a
post?modifier (such as a prepositional phrase or rel-
ative clause). For hidden value NNS3, 12,003 out
of the 12,664 instances in our corpus have a post?
modifier, but for hidden value NNS5, only 4,099 of
the 39,763 occurrences have a post?modifier. Sim-
ilar contextual effects were observed for other noun
categories such as singular or proper nouns.
7 Conclusions and Future Research
The hidden?variable model is a novel method for
representing NLP structures in the reranking frame-
work. We can obtain versatile behavior from the
model simply by manipulating the definition of the
hidden?value domains, and we have experimented
with models that emulate word clustering, word re-
finement, and mappings from words into an existing
ontology. In the case of the parse reranking task,
513
the hidden?variable model achieves reranking per-
formance comparable to the reranking approach de-
scribed by Collins (2000), and the two rerankers can
be combined to yield an additive improvement.
Future work may consider the use of hidden?
value domains with mixed contents, such as a do-
main that contains 3 refinement?oriented lexical val-
ues and 3 clustering?oriented part?of?speech val-
ues. These mixed values would allow the hidden?
variable model to exploit interactions between clus-
tering and refinement at the level of words and de-
pendencies. Another area for future research is to
investigate the use of unlabeled data within the ap-
proach, for example by making use of clusters de-
rived from large amounts of unlabeled data (e.g., see
Miller et al (2004)). Finally, future work may apply
the models to NLP tasks other than parsing.
Acknowledgements
We would like to thank Regina Barzilay, Igor
Malioutov, and Luke Zettlemoyer for their many
comments on the paper. We gratefully acknowl-
edge the support of the National Science Founda-
tion (under grants 0347631 and 0434222) and the
DARPA/SRI CALO project (through subcontract
No. 03-000215).
References
Daniel Bikel. 2000. A statistical model for parsing and word?
sense disambiguation. In Proceedings of EMNLP.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class?based n?
gram models of natural language. Computational Linguis-
tics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse?to?fine n?
best parsing and maxent discriminative reranking. In Pro-
ceedings of the 43rd ACL.
Massimiliano Ciaramita and Mark Johnson. 2003. Supersense
tagging of unknown nouns in wordnet. In EMNLP 2003.
Stephen Clark and James R. Curran. 2004. Parsing the wsj
using ccg and log?linear models. In ACL, pages 103?110.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL 2002.
Michael Collins. 1999. Head?Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for natural
language parsing. In Proceedings of the 17th ICML.
Michael Collins. 2002. Ranking algorithms for named entity
extraction: Boosting and the voted perceptron. In ACL 2002.
Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and
David J. Spiegelhalter. 1999. Probabilistic Networks and
Expert Systems. Springer.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification?
based? grammars. In Proceedings of the 37th ACL.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998.
Gradient?based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278?2324, November.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus
of english: The penn treebank. Computational Linguistics,
19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2005.
Probabilistic cfg with latent annotations. In ACL.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1993. Five papers on
wordnet. Technical report, Stanford University.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative train-
ing. In HLT?NAACL, pages 337?342.
Franz Josef Och and Hermann Ney. 2002. Discriminative train-
ing and maximum entropy models for statistical machine
translation. In Proceedings of the 40th ACL, pages 295?302.
Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In Proceedings
of the 31st ACL, pages 183?190.
Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2005.
Conditional random fields for object recognition. In NIPS
17. MIT Press.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994.
A maximum entropy model for parsing. In ICSLP 1994.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard S.
Crouch, John T. Maxwell III, and Mark Johnson. 2002.
Parsing the wall street journal using a lexical?functional
grammar and discriminative estimation techniques. In ACL.
Libin Shen and Aravind K. Joshi. 2003. An svm?based vot-
ing algorithm with application to parse reranking. In Walter
Daelemans and Miles Osborne, editors, Proceedings of the
7th CoNLL, pages 9?16. Edmonton, Canada.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004. Dis-
criminative reranking for machine translation. In HLT?
NAACL, pages 177?184.
Marilyn A. Walker, Owen Rambow, and Monica Rogati. 2001.
Spot: A trainable sentence planner. In NAACL.
Jonathan S. Yedidia, William T. Freeman, and Yair Weiss,
2003. Exploring Artificial Intelligence in the New Millen-
nium, chapter 8: ?Understanding Belief Propagation and Its
Generalizations?, pages 239?236. Science & Technology
Books.
514
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 795?802, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Morphology and Reranking for the Statistical Parsing of Spanish
Brooke Cowan
MIT CSAIL
brooke@csail.mit.edu
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Abstract
We present two methods for incorporat-
ing detailed features in a Spanish parser,
building on a baseline model that is a lex-
icalized PCFG. The first method exploits
Spanish morphology, and achieves an F1
constituency score of 83.6%. This is an
improvement over 81.2% accuracy for the
baseline, which makes little or no use of
morphological information. The second
model uses a reranking approach to add
arbitrary global features of parse trees to
the morphological model. The reranking
model reaches 85.1% F1 accuracy on the
Spanish parsing task. The resulting model
for Spanish parsing combines an approach
that specifically targets morphological in-
formation with an approach that makes
use of general structural features.
1 Introduction
Initial methods for statistical parsing were mainly
developed through experimentation on English data
sets. Subsequent research has focused on apply-
ing these methods to other languages. There has
been widespread evidence that new languages ex-
hibit linguistic phenomena that pose considerable
challenges to techniques originally developed for
English; because of this, an important area of cur-
rent research concerns how to model these phenom-
ena more accurately within statistical approaches. In
this paper, we investigate this question within the
context of parsing Spanish. We describe two meth-
ods for incorporating detailed features in a Spanish
parser, building on a baseline model that is a lexical-
ized PCFG originally developed for English.
Our first model uses morphology to improve
the performance of the baseline model. English
is a morphologically-impoverished language, while
most of the world?s languages exhibit far richer mor-
phologies. Spanish is one of these languages. For
instance, the forms of Spanish nouns, determiners,
and adjectives reflect both number and gender; pro-
nouns reflect gender, number, person, and case. Fur-
thermore, morphological constraints may be mani-
fested at the syntactic level: certain constituents of a
noun phrase are constrained to agree in number and
gender, and a verb is constrained to agree in num-
ber and person with its subject. Hence, morphol-
ogy gives us important structural cues about how the
words in a Spanish sentence relate to one another.
The mechanism we employ for incorporating mor-
phology into the PCFG model (the Model 1 parser
in (Collins, 1999)) is the modification of its part-of-
speech (POS) tagset; in this paper, we explain how
this mechanism allows the parser to better capture
morphological constraints.
All of the experiments in this paper are carried
out using a freely-available Spanish treebank pro-
duced by the 3LB project (Navarro et al, 2003).
This resource contains around 3,500 hand-annotated
trees encoding ample morphological information.
We could not use all of this information and ade-
quately train the resulting parameters due to lim-
ited training data. Hence, we used development
data to test the performance of several models, each
incorporating a subset of morphological informa-
tion. The highest-accuracy model on the devel-
opment set uses the mode and number of verbs,
as well as the number of adjectives, determiners,
nouns, and pronouns. On test data, it reaches
F1 accuracy of 83.6%/83.9%/79.4% for labeled
constituents, unlabeled dependencies, and labeled
dependencies, respectively. The baseline model,
which makes almost no use of morphology, achieves
81.2%/82.5%/77.0% in these same measures.
We use the morphological model from the afore-
mentioned experiments as a base parser in a second
set of experiments. Here we investigate the efficacy
of a reranking approach for parsing Spanish by using
795
arbitrary structural features. Previous work in sta-
tistical parsing (Collins and Koo, 2005) has shown
that applying reranking techniques to the n-best out-
put of a base parser can improve parsing perfor-
mance. Applying an exponentiated gradient rerank-
ing algorithm (Bartlett et al, 2004) to the n-best out-
put of our morphologically-informed Spanish pars-
ing model gives us similar improvements. Using the
reranking model combined with the morphological
model raises performance to 85.1%/84.7%/80.2%
F1 accuracy for labeled constituents, unlabeled de-
pendencies, and labeled dependencies.
2 Related Work
The statistical parsing of English has surpassed 90%
accuracy in the precision and recall of labeled con-
stituents (e.g., (Collins, 1999; Charniak and John-
son, 2005)). A recent proliferation of treebanks in
various languages has fueled research in the pars-
ing of other languages. For instance, work has
been done in Chinese using the Penn Chinese Tree-
bank (Levy and Manning, 2003; Chiang and Bikel,
2002), in Czech using the Prague Dependency Tree-
bank (Collins et al, 1999), in French using the
French Treebank (Arun and Keller, 2005), in Ger-
man using the Negra Treebank (Dubey, 2005; Dubey
and Keller, 2003), and in Spanish using the UAM
Spanish Treebank (Moreno et al, 2000). The best-
reported F1 constituency scores from this work for
each language are 79.9% (Chinese (Chiang and
Bikel, 2002)), 81.0% (French (Arun and Keller,
2005), 76.2% (German (Dubey, 2005)), and 73.8%
(Spanish (Moreno et al, 2000)). The authors in
(Collins et al, 1999) describe an approach that gives
80% accuracy in recovering unlabeled dependencies
in Czech.1
The project that is arguably most akin to the work
presented in this paper is that on Spanish parsing
(Moreno et al, 2000). However, a direct compari-
son of scores is complicated by the fact that we have
used a different corpus as well as larger training and
test sets (2,800- vs. 1,500-sentence training sets, and
700- vs. 40-sentence test sets).
1Note that cross-linguistic comparison of results is compli-
cated: in addition to differences in corpus annotation schemes
and sizes, there may be significant differences in linguistic char-
acteristics.
Category Attributes
Adjective gender, number, participle
Determiner gender, number, person, possessor
Noun gender, number
Verb gender, number, person, mode, tense
Preposition gender, number, form
Pronoun gender, number, person, case, possessor
Table 1: A list of the morphological features from which we
created our models. For brevity, we only list attributes with at
least two values. See (Civit, 2000) for a comprehensive list of
the morphological attributes included in the Spanish treebank.
3 Models
This section details our two approaches for adding
features to a baseline parsing model. First, we de-
scribe how morphological information can be added
to a parsing model by modifying the POS tagset.
Second, we describe an approach that reranks the
n-best output of the morphologically-rich parser, us-
ing arbitrary, general features of the parse trees as
additional information.
3.1 Adding Morphological Information
The mechanism we employ for incorporating mor-
phological information is the modification of the
POS tagset of a lexicalized PCFG2 ? the Model 1
parser described in (Collins, 1999) (hereafter
Model 1). Each POS tagset can be thought of as a
particular morphological model or a subset of mor-
phological attributes. Table 1 shows the complete set
of morphological features we considered for Span-
ish. There are 22 morphological features in total in
this table; different POS sets can be created by de-
ciding whether or not to include each of these 22
features; hence, there are 222 different morpholog-
ical models we could have created. For instance,
one particular model might capture the modal infor-
mation of verbs. In this model, there would be six
POS tags for verbs (one for each of indicative, sub-
junctive, imperative, infinitive, gerund, and partici-
ple) instead of just one. A model that captured both
the number and mode of verbs would have 18 verbal
POS tags, assuming three values (singular, plural,
and neutral) for the number feature.
The Effect of the Tagset on Model 1 Modifying
the POS tagset alows Model 1 to better distinguish
2Hand-crafted head rules are used to lexicalize the trees.
796
S(corri?,v)
NP(gatos,n) VP(corri?,v)
Figure 1: An ungrammatical dependency: the plural noun gatos
is unlikely to modify the singular verb corrio?.
events that are unlikely from those that are likely, on
the basis of morphological evidence. An example
will help to illustrate this point.
Model 1 relies on statistics conditioned on lexi-
cal headwords for practically all parameters in the
model. This sensitivity to headwords is achieved by
propagating lexical heads and POS tags to the non-
terminals in the parse tree. Thus, any statistic based
on headwords may also be sensitive to the associated
POS tag. For instance, consider the subtree in Fig-
ure 1. Note that this structure is ungrammatical be-
cause the subject, gatos (cats), is plural, but the verb,
corrio? (ran), is singular. In Model 1, the probability
of generating the noun phrase (NP) with headword
gatos and headtag noun (n) is defined as follows:3
P (gatos, n, NP | corrio?, v, S, VP) =
P1(n, NP | corrio?, v, S, VP)?
P2(gatos | n, NP, corrio?, v, S, VP)
The parser smooths parameter values using backed-
off statistics, and in particular smooths statistics
based on headwords with coarser statistics based on
POS tags alone. This allows the parser to effectively
use POS tags as a way of separating different lexi-
cal items into subsets or classes depending on their
syntactic behavior. In our example, each term is es-
timated as follows:
P1(n, NP | corrio?, v, S, VP) =
?1,1P?1,1(n, NP | corrio?, v, S, VP) +
?1,2P?1,2(n, NP | v, S, VP) +
?1,3P?1,3(n, NP | S, VP)
and
P2(gatos | n, NP, corrio?, v, S, VP) =
?2,1P?2,1(gatos | n, NP, corrio?, v, S, VP) +
?2,2P?2,2(gatos | n, NP, v, S, VP) +
?2,3P?2,3(gatos | n)
3Note that the parsing model includes other features such as
distance which we omit from the parameter definition for the
sake of brevity.
Here the P?i,j terms are maximum likelihood es-
timates derived directly from counts in the train-
ing data. The ?i,j parameters are defined so that
?1,1+?1,2+?1,3 = ?2,1+?2,2+?2,3 = 1. They con-
trol the relative contribution of each level of back-off
to the final estimate.
Note that thus far our example has not included
any morphological information in the POS tags. Be-
cause of this, we will see that there is a danger of
the estimates P1 and P2 both being high, in spite
of the dependency being ungrammatical. P1 will be
high because all three estimates P?1,1, P?1,2 and P?1,3
will most likely be high. Next, consider P2. Of the
three estimates P?2,1, P?2,2, and P?2,3, only P?2,1 retains
the information that the noun is plural and the verb
is singular. Thus P2 will be sensitive to the morpho-
logical clash between gatos and corrio? only if ?2,1 is
high, reflecting a high level of confidence in the es-
timate of P?2,3. This will only happen if the context
?corrio?, v, S, VP? is seen frequently enough for ?2,1
to take a high value. This is unlikely, given that this
context is quite specific. In summary, the impover-
ished model can only capture morphological restric-
tions through lexically-specific estimates based on
extremely sparse statistics.
Now consider a model that incorporates morpho-
logical information ? in particular, number infor-
mation ? in the noun and verb POS tags. gatos will
have the POS pn, signifying a plural noun; corrio?
will have the POS sv, signifying a singular verb.
All estimates in the previous equations will reflect
these POS changes. For example, P1 will now be
estimated as follows:
P1(pn, NP | corrio?, sv, S, VP) =
?1,1P?1,1(pn, NP | corrio?, sv, S, VP) +
?1,2P?1,2(pn, NP | sv, S, VP) +
?1,3P?1,3(pn, NP | S, VP)
Note that the two estimates P?1,1 and P?1,2 include
an (unlikely) dependency between the POS tags pn
and sv. Both of these estimates will be 0, assum-
ing that a plural noun is never seen as the subject of
a singular verb. At the very least, the context ?sv,
S, VP? will be frequent enough for P?1,2 to be a re-
liable estimate. The value for ?1,2 will therefore be
high, leading to a low estimate for P1, thus correctly
assigning low probability to the ungrammatical de-
797
pendency. In summary, the morphologically-rich
model can make use of non-lexical statistics such as
P?1,2(pn, NP | sv, S, VP) which contain dependen-
cies between POS tags and which will most likely
be estimated reliably by the model.
3.2 The Reranking Model
In the reranking model, we use an n-best version of
the morphologically-rich parser to generate a num-
ber of candidate parse trees for each sentence in
training and test data. These parse trees are then
represented through a combination of the log prob-
ability under the initial model, together with a large
number of global features. A reranking model uses
the information from these features to derive a new
ranking of the n-best parses, with the hope of im-
proving upon the baseline model. Previous ap-
proaches (e.g., (Collins and Koo, 2005)) have used
a linear model to combine the log probability un-
der a base parser with arbitrary features derived from
parse trees. There are a variety of methods for train-
ing the parameters of the model. In this work, we
use the algorithm described in (Bartlett et al, 2004),
which applies the large-margin training criterion of
support vector machines (Cortes and Vapnik, 1995)
to the reranking problem.
The motivation for the reranking model is that a
wide variety of features, which can essentially be
sensitive to arbitrary context in the parse trees, can
be incorporated into the model. In our work, we in-
cluded all features described in (Collins and Koo,
2005). As far as we are aware, this is the first time
that a reranking model has been applied to parsing
a language other than English. One goal was to in-
vestigate whether the improvements seen on English
parsing can be carried across to another language.
We have found that features in (Collins and Koo,
2005), initially developed for English parsing, also
give appreciable gains in accuracy when applied to
Spanish.
4 Data
The Spanish 3LB treebank is a freely-available re-
source with about 3,500 sentence/tree pairs that we
have used to train our models. The average sen-
tence length is 28 tokens. The data is taken from
38 complete articles and short texts. Roughly 27%
Non-Terminal Significance
aq adjective
cc conjunction
COORD coordinated phrase
ESPEC determiner
GRUP base noun phrase
GV verb phrase
MORF impersonal pronoun
p pronoun
PREP base prepositional phrase
RELATIU relative pronoun phrase
s adjectival phrase
SN noun phrase
SP prepositional phrase
SADV adverbial phrase
S sentence
sps preposition
v verb
Table 2: The non-terminals and preterminals from the Spanish
3LB corpus used in this paper.
of the texts are news articles, 27% scientific articles,
14% narrative, 11% commentary, 11% sports arti-
cles, 6% essays, and 5% articles from weekly maga-
zines. The trees contain information about both con-
stituency structure and syntactic functions.
4.1 Preprocessing
It is well-known that tree representation influences
parsing performance (Johnson, 1998). Prior to train-
ing our models, we made some systematic modifica-
tions to the corpus trees in an effort to make it eas-
ier for Model 1 to represent the linguistic phenom-
ena present in the trees. For the convenience of the
reader, Table 2 gives a key to the non-terminal labels
in the 3LB treebank that are used in this section and
the remainder of the paper.
Relative and Subordinate Clauses Cases of rela-
tive and subordinate clauses appearing in the corpus
trees have the basic structure of the example in Fig-
ure 2a. Figure 2b shows the modifications we im-
pose on such structures. The modified structure has
the advantage that the SBAR selects the CP node as
its head, making the relative pronoun que the head-
word for the root of the subtree. This change allows,
for example, better modeling of verbs that select for
particular complementizers. In addition, the new
subtree rooted at the S node now looks like a top-
level sentence, making sentence types more uniform
in structure and easier to model statistically. Addi-
tionally, the new structure differentiates phrases em-
798
RELATIU?CP
a
p
quien
SP?CP
sps
PREP?CP
consideraban
v
GV
todos
SN
GRUP
p
CP
SBAR?S
S
todos
PREP
sps
a
SN
GRUP
p consideraban
v
GV
S
SP
RELATIU
p
quien
(a)
(b)
Figure 2: Figure (a) is the original structure from the 3LB tree-
bank for the phrase a quien todos consideraban or whom ev-
eryone considered. We transform structures like (a) into (b) by
inserting SBAR and CP nodes, and by marking all non-terminals
below the CP with a -CP tag.
bedded in the complementizers of SBARs from those
used in other contexts, allowing relative pronouns
like quien in Figure 2 to surface as lexical head-
words when embedded in larger phrases beneath the
CP node.4
Coordination In the treebank, coordinated con-
stituents and their coordinating conjunction are
placed as sister nodes in a flat structure. We enhance
the structure of such subtrees, as in Figure 3. Our
structure helps to rule out unlikely phrases such as
cats and dogs and; the model trained with the orig-
inal treebank structures will assign non-zero proba-
bility to ill-formed structures such as these.
5 Experiments
Our models were trained using a training set con-
sisting of 80% of the data (2,801 sentence/tree pairs,
75,372 words) available to us in the 3LB treebank.
We reserved the remaining 20% (692 sentences,
19,343 words) to use as unseen data in a test set.
We selected these subsets with two criteria in mind:
first, respecting the boundaries of the texts by plac-
ing articles in their entirety into either one subset or
the other; and second, maintaining, in each subset,
the same proportion of genres found in the original
set of trees. During development, we used a cross-
4This is achieved through our head rules.
(a)
(b)
civilesparlamentarios y
parlamentarios
COORD
y civiles
s
s?CC1
s s?CC2
s
aq
s
COORD ss
aq cc aq
aq
cc
Figure 3: In the 3LB corpus, phrases involving coordination,
are represented with a flat structure as in (a). For coordination
involving a non-terminal X (X = s in the example), we insert
new nodes X-CC1 and X-CC2 to form the structure in (b).
validation approach on the training set to test differ-
ent models. We divided the 2,800 training data trees
into 14 different development data sets, where each
of these data sets consisted of 2,600 training sen-
tences and 200 development sentences. We took the
average over the results of the 14 splits to gauge the
effectiveness of the model being tested.
To evaluate our models, we considered the recov-
ery of labeled and unlabeled dependencies as well as
labeled constituents. Unlabeled dependencies cap-
ture how the words in a sentence depend on one an-
other. Formally, they are tuples {headchild index,
modifier index}, where the indices indicate position
in the sentence. Labeled dependencies include the
labels of the modifier, headchild, and parent non-
terminals as well. The root of the tree has a special
dependency: {head index} in the unlabeled case and
{TOP, headchild index, root non-terminal} in the la-
beled case. The labeled constituents in a tree are all
of the non-terminals and, for each, the positions of
the words it spans. We use the standard definitions
of precision, recall, and F-measure.5
5When extracting dependencies, we replaced all non-
punctuation POS labels with a generic label TAG to avoid con-
flating tagging errors with dependency errors. We also included
the structural changes that we imposed during preprocessing.
Results for constituent precision and recall were computed af-
ter we restored the trees to the original treebank structure.
799
Labeled Dep Unlabeled Dep Labeled Const
<=70 words <=40 Words
Model Prec/Rec Gain Prec/Rec Gain Prec Rec Prec Rec
1 Baseline 76.0 ? 82.1 ? 81.6 80.4 82.6 81.4
2 n(P,N,V) 78.4 2.4 83.6 1.5 83.1 82.5 84.1 83.4
3 n(A,D,N,P,V) 78.2 2.2 83.5 1.4 83.3 82.4 84.2 83.3
4 n(V) 77.8 1.8 82.9 0.8 82.3 81.6 83.1 82.2
5 m(V) 78.4 2.4 83.1 1.0 82.8 82.0 83.8 82.9
6 t(V) 77.6 1.6 82.7 0.6 82.4 81.4 83.2 82.3
7 p(V) 78.1 2.1 83.3 1.2 82.9 82.0 83.8 82.8
8 g(V) 76.3 0.3 82.2 0.1 81.6 80.6 82.7 81.7
9 n(A,D,N,V,P)+m(V) 79.0 3.0 84.0 1.9 83.9 83.2 84.7 84.1
10 n(P,N,V)+m(V) 78.9 2.9 83.7/83.8 1.6/1.7 83.6 82.8 84.6 83.7
11 n(A,D,N,V,P)+m(V)+p(V) 78.7 2.7 83.6 1.5 83.6 82.9 84.4 83.8
12 n(A,D,N,V,P)+p(V) 78.4 2.4 83.5/83.6 1.4/1.5 83.3 82.6 84.2 83.5
13 n(A,D,N,V,P)+g(A,D,N,V,P) 78.1 2.1 83.2 1.1 83.1 82.5 83.9 83.4
Table 3: Results after training morphological models during development. When precision and recall differ in labeled or unlabeled
dependencies, both scores are shown. Row 1 shows results on a baseline model containing almost no morphological information.
The subsequent rows represent a subset of the models with which we experimented: n(P,N,V) uses number for pronouns, nouns,
and verbs; n(A,D,N,P,V) uses number for adjectives, determiners, nouns, pronouns, and verbs; n(V) uses number for verbs; m(V)
uses mode for verbs; t(V) uses tense for verbs; p(V) uses person for verbs; g(V) uses gender for verbs; the models in rows 9?12
are combinations of these models, and in row 13, n(A,D,N,V,P) combines with g(A,D,N,V,P), which uses gender for adjectives,
determiners, nouns, verbs, and pronouns. The results of the best-performing model are in bold.
Labeled Dep Unlabeled Dep Labeled Const
<=70 words <=40 Words
Model Prec/Rec Prec/Rec Prec Rec Prec Rec
1 Baseline 77.0 82.5 81.7 80.8 83.1 82.0
2 n(A,D,N,V,P)+m(V) 79.4 83.9 83.9 83.4 85.1 84.4
3 RERANK 80.2 84.7 85.2 85.0 86.3 85.9
Table 4: Results after running the morphological and reranking models on test data. Row 1 is our baseline model. Row 2 is the
morphological model that scored highest during development. Row 3 gives the accuracy of the reranking approach, when applied
to n-best output from the model in Row 2.
5.1 The Effects of Morphology
In our first experiments, we trained over 50 mod-
els, incorporating different morphological informa-
tion into each in the way described in Section 3.1.
Prior to running the parsers, we trained the POS tag-
ger described in (Collins, 2002). The output from
the tagger was used to assign a POS label for un-
known words. We only attempted to parse sentences
under 70 words in length.
Table 3 describes some of the models we tried
during development and gives results for each. Our
baseline model, which we used to evaluate the ef-
fects of using morphology, was Model 1 (Collins,
1999) with a simple POS tagset containing almost
no morphological information. The morphologi-
cal models we show are meant to be representative
of both the highest-scoring models and the perfor-
mance of various morphological features. For in-
stance, we found that, in general, gender had only a
slight impact on the performance of the parser. Note
that gender is not a morphological attribute of Span-
ish verbs, and that the inclusion of verbal features,
particularly number, mode, and person, generated
the strongest-performing models in our experiments.
Table 4 shows the results of running two mod-
els on the test set: the baseline model and the best-
performing morphological model from the develop-
ment stage. This model uses the number and mode
of verbs, as well as the number of adjectives, deter-
miners, nouns, and pronouns.
The results in Tables 3 and 4 show that adding
some amount of morphological information to a
parsing model is beneficial. We found, however, that
adding more information does not always lead to im-
proved performance (see, for example, rows 11 and
13 in Table 3). Presumably this is because the tagset
grows too large.
Table 5 takes a closer look at the performance
800
of the best-performing morphological model in the
recovery of particular labeled dependencies. The
breakdown shows the top 15 dependencies in the
gold-standard trees across the entire training set.
Collectively, these dependencies represent around
72% of the dependencies seen in this data.
We see an extraordinary gain in the recovery of
some of these dependencies when we add morpho-
logical information. Among these are the two in-
volving postmodifiers to verbs. When examining the
output of the morphological model, we found that
much of this gain is due to the fact that there are two
non-terminal labels used in the treebank that specify
modal information of verbs they dominate (infiniti-
vals and gerunds): with insufficient morphological
information, the baseline parser was unable to dis-
tinguish regular verb phrases from these more spe-
cific verb phrases.
Some dependencies are particularly difficult for
the parser, such as that in which SBAR modifies
a noun ({GRUP TAG SBAR R}). We found that
around 20% of cases of this type in the training set
involve structures like el proceso de negociones que
(in English the process of negotiation that). This
type of structure is inherently difficult to disam-
biguate. In Spanish, such structures may be more
common than in English, since phrases involving
nominal modifiers to nouns, like negotiation pro-
cess, are always formed as noun + de + noun.
5.2 Experiments with Reranking
In the reranking experiments, we follow the proce-
dure described in (Collins and Koo, 2005) for cre-
ation of a training set with n-best parses for each
sentence. This method involves jack-knifing the
data: the training set of 2,800 sentences was parsed
in 200-sentence chunks by an n-best morphologi-
cal parser trained on the remaining 2,600 sentences.
This ensured that each sentence in the training data
had n-best output from a baseline model that was
not trained on that sentence. We used the optimal
morphological model (n(A,D,N,V,P)+m(V)) to gen-
erate the n-best lists, and we used the feature set de-
scribed in (Collins and Koo, 2005). The test results
are given in Table 4.6
6Note that we also created development sets for develop-
ment of the reranking approach, and for cross-validation of the
single parameter C in approach of (Bartlett et al, 2004).
Dependency Count Model Prec/Rec
Determiner modifier 9680 BL 95.0/95.4
SN GRUP ESPEC L (15.5%) M 95.4/95.7
Complement of SP 9052 BL 92.4/92.9
SP PREP SN R (14.5%) M 93.2/93.9
SP modifier to noun 4500 BL 83.9/78.1
GRUP TAG SP R (7.2%) M 82.9/79.9
Subject 3106 BL 77.7/86.1
S GV SN L (5.0%) M 83.1/87.5
Sentential head 2758 BL 75.0/75.0
TOP S (4.4%) M 79.7/79.7
S modifier under SBAR 2728 BL 83.3/82.1
SBAR CP S R (4.4%) M 86.0/84.7
SP modifier to verb 2685 BL 62.4/78.8
S GV SP R (4.3%) M 72.6/82.5
SN modifier to verb 2677 BL 71.6/75.6
S GV SN R (4.3%) M 81.0/83.0
Adjective postmodifier 2522 BL 76.3/83.6
GRUP TAG s R (4.0%) M 76.4/83.5
Adjective premodifier 980 BL 79.2/80.0
GRUP TAG s L (1.6%) M 80.1/79.3
SBAR modifier to noun 928 BL 62.2/60.6
GRUP TAG SBAR R (1.4%) M 61.3/60.8
Coordination 895 BL 65.2/72.7
S-CC2 S coord L (1.4%) M 66.7/74.2
Coordination 870 BL 52.4/56.1
S-CC1 S-CC2 S L (1.4%) M 60.3/63.6
Impersonal pronoun 804 BL 93.3/96.4
S GV MORF L (1.3%) M 92.0/95.6
SN modifier to noun 736 BL 47.3/39.5
GRUP TAG SN R (1.2%) M 51.7/50.8
Table 5: Labeled dependency accuracy for the top 15 depen-
dencies (representing around 72% of all dependencies) in the
gold-standard trees across all training data. The first column
shows the type and subtype, where the subtype is specified as
the 4-tuple {parent non-terminal, head non-terminal, modifier
non-terminal, direction}; the second column shows the count
for that subtype and the percent of the total that it represents
(where the total is 62,372) . The model BL is the baseline, and
M is the morphological model n(A,D,N,V,P)+m(V).
5.3 Statistical Significance
We tested the significance of the labeled precision
and recall results in Table 4 using the sign test.
When applying the sign test, for each sentence in
the test data we calculate the sentence-level F1 con-
stituent score for the two parses being compared.
This indicates whether one model performs better
on that sentence than the other model, or whether
the two models perform equally well, information
used by the sign test. All differences were found to
be statistically significant at the level p = 0.01.7
7When comparing the baseline model to the morphological
model on the 692 test sentences, F1 scores improved on 314
sentences, and became worse on 164 sentences. When com-
paring the baseline model to the reranked model, 358/157 sen-
801
6 Conclusions and Future Work
We have developed a statistical parsing model for
Spanish that performs at 85.1% F1 constituency ac-
curacy. We find that an approach that explicitly
represents some of the particular features of Span-
ish (i.e., its morphology) does indeed help in pars-
ing. Moreover, this approach is compatible with
the reranking approach, which uses general fea-
tures that were first developed for use in an En-
glish parser. In fact, our best parsing model com-
bines both the language-specific morphological fea-
tures and the non-specific reranking features. The
morphological features are local, being restricted to
dependencies between words in the parse tree; the
reranking features are more global, relying on larger
portions of parse structures. Thus, we see our final
model as combining the strengths of two comple-
mentary approaches.
We are curious to know the extent to which a
close analysis of the dependency errors made by the
baseline parser can be corrected by the development
of features tailored to addressing these problems.
Some preliminary investigation of this suggests that
we see much higher gains when using generic fea-
tures than these more specific ones, but we leave a
thorough investigation of this to future work. An-
other avenue for future investigation is to try using a
more sophisticated baseline model such as Collins?
Model 2, which incorporates both subcategorization
and complement/adjunct information. Finally, we
would like to use the Spanish parser in an applica-
tion such as machine translation.
Acknowledgements
We would like to thank Xavier Carreras for point-
ing us to the Spanish 3LB treebank and Montserrat
Civit for providing access to the data and answering
questions about it. We also gratefully acknowledge
the support of the National Science Foundation un-
der grants 0347631 and 0434222.
tences had improved/worse parses. When comparing the mor-
phological model to the reranked model, 199/106 sentences had
improved/worse parses.
References
Abhishek Arun and Frank Keller. 2005. Lexicalization in
crosslinguistic probabilistic parsing: the case of French.
ACL 2005, Ann Arbor, MI.
Peter Bartlett, Michael Collins, Ben Taskar, and David
McAllester. 2004. Exponentiated gradient algorithms for
large-margin structured classification. Proceedings of NIPS
2004.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine
n-best parsing and MaxEnt discriminative reranking. ACL
2005, Ann Arbor, MI.
David Chiang and Daniel M. Bikel. 2002. Recovering latent
information in treebanks. Proceedings of COLING-2002,
pages 183?189.
Montserrat Civit Torruella. 2000. Gu??a para la anotacio?n mor-
fosinta?ctica del corpus CLiC-TALP. X-Tract Working Paper,
WP-00/06.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. University of Pennsylvania.
Michael Collins, Jan Hajic, Lance Ranshaw, and Christoph Till-
man. 1999. A statistical parser for Czech. ACL 99.
Michael Collins. 2002. Discriminative training methods for
hidden Markov models: theory and experiments with per-
ceptron algorithms. EMNLP 2002.
Michael Collins and Terry Koo. 2005. Discriminative Rerank-
ing for Natural Language Parsing. Computational Linguis-
tics, 31(1):25?69.
C. Cortes and V. Vapnik. 1995. Support Vector Networks. Ma-
chine Learning, 20:273?297.
Amit Dubey and Frank Keller. 2003. Probabilistic parsing for
German using sister-head dependencies. ACL 2003, pp. 96?
103.
Amit Dubey. 2005. What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. ACL 2005,
Ann Arbor, MI.
Mark Johnson. 1998. PCFG Models of Linguistic Tree Repre-
sentations. Computational Linguistics, 24(4):613?632.
Roger Levy and Christopher Manning. 2003. Is it harder to
parse Chinese, or the Chinese Treebank? ACL 2003, pp.
439?446.
Antonio Moreno, Ralph Grishman, Susana Lo?pez, Fernando
Sa?nchez, and Satoshi Sekine. 2000. A treebank of Span-
ish and its application to parsing. The Proceedings of the
Workshop on Using Evaluation within HLT Programs: Re-
sults and Trends, Athens, Greece.
Borja Navarro, Montserrat Civit, Ma. Anto`nia Mart??, Raquel
Marcos, and Bele?n Ferna?ndez. 2003. Syntactic, semantic
and pragmatic annotation in Cast3LB. Shallow Processing
of Large Corpora (SProLaC), a Workshop of Corpus Lin-
guistics, 2003, Lancaster, UK.
802
c? 2003 Association for Computational Linguistics
Head-Driven Statistical Models for
Natural Language Parsing
Michael Collins?
MIT Computer Science and
Artificial Intelligence Laboratory
This article describes three statistical models for natural language parsing. The models extend
methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches
in which a parse tree is represented as the sequence of decisions corresponding to a head-centered,
top-down derivation of the tree. Independence assumptions then lead to parameters that encode
the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram
lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences
are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn
Wall Street Journal Treebank, showing that their accuracy is competitive with other models in
the literature. To gain a better understanding of the models, we also give results on different
constituent types, as well as a breakdown of precision/recall results in recovering various types of
dependencies. We analyze various characteristics of the models through experiments on parsing
accuracy, by collecting frequencies of various structures in the treebank, and through linguistically
motivated examples. Finally, we compare the models to others that have been applied to parsing
the treebank, aiming to give some explanation of the difference in performance of the various
models.
1. Introduction
Ambiguity is a central problem in natural language parsing. Combinatorial effects
mean that even relatively short sentences can receive a considerable number of parses
under a wide-coverage grammar. Statistical parsing approaches tackle the ambiguity
problem by assigning a probability to each parse tree, thereby ranking competing trees
in order of plausibility. In many statistical models the probability for each candidate
tree is calculated as a product of terms, each term corresponding to some substructure
within the tree. The choice of parameterization is essentially the choice of how to
represent parse trees. There are two critical questions regarding the parameterization
of a parsing approach:
1. Which linguistic objects (e.g., context-free rules, parse moves) should the
model?s parameters be associated with? In other words, which features
should be used to discriminate among alternative parse trees?
2. How can this choice be instantiated in a sound probabilistic model?
In this article we explore these issues within the framework of generative models,
more precisely, the history-based models originally introduced to parsing by Black
? MIT Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology,
545 Technology Square, Cambridge, MA 02139. E-mail: mcollins@ai.mit.edu.
590
Computational Linguistics Volume 29, Number 4
et al (1992). In a history-based model, a parse tree is represented as a sequence of
decisions, the decisions being made in some derivation of the tree. Each decision has
an associated probability, and the product of these probabilities defines a probability
distribution over possible derivations.
We first describe three parsing models based on this approach. The models were
originally introduced in Collins (1997); the current article1 gives considerably more
detail about the models and discusses them in greater depth. In Model 1 we show
one approach that extends methods from probabilistic context-free grammars (PCFGs)
to lexicalized grammars. Most importantly, the model has parameters corresponding
to dependencies between pairs of headwords. We also show how to incorporate a
?distance? measure into these models, by generalizing the model to a history-based
approach. The distance measure allows the model to learn a preference for close at-
tachment, or right-branching structures.
In Model 2, we extend the parser to make the complement/adjunct distinction,
which will be important for most applications using the output from the parser. Model
2 is also extended to have parameters corresponding directly to probability distribu-
tions over subcategorization frames for headwords. The new parameters lead to an
improvement in accuracy.
In Model 3 we give a probabilistic treatment of wh-movement that is loosely based
on the analysis of wh-movement in generalized phrase structure grammar (GPSG)
(Gazdar et al 1985). The output of the parser is now enhanced to show trace coin-
dexations in wh-movement cases. The parameters in this model are interesting in that
they correspond directly to the probability of propagating GPSG-style slash features
through parse trees, potentially allowing the model to learn island constraints.
In the three models a parse tree is represented as the sequence of decisions cor-
responding to a head-centered, top-down derivation of the tree. Independence as-
sumptions then follow naturally, leading to parameters that encode the X-bar schema,
subcategorization, ordering of complements, placement of adjuncts, lexical dependen-
cies, wh-movement, and preferences for close attachment. All of these preferences are
expressed by probabilities conditioned on lexical heads. For this reason we refer to the
models as head-driven statistical models.
We describe evaluation of the three models on the Penn Wall Street Journal Tree-
bank (Marcus, Santorini, and Marcinkiewicz 1993). Model 1 achieves 87.7% constituent
precision and 87.5% consituent recall on sentences of up to 100 words in length in sec-
tion 23 of the treebank, and Models 2 and 3 give further improvements to 88.3%
constituent precision and 88.0% constituent recall. These results are competitive with
those of other models that have been applied to parsing the Penn Treebank. Models 2
and 3 produce trees with information about wh-movement or subcategorization. Many
NLP applications will need this information to extract predicate-argument structure
from parse trees.
The rest of the article is structured as follows. Section 2 gives background material
on probabilistic context-free grammars and describes how rules can be ?lexicalized?
through the addition of headwords to parse trees. Section 3 introduces the three prob-
abilistic models. Section 4 describes various refinments to these models. Section 5
discusses issues of parameter estimation, the treatment of unknown words, and also
the parsing algorithm. Section 6 gives results evaluating the performance of the mod-
els on the Penn Wall Street Journal Treebank (Marcus, Santorini, and Marcinkiewicz
1993). Section 7 investigates various aspects of the models in more detail. We give a
1 Much of this article is an edited version of chapters 7 and 8 of Collins (1999).
591
Collins Head-Driven Statistical Models for NL Parsing
detailed analysis of the parser?s performance on treebank data, including results on
different constituent types. We also give a breakdown of precision and recall results
in recovering various types of dependencies. The intention is to give a better idea
of the strengths and weaknesses of the parsing models. Section 7 goes on to discuss
the distance features in the models, the implicit assumptions that the models make
about the treebank annotation style, and the way that context-free rules in the original
treebank are broken down, allowing the models to generalize by producing new rules
on test data examples. We analyze these phenomena through experiments on parsing
accuracy, by collecting frequencies of various structures in the treebank, and through
linguistically motivated examples. Finally, section 8 gives more discussion, by com-
paring the models to others that have been applied to parsing the treebank. We aim to
give some explanation of the differences in performance among the various models.
2. Background
2.1 Probabilistic Context-Free Grammars
Probabilistic context-free grammars are the starting point for the models in this arti-
cle. For this reason we briefly recap the theory behind nonlexicalized PCFGs, before
moving to the lexicalized case.
Following Hopcroft and Ullman (1979), we define a context-free grammar G as a
4-tuple (N,?, A, R), where N is a set of nonterminal symbols, ? is an alphabet, A is a
distinguished start symbol in N, and R is a finite set of rules, in which each rule is of
the form X ? ? for some X ? N, ? ? (N ??)?. The grammar defines a set of possible
strings in the language and also defines a set of possible leftmost derivations under
the grammar. Each derivation corresponds to a tree-sentence pair that is well formed
under the grammar.
A probabilistic context-free grammar is a simple modification of a context-free
grammar in which each rule in the grammar has an associated probability P(? | X).
This can be interpreted as the conditional probability of X?s being expanded using
the rule X ? ?, as opposed to one of the other possibilities for expanding X listed
in the grammar. The probability of a derivation is then a product of terms, each
term corresponding to a rule application in the derivation. The probability of a given
tree-sentence pair (T, S) derived by n applications of context-free rules LHSi ? RHSi
(where LHS stands for ?left-hand side,? RHS for ?right-hand side?), 1 ? i ? n, under
the PCFG is
P(T, S) =
n
?
i=1
P(RHSi | LHSi)
Booth and Thompson (1973) specify the conditions under which the PCFG does in fact
define a distribution over the possible derivations (trees) generated by the underlying
grammar. The first condition is that the rule probabilities define conditional distribu-
tions over how each nonterminal in the grammar can expand. The second is a technical
condition that guarantees that the stochastic process generating trees terminates in a
finite number of steps with probability one.
A central problem in PCFGs is to define the conditional probability P(? | X) for
each rule X ? ? in the grammar. A simple way to do this is to take counts from a
treebank and then to use the maximum-likelihood estimates:
P(? | X) = Count(X ? ?)
Count(X)
(1)
592
Computational Linguistics Volume 29, Number 4
If the treebank has actually been generated from a probabilistic context-free grammar
with the same rules and nonterminals as the model, then in the limit, as the training
sample size approaches infinity, the probability distribution implied by these estimates
will converge to the distribution of the underlying grammar.2
Once the model has been trained, we have a model that defines P(T, S) for any
sentence-tree pair in the grammar. The output on a new test sentence S is the most
likely tree under this model,
Tbest = arg max
T
P(T | S) = arg max
T
P(T, S)
P(S)
= arg max
T
P(T, S)
The parser itself is an algorithm that searches for the tree, Tbest, that maximizes P(T, S).
In the case of PCFGs, this can be accomplished using a variant of the CKY algorithm
applied to weighted grammars (providing that the PCFG can be converted to an equiv-
alent PCFG in Chomsky normal form); see, for example, Manning and Schu?tze (1999).
If the model probabilities P(T, S) are the same as the true distribution generating
training and test examples, returning the most likely tree under P(T, S) will be op-
timal in terms of minimizing the expected error rate (number of incorrect trees) on
newly drawn test examples. Hence if the data are generated by a PCFG, and there are
enough training examples for the maximum-likelihood estimates to converge to the
true values, then this parsing method will be optimal. In practice, these assumptions
cannot be verified and are arguably quite strong, but these limitations have not pre-
vented generative models from being successfully applied to many NLP and speech
tasks. (See Collins [2002] for a discussion of other ways of conceptualizing the parsing
problem.)
In the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), which is the
source of data for our experiments, the rules are either internal to the tree, where LHS
is a nonterminal and RHS is a string of one or more nonterminals, or lexical, where
LHS is a part-of-speech tag and RHS is a word. (See Figure 1 for an example.)
2.2 Lexicalized PCFGs
A PCFG can be lexicalized3 by associating a word w and a part-of-speech (POS) tag t
with each nonterminal X in the tree. (See Figure 2 for an example tree.)
The PCFG model can be applied to these lexicalized rules and trees in exactly the
same way as before. Whereas before the nonterminals were simple (for example, S or
NP), they are now extended to include a word and part-of-speech tag (for example,
S(bought,VBD) or NP(IBM,NNP)). Thus we write a nonterminal as X(x), where x =
?w, t? and X is a constituent label. Formally, nothing has changed, we have just vastly
increased the number of nonterminals in the grammar (by up to a factor of |V| ? |T |,
2 This point is actually more subtle than it first appears (we thank one of the anonymous reviewers for
pointing this out), and we were unable to find proofs of this property in the literature for PCFGs. The
rule probabilities for any nonterminal that appears with probability greater than zero in parse
derivations will converge to their underlying values, by the usual properties of maximum-likelihood
estimation for multinomial distributions. Assuming that the underlying PCFG generating training
examples meet both criteria in Booth and Thompson (1973), it can be shown that convergence of rule
probabilities implies that the distribution over trees will converge to that of the underlying PCFG, at
least when Kullback-Liebler divergence or the infinity norm is taken to be the measure of distance
between the two distributions. Thanks to Tommi Jaakkola and Nathan Srebro for discussions on this
topic.
3 We find lexical heads in Penn Treebank data using the rules described in Appendix A of Collins (1999).
The rules are a modified version of a head table provided by David Magerman and used in the parser
described in Magerman (1995).
593
Collins Head-Driven Statistical Models for NL Parsing
Internal rules Lexical rules
TOP ? S JJ ? Last
S ? NP NP VP NN ? week
NP ? JJ NN NNP ? IBM
NP ? NNP VBD ? bought
VP ? VBD NP NNP ? Lotus
NP ? NNP
Figure 1
A nonlexicalized parse tree and a list of the rules it contains.
Internal Rules:
TOP ? S(bought,VBD)
S(bought,VBD) ? NP(week,NN) NP(IBM,NNP) VP(bought,VBD)
NP(week,NN) ? JJ(Last,JJ) NN(week,NN)
NP(IBM,NNP) ? NNP(IBM,NNP)
VP(bought,VBD) ? VBD(bought,VBD) NP(Lotus,NNP)
NP(Lotus,NNP) ? NNP(Lotus,NNP)
Lexical Rules:
JJ(Last,JJ) ? Last
NN(week,NN) ? week
NNP(IBM,NNP) ? IBM
VBD(bought,VBD) ? bought
NNP(Lotus,NN) ? Lotus
Figure 2
A lexicalized parse tree and a list of the rules it contains.
where |V| is the number of words in the vocabulary and |T | is the number of part-of-
speech tags).
Although nothing has changed from a formal point of view, the practical conse-
quences of expanding the number of nonterminals quickly become apparent when
one is attempting to define a method for parameter estimation. The simplest solution
would be to use the maximum-likelihood estimate as in equation (1), for example,
594
Computational Linguistics Volume 29, Number 4
estimating the probability associated with S(bought,VBD) ? NP(week,NN) NP(IBM,NNP)
VP(bought,VBD) as
P(NP(week,NN) NP(IBM,NNP) VP(bought,VBD) | S(bought,VBD)) =
Count(S(bought,VBD) ? NP(week,NN) NP(IBM,NNP) VP(bought,VBD))
Count(S(bought,VBD))
But the addition of lexical items makes the statistics for this estimate very sparse: The
count for the denominator is likely to be relatively low, and the number of outcomes
(possible lexicalized RHSs) is huge, meaning that the numerator is very likely to be
zero. Predicting the whole lexicalized rule in one go is too big a step.
One way to overcome these sparse-data problems is to break down the gener-
ation of the RHS of each rule into a sequence of smaller steps, and then to make
independence assumptions to reduce the number of parameters in the model. The de-
composition of rules should aim to meet two criteria. First, the steps should be small
enough for the parameter estimation problem to be feasible (i.e., in terms of having suf-
ficient training data to train the model, providing that smoothing techniques are used
to mitigate remaining sparse-data problems). Second, the independence assumptions
made should be linguistically plausible. In the next sections we describe three statisti-
cal parsing models that have an increasing degree of linguistic sophistication. Model 1
uses a decomposition of which parameters corresponding to lexical dependencies are a
natural result. The model also incorporates a preference for right-branching structures
through conditioning on ?distance? features. Model 2 extends the decomposition to
include a step in which subcategorization frames are chosen probabilistically. Model 3
handles wh-movement by adding parameters corresponding to slash categories being
passed from the parent of the rule to one of its children or being discharged as a trace.
3. Three Probabilistic Models for Parsing
3.1 Model 1
This section describes how the generation of the RHS of a rule is broken down into a
sequence of smaller steps in model 1. The first thing to note is that each internal rule
in a lexicalized PCFG has the form4
P(h) ? Ln(ln) . . . L1(l1)H(h)R1(r1) . . . Rm(rm) (2)
H is the head-child of the rule, which inherits the headword/tag pair h from its parent
P. L1(l1) . . . Ln(ln) and R1(r1) . . . Rm(rm) are left and right modifiers of H. Either n or m
may be zero, and n = m = 0 for unary rules. Figure 2 shows a tree that will be used
as an example throughout this article. We will extend the left and right sequences to
include a terminating STOP symbol, allowing a Markov process to model the left and
right sequences. Thus Ln+1 = Rm+1 = STOP.
For example, in S(bought,VBD) ? NP(week,NN) NP(IBM,NNP) VP(bought,VBD):
n = 2 m = 0 P = S
H = VP L1 = NP L2 = NP
L3 = STOP R1 = STOP h = ?bought, VBD?
l1 = ?IBM, NNP? l2 = ?week, NN?
4 With the exception of the top rule in the tree, which has the form TOP ? H(h).
595
Collins Head-Driven Statistical Models for NL Parsing
Note that lexical rules, in contrast to the internal rules, are completely deterministic.
They always take the form
P(h) ? w
where P is a part-of-speech tag, h is a word-tag pair ?w, t?, and the rule rewrites to just
the word w. (See Figure 2 for examples of lexical rules.) Formally, we will always take
a lexicalized nonterminal P(h) to expand deterministically (with probability one) in
this way if P is a part-of-speech symbol. Thus for the parsing models we require the
nonterminal labels to be partitioned into two sets: part-of-speech symbols and other
nonterminals. Internal rules always have an LHS in which P is not a part-of-speech
symbol. Because lexicalized rules are deterministic, they will not be discussed in the
remainder of this article: All of the modeling choices concern internal rules.
The probability of an internal rule can be rewritten (exactly) using the chain rule
of probabilities:
P(Ln+1(ln+1) . . . L1(l1)H(h)R1(r1) . . . Rm+1(rm+1) | P, h) =
Ph(H | P, h)?
?
i=1...n+1
Pl(Li(li) | L1(l1) . . . Li?1(li?1), P, h, H)?
?
j=1...m+1
Pr(Rj(rj) | L1(l1) . . . Ln+1(ln+1), R1(r1) . . . Rj?1(rj?1), P, h, H)
(The subscripts h, l and r are used to denote the head, left-modifier, and right-modifier
parameter types, respectively.) Next, we make the assumption that the modifiers are
generated independently of each other:
Pl(Li(li) | L1(l1) . . . Li?1(li?1), P, h, H) = Pl(Li(li) | P, h, H)
(3)
Pr(Rj(rj) | L1(l1) . . . Ln+1(ln+1), R1(r1) . . . Rj?1(rj?1), P, h, H) = Pr(Rj(rj) | P, h, H)
(4)
In summary, the generation of the RHS of a rule such as (2), given the LHS, has
been decomposed into three steps:5
1. Generate the head constituent label of the phrase, with probability
Ph(H | P, h).
2. Generate modifiers to the left of the head with probability
?
i=1...n+1 Pl(Li(li) | P, h, H), where Ln+1(ln+1) = STOP. The STOP symbol is
added to the vocabulary of nonterminals, and the model stops
generating left modifiers when the STOP symbol is generated.
3. Generate modifiers to the right of the head with probability
?
i=1...m+1 Pr(Ri(ri) | P, h, H). We define Rm+1(rm+1) as STOP.
For example, the probability of the rule S(bought) ? NP(week) NP(IBM) VP(bought)
would be estimated as
Ph(VP | S,bought)? Pl(NP(IBM) | S,VP,bought)? Pl(NP(week) | S,VP,bought)
?Pl(STOP | S,VP,bought)? Pr(STOP | S,VP,bought)
5 An exception is the first rule in the tree, TOP ? H(h), which has probability PTOP(H, h|TOP)
596
Computational Linguistics Volume 29, Number 4
In this example, and in the examples in the rest of the article, for brevity we omit
the part-of-speech tags associated with words, writing, for example S(bought) rather
than S(bought,VBD). We emphasize that throughout the models in this article, each
word is always paired with its part of speech, either when the word is generated or
when the word is being conditioned upon.
3.1.1 Adding Distance to the Model. In this section we first describe how the model
can be extended to be ?history-based.? We then show how this extension can be
utilized in incorporating ?distance? features into the model.
Black et al (1992) originally introduced history-based models for parsing. Equa-
tions (3) and (4) of the current article made the independence assumption that each
modifier is generated independently of the others (i.e., that the modifiers are generated
independently of everything except P, H, and h). In general, however, the probability
of generating each modifier could depend on any function of the previous modifiers,
head/parent category, and headword. Moreover, if the top-down derivation order is
fully specified, then the probability of generating a modifier can be conditioned on any
structure that has been previously generated. The remainder of this article assumes
that the derivation order is depth-first: that is, each modifier recursively generates the
subtree below it before the next modifier is generated. (Figure 3 gives an example that
illustrates this.)
The models in Collins (1996) showed that the distance between words standing in
head-modifier relationships was important, in particular, that it is important to capture
a preference for right-branching structures (which almost translates into a preference
for dependencies between adjacent words) and a preference for dependencies not to
cross a verb. In this section we describe how this information can be incorporated
into model 1. In section 7.2, we describe experiments that evaluate the effect of these
features on parsing accuracy.
Figure 3
A partially completed tree derived depth-first. ?????? marks the position of the next modifier
to be generated?it could be a nonterminal/headword/head-tag triple, or the STOP symbol.
The distribution over possible symbols in this position could be conditioned on any
previously generated structure, that is, any structure appearing in the figure.
597
Collins Head-Driven Statistical Models for NL Parsing
Figure 4
The next child, R3(r3), is generated with probability P(R3(r3) | P, H, h, distancer(2)). The distance
is a function of the surface string below previous modifiers R1 and R2. In principle the model
could condition on any structure dominated by H, R1, or R2 (or, for that matter, on any
structure previously generated elsewhere in the tree).
Distance can be incorporated into the model by modifying the independence as-
sumptions so that each modifier has a limited dependence on the previous modifiers:
Pl(Li(li) | H, P, h, L1(l1) . . . Li?1(li?1)) = Pl(Li(li) | H, P, h, distancel(i ? 1))
(5)
Pr(Ri(ri) | H, P, h, R1(r1) . . . Ri?1(ri?1)) = Pr(Ri(ri) | H, P, h, distancer(i ? 1))
(6)
Here distancel and distancer are functions of the surface string below the previous
modifiers. (See Figure 4 for illustration.) The distance measure is similar to that in
Collins (1996), a vector with the following two elements: (1) Is the string of zero
length? (2) Does the string contain a verb? The first feature allows the model to learn
a preference for right-branching structures. The second feature6 allows the model to
learn a preference for modification of the most recent verb.7
3.2 Model 2: The Complement/Adjunct Distinction and Subcategorization
The tree depicted in Figure 2 illustrates the importance of the complement/adjunct
distinction. It would be useful to identify IBM as a subject and Last week as an adjunct
(temporal modifier), but this distinction is not made in the tree, as both NPs are in
the same position8 (sisters to a VP under an S node). From here on we will identify
complements9 by attaching a -C suffix to nonterminals. Figure 5 shows the tree in
Figure 2 with added complement markings.
A postprocessing stage could add this detail to the parser output, but there are a
couple of reasons for making the distinction while parsing. First, identifying comple-
ments is complex enough to warrant a probabilistic treatment. Lexical information is
needed (for example, knowledge that week is likely to be a temporal modifier). Knowl-
edge about subcategorization preferences (for example, that a verb takes exactly one
subject) is also required. For example, week can sometimes be a subject, as in Last week
was a good one, so the model must balance the preference for having a subject against
6 Note that this feature means that dynamic programming parsing algorithms for the model must keep
track of whether each constituent does or does not have a verb in the string to the right or left of its
head. See Collins (1999) for a full description of the parsing algorithms.
7 In the models described in Collins (1997), there was a third question concerning punctuation: (3) Does
the string contain 0, 1, 2 or more than 2 commas? (where a comma is anything tagged as ?,? or ?:?).
The model described in this article has a cleaner incorporation of punctuation into the generative
process, as described in section 4.3.
8 Except that IBM is closer to the VP, but note that IBM is also the subject in IBM last week bought Lotus.
9 We use the term complement in a broad sense that includes both complements and specifiers under the
terminology of government and binding.
598
Computational Linguistics Volume 29, Number 4
Figure 5
A tree with the -C suffix used to identify complements. IBM and Lotus are in subject and
object position, respectively. Last week is an adjunct.
Figure 6
Two examples in which the assumption that modifiers are generated independently of one
another leads to errors. In (1) the probability of generating both Dreyfus and fund as subjects,
P(NP-C(Dreyfus) | S,VP,was) ? P(NP-C(fund) | S,VP,was), is unreasonably high. (2) is similar:
P(NP-C(bill),VP-C(funding) | VP,VB,was) = P(NP-C(bill) | VP,VB,was) ?
P(VP-C(funding) | VP,VB,was) is a bad independence assumption.
the relative improbability of week?s being the headword of a subject. These problems
are not restricted to NPs; compare The spokeswoman said (SBAR that the asbestos was dan-
gerous) with Bonds beat short-term investments (SBAR because the market is down), in which
an SBAR headed by that is a complement, but an SBAR headed by because is an adjunct.
A second reason for incorporating the complement/adjunct distinction into the
parsing model is that this may help parsing accuracy. The assumption that comple-
ments are generated independently of one another often leads to incorrect parses. (See
Figure 6 for examples.)
3.2.1 Identifying Complements and Adjuncts in the Penn Treebank. We add the -C
suffix to all nonterminals in training data that satisfy the following conditions:
1. The nonterminal must be (1) an NP, SBAR, or S whose parent is an S; (2)
an NP, SBAR, S, or VP whose parent is a VP; or (3) an S whose parent is
an SBAR.
599
Collins Head-Driven Statistical Models for NL Parsing
2. The nonterminal must not have one of the following semantic tags: ADV,
VOC, BNF, DIR, EXT, LOC, MNR, TMP, CLR or PRP. See Marcus et al
(1994) for an explanation of what these tags signify. For example, the NP
Last week in figure 2 would have the TMP (temporal) tag, and the SBAR in
(SBAR because the market is down) would have the ADV (adverbial) tag.
3. The nonterminal must not be on the RHS of a coordinated phrase. For
example, in the rule S ? S CC S, the two child Ss would not be marked
as complements.
In addition, the first child following the head of a prepositional phrase is marked as
a complement.
3.2.2 Probabilities over Subcategorization Frames. Model 1 could be retrained on
training data with the enhanced set of nonterminals, and it might learn the lexical
properties that distinguish complements and adjuncts (IBM vs. week, or that vs. because).
It would still suffer, however, from the bad independence assumptions illustrated in
Figure 6. To solve these kinds of problems, the generative process is extended to
include a probabilistic choice of left and right subcategorization frames:
1. Choose a head H with probability Ph(H | P, h).
2. Choose left and right subcategorization frames, LC and RC, with
probabilities Plc(LC | P, H, h) and Prc(RC | P, H, h). Each subcategorization
frame is a multiset10 specifying the complements that the head requires
in its left or right modifiers.
3. Generate the left and right modifiers with probabilities Pl(Li(li) |
H, P, h, distancel(i ? 1), LC) and Pr(Ri(ri) | H, P, h, distancer(i ? 1), RC),
respectively.
Thus the subcategorization requirements are added to the conditioning context. As
complements are generated they are removed from the appropriate subcategorization
multiset. Most importantly, the probability of generating the STOP symbol will be zero
when the subcategorization frame is non-empty, and the probability of generating a
particular complement will be zero when that complement is not in the subcatego-
rization frame; thus all and only the required complements will be generated.
The probability of the phrase S(bought) ? NP(week) NP-C(IBM) VP(bought) is
now
Ph(VP | S,bought)? Plc({NP-C} | S,VP,bought)? Prc({} | S,VP,bought)?
Pl(NP-C(IBM) | S,VP,bought, {NP-C})? Pl(NP(week) | S,VP,bought, {})?
Pl(STOP | S,VP,bought, {})? Pr(STOP | S,VP,bought, {})
Here the head initially decides to take a single NP-C (subject) to its left and no com-
plements to its right. NP-C(IBM) is immediately generated as the required subject, and
NP-C is removed from LC, leaving it empty when the next modifier, NP(week), is gen-
erated. The incorrect structures in Figure 6 should now have low probability, because
Plc({NP-C,NP-C} | S,VP,was) and Prc({NP-C,VP-C} | VP,VB,was) should be small.
10 A multiset, or bag, is a set that may contain duplicate nonterminal labels.
600
Computational Linguistics Volume 29, Number 4
3.3 Model 3: Traces and Wh-Movement
Another obstacle to extracting predicate-argument structure from parse trees is wh-
movement. This section describes a probabilistic treatment of extraction from relative
clauses. Noun phrases are most often extracted from subject position, object position,
or from within PPs:
(1) The store (SBAR that TRACE bought Lotus)
(2) The store (SBAR that IBM bought TRACE)
(3) The store (SBAR that IBM bought Lotus from TRACE)
It might be possible to write rule-based patterns that identify traces in a parse tree.
We argue again, however, that this task is best integrated into the parser: The task
is complex enough to warrant a probabilistic treatment, and integration may help
parsing accuracy. A couple of complexities are that modification by an SBAR does not
always involve extraction (e.g., the fact (SBAR that besoboru is played with a ball and a bat)),
and it is not uncommon for extraction to occur through several constituents (e.g., The
changes (SBAR that he said the government was prepared to make TRACE)).
One hope is that an integrated treatment of traces will improve the parameteri-
zation of the model. In particular, the subcategorization probabilities are smeared by
extraction. In examples (1), (2), and (3), bought is a transitive verb; but without knowl-
edge of traces, example (2) in training data will contribute to the probability of bought?s
being an intransitive verb.
Formalisms similar to GPSG (Gazdar et al 1985) handle wh-movement by adding
a gap feature to each nonterminal in the tree and propagating gaps through the tree
until they are finally discharged as a trace complement (see Figure 7). In extraction
cases the Penn Treebank annotation coindexes a TRACE with the WHNP head of the SBAR,
so it is straightforward to add this information to trees in training data.
(1) NP ? NP SBAR(+gap)
(2) SBAR(+gap) ? WHNP S-C(+gap)
(3) S(+gap) ? NP-C VP(+gap)
(4) VP(+gap) ? VB TRACE NP
Figure 7
A +gap feature can be added to nonterminals to describe wh-movement. The top-level NP
initially generates an SBAR modifier but specifies that it must contain an NP trace by adding
the +gap feature. The gap is then passed down through the tree, until it is discharged as a
TRACE complement to the right of bought.
601
Collins Head-Driven Statistical Models for NL Parsing
Given that the LHS of the rule has a gap, there are three ways that the gap can
be passed down to the RHS:
Head: The gap is passed to the head of the phrase, as in rule (3) in Figure 7.
Left, Right: The gap is passed on recursively to one of the left or right modifiers
of the head or is discharged as a TRACE argument to the left or right of
the head. In rule (2) in Figure 7, it is passed on to a right modifier, the S
complement. In rule (4), a TRACE is generated to the right of the head VB.
We specify a parameter type Pg(G |P, h, H) where G is either Head, Left, or Right. The
generative process is extended to choose among these cases after generating the head
of the phrase. The rest of the phrase is then generated in different ways depending
on how the gap is propagated. In the Head case the left and right modifiers are
generated as normal. In the Left and Right cases a +gap requirement is added to either
the left or right SUBCAT variable. This requirement is fulfilled (and removed from
the subcategorization list) when either a trace or a modifier nonterminal that has the
+gap feature, is generated. For example, rule (2) in Figure 7, SBAR(that)(+gap) ?
WHNP(that) S-C(bought)(+gap), has probability
Ph(WHNP | SBAR,that)? Pg(Right | SBAR,WHNP,that)? Plc({} | SBAR,WHNP,that)?
Prc({S-C} | SBAR,WHNP,that)? Pr(S-C(bought)(+gap) | SBAR,WHNP,that, {S-C,+gap})?
Pr(STOP | SBAR,WHNP,that, {})? Pl(STOP | SBAR,WHNP,that, {})
Rule (4), VP(bought)(+gap) ? VB(bought) TRACE NP(week), has probability
Ph(VB | VP,bought)? Pg(Right | VP,bought,VB)? Plc({} | VP,bought,VB)?
Prc({NP-C} | VP,bought,VB)? Pr(TRACE | VP,bought,VB, {NP-C, +gap})?
Pr(NP(week) | VP,bought,VB, {})? Pl(STOP | VP,bought,VB, {})?
Pr(STOP | VP,bought,VB, {})
In rule (2), Right is chosen, so the +gap requirement is added to RC. Generation of
S-C(bought)(+gap) fulfills both the S-C and +gap requirements in RC. In rule (4),
Right is chosen again. Note that generation of TRACE satisfies both the NP-C and +gap
subcategorization requirements.
4. Special Cases: Linguistically Motivated Refinements to the Models
Sections 3.1 to 3.3 described the basic framework for the parsing models in this article.
In this section we describe how some linguistic phenomena (nonrecursive NPs and
coordination, for example) clearly violate the independence assumptions of the general
models. We describe a number of these special cases, in each instance arguing that the
phenomenon violates the independence assumptions, then describing how the model
can be refined to deal with the problem.
4.1 Nonrecursive NPs
We define nonrecursive NPs (from here on referred to as base-NPs and labeled NPB
rather than NP) as NPs that do not directly dominate an NP themselves, unless the
dominated NP is a possessive NP (i.e., it directly dominates a POS-tag POS). Figure 8
gives some examples. Base-NPs deserve special treatment for three reasons:
? The boundaries of base-NPs are often strongly marked. In particular, the
start points of base-NPs are often marked with a determiner or another
602
Computational Linguistics Volume 29, Number 4
Figure 8
Three examples of structures with base-NPs.
distinctive item, such as an adjective. Because of this, the probability of
generating the STOP symbol should be greatly increased when the
previous modifier is, for example, a determiner. As they stand, the
independence assumptions in the three models lose this information. The
probability of NPB(dog) ? DT(the) NN(dog) would be estimated as11
Ph(NN | NPB,dog)? Pl(DT(the) | NPB,NN,dog)?
Pl(STOP | NPB,NN,dog)? Pr(STOP | NPB,NN,dog)
In making the independence assumption
Pl(STOP | DT(the), NPB,NN,dog) = Pl(STOP | NPB,NN,dog)
the model will fail to learn that the STOP symbol is very likely to follow
a determiner. As a result, the model will assign unreasonably high
probabilities to NPs such as [NP yesterday the dog] in sentences such as
Yesterday the dog barked.
? The annotation standard in the treebank leaves the internal structure of
base-NPs underspecified. For example, both pet food volume (where pet
modifies food and food modifies volume) and vanilla ice cream (where both
vanilla and ice modify cream) would have the structure NPB ? NN NN NN.
Because of this, there is no reason to believe that modifiers within NPBs
are dependent on the head rather than the previous modifier. In fact, if it
so happened that a majority of phrases were like pet food volume, then
conditioning on the previous modifier rather than the head would be
preferable.
? In general it is important (in particular for the distance measure to be
effective) to have different nonterminal labels for what are effectively
different X-bar levels. (See section 7.3.2 for further discussion.)
For these reasons the following modifications are made to the models:
? The nonterminal label for base-NPs is changed from NP to NPB. For
consistency, whenever an NP is seen with no pre- or postmodifiers, an
NPB level is added. For example, [S [NP the dog] [VP barks] ] would
be transformed into [S [NP [NPB the dog] ] [VP barks ] ]. These
?extra? NPBs are removed before scoring the output of the parser against
the treebank.
11 For simplicity, we give probability terms under model 1 with no distance variables; the probability
terms with distance variables, or for models 2 and 3, will be similar, but with the addition of various
pieces of conditioning information.
603
Collins Head-Driven Statistical Models for NL Parsing
? The independence assumptions are different when the parent
nonterminal is an NPB. Specifically, equations (5) and (6) are modified to
be
Pl(Li(li) | H, P, h, L1(l1) . . . Li?1(li?1)) = Pl(Li(li) | P, Li?1(li?1))
Pr(Ri(ri) | H, P, h, R1(r1) . . . Ri?1(ri?1)) = Pr(Ri(ri) | P, Ri?1(ri?1))
The modifier and previous-modifier nonterminals are always adjacent, so
the distance variable is constant and is omitted. For the purposes of this
model, L0(l0) and R0(r0) are defined to be H(h). The probability of the
previous example is now
Ph(NN | NPB,dog)? Pl(DT(the) | NPB,NN,dog)?
Pl(STOP | NPB,DT,the)? Pr(STOP | NPB,NN,dog)
Presumably Pl(STOP | NPB,DT,the) will be very close to one.
4.2 Coordination
Coordination constructions are another example in which the independence assump-
tions in the basic models fail badly (at least given the current annotation method in
the treebank). Figure 9 shows how coordination is annotated in the treebank.12 To
use an example to illustrate the problems, take the rule NP(man) ? NP(man) CC(and)
NP(dog), which has probability
Ph(NP | NP,man)? Pl(STOP | NP,NP,man)? Pr(CC(and) | NP,NP,man)?
Pr(NP(dog) | NP,NP,man)? Pr(STOP | NP,NP,man)
The independence assumptions mean that the model fails to learn that there is always
exactly one phrase following the coordinator (CC). The basic probability models will
give much too high probabilities to unlikely phrases such as NP ? NP CC or NP ?
NP CC NP NP. For this reason we alter the generative process to allow generation of
both the coordinator and the following phrase in one step; instead of just generating a
nonterminal at each step, a nonterminal and a binary-valued coord flag are generated.
coord = 1 if there is a coordination relationship. In the generative process, generation
of a coord = 1 flag along with a modifier triggers an additional step in the generative
Figure 9
(a) The generic way of annotating coordination in the treebank. (b) and (c) show specific
examples (with base-NPs added as described in section 4.1). Note that the first item of the
conjunct is taken as the head of the phrase.
12 See Appendix A of Collins (1999) for a description of how the head rules treat phrases involving
coordination.
604
Computational Linguistics Volume 29, Number 4
process, namely, the generation of the coordinator tag/word pair, parameterized by
the Pcc parameter. For the preceding example this would give probability
Ph(NP | NP,man)? Pl(STOP | NP,NP,man)? Pr(NP(dog), coord=1 | NP,NP,man)?
Pr(STOP | NP,NP,man)? Pcc(CC,and | NP,NP,NP,man,dog)
Note the new type of parameter, Pcc, for the generation of the coordinator word and
POS tag. The generation of coord=1 along with NP(dog) in the example implicitly
requires generation of a coordinator tag/word pair through the Pcc parameter. The
generation of this tag/word pair is conditioned on the two words in the coordination
dependency (man and dog in the example) and the label on their relationship (NP,NP,NP
in the example, representing NP coordination).
The coord flag is implicitly zero when normal nonterminals are generated; for ex-
ample, the phrase S(bought) ? NP(week) NP(IBM) VP(bought) now has probability
Ph(VP | S,bought)? Pl(NP(IBM),coord=0 | S,VP,bought)?
Pl(NP(week),coord=0 | S,VP,bought)? Pl(STOP | S,VP,bought)?
Pr(STOP | S,VP,bought)
4.3 Punctuation
This section describes our treatment of ?punctuation? in the model, where ?punctu-
ation? is used to refer to words tagged as a comma or colon. Previous work?the
generative models described in Collins (1996) and the earlier version of these mod-
els described in Collins (1997)?conditioned on punctuation as surface features of the
string, treating it quite differently from lexical items. In particular, the model in Collins
(1997) failed to generate punctuation, a deficiency of the model. This section describes
how punctuation is integrated into the generative models.
Our first step is to raise punctuation as high in the parse trees as possible. Punc-
tuation at the beginning or end of sentences is removed from the training/test data
altogether.13 All punctuation items apart from those tagged as comma or colon (items
such as quotation marks and periods, tagged ? ? or . ) are removed altogether. These
transformations mean that punctuation always appears between two nonterminals, as
opposed to appearing at the end of a phrase. (See Figure 10 for an example.)
Figure 10
A parse tree before and after punctuation transformations.
13 As one of the anonymous reviewers of this article pointed out, this choice of discarding the
sentence-final punctuation may not be optimal, as the final punctuation mark may well carry useful
information about the sentence structure.
605
Collins Head-Driven Statistical Models for NL Parsing
Punctuation is then treated in a very similar way to coordination: Our intuition
is that there is a strong dependency between the punctuation mark and the modi-
fier generated after it. Punctuation is therefore generated with the following phrase
through a punc flag that is similar to the coord flag (a binary-valued feature equal to
one if a punctuation mark is generated with the following phrase).
Under this model, NP(Vinken) ? NPB(Vinken) ,(,) ADJP(old) would have
probability
Ph(NPB | NP,Vinken)? Pl(STOP | NP,NPB,Vinken)?
Pr(ADJP(old),coord=0,punc=1 | NP,NPB,Vinken)?
Pr(STOP | NP,NPB,bought)? Pp(, , | NP,NPB,ADJP,Vinken,old) (7)
Pp is a new parameter type for generation of punctuation tag/word pairs. The genera-
tion of punc=1 along with ADJP(old) in the example implicitly requires generation of a
punctuation tag/word pair through the Pp parameter. The generation of this tag/word
pair is conditioned on the two words in the punctuation dependency (Vinken and old
in the example) and the label on their relationship (NP,NPB,ADJP in the example.)
4.4 Sentences with Empty (PRO) Subjects
Sentences in the treebank occur frequently with PRO subjects that may or may not be
controlled: As the treebank annotation currently stands, the nonterminal is S whether
or not a sentence has an overt subject. This is a problem for the subcategorization prob-
abilities in models 2 and 3: The probability of having zero subjects, Plc({} | S, VP,
verb), will be fairly high because of this. In addition, sentences with and without sub-
jects appear in quite different syntactic environments. For these reasons we modify
the nonterminal for sentences without subjects to be SG (see figure 11). The resulting
model has a cleaner division of subcategorization: Plc({NP-C} | S, VP, verb) ? 1 and
Plc({NP-C} | SG, VP, verb) = 0. The model will learn probabilistically the environ-
ments in which S and SG are likely to appear.
4.5 A Punctuation Constraint
As a final step, we use the rule concerning punctuation introduced in Collins (1996)
to impose a constraint as follows. If for any constituent Z in the chart Z ? <..X Y..>
two of its children X and Y are separated by a comma, then the last word in Y must be
directly followed by a comma, or must be the last word in the sentence. In training
data 96% of commas follow this rule. The rule has the benefit of improving efficiency
by reducing the number of constituents in the chart. It would be preferable to develop
a probabilistic analog of this rule, but we leave this to future research.
Figure 11
(a) The treebank annotates sentences with empty subjects with an empty -NONE- element
under subject position; (b) in training (and for evaluation), this null element is removed; (c) in
models 2 and 3, sentences without subjects are changed to have a nonterminal SG.
606
Computational Linguistics Volume 29, Number 4
Table 1
The conditioning variables for each level of back-off. For example, Ph estimation interpolates
e1 = Ph(H | P, w, t), e2 = Ph(H | P, t), and e3 = Ph(H | P). ? is the distance measure.
Back-off Ph(H | . . .) Pg(G | . . .) PL1(Li(lti), c, p | . . .) PL2(lwi | . . .)
level Plc(LC | . . .) PR1(Ri(rti), c, p | . . .) PR2(rwi | . . .)
Prc(RC | . . .)
1 P, w, t P, H, w, t P, H, w, t, ?, LC Li, lti, c, p, P, H, w, t, ?, LC
2 P, t P, H, t P, H, t, ?, LC Li, lti, c, p, P, H, t, ?, LC
3 P P, H P, H, ?, LC lti
5. Practical Issues
5.1 Parameter Estimation
Table 1 shows the various levels of back-off for each type of parameter in the model.
Note that we decompose PL(Li(lwi, lti), c, p | P, H, w, t,?, LC) (where lwi and lti are the
word and POS tag generated with nonterminal Li, c and p are the coord and punc
flags associated with the nonterminal, and ? is the distance measure) into the product
PL1(Li(lti), c, p | P, H, w, t,?, LC)? PL2(lwi | Li, lti, c, p, P, H, w, t,?, LC)
These two probabilities are then smoothed separately. Eisner (1996b) originally used
POS tags to smooth a generative model in this way. In each case the final estimate is
e = ?1e1 + (1 ? ?1)(?2e2 + (1 ? ?2)e3)
where e1, e2, and e3 are maximum-likelihood estimates with the context at levels 1, 2,
and 3 in the table, and ?1, ?2 and ?3 are smoothing parameters, where 0 ? ?i ? 1. We
use the smoothing method described in Bikel et al (1997), which is derived from a
method described in Witten and Bell (1991). First, say that the most specific estimate
e1 = n1f1 ; that is, f1 is the value of the denominator count in the relative frequency
estimate. Second, define u1 to be the number of distinct outcomes seen in the f1 events
in training data. The variable u1 can take any value from one to f1 inclusive. Then we
set
?1 =
f1
f1 + 5u1
Analogous definitions for f2 and u2 lead to ?2 =
f2
f2+5u2
. The coefficient five was chosen
to maximize accuracy on the development set, section 0 of the treebank (in practice it
was found that any value in the range 2?5 gave a very similar level of performance).
5.2 Unknown Words and Part-of-Speech Tagging
All words occurring less than six times14 in training data, and words in test data that
have never been seen in training, are replaced with the UNKNOWN token. This allows
the model to handle robustly the statistics for rare or new words. Words in test data
that have not been seen in training are deterministically assigned the POS tag that is
assigned by the tagger described in Ratnaparkhi (1996). As a preprocessing step, the
14 In Collins (1999) we erroneously stated that all words occuring less than five times in training data
were classified as ?unknown.? Thanks to Dan Bikel for pointing out this error.
607
Collins Head-Driven Statistical Models for NL Parsing
tagger is used to decode each test data sentence. All other words are tagged during
parsing, the output from Ratnaparkhi?s tagger being ignored. The POS tags allowed
for each word are limited to those that have been seen in training data for that word
(any tag/word pairs not seen in training would give an estimate of zero in the PL2
and PR2 distributions). The model is fully integrated, in that part-of-speech tags are
statistically generated along with words in the models, so that the parser will make a
statistical decision as to the most likely tag for each known word in the sentence.
5.3 The Parsing Algorithm
The parsing algorithm for the models is a dynamic programming algorithm, which is
very similar to standard chart parsing algorithms for probabilistic or weighted gram-
mars. The algorithm has complexity O(n5), where n is the number of words in the
string. In practice, pruning strategies (methods that discard lower-probability con-
stituents in the chart) can improve efficiency a great deal. The appendices of Collins
(1999) give a precise description of the parsing algorithms, an analysis of their compu-
tational complexity, and also a description of the pruning methods that are employed.
See Eisner and Satta (1999) for an O(n4) algorithm for lexicalized grammars that
could be applied to the models in this paper. Eisner and Satta (1999) also describe an
O(n3) algorithm for a restricted class of lexicalized grammars; it is an open question
whether this restricted class includes the models in this article.
6. Results
The parser was trained on sections 2?21 of the Wall Street Journal portion of the
Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) (approximately 40,000
sentences) and tested on section 23 (2,416 sentences). We use the PARSEVAL measures
(Black et al 1991) to compare performance:
Labeled precision =
number of correct constituents in proposed parse
number of constituents in proposed parse
Labeled recall =
number of correct constituents in proposed parse
number of constituents in treebank parse
Crossing brackets = number of constituents that violate constituent boundaries
with a constituent in the treebank parse
For a constituent to be ?correct,? it must span the same set of words (ignoring punctu-
ation, i.e., all tokens tagged as commas, colons, or quotation marks) and have the same
label15 as a constituent in the treebank parse. Table 2 shows the results for models 1, 2
and 3 and a variety of other models in the literature. Two models (Collins 2000; Char-
niak 2000) outperform models 2 and 3 on section 23 of the treebank. Collins (2000)
uses a technique based on boosting algorithms for machine learning that reranks n-best
output from model 2 in this article. Charniak (2000) describes a series of enhancements
to the earlier model of Charniak (1997).
The precision and recall of the traces found by Model 3 were 93.8% and 90.1%,
respectively (out of 437 cases in section 23 of the treebank), where three criteria must be
met for a trace to be ?correct?: (1) It must be an argument to the correct headword; (2)
It must be in the correct position in relation to that headword (preceding or following);
15 Magerman (1995) collapses ADVP and PRT into the same label; for comparison, we also removed this
distinction when calculating scores.
608
Computational Linguistics Volume 29, Number 4
Table 2
Results on Section 23 of the WSJ Treebank. LR/LP = labeled recall/precision. CBs is the
average number of crossing brackets per sentence. 0 CBs, ? 2 CBs are the percentage of
sentences with 0 or ? 2 crossing brackets respectively. All the results in this table are for
models trained and tested on the same data, using the same evaluation metric. (Note that
these results show a slight improvement over those in (Collins 97); the main model changes
were the improved treatment of punctuation (section 4.3) together with the addition of the Pp
and Pcc parameters.)
Model ? 40 Words (2,245 sentences)
LR LP CBs 0 CBs ? 2 CBs
Magerman 1995 84.6% 84.9% 1.26 56.6% 81.4%
Collins 1996 85.8% 86.3% 1.14 59.9% 83.6%
Goodman 1997 84.8% 85.3% 1.21 57.6% 81.4%
Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1%
Model 1 87.9% 88.2% 0.95 65.8% 86.3%
Model 2 88.5% 88.7% 0.92 66.7% 87.1%
Model 3 88.6% 88.7% 0.90 67.1% 87.4%
Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6%
Collins 2000 90.1% 90.4% 0.73 70.7% 89.6%
Model ? 100 Words (2,416 sentences)
LR LP CBs 0 CBs ? 2 CBs
Magerman 1995 84.0% 84.3% 1.46 54.0% 78.8%
Collins 1996 85.3% 85.7% 1.32 57.2% 80.8%
Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2%
Ratnaparkhi 1997 86.3% 87.5% 1.21 60.2% ?
Model 1 87.5% 87.7% 1.09 63.4% 84.1%
Model 2 88.1% 88.3% 1.06 64.0% 85.1%
Model 3 88.0% 88.3% 1.05 64.3% 85.4%
Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7%
Collins 2000 89.6% 89.9% 0.87 68.3% 87.7%
and (3) It must be dominated by the correct nonterminal label. For example, in Figure 7,
the trace is an argument to bought, which it follows, and it is dominated by a VP. Of the
437 cases, 341 were string-vacuous extraction from subject position, recovered with
96.3% precision and 98.8% recall; and 96 were longer distance cases, recovered with
81.4% precision and 59.4% recall.16
7. Discussion
This section discusses some aspects of the models in more detail. Section 7.1 gives a
much more detailed analysis of the parsers? performance. In section 7.2 we examine
16 We exclude infinitival relative clauses from these figures (for example, I called a plumber TRACE to fix the
sink, where plumber is coindexed with the trace subject of the infinitival). The algorithm scored 41%
precision and 18% recall on the 60 cases in section 23?but infinitival relatives are extremely difficult
even for human annotators to distinguish from purpose clauses (in this case, the infinitival could be a
purpose clause modifying called) (Ann Taylor, personal communication, 1997).
609
Collins Head-Driven Statistical Models for NL Parsing
the distance features in the model. In section 7.3 we examine how the model interacts
with the Penn Treebank style of annotation. Finally, in section 7.4 we discuss the need
to break down context-free rules in the treebank in such a way that the model will
generalize to give nonzero probability to rules not seen in training. In each case we
use three methods of analysis. First, we consider how various aspects of the model
affect parsing performance, through accuracy measurements on the treebank. Second,
we look at the frequency of different constructions in the treebank. Third, we consider
linguistically motivated examples as a way of justifying various modeling choices.
7.1 A Closer Look at the Results
In this section we look more closely at the parser, by evaluating its performance on
specific constituents or constructions. The intention is to get a better idea of the parser?s
strengths and weaknesses. First, Table 3 has a breakdown of precision and recall by
constituent type. Although somewhat useful in understanding parser performance,
a breakdown of accuracy by constituent type fails to capture the idea of attachment
accuracy. For this reason we also evaluate the parser?s precision and recall in recov-
ering dependencies between words. This gives a better indication of the accuracy on
different kinds of attachments. A dependency is defined as a triple with the following
elements (see Figure 12 for an example tree and its associated dependencies):
1. Modifier: The index of the modifier word in the sentence.
Table 3
Recall and precision for different constituent types, for section 0 of the treebank with model 2.
Label is the nonterminal label; Proportion is the percentage of constituents in the treebank
section 0 that have this label; Count is the number of constituents that have this label.
Proportion Count Label Recall Precision
42.21 15146 NP 91.15 90.26
19.78 7096 VP 91.02 91.11
13.00 4665 S 91.21 90.96
12.83 4603 PP 86.18 85.51
3.95 1419 SBAR 87.81 88.87
2.59 928 ADVP 82.97 86.52
1.63 584 ADJP 65.41 68.95
1.00 360 WHNP 95.00 98.84
0.92 331 QP 84.29 78.37
0.48 172 PRN 32.56 61.54
0.35 126 PRT 86.51 85.16
0.31 110 SINV 83.64 88.46
0.27 98 NX 12.24 66.67
0.25 88 WHADVP 95.45 97.67
0.08 29 NAC 48.28 63.64
0.08 28 FRAG 21.43 46.15
0.05 19 WHPP 100.00 100.00
0.04 16 UCP 25.00 28.57
0.04 16 CONJP 56.25 69.23
0.04 15 SQ 53.33 66.67
0.03 12 SBARQ 66.67 88.89
0.03 9 RRC 11.11 33.33
0.02 7 LST 57.14 100.00
0.01 3 X 0.00 ?
0.01 2 INTJ 0.00 ?
610
Computational Linguistics Volume 29, Number 4
?Raw? dependencies Normalized dependencies
Relation Modifier Head Relation Modifier Head
S VP NP-C L 0 1 S VP NP-C L 0 1
TOP TOP S R 1 ?1 TOP TOP S R 1 ?1
NPB NN DT L 2 3 NPB TAG TAG L 2 3
VP VB NP-C R 3 1 VP TAG NP-C R 3 1
NP-C NPB PP R 4 3 NP NPB PP R 4 3
NPB NN DT L 5 6 NPB TAG TAG L 5 6
PP IN NP-C R 6 4 PP TAG NP-C R 6 4
Figure 12
A tree and its associated dependencies. Note that in ?normalizing? dependencies, all POS tags
are replaced with TAG, and the NP-C parent in the fifth relation is replaced with NP.
2. Head: The index of the headword in the sentence.
3. Relation: A ?Parent, Head, Modifier, Direction? 4-tuple, where the four
elements are the parent, head, and modifier nonterminals involved in the
dependency and the direction of the dependency (L for left, R for right).
For example, ?S, VP, NP-C, L? would indicate a subject-verb dependency.
In coordination cases there is a fifth element of the tuple, CC. For
example, ?NP, NP, NP, R, CC? would be an instance of NP coordination.
In addition, the relation is ?normalized? to some extent. First, all POS tags are
replaced with the token TAG, so that POS-tagging errors do not lead to errors in
dependencies.17 Second, any complement markings on the parent or head nontermi-
nal are removed. For example, ?NP-C, NPB, PP, R? is replaced by ?NP, NPB, PP, R?. This
prevents parsing errors where a complement has been mistaken to be an adjunct (or
vice versa), leading to more than one dependency error. As an example, in Figure 12,
if the NP the man with the telescope was mistakenly identified as an adjunct, then without
normalization, this would lead to two dependency errors: Both the PP dependency and
the verb-object relation would be incorrect. With normalization, only the verb-object
relation is incorrect.
17 The justification for this is that there is an estimated 3% error rate in the hand-assigned POS tags in the
treebank (Ratnaparkhi 1996), and we didn?t want this noise to contribute to dependency errors.
611
Collins Head-Driven Statistical Models for NL Parsing
Table 4
Dependency accuracy on section 0 of the treebank with Model 2. No labels means that only the
dependency needs to be correct; the relation may be wrong; No complements means all
complement (-C) markings are stripped before comparing relations; All means complement
markings are retained on the modifying nonterminal.
Evaluation Precision Recall
No labels 91.0% 90.9%
No complements 88.5% 88.5%
All 88.3% 88.3%
Under this definition, gold-standard and parser-output trees can be converted to
sets of dependencies, and precision and recall can be calculated on these dependencies.
Dependency accuracies are given for section 0 of the treebank in table 4. Table 5 gives
a breakdown of the accuracies by dependency type.
Table 6 shows the dependency accuracies for eight subtypes of dependency that
together account for 94% of all dependencies:
1. Complement to a verb (93.76% recall, 92.96% precision): This subtype
includes any relations of the form ? S VP ** ?, where ** is any
complement, or ? VP TAG ** ?, where ** is any complement except VP-C
(i.e., auxiliary-verb?verb dependencies are excluded). The most frequent
verb complements, subject-verb and object-verb, are recovered with over
95% precision and 92% recall.
2. Other complements (94.47% recall, 94.12% precision): This subtype
includes any dependencies in which the modifier is a complement and
the dependency does not fall into the complement to a verb category.
3. PP modification (82.29% recall, 81.51% precision): Any dependencies in
which the modifier is a PP.
4. Coordination (61.47% recall, 62.20% precision).
5. Modification within base-NPs (93.20% recall, 92.59% precision): This
subtype includes any dependencies in which the parent is NPB.
6. Modification to NPs (73.20% recall, 75.49% precision): This subtype
includes any dependencies in which the parent is NP, the head is NPB,
and the modifier is not a PP.
7. Sentential head (94.99% recall, 94.99% precision): This subtype includes
any dependencies involving the headword of the entire sentence.
8. Adjunct to a verb (75.11% recall, 78.44% precision): This subtype includes
any dependencies in which the parent is VP, the head is TAG, and the
modifier is not a PP, or in which the parent is S, the head is VP, and the
modifier is not a PP.
A conclusion to draw from these accuracies is that the parser is doing very well at
recovering the core structure of sentences: complements, sentential heads, and base-NP
relationships (NP chunks) are all recovered with over 90% accuracy. The main sources
of errors are adjuncts. Coordination is especially difficult for the parser, most likely
612
Computational Linguistics Volume 29, Number 4
Table 5
Accuracy of the 50 most frequent dependency types in section 0 of the treebank, as recovered
by model 2.
Rank Cumulative Percentage Count Relation Recall Precision
percentage
1 29.65 29.65 11786 NPB TAG TAG L 94.60 93.46
2 40.55 10.90 4335 PP TAG NP-C R 94.72 94.04
3 48.72 8.17 3248 S VP NP-C L 95.75 95.11
4 54.03 5.31 2112 NP NPB PP R 84.99 84.35
5 59.30 5.27 2095 VP TAG NP-C R 92.41 92.15
6 64.18 4.88 1941 VP TAG VP-C R 97.42 97.98
7 68.71 4.53 1801 VP TAG PP R 83.62 81.14
8 73.13 4.42 1757 TOP TOP S R 96.36 96.85
9 74.53 1.40 558 VP TAG SBAR-C R 94.27 93.93
10 75.83 1.30 518 QP TAG TAG R 86.49 86.65
11 77.08 1.25 495 NP NPB NP R 74.34 75.72
12 78.28 1.20 477 SBAR TAG S-C R 94.55 92.04
13 79.48 1.20 476 NP NPB SBAR R 79.20 79.54
14 80.40 0.92 367 VP TAG ADVP R 74.93 78.57
15 81.30 0.90 358 NPB TAG NPB L 97.49 92.82
16 82.18 0.88 349 VP TAG TAG R 90.54 93.49
17 82.97 0.79 316 VP TAG SG-C R 92.41 88.22
18 83.70 0.73 289 NP NP NP R CC 55.71 53.31
19 84.42 0.72 287 S VP PP L 90.24 81.96
20 85.14 0.72 286 SBAR WHNP SG-C R 90.56 90.56
21 85.79 0.65 259 VP TAG ADJP R 83.78 80.37
22 86.43 0.64 255 S VP ADVP L 90.98 84.67
23 86.95 0.52 205 NP NPB VP R 77.56 72.60
24 87.45 0.50 198 ADJP TAG TAG L 75.76 70.09
25 87.93 0.48 189 NPB TAG TAG R 74.07 75.68
26 88.40 0.47 187 VP TAG NP R 66.31 74.70
27 88.85 0.45 180 VP TAG SBAR R 74.44 72.43
28 89.29 0.44 174 VP VP VP R CC 74.14 72.47
29 89.71 0.42 167 NPB TAG ADJP L 65.27 71.24
30 90.11 0.40 159 VP TAG SG R 60.38 68.57
31 90.49 0.38 150 VP TAG S-C R 74.67 78.32
32 90.81 0.32 129 S S S R CC 72.09 69.92
33 91.12 0.31 125 PP TAG SG-C R 94.40 89.39
34 91.43 0.31 124 QP TAG TAG L 77.42 83.48
35 91.72 0.29 115 S VP TAG L 86.96 90.91
36 92.00 0.28 110 NPB TAG QP L 80.91 81.65
37 92.27 0.27 106 SINV VP NP R 88.68 95.92
38 92.53 0.26 104 S VP S-C L 93.27 78.86
39 92.79 0.26 102 NP NP NP R 30.39 25.41
40 93.02 0.23 90 ADJP TAG PP R 75.56 78.16
41 93.24 0.22 89 TOP TOP SINV R 96.63 94.51
42 93.45 0.21 85 ADVP TAG TAG L 74.12 73.26
43 93.66 0.21 83 SBAR WHADVP S-C R 97.59 98.78
44 93.86 0.20 81 S VP SBAR L 88.89 85.71
45 94.06 0.20 79 VP TAG ADVP L 51.90 49.40
46 94.24 0.18 73 SINV VP S L 95.89 92.11
47 94.40 0.16 63 NP NPB SG R 88.89 81.16
48 94.55 0.15 58 S VP PRN L 25.86 48.39
49 94.70 0.15 58 NX TAG TAG R 10.34 75.00
50 94.83 0.13 53 NP NPB PRN R 45.28 60.00
613
Collins Head-Driven Statistical Models for NL Parsing
Table 6
Accuracy for various types/subtypes of dependency (part 1). Only subtypes occurring more
than 10 times are shown.
Type Sub-type Description Count Recall Precision
Complement to a verb S VP NP-C L Subject 3,248 95.75 95.11
VP TAG NP-C R Object 2,095 92.41 92.15
6,495 = 16.3% of all cases VP TAG SBAR-C R 558 94.27 93.93
VP TAG SG-C R 316 92.41 88.22
VP TAG S-C R 150 74.67 78.32
S VP S-C L 104 93.27 78.86
S VP SG-C L 14 78.57 68.75
. . .
Total 6,495 93.76 92.96
Other complements PP TAG NP-C R 4,335 94.72 94.04
VP TAG VP-C R 1,941 97.42 97.98
7,473 = 18.8% of all cases SBAR TAG S-C R 477 94.55 92.04
SBAR WHNP SG-C R 286 90.56 90.56
PP TAG SG-C R 125 94.40 89.39
SBAR WHADVP S-C R 83 97.59 98.78
PP TAG PP-C R 51 84.31 70.49
SBAR WHNP S-C R 42 66.67 84.85
SBAR TAG SG-C R 23 69.57 69.57
PP TAG S-C R 18 38.89 63.64
SBAR WHPP S-C R 16 100.00 100.00
S ADJP NP-C L 15 46.67 46.67
PP TAG SBAR-C R 15 100.00 88.24
. . .
Total 7,473 94.47 94.12
PP modification NP NPB PP R 2,112 84.99 84.35
VP TAG PP R 1,801 83.62 81.14
4,473 = 11.2% of all cases S VP PP L 287 90.24 81.96
ADJP TAG PP R 90 75.56 78.16
ADVP TAG PP R 35 68.57 52.17
NP NP PP R 23 0.00 0.00
PP PP PP L 19 21.05 26.67
NAC TAG PP R 12 50.00 100.00
. . .
Total 4,473 82.29 81.51
Coordination NP NP NP R 289 55.71 53.31
VP VP VP R 174 74.14 72.47
763 = 1.9% of all cases S S S R 129 72.09 69.92
ADJP TAG TAG R 28 71.43 66.67
VP TAG TAG R 25 60.00 71.43
NX NX NX R 25 12.00 75.00
SBAR SBAR SBAR R 19 78.95 83.33
PP PP PP R 14 85.71 63.16
. . .
Total 763 61.47 62.20
614
Computational Linguistics Volume 29, Number 4
Table 6
(cont.)
Type Subtype Description Count Recall Precision
Modification NPB TAG TAG L 11,786 94.60 93.46
within Base-NPs NPB TAG NPB L 358 97.49 92.82
12,742 = 29.6% of all cases NPB TAG TAG R 189 74.07 75.68
NPB TAG ADJP L 167 65.27 71.24
NPB TAG QP L 110 80.91 81.65
NPB TAG NAC L 29 51.72 71.43
NPB NX TAG L 27 14.81 66.67
NPB QP TAG L 15 66.67 76.92
. . .
Total 12,742 93.20 92.59
Modification to NPs NP NPB NP R Appositive 495 74.34 75.72
NP NPB SBAR R Relative clause 476 79.20 79.54
1,418 = 3.6% of all cases NP NPB VP R Reduced relative 205 77.56 72.60
NP NPB SG R 63 88.89 81.16
NP NPB PRN R 53 45.28 60.00
NP NPB ADVP R 48 35.42 54.84
NP NPB ADJP R 48 62.50 69.77
. . .
Total 1,418 73.20 75.49
Sentential head TOP TOP S R 1,757 96.36 96.85
TOP TOP SINV R 89 96.63 94.51
1,917 = 4.8% of all cases TOP TOP NP R 32 78.12 60.98
TOP TOP SG R 15 40.00 33.33
. . .
Total 1,917 94.99 94.99
Adjunct to a verb VP TAG ADVP R 367 74.93 78.57
VP TAG TAG R 349 90.54 93.49
2,242 = 5.6% of all cases VP TAG ADJP R 259 83.78 80.37
S VP ADVP L 255 90.98 84.67
VP TAG NP R 187 66.31 74.70
VP TAG SBAR R 180 74.44 72.43
VP TAG SG R 159 60.38 68.57
S VP TAG L 115 86.96 90.91
S VP SBAR L 81 88.89 85.71
VP TAG ADVP L 79 51.90 49.40
S VP PRN L 58 25.86 48.39
S VP NP L 45 66.67 63.83
S VP SG L 28 75.00 52.50
VP TAG PRN R 27 3.70 12.50
VP TAG S R 11 9.09 100.00
. . .
Total 2,242 75.11 78.44
615
Collins Head-Driven Statistical Models for NL Parsing
Table 7
Results on section 0 of the WSJ Treebank. A ?YES? in the A column means that the adjacency
conditions were used in the distance measure; likewise, a ?YES? in the V column indicates
that the verb conditions were used in the distance measure. LR = labeled recall; LP = labeled
precision. CBs is the average number of crossing brackets per sentence. 0 CBs ? 2 CBs are the
percentages of sentences with 0 and ? 2 crossing brackets, respectively.
Model A V LR LP CBs 0 CBs ? 2 CBs
Model 1 No No 75.0% 76.5% 2.18 38.5% 66.4
Model 1 Yes No 86.6% 86.7% 1.22 60.9% 81.8
Model 1 Yes Yes 87.8% 88.2% 1.03 63.7% 84.4
Model 2 No No 85.1% 86.8% 1.28 58.8% 80.3
Model 2 Yes No 87.7% 87.8% 1.10 63.8% 83.2
Model 2 Yes Yes 88.7% 89.0% 0.95 65.7% 85.6
because it often involves a dependency between two content words, leading to very
sparse statistics.
7.2 More about the Distance Measure
The distance measure, whose implementation was described in section 3.1.1, deserves
more discussion and motivation. In this section we consider it from three perspectives:
its influence on parsing accuracy; an analysis of distributions in training data that
are sensitive to the distance variables; and some examples of sentences in which the
distance measure is useful in discriminating among competing analyses.
7.2.1 Impact of the Distance Measure on Accuracy. Table 7 shows the results for
models 1 and 2 with and without the adjacency and verb distance measures. It is clear
that the distance measure improves the models? accuracy.
What is most striking is just how badly model 1 performs without the distance
measure. Looking at the parser?s output, the reason for this poor performance is that
the adjacency condition in the distance measure is approximating subcategorization
information. In particular, in phrases such as PPs and SBARs (and, to a lesser extent,
in VPs) that almost always take exactly one complement to the right of their head,
the adjacency feature encodes this monovalency through parameters P(STOP|PP/SBAR,
adjacent) = 0 and P(STOP|PP/SBAR, not adjacent) = 1. Figure 13 shows some par-
ticularly bad structures returned by model 1 with no distance variables.
Another surprise is that subcategorization can be very useful, but that the dis-
tance measure has masked this utility. One interpretation in moving from the least
parameterized model (Model 1 [No, No]) to the fully parameterized model (Model 2
[Yes, Yes]) is that the adjacency condition adds around 11% in accuracy; the verb
condition adds another 1.5%; and subcategorization finally adds a mere 0.8%. Under
this interpretation subcategorization information isn?t all that useful (and this was my
original assumption, as this was the order in which features were originally added
to the model). But under another interpretation subcategorization is very useful: In
moving from Model 1 (No, No) to Model 2 (No, No), we see a 10% improvement as a
result of subcategorization parameters; adjacency then adds a 1.5% improvement; and
the verb condition adds a final 1% improvement.
From an engineering point of view, given a choice of whether to add just distance
or subcategorization to the model, distance is preferable. But linguistically it is clear
that adjacency can only approximate subcategorization and that subcategorization is
616
Computational Linguistics Volume 29, Number 4
Figure 13
Two examples of bad parses produced by model 1 with no distance or subcategorization
conditions (Model 1 (No, No) in table 7). In (a) one PP has two complements, the other has
none; in (b) the SBAR has two complements. In both examples either the adjacency condition
or the subcategorization parameters will correct the errors, so these are examples in which the
adjacency and subcategorization variables overlap in their utility.
Table 8
Distribution of nonterminals generated as postmodifiers to an NP (see tree to the left), at
various distances from the head. A = True means the modifier is adjacent to the head, V =
True means there is a verb between the head and the modifier. Distributions were calculated
from the first 10000 events for each of the three cases in sections 2-21 of the treebank.
A = True, V = False A = False, V = False A = False, V = True
Percentage ? Percentage ? Percentage ?
70.78 STOP 88.53 STOP 97.65 STOP
17.7 PP 5.57 PP 0.93 PP
3.54 SBAR 2.28 SBAR 0.55 SBAR
3.43 NP 1.55 NP 0.35 NP
2.22 VP 0.92 VP 0.22 VP
0.61 SG 0.38 SG 0.09 SG
0.56 ADJP 0.26 PRN 0.07 PRN
0.54 PRN 0.22 ADVP 0.04 ADJP
0.36 ADVP 0.15 ADJP 0.03 ADVP
0.08 TO 0.09 -RRB- 0.02 S
0.08 CONJP 0.02 UCP 0.02 -RRB-
0.03 UCP 0.01 X 0.01 X
0.02 JJ 0.01 RRC 0.01 VBG
0.01 VBN 0.01 RB 0.01 RB
0.01 RRC
0.01 FRAG
0.01 CD
0.01 -LRB-
more ?correct? in some sense. In free-word-order languages, distance may not approx-
imate subcategorization at all well: A complement may appear to either the right or
left of the head, confusing the adjacency condition.
7.2.2 Frequencies in Training Data. Tables 8 and 9 show the effect of distance on the
distribution of modifiers in two of the most frequent syntactic environments: NP and
verb modification. The distribution varies a great deal with distance. Most striking is
the way that the probability of STOP increases with increasing distance: from 71% to
89% to 98% in the NP case, from 8% to 60% to 96% in the verb case. Each modifier
probability generally decreases with distance. For example, the probability of seeing
a PP modifier to an NP decreases from 17.7% to 5.57% to 0.93%.
617
Collins Head-Driven Statistical Models for NL Parsing
Table 9
Distribution of nonterminals generated as postmodifiers to a verb within a VP (see tree to the
left), at various distances from the head. A = True means the modifier is adjacent to the head;
V = True means there is a verb between the head and the modifier. The distributions were
calculated from the first 10000 events for each of the distributions in sections 2?21. Auxiliary
verbs (verbs taking a VP complement to their right) were excluded from these statistics.
A = True, V = False A = False, V = False A = False, V = True
Percentage ? Percentage ? Percentage ?
39 NP-C 59.87 STOP 95.92 STOP
15.8 PP 22.7 PP 1.73 PP
8.43 SBAR-C 3.3 NP-C 0.92 SBAR
8.27 STOP 3.16 SG 0.5 NP
5.35 SG-C 2.71 ADVP 0.43 SG
5.19 ADVP 2.65 SBAR 0.16 ADVP
5.1 ADJP 1.5 SBAR-C 0.14 SBAR-C
3.24 S-C 1.47 NP 0.05 NP-C
2.82 RB 1.11 SG-C 0.04 PRN
2.76 NP 0.82 ADJP 0.02 S-C
2.28 PRT 0.2 PRN 0.01 VBN
0.63 SBAR 0.19 PRT 0.01 VB
0.41 SG 0.09 S 0.01 UCP
0.16 VB 0.06 S-C 0.01 SQ
0.1 S 0.06 -RRB- 0.01 S
0.1 PRN 0.03 FRAG 0.01 FRAG
0.08 UCP 0.02 -LRB- 0.01 ADJP
0.04 VBZ 0.01 X 0.01 -RRB-
0.03 VBN 0.01 VBP 0.01 -LRB-
0.03 VBD 0.01 VB
0.03 FRAG 0.01 UCP
0.03 -LRB- 0.01 RB
0.02 VBG 0.01 INTJ
0.02 SBARQ
0.02 CONJP
0.01 X
0.01 VBP
0.01 RBR
0.01 INTJ
0.01 DT
0.01 -RRB-
7.2.3 Distance Features and Right-Branching Structures. Both the adjacency and verb
components of the distance measure allow the model to learn a preference for right-
branching structures. First, consider the adjacency condition. Figure 14 shows some
examples in which right-branching structures are more frequent. Using the statistics
from Tables 8 and 9, the probability of the alternative structures can be calculated. The
results are given below. The right-branching structures get higher probability (although
this is before the lexical-dependency probabilities are multiplied in, so this ?prior?
preference for right-branching structures can be overruled by lexical preferences). If
the distance variables were not conditioned on, the product of terms for the two
alternatives would be identical, and the model would have no preference for one
structure over another.
Probabilities for the two alternative PP structures in Figure 14 (excluding probabil-
ity terms that are constant across the two structures; A=1 means distance is adjacent,
A=0 means not adjacent) are as follows:
618
Computational Linguistics Volume 29, Number 4
Figure 14
Some alternative structures for the same surface sequence of chunks (NPB PP PP in the first
case, NPB PP SBAR in the second case) in which the adjacency condition distinguishes between
the two structures. The percentages are taken from sections 2?21 of the treebank. In both cases
right-branching structures are more frequent.
Right-branching:
P(PP|NP,NPB,A=1)P(STOP|NP,NPB,A=0)
P(PP|NP,NPB,A=1)P(STOP|NP,NPB,A=0)
= 0.177 ? 0.8853 ? 0.177 ? 0.8853 = 0.02455
Non-right-branching:
P(PP|NP,NPB,A=1)P(PP|NP,NPB,A=0)
P(STOP|NP,NPB,A=0)P(STOP|NP,NPB,A=1)
= 0.177 ? 0.0557 ? 0.8853 ? 0.7078 = 0.006178
Probabilities for the SBAR case in Figure 14, assuming the SBAR contains a verb (V=0
means modification does not cross a verb, V=1 means it does), are as follows:
Right-branching:
P(PP|NP,NPB,A=1,V=0)P(SBAR|NP,NPB,A=1,V=0)
P(STOP|NP,NPB,A=0,V=1)P(STOP|NP,NPB,A=0,V=1)
= 0.177 ? 0.0354 ? 0.9765 ? 0.9765 = 0.005975
Non-right-branching:
P(PP|NP,NPB,A=1)P(STOP|NP,NPB,A=1)
P(SBAR|NP,NPB,A=0)P(STOP|NP,NPB,A=0,V=1)
= 0.177 ? 0.7078 ? 0.0228 ? 0.9765 = 0.002789
619
Collins Head-Driven Statistical Models for NL Parsing
Figure 15
Some alternative structures for the same surface sequence of chunks in which the verb
condition in the distance measure distinguishes between the two structures. In both cases the
low-attachment analyses will get higher probability under the model, because of the low
probability of generating a PP modifier involving a dependency that crosses a verb. (X stands
for any nonterminal.)
7.2.4 Verb Condition and Right-Branching Structures. Figure 15 shows some exam-
ples in which the verb condition is important in differentiating the probability of two
structures. In both cases an adjunct can attach either high or low, but high attachment
results in a dependency?s crossing a verb and has lower probability.
An alternative to the surface string feature would be a predicate such as were any
of the previous modifiers in X, where X is a set of nonterminals that are likely to contain
a verb, such as VP, SBAR, S, or SG. This would allow the model to handle cases like the
first example in Figure 15 correctly. The second example shows why it is preferable to
condition on the surface string. In this case the verb is ?invisible? to the top level, as
it is generated recursively below the NP object.
7.2.5 Structural versus Semantic Preferences. One hypothesis would be that lexical
statistics are really what is important in parsing: that arriving at a correct interpretation
for a sentence is simply a matter of finding the most semantically plausible analysis,
and that the statistics related to lexical dependencies approximate this notion of plau-
sibility. Implicitly, we would be just as well off (maybe even better off) if statistics were
calculated between items at the predicate-argument level, with no reference to struc-
ture. The distance preferences under this interpretation are just a way of mitigating
sparse-data problems: When the lexical statistics are too sparse, then falling back on
some structural preference is not ideal, but is at least better than chance. This hypoth-
esis is suggested by previous work on specific cases of attachment ambiguity such
as PP attachment (see, e.g., Collins and Brooks 1995), which has showed that models
will perform better given lexical statistics, and that a straight structural preference is
merely a fallback.
But some examples suggest this is not the case: that, in fact, many sentences
have several equally semantically plausible analyses, but that structural preferences
620
Computational Linguistics Volume 29, Number 4
distinguish strongly among them. Take the following example (from Pereira and War-
ren 1980):
(4) John was believed to have been shot by Bill.
Surprisingly, this sentence has two analyses: Bill can be the deep subject of either
believed or shot. Yet people have a very strong preference for Bill to be doing the
shooting, so much so that they may even miss the second analysis. (To see that the
dispreferred analysis is semantically quite plausible, consider Bill believed John to have
been shot.)
As evidence that structural preferences can even override semantic plausibility,
take the following example (from Pinker 1994):
(5) Flip said that Squeaky will do the work yesterday.
This sentence is a garden path: The structural preference for yesterday to modify the
most recent verb is so strong that it is easy to miss the (only) semantically plausible
interpretation, paraphrased as Flip said yesterday that Squeaky will do the work.
The model makes the correct predictions in these cases. In example (4), the statistics
in Table 9 show that a PP is nine times as likely to attach low as to attach high when
two verbs are candidate attachment points (the chances of seeing a PP modifier are
15.8% and 1.73% in columns 1 and 5 of the table, respectively). In example (5), the
probability of seeing an NP (adjunct) modifier to do in a nonadjacent but non-verb-
crossing environment is 2.11% in sections 2?21 of the treebank (8 out of 379 cases); in
contrast, the chance of seeing an NP adjunct modifying said across a verb is 0.026% (1
out of 3,778 cases). The two probabilities differ by a factor of almost 80.
7.3 The Importance of the Choice of Tree Representation
Figures 16 and 17 show some alternative styles of syntactic annotation. The Penn
Treebank annotation style tends to leave trees quite flat, typically with one level of
structure for each X-bar level; at the other extreme are completely binary-branching
representations. The two annotation styles are in some sense equivalent, in that it
is easy to define a one-to-one mapping between them. But crucially, two different
annotation styles may lead to quite different parsing accuracies for a given model,
even if the two representations are equivalent under some one-to-one mapping.
A parsing model does not need to be tied to the annotation style of the treebank
on which it is trained. The following procedure can be used to transform trees in both
training and test data into a new representation:
Figure 16
Alternative annotation styles for a sentence S with a verb head V, left modifiers X1, X2, and
right modifiers Y1, Y2: (a) the Penn Treebank style of analysis (one level of structure for each
bar level); (b) an alternative but equivalent binary branching representation.
621
Collins Head-Driven Statistical Models for NL Parsing
Figure 17
Alternative annotation styles for a noun phrase with a noun head N, left modifiers X1, X2,
and right modifiers Y1, Y2: (a) the Penn Treebank style of analysis (one level of structure for
each bar level, although note that both the nonrecursive and the recursive noun phrases are
labeled NP; (b) an alternative but equivalent binary branching representation; (a?) our
modification of the Penn Treebank style to differentiate recursive and nonrecursive NPs (in
some sense NPB is a bar 1 structure and NP is a bar 2 structure).
1. Transform training data trees into the new representation and train the
model.
2. Recover parse trees in the new representation when running the model
over test data sentences.
3. Convert the test output back into the treebank representation for scoring
purposes.
As long as there is a one-to-one mapping between the treebank and the new rep-
resentation, nothing is lost in making such a transformation. Goodman (1997) and
Johnson (1997) both suggest this strategy. Goodman (1997) converts the treebank into
binary-branching trees. Johnson (1997) considers conversion to a number of different
representations and discusses how this influences accuracy for nonlexicalized PCFGs.
The models developed in this article have tacitly assumed the Penn Treebank
style of annotation and will perform badly given other representations (for example,
binary-branching trees). This section makes this point more explicit, describing exactly
what annotation style is suitable for the models and showing how other annotation
styles will cause problems. This dependence on Penn Treebank?style annotations does
not imply that the models are inappropriate for a treebank annotated in a different
style: In this case we simply recommend transforming the trees into flat, one-level-
per-X-bar-level trees before training the model, as in the three-step procedure outlined
above.
Other models in the literature are also very likely to be sensitive to annotation
style. Charniak?s (1997) models will most likely perform quite differently with binary-
branching trees (for example, his current models will learn that rules such as VP ?
V SG PP are very rare, but with binary-branching structures, this context sensitivity
will be lost). The models of Magerman (1995) and Ratnaparkhi (1997) use contextual
predicates that would most likely need to be modified given a different annotation
style. Goodman?s (1997) models are the exception, as he already specifies that the
treebank should be transformed into his chosen representation, binary-branching trees.
7.3.1 Representation Affects Structural, not Lexical, Preferences. The alternative rep-
resentations in Figures 16 and 17 have the same lexical dependencies (providing that
the binary-branching structures are centered about the head of the phrase, as in the
examples). The difference between the representations involves structural preferences
such as the right-branching preferences encoded by the distance measure. Applying
the models in this article to treebank analyses that use this type of ?head-centered?
622
Computational Linguistics Volume 29, Number 4
Figure 18
BB = binary-branching structures; FLAT = Penn treebank style annotations. In each case the
binary-branching annotation style prevents the model from learning that these structures
should receive low probability because of the long distance dependency associated with the
final PP (in boldface).
binary-branching tree will result in a distance measure that incorrectly encodes a pref-
erence for right-branching structures.
To see this, consider the examples in Figure 18. In each binary-branching example,
the generation of the final modifying PP is ?blind? to the distance between it and the
head that it modifies. At the top level of the tree, it is apparently adjacent to the head;
crucially, the closer modifier (SG in (a), the other PP in (b)) is hidden lower in the tree
structure. So the model will be unable to differentiate generation of the PP in adjacent
versus nonadjacent or non-verb-crossing versus verb-crossing environments, and the
structures in Figure 18 will be assigned unreasonably high probabilities.
This does not mean that distance preferences cannot be encoded in a binary-
branching PCFG. Goodman (1997) achieves this by adding distance features to the non-
terminals. The spirit of this implementation is that the top-level rules VP ? VP PP and
NP ? NP PP would be modified to VP ? VP(+rverb) PP and NP ? NP(+rmod) PP,
respectively, where (+rverb) means a phrase in which the head has a verb in its right
modifiers, and (+rmod) means a phrase that has at least one right modifier to the
head. The model will learn from training data that P(VP ? VP(+rverb) PP|VP) 
P(VP ? VP(-rverb) PP|VP), that is, that a prepositional-phrase modification is much
more likely when it does not cross a verb.
7.3.2 The Importance of Differentiating Nonrecursive from Recursive NPs. Figure 19
shows the modification to the Penn Treebank annotation to relabel base-NPs as NPB.
It also illustrates a problem that arises if a distinction between the two is not made:
Structures such as that in Figure 19(b) are assigned high probabilities even if they
Figure 19
(a) The way the Penn Treebank annotates NPs. (a?) Our modification to the annotation, to
differentiate recursive (NP) from nonrecursive (NPB) noun phrases. (b) A structure that is never
seen in training data but will receive much too high a probability from a model trained on
trees of style (a).
623
Collins Head-Driven Statistical Models for NL Parsing
Figure 20
Examples of other phrases in the Penn Treebank in which nonrecursive and recursive phrases
are not differentiated.
are never seen in training data. (Johnson [1997] notes that this structure has a higher
probability than the correct, flat structure, given counts taken from the treebank for
a standard PCFG.) The model is fooled by the binary-branching style into modeling
both PPs as being adjacent to the head of the noun phrase, so 19(b) will be assigned a
very high probability.
This problem does not apply only to NPs: Other types of phrases such as adjectival
phrases (ADJPs) or adverbial phrases (ADVPs) also have nonrecursive (bar 1) and recur-
sive (bar 2) levels, which are not differentiated in the Penn Treebank. (See Figure 20 for
examples.) Ideally these cases should be differentiated too: We did not implement this
change because it is unlikely to make much difference in accuracy, given the relative
infrequency of these cases (excluding coordination cases, and looking at the 80,254
instances in sections 2?21 of the Penn Treebank in which a parent and head nonter-
minal are the same: 94.5% are the NP case; 2.6% are cases of coordination in which a
punctuation mark is the coordinator;18 only 2.9% are similar to those in Figure 20).
7.3.3 Summary. To summarize, the models in this article assume the following:
1. Tree representations are ?flat?: that is, one level per X-bar level.
2. Different X-bar levels have different labels (in particular, nonrecursive
and recursive levels are differentiated, at least for the most frequent case
of NPs).
7.4 The Need to Break Down Rules
The parsing approaches we have described concentrate on breaking down context-free
rules in the treebank into smaller components. Lexicalized rules were initially broken
down to bare-bones Markov processes, then increased dependency on previously gen-
erated modifiers was built back up through the distance measure and subcategoriza-
tion. Even with this additional context, the models are still able to recover rules in test
data that have never been seen in training data.
An alternative, proposed in Charniak (1997), is to limit parsing to those context-
free rules seen in training data. A lexicalized rule is predicted in two steps. First,
the whole context-free rule is generated. Second, the lexical items are filled in. The
probability of a rule is estimated as19
P(Ln(ln) . . . L1(l1)H(h)R1(r1) . . . Rm(rm) | P, h) =
P(Ln . . . L1HR1 . . . Rm) | P, h)?
?
i=1...n
Pl(li | Li, P, h)?
?
j=1...m
Pr(rj | Rj, P, h)
18 For example, (S (S John eats apples); (S Mary eats bananas)).
19 Charniak?s model also conditions on the parent of the nonterminal being expanded; we omit this here
for brevity.
624
Computational Linguistics Volume 29, Number 4
The estimation technique used in Charniak (1997) for the CF rule probabilities inter-
polates several estimates, the lowest being P(Ln . . . L1HR1 . . . Rm) | P). Any rules not
seen in training data will be assigned zero probability with this model. Parse trees in
test data will be limited to include rules seen in training.
A problem with this approach is coverage. As shown in this section, many test data
sentences will require rules that have not been seen in training. This gives motivation
for breaking down rules into smaller components. This section motivates the need to
break down rules from four perspectives. First, we discuss how the Penn Treebank
annotation style leads to a very large number of grammar rules. Second, we assess the
extent of the coverage problem by looking at rule frequencies in training data. Third,
we conduct experiments to assess the impact of the coverage problem on accuracy.
Fourth, we discuss how breaking rules down may improve estimation as well as
coverage.
7.4.1 The Penn Treebank Annotation Style Leads to Many Rules. The ?flatness? of
the Penn Treebank annotation style has already been discussed, in section 7.3. The
flatness of the trees leads to a very large (and constantly growing) number of rules,
primarily because the number of adjuncts to a head is potentially unlimited: For ex-
ample, there can be any number of PP adjuncts to a head verb. A binary-branching
(Chomsky adjunction) grammar can generate an unlimited number of adjuncts with
very few rules. For example, the following grammar generates any sequence VP ? V
NP PP*:
VP ? V NP
VP ? VP PP
In contrast, the Penn Treebank style would create a new rule for each number of PPs
seen in training data. The grammar would be
VP ? V NP
VP ? V NP PP
VP ? V NP PP PP
VP ? V NP PP PP PP
and so on
Other adverbial adjuncts, such as adverbial phrases or adverbial SBARs, can also modify
a verb several times, and all of these different types of adjuncts can be seen together in
the same rule. The result is a combinatorial explosion in the number of rules. To give
a flavor of this, here is a random sample of rules of the format VP ? VB modifier*
that occurred only once in sections 2?21 of the Penn Treebank:
VP ? VB NP NP NP PRN
VP ? VB NP SBAR PP SG ADVP
VP ? VB NP ADVP ADVP PP PP
VP ? VB RB
VP ? VB NP PP NP SBAR
VP ? VB NP PP SBAR PP
It is not only verb phrases that cause this kind of combinatorial explosion: Other
phrases, in particular nonrecursive noun phrases, also contribute a huge number of
rules. The next section considers the distributional properties of the rules in more
detail.
625
Collins Head-Driven Statistical Models for NL Parsing
Note that there is good motivation for the Penn Treebank?s decision to repre-
sent rules in this way, rather than with rules expressing Chomsky adjunction (i.e., a
schema in which complements and adjuncts are separated, through rule types ?VP ?
VB {complement}*? and ?VP ? VP {adjunct}?). First, it allows the argument/adjunct
distinction for PP modifiers to verbs to be left undefined: This distinction was found
to be very difficult for annotators. Second, in the surface ordering (as opposed to deep
structure), adjuncts are often found closer to the head than complements, thereby yield-
ing structures that fall outside the Chomsky adjunction schema. For example, a rule
such as ?VP ? VB NP-C PP SBAR-C? is found very frequently in the Penn Treebank;
SBAR complements nearly always extrapose over adjuncts.
7.4.2 Quantifying the Coverage Problem. To quantify the coverage problem, rules
were collected from sections 2?21 of the Penn Treebank. Punctuation was raised as
high as possible in the tree, and the rules did not have complement markings or the
distinction between base-NPs and recursive NPs. Under these conditions, 939,382 rule
tokens were collected; there were 12,409 distinct rule types. We also collected the count
for each rule. Table 10 shows some statistics for these rules.
A majority of rules in the grammar (6,765, or 54.5%) occur only once. These rules
account for 0.72% of rules by token. That is, if one of the 939,382 rule tokens in sections
2?21 of the treebank were drawn at random, there would be a 0.72% chance of its being
the only instance of that rule in the 939,382 tokens. On the other hand, if a rule were
drawn at random from the 12,409 rules in the grammar induced from those sections,
there would be a 54.5% chance of that rule?s having occurred only once.
The percentage by token of the one-count rules is an indication of the coverage
problem. From this estimate, 0.72% of all rules (or 1 in 139 rules) required in test data
would never have been seen in training. It was also found that 15.0% (1 in 6.67) of all
sentences have at least one rule that occurred just once. This gives an estimate that
roughly 1 in 6.67 sentences in test data will not be covered by a grammar induced
from 40,000 sentences in the treebank.
If the complement markings are added to the nonterminals, and the base-NP/non-
recursive NP distinction is made, then the coverage problem is made worse. Table 11
gives the statistics in this case. By our counts, 17.1% of all sentences (1 in 5.8 sentences)
contain at least 1 one-count rule.
Table 10
Statistics for rules taken from sections 2?21 of the treebank, with complement markings not
included on nonterminals.
Rule count Number of Rules Percentage Number of Rules Percentage of rules
by type by type by token by token
1 6765 54.52 6765 0.72
2 1688 13.60 3376 0.36
3 695 5.60 2085 0.22
4 457 3.68 1828 0.19
5 329 2.65 1645 0.18
6 . . . 10 835 6.73 6430 0.68
11 . . . 20 496 4.00 7219 0.77
21 . . . 50 501 4.04 15931 1.70
51 . . . 100 204 1.64 14507 1.54
> 100 439 3.54 879596 93.64
626
Computational Linguistics Volume 29, Number 4
Table 11
Statistics for rules taken from sections 2?21 of the treebank, with complement markings
included on nonterminals.
Rule count Number of Rules Percentage of rules Number of Rules Percentage of rules
by type by type by token by token
1 7865 55.00 7865 0.84
2 1918 13.41 3836 0.41
3 815 5.70 2445 0.26
4 528 3.69 2112 0.22
5 377 2.64 1885 0.20
6 . . . 10 928 6.49 7112 0.76
11 . . . 20 595 4.16 8748 0.93
21 . . . 50 552 3.86 17688 1.88
51 . . . 100 240 1.68 16963 1.81
> 100 483 3.38 870728 92.69
Table 12
Results on section 0 of the Treebank. The label restricted means the model is restricted to
recovering rules that have been seen in training data. LR = labeled recall. LP = labeled
precision. CBs is the average number of crossing brackets per sentence. 0 CBs and ? 2 CBs are
the percentages of sentences with 0 and ? 2 crossing brackets, respectively.
Model Accuracy
LR LP CBs 0 CBs ? 2 CBs
Model 1 87.9 88.3 1.02 63.9 84.4
Model 1 (restricted) 87.4 86.7 1.19 61.7 81.8
Model 2 88.8 89.0 0.94 65.9 85.6
Model 2 (restricted) 87.9 87.0 1.19 62.5 82.4
7.4.3 The Impact of Coverage on Accuracy. Parsing experiments were used to assess
the impact of the coverage problem on parsing accuracy. Section 0 of the treebank was
parsed with models 1 and 2 as before, but the parse trees were restricted to include
rules already seen in training data. Table 12 shows the results. Restricting the rules
leads to a 0.5% decrease in recall and a 1.6% decrease in precision for model 1, and a
0.9% decrease in recall and a 2.0% decrease in precision for model 2.
7.4.4 Breaking Down Rules Improves Estimation. Coverage problems are not the
only motivation for breaking down rules. The method may also improve estimation.
To see this, consider the rules headed by told, whose counts are shown in Table 13.
Estimating the probability P(Rule | VP, told) using Charniak?s (1997) method would
interpolate two maximum-likelihood estimates:
?Pml(Rule | VP, told) + (1 ? ?)Pml(Rule | VP)
Estimation interpolates between the specific, lexically sensitive distribution in Table 13
and the nonlexical estimate based on just the parent nonterminal, VP. There are many
different rules in the more specific distribution (26 different rule types, out of 147
tokens in which told was a VP head), and there are several one-count rules (11 cases).
From these statistics ? would have to be relatively low. There is a high chance that
a new rule for told will be required in test data; therefore a reasonable amount of
627
Collins Head-Driven Statistical Models for NL Parsing
Table 13
(a) Distribution over rules with told as the head (from sections 2?21 of the treebank); (b)
distribution over subcategorization frames with told as the head.
(a)
Count Rule
70 VP told ? VBD NP-C SBAR-C
23 VP told ? VBD NP-C
6 VP told ? VBD NP-C SG-C
5 VP told ? VBD NP-C NP SBAR-C
5 VP told ? VBD NP-C : S-C
4 VP told ? VBD NP-C PP SBAR-C
4 VP told ? VBD NP-C PP
4 VP told ? VBD NP-C NP
3 VP told ? VBD NP-C PP NP SBAR-C
2 VP told ? VBD NP-C PP PP
2 VP told ? VBD NP-C NP PP
2 VP told ? VBD NP-C , SBAR-C
2 VP told ? VBD NP-C , S-C
2 VP told ? VBD
2 VP told ? ADVP VBD NP-C SBAR-C
1 VP told ? VBD NP-C SG-C SBAR
1 VP told ? VBD NP-C SBAR-C PP
1 VP told ? VBD NP-C SBAR , PP
1 VP told ? VBD NP-C PP SG-C
1 VP told ? VBD NP-C PP NP
1 VP told ? VBD NP-C PP : S-C
1 VP told ? VBD NP-C NP : S-C
1 VP told ? VBD NP-C ADVP SBAR-C
1 VP told ? VBD NP-C ADVP PP NP
1 VP told ? VBD NP-C ADVP
1 VP told ? VBD NP-C , PRN , SBAR-C
147 Total
(b)
Count Subcategorization frame
89 {NP-C, SBAR-C}
39 {NP-C}
9 {NP-C, S-C}
8 {NP-C, SG-C}
2 {}
147 Total
probability mass must be left to the backed-off estimate Pml(Rule | VP).
This estimation method is missing a crucial generalization: In spite of there being
many different rules, the distribution over subcategorization frames is much sharper.
Told is seen with only five subcategorization frames in training data: The large number
of rules is almost entirely due to adjuncts or punctuation appearing after or between
complements. The estimation method in model 2 effectively estimates the probability
of a rule as
Plc(LC | VP, told)? Prc(RC | VP, told)? P(Rule | VP, told, LC, RC)
The left and right subcategorization frames, LC and RC, are chosen first. The entire
rule is then generated by Markov processes.
Once armed with the Plc and Prc parameters, the model has the ability to learn the
generalization that told appears with a quite limited, sharp distribution over subcatego-
rization frames. Say that these parameters are again estimated through interpolation,
for example
?Pml(LC | VP, told) + (1 ? ?)Pml(LC | VP)
In this case ? can be quite high. Only five subcategorization frames (as opposed to
26 rule types) have been seen in the 147 cases. The lexically specific distribution
628
Computational Linguistics Volume 29, Number 4
Pml(LC | VP, told) can therefore be quite highly trusted. Relatively little probability
mass is left to the backed-off estimate.
In summary, from the distributions in Table 13, the model should be quite uncertain
about what rules told can appear with. It should be relatively certain, however, about
the subcategorization frame. Introducing subcategorization parameters allows the
model to generalize in an important way about rules. We have carefully isolated the
?core? of rules?the subcategorization frame?that the model should be certain about.
We should note that Charniak?s method will certainly have some advantages in
estimation: It will capture some statistical properties of rules that our independence
assumptions will lose (e.g., the distribution over the number of PP adjuncts seen for a
particular head).
8. Related Work
Unfortunately, because of space limitations, it is not possible to give a complete review
of previous work in this article. In the next two sections we give a detailed comparison
of the models in this article to the lexicalized PCFG model of Charniak (1997) and the
history-based models of Jelinek et al (1994), Magerman (1995), and Ratnaparkhi (1997).
For discussion of additional related work, chapter 4 of Collins (1999) attempts to
give a comprehensive review of work on statistical parsing up to around 1998. Of
particular relevance is other work on parsing the Penn WSJ Treebank (Jelinek et al
1994; Magerman 1995; Eisner 1996a, 1996b; Collins 1996; Charniak 1997; Goodman
1997; Ratnaparkhi 1997; Chelba and Jelinek 1998; Roark 2001). Eisner (1996a, 1996b)
describes several dependency-based models that are also closely related to the mod-
els in this article. Collins (1996) also describes a dependency-based model applied
to treebank parsing. Goodman (1997) describes probabilistic feature grammars and
their application to parsing the treebank. Chelba and Jelinek (1998) describe an in-
cremental, history-based parsing approach that is applied to language modeling for
speech recognition. History-based approaches were introduced to parsing in Black et
al. (1992). Roark (2001) describes a generative probabilistic model of an incremental
parser, with good results in terms of both parse accuracy on the treebank and also
perplexity scores for language modeling.
Earlier work that is of particular relevance considered the importance of relations
between lexical heads for disambiguation in parsing. See Hindle and Rooth (1991) for
one of the earliest pieces of research on this topic in the context of prepositional-phrase
attachment ambiguity. For work that uses lexical relations for parse disambiguation?
all with very promising results?see Sekine et al (1992), Jones and Eisner (1992a,
1992b), and Alshawi and Carter (1994). Statistical models of lexicalized grammatical
formalisms also lead to models with parameters corresponding to lexical dependen-
cies. See Resnik (1992), Schabes (1992), and Schabes and Waters (1993) for work on
stochastic tree-adjoining grammars. Joshi and Srinivas (1994) describe an alternative
?supertagging? model for tree-adjoining grammars. See Alshawi (1996) for work on
stochastic head-automata, and Lafferty, Sleator, and Temperley (1992) for a stochastic
version of link grammar. De Marcken (1995) considers stochastic lexicalized PCFGs,
with specific reference to EM methods for unsupervised training. Seneff (1992) de-
scribes the use of Markov models for rule generation, which is closely related to
the Markov-style rules in the models in the current article. Finally, note that not all
machine-learning methods for parsing are probabilistic. See Brill (1993) and Hermjakob
and Mooney (1997) for rule-based learning systems.
In recent work, Chiang (2000) has shown that the models in the current article
can be implemented almost unchanged in a stochastic tree-adjoining grammar. Bikel
629
Collins Head-Driven Statistical Models for NL Parsing
(2000) has developed generative statistical models that integrate word sense informa-
tion into the parsing process. Eisner (2002) develops a sophisticated generative model
for lexicalized context-free rules, making use of a probabilistic model of lexicalized
transformations between rules. Blaheta and Charniak (2000) describe methods for the
recovery of the semantic tags in the Penn Treebank annotations, a significant step
forward from the complement/adjunct distinction recovered in model 2 of the cur-
rent article. Charniak (2001) gives measurements of perplexity for a lexicalized PCFG.
Gildea (2001) reports on experiments investigating the utility of different features in
bigram lexical-dependency models for parsing. Miller et al (2000) develop generative,
lexicalized models for information extraction of relations. The approach enhances non-
terminals in the parse trees to carry semantic labels and develops a probabilistic model
that takes these labels into account. Collins et al (1999) describe how the models in
the current article were applied to parsing Czech. Charniak (2000) describes a pars-
ing model that also uses Markov processes to generate rules. The model takes into
account much additional context (such as previously generated modifiers, or nonter-
minals higher in the parse trees) through a maximum-entropy-inspired model. The use
of additional features gives clear improvements in performance. Collins (2000) shows
similar improvements through a quite different model based on boosting approaches
to reranking (Freund et al 1998). An initial model?in fact Model 2 described in the
current article?is used to generate N-best output. The reranking approach attempts to
rerank the N-best lists using additional features that are not used in the initial model.
The intention of this approach is to allow greater flexibility in the features that can be
included in the model. Finally, Bod (2001) describes a very different approach (a DOP
approach to parsing) that gives excellent results on treebank parsing, comparable to
the results of Charniak (2000) and Collins (2000).
8.1 Comparison to the Model of Charniak (1997)
We now give a more detailed comparison of the models in this article to the parser of
Charniak (1997). The model described in Charniak (1997) has two types of parameters:
1. Lexical-dependency parameters. Charniak?s dependency parameters are
similar to the L2 parameters of section 5.1. Whereas our parameters are
PL2(lwi | Li, lti, c, p, P, H, w, t,?, LC)
Charniak?s parameters in our notation would be
PL2(lwi | Li, P, w)
For example, the dependency parameter for an NP headed by profits,
which is the subject of the verb rose, would be
P(profits | NP, S, rose).
2. Rule parameters. The second type of parameters are associated with
context-free rules in the tree. As an example, take the S node in the
following tree:
630
Computational Linguistics Volume 29, Number 4
This nonterminal could expand with any of the rules S ? ? in the
grammar. The rule probability is defined as P(S ? ?|rose, S, VP). So the
rule probability depends on the nonterminal being expanded, its
headword, and also its parent.
The next few sections give further explanation of the differences between Charniak?s
models and the models in this article.
8.1.1 Additional Features of Charniak?s Model. There are some notable additional
features of Charniak?s model. First, the rule probabilities are conditioned on the par-
ent of the nonterminal being expanded. Our models do not include this information,
although distinguishing recursive from nonrecursive NPs can be considered a reduced
form of this information. (See section 7.3.2 for a discussion of this distinction; the argu-
ments in that section are also motivation for Charniak?s choice of conditioning on the
parent.) Second, Charniak uses word-class information to smooth probabilities and re-
ports a 0.35% improvement from this feature. Finally, Charniak uses 30 million words
of text for unsupervised training. A parser is trained from the treebank and used to
parse this text; statistics are then collected from this machine-parsed text and merged
with the treebank statistics to train a second model. This gives a 0.5% improvement
in performance.
8.1.2 The Dependency Parameters of Charniak?s Model. Though similar to ours,
Charniak?s dependency parameters are conditioned on less information. As noted
previously, whereas our parameters are PL2(lwi | Li, lti, c, p, P, H, w, t,?, LC), Charniak?s
parameters in our notation would be PL2(lwi | Li, P, w). The additional information
included in our models is as follows:
H The head nonterminal label (VP in the previous profits/rose example). At first
glance this might seem redundant: For example, an S will usually take
a VP as its head. In some cases, however, the head label can vary: For
example, an S can take another S as its head in coordination cases.
lti, t The POS tags for the head and modifier words. Inclusion of these tags al-
lows our models to use POS tags as word class information. Charniak?s
model may be missing an important generalization in this respect. Char-
niak (2000) shows that using the POS tags as word class information in
the model is important for parsing accuracy.
c The coordination flag. This distinguishes, for example, coordination cases from
appositives: Charniak?s model will have the same parameter?P(modifier|
head, NP, NP)?in both of these cases.
p, ?,LC/RC The punctuation, distance, and subcategorization variables. It is dif-
ficult to tell without empirical tests whether these features are important.
631
Collins Head-Driven Statistical Models for NL Parsing
8.1.3 The Rule Parameters of Charniak?s Model. The rule parameters in Charniak?s
model are effectively decomposed into our L1 parameters (section 5.1), the head pa-
rameters, and?in models 2 and 3?the subcategorization and gap parameters. This
decomposition allows our model to assign probability to rules not seen in training
data: See section 7.4 for an extensive discussion.
8.1.4 Right-Branching Structures in Charniak?s Model. Our models use distance fea-
tures to encode preferences for right-branching structures. Charniak?s model does not
represent this information explicitly but instead learns it implicitly through rule prob-
abilities. For example, for an NP PP PP sequence, the preference for a right-branching
structure is encoded through a much higher probability for the rule NP ? NP PP than
for the rule NP ? NP PP PP. (Note that conditioning on the rule?s parent is needed to
disallow the structure [NP [NP PP] PP]; see Johnson [1997] for further discussion.)
This strategy does not encode all of the information in the distance measure. The
distance measure effectively penalizes rules NP ? NPB NP PP where the middle NP
contains a verb: In this case the PP modification results in a dependency that crosses a
verb. Charniak?s model is unable to distinguish cases in which the middle NP contains
a verb (i.e., the PP modification crosses a verb) from those in which it does not.
8.2 A Comparison to the Models of Jelinek et al (1994), Magerman (1995), and Rat-
naparkhi (1997)
We now make a detailed comparison of our models to the history-based models of Rat-
naparkhi (1997), Jelinek et al (1994), and Magerman (1995). A strength of these models
is undoubtedly the powerful estimation techniques that they use: maximum-entropy
modeling (in Ratnaparkhi 1997) or decision trees (in Jelinek et al 1994 and Magerman
1995). A weakness, we will argue in this section, is the method of associating parame-
ters with transitions taken by bottom-up, shift-reduce-style parsers. We give examples
in which this method leads to the parameters? unnecessarily fragmenting the training
data in some cases or ignoring important context in other cases. Similar observations
have been made in the context of tagging problems using maximum-entropy models
(Lafferty, McCallum, and Pereira 2001; Klein and Manning 2002).
We first analyze the model of Magerman (1995) through three common examples
of ambiguity: PP attachment, coordination, and appositives. In each case a word se-
quence S has two competing structures, T1 and T2, with associated decision sequences
?d1, . . . , dn? and ?e1, . . . , em?, respectively. Thus the probability of the two structures can
be written as
P(T1|S) =
?
i=1...n
P(di|d1 . . . di?1, S)
P(T2|S) =
?
i=1...m
P(ei|e1 . . . ei?1, S)
It will be useful to isolate the decision between the two structures to a single probability
term. Let the value j be the minimum value of i such that di = ei. Then we can rewrite
the two probabilities as follows:
P(T1|S) =
?
i=1...j?1
P(di|d1 . . . di?1, S)? P(dj|d1 . . . dj?1, S)?
?
i=j+1...n
P(di|d1 . . . di?1, S)
P(T2|S) =
?
i=1...j?1
P(ei|e1 . . . ei?1, S)? P(ej|e1 . . . ej?1, S)?
?
i=j+1...m
P(ei|e1 . . . ei?1, S)
632
Computational Linguistics Volume 29, Number 4
The first thing to note is that
?
i=1...j?1 P(di|d1 . . . di?1, S) =
?
i=1...j?1 P(ei|e1 . . . ei?1, S),
so that these probability terms are irrelevant to the decision between the two structures.
We make one additional assumption, that
?
i=j+1...n
P(di|d1 . . . di?1, S) ?
?
i=j+1...m
P(ei|e1 . . . ei?1, S) ? 1
This is justified for the examples in this section, because once the jth decision is made,
the following decisions are practically deterministic. Equivalently, we are assuming
that P(T1|S)+P(T2|S) ? 1, that is, that very little probability mass is lost to trees other
than T1 or T2. Given these two equalities, we have isolated the decision between the
two structures to the parameters P(dj|d1 . . . dj?1, S) and P(ej|e1 . . . ej?1, S).
Figure 21 shows a case of PP attachment. The first thing to note is that the PP
attachment decision is made before the PP is even built. The decision is linked to the
NP preceding the preposition: whether the arc above the NP should go left or right.
The next thing to note is that at least one important feature, the verb, falls outside
of the conditioning context. (The model considers only information up to two con-
stituents preceding or following the location of the decision.) This could be repaired
by considering additional context, but there is no fixed bound on how far the verb
can be from the decision point. Note also that in other cases the method fragments
the data in unnecessary ways. Cases in which the verb directly precedes the NP, or is
one place farther to the left, are treated separately.
Figure 22 shows a similar example, NP coordination ambiguity. Again, the pivotal
decision is made in a somewhat counterintuitive location: at the NP preceding the
coordinator. At this point the NP following the coordinator has not been built, and its
head noun is not in the contextual window. Figure 23 shows an appositive example
in which the head noun of the appositive NP is not in the contextual window when
the decision is made.
These last two examples can be extended to illustrate another problem. The NP
after the conjunct or comma could be the subject of a following clause. For example,
Figure 21
(a) and (b) are two candidate structures for the same sequence of words. (c) shows the first
decision (labeled ???) where the two structures differ. The arc above the NP can go either left
(for verb attachment of the PP, as in (a)) or right (for noun attachment of the PP, as in (b)).
633
Collins Head-Driven Statistical Models for NL Parsing
Figure 22
(a) and (b) are two candidate structures for the same sequence of words. (c) shows the first
decision (labeled ???) where the two structures differ. The arc above the NP can go either left
(for high attachment (a) of the coordinated phrase) or right (for low attachment (b) of the
coordinated phrase).
Figure 23
(a) and (b) are two candidate structures for the same sequence of words. (c) shows the first
decision (labeled ???) in which the two structures differ. The arc above the NP can go either
left (for high attachment (a) of the appositive phrase) or right (for noun attachment (b) of the
appositive phrase).
in John likes Mary and Bill loves Jill, the decision not to coordinate Mary and Bill is made
just after the NP Mary is built. At this point, the verb loves is outside the contextual
window, and the model has no way of telling that Bill is the subject of the following
clause. The model is assigning probability mass to globally implausible structures as
a result of points of local ambiguity in the parsing process.
Some of these problems can be repaired by changing the derivation order or the
conditioning context. Ratnaparkhi (1997) has an additional chunking stage, which
means that the head noun does fall within the contextual window for the coordination
and appositive cases.
9. Conclusions
The models in this article incorporate parameters that track a number of linguistic
phenomena: bigram lexical dependencies, subcategorization frames, the propagation
of slash categories, and so on. The models are generative models in which parse
trees are decomposed into a number of steps in a top-down derivation of the tree
634
Computational Linguistics Volume 29, Number 4
and the decisions in the derivation are modeled as conditional probabilities. With a
careful choice of derivation and independence assumptions, the resulting model has
parameters corresponding to the desired linguistic phenomena.
In addition to introducing the three parsing models and evaluating their perfor-
mance on the Penn Wall Street Journal Treebank, we have aimed in our discussion
(in sections 7 and 8) to give more insight into the models: their strengths and weak-
nesses, the effect of various features on parsing accuracy, and the relationship of the
models to other work on statistical parsing. In conclusion, we would like to highlight
the following points:
? Section 7.1 showed, through an analysis of accuracy on different types of
dependencies, that adjuncts are the main sources of error in the parsing
models. In contrast, dependencies forming the ?core? structure of
sentences (for example, dependencies involving complements, sentential
heads, and NP chunks) are all recovered with over 90% precision and
recall.
? Section 7.2 evaluated the effect of the distance measure on parsing
accuracy. A model without either the adjacency distance feature or
subcategorization parameters performs very poorly (76.5% precision,
75% recall), suggesting that the adjacency feature is capturing some
subcategorization information in the model 1 parser. The results in
Table 7 show that the subcategorization, adjacency, and ?verb-crossing?
features all contribute significantly to model 2?s (and by implication
model 3?s) performance.
? Section 7.3 described how the three models are well-suited to the Penn
Treebank style of annotation, and how certain phenomena (particularly
the distance features) may fail to be modeled correctly given treebanks
with different annotation styles. This may be an important point to bear
in mind when applying the models to other treebanks or other
languages. In particular, it may be important to perform transformations
on some structures in treebanks with different annotation styles.
? Section 7.4 gave evidence showing the importance of the models? ability
to break down the context-free rules in the treebank, thereby
generalizing to produce new rules on test examples. Table 12 shows that
precision on section 0 of the treebank decreases from 89.0% to 87.0% and
recall decreases from 88.8% to 87.9% when the model is restricted to
produce only those context-free rules seen in training data.
? Section 8 discussed relationships to the generative model of Charniak
(1997) and the history-based (conditional) models of Ratnaparkhi (1997),
Jelinek et al (1994), and Magerman (1995). Although certainly similar to
Charniak?s model, the three models in this article have some significant
differences, which are identified in section 8.1. (Another important
difference?the ability of models 1, 2, and 3 to generalize to produce
context-free rules not seen in training data?was described in section 7.4.)
Section 8.2 showed that the parsing models of Ratnaparkhi (1997),
Jelinek et al (1994), and Magerman (1995) can suffer from very similar
problems to the ?label bias? or ?observation bias? problem observed in
tagging models, as described in Lafferty, McCallum, and Pereira (2001)
and Klein and Manning (2002).
635
Collins Head-Driven Statistical Models for NL Parsing
Acknowledgments
My Ph.D. thesis is the basis of the work in
this article; I would like to thank Mitch
Marcus for being an excellent Ph.D. thesis
adviser, and for contributing in many ways
to this research. I would like to thank the
members of my thesis committee?Aravind
Joshi, Mark Liberman, Fernando Pereira,
and Mark Steedman?for the remarkable
breadth and depth of their feedback. The
work benefited greatly from discussions
with Jason Eisner, Dan Melamed, Adwait
Ratnaparkhi, and Paola Merlo. Thanks to
Dimitrios Samaras for giving feedback on
many portions of the work. I had
discussions with many other people at
IRCS, University of Pennsylvnia, which
contributed quite directly to this research:
Breck Baldwin, Srinivas Bangalore, Dan
Bikel, James Brooks, Mickey Chandresekhar,
David Chiang, Christy Doran, Kyle Hart, Al
Kim, Tony Kroch, Robert Macintyre, Max
Mintz, Tom Morton, Martha Palmer, Jeff
Reynar, Joseph Rosenzweig, Anoop Sarkar,
Debbie Steinig, Matthew Stone, Ann Taylor,
John Trueswell, Bonnie Webber, Fei Xia, and
David Yarowsky. There was also some
crucial input from sources outside of Penn.
In the summer of 1996 I worked at BBN
Technologies: discussions with Scott Miller,
Richard Schwartz, and Ralph Weischedel
had a deep influence on the research.
Manny Rayner and David Carter from SRI
Cambridge supervised my master?s thesis at
Cambridge University: Their technical
supervision was the beginning of this
research. Finally, thanks to the anonymous
reviewers for their comments.
References
Alshawi, Hiyan. 1996. Head automata and
bilingual tiling: Translation with minimal
representations. Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 167?176.
Alshawi, Hiyan and David Carter. 1994.
Training and scaling preference functions
for disambiguation. Computational
Linguistics, 20(4):635?648.
Bikel, Dan. 2000. A statistical model for
parsing and word-sense disambiguation.
In Proceedings of the Student Research
Workshop at ACL 2000.
Bikel, Dan, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: A
high-performance learning name-finder.
In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages
194?201.
Black, Ezra, Steven Abney, Dan Flickinger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Frederick Jelinek, Judith Klavans, Mark
Liberman, Mitch Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A Procedure for
quantitatively comparing the syntactic
coverage of english grammars. In
Proceedings of the February 1991 DARPA
Speech and Natural Language Workshop.
Black, Ezra, Frederick Jelinek, John Lafferty,
David Magerman, Robert Mercer and
Salim Roukos. 1992. Towards
history-based grammars: Using richer
models for probabilistic parsing. In
Proceedings of the Fifth DARPA Speech and
Natural Language Workshop, Harriman, NY.
Blaheta, Don, and Eugene Charniak. 2000.
Assigning function tags to parsed text. In
Proceedings of the First Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics, pages 234?240,
Seattle.
Bod, Rens. 2001. What is the minimal set of
fragments that achieves maximal parse
accuracy? In Proceedings of ACL 2001.
Booth, Taylor L., and Richard A. Thompson.
1973. Applying probability measures to
abstract languages. IEEE Transactions on
Computers, C-22(5):442?450.
Brill, Eric. 1993. Automatic grammar
induction and parsing free text: A
transformation-based approach. In
Proceedings of the 21st Annual Meeting of the
Association for Computational Linguistics.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. In Proceedings of the Fourteenth
National Conference on Artificial Intelligence,
AAAI Press/MIT Press, Menlo Park, CA.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of NAACL 2000.
Charniak, Eugene. 2001. Immediate-head
parsing for language models. In
Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics.
Chelba, Ciprian, and Frederick Jelinek. 1998.
Exploiting syntactic structure for
language modeling. In Proceedings of
COLING-ACL 1998, Montreal.
Chiang, David. 2000. Statistical parsing with
an automatically-extracted tree adjoining
grammar. In Proceedings of ACL 2000,
Hong Kong, pages 456?463.
Collins, Michael, and James Brooks. 1995.
Prepositional phrase attachment through
a backed-off model. Proceedings of the Third
636
Computational Linguistics Volume 29, Number 4
Workshop on Very Large Corpora, pages
27?38.
Collins, Michael. 1996. A new statistical
parser based on bigram lexical
dependencies. Proceedings of the 34th
Annual Meeting of the Association for
Computational Linguistics, pages 184?191.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
and 8th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 16?23.
Collins, Michael. 1999. Head-driven
statistical models for natural language
parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Collins, Michael, Jan Hajic, Lance Ramshaw,
and Christoph Tillmann. 1999. A
statistical parser for Czech. In Proceedings
of the 37th Annual Meeting of the ACL,
College Park, Maryland.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
Proceedings of the Seventeenth International
Conference on Machine Learning (ICML
2000).
Collins, Michael. 2002. Parameter estimation
for statistical parsing models: Theory and
practice of distribution-free methods. To
appear as a book chapter.
De Marcken, Carl. 1995. On the
unsupervised induction of
phrase-structure grammars. In Proceedings
of the Third Workshop on Very Large Corpora.
Eisner, Jason. 1996a. Three new probabilistic
models for dependency parsing: An
exploration. Proceedings of COLING-96,
pages 340?345.
Eisner, Jason. 1996b. An empirical
comparison of probability models for
dependency grammar. Technical Report
IRCS-96-11, Institute for Research in
Cognitive Science, University of
Pennsylvania, Philadelphia.
Eisner, Jason. 2002. Transformational priors
over grammars. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP),
Philadelphia.
Eisner, Jason, and Giorgio Satta. 1999.
Efficient parsing for bilexical context-free
grammars and head automaton
grammars. In Proceedings of the 37th Annual
Meeting of the ACL.
Freund, Yoav, Raj Iyer, Robert E. Schapire,
and Yoram Singer. 1998. An efficient
boosting algorithm for combining
preferences. In Machine Learning:
Proceedings of the Fifteenth International
Conference. Morgan Kaufmann.
Gazdar, Gerald, E. H. Klein, G. K. Pullum,
and Ivan Sag. 1985. Generalized Phrase
Structure Grammar. Harvard University
Press, Cambridge, MA.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of 2001
Conference on Empirical Methods in Natural
Language Processing (EMNLP), Pittsburgh,
PA.
Goodman, Joshua. 1997. Probabilistic
feature grammars. In Proceedings of the
Fourth International Workshop on Parsing
Technologies.
Hermjakob, Ulf, and Ray Mooney. 1997.
Learning parse and translation decisions
from examples with rich context. In
Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics and
8th Conference of the European Chapter of the
Association for Computational Linguistics,
pages 482?489.
Hindle, Don, and Mats Rooth. 1991.
Structural Ambiguity and Lexical
Relations. In Proceedings of the 29th Annual
Meeting of the Association for Computational
Linguistics.
Hopcroft, John, and J. D. Ullman. 1979.
Introduction to automata theory, languages,
and computation. Addison-Wesley,
Reading, MA.
Jelinek, Frederick, John Lafferty, David
Magerman, Robert Mercer, Adwait
Ratnaparkhi, and Salim Roukos. 1994.
Decision tree parsing using a hidden
derivation model. In Proceedings of the 1994
Human Language Technology Workshop,
pages 272?277.
Johnson, Mark. 1997. The effect of
alternative tree representations on tree
bank grammars. In Proceedings of NeMLAP
3.
Jones, Mark and Jason Eisner. 1992a. A
probabilistic parser applied to software
testing documents. In Proceedings of
National Conference on Artificial Intelligence
(AAAI-92), pages 322?328, San Jose, CA.
Jones, Mark and Jason Eisner. 1992b. A
probabilistic parser and its application. In
Proceedings of the AAAI-92 Workshop on
Statistically-Based Natural Language
Processing Techniques, San Jose, CA.
Joshi, Aravind and Bangalore Srinivas. 1994.
Disambiguation of super parts of speech
(or supertags): Almost parsing. In
International Conference on Computational
Linguistics (COLING 1994), Kyoto
University, Japan, August.
Klein, Dan and Christopher Manning. 2002.
Conditional structure versus conditional
estimation in NLP models. In Proceedings
637
Collins Head-Driven Statistical Models for NL Parsing
of the Conference on Empirical Methods in
Natural Language Processing (EMNLP),
Philadelphia.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of ICML 2001.
Lafferty, John, Daniel Sleator, and David
Temperley. 1992. Grammatical trigrams: A
probabilistic model of link grammar.
Proceedings of the 1992 AAAI Fall Symposium
on Probabilistic Approaches to Natural
Language.
Magerman, David. 1995. Statistical
decision-tree models for parsing. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics,
pages 276?283.
Manning, Christopher D., and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and
Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument
structure. Proceedings of the 1994 Human
Language Technology Workshop, pages
110?115.
Marcus, Mitchell, Beatrice Santorini and M.
Marcinkiewicz. 1993. Building a large
annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19(2):313?330.
Miller, Scott, Heidi Fox, Lance Ramshaw,
and Ralph Weischedel. 2000. A novel use
of statistical parsing to extract
information from text. In Proceedings of the
1st Meeting of the North American Chapter of
the Association for Computational Linguistics
(NAACL), pages 226?233.
Pereira, Fernando, and David Warren. 1980.
Definite clause grammars for language
analysis: A survey of the formalism and a
comparison with augmented transition
networks. Artificial Intelligence, 13:231?278.
Pinker, Stephen. 1994. The Language Instinct.
William Morrow, New York.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
May.
Ratnaparkhi, Adwait. 1997. A linear
observed time statistical parser based on
maximum entropy models. In Proceedings
of the Second Conference on Empirical
Methods in Natural Language Processing,
Brown University, Providence, RI.
Resnik, Philip. 1992. Probabilistic
tree-adjoining grammar as a framework
for statistical natural language processing.
In Proceedings of COLING 1992, vol. 2,
pages 418?424.
Roark, Brian. 2001. Probabilistic top-down
parsing and language modeling.
Computational Linguistics, 27(2):249?276.
Schabes, Yves. 1992. Stochastic lexicalized
tree-adjoining grammars. In Proceedings of
COLING 1992, vol. 2, pages 426?432.
Schabes, Yves and Richard Waters. 1993.
Stochastic lexicalized context-free
grammar. In Proceedings of the Third
International Workshop on Parsing
Technologies.
Sekine, Satoshi, John Carroll, S. Ananiadou,
and J. Tsujii. 1992. Automatic Learning for
Semantic Collocation. In Proceedings of the
Third Conference on Applied Natural
Language Processing.
Seneff, Stephanie. 1992. TINA: A natural
language system for spoken language
applications. Computational Linguistics,
18(1):61-86.
Witten, Ian and Timothy C. Bell. 1991. The
zero-frequency problem: Estimating the
probabilities of novel events in adaptive
text compression. IEEE Transactions on
Information Theory, 37(4):1085?1094.
Articles
Discriminative Reranking for Natural
Language Parsing
Michael Collins
Massachusetts Institute of Technology
Terry Koo
Massachusetts Institute of Technology
This article considers approaches which rerank the output of an existing probabilistic parser.
The base parser produces a set of candidate parses for each input sentence, with associated
probabilities that define an initial ranking of these parses. A second model then attempts to
improve upon this initial ranking, using additional features of the tree as evidence. The strength
of our approach is that it allows a tree to be represented as an arbitrary set of features, without
concerns about how these features interact or overlap and without the need to define a
derivation or a generative model which takes these features into account. We introduce a new
method for the reranking task, based on the boosting approach to ranking problems described in
Freund et al (1998). We apply the boosting method to parsing the Wall Street Journal treebank.
The method combined the log-likelihood under a baseline model (that of Collins [1999]) with
evidence from an additional 500,000 features over parse trees that were not included in the
original model. The new model achieved 89.75% F-measure, a 13% relative decrease in F-
measure error over the baseline model?s score of 88.2%. The article also introduces a new
algorithm for the boosting approach which takes advantage of the sparsity of the feature space in
the parsing data. Experiments show significant efficiency gains for the new algorithm over the
obvious implementation of the boosting approach. We argue that the method is an appealing
alternative?in terms of both simplicity and efficiency?to work on feature selection methods
within log-linear (maximum-entropy) models. Although the experiments in this article are on
natural language parsing (NLP), the approach should be applicable to many other NLP
problems which are naturally framed as ranking tasks, for example, speech recognition, machine
translation, or natural language generation.
1. Introduction
Machine-learning approaches to natural language parsing have recently shown some
success in complex domains such as news wire text. Many of these methods fall into
the general category of history-based models, in which a parse tree is represented as a
derivation (sequence of decisions) and the probability of the tree is then calculated as a
product of decision probabilities. While these approaches have many advantages, it
can be awkward to encode some constraints within this framework. In the ideal case,
the designer of a statistical parser would be able to easily add features to the model
* 2005 Association for Computational Linguistics
 MIT Computer Science and Artificial Intelligence Laboratory (CSAIL), the Stata Center, Building 32,
32 Vassar Street, Cambridge, MA 02139. Email: mcollins@csail.mit.edu, maestro@mit.edu.
Submission received: 15th October 2003; Accepted for publication: 29th April 2004
that are believed to be useful in discriminating among candidate trees for a sentence.
In practice, however, adding new features to a generative or history-based model can
be awkward: The derivation in the model must be altered to take the new features into
account, and this can be an intricate task.
This article considers approaches which rerank the output of an existing
probabilistic parser. The base parser produces a set of candidate parses for each
input sentence, with associated probabilities that define an initial ranking of these
parses. A second model then attempts to improve upon this initial ranking, using
additional features of the tree as evidence. The strength of our approach is that it
allows a tree to be represented as an arbitrary set of features, without concerns about
how these features interact or overlap and without the need to define a derivation
which takes these features into account.
We introduce a new method for the reranking task, based on the boosting
approach to ranking problems described in Freund et al (1998). The algorithm can be
viewed as a feature selection method, optimizing a particular loss function (the
exponential loss function) that has been studied in the boosting literature. We applied
the boosting method to parsing the Wall Street Journal (WSJ) treebank (Marcus,
Santorini, and Marcinkiewicz 1993). The method combines the log-likelihood under a
baseline model (that of Collins [1999]) with evidence from an additional 500,000
features over parse trees that were not included in the original model. The baseline
model achieved 88.2% F-measure on this task. The new model achieves 89.75% F-
measure, a 13% relative decrease in F-measure error.
Although the experiments in this article are on natural language parsing, the
approach should be applicable to many other natural language processing (NLP)
problems which are naturally framed as ranking tasks, for example, speech
recognition, machine translation, or natural language generation. See Collins (2002a)
for an application of the boosting approach to named entity recognition, and Walker,
Rambow, and Rogati (2001) for the application of boosting techniques for ranking in
the context of natural language generation.
The article also introduces a new, more efficient algorithm for the boosting
approach which takes advantage of the sparse nature of the feature space in the
parsing data. Other NLP tasks are likely to have similar characteristics in terms of
sparsity. Experiments show an efficiency gain of a factor of 2,600 for the new algorithm
over the obvious implementation of the boosting approach. Efficiency issues are
important, because the parsing task is a fairly large problem, involving around one
million parse trees and over 500,000 features. The improved algorithm can perform
100,000 rounds of feature selection on our task in a few hours with current processing
speeds. The 100,000 rounds of feature selection require computation equivalent to
around 40 passes over the entire training set (as opposed to 100,000 passes for the
??naive?? implementation).
The problems with history-based models and the desire to be able to specify
features as arbitrary predicates of the entire tree have been noted before. In particular,
previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della
Pietra, and Lafferty 1997; Johnson et al 1999; Riezler et al 2002) has investigated the
use of Markov random fields (MRFs) or log-linear models as probabilistic models with
global features for parsing and other NLP tasks. (Log-linear models are often referred
to as maximum-entropy models in the NLP literature.) Similar methods have also been
proposed for machine translation (Och and Ney 2002) and language understanding in
dialogue systems (Papineni, Roukos, and Ward 1997, 1998). Previous work (Friedman,
Hastie, and Tibshirani 1998) has drawn connections between log-linear models and
26
Computational Linguistics Volume 31, Number 1
27
boosting for classification problems. One contribution of our research is to draw
similar connections between the two approaches to ranking problems.
We argue that the efficient boosting algorithm introduced in this article is an
attractive alternative to maximum-entropy models, in particular, feature selection
methods that have been proposed in the literature on maximum-entropy models. The
earlier methods for maximum-entropy feature selection methods (Ratnaparkhi,
Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra,
Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require
several full passes over the training set for each round of feature selection, suggesting
that at least for the parsing data, the improved boosting algorithm is several orders of
magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to
these earlier methods for feature selection, as well as the more recent work of
McCallum (2003); Zhou et al (2003); and Riezler and Vasserman (2004).
The remainder of this article is structured as follows. Section 2 reviews history-
based models for NLP and highlights the perceived shortcomings of history-based
models which motivate the reranking approaches described in the remainder of the
article. Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000;
Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty
2001; Collins, Schapire, and Singer 2002) that derives connections between boosting
and maximum-entropy models for the simpler case of classification problems; this
work forms the basis for the reranking methods. Section 4 describes how these
approaches can be generalized to ranking problems. We introduce loss functions for
boosting and MRF approaches and discuss optimization methods. We also derive the
efficient algorithm for boosting in this section. Section 5 gives experimental results,
investigating the performance improvements on parsing, efficiency issues, and the
effect of various parameters of the boosting algorithm. Section 6 discusses related work
in more detail. Finally, section 7 gives conclusions.
The reranking models in this article were originally introduced in Collins (2000). In
this article we give considerably more detail in terms of the algorithms involved, their
justification, and their performance in experiments on natural language parsing.
2. History-Based Models
Before discussing the reranking approaches, we describe history-based models (Black
et al 1992). They are important for a few reasons. First, several of the best-performing
parsers on the WSJ treebank (e.g., Ratnaparkhi 1997; Charniak 1997, 2000; Collins 1997,
1999; Henderson 2003) are cases of history-based models. Many systems applied to
part-of-speech tagging, speech recognition, and other language or speech tasks also fall
into this class of model. Second, a particular history-based model (that of Collins
[1999]) is used as the initial model for our approach. Finally, it is important to describe
history-based models?and to explain their limitations?to motivate our departure
from them.
Parsing can be framed as a supervised learning task, to induce a function f : XYY
given training examples ?xi, yi?, where xi Z X , yi Z Y. We define GEN?x??Y to be the
set of candidates for a given input x. In the parsing problem x is a sentence, and
1 Note, however, that log-linear models which employ regularization methods instead of feature
selection?see, for example, Johnson et al (1999) and Lafferty, McCallum, and Pereira (2001)?are likely
to be comparable in terms of efficiency to our feature selection approach. See section 6.3 for more
discussion.
Collins and Koo Discriminative Reranking for NLP
GEN?x? is a set of candidate trees for that sentence. A particular characteristic of the
problem is the complexity of GEN?x? : GEN?x? can be very large, and each member of
GEN?x? has a rich internal structure. This contrasts with ??typical?? classification prob-
lems in which GEN?x? is a fixed, small set, for example, f1;?1g in binary
classification problems.
In probabilistic approaches, a model is defined which assigns a probability P?x, y?
to each ?x, y? pair.2 The most likely parse for each sentence x is then arg maxyZGEN(x)
P?x, y?. This leaves the question of how to define P?x, y?. In history-based approaches,
a one-to-one mapping is defined between each pair ?x, y? and a decision sequence
bd1::: dn?. The sequence bd1::: dn? can be thought of as the sequence of moves that build
?x, y? in some canonical order. Given this mapping, the probability of a tree can be
written as
P?x, y? ?
Y
i?1:::n
P?di)F?d1::: di1??
Here, ?d1::: di1? is the history for the ith decision. F is a function which groups histo-
ries into equivalence classes, thereby making independence assumptions in the model.
Probabilistic context-free grammars (PCFGs) are one example of a history-based
model. The decision sequence bd1::: dn? is defined as the sequence of rule expansions in
a top-down, leftmost derivation of the tree. The history is equivalent to a partially built
tree, and F picks out the nonterminal being expanded (i.e., the leftmost nonterminal in
the fringe of this tree), making the assumption that P?dijd1::: di1? depends only on
the nonterminal being expanded. In the resulting model a tree with rule expansions
bAiYbi? is assigned a probability
Q
i?1
n P?bijAi?.
Our base model, that of Collins (1999), is also a history-based model. It can be con-
sidered to be a type of PCFG, where the rules are lexicalized. An example rule would be
VP?saw? > VBD?saw? NPC?her? NP?today?
Lexicalization leads to a very large number of rules; to make the number of parameters
manageable, the generation of the right-hand side of a rule is broken down into a number
of decisions, as follows:
 First the head nonterminal (VBD in the above example) is chosen.
 Next, left and right subcategorization frames are chosen
({} and {NP-C}).
 Nonterminal sequences to the left and right of the VBD are chosen
(an empty sequence to the left, bNP-C, NP? to the right).
 Finally, the lexical heads of the modifiers are chosen (her and today).
28
2 To be more precise, generative probabilistic models assign joint probabilities P?x, y? to each ?x, y? pair.
Similar arguments apply to conditional history-based models, which define conditional probabilities
P?y j x? through a definition
P?y j x? ?
Y
i?1:::n
P?di j F?d1::: di1, x??
where d1 . . . dn are again the decisions made in building a parse, and F is a function that groups histories
into equivalence classes. Note that x is added to the domain of F (the context on which decisions are
conditioned). See Ratnaparkhi (1997) for one example of a method using this approach.
Computational Linguistics Volume 31, Number 1
29
Figure 1 illustrates this process. Each of the above decisions has an associated
probability conditioned on the left-hand side of the rule (VP(saw)) and other infor-
mation in some cases.
History-based approaches lead to models in which the log-probability of a parse
tree can be written as a linear sum of parameters ak multiplied by features hk. Each
feature hk?x, y? is the count of a different ??event?? or fragment within the tree. As an
example, consider a PCFG with rules bAkYbk? for 1  k  m. If hk?x, y? is the number
of times bAkYbk? is seen in the tree, and ak ? log P?bkjAk? is the parameter associated
with that rule, then
log P?x, y? ?
Xm
k?1
akhk?x, y?
All models considered in this article take this form, although in the boosting models
the score for a parse is not a log-probability. The features hk define an m-dimensional
vector of counts which represent the tree. The parameters ak represent the influence of
each feature on the score of a tree.
A drawback of history-based models is that the choice of derivation has a
profound influence on the parameterization of the model. (Similar observations have
been made in the related cases of belief networks [Pearl 1988], and language models
for speech recognition [Rosenfeld 1997].) When designing a model, it would be
desirable to have a framework in which features can be easily added to the model.
Unfortunately, with history-based models adding new features often requires a
modification of the underlying derivations in the model. Modifying the derivation to
include a new feature type can be a laborious task. In an ideal situation we would be
able to encode arbitrary features hk, without having to worry about formulating a
derivation that included these features.
To take a concrete example, consider part-of-speech tagging using a hidden
Markov model (HMM). We might have the intuition that almost every sentence has at
least one verb and therefore that sequences including at least one verb should have
increased scores under the model. Encoding this constraint in a compact way in an
HMM takes some ingenuity. The obvious approach?to add to each state the
information about whether or not a verb has been generated in the history?doubles
Figure 1
The sequence of decisions involved in generating the right-hand side of a lexical rule.
Collins and Koo Discriminative Reranking for NLP
the number of states (and parameters) in the model. In contrast, it would be trivial to
implement a feature hk?x, y? which is 1 if y contains a verb, 0 otherwise.
3. Logistic Regression and Boosting
We now turn to machine-learning methods for the ranking task. In this section we
review two methods for binary classification problems: logistic regression (or
maximum-entropy) models and boosting. These methods form the basis for the
reranking approaches described in later sections of the article. Maximum-entropy
models are a very popular method within the computational linguistics community;
see, for example, Berger, Della Pietra, and Della Pietra (1996) for an early article which
introduces the models and motivates them. Boosting approaches to classification have
received considerable attention in the machine-learning community since the intro-
duction of AdaBoost by Freund and Schapire (1997).
Boosting algorithms, and in particular the relationship between boosting
algorithms and maximum-entropy models, are perhaps not familiar topics in the
NLP literature. However there has recently been much work drawing connections
between the two methods (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy
and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001;
Collins, Schapire, and Singer 2002); in this section we review this work. Much of this
work has focused on binary classification problems, and this section is also restricted
to problems of this type. Later in the article we show how several of the ideas can be
carried across to reranking problems.
3.1 Binary Classification Problems
The general setup for binary classification problems is as follows:
 The ??input domain?? (set of possible inputs) is X .
 The ??output domain?? (set of possible labels) is simply a set of two
labels, Y = {1, +1}.3
 The training set is an array of n labeled examples,
b?x1, y1?, ?x2; y2?, : : : , ?xn, yn??, where each xi ZX , yi ZY.
 Input examples are represented through m ??features,?? which are
functions hk : XY < for k = 1, . . . , m. It is also sometimes convenient
to think of an example x as being represented by an m-dimensional
??feature vector?? 7?x? ? bh1?x?, h2?x?, : : : , hm?x??.
 Finally, there is a parameter vector, a ? ba1, : : : ,am?,
where each ak Z <, hence a? is an m-dimensional real-valued vector.
We show that both logistic regression and boosting implement a linear, or hyperplane,
classifier. This means that given an input example x and parameter values a?, the
output from the classifier is
sign ?F?x, a??? ?1?
30
3 It turns out to be convenient to define Y = {1, +1} rather than Y = {0, +1}, for example.
Computational Linguistics Volume 31, Number 1
31
where
F?x, a?? ?
Xm
k?1
akhk?x? ? a? I 7?x? ?2?
Here a? I 7?x? is the inner or dot product between the vectors a? and 7?x?, and sign(z) =
1 if z  0, sign(z) = 1 otherwise. Geometrically, the examples x are represented as
vectors 7(x) in some m-dimensional vector space, and the parameters a? define a
hyperplane which passes through the origin4 of the space and has a? as its normal.
Points lying on one side of this hyperplane are classified as +1; points on the other side
are classified as 1. The central question in learning is how to set the parameters a?,
given the training examples b?x1, y1?, ?x2, y2?, : : : ,?xn, yn??. Logistic regression and
boosting involve different algorithms and criteria for training the parameters a?, but
recent work (Friedman, Hastie, and Tibshirani 2000; Lafferty 1999; Duffy and
Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins,
Schapire, and Singer 2002) has shown that the methods have strong similarities. The
next section describes parameter estimation methods.
3.2 Loss Functions for Logistic Regression and Boosting
A central idea in both logistic regression and boosting is that of a loss function, which
drives the parameter estimation methods of the two approaches. This section describes
loss functions for binary classification. Later in the article, we introduce loss functions
for reranking tasks which are closely related to the loss functions for classification tasks.
First, consider a logistic regression model. The parameters of the model a? are used
to define a conditional probability
P?y j x, a?? ? e
yF?x, a??
1 ? eyF?x, a??
?3?
where F?x, a?? is as defined in equation (2). Some form of maximum-likelihood
estimation is often used for parameter estimation. The parameters are chosen to
maximize the log-likelihood of the training set; equivalently: we talk (to emphasize the
similarities to the boosting approach) about minimizing the negative log-likelihood.
The negative log-likelihood, LogLoss(a?), is defined as
LogLoss ?a?? ? 
Xn
i?1
log P?yi j xi, a??? 
Xn
i?1
log
eyiF?xi, a??
1 ? eyiF?xi, a??
 
?
Xn
i?1
log 1 ? eyiF?xi, a??
 
?4?
There are many methods in the literature for minimizing LogLoss(a?) with respect to
a?, for example, generalized or improved iterative scaling (Berger, Della Pietra, and
4 It might seem to be a restriction to have the hyperplane passing through the origin of the space. However
if a constant ??bias?? feature hm?1?x? ? 1 for all x is added to the representation, a hyperplane passing
through the origin in this new space is equivalent to a hyperplane in general position in the original
m-dimensional space.
Collins and Koo Discriminative Reranking for NLP
Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradient
methods (Malouf 2002). In the next section we describe feature selection methods, as
described in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra,
and Lafferty (1997).
Once the parameters a? are estimated on training examples, the output for an
example x is the most likely label under the model,
arg max
yZY
P?y j x, a?? ? arg max
yZf1,?1g
yF?x, a?? ? sign?F?x, a??? ?5?
where as before, sign ?z? ? 1 if z  0, sign ?z? ? 1 otherwise. Thus we see that the
logistic regression model implements a hyperplane classifier.
In boosting, a different loss function is used, namely, ExpLoss(a?), which is defined
as
ExpLoss?a?? ?
Xn
i?1
eyiF?xi, a?? ?6?
This loss function is minimized using a feature selection method, which we describe in
the next section.
There are strong similarities between LogLoss (equation (4)) and ExpLoss
(equation (6)). In making connections between the two functions, it is useful to
consider a third function of the parameters and training examples,
Error?a?? ?
Xn
i?1
gyiF?xi, a??  0? ?7?
where gp? is one if p is true, zero otherwise. Error(a?) is the number of incorrectly
classified training examples under parameter values a?.
Finally, it will be useful to define the margin on the ith training example, given
parameter values a?, as
Mi?a?? ? yiF?xi, a?? ?8?
With these definitions, the three loss functions can be written in the following form:
LogLoss?a?? ?
Xn
i?1
f ?Mi?a???, where f ?z? ? log?1 ? ez?
ExpLoss?a?? ?
Xn
i?1
f ?Mi?a???, where f ?z? ? ez
Error?a?? ?
Xn
i?1
f ?Mi?a???, where f ?z? ? gz  0?
The three loss functions differ only in their choice of an underlying ??potential
function?? of the margins, f(z). This function is f(z) = log (1 + ez), f(z) = ez, or
32
Computational Linguistics Volume 31, Number 1
33
f ?z? ? gz  0? for LogLoss, ExpLoss, and Error, respectively. The f(z) functions penalize
nonpositive margins on training examples. The simplest function, f ?z? ? gz  0?, gives
a cost of one if a margin is negative (an error is made), zero otherwise. ExpLoss and
LogLoss involve definitions for f?z? which quickly tend to zero as z Y V but heavily
penalize increasingly negative margins.
Figure 2 shows plots for the three definitions of f ?z?. The functions f ?z? ? ez and
f ?z? ? log ?1 ? ez? are both upper bounds on the error function, so that mini-
mizing either LogLoss or ExpLoss can be seen as minimizing an upper bound on
the number of training errors. (Note that minimizing Error(a?) itself is known to be
at least NP-hard if no parameter settings can achieve zero errors on the training
set; see, for example, Hoffgen, van Horn, and Simon [1995].) As zYV, the func-
tions f ?z? ? ez and f ?z? ? log?1 ? ez? become increasingly similar, because log
?1 ? ez?Y ez as ez Y 0. For negative z, the two functions behave quite differently.
f ?z? ? ez shows an exponentially growing cost function as zY V. In contrast, as
zYV it can be seen that log?1 ? ez?Y log?ez? ? z, so this function shows
asymptotically linear growth for negative z. As a final remark, note that both f ?z? ?
ez and f ?z? ? log?1 ? ez? are convex in z, with the result that LogLoss?a?? and
ExpLoss?a?? are convex in the parameters a?. This means that there are no problems
with local minima when optimizing these two loss functions.
3.3 Feature Selection Methods
In this article we concentrate on feature selection methods: algorithms which aim to
make progress in minimizing the loss functions LogLoss?a?? and ExpLoss?a?? while
using a small number of features (equivalently, ensuring that most parameter values in
Figure 2
Potential functions underlying ExpLoss, LogLoss, and Error. The graph labeled ExpLoss is a plot
of f ?z? ? ez for z ? ?1:5 : : :1:5; LogLoss shows a similar plot for f ?z? ? log?1 ? ez?; Error is a
plot of f ?z? ? gz  0?.
Collins and Koo Discriminative Reranking for NLP
a? are zero). Roughly speaking, the motivation for using a small number of features is
the hope that this will prevent overfitting in the models.
Feature selection methods have been proposed in the maximum-entropy literature
by several authors (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and
Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and
Ward 1997, 1998; McCallum 2003; Zhou et al 2003; Riezler and Vasserman 2004). The
most basic approach?for example see Ratnaparkhi, Roukos, and Ward (1994) and
Berger, Della Pietra, and Della Pietra (1996)?involves selection of a single feature at
each iteration, followed by an update to the entire model, as follows:
Step 1: Throughout the algorithm, maintain a set of active features. Initialize this set to
be empty.
Step 2: Choose a feature from outside of the set of active features which has the largest
estimated impact in terms of reducing the loss function LogLoss, and add this to the
active feature set.
Step 3: Minimize LogLoss?a?? with respect to the set of active features; that is, allow
only the active features to take nonzero parameter values when minimizing LogLoss.
Return to Step 2.
Methods in the boosting literature (see, for example, Schapire and Singer [1999]) can
be considered to be feature selection methods of the following form:
Step 1: Start with all parameter values set to zero.
Step 2: Choose a feature which has largest estimated impact in terms of reducing the
loss function ExpLoss.
Step 3: Update the parameter for the feature chosen at Step 2 in such a way as to
minimize ExpLoss?a?? with respect to this one parameter. All other parameter values
are left fixed. Return to Step 2.
The difference with this latter ??boosting?? approach is that in Step 3, only one
parameter value is adjusted, namely, the parameter corresponding to the newly chosen
feature. Note that in this framework, the same feature may be chosen at more than one
iteration.5 The maximum-entropy feature selection method can be quite inefficient,
as the entire model is updated at each step. For example, Ratnaparkhi (1998) quotes
times of around 30 hours for 500 rounds of feature selection on a prepositional-
phrase attachment task. These experiments were performed in 1998, when pro-
cessors were no doubt considerably slower than those available today. However,
the PP attachment task is much smaller than the parsing task that we are address-
ing: Our task involves around 1,000,000 examples, with perhaps a few hundred features
per example, and 100,000 rounds of feature selection; this compares to 20,000 exam-
ples, 16 features per example, and 500 rounds of feature selection for the PP attach-
ment task in Ratnaparkhi (1998). As an estimate, assuming that computational
complexity scales linearly in these factors,6 our task is 1,000,00020,000  32016 
100,000
500 ? 200,000
34
5 That is, the feature may be repeatedly updated, although the same feature will never be chosen
in consecutive iterations, because after an update the model is minimized with respect to the
selected feature.
6 We believe this is a realistic assumption, as each round of feature selection takes O?nf ? time, where n
is the number of training examples, and f is the number of active features on each example.
Computational Linguistics Volume 31, Number 1
35
as large as the PP attachment task. These figures suggest that the maximum-entropy
feature selection approach may be infeasible for large-scale tasks such as the one in this
article.
The fact that the boosting approach does not update the entire model at each
round of feature selection may be a disadvantage in terms of the number of features or
the test data accuracy of the final model. There is reason for concern that Step 2 will at
some iterations mistakenly choose features which are apparently useful in reducing
the loss function, but which would have little utility if the entire model had been
optimized at the previous iteration of Step 3. However, previous empirical results for
boosting have shown that it is a highly effective learning method, suggesting that this
is not in fact a problem for the approach. Given the previous strong results for the
boosting approach, and for reasons of computational efficiency, we pursue the
boosting approach to feature selection in this article.
3.4 Statistical Justification for the Methods
Minimization of LogLoss is most often justified as a parametric, maximum-likelihood
(ML) approach to estimation. Thus this approach benefits from the usual guarantees
for ML estimation: If the distribution generating examples is within the class of
distributions specified by the log-linear form, then in the limit as the sample size goes
to infinity, the model will be optimal in the sense of convergence to the true underlying
distribution generating examples. As far as we are aware, behavior of the models for
finite sample sizes is less well understood. In particular, while feature selection
methods have often been proposed for maximum-entropy models, little theoretical
justification (in terms of guarantees about generalization) has been given for them. It
seems intuitive that a model with a smaller number of parameters will require fewer
samples for convergence, but this is not necessarily the case, and at present this
intuition lacks a theoretical basis. Feature selection methods can probably be
motivated either from a Bayesian perspective (through a prior favoring models with
a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit
perspective (models with fewer parameters are less likely to fit the data by chance), but
this requires additional research.
The statistical justification for boosting approaches is quite different. Boosting
algorithms were originally developed within the PAC framework (Valiant 1984) for
machine learning, specifically to address questions regarding the equivalence of weak
and strong learning. Freund and Schapire (1997) originally introduced AdaBoost and
gave a first set of statistical guarantees for the algorithm. Schapire et al (1998) gave a
second set of guarantees based on the analysis of margins on training examples. Both
papers assume that a fixed distribution D(x, y) is generating both training and test
examples and that the goal is to find a hypothesis with a small number of expected
errors with respect to this distribution. The form of the distribution is not assumed to
be known, and in this sense the guarantees are nonparametric, or ??distribution free.??
Freund and Schapire (1997) show that if the weak learning assumption holds (i.e.,
roughly speaking, a feature with error rate better than chance can be found for any
distribution over the sample space X  {1, +1}), then the training error for the
ExpLoss method decreases rapidly enough for there to be good generalization to test
examples. Schapire et al (1998) show that under the same assumption, minimization of
ExpLoss using the feature selection method ensures that the distribution of margins on
training data develops in such a way that good generalization performance on test
examples is guaranteed.
Collins and Koo Discriminative Reranking for NLP
3.5 Boosting with Complex Feature Spaces
Thus far in this article we have presented boosting as a feature selection approach. In
this section, we note that there is an alternative view of boosting in which it is
described as a method for combining multiple models, for example, as a method for
forming a linear combination of decision trees. We consider only the simpler, feature
selection view of boosting in this article. This section is included for completeness and
because the more general view of boosting may be relevant to future work on boosting
approaches for parse reranking (note, however, that the discussion in this section is not
essential to the rest of the article, so the reader may safely skip this section if she or he
wishes to do so).
In feature selection approaches, as described in this article, the set of possible
features hk?x? for k = 1, . . . , m is taken to be a fixed set of relatively simple functions. In
particular, we have assumed that m is relatively small (for example, small enough for
algorithms that require O(m) time or space to be feasible). More generally, however,
boosting can be applied in more complex settings. For example, a common use of
boosting is to form a linear combination of decision trees. In this case each example x is
represented as a number of attribute-value pairs, and each ??feature?? hk(x) is a complete
decision tree built on predicates over the attribute values in x. In this case the number
of features m is huge: There are as many features as there are decision trees over the
given set of attributes, thus m grows exponentially quickly with the number of
attributes that are used to represent an example x. Boosting may even be applied in
situations in which the number of features is infinite. For example, it may be used to
form a linear combination of neural networks. In this case each feature hk(x)
corresponds to a different parameter setting within the (infinite) set of possible
parameter settings for the neural network.
In more complex settings such as boosting of decision trees or neural networks, it
is generally not feasible to perform an exhaustive search (with O(m) time complexity)
for the feature which has the greatest impact on the exponential7 loss function. Instead,
an approximate search is performed. In boosting approaches, this approximate search
is achieved through a protocol in which at each round of boosting, a ??distribution??
over the training examples is maintained. The distribution can be interpreted as
assigning an importance weight to each training example, most importantly giving
higher weight to examples which are incorrectly classified. At each round of boosting
the distribution is passed to an algorithm such as a decision tree or neural network
learning method, which attempts to return a feature (a decision tree, or a neural
network parameter setting) which has a relatively low error rate with respect to the
distribution. The feature that is returned is then incorporated into the linear
combination of features. The algorithm which generates a classifier given a
distribution over the examples (for example, the decision tree induction method) is
usually referred to as ??the weak learner.?? The weak learner generally uses an
approximate (for example, greedy) method to find a function with a low error rate
with respect to the distribution. Freund and Schapire (1997) show that provided that at
each round of boosting the weak learner returns a feature with greater than (50 + &) %
accuracy for some fixed &, the number of training errors falls exponentially quickly
with the number of rounds of boosting. This fast drop in training errors translates to
statistical bounds on generalization performance (Freund and Schapire 1997).
36
7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman
et al (2000) and Duffy and Helmbold (1999).
Computational Linguistics Volume 31, Number 1
37
Under this view of boosting, the feature selection methods in this article are a
particularly simple case in which the weak learner can afford to exhaustively search
through the space of possible features. Future work on reranking approaches might
consider other approaches?such as boosting of decision trees?which can effectively
consider more complex features.
4. Reranking Approaches
This section describes how the ideas from classification problems can be extended to
reranking tasks. A baseline statistical parser is used to generate N-best output both for
its training set and for test data sentences. Each candidate parse for a sentence is
represented as a feature vector which includes the log-likelihood under the baseline
model, as well as a large number of additional features. The additional features can in
principle be any predicates over sentence/tree pairs. Evidence from the initial log-
likelihood and the additional features is combined using a linear model. Parameter
estimation becomes a problem of learning how to combine these different sources of
information. The boosting algorithm we use is related to the generalization of boosting
methods to ranking problems in Freund et al (1998); we also introduce an approach
related to the conditional log-linear models of Ratnaparkhi, Roukos, and Ward (1994),
Papineni, Roukos, and Ward (1997, 1998), Johnson et al (1999), Riezler et al (2002), and
Och and Ney (2002).
Section 4.1 gives a formal definition of the reranking problem. Section 4.2
introduces loss functions for reranking that are analogous to the LogLoss and ExpLoss
functions in section 3.2. Section 4.3 describes a general approach to feature selection
methods with these loss functions. Section 4.4 describes a first algorithm for the
exponential loss (ExpLoss) function; section 4.5 introduces a more efficient algorithm
for the case of ExpLoss. Finally, section 4.6 describes issues in feature selection
algorithms for the LogLoss function.
4.1 Problem Definition
We use the following notation in the rest of this article:
 si is the ith sentence in the training set. There are n sentences in training
data, so that 1  i  n.
 xi, j is the jth parse of the ith sentence. There are ni parses for the ith
sentence, so that 1  i  n and 1  j  ni. Each xi, j contains both the
tree and the underlying sentence (i.e., each xi, j is a pair bsi, ti, j?,
where si is the ith sentence in the training data, and ti, j is the jth tree
for this sentence). We assume that the parses are distinct, that is, that
xi, j m xi, j? for j m j?.
 Score?xi, j? is the ??score?? for parse xi, j, a measure of the similarity of
xi, j to the gold-standard parse. For example, Score?xi, j? might be the
F-measure accuracy of parse xi, j compared to the gold-standard parse
for si.
 Q?xi, j) is the probability that the base parsing model assigns to parse xi, j.
L?xi, j? ? log Q?xi, j? is the log-probability.
Collins and Koo Discriminative Reranking for NLP
 Without loss of generality, we assume xi,1 to be the highest-scoring
parse for the ith sentence.8 More precisely, for all i, 2  j  ni,
Score?xi,1? > Score?xi, j?. Note that xi,1 may not be identical to the
gold-standard parse; in some cases the parser may fail to propose
the correct parse anywhere in its list of candidates.9
Thus our training data consist of a set of parses, fxi, j : i ? 1, : : : , n, j ? 1, : : : , nig,
together with scores Score?xi, j? and log-probabilities L?xi, j?.
We represent candidate parse trees through m features, hk for k ? 1, : : : , m: Each hk
is an indicator function, for example,
hk?x? ? 1 if x contains the rule bS Y NP VP?
0 otherwise
We show that the restriction to binary-valued features is important for the simplicity
and efficiency of the algorithms.10 We also assume a vector of m + 1 parameters, a? =
{a0, a1, . . . , am}. Each ai can take any value in the reals. The ranking function for a
parse tree x implied by a parameter vector a? is defined as
F?x, a?? ? a0L?x? ?
Xm
k?1
akhk?x?
Given a new test sentence s, with parses xj for j = 1, : : : , N, the output of the model is
the highest-scoring tree under the ranking function
arg max
xZfx1 : : : xNg
F?x, a??
Thus F?x, a?? can be interpreted as a measure of how plausible a parse x is, with higher
scores meaning that x is more plausible. Competing parses for the same sentence are
ranked in order of plausibility by this function. We can recover the base ranking
function?the log-likelihood L?x??by setting a0 to a positive constant and setting all
other parameter values to be zero. Our intention is to use the training examples to pick
parameter values which improve upon this initial ranking.
We now discuss how to set these parameters. First we discuss loss functions
Loss?a?? which can be used to drive the training process. We then go on to describe
feature selection methods for the different loss functions.
38
8 In the event that multiple parses get the (same) highest score, the parse with the highest value of log-
likelihood L under the baseline model is taken as xi, 1. In the event that two parses have the same score
and the same log-likelihood?which occurred rarely if ever in our experiments?we make a random
choice between the two parses.
9 This is not necessarily a significant issue if an application using the output from the parser is sensitive to
improvements in evaluation measures such as precision and recall that give credit for partial matches
between the parser?s output and the correct parse. In this case, it is important only that the precision/
recall for xi, 1 is significantly higher than that of the baseline parser, that is, that there is some ??head room??
for the reranking module in terms of precision and recall.
10 In particular, this restriction allows closed-form parameter updates for the models based on ExpLoss that
we consider. Note that features tracking the counts of different rules can be simulated through several
features which take value one if a rule is seen  1 time,  2 times  3 times, and so on.
Computational Linguistics Volume 31, Number 1
39
4.2 Loss Functions for Ranking Problems
4.2.1 Ranking Errors and Margins. The loss functions we consider are all related to
the number of ranking errors a function F makes on the training set. The ranking error
rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best
parse:
Error ?a?? ?
X
i
Xni
j?2
gF?xi,1, a??  F?xi,j, a??? ?
X
i
Xni
j?2
gF?xi,1, a??  F?xi,j, a??  0?
where again, gp? is one if p is true, zero otherwise. In the ranking problem we define the
margin for each example xi,j such that i = 1, : : : , n, j = 2, : : : , ni, as
Mij?a?? ? F?xi,1, a??  F?xi,j, a??
Thus Mij?a?? is the difference in ranking score between the correct parse of a sentence
and a competing parse xi,j. It follows that
Error?a?? ?
X
i
Xni
j?2
gMij?a??  0?
The ranking error is zero if all margins are positive. The loss functions we discuss all
turn out to be direct functions of the margins on training examples.
4.2.2 Log-Likelihood. The first loss function is that suggested by Markov random
fields. As suggested by Ratnaparkhi, Roukos, and Ward (1994) and Johnson et al
(1999), the conditional probability of xi,q being the correct parse for the ith sentence is
defined as
P?xi,q j si, a?? ?
eF?xi,q, a??
Pni
j?1
eF?xi,j, a??
Given a new test sentence s, with parses xj for j = 1, : : : , N, the most likely tree is
arg max
xj
eF?xj,a??
PN
q?1
eF?xq,a??
? arg max
xj
F?xj, a??
Hence once the parameters are trained, the ranking function is used to order candidate
trees for test examples.
The log-likelihood of the training data is
X
i
log P?xi,1 j si, a?? ?
X
i
log
eF?xi,1, a??P
j?1
ni eF?xi,j, a??
Under maximum-likelihood estimation, the parameters a? would be set to maxi-
mize the log-likelihood. Equivalently, we again talk about minimizing the negative
Collins and Koo Discriminative Reranking for NLP
log-likelihood. Some manipulation shows that the negative log-likelihood is a function
of the margins on training data:
LogLoss ?a?? ?
X
i
log e
F?xi,1, a??
P
j?1
ni eF?xi,j, a??
?
X
i
log 1P
j?1
ni e?F?xi,1, a??F?xi,j, a???
?
X
i
log?1 ?Xni
j?2
e?F?xi,1, a??F?xi,j, a???? ?X
i
log?1 ?Xni
j?2
eMi,j?a??? ?9?
Note the similarity of equation (9) to the LogLoss function for classification in equa-
tion (4).
4.2.3 Exponential Loss. The next loss function is based on the boosting method
described in Schapire and Singer (1999). It is a special case of the general ranking
methods described in Freund et al (1998), with the ranking ??feedback?? being a simple
binary distinction between the highest-scoring parse and the other parses. Again, the
loss function is a function of the margins on training data:
ExpLoss?a?? ?
X
i
Xni
j?2
e?F?xi,1, a??F?xi,j, a??? ?
X
i
Xni
j?2
eMi,j?a?? ?10?
Note the similarity of equation (10) to the ExpLoss function for classification
in equation (6). It can be shown that ExpLoss?a??  Error?a??, so that minimizing
ExpLoss?a?? is closely related to minimizing the number of ranking errors.11 This
follows from the fact that for any x, ex  gx < 0?, and therefore that
X
i
Xni
j?2
eMi,j?a?? 
X
i
Xni
j?2
gMi,j?a??  0?
We generalize the ExpLoss function slightly, by allowing a weight for each example
xi,j, for i = 1, . . . , n, j = 2, . . . , ni. We use Si,j to refer to this weight. In particular, in some
experiments in this article, we use the following definition:
Si,j ? Score?xi,1?  Score?xi,j? ?11?
40
11 Note that LogLoss is not a direct upper bound on the number of ranking errors, although it can be shown
that it is a (relatively loose) upper bound on the number of times the correct parse is not the highest-
ranked parse on the model. The latter observation follows from the property that the correct parse must
be highest ranked if its probability is greater than 0.5.
Computational Linguistics Volume 31, Number 1
41
where, as defined in section 4.1, Score?xi, j? is some measure of the ??goodness?? of a
parse, such as the F-measure (see section 5 for the exact definition of Score used in our
experiments). The definition for ExpLoss is modified to be
ExpLoss?a?? ?
X
i
Xni
j?2
Si,jeMi,j?a??
This definition now takes into account the importance, Si,j, of each example. It is an
upper bound on the following quantity:
X
i
Xni
j?2
Si,jgMi,j?a??  0?
which is the number of errors weighted by the factors Si,j. The original definition of
ExpLoss in equation (10) can be recovered by setting Si,j = 1 for all i, j (i.e., by giving
equal weight to all examples). In our experiments we found that a definition of Si,j such
as that in equation (11) gave improved performance on development data, presumably
because it takes into account the relative cost of different ranking errors in training-
data examples.
4.3 A General Approach to Feature Selection
At this point we have definitions for ExpLoss and LogLoss which are analogous to the
definitions in section 3.2 for binary classification tasks. Section 3.3 introduced the idea
of feature selection methods; the current section gives a more concrete description of
the methods used in our experiments.
The goal of feature selection methods is to find a small subset of the features that
contribute most to reducing the loss function. The methods we consider are greedy, at
each iteration picking the feature hk with additive weight d which has the most impact
on the loss function. In general, a separate set of instances is used in cross-validation to
choose the stopping point, that is, to decide on the number of features in the model.
At this point we introduce some notation concerning feature selection methods.
We define Upd?a?, k, d? to be an updated parameter vector, with the same parameter
values as a? with the exception of ak, which is incremented by d:
Upd?a?, k, d? ? fa0,a1, : : : ,ak ? d, : : : ,amg
The d parameter can potentially take any value in the reals. The loss for the updated
model is Loss?Upd?a?, k, d??. Assuming we greedily pick a single feature with some
weight to update the model, and given that the current parameter settings are a?, the
optimal feature/weight pair ?k, d? is
?k, d? ? arg min
k, d
Loss?Upd?a?, k, d??
Collins and Koo Discriminative Reranking for NLP
The feature selection algorithms we consider take the following form (a?t is the
parameter vector at the tth iteration):
Step 1: Initialize a?0 to some value. (This will generally involve values of zero for
a1, . . . , am and a nonzero value for a0, for example, a?0 = {1, 0, 0, . . .}.)
Step 2: For t = 1 to N (the number of iterations N will be chosen by cross-
validation)
a: Find ?k, d? = arg mink,d Loss?Upd ?a?t1, k, d??
b: Set a?t = Upd?a?t1, k, d?
Note that this is essentially the idea behind the ??boosting?? approach to feature se-
lection introduced in section 3.3. In contrast, the feature selection method of Berger,
Della Pietra, and Della Pietra (1996), also described in section 3.3, would involve
updating parameter values for all selected features at step 2b.
The main computation for both loss functions involves searching for the optimal
feature/weight pair ?k, d?. In both cases we take a two-step approach to solving this
problem. In the first step the optimal update for each feature hk is calculated. We
define BestWt?k, a?? as the optimal update for the kth feature (it must be calculated for
all features k = 1, . . . , m):
BestWt?k, a?? ? arg min
d
Loss?Upd?a?, k, d??
The next step is to calculate the Loss for each feature with its optimal update, which
we will call
BestLoss?k, a?? ? min
d
Loss?Upd?a?, k, d?? ? Loss?Upd?a?, k, BestWt?k, a????
BestWt and BestLoss for each feature having been computed, the optimal feature/
weight pair can be found:
k ? arg min
k
BestLoss?k, a??, d ? BestWt?k, a??
The next sections describe how BestWt and BestLoss can be computed for the two loss
functions.
4.4 Feature Selection for ExpLoss
At the first iteration, a0 is set to optimize ExpLoss (recall that L?xi,j? is the log-
likelihood for parse xi,j under the base parsing model):
a0 ? arg mina
X
i
Xni
j?2
Si,je?a?L?xi,1?L?xi,j?? ?12?
In initial experiments we found that this step was crucial to the performance of the
method (as opposed to simply setting a0 ? 1, for example). It ensures that the
42
Computational Linguistics Volume 31, Number 1
43
contribution of the log-likelihood feature is well-calibrated with respect to the
exponential loss function. In our implementation a0 was optimized using simple brute-
force search. All values of a0 between 0.001 and 10 at increments of 0.001 were tested,
and the value which minimized the function in equation (12) was chosen.12
Feature selection then proceeds to search for values of the remaining param-
eters, a1, . . . , am. (Note that it might be preferable to also allow a0 to be adjusted as
features are added; we leave this to future work.) This requires calculation of the
terms BestWt?k, a?? and BestLoss?k, a?? for each feature. For binary-valued features
these values have closed-form solutions, which is computationally very convenient.
We now describe the form of these updates. See appendix A for how the updates
can be derived (the derivation is essentially the same as that in Schapire and Singer
[1999]).
First, we note that for any feature, ?hk?xi,1?  hk?xi,j? can take on three values: +1,
1, or 0 (this follows from our assumption of binary-valued feature values). For each
k we define the following sets:
A?k ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 1g
Ak ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 1g
Thus A+k is the set of training examples in which the kth feature is seen in the correct
parse but not in the competing parse; Ak is the set in which the kth feature is seen in
the incorrect but not the correct parse.
Based on these definitions, we next define W?k and W

k as follows:
W?k ?
X
?i,j?ZA?k
Si,jeMi,j?a?? ?13?
Wk ?
X
?i,j?ZAk
si,jeMi,j?a?? ?14?
Given these definitions, it can be shown (see appendix A) that
BestWt?k, a?? ? 1
2
log
W?k
Wk
?15?
and
BestLoss?k, a?? ? Z 
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2
?16?
12 A more precise approach, for example, binary search, could also be used to solve this optimization
problem. We used the methods that searches through a set of fixed values for simplicity, implicitly
assuming that a precision of 0.001 was sufficient for our problem.
Collins and Koo Discriminative Reranking for NLP
where Z ?
P
i
Pni
j?2 Si,je
Mi,j?a?? ? ExpLoss?a?? is a constant (for fixed a?) which appears
in the BestLoss for all features and therefore does not affect their ranking.
As Schapire and Singer (1999) point out, the updates in equation (15) can be
problematic, as they are undefined (infinite) when either W?k or W

k is zero. Following
Schapire and Singer (1999), we introduce smoothing through a parameter & and the
following new definition of BestWt:
BestWt?k, a?? ? 1
2
log
W?k ? &Z
Wk ? &Z
?17?
The smoothing parameter & is chosen through optimization on a development set.
See Figure 3 for a direct implementation of the feature selection method for
ExpLoss. We use an array of values
Gk ? j
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q
j
to indicate the gain of each feature (i.e., the impact that choosing this feature will have
on the ExpLoss function). The features are ranked by this quantity. It can be seen that
almost all of the computation involves the calculation of Z and W?k and W

k for each
feature hk. Once these values have been computed, the optimal feature and its update
can be chosen.
4.5 A New, More Efficient Algorithm for ExpLoss
This section presents a new algorithm which is equivalent to the ExpLoss algo-
rithm in Figure 3, but can be vastly more efficient for problems with sparse fea-
ture spaces. In the experimental section of this article we show that it is almost
2,700 times more efficient for our task than the algorithm in Figure 3. The efficiency
of the different algorithms is important in the parsing problem. The training data we
eventually used contained around 36,000 sentences, with an average of 27 parses per
sentence, giving around 1,000,000 parse trees in total. There were over 500,000 dif-
ferent features.
The new algorithm is also applicable, with minor modifications, to boosting
approaches for classification problems in which the representation also involves sparse
binary features (for example, the text classification problems in Schapire and Singer
[2000]). As far as we are aware, the new algorithm has not appeared elsewhere in the
boosting literature.
Figure 4 shows the improved boosting algorithm. Inspection of the algorithm in
Figure 3 shows that only margins on examples in the sets A?k and A

k are modified
when a feature k is selected. The feature space in many NLP problems is very sparse
(most features only appear on relatively few training examples, or equivalently, most
training examples will have only a few nonzero features). It follows that in many cases,
the sets A?k and A

k will be much smaller than the overall size of the training set.
Therefore when updating the model from a? to Upd?a?, k, d?, the values W?k and Wk
remain unchanged for many features and do not need to be recalculated. In fact, only
44
Computational Linguistics Volume 31, Number 1
45
features which co-occur with k* on some example must be updated. The algorithm in
Figure 4 recalculates the values of A?k and A

k only for those features which co-occur
with the selected feature k*.
To achieve this, the algorithm relies on a second pair of indices. For all i, 2  j  ni,
we define
B?i,j ? fk : ?hk?xi,1?  hk?xi,j? ? 1 g
Bi,j ? fk : ?hk?xi,1?  hk?xi,j? ? 1g ?18?
Figure 3
A naive algorithm for the boosting loss function.
Collins and Koo Discriminative Reranking for NLP
46
Figure 4
An improved algorithm for the boosting loss function.
Computational Linguistics Volume 31, Number 1
47
So Bi,j
+ and Bi,j
 are indices from training examples to features. With the algorithm in
Figure 4, updating the values of W?k and W

k for the features which co-occur with k*
involves the following number of steps:
C ?
X
?i,j?ZA?
k
?jB?i,j j ? jBi,j j? ?
X
?i,j?ZAk
?jB?i,j j ? jBi,j j? ?19?
In contrast, the naive algorithm requires a pass over the entire training set, which
requires the following number of steps:
T ?
Xn
i?1
Xni
j?2
?jB?i,j j ? jBi,j j? ?20?
The relative efficiency of the two algorithms depends on the value of C/T at each
iteration. In the worst case, when every feature chosen appears on every training
example, then C/T = 1, and the two algorithms essentially have the same running time.
However in sparse feature spaces there is reason to believe that C/T will be small for
most iterations. In section 5.4.3 we show that this is the case for our experiments.
4.6 Feature Selection for LogLoss
We now describe an approach that was implemented for LogLoss. At the first iteration,
a0 is set to one. Feature selection then searches for values of the remaining parameters,
a1, : : : ,am. We now describe how to calculate the optimal update for a feature k with
the LogLoss function. First we recap the definition of the probability of a particular
parse xi,q given parameter settings a?:
P?xi,q j si, a?? ?
eF?xi,q,a??P
j?1
ni eF?xi,j,a??
Recall that the log-loss is
LogLoss?a?? ?
X
i
 log P?xi,1 j si, a??
Unfortunately, unlike the case of ExpLoss, in general an analytic solution for BestWt
does not exist. However, we can define an iterative solution using techniques from
iterative scaling (Della Pietra, Della Pietra, and Lafferty 1997). We first define h?k, the
number of times that feature k is seen in the best parse, and p?k?a??, the expected number
of times under the model that feature k is seen:
h?k ?
X
i
hk?xi,1?, p?k?a?? ?
X
i
Xni
j?1
hk?xi,j?P?xi,j j si, a??
Collins and Koo Discriminative Reranking for NLP
Iterative scaling then defines the following update d?
d? ? log h?k
p?k?a??
While in general it is not true that d? ? BestWt?k, a??, it can be shown that this update
leads to an improvement in the LogLoss (i.e., that LogLoss?Upd?a?, k, d???  LogLoss a??),
with equality holding only when a?k is already at the optimal value, in other words,
when arg mind LogLoss?Upd?a?, k, d?? = 0. This suggests the following iterative method
for finding BestWt?k, a??:
Step 1: Initialization: Set d = 0 and a?? ? a?, calculate h?k.
Step 2: Repeat until convergence of d:
a: Calculate p?k(a??).
b: d@ d? log h?kp?k?a??? .
c : a??@Upd?a?, k, d?.
Step 3: Return BestWt?k, a?? ? d.
Given this method for calculating BestWt?k, a??, BestLoss?k, a?? can be calculated as
Loss?k, BestWt?k, a???. Note that this is only one of a number of methods for finding
BestWt?k, a??: Given that this is a one-parameter, convex optimization problem, it is a
fairly simple task, and there are many methods which could be used.
Unfortunately there does not appear to be an efficient algorithm for LogLoss that is
analogous to the ExpLoss algorithm in Figure 4 (at least if the feature selection method
is required to pick the feature with highest impact on the loss function at each
iteration). A similar observation for LogLoss can be made, in that when the model is
updated with a feature/weight pair ?k, d?, many features will have their values for
BestWt and BestLoss unchanged. Only those features which co-occur with k on some
example will need to have their values of BestWt and BestLoss updated. However, this
observation does not lead to an efficient algorithm: Updating these values is much
more expensive than in the ExpLoss case. The procedure for finding the optimal value
BestWt?k, a?? must be applied for each feature which co-occurs with the chosen feature
k. For example, the iterative scaling procedure described above must be applied for a
number of features. For each feature, this will involve recalculation of the distribution
fP?xi, 1 j si?, P?xi,2 j si?, : : : , P?xi,ni j si?g for each example i on which the feature occurs.
13 It
takes only one feature that is seen on all training examples for the algorithm to involve
recalculation of P?xi,j j si? for the entire training set. This contrasts with the simple
updates in the improved boosting algorithm ?W?k ? W
?
k ? D and Wk ? Wk ? D?. In
fact in the parsing experiments, we were forced to give up on the LogLoss feature
selection methods because of their inefficiency (see section 6.4 for more discussion about
efficiency).
48
13 This is not a failure of iterative scaling alone: Given that in the general case, closed-form solutions for
BestWt and BestLoss do not exist, it is hard to imagine a method that computes these values exactly
without some kind of iterative method which requires repeatedly visiting the examples on which a
feature is seen.
Computational Linguistics Volume 31, Number 1
49
Note, however, that approximate methods for finding the best feature and
updating its weight may lead to efficient algorithms. Appendix B gives a sketch of one
such approach, which is based on results from Collins, Schapire, and Singer (2002). We
did not test this method; we leave this to future work.
5. Experimental Evaluation
5.1 Generation of Parsing Data Sets
We used the Penn Wall Street Journal treebank (Marcus, Santorini, and
Marcinkiewicz 1993) as training and test data. Sections 2?21 inclusive (around
40,000 sentences) were used as training data, section 23 was used as the final test set.
Of the 40,000 training sentences, the first 36,000 were used as the main training set.
The remaining 4,000 sentences were used as development data and to cross-validate
the number of rounds (features) in the model. Model 2 of Collins (1999) was used to
parse both the training and test data, producing multiple hypotheses for each
sentence. We achieved this by disabling dynamic programming in the parser and
choosing a relatively narrow beam width of 1,000. The resulting parser returns all
parses that fall within the beam. The number of such parses varies sentence by
sentence.
In order to gain a representative set of training data, the 36,000 training sentences
were parsed in 2,000 sentence chunks, each chunk being parsed with a model trained
on the remaining 34,000 sentences (this prevented the initial model from being
unrealistically ??good?? on the training sentences). The 4,000 development sentences
were parsed with a model trained on the 36,000 training sentences. Section 23 was
parsed with a model trained on all 40,000 sentences.
In the experiments we used the following definition for the Score of the parse:
Score?xi,j? ?
Fmeasure?xi,j?
100
 Size?xi,j?
where F-measure(xi,j) is the F1 score
14 of the parse when compared to the gold-
standard parse (a value between 0 and 100), and Size(xi,j) is the number of constituents
in the gold-standard parse for the ith sentence. Hence the Score function is sensitive to
both the accuracy of the parse, and also the number of constituents in the gold-
standard parse.
5.2 Features
The following types of features were included in the model. We will use the rule VP Y
PP VBD NP NP SBAR with head VBD as an example. Note that the output of our
baseline parser produces syntactic trees with headword annotations (see Collins
[1999]) for a description of the rules used to find headwords).
14 Note that in the rare cases in which the baseline parser produces no constituents, the precision is
undefined; in these cases we defined the F-measure to be 0.
Collins and Koo Discriminative Reranking for NLP
Rules. These include all context-free rules in the
tree, for example, VP Y PP VBD NP NP SBAR.
Bigrams. These are adjacent pairs of nonterminals
to the left and right of the head. As shown, the
example rule would contribute the bigrams
(Right,VP,NP,NP), (Right,VP,NP,SBAR),
(Right,VP,SBAR,STOP) to the right of the head
and (Left,VP,PP,STOP) to the left of the head.
Grandparent rules. Same as Rules, but also
including the nonterminal above the rule.
Grandparent bigrams. Same as Bigrams,
but also including the nonterminal above
the bigrams.
Lexical bigrams.
Same as Bigrams,
but with the lexical
heads of the two
nonterminals also
included.
Two-level rules. Same as Rules, but also
including the entire rule above the rule.
Two-level bigrams. Same as Bigrams, but
also including the entire rule above the rule.
Trigrams. All trigrams within the rule. The
example rule would contribute the trigrams
(VP, STOP, PP, VBD!), (VP, PP, VBD!, NP),
(VP, VBD!, NP, NP), (VP, NP, NP, SBAR),
and (VP,NP, SBAR, STOP) (! is used to mark
the head of the rule).
50
Computational Linguistics Volume 31, Number 1
51
Head Modifiers. All head-modifier pairs, with
the grandparent nonterminal also included.
An adj flag is also included, which is one if
the modifier is adjacent to the head, zero
otherwise. As an example, say the nonterminal
dominating the example rule is S. The
example rule would contribute (Left, S, VP,
VBD, PP, adj = 1), (Right, S, VP, VBD, NP,
adj = 1), (Right, S, VP, VBD, NP, adj = 0),
and (Right, S, VP, VBD, SBAR, adj = 0).
PPs. Lexical trigrams involving the heads
of arguments of prepositional phrases.
The example shown at right would
contribute the trigram (NP, NP, PP, NP,
president, of, U.S.), in addition to the
relation (NP, NP, PP, NP, of, U.S.), which
ignores the headword of the constituent
being modified by the PP. The three
nonterminals (for example, NP, NP, PP)
identify the parent of the entire phrase,
the nonterminal of the head of the phrase,
and the nonterminal label for the PP.
Distance head modifiers. Features involving the distance between
headwords. For example, assume dist is the number of words between
the headwords of the VBD and SBAR in the (VP, VBD, SBAR) head-modifier
relation in the above rule. This relation would then generate features
(VP, VBD, SBAR, = dist), and (VP, VBD, SBAR,  x) for all dist  x  9 and
(VP, VBD, SBAR,  x) for all 1  x  dist.
Further lexicalization. In order to generate more features, a second pass
was made in which all nonterminals were augmented with their lexical
heads when these headwords were closed-class words. All features
apart from head modifiers, PPs, and distance head modifiers were then
generated with these augmented nonterminals.
All of these features were initially generated, but only features seen on at least
one parse for at least five different sentences were included in the final model (this
count cutoff was implemented to keep the number of features down to a tractable
number).
5.3 Applying the Reranking Methods
The ExpLoss method was trained with several values for the smoothing parameter &:
{0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, 0.005, 0.0075}. For each value of &, the
method was run for 100,000 rounds on the training data. The implementation was
such that the feature updates for all 100,000 rounds for each training run were
recorded in a file. This made it simple to test the model on development data for all
values of N between 0 and 100,000.
Collins and Koo Discriminative Reranking for NLP
The different values of & and N were compared on development data through the
following criterion:
X
i
Score?zi? ?21?
where Score is as defined above, and zi is the output of the model on the ith
development set example. The &, N values which maximized this quantity were used
to define the final model applied to the test data (section 23 of the treebank). The
optimal values were & ? 0.0025 and N ? 90,386, at which point 11,673 features had
nonzero values (note that the feature selection techniques may result in a given feature
being updated more than once). The computation took roughly 3?4 hours on a
machine with a 1.6 GHz pentium processor and around 2 GB of memory.
Table 1 shows results for the method. The model of Collins (1999) was the base
model; the ExpLoss model gave a 1.5% absolute improvement over this method. The
method gives very similar accuracy to the model of Charniak (2000), which also uses a
rich set of initial features in addition to Charniak?s (1997) original model.
The LogLoss method was too inefficient to run on the full data set. Instead we
made some tests on a smaller subset of the data (5,934 sentences, giving 200,000 parse
trees) and 52,294 features.15 On an older machine (an order of magnitude or more
slower than the machine used for the final tests) the boosting method took 40 minutes
for 10,000 rounds on this data set. The LogLoss method took 20 hours to complete
3,500 rounds (a factor of about 85 times slower). This was in spite of various heuristics
that were implemented in an attempt to speed up LogLoss: for example, selecting
multiple features at each round or recalculating the statistics for only the best K
features for some small K at the previous round of feature selection. In initial
experiments we found ExpLoss to give similar, perhaps slightly better, accuracy than
LogLoss.
5.4 Further Experiments
This section describes further experiments investigating various aspects of the
boosting algorithm: the effect of the & and N parameters, learning curves, the choice
of the Si,j weights, and efficiency issues.
5.4.1 The Effect of the & and N Parameters. Figure 5 shows the learning curve on
development data for the optimal value of & (0.0025). The accuracy shown is the
performance relative to the baseline method of using the probability from the
generative model alone in ranking parses, where the measure in equation (21) is used
to measure performance. For example, a score of 101.5 indicates a 1.5% increase in this
score. The learning curve is initially steep, eventually flattening off, but reaching its
peak value after a large number (90,386) of rounds of feature selection.
Table 2 indicates how the peak performance varies with the smoothing parameter
&. Figure 6 shows learning curves for various values of &. It can be seen that values
other than & ? 0.0025 can lead to undertraining or overtraining of the model.
52
15 All features described above except distance head modifiers and further lexicalization were included.
Computational Linguistics Volume 31, Number 1
53
Figure 5
Learning curve on development data for the optimal value for & (0.0025). The y-axis is the level of
accuracy (100 is the baseline score), and the x-axis is the number of rounds of boosting.
Table 1
Results on section 23 of the WSJ Treebank. ??LR?? is labeled recall; ??LP?? is labeled precision;
??CBs?? is the average number of crossing brackets per sentence; ??0 CBs?? is the percentage of
sentences with 0 crossing brackets; ??2 CBs?? is the percentage of sentences with two or more
crossing brackets. All the results in this table are for models trained and tested on the same data,
using the same evaluation metric. Note that the ExpLoss results are very slightly different from
the original results published in Collins (2000). We recently reimplemented the boosting code
and reran the experiments, and minor differences in the code and & values tested on
development data led to minor improvements in the results.
Model  40 Words (2,245 sentences)
LR LP CBs 0 CBs 2 CBs
Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1%
Collins 1999 88.5% 88.7% 0.92 66.7% 87.1%
Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6%
ExpLoss 90.2% 90.4% 0.73 71.2% 90.2%
Model  100 Words (2,416 sentences)
LR LP CBs 0 CBs 2 CBs
Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2%
Ratnaparkhi 1998 86.3% 87.5% 1.21 60.2% ?
Collins 1999 88.1% 88.3% 1.06 64.0% 85.1%
Charniak 2000 89.6% 89.5% 0.88 67.6% 87.7%
ExpLoss 89.6% 89.9% 0.86 68.7% 88.3%
Collins and Koo Discriminative Reranking for NLP
5.4.2 The Effect of the Si,j Weights on Examples. In section 4.2.3 we introduced the
idea of weights Si,j representing the importance of examples. Thus far, in the
experiments in this article, we have used the definition
Si,j ? Score?xi,1?  Score?xi,j? ?22?
thereby weighting examples in proportion to their difference in score from the correct
parse for the sentence in question. In this section we compare this approach to a
default definition of Si,j, namely,
Si,j ? 1 ?23?
Using this definition, we trained the ExpLoss method on the same training set for
several values of the smoothing parameter & and evaluated the performance on
development data. Table 3 compares the peak performance achieved under the two
definitions of Si,j on the development set. It can be seen that the definition in equation
(22) outperforms the simpler method in equation (23). Figure 7 shows the learning
curves for the optimal values of & for the two methods. It can be seen that the learning
curve for the definition of Si,j in equation (22) consistently dominates the curve for the
simpler definition.
5.4.3 Efficiency Gains. Section 4.5 introduced an efficient algorithm for optimizing
ExpLoss. In this section we explore the empirical gains in efficiency seen on the
parsing data sets in this article.
We first define the quantity T as follows:
T ?
X
i
Xni
j?2
?jB?i,j j ? jBi,j j?
54
Table 2
Peak performance achieved for various values of &. ??Best N?? refers to the number of
rounds at which peak development set accuracy was reached. ??Best score?? indicates the
relative performance, compared to the baseline method, at the optimal value for N.
& Best N Best score
0.0001 29,471 101.743
0.00025 22,468 101.849
0.0005 48,795 101.845
0.00075 43,386 101.809
0.001 43,975 101.849
0.0025 90,386 101.864
0.005 66,378 101.824
0.0075 80,746 101.722
Computational Linguistics Volume 31, Number 1
55
Figure 6
Learning curves on development data for various values of &. In each case the y-axis is the
level of accuracy (100 is the baseline score), and the x-axis is the number of rounds of
boosting. The three graphs compare the curve for & = 0.0025 (the optimal value) to (from top
to bottom) & = 0.0001, & = 0.0075, and & = 0.001. The top graph shows that & = 0.0001 leads to
undersmoothing (overtraining). Initially the graph is higher than that for & = 0.0025, but on later
rounds the performance starts to decrease. The middle graph shows that & = 0.0075 leads to
oversmoothing (undertraining). The graph shows consistently lower performance than that for
& = 0.0025. The bottom graph shows that there is little difference in performance for & = 0.001
versus & = 0.0025.
Collins and Koo Discriminative Reranking for NLP
This is a measure of the number of updates to the W?k and W

k variables required in
making a pass over the entire training set. Thus it is a measure of the amount of
computation that the naive algorithm for ExpLoss, presented in Figure 3, requires for
each round of feature selection.
Next, say the improved algorithm in Figure 4 selects feature k* on the t th round of
feature selection. Then we define the following quantity:
Ct ?
X
?i,j?&A?
k
?jB?i,j j ? jBi,j j? ?
X
?i,j?&Ak
?jB?i,j j ? jBi,j j?
This is a measure of the number of summations required by the improved algorithm in
Figure 4 at the t th round of feature selection.
56
Figure 7
Performance versus number of rounds of boosting for Si,j ? Score?xi,1?  Score?xi, j?
(curve labeled ??Weighted??) and Si,j ? 1 (curve labeled ??Not Weighted??).
Table 3
Peak performance achieved for various values of & for Si, j ? Score?xi,1?  Score?xi, j? (column
labeled ??weighted??) and Si,j ? 1 (column labeled ??unweighted??).
& Best score (weighted) Best score (unweighted)
0.0001 101.743 101.744
0.00025 101.849 101.754
0.0005 101.845 101.778
0.00075 101.809 101.762
0.001 101.849 101.778
0.0025 101.864 101.699
0.005 101.824 101.610
0.0075 101.722 101.604
Computational Linguistics Volume 31, Number 1
57
We are now in a position to compare the running times of the two algorithms. We
define the following quantities:
Work?n? ?
Xn
t?1
Ct
T
?24?
Savings?n? ? nTPn
t?1
Ct
?25?
Savings?a, b? ? ?1 ? b  a?TPb
t?a
Ct
?26?
Here, Work?n? is the computation required for n rounds of feature selection, where
a single unit of computation corresponds to a pass over the entire training set.
Savings?n? tracks the relative efficiency of the two algorithms as a function of the
number of features, n. For example, if Savings?100? ? 1,200, this signifies that for the
first 100 rounds of feature selection, the improved algorithm is 1,200 times as efficient
as the naive algorithm. Finally, Savings?a, b? indicates the relative efficiency between
rounds a and b, inclusive, of feature selection. For example, Savings?11, 100? ? 83
signifies that between rounds 11 and 100 inclusive of the algorithm, the improved
algorithm was 83 times as efficient.
Figures 8 and 9 show graphs of Work?n? and Savings?n? versus n. The savings
from the improved algorithm are dramatic. In 100,000 rounds of feature selection,
the improved algorithm requires total computation that is equivalent to a mere 37.1
passes over the training set. This is a saving of a factor of 2,692 over the naive algorithm.
Table 4 shows the value of Savings?a,b? for various values of ?a,b?. It can be
seen that the performance gains are significantly larger in later rounds of feature
selection, presumably because in later stages relatively infrequent features are being
selected. Even so, there are still savings of a factor of almost 50 in the early stages of
the method.
6. Related Work
6.1 History-Based Models with Complex Features
Charniak (2000) describes a parser which incorporates additional features into a
previously developed parser, that of Charniak (1997). The method gives substantial
improvements over the original parser and results which are very close to the results
of the boosting method we have described in this article (see section 5 for experimental
results comparing the two methods). Our features are in many ways similar to those of
Charniak (2000). The model in Charniak (2000) is quite different, however. The
additional features are incorporated using a method inspired by maximum-entropy
models (e.g., the model of Ratnaparkhi [1997]).
Ratnaparkhi (1997) describes the use of maximum-entropy techniques ap-
plied to parsing. Log-linear models are used to estimate the conditional probabilities
P?di j F ?d1, : : : , di1?? in a history-based parser. As a result the model can take into
account quite a rich set of features in the history.
Collins and Koo Discriminative Reranking for NLP
58
Figure 8
Work?n??yaxis? versus n ?xaxis?.
Figure 9
Savings?n??y-axis? versus n?x-axis?.
Computational Linguistics Volume 31, Number 1
59
Both approaches still rely on decomposing a parse tree into a sequence of
decisions, and we would argue that the techniques described in this article have more
flexibility in terms of the features that can be included in the model.
6.2 Joint Log-Linear Models
Abney (1997) describes the application of log-linear models to stochastic head-
driven phrase structure grammars (HPSGs). Della Pietra, Della Pietra, and Lafferty
(1997) describe feature selection methods for log-linear models, and Rosenfeld
(1997) describes application of these methods to language modeling for speech
recognition. These methods all emphasize models which define a joint proba-
bility over the space of all parse trees (or structures in question): For this reason
we describe these approaches as ??Joint log-linear models.?? The probability of a tree
xi,j is
P?xi,j? ?
eF?xi,j?P
xZZ
eF?x?
?27?
Here Z is the (infinite) set of possible trees, and the denominator cannot be
calculated explicitly. This is a problem for parameter estimation, in which an estimate
of the denominator is required, and Monte Carlo methods have been proposed
(Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as a
technique for estimation of this value. Our sense is that these methods can be
computationally expensive. Notice that the joint likelihood in equation (27) is not a
direct function of the margins on training examples, and its relation to error
rate is therefore not so clear as in the discriminative approaches described in this
article.
6.3 Conditional Log-Linear Models
Ratnaparkhi, Roukos, and Ward (1994), Johnson et al (1999), and Riezler et al (2002)
suggest training log-linear models (i.e., the LogLoss function in equation (9)) for
parsing problems. Ratnaparkhi, Roukos, and Ward (1994) use feature selection
techniques for the task. Johnson et al (1999) and Riezler et al (2002) do not use a
feature selection technique, employing instead an objective function which includes a
Table 4
Values of Savings (a, b) for various values of a, b.
a?b Savings (a, b)
1?100,000 2,692.7
1?10 48.6
11?100 83.5
101?1,000 280.0
1,001?10,000 1,263.9
10,001?50,000 2,920.2
50,001?100,000 4,229.8
Collins and Koo Discriminative Reranking for NLP
Gaussian prior on the parameter values, thereby penalizing parameter values which
become too large:
a? ? arg min
a ?LogLoss?a?? ? X
k?0 : : :m
a2k
7
2
k
? ?28?
Closed-form updates under iterative scaling are not possible with this objective
function; instead, optimization algorithms such as gradient descent or conjugate
gradient methods are used to estimate parameter values.
In more recent work, Lafferty, McCallum, and Pereira (2001) describe the use of
conditional Markov random fields (CRFs) for tagging tasks such as named entity
recognition or part-of-speech tagging (hidden Markov models are a common method
applied to these tasks). CRFs employ the objective function in equation (28). A key
insight of Lafferty, McCallum, and Pereira (2001) is that when features are of a
significantly local nature, the gradient of the function in equation (28) can be calculated
efficiently using dynamic programming, even in cases in which the set of candidates
involves all possible tagged sequences and is therefore exponential in size. See also Sha
and Pereira (2003) for more recent work on CRFs.
Optimizing a log-linear model with a Gaussian prior (i.e., choosing parameter
values which achieve the global minimum of the objective function in equation (28)) is
a plausible alternative to the feature selection approaches described in the current
article or to the feature selection methods previously applied to log-linear models. The
Gaussian prior (i.e., the
P
k a2k=7
2
k penalty) has been found in practice to be very
effective in combating overfitting of the parameters to the training data (Chen and
Rosenfeld 1999; Johnson et al 1999; Lafferty, McCallum, and Pereira 2001; Riezler et al
2002). The function in equation (28) can be optimized using variants of gradient
descent, which in practice require tens or at most hundreds of passes over the training
data (see, e.g., Sha and Pereira 2003). Thus log-linear models with a Gaussian prior are
likely to be comparable in terms of efficiency to the feature selection approach
described in this article (in the experimental section, we showed that for the parse-
reranking task, the efficient boosting algorithm requires computation that is equivalent
to around 40 passes over the training data).
Note, however, that the two methods will differ considerably in terms of the
sparsity of the resulting reranker. Whereas the feature selection approach leads to
around 11,000 (2%) of the features in our model having nonzero parameter values,
log-linear models with Gaussian priors typically have very few nonzero parameters
(see, e.g., Riezler and Vasserman 2004). This may be important in some domains, for
example, those in which there are a very large number of features and this large
number leads to difficulties in terms of memory requirements or computation time.
6.4 Feature Selection Methods
A number of previous papers (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi
1998; Della Pietra, Della Pietra, and Lafferty 1997; McCallum 2003; Zhou et al 2003;
Riezler and Vasserman 2004) describe feature selection approaches for log-linear
models applied to NLP problems. Earlier work (Berger, Della Pietra, and Della Pietra
1996; Ratnaparkhi 1998; Della Pietra, Della Pietra, and Lafferty 1997) suggested
methods that added a feature at a time to the model and updated all parameters in the
current model at each step (for more detail, see section 3.3).
60
Computational Linguistics Volume 31, Number 1
61
Assuming that selection of a feature takes one pass over the training set and that
fitting a model takes p passes over the training set, these methods require f  ?p + 1?
passes over the training set, where f is the number of features selected. In our
experiments, f , 10,000. It is difficult to estimate the value for p, but assuming (very
conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over
the training set. This is around 1,000 times as much computation as that required for
the efficient boosting algorithm applied to our data, suggesting that the feature
selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998),
and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the
parsing task.
More recent work (McCallum 2003; Zhou et al 2003; Riezler and Vasserman 2004)
has considered methods for speeding up the feature selection methods described in
Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della
Pietra, and Lafferty (1997). McCallum (2003) and Riezler and Vasserman (2004)
describe approaches that add k features at each step, where k is some constant greater
than one. The running time for these methods is therefore O? f  ?p ? 1?=k?. Riezler
and Vasserman (2004) test a variety of values for k, finding that k = 100 gives optimal
performance. McCallum (2003) uses a value of k = 1,000. Zhou et al (2003) use a
different heuristic that avoids having to recompute the gain for every feature at every
iteration.
We would argue that the alternative feature selection methods in the current
article may be preferable on the grounds of both efficiency and simplicity. Even with
large values of k in the approach of McCallum (2003) and Riezler and Vasserman
(2004) (e.g., k = 1,000), the approach we describe is likely to be at least as efficient as
these alternative approaches. In terms of simplicity, the methods in McCallum (2003)
and Riezler and Vasserman (2004) require selection of a number of free parameters
governing the behavior of the algorithm: the value for k, the value for a regularizer
constant (used in both McCallum [2003] and Riezler and Vasserman [2004]), and the
precision with which the model is optimized at each stage of feature selection
(McCallum [2003] describes using ??just a few BFGS iterations?? at each stage). In
contrast, our method requires a single parameter to be chosen (the value for the &
smoothing parameter) and makes a single approximation (that only a single feature is
updated at each round of feature selection). The latter approximation is particularly
important, as it leads to the efficient algorithm in Figure 4, which avoids a pass over
the training set at each iteration of feature selection (note that in sparse feature spaces, f
rounds of feature selection in our approach can take considerably fewer than f passes
over the training set, in contrast to other work on feature selection within log-linear
models).
Note that there are other important differences among the approaches. Both Della
Pietra, Della Pietra, and Lafferty (1997) and McCallum (2003) describe methods that
induce conjunctions of ??base?? features, in a way similar to decision tree learners. Thus
a relatively small number of base features can lead to a very large number of possible
conjoined features. In future work it might be interesting to consider these kinds of
approaches for the parsing problem. Another difference is that both McCallum, and
Riezler and Vasserman, describe approaches that use a regularizer in addition to
feature selection: McCallum uses a two-norm regularizer; Riezler and Vasserman use a
one-norm regularizer.
Finally, note that other feature selection methods have been proposed within the
machine-learning community: for example, ??filter?? methods, in which feature
selection is performed as a preprocessing step before applying a learning method,
Collins and Koo Discriminative Reranking for NLP
and backward selection methods (Koller and Sahami 1996), in which initially all
features are added to the model and features are then incrementally removed from the
model.
6.5 Boosting, Perceptron, and Support Vector Machine Approaches for Ranking
Problems
Freund et al (1998) introduced a formulation of boosting for ranking problems. The
problem we have considered is a special case of the problem in Freund et al (1998), in
that we have considered a binary distinction between candidates (i.e., the best parse
vs. other parses), whereas Freund et al consider learning full or partial orderings over
candidates. The improved algorithm that we introduced in Figure 4 is, however, a new
algorithm that could perhaps be generalized to the full problem of Freund et al (1998);
we leave this to future research.
Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofmann (2003)
describe experiments on tagging tasks using the ExpLoss function, in contrast to the
LogLoss function used in Lafferty, McCallum, and Pereira (2001). Altun, Hofmann,
and Johnson (2003) describe how dynamic programming methods can be used to
calculate gradients of the ExpLoss function even in cases in which the set of candidates
again includes all possible tagged sequences, a set which grows exponentially in size
with the length of the sentence being tagged. Results in Altun, Johnson, and Hofmann
(2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact
on accuracy for the tagging task in question.
Perceptron-based algorithms, or the voted perceptron approach of Freund and
Schapire (1999), are another alternative to boosting and LogLoss methods. See Collins
(2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron
algorithm. Collins (2002b) gives convergence proofs for the methods; Collins (2002a)
directly compares the boosting and perceptron approaches on a named entity task;
and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which
allow representations of parse trees or labeled sequences in very-high-dimensional
spaces.
Shen, Sarkar, and Joshi (2003) describe support vector machine approaches to
ranking problems and apply support vector machines (SVMs) using tree-adjoining
grammar (Joshi, Levy, and Takahashi 1975) features to the parsing data sets we have
described in this article, with good empirical results.
See Collins (2004) for a discussion of many of these methods, including an
overview of statistical bounds for the boosting, perceptron, and SVM methods, as well
as a discussion of the computational issues involved in the different algorithms.
7. Conclusions
This article has introduced a new algorithm, based on boosting approaches in machine
learning, for ranking problems in natural language processing. The approach gives a
13% relative reduction in error on parsing Wall Street Journal data. While in this article
the experimental focus has been on parsing, many other problems in natural language
processing or speech recognition can also be framed as reranking problems, so the
methods described should be quite broadly applicable. The boosting approach to
ranking has been applied to named entity segmentation (Collins 2002a) and natural
language generation (Walker, Rambow, and Rogati 2001). The key characteristics of
the approach are the use of global features and of a training criterion (optimization
62
Computational Linguistics Volume 31, Number 1
63
problem) that is discriminative and closely related to the task at hand (i.e., parse
accuracy).
In addition, the article introduced a new algorithm for the boosting approach
which takes advantage of the sparse nature of the feature space in the parsing data that
we use. Other NLP tasks are likely to have similar characteristics in terms of sparsity.
Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for
the new algorithm over the obvious implementation of the boosting approach. We
would argue that the improved boosting algorithm is a natural alternative to
maximum-entropy or (conditional) log-linear models. The article has drawn connec-
tions between boosting and maximum-entropy models in terms of the optimization
problems that they involve, the algorithms used, their relative efficiency, and their
performance in empirical tests.
Appendix A: Derivation of Updates for ExpLoss
This appendix gives a derivation of the optimal updates for ExpLoss. The derivation is
very close to that in Schapire and Singer (1999). Recall that for parameter values a?, we
need to compute BestWt?k, a?? and BestLoss?k, a?? for k ? 1, . . . , m, where
BestWt?k, a?? ? arg min
d
ExpLoss?Upd?a?, k, d??
and
BestLoss?k, a?? ? ExpLoss?Upd?a?, k, BestWt?k, a????
The first thing to note is that an update in parameters from a? to Upd?a?, k, d??
results in a simple additive update to the ranking function F:
F?xi,j, Upd?a?, k, d?? ? F?xi,j,a? ? dhk?xi,j?
It follows that the margin on example ?i, j? also has a simple update:
Mi,j?Upd?a?, k, d?? ? F?xi,1, Upd?a?, k, d??  F?xi,j, Upd?a?, k, d??
? F?xi,1, a??  F?xi,j, a?? ? d?hk?xi,1?  hk?xi,j?
? Mi,j?a?? ? d?hk?xi,1?  hk?xi,j?
The updated ExpLoss function can then be written as
ExpLoss ?Upd?a?,k, d?? ?
X
i
Xni
j?2
Si,jeMi,j?Upd?a?, k, d??
?
X
i
Xni
j?2
Si,jeMi,j?a?d?hk?xi,1?hk?xi,j?
Next, we note that ?hk?xi,1?  hk?xi,j? can take on three values: +1, 1, or 0. We split the
training sample into three sets depending on this value:
A?k ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 1g
Collins and Koo Discriminative Reranking for NLP
Ak ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 1g
A0k ? f?i,j? : ?hk?xi,1?  hk?xi,j? ? 0g
Given these definitions, we define Wk
+, Wk
, and Wk
0 as
W?k ?
X
?i,j?&A?k
Si,jeMi,j?a??
Wk ?
X
?i,j?&Ak
Si,jeMi,j?a??
W0k ?
X
?i,j?&A0k
Si,jeMi,j?a??
ExpLoss is now rewritten in terms of these quantities:
ExpLoss?Upd?a?, k, d?? ?
X
?i,j?&A?k
Si,jeMi,j?a??d ?
X
?i,j?&Ak
Si,jeMi,j?a???d ?
X
?i,j?&A0k
Si,jeMi,j?a??
? edW?k ? e
dWk ? W0k ?A:1?
To find the value of d that minimizes this loss, we set the derivative of (A.1) with
respect to d to zero, giving the following solution:
BestWt?k, a?? ? 1
2
log
W?k
Wk
Plugging this value of d back into (A.1) gives the best loss:
BestLoss?k, a?? ? 2
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
W?k W

k
q
? W0k
? 2
ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi
W?k W

k
q
? Z  W?k  Wk
? Z 
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2
?A:2?
where Z ? ExpLoss?a?? ?
P
i
Pni
j?2 Si,je
Mi,j?a?? is a constant (for constant a?) which
appears in the BestLoss for all features and therefore does not affect their ranking.
Appendix B: An Alternative Method for LogLoss
In this appendix we sketch an alternative approach for feature selection in LogLoss
that is potentially an efficient method, at the cost of introducing an approximation
64
Computational Linguistics Volume 31, Number 1
65
in the feature selection method. Until now, we have defined BestLoss?k, a?? to be the
minimum of the loss given that the kth feature is updated an optimal amount:
BestLoss?k, a?? ? min
d
LogLoss?Upd?a?,k, d??
In this section we sketch a different approach, based on results from Collins, Schapire,
and Singer (2002), which leads to an algorithm very similar to that for ExpLoss in
Figures 3 and 4. Take the following definitions (note the similarity to the definitions in
equations (13), (14), (15), and (16), with only the definitions for Wk
+ and Wk
 being
altered):
W?k ?
X
?i,j?&A?k
qi,j, Wk ?
X
?i,j?&Ak
qi,j, where qi,j ?
eMi,j?a??
1 ?
Pni
q?2 e
Mi,q?a?? ?B:1?
BestWt?k, a?? ? 1
2
log
W?k
Wk
?B:2?
BestLoss?k, a?? ? LogLoss?a?? 
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2
?B:3?
Note that the ExpLoss computations can be recovered by replacing qi,j in equation
(B.1) with qi,j ? eMi,j?a??. This is the only essential difference between the new
algorithm and the ExpLoss method.
Results from Collins, Schapire and Singer (2002) show that under these definitions
the following guarantee holds:
LogLoss?Upd?a?,k, BestWt?k, a????  BestLoss?k, a??
So it can be seen that the update from a? to Upd?a?, k, BestWt?k, a??? is guaranteed to
decrease LogLoss by at least
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2. From these results, the algorithms in Figures 3
and 4 could be altered to take the revised definitions of W?k and W

k into account.
Selecting the feature with the minimum value of BestLoss?k, a?? at each iteration leads
to the largest guaranteed decrease in LogLoss. Note that this is now an approximation,
in that BestLoss?k, a?) is an upper bound on the log-likelihood which may or may not be
tight. There are convergence guarantees for the method, however, in that as the
number of rounds of feature selection goes to infinity, the LogLoss approaches its
minimum value.
The algorithms in Figures 3 and 4 could be modified to take the alternative
definitions of W?k and W

k into account, thereby being modified to optimize LogLoss
instead of ExpLoss. The denominator terms in the qi,j definitions in equation (B.1) may
complicate the algorithms somewhat, but it should still be possible to derive relatively
efficient algorithms using the technique.
For a full derivation of the modified updates and for quite technical convergence
proofs, see Collins, Schapire and Singer (2002). We give a sketch of the argument here.
First, we show that
LogLoss?Upd?a?; k; d??  LogLoss?a? W?k  W

k ? W?k e
d ? Wk ed? ?B:4?
Collins and Koo Discriminative Reranking for NLP
This can be derived as follows (in this derivation we use gk?xi,j? ? hk?xi,1?  hk?xi,j??:
LogLoss?Upd?a?,k, d??
? LogLoss?a?? ? LogLoss?Upd?a?, k, d??  LogLoss?a??
? LogLoss?a?? ?
X
i
log
1 ?
Pni
j?2 e
Mi,j?a??dgk?xi,j?
1 ?
Pni
j?2 e
Mi,j?a??
 !
? LogLoss?a?? ?
X
i
log
1
1 ?
Pni
j?2 e
Mi,j?a?? ?
Pni
j?2 e
Mi,j?a??dgk?xi,j?
1 ?
Pni
j?2 e
Mi,j?a??
 !
? LogLoss?a?? ?
X
i
log 1 
Xni
j?2
qi,j ?
Xni
j?2
qi,jedgk?xi,j?
0
@
1
A
?B:5?
 LogLoss ?a?? 
X
i
Xni
j?2
qi,j ?
X
i
Xni
j?2
qi,jedgk?xi,j?
? LogLoss?a??  ?W0k ? W?k ? W

k ? ? W0k ? W?k e
d ? Wk ed
? LogLoss?a??  W?k  W

k ? W?k e
d ? Wk ed
?B:6?
Equation (B.6) can be derived from equation (B.5) through the bound log?1 + x?  x
for all x.
The second step is to minimize the right-hand side of the bound in equation (B.4)
with respect to d. It can be verified that the minimum is found at
d ? 1
2
log
W?k
Wk
at which value the right-hand side of equation (B.4) is equal to
LogLoss?d? 
ffiffiffiffiffiffiffiffi
W?k
q

ffiffiffiffiffiffiffiffi
Wk
q 2
66
Acknowledgments
Thanks to Rob Schapire and Yoram Singer for
useful discussions on boosting algorithms and
to Mark Johnson for useful discussions about
linear models for parse ranking. Steve Abney
and Fernando Pereira gave useful feedback on
earlier drafts of this work. Finally, thanks to
the anonymous reviewers for several useful
comments.
References
Abney, Steven. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597?618.
Altun, Yasemin, Thomas Hofmann, and
Mark Johnson. 2003. Discriminative
learning for label sequences via boosting.
In Advances in Neural Information Processing
Systems (NIPS 15), Vancouver.
Altun, Yasemin, Mark Johnson, and
Thomas Hofmann. 2003. Loss functions
and optimization methods for
discriminative learning of label sequences.
In Proceedings of Empirical Methods in
Natural Language Processing (EMNLP 2003),
Sapporo, Japan.
Berger, Adam L., Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum
entropy approach to natural language
Computational Linguistics Volume 31, Number 1
67
processing. Computational Linguistics,
22(1):39?71.
Black, Ezra, Frederick Jelinek, John Lafferty,
David Magerman, Robert Mercer, and Salim
Roukos. 1992. Towards history-based
grammars: Using richer models for
probabilistic parsing. In Proceedings of the
Fifth DARPA Speech and Natural Language
Workshop, Harriman, NY.
Charniak, Eugene. 1997. Statistical parsing
with a context-free grammar and word
statistics. Proceedings of the 14th National
Conference on Artificial Intelligence, Menlo
Park, CA. AAAI Press/MIT Press.
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of NAACL-2000, Seattle.
Chen, Stanley F., and Ronald Rosenfeld.
1999. A gaussian prior for smoothing
maximum entropy models. Technical
Report CMU-CS-99-108, Computer
Science Department, Carnegie Mellon
University.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
and Eighth Conference of the European Chapter
of the Association for Computational
Linguistics, pages 16?23, Madrid.
Collins, Michael. 1999. Head-Driven
Statistical Models for Natural Language
Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Collins, Michael. 2000. Discriminative
reranking for natural language parsing.
In Proceedings of the 17th International
Conference on Machine Learning (ICML 2000),
Stanford, CA. Morgan Kaufmann,
San Francisco.
Collins, Michael. 2002a. Ranking algorithms
for named-entity extraction: Boosting and
the voted perceptron. In ACL 2002:
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Collins, Michael. 2002b. Discriminative
training methods for hidden Markov
models: Theory and experiments with the
perceptron algorithm. In Proceedings of
EMNLP 2002, Philadelphia.
Collins, Michael. 2004. Parameter estimation
for statistical parsing models: Theory and
practice of distribution-free methods. In
Harry Bunt, John Carroll, and Giorgio Satta,
editors, New Developments in Parsing
Technology. Kluwer.
Collins, Michael and Nigel Duffy. 2001.
Convolution kernels for natural language.
In Advances in Neural Information Processing
Systems (NIPS 14), Vancouver.
Collins, Michael, and Nigel Duffy. 2002.
New ranking algorithms for parsing and
tagging: Kernels over discrete structures,
and the voted perceptron. In ACL 2002:
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Collins, Michael, Robert E. Schapire, and
Yoram Singer. 2002. Logistic regression,
AdaBoost and Bregman distances. Machine
Learning, 48(1/2/3):253?285.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
19(4):380?393.
Duffy, Nigel and David Helmbold. 1999.
Potential boosters? In Advances in
Neural Information Processing Systems
(NIPS 12), Denver.
Freund, Yoav, Raj Iyer, Robert E. Schapire,
and Yoram Singer. 1998. An efficient
boosting algorithm for combining
preferences. In Machine Learning:
Proceedings of the 15th International
Conference, Madison, WI.
Freund, Yoav and Robert E. Schapire. 1997. A
decision-theoretic generalization of on-line
learning and an application to boosting.
Journal of Computer and System Sciences,
55(1):119?139.
Freund, Yoav and Robert E. Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277?296.
Friedman, Jerome H., Trevor Hastie, and
Robert Tibshirani. 2000. Additive logistic
regression: A statistical view of boosting.
Annals of Statistics, 38(2):337?374.
Henderson, James. 2003. Inducing history
representations for broad coverage
statistical parsing. In Proceedings of the Joint
Meeting of the North American Chapter of the
Association for Computational Linguistics and
the Human Language Technology Conference
(HLT-NAACL 2003), pages 103?110,
Edmonton, Alberta, Canada.
Hoffgen, Klauss U., Kevin S. van Horn, and
Hans U. Simon. 1995. Robust trainability of
single neurons. Journal of Computer and
System Sciences, 50:114?125.
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic
??unification-based?? grammars. In
Proceedings of ACL 1999, College
Park, MD.
Collins and Koo Discriminative Reranking for NLP
68
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Science 10,
no. 1:136?163.
Koller, Daphne, and Mehran Sahami. 1996.
Toward optimal feature selection. In
Proceedings of the 13th International
Conference on Machine Learning (ICML),
pages 284?292, Bari, Italy, July.
Lafferty, John. 1999. Additive models,
boosting, and inference for generalized
divergences. In Proceedings of the 12th
Annual Conference on Computational Learning
Theory (COLT?99), Santa Cruz, CA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of ICML 2001,
Williamstown, MA.
Lebanon, Guy and John Lafferty. 2001.
Boosting and maximum likelihood for
exponential models. In Advances in
Neural Information Processing Systems
(NIPS 14), Vancouver.
Malouf, Robert. 2002. A comparison of
algorithms for maximum entropy
parameter estimation. In Proceedings of the
Sixth Conference on Natural Language
Learning (CoNNL-2002), Taipei, Taiwan.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The Penn
Treebank. Computational Linguistics,
19(2):313?330.
Mason, Llew, Peter L. Bartlett, and Jonathan
Baxter. 1999. Direct optimization of margins
improves generalization in combined
classifiers. In Advances in Neural Information
Processing Systems (NIPS 12), Denver.
McCallum, Andrew. 2003. Efficiently
inducing features of conditional random
fields. In Proceedings of the Conference on
Uncertainty in Artificial Intelligence (UAI
2003), Acapulco.
Och, Franz Josef, and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In ACL 2002: Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics, pages 295?302,
Philadelphia.
Papineni, Kishore A., Salim Roukos, and R. T.
Ward. 1997. Feature-based language
understanding. In Proceedings of
EuroSpeech?97, vol. 3, pages 1435?1438,
Rhodes, Greece.
Papineni, Kishore A., Salim Roukos, and
R. T. Ward. 1998. Maximum likelihood
and discriminative training of direct
translation models. In Proceedings of the
1998 IEEE International Conference on
Acoustics, Speech and Signal Processing,
vol. 1, pages 189?192, Seattle.
Pearl, Judea. 1988. Probabilistic Reasoning in
Intelligent Systems. Morgan Kaufmann, San
Mateo, CA.
Ratnaparkhi, Adwait. 1997. A linear observed
time statistical parser based on maximum
entropy models. In Proceedings of the Second
Conference on Empirical Methods in Natural
Language Processing, Brown University,
Providence, RI.
Ratnaparkhi, Adwait. 1998. Maximum Entropy
Models for Natural Language Ambiguity
Resolution. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
Ratnaparkhi, Adwait, Salim Roukos, and
R. T. Ward. 1994. A maximum entropy
model for parsing. In Proceedings of the
International Conference on Spoken Language
Processing, pages 803?806, Yokohama,
Japan.
Riezler, Stefan, Tracy H. King,
Ronald M. Kaplan, Richard Crouch,
John T. Maxwell III, and Mark Johnson.
2002. Parsing the Wall Street Journal
using a lexical-functional grammar
and discriminative estimation
techniques. In ACL 2002: Proceedings
of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia.
Riezler, Stefan and Alexander Vasserman.
2004. Incremental feature selection and
l1 regularization for relaxed
maximum-entropy modeling. In Proceedings
of the 2004 Conference on Empirical Methods in
Natural Language Processing (EMNLP?04),
Barcelona, Spain.
Rosenfeld, Ronald. 1997. A whole sentence
maximum entropy language model. In
Proceedings of the IEEE Workshop on
Speech Recognition and Understanding,
Santa Barbara, CA, December.
Schapire, Robert E., Yoav Freund, Peter
Bartlett, and W. S. Lee. 1998. Boosting the
margin: A new explanation for the
effectiveness of voting methods. Annals of
Statistics, 26(5):1651?1686.
Schapire, Robert E. and Yoram Singer. 1999.
Improved boosting algorithms using
confidence-rated predictions. Machine
Learning, 37(3):297?336.
Schapire, Robert E. and Yoram Singer. 2000.
BoosTexter: A boosting-based system for
text categorization. Machine Learning,
39(2/3):135?168.
Computational Linguistics Volume 31, Number 1
69
Sha, Fei and Fernando Pereira. 2003. Shallow
parsing with conditional random fields. In
Proceedings of HLT-NAACL 2003,
Edmonton, Alberta, Canada.
Shen, Libin, Anoop Sarkar, and Aravind K.
Joshi. 2003. Using LTAG based features in
parse reranking. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
Sapporo, Japan.
Valiant, Leslie G. 1984. A theory of the
learnable. Communications of the ACM,
27(11):1134?1142.
Walker, Marilyn, Owen Rambow, and
Monica Rogati. 2001. SPoT: A trainable
sentence planner. In Proceedings of the Second
Meeting of the North American Chapter of the
Association for Computational
Linguistics (NAACL 2001), Pittsburgh.
Zhou, Yaqian, Fuliang Weng, Lide Wu, and
Hauke Schmidt. 2003. A fast algorithm for
feature selection in conditional maximum
entropy modeling. In Proceedings of the 2003
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
Sapporo, Japan.
Collins and Koo Discriminative Reranking for NLP

Discriminative Language Modeling with
Conditional Random Fields and the Perceptron Algorithm
Brian Roark Murat Saraclar
AT&T Labs - Research
{roark,murat}@research.att.com
Michael Collins Mark Johnson
MIT CSAIL Brown University
mcollins@csail.mit.edu Mark Johnson@Brown.edu
Abstract
This paper describes discriminative language modeling
for a large vocabulary speech recognition task. We con-
trast two parameter estimation methods: the perceptron
algorithm, and a method based on conditional random
fields (CRFs). The models are encoded as determin-
istic weighted finite state automata, and are applied by
intersecting the automata with word-lattices that are the
output from a baseline recognizer. The perceptron algo-
rithm has the benefit of automatically selecting a rela-
tively small feature set in just a couple of passes over the
training data. However, using the feature set output from
the perceptron algorithm (initialized with their weights),
CRF training provides an additional 0.5% reduction in
word error rate, for a total 1.8% absolute reduction from
the baseline of 39.2%.
1 Introduction
A crucial component of any speech recognizer is the lan-
guage model (LM), which assigns scores or probabilities
to candidate output strings in a speech recognizer. The
language model is used in combination with an acous-
tic model, to give an overall score to candidate word se-
quences that ranks them in order of probability or plau-
sibility.
A dominant approach in speech recognition has been
to use a ?source-channel?, or ?noisy-channel? model. In
this approach, language modeling is effectively framed
as density estimation: the language model?s task is to
define a distribution over the source ? i.e., the possible
strings in the language. Markov (n-gram) models are of-
ten used for this task, whose parameters are optimized
to maximize the likelihood of a large amount of training
text. Recognition performance is a direct measure of the
effectiveness of a language model; an indirect measure
which is frequently proposed within these approaches is
the perplexity of the LM (i.e., the log probability it as-
signs to some held-out data set).
This paper explores alternative methods for language
modeling, which complement the source-channel ap-
proach through discriminatively trained models. The lan-
guage models we describe do not attempt to estimate a
generative model P (w) over strings. Instead, they are
trained on acoustic sequences with their transcriptions,
in an attempt to directly optimize error-rate. Our work
builds on previous work on language modeling using the
perceptron algorithm, described in Roark et al (2004).
In particular, we explore conditional random field meth-
ods, as an alternative training method to the perceptron.
We describe how these models can be trained over lat-
tices that are the output from a baseline recognizer. We
also give a number of experiments comparing the two ap-
proaches. The perceptron method gave a 1.3% absolute
improvement in recognition error on the Switchboard do-
main; the CRF methods we describe give a further gain,
the final absolute improvement being 1.8%.
A central issue we focus on concerns feature selection.
The number of distinct n-grams in our training data is
close to 45 million, and we show that CRF training con-
verges very slowly even when trained with a subset (of
size 12 million) of these features. Because of this, we ex-
plore methods for picking a small subset of the available
features.1 The perceptron algorithm can be used as one
method for feature selection, selecting around 1.5 million
features in total. The CRF trained with this feature set,
and initialized with parameters from perceptron training,
converges much more quickly than other approaches, and
also gives the optimal performance on the held-out set.
We explore other approaches to feature selection, but find
that the perceptron-based approach gives the best results
in our experiments.
While we focus on n-gram models, we stress that our
methods are applicable to more general language mod-
eling features ? for example, syntactic features, as ex-
plored in, e.g., Khudanpur and Wu (2000). We intend
to explore methods with new features in the future. Ex-
perimental results with n-gram models on 1000-best lists
show a very small drop in accuracy compared to the use
of lattices. This is encouraging, in that it suggests that
models with more flexible features than n-gram models,
which therefore cannot be efficiently used with lattices,
may not be unduly harmed by their restriction to n-best
lists.
1.1 Related Work
Large vocabulary ASR has benefitted from discrimina-
tive estimation of Hidden Markov Model (HMM) param-
eters in the form of Maximum Mutual Information Es-
timation (MMIE) or Conditional Maximum Likelihood
Estimation (CMLE). Woodland and Povey (2000) have
shown the effectiveness of lattice-based MMIE/CMLE in
challenging large scale ASR tasks such as Switchboard.
In fact, state-of-the-art acoustic modeling, as seen, for
example, at annual Switchboard evaluations, invariably
includes some kind of discriminative training.
Discriminative estimation of language models has also
been proposed in recent years. Jelinek (1995) suggested
an acoustic sensitive language model whose parameters
1Note also that in addition to concerns about training time, a lan-
guage model with fewer features is likely to be considerably more effi-
cient when decoding new utterances.
are estimated by minimizing H(W |A), the expected un-
certainty of the spoken text W, given the acoustic se-
quence A. Stolcke and Weintraub (1998) experimented
with various discriminative approaches including MMIE
with mixed results. This work was followed up with
some success by Stolcke et al (2000) where an ?anti-
LM?, estimated from weighted N-best hypotheses of a
baseline ASR system, was used with a negative weight
in combination with the baseline LM. Chen et al (2000)
presented a method based on changing the trigram counts
discriminatively, together with changing the lexicon to
add new words. Kuo et al (2002) used the generalized
probabilistic descent algorithm to train relatively small
language models which attempt to minimize string error
rate on the DARPA Communicator task. Banerjee et al
(2003) used a language model modification algorithm in
the context of a reading tutor that listens. Their algorithm
first uses a classifier to predict what effect each parame-
ter has on the error rate, and then modifies the parameters
to reduce the error rate based on this prediction.
2 Linear Models, the Perceptron
Algorithm, and Conditional Random
Fields
This section describes a general framework, global linear
models, and two parameter estimation methods within
the framework, the perceptron algorithm and a method
based on conditional random fields. The linear models
we describe are general enough to be applicable to a di-
verse range of NLP and speech tasks ? this section gives
a general description of the approach. In the next section
of the paper we describe how global linear models can
be applied to speech recognition. In particular, we focus
on how the decoding and parameter estimation problems
can be implemented over lattices using finite-state tech-
niques.
2.1 Global linear models
We follow the framework outlined in Collins (2002;
2004). The task is to learn a mapping from inputs x ? X
to outputs y ? Y . We assume the following compo-
nents: (1) Training examples (xi, yi) for i = 1 . . . N .
(2) A function GEN which enumerates a set of candi-
dates GEN(x) for an input x. (3) A representation
? mapping each (x, y) ? X ? Y to a feature vector
?(x, y) ? Rd. (4) A parameter vector ?? ? Rd.
The components GEN,? and ?? define a mapping
from an input x to an output F (x) through
F (x) = argmax
y?GEN(x)
?(x, y) ? ?? (1)
where ?(x, y) ? ?? is the inner product
?
s ?s?s(x, y).
The learning task is to set the parameter values ?? using
the training examples as evidence. The decoding algo-
rithm is a method for searching for the y that maximizes
Eq. 1.
2.2 The Perceptron algorithm
We now turn to methods for training the parameters
?? of the model, given a set of training examples
Inputs: Training examples (xi, yi)
Initialization: Set ?? = 0
Algorithm:
For t = 1 . . . T , i = 1 . . . N
Calculate zi = argmaxz?GEN(xi) ?(xi, z) ? ??
If(zi 6= yi) then ?? = ??+ ?(xi, yi)? ?(xi, zi)
Output: Parameters ??
Figure 1: A variant of the perceptron algorithm.
(x1, y1) . . . (xN , yN ). This section describes the per-
ceptron algorithm, which was previously applied to lan-
guage modeling in Roark et al (2004). The next section
describes an alternative method, based on conditional
random fields.
The perceptron algorithm is shown in figure 1. At
each training example (xi, yi), the current best-scoring
hypothesis zi is found, and if it differs from the refer-
ence yi , then the cost of each feature2 is increased by
the count of that feature in zi and decreased by the count
of that feature in yi. The features in the model are up-
dated, and the algorithm moves to the next utterance.
After each pass over the training data, performance on
a held-out data set is evaluated, and the parameterization
with the best performance on the held out set is what is
ultimately produced by the algorithm.
Following Collins (2002), we used the averaged pa-
rameters from the training algorithm in decoding held-
out and test examples in our experiments. Say ??ti is the
parameter vector after the i?th example is processed on
the t?th pass through the data in the algorithm in fig-
ure 1. Then the averaged parameters ??AVG are defined
as ??AVG =
?
i,t ??
t
i/NT . Freund and Schapire (1999)
originally proposed the averaged parameter method; it
was shown to give substantial improvements in accuracy
for tagging tasks in Collins (2002).
2.3 Conditional Random Fields
Conditional Random Fields have been applied to NLP
tasks such as parsing (Ratnaparkhi et al, 1994; Johnson
et al, 1999), and tagging or segmentation tasks (Lafferty
et al, 2001; Sha and Pereira, 2003; McCallum and Li,
2003; Pinto et al, 2003). CRFs use the parameters ??
to define a conditional distribution over the members of
GEN(x) for a given input x:
p??(y|x) =
1
Z(x, ??)
exp (?(x, y) ? ??)
where Z(x, ??) =
?
y?GEN(x) exp (?(x, y) ? ??) is a
normalization constant that depends on x and ??.
Given these definitions, the log-likelihood of the train-
ing data under parameters ?? is
LL(??) =
N?
i=1
log p??(yi|xi)
=
N?
i=1
[?(xi, yi) ? ??? logZ(xi, ??)] (2)
2Note that here lattice weights are interpreted as costs, which
changes the sign in the algorithm presented in figure 1.
Following Johnson et al (1999) and Lafferty et al
(2001), we use a zero-mean Gaussian prior on the pa-
rameters resulting in the regularized objective function:
LLR(??) =
N?
i=1
[?(xi, yi) ? ??? logZ(xi, ??)]?
||??||2
2?2
(3)
The value ? dictates the relative influence of the log-
likelihood term vs. the prior, and is typically estimated
using held-out data. The optimal parameters under this
criterion are ??? = argmax?? LLR(??).
We use a limited memory variable metric method
(Benson and More?, 2002) to optimize LLR. There is a
general implementation of this method in the Tao/PETSc
software libraries (Balay et al, 2002; Benson et al,
2002). This technique has been shown to be very effec-
tive in a variety of NLP tasks (Malouf, 2002; Wallach,
2002). The main interface between the optimizer and the
training data is a procedure which takes a parameter vec-
tor ?? as input, and in turn returns LLR(??) as well as
the gradient of LLR at ??. The derivative of the objec-
tive function with respect to a parameter ?s at parameter
values ?? is
?LLR
??s
=
N?
i=1
?
??s(xi, yi)?
?
y?GEN(xi)
p??(y|xi)?s(xi, y)
?
??
?s
?2
(4)
Note that LLR(??) is a convex function, so that there is
a globally optimal solution and the optimization method
will find it. The use of the Gaussian prior term ||??||2/2?2
in the objective function has been found to be useful in
several NLP settings. It effectively ensures that there is a
large penalty for parameter values in the model becoming
too large ? as such, it tends to control over-training. The
choice ofLLR as an objective function can be justified as
maximum a-posteriori (MAP) training within a Bayesian
approach. An alternative justification comes through a
connection to support vector machines and other large
margin approaches. SVM-based approaches use an op-
timization criterion that is closely related to LLR ? see
Collins (2004) for more discussion.
3 Linear models for speech recognition
We now describe how the formalism and algorithms in
section 2 can be applied to language modeling for speech
recognition.
3.1 The basic approach
As described in the previous section, linear models re-
quire definitions of X , Y , xi, yi, GEN, ? and a param-
eter estimation method. In the language modeling setting
we take X to be the set of all possible acoustic inputs; Y
is the set of all possible strings, ??, for some vocabu-
lary ?. Each xi is an utterance (a sequence of acous-
tic feature-vectors), and GEN(xi) is the set of possible
transcriptions under a first pass recognizer. (GEN(xi)
is a huge set, but will be represented compactly using a
lattice ? we will discuss this in detail shortly). We take
yi to be the member of GEN(xi) with lowest error rate
with respect to the reference transcription of xi.
All that remains is to define the feature-vector repre-
sentation, ?(x, y). In the general case, each component
?i(x, y) could be essentially any function of the acous-
tic input x and the candidate transcription y. The first
feature we define is ?0(x, y) as the log-probability of y
given x under the lattice produced by the baseline recog-
nizer. Thus this feature will include contributions from
the acoustic model and the original language model. The
remaining features are restricted to be functions over the
transcription y alone and they track all n-grams up to
some length (say n = 3), for example:
?1(x, y) = Number of times ?the the of? is seen in y
At an abstract level, features of this form are introduced
for all n-grams up to length 3 seen in some training data
lattice, i.e., n-grams seen in any word sequence within
the lattices. In practice, we consider methods that search
for sparse parameter vectors ??, thus assigning many n-
grams 0 weight. This will lead to more efficient algo-
rithms that avoid dealing explicitly with the entire set of
n-grams seen in training data.
3.2 Implementation using WFA
We now give a brief sketch of how weighted finite-state
automata (WFA) can be used to implement linear mod-
els for speech recognition. There are several papers de-
scribing the use of weighted automata and transducers
for speech in detail, e.g., Mohri et al (2002), but for clar-
ity and completeness this section gives a brief description
of the operations which we use.
For our purpose, a WFA A = (?, Q, qs, F, E, ?),
where ? is the vocabulary, Q is a (finite) set of states,
qs ? Q is a unique start state, F ? Q is a set of final
states, E is a (finite) set of transitions, and ? : F ? R
is a function from final states to final weights. Each tran-
sition e ? E is a tuple e = (l[e], p[e], n[e], w[e]), where
l[e] ? ? is a label (in our case, words), p[e] ? Q is the
origin state of e, n[e] ? Q is the destination state of e,
and w[e] ? R is the weight of the transition. A suc-
cessful path pi = e1 . . . ej is a sequence of transitions,
such that p[e1] = qs, n[ej ] ? F , and for 1 < k ? j,
n[ek?1] = p[ek]. Let ?A be the set of successful paths pi
in a WFA A. For any pi = e1 . . . ej , l[pi] = l[e1] . . . l[ej ].
The weights of the WFA in our case are always in the
log semiring, which means that the weight of a path pi =
e1 . . . ej ? ?A is defined as:
wA[pi] =
(
j?
k=1
w[ek]
)
+ ?(ej) (5)
By convention, we use negative log probabilities as
weights, so lower weights are better. All WFA that we
will discuss in this paper are deterministic, i.e. there are
no  transitions, and for any two transitions e, e? ? E,
if p[e] = p[e?], then l[e] 6= l[e?]. Thus, for any string
w = w1 . . . wj , there is at most one successful path
pi ? ?A, such that pi = e1 . . . ej and for 1 ? k ? j,
l[ek] = wk, i.e. l[pi] = w. The set of strings w such that
there exists a pi ? ?A with l[pi] = w define a regular
language LA ? ?.
We can now define some operations that will be used
in this paper.
? ?A. For a set of transitions E and ? ? R, define
?E = {(l[e], p[e], n[e], ?w[e]) : e ? E}. Then, for
any WFA A = (?, Q, qs, F, E, ?), define ?A for ? ? R
as follows: ?A = (?, Q, qs, F, ?E, ??).
? A ?A?. The intersection of two deterministic WFAs
A ? A? in the log semiring is a deterministic WFA
such that LA?A? = LA
?
LA? . For any pi ? ?A?A? ,
wA?A? [pi] = wA[pi1] + wA? [pi2], where l[pi] = l[pi1] =
l[pi2].
?BestPath(A). This operation takes a WFA A, and
returns the best scoring path p?i = argminpi??A wA[pi].
? MinErr(A, y). Given a WFA A, a string y, and
an error-function E(y,w), this operation returns p?i =
argminpi??A E(y, l[pi]). This operation will generally be
used with y as the reference transcription for a particular
training example, and E(y,w) as some measure of the
number of errors in w when compared to y. In this case,
the MinErr operation returns the path pi ? ?A such
l[pi] has the smallest number of errors when compared to
y.
? Norm(A). Given a WFA A, this operation yields
a WFA A? such that LA = LA? and for every pi ? ?A
there is a pi? ? ?A? such that l[pi] = l[pi?] and
wA? [pi
?] = wA[pi] + log
(
?
p?i??A
exp(?wA[p?i])
)
(6)
Note that
?
pi?Norm(A)
exp(?wNorm(A)[pi]) = 1 (7)
In other words the weights define a probability distribu-
tion over the paths.
? ExpCount(A,w). Given a WFA A and an n-gram
w, we define the expected count of w in A as
ExpCount(A,w) =
?
pi??A
wNorm(A)[pi]C(w, l[pi])
where C(w, l[pi]) is defined to be the number of times
the n-gram w appears in a string l[pi].
Given an acoustic input x, let Lx be a deterministic
word-lattice produced by the baseline recognizer. The
lattice Lx is an acyclic WFA, representing a weighted set
of possible transcriptions of x under the baseline recog-
nizer. The weights represent the combination of acoustic
and language model scores in the original recognizer.
The new, discriminative language model constructed
during training consists of a deterministic WFA which
we will denote D, together with a single parameter ?0.
The parameter ?0 is the weight for the log probability
feature ?0 given by the baseline recognizer. The WFA
D is constructed so that LD = ?? and for all pi ? ?D
wD[pi] =
d?
j=1
?j(x, l[pi])?j
Recall that ?j(x,w) for j > 0 is the count of the j?th n-
gram in w, and ?j is the parameter associated with that
w  wi-2     i-1 w   wi-1     iwi
wi-1
?
wi
?wi
?
? wi
Figure 2: Representation of a trigram model with failure transitions.
n-gram. Then, by definition, ?0L ? D accepts the same
set of strings as L, but
w?0L?D[pi] =
d?
j=0
?j(x, l[pi])?j
and argmin
pi?L
?(x, l[pi]) ? ?? = BestPath(?0L ? D).
Thus decoding under our new model involves first pro-
ducing a lattice L from the baseline recognizer; second,
scaling L with ?0 and intersecting it with the discrimi-
native language model D; third, finding the best scoring
path in the new WFA.
We now turn to training a model, or more explicitly,
deriving a discriminative language model (D, ?0) from a
set of training examples. Given a training set (xi, ri) for
i = 1 . . . N , where xi is an acoustic sequence, and ri is
a reference transcription, we can construct lattices Li for
i = 1 . . . N using the baseline recognizer. We can also
derive target transcriptions yi = MinErr(Li, ri). The
training algorithm is then a mapping from (Li, yi) for
i = 1 . . . N to a pair (D, ?0). Note that the construction
of the language model requires two choices. The first
concerns the choice of the set of n-gram features ?i for
i = 1 . . . d implemented by D. The second concerns
the choice of parameters ?i for i = 0 . . . d which assign
weights to the n-gram features as well as the baseline
feature ?0.
Before describing methods for training a discrimina-
tive language model using perceptron and CRF algo-
rithms, we give a little more detail about the structure
of D, focusing on how n-gram language models can be
implemented with finite-state techniques.
3.3 Representation of n-gram language models
An n-gram model can be efficiently represented in a de-
terministic WFA, through the use of failure transitions
(Allauzen et al, 2003). Every string accepted by such an
automaton has a single path through the automaton, and
the weight of the string is the sum of the weights of the
transitions in that path. In such a representation, every
state in the automaton represents an n-gram history h,
e.g. wi?2wi?1, and there are transitions leaving the state
for every word wi such that the feature hwi has a weight.
There is also a failure transition leaving the state, labeled
with some reserved symbol ?, which can only be tra-
versed if the next symbol in the input does not match any
transition leaving the state. This failure transition points
to the backoff state h?, i.e. the n-gram history h minus
its initial word. Figure 2 shows how a trigram model can
be represented in such an automaton. See Allauzen et al
(2003) for more details.
Note that in such a deterministic representation, the
entire weight of all features associated with the word
wi following history h must be assigned to the transi-
tion labeled with wi leaving the state h in the automa-
ton. For example, if h = wi?2wi?1, then the trigram
wi?2wi?1wi is a feature, as is the bigram wi?1wi and
the unigram wi. In this case, the weight on the transi-
tion wi leaving state h must be the sum of the trigram,
bigram and unigram feature weights. If only the trigram
feature weight were assigned to the transition, neither the
unigram nor the bigram feature contribution would be in-
cluded in the path weight. In order to ensure that the cor-
rect weights are assigned to each string, every transition
encoding an order k n-gram must carry the sum of the
weights for all n-gram features of orders ? k. To ensure
that every string in ?? receives the correct weight, for
any n-gram hw represented explicitly in the automaton,
h?w must also be represented explicitly in the automaton,
even if its weight is 0.
3.4 The perceptron algorithm
The perceptron algorithm is incremental, meaning that
the language model D is built one training example at
a time, during several passes over the training set. Ini-
tially, we build D to accept all strings in ?? with weight
0. For the perceptron experiments, we chose the param-
eter ?0 to be a fixed constant, chosen by optimization on
the held-out set. The loop in the algorithm in figure 1 is
implemented as:
For t = 1 . . . T, i = 1 . . . N :
? Calculate zi = argmaxy?GEN(x) ?(x, y) ? ??
= BestPath(?0Li ? D)
? If zi 6= MinErr(Li, ri), then update the feature
weights as in figure 1 (modulo the sign, because of
the use of costs), and modify D so as to assign the
correct weight to all strings.
In addition, averaged parameters need to be stored
(see section 2.2). These parameters will replace the un-
averaged parameters in D once training is completed.
Note that the only n-gram features to be included in
D at the end of the training process are those that oc-
cur in either a best scoring path zi or a minimum error
path yi at some point during training. Thus the percep-
tron algorithm is in effect doing feature selection as a
by-product of training. Given N training examples, and
T passes over the training set,O(NT ) n-grams will have
non-zero weight after training. Experiments in Roark et
al. (2004) suggest that the perceptron reaches optimal
performance after a small number of training iterations,
for example T = 1 or T = 2. Thus O(NT ) can be very
small compared to the full number of n-grams seen in
all training lattices. In our experiments, the perceptron
method chose around 1.4 million n-grams with non-zero
weight. This compares to 43.65 million possible n-grams
seen in the training data.
This is a key contrast with conditional random fields,
which optimize the parameters of a fixed feature set. Fea-
ture selection can be critical in our domain, as training
and applying a discriminative language model over all
n-grams seen in the training data (in either correct or in-
correct transcriptions) may be computationally very de-
manding. One training scenario that we will consider
will be using the output of the perceptron algorithm (the
averaged parameters) to provide the feature set and the
initial feature weights for use in the CRF algorithm. This
leads to a model which is reasonably sparse, but has the
benefit of CRF training, which as we will see gives gains
in performance.
3.5 Conditional Random Fields
The CRF methods that we use assume a fixed definition
of the n-gram features ?i for i = 1 . . . d in the model.
In the experimental section we will describe a number of
ways of defining the feature set. The optimization meth-
ods we use begin at some initial setting for ??, and then
search for the parameters ??? which maximize LLR(??)
as defined in Eq. 3.
The optimization method requires calculation of
LLR(??) and the gradient of LLR(??) for a series of val-
ues for ??. The first step in calculating these quantities is
to take the parameter values ??, and to construct an ac-
ceptor D which accepts all strings in ??, such that
wD[pi] =
d?
j=1
?j(x, l[pi])?j
For each training lattice Li, we then construct a new lat-
tice L?i = Norm(?0Li ? D). The lattice L?i represents
(in the log domain) the distribution p??(y|xi) over strings
y ? GEN(xi). The value of log p??(yi|xi) for any i can
be computed by simply taking the path weight of pi such
that l[pi] = yi in the new lattice L?i. Hence computation
of LLR(??) in Eq. 3 is straightforward.
Calculating the n-gram feature gradients for the CRF
optimization is also relatively simple, once L?i has been
constructed. From the derivative in Eq. 4, for each i =
1 . . . N, j = 1 . . . d the quantity
?j(xi, yi)?
?
y?GEN(xi)
p??(y|xi)?j(xi, y) (8)
must be computed. The first term is simply the num-
ber of times the j?th n-gram feature is seen in yi. The
second term is the expected number of times that the
j?th n-gram is seen in the acceptor L?i. If the j?th
n-gram is w1 . . . wn, then this can be computed as
ExpCount(L?i, w1 . . . wn). The GRM library, which
was presented in Allauzen et al (2003), has a direct im-
plementation of the function ExpCount, which simul-
taneously calculates the expected value of all n-grams of
order less than or equal to a given n in a lattice L.
The one non-ngram feature weight that is being esti-
mated is the weight ?0 given to the baseline ASR nega-
tive log probability. Calculation of the gradient of LLR
with respect to this parameter again requires calculation
of the term in Eq. 8 for j = 0 and i = 1 . . . N . Com-
putation of
?
y?GEN(xi)
p??(y|xi)?0(xi, y) turns out to
be not as straightforward as calculating n-gram expec-
tations. To do so, we rely upon the fact that ?0(xi, y),
the negative log probability of the path, decomposes to
the sum of negative log probabilities of each transition
in the path. We index each transition in the lattice Li,
and store its negative log probability under the baseline
model. We can then calculate the required gradient from
L?i, by calculating the expected value in L?i of each in-
dexed transition in Li.
We found that an approximation to the gradient of
?0, however, performed nearly identically to this exact
gradient, while requiring substantially less computation.
Let wn1 be a string of n words, labeling a path in word-
lattice L?i. For brevity, let Pi(wn1 ) = p??(wn1 |xi) be the
conditional probability under the current model, and let
Qi(wn1 ) be the probability of wn1 in the normalized base-
line ASR lattice Norm(Li). Let Li be the set of strings
in the language defined by Li. Then we wish to compute
Ei for i = 1 . . . N , where
Ei =
?
wn1 ?Li
Pi(w
n
1 ) log Qi(w
n
1 )
=
?
wn1 ?Li
?
k=1...n
Pi(w
n
1 ) log Qi(wk|w
k?1
1 ) (9)
The approximation is to make the following Markov
assumption:
Ei ?
?
wn1 ?Li
?
k=1...n
Pi(w
n
1 ) log Qi(wk|w
k?1
k?2)
=
?
xyz?Si
ExpCount(L?i, xyz) log Qi(z|xy)(10)
where Si is the set of all trigrams seen in Li. The term
log Qi(z|xy) can be calculated once before training for
every lattice in the training set; the ExpCount term is
calculated as before using the GRM library. We have
found this approximation to be effective in practice, and
it was used for the trials reported below.
When the gradients and conditional likelihoods are
collected from all of the utterances in the training set, the
contributions from the regularizer are combined to give
an overall gradient and objective function value. These
values are provided to the parameter estimation routine,
which then returns the parameters for use in the next it-
eration. The accumulation of gradients for the feature set
is the most time consuming part of the approach, but this
is parallelizable, so that the computation can be divided
among many processors.
4 Empirical Results
We present empirical results on the Rich Transcription
2002 evaluation test set (rt02), which we used as our de-
velopment set, as well as on the Rich Transcription 2003
Spring evaluation CTS test set (rt03). The rt02 set con-
sists of 6081 sentences (63804 words) and has three sub-
sets: Switchboard 1, Switchboard 2, Switchboard Cel-
lular. The rt03 set consists of 9050 sentences (76083
words) and has two subsets: Switchboard and Fisher.
We used the same training set as that used in Roark
et al (2004). The training set consists of 276726 tran-
scribed utterances (3047805 words), with an additional
20854 utterances (249774 words) as held out data. For
0 500 100037
37.5
38
38.5
39
39.5
40
Iterations over training
Word
 erro
r rate
Baseline recognizerPerceptron, Feat=PL, LatticePerceptron, Feat=PN, N=1000CRF, ? = ?, Feat=PL, LatticeCRF, ? = 0.5, Feat=PL, LatticeCRF, ? = 0.5, Feat=PN, N=1000
Figure 3: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
each utterance, a weighted word-lattice was produced,
representing alternative transcriptions, from the ASR
system. From each word-lattice, the oracle best path
was extracted, which gives the best word-error rate from
among all of the hypotheses in the lattice. The oracle
word-error rate for the training set lattices was 12.2%.
We also performed trials with 1000-best lists for the same
training set, rather than lattices. The oracle score for the
1000-best lists was 16.7%.
To produce the word-lattices, each training utterance
was processed by the baseline ASR system. However,
these same utterances are what the acoustic and language
models are built from, which leads to better performance
on the training utterances than can be expected when the
ASR system processes unseen utterances. To somewhat
control for this, the training set was partitioned into 28
sets, and baseline Katz backoff trigram models were built
for each set by including only transcripts from the other
27 sets. Since language models are generally far more
prone to overtrain than standard acoustic models, this
goes a long way toward making the training conditions
similar to testing conditions.
There are three baselines against which we are com-
paring. The first is the ASR baseline, with no reweight-
ing from a discriminatively trained n-gram model. The
other two baselines are with perceptron-trained n-gram
model re-weighting, and were reported in Roark et al
(2004). The first of these is for a pruned-lattice trained
trigram model, which showed a reduction in word er-
ror rate (WER) of 1.3%, from 39.2% to 37.9% on rt02.
The second is for a 1000-best list trained trigram model,
which performed only marginally worse than the lattice-
trained perceptron, at 38.0% on rt02.
4.1 Perceptron feature set
We use the perceptron-trained models as the starting
point for our CRF algorithm: the feature set given to
the CRF algorithm is the feature set selected by the per-
ceptron algorithm; the feature weights are initialized to
those of the averaged perceptron. Figure 3 shows the
performance of our three baselines versus three trials of
0 500 1000 1500 2000 250037
37.5
38
38.5
39
39.5
40
Iterations over training
Word
 erro
r rate
Baseline recognizerPerceptron, Feat=PL, LatticeCRF, ? = 0.5, Feat=PL, LatticeCRF, ? = 0.5, Feat=E,  ?=0.01CRF, ? = 0.5, Feat=E,  ?=0.9
Figure 4: Word error rate on the rt02 eval set versus training
iterations for CRF trials, contrasted with baseline recognizer
performance and perceptron performance. Points are at every
20 iterations. Each point (x,y) is the WER at the iteration with
the best objective function value in the interval (x-20,x].
the CRF algorithm. In the first two trials, the training
set consists of the pruned lattices, and the feature set
is from the perceptron algorithm trained on pruned lat-
tices. There were 1.4 million features in this feature set.
The first trial set the regularizer constant ? =?, so that
the algorithm was optimizing raw conditional likelihood.
The second trial is with the regularizer constant ? = 0.5,
which we found empirically to be a good parameteriza-
tion on the held-out set. As can be seen from these re-
sults, regularization is critical.
The third trial in this set uses the feature set from the
perceptron algorithm trained on 1000-best lists, and uses
CRF optimization on these on these same 1000-best lists.
There were 0.9 million features in this feature set. For
this trial, we also used ? = 0.5. As with the percep-
tron baselines, the n-best trial performs nearly identically
with the pruned lattices, here also resulting in 37.4%
WER. This may be useful for techniques that would be
more expensive to extend to lattices versus n-best lists
(e.g. models with unbounded dependencies).
These trials demonstrate that the CRF algorithm can
do a better job of estimating feature weights than the per-
ceptron algorithm for the same feature set. As mentioned
in the earlier section, feature selection is a by-product of
the perceptron algorithm, but the CRF algorithm is given
a set of features. The next two trials looked at selecting
feature sets other than those provided by the perceptron
algorithm.
4.2 Other feature sets
In order for the feature weights to be non-zero in this ap-
proach, they must be observed in the training set. The
number of unigram, bigram and trigram features with
non-zero observations in the training set lattices is 43.65
million, or roughly 30 times the size of the perceptron
feature set. Many of these features occur only rarely
with very low conditional probabilities, and hence cannot
meaningfully impact system performance. We pruned
this feature set to include all unigrams and bigrams, but
only those trigrams with an expected count of greater
than 0.01 in the training set. That is, to be included, a
Trial Iter rt02 rt03
ASR Baseline - 39.2 38.2
Perceptron, Lattice - 37.9 36.9
Perceptron, N-best - 38.0 37.2
CRF, Lattice, Percep Feats (1.4M) 769 37.4 36.5
CRF, N-best, Percep Feats (0.9M) 946 37.4 36.6
CRF, Lattice, ? = 0.01 (12M) 2714 37.6 36.5
CRF, Lattice, ? = 0.9 (1.5M) 1679 37.5 36.6
Table 1: Word-error rate results at convergence iteration for
various trials, on both Switchboard 2002 test set (rt02), which
was used as the dev set, and Switchboard 2003 test set (rt03).
trigram must occur in a set of paths, the sum of the con-
ditional probabilities of which must be greater than our
threshold ? = 0.01. This threshold resulted in a feature
set of roughly 12 million features, nearly 10 times the
size of the perceptron feature set. For better comparabil-
ity with that feature set, we set our thresholds higher, so
that trigrams were pruned if their expected count fell be-
low ? = 0.9, and bigrams were pruned if their expected
count fell below ? = 0.1. We were concerned that this
may leave out some of the features on the oracle paths, so
we added back in all bigram and trigram features that oc-
curred on oracle paths, giving a feature set of 1.5 million
features, roughly the same size as the perceptron feature
set.
Figure 4 shows the results for three CRF trials versus
our ASR baseline and the perceptron algorithm baseline
trained on lattices. First, the result using the perceptron
feature set provides us with a WER of 37.4%, as pre-
viously shown. The WER at convergence for the big
feature set (12 million features) is 37.6%; the WER at
convergence for the smaller feature set (1.5 million fea-
tures) is 37.5%. While both of these other feature sets
converge to performance close to that using the percep-
tron features, the number of iterations over the training
data that are required to reach that level of performance
are many more than for the perceptron-initialized feature
set.
Table 1 shows the word-error rate at the convergence
iteration for the various trials, on both rt02 and rt03. All
of the CRF trials are significantly better than the percep-
tron performance, using the Matched Pair Sentence Seg-
ment test for WER included with SCTK (NIST, 2000).
On rt02, the N-best and perceptron initialized CRF trials
were were significantly better than the lattice perceptron
at p < 0.001; the other two CRF trials were significantly
better than the lattice perceptron at p < 0.01. On rt03,
the N-best CRF trial was significantly better than the lat-
tice perceptron at p < 0.002; the other three CRF tri-
als were significantly better than the lattice perceptron at
p < 0.001.
Finally, we measured the time of a single iteration over
the training data on a single machine for the perceptron
algorithm, the CRF algorithm using the approximation to
the gradient of ?0, and the CRF algorithm using an exact
gradient of ?0. Table 2 shows these times in hours. Be-
cause of the frequent update of the weights in the model,
the perceptron algorithm is more expensive than the CRF
algorithm for a single iteration. Further, the CRF algo-
rithm is parallelizable, so that most of the work of an
CRF
Features Percep approx exact
Lattice, Percep Feats (1.4M) 7.10 1.69 3.61
N-best, Percep Feats (0.9M) 3.40 0.96 1.40
Lattice, ? = 0.01 (12M) - 2.24 4.75
Table 2: Time (in hours) for one iteration on a single Intel
Xeon 2.4Ghz processor with 4GB RAM.
iteration can be shared among multiple processors. Our
most common training setup for the CRF algorithm was
parallelized between 20 processors, using the approxi-
mation to the gradient. In that setup, using the 1.4M fea-
ture set, one iteration of the perceptron algorithm took
the same amount of real time as approximately 80 itera-
tions of CRF.
5 Conclusion
We have contrasted two approaches to discriminative
language model estimation on a difficult large vocabu-
lary task, showing that they can indeed scale effectively
to handle this size of a problem. Both algorithms have
their benefits. The perceptron algorithm selects a rela-
tively small subset of the total feature set, and requires
just a couple of passes over the training data. The CRF
algorithm does a better job of parameter estimation for
the same feature set, and is parallelizable, so that each
pass over the training set can require just a fraction of
the real time of the perceptron algorithm.
The best scenario from among those that we investi-
gated was a combination of both approaches, with the
output of the perceptron algorithm taken as the starting
point for CRF estimation.
As a final point, note that the methods we describe do
not replace an existing language model, but rather com-
plement it. The existing language model has the benefit
that it can be trained on a large amount of text that does
not have speech transcriptions. It has the disadvantage
of not being a discriminative model. The new language
model is trained on the speech transcriptions, meaning
that it has less training data, but that it has the advan-
tage of discriminative training ? and in particular, the ad-
vantage of being able to learn negative evidence in the
form of negative weights on n-grams which are rarely
or never seen in natural language text (e.g., ?the of?),
but are produced too frequently by the recognizer. The
methods we describe combines the two language models,
allowing them to complement each other.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized
algorithms for constructing language models. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguis-
tics, pages 40?47.
Satish Balay, William D. Gropp, Lois Curfman McInnes, and Barry F.
Smith. 2002. Petsc users manual. Technical Report ANL-95/11-
Revision 2.1.2, Argonne National Laboratory.
Satanjeev Banerjee, Jack Mostow, Joseph Beck, and Wilson Tam.
2003. Improving language models by learning from speech recog-
nition errors in a reading tutor that listens. In Proceedings of the
Second International Conference on Applied Artificial Intelligence,
Fort Panhala, Kolhapur, India.
Steven J. Benson and Jorge J. More?. 2002. A limited memory vari-
able metric method for bound constrained minimization. Preprint
ANL/ACSP909-0901, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge J. More?, and Jason
Sarich. 2002. Tao users manual. Technical Report ANL/MCS-TM-
242-Revision 1.4, Argonne National Laboratory.
Zheng Chen, Kai-Fu Lee, and Ming Jing Li. 2000. Discriminative
training on language model. In Proceedings of the Sixth Interna-
tional Conference on Spoken Language Processing (ICSLP), Bei-
jing, China.
Michael Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron algo-
rithms. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages 1?8.
Michael Collins. 2004. Parameter estimation for statistical parsing
models: Theory and practice of distribution-free methods. In Harry
Bunt, John Carroll, and Giorgio Satta, editors, New Developments
in Parsing Technology. Kluwer.
Yoav Freund and Robert Schapire. 1999. Large margin classification
using the perceptron algorithm. Machine Learning, 3(37):277?296.
Frederick Jelinek. 1995. Acoustic sensitive language modeling. Tech-
nical report, Center for Language and Speech Processing, Johns
Hopkins University, Baltimore, MD.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and Stefan
Riezler. 1999. Estimators for stochastic ?unification-based? gram-
mars. In Proceedings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 535?541.
Sanjeev Khudanpur and Jun Wu. 2000. Maximum entropy techniques
for exploiting syntactic, semantic and collocational dependencies in
language modeling. Computer Speech and Language, 14(4):355?
372.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and Chin-
Hui Lee. 2002. Discriminative training of language models for
speech recognition. In Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing (ICASSP), Orlando,
Florida.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Con-
ditional random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282?289, Williams
College, Williamstown, MA, USA.
Robert Malouf. 2002. A comparison of algorithms for maximum en-
tropy parameter estimation. In Proc. CoNLL, pages 49?55.
Andrew McCallum and Wei Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction and
web-enhanced lexicons. In Proc. CoNLL.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002.
Weighted finite-state transducers in speech recognition. Computer
Speech and Language, 16(1):69?88.
NIST. 2000. Speech recognition scoring toolkit (sctk) version 1.2c.
Available at http://www.nist.gov/speech/tools.
David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. 2003.
Table extraction using conditional random fields. In Proc. ACM SI-
GIR.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward. 1994. A max-
imum entropy model for parsing. In Proceedings of the Interna-
tional Conference on Spoken Language Processing (ICSLP), pages
803?806.
Brian Roark, Murat Saraclar, and Michael Collins. 2004. Corrective
language modeling for large vocabulary ASR with the perceptron al-
gorithm. In Proceedings of the International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP), pages 749?752.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. HLT-NAACL, Edmonton, Canada.
A. Stolcke and M. Weintraub. 1998. Discriminitive language model-
ing. In Proceedings of the 9th Hub-5 Conversational Speech Recog-
nition Workshop.
A. Stolcke, H. Bratt, J. Butzberger, H. Franco, V. R. Rao Gadde,
M. Plauche, C. Richey, E. Shriberg, K. Sonmez, F. Weng, and
J. Zheng. 2000. The SRI March 2000 Hub-5 conversational speech
transcription system. In Proceedings of the NIST Speech Transcrip-
tion Workshop.
Hanna Wallach. 2002. Efficient training of conditional random fields.
Master?s thesis, University of Edinburgh.
P.C. Woodland and D. Povey. 2000. Large scale discriminative training
for speech recognition. In Proc. ISCA ITRW ASR2000, pages 7?16.
Incremental Parsing with the Perceptron Algorithm
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Brian Roark
AT&T Labs - Research
roark@research.att.com
Abstract
This paper describes an incremental parsing approach
where parameters are estimated using a variant of the
perceptron algorithm. A beam-search algorithm is used
during both training and decoding phases of the method.
The perceptron approach was implemented with the
same feature set as that of an existing generative model
(Roark, 2001a), and experimental results show that it
gives competitive performance to the generative model
on parsing the Penn treebank. We demonstrate that train-
ing a perceptron model to combine with the generative
model during search provides a 2.1 percent F-measure
improvement over the generative model alone, to 88.8
percent.
1 Introduction
In statistical approaches to NLP problems such as tag-
ging or parsing, it seems clear that the representation
used as input to a learning algorithm is central to the ac-
curacy of an approach. In an ideal world, the designer
of a parser or tagger would be free to choose any fea-
tures which might be useful in discriminating good from
bad structures, without concerns about how the features
interact with the problems of training (parameter estima-
tion) or decoding (search for the most plausible candidate
under the model). To this end, a number of recently pro-
posed methods allow a model to incorporate ?arbitrary?
global features of candidate analyses or parses. Exam-
ples of such techniques are Markov Random Fields (Rat-
naparkhi et al, 1994; Abney, 1997; Della Pietra et al,
1997; Johnson et al, 1999), and boosting or perceptron
approaches to reranking (Freund et al, 1998; Collins,
2000; Collins and Duffy, 2002).
A drawback of these approaches is that in the general
case, they can require exhaustive enumeration of the set
of candidates for each input sentence in both the train-
ing and decoding phases1. For example, Johnson et al
(1999) and Riezler et al (2002) use all parses generated
by an LFG parser as input to an MRF approach ? given
the level of ambiguity in natural language, this set can
presumably become extremely large. Collins (2000) and
Collins and Duffy (2002) rerank the top N parses from
an existing generative parser, but this kind of approach
1Dynamic programming methods (Geman and Johnson, 2002; Laf-
ferty et al, 2001) can sometimes be used for both training and decod-
ing, but this requires fairly strong restrictions on the features in the
model.
presupposes that there is an existing baseline model with
reasonable performance. Many of these baseline models
are themselves used with heuristic search techniques, so
that the potential gain through the use of discriminative
re-ranking techniques is further dependent on effective
search.
This paper explores an alternative approach to pars-
ing, based on the perceptron training algorithm intro-
duced in Collins (2002). In this approach the training
and decoding problems are very closely related ? the
training method decodes training examples in sequence,
and makes simple corrective updates to the parameters
when errors are made. Thus the main complexity of the
method is isolated to the decoding problem. We describe
an approach that uses an incremental, left-to-right parser,
with beam search, to find the highest scoring analysis un-
der the model. The same search method is used in both
training and decoding. We implemented the perceptron
approach with the same feature set as that of an existing
generative model (Roark, 2001a), and show that the per-
ceptron model gives performance competitive to that of
the generative model on parsing the Penn treebank, thus
demonstrating that an unnormalized discriminative pars-
ing model can be applied with heuristic search. We also
describe several refinements to the training algorithm,
and demonstrate their impact on convergence properties
of the method.
Finally, we describe training the perceptron model
with the negative log probability given by the generative
model as another feature. This provides the perceptron
algorithm with a better starting point, leading to large
improvements over using either the generative model or
the perceptron algorithm in isolation (the hybrid model
achieves 88.8% f-measure on the WSJ treebank, com-
pared to figures of 86.7% and 86.6% for the separate
generative and perceptron models). The approach is an
extremely simple method for integrating new features
into the generative model: essentially all that is needed
is a definition of feature-vector representations of entire
parse trees, and then the existing parsing algorithms can
be used for both training and decoding with the models.
2 The General Framework
In this section we describe a general framework ? linear
models for NLP ? that could be applied to a diverse range
of tasks, including parsing and tagging. We then describe
a particular method for parameter estimation, which is a
generalization of the perceptron algorithm. Finally, we
give an abstract description of an incremental parser, and
describe how it can be used with the perceptron algo-
rithm.
2.1 Linear Models for NLP
We follow the framework outlined in Collins (2002;
2004). The task is to learn a mapping from inputs x ? X
to outputs y ? Y . For example, X might be a set of sen-
tences, with Y being a set of possible parse trees. We
assume:
. Training examples (xi, yi) for i = 1 . . . n.
. A function GEN which enumerates a set of candi-
dates GEN(x) for an input x.
. A representation ? mapping each (x, y) ? X ?Y
to a feature vector ?(x, y) ? Rd.
. A parameter vector ?? ? Rd.
The components GEN,? and ?? define a mapping from
an input x to an output F (x) through
F (x) = arg max
y?GEN(x)
?(x, y) ? ?? (1)
where ?(x, y) ? ?? is the inner product
?
s ?s?s(x, y).
The learning task is to set the parameter values ?? using
the training examples as evidence. The decoding algo-
rithm is a method for searching for the arg max in Eq. 1.
This framework is general enough to encompass sev-
eral tasks in NLP. In this paper we are interested in pars-
ing, where (xi, yi), GEN, and ? can be defined as fol-
lows:
? Each training example (xi, yi) is a pair where xi is
a sentence, and yi is the gold-standard parse for that
sentence.
? Given an input sentence x, GEN(x) is a set of
possible parses for that sentence. For example,
GEN(x) could be defined as the set of possible
parses for x under some context-free grammar, per-
haps a context-free grammar induced from the train-
ing examples.
? The representation ?(x, y) could track arbitrary
features of parse trees. As one example, suppose
that there are m rules in a context-free grammar
(CFG) that defines GEN(x). Then we could define
the i?th component of the representation, ?i(x, y),
to be the number of times the i?th context-free rule
appears in the parse tree (x, y). This is implicitly
the representation used in probabilistic or weighted
CFGs.
Note that the difficulty of finding the arg max in Eq. 1
is dependent on the interaction of GEN and ?. In many
cases GEN(x) could grow exponentially with the size
of x, making brute force enumeration of the members
of GEN(x) intractable. For example, a context-free
grammar could easily produce an exponentially growing
number of analyses with sentence length. For some rep-
resentations, such as the ?rule-based? representation de-
scribed above, the arg max in the set enumerated by the
CFG can be found efficiently, using dynamic program-
ming algorithms, without having to explicitly enumer-
ate all members of GEN(x). However in many cases
we may be interested in representations which do not al-
low efficient dynamic programming solutions. One way
around this problem is to adopt a two-pass approach,
where GEN(x) is the top N analyses under some initial
model, as in the reranking approach of Collins (2000).
In the current paper we explore alternatives to rerank-
ing approaches, namely heuristic methods for finding the
arg max, specifically incremental beam-search strategies
related to the parsers of Roark (2001a) and Ratnaparkhi
(1999).
2.2 The Perceptron Algorithm for Parameter
Estimation
We now consider the problem of setting the parameters,
??, given training examples (xi, yi). We will briefly re-
view the perceptron algorithm, and its convergence prop-
erties ? see Collins (2002) for a full description. The
algorithm and theorems are based on the approach to
classification problems described in Freund and Schapire
(1999).
Figure 1 shows the algorithm. Note that the
most complex step of the method is finding zi =
arg maxz?GEN(xi) ?(xi, z)??? ? and this is precisely the
decoding problem. Thus the training algorithm is in prin-
ciple a simple part of the parser: any system will need
a decoding method, and once the decoding algorithm is
implemented the training algorithm is relatively straight-
forward.
We will now give a first theorem regarding the con-
vergence of this algorithm. First, we need the following
definition:
Definition 1 Let GEN(xi) = GEN(xi) ? {yi}. In
other words GEN(xi) is the set of incorrect candidates
for an example xi. We will say that a training sequence
(xi, yi) for i = 1 . . . n is separable with margin ? > 0
if there exists some vector U with ||U|| = 1 such that
?i, ?z ? GEN(xi), U ? ?(xi, yi)?U ? ?(xi, z) ? ?
(2)
(||U|| is the 2-norm of U, i.e., ||U|| = ??s U2s.)
Next, define Ne to be the number of times an error is
made by the algorithm in figure 1 ? that is, the number of
times that zi 6= yi for some (t, i) pair. We can then state
the following theorem (see (Collins, 2002) for a proof):
Theorem 1 For any training sequence (xi, yi) that is
separable with margin ?, for any value of T , then for
the perceptron algorithm in figure 1
Ne ?
R2
?2
where R is a constant such that ?i, ?z ?
GEN(xi) ||?(xi, yi)? ?(xi, z)|| ? R.
This theorem implies that if there is a parameter vec-
tor U which makes zero errors on the training set, then
after a finite number of iterations the training algorithm
will converge to parameter values with zero training er-
ror. A crucial point is that the number of mistakes is in-
dependent of the number of candidates for each example
Inputs: Training examples (xi, yi) Algorithm:
Initialization: Set ?? = 0 For t = 1 . . . T , i = 1 . . . n
Output: Parameters ?? Calculate zi = arg maxz?GEN(xi) ?(xi, z) ? ??
If(zi 6= yi) then ?? = ??+ ?(xi, yi)? ?(xi, zi)
Figure 1: A variant of the perceptron algorithm.
(i.e. the size of GEN(xi) for each i), depending only
on the separation of the training data, where separation
is defined above. This is important because in many NLP
problems GEN(x) can be exponential in the size of the
inputs. All of the convergence and generalization results
in Collins (2002) depend on notions of separability rather
than the size of GEN.
Two questions come to mind. First, are there guar-
antees for the algorithm if the training data is not sepa-
rable? Second, performance on a training sample is all
very well, but what does this guarantee about how well
the algorithm generalizes to newly drawn test examples?
Freund and Schapire (1999) discuss how the theory for
classification problems can be extended to deal with both
of these questions; Collins (2002) describes how these
results apply to NLP problems.
As a final note, following Collins (2002), we used the
averaged parameters from the training algorithm in de-
coding test examples in our experiments. Say ??ti is the
parameter vector after the i?th example is processed on
the t?th pass through the data in the algorithm in fig-
ure 1. Then the averaged parameters ??AVG are defined
as ??AVG =
?
i,t ??
t
i/NT . Freund and Schapire (1999)
originally proposed the averaged parameter method; it
was shown to give substantial improvements in accuracy
for tagging tasks in Collins (2002).
2.3 An Abstract Description of Incremental
Parsing
This section gives a description of the basic incremental
parsing approach. The input to the parser is a sentence
x with length n. A hypothesis is a triple ?x, t, i? such
that x is the sentence being parsed, t is a partial or full
analysis of that sentence, and i is an integer specifying
the number of words of the sentence which have been
processed. Each full parse for a sentence will have the
form ?x, t, n?. The initial state is ?x, ?, 0? where ? is a
?null? or empty analysis.
We assume an ?advance? function ADV which takes
a hypothesis triple as input, and returns a set of new hy-
potheses as output. The advance function will absorb
another word in the sentence: this means that if the input
to ADV is ?x, t, i?, then each member of ADV(?x, t, i?)
will have the form ?x, t?,i+1?. Each new analysis t? will
be formed by somehow incorporating the i+1?th word
into the previous analysis t.
With these definitions in place, we can iteratively de-
fine the full set of partial analysesHi for the first i words
of the sentence as H0(x) = {?x, ?, 0?}, and Hi(x) =
?h??Hi?1(x)ADV(h?) for i = 1 . . . n. The full set of
parses for a sentence x is then GEN(x) = Hn(x) where
n is the length of x.
Under this definition GEN(x) can include a huge
number of parses, and searching for the highest scor-
ing parse, arg maxh?Hn(x) ?(h) ? ??, will be intractable.
For this reason we introduce one additional function,
FILTER(H), which takes a set of hypotheses H, and re-
turns a much smaller set of ?filtered? hypotheses. Typi-
cally, FILTER will calculate the score ?(h) ? ?? for each
h ? H, and then eliminate partial analyses which have
low scores under this criterion. For example, a simple
version of FILTER would take the top N highest scoring
members of H for some constant N . We can then rede-
fine the set of partial analyses as follows (we use Fi(x)
to denote the set of filtered partial analyses for the first i
words of the sentence):
F0(x) = {?x, ?, 0?}
Fi(x) = FILTER
(
?h??Fi?1(x)ADV(h?)
)
for i=1 . . . n
The parsing algorithm returns arg maxh?Fn ?(h) ? ??.
Note that this is a heuristic, in that there is no guar-
antee that this procedure will find the highest scoring
parse, arg maxh?Hn ?(h) ? ??. Search errors, where
arg maxh?Fn ?(h) ? ?? 6= arg maxh?Hn ?(h) ? ??, will
create errors in decoding test sentences, and also errors in
implementing the perceptron training algorithm in Fig-
ure 1. In this paper we give empirical results that suggest
that FILTER can be chosen in such a way as to give ef-
ficient parsing performance together with high parsing
accuracy.
The exact implementation of the parser will depend on
the definition of partial analyses, of ADV and FILTER,
and of the representation ?. The next section describes
our instantiation of these choices.
3 A full description of the parsing
approach
The parser is an incremental beam-search parser very
similar to the sort described in Roark (2001a; 2004), with
some changes in the search strategy to accommodate the
perceptron feature weights. We first describe the parsing
algorithm, and then move on to the baseline feature set
for the perceptron model.
3.1 Parser control
The input to the parser is a string wn0 , a grammar G, a
mapping ? from derivations to feature vectors, and a pa-
rameter vector ??. The grammar G = (V, T,S?, S?, C,B)
consists of a set of non-terminal symbols V , a set of ter-
minal symbols T , a start symbol S? ? V , an end-of-
constituent symbol S? ? V , a set of ?allowable chains?C,
and a set of ?allowable triples? B. S? is a special empty
non-terminal that marks the end of a constituent. Each
chain is a sequence of non-terminals followed by a ter-
minal symbol, for example ?S? ? S ? NP ? NN ?
S?
S
!!
NP
NN
Trash
. . . . . . . . . . . . .
NN
can
. . . . . . . . . . . . . . . VP
MD
can
. . . . . . . . . . . . . . . . . . VP
VP
MD
can
Figure 2: Left child chains and connection paths. Dotted
lines represent potential attachments
Trash?. Each ?allowable triple? is a tuple ?X,Y, Z?
where X,Y, Z ? V . The triples specify which non-
terminals Z are allowed to follow a non-terminal Y un-
der a parent X . For example, the triple ?S,NP,VP?
specifies that a VP can follow an NP under an S. The
triple ?NP,NN,S?? would specify that the S? symbol can
follow an NN under an NP ? i.e., that the symbol NN is
allowed to be the final child of a rule with parent NP
The initial state of the parser is the input string alone,
wn0 . In absorbing the first word, we add all chains of the
form S? . . . ? w0. For example, in figure 2 the chain
?S? ? S ? NP ? NN ? Trash? is used to construct
an analysis for the first word alone. Other chains which
start with S? and end with Trash would give competing
analyses for the first word of the string.
Figure 2 shows an example of how the next word in
a sentence can be incorporated into a partial analysis for
the previous words. For any partial analysis there will
be a set of potential attachment sites: in the example, the
attachment sites are under the NP or the S. There will
also be a set of possible chains terminating in the next
word ? there are three in the example. Each chain could
potentially be attached at each attachment site, giving
6 ways of incorporating the next word in the example.
For illustration, assume that the set B is {?S,NP,VP?,
?NP,NN,NN?, ?NP,NN,S??, ?S,NP,VP?}. Then some
of the 6 possible attachments may be disallowed because
they create triples that are not in the set B. For example,
in figure 2 attaching either of the VP chains under the
NP is disallowed because the triple ?NP,NN,VP? is not
in B. Similarly, attaching the NN chain under the S will
be disallowed if the triple ?S,NP,NN? is not in B. In
contrast, adjoining ?NN ? can? under the NP creates a
single triple, ?NP,NN,NN?, which is allowed. Adjoining
either of the VP chains under the S creates two triples,
?S,NP,VP? and ?NP,NN,S??, which are both in the set
B.
Note that the ?allowable chains? in our grammar are
what Costa et al (2001) call ?connection paths? from
the partial parse to the next word. It can be shown that
the method is equivalent to parsing with a transformed
context-free grammar (a first-order ?Markov? grammar)
? for brevity we omit the details here.
In this way, given a set of candidatesFi(x) for the first
i words of the string, we can generate a set of candidates
Tree POS f24 f2-21 f2-21, # > 1
transform tags Type Type OOV Type OOV
None Gold 386 1680 0.1% 1013 0.1%
None Tagged 401 1776 0.1% 1043 0.2%
FSLC Gold 289 1214 0.1% 746 0.1%
FSLC Tagged 300 1294 0.1% 781 0.1%
Table 1: Left-child chain type counts (of length > 2) for
sections of the Wall St. Journal Treebank, and out-of-
vocabulary (OOV) rate on the held-out corpus.
for the first i + 1 words, ?h??Fi(x)ADV(h?), where the
ADV function uses the grammar as described above. We
then calculate ?(h) ? ?? for all of these partial hypotheses,
and rank the set from best to worst. A FILTER function is
then applied to this ranked set to giveFi+1. Let hk be the
kth ranked hypothesis in Hi+1(x). Then hk ? Fi+1 if
and only if ?(hk) ? ?? ? ?k. In our case, we parameterize
the calculation of ?k with ? as follows:
?k = ?(h0) ? ???
?
k3
. (3)
The problem with using left-child chains is limiting
them in number. With a left-recursive grammar, of
course, the set of all possible left-child chains is infinite.
We use two techniques to reduce the number of left-child
chains: first, we remove some (but not all) of the recur-
sion from the grammar through a tree transform; next,
we limit the left-child chains consisting of more than
two non-terminal categories to those actually observed
in the training data more than once. Left-child chains of
length less than or equal to two are all those observed
in training data. As a practical matter, the set of left-
child chains for a terminal x is taken to be the union of
the sets of left-child chains for all pre-terminal part-of-
speech (POS) tags T for x.
Before inducing the left-child chains and allowable
triples from the treebank, the trees are transformed with a
selective left-corner transformation (Johnson and Roark,
2000) that has been flattened as presented in Roark
(2001b). This transform is only applied to left-recursive
productions, i.e. productions of the form A ? A?.
The transformed trees look as in figure 3. The transform
has the benefit of dramatically reducing the number of
left-child chains, without unduly disrupting the immedi-
ate dominance relationships that provide features for the
model. The parse trees that are returned by the parser are
then de-transformed to the original form of the grammar
for evaluation2.
Table 1 presents the number of left-child chains of
length greater than 2 in sections 2-21 and 24 of the Penn
Wall St. Journal Treebank, both with and without the
flattened selective left-corner transformation (FSLC), for
gold-standard part-of-speech (POS) tags and automati-
cally tagged POS tags. When the FSLC has been applied
and the set is restricted to those occurring more than once
2See Johnson (1998) for a presentation of the transform/de-
transform paradigm in parsing.
(a)
NP

NP

NP

NNP
Jim
bb
POS
?s
HHH
NN
dog
PPPP
PP
,
IN
with . . .
l
NP
(b)
NP

NNP
Jim
POS
?s
XXXXX
NP/NP

NN
dog
HHH
NP/NP
PP

IN
with . . .
l
NP
(c)
NP
      
NNP
Jim
!!!
POS
?s
l
NP/NP
NN
dog
``````
NP/NP
PP
,
IN
with . . .
l
NP
Figure 3: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner
representation; and (c) a flat structure that is unambiguously equivalent to (b)
F0 = {L00, L10} F4 = F3 ? {L03} F8 = F7 ? {L21} F12 = F11 ? {L11}
F1 = F0 ? {LKP} F5 = F4 ? {L20} F9 = F8 ? {CL} F13 = F12 ? {L30}
F2 = F1 ? {L01} F6 = F5 ? {L11} F10 = F9 ? {LK} F14 = F13 ? {CCP}
F3 = F2 ? {L02} F7 = F6 ? {L30} F11 = F0 ? {L20} F15 = F14 ? {CC}
Table 2: Baseline feature set. Features F0 ? F10 fire at non-terminal nodes. Features F0, F11 ? F15 fire at terminal
nodes.
in the training corpus, we can reduce the total number of
left-child chains of length greater than 2 by half, while
leaving the number of words in the held-out corpus with
an unobserved left-child chain (out-of-vocabulary rate ?
OOV) to just one in every thousand words.
3.2 Features
For this paper, we wanted to compare the results of a
perceptron model with a generative model for a compa-
rable feature set. Unlike in Roark (2001a; 2004), there
is no look-ahead statistic, so we modified the feature set
from those papers to explicitly include the lexical item
and POS tag of the next word. Otherwise the features
are basically the same as in those papers. We then built
a generative model with this feature set and the same
tree transform, for use with the beam-search parser from
Roark (2004) to compare against our baseline perceptron
model.
To concisely present the baseline feature set, let us
establish a notation. Features will fire whenever a new
node is built in the tree. The features are labels from the
left-context, i.e. the already built part of the tree. All
of the labels that we will include in our feature sets are
i levels above the current node in the tree, and j nodes
to the left, which we will denote Lij . Hence, L00 is the
node label itself; L10 is the label of parent of the current
node; L01 is the label of the sibling of the node, imme-
diately to its left; L11 is the label of the sibling of the
parent node, etc. We also include: the lexical head of the
current constituent (CL); the c-commanding lexical head
(CC) and its POS (CCP); and the look-ahead word (LK)
and its POS (LKP). All of these features are discussed at
more length in the citations above. Table 2 presents the
baseline feature set.
In addition to the baseline feature set, we will also
present results using features that would be more dif-
ficult to embed in a generative model. We included
some punctuation-oriented features, which included (i)
a Boolean feature indicating whether the final punctua-
tion is a question mark or not; (ii) the POS label of the
word after the current look-ahead, if the current look-
ahead is punctuation or a coordinating conjunction; and
(iii) a Boolean feature indicating whether the look-ahead
is punctuation or not, that fires when the category imme-
diately to the left of the current position is immediately
preceded by punctuation.
4 Refinements to the Training Algorithm
This section describes two modifications to the ?basic?
training algorithm in figure 1.
4.1 Making Repeated Use of Hypotheses
Figure 4 shows a modified algorithm for parameter es-
timation. The input to the function is a gold standard
parse, together with a set of candidates F generated
by the incremental parser. There are two steps. First,
the model is updated as usual with the current example,
which is then added to a cache of examples. Second, the
method repeatedly iterates over the cache, updating the
model at each cached example if the gold standard parse
is not the best scoring parse from among the stored can-
didates for that example. In our experiments, the cache
was restricted to contain the parses from up to N pre-
viously processed sentences, where N was set to be the
size of the training set.
The motivation for these changes is primarily effi-
ciency. One way to think about the algorithms in this
paper is as methods for finding parameter values that sat-
isfy a set of linear constraints ? one constraint for each
incorrect parse in training data. The incremental parser is
Input: A gold-standard parse = g for sentence k of N . A set of candidate parses F . Current parameters
??. A Cache of triples ?gj ,Fj , cj? for j = 1 . . . N where each gj is a previously generated gold standard
parse, Fj is a previously generated set of candidate parses, and cj is a counter of the number of times that ??
has been updated due to this particular triple. Parameters T1 and T2 controlling the number of iterations be-
low. In our experiments, T1 = 5 and T2 = 50. Initialize the Cache to include, for j = 1 . . . N , ?gj , ?, T2?.
Step 1: Step 2:
Calculate z = arg maxt?F ?(t) ? ?? For t = 1 . . . T1, j = 1 . . . N
If (z 6= g) then ?? = ??+ ?(g)? ?(z) If cj < T2 then
Set the kth triple in the Cache to ?g,F , 0? Calculate z = arg maxt?Fj ?(t) ? ??
If (z 6= gj) then
?? = ??+ ?(gj)? ?(z)
cj = cj + 1
Figure 4: The refined parameter update method makes repeated use of hypotheses
a method for dynamically generating constraints (i.e. in-
correct parses) which are violated, or close to being vio-
lated, under the current parameter settings. The basic al-
gorithm in Figure 1 is extremely wasteful with the gener-
ated constraints, in that it only looks at one constraint on
each sentence (the arg max), and it ignores constraints
implied by previously parsed sentences. This is ineffi-
cient because the generation of constraints (i.e., parsing
an input sentence), is computationally quite demanding.
More formally, it can be shown that the algorithm in
figure 4 also has the upper bound in theorem 1 on the
number of parameter updates performed. If the cost of
steps 1 and 2 of the method are negligible compared to
the cost of parsing a sentence, then the refined algorithm
will certainly converge no more slowly than the basic al-
gorithm, and may well converge more quickly.
As a final note, we used the parameters T1 and T2 to
limit the number of passes over examples, the aim being
to prevent repeated updates based on outlier examples
which are not separable.
4.2 Early Update During Training
As before, define yi to be the gold standard parse for the
i?th sentence, and also define yji to be the partial analy-
sis under the gold-standard parse for the first j words of
the i?th sentence. Then if yji /? Fj(xi) a search error has
been made, and there is no possibility of the gold stan-
dard parse yi being in the final set of parses, Fn(xi). We
call the following modification to the parsing algorithm
during training ?early update?: if yji /? Fj(xi), exit the
parsing process, pass yji , Fj(xi) to the parameter estima-
tion method, and move on to the next string in the train-
ing set. Intuitively, the motivation behind this is clear. It
makes sense to make a correction to the parameter values
at the point that a search error has been made, rather than
allowing the parser to continue to the end of the sentence.
This is likely to lead to less noisy input to the parameter
estimation algorithm; and early update will also improve
efficiency, as at the early stages of training the parser will
frequently give up after a small proportion of each sen-
tence is processed. It is more difficult to justify from a
formal point of view, we leave this to future work.
Figure 5 shows the convergence of the training algo-
rithm with neither of the two refinements presented; with
just early update; and with both. Early update makes
1 2 3 4 5 682
83
84
85
86
87
88
Number of passes over training data
F?m
eas
ure 
pars
ing a
ccur
acy
No early update, no repeated use of examplesEarly update, no repeated use of examplesEarly update, repeated use of examples
Figure 5: Performance on development data (section f24)
after each pass over the training data, with and without
repeated use of examples and early update.
an enormous difference in the quality of the resulting
model; repeated use of examples gives a small improve-
ment, mainly in recall.
5 Empirical results
The parsing models were trained and tested on treebanks
from the Penn Wall St. Journal Treebank: sections 2-21
were kept training data; section 24 was held-out devel-
opment data; and section 23 was for evaluation. After
each pass over the training data, the averaged perceptron
model was scored on the development data, and the best
performing model was used for test evaluation. For this
paper, we used POS tags that were provided either by
the Treebank itself (gold standard tags) or by the per-
ceptron POS tagger3 presented in Collins (2002). The
former gives us an upper bound on the improvement that
we might expect if we integrated the POS tagging with
the parsing.
3For trials when the generative or perceptron parser was given POS
tagger output, the models were trained on POS tagged sections 2-21,
which in both cases helped performance slightly.
Model Gold-standard tags POS-tagger tags
LP LR F LP LR F
Generative 88.1 87.6 87.8 86.8 86.5 86.7
Perceptron (baseline) 87.5 86.9 87.2 86.2 85.5 85.8
Perceptron (w/ punctuation features) 88.1 87.6 87.8 87.0 86.3 86.6
Table 3: Parsing results, section 23, all sentences, including labeled precision (LP), labeled recall (LR), and F-measure
Table 3 shows results on section 23, when either gold-
standard or POS-tagger tags are provided to the parser4.
With the base features, the generative model outperforms
the perceptron parser by between a half and one point,
but with the additional punctuation features, the percep-
tron model matches the generative model performance.
Of course, using the generative model and using the
perceptron algorithm are not necessarily mutually ex-
clusive. Another training scenario would be to include
the generative model score as another feature, with some
weight in the linear model learned by the perceptron al-
gorithm. This sort of scenario was used in Roark et al
(2004) for training an n-gram language model using the
perceptron algorithm. We follow that paper in fixing the
weight of the generative model, rather than learning the
weight along the the weights of the other perceptron fea-
tures. The value of the weight was empirically optimized
on the held-out set by performing trials with several val-
ues. Our optimal value was 10.
In order to train this model, we had to provide gen-
erative model scores for strings in the training set. Of
course, to be similar to the testing conditions, we can-
not use the standard generative model trained on every
sentence, since then the generative score would be from
a model that had already seen that string in the training
data. To control for this, we built ten generative models,
each trained on 90 percent of the training data, and used
each of the ten to score the remaining 10 percent that was
not seen in that training set. For the held-out and testing
conditions, we used the generative model trained on all
of sections 2-21.
In table 4 we present the results of including the gen-
erative model score along with the other perceptron fea-
tures, just for the run with POS-tagger tags. The gen-
erative model score (negative log probability) effectively
provides a much better initial starting point for the per-
ceptron algorithm. The resulting F-measure on section
23 is 2.1 percent higher than either the generative model
or perceptron-trained model used in isolation.
6 Conclusions
In this paper we have presented a discriminative train-
ing approach, based on the perceptron algorithm with
a couple of effective refinements, that provides a model
capable of effective heuristic search over a very difficult
search space. In such an approach, the unnormalized dis-
criminative parsing model can be applied without either
4When POS tagging is integrated directly into the generative pars-
ing process, the baseline performance is 87.0. For comparison with the
perceptron model, results are shown with pre-tagged input.
Model POS-tagger tags
LP LR F
Generative baseline 86.8 86.5 86.7
Perceptron (w/ punctuation features) 87.0 86.3 86.6
Generative + Perceptron (w/ punct) 89.1 88.4 88.8
Table 4: Parsing results, section 23, all sentences, in-
cluding labeled precision (LP), labeled recall (LR), and
F-measure
an external model to present it with candidates, or poten-
tially expensive dynamic programming. When the train-
ing algorithm is provided the generative model scores as
an additional feature, the resulting parser is quite com-
petitive on this task. The improvement that was derived
from the additional punctuation features demonstrates
the flexibility of the approach in incorporating novel fea-
tures in the model.
Future research will look in two directions. First, we
will look to include more useful features that are diffi-
cult for a generative model to include. This paper was
intended to compare search with the generative model
and the perceptron model with roughly similar feature
sets. Much improvement could potentially be had by
looking for other features that could improve the mod-
els. Secondly, combining with the generative model can
be done in several ways. Some of the constraints on the
search technique that were required in the absence of the
generative model can be relaxed if the generative model
score is included as another feature. In the current paper,
the generative score was simply added as another feature.
Another approach might be to use the generative model
to produce candidates at a word, then assign perceptron
features for those candidates. Such variants deserve in-
vestigation.
Overall, these results show much promise in the use of
discriminative learning techniques such as the perceptron
algorithm to help perform heuristic search in difficult do-
mains such as statistical parsing.
Acknowledgements
The work by Michael Collins was supported by the Na-
tional Science Foundation under Grant No. 0347631.
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?617.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures and the voted perceptron. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 263?270.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In The Proceedings of the
17th International Conference on Machine Learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?8.
Michael Collins. 2004. Parameter estimation for sta-
tistical parsing models: Theory and practice of
distribution-free methods. In Harry Bunt, John Car-
roll, and Giorgio Satta, editors, New Developments in
Parsing Technology. Kluwer.
Fabrizio Costa, Vincenzo Lombardo, Paolo Frasconi,
and Giovanni Soda. 2001. Wide coverage incremental
parsing by learning attachment preferences. In Con-
ference of the Italian Association for Artificial Intelli-
gence (AIIA), pages 297?307.
Stephen Della Pietra, Vincent Della Pietra, and John Laf-
ferty. 1997. Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 19:380?393.
Yoav Freund and Robert Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 3(37):277?296.
Yoav Freund, Raj Iyer, Robert Schapire, and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. In Proc. of the 15th Intl. Con-
ference on Machine Learning.
Stuart Geman and Mark Johnson. 2002. Dynamic pro-
gramming for parsing and estimation of stochastic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 279?286.
Mark Johnson and Brian Roark. 2000. Compact non-
left-recursive grammars using the selective left-corner
transform and factoring. In Proceedings of the 18th
International Conference on Computational Linguis-
tics (COLING), pages 355?361.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics, pages 535?541.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):617?636.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282?289.
Adwait Ratnaparkhi, Salim Roukos, and R. Todd Ward.
1994. A maximum entropy model for parsing. In Pro-
ceedings of the International Conference on Spoken
Language Processing (ICSLP), pages 803?806.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34:151?175.
Stefan Riezler, Tracy King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell III, and Mark Johnson.
2002. Parsing the wall street journal using a lexical-
functional grammar and discriminative estimation
techniques. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 271?278.
Brian Roark, Murat Saraclar, and Michael Collins. 2004.
Corrective language modeling for large vocabulary
ASR with the perceptron algorithm. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), pages 749?752.
Brian Roark. 2001a. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Brian Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
Proceedings of the 43rd Annual Meeting of the ACL, pages 507?514,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Discriminative Syntactic Language Modeling for Speech Recognition
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Brian Roark
OGI/OHSU
roark@cslu.ogi.edu
Murat Saraclar
Bogazici University
murat.saraclar@boun.edu.tr
Abstract
We describe a method for discriminative
training of a language model that makes
use of syntactic features. We follow
a reranking approach, where a baseline
recogniser is used to produce 1000-best
output for each acoustic input, and a sec-
ond ?reranking? model is then used to
choose an utterance from these 1000-best
lists. The reranking model makes use of
syntactic features together with a parame-
ter estimation method that is based on the
perceptron algorithm. We describe exper-
iments on the Switchboard speech recog-
nition task. The syntactic features provide
an additional 0.3% reduction in test?set
error rate beyond the model of (Roark et
al., 2004a; Roark et al, 2004b) (signifi-
cant at p < 0.001), which makes use of
a discriminatively trained n-gram model,
giving a total reduction of 1.2% over the
baseline Switchboard system.
1 Introduction
The predominant approach within language model-
ing for speech recognition has been to use an n-
gram language model, within the ?source-channel?
or ?noisy-channel? paradigm. The language model
assigns a probability Pl(w) to each string w in the
language; the acoustic model assigns a conditional
probability Pa(a|w) to each pair (a,w) where a is a
sequence of acoustic vectors, and w is a string. For
a given acoustic input a, the highest scoring string
under the model is
w? = arg max
w
(? log Pl(w) + log Pa(a|w)) (1)
where ? > 0 is some value that reflects the rela-
tive importance of the language model; ? is typi-
cally chosen by optimization on held-out data. In
an n-gram language model, a Markov assumption
is made, namely that each word depends only on
the previous (n ? 1) words. The parameters of the
language model are usually estimated from a large
quantity of text data. See (Chen and Goodman,
1998) for an overview of estimation techniques for
n-gram models.
This paper describes a method for incorporating
syntactic features into the language model, using
discriminative parameter estimation techniques. We
build on the work in Roark et al (2004a; 2004b),
which was summarized and extended in Roark et al
(2005). These papers used discriminative methods
for n-gram language models. Our approach reranks
the 1000-best output from the Switchboard recog-
nizer of Ljolje et al (2003).1 Each candidate string
w is parsed using the statistical parser of Collins
(1999) to give a parse tree T (w). Information from
the parse tree is incorporated in the model using
a feature-vector approach: we define ?(a,w) to
be a d-dimensional feature vector which in princi-
ple could track arbitrary features of the string w
together with the acoustic input a. In this paper
we restrict ?(a,w) to only consider the string w
and/or the parse tree T (w) for w. For example,
?(a,w) might track counts of context-free rule pro-
ductions in T (w), or bigram lexical dependencies
within T (w). The optimal string under our new
model is defined as
w? = arg max
w
(? log Pl(w) + ???, ?(a,w)?+
log Pa(a|w)) (2)
where the arg max is taken over all strings in the
1000-best list, and where ?? ? Rd is a parameter
vector specifying the ?weight? for each feature in
? (note that we define ?x, y? to be the inner, or dot
1Note that (Roark et al, 2004a; Roark et al, 2004b) give
results for an n-gram approach on this data which makes use of
both lattices and 1000-best lists. The results on 1000-best lists
were very close to results on lattices for this domain, suggesting
that the 1000-best approximation is a reasonable one.
507
product, between vectors x and y). For this paper,
we train the parameter vector ?? using the perceptron
algorithm (Collins, 2004; Collins, 2002). The per-
ceptron algorithm is a very fast training method, in
practice requiring only a few passes over the train-
ing set, allowing for a detailed comparison of a wide
variety of feature sets.
A number of researchers have described work
that incorporates syntactic language models into a
speech recognizer. These methods have almost ex-
clusively worked within the noisy channel paradigm,
where the syntactic language model has the task
of modeling a distribution over strings in the lan-
guage, in a very similar way to traditional n-gram
language models. The Structured Language Model
(Chelba and Jelinek, 1998; Chelba and Jelinek,
2000; Chelba, 2000; Xu et al, 2002; Xu et al, 2003)
makes use of an incremental shift-reduce parser to
enable the probability of words to be conditioned on
k previous c-commanding lexical heads, rather than
simply on the previous k words. Incremental top-
down and left-corner parsing (Roark, 2001a; Roark,
2001b) and head-driven parsing (Charniak, 2001)
approaches have directly used generative PCFG
models as language models. In the work of Wen
Wang and Mary Harper (Wang and Harper, 2002;
Wang, 2003; Wang et al, 2004), a constraint depen-
dency grammar and a finite-state tagging model de-
rived from that grammar were used to exploit syn-
tactic dependencies.
Our approach differs from previous work in a cou-
ple of important respects. First, through the feature-
vector representations ?(a,w) we can essentially
incorporate arbitrary sources of information from
the string or parse tree into the model. We would ar-
gue that our method allows considerably more flexi-
bility in terms of the choice of features in the model;
in previous work features were incorporated in the
model through modification of the underlying gen-
erative parsing or tagging model, and modifying a
generative model is a rather indirect way of chang-
ing the features used by a model. In this respect, our
approach is similar to that advocated in Rosenfeld et
al. (2001), which used Maximum Entropy modeling
to allow for the use of shallow syntactic features for
language modeling.
A second contrast between our work and previ-
ous work, including that of Rosenfeld et al (2001),
is in the use of discriminative parameter estimation
techniques. The criterion we use to optimize the pa-
rameter vector ?? is closely related to the end goal
in speech recognition, i.e., word error rate. Previ-
ous work (Roark et al, 2004a; Roark et al, 2004b)
has shown that discriminative methods within an n-
gram approach can lead to significant reductions in
WER, in spite of the features being of the same type
as the original language model. In this paper we ex-
tend this approach, by including syntactic features
that were not in the baseline speech recognizer.
This paper describe experiments using a variety
of syntactic features within this approach. We tested
the model on the Switchboard (SWB) domain, using
the recognizer of Ljolje et al (2003). The discrim-
inative approach for n-gram modeling gave a 0.9%
reduction in WER on this domain; the syntactic fea-
tures we describe give a further 0.3% reduction.
In the remainder of this paper, section 2 describes
previous work, including the parameter estimation
methods we use, and section 3 describes the feature-
vector representations of parse trees that we used in
our experiments. Section 4 describes experiments
using the approach.
2 Background
2.1 Previous Work
Techniques for exploiting stochastic context-free
grammars for language modeling have been ex-
plored for more than a decade. Early approaches
included algorithms for efficiently calculating string
prefix probabilities (Jelinek and Lafferty, 1991; Stol-
cke, 1995) and approaches to exploit such algo-
rithms to produce n-gram models (Stolcke and Se-
gal, 1994; Jurafsky et al, 1995). The work of Chelba
and Jelinek (Chelba and Jelinek, 1998; Chelba and
Jelinek, 2000; Chelba, 2000) involved the use of a
shift-reduce parser trained on Penn treebank style
annotations, that maintains a weighted set of parses
as it traverses the string from left-to-right. Each
word is predicted by each candidate parse in this set
at the point when the word is shifted, and the con-
ditional probability of the word given the previous
words is taken as the weighted sum of the condi-
tional probabilities provided by each parse. In this
approach, the probability of a word is conditioned
by the top two lexical heads on the stack of the par-
508
ticular parse. Enhancements in the feature set and
improved parameter estimation techniques have ex-
tended this approach in recent years (Xu et al, 2002;
Xu et al, 2003).
Roark (2001a; 2001b) pursued a different deriva-
tion strategy from Chelba and Jelinek, and used the
parse probabilities directly to calculate the string
probabilities. This work made use of a left-to-right,
top-down, beam-search parser, which exploits rich
lexico-syntactic features from the left context of
each derivation to condition derivation move proba-
bilities, leading to a very peaked distribution. Rather
than normalizing a prediction of the next word over
the beam of candidates, as in Chelba and Jelinek,
in this approach the string probability is derived by
simply summing the probabilities of all derivations
for that string in the beam.
Other work on syntactic language modeling in-
cludes that of Charniak (2001), which made use of
a non-incremental, head-driven statistical parser to
produce string probabilities. In the work of Wen
Wang and Mary Harper (Wang and Harper, 2002;
Wang, 2003; Wang et al, 2004), a constraint depen-
dency grammar and a finite-state tagging model de-
rived from that grammar, were used to exploit syn-
tactic dependencies. The processing advantages of
the finite-state encoding of the model has allowed
for the use of probabilities calculated off-line from
this model to be used in the first pass of decoding,
which has provided additional benefits. Finally, Och
et al (2004) use a reranking approach with syntactic
information within a machine translation system.
Rosenfeld et al (2001) investigated the use of
syntactic features in a Maximum Entropy approach.
In their paper, they used a shallow parser to anno-
tate base constituents, and derived features from se-
quences of base constituents. The features were in-
dicator features that were either (1) exact matches
between a set or sequence of base constituents with
those annotated on the hypothesis transcription; or
(2) tri-tag features from the constituent sequence.
The generative model that resulted from their fea-
ture set resulted in only a very small improvement
in either perplexity or word-error-rate.
2.2 Global Linear Models
We follow the framework of Collins (2002; 2004),
recently applied to language modeling in Roark et
al. (2004a; 2004b). The model we propose consists
of the following components:
? GEN(a) is a set of candidate strings for an
acoustic input a. In our case, GEN(a) is a set of
1000-best strings from a first-pass recognizer.
? T (w) is the parse tree for string w.
? ?(a,w) ? Rd is a feature-vector representation
of an acoustic input a together with a string w.
? ?? ? Rd is a parameter vector.
? The output of the recognizer for an input a is
defined as
F (a) = argmax
w?GEN(a)
??(a,w), ??? (3)
In principle, the feature vector ?(a,w) could take
into account any features of the acoustic input a to-
gether with the utterance w. In this paper we make
a couple of restrictions. First, we define the first fea-
ture to be
?1(a,w) = ? log Pl(w) + log Pa(a|w)
where Pl(w) and Pa(a|w) are language and acous-
tic model scores from the baseline speech recog-
nizer. In our experiments we kept ? fixed at the
value used in the baseline recogniser. It can then
be seen that our model is equivalent to the model
in Eq. 2. Second, we restrict the remaining features
?2(a,w) . . . ?d(a,w) to be sensitive to the string
w alone.2 In this sense, the scope of this paper is
limited to the language modeling problem. As one
example, the language modeling features might take
into account n-grams, for example through defini-
tions such as
?2(a,w) = Count of the the in w
Previous work (Roark et al, 2004a; Roark et al,
2004b) considered features of this type. In this pa-
per, we introduce syntactic features, which may be
sensitive to the parse tree for w, for example
?3(a,w) = Count of S ? NP VP in T (w)
where S ? NP VP is a context-free rule produc-
tion. Section 3 describes the full set of features used
in the empirical results presented in this paper.
2Future work may consider features of the acoustic sequence
a together with the string w, allowing the approach to be ap-
plied to acoustic modeling.
509
2.2.1 Parameter Estimation
We now describe how the parameter vector ?? is
estimated from a set of training utterances. The
training set consists of examples (ai,wi) for i =
1 . . .m, where ai is the i?th acoustic input, and wi
is the transcription of this input. We briefly review
the two training algorithms described in Roark et al
(2004b), the perceptron algorithm and global condi-
tional log-linear models (GCLMs).
Figure 1 shows the perceptron algorithm. It is an
online algorithm, which makes several passes over
the training set, updating the parameter vector after
each training example. For a full description of the
algorithm, see Collins (2004; 2002).
A second parameter estimation method, which
was used in (Roark et al, 2004b), is to optimize
the log-likelihood under a log-linear model. Sim-
ilar approaches have been described in Johnson et
al. (1999) and Lafferty et al (2001). The objective
function used in optimizing the parameters is
L(??) =
?
i
log P (si|ai, ??) ? C
?
j
?2j (4)
where P (si|ai, ??) = e
??(ai,si),???
?
w?GEN(ai) e
??(ai,w),??? .
Here, each si is the member of GEN(ai) which
has lowest WER with respect to the target transcrip-
tion wi. The first term in L(??) is the log-likelihood
of the training data under a conditional log-linear
model. The second term is a regularization term
which penalizes large parameter values. C is a con-
stant that dictates the relative weighting given to the
two terms. The optimal parameters are defined as
??? = arg max
??
L(??)
We refer to these models as global conditional log-
linear models (GCLMs).
Each of these algorithms has advantages. A num-
ber of results?e.g., in Sha and Pereira (2003) and
Roark et al (2004b)?suggest that the GCLM ap-
proach leads to slightly higher accuracy than the per-
ceptron training method. However the perceptron
converges very quickly, often in just a few passes
over the training set?in comparison GCLM?s can
take tens or hundreds of gradient calculations before
convergence. In addition, the perceptron can be used
as an effective feature selection technique, in that
Input: A parameter specifying the number of iterations over
the training set, T . A value for the first parameter, ?. A
feature-vector representation ?(a,w) ? Rd. Training exam-
ples (ai,wi) for i = 1 . . . m. An n-best list GEN(ai) for each
training utterance. We take si to be the member of GEN(ai)
which has the lowest WER when compared to wi.
Initialization: Set ?1 = ?, and ?j = 0 for j =
2 . . . d.
Algorithm: For t = 1 . . . T, i = 1 . . . m
?Calculate yi = arg maxw?GEN(ai) ??(ai,w), ???
? For j = 2 . . .m, set ??j = ??j + ?j(ai, si) ?
?j(ai,yi)
Output: Either the final parameters ??, or the averaged pa-
rameters ??avg defined as ??avg =
?
t,i ??t,i/mT where ??t,i is
the parameter vector after training on the i?th training example
on the t?th pass through the training data.
Figure 1: The perceptron training algorithm. Following
Roark et al (2004a), the parameter ?1 is set to be some con-
stant ? that is typically chosen through optimization over the
development set. Recall that ?1 dictates the weight given to the
baseline recognizer score.
at each training example it only increments features
seen on si or yi, effectively ignoring all other fea-
tures seen on members of GEN(ai). For example,
in the experiments in Roark et al (2004a), the per-
ceptron converged in around 3 passes over the train-
ing set, while picking non-zero values for around 1.4
million n-gram features out of a possible 41 million
n-gram features seen in the training set.
For the present paper, to get a sense of the relative
effectiveness of various kinds of syntactic features
that can be derived from the output of a parser, we
are reporting results using just the perceptron algo-
rithm. This has allowed us to explore more of the po-
tential feature space than we would have been able
to do using the more costly GCLM estimation tech-
niques. In future we plan to apply GLCM parameter
estimation methods to the task.
3 Parse Tree Features
We tagged each candidate transcription with (1)
part-of-speech tags, using the tagger documented in
Collins (2002); and (2) a full parse tree, using the
parser documented in Collins (1999). The models
for both of these were trained on the Switchboard
510
SNP
PRP
we
VP
VBD
helped
NP
PRP
her
VP
VB
paint
NP
DT
the
NN
house
Figure 2: An example parse tree
treebank, and applied to candidate transcriptions in
both the training and test sets. Each transcription
received one POS-tag annotation and one parse tree
annotation, from which features were extracted.
Figure 2 shows a Penn Treebank style parse tree
that is of the sort produced by the parser. Given such
a structure, there is a tremendous amount of flexibil-
ity in selecting features. The first approach that we
follow is to map each parse tree to sequences encod-
ing part-of-speech (POS) decisions, and ?shallow?
parsing decisions. Similar representations have been
used by (Rosenfeld et al, 2001; Wang and Harper,
2002). Figure 3 shows the sequential representations
that we used. The first simply makes use of the POS
tags for each word. The latter representations make
use of sequences of non-terminals associated with
lexical items. In 3(b), each word in the string is asso-
ciated with the beginning or continuation of a shal-
low phrase or ?chunk? in the tree. We include any
non-terminals above the level of POS tags as poten-
tial chunks: a new ?chunk? (VP, NP, PP etc.) begins
whenever we see the initial word of the phrase dom-
inated by the non-terminal. In 3(c), we show how
POS tags can be added to these sequences. The final
type of sequence mapping, shown in 3(d), makes a
similar use of chunks, but preserves only the head-
word seen with each chunk.3
From these sequences of categories, various fea-
tures can be extracted, to go along with the n-gram
features used in the baseline. These include n-tag
features, e.g. ti?2ti?1ti (where ti represents the
3It should be noted that for a very small percentage of hy-
potheses, the parser failed to return a full parse tree. At the
end of every shallow tag or category sequence, a special end of
sequence tag/word pair ?</parse> </parse>? was emit-
ted. In contrast, when a parse failed, the sequence consisted of
solely ?<noparse> <noparse>?.
(a)
we/PRP helped/VBD her/PRP paint/VB the/DT
house/NN
(b)
we/NPb helped/VPb her/NPb paint/VPb the/NPb
house/NPc
(c)
we/PRP-NPb helped/VBD-VPb her/PRP-NPb
paint/VB-VPb the/DT-NPb house/NN-NPc
(d)
we/NP helped/VP her/NP paint/VP house/NP
Figure 3: Sequences derived from a parse tree: (a) POS-tag
sequence; (b) Shallow parse tag sequence?the superscripts b
and c refer to the beginning and continuation of a phrase re-
spectively; (c) Shallow parse tag plus POS tag sequence; and
(d) Shallow category with lexical head sequence
tag in position i); and composite tag/word features,
e.g. tiwi (where wi represents the word in posi-
tion i) or, more complicated configurations, such as
ti?2ti?1wi?1tiwi. These features can be extracted
from whatever sort of tag/word sequence we pro-
vide for feature extraction, e.g. POS-tag sequences
or shallow parse tag sequences.
One variant that we performed in feature extrac-
tion had to do with how speech repairs (identified as
EDITED constituents in the Switchboard style parse
trees) and filled pauses or interjections (labeled with
the INTJ label) were dealt with. In the simplest ver-
sion, these are simply treated like other constituents
in the parse tree. However, these can disrupt what
may be termed the intended sequence of syntactic
categories in the utterance, so we also tried skipping
these constituents when mapping from the parse tree
to shallow parse sequences.
The second set of features we employed made
use of the full parse tree when extracting features.
For this paper, we examined several features tem-
plates of this type. First, we considered context-free
rule instances, extracted from each local node in the
tree. Second, we considered features based on lex-
ical heads within the tree. Let us first distinguish
between POS-tags and non-POS non-terminal cate-
gories by calling these latter constituents NTs. For
each constituent NT in the tree, there is an associ-
ated lexical head (HNT) and the POS-tag of that lex-
ical head (HPNT). Two simple features are NT/HNT
and NT/HPNT for every NT constituent in the tree.
511
Feature Examples from figure 2
(P,HCP,Ci,{+,-}{1,2},HP,HCi ) (VP,VB,NP,1,paint,house)
(S,VP,NP,-1,helped,we)
(P,HCP,Ci,{+,-}{1,2},HP,HPCi ) (VP,VB,NP,1,paint,NN)
(S,VP,NP,-1,helped,PRP)
(P,HCP,Ci,{+,-}{1,2},HPP,HCi ) (VP,VB,NP,1,VB,house)
(S,VP,NP,-1,VBD,we)
(P,HCP,Ci,{+,-}{1,2},HPP,HPCi ) (VP,VB,NP,1,VB,NN)
(S,VP,NP,-1,VBD,PRP)
Table 1: Examples of head-to-head features. The examples
are derived from the tree in figure 2.
Using the heads as identified in the parser, example
features from the tree in figure 2 would be S/VBD,
S/helped, NP/NN, and NP/house.
Beyond these constituent/head features, we can
look at the head-to-head dependencies of the sort
used by the parser. Consider each local tree, con-
sisting of a parent node (P), a head child (HCP), and
k non-head children (C1 . . . Ck). For each non-head
child Ci, it is either to the left or right of HCP, and is
either adjacent or non-adjacent to HCP. We denote
these positional features as an integer, positive if to
the right, negative if to the left, 1 if adjacent, and 2 if
non-adjacent. Table 1 shows four head-to-head fea-
tures that can be extracted for each non-head child
Ci. These features include dependencies between
pairs of lexical items, between a single lexical item
and the part-of-speech of another item, and between
pairs of part-of-speech tags in the parse.
4 Experiments
The experimental set-up we use is very similar to
that of Roark et al (2004a; 2004b), and the exten-
sions to that work in Roark et al (2005). We make
use of the Rich Transcription 2002 evaluation test
set (rt02) as our development set, and use the Rich
Transcription 2003 Spring evaluation CTS test set
(rt03) as test set. The rt02 set consists of 6081 sen-
tences (63804 words) and has three subsets: Switch-
board 1, Switchboard 2, Switchboard Cellular. The
rt03 set consists of 9050 sentences (76083 words)
and has two subsets: Switchboard and Fisher.
The training set consists of 297580 transcribed
utterances (3297579 words)4. For each utterance,
4Note that Roark et al (2004a; 2004b; 2005) used 20854 of
these utterances (249774 words) as held out data. In this work
we simply use the rt02 test set as held out and development data.
a weighted word-lattice was produced, represent-
ing alternative transcriptions, from the ASR system.
The baseline ASR system that we are comparing
against then performed a rescoring pass on these first
pass lattices, allowing for better silence modeling,
and replaces the trigram language model score with
a 6-gram model. 1000-best lists were then extracted
from these lattices. For each candidate in the 1000-
best lists, we identified the number of edits (inser-
tions, deletions or substitutions) for that candidate,
relative to the ?target? transcribed utterance. The or-
acle score for the 1000-best lists was 16.7%.
To produce the word-lattices, each training utter-
ance was processed by the baseline ASR system. In
a naive approach, we would simply train the base-
line system (i.e., an acoustic model and language
model) on the entire training set, and then decode
the training utterances with this system to produce
lattices. We would then use these lattices with the
perceptron algorithm. Unfortunately, this approach
is likely to produce a set of training lattices that are
very different from test lattices, in that they will have
very low word-error rates, given that the lattice for
each utterance was produced by a model that was
trained on that utterance. To somewhat control for
this, the training set was partitioned into 28 sets, and
baseline Katz backoff trigram models were built for
each set by including only transcripts from the other
27 sets. Lattices for each utterance were produced
with an acoustic model that had been trained on the
entire training set, but with a language model that
was trained on the 27 data portions that did not in-
clude the current utterance. Since language mod-
els are generally far more prone to overtraining than
standard acoustic models, this goes a long way to-
ward making the training conditions similar to test-
ing conditions. Similar procedures were used to
train the parsing and tagging models for the training
set, since the Switchboard treebank overlaps exten-
sively with the ASR training utterances.
Table 2 presents the word-error rates on rt02 and
rt03 of the baseline ASR system, 1000-best percep-
tron and GCLM results from Roark et al (2005)
under this condition, and our 1000-best perceptron
results. Note that our n-best result, using just n-
gram features, improves upon the perceptron result
of (Roark et al, 2005) by 0.2 percent, putting us
within 0.1 percent of their GCLM result for that
512
WER
Trial rt02 rt03
ASR system output 37.1 36.4
Roark et al (2005) perceptron 36.6 35.7
Roark et al (2005) GCLM 36.3 35.4
n-gram perceptron 36.4 35.5
Table 2: Baseline word-error rates versus Roark et al (2005)
rt02
Trial WER
ASR system output 37.1
n-gram perceptron 36.4
n-gram + POS (1) perceptron 36.1
n-gram + POS (1,2) perceptron 36.1
n-gram + POS (1,3) perceptron 36.1
Table 3: Use of POS-tag sequence derived features
condition. (Note that the perceptron?trained n-gram
features were trigrams (i.e., n = 3).) This is due to
a larger training set being used in our experiments;
we have added data that was used as held-out data in
(Roark et al, 2005) to the training set that we use.
The first additional features that we experimented
with were POS-tag sequence derived features. Let
ti and wi be the POS tag and word at position i,
respectively. We experimented with the following
three feature definitions:
1. (ti?2ti?1ti), (ti?1ti), (ti), (tiwi)
2. (ti?2ti?1wi)
3. (ti?2wi?2ti?1wi?1tiwi), (ti?2ti?1wi?1tiwi),
(ti?1wi?1tiwi), (ti?1tiwi)
Table 3 summarizes the results of these trials on
the held out set. Using the simple features (num-
ber 1 above) yielded an improvement beyond just
n-grams, but additional, more complicated features
failed to yield additional improvements.
Next, we considered features derived from shal-
low parsing sequences. Given the results from the
POS-tag sequence derived features, for any given se-
quence, we simply use n-tag and tag/word features
(number 1 above). The first sequence type from
which we extracted features was the shallow parse
tag sequence (S1), as shown in figure 3(b). Next,
we tried the composite shallow/POS tag sequence
(S2), as in figure 3(c). Finally, we tried extract-
ing features from the shallow constituent sequence
(S3), as shown in figure 3(d). When EDITED and
rt02
Trial WER
ASR system output 37.1
n-gram perceptron 36.4
n-gram + POS perceptron 36.1
n-gram + POS + S1 perceptron 36.1
n-gram + POS + S2 perceptron 36.0
n-gram + POS + S3 perceptron 36.0
n-gram + POS + S3-E perceptron 36.0
n-gram + POS + CF perceptron 36.1
n-gram + POS + H2H perceptron 36.0
Table 4: Use of shallow parse sequence and full parse derived
features
INTJ nodes are ignored, we refer to this condition
as S3-E. For full-parse feature extraction, we tried
context-free rule features (CF) and head-to-head fea-
tures (H2H), of the kind shown in table 1. Table 4
shows the results of these trials on rt02.
Although the single digit precision in the table
does not show it, the H2H trial, using features ex-
tracted from the full parses along with n-grams and
POS-tag sequence features, was the best performing
model on the held out data, so we selected it for ap-
plication to the rt03 test data. This yielded 35.2%
WER, a reduction of 0.3% absolute over what was
achieved with just n-grams, which is significant at
p < 0.001,5 reaching a total reduction of 1.2% over
the baseline recognizer.
5 Conclusion
The results presented in this paper are a first step in
examining the potential utility of syntactic features
for discriminative language modeling for speech
recognition. We tried two possible sets of features
derived from the full annotation, as well as a va-
riety of possible feature sets derived from shallow
parse and POS tag sequences, the best of which
gave a small but significant improvement beyond
what was provided by the n-gram features. Future
work will include a further investigation of parser?
derived features. In addition, we plan to explore the
alternative parameter estimation methods described
in (Roark et al, 2004a; Roark et al, 2004b), which
were shown in this previous work to give further im-
provements over the perceptron.
5We use the Matched Pair Sentence Segment test for WER,
a standard measure of significance, to calculate this p-value.
513
References
Eugene Charniak. 2001. Immediate-head parsing for language
models. In Proc. ACL.
Ciprian Chelba and Frederick Jelinek. 1998. Exploiting syntac-
tic structure for language modeling. In Proceedings of the
36th Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Computa-
tional Linguistics, pages 225?231.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for Nat-
ural Language Modeling. Ph.D. thesis, The Johns Hopkins
University.
Stanley Chen and Joshua Goodman. 1998. An empirical study
of smoothing techniques for language modeling. Technical
Report, TR-10-98, Harvard University.
Michael J. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proc. EMNLP, pages 1?8.
Michael Collins. 2004. Parameter estimation for statistical
parsing models: Theory and practice of distribution-free
methods. In Harry Bunt, John Carroll, and Giorgio Satta,
editors, New Developments in Parsing Technology. Kluwer
Academic Publishers, Dordrecht.
Frederick Jelinek and John Lafferty. 1991. Computation of
the probability of initial substring generation by stochas-
tic context-free grammars. Computational Linguistics,
17(3):315?323.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi Chi, and
Stefan Riezler. 1999. Estimators for stochastic ?unification-
based? grammars. In Proc. ACL, pages 535?541.
Daniel Jurafsky, Chuck Wooters, Jonathan Segal, Andreas
Stolcke, Eric Fosler, Gary Tajchman, and Nelson Morgan.
1995. Using a stochastic context-free grammar as a lan-
guage model for speech recognition. In Proceedings of the
IEEE Conference on Acoustics, Speech, and Signal Process-
ing, pages 189?192.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML, pages
282?289, Williams College, Williamstown, MA, USA.
Andrej Ljolje, Enrico Bocchieri, Michael Riley, Brian Roark,
Murat Saraclar, and Izhak Shafran. 2003. The AT&T 1xRT
CTS system. In Rich Transcription Workshop.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and
Dragomir Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In Proceedings of HLT-NAACL
2004.
Brian Roark, Murat Saraclar, and Michael Collins. 2004a. Cor-
rective language modeling for large vocabulary ASR with the
perceptron algorithm. In Proc. ICASSP, pages 749?752.
Brian Roark, Murat Saraclar, Michael Collins, and Mark John-
son. 2004b. Discriminative language modeling with condi-
tional random fields and the perceptron algorithm. In Proc.
ACL.
Brian Roark, Murat Saraclar, and Michael Collins. 2005. Dis-
criminative n-gram language modeling. Computer Speech
and Language. submitted.
Brian Roark. 2001a. Probabilistic top-down parsing and lan-
guage modeling. Computational Linguistics, 27(2):249?
276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. 2001.
Whole-sentence exponential language models: a vehicle for
linguistic-statistical integration. In Computer Speech and
Language.
Fei Sha and Fernando Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of the Human
Language Technology Conference and Meeting of the North
American Chapter of the Association for Computational Lin-
guistics (HLT-NAACL), Edmonton, Canada.
Andreas Stolcke and Jonathan Segal. 1994. Precise n-gram
probabilities from stochastic context-free grammars. In Pro-
ceedings of the 32nd Annual Meeting of the Association for
Computational Linguistics, pages 74?79.
Andreas Stolcke. 1995. An efficient probabilistic context-free
parsing algorithm that computes prefix probabilities. Com-
putational Linguistics, 21(2):165?202.
Wen Wang and Mary P. Harper. 2002. The superARV language
model: Investigating the effectiveness of tightly integrating
multiple knowledge sources. In Proc. EMNLP, pages 238?
247.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004. The
use of a linguistically motivated language model in conver-
sational speech recognition. In Proc. ICASSP.
Wen Wang. 2003. Statistical parsing and language model-
ing based on constraint dependency grammar. Ph.D. thesis,
Purdue University.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A
study on richer syntactic dependencies for structured lan-
guage modeling. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics, pages
191?198.
Peng Xu, Ahmad Emami, and Frederick Jelinek. 2003. Train-
ing connectionist models for the structured language model.
In Proc. EMNLP, pages 160?167.
514
Proceedings of the 43rd Annual Meeting of the ACL, pages 531?540,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Clause Restructuring for Statistical Machine Translation
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Philipp Koehn
School of Informatics
University of Edinburgh
pkoehn@inf.ed.ac.uk
Ivona Kuc?erova?
MIT Linguistics Department
kucerova@mit.edu
Abstract
We describe a method for incorporating syntactic informa-
tion in statistical machine translation systems. The first step
of the method is to parse the source language string that is be-
ing translated. The second step is to apply a series of trans-
formations to the parse tree, effectively reordering the surface
string on the source language side of the translation system. The
goal of this step is to recover an underlying word order that is
closer to the target language word-order than the original string.
The reordering approach is applied as a pre-processing step in
both the training and decoding phases of a phrase-based statis-
tical MT system. We describe experiments on translation from
German to English, showing an improvement from 25.2% Bleu
score for a baseline system to 26.8% Bleu score for the system
with reordering, a statistically significant improvement.
1 Introduction
Recent research on statistical machine translation
(SMT) has lead to the development of phrase-
based systems (Och et al, 1999; Marcu and Wong,
2002; Koehn et al, 2003). These methods go be-
yond the original IBM machine translation models
(Brown et al, 1993), by allowing multi-word units
(?phrases?) in one language to be translated directly
into phrases in another language. A number of em-
pirical evaluations have suggested that phrase-based
systems currently represent the state?of?the?art in
statistical machine translation.
In spite of their success, a key limitation of
phrase-based systems is that they make little or no
direct use of syntactic information. It appears likely
that syntactic information will be crucial in accu-
rately modeling many phenomena during transla-
tion, for example systematic differences between the
word order of different languages. For this reason
there is currently a great deal of interest in meth-
ods which incorporate syntactic information within
statistical machine translation systems (e.g., see (Al-
shawi, 1996; Wu, 1997; Yamada and Knight, 2001;
Gildea, 2003; Melamed, 2004; Graehl and Knight,
2004; Och et al, 2004; Xia and McCord, 2004)).
In this paper we describe an approach for the use
of syntactic information within phrase-based SMT
systems. The approach constitutes a simple, direct
method for the incorporation of syntactic informa-
tion in a phrase?based system, which we will show
leads to significant improvements in translation ac-
curacy. The first step of the method is to parse the
source language string that is being translated. The
second step is to apply a series of transformations
to the resulting parse tree, effectively reordering the
surface string on the source language side of the
translation system. The goal of this step is to re-
cover an underlying word order that is closer to the
target language word-order than the original string.
Finally, we apply a phrase-based system to the re-
ordered string to give a translation into the target
language.
We describe experiments involving machine
translation from German to English. As an illustra-
tive example of our method, consider the following
German sentence, together with a ?translation? into
English that follows the original word order:
Original sentence: Ich werde Ihnen die entsprechenden An-
merkungen aushaendigen, damit Sie das eventuell bei der
Abstimmung uebernehmen koennen.
English translation: I will to you the corresponding comments
pass on, so that you them perhaps in the vote adopt can.
The German word order in this case is substan-
tially different from the word order that would be
seen in English. As we will show later in this pa-
per, translations of sentences of this type pose dif-
ficulties for phrase-based systems. In our approach
we reorder the constituents in a parse of the German
sentence to give the following word order, which is
much closer to the target English word order (words
which have been ?moved? are underlined):
Reordered sentence: Ich werde aushaendigen Ihnen die
entsprechenden Anmerkungen, damit Sie koennen
uebernehmen das eventuell bei der Abstimmung.
English translation: I will pass on to you the corresponding
comments, so that you can adopt them perhaps in the vote.
531
We applied our approach to translation from Ger-
man to English in the Europarl corpus. Source lan-
guage sentences are reordered in test data, and also
in training data that is used by the underlying phrase-
based system. Results using the method show an
improvement from 25.2% Bleu score to 26.8% Bleu
score (a statistically significant improvement), using
a phrase-based system (Koehn et al, 2003) which
has been shown in the past to be a highly competi-
tive SMT system.
2 Background
2.1 Previous Work
2.1.1 Research on Phrase-Based SMT
The original work on statistical machine transla-
tion was carried out by researchers at IBM (Brown
et al, 1993). More recently, phrase-based models
(Och et al, 1999; Marcu and Wong, 2002; Koehn
et al, 2003) have been proposed as a highly suc-
cessful alternative to the IBM models. Phrase-based
models generalize the original IBM models by al-
lowing multiple words in one language to corre-
spond to multiple words in another language. For
example, we might have a translation entry specify-
ing that I will in English is a likely translation for Ich
werde in German.
In this paper we use the phrase-based system
of (Koehn et al, 2003) as our underlying model.
This approach first uses the original IBM models
to derive word-to-word alignments in the corpus
of example translations. Heuristics are then used
to grow these alignments to encompass phrase-to-
phrase pairs. The end result of the training process is
a lexicon of phrase-to-phrase pairs, with associated
costs or probabilities. In translation with the sys-
tem, a beam search method with left-to-right search
is used to find a high scoring translation for an in-
put sentence. At each stage of the search, one or
more English words are added to the hypothesized
string, and one or more consecutive German words
are ?absorbed? (i.e., marked as having already been
translated?note that each word is absorbed at most
once). Each step of this kind has a number of costs:
for example, the log probability of the phrase-to-
phrase correspondance involved, the log probability
from a language model, and some ?distortion? score
indicating how likely it is for the proposed words in
the English string to be aligned to the corresponding
position in the German string.
2.1.2 Research on Syntax-Based SMT
A number of researchers (Alshawi, 1996; Wu,
1997; Yamada and Knight, 2001; Gildea, 2003;
Melamed, 2004; Graehl and Knight, 2004; Galley
et al, 2004) have proposed models where the trans-
lation process involves syntactic representations of
the source and/or target languages. One class of ap-
proaches make use of ?bitext? grammars which si-
multaneously parse both the source and target lan-
guages. Another class of approaches make use of
syntactic information in the target language alone,
effectively transforming the translation problem into
a parsing problem. Note that these models have radi-
cally different structures and parameterizations from
phrase?based models for SMT. As yet, these sys-
tems have not shown significant gains in accuracy
in comparison to phrase-based systems.
Reranking methods have also been proposed as a
method for using syntactic information (Koehn and
Knight, 2003; Och et al, 2004; Shen et al, 2004). In
these approaches a baseline system is used to gener-
ate
 
-best output. Syntactic features are then used
in a second model that reranks the   -best lists, in
an attempt to improve over the baseline approach.
(Koehn and Knight, 2003) apply a reranking ap-
proach to the sub-task of noun-phrase translation.
(Och et al, 2004; Shen et al, 2004) describe the
use of syntactic features in reranking the output of
a full translation system, but the syntactic features
give very small gains: for example the majority of
the gain in performance in the experiments in (Och
et al, 2004) was due to the addition of IBM Model
1 translation probabilities, a non-syntactic feature.
An alternative use of syntactic information is to
employ an existing statistical parsing model as a lan-
guage model within an SMT system. See (Charniak
et al, 2003) for an approach of this form, which
shows improvements in accuracy over a baseline
system.
2.1.3 Research on Preprocessing Approaches
Our approach involves a preprocessing step,
where sentences in the language being translated are
modified before being passed to an existing phrase-
based translation system. A number of other re-
532
searchers (Berger et al, 1996; Niessen and Ney,
2004; Xia and McCord, 2004) have described previ-
ous work on preprocessing methods. (Berger et al,
1996) describe an approach that targets translation
of French phrases of the form NOUN de NOUN
(e.g., conflit d?inte?re?t). This was a relatively lim-
ited study, concentrating on this one syntactic phe-
nomenon which involves relatively local transfor-
mations (a parser was not required in this study).
(Niessen and Ney, 2004) describe a method that
combines morphologically?split verbs in German,
and also reorders questions in English and German.
Our method goes beyond this approach in several
respects, for example considering phenomena such
as declarative (non-question) clauses, subordinate
clauses, negation, and so on.
(Xia and McCord, 2004) describe an approach for
translation from French to English, where reorder-
ing rules are acquired automatically. The reorder-
ing rules in their approach operate at the level of
context-free rules in the parse tree. Our method
differs from that of (Xia and McCord, 2004) in a
couple of important respects. First, we are consid-
ering German, which arguably has more challeng-
ing word order phenonema than French. German
has relatively free word order, in contrast to both
English and French: for example, there is consid-
erable flexibility in terms of which phrases can ap-
pear in the first position in a clause. Second, Xia
et. al?s (2004) use of reordering rules stated at the
context-free level differs from ours. As one exam-
ple, in our approach we use a single transformation
that moves an infinitival verb to the first position in
a verb phrase. Xia et. al?s approach would require
learning of a different rule transformation for every
production of the form VP => .... In practice the
German parser that we are using creates relatively
?flat? structures at the VP and clause levels, leading
to a huge number of context-free rules (the flatness
is one consequence of the relatively free word order
seen within VP?s and clauses in German). There are
clearly some advantages to learning reordering rules
automatically, as in Xia et. al?s approach. How-
ever, we note that our approach involves a hand-
ful of linguistically?motivated transformations and
achieves comparable improvements (albeit on a dif-
ferent language pair) to Xia et. al?s method, which
in contrast involves over 56,000 transformations.
S PPER-SB Ich
VAFIN-HD werde
VP PPER-DA Ihnen
NP-OA ART die
ADJA entsprechenden
NN Anmerkungen
VVINF-HD aushaendigen
, ,
S KOUS damit
PPER-SB Sie
VP PDS-OA das
ADJD eventuell
PP APPR bei
ART der
NN Abstimmung
VVINF-HD uebernehmen
VMFIN-HD koennen
Figure 1: An example parse tree. Key to non-terminals:
PPER = personal pronoun; VAFIN = finite verb; VVINF = in-
finitival verb; KOUS = complementizer; APPR = preposition;
ART = article; ADJA = adjective; ADJD = adverb; -SB = sub-
ject; -HD = head of a phrase; -DA = dative object; -OA = ac-
cusative object.
2.2 German Clause Structure
In this section we give a brief description of the syn-
tactic structure of German clauses. The character-
istics we describe motivate the reordering rules de-
scribed later in the paper.
Figure 1 gives an example parse tree for a German
sentence. This sentence contains two clauses:
Clause 1: Ich/I werde/will Ihnen/to you die/the
entsprechenden/corresponding
Anmerkungen/comments aushaendigen/pass on
Clause 2: damit/so that Sie/you das/them
eventuell/perhaps bei/in der/the Abstimmung/vote
uebernehmen/adopt koennen/can
These two clauses illustrate a number of syntactic
phenomena in German which lead to quite different
word order from English:
Position of finite verbs. In Clause 1, which is a
matrix clause, the finite verb werde is in the second
position in the clause. Finite verbs appear rigidly in
2nd position in matrix clauses. In contrast, in sub-
ordinate clauses, such as Clause 2, the finite verb
comes last in the clause. For example, note that
koennen is a finite verb which is the final element
of Clause 2.
Position of infinitival verbs. In German, infini-
tival verbs are final within their associated verb
533
phrase. For example, returning to Figure 1, no-
tice that aushaendigen is the last element in its verb
phrase, and that uebernehmen is the final element of
its verb phrase in the figure.
Relatively flexible word ordering. German has
substantially freer word order than English. In par-
ticular, note that while the verb comes second in ma-
trix clauses, essentially any element can be in the
first position. For example, in Clause 1, while the
subject Ich is seen in the first position, potentially
any of the other constituents (e.g., Ihnen) could also
appear in this position. Note that this often leads
to the subject following the finite verb, something
which happens very rarely in English.
There are many other phenomena which lead to
differing word order between German and English.
Two others that we focus on in this paper are nega-
tion (the differing placement of items such as not in
English and nicht in German), and also verb-particle
constructions. We describe our treatment of these
phenomena later in this paper.
2.3 Reordering with Phrase-Based SMT
We have seen in the last section that German syntax
has several characteristics that lead to significantly
different word order from that of English. We now
describe how these characteristics can lead to dif-
ficulties for phrase?based translation systems when
applied to German to English translation.
Typically, reordering models in phrase-based sys-
tems are based solely on movement distance. In par-
ticular, at each point in decoding a ?cost? is associ-
ated with skipping over 1 or more German words.
For example, assume that in translating
Ich werde Ihnen die entsprechenden An-
merkungen aushaendigen.
we have reached a state where ?Ich? and ?werde?
have been translated into ?I will? in English. A
potential decoding decision at this point is to add
the phrase ?pass on? to the English hypothesis, at
the same time absorbing ?aushaendigen? from the
German string. The cost of this decoding step
will involve a number of factors, including a cost
of skipping over a phrase of length 4 (i.e., Ihnen
die entsprechenden Anmerkungen) in the German
string.
The ability to penalise ?skips? of this type, and
the potential to model multi-word phrases, are es-
sentially the main strategies that the phrase-based
system is able to employ when modeling differing
word-order across different languages. In practice,
when training the parameters of an SMT system, for
example using the discriminative methods of (Och,
2003), the cost for skips of this kind is typically set
to a very high value. In experiments with the sys-
tem of (Koehn et al, 2003) we have found that in
practice a large number of complete translations are
completely monotonic (i.e., have   skips), suggest-
ing that the system has difficulty learning exactly
what points in the translation should allow reorder-
ing. In summary, phrase-based systems have rela-
tively limited potential to model word-order differ-
ences between different languages.
The reordering stage described in this paper at-
tempts to modify the source language (e.g., German)
in such a way that its word order is very similar to
that seen in the target language (e.g., English). In
an ideal approach, the resulting translation problem
that is passed on to the phrase-based system will be
solvable using a completely monotonic translation,
without any skips, and without requiring extremely
long phrases to be translated (for example a phrasal
translation corresponding to Ihnen die entsprechen-
den Anmerkungen aushaendigen).
Note than an additional benefit of the reordering
phase is that it may bring together groups of words
in German which have a natural correspondance to
phrases in English, but were unseen or rare in the
original German text. For example, in the previous
example, we might derive a correspondance between
werde aushaendigen and will pass on that was not
possible before reordering. Another example con-
cerns verb-particle constructions, for example in
Wir machen die Tuer auf
machen and auf form a verb-particle construction.
The reordering stage moves auf to precede machen,
allowing a phrasal entry that ?auf machen? is trans-
lated to to open in English. Without the reordering,
the particle can be arbitrarily far from the verb that
it modifies, and there is a danger in this example of
translating machen as to make, the natural transla-
tion when no particle is present.
534
Original sentence: Ich werde Ihnen die entsprechenden
Anmerkungen aushaendigen, damit Sie das eventuell bei
der Abstimmung uebernehmen koennen. (I will to you the
corresponding comments pass on, so that you them perhaps
in the vote adopt can.)
Reordered sentence: Ich werde aushaendigen Ihnen
die entsprechenden Anmerkungen, damit Sie koennen ue-
bernehmen das eventuell bei der Abstimmung.
(I will pass on to you the corresponding comments, so that you
can adopt them perhaps in the vote.)
Figure 2: An example of the reordering process, showing the
original German sentence and the sentence after reordering.
3 Clause Restructuring
We now describe the method we use for reordering
German sentences. As a first step in the reordering
process, we parse the sentence using the parser de-
scribed in (Dubey and Keller, 2003). The second
step is to apply a sequence of rules that reorder the
German sentence depending on the parse tree struc-
ture. See Figure 2 for an example German sentence
before and after the reordering step.
In the reordering phase, each of the following six
restructuring steps were applied to a German parse
tree, in sequence (see table 1 also, for examples of
the reordering steps):
[1] Verb initial In any verb phrase (i.e., phrase
with label VP-...) find the head of the phrase (i.e.,
the child with label -HD) and move it into the ini-
tial position within the verb phrase. For example,
in the parse tree in Figure 1, aushaendigen would be
moved to precede Ihnen in the first verb phrase (VP-
OC), and uebernehmen would be moved to precede
das in the second VP-OC. The subordinate clause
would have the following structure after this trans-
formation:
S-MO KOUS-CP damit
PPER-SB Sie
VP-OC VVINF-HD uebernehmen
PDS-OA das
ADJD-MO eventuell
PP-MO APPR-DA bei
ART-DA der
NN-NK Abstimmung
VMFIN-HD koennen
[2] Verb 2nd In any subordinate clause labelled
S-..., with a complementizer KOUS, PREL, PWS
or PWAV, find the head of the clause, and move it to
directly follow the complementizer.
For example, in the subordinate clause in Fig-
ure 1, the head of the clause koennen would be
moved to follow the complementizer damit, giving
the following structure:
S-MO KOUS-CP damit
VMFIN-HD koennen
PPER-SB Sie
VP-OC VVINF-HD uebernehmen
PDS-OA das
ADJD-MO eventuell
PP-MO APPR-DA bei
ART-DA der
NN-NK Abstimmung
[3] Move Subject For any clause (i.e., phrase with
label S...), move the subject to directly precede
the head. We define the subject to be the left-most
child of the clause with label ...-SB or PPER-
EP, and the head to be the leftmost child with label
...-HD.
For example, in the subordinate clause in Fig-
ure 1, the subject Sie would be moved to precede
koennen, giving the following structure:
S-MO KOUS-CP damit
PPER-SB Sie
VMFIN-HD koennen
VP-OC VVINF-HD uebernehmen
PDS-OA das
ADJD-MO eventuell
PP-MO APPR-DA bei
ART-DA der
NN-NK Abstimmung
[4] Particles In verb particle constructions, move
the particle to immediately precede the verb. More
specifically, if a finite verb (i.e., verb tagged as
VVFIN) and a particle (i.e., word tagged as PTKVZ)
are found in the same clause, move the particle to
precede the verb.
As one example, the following clause contains
both a verb (forden) as well as a particle (auf):
S PPER-SB Wir
VVFIN-HD fordern
NP-OA ART das
NN Praesidium
PTKVZ-SVP auf
After the transformation, the clause is altered to:
S PPER-SB Wir
PTKVZ-SVP auf
VVFIN-HD fordern
NP-OA ART das
NN Praesidium
535
Transformation Example
Verb Initial
Before: Ich werde Ihnen die entsprechenden Anmerkungen aushaendigen,    
After: Ich werde aushaendigen Ihnen die entsprechenden Anmerkungen,    
English: I shall be passing on to you some comments,    
Verb 2nd
Before:     damit Sie uebernehmen das eventuell bei der Abstimmung koennen.
After:     damit koennen Sie uebernehmen das eventuell bei der Abstimmung .
English:     so that could you adopt this perhaps in the voting.
Move Subject
Before:     damit koennen Sie uebernehmen das eventuell bei der Abstimmung.
After:     damit Sie koennen uebernehmen das eventuell bei der Abstimmung .
English:     so that you could adopt this perhaps in the voting.
Particles
Before: Wir fordern das Praesidium auf,    
After: Wir auf fordern das Praesidium,    
English: We ask the Bureau,    
Infinitives
Before: Ich werde der Sache nachgehen dann,    
After: Ich werde nachgehen der Sache dann,    
English: I will look into the matter then,    
Negation
Before: Wir konnten einreichen es nicht mehr rechtzeitig,    
After: Wir konnten nicht einreichen es mehr rechtzeitig,    
English: We could not hand it in in time,    
Table 1: Examples for each of the reordering steps. In each case the item that is moved is underlined.
[5] Infinitives In some cases, infinitival verbs are
still not in the correct position after transformations
[1]?[4]. For this reason we add a second step that
involves infinitives. First, we remove all internal VP
nodes within the parse tree. Second, for any clause
(i.e., phrase labeled S...), if the clause dominates
both a finite and infinitival verb, and there is an argu-
ment (i.e., a subject, or an object) between the two
verbs, then the infinitive is moved to directly follow
the finite verb.
As an example, the following clause contains an
infinitival (einreichen) that is separated from a finite
verb konnten by the direct object es:
S PPER-SB Wir
VMFIN-HD konnten
PPER-OA es
PTKNEG-NG nicht
VP-OC VVINF-HD einreichen
AP-MO ADV-MO mehr
ADJD-HD rechtzeitig
The transformation removes the VP-OC, and
moves the infinitive, giving:
S PPER-SB Wir
VMFIN-HD konnten
VVINF-HD einreichen
PPER-OA es
PTKNEG-NG nicht
AP-MO ADV-MO mehr
ADJD-HD rechtzeitig
[6] Negation As a final step, we move negative
particles. If a clause dominates both a finite and in-
finitival verb, as well as a negative particle (i.e., a
word tagged as PTKNEG), then the negative particle
is moved to directly follow the finite verb.
As an example, the previous example now has the
negative particle nicht moved, to give the following
clause structure:
S PPER-SB Wir
VMFIN-HD konnten
PTKNEG-NG nicht
VVINF-HD einreichen
PPER-OA es
AP-MO ADV-MO mehr
ADJD-HD rechtzeitig
4 Experiments
This section describes experiments with the reorder-
ing approach. Our baseline is the phrase-based
MT system of (Koehn et al, 2003). We trained
this system on the Europarl corpus, which consists
of 751,088 sentence pairs with 15,256,792 German
words and 16,052,269 English words. Translation
performance is measured on a 2000 sentence test set
from a different part of the Europarl corpus, with av-
erage sentence length of 28 words.
We use BLEU scores (Papineni et al, 2002) to
measure translation accuracy. We applied our re-
536
Annotator 2
Annotator 1 R B E
R 33 2 5
B 2 13 5
E 9 4 27
Table 2: Table showing the level of agreement between two
annotators on 100 translation judgements. R gives counts cor-
responding to translations where an annotator preferred the re-
ordered system; B signifies that the annotator preferred the
baseline system; E means an annotator judged the two systems
to give equal quality translations.
ordering method to both the training and test data,
and retrained the system on the reordered training
data. The BLEU score for the new system was
26.8%, an improvement from 25.2% BLEU for the
baseline system.
4.1 Human Translation Judgements
We also used human judgements of translation qual-
ity to evaluate the effectiveness of the reordering
rules. We randomly selected 100 sentences from the
test corpus where the English reference translation
was between 10 and 20 words in length.1 For each
of these 100 translations, we presented the two anno-
tators with three translations: the reference (human)
translation, the output from the baseline system, and
the output from the system with reordering. No in-
dication was given as to which system was the base-
line system, and the ordering in which the baseline
and reordered translations were presented was cho-
sen at random on each example, to prevent ordering
effects in the annotators? judgements. For each ex-
ample, we asked each of the annotators to make one
of two choices: 1) an indication that one translation
was an improvement over the other; or 2) an indica-
tion that the translations were of equal quality.
Annotator 1 judged 40 translations to be improved
by the reordered model; 40 translations to be of
equal quality; and 20 translations to be worse under
the reordered model. Annotator 2 judged 44 trans-
lations to be improved by the reordered model; 37
translations to be of equal quality; and 19 transla-
tions to be worse under the reordered model. Ta-
ble 2 gives figures indicating agreement rates be-
tween the annotators. Note that if we only consider
preferences where both annotators were in agree-
1We chose these shorter sentences for human evaluation be-
cause in general they include a single clause, which makes hu-
man judgements relatively straightforward.
ment (and consider all disagreements to fall into the
?equal? category), then 33 translations improved un-
der the reordering system, and 13 translations be-
came worse. Figure 3 shows a random selection
of the translations where annotator 1 judged the re-
ordered model to give an improvement; Figure 4
shows examples where the baseline system was pre-
ferred by annotator 1. We include these examples to
give a qualitative impression of the differences be-
tween the baseline and reordered system. Our (no
doubt subjective) impression is that the cases in fig-
ure 3 are more clear cut instances of translation im-
provements, but we leave the reader to make his/her
own judgement on this point.
4.2 Statistical Significance
We now describe statistical significance tests for our
results. We believe that applying significance tests
to Bleu scores is a subtle issue, for this reason we go
into some detail in this section.
We used the sign test (e.g., see page 166 of
(Lehmann, 1986)) to test the statistical significance
of our results. For a source sentence
 
, the sign test
requires a function 
 
that is defined as follows:

	















 If reordered system produces a better
translation for
	
than the baseline
 If baseline produces a better translation
for
	
than the reordered system.

If the two systems produce equal
quality translations on
	
We assume that sentences
 
are drawn from
some underlying distribution    , and that the test
set consists of independently, identically distributed
(IID) sentences from this distribution. We can define
the following probabilities:
 Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Simple Semi-supervised Dependency Parsing
Terry Koo, Xavier Carreras, and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,carreras,mcollins}@csail.mit.edu
Abstract
We present a simple and effective semi-
supervised method for training dependency
parsers. We focus on the problem of lex-
ical representation, introducing features that
incorporate word clusters derived from a large
unannotated corpus. We demonstrate the ef-
fectiveness of the approach in a series of de-
pendency parsing experiments on the Penn
Treebank and Prague Dependency Treebank,
and we show that the cluster-based features
yield substantial gains in performance across
a wide range of conditions. For example, in
the case of English unlabeled second-order
parsing, we improve from a baseline accu-
racy of 92.02% to 93.16%, and in the case
of Czech unlabeled second-order parsing, we
improve from a baseline accuracy of 86.13%
to 87.13%. In addition, we demonstrate that
our method also improves performance when
small amounts of training data are available,
and can roughly halve the amount of super-
vised data required to reach a desired level of
performance.
1 Introduction
In natural language parsing, lexical information is
seen as crucial to resolving ambiguous relationships,
yet lexicalized statistics are sparse and difficult to es-
timate directly. It is therefore attractive to consider
intermediate entities which exist at a coarser level
than the words themselves, yet capture the informa-
tion necessary to resolve the relevant ambiguities.
In this paper, we introduce lexical intermediaries
via a simple two-stage semi-supervised approach.
First, we use a large unannotated corpus to define
word clusters, and then we use that clustering to
construct a new cluster-based feature mapping for
a discriminative learner. We are thus relying on the
ability of discriminative learning methods to identify
and exploit informative features while remaining ag-
nostic as to the origin of such features. To demon-
strate the effectiveness of our approach, we conduct
experiments in dependency parsing, which has been
the focus of much recent research?e.g., see work
in the CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007).
The idea of combining word clusters with dis-
criminative learning has been previously explored
by Miller et al (2004), in the context of named-
entity recognition, and their work directly inspired
our research. However, our target task of depen-
dency parsing involves more complex structured re-
lationships than named-entity tagging; moreover, it
is not at all clear that word clusters should have any
relevance to syntactic structure. Nevertheless, our
experiments demonstrate that word clusters can be
quite effective in dependency parsing applications.
In general, semi-supervised learning can be mo-
tivated by two concerns: first, given a fixed amount
of supervised data, we might wish to leverage ad-
ditional unlabeled data to facilitate the utilization of
the supervised corpus, increasing the performance of
the model in absolute terms. Second, given a fixed
target performance level, we might wish to use un-
labeled data to reduce the amount of annotated data
necessary to reach this target.
We show that our semi-supervised approach
yields improvements for fixed datasets by perform-
ing parsing experiments on the Penn Treebank (Mar-
cus et al, 1993) and Prague Dependency Treebank
(Hajic?, 1998; Hajic? et al, 2001) (see Sections 4.1
and 4.3). By conducting experiments on datasets of
varying sizes, we demonstrate that for fixed levels of
performance, the cluster-based approach can reduce
the need for supervised data by roughly half, which
is a substantial savings in data-annotation costs (see
Sections 4.2 and 4.4).
The remainder of this paper is divided as follows:
595
Ms. Haag plays Elianti .*
obj
proot
nmod sbj
Figure 1: An example of a labeled dependency tree. The
tree contains a special token ?*? which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
Section 2 gives background on dependency parsing
and clustering, Section 3 describes the cluster-based
features, Section 4 presents our experimental results,
Section 5 discusses related work, and Section 6 con-
cludes with ideas for future research.
2 Background
2.1 Dependency parsing
Recent work (Buchholz and Marsi, 2006; Nivre
et al, 2007) has focused on dependency parsing.
Dependency syntax represents syntactic informa-
tion as a network of head-modifier dependency arcs,
typically restricted to be a directed tree (see Fig-
ure 1 for an example). Dependency parsing depends
critically on predicting head-modifier relationships,
which can be difficult due to the statistical sparsity
of these word-to-word interactions. Bilexical depen-
dencies are thus ideal candidates for the application
of coarse word proxies such as word clusters.
In this paper, we take a part-factored structured
classification approach to dependency parsing. For a
given sentence x, let Y(x) denote the set of possible
dependency structures spanning x, where each y ?
Y(x) decomposes into a set of ?parts? r ? y. In the
simplest case, these parts are the dependency arcs
themselves, yielding a first-order or ?edge-factored?
dependency parsing model. In higher-order parsing
models, the parts can consist of interactions between
more than two words. For example, the parser of
McDonald and Pereira (2006) defines parts for sib-
ling interactions, such as the trio ?plays?, ?Elianti?,
and ?.? in Figure 1. The Carreras (2007) parser
has parts for both sibling interactions and grandpar-
ent interactions, such as the trio ?*?, ?plays?, and
?Haag? in Figure 1. These kinds of higher-order
factorizations allow dependency parsers to obtain a
limited form of context-sensitivity.
Given a factorization of dependency structures
into parts, we restate dependency parsing as the fol-
apple pear Apple IBM bought run of in
01
100 101 110 111000 001 010 011
00
0
10
1
11
Figure 2: An example of a Brown word-cluster hierarchy.
Each node in the tree is labeled with a bit-string indicat-
ing the path from the root node to that node, where 0
indicates a left branch and 1 indicates a right branch.
lowing maximization:
PARSE(x;w) = argmax
y?Y(x)
?
r?y
w ? f(x, r)
Above, we have assumed that each part is scored
by a linear model with parameters w and feature-
mapping f(?). For many different part factoriza-
tions and structure domains Y(?), it is possible to
solve the above maximization efficiently, and several
recent efforts have concentrated on designing new
maximization algorithms with increased context-
sensitivity (Eisner, 2000; McDonald et al, 2005b;
McDonald and Pereira, 2006; Carreras, 2007).
2.2 Brown clustering algorithm
In order to provide word clusters for our exper-
iments, we used the Brown clustering algorithm
(Brown et al, 1992). We chose to work with the
Brown algorithm due to its simplicity and prior suc-
cess in other NLP applications (Miller et al, 2004;
Liang, 2005). However, we expect that our approach
can function with other clustering algorithms (as in,
e.g., Li and McCallum (2005)). We briefly describe
the Brown algorithm below.
The input to the algorithm is a vocabulary of
words to be clustered and a corpus of text containing
these words. Initially, each word in the vocabulary
is considered to be in its own distinct cluster. The al-
gorithm then repeatedly merges the pair of clusters
which causes the smallest decrease in the likelihood
of the text corpus, according to a class-based bigram
language model defined on the word clusters. By
tracing the pairwise merge operations, one obtains
a hierarchical clustering of the words, which can be
represented as a binary tree as in Figure 2.
Within this tree, each word is uniquely identified
by its path from the root, and this path can be com-
pactly represented with a bit string, as in Figure 2.
In order to obtain a clustering of the words, we se-
lect all nodes at a certain depth from the root of the
596
hierarchy. For example, in Figure 2 we might select
the four nodes at depth 2 from the root, yielding the
clusters {apple,pear}, {Apple,IBM}, {bought,run},
and {of,in}. Note that the same clustering can be ob-
tained by truncating each word?s bit-string to a 2-bit
prefix. By using prefixes of various lengths, we can
produce clusterings of different granularities (Miller
et al, 2004).
For all of the experiments in this paper, we used
the Liang (2005) implementation of the Brown algo-
rithm to obtain the necessary word clusters.
3 Feature design
Key to the success of our approach is the use of fea-
tures which allow word-cluster-based information to
assist the parser. The feature sets we used are simi-
lar to other feature sets in the literature (McDonald
et al, 2005a; Carreras, 2007), so we will not attempt
to give a exhaustive description of the features in
this section. Rather, we describe our features at a
high level and concentrate on our methodology and
motivations. In our experiments, we employed two
different feature sets: a baseline feature set which
draws upon ?normal? information sources such as
word forms and parts of speech, and a cluster-based
feature set that also uses information derived from
the Brown cluster hierarchy.
3.1 Baseline features
Our first-order baseline feature set is similar to the
feature set of McDonald et al (2005a), and consists
of indicator functions for combinations of words and
parts of speech for the head and modifier of each
dependency, as well as certain contextual tokens.1
Our second-order baseline features are the same as
those of Carreras (2007) and include indicators for
triples of part of speech tags for sibling interactions
and grandparent interactions, as well as additional
bigram features based on pairs of words involved
these higher-order interactions. Examples of base-
line features are provided in Table 1.
1We augment the McDonald et al (2005a) feature set with
backed-off versions of the ?Surrounding Word POS Features?
that include only one neighboring POS tag. We also add binned
distance features which indicate whether the number of tokens
between the head and modifier of a dependency is greater than
2, 5, 10, 20, 30, or 40 tokens.
Baseline Cluster-based
ht,mt hc4,mc4
hw,mw hc6,mc6
hw,ht,mt hc*,mc*
hw,ht,mw hc4,mt
ht,mw,mt ht,mc4
hw,mw,mt hc6,mt
hw,ht,mw,mt ht,mc6
? ? ? hc4,mw
hw,mc4
? ? ?
ht,mt,st hc4,mc4,sc4
ht,mt,gt hc6,mc6,sc6
? ? ? ht,mc4,sc4
hc4,mc4,gc4
? ? ?
Table 1: Examples of baseline and cluster-based feature
templates. Each entry represents a class of indicators for
tuples of information. For example, ?ht,mt? represents
a class of indicator features with one feature for each pos-
sible combination of head POS-tag and modifier POS-
tag. Abbreviations: ht = head POS, hw = head word,
hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head,
hc* = full bit string of head; mt,mw,mc4,mc6,mc* =
likewise for modifier; st,gt,sc4,gc4,. . . = likewise
for sibling and grandchild.
3.2 Cluster-based features
The first- and second-order cluster-based feature sets
are supersets of the baseline feature sets: they in-
clude all of the baseline feature templates, and add
an additional layer of features that incorporate word
clusters. Following Miller et al (2004), we use pre-
fixes of the Brown cluster hierarchy to produce clus-
terings of varying granularity. We found that it was
nontrivial to select the proper prefix lengths for the
dependency parsing task; in particular, the prefix
lengths used in the Miller et al (2004) work (be-
tween 12 and 20 bits) performed poorly in depen-
dency parsing.2 After experimenting with many dif-
ferent feature configurations, we eventually settled
on a simple but effective methodology.
First, we found that it was helpful to employ two
different types of word clusters:
1. Short bit-string prefixes (e.g., 4?6 bits), which
we used as replacements for parts of speech.
2One possible explanation is that the kinds of distinctions
required in a named-entity recognition task (e.g., ?Alice? versus
?Intel?) are much finer-grained than the kinds of distinctions
relevant to syntax (e.g., ?apple? versus ?eat?).
597
2. Full bit strings,3 which we used as substitutes
for word forms.
Using these two types of clusters, we generated new
features by mimicking the template structure of the
original baseline features. For example, the baseline
feature set includes indicators for word-to-word and
tag-to-tag interactions between the head and mod-
ifier of a dependency. In the cluster-based feature
set, we correspondingly introduce new indicators for
interactions between pairs of short bit-string pre-
fixes and pairs of full bit strings. Some examples
of cluster-based features are given in Table 1.
Second, we found it useful to concentrate on
?hybrid? features involving, e.g., one bit-string and
one part of speech. In our initial attempts, we fo-
cused on features that used cluster information ex-
clusively. While these cluster-only features provided
some benefit, we found that adding hybrid features
resulted in even greater improvements. One possible
explanation is that the clusterings generated by the
Brown algorithm can be noisy or only weakly rele-
vant to syntax; thus, the clusters are best exploited
when ?anchored? to words or parts of speech.
Finally, we found it useful to impose a form of
vocabulary restriction on the cluster-based features.
Specifically, for any feature that is predicated on a
word form, we eliminate this feature if the word
in question is not one of the top-N most frequent
words in the corpus. When N is between roughly
100 and 1,000, there is little effect on the perfor-
mance of the cluster-based feature sets.4 In addition,
the vocabulary restriction reduces the size of the fea-
ture sets to managable proportions.
4 Experiments
In order to evaluate the effectiveness of the cluster-
based feature sets, we conducted dependency pars-
ing experiments in English and Czech. We test the
features in a wide range of parsing configurations,
including first-order and second-order parsers, and
labeled and unlabeled parsers.5
3As in Brown et al (1992), we limit the clustering algorithm
so that it recovers at most 1,000 distinct bit-strings; thus full bit
strings are not equivalent to word forms.
4We used N = 800 for all experiments in this paper.
5In an ?unlabeled? parser, we simply ignore dependency la-
bel information, which is a common simplification.
The English experiments were performed on the
Penn Treebank (Marcus et al, 1993), using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to a dependency tree represen-
tation.6 We split the Treebank into a training set
(Sections 2?21), a development set (Section 22), and
several test sets (Sections 0,7 1, 23, and 24). The
data partition and head rules were chosen to match
previous work (Yamada and Matsumoto, 2003; Mc-
Donald et al, 2005a; McDonald and Pereira, 2006).
The part of speech tags for the development and test
data were automatically assigned by MXPOST (Rat-
naparkhi, 1996), where the tagger was trained on
the entire training corpus; to generate part of speech
tags for the training data, we used 10-way jackknif-
ing.8 English word clusters were derived from the
BLLIP corpus (Charniak et al, 2000), which con-
tains roughly 43 million words of Wall Street Jour-
nal text.9
The Czech experiments were performed on the
Prague Dependency Treebank 1.0 (Hajic?, 1998;
Hajic? et al, 2001), which is directly annotated
with dependency structures. To facilitate compar-
isons with previous work (McDonald et al, 2005b;
McDonald and Pereira, 2006), we used the train-
ing/development/test partition defined in the corpus
and we also used the automatically-assigned part of
speech tags provided in the corpus.10 Czech word
clusters were derived from the raw text section of
the PDT 1.0, which contains about 39 million words
of newswire text.11
We trained the parsers using the averaged percep-
tron (Freund and Schapire, 1999; Collins, 2002),
which represents a balance between strong perfor-
mance and fast training times. To select the number
6We used Joakim Nivre?s ?Penn2Malt? conversion tool
(http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html). Depen-
dency labels were obtained via the ?Malt? hard-coded setting.
7For computational reasons, we removed a single 249-word
sentence from Section 0.
8That is, we tagged each fold with the tagger trained on the
other 9 folds.
9We ensured that the sentences of the Penn Treebank were
excluded from the text used for the clustering.
10Following Collins et al (1999), we used a coarsened ver-
sion of the Czech part of speech tags; this choice also matches
the conditions of previous work (McDonald et al, 2005b; Mc-
Donald and Pereira, 2006).
11This text was disjoint from the training and test corpora.
598
Sec dep1 dep1c MD1 dep2 dep2c MD2 dep1-L dep1c-L dep2-L dep2c-L
00 90.48 91.57 (+1.09) ? 91.76 92.77 (+1.01) ? 90.29 91.03 (+0.74) 91.33 92.09 (+0.76)
01 91.31 92.43 (+1.12) ? 92.46 93.34 (+0.88) ? 90.84 91.73 (+0.89) 91.94 92.65 (+0.71)
23 90.84 92.23 (+1.39) 90.9 92.02 93.16 (+1.14) 91.5 90.32 91.24 (+0.92) 91.38 92.14 (+0.76)
24 89.67 91.30 (+1.63) ? 90.92 91.85 (+0.93) ? 89.55 90.06 (+0.51) 90.42 91.18 (+0.76)
Table 2: Parent-prediction accuracies on Sections 0, 1, 23, and 24. Abbreviations: dep1/dep1c = first-order parser with
baseline/cluster-based features; dep2/dep2c = second-order parser with baseline/cluster-based features; MD1 = Mc-
Donald et al (2005a); MD2 = McDonald and Pereira (2006); suffix -L = labeled parser. Unlabeled parsers are scored
using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. Improvements of
cluster-based features over baseline features are shown in parentheses.
of iterations of perceptron training, we performed up
to 30 iterations and chose the iteration which opti-
mized accuracy on the development set. Our feature
mappings are quite high-dimensional, so we elimi-
nated all features which occur only once in the train-
ing data. The resulting models still had very high
dimensionality, ranging from tens of millions to as
many as a billion features.12
All results presented in this section are given
in terms of parent-prediction accuracy, which mea-
sures the percentage of tokens that are attached to
the correct head token. For labeled dependency
structures, both the head token and dependency label
must be correctly predicted. In addition, in English
parsing we ignore the parent-predictions of punc-
tuation tokens,13 and in Czech parsing we retain
the punctuation tokens; this matches previous work
(Yamada and Matsumoto, 2003; McDonald et al,
2005a; McDonald and Pereira, 2006).
4.1 English main results
In our English experiments, we tested eight differ-
ent parsing configurations, representing all possi-
ble choices between baseline or cluster-based fea-
ture sets, first-order (Eisner, 2000) or second-order
(Carreras, 2007) factorizations, and labeled or unla-
beled parsing.
Table 2 compiles our final test results and also
includes two results from previous work by Mc-
Donald et al (2005a) and McDonald and Pereira
(2006), for the purposes of comparison. We note
a few small differences between our parsers and the
12Due to the sparsity of the perceptron updates, however,
only a small fraction of the possible features were active in our
trained models.
13A punctuation token is any token whose gold-standard part
of speech tag is one of {?? ?? : , .}.
parsers evaluated in this previous work. First, the
MD1 and MD2 parsers were trained via the MIRA
algorithm (Crammer and Singer, 2003; Crammer et
al., 2004), while we use the averaged perceptron. In
addition, the MD2 model uses only sibling interac-
tions, whereas the dep2/dep2c parsers include both
sibling and grandparent interactions.
There are some clear trends in the results of Ta-
ble 2. First, performance increases with the order of
the parser: edge-factored models (dep1 and MD1)
have the lowest performance, adding sibling rela-
tionships (MD2) increases performance, and adding
grandparent relationships (dep2) yields even better
accuracies. Similar observations regarding the ef-
fect of model order have also been made by Carreras
(2007).
Second, note that the parsers using cluster-based
feature sets consistently outperform the models us-
ing the baseline features, regardless of model order
or label usage. Some of these improvements can be
quite large; for example, a first-order model using
cluster-based features generally performs as well as
a second-order model using baseline features. More-
over, the benefits of cluster-based feature sets com-
bine additively with the gains of increasing model
order. For example, consider the unlabeled parsers
in Table 2: on Section 23, increasing the model or-
der from dep1 to dep2 results in a relative reduction
in error of roughly 13%, while introducing cluster-
based features from dep2 to dep2c yields an addi-
tional relative error reduction of roughly 14%. As a
final note, all 16 comparisons between cluster-based
features and baseline features shown in Table 2 are
statistically significant.14
14We used the sign test at the sentence level. The comparison
between dep1-L and dep1c-L is significant at p < 0.05, and all
other comparisons are significant at p < 0.0005.
599
Tagger always trained on full Treebank Tagger trained on reduced dataset
Size dep1 dep1c ? dep2 dep2c ?
1k 84.54 85.90 1.36 86.29 87.47 1.18
2k 86.20 87.65 1.45 87.67 88.88 1.21
4k 87.79 89.15 1.36 89.22 90.46 1.24
8k 88.92 90.22 1.30 90.62 91.55 0.93
16k 90.00 91.27 1.27 91.27 92.39 1.12
32k 90.74 92.18 1.44 92.05 93.36 1.31
All 90.89 92.33 1.44 92.42 93.30 0.88
Size dep1 dep1c ? dep2 dep2c ?
1k 80.49 84.06 3.57 81.95 85.33 3.38
2k 83.47 86.04 2.57 85.02 87.54 2.52
4k 86.53 88.39 1.86 87.88 89.67 1.79
8k 88.25 89.94 1.69 89.71 91.37 1.66
16k 89.66 91.03 1.37 91.14 92.22 1.08
32k 90.78 92.12 1.34 92.09 93.21 1.12
All 90.89 92.33 1.44 92.42 93.30 0.88
Table 3: Parent-prediction accuracies of unlabeled English parsers on Section 22. Abbreviations: Size = #sentences in
training corpus; ? = difference between cluster-based and baseline features; other abbreviations are as in Table 2.
4.2 English learning curves
We performed additional experiments to evaluate the
effect of the cluster-based features as the amount
of training data is varied. Note that the depen-
dency parsers we use require the input to be tagged
with parts of speech; thus the quality of the part-of-
speech tagger can have a strong effect on the per-
formance of the parser. In these experiments, we
consider two possible scenarios:
1. The tagger has a large training corpus, while
the parser has a smaller training corpus. This
scenario can arise when tagged data is cheaper
to obtain than syntactically-annotated data.
2. The same amount of labeled data is available
for training both tagger and parser.
Table 3 displays the accuracy of first- and second-
order models when trained on smaller portions of
the Treebank, in both scenarios described above.
Note that the cluster-based features obtain consistent
gains regardless of the size of the training set. When
the tagger is trained on the reduced-size datasets,
the gains of cluster-based features are more pro-
nounced, but substantial improvements are obtained
even when the tagger is accurate.
It is interesting to consider the amount by which
cluster-based features reduce the need for supervised
data, given a desired level of accuracy. Based on
Table 3, we can extrapolate that cluster-based fea-
tures reduce the need for supervised data by roughly
a factor of 2. For example, the performance of the
dep1c and dep2c models trained on 1k sentences is
roughly the same as the performance of the dep1
and dep2 models, respectively, trained on 2k sen-
tences. This approximate data-halving effect can be
observed throughout the results in Table 3.
When combining the effects of model order and
cluster-based features, the reductions in the amount
of supervised data required are even larger. For ex-
ample, in scenario 1 the dep2c model trained on 1k
sentences is close in performance to the dep1 model
trained on 4k sentences, and the dep2c model trained
on 4k sentences is close to the dep1 model trained on
the entire training set (roughly 40k sentences).
4.3 Czech main results
In our Czech experiments, we considered only unla-
beled parsing,15 leaving four different parsing con-
figurations: baseline or cluster-based features and
first-order or second-order parsing. Note that our
feature sets were originally tuned for English pars-
ing, and except for the use of Czech clusters, we
made no attempt to retune our features for Czech.
Czech dependency structures may contain non-
projective edges, so we employ a maximum directed
spanning tree algorithm (Chu and Liu, 1965; Ed-
monds, 1967; McDonald et al, 2005b) as our first-
order parser for Czech. For the second-order pars-
ing experiments, we used the Carreras (2007) parser.
Since this parser only considers projective depen-
dency structures, we ?projectivized? the PDT 1.0
training set by finding, for each sentence, the pro-
jective tree which retains the most correct dependen-
cies; our second-order parsers were then trained with
respect to these projective trees. The development
and test sets were not projectivized, so our second-
order parser is guaranteed to make errors in test sen-
tences containing non-projective dependencies. To
overcome this, McDonald and Pereira (2006) use a
15We leave labeled parsing experiments to future work.
600
dep1 dep1c dep2 dep2c
84.49 86.07 (+1.58) 86.13 87.13 (+1.00)
Table 4: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 test set, for baseline features and
cluster-based features. Abbreviations are as in Table 2.
Parser Accuracy
Nivre and Nilsson (2005) 80.1
McDonald et al (2005b) 84.4
Hall and Nova?k (2005) 85.1
McDonald and Pereira (2006) 85.2
dep1c 86.07
dep2c 87.13
Table 5: Unlabeled parent-prediction accuracies of Czech
parsers on the PDT 1.0 test set, for our models and for
previous work.
Size dep1 dep1c ? dep2 dep2c ?
1k 72.79 73.66 0.87 74.35 74.63 0.28
2k 74.92 76.23 1.31 76.63 77.60 0.97
4k 76.87 78.14 1.27 78.34 79.34 1.00
8k 78.17 79.83 1.66 79.82 80.98 1.16
16k 80.60 82.44 1.84 82.53 83.69 1.16
32k 82.85 84.65 1.80 84.66 85.81 1.15
64k 84.20 85.98 1.78 86.01 87.11 1.10
All 84.36 86.09 1.73 86.09 87.26 1.17
Table 6: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 development set. Abbreviations
are as in Table 3.
two-stage approximate decoding process in which
the output of their second-order parser is ?deprojec-
tivized? via greedy search. For simplicity, we did
not implement a deprojectivization stage on top of
our second-order parser, but we conjecture that such
techniques may yield some additional performance
gains; we leave this to future work.
Table 4 gives accuracy results on the PDT 1.0
test set for our unlabeled parsers. As in the En-
glish experiments, there are clear trends in the re-
sults: parsers using cluster-based features outper-
form parsers using baseline features, and second-
order parsers outperform first-order parsers. Both of
the comparisons between cluster-based and baseline
features in Table 4 are statistically significant.16 Ta-
ble 5 compares accuracy results on the PDT 1.0 test
set for our parsers and several other recent papers.
16We used the sign test at the sentence level; both compar-
isons are significant at p < 0.0005.
N dep1 dep1c dep2 dep2c
100 89.19 92.25 90.61 93.14
200 90.03 92.26 91.35 93.18
400 90.31 92.32 91.72 93.20
800 90.62 92.33 91.89 93.30
1600 90.87 ? 92.20 ?
All 90.89 ? 92.42 ?
Table 7: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: N = thresh-
old value; other abbreviations are as in Table 2. We
did not train cluster-based parsers using threshold values
larger than 800 due to computational limitations.
dep1-P dep1c-P dep1 dep2-P dep2c-P dep2
77.19 90.69 90.89 86.73 91.84 92.42
Table 8: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: suffix -P =
model without POS; other abbreviations are as in Table 2.
4.4 Czech learning curves
As in our English experiments, we performed addi-
tional experiments on reduced sections of the PDT;
the results are shown in Table 6. For simplicity, we
did not retrain a tagger for each reduced dataset,
so we always use the (automatically-assigned) part
of speech tags provided in the corpus. Note that
the cluster-based features obtain improvements at all
training set sizes, with data-reduction factors simi-
lar to those observed in English. For example, the
dep1c model trained on 4k sentences is roughly as
good as the dep1 model trained on 8k sentences.
4.5 Additional results
Here, we present two additional results which fur-
ther explore the behavior of the cluster-based fea-
ture sets. In Table 7, we show the development-set
performance of second-order parsers as the thresh-
old for lexical feature elimination (see Section 3.2)
is varied. Note that the performance of cluster-based
features is fairly insensitive to the threshold value,
whereas the performance of baseline features clearly
degrades as the vocabulary size is reduced.
In Table 8, we show the development-set perfor-
mance of the first- and second-order parsers when
features containing part-of-speech-based informa-
tion are eliminated. Note that the performance ob-
tained by using clusters without parts of speech is
close to the performance of the baseline features.
601
5 Related Work
As mentioned earlier, our approach was inspired by
the success of Miller et al (2004), who demon-
strated the effectiveness of using word clusters as
features in a discriminative learning approach. Our
research, however, applies this technique to depen-
dency parsing rather than named-entity recognition.
In this paper, we have focused on developing new
representations for lexical information. Previous re-
search in this area includes several models which in-
corporate hidden variables (Matsuzaki et al, 2005;
Koo and Collins, 2005; Petrov et al, 2006; Titov
and Henderson, 2007). These approaches have the
advantage that the model is able to learn different
usages for the hidden variables, depending on the
target problem at hand. Crucially, however, these
methods do not exploit unlabeled data when learn-
ing their representations.
Wang et al (2005) used distributional similarity
scores to smooth a generative probability model for
dependency parsing and obtained improvements in
a Chinese parsing task. Our approach is similar to
theirs in that the Brown algorithm produces clusters
based on distributional similarity, and the cluster-
based features can be viewed as being a kind of
?backed-off? version of the baseline features. How-
ever, our work is focused on discriminative learning
as opposed to generative models.
Semi-supervised phrase structure parsing has
been previously explored by McClosky et al (2006),
who applied a reranked parser to a large unsuper-
vised corpus in order to obtain additional train-
ing data for the parser; this self-training appraoch
was shown to be quite effective in practice. How-
ever, their approach depends on the usage of a
high-quality parse reranker, whereas the method de-
scribed here simply augments the features of an ex-
isting parser. Note that our two approaches are com-
patible in that we could also design a reranker and
apply self-training techniques on top of the cluster-
based features.
6 Conclusions
In this paper, we have presented a simple but effec-
tive semi-supervised learning approach and demon-
strated that it achieves substantial improvement over
a competitive baseline in two broad-coverage depen-
dency parsing tasks. Despite this success, there are
several ways in which our approach might be im-
proved.
To begin, recall that the Brown clustering algo-
rithm is based on a bigram language model. Intu-
itively, there is a ?mismatch? between the kind of
lexical information that is captured by the Brown
clusters and the kind of lexical information that is
modeled in dependency parsing. A natural avenue
for further research would be the development of
clustering algorithms that reflect the syntactic be-
havior of words; e.g., an algorithm that attempts to
maximize the likelihood of a treebank, according to
a probabilistic dependency model. Alternately, one
could design clustering algorithms that cluster entire
head-modifier arcs rather than individual words.
Another idea would be to integrate the cluster-
ing algorithm into the training algorithm in a limited
fashion. For example, after training an initial parser,
one could parse a large amount of unlabeled text and
use those parses to improve the quality of the clus-
ters. These improved clusters can then be used to
retrain an improved parser, resulting in an overall
algorithm similar to that of McClosky et al (2006).
Setting aside the development of new clustering
algorithms, a final area for future work is the exten-
sion of our method to new domains, such as con-
versational text or other languages, and new NLP
problems, such as machine translation.
Acknowledgments
The authors thank the anonymous reviewers for
their insightful comments. Many thanks also to
Percy Liang for providing his implementation of
the Brown algorithm, and Ryan McDonald for his
assistance with the experimental setup. The au-
thors gratefully acknowledge the following sources
of support. Terry Koo was funded by NSF grant
DMS-0434222 and a grant from NTT, Agmt. Dtd.
6/21/1998. Xavier Carreras was supported by the
Catalan Ministry of Innovation, Universities and
Enterprise, and a grant from NTT, Agmt. Dtd.
6/21/1998. Michael Collins was funded by NSF
grants 0347631 and DMS-0434222.
602
References
P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-Based n-gram Mod-
els of Natural Language. Computational Linguistics,
18(4):467?479.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task
on Multilingual Dependency Parsing. In Proceedings
of CoNLL, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proceedings of
EMNLP-CoNLL, pages 957?961.
E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson.
2000. BLLIP 1987?89 WSJ Corpus Release 1, LDC
No. LDC2000T43. Linguistic Data Consortium.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A Statistical Parser for Czech. In Proceedings of ACL,
pages 505?512.
M. Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP, pages 1?8.
K. Crammer and Y. Singer. 2003. Ultraconservative On-
line Algorithms for Multiclass Problems. Journal of
Machine Learning Research, 3:951?991.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2004. Online Passive-Aggressive Algorithms. In
S. Thrun, L. Saul, and B. Scho?lkopf, editors, NIPS 16,
pages 1229?1236.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner. 2000. Bilexical Grammars and Their Cubic-
Time Parsing Algorithms. In H. Bunt and A. Nijholt,
editors, Advances in Probabilistic and Other Parsing
Technologies, pages 29?62. Kluwer Academic Pub-
lishers.
Y. Freund and R. Schapire. 1999. Large Margin Clas-
sification Using the Perceptron Algorithm. Machine
Learning, 37(3):277?296.
J. Hajic?, E. Hajic?ova?, P. Pajas, J. Panevova, and P. Sgall.
2001. The Prague Dependency Treebank 1.0, LDC
No. LDC2001T10. Linguistics Data Consortium.
J. Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In
E. Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
K. Hall and V. Nova?k. 2005. Corrective Modeling for
Non-Projective Dependency Parsing. In Proceedings
of IWPT, pages 42?52.
T. Koo and M. Collins. 2005. Hidden-Variable Models
for Discriminative Reranking. In Proceedings of HLT-
EMNLP, pages 507?514.
W. Li and A. McCallum. 2005. Semi-Supervised Se-
quence Modeling with Syntactic Topic Models. In
Proceedings of AAAI, pages 813?818.
P. Liang. 2005. Semi-Supervised Learning for Natural
Language. Master?s thesis, Massachusetts Institute of
Technology.
M.P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with Latent Annotations. In Proceedings of
ACL, pages 75?82.
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective Self-Training for Parsing. In Proceedings of
HLT-NAACL, pages 152?159.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proceedings of EACL, pages 81?88.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line Large-Margin Training of Dependency Parsers. In
Proceedings of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-Projective Dependency Parsing using Spanning
Tree Algorithms. In Proceedings of HLT-EMNLP,
pages 523?530.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
Tagging with Word Clusters and Discriminative Train-
ing. In Proceedings of HLT-NAACL, pages 337?342.
J. Nivre and J. Nilsson. 2005. Pseudo-Projective Depen-
dency Parsing. In Proceedings of ACL, pages 99?106.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
Shared Task on Dependency Parsing. In Proceedings
of EMNLP-CoNLL 2007, pages 915?932.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of COLING-ACL, pages
433?440.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-Of-Speech Tagging. In Proceedings of EMNLP,
pages 133?142.
I. Titov and J. Henderson. 2007. Constituent Parsing
with Incremental Sigmoid Belief Networks. In Pro-
ceedings of ACL, pages 632?639.
Q.I. Wang, D. Schuurmans, and D. Lin. 2005. Strictly
Lexical Dependency Parsing. In Proceedings of IWPT,
pages 152?159.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis With Support Vector Machines. In
Proceedings of IWPT, pages 195?206.
603
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 976?984,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning Context-Dependent Mappings from Sentences to Logical Form
Luke S. Zettlemoyer and Michael Collins
MIT CSAIL
Cambridge, MA 02139
{lsz,mcollins}@csail.mit.com
Abstract
We consider the problem of learning
context-dependent mappings from sen-
tences to logical form. The training ex-
amples are sequences of sentences anno-
tated with lambda-calculus meaning rep-
resentations. We develop an algorithm that
maintains explicit, lambda-calculus repre-
sentations of salient discourse entities and
uses a context-dependent analysis pipeline
to recover logical forms. The method uses
a hidden-variable variant of the percep-
tion algorithm to learn a linear model used
to select the best analysis. Experiments
on context-dependent utterances from the
ATIS corpus show that the method recov-
ers fully correct logical forms with 83.7%
accuracy.
1 Introduction
Recently, researchers have developed algorithms
that learn to map natural language sentences to
representations of their underlying meaning (He
and Young, 2006; Wong and Mooney, 2007;
Zettlemoyer and Collins, 2005). For instance, a
training example might be:
Sent. 1: List flights to Boston on Friday night.
LF 1: ?x.flight(x) ? to(x, bos)
? day(x, fri) ? during(x, night)
Here the logical form (LF) is a lambda-calculus
expression defining a set of entities that are flights
to Boston departing on Friday night.
Most of this work has focused on analyzing sen-
tences in isolation. In this paper, we consider the
problem of learning to interpret sentences whose
underlying meanings can depend on the context in
which they appear. For example, consider an inter-
action where Sent. 1 is followed by the sentence:
Sent. 2: Show me the flights after 3pm.
LF 2: ?x.flight(x) ? to(x, bos)
?day(x, fri) ? depart(x) > 1500
In this case, the fact that Sent. 2 describes flights
to Boston on Friday must be determined based on
the context established by the first sentence.
We introduce a supervised, hidden-variable ap-
proach for learning to interpret sentences in con-
text. Each training example is a sequence of sen-
tences annotated with logical forms. Figure 1
shows excerpts from three training examples in the
ATIS corpus (Dahl et al, 1994).
For context-dependent analysis, we develop an
approach that maintains explicit, lambda-calculus
representations of salient discourse entities and
uses a two-stage pipeline to construct context-
dependent logical forms. The first stage uses
a probabilistic Combinatory Categorial Grammar
(CCG) parsing algorithm to produce a context-
independent, underspecified meaning representa-
tion. The second stage resolves this underspecified
meaning representation by making a sequence of
modifications to it that depend on the context pro-
vided by previous utterances.
In general, there are a large number of possi-
ble context-dependent analyses for each sentence.
To select the best one, we present a weighted lin-
ear model that is used to make a range of parsing
and context-resolution decisions. Since the train-
ing data contains only the final logical forms, we
model these intermediate decisions as hidden vari-
ables that must be estimated without explicit su-
pervision. We show that this model can be effec-
tively trained with a hidden-variable variant of the
perceptron algorithm.
In experiments on the ATIS DEC94 test set, the
approach recovers fully correct logical forms with
83.7% accuracy.
2 The Learning Problem
We assume access to a training set that consists of
n interactions D = ?I1, . . . , In?. The i?th interac-
tion Ii contains ni sentences,wi,1, . . . , wi,ni . Each
sentence wi,j is paired with a lambda-calculus ex-
976
Example #1:
(a) show me the flights from boston to philly
?x.flight(x) ? from(x, bos) ? to(x, phi)
(b) show me the ones that leave in the morning
?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning)
(c) what kind of plane is used on these flights
?y.?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning) ? aircraft(x) = y
Example #2:
(a) show me flights from milwaukee to orlando
?x.flight(x) ? from(x,mil) ? to(x, orl)
(b) cheapest
argmin(?x.flight(x) ? from(x,mil) ? to(x, orl),
?y.fare(y))
(c) departing wednesday after 5 o?clock
argmin(?x.flight(x) ? from(x,mil) ? to(x, orl)
? day(x,wed) ? depart(x) > 1700 ,
?y.fare(y))
Example #3:
(a) show me flights from pittsburgh to la thursday evening
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? during(x, evening)
(b) thursday afternoon
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? during(x, afternoon)
(c) thursday after 1700 hours
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? depart(x) > 1700
Figure 1: ATIS interaction excerpts.
pression zi,j specifying the target logical form.
Figure 1 contains example interactions.
The logical forms in the training set are repre-
sentations of each sentence?s underlying meaning.
In most cases, context (the previous utterances and
their interpretations) is required to recover the log-
ical form for a sentence. For instance, in Exam-
ple 1(b) in Figure 1, the sentence ?show me the
ones that leave in the morning? is paired with
?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning)
Some parts of this logical form (from(x, bos) and
to(x, phi)) depend on the context. They have to be
recovered from the previous logical forms.
At step j in interaction i, we define the con-
text ?zi,1, . . . , zi,j?1? to be the j ? 1 preceding
logical forms.1 Now, given the training data, we
can create training examples (xi,j , zi,j) for i =
1 . . . n, j = 1 . . . ni. Each xi,j is a sentence and
a context, xi,j = (wi,j , ?zi,1, . . . , zi,j?1?). Given
this set up, we have a supervised learning problem
with input xi,j and output zi,j .
1In general, the context could also include the previous
sentences wi,k for k < j. In our data, we never observed any
interactions where the choice of the correct logical form zi,j
depended on the words in the previous sentences.
3 Overview of Approach
In general, the mapping from a sentence and a con-
text to a logical form can be quite complex. In this
section, we present an overview of our learning
approach. We assume the learning algorithm has
access to:
? A training set D, defined in Section 2.
? A CCG lexicon.2 See Section 4 for an
overview of CCG. Each entry in the lexicon
pairs a word (or sequence of words), with
a CCG category specifying both the syntax
and semantics for that word. One example
CCG entry would pair flights with the cate-
gory N : ?x.flight(x).
Derivations A derivation for the j?th sentence
in an interaction takes as input a pair x = (wj , C),
where C = ?z1 . . . zj?1? is the current context. It
produces a logical form z. There are two stages:
? First, the sentence wj is parsed using
the CCG lexicon to form an intermediate,
context-independent logical form pi.
? Second, in a series of steps, pi is mapped to z.
These steps depend on the context C.
As one sketch of a derivation, consider how we
might analyze Example 1(b) in Figure 1. In this
case the sentence is ?show me the ones that leave
in the morning.? The CCG parser would produce
the following context-independent logical form:
?x.!?e, t?(x) ? during(x,morning)
The subexpression !?e, t? results directly from the
referential phrase the ones; we discuss this in more
detail in Section 4.2, but intuitively this subexpres-
sion specifies that a lambda-calculus expression of
type ?e, t? must be recovered from the context and
substituted in its place.
In the second (contextually dependent) stage of
the derivation, the expression
?x.flight(x) ? from(x, bos) ? to(x, phi)
is recovered from the context, and substituted for
the !?e, t? subexpression, producing the desired fi-
nal logical form, seen in Example 1(b).
2Developing algorithms that learn the CCG lexicon from
the data described in this paper is an important area for future
work. We could possibly extend algorithms that learn from
context-independent data (Zettlemoyer and Collins, 2005).
977
In addition to substitutions of this type, we will
also perform other types of context-dependent res-
olution steps, as described in Section 5.
In general, both of the stages of the derivation
involve considerable ambiguity ? there will be a
large number of possible context-independent log-
ical forms pi for wj and many ways of modifying
each pi to create a final logical form zj .
Learning We model the problem of selecting
the best derivation as a structured prediction prob-
lem (Johnson et al, 1999; Lafferty et al, 2001;
Collins, 2002; Taskar et al, 2004). We present
a linear model with features for both the parsing
and context resolution stages of the derivation. In
our setting, the choice of the context-independent
logical form pi and all of the steps that map pi to
the output z are hidden variables; these steps are
not annotated in the training data. To estimate the
parameters of the model, we use a hidden-variable
version of the perceptron algorithm. We use an ap-
proximate search procedure to find the best deriva-
tion both while training the model and while ap-
plying it to test examples.
Evaluation We evaluate the approach on se-
quences of sentences ?w1, . . . , wk?. For each wj ,
the algorithm constructs an output logical form zj
which is compared to a gold standard annotation to
check correctness. At step j, the context contains
the previous zi, for i < j, output by the system.
4 Context-independent Parsing
In this section, we first briefly review the CCG
parsing formalism. We then define a set of ex-
tensions that allow the parser to construct logical
forms containing references, such as the !?e, t? ex-
pression from the example derivation in Section 3.
4.1 Background: CCG
CCG is a lexicalized, mildly context-sensitive
parsing formalism that models a wide range of
linguistic phenomena (Steedman, 1996; Steed-
man, 2000). Parses are constructed by combining
lexical entries according to a small set of relatively
simple rules. For example, consider the lexicon
flights := N : ?x.flight(x)
to := (N\N)/NP : ?y.?f.?x.f(x) ? to(x, y)
boston := NP : boston
Each lexical entry consists of a word and a cat-
egory. Each category includes syntactic and se-
mantic content. For example, the first entry
pairs the word flights with the category N :
?x.flight(x). This category has syntactic typeN ,
and includes the lambda-calculus semantic expres-
sion ?x.flight(x). In general, syntactic types can
either be simple types such as N , NP , or S, or
can be more complex types that make use of slash
notation, for example (N\N)/NP .
CCG parses construct parse trees according to
a set of combinator rules. For example, consider
the functional application combinators:3
A/B : f B : g ? A : f(g) (>)
B : g A\B : f ? A : f(g) (<)
The first rule is used to combine a category with
syntactic type A/B with a category to the right
of syntactic type B to create a new category of
type A. It also constructs a new lambda-calculus
expression by applying the function f to the
expression g. The second rule handles arguments
to the left. Using these rules, we can parse the
following phrase:
flights to boston
N (N\N)/NP NP
?x.flight(x) ?y.?f.?x.f(x) ? to(x, y) boston
>
(N\N)
?f.?x.f(x) ? to(x, boston)
<
N
?x.flight(x) ? to(x, boston)
The top-most parse operations pair each word with
a corresponding category from the lexicon. The
later steps are labeled with the rule that was ap-
plied (?> for the first and ?< for the second).
4.2 Parsing with References
In this section, we extend the CCG parser to intro-
duce references. We use an exclamation point fol-
lowed by a type expression to specify references
in a logical form. For example, !e is a reference to
an entity and !?e, t? is a reference to a function. As
motivated in Section 3, we introduce these expres-
sions so they can later be replaced with appropriate
lambda-calculus expressions from the context.
Sometimes references are lexically triggered.
For example, consider parsing the phrase ?show
me the ones that leave in the morning? from Ex-
ample 1(b) in Figure 1. Given the lexical entry:
ones := N : ?x.!?e, t?(x)
a CCG parser could produce the desired context-
3In addition to application, we make use of composition,
type raising and coordination combinators. A full description
of these combinators is beyond the scope of this paper. Steed-
man (1996; 2000) presents a detailed description of CCG.
978
independent logical form:
?x.!?e, t?(x) ? during(x,morning)
Our first extension is to simply introduce lexical
items that include references into the CCG lexi-
con. They describe anaphoric words, for example
including ?ones,? ?those,? and ?it.?
In addition, we sometimes need to introduce
references when there is no explicit lexical trig-
ger. For instance, Example 2(c) in Figure 1 con-
sists of the single word ?cheapest.? This query has
the same meaning as the longer request ?show me
the cheapest one,? but it does not include the lex-
ical reference. We add three CCG type-shifting
rules to handle these cases.
The first two new rules are applicable when
there is a category that is expecting an argument
with type ?e, t?. This argument is replaced with a
!?e, t? reference:
A/B : f ? A : f(?x.!?e, t?(x))
A\B : f ? A : f(?x.!?e, t?(x))
For example, using the first rule, we could produce
the following parse for Example 2(c)
cheapest
NP/N
?g.argmin(?x.g(x), ?y.fare(y))
NP
argmin(?x.!?e, t?(x), ?y.fare(y))
where the final category has the desired lambda-
caculus expression.
The third rule is motivated by examples such as
?show me nonstop flights.? Consider this sentence
being uttered after Example 1(a) in Figure 1. Al-
though there is a complete, context-independent
meaning, the request actually restricts the salient
set of flights to include only the nonstop ones. To
achieve this analysis, we introduce the rule:
A : f ? A : ?x.f(x) ? !?e, t?(x)
where f is an function of type ?e, t?.
With this rule, we can construct the parse
nonstop flights
N/N N
?f.?x.f(x) ? nonstop(x) ?x.flight(x)
>
N
?x.nonstop(x) ? flight(x)
N
?x.nonstop(x) ? flight(x) ? !?e, t?(x)
where the last parsing step is achieved with the
new type-shifting rule.
These three new parsing rules allow significant
flexibility when introducing references. Later, we
develop an approach that learns when to introduce
references and how to best resolve them.
5 Contextual Analysis
In this section, we first introduce the general pat-
terns of context-dependent analysis that we con-
sider. We then formally define derivations that
model these phenomena.
5.1 Overview
This section presents an overview of the ways that
the context C is used during the analysis.
References Every reference expression (!e or
!?e, t?) must be replaced with an expression from
the context. For example, in Section 3, we consid-
ered the following logical form:
?x.!?e, t?(x) ? during(x,morning)
In this case, we saw that replacing the !?e, t?
subexpression with the logical form for Exam-
ple 1(a), which is directly available in C, produces
the desired final meaning.
Elaborations Later statements can expand the
meaning of previous ones in ways that are diffi-
cult to model with references. For example, con-
sider analyzing Example 2(c) in Figure 1. Here the
phrase ?departing wednesday after 5 o?clock? has
a context-independent logical form:4
?x.day(x,wed) ? depart(x) > 1700 (1)
that must be combined with the meaning of the
previous sentence from the context C:
argmin(?x.fight(x) ? from(x,mil) ? to(x, orl),
?y.fare(y))
to produce the expression
argmin(?x.fight(x) ? from(x,mil) ? to(x, orl)
?day(x,wed) ? depart(x) > 1700,
?y.fare(y))
Intuitively, the phrase ?departing wednesday af-
ter 5 o?clock? is providing new constraints for the
set of flights embedded in the argmin expression.
We handle examples of this type by construct-
ing elaboration expressions from the zi in C. For
example, if we constructed the following function:
?f.argmin(?x.fight(x) ? from(x,mil)
? to(x, orl) ? f(x), (2)
?y.fare(y))
4Another possible option is the expression ?x.!?e, t? ?
day(x,wed)? depart(x) > 1700. However, there is no ob-
vious way to resolve the !?e, t? expression that would produce
the desired final meaning.
979
we could apply this function to Expression 1 and
produce the desired result. The introduction of the
new variable f provides a mechanism for expand-
ing the embedded subexpression.
References with Deletion When resolving ref-
erences, we will sometimes need to delete subparts
of the expressions that we substitute from the con-
text. For instance, consider Example 3(b) in Fig-
ure 1. The desired, final logical form is:
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? during(x, afternoon)
We need to construct this from the context-
independent logical form:
?x.!?e, t? ? day(x, thur) ? during(x, afternoon)
The reference !?e, t? must be resolved. The only
expression in the context C is the meaning from
the previous sentence, Example 3(a):
?x.flight(x) ? from(x, pit) ? to(x, la) (3)
? day(x, thur) ? during(x, evening)
Substituting this expression directly would pro-
duce the following logical form:
?x.flight(x) ? from(x, pit) ? to(x, la)
? day(x, thur) ? during(x, evening)
? day(x, thur) ? during(x, afternoon)
which specifies the day twice and has two different
time spans.
We can achieve the desired analysis by deleting
parts of expressions before they are substituted.
For example, we could remove the day and time
constraints from Expression 3 to create:
?x.flight(x) ? from(x, pit) ? to(x, la)
which would produce the desired final meaning
when substituted into the original expression.
Elaborations with Deletion We also allow
deletions for elaborations. In this case, we delete
subexpressions of the elaboration expression that
is constructed from the context.
5.2 Derivations
We now formally define a derivation that maps a
sentence wj and a context C = {z1, . . . , zj?1} to
an output logical form zj . We first introduce no-
tation for expressions in C that we will use in the
derivation steps. We then present a definition of
deletion. Finally, we define complete derivations.
Context Sets Given a context C, our algorithm
constructs three sets of expressions:
? Re(C): A set of e-type expressions that can
be used to resolve references.
? R?e,t?(C): A set of ?e, t?-type expressions
that can be used to resolve references.
? E(C): A set of possible elaboration expres-
sions (for example, see Expression 2).
We will provide the details of how these sets
are defined in Section 5.3. As an example, if C
contains only the logical form
?x.flight(x) ? from(x, pit) ? to(x, la)
then Re(C) = {pit, la} and R?e,t?(C) is a set that
contains a single entry, the complete logical form.
Deletion A deletion operator accepts a logical
form l and produces a new logical form l?. It con-
structs l? by removing a single subexpression that
appears in a coordination (conjunction or disjunc-
tion) in l. For example, if l is
?x.flight(x) ? from(x, pit) ? to(x, la)
there are three possible deletion operations, each
of which removes a single subexpression.
Derivations We now formally define a deriva-
tion to be a sequence d = (?, s1, . . . , sm). ? is a
CCG parse that constructs a context-independent
logical form pi with m? 1 reference expressions.5
Each si is a function that accepts as input a logi-
cal form, makes some change to it, and produces a
new logical form that is input to the next function
si+1. The initial si for i < m are reference steps.
The final sm is an optional elaboration step.
? Reference Steps: A reference step is a tuple
(l, l?, f, r, r1, . . . , rp). This operator selects a
reference f in the input logical form l and
an appropriately typed expression r from ei-
ther Re(C) or R?e,t?(C). It then applies a se-
quence of p deletion operators to create new
expressions r1 . . . rp. Finally, it constructs
the output logical form l? by substituting rp
for the selected reference f in l.
? Elaboration Steps: An elaboration step is a
tuple (l, l?, b, b1, . . . , bq). This operator se-
lects an expression b from E(C) and ap-
plies q deletions to create new expressions
b1 . . . bq. The output expression l? is bq(l).
5In practice, pi rarely contains more than one reference.
980
In general, the space of possible derivations is
large. In Section 6, we describe a linear model
and decoding algorithm that we use to find high
scoring derivations.
5.3 Context Sets
For a context C = {z1, . . . , zj?1}, we define sets
Re(C), R?e,t?(C), and E(C) as follows.
e-type Expressions Re(z) is a set of e-type ex-
pressions extracted from a logical form z. We de-
fine Re(C) =
?j?1
i=1 Re(zi).
Re(z) includes all e-type subexpressions of z.6
For example, if z is
argmin(?x.flight(x) ? from(x,mil) ? to(x, orl),
?y.fare(y))
the resulting set isRe(z) = {mil, orl, z}, where z
is included because the entire argmin expression
has type e.
?e, t?-type Expressions R?e,t?(z) is a set of
?e, t?-type expressions extracted from a logical
form z. We define R?e,t?(C) =
?j?1
i=1 R?e,t?(zi).
The set R?e,t?(z) contains all of the ?e, t?-type
subexpressions of z. For each quantified vari-
able x in z, it also contains a function ?x.g. The
expression g contains the subexpressions in the
scope of x that do not have free variables. For
example, if z is
?y.?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning) ? aircraft(x) = y
R?e,t?(z) would contain two functions: the entire
expression z and the function
?x.flight(x) ? from(x, bos) ? to(x, phi)
? during(x,morning)
constructed from the variable x, where the subex-
pression aircraft(x) = y has been removed be-
cause it contains the free variable y.
Elaboration Expressions Finally, E(z) is a set
of elaboration expressions constructed from a log-
ical form z. We define E(C) =
?j?1
i=1 E(zi).
E(z) is defined by enumerating the places
where embedded variables are found in z. For
each logical variable x and each coordination
(conjunction or disjunction) in the scope of x, a
new expression is created by defining a function
?f.z? where z? has the function f(x) added to the
appropriate coordination. This procedure would
6A lambda-calculus expression can be represented as a
tree structure with flat branching for coordination (conjunc-
tion and disjunction). The subexpressions are the subtrees.
produce the example elaboration Expression 2 and
elaborations that expand other embedded expres-
sions, such as the quantifier in Example 1(c).
6 A Linear Model
In general, there will be many possible derivations
d for an input sentence w in the current context
C. In this section, we introduce a weighted lin-
ear model that scores derivations and a decoding
algorithm that finds high scoring analyses.
We define GEN(w;C) to be the set of possible
derivations d for an input sentence w given a con-
textC, as described in Section 5.2. Let ?(d) ? Rm
be an m-dimensional feature representation for a
derivation d and ? ? Rm be an m-dimensional pa-
rameter vector. The optimal derivation for a sen-
tence w given context C and parameters ? is
d?(w;C) = arg max
d?GEN(w;C)
? ? ?(d)
Decoding We now describe an approximate al-
gorithm for computing d?(w;C).
The CCG parser uses a CKY-style chart parsing
algorithm that prunes to the top N = 50 entries
for each span in the chart.
We use a beam search procedure to find the
best contextual derivations, with beam size N =
50. The beam is initialized to the top N logi-
cal forms from the CCG parser. The derivations
are extended with reference and elaboration steps.
The only complication is selecting the sequence of
deletions. For each possible step, we use a greedy
search procedure that selects the sequence of dele-
tions that would maximize the score of the deriva-
tion after the step is applied.
7 Learning
Figure 2 details the complete learning algorithm.
Training is online and error-driven. Step 1 parses
the current sentence in context. If the optimal logi-
cal form is not correct, Step 2 finds the best deriva-
tion that produces the labeled logical form7 and
does an additive, perceptron-style parameter up-
date. Step 3 updates the context. This algorithm is
a direct extension of the one introduced by Zettle-
moyer and Collins (2007). It maintains the context
but does not have the lexical induction step that
was previously used.
7For this computation, we use a modified version of the
beam search algorithm described in Section 6, which prunes
derivations that could not produce the desired logical form.
981
Inputs: Training examples {Ii|i = 1 . . . n}. Each Ii is a
sequence {(wi,j , zi,j) : j = 1 . . . ni} where wi,j is a
sentence and zi,j is a logical form. Number of training
iterations T . Initial parameters ?.
Definitions: The function ?(d) represents the features de-
scribed in Section 8. GEN(w;C) is the set of deriva-
tions for sentence w in context C. GEN(w, z;C) is
the set of derivations for sentence w in context C that
produce the final logical form z. The function L(d)
maps a derivation to its associated final logical form.
Algorithm:
? For t = 1 . . . T, i = 1 . . . n: (Iterate interactions)
? Set C = {}. (Reset context)
? For j = 1 . . . ni: (Iterate training examples)
Step 1: (Check correctness)
? Let d? = argmaxd?GEN(wi,j ;C) ? ? ?(d) .
? If L(d?) = zi,j , go to Step 3.
Step 2: (Update parameters)
? Let d? = argmaxd?GEN(wi,j ,zi,j ;C) ? ? ?(d) .
? Set ? = ? + ?(d?) ? ?(d?) .
Step 3: (Update context)
? Append zi,j to the current context C.
Output: Estimated parameters ?.
Figure 2: An online learning algorithm.
8 Features
We now describe the features for both the parsing
and context resolution stages of the derivation.
8.1 Parsing Features
The parsing features are used to score the context-
independent CCG parses during the first stage of
analysis. We use the set developed by Zettlemoyer
and Collins (2007), which includes features that
are sensitive to lexical choices and the structure of
the logical form that is constructed.
8.2 Context Features
The context features are functions of the deriva-
tion steps described in Section 5.2. In a deriva-
tion for sentence j of an interaction, let l be the
input logical form when considering a new step s
(a reference or elaboration step). Let c be the ex-
pression that s selects from a context set Re(zi),
R?e,t?(zi), or E(zi), where zi, i < j, is an ex-
pression in the current context. Also, let r be a
subexpression deleted from c. Finally, let f1 and
f2 be predicates, for example from or to.
Distance Features The distance features are bi-
nary indicators on the distance j ? i. These fea-
tures allow the model to, for example, favor re-
solving references with lambda-calculus expres-
sions recovered from recent sentences.
Copy Features For each possible f1 there is a
feature that tests if f1 is present in the context
expression c but not in the current expression l.
These features allow the model to learn to select
expressions from the context that introduce ex-
pected predicates. For example, flights usually
have a from predicate in the current expression.
Deletion Features For each pair (f1, f2) there
is a feature that tests if f1 is in the current expres-
sion l and f2 is in the deleted expression r. For
example, if f1 = f2 = days the model can favor
overriding old constraints about the departure day
with new ones introduced in the current utterance.
When f1 = during and f2 = depart time the
algorithm can learn that specific constraints on the
departure time override more general constraints
about the period of day.
9 Related Work
There has been a significant amount of work on
the problem of learning context-independent map-
pings from sentences to meaning representations.
Researchers have developed approaches using
models and algorithms from statistical machine
translation (Papineni et al, 1997; Ramaswamy
and Kleindienst, 2000; Wong and Mooney, 2007),
statistical parsing (Miller et al, 1996; Ge and
Mooney, 2005), inductive logic programming
(Zelle and Mooney, 1996; Tang and Mooney,
2000) and probabilistic push-down automata (He
and Young, 2006).
There were a large number of successful hand-
engineered systems developed for the original
ATIS task and other related tasks (e.g., (Carbonell
and Hayes, 1983; Seneff, 1992; Ward and Is-
sar, 1994; Levin et al, 2000; Popescu et al,
2004)). We are only aware of one system that
learns to construct context-dependent interpreta-
tions (Miller et al, 1996). The Miller et al (1996)
approach is fully supervised and produces a fi-
nal meaning representation in SQL. It requires
complete annotation of all of the syntactic, se-
mantic, and discourse decisions required to cor-
rectly analyze each training example. In contrast,
we learn from examples annotated with lambda-
calculus expressions that represent only the final,
context-dependent logical forms.
Finally, the CCG (Steedman, 1996; Steedman,
982
Train Dev. Test All
Interactions 300 99 127 526
Sentences 2956 857 826 4637
Table 1: Statistics of the ATIS training, development and
test (DEC94) sets, including the total number of interactions
and sentences. Each interaction is a sequence of sentences.
2000) parsing setup is closely related to previous
CCG research, including work on learning parsing
models (Clark and Curran, 2003), wide-coverage
semantic parsing (Bos et al, 2004) and grammar
induction (Watkinson and Manandhar, 1999).
10 Evaluation
Data In this section, we present experiments in
the context-dependent ATIS domain (Dahl et al,
1994). Table 1 presents statistics for the train-
ing, development, and test sets. To facilitate com-
parison with previous work, we used the standard
DEC94 test set. We randomly split the remaining
data to make training and development sets. We
manually converted the original SQL meaning an-
notations to lambda-calculus expressions.
Evaluation Metrics Miller et al (1996) report
accuracy rates for recovering correct SQL annota-
tions on the test set. For comparison, we report ex-
act accuracy rates for recovering completely cor-
rect lambda-calculus expressions.
We also present precision, recall and F-measure
for partial match results that test if individual at-
tributes, such as the from and to cities, are cor-
rectly assigned. See the discussion by Zettlemoyer
and Collins (2007) (ZC07) for the full details.
Initialization and Parameters The CCG lexi-
con is hand engineered. We constructed it by run-
ning the ZC07 algorithm to learn a lexicon on
the context-independent ATIS data set and making
manual corrections to improve performance on the
training set. We also added lexical items with ref-
erence expressions, as described in Section 4.
We ran the learning algorithm for T = 4 train-
ing iterations. The parsing feature weights were
initialized as in ZC07, the context distance fea-
tures were given small negative weights, and all
other feature weights were initially set to zero.
Test Setup During evaluation, the context C =
{z1 . . . zj?1} contains the logical forms output by
the learned system for the previous sentences. In
general, errors made while constructing these ex-
pressions can propogate if they are used in deriva-
tions for new sentences.
System
Partial Match Exact
Prec. Rec. F1 Acc.
Full Method 95.0 96.5 95.7 83.7
Miller et al ? ? ? 78.4
Table 2: Performance on the ATIS DEC94 test set.
Limited Context
Partial Match Exact
Prec. Rec. F1 Acc.
M = 0 96.2 57.3 71.8 45.4
M = 1 94.9 91.6 93.2 79.8
M = 2 94.8 93.2 94.0 81.0
M = 3 94.5 94.3 94.4 82.1
M = 4 94.9 92.9 93.9 81.6
M = 10 94.2 94.0 94.1 81.4
Table 3: Performance on the ATIS development set for
varying context window lengths M .
Results Table 2 shows performance on the ATIS
DEC94 test set. Our approach correctly recov-
ers 83.7% of the logical forms. This result com-
pares favorably to Miller et al?s fully-supervised
approach (1996) while requiring significantly less
annotation effort.
We also evaluated performance when the con-
text is limited to contain only the M most recent
logical forms. Table 3 shows results on the devel-
opment set for different values of M . The poor
performance with no context (M = 0) demon-
strates the need for context-dependent analysis.
Limiting the context to the most recent statement
(M = 1) significantly improves performance
while using the last three utterances (M = 3) pro-
vides the best results.
Finally, we evaluated a variation where the con-
text contains gold-standard logical forms during
evaluation instead of the output of the learned
model. On the development set, this approach
achieved 85.5% exact-match accuracy, an im-
provement of approximately 3% over the standard
approach. This result suggests that incorrect log-
ical forms in the context have a relatively limited
impact on overall performance.
11 Conclusion
In this paper, we addressed the problem of
learning context-dependent mappings from sen-
tences to logical form. We developed a context-
dependent analysis model and showed that it can
be effectively trained with a hidden-variable vari-
ant of the perceptron algorithm. In the experi-
ments, we showed that the approach recovers fully
correct logical forms with 83.7% accuracy.
983
References
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of the International Confer-
ence on Computational Linguistics.
Jaime G. Carbonell and Philip J. Hayes. 1983. Re-
covery strategies for parsing extragrammatical lan-
guage. American Journal of Computational Lin-
guistics, 9.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage CCG parsing. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Deborah A. Dahl, Madeleine Bates, Michael Brown,
William Fisher, Kate Hunicke-Smith, David Pallett,
Christine Pao, Alexander Rudnicky, and Elizabeth
Shriberg. 1994. Expanding the scope of the ATIS
task: the ATIS-3 corpus. In ARPA HLT Workshop.
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
tical semantic parser that integrates syntax and se-
mantics. In Proceedings of the Conference on Com-
putational Natural Language Learning.
Yulan He and Steve Young. 2006. Spoken language
understanding using the hidden vector state model.
Speech Communication, 48(3-4).
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Proc.
of the Association for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning.
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov,
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. Lee,
A. Pokrovsky, M. Rahim, P. Ruscitti, and M.Walker.
2000. The AT&T darpa communicator mixed-
initiative spoken dialogue system. In Proceedings of
the International Conference on Spoken Language
Processing.
Scott Miller, David Stallard, Robert J. Bobrow, and
Richard L. Schwartz. 1996. A fully statistical ap-
proach to natural language interfaces. In Proc. of
the Association for Computational Linguistics.
K. A. Papineni, S. Roukos, and T. R. Ward. 1997.
Feature-based language understanding. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology.
Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,
David Ko, and Alexander Yates. 2004. Modern
natural language interfaces to databases: Composing
statistical parsing with semantic tractability. In Pro-
ceedings of the International Conference on Compu-
tational Linguistics.
Ganesh N. Ramaswamy and Jan Kleindienst. 2000.
Hierarchical feature-based translation for scalable
natural language understanding. In Proceedings of
International Conference on Spoken Language Pro-
cessing.
Stephanie Seneff. 1992. Robust parsing for spoken
language systems. In Proc. of the IEEE Conference
on Acoustics, Speech, and Signal Processing.
Mark Steedman. 1996. Surface Structure and Inter-
pretation. The MIT Press.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press.
Lappoon R. Tang and Raymond J. Mooney. 2000.
Automated construction of database interfaces: In-
tegrating statistical and relational learning for se-
mantic parsing. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
margin parsing. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing.
Wayne Ward and Sunil Issar. 1994. Recent improve-
ments in the CMU spoken language understanding
system. In Proceedings of the workshop on Human
Language Technology.
Stephen Watkinson and Suresh Manandhar. 1999. Un-
supervised lexical learning with categorial gram-
mars using the LLL corpus. In Proceedings of the
1st Workshop on Learning Language in Logic.
Yuk Wah Wong and Raymond Mooney. 2007. Learn-
ing synchronous grammars for semantic parsing
with lambda calculus. In Proceedings of the Asso-
ciation for Computational Linguistics.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings of the National Con-
ference on Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the Conference on Un-
certainty in Artificial Intelligence.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proc. of the Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning.
984
Max-Margin Parsing
Ben Taskar
Computer Science Dept.
Stanford University
btaskar@cs.stanford.edu
Dan Klein
Computer Science Dept.
Stanford University
klein@cs.stanford.edu
Michael Collins
CS and AI Lab
MIT
mcollins@csail.mit.edu
Daphne Koller
Computer Science Dept.
Stanford University
koller@cs.stanford.edu
Christopher Manning
Computer Science Dept.
Stanford University
manning@cs.stanford.edu
Abstract
We present a novel discriminative approach to parsing
inspired by the large-margin criterion underlying sup-
port vector machines. Our formulation uses a factor-
ization analogous to the standard dynamic programs for
parsing. In particular, it allows one to efficiently learn
a model which discriminates among the entire space of
parse trees, as opposed to reranking the top few candi-
dates. Our models can condition on arbitrary features of
input sentences, thus incorporating an important kind of
lexical information without the added algorithmic com-
plexity of modeling headedness. We provide an efficient
algorithm for learning such models and show experimen-
tal evidence of the model?s improved performance over
a natural baseline model and a lexicalized probabilistic
context-free grammar.
1 Introduction
Recent work has shown that discriminative
techniques frequently achieve classification ac-
curacy that is superior to generative techniques,
over a wide range of tasks. The empirical utility
of models such as logistic regression and sup-
port vector machines (SVMs) in flat classifica-
tion tasks like text categorization, word-sense
disambiguation, and relevance routing has been
repeatedly demonstrated. For sequence tasks
like part-of-speech tagging or named-entity ex-
traction, recent top-performing systems have
also generally been based on discriminative se-
quence models, like conditional Markov mod-
els (Toutanova et al, 2003) or conditional ran-
dom fields (Lafferty et al, 2001).
A number of recent papers have consid-
ered discriminative approaches for natural lan-
guage parsing (Johnson et al, 1999; Collins,
2000; Johnson, 2001; Geman and Johnson,
2002; Miyao and Tsujii, 2002; Clark and Cur-
ran, 2004; Kaplan et al, 2004; Collins, 2004).
Broadly speaking, these approaches fall into two
categories, reranking and dynamic programming
approaches. In reranking methods (Johnson
et al, 1999; Collins, 2000; Shen et al, 2003),
an initial parser is used to generate a number
of candidate parses. A discriminative model
is then used to choose between these candi-
dates. In dynamic programming methods, a
large number of candidate parse trees are repre-
sented compactly in a parse tree forest or chart.
Given sufficiently ?local? features, the decod-
ing and parameter estimation problems can be
solved using dynamic programming algorithms.
For example, (Johnson, 2001; Geman and John-
son, 2002; Miyao and Tsujii, 2002; Clark and
Curran, 2004; Kaplan et al, 2004) describe ap-
proaches based on conditional log-linear (max-
imum entropy) models, where variants of the
inside-outside algorithm can be used to effi-
ciently calculate gradients of the log-likelihood
function, despite the exponential number of
trees represented by the parse forest.
In this paper, we describe a dynamic pro-
gramming approach to discriminative parsing
that is an alternative to maximum entropy
estimation. Our method extends the max-
margin approach of Taskar et al (2003) to
the case of context-free grammars. The present
method has several compelling advantages. Un-
like reranking methods, which consider only
a pre-pruned selection of ?good? parses, our
method is an end-to-end discriminative model
over the full space of parses. This distinction
can be very significant, as the set of n-best
parses often does not contain the true parse. For
example, in the work of Collins (2000), 41% of
the correct parses were not in the candidate pool
of ?30-best parses. Unlike previous dynamic
programming approaches, which were based on
maximum entropy estimation, our method in-
corporates an articulated loss function which
penalizes larger tree discrepancies more severely
than smaller ones.1
Moreover, like perceptron-based learning, it
requires only the calculation of Viterbi trees,
rather than expectations over all trees (for ex-
ample using the inside-outside algorithm). In
practice, it converges in many fewer iterations
than CRF-like approaches. For example, while
our approach generally converged in 20-30 iter-
ations, Clark and Curran (2004) report exper-
iments involving 479 iterations of training for
one model, and 1550 iterations for another.
The primary contribution of this paper is the
extension of the max-margin approach of Taskar
et al (2003) to context free grammars. We
show that this framework allows high-accuracy
parsing in cubic time by exploiting novel kinds
of lexical information.
2 Discriminative Parsing
In the discriminative parsing task, we want to
learn a function f : X ? Y, where X is a set
of sentences, and Y is a set of valid parse trees
according to a fixed grammar G. G maps an
input x ? X to a set of candidate parses G(x) ?
Y.2
We assume a loss function L : X ? Y ?
Y ? R+. The function L(x, y, y?) measures the
penalty for proposing the parse y? for x when y
is the true parse. This penalty may be defined,
for example, as the number of labeled spans on
which the two trees do not agree. In general we
assume that L(x, y, y?) = 0 for y = y?. Given
labeled training examples (xi, yi) for i = 1 . . . n,
we seek a function f with small expected loss
on unseen sentences.
The functions we consider take the following
linear discriminant form:
fw(x) = arg max
y?G(x)
?w,?(x, y)?,
1This articulated loss is supported by empirical suc-
cess and theoretical generalization bound in Taskar et al
(2003).
2For all x, we assume here that G(x) is finite. The
space of parse trees over many grammars is naturally in-
finite, but can be made finite if we disallow unary chains
and empty productions.
where ??, ?? denotes the vector inner product,
w ? Rd and ? is a feature-vector representation
of a parse tree ? : X ? Y ? Rd (see examples
below).3
Note that this class of functions includes
Viterbi PCFG parsers, where the feature-vector
consists of the counts of the productions used
in the parse, and the parameters w are the log-
probabilities of those productions.
2.1 Probabilistic Estimation
The traditional method of estimating the pa-
rameters of PCFGs assumes a generative gram-
mar that defines P (x, y) and maximizes the
joint log-likelihood ?i log P (xi, yi) (with some
regularization). A alternative probabilistic
approach is to estimate the parameters dis-
criminatively by maximizing conditional log-
likelihood. For example, the maximum entropy
approach (Johnson, 2001) defines a conditional
log-linear model:
Pw(y | x) =
1
Zw(x)
exp{?w,?(x, y)?},
where Zw(x) =
?
y?G(x) exp{?w,?(x, y)?}, and
maximizes the conditional log-likelihood of the
sample, ?i log P (yi | xi), (with some regular-
ization).
2.2 Max-Margin Estimation
In this paper, we advocate a different estima-
tion criterion, inspired by the max-margin prin-
ciple of SVMs. Max-margin estimation has been
used for parse reranking (Collins, 2000). Re-
cently, it has also been extended to graphical
models (Taskar et al, 2003; Altun et al, 2003)
and shown to outperform the standard max-
likelihood methods. The main idea is to forego
the probabilistic interpretation, and directly en-
sure that
yi = arg max
y?G(xi)
?w,?(xi, y)?,
for all i in the training data. We define the
margin of the parameters w on the example i
and parse y as the difference in value between
the true parse yi and y:
?w,?(xi, yi)? ? ?w,?(xi, y)? = ?w,?i,yi ??i,y?,
3Note that in the case that two members y1 and y2
have the same tied value for ?w,?(x, y)?, we assume that
there is some fixed, deterministic way for breaking ties.
For example, one approach would be to assume some
default ordering on the members of Y.
where ?i,y = ?(xi, y), and ?i,yi = ?(xi, yi). In-
tuitively, the size of the margin quantifies the
confidence in rejecting the mistaken parse y us-
ing the function fw(x), modulo the scale of the
parameters ||w||. We would like this rejection
confidence to be larger when the mistake y is
more severe, i.e. L(xi, yi, y) is large. We can ex-
press this desideratum as an optimization prob-
lem:
max ? (1)
s.t. ?w,?i,yi ? ?i,y? ? ?Li,y ?y ? G(xi);
||w||2 ? 1,
where Li,y = L(xi, yi, y). This quadratic pro-
gram aims to separate each y ? G(xi) from
the target parse yi by a margin that is propor-
tional to the loss L(xi, yi, y). After a standard
transformation, in which maximizing the mar-
gin is reformulated as minimizing the scale of
the weights (for a fixed margin of 1), we get the
following program:
min 12?w?
2 + C
?
i
?i (2)
s.t. ?w,?i,yi ? ?i,y? ? Li,y ? ?i ?y ? G(xi).
The addition of non-negative slack variables ?i
allows one to increase the global margin by pay-
ing a local penalty on some outlying examples.
The constant C dictates the desired trade-off
between margin size and outliers. Note that this
formulation has an exponential number of con-
straints, one for each possible parse y for each
sentence i. We address this issue in section 4.
2.3 The Max-Margin Dual
In SVMs, the optimization problem is solved by
working with the dual of a quadratic program
analogous to Eq. 2. For our problem, just as for
SVMs, the dual has important computational
advantages, including the ?kernel trick,? which
allows the efficient use of high-dimensional fea-
tures spaces endowed with efficient dot products
(Cristianini and Shawe-Taylor, 2000). More-
over, the dual view plays a crucial role in cir-
cumventing the exponential size of the primal
problem.
In Eq. 2, there is a constraint for each mistake
y one might make on each example i, which rules
out that mistake. For each mistake-exclusion
constraint, the dual contains a variable ?i,y. In-
tuitively, the magnitude of ?i,y is proportional
to the attention we must pay to that mistake in
order not to make it.
The dual of Eq. 2 (after adding additional
variables ?i,yi and renormalizing by C) is given
by:
max C
?
i,y
?i,yLi,y ?
1
2
?
?
?
?
?
?
?
?
?
?
?
?
C
?
i,y
(Ii,y ? ?i,y)?i,y
?
?
?
?
?
?
?
?
?
?
?
?
2
s.t.
?
y
?i,y = 1, ?i; ?i,y ? 0, ?i, y, (3)
where Ii,y = I(xi, yi, y) indicates whether y is
the true parse yi. Given the dual solution ??,
the solution to the primal problem w? is sim-
ply a weighted linear combination of the feature
vectors of the correct parse and mistaken parses:
w? = C
?
i,y
(Ii,y ? ??i,y)?i,y.
This is the precise sense in which mistakes with
large ? contribute more strongly to the model.
3 Factored Models
There is a major problem with both the pri-
mal and the dual formulations above: since each
potential mistake must be ruled out, the num-
ber of variables or constraints is proportional to
|G(x)|, the number of possible parse trees. Even
in grammars without unary chains or empty el-
ements, the number of parses is generally ex-
ponential in the length of the sentence, so we
cannot expect to solve the above problem with-
out any assumptions about the feature-vector
representation ? and loss function L.
For that matter, for arbitrary representa-
tions, to find the best parse given a weight vec-
tor, we would have no choice but to enumerate
all trees and score them. However, our gram-
mars and representations are generally struc-
tured to enable efficient inference. For exam-
ple, we usually assign scores to local parts of
the parse such as PCFG productions. Such
factored models have shared substructure prop-
erties which permit dynamic programming de-
compositions. In this section, we describe how
this kind of decomposition can be done over the
dual ? distributions. The idea of this decom-
position has previously been used for sequences
and other Markov random fields in Taskar et
al. (2003), but the present extension to CFGs
is novel.
For clarity of presentation, we restrict the
grammar to be in Chomsky normal form (CNF),
where all rules in the grammar are of the form
?A ? B C? or ?A ? a?, where A,B and C are
SNP
DT
The
NN
screen
VP
VBD
was
NP
NP
DT
a
NN
sea
PP
IN
of
NP
NN
red
0
1
2
3
4
5
6
0 1 2 3 4 5 6 7
DT
NN
VBD
DT
NN
IN
NN
NP
NP
PP
VP
S
NP
r = ?NP, 3, 5?
q = ?S ? NP VP, 0, 2, 7?
(a) (b)
Figure 1: Two representations of a binary parse tree: (a) nested tree structure, and (b) grid of labeled spans.
non-terminal symbols, and a is some terminal
symbol. For example figure 1(a) shows a tree
in this form.
We will represent each parse as a set of two
types of parts. Parts of the first type are sin-
gle constituent tuples ?A, s, e, i?, consisting of
a non-terminal A, start-point s and end-point
e, and sentence i, such as r in figure 1(b). In
this representation, indices s and e refer to po-
sitions between words, rather than to words
themselves. These parts correspond to the tra-
ditional notion of an edge in a tabular parser.
Parts of the second type consist of CF-rule-
tuples ?A ? B C, s,m, e, i?. The tuple specifies
a particular rule A ? B C, and its position,
including split point m, within the sentence i,
such as q in figure 1(b), and corresponds to the
traditional notion of a traversal in a tabular
parser. Note that parts for a basic PCFG model
are not just rewrites (which can occur multiple
times), but rather anchored items.
Formally, we assume some countable set of
parts, R. We also assume a function R which
maps each object (x, y) ? X ? Y to a finite
subset of R. Thus R(x, y) is the set of parts be-
longing to a particular parse. Equivalently, the
function R(x, y) maps a derivation y to the set
of parts which it includes. Because all rules are
in binary-branching form, |R(x, y)| is constant
across different derivations y for the same input
sentence x. We assume that the feature vector
for a sentence and parse tree (x, y) decomposes
into a sum of the feature vectors for its parts:
?(x, y) =
?
r?R(x,y)
?(x, r).
In CFGs, the function ?(x, r) can be any func-
tion mapping a rule production and its posi-
tion in the sentence x, to some feature vector
representation. For example, ? could include
features which identify the rule used in the pro-
duction, or features which track the rule iden-
tity together with features of the words at po-
sitions s,m, e, and neighboring positions in the
sentence x.
In addition, we assume that the loss function
L(x, y, y?) also decomposes into a sum of local
loss functions l(x, y, r) over parts, as follows:
L(x, y, y?) =
?
r?R(x,y?)
l(x, y, r).
One approach would be to define l(x, y, r) to
be 0 only if the non-terminal A spans words
s . . . e in the derivation y and 1 otherwise. This
would lead to L(x, y, y?) tracking the number of
?constituent errors? in y?, where a constituent is
a tuple such as ?A, s, e, i?. Another, more strict
definition would be to define l(x, y, r) to be 0
if r of the type ?A ? B C, s,m, e, i? is in the
derivation y and 1 otherwise. This definition
would lead to L(x, y, y?) being the number of CF-
rule-tuples in y? which are not seen in y.4
Finally, we define indicator variables I(x, y, r)
which are 1 if r ? R(x, y), 0 otherwise. We
also define sets R(xi) = ?y?G(xi)R(xi, y) for the
training examples i = 1 . . . n. Thus, R(xi) is
the set of parts that is seen in at least one of
the objects {(xi, y) : y ? G(xi)}.
4 Factored Dual
The dual in Eq. 3 involves variables ?i,y for
all i = 1 . . . n, y ? G(xi), and the objec-
tive is quadratic in these ? variables. In addi-
tion, it turns out that the set of dual variables
?i = {?i,y : y ? G(xi)} for each example i is
constrained to be non-negative and sum to 1.
It is interesting that, while the parameters w
lose their probabilistic interpretation, the dual
variables ?i for each sentence actually form a
kind of probability distribution. Furthermore,
the objective can be expressed in terms of ex-
pectations with respect to these distributions:
C
?
i
E?i [Li,y]?
1
2
?
?
?
?
?
?
?
?
?
?
C
?
i
?i,yi ?E?i [?i,y]
?
?
?
?
?
?
?
?
?
?
2
.
We now consider how to efficiently solve
the max-margin optimization problem for a
factored model. As shown in Taskar et al
(2003), the dual in Eq. 3 can be reframed using
?marginal? terms. We will also find it useful to
consider this alternative formulation of the dual.
Given dual variables ?, we define the marginals
?i,r(?) for all i, r, as follows:
?i,r(?i) =
?
y
?i,yI(xi, y, r) = E?i [I(xi, y, r)] .
Since the dual variables ?i form probability dis-
tributions over parse trees for each sentence i,
the marginals ?i,r(?i) represent the proportion
of parses that would contain part r if they were
drawn from a distribution ?i. Note that the
number of such marginal terms is the number
of parts, which is polynomial in the length of
the sentence.
Now consider the dual objective Q(?) in
Eq. 3. It can be shown that the original ob-
jective Q(?) can be expressed in terms of these
4The constituent loss function does not exactly cor-
respond to the standard scoring metrics, such as F1 or
crossing brackets, but shares the sensitivity to the num-
ber of differences between trees. We have not thoroughly
investigated the exact interplay between the various loss
choices and the various parsing metrics. We used the
constituent loss in our experiments.
marginals as Qm(?(?)), where ?(?) is the vector
with components ?i,r(?i), and Qm(?) is defined
as:
C
?
i,r?R(xi)
?i,rli,r ?
1
2
?
?
?
?
?
?
?
?
?
?
?
?
C
?
i,r?R(xi)
(Ii,r ? ?i,r)?i,r
?
?
?
?
?
?
?
?
?
?
?
?
2
where li,r = l(xi, yi, r), ?i,r = ?(xi, r) and Ii,r =
I(xi, yi, r).
This follows from substituting the factored
definitions of the feature representation ? and
loss function L together with definition of
marginals.
Having expressed the objective in terms of a
polynomial number of variables, we now turn to
the constraints on these variables. The feasible
set for ? is
? = {? : ?i,y ? 0, ?i, y
?
y
?i,y = 1, ?i}.
Now let ?m be the space of marginal vectors
which are feasible:
?m = {? : ?? ? ? s.t. ? = ?(?)}.
Then our original optimization problem can be
reframed as max???m Qm(?).
Fortunately, in case of PCFGs, the domain
?m can be described compactly with a polyno-
mial number of linear constraints. Essentially,
we need to enforce the condition that the ex-
pected proportions of parses having particular
parts should be consistent with each other. Our
marginals track constituent parts ?A, s, e, i? and
CF-rule-tuple parts ?A ? B C, s,m, e, i? The
consistency constraints are precisely the inside-
outside probability relations:
?i,A,s,e =
?
B,C
s<m<e
?i,A?B C,s,m,e
and
?i,A,s,e =
?
B,C
e<m?ni
?i,B?AC +
?
B,C
0?m<s
?i,B?CA
where ni is the length of the sentence. In ad-
dition, we must ensure non-negativity and nor-
malization to 1:
?i,r ? 0;
?
A
?i,A,0,ni = 1.
The number of variables in our factored dual
for CFGs is cubic in the length of the sentence,
Model P R F1
GENERATIVE 87.70 88.06 87.88
BASIC 87.51 88.44 87.98
LEXICAL 88.15 88.62 88.39
LEXICAL+AUX 89.74 90.22 89.98
Figure 2: Development set results of the various
models when trained and tested on Penn treebank
sentences of length ? 15.
Model P R F1
GENERATIVE 88.25 87.73 87.99
BASIC 88.08 88.31 88.20
LEXICAL 88.55 88.34 88.44
LEXICAL+AUX 89.14 89.10 89.12
COLLINS 99 89.18 88.20 88.69
Figure 3: Test set results of the various models when
trained and tested on Penn treebank sentences of
length ? 15.
while the number of constraints is quadratic.
This polynomial size formulation should be con-
trasted with the earlier formulation in Collins
(2004), which has an exponential number of
constraints.
5 Factored SMO
We have reduced the problem to a polynomial
size QP, which, in principle, can be solved us-
ing standard QP toolkits. However, although
the number of variables and constraints in the
factored dual is polynomial in the size of the
data, the number of coefficients in the quadratic
term in the objective is very large: quadratic in
the number of sentences and dependent on the
sixth power of sentence length. Hence, in our
experiments we use an online coordinate descent
method analogous to the sequential minimal op-
timization (SMO) used for SVMs (Platt, 1999)
and adapted to structured max-margin estima-
tion in Taskar et al (2003).
We omit the details of the structured SMO
procedure, but the important fact about this
kind of training is that, similar to the basic per-
ceptron approach, it only requires picking up
sentences one at a time, checking what the best
parse is according to the current primal and
dual weights, and adjusting the weights.
6 Results
We used the Penn English Treebank for all of
our experiments. We report results here for
each model and setting trained and tested on
only the sentences of length ? 15 words. Aside
from the length restriction, we used the stan-
dard splits: sections 2-21 for training (9753 sen-
tences), 22 for development (603 sentences), and
23 for final testing (421 sentences).
As a baseline, we trained a CNF transforma-
tion of the unlexicalized model of Klein and
Manning (2003) on this data. The resulting
grammar had 3975 non-terminal symbols and
contained two kinds of productions: binary non-
terminal rewrites and tag-word rewrites.5 The
scores for the binary rewrites were estimated us-
ing unsmoothed relative frequency estimators.
The tagging rewrites were estimated with a
smoothed model of P (w|t), also using the model
from Klein and Manning (2003). Figure 3 shows
the performance of this model (generative):
87.99 F1 on the test set.
For the basic max-margin model, we used
exactly the same set of allowed rewrites (and
therefore the same set of candidate parses) as in
the generative case, but estimated their weights
according to the discriminative method of sec-
tion 4. Tag-word production weights were fixed
to be the log of the generative P (w|t) model.
That is, the only change between genera-
tive and basic is the use of the discriminative
maximum-margin criterion in place of the gen-
erative maximum likelihood one. This change
alone results in a small improvement (88.20 vs.
87.99 F1).
On top of the basic model, we first added lex-
ical features of each span; this gave a lexical
model. For a span ?s, e? of a sentence x, the
base lexical features were:
? xs, the first word in the span
? xs?1, the preceding adjacent word
? xe?1, the last word in the span
? xe, the following adjacent word
? ?xs?1, xs?
? ?xe?1, xe?
? xs+1 for spans of length 3
These base features were conjoined with the
span length for spans of length 3 and below,
since short spans have highly distinct behaviors
(see the examples below). The features are lex-
ical in the sense than they allow specific words
5Unary rewrites were compiled into a single com-
pound symbol, so for example a subject-gapped sentence
would have label like s+vp. These symbols were ex-
panded back into their source unary chain before parses
were evaluated.
and word pairs to influence the parse scores, but
are distinct from traditional lexical features in
several ways. First, there is no notion of head-
word here, nor is there any modeling of word-to-
word attachment. Rather, these features pick
up on lexical trends in constituent boundaries,
for example the trend that in the sentence The
screen was a sea of red., the (length 2) span
between the word was and the word of is un-
likely to be a constituent. These non-head lex-
ical features capture a potentially very differ-
ent source of constraint on tree structures than
head-argument pairs, one having to do more
with linear syntactic preferences than lexical
selection. Regardless of the relative merit of
the two kinds of information, one clear advan-
tage of the present approach is that inference in
the resulting model remains cubic, since the dy-
namic program need not track items with distin-
guished headwords. With the addition of these
features, the accuracy jumped past the genera-
tive baseline, to 88.44.
As a concrete (and particularly clean) exam-
ple of how these features can sway a decision,
consider the sentence The Egyptian president
said he would visit Libya today to resume the
talks. The generative model incorrectly consid-
ers Libya today to be a base np. However, this
analysis is counter to the trend of today being a
one-word constituent. Two features relevant to
this trend are: (constituent ? first-word =
today ? length = 1) and (constituent ? last-
word = today ? length = 1). These features rep-
resent the preference of the word today for being
the first and and last word in constituent spans
of length 1.6 In the lexical model, however,
these features have quite large positive weights:
0.62 each. As a result, this model makes this
parse decision correctly.
Another kind of feature that can usefully be
incorporated into the classification process is
the output of other, auxiliary classifiers. For
this kind of feature, one must take care that its
reliability on the training not be vastly greater
than its reliability on the test set. Otherwise,
its weight will be artificially (and detrimentally)
high. To ensure that such features are as noisy
on the training data as the test data, we split
the training into two folds. We then trained the
auxiliary classifiers in jacknife fashion on each
6In this length 1 case, these are the same feature.
Note also that the features are conjoined with only one
generic label class ?constituent? rather than specific con-
stituent types.
fold, and using their predictions as features on
the other fold. The auxiliary classifiers were
then retrained on the entire training set, and
their predictions used as features on the devel-
opment and test sets.
We used two such auxiliary classifiers, giving
a prediction feature for each span (these classi-
fiers predicted only the presence or absence of a
bracket over that span, not bracket labels). The
first feature was the prediction of the genera-
tive baseline; this feature added little informa-
tion, but made the learning phase faster. The
second feature was the output of a flat classi-
fier which was trained to predict whether sin-
gle spans, in isolation, were constituents or not,
based on a bundle of features including the list
above, but also the following: the preceding,
first, last, and following tag in the span, pairs
of tags such as preceding-first, last-following,
preceding-following, first-last, and the entire tag
sequence.
Tag features on the test sets were taken from
a pretagging of the sentence by the tagger de-
scribed in Toutanova et al (2003). While the
flat classifier alone was quite poor (P 78.77 /
R 63.94 / F1 70.58), the resulting max-margin
model (lexical+aux) scored 89.12 F1. To sit-
uate these numbers with respect to other mod-
els, the parser in Collins (1999), which is genera-
tive, lexicalized, and intricately smoothed scores
88.69 over the same train/test configuration.
It is worth considering the cost of this kind of
method. At training time, discriminative meth-
ods are inherently expensive, since they all in-
volve iteratively checking current model perfor-
mance on the training set, which means parsing
the training set (usually many times). In our
experiments, 10-20 iterations were generally re-
quired for convergence (except the basic model,
which took about 100 iterations.) There are
several nice aspects of the approach described
here. First, it is driven by the repeated extrac-
tion, over the training examples, of incorrect
parses which the model currently prefers over
the true parses. The procedure that provides
these parses need not sum over all parses, nor
even necessarily find the Viterbi parses, to func-
tion. This allows a range of optimizations not
possible for CRF-like approaches which must
extract feature expectations from the entire set
of parses.7 Nonetheless, generative approaches
7One tradeoff is that this approach is more inherently
sequential and harder to parallelize.
are vastly cheaper to train, since they must only
collect counts from the training set.
On the other hand, the max-margin approach
does have the potential to incorporate many
new kinds of features over the input, and the
current feature set alows limited lexicalization
in cubic time, unlike other lexicalized models
(including the Collins model which it outper-
forms in the present limited experiments).
7 Conclusion
We have presented a maximum-margin ap-
proach to parsing, which allows a discriminative
SVM-like objective to be applied to the parsing
problem. Our framework permits the use of a
rich variety of input features, while still decom-
posing in a way that exploits the shared sub-
structure of parse trees in the standard way. On
a test set of ? 15 word sentences, the feature-
rich model outperforms both its own natural
generative baseline and the Collins parser on
F1. While like most discriminative models it is
compute-intensive to train, it allows fast pars-
ing, remaining cubic despite the incorporation
of lexical features. This trade-off between the
complexity, accuracy and efficiency of a parsing
model is an important area of future research.
Acknowledgements
This work was supported in part by the Depart-
ment of the Interior/DARPA under contract
number NBCHD030010, a Microsoft Graduate
Fellowship to the second author, and National
Science Foundation grant 0347631 to the third
author.
References
Y. Altun, I. Tsochantaridis, and T. Hofmann.
2003. Hidden markov support vector ma-
chines. In Proc. ICML.
S. Clark and J. R. Curran. 2004. Parsing
the wsj using ccg and log-linear models. In
Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguis-
tics (ACL ?04).
M. Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. the-
sis, University of Pennsylvania.
M. Collins. 2000. Discriminative reranking for
natural language parsing. In ICML 17, pages
175?182.
M. Collins. 2004. Parameter estimation for sta-
tistical parsing models: Theory and practice
of distribution-free methods. In Harry Bunt,
John Carroll, and Giorgio Satta, editors, New
Developments in Parsing Technology. Kluwer.
N. Cristianini and J. Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and
Other Kernel-Based Learning Methods. Cam-
bridge University Press.
S. Geman and M. Johnson. 2002. Dynamic
programming for parsing and estimation of
stochastic unification-based grammars. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics.
M. Johnson, S. Geman, S. Canon, Z. Chi, and
S. Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceed-
ings of ACL 1999.
M. Johnson. 2001. Joint and conditional es-
timation of tagging and parsing models. In
ACL 39.
R. Kaplan, S. Riezler, T. King, J. Maxwell,
A. Vasserman, and R. Crouch. 2004. Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of HLT-NAACL?04).
D. Klein and C. D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 41, pages 423?
430.
J. Lafferty, A. McCallum, and F. Pereira.
2001. Conditional random fields: Probabi-
listic models for segmenting and labeling se-
quence data. In ICML.
Y. Miyao and J. Tsujii. 2002. Maximum
entropy estimation for feature forests. In
Proceedings of Human Language Technology
Conference (HLT 2002).
J. Platt. 1999. Using sparseness and analytic
QP to speed training of support vector ma-
chines. In NIPS.
L. Shen, A. Sarkar, and A. K. Joshi. 2003. Us-
ing ltag based features in parse reranking. In
Proc. EMNLP.
B. Taskar, C. Guestrin, and D. Koller. 2003.
Max margin Markov networks. In NIPS.
K. Toutanova, D. Klein, C. D. Manning, and
Y. Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In
NAACL 3, pages 252?259.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 232?241,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Discriminative Model for Tree-to-Tree Translation
Brooke Cowan
MIT CSAIL
brooke@csail.mit.edu
Ivona Kuc?erova?
MIT Linguistics Department
kucerova@mit.edu
Michael Collins
MIT CSAIL
mcollins@csail.mit.edu
Abstract
This paper proposes a statistical, tree-
to-tree model for producing translations.
Two main contributions are as follows:
(1) a method for the extraction of syn-
tactic structures with alignment informa-
tion from a parallel corpus of translations,
and (2) use of a discriminative, feature-
based model for prediction of these target-
language syntactic structures?which we
call aligned extended projections, or
AEPs. An evaluation of the method on
translation from German to English shows
similar performance to the phrase-based
model of Koehn et al (2003).
1 Introduction
Phrase-based approaches (Och and Ney, 2004)
to statistical machine translation (SMT) have re-
cently achieved impressive results, leading to sig-
nificant improvements in accuracy over the origi-
nal IBM models (Brown et al, 1993). However,
phrase-based models lack a direct representation
of syntactic information in the source or target lan-
guages; this has prompted several researchers to
consider various approaches that make use of syn-
tactic information.
This paper describes a framework for tree-to-
tree based statistical translation. Our goal is to
learn a model that maps parse trees in the source
language to parse trees in the target language.
The model is learned from a corpus of transla-
tion pairs, where each sentence in the source or
target language has an associated parse tree. We
see two major benefits of tree-to-tree based trans-
lation. First, it is possible to explicitly model the
syntax of the target language, thereby improving
grammaticality. Second, we can build a detailed
model of the correspondence between the source
and target parse trees, with the aim of constructing
translations that preserve the meaning of source
language sentences.
Our translation framework involves a process
where the target-language parse tree is broken
down into a sequence of clauses, and each clause
is then translated separately. A central concept we
introduce in the translation of clauses is that of an
aligned extended projection (AEP). AEPs are de-
rived from the concept of an extended projection
in lexicalized tree adjoining grammars (LTAG)
(Frank, 2002), with the addition of alignment in-
formation that is based on work in synchronous
LTAG (Shieber and Schabes, 1990). A key con-
tribution of this paper is a method for learning
to map German clauses to AEPs using a feature-
based model with a perceptron learning algorithm.
We performed experiments on translation from
German to English on the Europarl data set. Eval-
uation in terms of both BLEU scores and human
judgments shows that our system performs sim-
ilarly to the phrase-based model of Koehn et al
(2003).
1.1 A Sketch of the Approach
This section provides an overview of the transla-
tion process. We will use the German sentence wir
wissen da? das haupthemmnis der vorhersehbare
widerstand der hersteller war as a running exam-
ple. For this example we take the desired transla-
tion to be we know that the main obstacle has been
the predictable resistance of manufacturers.
Translation of a German sentence proceeds in
the following four steps:
Step 1: The German sentence is parsed and then
broken down into separate parse structures for a
sequence of clauses. For example, the German ex-
ample above is broken into a parse structure for
the clause wir wissen followed by a parse struc-
ture for the subordinate clause da?. . .war. Each
of these clauses is then translated separately, using
steps 2?3 below.
Step 2: An aligned extended projection (AEP)
is predicted for each German clause. To illustrate
this step, consider translation of the second Ger-
man clause, which has the following parse struc-
ture:
232
s-oc kous-cp da?
np-sb 1 art das
nn haupthemmnis
np-pd 2 art der
adja vorhersehbare
nn widerstand
np-ag art der
nn hersteller
vafin-hd war
Note that we use the symbols 1 and 2 to identify
the two modifiers (arguments or adjuncts) in the
clause, in this case a subject and an object.
A major part of the AEP is a parse-tree frag-
ment, that is similar to a TAG elementary tree (see
also Figure 2):
SBAR
that S
NP VP
V
has
VP
V
been
NP
Following the work of Frank (2002), we will refer
to a structure like this as an extended projection
(EP). The EP encapsulates the core syntactic struc-
ture in the English clause. It contains the main
verb been, as well as the function words that and
has. It also contains a parse tree ?spine? which has
the main verb been as one of its leaves, and has the
clause label SBAR as its root. In addition, it spec-
ifies positions for arguments in the clause?in this
case NPs corresponding to the subject and object.
An AEP contains an EP, as well as alignment
information about where the German modifiers
should be placed in the extended projection. For
example, the AEP in this case would contain the
tree fragment shown above, together with an align-
ment specifying that the modifiers 1 and 2 from
the German parse will appear in the EP as subject
and object, respectively.
Step 3: The German modifiers are translated
and placed in the appropriate positions within the
AEP. For example, the modifiers das haupthemm-
nis and der vorhersehbare widerstand der her-
steller would be translated as the main obstacle,
and the predictable resistance of manufacturers,
respectively, and then placed into the subject and
object positions in the AEP.
Step 4: The individual clause translations are
combined to give a final translation. For example,
the translations we know and that the main obsta-
cle has been . . . would be concatenated to give we
know that the main obstacle has been . . ..
The main focus of this paper will be Step 2: the
prediction of AEPs from German clauses. AEPs
are detailed structural objects, and their relation-
ship to the source-language clause can be quite
complex. We use a discriminative feature-based
model, trained with the perceptron algorithm, to
incrementally predict the AEP in a sequence of
steps. At each step we define features that allow
the model to capture a wide variety of dependen-
cies within the AEP itself, or between the AEP and
the source-language clause.
1.2 Motivation for the Approach
Our approach to tree-to-tree translation is mo-
tivated by several observations. Breaking the
source-language tree into clauses (Step 1) consid-
erably simplifies the difficult problem of defining
an alignment between source and target trees. Our
impression is that high-quality translations can be
produced in a clause-by-clause fashion.1 The use
of a feature-based model for AEP prediction (Step
2) allows us to capture complex syntactic corre-
spondences between English and German, as well
as grammaticality constraints on the English side.
In this paper, we implement the translation of
modifiers (Step 3) with the phrase-based system
of Koehn et al (2003). The modifiers in our data
set are generally small chunks of text such as NPs,
PPs, and ADJPs, which by definition do not in-
clude clauses or verbs. In our approach, we use
the phrase-based system to generate n-best lists of
candidate translations and then rerank the trans-
lations based on grammaticality, i.e., using crite-
ria that judge how well they fit the position in the
AEP. In future work, we might use finite state ma-
chines in place of a reranking approach, or recur-
sively apply the AEP approach to the modifiers.
Stitching translated clauses back together (Step
4) is a relatively simple task: in a substantial ma-
jority of cases, the German clauses are not embed-
ded, but instead form a linear sequence that ac-
counts for the entire sentence. In these cases we
can simply concatenate the English clause trans-
lations to form the full translation. Embedded
clauses in German are slightly more complicated,
but it is not difficult to form embedded structures
in the English translations.
Section 5.2 of this paper describes the features
1Note that we do not assume that all of the translations
in the training data have been produced in a clause-by-clause
fashion. Rather, we assume that good translations for test
examples can be produced in this way.
233
we use for AEP prediction in translation from
German to English. Many of the features of the
AEP prediction model are specifically tuned to the
choice of German and English as the source and
target languages. However, it should be easy to
develop new feature sets to deal with other lan-
guages or treebanking styles. We see this as one
of the strengths of the feature-based approach.
In the work presented in this paper, we focus on
the prediction of clausal AEPs, i.e., AEPs associ-
ated with main verbs. One reason for this is that
clause structures are particularly rich and com-
plex from a syntactic perspective. This means that
there should be considerable potential in improv-
ing translation quality if we can accurately predict
these structures. It also means that clause-level
AEPs are a good test-bed for the discriminative
approach to AEP prediction; future work may con-
sider applying these methods to other structures
such as NPs, PPs, ADJPs, and so on.
2 Related Work
There has been a substantial amount of previous
work on approaches that make use of syntactic in-
formation in statistical machine translation. Wu
(1997) and Alshawi (1996) describe early work on
formalisms that make use of transductive gram-
mars; Graehl and Knight (2004) describe meth-
ods for training tree transducers. Melamed (2004)
establishes a theoretical framework for general-
ized synchronous parsing and translation. Eisner
(2003) discusses methods for learning synchro-
nized elementary tree pairs from a parallel corpus
of parsed sentences. Chiang (2005) has recently
shown significant improvements in translation ac-
curacy, using synchronous grammars. Riezler and
Maxwell (2006) describe a method for learning
a probabilistic model that maps LFG parse struc-
tures in German into LFG parse structures in En-
glish.
Yamada and Knight (2001) and Galley et al
(2004) describe methods that make use of syn-
tactic information in the target language alone;
Quirk et al (2005) describe similar methods that
make use of dependency representations. Syntac-
tic parsers in the target language have been used
as language models in translation, giving some
improvement in accuracy (Charniak et al, 2001).
The work of Gildea (2003) involves methods that
make use of syntactic information in both the
source and target languages.
Other work has attempted to incorporate syntac-
S
NP-A VP
V
know
SBAR-A
SBAR-A
IN
that
S
NP-A VP
V
has
VP
V
been
NP-A
NP
D
the
N
obstacle
Figure 1: Extended projections for the verbs know and been,
and for the noun obstacle. The EPs were taken from the parse
tree for the sentence We know that the main obstacle has been
the predictable resistance of manufacturers.
tic information through reranking approaches ap-
plied to n-best output from phrase-based systems
(Och et al, 2004). Another class of approaches
has shown improvements in translation through re-
ordering, where source language strings are parsed
and then reordered, in an attempt to recover a word
order that is closer to the target language (Collins
et al, 2005; Xia and McCord, 2004).
Our approach is closely related to previous
work on synchronous tree adjoining grammars
(Shieber and Schabes, 1990; Shieber, 2004), and
the work on TAG approaches to syntax described
by Frank (2002). A major departure from previous
work on synchronous TAGs is in our use of a dis-
criminative model that incrementally predicts the
information in the AEP. Note also that our model
may include features that take into account any
part of the German clause.
3 A Translation Architecture Based on
Aligned Extended Projections
3.1 Background: Extended Projections (EPs)
Extended projections (EPs) play a crucial role in
the lexicalized tree adjoining grammar (LTAG)
(Joshi, 1985) approach to syntax described by
Frank (2002). In this paper we focus almost ex-
clusively on extended projections associated with
main verbs; note, however, that EPs are typically
associated with all content words (nouns, adjec-
tives, etc.). As an example, a parse tree for the
sentence we know that the main obstacle has been
the predictable resistance of manufacturers would
make use of EPs for the words we, know, main, ob-
stacle, been, predictable, resistance, and manufac-
turers. Function words (in this sentence that, the,
has, and of) do not have EPs; instead, as we de-
scribe shortly, each function word is incorporated
in an EP of some content word.
Figure 1 has examples of EPs. Each one is
an LTAG elementary tree which contains a sin-
234
gle content word as one of its leaves. Substitution
nodes (such as NP-A or SBAR-A) in the elemen-
tary trees specify the positions of arguments of the
content words. Each EP may contain one or more
function words that are associated with the con-
tent word. For verbs, these function words include
items such as modal verbs and auxiliaries (e.g.,
should and has); complementizers (e.g., that);
and wh-words (e.g., which). For nouns, function
words include determiners and prepositions.
Elementary trees corresponding to EPs form the
basic units in the LTAG approach described by
Frank (2002). They are combined to form a full
parse tree for a sentence using the TAG operations
of substitution and adjunction. For example, the
EP for been in Figure 1 can be substituted into the
SBAR-A position in the EP for know; the EP for
obstacle can be substituted into the subject posi-
tion of the EP for been.
3.2 Aligned Extended Projections (AEPs)
We now build on the idea of extended projections
to give a detailed description of AEPs. Figure 2
shows examples of German clauses paired with the
AEPs found in training data.2 The German clause
is assumed to have n (where n ? 0) modifiers. For
example, the first German parse in Figure 2 has
two arguments, indexed as 1 and 2. Each of these
modifiers must either have a translation in the cor-
responding English clause, or must be deleted.
An AEP consists of the following parts:
STEM: A string specifying the stemmed form
of the main verb in the clause.
SPINE: A syntactic structure associated with
the main verb. The structure has the symbol V
as one of its leaf nodes; this is the position of
the main verb. It includes higher projections of
the verb such as VPs, Ss, and SBARs. It also in-
cludes leaf nodes NP-A in positions correspond-
ing to noun-phrase arguments (e.g., the subject
or object) of the main verb. In addition, it may
contain leaf nodes labeled with categories such
as WHNP or WHADVP where a wh-phrase may be
placed. It may include leaf nodes corresponding
to one or more complementizers (common exam-
ples being that, if, so that, and so on).
VOICE: One of two alternatives, active or
passive, specifying the voice of the main verb.
2Note that in this paper we consider translation from Ger-
man to English; in the remainder of the paper we take English
to be synonymous with the target language in translation and
German to be synonymous with the source language.
SUBJECT: This variable can be one of three
types. If there is no subject position in the SPINE
variable, then the value for SUBJECT is NULL.
Otherwise, SUBJECT can either be a string, for
example there,3 or an index of one of the n modi-
fiers in the German clause.
OBJECT: This variable is similar to SUBJECT,
and can also take three types: NULL, a specific
string, or an index of one of the n German modi-
fiers. It is always NULL if there is no object posi-
tion in the SPINE; it can never be a modifier index
that has already been assigned to SUBJECT.
WH: This variable is always NULL if there is no
wh-phrase position within the SPINE; it is always
a non-empty string (such as which, or in which) if
a wh-phrase position does exist.
MODALS: This is a string of verbs that consti-
tute the modals that appear within the clause. We
use NULL to signify an absence of modals.
INFL: The inflected form of the verb.
MOD(i): There are n modifier variables
MOD(1), MOD(2), . . ., MOD(n) that spec-
ify the positions for German arguments that have
not already been assigned to the SUBJECT or
OBJECT positions in the spine. Each variable
MOD(i) can take one of five possible values:
? null: This value is chosen if and only if
the modifier has already been assigned to the
subject or object position.
? deleted: This means that a translation of
the i?th German modifier is not present in the
English clause.
? pre-sub: The modifier appears after any
complementizers or wh-phrases, but before
the subject of the English clause.
? post-sub: The modifier appears after the
subject of the English clause, but before the
modals.
? in-modals: The modifier appears after the
first modal in the sequence of modals, but be-
fore the second modal or the main verb.
? post-verb: The modifier appears some-
where after the main verb.
3This happens in the case where there exists a subject in
the English clause which is not aligned to a modifier in the
German clause. See, for instance, the second example in Fig-
ure 2.
235
German Clause English AEP
s-oc kous-cp da?
np-sb 1 art das
nn haupthemmnis
np-pd 2 art der
adja vorhersehbare
nn widerstand
np-ag art der
nn hersteller
vafin-hd war
Paraphrase: that [np-sb the
main obstacle] [np-pd the
predictable resistance of man-
ufacturers] was
STEM: be
SPINE:
SBAR-A IN that
S NP-A
VP V
NP-A
VOICE: active
SUBJECT: 1
OBJECT: 2
WH: NULL
MODALS: has
INFL: been
MOD1: null
MOD2: null
s pp-mo 1 appr zwischen
piat beiden
nn gesetzen
vvfin-hd bestehen
adv-mo 2 also
np-sb 3 adja erhebliche
adja rechtliche
$, ,
adja praktische
kon und
adja wirtschaftliche
nn unterschiede
Paraphrase: [pp-mo between
the two pieces of legislation]
exist so [np-sb significant
legal, practical and economic
differences]
STEM: be
SPINE:
S NP-A
VP V
NP-A
VOICE: active
SUBJECT: ?there?
OBJECT: 3
WH: NULL
MODALS: NULL
INFL: are
MOD1: post-verb
MOD2: pre-sub
MOD3: null
s-rc prels-sb die
vp pp-mo 1 appr an
pdat jenem
nn tag
pp-mo 2 appr in
ne tschernobyl
vvpp-hd gezu?ndet
vafin-hd wurde
Paraphrase: which [pp-mo on
that day] [pp-mo in cher-
nobyl] released were
STEM: release
SPINE:
SBAR WHNP
SG-A VP V
VOICE: passive
SUBJECT: NULL
OBJECT: NULL
WH: which
MODALS: was
INFL: released
MOD1: post-verb
MOD2: post-verb
Figure 2: Three examples of German parse trees, together
with their aligned extended projections (AEPs) in the train-
ing data. Note that in the second example the correspondence
between the German clause and its English translation is not
entirely direct. The subject in the English is the expletive
there; the subject in the German clause becomes the object
in English. This is a typical pattern for the German verb
bestehen. The German PP zwischen ... appears at the start
of the clause in German, but is post-verbal in the English.
The modifier also?whose English translation is so?is in an
intermediate position in the German clause, but appears in the
pre-subject position in the English clause.
4 Extracting AEPs from a Corpus
A crucial step in our approach is the extraction
of training examples from a translation corpus.
Each training example consists of a German clause
paired with an English AEP (see Figure 2).
In our experiments, we used the Europarl cor-
pus (Koehn, 2005). For each sentence pair from
this data, we used a version of the German parser
described by Dubey (2005) to parse the German
component, and a version of the English parser
described by Collins (1999) to parse the English
component. To extract AEPs, we perform the fol-
lowing steps:
NP and PP Alignment To align NPs and PPs,
first all German and English nouns, personal
and possessive pronouns, numbers, and adjectives
are identified in each sentence and aligned using
GIZA++ (Och and Ney, 2003). Next, each NP in
an English tree is aligned to an NP or PP in the
corresponding German tree in a way that is consis-
tent with the word-alignment information. That is,
the words dominated by the English node must be
aligned only to words dominated by the German
node, and vice versa. Note that if there is more
than one German node that is consistent, then the
one rooted at the minimal subtree is selected.
Clause alignment, and AEP Extraction The
next step in the training process is to identify
German/English clause pairs which are transla-
tions of each other. We first break each English
or German parse tree into a set of clauses; see
Appendix A for a description of how we iden-
tify clauses. We retain only those training ex-
amples where the English and German sentences
have the same number of clauses. For these re-
tained examples, define the English sentence to
contain the clause sequence ?e1, e2, . . . , en?, and
the German sentence to contain the clause se-
quence ?g1, g2, . . . , gn?. The clauses are ordered
according to the position of their main verbs in
the original sentence. We create n candidate pairs
?(e1, g1), (e2, g2), . . . , (en, gn)? (i.e., force a one-
to-one correspondence between the two clause se-
quences). We then discard any clause pairs (e, g)
which are inconsistent with the NP/PP alignments
for that sentence.4
4A clause pair is inconsistent with the NP/PP alignments
if it contains an NP/PP on either the German or English side
which is aligned to another NP/PP which is not within the
clause pair.
236
Note that this method is deliberately conserva-
tive (i.e., high precision, but lower recall), in that it
discards sentence pairs where the English/German
sentences have different numbers of clauses. In
practice, we have found that the method yields a
large number of training examples, and that these
training examples are of relatively high quality.
Future work may consider improved methods for
identifying clause pairs, for example methods that
make use of labeled training examples.
An AEP can then be extracted from each
clause pair. The EP for the English clause is
first extracted, giving values for all variables ex-
cept for SUBJECT, OBJECT, and MOD(1), . . . ,
MOD(n). The values for the SUBJECT, OBJECT,
and MOD(i) variables are derived from the align-
ments between NPs/PPs, and an alignment of
other clauses (ADVPs, ADJPs, etc.) derived from
GIZA++ alignments. If the English clause has a
subject or object which is not aligned to a German
modifier, then the value for SUBJECT or OBJECT
is taken to be the full English string.
5 The Model
5.1 Beam search and the perceptron
In this section we describe linear history-based
models with beam search, and the perceptron al-
gorithm for learning in these models. These meth-
ods will form the basis for our model that maps
German clauses to AEPs.
We have a training set of n examples, (xi, yi)
for i = 1 . . . n, where each xi is a German parse
tree, and each yi is an AEP. We follow previous
work on history-based models, by representing
each yi as a series of N decisions ?d1, d2, . . . dN ?.
In our approach, N will be a fixed number for any
input x: we take the N decisions to correspond to
the sequence of variables STEM, SPINE, . . .,
MOD(1), MOD(2), . . ., MOD(n) described
in section 3. Each di is a member of a set Di
which specifies the set of allowable decisions at
the i?th point (for example, D2 would be the set
of all possible values for SPINE). We assume a
function ADVANCE(x, ?d1, d2, . . . , di?1?) which
maps an input x together with a prefix of decisions
d1 . . . di?1 to a subset ofDi. ADVANCE is a func-
tion that specifies which decisions are allowable
for a past history ?d1, . . . , di?1? and an input x. In
our case the ADVANCE function implements hard
constraints on AEPs (for example, the constraint
that the SUBJECT variable must be NULL if no
subject position exists in the SPINE). For any in-
put x, a well-formed decision sequence for x is a
sequence ?d1, . . . , dN ? such that for i = 1 . . . n,
di ? ADVANCE(x, ?d1, . . . , di?1?). We define
GEN(x) to be the set of all decision sequences (or
AEPs) which are well-formed for x.
The model that we will use is a
discriminatively-trained, feature-based model. A
significant advantage to feature-based mod-
els is their flexibility: it is very easy to
sensitize the model to dependencies in the
data by encoding new features. To define a
feature-based model, we assume a function
??(x, ?d1, . . . , di?1?, di) ? Rd which maps a deci-
sion di in context (x, ?d1, . . . , di?1?) to a feature
vector. We also assume a vector ?? ? Rd of param-
eter values. We define the score for any partial or
complete decision sequence y = ?d1, d2, . . . , dm?
paired with x as:
SCORE(x, y) = ?(x, y) ? ?? (1)
where ?(x, y) = ?mi=1 ??(x, ?d1, . . . , di?1?, di).
In particular, given the definitions above, the out-
put structure F (x) for an input x is the highest?
scoring well?formed structure for x:
F (x) = arg max
y?GEN(x)
SCORE(x, y) (2)
To decode with the model we use a beam-search
method. The method incrementally builds an AEP
in the decision order d1, d2, . . . , dN . At each
point, a beam contains the top M highest?scoring
partial paths for the first m decisions, where M
is taken to be a fixed number. The score for any
partial path is defined in Eq. 1. The ADVANCE
function is used to specify the set of possible deci-
sions that can extend any given path in the beam.
To train the model, we use the averaged per-
ceptron algorithm described by Collins (2002).
This combination of the perceptron algorithm with
beam-search is similar to that described by Collins
and Roark (2004).5 The perceptron algorithm is a
convenient choice because it converges quickly ?
usually taking only a few iterations over the train-
ing set (Collins, 2002; Collins and Roark, 2004).
5.2 The Features of the Model
The model?s features allow it to capture depen-
dencies between the AEP and the German clause,
as well as dependencies between different parts
of the AEP itself. The features included in ??
5Future work may consider alternative algorithms, such
as those described by Daume? and Marcu (2005).
237
1 main verb
2 any verb in the clause
3 all verbs, in sequence
4 spine
5 tree
6 preterminal label of left-most child of subject
7 terminal label of left-most child of subject
8 suffix of terminal label of right-most child of subject
9 preterminal label of left-most child of object
10 terminal label of left-most child of object
11 suffix of terminal label of right-most child of object
12 preterminal label of the negation word nicht (not)
13 is either of the strings es gibt (there is/are)
or es gab (there was/were) present?
14 complementizers and wh-words
15 labels of all wh-nonterminals
16 terminal labels of all wh-words
17 preterminal label of a verb in first position
18 terminal label of a verb in first position
19 terminal labels of all words in any relative pronoun
under a PP
20 are all of the verbs at the end?
21 nonterminal label of the root of the tree
22 terminal labels of all words constituting the subject
23 terminal labels of all words constituting the object
24 the leaves dominated by each node in the tree
25 each node in the context of a CFG rule
26 each node in the context of the RHS of a CFG rule
27 each node with its left and right sibling
28 the number of leaves dominated by each node
in the tree
Table 1: Functions of the German clause used for making
features in the AEP prediction model.
can consist of any function of the decision history
?d1, . . . , di?1?, the current decision di, or the Ger-
man clause. In defining features over AEP/clause
pairs, we make use of some basic functions which
look at the German clause and the AEP (see Ta-
bles 1 and 2). We use various combinations of
these basic functions in the prediction of each de-
cision di, as described below.
STEM: Features for the prediction of STEM
conjoin the value of this variable with each of the
functions in lines 1?13 of Table 1. For example,
one feature is the value of STEM conjoined with
the main verb of the German clause. In addition,
?? includes features sensitive to the rank of a can-
didate stem in an externally-compiled lexicon.6
SPINE: Spine prediction features make use of
the values of the variables SPINE and STEM from
the AEP, as well as functions of the spine in lines
1?7 of Table 2, conjoined in various ways with
the functions in lines 4, 12, and 14?21 of Table 1.
Note that the functions in Table 2 allow us to look
6The lexicon is derived from GIZA++ and provides, for a
large number of German main verbs, a ranked list of possible
English translations.
1 does the SPINE have a subject?
2 does the SPINE have an object?
3 does the SPINE have any wh-words?
4 the labels of any complementizer nonterminals
in the SPINE
5 the labels of any wh-nonterminals in the SPINE
6 the nonterminal labels SQ or SBARQ in the SPINE
7 the nonterminal label of the root of the SPINE
8 the grammatical category of the finite verbal form
INFL (i.e., infinitive, 1st-, 2nd-, or 3rd-person pres,
pres participle, sing past, plur past, past participle)
Table 2: Functions of the English AEP used for making fea-
tures in the AEP prediction model.
at substructure in the spine. For instance, one of
the features for SPINE is the label SBARQ or SQ,
if it exists in the candidate spine, conjoined with
a verbal preterminal label if there is a verb in the
first position of the German clause. This feature
captures the fact that German yes/no questions be-
gin with a verb in the first position.
VOICE: Voice features in general combine val-
ues of VOICE, SPINE, and STEM, with the func-
tions in lines 1?5, 22, and 23 of Table 1.
SUBJECT: Features used for subject prediction
make use of the AEP variables VOICE and STEM.
In addition, if the value of SUBJECT is an index
i (see section 3), then ?? looks at the nontermi-
nal label of the German node indexed by i as well
as the surrounding context in the German clausal
tree. Otherwise, ?? looks at the value of SUBJECT.
These basic features are combined with the func-
tions in lines 1, 3, and 24?27 of Table 1.
OBJECT: We make similar features to those for
the prediction of SUBJECT. In addition, ?? can
look at the value predicted for SUBJECT.
WH: Features for WH look at the values of WH
and SPINE, conjoined with the functions in lines
1, 15, and 19 of Table 1.
MODALS: For the prediction of MODALS, ??
looks at MODALS, SPINE, and STEM, conjoined
with the functions in lines 2?5 and 12 of Table 1.
INFL: The features for INFL include the values
of INFL, MODALS, and SUBJECT, and VOICE,
and the function in line 8 of Table 2.
MOD(i): For the MOD(i) variables, ?? looks
at the value of MODALS, SPINE and the current
MOD(i), as well as the nonterminal label of the
root node of the German modifier being placed,
and the functions in lines 24 and 28 of Table 1.
238
6 Deriving Full Translations
As we described in section 1.1, the translation of a
full German sentence proceeds in a series of steps:
a German parse tree is broken into a sequence of
clauses; each clause is individually translated; and
finally, the clause-level translations are combined
to form the translation for a full sentence. The first
and last steps are relatively straightforward. We
now show how the second step is achieved?i.e.,
how AEPs can be used to derive English clause
translations from German clauses.
We will again use the following translation
pair as an example: da? das haupthemmnis der
vorhersehbare widerstand der hersteller war./that
the main obstacle has been the predictable resis-
tance of manufacturers.
First, an AEP like the one at the top of Fig-
ure 2 is predicted. Then, for each German mod-
ifier which does not have the value deleted, an
English translation is predicted. In the example,
the modifiers das haupthemmnis and der vorherse-
hbare widerstand der hersteller would be trans-
lated to the main obstacle, and the predictable re-
sistance of manufacturers, respectively.
A number of methods could be used for trans-
lation of the modifiers. In this paper, we use the
phrase-based system of Koehn et al (2003) to
generate n-best translations for each of the mod-
ifiers, and we then use a discriminative rerank-
ing algorithm (Bartlett et al, 2004) to choose be-
tween these modifiers. The features in the rerank-
ing model can be sensitive to various properties of
the candidate English translation, for example the
words, the part-of-speech sequence or the parse
tree for the string. The reranker can also take into
account the original German string. Finally, the
features can be sensitive to properties of the AEP,
such as the main verb or the position in which the
modifier appears (e.g., subject, object, pre-sub,
post-verb, etc.) in the English clause. See
Appendix B for a full description of the features
used in the modifier translation model. Note that
the reranking stage allows us to filter translation
candidates which do not fit syntactically with the
position in the English tree. For example, we can
parse the members of the n-best list, and then learn
a feature which strongly disprefers prepositional
phrases if the modifier appears in subject position.
Finally, the full string is predicted. In our
example, the AEP variables SPINE, MODALS,
and INFL in Figure 2 give the ordering <that
SUBJECT has been OBJECT>. The AEP
and modifier translations would be combined
to give the final English string. In gen-
eral, any modifiers assigned to pre-sub,
post-sub, in-modals or post-verb are
placed in the corresponding position within the
spine. For example, the second AEP in Fig-
ure 2 has a spine with ordering <SUBJECT
are OBJECT>; modifiers 1 and 2 would be
placed in positions pre-sub and post-verb,
giving the ordering <MOD2 SUBJECT are
OBJECT MOD1>. Note that modifiers assigned
post-verb are placed after the object. If mul-
tiple modifiers appear in the same position (e.g.,
post-verb), then they are placed in the order
seen in the original German clause.
7 Experiments
We applied the approach to translation from Ger-
man to English, using the Europarl corpus (Koehn,
2005) for our training data. This corpus contains
over 750,000 training sentences; we extracted over
441,000 training examples for the AEP model
from this corpus, using the method described in
section 4. We reserved 35,000 of these training
examples as development data for the model. We
used a set of features derived from the those de-
scribed in section 5.2. This set was optimized us-
ing the development data through experimentation
with several different feature subsets.
Modifiers within German clauses were trans-
lated using the phrase-based model of Koehn et
al. (2003). We first generated n-best lists for each
modifier. We then built a reranking model?see
section 6?to choose between the elements in the
n-best lists. The reranker was trained using around
800 labeled examples from a development set.
The test data for the experiments consisted of
2,000 sentences, and was the same test set as that
used by Collins et al (2005). We use the model
of Koehn et al (2003) as a baseline for our ex-
periments. The AEP-driven model was used to
translate all test set sentences where all clauses
within the German parse tree contained at least
one verb and there was no embedding of clauses?
there were 1,335 sentences which met these crite-
ria. The remaining 665 sentences were translated
with the baseline system. This set of 2,000 trans-
lations had a BLEU score of 23.96. The baseline
system alone achieved a BLEU score of 25.26 on
the same set of 2,000 test sentences. We also ob-
tained judgments from two human annotators on
239
100 randomly-drawn sentences on which the base-
line and AEP-based outputs differed. For each ex-
ample the annotator viewed the reference transla-
tion, together with the two systems? translations
presented in a random order. Annotator 1 judged
62 translations to be equal in quality, 16 transla-
tions to be better under the AEP system, and 22
to be better for the baseline system. Annotator 2
judged 37 translations to be equal in quality, 32 to
be better under the baseline, and 31 to be better
under the AEP-based system.
8 Conclusions and Future Work
We have presented an approach to tree-to-
tree based translation which models a new
representation?aligned extended projections?
within a discriminative, feature-based framework.
Our model makes use of an explicit representation
of syntax in the target language, together with con-
straints on the alignments between source and tar-
get parse trees.
The current system presents many opportuni-
ties for future work. For example, improve-
ment in accuracy may come from a tighter in-
tegration of modifier translation into the over-
all translation process. The current method?
using an n-best reranking model to select the best
candidate?chooses each modifier independently
and then places it into the translation. We in-
tend to explore an alternative method that com-
bines finite-state machines representing the n-best
output from the phrase-based system with finite-
state machines representing the complementiz-
ers, verbs, modals, and other substrings of the
translation derived from the AEP. Selecting mod-
ifiers using this representation would correspond
to searching the finite-state network for the most
likely path. A finite-state representation has many
advantages, including the ability to easily incorpo-
rate an n-gram language model.
Future work may also consider expanded defi-
nitions of AEPs. For example, we might consider
AEPs that include larger chunks of phrase struc-
ture, or we might consider AEPs that contain more
detailed information about the relative ordering of
modifiers. There is certainly room for improve-
ment in the accuracy with which AEPs are pre-
dicted in our data; the feature-driven approach al-
lows a wide range of features to be tested. For ex-
ample, it would be relatively easy to incorporate a
syntactic language model (i.e., a prior distribution
over AEP structures) induced from a large amount
of English monolingual data.
Appendix A: Identification of Clauses
In the English parse trees, we identify clauses as
follows. Any non-terminal labeled by the parser
of (Collins, 1999) as SBAR or SBAR-A is labeled
as a clause root. Any node labeled by the parser as
S or S-A is also labeled as the root of a clause, un-
less it is directly dominated by a non-terminal la-
beled SBAR or SBAR-A. Any node labeled SG or
SG-A by the parser is labeled as a clause root, un-
less (1) the node is directly dominated by SBAR or
SBAR-A; or (2) the node is directly dominated by
a VP, and the node is directly preceded by a verb
(POS tag beginning with V) or modal (POS tag be-
ginning with M). Any node labeled VP is marked
as a clause root if (1) the node is not directly dom-
inated by a VP, S, S-A, SBAR, SBAR-A, SG, or
SG-A; or (2) the node is directly preceded by a
coordinating conjunction (i.e., a POS tag labeled
as CC).
In German parse trees, we identify any nodes
labeled as S or CS as clause roots. In addition,
we mark any node labeled as VP as a clause root,
provided that (1) it is preceded by a coordinating
conjunction, i.e., a POS tag labeled as KON; or (2)
it has one of the functional tags -mo, -re or -sb.
Appendix B: Reranking Modifier
Translations
The n-best reranking model for the translation of
modifiers considers a list of candidate translations.
We hand-labeled 800 examples, marking the ele-
ment in each list that would lead to the best trans-
lation. The features of the n-best reranking algo-
rithm are combinations of the basic features in Ta-
bles 3 and 4.
Each list contained the n-best translations pro-
duced by the phrase-based system of Koehn et al
(2003). The lists also contained a supplementary
candidate ?DELETED?, signifying that the mod-
ifier should be deleted from the English transla-
tion. In addition, each candidate derived from the
phrase-based system contributed one new candi-
date to the list signifying that the first word of
the candidate should be deleted. These additional
candidates were motivated by our observation that
the optimal candidate in the n-best list produced
by the phrase-based system often included an un-
wanted preposition at the beginning of the string.
240
1 candidate string
2 should the first word of the candidate be deleted?
3 POS tag of first word of candidate
4 POS tag of last word of candidate
5 top nonterminal of parse of candidate
6 modifier deleted from English translation?
7 first candidate on n-best list
8 first word of candidate
9 last word of candidate
10 rank of candidate in n-best list
11 is there punctuation at the beginning, middle,
or end of the string?
12 if the first word of the candidate should be deleted,
what is the string that is deleted?
13 if the first word of the candidate should be deleted,
what is the POS tag of the word that is deleted?
Table 3: Functions of the candidate modifier translations used
for making features in the n-best reranking model.
1 the position of the modifier (0?4) in AEP
2 main verb
3 voice
4 subject prediction
5 German input string
Table 4: Functions of the German input string and predicted
AEP output used for making features in the n-best reranking
model.
Acknowledgements
We would like to thank Luke Zettlemoyer, Regina Barzilay,
Ed Filisko, and Ben Snyder for their valuable comments and
help during the writing of this paper. Thanks also to Jason
Rennie and John Barnett for providing human judgments of
the translation output. This work was funded by NSF grants
IIS-0347631, IIS-0415030, and DMS-0434222, as well as a
grant from NTT, Agmt. Dtd. 6/21/1998.
References
H. Alshawi. 1996. Head automata and bilingual tiling: trans-
lation with minimal representations. ACL 96.
P. Bartlett, M. Collins, B. Taskar, and D. McAllester. 2004.
Exponentiated gradient algorithms for large-margin struc-
tured classification. Proceedings of NIPS 2004.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine translation.
Computational Linguistics, 22(1):39?69.
E. Charniak, K. Knight, and K. Yamada. 2001. Syntax-based
language models for statistical machine translation. ACL
01.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. ACL 05.
M. Collins. 1999. Head-Driven Statistical Models for Natu-
ral Language Parsing. University of Pennsylvania.
M. Collins. 2002. Discriminative training methods for hid-
den markov models: theory and experiments with percep-
tron algorithms. EMNLP 02.
M. Collins and B. Roark. 2004. Incremental parsing with the
perceptron algorithm. ACL 04.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause restruc-
turing for statistical machine translation. ACL 05.
H. Daume? III and D. Marcu. 2005. Learning as search op-
timization: approximate large margin methods for struc-
tured prediction. ICML 05.
A. Dubey. 2005. What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. ACL 05.
J. Eisner. 2003. Learning non-isomorphic tree mappings for
machine translation. ACL 03, Companion Volume.
R. Frank. 2002. Phrase Structure Composition and Syntactic
Dependencies. Cambridge, MA: MIT Press.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? HLT-NAACL 04.
D. Gildea. 2003. Loosely tree-based alignment for machine
translation. ACL 03.
J. Graehl and K. Knight. 2004. Training tree transducers.
NAACL-HLT 04.
A. Joshi. 1985. How much context-sensitivity is necessary
for characterizing structural descriptions ? tree-adjoining
grammar. Cambridge University Press.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase
based translation. HLT-NAACL 03.
P. Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. MT Summit 05.
I. D. Melamed 2004. Statistical machine translation by pars-
ing. ACL 04.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain,
Z. Jin, D. Radev. 2004. A smorgasbord of features for
statistical machine translation. HLT/NAACL 04
F. J. Och and H. Ney. 2004. The alignment template ap-
proach to statistical machine translation. Computational
Linguistics, 30(4):417?449.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational Lin-
guistics, 29(1):19?51.
C. Quirk, A. Menezes, and C. Cherry. 2005. Depen-
dency tree translation: syntactically informed phrasal
SMT. EACL 05.
S. Riezler and J. Maxwell. 2006. Grammatical machine
translation. In NLT-NAACL 06.
S. Shieber. 2004. Synchronous grammars as tree transduc-
ers. In Proceedings of the Seventh International Workshop
on Tree Adjoining Grammar and Related Formalisms.
S. Shieber and Y. Schabes. 1990. Synchronous tree-
adjoining grammars. In Proceedings of the 13th Interna-
tional Conference on Computational Linguistics.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
F. Xia and M. McCord. 2004. Improving a statistical MT
system with automatically learned rewrite patterns. COL-
ING 04.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. ACL 01.
241
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1?11,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
On Dual Decomposition and Linear Programming Relaxations
for Natural Language Processing
Alexander M. Rush David Sontag Michael Collins Tommi Jaakkola
MIT CSAIL, Cambridge, MA 02139, USA
{srush,dsontag,mcollins,tommi}@csail.mit.edu
Abstract
This paper introduces dual decomposition as a
framework for deriving inference algorithms
for NLP problems. The approach relies on
standard dynamic-programming algorithms as
oracle solvers for sub-problems, together with
a simple method for forcing agreement be-
tween the different oracles. The approach
provably solves a linear programming (LP) re-
laxation of the global inference problem. It
leads to algorithms that are simple, in that they
use existing decoding algorithms; efficient, in
that they avoid exact algorithms for the full
model; and often exact, in that empirically
they often recover the correct solution in spite
of using an LP relaxation. We give experimen-
tal results on two problems: 1) the combina-
tion of two lexicalized parsing models; and
2) the combination of a lexicalized parsing
model and a trigram part-of-speech tagger.
1 Introduction
Dynamic programming algorithms have been re-
markably useful for inference in many NLP prob-
lems. Unfortunately, as models become more com-
plex, for example through the addition of new fea-
tures or components, dynamic programming algo-
rithms can quickly explode in terms of computa-
tional or implementational complexity.1 As a re-
sult, efficiency of inference is a critical bottleneck
for many problems in statistical NLP.
This paper introduces dual decomposition
(Dantzig and Wolfe, 1960; Komodakis et al, 2007)
as a framework for deriving inference algorithms in
NLP. Dual decomposition leverages the observation
that complex inference problems can often be
decomposed into efficiently solvable sub-problems.
The approach leads to inference algorithms with the
following properties:
1The same is true for NLP inference algorithms based on
other exact combinatorial methods, for example methods based
on minimum-weight spanning trees (McDonald et al, 2005), or
graph cuts (Pang and Lee, 2004).
? The resulting algorithms are simple and efficient,
building on standard dynamic-programming algo-
rithms as oracle solvers for sub-problems,2 to-
gether with a method for forcing agreement be-
tween the oracles.
? The algorithms provably solve a linear program-
ming (LP) relaxation of the original inference
problem.
? Empirically, the LP relaxation often leads to an
exact solution to the original problem.
The approach is very general, and should be appli-
cable to a wide range of problems in NLP. The con-
nection to linear programming ensures that the algo-
rithms provide a certificate of optimality when they
recover the exact solution, and also opens up the
possibility of methods that incrementally tighten the
LP relaxation until it is exact (Sherali and Adams,
1994; Sontag et al, 2008).
The structure of this paper is as follows. We
first give two examples as an illustration of the ap-
proach: 1) integrated parsing and trigram part-of-
speech (POS) tagging; and 2) combined phrase-
structure and dependency parsing. In both settings,
it is possible to solve the integrated problem through
an ?intersected? dynamic program (e.g., for integra-
tion of parsing and tagging, the construction from
Bar-Hillel et al (1964) can be used). However,
these methods, although polynomial time, are sub-
stantially less efficient than our algorithms, and are
considerably more complex to implement.
Next, we describe exact polyhedral formula-
tions for the two problems, building on connec-
tions between dynamic programming algorithms
and marginal polytopes, as described in Martin et al
(1990). These allow us to precisely characterize the
relationship between the exact formulations and the
2More generally, other exact inference methods can be
used as oracles, for example spanning tree algorithms for non-
projective dependency structures.
1
LP relaxations that we solve. We then give guaran-
tees of convergence for our algorithms by showing
that they are instantiations of Lagrangian relaxation,
a general method for solving linear programs of a
particular form.
Finally, we describe experiments that demonstrate
the effectiveness of our approach. First, we con-
sider the integration of the generative model for
phrase-structure parsing of Collins (2003), with the
second-order discriminative dependency parser of
Koo et al (2008). This is an interesting problem
in its own right: the goal is to inject the high per-
formance of discriminative dependency models into
phrase-structure parsing. The method uses off-the-
shelf decoders for the two models. We find three
main results: 1) in spite of solving an LP relax-
ation, empirically the method finds an exact solution
on over 99% of the examples; 2) the method con-
verges quickly, typically requiring fewer than 10 it-
erations of decoding; 3) the method gives gains over
a baseline method that forces the phrase-structure
parser to produce the same dependencies as the first-
best output from the dependency parser (the Collins
(2003) model has an F1 score of 88.1%; the base-
line method has an F1 score of 89.7%; and the dual
decomposition method has an F1 score of 90.7%).
In a second set of experiments, we use dual de-
composition to integrate the trigram POS tagger of
Toutanova and Manning (2000) with the parser of
Collins (2003). We again find that the method finds
an exact solution in almost all cases, with conver-
gence in just a few iterations of decoding.
Although the focus of this paper is on dynamic
programming algorithms?both in the experiments,
and also in the formal results concerning marginal
polytopes?it is straightforward to use other com-
binatorial algorithms within the approach. For ex-
ample, Koo et al (2010) describe a dual decompo-
sition approach for non-projective dependency pars-
ing, which makes use of both dynamic programming
and spanning tree inference algorithms.
2 Related Work
Dual decomposition is a classical method for solv-
ing optimization problems that can be decomposed
into efficiently solvable sub-problems. Our work is
inspired by dual decomposition methods for infer-
ence in Markov random fields (MRFs) (Wainwright
et al, 2005a; Komodakis et al, 2007; Globerson and
Jaakkola, 2007). In this approach, the MRF is de-
composed into sub-problems corresponding to tree-
structured subgraphs that together cover all edges
of the original graph. The resulting inference algo-
rithms provably solve an LP relaxation of the MRF
inference problem, often significantly faster than
commercial LP solvers (Yanover et al, 2006).
Our work is also related to methods that incorpo-
rate combinatorial solvers within loopy belief prop-
agation (LBP), either for MAP inference (Duchi et
al., 2007) or for computing marginals (Smith and
Eisner, 2008). Our approach similarly makes use
of combinatorial algorithms to efficiently solve sub-
problems of the global inference problem. However,
unlike LBP, our algorithms have strong theoretical
guarantees, such as guaranteed convergence and the
possibility of a certificate of optimality. These guar-
antees are possible because our algorithms directly
solve an LP relaxation.
Other work has considered LP or integer lin-
ear programming (ILP) formulations of inference in
NLP (Martins et al, 2009; Riedel and Clarke, 2006;
Roth and Yih, 2005). These approaches typically
use general-purpose LP or ILP solvers. Our method
has the advantage that it leverages underlying struc-
ture arising in LP formulations of NLP problems.
We will see that dynamic programming algorithms
such as CKY can be considered to be very effi-
cient solvers for particular LPs. In dual decomposi-
tion, these LPs?and their efficient solvers?can be
embedded within larger LPs corresponding to more
complex inference problems.
3 Background: Structured Models for NLP
We now describe the type of models used throughout
the paper. We take some care to set up notation that
will allow us to make a clear connection between
inference problems and linear programming.
Our first example is weighted CFG parsing. We
assume a context-free grammar, in Chomsky normal
form, with a set of non-terminals N . The grammar
contains all rules of the form A ? B C and A ?
w where A,B,C ? N and w ? V (it is simple
to relax this assumption to give a more constrained
grammar). For rules of the form A ? w we refer
to A as the part-of-speech tag for w. We allow any
non-terminal to be at the root of the tree.
2
Given a sentence with n words, w1, w2, . . . wn, a
parse tree is a set of rule productions of the form
?A ? B C, i, k, j? where A,B,C ? N , and
1 ? i ? k < j ? n. Each rule production rep-
resents the use of CFG rule A ? B C where non-
terminal A spans words wi . . . wj , non-terminal B
spans words wi . . . wk, and non-terminal C spans
words wk+1 . . . wj . There are O(|N |3n3) such rule
productions. Each parse tree corresponds to a subset
of these rule productions, of size n? 1, that forms a
well-formed parse tree.3
We now define the index set for CFG parsing as
I = {?A? B C, i, k, j?: A,B,C ? N ,
1 ? i ? k < j ? n}
Each parse tree is a vector y = {yr : r ? I},
with yr = 1 if rule r is in the parse tree, and yr =
0 otherwise. Hence each parse tree is represented
as a vector in {0, 1}m, where m = |I|. We use Y
to denote the set of all valid parse-tree vectors; the
set Y is a subset of {0, 1}m (not all binary vectors
correspond to valid parse trees).
In addition, we assume a vector ? = {?r : r ?
I} that specifies a weight for each rule production.4
Each ?r can take any value in the reals. The optimal
parse tree is y? = arg maxy?Y y ? ? where y ? ? =?
r yr?r is the inner product between y and ?.
We use yr and y(r) interchangeably (similarly for
?r and ?(r)) to refer to the r?th component of the
vector y. For example ?(A ? B C, i, k, j) is a
weight for the rule ?A? B C, i, k, j?.
We will use similar notation for other problems.
As a second example, in POS tagging the task is to
map a sentence of n words w1 . . . wn to a tag se-
quence t1 . . . tn, where each ti is chosen from a set
T of possible tags. We assume a trigram tagger,
where a tag sequence is represented through deci-
sions ?(A,B) ? C, i? where A,B,C ? T , and
i ? {3 . . . n}. Each production represents a tran-
sition where C is the tag of word wi, and (A,B) are
3We do not require rules of the form A ? wi in this repre-
sentation, as they are redundant: specifically, a rule production
?A ? B C, i, k, j? implies a rule B ? wi iff i = k, and
C ? wj iff j = k + 1.
4We do not require parameters for rules of the formA? w,
as they can be folded into rule production parameters. E.g.,
under a PCFG we define ?(A ? B C, i, k, j) = logP (A ?
B C | A) + ?i,k logP (B ? wi|B) + ?k+1,j logP (C ?
wj |C) where ?x,y = 1 if x = y, 0 otherwise.
the previous two tags. The index set for tagging is
Itag = {?(A,B)? C, i? : A,B,C ? T , 3 ? i ? n}
Note that we do not need transitions for i = 1 or i =
2, because the transition ?(A,B) ? C, 3? specifies
the first three tags in the sentence.5
Each tag sequence is represented as a vector z =
{zr : r ? Itag}, and we denote the set of valid tag
sequences, a subset of {0, 1}|Itag|, as Z . Given a
parameter vector ? = {?r : r ? Itag}, the optimal
tag sequence is arg maxz?Z z ? ?.
As a modification to the above approach, we will
find it convenient to introduce extended index sets
for both the CFG and POS tagging examples. For
the CFG case we define the extended index set to be
I ? = I ? Iuni where
Iuni = {(i, t) : i ? {1 . . . n}, t ? T}
Here each pair (i, t) represents word wi being as-
signed the tag t. Thus each parse-tree vector y will
have additional (binary) components y(i, t) spec-
ifying whether or not word i is assigned tag t.
(Throughout this paper we will assume that the tag-
set used by the tagger, T , is a subset of the set of non-
terminals considered by the parser, N .) Note that
this representation is over-complete, since a parse
tree determines a unique tagging for a sentence:
more explicitly, for any i ? {1 . . . n}, Y ? T , the
following linear constraint holds:
y(i, Y ) =
n?
k=i+1
?
X,Z?N
y(X ? Y Z, i, i, k) +
i?1?
k=1
?
X,Z?N
y(X ? Z Y, k, i? 1, i)
We apply the same extension to the tagging index
set, effectively mapping trigrams down to unigram
assignments, again giving an over-complete repre-
sentation. The extended index set for tagging is re-
ferred to as I ?tag.
From here on we will make exclusive use of ex-
tended index sets for CFG parsing and trigram tag-
ging. We use the set Y to refer to the set of valid
parse structures under the extended representation;
5As one example, in an HMM, the parameter ?((A,B) ?
C, 3) would be logP (A|??)+logP (B|?A)+logP (C|AB)+
logP (w1|A) + logP (w2|B) + logP (w3|C), where ? is the
start symbol.
3
each y ? Y is a binary vector of length |I ?|. We
similarly use Z to refer to the set of valid tag struc-
tures under the extended representation. We assume
parameter vectors for the two problems, ?cfg ? R|I
?|
and ?tag ? R|I
?
tag|.
4 Two Examples
This section describes the dual decomposition ap-
proach for two inference problems in NLP.
4.1 Integrated Parsing and Trigram Tagging
We now describe the dual decomposition approach
for integrated parsing and trigram tagging. First, de-
fine the set Q as follows:
Q = {(y, z) : y ? Y, z ? Z,
y(i, t) = z(i, t) for all (i, t) ? Iuni} (1)
Hence Q is the set of all (y, z) pairs that agree
on their part-of-speech assignments. The integrated
parsing and trigram tagging problem is then to solve
max
(y,z)?Q
(
y ? ?cfg + z ? ?tag
)
(2)
This problem is equivalent to
max
y?Y
(
y ? ?cfg + g(y) ? ?tag
)
where g : Y ? Z is a function that maps a parse
tree y to its set of trigrams z = g(y). The benefit of
the formulation in Eq. 2 is that it makes explicit the
idea of maximizing over all pairs (y, z) under a set
of agreement constraints y(i, t) = z(i, t)?this con-
cept will be central to the algorithms in this paper.
With this in mind, we note that we have effi-
cient methods for the inference problems of tagging
and parsing alone, and that our combined objective
almost separates into these two independent prob-
lems. In fact, if we drop the y(i, t) = z(i, t) con-
straints from the optimization problem, the problem
splits into two parts, each of which can be efficiently
solved using dynamic programming:
(y?, z?) = (arg max
y?Y
y ? ?cfg, arg max
z?Z
z ? ?tag)
Dual decomposition exploits this idea; it results in
the algorithm given in figure 1. The algorithm opti-
mizes the combined objective by repeatedly solving
the two sub-problems separately?that is, it directly
Set u(1)(i, t)? 0 for all (i, t) ? Iuni
for k = 1 to K do
y(k) ? arg max
y?Y
(y ? ?cfg ?
?
(i,t)?Iuni
u(k)(i, t)y(i, t))
z(k) ? arg max
z?Z
(z ? ?tag +
?
(i,t)?Iuni
u(k)(i, t)z(i, t))
if y(k)(i, t) = z(k)(i, t) for all (i, t) ? Iuni then
return (y(k), z(k))
for all (i, t) ? Iuni,
u(k+1)(i, t)? u(k)(i, t)+?k(y(k)(i, t)?z(k)(i, t))
return (y(K), z(K))
Figure 1: The algorithm for integrated parsing and tag-
ging. The parameters ?k > 0 for k = 1 . . .K specify
step sizes for each iteration, and are discussed further in
the Appendix. The two arg max problems can be solved
using dynamic programming.
solves the harder optimization problem using an ex-
isting CFG parser and trigram tagger. After each
iteration the algorithm adjusts the weights u(i, t);
these updates modify the objective functions for the
two models, encouraging them to agree on the same
POS sequence. In section 6.1 we will show that the
variables u(i, t) are Lagrange multipliers enforcing
agreement constraints, and that the algorithm corre-
sponds to a (sub)gradient method for optimization
of a dual function. The algorithm is easy to imple-
ment: all that is required is a decoding algorithm for
each of the two models, and simple additive updates
to the Lagrange multipliers enforcing agreement be-
tween the two models.
4.2 Integrating Two Lexicalized Parsers
Our second example problem is the integration of
a phrase-structure parser with a higher-order depen-
dency parser. The goal is to add higher-order fea-
tures to phrase-structure parsing without greatly in-
creasing the complexity of inference.
First, we define an index set for second-order un-
labeled projective dependency parsing. The second-
order parser considers first-order dependencies, as
well as grandparent and sibling second-order depen-
dencies (e.g., see Carreras (2007)). We assume that
Idep is an index set containing all such dependen-
cies (for brevity we omit the details of this index
set). For convenience we define an extended index
set that makes explicit use of first-order dependen-
4
cies, I ?dep = Idep ? Ifirst, where
Ifirst = {(i, j) : i ? {0 . . . n}, j ? {1 . . . n}, i 6= j}
Here (i, j) represents a dependency with head wi
and modifier wj (i = 0 corresponds to the root sym-
bol in the parse). We use D ? {0, 1}|I
?
dep| to denote
the set of valid projective dependency parses.
The second model we use is a lexicalized CFG.
Each symbol in the grammar takes the form A(h)
where A ? N is a non-terminal, and h ? {1 . . . n}
is an index specifying that wh is the head of the con-
stituent. Rule productions take the form ?A(a) ?
B(b) C(c), i, k, j? where b ? {i . . . k}, c ? {(k +
1) . . . j}, and a is equal to b or c, depending on
whether A receives its head-word from its left or
right child. Each such rule implies a dependency
(a, b) if a = c, or (a, c) if a = b. We take Ihead
to be the index set of all such rules, and I ?head =
Ihead?Ifirst to be the extended index set. We define
H ? {0, 1}|I
?
head| to be the set of valid parse trees.
The integrated parsing problem is then to find
(y?, d?) = arg max
(y,d)?R
(
y ? ?head + d ? ?dep
)
(3)
where R = {(y, d) : y ? H, d ? D,
y(i, j) = d(i, j) for all (i, j) ? Ifirst}
This problem has a very similar structure to the
problem of integrated parsing and tagging, and we
can derive a similar dual decomposition algorithm.
The Lagrange multipliers u are a vector in R|Ifirst|
enforcing agreement between dependency assign-
ments. The algorithm (omitted for brevity) is identi-
cal to the algorithm in figure 1, but with Iuni, Y , Z ,
?cfg, and ?tag replaced with Ifirst, H, D, ?head, and
?dep respectively. The algorithm only requires de-
coding algorithms for the two models, together with
simple updates to the Lagrange multipliers.
5 Marginal Polytopes and LP Relaxations
We now give formal guarantees for the algorithms
in the previous section, showing that they solve LP
relaxations of the problems in Eqs. 2 and 3.
To make the connection to linear programming,
we first introduce the idea of marginal polytopes in
section 5.1. In section 5.2, we give a precise state-
ment of the LP relaxations that are being solved
by the example algorithms, making direct use of
marginal polytopes. In section 6 we will prove that
the example algorithms solve these LP relaxations.
5.1 Marginal Polytopes
For a finite set Y , define the set of all distributions
over elements in Y as ? = {? ? R|Y| : ?y ?
0,
?
y?Y ?y = 1}. Each ? ? ? gives a vector of
marginals, ? =
?
y?Y ?yy, where ?r can be inter-
preted as the probability that yr = 1 for a y selected
at random from the distribution ?.
The set of all possible marginal vectors, known as
the marginal polytope, is defined as follows:
M = {? ? Rm : ?? ? ? such that ? =
?
y?Y
?yy}
M is also frequently referred to as the convex hull of
Y , written as conv(Y). We use the notation conv(Y)
in the remainder of this paper, instead ofM.
For an arbitrary set Y , the marginal polytope
conv(Y) can be complex to describe.6 However,
Martin et al (1990) show that for a very general
class of dynamic programming problems, the cor-
responding marginal polytope can be expressed as
conv(Y) = {? ? Rm : A? = b, ? ? 0} (4)
where A is a p?m matrix, b is vector in Rp, and the
value p is linear in the size of a hypergraph repre-
sentation of the dynamic program. Note that A and
b specify a set of p linear constraints.
We now give an explicit description of the re-
sulting constraints for CFG parsing:7 similar con-
straints arise for other dynamic programming algo-
rithms for parsing, for example the algorithms of
Eisner (2000). The exact form of the constraints, and
the fact that they are polynomial in number, is not
essential for the formal results in this paper. How-
ever, a description of the constraints gives valuable
intuition for the structure of the marginal polytope.
The constraints are given in figure 2. To develop
some intuition, consider the case where the variables
?r are restricted to be binary: hence each binary
vector ? specifies a parse tree. The second con-
straint in Eq. 5 specifies that exactly one rule must
be used at the top of the tree. The set of constraints
in Eq. 6 specify that for each production of the form
6For any finite set Y , conv(Y) can be expressed as {? ?
Rm : A? ? b} where A is a matrix of dimension p ?m, and
b ? Rp (see, e.g., Korte and Vygen (2008), pg. 65). The value
for p depends on the set Y , and can be exponential in size.
7Taskar et al (2004) describe the same set of constraints, but
without proof of correctness or reference to Martin et al (1990).
5
?r ? I?, ?r ? 0 ;
X
X,Y,Z?N
k=1...(n?1)
?(X ? Y Z, 1, k, n) = 1 (5)
?X ? N , ?(i, j) such that 1 ? i < j ? n and (i, j) 6= (1, n):
X
Y,Z?N
k=i...(j?1)
?(X ? Y Z, i, k, j) =
X
Y,Z?N
k=1...(i?1)
?(Y ? Z X, k, i? 1, j)
+
X
Y,Z?N
k=(j+1)...n
?(Y ? X Z, i, j, k) (6)
?Y ? T, ?i ? {1 . . . n} : ?(i, Y ) =
X
X,Z?N
k=(i+1)...n
?(X ? Y Z, i, i, k) +
X
X,Z?N
k=1...(i?1)
?(X ? Z Y, k, i? 1, i) (7)
Figure 2: The linear constraints defining the marginal
polytope for CFG parsing.
?X ? Y Z, i, k, j? in a parse tree, there must be
exactly one production higher in the tree that gener-
ates (X, i, j) as one of its children. The constraints
in Eq. 7 enforce consistency between the ?(i, Y )
variables and rule variables higher in the tree. Note
that the constraints in Eqs.(5?7) can be written in the
form A? = b, ? ? 0, as in Eq. 4.
Under these definitions, we have the following:
Theorem 5.1 Define Y to be the set of all CFG
parses, as defined in section 4. Then
conv(Y) = {? ? Rm : ? satisifies Eqs.(5?7)}
Proof: This theorem is a special case of Martin et al
(1990), theorem 2.
The marginal polytope for tagging, conv(Z), can
also be expressed using linear constraints as in Eq. 4;
see figure 3. These constraints follow from re-
sults for graphical models (Wainwright and Jordan,
2008), or from the Martin et al (1990) construction.
As a final point, the following theorem gives an
important property of marginal polytopes, which we
will use at several points in this paper:
Theorem 5.2 (Korte and Vygen (2008), page 66.)
For any set Y ? {0, 1}k, and for any vector ? ? Rk,
max
y?Y
y ? ? = max
??conv(Y)
? ? ? (8)
The theorem states that for a linear objective func-
tion, maximization over a discrete set Y can be
replaced by maximization over the convex hull
?r ? I?tag, ?r ? 0 ;
X
X,Y,Z?T
?((X,Y )? Z, 3) = 1
?X ? T , ?i ? {3 . . . n? 1}:
X
Y,Z?T
?((Y,Z)? X, i) =
X
Y,Z?T
?((Y,X)? Z, i+ 1)
?X ? T , ?i ? {3 . . . n? 2}:
X
Y,Z?T
?((Y,Z)? X, i) =
X
Y,Z?T
?((X,Y )? Z, i+ 2)
?X ? T,?i ? {3 . . . n} : ?(i,X) =
X
Y,Z?T
?((Y,Z)? X, i)
?X ? T : ?(1, X) =
X
Y,Z?T
?((X,Y )? Z, 3)
?X ? T : ?(2, X) =
X
Y,Z?T
?((Y,X)? Z, 3)
Figure 3: The linear constraints defining the marginal
polytope for trigram POS tagging.
conv(Y). The problem max??conv(Y) ? ?? is a linear
programming problem.
For parsing, this theorem implies that:
1. Weighted CFG parsing can be framed as a linear
programming problem, of the form max??conv(Y) ??
?, where conv(Y) is specified by a polynomial num-
ber of linear constraints.
2. Conversely, dynamic programming algorithms
such as the CKY algorithm can be considered to
be oracles that efficiently solve LPs of the form
max??conv(Y) ? ? ?.
Similar results apply for the POS tagging case.
5.2 Linear Programming Relaxations
We now describe the LP relaxations that are solved
by the example algorithms in section 4. We begin
with the algorithm in Figure 1.
The original optimization problem was to find
max(y,z)?Q
(
y ? ?cfg + z ? ?tag
)
(see Eq. 2). By the-
orem 5.2, this is equivalent to solving
max
(?,?)?conv(Q)
(
? ? ?cfg + ? ? ?tag
)
(9)
To formulate our approximation, we first define:
Q? = {(?, ?) : ? ? conv(Y), ? ? conv(Z),
?(i, t) = ?(i, t) for all (i, t) ? Iuni}
6
The definition of Q? is very similar to the definition
of Q (see Eq. 1), the only difference being that Y
and Z are replaced by conv(Y) and conv(Z) re-
spectively. Hence any point inQ is also inQ?. It fol-
lows that any point in conv(Q) is also inQ?, because
Q? is a convex set defined by linear constraints.
The LP relaxation then corresponds to the follow-
ing optimization problem:
max
(?,?)?Q?
(
? ? ?cfg + ? ? ?tag
)
(10)
Q? is defined by linear constraints, making this a
linear program. Since Q? is an outer bound on
conv(Q), i.e. conv(Q) ? Q?, we obtain the guaran-
tee that the value of Eq. 10 always upper bounds the
value of Eq. 9.
In Appendix A we give an example showing
that in general Q? includes points that are not in
conv(Q). These points exist because the agreement
between the two parts is now enforced in expecta-
tion (?(i, t) = ?(i, t) for (i, t) ? Iuni) rather than
based on actual assignments. This agreement con-
straint is weaker since different distributions over
assignments can still result in the same first order
expectations. Thus, the solution to Eq. 10 may be
in Q? but not in conv(Q). It can be shown that
all such solutions will be fractional, making them
easy to distinguish from Q. In many applications of
LP relaxations?including the examples discussed
in this paper?the relaxation in Eq. 10 turns out to
be tight, in that the solution is often integral (i.e., it
is in Q). In these cases, solving the LP relaxation
exactly solves the original problem of interest.
In the next section we prove that the algorithm
in Figure 1 solves the problem in Eq 10. A similar
result holds for the algorithm in section 4.2: it solves
a relaxation of Eq. 3, whereR is replaced by
R? = {(?, ?) : ? ? conv(H), ? ? conv(D),
?(i, j) = ?(i, j) for all (i, j) ? Ifirst}
6 Convergence Guarantees
6.1 Lagrangian Relaxation
We now show that the example algorithms solve
their respective LP relaxations given in the previ-
ous section. We do this by first introducing a gen-
eral class of linear programs, together with an op-
timization method, Lagrangian relaxation, for solv-
ing these LPs. We then show that the algorithms in
section 4 are special cases of the general algorithm.
The linear programs we consider take the form
max
x1?X1,x2?X2
(?1 ? x1 + ?2 ? x2) such that Ex1 = Fx2
The matricesE ? Rq?m andF ? Rq?l specify q lin-
ear ?agreement? constraints between x1 ? Rm and
x2 ? Rl. The setsX1,X2 are also specified by linear
constraints, X1 = {x1 ? Rm : Ax1 = b, x1 ? 0}
and X2 =
{
x2 ? Rl : Cx2 = d, x2 ? 0
}
, hence the
problem is an LP.
Note that if we set X1 = conv(Y), X2 =
conv(Z), and define E and F to specify the agree-
ment constraints ?(i, t) = ?(i, t), then we have the
LP relaxation in Eq. 10.
It is natural to apply Lagrangian relaxation in
cases where the sub-problems maxx1?X1 ?1 ?x1 and
maxx2?X2 ?2 ? x2 can be efficiently solved by com-
binatorial algorithms for any values of ?1, ?2, but
where the constraints Ex1 = Fx2 ?complicate? the
problem. We introduce Lagrange multipliers u ? Rq
that enforce the latter set of constraints, giving the
Lagrangian:
L(u, x1, x2) = ?1 ? x1 + ?2 ? x2 + u ? (Ex1 ? Fx2)
The dual objective function is
L(u) = max
x1?X1,x2?X2
L(u, x1, x2)
and the dual problem is to find minu?Rq L(u).
Because X1 and X2 are defined by linear con-
straints, by strong duality we have
min
u?Rq
L(u) = max
x1?X1,x2?X2:Ex1=Fx2
(?1 ? x1 + ?2 ? x2)
Hence minimizing L(u) will recover the maximum
value of the original problem. This leaves open the
question of how to recover the LP solution (i.e., the
pair (x?1, x
?
2) that achieves this maximum); we dis-
cuss this point in section 6.2.
The dual L(u) is convex. However, L(u) is
not differentiable, so we cannot use gradient-based
methods to optimize it. Instead, a standard approach
is to use a subgradient method. Subgradients are tan-
gent lines that lower bound a function even at points
of non-differentiability: formally, a subgradient of a
convex function L : Rn ? R at a point u is a vector
gu such that for all v, L(v) ? L(u) + gu ? (v ? u).
7
u(1) ? 0
for k = 1 to K do
x(k)1 ? arg maxx1?X1(?1 + (u
(k))TE) ? x1
x(k)2 ? arg maxx2?X2(?2 ? (u
(k))TF ) ? x2
if Ex(k)1 = Fx
(k)
2 return u
(k)
u(k+1) ? u(k) ? ?k(Ex
(k)
1 ? Fx
(k)
2 )
return u(K)
Figure 4: The Lagrangian relaxation algorithm.
By standard results, the subgradient for L at a point
u takes a simple form, gu = Ex?1 ? Fx
?
2, where
x?1 = arg max
x1?X1
(?1 + (u
(k))TE) ? x1
x?2 = arg max
x2?X2
(?2 ? (u
(k))TF ) ? x2
The beauty of this result is that the values of x?1 and
x?2, and by implication the value of the subgradient,
can be computed using oracles for the two arg max
sub-problems.
Subgradient algorithms perform updates that are
similar to gradient descent:
u(k+1) ? u(k) ? ?kg
(k)
where g(k) is the subgradient ofL at u(k) and ?k > 0
is the step size of the update. The complete sub-
gradient algorithm is given in figure 4. The follow-
ing convergence theorem is well-known (e.g., see
page 120 of Korte and Vygen (2008)):
Theorem 6.1 If limk?? ?k = 0 and
??
k=1 ?k =
?, then limk?? L(u(k)) = minu L(u).
The following proposition is easily verified:
Proposition 6.1 The algorithm in figure 1 is an in-
stantiation of the algorithm in figure 4,8 with X1 =
conv(Y), X2 = conv(Z), and the matrices E and
F defined to be binary matrices specifying the con-
straints ?(i, t) = ?(i, t) for all (i, t) ? Iuni.
Under an appropriate definition of the step sizes ?k,
it follows that the algorithm in figure 1 defines a
sequence of Lagrange multiplers u(k) minimizing a
dual of the LP relaxation in Eq. 10. A similar result
holds for the algorithm in section 4.2.
8with the caveat that it returns (x(k)1 , x
(k)
2 ) rather than u
(k).
6.2 Recovering the LP Solution
The previous section described how the method in
figure 4 can be used to minimize the dualL(u) of the
original linear program. We now turn to the problem
of recovering a primal solution (x?1, x
?
2) of the LP.
The method we propose considers two cases:
(Case 1) If Ex(k)1 = Fx
(k)
2 at any stage during
the algorithm, then simply take (x(k)1 , x
(k)
2 ) to be the
primal solution. In this case the pair (x(k)1 , x
(k)
2 ) ex-
actly solves the original LP.9 If this case arises in the
algorithm in figure 1, then the resulting solution is
binary (i.e., it is a member of Q), and the solution
exactly solves the original inference problem.
(Case 2) If case 1 does not arise, then a couple of
strategies are possible. (This situation could arise
in cases where the LP is not tight?i.e., it has a
fractional solution?or where K is not large enough
for convergence.) The first is to define the pri-
mal solution to be the average of the solutions en-
countered during the algorithm: x?1 =
?
k x
(k)
1 /K,
x?2 =
?
k x
(k)
2 /K. Results from Nedic? and Ozdaglar
(2009) show that as K ? ?, these averaged solu-
tions converge to the optimal primal solution.10 A
second strategy (as given in figure 1) is to simply
take (x(K)1 , x
(K)
2 ) as an approximation to the primal
solution. This method is a heuristic, but previous
work (e.g., Komodakis et al (2007)) has shown that
it is effective in practice; we use it in this paper.
In our experiments we found that in the vast ma-
jority of cases, case 1 applies, after a small number
of iterations; see the next section for more details.
7 Experiments
7.1 Integrated Phrase-Structure and
Dependency Parsing
Our first set of experiments considers the integration
of Model 1 of Collins (2003) (a lexicalized phrase-
structure parser, from here on referred to as Model
9We have that ?1 ? x
(k)
1 + ?2 ? x
(k)
2 = L(u
(k), x(k)1 , x
(k)
2 ) =
L(u(k)), where the last equality is because x(k)1 and x
(k)
2 are de-
fined by the respective argmax?s. Thus, (x(k)1 , x
(k)
2 ) and u
(k)
are primal and dual optimal.
10The resulting fractional solution can be projected back to
the setQ, see (Smith and Eisner, 2008; Martins et al, 2009).
8
Itn. 1 2 3 4 5-10 11-20 20-50 **
Dep 43.5 20.1 10.2 4.9 14.0 5.7 1.4 0.4
POS 58.7 15.4 6.3 3.6 10.3 3.8 0.8 1.1
Table 1: Convergence results for Section 23 of the WSJ
Treebank for the dependency parsing and POS experi-
ments. Each column gives the percentage of sentences
whose exact solutions were found in a given range of sub-
gradient iterations. ** is the percentage of sentences that
did not converge by the iteration limit (K=50).
1),11 and the 2nd order discriminative dependency
parser of Koo et al (2008). The inference problem
for a sentence x is to find
y? = arg max
y?Y
(f1(y) + ?f2(y)) (11)
where Y is the set of all lexicalized phrase-structure
trees for the sentence x; f1(y) is the score (log prob-
ability) under Model 1; f2(y) is the score under Koo
et al (2008) for the dependency structure implied
by y; and ? > 0 is a parameter dictating the relative
weight of the two models.12 This problem is simi-
lar to the second example in section 4; a very sim-
ilar dual decomposition algorithm to that described
in section 4.2 can be derived.
We used the Penn Wall Street Treebank (Marcus
et al, 1994) for the experiments, with sections 2-21
for training, section 22 for development, and section
23 for testing. The parameter ? was chosen to opti-
mize performance on the development set.
We ran the dual decomposition algorithm with a
limit of K = 50 iterations. The dual decomposi-
tion algorithm returns an exact solution if case 1 oc-
curs as defined in section 6.2; we found that of 2416
sentences in section 23, case 1 occurred for 2407
(99.6%) sentences. Table 1 gives statistics showing
the number of iterations required for convergence.
Over 80% of the examples converge in 5 iterations or
fewer; over 90% converge in 10 iterations or fewer.
We compare the accuracy of the dual decomposi-
tion approach to two baselines: first, Model 1; and
second, a naive integration method that enforces the
hard constraint that Model 1 must only consider de-
11We use a reimplementation that is a slight modification of
Collins Model 1, with very similar performance, and which uses
the TAG formalism of Carreras et al (2008).
12Note that the models f1 and f2 were trained separately,
using the methods described by Collins (2003) and Koo et al
(2008) respectively.
Precision Recall F1 Dep
Model 1 88.4 87.8 88.1 91.4
Koo08 Baseline 89.9 89.6 89.7 93.3
DD Combination 91.0 90.4 90.7 93.8
Table 2: Performance results for Section 23 of the WSJ
Treebank. Model 1: a reimplementation of the genera-
tive parser of (Collins, 2002). Koo08 Baseline: Model 1
with a hard restriction to dependencies predicted by the
discriminative dependency parser of (Koo et al, 2008).
DD Combination: a model that maximizes the joint score
of the two parsers. Dep shows the unlabeled dependency
accuracy of each system.
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50
Per
cen
tage
Maximum Number of Dual Decomposition Iterations
f score% certificates% match K=50
Figure 5: Performance on the parsing task assuming a
fixed number of iterations K. f-score: accuracy of the
method. % certificates: percentage of examples for which
a certificate of optimality is provided. % match: percent-
age of cases where the output from the method is identical
to the output when using K = 50.
pendencies seen in the first-best output from the de-
pendency parser. Table 2 shows all three results. The
dual decomposition method gives a significant gain
in precision and recall over the naive combination
method, and boosts the performance of Model 1 to
a level that is close to some of the best single-pass
parsers on the Penn treebank test set. Dependency
accuracy is also improved over the Koo et al (2008)
model, in spite of the relatively low dependency ac-
curacy of Model 1 alone.
Figure 5 shows performance of the approach as a
function ofK, the maximum number of iterations of
dual decomposition. For this experiment, for cases
where the method has not converged for k ? K,
the output from the algorithm is chosen to be the
y(k) for k ? K that maximizes the objective func-
tion in Eq. 11. The graphs show that values of K
less than 50 produce almost identical performance to
K = 50, but with fewer cases giving certificates of
optimality (with K = 10, the f-score of the method
is 90.69%; with K = 5 it is 90.63%).
9
Precision Recall F1 POS Acc
Fixed Tags 88.1 87.6 87.9 96.7
DD Combination 88.7 88.0 88.3 97.1
Table 3: Performance results for Section 23 of the WSJ.
Model 1 (Fixed Tags): a baseline parser initialized to the
best tag sequence of from the tagger of Toutanova and
Manning (2000). DD Combination: a model that maxi-
mizes the joint score of parse and tag selection.
7.2 Integrated Phrase-Structure Parsing and
Trigram POS tagging
In a second experiment, we used dual decomposi-
tion to integrate the Model 1 parser with the Stan-
ford max-ent trigram POS tagger (Toutanova and
Manning, 2000), using a very similar algorithm to
that described in section 4.1. We use the same train-
ing/dev/test split as in section 7.1. The two models
were again trained separately.
We ran the algorithm with a limit of K = 50 it-
erations. Out of 2416 test examples, the algorithm
found an exact solution in 98.9% of the cases. Ta-
ble 1 gives statistics showing the speed of conver-
gence for different examples: over 94% of the exam-
ples converge to an exact solution in 10 iterations or
fewer. In terms of accuracy, we compare to a base-
line approach of using the first-best tag sequence
as input to the parser. The dual decomposition ap-
proach gives 88.3 F1 measure in recovering parse-
tree constituents, compared to 87.9 for the baseline.
8 Conclusions
We have introduced dual-decomposition algorithms
for inference in NLP, given formal properties of the
algorithms in terms of LP relaxations, and demon-
strated their effectiveness on problems that would
traditionally be solved using intersections of dy-
namic programs (Bar-Hillel et al, 1964). Given the
widespread use of dynamic programming in NLP,
there should be many applications for the approach.
There are several possible extensions of the
method we have described. We have focused on
cases where two models are being combined; the
extension to more than two models is straightfor-
ward (e.g., see Komodakis et al (2007)). This paper
has considered approaches for MAP inference; for
closely related methods that compute approximate
marginals, see Wainwright et al (2005b).
A Fractional Solutions
We now give an example of a point (?, ?) ? Q?\conv(Q)
that demonstrates that the relaxation Q? is strictly larger
than conv(Q). Fractional points such as this one can arise
as solutions of the LP relaxation for worst case instances,
preventing us from finding an exact solution.
Recall that the constraints for Q? specify that ? ?
conv(Y), ? ? conv(Z), and ?(i, t) = ?(i, t) for all
(i, t) ? Iuni. Since ? ? conv(Y), ? must be a con-
vex combination of 1 or more members of Y; a similar
property holds for ?. The example is as follows. There
are two possible parts of speech, A and B, and an addi-
tional non-terminal symbol X . The sentence is of length
3, w1 w2 w3. Let ? be the convex combination of the
following two tag sequences, each with probability 0.5:
w1/A w2/A w3/A and w1/A w2/B w3/B. Let ? be
the convex combination of the following two parses, each
with probability 0.5: (X(A w1)(X(A w2)(B w3))) and
(X(A w1)(X(B w2)(A w3))). It can be verified that
?(i, t) = ?(i, t) for all (i, t), i.e., the marginals for single
tags for ? and ? agree. Thus, (?, ?) ? Q?.
To demonstrate that this fractional point is not in
conv(Q), we give parameter values such that this frac-
tional point is optimal and all integral points (i.e., ac-
tual parses) are suboptimal. For the tagging model, set
?(AA? A, 3) = ?(AB ? B, 3) = 0, with all other pa-
rameters having a negative value. For the parsing model,
set ?(X ? A X, 1, 1, 3) = ?(X ? A B, 2, 2, 3) =
?(X ? B A, 2, 2, 3) = 0, with all other rule parameters
being negative. For this objective, the fractional solution
has value 0, while all integral points (i.e., all points inQ)
have a negative value. By Theorem 5.2, the maximum of
any linear objective over conv(Q) is equal to the maxi-
mum over Q. Thus, (?, ?) 6? conv(Q).
B Step Size
We used the following step size in our experiments. First,
we initialized ?0 to equal 0.5, a relatively large value.
Then we defined ?k = ?0 ? 2??k , where ?k is the num-
ber of times that L(u(k
?)) > L(u(k
??1)) for k? ? k. This
learning rate drops at a rate of 1/2t, where t is the num-
ber of times that the dual increases from one iteration to
the next. See Koo et al (2010) for a similar, but less ag-
gressive step size used to solve a different task.
Acknowledgments MIT gratefully acknowledges the
support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government. Alexander Rush was supported under the GALE
program of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022. David Sontag was supported
by a Google PhD Fellowship.
10
References
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In Proc CONLL, pages 9?16.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. CoNLL, pages
957?961.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP, page 8.
M. Collins. 2003. Head-driven statistical models for nat-
ural language parsing. In Computational linguistics,
volume 29, pages 589?637.
G.B. Dantzig and P. Wolfe. 1960. Decomposition princi-
ple for linear programs. In Operations research, vol-
ume 8, pages 101?111.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. In NIPS, volume 19.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Advances in Probabilistic and
Other Parsing Technologies, pages 29?62.
A. Globerson and T. Jaakkola. 2007. Fixing max-
product: Convergent message passing algorithms for
MAP LP-relaxations. In NIPS, volume 21.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In International Conference on
Computer Vision.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. ACL/HLT.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual Decomposition for Parsing with Non-
Projective Head Automata. In Proc. EMNLP, pages
63?70.
B.H. Korte and J. Vygen. 2008. Combinatorial optimiza-
tion: theory and algorithms. Springer Verlag.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of English:
The Penn Treebank. In Computational linguistics, vol-
ume 19, pages 313?330.
R.K. Martin, R.L. Rardin, and B.A. Campbell. 1990.
Polyhedral characterization of discrete dynamic pro-
gramming. Operations research, 38(1):127?138.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT/EMNLP, pages 523?
530.
Angelia Nedic? and Asuman Ozdaglar. 2009. Approxi-
mate primal solutions and rate analysis for dual sub-
gradient methods. SIAM Journal on Optimization,
19(4):1757?1780.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. ACL.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. EMNLP, pages 129?137.
D. Roth and W. Yih. 2005. Integer linear program-
ming inference for conditional random fields. In Proc.
ICML, pages 737?744.
Hanif D. Sherali and Warren P. Adams. 1994. A hi-
erarchy of relaxations and convex hull characteriza-
tions for mixed-integer zero?one programming prob-
lems. Discrete Applied Mathematics, 52(1):83 ? 106.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. EMNLP, pages 145?156.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proc. UAI.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP,
pages 1?8.
K. Toutanova and C.D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. EMNLP, pages 63?70.
M. Wainwright and M. I. Jordan. 2008. Graphical Mod-
els, Exponential Families, and Variational Inference.
Now Publishers Inc., Hanover, MA, USA.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005a.
MAP estimation via agreement on trees: message-
passing and linear programming. In IEEE Transac-
tions on Information Theory, volume 51, pages 3697?
3717.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005b. A
new class of upper bounds on the log partition func-
tion. In IEEE Transactions on Information Theory,
volume 51, pages 2313?2335.
C. Yanover, T. Meltzer, and Y. Weiss. 2006. Linear
Programming Relaxations and Belief Propagation?An
Empirical Study. In The Journal of Machine Learning
Research, volume 7, page 1907. MIT Press.
11
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288?1298,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Dual Decomposition for Parsing with Non-Projective Head Automata
Terry Koo Alexander M. Rush Michael Collins Tommi Jaakkola David Sontag
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,srush,mcollins,tommi,dsontag}@csail.mit.edu
Abstract
This paper introduces algorithms for non-
projective parsing based on dual decomposi-
tion. We focus on parsing algorithms for non-
projective head automata, a generalization of
head-automata models to non-projective struc-
tures. The dual decomposition algorithms are
simple and efficient, relying on standard dy-
namic programming and minimum spanning
tree algorithms. They provably solve an LP
relaxation of the non-projective parsing prob-
lem. Empirically the LP relaxation is very of-
ten tight: for many languages, exact solutions
are achieved on over 98% of test sentences.
The accuracy of our models is higher than pre-
vious work on a broad range of datasets.
1 Introduction
Non-projective dependency parsing is useful for
many languages that exhibit non-projective syntactic
structures. Unfortunately, the non-projective parsing
problem is known to be NP-hard for all but the sim-
plest models (McDonald and Satta, 2007). There has
been a long history in combinatorial optimization of
methods that exploit structure in complex problems,
using methods such as dual decomposition or La-
grangian relaxation (Lemare?chal, 2001). Thus far,
however, these methods are not widely used in NLP.
This paper introduces algorithms for non-
projective parsing based on dual decomposition. We
focus on parsing algorithms for non-projective head
automata, a generalization of the head-automata
models of Eisner (2000) and Alshawi (1996) to non-
projective structures. These models include non-
projective dependency parsing models with higher-
order (e.g., sibling and/or grandparent) dependency
relations as a special case. Although decoding of full
parse structures with non-projective head automata
is intractable, we leverage the observation that key
components of the decoding can be efficiently com-
puted using combinatorial algorithms. In particular,
1. Decoding for individual head-words can be ac-
complished using dynamic programming.
2. Decoding for arc-factored models can be ac-
complished using directed minimum-weight
spanning tree (MST) algorithms.
The resulting parsing algorithms have the following
properties:
? They are efficient and easy to implement, relying
on standard dynamic programming and MST al-
gorithms.
? They provably solve a linear programming (LP)
relaxation of the original decoding problem.
? Empirically the algorithms very often give an ex-
act solution to the decoding problem, in which
case they also provide a certificate of optimality.
In this paper we first give the definition for non-
projective head automata, and describe the parsing
algorithm. The algorithm can be viewed as an in-
stance of Lagrangian relaxation; we describe this
connection, and give convergence guarantees for the
method. We describe a generalization to models
that include grandparent dependencies. We then in-
troduce a perceptron-driven training algorithm that
makes use of point 1 above.
We describe experiments on non-projective pars-
ing for a number of languages, and in particu-
lar compare the dual decomposition algorithm to
approaches based on general-purpose linear pro-
gramming (LP) or integer linear programming (ILP)
solvers (Martins et al, 2009). The accuracy of our
models is higher than previous work on a broad
range of datasets. The method gives exact solutions
to the decoding problem, together with a certificate
of optimality, on over 98% of test examples for many
of the test languages, with parsing times ranging be-
tween 0.021 seconds/sentence for the most simple
languages/models, to 0.295 seconds/sentence for the
1288
most complex settings. The method compares favor-
ably to previous work using LP/ILP formulations,
both in terms of efficiency, and also in terms of the
percentage of exact solutions returned.
While the focus of the current paper is on non-
projective dependency parsing, the approach opens
up new ways of thinking about parsing algorithms
for lexicalized formalisms such as TAG (Joshi and
Schabes, 1997), CCG (Steedman, 2000), and pro-
jective head automata.
2 Related Work
McDonald et al (2005) describe MST-based parsing
for non-projective dependency parsing models with
arc-factored decompositions; McDonald and Pereira
(2006) make use of an approximate (hill-climbing)
algorithm for parsing with more complex models.
McDonald and Pereira (2006) and McDonald and
Satta (2007) describe complexity results for non-
projective parsing, showing that parsing for a variety
of models is NP-hard. Riedel and Clarke (2006) de-
scribe ILP methods for the problem; Martins et al
(2009) recently introduced alternative LP and ILP
formulations. Our algorithm differs in that we do not
use general-purpose LP or ILP solvers, instead using
an MST solver in combination with dynamic pro-
gramming; thus we leverage the underlying struc-
ture of the problem, thereby deriving more efficient
decoding algorithms.
Both dual decomposition and Lagrangian relax-
ation have a long history in combinatorial optimiza-
tion. Our work was originally inspired by recent
work on dual decomposition for inference in graph-
ical models (Wainwright et al, 2005; Komodakis
et al, 2007). However, the non-projective parsing
problem has a very different structure from these
models, and the decomposition we use is very dif-
ferent in nature from those used in graphical mod-
els. Other work has made extensive use of de-
composition approaches for efficiently solving LP
relaxations for graphical models (e.g., Sontag et
al. (2008)). Methods that incorporate combinato-
rial solvers within loopy belief propagation (LBP)
(Duchi et al, 2007; Smith and Eisner, 2008) are
also closely related to our approach. Unlike LBP,
our method has strong theoretical guarantees, such
as guaranteed convergence and the possibility of a
certificate of optimality.
Finally, in other recent work, Rush et al (2010)
describe dual decomposition approaches for other
NLP problems.
3 Sibling Models
This section describes a particular class of models,
sibling models; the next section describes a dual-
decomposition algorithm for decoding these models.
Consider the dependency parsing problem for a
sentence with n words. We define the index set
for dependency parsing to be I = {(i, j) : i ?
{0 . . . n}, j ? {1 . . . n}, i 6= j}. A dependency
parse is a vector y = {y(i, j) : (i, j) ? I}, where
y(i, j) = 1 if a dependency with head word i and
modifier j is in the parse, 0 otherwise. We use i = 0
for the root symbol. We define Y to be the set of all
well-formed non-projective dependency parses (i.e.,
the set of directed spanning trees rooted at node 0).
Given a function f : Y 7? R that assigns scores to
parse trees, the optimal parse is
y? = argmax
y?Y
f(y) (1)
A particularly simple definition of f(y) is f(y) =
?
(i,j)?I y(i, j)?(i, j) where ?(i, j) is the score for
dependency (i, j). Models with this form are often
referred to as arc-factored models. In this case the
optimal parse tree y? can be found efficiently using
MST algorithms (McDonald et al, 2005).
This paper describes algorithms that compute y?
for more complex definitions of f(y); in this sec-
tion, we focus on algorithms for models that capture
interactions between sibling dependencies. To this
end, we will find it convenient to define the follow-
ing notation. Given a vector y, define
y|i = {y(i, j) : j = 1 . . . n, j 6= i}
Hence y|i specifies the set of modifiers to word i;
note that the vectors y|i for i = 0 . . . n form a parti-
tion of the full set of variables.
We then assume that f(y) takes the form
f(y) =
n?
i=0
fi(y|i) (2)
Thus f(y) decomposes into a sum of terms, where
each fi considers modifiers to the i?th word alone.
In the general case, finding y? =
argmaxy?Y f(y) under this definition of f(y)
is an NP-hard problem. However for certain
1289
definitions of fi, it is possible to efficiently compute
argmaxy|i?Zi fi(y|i) for any value of i, typically
using dynamic programming. (Here we use Zi to
refer to the set of all possible values for y|i: specifi-
cally, Z0 = {0, 1}n and for i 6= 0, Zi = {0, 1}n?1.)
In these cases we can efficiently compute
z? = argmax
z?Z
f(z) = argmax
z?Z
?
i
fi(z|i) (3)
where Z = {z : z|i ? Zi for i = 0 . . . n} by
simply computing z?|i = argmaxz|i?Zi fi(z|i) for
i = 0 . . . n. Eq. 3 can be considered to be an approx-
imation to Eq. 1, where we have replaced Y with
Z . We will make direct use of this approximation
in the dual decomposition parsing algorithm. Note
that Y ? Z , and in all but trivial cases, Y is a strict
subset of Z . For example, a structure z ? Z could
have z(i, j) = z(j, i) = 1 for some (i, j); it could
contain longer cycles; or it could contain words that
do not modify exactly one head. Nevertheless, with
suitably powerful functions fi?for example func-
tions based on discriminative models?z? may be a
good approximation to y?. Later we will see that
dual decomposition can effectively use MST infer-
ence to rule out ill-formed structures.
We now give the main assumption underlying sib-
ling models:
Assumption 1 (Sibling Decompositions) A model
f(y) satisfies the sibling-decomposition assumption
if: 1) f(y) =
?n
i=0 fi(y|i) for some set of functions
f0 . . . fn. 2) For any i ? {0 . . . n}, for any value
of the variables u(i, j) ? R for j = 1 . . . n, it is
possible to compute
argmax
y|i?Zi
?
?fi(y|i)?
?
j
u(i, j)y(i, j)
?
?
in polynomial time.
The second condition includes additional terms in-
volving u(i, j) variables that modify the scores of
individual dependencies. These terms are benign for
most definitions of fi, in that they do not alter de-
coding complexity. They will be of direct use in the
dual decomposition parsing algorithm.
Example 1: Bigram Sibling Models. Recall that
y|i is a binary vector specifying which words are
modifiers to the head-word i. Define l1 . . . lp to be
the sequence of left modifiers to word i under y|i,
and r1 . . . rq to be the set of right modifiers (e.g.,
consider the case where n = 5, i = 3, and we have
y(3, 1) = y(3, 5) = 0, and y(3, 2) = y(3, 4) = 1:
in this case p = 1, l1 = 2, and q = 1, r1 = 4). In
bigram sibling models, we have
fi(y|i) =
p+1?
k=1
gL(i, lk?1, lk) +
q+1?
k=1
gR(i, rk?1, rk)
where l0 = r0 = START is the initial state, and
lp+1 = rq+1 = END is the end state. The functions
gL and gR assign scores to bigram dependencies to
the left and right of the head. Under this model cal-
culating argmaxy|i?Zi
(
fi(y|i)?
?
j u(i, j)y(i, j)
)
takes O(n2) time using dynamic programming,
hence the model satisfies Assumption 1.
Example 2: Head Automata Head-automata
models constitute a second important model type
that satisfy the sibling-decomposition assumption
(bigram sibling models are a special case of head
automata). These models make use of functions
gR(i, s, s?, r) where s ? S, s? ? S are variables in a
set of possible states S, and r is an index of a word
in the sentence such that i < r ? n. The function
gR returns a cost for taking word r as the next depen-
dency, and transitioning from state s to s?. A similar
function gL is defined for left modifiers. We define
fi(y|i, s0 . . . sq, t0 . . . tp) =
q?
k=1
gR(i, sk?1, sk, rk) +
p?
k=1
gL(i, tk?1, tk, ll)
to be the joint score for dependencies y|i, and left
and right state sequences s0 . . . sq and t0 . . . tp. We
specify that s0 = t0 = START and sq = tp = END.
In this case we define
fi(y|i) = maxs0...sq ,t0...tp
fi(y|i, s0 . . . sq, t0 . . . tp)
and it follows that argmaxy|i?Zi fi(y|i) can be com-
puted inO(n|S|2) time using a variant of the Viterbi
algorithm, hence the model satisfies the sibling-
decomposition assumption.
4 The Parsing Algorithm
We now describe the dual decomposition parsing al-
gorithm for models that satisfy Assumption 1. Con-
sider the following generalization of the decoding
1290
Set u(1)(i, j)? 0 for all (i, j) ? I
for k = 1 to K do
y(k) ? argmax
y?Y
?
(i,j)?I
(
?(i, j) + u(k)(i, j)
)
y(i, j)
for i ? {0 . . . n},
z(k)|i ? argmax
z|i?Zi
(fi(z|i)?
?
j
u(k)(i, j)z(i, j))
if y(k)(i, j) = z(k)(i, j) for all (i, j) ? I then
return (y(k), z(k))
for all (i, j) ? I,
u(k+1)(i, j)? u(k)(i, j)+?k(z(k)(i, j)?y(k)(i, j))
return (y(K), z(K))
Figure 1: The parsing algorithm for sibling decompos-
able models. ?k ? 0 for k = 1 . . .K are step sizes, see
Appendix A for details.
problem from Eq. 1, where f(y) =
?
i fi(y|i),
h(y) =
?
(i,j)?I ?(i, j)y(i, j), and ?(i, j) ? R for
all (i, j):1
argmax
z?Z,y?Y
f(z) + h(y) (4)
such that z(i, j) = y(i, j) for all (i, j) ? I (5)
Although the maximization w.r.t. z is taken over the
set Z , the constraints in Eq. 5 ensure that z = y for
some y ? Y , and hence that z ? Y .
Without the z(i, j) = y(i, j) constraints, the
objective would decompose into the separate max-
imizations z? = argmaxz?Z f(z), and y
? =
argmaxy?Y h(y), which can be easily solved us-
ing dynamic programming and MST, respectively.
Thus, it is these constraints that complicate the op-
timization. Our approach gets around this difficulty
by introducing new variables, u(i, j), that serve to
enforce agreement between the y(i, j) and z(i, j)
variables. In the next section we will show that these
u(i, j) variables are actually Lagrange multipliers
for the z(i, j) = y(i, j) constraints.
Our parsing algorithm is shown in Figure 1. At
each iteration k, the algorithm finds y(k) ? Y us-
ing an MST algorithm, and z(k) ? Z through sep-
arate decoding of the (n + 1) sibling models. The
u(k) variables are updated if y(k)(i, j) 6= z(k)(i, j)
1This is equivalent to Eq. 1 when ?(i, j) = 0 for all (i, j).
In some cases, however, it is convenient to have a model with
non-zero values for the ? variables; see the Appendix. Note that
this definition of h(y) allows argmaxy?Y h(y) to be calculated
efficiently, using MST inference.
for some (i, j); these updates modify the objective
functions for the two decoding steps, and intuitively
encourage the y(k) and z(k) variables to be equal.
4.1 Lagrangian Relaxation
Recall that the main difficulty in solving Eq. 4 was
the z = y constraints. We deal with these con-
straints using Lagrangian relaxation (Lemare?chal,
2001). We first introduce Lagrange multipliers u =
{u(i, j) : (i, j) ? I}, and define the Lagrangian
L(u, y, z) = (6)
f(z) + h(y) +
?
(i,j)?I
u(i, j)
(
y(i, j)? z(i, j)
)
If L? is the optimal value of Eq. 4 subject to the
constraints in Eq. 5, then for any value of u,
L? = max
z?Z,y?Y,y=z
L(u, y, z) (7)
This follows because if y = z, the right term in Eq. 6
is zero for any value of u. The dual objective L(u)
is obtained by omitting the y = z constraint:
L(u) = max
z?Z,y?Y
L(u, y, z)
= max
z?Z
(
f(z)?
?
i,j
u(i, j)z(i, j)
)
+max
y?Y
(
h(y) +
?
i,j
u(i, j)y(i, j)
)
.
Since L(u) maximizes over a larger space (y may
not equal z), we have that L? ? L(u) (compare this
to Eq. 7). The dual problem, which our algorithm
optimizes, is to obtain the tightest such upper bound,
(Dual problem) min
u?R|I|
L(u). (8)
The dual objective L(u) is convex, but not differen-
tiable. However, we can use a subgradient method
to derive an algorithm that is similar to gradient de-
scent, and which minimizes L(u). A subgradient of
a convex function L(u) at u is a vector du such that
for all v ? R|I|, L(v) ? L(u) + du ? (v ? u). By
standard results,
du(k) = y
(k) ? z(k)
is a subgradient for L(u) at u = u(k), where z(k) =
argmaxz?Z f(z)?
?
i,j u
(k)(i, j)z(i, j) and y(k) =
1291
argmaxy?Y h(y) +
?
i,j u
(k)(i, j)y(i, j). Subgra-
dient optimization methods are iterative algorithms
with updates that are similar to gradient descent:
u(k+1) = u(k) ? ?kdu(k) = u
(k) ? ?k(y
(k) ? z(k)),
where ?k is a step size. It is easily verified that the
algorithm in Figure 1 uses precisely these updates.
4.2 Formal Guarantees
With an appropriate choice of the step sizes ?k, the
subgradient method can be shown to solve the dual
problem, i.e.
lim
k??
L(u(k)) = min
u
L(u).
See Korte and Vygen (2008), page 120, for details.
As mentioned before, the dual provides an up-
per bound on the optimum of the primal problem
(Eq. 4),
max
z?Z,y?Y,y=z
f(z) + h(y) ? min
u?R|I|
L(u). (9)
However, we do not necessarily have strong
duality?i.e., equality in the above equation?
because the sets Z and Y are discrete sets. That
said, for some functions h(y) and f(z) strong du-
ality does hold, as stated in the following:
Theorem 1 If for some k ? {1 . . .K} in the al-
gorithm in Figure 1, y(k)(i, j) = z(k)(i, j) for all
(i, j) ? I, then (y(k), z(k)) is a solution to the max-
imization problem in Eq. 4.
Proof. We have that f(z(k)) + h(y(k)) =
L(u(k), z(k), y(k)) = L(u(k)), where the last equal-
ity is because y(k), z(k) are defined as the respective
argmax?s. Thus, the inequality in Eq. 9 is tight, and
(y(k), z(k)) and u(k) are primal and dual optimal.
Although the algorithm is not guaranteed to sat-
isfy y(k) = z(k) for some k, by Theorem 1 if it does
reach such a state, then we have the guarantee of an
exact solution to Eq. 4, with the dual solution u pro-
viding a certificate of optimality. We show in the
experiments that this occurs very frequently, in spite
of the parsing problem being NP-hard.
It can be shown that Eq. 8 is the dual of an LP
relaxation of the original problem. When the con-
ditions of Theorem 1 are satisfied, it means that the
LP relaxation is tight for this instance. For brevity
we omit the details, except to note that when the LP
relaxation is not tight, the optimal primal solution to
the LP relaxation could be recovered by averaging
methods (Nedic? and Ozdaglar, 2009).
5 Grandparent Dependency Models
In this section we extend the approach to consider
grandparent relations. In grandparent models each
parse tree y is represented as a vector
y = {y(i, j) : (i, j) ? I} ? {y?(i, j) : (i, j) ? I}
where we have added a second set of duplicate vari-
ables, y?(i, j) for all (i, j) ? I. The set of all valid
parse trees is then defined as
Y = {y : y(i, j) variables form a directed tree,
y?(i, j) = y(i, j) for all (i, j) ? I}
We again partition the variables into n + 1 subsets,
y|0 . . . y|n, by (re)defining
y|i = {y(i, j) : j = 1 . . . n, j 6= i}
?{y?(k, i) : k = 0 . . . n, k 6= i}
So as before y|i contains variables y(i, j) which in-
dicate which words modify the i?th word. In addi-
tion, y|i includes y?(k, i) variables that indicate the
word that word i itself modifies.
The set of all possible values of y|i is now
Zi = {y|i : y(i, j) ? {0, 1} for j = 1 . . . n, j 6= i;
y?(k, i) ? {0, 1} for k = 0 . . . n, k 6= i;
?
k
y?(k, i) = 1}
Hence the y(i, j) variables can take any values, but
only one of the y?(k, i) variables can be equal to 1
(as only one word can be a parent of word i). As be-
fore, we define Z = {y : y|i ? Zi for i = 0 . . . n}.
We introduce the following assumption:
Assumption 2 (GS Decompositions)
A model f(y) satisfies the grandparent/sibling-
decomposition (GSD) assumption if: 1) f(z) =
?n
i=0 fi(z|i) for some set of functions f0 . . . fn. 2)
For any i ? {0 . . . n}, for any value of the variables
u(i, j) ? R for j = 1 . . . n, and v(k, i) ? R for
k = 0 . . . n, it is possible to compute
argmax
z|i?Zi
(fi(z|i)?
?
j
u(i, j)z(i, j)?
?
k
v(k, i)z?(k, i))
in polynomial time.
1292
Again, it follows that we can approxi-
mate y? = argmaxy?Y
?n
i=0 fi(y|i) by
z? = argmaxz?Z
?n
i=0 fi(z|i), by defining
z?|i = argmaxz|i?Zi fi(z|i) for i = 0 . . . n. The
resulting vector z? may be deficient in two respects.
First, the variables z?(i, j) may not form a well-
formed directed spanning tree. Second, we may
have z??(i, j) 6= z
?(i, j) for some values of (i, j).
Example 3: Grandparent/Sibling Models An
important class of models that satisfy Assumption 2
are defined as follows. Again, for a vector y|i de-
fine l1 . . . lp to be the sequence of left modifiers to
word i under y|i, and r1 . . . rq to be the set of right
modifiers. Define k? to the value for k such that
y?(k, i) = 1. Then the model is defined as follows:
fi(y|i) =
p+1?
j=1
gL(i, k
?, lj?1, lj)+
q+1?
j=1
gR(i, k
?, rj?1, rj)
This is very similar to the bigram-sibling model, but
with the modification that the gL and gR functions
depend in addition on the value for k?. This al-
lows these functions to model grandparent depen-
dencies such as (k?, i, lj) and sibling dependencies
such as (i, lj?1, lj). Finding z?|i under the definition
can be accomplished inO(n3) time, by decoding the
model using dynamic programming separately for
each of the O(n) possible values of k?, and pick-
ing the value for k? that gives the maximum value
under these decodings.
A dual-decomposition algorithm for models that
satisfy the GSD assumption is shown in Figure 2.
The algorithm can be justified as an instance of La-
grangian relaxation applied to the problem
argmax
z?Z,y?Y
f(z) + h(y) (10)
with constraints
z(i, j) = y(i, j) for all (i, j) ? I (11)
z?(i, j) = y(i, j) for all (i, j) ? I (12)
The algorithm employs two sets of Lagrange mul-
tipliers, u(i, j) and v(i, j), corresponding to con-
straints in Eqs. 11 and 12. As in Theorem 1, if at any
point in the algorithm z(k) = y(k), then (z(k), y(k))
is an exact solution to the problem in Eq. 10.
Set u(1)(i, j)? 0, v(1)(i, j)? 0 for all (i, j) ? I
for k = 1 to K do
y(k) ? argmax
y?Y
?
(i,j)?I
y(i, j)?(i, j)
where ?(i, j) = ?(i, j) + u(k)(i, j) + v(k)(i, j).
for i ? {0 . . . n},
z(k)|i ? argmax
z|i?Zi
(fi(z|i) ?
?
j
u(k)(i, j)z(i, j)
?
?
j
v(k)(j, i)z?(j, i))
if y(k)(i, j) = z(k)(i, j) = z(k)? (i, j) for all (i, j) ? I
then
return (y(k), z(k))
for all (i, j) ? I,
u(k+1)(i, j)? u(k)(i, j)+?k(z(k)(i, j)?y(k)(i, j))
v(k+1)(i, j)? v(k)(i, j)+?k(z
(k)
? (i, j)?y
(k)(i, j))
return (y(K), z(K))
Figure 2: The parsing algorithm for grandparent/sibling-
decomposable models.
6 The Training Algorithm
In our experiments we make use of discriminative
linear models, where for an input sentence x, the
score for a parse y is f(y) = w ? ?(x, y) where
w ? Rd is a parameter vector, and ?(x, y) ? Rd
is a feature-vector representing parse tree y in con-
junction with sentence x. We will assume that the
features decompose in the same way as the sibling-
decomposable or grandparent/sibling-decomposable
models, that is ?(x, y) =
?n
i=0 ?(x, y|i) for some
feature vector definition ?(x, y|i). In the bigram sib-
ling models in our experiments, we assume that
?(x, y|i) =
p+1?
k=1
?L(x, i, lk?1, lk) +
q+1?
k=1
?R(x, i, rk?1, rk)
where as before l1 . . . lp and r1 . . . rq are left and
right modifiers under y|i, and where ?L and ?R
are feature vector definitions. In the grandparent
models in our experiments, we use a similar defi-
nition with feature vectors ?L(x, i, k?, lk?1, lk) and
?R(x, i, k?, rk?1, rk), where k? is the parent for
word i under y|i.
We train the model using the averaged perceptron
for structured problems (Collins, 2002). Given the
i?th example in the training set, (x(i), y(i)), the per-
ceptron updates are as follows:
? z? = argmaxy?Z w ? ?(x
(i), y)
? If z? 6= y(i), w = w+?(x(i), y(i))??(x(i), z?)
1293
The first step involves inference over the set Z ,
rather than Y as would be standard in the percep-
tron. Thus, decoding during training can be achieved
by dynamic programming over head automata alone,
which is very efficient.
Our training approach is closely related to local
training methods (Punyakanok et al, 2005). We
have found this method to be effective, very likely
because Z is a superset of Y . Our training algo-
rithm is also related to recent work on training using
outer bounds (see, e.g., (Taskar et al, 2003; Fin-
ley and Joachims, 2008; Kulesza and Pereira, 2008;
Martins et al, 2009)). Note, however, that the LP re-
laxation optimized by dual decomposition is signifi-
cantly tighter than Z . Thus, an alternative approach
would be to use the dual decomposition algorithm
for inference during training.
7 Experiments
We report results on a number of data sets. For
comparison to Martins et al (2009), we perform ex-
periments for Danish, Dutch, Portuguese, Slovene,
Swedish and Turkish data from the CoNLL-X
shared task (Buchholz and Marsi, 2006), and En-
glish data from the CoNLL-2008 shared task (Sur-
deanu et al, 2008). We use the official training/test
splits for these data sets, and the same evaluation
methodology as Martins et al (2009). For com-
parison to Smith and Eisner (2008), we also re-
port results on Danish and Dutch using their alter-
nate training/test split. Finally, we report results on
the English WSJ treebank, and the Prague treebank.
We use feature sets that are very similar to those
described in Carreras (2007). We use marginal-
based pruning, using marginals calculated from an
arc-factored spanning tree model using the matrix-
tree theorem (McDonald and Satta, 2007; Smith and
Smith, 2007; Koo et al, 2007).
In all of our experiments we set the value K, the
maximum number of iterations of dual decompo-
sition in Figures 1 and 2, to be 5,000. If the al-
gorithm does not terminate?i.e., it does not return
(y(k), z(k)) within 5,000 iterations?we simply take
the parse y(k) with the maximum value of f(y(k)) as
the output from the algorithm. At first sight 5,000
might appear to be a large number, but decoding is
still fast?see Sections 7.3 and 7.4 for discussion.2
2Note also that the feature vectors ? and inner productsw ??
The strategy for choosing step sizes ?k is described
in Appendix A, along with other details.
We first discuss performance in terms of accu-
racy, success in recovering an exact solution, and
parsing speed. We then describe additional experi-
ments examining various aspects of the algorithm.
7.1 Accuracy
Table 1 shows results for previous work on the var-
ious data sets, and results for an arc-factored model
with pure MST decoding with our features. (We use
the acronym UAS (unlabeled attachment score) for
dependency accuracy.) We also show results for the
bigram-sibling and grandparent/sibling (G+S) mod-
els under dual decomposition. Both the bigram-
sibling and G+S models show large improvements
over the arc-factored approach; they also compare
favorably to previous work?for example the G+S
model gives better results than all results reported in
the CoNLL-X shared task, on all languages. Note
that we use different feature sets from both Martins
et al (2009) and Smith and Eisner (2008).
7.2 Success in Recovering Exact Solutions
Next, we consider how often our algorithms return
an exact solution to the original optimization prob-
lem, with a certificate?i.e., how often the algo-
rithms in Figures 1 and 2 terminate with y(k) = z(k)
for some value of k < 5000 (and are thus optimal,
by Theorem 1). The CertS and CertG columns in Ta-
ble 1 give the results for the sibling and G+S models
respectively. For all but one setting3 over 95% of the
test sentences are decoded exactly, with 99% exact-
ness in many cases.
For comparison, we also ran both the single-
commodity flow and multiple-commodity flow LP
relaxations of Martins et al (2009) with our mod-
els and features. We measure how often these re-
laxations terminate with an exact solution. The re-
sults in Table 2 show that our method gives exact
solutions more often than both of these relaxations.4
In computing the accuracy figures for Martins et al
only need to be computed once, thus saving computation.
3The exception is Slovene, which has the smallest training
set at only 1534 sentences.
4Note, however, that it is possible that the Martins et al re-
laxations would have given a higher proportion of integral solu-
tions if their relaxation was used during training.
1294
Ma09 MST Sib G+S Best CertS CertG TimeS TimeG TrainS TrainG
Dan 91.18 89.74 91.08 91.78 91.54 99.07 98.45 0.053 0.169 0.051 0.109
Dut 85.57 82.33 84.81 85.81 85.57 98.19 97.93 0.035 0.120 0.046 0.048
Por 92.11 90.68 92.57 93.03 92.11 99.65 99.31 0.047 0.257 0.077 0.103
Slo 85.61 82.39 84.89 86.21 85.61 90.55 95.27 0.158 0.295 0.054 0.130
Swe 90.60 88.79 90.10 91.36 90.60 98.71 98.97 0.035 0.141 0.036 0.055
Tur 76.34 75.66 77.14 77.55 76.36 98.72 99.04 0.021 0.047 0.016 0.036
Eng1 91.16 89.20 91.18 91.59 ? 98.65 99.18 0.082 0.200 0.032 0.076
Eng2 ? 90.29 92.03 92.57 ? 98.96 99.12 0.081 0.168 0.032 0.076
Sm08 MST Sib G+S ? CertS CertG TimeS TimeG TrainS TrainG
Dan 86.5 87.89 89.58 91.00 ? 98.50 98.50 0.043 0.120 0.053 0.065
Dut 88.5 88.86 90.87 91.76 ? 98.00 99.50 0.036 0.046 0.050 0.054
Mc06 MST Sib G+S ? CertS CertG TimeS TimeG TrainS TrainG
PTB 91.5 90.10 91.96 92.46 ? 98.89 98.63 0.062 0.210 0.028 0.078
PDT 85.2 84.36 86.44 87.32 ? 96.67 96.43 0.063 0.221 0.019 0.051
Table 1: A comparison of non-projective automaton-based parsers with results from previous work. MST: Our first-
order baseline. Sib/G+S: Non-projective head automata with sibling or grandparent/sibling interactions, decoded via
dual decomposition. Ma09: The best UAS of the LP/ILP-based parsers introduced in Martins et al (2009). Sm08:
The best UAS of any LBP-based parser in Smith and Eisner (2008). Mc06: The best UAS reported by McDonald
and Pereira (2006). Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task
(Buchholz and Marsi, 2006) or in any column of Martins et al (2009, Table 1); note that the latter includes McDonald
and Pereira (2006), Nivre and McDonald (2008), and Martins et al (2008). CertS/CertG: Percent of test examples
for which dual decomposition produced a certificate of optimality, for Sib/G+S. TimeS/TimeG: Seconds/sentence for
test decoding, for Sib/G+S. TrainS/TrainG: Seconds/sentence during training, for Sib/G+S. For consistency of timing,
test decoding was carried out on identical machines with zero additional load; however, training was conducted on
machines with varying hardware and load. We ran two tests on the CoNLL-08 corpus. Eng1: UAS when testing on
the CoNLL-08 validation set, following Martins et al (2009). Eng2: UAS when testing on the CoNLL-08 test set.
(2009), we project fractional solutions to a well-
formed spanning tree, as described in that paper.
Finally, to better compare the tightness of our
LP relaxation to that of earlier work, we consider
randomly-generated instances. Table 2 gives results
for our model and the LP relaxations of Martins et al
(2009) with randomly generated scores on automata
transitions. We again recover exact solutions more
often than the Martins et al relaxations. Note that
with random parameters the percentage of exact so-
lutions is significantly lower, suggesting that the ex-
actness of decoding of the trained models is a special
case. We speculate that this is due to the high perfor-
mance of approximate decoding with Z in place of
Y under the trained models for fi; the training algo-
rithm described in section 6 may have the tendency
to make the LP relaxation tight.
7.3 Speed
Table 1, columns TimeS and TimeG, shows decod-
ing times for the dual decomposition algorithms.
Table 2 gives speed comparisons to Martins et al
(2009). Our method gives significant speed-ups over
 0
 5
 10
 15
 20
 25
 30
 0  1000  2000  3000  4000  5000%
 of 
He
ad 
Au
tom
ata
 Re
com
put
ed
Iterations of Dual Decomposition
% recomputed, g+s% recomputed, sib
Figure 3: The average percentage of head automata that
must be recomputed on each iteration of dual decompo-
sition on the PTB validation set.
the Martins et al (2009) method, presumably be-
cause it leverages the underlying structure of the
problem, rather than using a generic solver.
7.4 Lazy Decoding
Here we describe an important optimization in the
dual decomposition algorithms. Consider the algo-
rithm in Figure 1. At each iteration we must find
z(k)|i = argmax
z|i?Zi
(fi(z|i)?
?
j
u(k)(i, j)z(i, j))
1295
Sib Acc Int Time Rand
LP(S) 92.14 88.29 0.14 11.7
LP(M) 92.17 93.18 0.58 30.6
ILP 92.19 100.0 1.44 100.0
DD-5000 92.19 98.82 0.08 35.6
DD-250 92.23 89.29 0.03 10.2
G+S Acc Int Time Rand
LP(S) 92.60 91.64 0.23 0.0
LP(M) 92.58 94.41 0.75 0.0
ILP 92.70 100.0 1.79 100.0
DD-5000 92.71 98.76 0.23 6.8
DD-250 92.66 85.47 0.12 0.0
Table 2: A comparison of dual decomposition with lin-
ear programs described by Martins et al (2009). LP(S):
Linear Program relaxation based on single-commodity
flow. LP(M): Linear Program relaxation based on
multi-commodity flow. ILP: Exact Integer Linear Pro-
gram. DD-5000/DD-250: Dual decomposition with non-
projective head automata, with K = 5000/250. Upper
results are for the sibling model, lower results are G+S.
Columns give scores for UAS accuracy, percentage of so-
lutions which are integral, and solution speed in seconds
per sentence. These results are for Section 22 of the PTB.
The last column is the percentage of integral solutions on
a random problem of length 10 words. The (I)LP experi-
ments were carried out using Gurobi, a high-performance
commercial-grade solver.
for i = 0 . . . n. However, if for some i, u(k)(i, j) =
u(k?1)(i, j) for all j, then z(k)|i = z
(k?1)
|i . In
lazy decoding we immediately set z(k)|i = z
(k?1)
|i if
u(k)(i, j) = u(k?1)(i, j) for all j; this check takes
O(n) time, and saves us from decoding with the i?th
automaton. In practice, the updates to u are very
sparse, and this condition occurs very often in prac-
tice. Figure 3 demonstrates the utility of this method
for both sibling automata and G+S automata.
7.5 Early Stopping
We also ran experiments varying the value of K?
the maximum number of iterations?in the dual de-
composition algorithms. As before, if we do not find
y(k) = z(k) for some value of k ? K, we choose
the y(k) with optimal value for f(y(k)) as the final
solution. Figure 4 shows three graphs: 1) the accu-
racy of the parser on PTB validation data versus the
value for K; 2) the percentage of examples where
y(k) = z(k) at some point during the algorithm,
hence the algorithm returns a certificate of optimal-
ity; 3) the percentage of examples where the solution
 50
 60
 70
 80
 90
 100
 0  200  400  600  800  1000
Pe
rce
nta
ge
Maximum Number of Dual Decomposition Iterations
% validation UAS% certificates% match K=5000
Figure 4: The behavior of the dual-decomposition parser
with sibling automata as the value of K is varied.
Sib P-Sib G+S P-G+S
PTB 92.19 92.34 92.71 92.70
PDT 86.41 85.67 87.40 86.43
Table 3: UAS of projective and non-projective decoding
for the English (PTB) and Czech (PDT) validation sets.
Sib/G+S: as in Table 1. P-Sib/P-G+S: Projective versions
of Sib/G+S, where the MST component has been re-
placed with the Eisner (2000) first-order projective parser.
returned is the same as the solution for the algorithm
with K = 5000 (our original setting). It can be seen
for K as small as 250 we get very similar accuracy
to K = 5000 (see Table 2). In fact, for this set-
ting the algorithm returns the same solution as for
K = 5000 on 99.59% of the examples. However
only 89.29% of these solutions are produced with a
certificate of optimality (y(k) = z(k)).
7.6 How Good is the Approximation z??
We ran experiments measuring the quality of z? =
argmaxz?Z f(z), where f(z) is given by the
perceptron-trained bigram-sibling model. Because
z? may not be a well-formed tree with n dependen-
cies, we report precision and recall rather than con-
ventional dependency accuracy. Results on the PTB
validation set were 91.11%/88.95% precision/recall,
which is accurate considering the unconstrained na-
ture of the predictions. Thus the z? approximation is
clearly a good one; we suspect that this is one reason
for the good convergence results for the method.
7.7 Importance of Non-Projective Decoding
It is simple to adapt the dual-decomposition algo-
rithms in figures 1 and 2 to give projective depen-
dency structures: the set Y is redefined to be the set
1296
of all projective structures, with the argmax over Y
being calculated using a projective first-order parser
(Eisner, 2000). Table 3 shows results for projec-
tive and non-projective parsing using the dual de-
composition approach. For Czech data, where non-
projective structures are common, non-projective
decoding has clear benefits. In contrast, there is little
difference in accuracy between projective and non-
projective decoding on English.
8 Conclusions
We have described dual decomposition algorithms
for non-projective parsing, which leverage existing
dynamic programming and MST algorithms. There
are a number of possible areas for future work. As
described in section 7.7, the algorithms can be easily
modified to consider projective structures by replac-
ing Y with the set of projective trees, and then using
first-order dependency parsing algorithms in place
of MST decoding. This method could be used to
derive parsing algorithms that include higher-order
features, as an alternative to specialized dynamic
programming algorithms. Eisner (2000) describes
extensions of head automata to include word senses;
we have not discussed this issue in the current pa-
per, but it is simple to develop dual decomposition
algorithms for this case, using similar methods to
those used for the grandparent models. The gen-
eral approach should be applicable to other lexical-
ized syntactic formalisms, and potentially also to de-
coding in syntax-driven translation. In addition, our
dual decomposition approach is well-suited to paral-
lelization. For example, each of the head-automata
could be optimized independently in a multi-core or
GPU architecture. Finally, our approach could be
used with other structured learning algorithms, e.g.
Meshi et al (2010).
A Implementation Details
This appendix describes details of the algorithm,
specifically choice of the step sizes ?k, and use of
the ?(i, j) parameters.
A.1 Choice of Step Sizes
We have found the following method to be effec-
tive. First, define ? = f(z(1)) ? f(y(1)), where
(z(1), y(1)) is the output of the algorithm on the first
iteration (note that we always have ? ? 0 since
f(z(1)) = L(u(1))). Then define ?k = ?/(1 + ?k),
where ?k is the number of times that L(u(k
?)) >
L(u(k
??1)) for k? ? k. Hence the learning rate drops
at a rate of 1/(1+ t), where t is the number of times
that the dual increases from one iteration to the next.
A.2 Use of the ?(i, j) Parameters
The parsing algorithms both consider a general-
ized problem that includes ?(i, j) parameters. We
now describe how these can be useful. Re-
call that the optimization problem is to solve
argmaxz?Z,y?Y f(z) + h(y), subject to a set of
agreement constraints. In our models, f(z) can
be written as f ?(z) +
?
i,j ?(i, j)z(i, j) where
f ?(z) includes only terms depending on higher-
order (non arc-factored features), and ?(i, j) are
weights that consider the dependency between i
and j alone. For any value of 0 ? ? ?
1, the problem argmaxz?Z,y?Y f2(z) + h2(y) is
equivalent to the original problem, if f2(z) =
f ?(z) + (1 ? ?)
?
i,j ?(i, j)z(i, j) and h2(y) =
?
?
i,j ?(i, j)y(i, j). We have simply shifted the
?(i, j) weights from one model to the other. While
the optimization problem remains the same, the al-
gorithms in Figure 1 and 2 will converge at differ-
ent rates depending on the value for ?. In our ex-
periments we set ? = 0.001, which puts almost
all the weight in the head-automata models, but al-
lows weights on spanning tree edges to break ties in
MST inference in a sensible way. We suspect this is
important in early iterations of the algorithm, when
many values for u(i, j) or v(i, j) will be zero, and
where with ? = 0 many spanning tree solutions y(k)
would be essentially random, leading to very noisy
updates to the u(i, j) and v(i, j) values. We have
not tested other values for ?.
Acknowledgments MIT gratefully acknowledges the
support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government. A. Rush was supported by the GALE program of
the DARPA, Contract No. HR0011-06-C-0022. D. Sontag was
supported by a Google PhD Fellowship.
1297
References
H. Alshawi. 1996. Head Automata and Bilingual Tiling:
Translation with Minimal Representations. In Proc.
ACL, pages 167?176.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared
Task on Multilingual Dependency Parsing. In Proc.
CoNLL, pages 149?164.
X. Carreras. 2007. Experiments with a Higher-Order
Projective Dependency Parser. In Proc. EMNLP-
CoNLL, pages 957?961.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proc. EMNLP, pages
1?8.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007. Us-
ing Combinatorial Optimization within Max-Product
Belief Propagation. In NIPS, pages 369?376.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
T. Finley and T. Joachims. 2008. Training structural
svms when exact inference is intractable. In ICML,
pages 304?311.
A.K. Joshi and Y. Schabes. 1997. Tree-Adjoining
Grammars. Handbook of Formal Languages: Beyond
Words, 3:69?123.
N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF
Optimization via Dual Decomposition: Message-
Passing Revisited. In Proc. ICCV.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured Prediction Models via the Matrix-Tree The-
orem. In Proc. EMNLP-CoNLL, pages 141?150.
B.H. Korte and J. Vygen. 2008. Combinatorial Opti-
mization: Theory and Algorithms. Springer Verlag.
A. Kulesza and F. Pereira. 2008. Structured learning
with approximate inference. In NIPS.
C. Lemare?chal. 2001. Lagrangian Relaxation. In Com-
putational Combinatorial Optimization, Optimal or
Provably Near-Optimal Solutions [based on a Spring
School], pages 112?156, London, UK. Springer-
Verlag.
A.F.T. Martins, D. Das, N.A. Smith, and E.P. Xing. 2008.
Stacking Dependency Parsers. In Proc. EMNLP,
pages 157?166.
A.F.T. Martins, N.A. Smith., and E.P. Xing. 2009. Con-
cise Integer Linear Programming Formulations for De-
pendency Parsing. In Proc. ACL, pages 342?350.
R. McDonald and F. Pereira. 2006. Online Learning
of Approximate Dependency Parsing Algorithms. In
Proc. EACL, pages 81?88.
R. McDonald and G. Satta. 2007. On the Complexity of
Non-Projective Data-Driven Dependency Parsing. In
Proc. IWPT.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-Projective Dependency Parsing using Spanning
Tree Algorithms. In Proc. HLT-EMNLP, pages 523?
530.
O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson.
2010. Learning Efficiently with Approximate Infer-
ence via Dual Losses. In Proc. ICML.
A. Nedic? and A. Ozdaglar. 2009. Approximate
Primal Solutions and Rate Analysis for Dual Sub-
gradient Methods. SIAM Journal on Optimization,
19(4):1757?1780.
J. Nivre and R. McDonald. 2008. Integrating Graph-
Based and Transition-Based Dependency Parsers. In
Proc. ACL, pages 950?958.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2005.
Learning and Inference over Constrained Output. In
Proc. IJCAI, pages 1124?1129.
S. Riedel and J. Clarke. 2006. Incremental Integer Linear
Programming for Non-projective Dependency Parsing.
In Proc. EMNLP, pages 129?137.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On Dual Decomposition and Linear Program-
ming Relaxations for Natural Language Processing. In
Proc. EMNLP.
D.A. Smith and J. Eisner. 2008. Dependency Parsing by
Belief Propagation. In Proc. EMNLP, pages 145?156.
D.A. Smith and N.A. Smith. 2007. Probabilistic Mod-
els of Nonprojective Dependency Trees. In Proc.
EMNLP-CoNLL, pages 132?140.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP Relaxations for MAP
using Message Passing. In Proc. UAI.
M. Steedman. 2000. The Syntactic Process. MIT Press.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The CoNLL-2008 Shared Task on
Joint Parsing of Syntactic and Semantic Dependencies.
In Proc. CoNLL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697?3717.
1298
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 26?37,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Exact Decoding of Phrase-Based Translation Models
through Lagrangian Relaxation
Yin-Wen Chang
MIT CSAIL
Cambridge, MA 02139, USA
yinwen@csail.mit.edu
Michael Collins
Department of Computer Science,
Columbia University,
New York, NY 10027, USA
mcollins@cs.columbia.edu
Abstract
This paper describes an algorithm for exact
decoding of phrase-based translation models,
based on Lagrangian relaxation. The method
recovers exact solutions, with certificates of
optimality, on over 99% of test examples.
The method is much more efficient than ap-
proaches based on linear programming (LP)
or integer linear programming (ILP) solvers:
these methods are not feasible for anything
other than short sentences. We compare our
method to MOSES (Koehn et al, 2007), and
give precise estimates of the number and mag-
nitude of search errors that MOSES makes.
1 Introduction
Phrase-based models (Och et al, 1999; Koehn et
al., 2003; Koehn et al, 2007) are a widely-used
approach for statistical machine translation. The
decoding problem for phrase-based models is NP-
hard1; because of this, previous work has generally
focused on approximate search methods, for exam-
ple variants of beam search, for decoding.
This paper describes an algorithm for exact
decoding of phrase-based models, based on La-
grangian relaxation (Lemare?chal, 2001). The core
of the algorithm is a dynamic program for phrase-
based translation which is efficient, but which allows
some ill-formed translations. More specifically, the
dynamic program searches over the space of transla-
tions where exactly N words are translated (N is
the number of words in the source-language sen-
tence), but where some source-language words may
be translated zero times, or some source-language
words may be translated more than once. La-
grangian relaxation is used to enforce the constraint
1We refer here to the phrase-based models of (Koehn et al,
2003; Koehn et al, 2007), considered in this paper. Other vari-
ants of phrase-based models, which allow polynomial time de-
coding, have been proposed, see the related work section.
that each source-language word should be translated
exactly once. A subgradient algorithm is used to op-
timize the dual problem arising from the relaxation.
The first technical contribution of this paper is the
basic Lagrangian relaxation algorithm. By the usual
guarantees for Lagrangian relaxation, if this algo-
rithm converges to a solution where all constraints
are satisfied (i.e., where each word is translated ex-
actly once), then the solution is guaranteed to be
optimal. For some source-language sentences how-
ever, the underlying relaxation is loose, and the algo-
rithm will not converge. The second technical con-
tribution of this paper is a method that incrementally
adds constraints to the underlying dynamic program,
thereby tightening the relaxation until an exact solu-
tion is recovered.
We describe experiments on translation from Ger-
man to English, using phrase-based models trained
by MOSES (Koehn et al, 2007). The method
recovers exact solutions, with certificates of opti-
mality, on over 99% of test examples. On over
78% of examples, the method converges with zero
added constraints (i.e., using the basic algorithm);
99.67% of all examples converge with 9 or fewer
constraints. We compare to a linear programming
(LP)/integer linear programming (ILP) based de-
coder. Our method is much more efficient: LP or
ILP decoding is not feasible for anything other than
short sentences,2 whereas the average decoding time
for our method (for sentences of length 1-50 words)
is 121 seconds per sentence. We also compare our
method to MOSES, and give precise estimates of the
number and magnitude of search errors that MOSES
makes. Even with large beam sizes, MOSES makes
a significant number of search errors. As far as we
are aware, previous work has not successfully re-
2For example ILP decoding for sentences of lengths 11-15
words takes on average 2707.8 seconds.
26
covered exact solutions for the type of phrase-based
models used in MOSES.
2 Related Work
Lagrangian relaxation is a classical technique for
solving combinatorial optimization problems (Korte
and Vygen, 2008; Lemare?chal, 2001). Dual decom-
position, a special case of Lagrangian relaxation, has
been applied to inference problems in NLP (Koo et
al., 2010; Rush et al, 2010), and also to Markov ran-
dom fields (Wainwright et al, 2005; Komodakis et
al., 2007; Sontag et al, 2008). Earlier work on be-
lief propagation (Smith and Eisner, 2008) is closely
related to dual decomposition. Recently, Rush and
Collins (2011) describe a Lagrangian relaxation al-
gorithm for decoding for syntactic translation; the
algorithmic construction described in the current pa-
per is, however, very different in nature to this work.
Beam search stack decoders (Koehn et al, 2003)
are the most commonly used decoding algorithm
for phrase-based models. Dynamic-programming-
based beam search algorithms are discussed for both
word-based and phrase-based models by Tillmann
and Ney (2003) and Tillmann (2006).
Several works attempt exact decoding, but effi-
ciency remains an issue. Exact decoding via integer
linear programming (ILP) for IBM model 4 (Brown
et al, 1993) has been studied by Germann et al
(2001), with experiments using a bigram language
model for sentences up to eight words in length.
Riedel and Clarke (2009) have improved the effi-
ciency of this work by using a cutting-plane algo-
rithm, and experimented with sentence lengths up
to 30 words (again with a bigram LM). Zaslavskiy
et al (2009) formulate the phrase-based decoding
problem as a traveling salesman problem (TSP), and
take advantage of existing exact and approximate
approaches designed for TSP. Their translation ex-
periment uses a bigram language model and applies
an approximate algorithm for TSP. Och et al (2001)
propose an A* search algorithm for IBM model 4,
and test on sentence lengths up to 14 words. Other
work (Kumar and Byrne, 2005; Blackwood et al,
2009) has considered variants of phrase-based mod-
els with restrictions on reordering that allow exact,
polynomial time decoding, using finite-state trans-
ducers.
The idea of incrementally adding constraints to
tighten a relaxation until it is exact is a core idea in
combinatorial optimization. Previous work on this
topic in NLP or machine learning includes work on
inference in Markov random fields (Sontag et al,
2008); work that encodes constraints using finite-
state machines (Tromble and Eisner, 2006); and
work on non-projective dependency parsing (Riedel
and Clarke, 2006).
3 The Phrase-based Translation Model
This section establishes notation for phrase-based
translation models, and gives a definition of the de-
coding problem. The phrase-based model we use is
the same as that described by Koehn et al (2003), as
implemented in MOSES (Koehn et al, 2007).
The input to a phrase-based translation sys-
tem is a source-language sentence with N words,
x1x2 . . . xN . A phrase table is used to define the
set of possible phrases for the sentence: each phrase
is a tuple p = (s, t, e), where (s, t) are indices rep-
resenting a contiguous span in the source-language
sentence (we have s ? t), and e is a target-language
string consisting of a sequence of target-language
words. For example, the phrase p = (2, 5, the dog)
would specify that words x2 . . . x5 have a translation
in the phrase table as ?the dog?. Each phrase p has
a score g(p) = g(s, t, e): this score will typically
be calculated as a log-linear combination of features
(e.g., see Koehn et al (2003)).
We use s(p), t(p) and e(p) to refer to the three
components (s, t, e) of a phrase p.
The output from a phrase-based model is a
sequence of phrases y = ?p1p2 . . . pL?. We
will often refer to an output y as a derivation.
The derivation y defines a target-language transla-
tion e(y), which is formed by concatenating the
strings e(p1), e(p2), . . . , e(pL). For two consecutive
phrases pk = (s, t, e) and pk+1 = (s?, t?, e?), the dis-
tortion distance is defined as ?(t, s?) = |t+ 1? s?|.
The score for a translation is then defined as
f(y) = h(e(y))+
L?
k=1
g(pk)+
L?1?
k=1
???(t(pk), s(pk+1))
where ? ? R is often referred to as the distortion
penalty, and typically takes a negative value. The
function h(e(y)) is the score of the string e(y) under
27
a language model.3
The decoding problem is to find
argmax
y?Y
f(y)
where Y is the set of valid derivations. The set Y can
be defined as follows. First, for any derivation y =
?p1p2 . . . pL?, define y(i) to be the number of times
that the source-language word xi has been translated
in y: that is, y(i) = ?Lk=1[[s(pk) ? i ? t(pk)]],
where [[pi]] = 1 if pi is true, and 0 otherwise. Then
Y is defined as the set of finite length sequences
?p1p2 . . . pL? such that:
1. Each word in the input is translated exactly
once: that is, y(i) = 1 for i = 1 . . . N .
2. For each pair of consecutive phrases
pk, pk+1 for k = 1 . . . L ? 1, we have
?(t(pk), s(pk+1)) ? d, where d is the
distortion limit.
An exact dynamic programming algorithm for
this problem uses states (w1, w2, b, r), where
(w1, w2) is a target-language bigram that the par-
tial translation ended with, b is a bit-string denoting
which source-language words have been translated,
and r is the end position of the previous phrase (e.g.,
see Koehn et al (2003)). The bigram (w1, w2) is
needed for calculation of trigram language model
scores; r is needed to enforce the distortion limit,
and to calculate distortion costs. The bit-string b
is needed to ensure that each word is translated ex-
actly once. Since the number of possible bit-strings
is exponential in the length of sentence, exhaustive
dynamic programming is in general intractable. In-
stead, people commonly use heuristic search meth-
ods such as beam search for decoding. However,
these methods have no guarantee of returning the
highest scoring translation.
4 A Decoding Algorithm based on
Lagrangian Relaxation
We now describe a decoding algorithm for phrase-
based translation, based on Lagrangian relaxation.
3The language model score usually includes a word inser-
tion score that controls the length of translations. The relative
weights of the g(p) and h(e(y)) terms, and the value for ?, are
typically chosen using MERT training (Och, 2003).
We first describe a dynamic program for decoding
which is efficient, but which relaxes the y(i) = 1
constraints described in the previous section. We
then describe the Lagrangian relaxation algorithm,
which introduces Lagrange multipliers for each con-
straint of the form y(i) = 1, and uses a subgradient
algorithm to minimize the dual arising from the re-
laxation. We conclude with theorems describing for-
mal properties of the algorithm, and with an example
run of the algorithm.
4.1 An Efficient Dynamic Program
As described in the previous section, our goal is to
find the optimal translation y? = argmaxy?Y f(y).
We will approach this problem by defining a set Y ?
such that Y ? Y ?, and such that
argmax
y?Y ?
f(y)
can be found efficiently using dynamic program-
ming. The set Y ? omits some constraints?
specifically, the constraints that each source-
language word is translated once, i.e., that y(i) = 1
for i = 1 . . . N?that are enforced for members
of Y . In the next section we describe how to re-
introduce these constraints using Lagrangian relax-
ation. The set Y ? does, however, include a looser
constraint, namely that ?Ni=1 y(i) = N , which re-
quires that exactly N words are translated.
We now give the dynamic program that defines
Y ?. The main idea will be to replace bit-strings (as
described in the previous section) by a much smaller
number of dynamic programming states. Specifi-
cally, the states of the new dynamic program will
be tuples (w1, w2, n, l,m, r). The pair (w1, w2) is
again a target-language bigram corresponding to the
last two words in the partial translation, and the inte-
ger r is again the end position of the previous phrase.
The integer n is the number of words that have been
translated thus far in the dynamic programming al-
gorithm. The integers l and m specify a contiguous
span xl . . . xm in the source-language sentence; this
span is the last contiguous span of words that have
been translated thus far.
The dynamic program can be viewed as a
shortest-path problem in a directed graph, with
nodes in the graph corresponding to states
(w1, w2, n, l,m, r). The transitions in the
28
graph are defined as follows. For each state
(w1, w2, n, l,m, r), we consider any phrase
p = (s, t, e) with e = (e0 . . . eM?1eM ) such that:
1) ?(r, s) ? d; and 2) t < l or s > m. The former
condition states that the phrase should satisfy the
distortion limit. The latter condition requires that
there is no overlap of the new phrase?s span (s, t)
with the span (l,m). For any such phrase, we create
a transition
(w1, w2, n, l,m, r)
p=(s,t,e)?????? (w?1, w?2, n?, l?,m?, r?)
where
? (w?1, w?2) =
{
(eM?1, eM ) if M ? 2
(w2, e1) if M = 1
? n? = n+ t? s+ 1
? (l?,m?) =
?
?
?
(l, t ) if s = m+ 1
(s,m) if t = l ? 1
(s, t ) otherwise
? r? = t
The new target-language bigram (w?1, w?2) is the last
two words of the partial translation after including
phrase p. It comes from either the last two words
of e, or, if e consists of a single word, the last word
of the previous bigram, w2, and the first and only
word, e1, in e. (l?,m?) is expanded from (l,m) if
the spans (l,m) and (s, t) are adjacent. Otherwise,
(l?,m?) will be the same as (s, t).
The score of the transition is given by a sum
of the phrase translation score g(p), the language
model score, and the distortion cost ?? ?(r, s). The
trigram language model score is h(e1|w1, w2) +
h(e2|w2, e1) +
?M?2
i=1 h(ei+2|ei, ei+1), where
h(w3|w1, w2) is a trigram score (typically a log
probability plus a word insertion score).
We also include start and end states in the directed
graph. The start state is (<s>,<s>, 0, 0, 0, 0) where
<s> is the start symbol in the language model. For
each state (w1, w2, n, l,m, r), such that n = N , we
create a transition to the end state. This transition
takes the form
(w1, w2, N, l,m, r)
(N,N+1,</s>)???????????? END
For this transition, we define the score as score =
h(</s>|w1, w2); thus this transition incorporates
the end symbol </s> in the language model.
The states and transitions we have described form
a directed graph, where each path from the start state
to the end state corresponds to a sequence of phrases
p1p2 . . . pL. We define Y ? to be the full set of such
sequences. We can use the Viterbi algorithm to solve
argmaxy?Y ? f(y) by simply searching for the high-
est scoring path from the start state to the end state.
The set Y ? clearly includes derivations that are ill-
formed, in that they may include words that have
been translated 0 times, or more than 1 time. The
first line of Figure 2 shows one such derivation (cor-
responding to the translation the quality and also the
and the quality and also .). For each phrase we show
the English string (e.g., the quality) together with the
span of the phrase (e.g., 3, 6). The values for y(i) are
also shown. It can be verified that this derivation is a
valid member of Y ?. However, y(i) 6= 1 for several
values of i: for example, words 1 and 2 are trans-
lated 0 times, while word 3 is translated twice.
Other dynamic programs, and definitions of Y ?,
are possible: for example an alternative would be
to use a dynamic program with states (w1, w2, n, r).
However, including the previous contiguous span
(l,m) makes the set Y ? a closer approximation to
Y . In experiments we have found that including the
previous span (l,m) in the dynamic program leads
to faster convergence of the subgradient algorithm
described in the next section, and in general to more
stable results. This is in spite of the dynamic pro-
gram being larger; it is no doubt due to Y ? being a
better approximation of Y .
4.2 The Lagrangian Relaxation Algorithm
We now describe the Lagrangian relaxation decod-
ing algorithm for the phrase-based model. Recall
that in the previous section, we defined a set Y ? that
allowed efficient dynamic programming, and such
that Y ? Y ?. It is easy to see that Y = {y : y ?
Y ?, and ?i, y(i) = 1}. The original decoding
problem can therefore be stated as:
argmax
y?Y ?
f(y) such that ?i, y(i) = 1
We use Lagrangian relaxation (Korte and Vygen,
2008) to deal with the y(i) = 1 constraints. We
introduce Lagrange multipliers u(i) for each such
constraint. The Lagrange multipliers u(i) can take
any positive or negative value. The Lagrangian is
L(u, y) = f(y) +
?
i
u(i)(y(i)? 1)
29
Initialization: u0(i)? 0 for i = 1 . . . N
for t = 1 . . . T
yt = argmaxy?Y? L(ut?1, y)
if yt(i) = 1 for i = 1 . . . N
return yt
else
for i = 1 . . . N
ut(i) = ut?1(i)? ?t (yt(i)? 1)
Figure 1: The decoding algorithm. ?t > 0 is the step size
at the t?th iteration.
The dual objective is then
L(u) = max
y?Y ?
L(u, y).
and the dual problem is to solve
min
u
L(u).
The next section gives a number of formal results de-
scribing how solving the dual problem will be useful
in solving the original optimization problem.
We now describe an algorithm that solves the dual
problem. By standard results for Lagrangian re-
laxation (Korte and Vygen, 2008), L(u) is a con-
vex function; it can be minimized by a subgradient
method. If we define
yu = argmaxy?Y ? L(u, y)
and ?u(i) = yu(i) ? 1 for i = 1 . . . N , then ?u is
a subgradient of L(u) at u. A subgradient method
is an iterative method for minimizing L(u), which
perfoms updates ut ? ut?1??t?ut?1 where ?t > 0
is the step size for the t?th subgradient step.
Figure 1 depicts the resulting algorithm. At each
iteration, we solve
argmax
y?Y ?
(
f(y) +
?
i
u(i)(y(i)? 1)
)
=argmax
y?Y ?
(
f(y) +
?
i
u(i)y(i)
)
by the dynamic program described in the previous
section. Incorporating the ?i u(i)y(i) terms in the
dynamic program is straightforward: we simply re-
define the phrase scores as
g?(s, t, e) = g(s, t, e) +
t?
i=s
u(i)
Intuitively, each Lagrange multiplier u(i) penal-
izes or rewards phrases that translate word i; the al-
gorithm attempts to adjust the Lagrange multipliers
in such a way that each word is translated exactly
once. The updates ut(i) = ut?1(i) ? ?t(yt(i) ? 1)
will decrease the value for u(i) if yt(i) > 1, in-
crease the value for u(i) if yt(i) = 0, and leave u(i)
unchanged if yt(i) = 1.
4.3 Properties
We now give some theorems stating formal proper-
ties of the Lagrangian relaxation algorithm. These
results for Lagrangian relaxation are well known:
for completeness, we state them here. First, define
y? to be the optimal solution for our original prob-
lem:
Definition 1. y? = argmaxy?Y f(y)
Our first theorem states that the dual function pro-
vides an upper bound on the score for the optimal
translation, f(y?):
Theorem 1. For any value of u ? RN , L(u) ?
f(y?).
Proof.
L(u) = max
y?Y ?
f(y) +
?
i
u(i)(y(i)? 1)
? max
y?Y
f(y) +
?
i
u(i)(y(i)? 1)
= max
y?Y
f(y)
The first inequality follows because Y ? Y ?. The
final equality is true since any y ? Y has y(i) =
1 for all i, implying that?i u(i)(y(i)?1) = 0.
The second theorem states that under an appropri-
ate choice of the step sizes ?t, the method converges
to the minimum ofL(u). Hence we will successfully
find the tightest possible upper bound defined by the
dual L(u).
Theorem 2. For any sequence ?1, ?2, . . . If 1)
limt?? ?t ? 0; 2) ??t=1 ?t = ?, then
limt?? L(ut) = minu L(u)
Proof. See Korte and Vygen (2008).
30
Input German: dadurch ko?nnen die qualita?t und die regelma??ige postzustellung auch weiterhin sichergestellt werden .
t L(ut?1) yt(i) derivation yt
1 -10.0988 0 0 2 2 3 3 0 0 2 0 0 0 1
?
?
?
?
3, 6
the quality and
?
?
?
?
9, 9
also
?
?
?
?
6, 6
the
?
?
?
?
5, 5
and
?
?
?
?
3, 3
the
?
?
?
?
4, 6
quality and
?
?
?
?
9, 9
also
?
?
?
?
13, 13
.
?
?
?
?
2 -11.1597 0 0 1 0 0 0 1 0 0 4 1 5 1
?
?
?
?
3, 3
the
?
?
?
?
7, 7
regular
?
?
?
?
12, 12
will
?
?
?
?
10, 10
continue to
?
?
?
?
12, 12
be
?
?
?
?
10, 10
continue to
?
?
?
?
12, 12
be
?
?
?
?
10, 10
continue to
?
?
?
?
12, 12
be
?
?
?
?
10, 10
continue to
?
?
?
?
11, 13
be guaranteed .
?
?
?
?
3 -12.3742 3 3 1 2 2 0 0 0 1 0 0 0 1
?
?
?
?
1, 2
in that way ,
?
?
?
?
5, 5
and
?
?
?
?
2, 2
can
?
?
?
?
1, 1
thus
?
?
?
?
4, 4
quality
?
?
?
?
1, 2
in that way ,
?
?
?
?
3, 5
the quality and
?
?
?
?
9, 9
also
?
?
?
?
13, 13
.
?
?
?
?
4 -11.8623 0 1 0 0 0 1 1 3 3 0 3 0 1
?
?
?
?
2, 2
can
?
?
?
?
6, 7
the regular
?
?
?
?
8, 8
distribution should
?
?
?
?
9, 9
also
?
?
?
?
11, 11
ensure
?
?
?
?
8, 8
distribution should
?
?
?
?
9, 9
also
?
?
?
?
11, 11
ensure
?
?
?
?
8, 8
distribution should
?
?
?
?
9, 9
also
?
?
?
?
11, 11
ensure
?
?
?
?
13, 13
.
?
?
?
?
5 -13.9916 0 0 1 1 3 2 4 0 0 0 1 0 1
?
?
?
?
3, 3
the
?
?
?
?
7, 7
regular
?
?
?
?
5, 5
and
?
?
?
?
7, 7
regular
?
?
?
?
5, 5
and
?
?
?
?
7, 7
regular
?
?
?
?
6, 6
the
?
?
?
?
4, 4
quality
?
?
?
?
5, 7
and the regular
?
?
?
?
11, 11
ensured
?
?
?
?
13, 13
.
?
?
?
?
6 -15.6558 1 1 1 2 0 2 0 1 1 1 1 1 1
?
?
?
?
1, 2
in that way ,
?
?
?
?
3, 4
the quality of
?
?
?
?
6, 6
the
?
?
?
?
4, 4
quality of
?
?
?
?
6, 6
the
?
?
?
?
8, 8
distribution should
?
?
?
?
9, 10
continue to
?
?
?
?
11, 13
be guaranteed .
?
?
?
?
7 -16.1022 1 1 1 1 1 1 1 1 1 1 1 1 1
?
?
?
?
1, 2
in that way ,
?
?
?
?
3, 4
the quality
?
?
?
?
5, 7
and the regular
?
?
?
?
8, 8
distribution should
?
?
?
?
9, 10
continue to
?
?
?
?
11, 13
be guaranteed .
?
?
?
?
Figure 2: An example run of the algorithm in Figure 1. For each value of t we show the dual value L(ut?1), the
derivation yt, and the number of times each word is translated, yt(i) for i = 1 . . . N . For each phrase in a derivation
we show the English string e, together with the span (s, t): for example, the first phrase in the first derivation has
English string the quality and, and span (3, 6). At iteration 7 we have yt(i) = 1 for i = 1 . . . N , and the translation is
returned, with a guarantee that it is optimal.
Our final theorem states that if at any iteration the
algorithm finds a solution yt such that yt(i) = 1 for
i = 1 . . . N , then this is guaranteed to be the optimal
solution to our original problem. First, define
Definition 2. yu = argmaxy?Y ? L(u, y).
We then have the theorem
Theorem 3. If ? u, s.t. yu(i) = 1 for i = 1 . . . N ,
then f(yu) = f(y?), i.e. yu is optimal.
Proof. We have
L(u) = max
y?Y ?
f(y) +
?
i
u(i)(y(i)? 1)
= f(yu) +
?
i
u(i)(yu(i)? 1)
= f(yu)
The second equality is true because of the defini-
tion of yu. The third equality follows because by
assumption yu(i) = 1 for i = 1 . . . N . Because
L(u) = f(yu) and L(u) ? f(y?) for all u, we have
f(yu) ? f(y?). But y? = argmaxy?Y f(y), and
yu ? Y , hence we must also have f(yu) ? f(y?). It
follows that f(yu) = f(y?).
In some cases, however, the algorithm in Figure 1
may not return a solution yt such that yt(i) = 1
for all i. There could be two reasons for this. In
the first case, we may not have run the algorithm
for enough iterations T to see convergence. In the
second case, the underlying relaxation may not be
tight, in that there may not be any settings u for the
Lagrange multipliers such that yu(i) = 1 for all i.
Section 5 describes a method for tightening
the underlying relaxation by introducing hard con-
straints (of the form y(i) = 1 for selected values of
i). We will see that this method is highly effective
in tightening the relaxation until the algorithm con-
verges to an optimal solution.
4.4 An Example of the Algorithm
Figure 2 shows an example of how the algorithm
works when translating a German sentence into an
English sentence. After the first iteration, there are
words that have been translated two or three times,
and words that have not been translated. At each
iteration, the Lagrangian multipliers are updated to
encourage each word to be translated once. On
this example, the algorithm converges to a solution
where all words are translated exactly once, and the
solution is guaranteed to be optimal.
5 Tightening the Relaxation
In some cases the algorithm in Figure 1 will not
converge to y(i) = 1 for i = 1 . . . N because
the underlying relaxation is not tight. We now de-
scribe a method that incrementally tightens the La-
grangian relaxation algorithm until it provides an ex-
act answer. In cases that do not converge, we in-
troduce hard constraints to force certain words to be
translated exactly once in the dynamic programming
solver. In experiments we show that typically only a
31
Optimize(C, u)
while (dual value still improving)
y? = argmaxy?Y?C L(u, y)if y?(i) = 1 for i = 1 . . . N return y?
else for i = 1 . . . N
u(i) = u(i)? ? (y?(i)? 1)
count(i) = 0 for i = 1 . . . N
for k = 1 . . .K
y? = argmaxy?Y?C L(u, y)if y?(i) = 1 for i = 1 . . . N return y?
else for i = 1 . . . N
u(i) = u(i)? ? (y?(i)? 1)
count(i) = count(i) + [[y?(i) 6= 1]]
Let C? = set of G i?s that have the largest value for
count(i), that are not in C, and that are not adjacent to
each other
return Optimize(C ? C?, u)
Figure 3: A decoding algorithm with incremental addi-
tion of constraints. The function Optimize(C, u) is a re-
cursive function, which takes as input a set of constraints
C, and a vector of Lagrange multipliers, u. The initial
call to the algorithm is with C = ?, and u = 0. ? > 0 is
the step size. In our experiments, the step size decreases
each time the dual value increases from one iteration to
the next; see Appendix A.
few constraints are necessary.
Given a set C ? {1, 2, . . . , N}, we define
Y ?C = {y : y ? Y ?, and ? i ? C, y(i) = 1}
Thus Y ?C is a subset of Y ?, formed by adding hard
constraints of the form y(i) = 1 to Y ?. Note that Y ?C
remains as a superset of Y , which enforces y(i) =
1 for all i. Finding argmaxy?Y ?C f(y) can againbe achieved using dynamic programming, with the
number of dynamic programming states increased
by a factor of 2|C|: dynamic programming states of
the form (w1, w2, n, l,m, r) are replaced by states
(w1, w2, n, l,m, r, bC) where bC is a bit-string of
length |C|, which records which words in the set C
have or haven?t been translated in a hypothesis (par-
tial derivation). Note that if C = {1 . . . N}, we have
Y ?C = Y , and the dynamic program will correspond
to exhaustive dynamic programming.
We can again run a Lagrangian relaxation algo-
rithm, using the set Y ?C in place of Y ?. We will use
Lagrange multipliers u(i) to enforce the constraints
y(i) = 1 for i /? C. Our goal will be to find a
small set of constraints C, such that Lagrangian re-
laxation will successfully recover an optimal solu-
tion. We will do this by incrementally adding el-
ements to C; that is, by incrementally adding con-
straints that tighten the relaxation.
The intuition behind our approach is as follows.
Say we run the original algorithm, with the set Y ?,
for several iterations, so that L(u) is close to con-
vergence (i.e., L(u) is close to its minimal value).
However, assume that we have not yet generated a
solution yt such that yt(i) = 1 for all i. In this case
we have some evidence that the relaxation may not
be tight, and that we need to add some constraints.
The question is, which constraints to add? To an-
swer this question, we run the subgradient algorithm
for K more iterations (e.g., K = 10), and at each it-
eration track which constraints of the form y(i) = 1
are violated. We then choose C to be the G con-
straints (e.g., G = 3) that are violated most often
during the K additional iterations, and are not ad-
jacent to each other. We recursively call the algo-
rithm, replacing Y ? by Y ?C ; the recursive call may
then return an exact solution, or alternatively again
add more constraints and make a recursive call.4
Figure 3 depicts the resulting algorithm. We ini-
tially make a call to the algorithm Optimize(C, u)
with C equal to the empty set (i.e., no hard con-
straints), and with u(i) = 0 for all i. In an initial
phase the algorithm runs subgradient steps, while
the dual is still improving. In a second step, if a so-
lution has not been found, the algorithm runs for K
more iterations, thereby choosing G additional con-
straints, then recursing.
If at any stage the algorithm finds a solution y?
such that y?(i) = 1 for all i, then this is the so-
lution to our original problem, argmaxy?Y f(y).
This follows because for any C ? {1 . . . N} we
have Y ? Y ?C ; hence the theorems in section 4.3 go
through for Y ?C in place of Y ?, with trivial modifica-
tions. Note also that the algorithm is guaranteed to
eventually find the optimal solution, because even-
tually C = {1 . . . N}, and Y = Y ?C .
4Formal justification for the method comes from the rela-
tionship between Lagrangian relaxation and linear program-
ming relaxations. In cases where the relaxation is not tight,
the subgradient method will essentially move between solu-
tions whose convex combination form a fractional solution to
an underlying LP relaxation (Nedic? and Ozdaglar, 2009). Our
method eliminates the fractional solution through the introduc-
tion of hard constraints.
32
# iter. 1-10 words 11-20 words 21-30 words 31-40 words 41-50 words All sentences
0-7 166 (89.7 %) 219 (39.2 %) 34 ( 6.0 %) 2 ( 0.6 %) 0 ( 0.0 %) 421 (23.1 %) 23.1 %
8-15 17 ( 9.2 %) 187 (33.5 %) 161 (28.4 %) 30 ( 8.6 %) 3 ( 1.8 %) 398 (21.8 %) 44.9 %
16-30 1 ( 0.5 %) 93 (16.7 %) 208 (36.7 %) 112 (32.3 %) 22 ( 13.1 %) 436 (23.9 %) 68.8 %
31-60 1 ( 0.5 %) 52 ( 9.3 %) 105 (18.6 %) 99 (28.5 %) 62 ( 36.9 %) 319 (17.5 %) 86.3 %
61-120 0 ( 0.0 %) 7 ( 1.3 %) 54 ( 9.5 %) 89 (25.6 %) 45 ( 26.8 %) 195 (10.7 %) 97.0 %
121-250 0 ( 0.0 %) 0 ( 0.0 %) 4 ( 0.7 %) 14 ( 4.0 %) 31 ( 18.5 %) 49 ( 2.7 %) 99.7 %
x 0 ( 0.0 %) 0 ( 0.0 %) 0 ( 0.0 %) 1 ( 0.3 %) 5 ( 3.0 %) 6 ( 0.3 %) 100.0 %
Table 1: Table showing the number of iterations taken for the algorithm to converge. x indicates sentences that fail to
converge after 250 iterations. 97% of the examples converge within 120 iterations.
# cons. 1-10 words 11-20 words 21-30 words 31-40 words 41-50 words All sentences
0-0 183 (98.9 %) 511 (91.6 %) 438 (77.4 %) 222 (64.0 %) 82 ( 48.8 %) 1,436 (78.7 %) 78.7 %
1-3 2 ( 1.1 %) 45 ( 8.1 %) 94 (16.6 %) 87 (25.1 %) 50 ( 29.8 %) 278 (15.2 %) 94.0 %
4-6 0 ( 0.0 %) 2 ( 0.4 %) 27 ( 4.8 %) 24 ( 6.9 %) 19 ( 11.3 %) 72 ( 3.9 %) 97.9 %
7-9 0 ( 0.0 %) 0 ( 0.0 %) 7 ( 1.2 %) 13 ( 3.7 %) 12 ( 7.1 %) 32 ( 1.8 %) 99.7 %
x 0 ( 0.0 %) 0 ( 0.0 %) 0 ( 0.0 %) 1 ( 0.3 %) 5 ( 3.0 %) 6 ( 0.3 %) 100.0 %
Table 2: Table showing the number of constraints added before convergence of the algorithm in Figure 3, broken down by sentence
length. Note that a maximum of 3 constraints are added at each recursive call, but that fewer than 3 constraints are added in cases
where fewer than 3 constraints have count(i) > 0. x indicates the sentences that fail to converge after 250 iterations. 78.7% of the
examples converge without adding any constraints.
The remaining question concerns the ?dual still
improving? condition; i.e., how to determine that the
first phase of the algorithm should terminate. We do
this by recording the first and second best dual val-
ues L(u?) and L(u??) in the sequence of Lagrange
multipliers u1, u2, . . . generated by the algorithm.
Suppose that L(u??) first occurs at iteration t??. If
L(u?)?L(u??)
t?t?? < , we say that the dual value does notdecrease enough. The value for  is a parameter of
the approach: in experiments we used  = 0.002.
See the supplementary material for this submis-
sion for an example run of the algorithm.
When C 6= ?, A* search can be used for de-
coding, with the dynamic program for Y ? provid-
ing admissible estimates for the dynamic program
for Y ?C . Experiments show that A* gives significant
improvements in efficiency. The supplementary ma-
terial contains a full description of the A* algorithm.
6 Experiments
In this section, we present experimental results to
demonstrate the efficiency of the decoding algo-
rithm. We compare to MOSES (Koehn et al, 2007),
a phrase-based decoder using beam search, and to
a general purpose integer linear programming (ILP)
solver, which solves the problem exactly.
The experiments focus on translation from Ger-
man to English, using the Europarl data (Koehn,
2005). We tested on 1,824 sentences of length at
most 50 words. The experiments use the algorithm
shown in Figure 3. We limit the algorithm to a max-
imum of 250 iterations and a maximum of 9 hard
constraints. The distortion limit d is set to be four,
and we prune the phrase translation table to have 10
English phrases per German phrase.
Our method finds exact solutions on 1,818 out
of 1,824 sentences (99.67%). (6 examples do not
converge within 250 iterations.) Table 1 shows the
number of iterations required for convergence, and
Table 2 shows the number of constraints required
for convergence, broken down by sentence length.
In 1,436/1,818 (78.7%) sentences, the method con-
verges without adding hard constraints to tighten the
relaxation. For sentences with 1-10 words, the vast
majority (183 out of 185 examples) converge with
0 constraints added. As sentences get longer, more
constraints are often required. However most exam-
ples converge with 9 or fewer constraints.
Table 3 shows the average times for decoding,
broken down by sentence length, and by the number
of constraints that are added. As expected, decod-
ing times increase as the length of sentences, and
the number of constraints required, increase. The
average run time across all sentences is 120.9 sec-
onds. Table 3 also shows the run time of the method
without the A* algorithm for decoding. The A* al-
gorithm gives significant reductions in runtime.
33
# cons. 1-10 words 11-20 words 21-30 words 31-40 words 41-50 words All sentencesA* w/o A* w/o A* w/o A* w/o A* w/o A* w/o
0-0 0.8 0.8 9.7 10.7 47.0 53.7 153.6 178.6 402.6 492.4 64.6 76.1
1-3 2.4 2.9 23.2 28.0 80.9 102.3 277.4 360.8 686.0 877.7 241.3 309.7
4-6 0.0 0.0 28.2 38.8 111.7 163.7 309.5 575.2 1,552.8 1,709.2 555.6 699.5
7-9 0.0 0.0 0.0 0.0 166.1 500.4 361.0 1,467.6 1,167.2 3,222.4 620.7 1,914.1
mean 0.8 0.9 10.9 12.3 57.2 72.6 203.4 299.2 679.9 953.4 120.9 168.9
median 0.7 0.7 8.9 9.9 48.3 54.6 169.7 202.6 484.0 606.5 35.2 40.0
Table 3: The average time (in seconds) for decoding using the algorithm in Figure 3, with and without A* algorithm, broken down
by sentence length and the number of constraints that are added. A* indicates speeding up using A* search; w/o denotes without
using A*.
method ILP LP
set length mean median mean median % frac.
Y ?? 1-10 275.2 132.9 10.9 4.4 12.4 %11-15 2,707.8 1,138.5 177.4 66.1 40.8 %
16-20 20,583.1 3,692.6 1,374.6 637.0 59.7 %
Y ? 1-10 257.2 157.7 18.4 8.9 1.1 %
11-15 3607.3 1838.7 476.8 161.1 3.0 %
Table 4: Average and median time of the LP/ILP solver (in
seconds). % frac. indicates how often the LP gives a fractional
answer. Y ? indicates the dynamic program using set Y ? as de-
fined in Section 4.1, and Y ?? indicates the dynamic program us-
ing states (w1, w2, n, r). The statistics for ILP for length 16-20
are based on 50 sentences.
6.1 Comparison to an LP/ILP solver
To compare to a linear programming (LP) or inte-
ger linear programming (ILP) solver, we can im-
plement the dynamic program (search over the set
Y ?) through linear constraints, with a linear ob-
jective. The y(i) = 1 constraints are also lin-
ear. Hence we can encode our relaxation within an
LP or ILP. Having done this, we tested the result-
ing LP or ILP using Gurobi, a high-performance
commercial grade solver. We also compare to
an LP or ILP where the dynamic program makes
use of states (w1, w2, n, r)?i.e., the span (l,m) is
dropped, making the dynamic program smaller. Ta-
ble 4 shows the average time taken by the LP/ILP
solver. Both the LP and the ILP require very long
running times on these shorter sentences, and run-
ning times on longer sentences are prohibitive. Our
algorithm is more efficient because it leverages the
structure of the problem, by directly using a combi-
natorial algorithm (dynamic programming).
6.2 Comparison to MOSES
We now describe comparisons to the phrase-based
decoder implemented in MOSES. MOSES uses
beam search to find approximate solutions.
The distortion limit described in section 3 is the
same as that in Koehn et al (2003), and is the same
as that described in the user manual for MOSES
(Koehn et al, 2007). However, a complicating fac-
tor for our comparisons is that MOSES uses an ad-
ditional distortion constraint, not documented in the
manual, which we describe here.5 We call this con-
straint the gap constraint. We will show in experi-
ments that without the gap constraint, MOSES fails
to produce translations on many examples. In our
experiments we will compare to MOSES both with
and without the gap constraint (in the latter case, we
discard examples where MOSES fails).
We now describe the gap constraint. For a se-
quence of phrases p1, . . . , pk define ?(p1 . . . pk) to
be the index of the left-most source-language word
not translated in this sequence. For example, if
the bit-string for p1 . . . pk is 111001101000, then
?(p1 . . . pk) = 4. A sequence of phrases p1 . . . pL
satisfies the gap constraint if and only if for k =
2 . . . L, |t(pk) + 1 ? ?(p1 . . . pk)| ? d, where d is
the distortion limit. We will call MOSES without
this restriction MOSES-nogc, and MOSES with this
restriction MOSES-gc.
Results for MOSES-nogc Table 5 shows the
number of examples where MOSES-nogc fails to
give a translation, and the number of search errors
for those cases where it does give a translation, for
a range of beam sizes. A search error is defined as a
case where our algorithm produces an exact solution
that has higher score than the output from MOSES-
nogc. The number of search errors is significant,
even for large beam sizes.
5Personal communication from Philipp Koehn; see also the
software for MOSES.
34
Beam size Fails # search errors percentage
100 650/1,818 214/1,168 18.32 %
200 531/1,818 207/1,287 16.08 %
1000 342/1,818 115/1,476 7.79 %
10000 169/1,818 68/1,649 4.12 %
Table 5: Table showing the number of examples where
MOSES-nogc fails to give a translation, and the num-
ber/percentage of search errors for cases where it does give a
translation.
Diff. MOSES-gc MOSES-gc MOSES-nogcs =100 s =200 s=1000
0.000 ? 0.125 66 (24.26%) 65 (24.07%) 32 ( 27.83%)
0.125 ? 0.250 59 (21.69%) 58 (21.48%) 25 ( 21.74%)
0.250 ? 0.500 65 (23.90%) 65 (24.07%) 25 ( 21.74%)
0.500 ? 1.000 49 (18.01%) 49 (18.15%) 23 ( 20.00%)
1.000 ? 2.000 31 (11.40%) 31 (11.48%) 5 ( 4.35%)
2.000 ? 4.000 2 ( 0.74%) 2 ( 0.74%) 3 ( 2.61%)
4.000 ?13.000 0 ( 0.00%) 0 ( 0.00%) 2 ( 1.74%)
Table 6: Table showing statistics for the difference between the
translation score from MOSES, and from the optimal deriva-
tion, for those sentences where a search error is made. For
MOSES-gc we include cases where the translation produced by
our system is not reachable by MOSES-gc. The average score
of the optimal derivations is -23.4.
Results for MOSES-gc MOSES-gc uses the gap
constraint, and thus in some cases our decoder will
produce derivations which MOSES-gc cannot reach.
Among the 1,818 sentences where we produce a so-
lution, there are 270 such derivations. For the re-
maining 1,548 sentences, MOSES-gc makes search
errors on 2 sentences (0.13%) when the beam size is
100, and no search errors when the beam size is 200,
1,000, or 10,000.
Table 6 shows statistics for the magnitude of
the search errors that MOSES-gc and MOSES-nogc
make.
BLEU Scores Finally, table 7 gives BLEU scores
(Papineni et al, 2002) for decoding using MOSES
and our method. The BLEU scores under the two
decoders are almost identical; hence while MOSES
makes a significant proportion of search errors, these
search errors appear to be benign in terms of their
impact on BLEU scores, at least for this particular
translation model. Future work should investigate
why this is the case, and whether this applies to other
models and language pairs.
7 Conclusions
We have described an exact decoding algorithm for
phrase-based translation models, using Lagrangian
type of Moses beam size # sents Moses our method
MOSES-gc
100 1,818 24.4773 24.5395
200 1,818 24.4765 24.5395
1,000 1,818 24.4765 24.5395
10,000 1,818 24.4765 24.5395
MOSES-nogc
100 1,168 27.3546 27.3249
200 1,287 27.0591 26.9907
1,000 1,476 26.5734 26.6128
10,000 1,649 25.6531 25.6620
Table 7: BLEU score comparisons. We consider only
those sentences where both decoders produce a transla-
tion.
relaxation. The algorithmic construction we have
described may also be useful in other areas of NLP,
for example natural language generation. Possi-
ble extensions to the approach include methods that
incorporate the Lagrangian relaxation formulation
within learning algorithms for statistical MT: we see
this as an interesting avenue for future research.
A Step Size
Similar to Koo et al (2010), we set the step size at
the t?th iteration to be ?t = 1/(1 + ?t), where ?t is
the number of times that L(u(t?)) > L(u(t??1)) for
all t? ? t. Thus the step size decreases each time the
dual value increases from one iteration to the next.
Acknowledgments Yin-Wen Chang and Michael
Collins were supported under the GALE program
of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022. Michael Collins
was also supported by NSF grant IIS-0915176.
References
Graeme Blackwood, Adria` de Gispert, Jamie Brunning,
and William Byrne. 2009. Large-scale statistical
machine translation with weighted finite state trans-
ducers. In Proceeding of the 2009 conference on
Finite-State Methods and Natural Language Process-
ing: Post-proceedings of the 7th International Work-
shop FSMNLP 2008, pages 39?49, Amsterdam, The
Netherlands, The Netherlands. IOS Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311, June.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
35
ings of the 39th Annual Meeting on Association for
Computational Linguistics, ACL ?01, pages 228?235.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology, NAACL ?03,
pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
MT Summit.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In Proceedings of the 11th
International Conference on Computer Vision.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1288?1298, Cambridge, MA, October. Association for
Computational Linguistics.
Bernhard Korte and Jens Vygen. 2008. Combinatorial
Optimization: Theory and Application. Springer Ver-
lag.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ?05, pages 161?168.
Claude Lemare?chal. 2001. Lagrangian Relaxation.
In Computational Combinatorial Optimization, Op-
timal or Provably Near-Optimal Solutions [based
on a Spring School], pages 112?156, London, UK.
Springer-Verlag.
Angelia Nedic? and Asuman Ozdaglar. 2009. Approxi-
mate primal solutions and rate analysis for dual sub-
gradient methods. SIAM Journal on Optimization,
19(4):1757?1780.
Franz Josef Och, Christoph Tillmann, Hermann Ney, and
Lehrstuhl Fiir Informatik. 1999. Improved alignment
models for statistical machine translation. In Pro-
ceedings of the Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora, pages 20?28.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statisti-
cal machine translation. In Proceedings of the work-
shop on Data-driven methods in machine translation -
Volume 14, DMMT ?01, pages 1?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, ACL ?03, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
ACL 2002.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?06, pages 129?137, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Sebastian Riedel and James Clarke. 2009. Revisiting
optimal decoding for machine translation IBM model
4. In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Papers, NAACL-
Short ?09, pages 5?8, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through La-
grangian relaxation. In Proceedings of ACL.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11, Cambridge, MA, October. Association for
Computational Linguistics.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?08, pages 145?156.
David Sontag, Talya Meltzer, Amir Globerson, Tommi
Jaakkola, and Yair Weiss. 2008. Tightening LP relax-
ations for MAP using message passing. In Proceed-
ings of the 24th Conference on Uncertainty in Artifi-
cial Intelligence, pages 503?510.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29:97?133, March.
Christoph Tillmann. 2006. Efficient dynamic pro-
gramming search algorithms for phrase-based SMT.
36
In Proceedings of the Workshop on Computationally
Hard Problems and Joint Inference in Speech and Lan-
guage Processing, CHSLP ?06, pages 9?16.
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, HLT-NAACL
?06, pages 423?430, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Martin Wainwright, Tommi Jaakkola, and Alan Will-
sky. 2005. MAP estimation via agreement on
trees: Message-passing and linear programming. In
IEEE Transactions on Information Theory, volume 51,
pages 3697?3717.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine transla-
tion as a traveling salesman problem. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1 - Volume 1, ACL ?09, pages 333?341, Stroudsburg,
PA, USA. Association for Computational Linguistics.
37
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 205?213, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Spectral Dependency Parsing with Latent Variables
Paramveer S. Dhillon1, Jordan Rodu2, Michael Collins3, Dean P. Foster2
and Lyle H. Ungar1
1Computer & Information Science/ 2Statistics, University of Pennsylvania, Philadelphia, PA, U.S.A
3 Computer Science, Columbia University, New York, NY, U.S.A
{dhillon|ungar@cis.upenn.edu}, {jrodu|foster@wharton.upenn.edu}
mcollins@cs.columbia.edu
Abstract
Recently there has been substantial interest in
using spectral methods to learn generative se-
quence models like HMMs. Spectral meth-
ods are attractive as they provide globally con-
sistent estimates of the model parameters and
are very fast and scalable, unlike EM meth-
ods, which can get stuck in local minima. In
this paper, we present a novel extension of
this class of spectral methods to learn depen-
dency tree structures. We propose a simple
yet powerful latent variable generative model
for dependency parsing, and a spectral learn-
ing method to efficiently estimate it. As a pi-
lot experimental evaluation, we use the spec-
tral tree probabilities estimated by our model
to re-rank the outputs of a near state-of-the-
art parser. Our approach gives us a moderate
reduction in error of up to 4.6% over the base-
line re-ranker.
1 Introduction
Markov models have been for two decades a
workhorse of statistical pattern recognition with ap-
plications ranging from speech to vision to lan-
guage. Adding latent variables to these models gives
us additional modeling power and have shown suc-
cess in applications like POS tagging (Merialdo,
1994), speech recognition (Rabiner, 1989) and ob-
ject recognition (Quattoni et al2004). However,
this comes at the cost that the resulting parameter
estimation problem becomes non-convex and tech-
niques like EM (Dempster et al1977) which are
used to estimate the parameters can only lead to lo-
cally optimal solutions.
Recent work by Hsu et al2008) has shown that
globally consistent estimates of the parameters of
HMMs can be found by using spectral methods, par-
ticularly by singular value decomposition (SVD) of
appropriately defined linear systems. They avoid the
NP Hard problem of the global optimization prob-
lem of the HMM parameters (Terwijn, 2002), by
putting restrictions on the smallest singular value
of the HMM parameters. The main intuition be-
hind the model is that, although the observed data
(i.e. words) seem to live in a very high dimensional
space, but in reality they live in a very low dimen-
sional space (size k ? 30 ? 50) and an appropriate
eigen decomposition of the observed data will re-
veal the underlying low dimensional dynamics and
thereby revealing the parameters of the model. Be-
sides ducking the NP hard problem, the spectral
methods are very fast and scalable to train compared
to EM methods.
In this paper we generalize the approach of Hsu
et al2008) to learn dependency tree structures with
latent variables.1 Petrov et al2006) and Musillo
and Merlo (2008) have shown that learning PCFGs
and dependency grammars respectively with latent
variables can produce parsers with very good gen-
eralization performance. However, both these ap-
proaches rely on EM for parameter estimation and
can benefit from using spectral methods.
We propose a simple yet powerful latent vari-
able generative model for use with dependency pars-
1Actually, instead of using the model by Hsu et al2008)
we work with a related model proposed by Foster et al2012)
which addresses some of the shortcomings of the earlier model
which we detail below.
205
ing which has one hidden node for each word in
the sentence, like the one shown in Figure 1 and
work out the details for the parameter estimation
of the corresponding spectral learning model. At
a very high level, the parameter estimation of our
model involves collecting unigram, bigram and tri-
gram counts sensitive to the underlying dependency
structure of the given sentence.
Recently, Luque et al2012) have also proposed
a spectral method for dependency parsing, however
they deal with horizontal markovization and use hid-
den states to model sequential dependencies within a
word?s sequence of children. In contrast with that, in
this paper, we propose a spectral learning algorithm
where latent states are not restricted to HMM-like
distributions of modifier sequences for a particular
head, but instead allow information to be propagated
through the entire tree.
More recently, Cohen et al2012) have proposed
a spectral method for learning PCFGs.
Its worth noting that recent work by Parikh et al
(2011) also extends Hsu et al2008) to latent vari-
able dependency trees like us but under the restric-
tive conditions that model parameters are trained for
a specified, albeit arbitrary, tree topology.2 In other
words, all training sentences and test sentences must
have identical tree topologies. By doing this they al-
low for node-specific model parameters, but must re-
train the model entirely when a different tree topol-
ogy is encountered. Our model on the other hand al-
lows the flexibility and efficiency of processing sen-
tences with a variety of tree topologies from a single
training run.
Most of the current state-of-the-art dependency
parsers are discriminative parsers (Koo et al2008;
McDonald, 2006) due to the flexibility of represen-
tations which can be used as features leading to bet-
ter accuracies and the ease of reproducibility of re-
sults. However, unlike discriminative models, gen-
erative models can exploit unlabeled data. Also, as
is common in statistical parsing, re-ranking the out-
puts of a parser leads to significant reductions in er-
ror (Collins and Koo, 2005).
Since our spectral learning algorithm uses a gen-
2This can be useful in modeling phylogeny trees for in-
stance, but precludes most NLP applications, since there is a
need to model the full set of different tree topologies possible
in parsing.
h0
h1 h2
was
Kilroy here
Figure 1: Sample dependency parsing tree for ?Kilroy
was here?
erative model of words given a tree structure, it can
score a tree structure i.e. its probability of genera-
tion. Thus, it can be used to re-rank the n-best out-
puts of a given parser.
The remainder of the paper is organized as fol-
lows. In the next section we introduce the notation
and give a brief overview of the spectral algorithm
for learning HMMs (Hsu et al2008; Foster et al
2012). In Section 3 we describe our proposed model
for dependency parsing in detail and work out the
theory behind it. Section 4 provides experimental
evaluation of our model on Penn Treebank data. We
conclude with a brief summary and future avenues
for research.
2 Spectral Algorithm For Learning HMMs
In this section we describe the spectral algorithm for
learning HMMs.3
2.1 Notation
The HMM that we consider in this section is a se-
quence of hidden states h ? {1, . . . , k} that follow
the Markov property:
p(ht|h1, . . . , ht?1) = p(ht|ht?1)
and a sequence of observations x ? {1, . . . , n} such
that
p(xt|x1, . . . , xt?1, h1, . . . , ht) = p(xt|ht)
3As mentioned earlier, we use the model by Foster et al
(2012) which is conceptually similar to the one by Hsu et al
(2008), but does further dimensionality reduction and thus has
lower sample complexity. Also, critically, the fully reduced di-
mension model that we use generalizes much more cleanly to
trees.
206
The parameters of this HMM are:
? A vector pi of length k where pii = p(h1 = i):
The probability of the start state in the sequence
being i.
? A matrix T of size k ? k where
Ti,j = p(ht+1 = i|ht = j): The probability of
transitioning to state i, given that the previous
state was j.
? A matrix O of size n? k where
Oi,j = p(x = i|h = j): The probability of
state h emitting observation x.
Define ?j to be the vector of length n with a 1 in
the jth entry and 0 everywhere else, and diag(v) to
be the matrix with the entries of v on the diagonal
and 0 everywhere else.
The joint distribution of a sequence of observa-
tions x1, . . . , xm and a sequence of hidden states
h1, . . . , hm is:
p(x1, . . . ,xm, h1, . . . , hm)
= pih1
m?1?
j=2
Thj ,hj?1
m?
j=1
Oxj ,hj
Now, we can write the marginal probability of a
sequence of observations as
p(x1, . . . xm)
=
?
h1,...,hm
p(x1, . . . , xm, h1, . . . , hm)
which can be expressed in matrix form4 as:
p(x1, . . . , xm) = 1>AxmAxm?1 ? ? ?Am1pi
where Axm ? Tdiag(O
>?xm), and 1 is a k-
dimensional vector with every entry equal to 1.
A is called an ?observation operator?, and is ef-
fectively a third order tensor, and Axm which is a
matrix, gives the distribution vector over states at
timem+1 as a function of the state distribution vec-
tor at the current time m and the current observation
?xm . SinceAxm depends on the hidden state, it is not
observable, and hence cannot be directly estimated.
4This is essentially the matrix form of the standard dynamic
program (forward algorithm) used to estimate HMMs.
However, Hsu et al2008) and Foster et al2012)
showed that under certain conditions there exists a
fully observable representation of the observable op-
erator model.
2.2 Fully observable representation
Before presenting the model, we need to address a
few more points. First, let U be a ?representation
matrix? (eigenfeature dictionary) which maps each
observation to a reduced dimension space (n ? k)
that satisfies the conditions:
? U>O is invertible
? |Uij | < 1.
Hsu et al2008); Foster et al2012) discuss U
in more detail, but U can, for example, be obtained
by the SVD of the bigram probability matrix (where
Pij = p(xt+1 = i|xt = j)) or by doing CCA on
neighboring n-grams (Dhillon et al2011).
Letting yi = U>?xi , we have
p(x1, . . . , xm)
= c>?C(ym)C(ym?1) . . . C(y1)c1 (1)
where
c1 = ?
c? = ?
>??1
C(y) = K(y)??1
and ?, ? and K, described in more detail below, are
quantities estimated by frequencies of unigrams, bi-
grams, and trigrams in the observed (training) data.
Under the assumption that data is generated by
an HMM, the distribution p? obtained by substituting
the estimated values c?1, c??, and C?(y) into equation
(1) converges to p sufficiently fast as the amount of
training data increases, giving us consistent param-
eter estimates. For details of the convergence proof,
please see Hsu et al2008) and Foster et al2012).
3 Spectral Algorithm For Learning
Dependency Trees
In this section, we first describe a simple latent vari-
able generative model for dependency parsing. We
then define some extra notation and finally present
207
the details of the corresponding spectral learning al-
gorithm for dependency parsing, and prove that our
learning algorithm provides a consistent estimation
of the marginal probabilities.
It is worth mentioning that an alternate way of ap-
proaching the spectral estimation of latent states for
dependency parsing is by converting the dependency
trees into linear sequences from root-to-leaf and do-
ing a spectral estimation of latent states using Hsu
et al2008). However, this approach would not
give us the correct probability distribution over trees
as the probability calculations for different paths
through the trees are not independent. Thus, al-
though one could calculate the probability of a path
from the root to a leaf, one cannot generalize from
this probability to say anything about the neighbor-
ing nodes or words. Put another way, when a par-
ent has more than the one descendant, one has to be
careful to take into account that the hidden variables
at each child node are all conditioned on the hidden
variable of the parent.
3.1 A latent variable generative model for
dependency parsing
In the standard setting, we are given training exam-
ples where each training example consists of a se-
quence of words x1, . . . , xm together with a depen-
dency structure over those words, and we want to
estimate the probability of the observed structure.
This marginal probability estimates can then be used
to build an actual generative dependency parser or,
since the marginal probability is conditioned on the
tree structure, it can be used re-rank the outputs of a
parser.
As in the conventional HMM described in the pre-
vious section, we can define a simple latent variable
first order dependency parsing model by introduc-
ing a hidden variable hi for each word xi. The
joint probability of a sequence of observed nodes
x1, . . . , xm together with hidden nodes h1, . . . , hm
can be written as
p(x1, . . . ,xm, h1, . . . , hm)
= pih1
m?
j=2
td(j)(hj |hpa(j))
m?
j=1
o(xj |hj)
(2)
h1
h2 h3
y1
y2 y3
Figure 2: Dependency parsing tree with observed vari-
ables y1, y2, and y3.
where pa(j) is the parent of node j and d(j) ?
{L,R} indicates whether hj is a left or a right node
of hpa(j). For simplicity, the number of hidden and
observed nodes in our tree are the same, however
they are not required to be so.
As is the case with the conventional HMM, the
parameters used to calculate this joint probability
are unobservable, but it turns out that under suitable
conditions a fully observable model is also possible
for the dependency tree case with the parameteriza-
tion as described below.
3.2 Model parameters
We will define both the theoretical representations
of our observable parameters, and the sampling ver-
sions of these parameters. Note that in all the cases,
the estimated versions are unbiased estimates of the
theoretical quantities.
Define Td and T ud where d ? {L,R} to be the
hidden state transition matrices from parent to left
or right child, and from left or right child to parent
(hence the u for ?up?), respectively. In other words
(referring to Figure 2)
TR = t(h3|h1)
TL = t(h2|h1)
T uR = t(h1|h3)
T uL = t(h1|h2)
Let Ux(i) be the i
th entry of vector U>?x andG =
U>O. Further, recall the notation diag(v), which is
a matrix with elements of v on its diagonal, then:
? Define the k-dimensional vector ? (unigram
208
counts):
? = Gpi
[??]i =
n?
u=1
c?(u)Uu(i)
where c?(u) = c(u)N1 , c(u) is the count of ob-
servation u in the training sample, and N1 =?
u?n c(u).
? Define the k?k matrices ?L and ?R (left child-
parent and right child-parent bigram counts):
[??L]i,j =
n?
u=1
n?
v=1
c?L(u, v)Uu(j)Uv(i)
?L = GT
u
Ldiag(pi)G
>
[??R]i,j =
n?
u=1
n?
v=1
c?R(u, v)Uu(j)Uv(i)
?R = GT
u
Rdiag(pi)G
>
where c?L(u, v) =
cL(u,v)
N2L
, cL(u, v) is the count
of bigram (u, v) where u is the left child and
v is the parent in the training sample, and
N2L =
?
(u,v)?n?n cL(u, v). Define c?R(u, v)
similarly.
? Define k ? k ? k tensor K (left child-parent-
right child trigram counts):
K?i,j,l =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(i)Uu(j)Uv(l)
K(y) = GTLdiag(G
>y)T uRdiag(pi)G
>
where c?(w, u, v) = c(w,u,v)N3 , c(w, u, v) is
the count of bigram (w, u, v) where w is
the left child, u is the parent and v is the
right child in the training sample, and N3 =?
(w,u,v)?n?n?n c(w, u, v).
? Define k?k matrices ?L and ?R (skip-bigram
counts (left child-right child) and (right child-
left child)) 5:
[??L]i,j =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(i)Uu(j)
?L = GTLT
u
Rdiag(pi)G
>
[??R]i,j =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(j)Uu(i)
?R = GTRT
u
Ldiag(pi)G
>
3.3 Parameter estimation
Using the above definitions, we can estimate the pa-
rameters of the model, namely ?,?L,?R,?L,?R
andK, from the training data and define observables
useful for the dependency model as6
c1 = ?
cT? = ?
T??1R
EL = ?L?
?1
R
ER = ?R?
?1
L
D(y) = E?1L K(y)?
?1
R
As we will see, these quantities allow us to recur-
sively compute the marginal probability of the de-
pendency tree, p?(x1, . . . , xm), in a bottom up man-
ner by using belief propagation.
To see this, let hch(i) be the set of hidden chil-
dren of hidden node i (in Figure 2 for instance,
hch(1) = {2, 3}) and let och(i) be the set of ob-
served children of hidden node i (in the same figure
och(i) = {1}). Then compute the marginal proba-
bility p(x1, . . . , xm) from Equation 2 as
ri(h) =
?
j?hch(i)
?j(h)
?
j?och(i)
o(xj |h) (3)
where ?i(h) is defined by summing over all
the hidden random variables i.e., ?i(h) =?
h? p(h
?|h)ri(h?).
This can be written in a compact matrix form as
??ri
> = 1>
?
j?hch(i)
diag(T>dj
??rj )
?
?
j?och(i)
diag(O>?xj ) (4)
5Note than ?R = ?TL , which is not immediately obvious
from the matrix representations.
6The details of the derivation follow directly from the matrix
versions of the variables.
209
where ??ri is a vector of size k (the dimensionality of
the hidden space) of values ri(h). Note that since in
Equation 2 we condition on whether xj is the left or
right child of its parent, we have separate transition
matrices for left and right transitions from a given
hidden node dj ? {L,R}.
The recursive computation can be written in terms
of observables as:
??ri
> = c>?
?
j?hch(i)
D(E>dj
??rj )
?
?
j?och(i)
D((U>U)?1U>?xj )
The final calculation for the marginal probability
of a given sequence is
p?(x1, . . . , xm) =
??r1
>c1 (5)
The spectral estimation procedure is described be-
low in Algorithm 1.
Algorithm 1 Spectral dependency parsing (Comput-
ing marginal probability of a tree.)
1: Input: Training examples- x(i) for i ? {1, . . . ,M}
along with dependency structures where each se-
quence x(i) = x(i)1 , . . . , x
(i)
mi .
2: Compute the spectral parameters ??, ??R, ??L, ??R,
??L, and K?
#Now, for a given sentence, we can recursively com-
pute the following:
3: for x(i)j for j ? {mi, . . . , 1} do
4: Compute:
??ri
> = c>?
?
j?hch(i)
D(E>dj
??rj )
?
?
j?och(i)
D((U>U)?1U>?xj )
5: end for
6: Finally compute
p?(x1, . . . , xmi) =
??r1
>c1
#The marginal probability of an entire tree.
3.4 Sample complexity
Our main theoretical result states that the above
scheme for spectral estimation of marginal proba-
bilities provides a guaranteed consistent estimation
scheme for the marginal probabilities:
Theorem 3.1. Let the sequence {x1, . . . , xm} be
generated by an k ? 2 state HMM. Suppose we are
given a U which has the property that U>O is in-
vertible, and |Uij | ? 1. Suppose we use equation
(5) to estimate the probability based on N indepen-
dent triples. Then
N ? Cm
k2
2
log
(
k
?
)
(6)
where Cm is specified in the appendix, implies that
1?  ?
?
?
?
?
p?(x1, . . . , xm)
p(x1, . . . , xm)
?
?
?
? ? 1 + 
holds with probability at least 1? ?.
Proof. A sketch of the proof, in the case without di-
rectional transition parameters, can be found in the
appendix. The proof with directional transition pa-
rameters is almost identical.
4 Experimental Evaluation
Since our algorithm can score any given tree struc-
ture by computing its marginal probability, a natu-
ral way to benchmark our parser is to generate n-
best dependency trees using some standard parser
and then use our algorithm to re-rank the candidate
dependency trees, e.g. using the log spectral prob-
ability as described in Algorithm 1 as a feature in a
discriminative re-ranker.
4.1 Experimental Setup
Our base parser was the discriminatively trained
MSTParser (McDonald, 2006), which implements
both first and second order parsers and is trained
using MIRA (Crammer et al2006) and used the
standard baseline features as described in McDon-
ald (2006).
We tested our methods on the English Penn Tree-
bank (Marcus et al1993). We use the standard
splits of Penn Treebank; i.e., we used sections 2-21
for training, section 22 for development and section
23 for testing. We used the PennConverter7 tool to
convert Penn Treebank from constituent to depen-
dency format. Following (McDonald, 2006; Koo
7http://nlp.cs.lth.se/software/treebank_
converter/
210
et al2008), we used the POS tagger by Ratnaparkhi
(1996) trained on the full training data to provide
POS tags for development and test sets and used 10-
way jackknifing to generate tags for the training set.
As is common practice we stripped our sentences of
all the punctuation. We evaluated our approach on
sentences of all lengths.
4.2 Details of spectral learning
For the spectral learning phase, we need to just col-
lect word counts from the training data as described
above, so there are no tunable parameters as such.
However, we need to have access to an attribute dic-
tionary U which contains a k dimensional represen-
tation for each word in the corpus. A possible way
of generating U as suggested by Hsu et al2008) is
by performing SVD on bigrams P21 and using the
left eigenvectors as U . We instead used the eigen-
feature dictionary proposed by Dhillon et al2011)
(LR-MVL) which is obtained by performing CCA
on neighboring words and has provably better sam-
ple complexity for rare words compared to the SVD
alternative.
We induced the LR-MVL embeddings for words
using the Reuters RCV1 corpus which contains
about 63 million tokens in 3.3 million sentences and
used their context oblivious embeddings as our esti-
mate of U . We experimented with different choices
of k (the size of the low dimensional projection)
on the development set and found k = 10 to work
reasonably well and fast. Using k = 10 we were
able to estimate our spectral learning parameters
?,?L,R,?L,R,K from the entire training data in un-
der 2 minutes on a 64 bit Intel 2.4 Ghz processor.
4.3 Re-ranking the outputs of MST parser
We could not find any previous work which de-
scribes features for discriminative re-ranking for de-
pendency parsing, which is due to the fact that un-
like constituency parsing, the base parsers for depen-
dency parsing are discriminative (e.g. MST parser)
which obviates the need for re-ranking as one could
add a variety of features to the baseline parser itself.
However, parse re-ranking is a good testbed for our
spectral dependency parser which can score a given
tree. So, we came up with a baseline set of features
to use in an averaged perceptron re-ranker (Collins,
2002). Our baseline features comprised of two main
Method Accuracy Complete
I Order
MST Parser (No RR) 90.8 37.2
RR w. Base. Features 91.3 37.5
RR w. Base. Features +log p? 91.7 37.8
II Order
MST Parser (No RR) 91.8 40.6
RR w. Base. Features 92.4 41.0
RR w. Base. Features +log p? 92.7 41.3
Table 1: (Unlabeled) Dependency Parse re-ranking re-
sults for English test set (Section 23). Note: 1). RR =
Re-ranking 2). Accuracy is the number of words which
correctly identified their parent and Complete is the num-
ber of sentences for which the entire dependency tree was
correct. 3). Base. Features are the two re-ranking fea-
tures described in Section 4.3. 4). log p? is the spectral log
probability feature.
features which capture information that varies across
the different n-best parses and moreover were not
used as features by the baseline MST parser, ?POS-
left-modifier ? POS-head ? POS-right-modifier?
and ?POS-left/right-modifier ? POS-head ? POS-
grandparent?8. In addition to that we used the log of
spectral probability (p?(x1, . . . , xm) - as calculated
using Algorithm 1) as a feature.
We used the MST parser trained on entire training
data to generate a list of n-best parses for the devel-
opment and test sets. The n-best parses for the train-
ing set were generated by 3-fold cross validation,
where we train on 2 folds to get the parses for the
third fold. In all our experiments we used n = 50.
The results are shown in Table 1. As can be seen,
the best results give up to 4.6% reduction in error
over the re-ranker which uses just the baseline set of
features.
5 Discussion and Future Work
Spectral learning of structured latent variable mod-
els in general is a promising direction as has been
shown by the recent interest in this area. It al-
lows us to circumvent the ubiquitous problem of get-
ting stuck in local minima when estimating the la-
tent variable models via EM. In this paper we ex-
8One might be able to come up with better features for de-
pendency parse re-ranking. Our goal in this paper was just to
get a reasonable baseline.
211
tended the spectral learning ideas to learn a simple
yet powerful dependency parser. As future work, we
are working on building an end-to-end parser which
would involve coming up with a spectral version of
the inside-outside algorithm for our setting. We are
also working on extending it to learn more power-
ful grammars e.g. split head-automata grammars
(SHAG) (Eisner and Satta, 1999).
6 Conclusion
In this paper we proposed a novel spectral method
for dependency parsing. Unlike EM trained gen-
erative latent variable models, our method does not
get stuck in local optima, it gives consistent param-
eter estimates, and it is extremely fast to train. We
worked out the theory of a simple yet powerful gen-
erative model and showed how it can be learned us-
ing a spectral method. As a pilot experimental evalu-
ation we showed the efficacy of our approach by us-
ing the spectral probabilities output by our model for
re-ranking the outputs of MST parser. Our method
reduced the error of the baseline re-ranker by up to
a moderate 4.6%.
7 Appendix
This appendix offers a sketch of the proof of The-
orem 1. The proof uses the following definitions,
which are slightly modified from those of Foster
et al2012).
Definition 1. Define ? as the smallest element of ?,
??1, ??1, and K(). In other words,
? ?min{min
i
|?i|,min
i,j
|??1ij |,mini,j
|??1ij |,
min
i,j,k
|Kijk|,min
i,j
|?ij |,min
i,j
|?ij |, }
where Kijk = K(?j)ik are the elements of the ten-
sor K().
Definition 2. Define ?k as the smallest singular
value of ? and ?.
The proof relies on the fact that a row vector mul-
tiplied by a series of matrices, and finally multiplied
by a column vector amounts to a sum over all possi-
ble products of individual entries in the vectors and
matrices. With this in mind, if we bound the largest
relative error of any particular entry in the matrix by,
say, ?, and there are, say, s parameters (vectors and
matrices) being multiplied together, then by simple
algebra the total relative error of the sum over the
products is bounded by ?s.
The proof then follows from two basic steps.
First, one must bound the maximal relative error, ?
for any particular entry in the parameters, which can
be done using central limit-type theorems and the
quantity ? described above. Then, to calculate the
exponent s one simply counts the number of param-
eters multiplied together when calculating the prob-
ability of a particular sequence of observations.
Since each hidden node is associated with exactly
one observed node, it follows that s = 12m + 2L,
where L is the number of levels (for instance in our
example ?Kilroy was here? there are two levels). s
can be easily computed for arbitrary tree topologies.
It follows from Foster et al2012) that we achieve
a sample complexity
N ?
128k2s2
2 ?2?4k
log
(
2k
?
)
?
?1
? ?? ?
2/s2
( s
?
1 + ? 1)2
(7)
leading to the theorem stated above.
Lastly, note that in reality one does not see ? and
?k but instead estimates of these quantities; Foster
et al2012) shows how to incorporate the accuracy
of the estimates into the sample complexity.
Acknowledgement: We would like to thank
Emily Pitler for valuable feedback on the paper.
References
Shay Cohen, Karl Stratos, Michael Collins, Dean
Foster, and Lyle Ungar. Spectral learning of
latent-variable pcfgs. In Association of Compu-
tational Linguistics (ACL), volume 50, 2012.
Michael Collins. Ranking algorithms for named-
entity extraction: boosting and the voted percep-
tron. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguis-
tics, ACL ?02, pages 489?496, Stroudsburg, PA,
USA, 2002. Association for Computational Lin-
guistics. URL http://dx.doi.org/10.
3115/1073083.1073165.
Michael Collins and Terry Koo. Discriminative
reranking for natural language parsing. Comput.
212
Linguist., 31(1):25?70, March 2005. ISSN 0891-
2017.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. Online
passive-aggressive algorithms. Journal of Ma-
chine Learning Research, 7:551?585, 2006.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the em
algorithm. JRSS, SERIES B, 39(1):1?38, 1977.
Paramveer S. Dhillon, Dean Foster, and Lyle Un-
gar. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Process-
ing Systems (NIPS), volume 24, 2011.
Jason Eisner and Giorgio Satta. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 457?464, Univer-
sity of Maryland, June 1999. URL http://cs.
jhu.edu/?jason/papers/#acl99.
Dean Foster, Jordan Rodu, and Lyle Ungar. Spec-
tral dimensionality reduction for HMMs. ArXiV
http://arxiv.org/abs/1203.6130, 2012.
D Hsu, S M. Kakade, and Tong Zhang. A spec-
tral algorithm for learning hidden markov models.
arXiv:0811.4413v2, 2008.
Terry Koo, Xavier Carreras, and Michael Collins.
Simple semi-supervised dependency parsing. In
In Proc. ACL/HLT, 2008.
F. Luque, A. Quattoni, B. Balle, and X. Carreras.
Spectral learning for non-deterministic depen-
dency parsing. In EACL, 2012.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. Building a large annotated
corpus of english: the penn treebank. Comput.
Linguist., 19:313?330, June 1993. ISSN 0891-
2017.
Ryan McDonald. Discriminative learning and span-
ning tree algorithms for dependency parsing. PhD
thesis, University of Pennsylvania, Philadelphia,
PA, USA, 2006. AAI3225503.
Bernard Merialdo. Tagging english text with a prob-
abilistic model. Comput. Linguist., 20:155?171,
June 1994. ISSN 0891-2017.
Gabriele Antonio Musillo and Paola Merlo. Un-
lexicalised hidden variable models of split de-
pendency grammars. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technolo-
gies: Short Papers, HLT-Short ?08, pages 213?
216, Stroudsburg, PA, USA, 2008. Association
for Computational Linguistics.
Ankur P. Parikh, Le Song, and Eric P. Xing. A spec-
tral algorithm for latent tree graphical models. In
ICML, pages 1065?1072, 2011.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440, Stroudsburg, PA, USA, 2006. As-
sociation for Computational Linguistics.
Ariadna Quattoni, Michael Collins, and Trevor Dar-
rell. Conditional random fields for object recog-
nition. In In NIPS, pages 1097?1104. MIT Press,
2004.
Lawrence R. Rabiner. A tutorial on hidden markov
models and selected applications in speech recog-
nition. In Proceedings of the IEEE, pages 257?
286, 1989.
Adwait Ratnaparkhi. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Eric Brill and
Kenneth Church, editors, Proceedings of the Em-
pirical Methods in Natural Language Processing,
pages 133?142, 1996.
Sebastiaan Terwijn. On the learnability of hidden
markov models. In Proceedings of the 6th Inter-
national Colloquium on Grammatical Inference:
Algorithms and Applications, ICGI ?02, pages
261?268, London, UK, UK, 2002. Springer-
Verlag. ISBN 3-540-44239-1.
213
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1434?1444, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Improved Parsing and POS Tagging Using Inter-Sentence
Consistency Constraints
Alexander M. Rush1? Roi Reichart1? Michael Collins2 Amir Globerson3
1MIT CSAIL, Cambridge, MA, 02139, USA
{srush|roiri}@csail.mit.edu
2Department of Computer Science, Columbia University, New-York, NY 10027, USA
mcollins@cs.columbia.edu
3School of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904, Israel
gamir@cs.huji.ac.il
Abstract
State-of-the-art statistical parsers and POS
taggers perform very well when trained with
large amounts of in-domain data. When train-
ing data is out-of-domain or limited, accuracy
degrades. In this paper, we aim to compen-
sate for the lack of available training data by
exploiting similarities between test set sen-
tences. We show how to augment sentence-
level models for parsing and POS tagging with
inter-sentence consistency constraints. To deal
with the resulting global objective, we present
an efficient and exact dual decomposition de-
coding algorithm. In experiments, we add
consistency constraints to the MST parser
and the Stanford part-of-speech tagger and
demonstrate significant error reduction in the
domain adaptation and the lightly supervised
settings across five languages.
1 Introduction
State-of-the-art statistical parsers and POS taggers
perform very well when trained with large amounts
of data from their test domain. When training data is
out-of-domain or limited, the performance of the re-
sulting model often degrades. In this paper, we aim
to compensate for the lack of available training data
by exploiting similarities between test set sentences.
Most parsing and tagging models are defined at the
sentence-level, which makes such inter-sentence in-
formation sharing difficult. We show how to aug-
ment sentence-level models with inter-sentence con-
straints to encourage consistent descisions in similar
? Both authors contributed equally to this work.
contexts, and we give an efficient algorithm with for-
mal guarantees for decoding such models.
In POS tagging, most taggers perform very well
on word types that they have observed in training
data, but they perform poorly on unknown words.
With a global objective, we can include constraints
that encourage a consistent tag across all occur-
rences of an unknown word type to improve accu-
racy. In dependency parsing, the parser can benefit
from surface-level features of the sentence, but with
sparse or out-of-domain training data these features
are very noisy. Using a global objective, we can add
constraints that encourage similar surface-level con-
texts to exhibit similar syntactic behaviour.
The first contribution of this work is the use of
Markov random fields (MRFs) to model global con-
straints between sentences in dependency parsing
and POS tagging. We represent each word as a node,
the tagging or parse decision as its label, and add
constraints through edges. MRFs allow us to include
global constraints tailored to these problems, and to
reason about inference in the corresponding global
models.
The second contribution is an efficient dual de-
composition algorithm for decoding a global ob-
jective with inter-sentence constraints. These con-
straints generally make direct inference challenging
since they tie together the entire test corpus. To alle-
viate this issue, our algorithm splits the global infer-
ence problem into subproblems - decoding of indi-
vidual sentences, and decoding of the global MRF.
These subproblems can be solved efficiently through
known methods. We show empirically that by iter-
atively solving these subproblems, we can find the
1434
exact solution to the global model.
We experiment with domain adaptation and
lightly supervised training. We demonstrate that
global models with consistency constraints can im-
prove upon sentence-level models for dependency
parsing and part-of-speech tagging. For domain
adaptation, we show an error reduction of up to 7.7%
when adapting the second-order projective MST
parser (McDonald et al 2005) from newswire to
the QuestionBank domain. For lightly supervised
learning, we show an error reduction of up to 12.8%
over the same parser for five languages and an error
reduction of up to 10.3% over the Stanford trigram
tagger (Toutanova et al 2003) for English POS tag-
ging. The algorithm requires, on average, only 1.7
times the costs of sentence-level inference and finds
the exact solution on the vast majority of sentences.
2 Related Work
Methods that combine inter-sentence information
with sentence-level algorithms have been applied to
a number of NLP tasks. The most similar models to
our work are skip-chain CRFs (Sutton and Mccal-
lum, 2004), relational markov networks (Taskar et
al., 2002), and collective inference with symmetric
clique potentials (Gupta et al 2010). These mod-
els use a linear-chain CRF or MRF objective mod-
ified by potentials defined over pairs of nodes or
clique templates. The latter model makes use of La-
grangian relaxation. Skip-chain CRFs and collective
inference have been applied to problems in IE, and
RMNs to named entity recognition (NER) (Bunescu
and Mooney, 2004). Finkel et al(2005) also inte-
grated non-local information into entity annotation
algorithms using Gibbs sampling.
Our model can be applied to a variety of off-the-
shelf structured prediction models. In particular, we
focus on dependency parsing which is characterized
by a more complicated structure compared to the IE
tasks addressed by previous work.
Another line of work that integrates corpus-level
declarative information into sentence-level models
includes the posterior regularization (Ganchev et al
2010; Gillenwater et al 2010), generalized expec-
tation (Mann and McCallum, 2007; Mann and Mc-
Callum, ), and Bayesian measurements (Liang et al
2009) frameworks. The power of these methods has
been demonstrated for a variety of NLP tasks, such
as unsupervised and semi-supervised POS tagging
and parsing. The constraints used by these works
differ from ours in that they encourage the posterior
label distribution to have desired properties such as
sparsity (e.g. a given word can take a small number
of labels with a high probability). In addition, these
methods use global information during training as
opposed to our approach which applies test-time in-
ference global constraints.
The application of dual decomposition for infer-
ence in MRFs has been explored by Wainwright et
al. (2005), Komodakis et al(2007), and Globerson
and Jaakkola (2007). In NLP, Rush et al(2010)
and Koo et al(2010) applied dual decomposition to
enforce agreement between different sentence-level
algorithms for parsing and POS tagging. Work on
dual decomposition for NLP is related to the work
of Smith and Eisner (2008) who apply belief prop-
agation to inference in dependency parsing, and to
constrained conditional models (CCM) (Roth and
Yih, 2005) that impose inference-time constraints
through an ILP formulation.
Several works have addressed semi-supervised
learning for structured prediction, suggesting objec-
tives based on the max-margin principles (Altun and
Mcallester, 2005), manifold regularization (Belkin
et al 2005), a structured version of co-training
(Brefeld and Scheffer, 2006) and an entropy-based
regularizer for CRFs (Wang et al 2009). The com-
plete literature on domain adaptation is beyond the
scope of this paper, but we refer the reader to Blitzer
and Daume (2010) for a recent survey.
Specifically for parsing and POS tagging, self-
training (Reichart and Rappoport, 2007), co-training
(Steedman et al 2003) and active learning (Hwa,
2004) have been shown useful in the lightly su-
pervised setup. For parser adaptation, self-training
(McClosky et al 2006; McClosky and Charniak,
2008), using weakly annotated data from the tar-
get domain (Lease and Charniak, 2005; Rimell and
Clark, 2008), ensemble learning (McClosky et al
2010), hierarchical bayesian models (Finkel and
Manning, 2009) and co-training (Sagae and Tsujii,
2007) achieve substantial performance gains. For a
recent survey see Plank (2011). Constraints simi-
lar to those we use for POS tagging were used by
Subramanya et al(2010) for POS tagger adaptation.
1435
Their work, however, does not show how to decode
a global, corpus-level, objective that enforces these
constraints, which is a major contribution of this pa-
per.
Inter-sentence syntactic consistency has been ex-
plored in the psycholinguistics and NLP literature.
Phenomena such as parallelism and syntactic prim-
ing ? the tendency to repeat recently used syntactic
structures ? have been demonstrated in human lan-
guage corpora (e.g. WSJ and Brown) (Dubey et al
2009) and were shown useful in generative and dis-
criminative parsers (e.g. (Cheung and Penn, 2010)).
We complement these works, which focus on con-
sistency between consecutive sentences, and explore
corpus level consistency.
3 Structured Models
We begin by introducing notation for sentence-
level dependency parsing as a structured prediction
problem. The goal of dependency parsing is to
find the best parse y for a tagged sentence x =
(w1/t1, . . . , wn/tn) with words w and POS tags t.
Define the index set for dependency parsing as
I(x) = {(m,h) : m ? {1 . . . n},
h ? {0 . . . n},m 6= h}
where h = 0 represents the root word. A depen-
dency parse is a vector y = {y(m,h) : (m,h) ?
I(x)} where y(m,h) = 1 if m is a modifier of the
head word h. We define the set Y(x) ? {0, 1}|I(x)|
to be the set of all valid dependency parses for a sen-
tence x. In this work, we use projective dependency
parses, but the method also applies to the set of non-
projective parse trees.
Additionally, we have a scoring function f :
Y(x)? R. The optimal parse y? for a sentence x is
given by, y? = argmaxy?Y(x) f(y). This sentence-
level decoding problem can often be solved effi-
ciently. For example in commonly used projec-
tive dependency parsing models (McDonald et al
2005), we can compute y? efficiently using variants
of the Viterbi algorithm.
For this work, we make the assumption that we
have an efficient algorithm to find the argmax of
f(y) +
?
(m,h)?I(x)
u(m,h)y(m,h) = f(y) + u ? y
where u is a vector in R|I(x)|. In practice, u will be
a vector of Lagrange multipliers associated with the
dependencies of y in our dual decomposition algo-
rithm given in Section 6.
We can construct a very similar setting for POS
tagging where the goal is to find the best tagging
y for a sentence x = (w1, . . . , wn). We skip the
formal details here.
We next introduce notation for Markov random
fields (MRFs) (Koller and Friedman, 2009). An
MRF consists of an undirected graph G = (V,E),
a set of possible labels for each node Li for i ?
{1, . . . , |V |}, and a scoring function g. The index
set for MRFs is
IMRF = {(i, l) : i ? {1 . . . |V |}, l ? Li}
? {((i, j), li, lj) : (i, j) ? E, li ? Li, lj ? Lj}
A label assignment in the MRF is a binary vector
z with z(i, l) = 1 if the label l is selected at node i
and z((i, j), li, lj) = 1 if the labels li, lj are selected
for the nodes i, j.
In applications such as parsing and POS tagging,
some of the label assignments are not allowed. For
example, in dependency parsing the resulting struc-
ture must be a tree. Consequently, if every node
in the MRF corresponds to a word in a document
and its label corresponds to the index of its head
word, the resulting dependency structure for each
sentence must be acyclic. The set of all valid la-
bel assignments (one label per node) is given by
Z ? {0, 1}|IMRF|.
We score label assignments in the MRF with a
scoring function g : Z ? R. The best assignment
z? in an MRF is given by, z? = argmaxz?Z g(z).
We focus on pairwise MRFs where this function g is
a linear function of z whose parameters are denoted
by ?
g(z) = z ? ? =
?
(i,l)?IMRF
z(i, l)?(i, l) +
?
((i,j),li,lj)?IMRF
z((i, j), li, lj)?((i, j), li, lj)
As in parsing, we make the assumption that we
have an efficient algorithm to find the argmax of
g(z) +
?
(i,l)?IMRF(x)
u(i, l)z(i, l)
1436
He/PRP saw/VBD an/DT American/JJ man/NN
The/DT smart/JJ girls/NNS stood/VBD outside/RB
Danny/DT walks/VBZ a/DT long/JJ distance/NN
NN
Figure 1: An example constraint from dependency pars-
ing. The black nodes are modifiers observed in the train-
ing data. Each gray node corresponds to a possible mod-
ifier in the test corpus. The constraint applies to all mod-
ifiers in the context DT JJ. The white node corresponds
to the consensus POS tag of the head word of these mod-
ifiers.
4 A Parsing Example
In this section we give a detailed example of global
constraints for dependency parsing. The aim is to
construct a global objective that encourages similar
contexts across the corpus to exhibit similar syntac-
tic behaviour. We implement this objective using an
MRF with a node for each word in the test set. The
label of each node is the index of the word it mod-
ifies. We add edges to this MRF to reward consis-
tency among similar contexts. Furthermore, we add
nodes with a fixed label to incorporate contexts seen
in the training data.
Specifically, we say that the context of a word is
its POS tag and the POS tags of some set of the
words around it. We expand on this notion of con-
text in Section 8; for simplicity we assume here that
the context includes only the previous word?s POS
tag. Our constraints are designed to bias words in
the same context to modify words with similar POS
tags.
Figure 1 shows a global MRF over a small parsing
example with one training sentence and two test sen-
tences. The MRF contains a node associated with
each word instance, where the label of the node is
the index of the word it modifies. In this corpus, the
context DT JJ appears once in training and twice in
testing. We hope to choose head words with similar
POS tags for these two test contexts biased by the
observed training context.
More concretely, for each context c ?
{1, . . . , C}, we have a set Sc of associated
word indices (s,m) that appear in the context,
where s is a sentence index and m is a position
in that sentence. For instance, in our example
S1 = {(1, 2), (2, 4)} consists of all positions in
the test set where we see JJ preceded by DT.
Futhermore, we have a set Oc of indices (s,m,TR)
of observed instances of the context in the training
data where TR denotes a training index. In our
example O1 = {(1, 4,TR)} consists of the one
training instance. We associate each word instance
with a single context c.
We then define our MRF to include one consensus
node for each set Sc as well as a word node for each
instance in the set Sc ?Oc. Thus the set of variables
corresponds to V = {1, . . . , C} ? (
?C
c=1 Sc ? Oc).
Additionally, we include an edge from each node
i ? Sc?Oc to its consensus node c,E = {(i, c) : c ?
{1, . . . , C}, i ? Sc ?Oc}. The word nodes from Sc
have the label set of possible head indices L(s,m) =
{0, . . . , ns} where ns is the length of the sentence s.
The observed nodes from Oc have a singleton label
set L(s,m,TR) with the observed index. The consen-
sus nodes have the label set Lc = T ? {NULL}
where T is the set of POS tags and the NULL sym-
bol represents the constraint being turned off.
We can now define the scoring function g for this
MRF. The scoring function aims to reward consis-
tency among the head POS tag at each word and the
consensus node
?((i, c), li, lc) =
?
???
???
?1 if pos(li) = lc
?2 if pos(li) is close to lc
?3 lc = NULL
0 otherwise
where posmaps a word index to its POS tag. The pa-
rameters ?1 ? ?2 ? ?3 ? 0 determine the bonus for
identical POS tags, similar POS tags, and for turning
off the constraint .
We construct a similar model for POS tagging.
We choose sets Tc corresponding to the c?th un-
known word type in the corpus. The MRF graph
is identical to the parsing case with Tc replacing Sc
and we no longer have Oc. The label sets for the
word nodes are now L(s,m) = T where the label is
1437
the POS tag chosen at that word, and the label set for
the consensus node is Lc = T ? {NULL}. We use
the same scoring function as in parsing to enforce
consistency between word nodes and the consensus
node.
5 Global Objective
Recall the definition of sentence-level parsing,
where the optimal parse y? for a single sentence
x under a scoring function f is given by: y? =
argmaxy?Y(x) f(y). We apply this objective to
a set of sentences, specified by the tuple X =
(x1, ..., xr), and the product of possible parses
Y(X) = Y(x1) ? . . . ? Y(xr). The sentence-level
decoding problem is to find the optimal dependency
parses Y ? = (Y ?1 , ..., Y ?r ) ? Y(X) under a global
objective
Y ? = argmax
Y ?Y(X)
F (Y ) = argmax
Y ?Y(X)
r?
s=1
f(Ys)
where F : Y(X) ? R is the global scoring func-
tion.
We now consider scoring functions where the
global objective includes inter-sentence constraints.
Objectives of this form will not factor directly
into individual parsing problems; however, we can
choose to write them as the sum of two convenient
terms: (1) A simple sum of sentence-level objec-
tives; and (2) A global MRF that connects the local
structures.
For convenience, we define the following index
set.
J (X) = {(s,m, h) : s ? {1, . . . , r},
(m,h) ? I(xs)}
This set enumerates all possible dependencies at
each sentence in the corpus. We say the parses Ys
are consistent with a label assignment z if for all
(s,m, h) ? J (X) we have that z((s,m), h) =
Ys(m,h). In other words, the labels in z match the
head words chosen in parse Ys.
With this notation we can write the full global de-
coding objective as
(Y ?, z?) = argmax
Y ?Y(X), z?Z
F (Y ) + g(z) (1)
s.t. ?(s,m, h) ? J (X), z((s,m), h) = Ys(m,h)
Set u(1)(s,m, h)? 0 for all (s,m, h) ? J (X)
for k = 1 to K do
z(k) ? argmax
z?Z
(g(z) +
?
(s,m,h)?J (X)
u(k)(s,m, h)z((s,m), h))
Y (k) ? argmax
Y ?Y(X)
(F (Y ) ?
?
(s,m,h)?J (X)
u(k)(s,m, h)Ys(m,h))
if Y (k)s (m,h) = z(k)((s,m), h)
for all (s,m, h) ? J (X) then
return (Y (k), z(k))
for all (s,m, h) ? J (X),
u(k+1)(s,m, h)? u(k)(s,m, h) +
?k(z(k)((s,m), h)? Y (k)s (m,h))
return (Y (K), z(K))
Figure 2: The global decoding algorithm for dependency
parsing models.
The solution to this objective maximizes the local
models as well as the global MRF, while maintain-
ing consistency among the models. Specifically, the
MRF we use in the experiments has a simple naive
Bayes structure with the consensus node connected
to all relevant word nodes.
The global objective for POS tagging has a similar
form. As before we add a node to the MRF for each
word in the corpus. We use the POS tag set as our
labels for each of these nodes. The index set con-
tains an element for each possible tag at each word
instance in the corpus.
6 A Global Decoding Algorithm
We now consider the decoding question: how to
find the structure Y ? that maximizes the global ob-
jective. We aim for an efficient solution that makes
use of the individual solvers at the sentence-level.
For this work, we make the assumption that the
graph chosen for the MRF has small tree-width, e.g.
our naive Bayes constraints, and can be solved effi-
ciently using dynamic programming.
Before we describe our dual decomposition al-
gorithm, we consider the difficulty of solving the
global objective directly. We have an efficient dy-
namic programming algorithm for solving depen-
dency parsing at the sentence-level, and efficient al-
gorithms for solving the MRF. It follows that we
1438
could construct an intersected dynamic program-
ming algorithm that maintains the product of states
over both models. This algorithm is exact, but it
is very inefficient. Solving the intersected dynamic
program requires decoding simultaneously over the
entire corpus, with an additional multiplicative fac-
tor for solving the MRF. On top of this cost, we need
to alter the internal structure of the sentence-level
models.
In contrast, we can construct a dual decomposi-
tion algorithm which is efficient, produces a certifi-
cate when it finds an exact solution, and directly
uses the sentence-level parsing models. Considering
again the global objective of equation 1, we note that
the difficulty in decoding this objective comes en-
tirely from the constraints z((s,m), h) = Ys(m,h).
If these were not there, the problem would factor
into two parts, an optimization of F over the test
corpus Y(X) and an optimization of g over possible
MRF assignments Z . The first problem factors nat-
urally into sentence-level parsing problems and the
second can be solved efficiently given our assump-
tions on the MRF topology G.
Recent work has shown that a relaxation based
on dual decomposition often produces an exact so-
lution for such problems (Koo et al 2010). To
apply dual decomposition, we introduce Lagrange
multipliers u(s,m, h) for the agreement constraints
between the sentence-level models and the global
MRF. The Lagrangian dual is the function L(u) =
maxz g(z, u) + maxy F (y, u) where
g(z, u) = g(z) +
?
(s,m,h)?J (X)
u(s,m, h)z((s,m), h)),
F (y, u) = F (Y ) ?
?
(s,m,h)?J (X)
u(s,m, h)Ys(m,h)
In order to find minu L(u), we use subgradient de-
scent. This requires computing g(z, u) and F (y, u)
for fixed values of u, which by our assumptions from
Section 3 are efficient to calculate.
The full algorithm is given in Figure 2. We start
with the values of u initialized to 0. At each itera-
tion k, we find the best set of parses Y (k) over the
entire corpus and the best MRF assignment z(k). We
then update the value of u based on the difference
between Y (k) and z(k) and a rate parameter ?. On
the next iteration, we solve the same decoding prob-
? 0.7 ?0.8 ? 0.9 1.0
All Contexts 66.8 57.9 46.8 33.3
Head in Context 76.0 67.9 57.2 42.3
Table 1: Exploratory statistics for constraint selection.
The table shows the percentage of context types for which
the probability of the most frequent head tag is at least p.
Head in Context refers to the subset of contexts where the
most frequent head is within the context itself. Numbers
are based on Section 22 of the Wall Street Journal and are
given for contexts that appear at least 10 times.
lems modified by the new value of u. If at any point
the current solutions Y (k) and z(k) satisfy the con-
sistency constraint, we return their current values.
Otherwise, we stop at a max iteration K and return
the values from the last iteration.
We now give a theorem for the formal guarantees
of this algorithm.
Theorem 1 If for some k ? {1 . . .K} in the algo-
rithm in Figure 2, Y (k)s (m,h) = z(k)(s,m, h) for
all (s,m, h) ? J , then (Y (k), z(k)) is a solution to
the maximization problem in equation 1.
We omit the proof for brevity. It is a slight variation
of the proof given by Rush et al(2010).
7 Consistency Constraints
In this section we describe the consistency con-
straints used for the global models of parsing and
tagging.
Parsing Constraints. Recall from Section 4 that
we choose parsing constraints based on the word
context. We encourage words in similar contexts to
choose head words with similar POS tags.
We use a simple procedure to select which con-
straints to add. First define a context template to
be a set of offsets {r, . . . , s} with r ? 0 ? s that
specify the neighboring words to include in a con-
text. In the example of Figure 1, the context tem-
plate {?1, 0, 1, 2} applied to the word girls/NNS
would produce the context JJ NNS VBD RB. For
each word in the corpus, we consider all possible
templates with s? r < 4. We use only contexts that
predict the head POS of the context in the training
data with probability 1 and prefer long over short
contexts. Once we select the context of each word,
we add a consensus node for each context type in
1439
the corpus. We connect each word node to its corre-
sponding consensus node.
Local context does not fully determine the POS
tag of the head word, but for certain contexts it pro-
vides a strong signal. Table 1 shows context statis-
tics for English. For 46.8% of the contexts, the most
frequent head tag is chosen ? 90% of the time. The
pattern is even stronger for contexts where the most
frequent head tag is within the context itself. In
this case, for 57.2% of the contexts the most fre-
quent head tag is chosen ? 90% of the time. Con-
sequently, if more than one context can be selected
for a word, we favor the contexts where the most
frequent head POS is inside the context.
POS Tagging Constraints. For POS tagging, our
constraints focus on words not observed in the train-
ing data. It is well-known that each word type ap-
pears only with a small number of POS tags. In Sec-
tion 22 of the WSJ corpus, 96.35% of word types
appear with a single POS tag.
In most test sets we are unlikely to see an un-
known word more than once or twice. To fix this
sparsity issue, we import additional unannotated
sentences for each unknown word from the New
York Times Section of the NANC corpus (Graff,
1995). These sentences give additional information
for unknown word types.
Additionally, we note that morphologically re-
lated words often have similar POS tags. We can
exploit this relationship by connecting related word
types to the same consensus node. We experimented
with various morphological variants and found that
connecting a word type with the type generated by
appending the suffix ?s? was most beneficial. For
each unknown word type, we also import sentences
for its morphologically related words.
8 Experiments and Results
We experiment in two common scenarios where
parsing performance is reduced from the fully su-
pervised, in-domain case. In domain adaptation, we
train our model completely in one source domain
and test it on a different target domain. In lightly su-
pervised training, we simulate the case where only
a limited amount of annotated data is available for a
language.
Base ST Model ER
WSJ? QTB 89.63 89.99 90.43 7.7
QTB?WSJ 74.89 74.97 75.76 3.5
Table 2: Dependency parsing UAS for domain adapta-
tion. WSJ is the Penn TreeBank. QTB is the Question-
Bank. ER is error reduction. Results are significant using
the sign test with p ? 0.05.
Data for Domain Adaptation We perform do-
main adaptation experiments in English using the
WSJ PennTreebank (Marcus et al 1993) and the
QuestionBank (QTB) (Judge et al 2006). In the
WSJ ? QTB scenario, we train on sections 2-21
of the WSJ and test on the entire QTB (4000 ques-
tions). In the QTB?WSJ scenario, we train on the
entire QTB and test on section 23 of the WSJ.
Data for Lightly Supervised Training For all
English experiments, our data was taken from the
WSJ PennTreebank: training sentences from Sec-
tion 0, development sentences from Section 22, and
test sentences from Section 23. For experiments
in Bulgarian, German, Japanese, and Spanish, we
use the CONLL-X data set (Buchholz and Marsi,
2006) with training data taken from the official train-
ing files. We trained the sentence-level models with
50-500 sentences. To verify the robustness of our
results, our test sets consist of the official test sets
augmented with additional sentences from the offi-
cial training files such that each test file consists of
25,000 words. Our results on the official test sets are
very similar to the results we report and are omitted
for brevity.
Parameters The model parameters, ?1, ?2, and ?3
of the scoring function (Section 4) and ? of the
Lagrange multipliers update rule (Section 6), were
tuned on the English development data. In our dual
decomposition inference algorithm, we use K =
200 maximum iterations and tune the decay rate fol-
lowing the protocol described by Koo et al(2010).
Sentence-Level Models For dependency parsing
we utilize the second-order projective MST parser
(McDonald et al 2005)1 with the gold-standard
POS tags of the corpus. For POS tagging we use
the Stanford POS tagger (Toutanova et al 2003)2.
1http://sourceforge.net/projects/mstparser/
2http://nlp.stanford.edu/software/tagger.shtml
1440
50 100 200 500
Base ST Model (ER) Base ST Model (ER) Base ST Model (ER) Base ST Model (ER)
Jap 79.10 80.19 81.78 (12.82) 81.53 81.59 83.09 (8.45) 84.84 85.05 85.50 (4.35) 87.14 87.24 87.44 (2.33)
Eng 69.60 69.73 71.62 (6.64) 73.97 74.01 75.27 (4.99) 77.67 77.68 78.69 (4.57) 81.83 81.90 82.18 (1.93)
Spa 71.67 71.72 73.19 (5.37) 74.53 74.63 75.41 (3.46) 77.11 77.09 77.44 (1.44) 79.97 79.88 80.04 (0.35)
Bul 71.10 70.59 72.13 (3.56) 73.35 72.96 74.61 (4.73) 75.38 75.54 76.17 (3.21) 81.95 81.75 82.18 (1.27)
Ger 68.21 68.28 68.83 (1.95) 72.19 72.29 72.76 (2.05) 74.34 74.45 74.95 (2.4) 77.20 77.09 77.51 (1.4)
Table 3: Dependency parsing UAS by size of training set and language. English data is from the WSJ. Bulgarian,
German, Japanese, and Spanish data is from the CONLL-X data sets. Base is the second-order, projective dependency
parser of McDonald et al(2005). ST is a self-training model based on Reichart and Rappoport (2007). Model is the
same parser augmented with inter-sentence constraints. ER is error reduction. Using the sign test with p ? 0.05, all
50, 100, and 200 results are significant, as are Eng and Ger 500.
50 100 200 500
Base Model (ER) Base Model (ER) Base Model (ER) Base Model (ER)
Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64)
Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33)
Table 4: POS tagging accuracy. Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova et
al. (2003). Our inter-sentence POS tagger augments this baseline with global constraints. ER is error reduction. All
results are significant using the sign test with p ? 0.05.
Evaluation and Baselines To measure parsing
performance, we use unlabeled attachment score
(UAS) given by the CONLL-X dependency parsing
shared task evaluation script (Buchholz and Marsi,
2006). We compare the accuracy of dependency
parsing with global constraints to the sentence-level
dependency parser of McDonald et al(2005) and to
a self-training baseline (Steedman et al 2003; Re-
ichart and Rappoport, 2007). The parsing baseline is
equivalent to a single round of dual decomposition.
For the self-training baseline, we parse the test cor-
pus, append the labeled test sentences to the training
corpus, train a new parser, and then re-parse the test
set. We run this procedure for a single iteration.
For POS tagging we measure token level POS ac-
curacy for all the words in the corpus and also for
unknown words (words not observed in the train-
ing data). We compare the accuracy of POS tagging
with global constraints to the accuracy of the Stan-
ford POS tagger 3.
Domain Adaptation Accuracy Results are pre-
sented in Table 2. The constrained model reduces
the error of the baseline on both cases. Note that
when the base parser is trained on the WSJ corpus its
UAS performance on the QTB is 89.63%. Yet, the
constrained model is still able to reduce the baseline
error by 7.7%.
3We do not run self-training for POS tagging as it has been
shown unuseful for this application (Clark et al 2003).
Lightly Supervised Accuracy The parsing results
are given in Table 3. Our model improves over
the baseline parser and self-training across all lan-
guages and training set sizes. The best results are
for Japanese and English with error reductions of
2.33 ? 12.82% and 1.93 ? 6.64% respectively. The
self-training baseline achieves small gains on some
languages, but generally performs similarly to the
standard parser.
The POS tagging results are given in Table 4. Our
model improves over the baseline tagger for the en-
tire training size range. For 50 training sentences
we reduce 10.33% of the overall error, and 11.53%
of the error on unknown words. Although the tagger
performance substantially improves when the train-
ing set grows to 500 sentences, our model still pro-
vides an overall error reduction of 4.64% and of
8.33% for unknown words.
9 Discussion
Efficiency Since dual decomposition often re-
quires hundreds of iterations to converge, a naive im-
plementation would be orders of magnitude slower
than the underlying sentence-level model. We use
two techniques to speed-up the algorithm.
First, we follow Koo et al(2010) and use lazy
decoding as part of dual decomposition. At each it-
eration k, we cache the result of the MRF z(k) and
set of parse tree Y (k). In the next iteration, we only
1441
50 100 150 200iteration0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
percen
tage of
 parsin
g time
englishgermanjapanese
Figure 3: Efficiency of dependency parsing decoding for
three languages. The plot shows the speed of each iter-
ation of the subgradient algorithm relative to a round of
unconstrained parsing.
Most Effective Contexts
WSJ? QTB QTB?WSJ
WRB VBP VBD NN NN ,
DT JJS NN IN IN PRP VBZ
VBP PRP VB JJ JJ NN ,
DT NN NN VB IN JJ JJ NN
RBS JJ NN IN NN POS NN NN
Table 5: The five most effective constraint contexts from
the domain adaptation experiments. The bold POS tag
indicates the modifier word of the context.
Where/
WRB
VBN
are/
VBP
diamonds/
NNS
mined/
VBN
?
How/
WRB
VBP
do/
VBP
you/
PRP
measure/
VB
earthquakes/
NNS
?
Why/
WRB
VBP
do/
VBP
people/
NNS
get/
VB
calluses/
NNS
?
VBP
Figure 4: Subset of sentences with the context WRB VBP
from WSJ? QTB domain adaptation. In the first round,
the parser chooses VBN for the first sentence, which is in-
consistent with similar contexts. The constraints correct
this choice in later rounds.
recompute the solution Y ?s for a sentence s if the
weight u(s,m, h) for some m,h was updated. A
similar technique is applied to the MRF.
Second, during the first iteration of the algorithm
we apply max-marginal based pruning using the
threshold defined by Weiss and Taskar (2010). This
produces a pruned hypergraph for each sentence,
which allows us to avoid recomputing parse features
and to solve a simplified search problem.
To measure efficiency, we compare the time spent
in dual decomposition to the speed of unconstrained
inference. Across experiments, the mean dual de-
composition time is 1.71 times the cost of uncon-
strained inference. Figure 3 shows how this time is
spent after the first iteration. The early iterations are
around 1% of the total cost, and because of lazy de-
coding this quickly drops to almost nothing.
Exactness To measure exactness, we count the
number of sentences for which we should remove
the constraints in order for the model to reach con-
vergence. For dependency parsing, across languages
removing constraints on 0.6% of sentences yields
exact convergence. Removing these constraints has
very little effect on the final outcome of the model.
For POS tagging, the algorithm finds an exact so-
lution after removing constraints from 0.2% of the
sentences.
Constraint Analysis We can also look at the num-
ber, size, and outcome of the constraints chosen in
the experiments. In the lightly supervised experi-
ments, the average number of constraints is 3298 for
25000 tokens, where the median constraint connects
19 different tokens. Of these constraints around 70%
are active (non-NULL). The domain adaptation ex-
periments have a similar number of constraints with
around 75% of constraints active. In both experi-
ments many of the constraints are found to be con-
sistent after the first iteration, but as Figure 3 im-
plies, other constraints take multiple iterations to
converge.
Qualitative Analysis In order to understand why
these simple consistency constraints are effective,
we take a qualitative look at the the domain adap-
tation experiments on the QuestionBank. Table 5
ranks the five most effective contextual constraints
from both experiments. For the WSJ? QTB exper-
iment, the most effective constraint relates the inital
question word with an adjacent verb. Figure 4 shows
1442
sentences where this constraint applies in the Ques-
tionBank. For the QTB?WSJ experiment, the ef-
fective contexts are mostly long base noun phrases.
These occur often in the WSJ but are rare in the sim-
pler QuestionBank sentences.
10 Conclusion
In this work we experiment with inter-sentence
consistency constraints for dependency parsing and
POS tagging. We have proposed a corpus-level ob-
jective that augments sentence-level models with
such constraints and described an exact and effi-
cient dual decomposition algorithm for its decod-
ing. In future work, we intend to explore efficient
techniques for joint parameter learning for both the
global MRF and the local models.
Acknowledgments Columbia University gratefully
acknowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not
necessarily reflect the view of DARPA, AFRL, or the
US government. Alexander Rush was supported by a
National Science Foundation Graduate Research Fellow-
ship.
References
Y. Altun and D. Mcallester. 2005. Maximum margin
semi-supervised learning for structured variables. In
NIPS.
M. Belkin, P. Niyogi, and V. Sindhwani. 2005. On man-
ifold regularization. In AISTATS.
John Blitzer and Hal Daume. 2010. Icml 2010 tutorial
on domain adaptation. In ICML.
U. Brefeld and T. Scheffer. 2006. Semi-supervised learn-
ing for structured output variables. In ICML.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
R.C. Bunescu and R.J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL.
J.C.K Cheung and G. Penn. 2010. Utilizing extra-
sentential context for parsing. In EMNLP.
Stephen Clark, James Curran, and Miles Osborne. 2003.
Bootstrapping pos taggers using unlabelled data. In
CoNLL.
A. Dubey, F. Keller, and P. Sturt. 2009. A proba-
bilistic corpus-based model of parallelism. Cognition,
109(2):193?210.
Jenny Rose Finkel and Christopher Manning. 2009. Hi-
erarchical bayesian domain adaptation. In NAACL.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior Regularization for Structured Latent
Variable Models. Journal of Machine Learning Re-
search, 11:2001?2049.
J. Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, and
B. Taskar. 2010. Sparsity in dependency grammar in-
duction. In Proceedings of the ACL Conference Short
Papers.
A. Globerson and T. Jaakkola. 2007. Fixing max-
product: Convergent message passing algorithms for
map lp-relaxations. In NIPS.
D. Graff. 1995. North american news text corpus. Lin-
guistic Data Consortium, LDC95T21.
Rahul Gupta, Sunita Sarawagi, and Ajit A. Diwan. 2010.
Collective inference for extraction mrfs coupled with
symmetric clique potentials. JMLR.
R. Hwa. 2004. Sample selection for statistical parsing.
Computational Linguistics, 30(3):253?276.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In ACL-COLING.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In ICCV.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In IJCNLP.
P. Liang, M. I. Jordan, and D. Klein. 2009. Learning
from measurements in exponential families. In ICML.
G.S. Mann and A. McCallum. Generalized expectation
criteria for semi-supervised learning with weakly la-
beled data. Journal of Machine Learning Research,
11:955?984.
G.S. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation reg-
ularization. In ICML.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguistics,
19(2):313?330.
1443
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In ACL, sort papers.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In ACL.
David McClosky, Eugene Charniak, and Mark Johnson.
2010. Automatic domain adapatation for parsing. In
NAACL.
R.T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In HLT/EMNLP.
Barbara Plank. 2011. Domain Adaptation for Parsing.
Ph.d. thesis, University of Groningen.
R. Reichart and A. Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In ACL.
Laura Rimell and Stephen Clark. 2008. Adapting a
lexicalized-grammar parser to contrasting domains. In
EMNLP.
D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random fields. In ICML.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
EMNLP.
Kenji Sagae and Junichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with lr models and parser
ensembles. In EMNLP-CoNLL.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In EMNLP.
M. Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,
J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.
2003. Bootstrapping statistical parsers from small
datasets. In EACL.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In EMNLP.
C. Sutton and A. Mccallum. 2004. Collective segmen-
tation and labeling of distant entities in information
extraction. In In ICML Workshop on Statistical Re-
lational Learning and Its Connections.
B. Taskar, P. Abbeel, and d. Koller. 2002. Discriminative
probabilistic models for relational data. In UAI.
K. Toutanova, D. Klein, C.D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697?3717.
Y. Wang, G. Haffari, S. Wang, and G. Mori. 2009.
A rate distortion approach for semi-supervised condi-
tional random fields. In NIPS.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS, volume 1284.
1444
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 210?221,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Optimal Beam Search for Machine Translation
Alexander M. Rush Yin-Wen Chang
MIT CSAIL,
Cambridge, MA 02139, USA
{srush, yinwen}@csail.mit.edu
Michael Collins
Department of Computer Science,
Columbia University,
New York, NY 10027, USA
mcollins@cs.columbia.edu
Abstract
Beam search is a fast and empirically effective
method for translation decoding, but it lacks
formal guarantees about search error. We de-
velop a new decoding algorithm that combines
the speed of beam search with the optimal cer-
tificate property of Lagrangian relaxation, and
apply it to phrase- and syntax-based transla-
tion decoding. The new method is efficient,
utilizes standard MT algorithms, and returns
an exact solution on the majority of transla-
tion examples in our test data. The algorithm
is 3.5 times faster than an optimized incremen-
tal constraint-based decoder for phrase-based
translation and 4 times faster for syntax-based
translation.
1 Introduction
Beam search (Koehn et al, 2003) and cube prun-
ing (Chiang, 2007) have become the de facto decod-
ing algorithms for phrase- and syntax-based trans-
lation. The algorithms are central to large-scale
machine translation systems due to their efficiency
and tendency to produce high-quality translations
(Koehn, 2004; Koehn et al, 2007; Dyer et al, 2010).
However despite practical effectiveness, neither al-
gorithm provides any bound on possible decoding
error.
In this work we present a variant of beam search
decoding for phrase- and syntax-based translation.
The motivation is to exploit the effectiveness and ef-
ficiency of beam search, but still maintain formal
guarantees. The algorithm has the following bene-
fits:
? In theory, it can provide a certificate of optimal-
ity; in practice, we show that it produces opti-
mal hypotheses, with certificates of optimality,
on the vast majority of examples.
? It utilizes well-studied algorithms and extends
off-the-shelf beam search decoders.
? Empirically it is very fast, results show that it is
3.5 times faster than an optimized incremental
constraint-based solver.
While our focus is on fast decoding for machine
translation, the algorithm we present can be applied
to a variety of dynamic programming-based decod-
ing problems. The method only relies on having a
constrained beam search algorithm and a fast uncon-
strained search algorithm. Similar algorithms exist
for many NLP tasks.
We begin in Section 2 by describing constrained
hypergraph search and showing how it generalizes
translation decoding. Section 3 introduces a variant
of beam search that is, in theory, able to produce
a certificate of optimality. Section 4 shows how to
improve the effectiveness of beam search by using
weights derived from Lagrangian relaxation. Sec-
tion 5 puts everything together to derive a fast beam
search algorithm that is often optimal in practice.
Experiments compare the new algorithm with
several variants of beam search, cube pruning, A?
search, and relaxation-based decoders on two trans-
lation tasks. The optimal beam search algorithm is
able to find exact solutions with certificates of opti-
mality on 99% of translation examples, significantly
more than other baselines. Additionally the optimal
210
beam search algorithm is much faster than other ex-
act methods.
2 Background
The focus of this work is decoding for statistical ma-
chine translation. Given a source sentence, the goal
is to find the target sentence that maximizes a com-
bination of translation model and language model
scores. In order to analyze this decoding problem,
we first abstract away from the specifics of transla-
tion into a general form, known as a hypergraph. In
this section, we describe the hypergraph formalism
and its relation to machine translation.
2.1 Notation
Throughout the paper, scalars and vectors are writ-
ten in lowercase, matrices are written in uppercase,
and sets are written in script-case, e.g. X . All vec-
tors are assumed to be column vectors. The function
?(j) yields an indicator vector with ?(j)j = 1 and
?(j)i = 0 for all i 6= j.
2.2 Hypergraphs and Search
A directed hypergraph is a pair (V, E) where V =
{1 . . . |V|} is a set of vertices, and E is a set of di-
rected hyperedges. Each hyperedge e ? E is a tuple?
?v2, . . . , v|v|?, v1
?
where vi ? V for i ? {1 . . . |v|}.
The head of the hyperedge is h(e) = v1. The tail
of the hyperedge is the ordered sequence t(e) =
?v2, . . . , v|v|?. The size of the tail |t(e)| may vary
across different hyperedges, but |t(e)| ? 1 for all
edges and is bounded by a constant. A directed
graph is a directed hypergraph with |t(e)| = 1 for
all edges e ? E .
Each vertex v ? V is either a non-terminal or a
terminal in the hypergraph. The set of non-terminals
is N = {v ? V : h(e) = v for some e ? E}. Con-
versely, the set of terminals is defined as T = V\N .
All directed hypergraphs used in this work are
acyclic: informally this implies that no hyperpath (as
defined below) contains the same vertex more than
once (see Martin et al (1990) for a full definition).
Acyclicity implies a partial topological ordering of
the vertices. We also assume there is a distinguished
root vertex 1 where for all e ? E , 1 6? t(e).
Next we define a hyperpath as x ? {0, 1}|E| where
x(e) = 1 if hyperedge e is used in the hyperpath,
procedure BESTPATHSCORE(?, ? )
pi[v]? 0 for all v ? T
for e ? E in topological order do
??v2, . . . , v|v|?, v1? ? e
s? ?(e) +
|v|?
i=2
pi[vi]
if s > pi[v1] then pi[v1]? s
return pi[1] + ?
Figure 1: Dynamic programming algorithm for uncon-
strained hypergraph search. Note that this version only
returns the highest score: maxx?X ?>x+ ? . The optimal
hyperpath can be found by including back-pointers.
x(e) = 0 otherwise. The set of valid hyperpaths is
defined as
X =
?
?????
?????
x :
?
e?E:h(e)=1
x(e) = 1,
?
e:h(e)=v
x(e) =
?
e:v?t(e)
x(e) ? v ? N \ {1}
?
?????
?????
The first problem we consider is unconstrained hy-
pergraph search. Let ? ? R|E| be the weight vector
for the hypergraph and let ? ? R be a weight offset.1
The unconstrained search problem is to find
max
x?X
?
e?E
?(e)x(e) + ? = max
x?X
?>x+ ?
This maximization can be computed for any
weights and directed acyclic hypergraph in time
O(|E|) using dynamic programming. Figure 1
shows this algorithm which is simply a version of
the CKY algorithm.
Next consider a variant of this problem: con-
strained hypergraph search. Constraints will be nec-
essary for both phrase- and syntax-based decoding.
In phrase-based models, the constraints will ensure
that each source word is translated exactly once. In
syntax-based models, the constraints will be used to
intersect a translation forest with a language model.
In the constrained hypergraph problem, hyper-
paths must fulfill additional linear hyperedge con-
straints. Define the set of constrained hyperpaths as
X ? = {x ? X : Ax = b}
1The purpose of the offset will be clear in later sections. For
this section, the value of ? can be taken as 0.
211
where we have a constraint matrix A ? R|b|?|E|
and vector b ? R|b| encoding |b| constraints.
The optimal constrained hyperpath is x? =
arg maxx?X ? ?>x+ ? .
Note that the constrained hypergraph search prob-
lem may be NP-Hard. Crucially this is true even
when the corresponding unconstrained search prob-
lem is solvable in polynomial time. For instance,
phrase-based decoding is known to be NP-Hard
(Knight, 1999), but we will see that it can be ex-
pressed as a polynomial-sized hypergraph with con-
straints.
Example: Phrase-Based Machine Translation
Consider translating a source sentencew1 . . . w|w| to
a target sentence in a language with vocabulary ?. A
simple phrase-based translation model consists of a
tuple (P, ?, ?) with
? P; a set of pairs (q, r) where q1 . . . q|q| is a se-
quence of source-language words and r1 . . . r|r|
is a sequence of target-language words drawn
from the target vocabulary ?.
? ? : R|P|; parameters for the translation model
mapping each pair in P to a real-valued score.
? ? : R|???|; parameters of the language model
mapping a bigram of target-language words to
a real-valued score.
The translation decoding problem is to find the
best derivation for a given source sentence. A
derivation consists of a sequence of phrases p =
p1 . . . pn. Define a phrase as a tuple (q, r, j, k)
consisting of a span in the source sentence q =
wj . . . wk and a sequence of target words r1 . . . r|r|,
with (q, r) ? P . We say the source words wj . . . wk
are translated to r.
The score of a derivation, f(p), is the sum of the
translation score of each phrase plus the language
model score of the target sentence
f(p) =
n?
i=1
?(q(pi), r(pi)) +
|u|+1?
i=0
?(ui?1, ui)
where u is the sequence of words in ? formed
by concatenating the phrases r(p1) . . . r(pn), with
boundary cases u0 = <s> and u|u|+1 = </s>.
Crucially for a derivation to be valid it must sat-
isfy an additional condition: it must translate every
source word exactly once. The decoding problem
for phrase-based translation is to find the highest-
scoring derivation satisfying this property.
We can represent this decoding problem as a con-
strained hypergraph using the construction of Chang
and Collins (2011). The hypergraph weights en-
code the translation and language model scores, and
its structure ensures that the count of source words
translated is |w|, i.e. the length of the source sen-
tence. Each vertex will remember the preceding
target-language word and the count of source words
translated so far.
The hypergraph, which for this problem is also a
directed graph, takes the following form.
? Vertices v ? V are labeled (c, u) where c ?
{1 . . . |w|} is the count of source words trans-
lated and u ? ? is the last target-language word
produced by a partial hypothesis at this vertex.
Additionally there is an initial terminal vertex
labeled (0,<s>).
? There is a hyperedge e ? E with head (c?, u?)
and tail ?(c, u)? if there is a valid corresponding
phrase (q, r, j, k) such that c? = c + |q| and
u? = r|r|, i.e. c
? is the count of words translated
and u? is the last word of target phrase r. We
call this phrase p(e).
The weight of this hyperedge, ?(e), is the trans-
lation model score of the pair plus its language
model score
?(e) = ?(q, r)+
?
?
|r|?
i=2
?(ri?1, ri)
?
?+?(u, r1)
? To handle the end boundary, there are hyper-
edges with head 1 and tail ?(|w|, u)? for all
u ? ?. The weight of these edges is the cost of
the stop bigram following u, i.e. ?(u,</s>).
While any valid derivation corresponds to a hy-
perpath in this graph, a hyperpath may not corre-
spond to a valid derivation. For instance, a hyper-
path may translate some source words more than
once or not at all.
212
Figure 2: Hypergraph for translating the sentence w = les1 pauvres2 sont3 demunis4 with set of pairs P =
{(les, the), (pauvres, poor), (sont demunis, don?t have any money)}. Hyperedges are color-coded
by source words translated: orange for les1, green for pauvres2, and red for sont3 demunis4. The dotted lines
show an invalid hyperpath x that has signature Ax = ?0, 0, 2, 2? 6= ?1, 1, 1, 1? .
We handle this problem by adding additional con-
straints. For all source words i ? {1 . . . |w|}, define
? as the set of hyperedges that translate wi
?(i) = {e ? E : j(p(e)) ? i ? k(p(e))}
Next define |w| constraints enforcing that each word
in the source sentence is translated exactly once
?
e??(i)
x(e) = 1 ? i ? {1 . . . |w|}
These linear constraints can be represented with
a matrix A ? {0, 1}|w|?|E| where the rows corre-
spond to source indices and the columns correspond
to edges. We call the product Ax the signature,
where in this case (Ax)i is the number of times word
i has been translated. The full set of constrained hy-
perpaths is X ? = {x ? X : Ax = 1 }, and the best
derivation under this phrase-based translation model
has score maxx?X ? ?>x+ ? .
Figure 2.2 shows an example hypergraph
with constraints for translating the sentence les
pauvres sont demunis into English using
a simple set of phrases. Even in this small exam-
ple, many of the possible hyperpaths violate the
constraints and correspond to invalid derivations.
Example: Syntax-Based Machine Translation
Syntax-based machine translation with a language
model can also be expressed as a constrained hyper-
graph problem. For the sake of space, we omit the
definition. See Rush and Collins (2011) for an in-
depth description of the constraint matrix used for
syntax-based translation.
3 A Variant of Beam Search
This section describes a variant of the beam
search algorithm for finding the highest-scoring con-
strained hyperpath. The algorithm uses three main
techniques: (1) dynamic programming with ad-
ditional signature information to satisfy the con-
straints, (2) beam pruning where some, possibly op-
timal, hypotheses are discarded, and (3) branch-and-
bound-style application of upper and lower bounds
to discard provably non-optimal hypotheses.
Any solution returned by the algorithm will be a
valid constrained hyperpath and a member of X ?.
Additionally the algorithm returns a certificate flag
opt that, if true, indicates that no beam pruning
was used, implying the solution returned is opti-
mal. Generally it will be hard to produce a certificate
even by reducing the amount of beam pruning; how-
ever in the next section we will introduce a method
based on Lagrangian relaxation to tighten the upper
bounds. These bounds will help eliminate most so-
lutions before they trigger pruning.
3.1 Algorithm
Figure 3 shows the complete beam search algorithm.
At its core it is a dynamic programming algorithm
filling in the chart pi. The beam search chart indexes
hypotheses by vertex v ? V as well as a signature
sig ? R|b| where |b| is the number of constraints. A
new hypothesis is constructed from each hyperedge
and all possible signatures of tail nodes. We define
the function SIGS to take the tail of an edge and re-
213
turn the set of possible signature combinations
SIGS(v2, . . . v|v|) =
|v|?
i=2
{sig : pi[vi, sig] 6= ??}
where the product is the Cartesian product over sets.
Line 8 loops over this entire set.2 For hypothesis x,
the algorithm ensures that its signature sig is equal
to Ax. This property is updated on line 9.
The signature provides proof that a hypothesis is
still valid. Let the function CHECK(sig) return true
if the hypothesis can still fulfill the constraints. For
example, in phrase-based decoding, we will define
CHECK(sig) = (sig ? 1); this ensures that each
word has been translated 0 or 1 times. This check is
applied on line 11.
Unfortunately maintaining all signatures is inef-
ficient. For example we will see that in phrase-
based decoding the signature is a bit-string recording
which source words have been translated; the num-
ber of possible bit-strings is exponential in the length
of the sentence. The algorithm includes two meth-
ods for removing hypotheses, bounding and prun-
ing.
Bounding allows us to discard provably non-
optimal solutions. The algorithm takes as arguments
a lower bound on the optimal score lb ? ?>x? + ? ,
and computes upper bounds on the outside score
for all vertices v: ubs[v], i.e. an overestimate of
the score for completing the hyperpath from v. If
a hypothesis has score s, it can only be optimal if
s+ ubs[v] ? lb. This bound check is performed on
line 11.
Pruning removes weak partial solutions based on
problem-specific checks. The algorithm invokes the
black-box function, PRUNE, on line 13, passing it
a pruning parameter ? and a vertex-signature pair.
The parameter ? controls a threshold for pruning.
For instance for phrase-based translation, it specifies
a hard-limit on the number of hypotheses to retain.
The function returns true if it prunes from the chart.
Note that pruning may remove optimal hypotheses,
so we set the certificate flag opt to false if the chart
is modified.
2For simplicity we write this loop over the entire set. In
practice it is important to use data structures to optimize look-
up. See Tillmann (2006) and Huang and Chiang (2005).
1: procedure BEAMSEARCH(?, ?, lb, ?)
2: ubs? OUTSIDE(?, ?)
3: opt? true
4: pi[v, sig]? ?? for all v ? V, sig ? R|b|
5: pi[v, 0]? 0 for all v ? T
6: for e ? E in topological order do
7: ??v2, . . . , v|v|?, v1? ? e
8: for sig(2) . . . sig(|v|) ? SIGS(v2, . . . , v|v|) do
9: sig ? A?(e) +
|v|?
i=2
sig(i)
10: s? ?(e) +
|v|?
i=2
pi[vi, sig
(i)]
11: if
?
?
s > pi[v1, sig] ?
CHECK(sig) ?
s+ ubs[v1] ? lb
?
? then
12: pi[v1, sig]? s
13: if PRUNE(pi, v1, sig, ?) then opt? false
14: lb? ? pi[1, c] + ?
15: return lb?, opt
Input:
?
?
?
?
(V, E , ?, ?) hypergraph with weights
(A, b) matrix and vector for constraints
lb ? R lower bound
? a pruning parameter
Output:
[
lb? resulting lower bound score
opt certificate of optimality
Figure 3: A variant of the beam search algorithm. Uses
dynamic programming to produce a lower bound on the
optimal constrained solution and, possibly, a certificate of
optimality. Function OUTSIDE computes upper bounds
on outside scores. Function SIGS enumerates all possi-
ble tail signatures. Function CHECK identifies signatures
that do not violate constraints. Bounds lb and ubs are
used to remove provably non-optimal solutions. Func-
tion PRUNE, taking parameter ?, returns true if it prunes
hypotheses from pi that could be optimal.
This variant on beam search satisfies the follow-
ing two properties (recall x? is the optimal con-
strained solution)
Property 3.1 (Primal Feasibility). The returned
score lb? lower bounds the optimal constrained
score, that is lb? ? ?>x? + ? .
Property 3.2 (Dual Certificate). If beam search re-
turns with opt = true, then the returned score is
optimal, i.e. lb? = ?>x? + ? .
An immediate consequence of Property 3.1 is that
the output of beam search, lb?, can be used as the in-
put lb for future runs of the algorithm. Furthermore,
214
procedure PRUNE(pi, v, sig, ?)
C ? {(v?, sig?) : ||sig?||1 = ||sig||1,
pi[v?, sig?] 6= ??}
D ? C \mBEST(?, C, pi)
pi[v?, sig?]? ?? for all v?, sig? ? D
if D = ? then return true
else return false
Input:
[
(v, sig) the last hypothesis added to the chart
? ? Z # of hypotheses to retain
Output: true, if pi is modified
Figure 4: Pruning function for phrase-based translation.
Set C contains all hypotheses with ||sig||1 source words
translated. The function prunes all but the top-? scoring
hypotheses in this set.
if we loosen the amount of beam pruning by adjust-
ing the pruning parameter ? we can produce tighter
lower bounds and discard more hypotheses. We can
then iteratively apply this idea with a sequence of
parameters ?1 . . . ?K producing lower bounds lb(1)
through lb(K). We return to this idea in Section 5.
Example: Phrase-based Beam Search. Recall
that the constraints for phrase-based translation con-
sist of a binary matrix A ? {0, 1}|w|?|E| and vec-
tor b = 1. The value sigi is therefore the num-
ber of times source word i has been translated in
the hypothesis. We define the predicate CHECK as
CHECK(sig) = (sig ? 1) in order to remove hy-
potheses that already translate a source word more
than once, and are therefore invalid. For this reason,
phrase-based signatures are called bit-strings.
A common beam pruning strategy is to group
together items into a set C and retain a (possibly
complete) subset. An example phrase-based beam
pruner is given in Figure 4. It groups together
hypotheses based on ||sigi||1, i.e. the number of
source words translated, and applies a hard pruning
filter that retains only the ? highest-scoring items
(v, sig) ? C based on pi[v, sig].
3.2 Computing Upper Bounds
Define the setO(v, x) to contain all outside edges of
vertex v in hyperpath x (informally, hyperedges that
do not have v as an ancestor). For all v ? V , we set
the upper bounds, ubs, to be the best unconstrained
outside score
ubs[v] = max
x?X :v?x
?
e?O(v,x)
?(e) + ?
This upper bound can be efficiently computed for
all vertices using the standard outside dynamic pro-
gramming algorithm. We will refer to this algorithm
as OUTSIDE(?, ? ).
Unfortunately, as we will see, these upper bounds
are often quite loose. The issue is that unconstrained
outside paths are able to violate the constraints with-
out being penalized, and therefore greatly overesti-
mate the score.
4 Finding Tighter Bounds with
Lagrangian Relaxation
Beam search produces a certificate only if beam
pruning is never used. In the case of phrase-based
translation, the certificate is dependent on all groups
C having ? or less hypotheses. The only way to en-
sure this is to bound out enough hypotheses to avoid
pruning. The effectiveness of the bounding inequal-
ity, s + ubs[v] < lb, in removing hypotheses is di-
rectly dependent on the tightness of the bounds.
In this section we propose using Lagrangian re-
laxation to improve these bounds. We first give a
brief overview of the method and then apply it to
computing bounds. Our experiments show that this
approach is very effective at finding certificates.
4.1 Algorithm
In Lagrangian relaxation, instead of solving the con-
strained search problem, we relax the constraints
and solve an unconstrained hypergraph problem
with modified weights. Recall the constrained hy-
pergraph problem: max
x?X :Ax=b
?>x + ? . The La-
grangian dual of this optimization problem is
L(?) = max
x?X
?>x+ ? ? ?>(Ax? b)
=
(
max
x?X
(? ?A>?)>x
)
+ ? + ?>b
= max
x?X
??>x+ ? ?
where ? ? R|b| is a vector of dual variables and
define ?? = ? ? A>? and ? ? = ? + ?>b. This
maximization is over X , so for any value of ?, L(?)
can be calculated as BestPathScore(??, ? ?).
Note that for all valid constrained hyperpaths x ?
X ? the termAx?b equals 0, which implies that these
hyperpaths have the same score under the modified
weights as under the original weights, ?>x + ? =
??>x+? ?. This leads to the following two properties,
215
procedure LRROUND(?k, ?)
x? arg max
x?X
?>x+ ? ? ?>(Ax? b)
?? ? ?? ?k(Ax? b)
opt? Ax = b
ub? ?>x+ ?
return ??,ub, opt
procedure LAGRANGIANRELAXATION(?)
?(0) ? 0
for k in 1 . . .K do
?(k),ub, opt? LRROUND(?k, ?(k?1))
if opt then return ?(k),ub, opt
return ?(K),ub, opt
Input: ?1 . . . ?K sequence of subgradient rates
Output:
?
?
? final dual vector
ub upper bound on optimal constrained solution
opt certificate of optimality
Figure 5: Lagrangian relaxation algorithm. The algo-
rithm repeatedly calls LRROUND to compute the subgra-
dient, update the dual vector, and check for a certificate.
where x ? X is the hyperpath computed within the
max,
Property 4.1 (Dual Feasibility). The valueL(?) up-
per bounds the optimal solution, that is L(?) ?
?>x? + ?
Property 4.2 (Primal Certificate). If the hyperpath
x is a member of X ?, i.e. Ax = b, then L(?) =
?>x? + ? .
Property 4.1 states that L(?) always produces
some upper bound; however, to help beam search,
we want as tight a bound as possible: min? L(?).
The Lagrangian relaxation algorithm, shown in
Figure 5, uses subgradient descent to find this min-
imum. The subgradient of L(?) is Ax ? b where
x is the argmax of the modified objective x =
arg maxx?X ??>x + ? ?. Subgradient descent itera-
tively solves unconstrained hypergraph search prob-
lems to compute these subgradients and updates ?.
See Rush and Collins (2012) for an extensive discus-
sion of this style of optimization in natural language
processing.
Example: Phrase-based Relaxation. For phrase-
based translation, we expand out the Lagrangian to
L(?) = max
x?X
?>x+ ? ? ?>(Ax? b) =
max
x?X
?
e?E
?
??(e)?
k(p(e))?
i=j(p(e))
?i
?
?x(e) + ? +
|s|?
i=1
?i
The weight of each edge ?(e) is modified by the
dual variables ?i for each source word translated by
the edge, i.e. if (q, r, j, k) = p(e), then the score
is modified by
?k
i=j ?i. A solution under these
weights may use source words multiple times or not
at all. However if the solution uses each source word
exactly once (Ax = 1), then we have a certificate
and the solution is optimal.
4.2 Utilizing Upper Bounds in Beam Search
For many problems, it may not be possible to satisfy
Property 4.2 by running the subgradient algorithm
alone. Yet even for these problems, applying sub-
gradient descent will produce an improved estimate
of the upper bound, min? L(?).
To utilize these improved bounds, we simply re-
place the weights in beam search and the outside al-
gorithm with the modified weights from Lagrangian
relaxation, ?? and ? ?. Since the result of beam search
must be a valid constrained hyperpath x ? X ?, and
for all x ? X ?, ?>x + ? = ??>x + ? ?, this sub-
stitution does not alter the necessary properties of
the algorithm; i.e. if the algorithm returns with opt
equal to true, then the solution is optimal.
Additionally the computation of upper bounds
now becomes
ubs[v] = max
x?X :v?x
?
e?O(v,x)
??(e) + ? ?
These outside paths may still violate constraints, but
the modified weights now include penalty terms to
discourage common violations.
5 Optimal Beam Search
The optimality of the beam search algorithm is de-
pendent on the tightness of the upper and lower
bounds. We can produce better lower bounds by
varying the pruning parameter ?; we can produce
better upper bounds by running Lagrangian relax-
ation. In this section we combine these two ideas
and present a complete optimal beam search algo-
rithm.
Our general strategy will be to use Lagrangian
relaxation to compute modified weights and to use
beam search over these modified weights to attempt
to find an optimal solution. One simple method for
doing this, shown at the top of Figure 6, is to run
216
in stages. The algorithm first runs Lagrangian relax-
ation to compute the best ? vector. The algorithm
then iteratively runs beam search using the parame-
ter sequence ?k. These parameters allow the algo-
rithm to loosen the amount of beam pruning. For
example in phrase based pruning, we would raise
the number of hypotheses stored per group until no
beam pruning occurs.
A clear disadvantage of the staged approach is
that it needs to wait until Lagrangian relaxation is
completed before even running beam search. Of-
ten beam search will be able to quickly find an opti-
mal solution even with good but non-optimal ?. In
other cases, beam search may still improve the lower
bound lb.
This motivates the alternating algorithm OPT-
BEAM shown Figure 6. In each round, the algo-
rithm alternates between computing subgradients to
tighten ubs and running beam search to maximize
lb. In early rounds we set ? for aggressive beam
pruning, and as the upper bounds get tighter, we
loosen pruning to try to get a certificate. If at any
point either a primal or dual certificate is found, the
algorithm returns the optimal solution.
6 Related Work
Approximate methods based on beam search and
cube-pruning have been widely studied for phrase-
based (Koehn et al, 2003; Tillmann and Ney, 2003;
Tillmann, 2006) and syntax-based translation mod-
els (Chiang, 2007; Huang and Chiang, 2007; Watan-
abe et al, 2006; Huang and Mi, 2010).
There is a line of work proposing exact algorithms
for machine translation decoding. Exact decoders
are often slow in practice, but help quantify the er-
rors made by other methods. Exact algorithms pro-
posed for IBM model 4 include ILP (Germann et al,
2001), cutting plane (Riedel and Clarke, 2009), and
multi-pass A* search (Och et al, 2001). Zaslavskiy
et al (2009) formulate phrase-based decoding as a
traveling salesman problem (TSP) and use a TSP
decoder. Exact decoding algorithms based on finite
state transducers (FST) (Iglesias et al, 2009) have
been studied on phrase-based models with limited
reordering (Kumar and Byrne, 2005). Exact decod-
ing based on FST is also feasible for certain hier-
archical grammars (de Gispert et al, 2010). Chang
procedure OPTBEAMSTAGED(?, ?)
?,ub, opt?LAGRANGIANRELAXATION(?)
if opt then return ub
?? ? ? ?A>?
? ? ? ? + ?>b
lb(0) ? ??
for k in 1 . . .K do
lb(k), opt? BEAMSEARCH(??, ? ?, lb(k?1), ?k)
if opt then return lb(k)
return maxk?{1...K} lb
(k)
procedure OPTBEAM(?, ?)
?(0) ? 0
lb(0) ? ??
for k in 1 . . .K do
?(k),ub(k), opt? LRROUND(?k, ?(k?1))
if opt then return ub(k)
?? ? ? ?A>?(k)
? ? ? ? + ?(k)>b
lb(k), opt? BEAMSEARCH(??, ? ?, lb(k?1), ?k)
if opt then return lb(k)
return maxk?{1...K} lb
(k)
Input:
[
?1 . . . ?K sequence of subgradient rates
?1 . . . ?K sequence of pruning parameters
Output: optimal constrained score or lower bound
Figure 6: Two versions of optimal beam search: staged
and alternating. Staged runs Lagrangian relaxation to
find the optimal ?, uses ? to compute upper bounds, and
then repeatedly runs beam search with pruning sequence
?1 . . . ?k. Alternating switches between running a round
of Lagrangian relaxation and a round of beam search with
the updated ?. If either produces a certificate it returns the
result.
and Collins (2011) and Rush and Collins (2011) de-
velop Lagrangian relaxation-based approaches for
exact machine translation.
Apart from translation decoding, this paper is
closely related to work on column generation for
NLP. Riedel et al (2012) and Belanger et al (2012)
relate column generation to beam search and pro-
duce exact solutions for parsing and tagging prob-
lems. The latter work also gives conditions for when
beam search-style decoding is optimal.
7 Results
To evaluate the effectiveness of optimal beam search
for translation decoding, we implemented decoders
for phrase- and syntax-based models. In this sec-
tion we compare the speed and optimality of these
217
decoders to several baseline methods.
7.1 Setup and Implementation
For phrase-based translation we used a German-to-
English data set taken from Europarl (Koehn, 2005).
We tested on 1,824 sentences of length at most 50
words. For experiments the phrase-based systems
uses a trigram language model and includes standard
distortion penalties. Additionally the unconstrained
hypergraph includes further derivation information
similar to the graph described in Chang and Collins
(2011).
For syntax-based translation we used a Chinese-
to-English data set. The model and hypergraphs
come from the work of Huang and Mi (2010). We
tested on 691 sentences from the newswire portion
of the 2008 NIST MT evaluation test set. For ex-
periments, the syntax-based model uses a trigram
language model. The translation model is tree-to-
string syntax-based model with a standard context-
free translation forest. The constraint matrix A
is based on the constraints described by Rush and
Collins (2011).
Our decoders use a two-pass architecture. The
first pass sets up the hypergraph in memory, and the
second pass runs search. When possible the base-
lines share optimized construction and search code.
The performance of optimal beam search is de-
pendent on the sequences ? and ?. For the step-
size ? we used a variant of Polyak?s rule (Polyak,
1987; Boyd and Mutapcic, 2007), substituting the
unknown optimal score for the last computed lower
bound: ?k ? ub
(k)?lb(k)
||Ax(k)?b||22
. We adjust the order of
the pruning parameter ? based on a function ? of
the current gap: ?k ? 10?(ub
(k)?lb(k)).
Previous work on these data sets has shown that
exact algorithms do not result in a significant in-
crease in translation accuracy. We focus on the effi-
ciency and model score of the algorithms.
7.2 Baseline Methods
The experiments compare optimal beam search
(OPTBEAM) to several different decoding meth-
ods. For both systems we compare to: BEAM, the
beam search decoder from Figure 3 using the orig-
inal weights ? and ? , and ? ? {100, 1000}; LR-
TIGHT, Lagrangian relaxation followed by incre-
Figure 7: Two graphs from phrase-based decoding.
Graph (a) shows the duality gap distribution for 1,824
sentences after 0, 5, and 10 rounds of LR. Graph (b)
shows the % of certificates found for sentences with dif-
fering gap sizes and beam search parameters ?. Duality
gap is defined as, ub - (?>x? + ? ).
mental tightening constraints, which is a reimple-
mentation of Chang and Collins (2011) and Rush
and Collins (2011).
For phrase-based translation we compare with:
MOSES-GC, the standard Moses beam search de-
coder with ? ? {100, 1000} (Koehn et al, 2007);
MOSES, a version of Moses without gap constraints
more similar to BEAM (see Chang and Collins
(2011)); ASTAR, an implementation of A? search
using original outside scores, i.e. OUTSIDE(?, ?),
and capped at 20,000,000 queue pops.
For syntax-based translation we compare with:
ILP, a general-purpose integer linear program-
ming solver (Gurobi Optimization, 2013) and
CUBEPRUNING, an approximate decoding method
similar to beam search (Chiang, 2007), tested with
? ? {100, 1000}.
7.3 Experiments
Table 1 shows the main results. For phrase-based
translation, OPTBEAM decodes the optimal trans-
lation with certificate in 99% of sentences with an
average time of 17.27 seconds per sentence. This
218
11-20 (558) 21-30 (566) 31-40 (347) 41-50 (168) all (1824)
Phrase-Based time cert exact time cert exact time cert exact time cert exact time cert exact
BEAM (100) 2.33 19.5 38.0 8.37 1.6 7.2 24.12 0.3 1.4 71.35 0.0 0.0 14.50 15.3 23.2
BEAM (1000) 2.33 37.8 66.3 8.42 3.4 18.9 21.60 0.6 3.2 53.99 0.6 1.2 12.44 22.6 36.9
BEAM (100000) 3.34 83.9 96.2 18.53 22.4 60.4 46.65 2.0 18.1 83.53 1.2 6.5 23.39 43.2 62.4
MOSES (100) 0.18 0.0 81.0 0.36 0.0 45.6 0.53 0.0 14.1 0.74 0.0 6.0 0.34 0.0 52.3
MOSES (1000) 2.29 0.0 97.8 4.39 0.0 78.8 6.52 0.0 43.5 9.00 0.0 19.6 4.20 0.0 74.6
ASTAR (cap) 11.11 99.3 99.3 91.39 53.9 53.9 122.67 7.8 7.8 139.61 1.2 1.2 67.99 58.8 58.8
LR-TIGHT 4.20 100.0 100.0 23.25 100.0 100.0 88.16 99.7 99.7 377.9 97.0 97.0 60.11 99.7 99.7
OPTBEAM 2.85 100.0 100.0 10.33 100.0 100.0 28.29 100.0 100.0 84.34 97.0 97.0 17.27 99.7 99.7
ChangCollins 10.90 100.0 100.0 57.20 100.0 100.0 203.4 99.7 99.7 679.9 97.0 97.0 120.9 99.7 99.7
MOSES-GC (100) 0.14 0.0 89.4 0.27 0.0 84.1 0.41 0.0 75.8 0.58 0.0 78.6 0.26 0.0 84.9
MOSES-GC (1000) 1.33 0.0 89.4 2.62 0.0 84.3 4.15 0.0 75.8 6.19 0.0 79.2 2.61 0.0 85.0
11-20 (192) 21-30 (159) 31-40 (136) 41-100 (123) all (691)
Syntax-Based time cert exact time cert exact time cert exact time cert exact time cert exact
BEAM (100) 0.40 4.7 75.9 0.40 0.0 66.0 0.75 0.0 43.4 1.66 0.0 25.8 0.68 5.72 58.7
BEAM (1000) 0.78 16.9 79.4 2.65 0.6 67.1 6.20 0.0 47.5 15.5 0.0 36.4 4.16 12.5 65.5
CUBE (100) 0.08 0.0 77.6 0.16 0.0 66.7 0.23 0.0 43.9 0.41 0.0 26.3 0.19 0.0 59.0
CUBE (1000) 1.76 0.0 91.7 4.06 0.0 95.0 5.71 0.0 82.9 10.69 0.0 60.9 4.66 0.0 85.0
LR-TIGHT 0.37 100.0 100.0 1.76 100.0 100.0 4.79 100.0 100.0 30.85 94.5 94.5 7.25 99.0 99.0
OPTBEAM 0.23 100.0 100.0 0.50 100.0 100.0 1.42 100.0 100.0 7.14 93.6 93.6 1.75 98.8 98.8
ILP 9.15 100.0 100.0 32.35 100.0 100.0 49.6 100.0 100.0 108.6 100.0 100.0 40.1 100.0 100.0
Table 1: Experimental results for translation experiments. Column time is the mean time per sentence in seconds,
cert is the percentage of sentences solved with a certificate of optimality, exact is the percentage of sentences solved
exactly, i.e. ?>x+ ? = ?>x? + ? . Results are grouped by sentence length (group 1-10 is omitted for space).
is seven times faster than the decoder of Chang and
Collins (2011) and 3.5 times faster then our reim-
plementation, LR-TIGHT. ASTAR performs poorly,
taking lots of time on difficult sentences. BEAM runs
quickly, but rarely finds an exact solution. MOSES
without gap constraints is also fast, but less exact
than OPTBEAM and unable to produce certificates.
For syntax-based translation. OPTBEAM finds a
certificate on 98.8% of solutions with an average
time of 1.75 seconds per sentence, and is four times
faster than LR-TIGHT. CUBE (100) is an order
of magnitude faster, but is rarely exact on longer
sentences. CUBE (1000) finds more exact solu-
tions, but is comparable in speed to optimal beam
search. BEAM performs better than in the phrase-
based model, but is not much faster than OPTBEAM.
Figure 7.2 shows the relationship between beam
search optimality and duality gap. Graph (a) shows
how a handful of LR rounds can significantly tighten
the upper bound score of many sentences. Graph (b)
shows how beam search is more likely to find opti-
mal solutions with tighter bounds. BEAM effectively
uses 0 rounds of LR, which may explain why it finds
so few optimal solutions compared to OPTBEAM.
Table 2 breaks down the time spent in each part
of the algorithm. For both methods, beam search has
the most time variance and uses more time on longer
sentences. For phrase-based sentences, Lagrangian
relaxation is fast, and hypergraph construction dom-
? 30 all
mean median mean median
Hypergraph 56.6% 69.8% 59.6% 69.6%
PB Lag. Relaxation 10.0% 5.5% 9.4% 7.6%
Beam Search 33.4% 24.6% 30.9% 22.8%
Hypergraph 0.5% 1.6% 0.8% 2.4%
SB Lag. Relaxation 15.0% 35.2% 17.3% 41.4%
Beam Search 84.4% 63.1% 81.9 % 56.1%
Table 2: Distribution of time within optimal beam search,
including: hypergraph construction, Lagrangian relax-
ation, and beam search. Mean is the percentage of total
time. Median is the distribution over the median values
for each row.
inates. If not for this cost, OPTBEAM might be com-
parable in speed to MOSES (1000).
8 Conclusion
In this work we develop an optimal variant of beam
search and apply it to machine translation decod-
ing. The algorithm uses beam search to produce
constrained solutions and bounds from Lagrangian
relaxation to eliminate non-optimal solutions. Re-
sults show that this method can efficiently find exact
solutions for two important styles of machine trans-
lation.
Acknowledgments Alexander Rush, Yin-Wen
Chang and Michael Collins were all supported by
NSF grant IIS-1161814. Alexander Rush was partially
supported by an NSF Graduate Research Fellowship.
219
References
David Belanger, Alexandre Passos, Sebastian Riedel, and
Andrew McCallum. 2012. Map inference in chains
using column generation. In NIPS, pages 1853?1861.
Stephen Boyd and Almir Mutapcic. 2007. Subgradient
methods.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through la-
grangian relaxation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 26?37. Association for Computational Lin-
guistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. computational linguistics, 33(2):201?228.
Adria de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hierar-
chical Phrase-Based Translation with Weighted Finite-
State Transducers and Shallow-n Grammars. In Com-
putational linguistics, volume 36, pages 505?533.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathen
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vlad Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting on Association for
Computational Linguistics, ACL ?01, pages 228?235.
Inc. Gurobi Optimization. 2013. Gurobi optimizer refer-
ence manual.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 53?64. As-
sociation for Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273?283, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 380?388, Athens, Greece,
March. Association for Computational Linguistics.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology, NAACL ?03,
pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. Machine translation: From real users to research,
pages 115?124.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 161?168, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
R. Kipp Martin, Rardin L. Rardin, and Brian A. Camp-
bell. 1990. Polyhedral characterization of dis-
crete dynamic programming. Operations research,
38(1):127?138.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for statisti-
cal machine translation. In Proceedings of the work-
shop on Data-driven methods in machine translation -
Volume 14, DMMT ?01, pages 1?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Boris Polyak. 1987. Introduction to Optimization. Opti-
mization Software, Inc.
Sebastian Riedel and James Clarke. 2009. Revisiting
optimal decoding for machine translation IBM model
4. In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Companion Volume: Short Papers, pages 5?8. As-
sociation for Computational Linguistics.
Sebastian Riedel, David Smith, and Andrew McCallum.
2012. Parse, price and cut: delayed column and row
generation for graph based parsers. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
220
Natural Language Learning, pages 732?743. Associa-
tion for Computational Linguistics.
Alexander M Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, vol-
ume 1, pages 72?82.
Alexander M Rush and Michael Collins. 2012. A tutorial
on dual decomposition and lagrangian relaxation for
inference in natural language processing. Journal of
Artificial Intelligence Research, 45:305?362.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a dynamic programming beam search al-
gorithm for statistical machine translation. Computa-
tional Linguistics, 29(1):97?133.
Christoph Tillmann. 2006. Efficient dynamic pro-
gramming search algorithms for phrase-based SMT.
In Proceedings of the Workshop on Computationally
Hard Problems and Joint Inference in Speech and Lan-
guage Processing, CHSLP ?06, pages 9?16.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 777?784,
Morristown, NJ, USA. Association for Computational
Linguistics.
Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-
cedda. 2009. Phrase-based statistical machine transla-
tion as a traveling salesman problem. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1 - Volume 1, ACL ?09, pages 333?341, Stroudsburg,
PA, USA. Association for Computational Linguistics.
221
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1574?1583,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Convex Alternative to IBM Model 2
Andrei Simion
Columbia University
IEOR Department
New York, NY, 10027
aas2148@columbia.edu
Michael Collins
Columbia University
Computer Science
New York, NY, 10027
mc3354@columbia.edu
Clifford Stein
Columbia University
IEOR Department
New York, NY, 10027
cs2035@columbia.edu
Abstract
The IBM translation models have been hugely
influential in statistical machine translation;
they are the basis of the alignment models
used in modern translation systems. Exclud-
ing IBM Model 1, the IBM translation mod-
els, and practically all variants proposed in the
literature, have relied on the optimization of
likelihood functions or similar functions that
are non-convex, and hence have multiple lo-
cal optima. In this paper we introduce a con-
vex relaxation of IBM Model 2, and describe
an optimization algorithm for the relaxation
based on a subgradient method combined
with exponentiated-gradient updates. Our ap-
proach gives the same level of alignment ac-
curacy as IBM Model 2.
1 Introduction
The IBM translation models (Brown et al, 1993)
have been tremendously important in statistical ma-
chine translation (SMT). The IBM models were the
first generation of SMT systems; in recent work,
they play a central role in deriving alignments used
within many modern SMT approaches, for exam-
ple phrase-based translation models (Koehn, 2008)
and syntax-based translation systems (e.g., (Chi-
ang, 2005; Marcu et al, 2006)). Since the origi-
nal IBM paper, there has been a large amount of re-
search exploring the original IBM models and mod-
ern variants (e.g., (Moore, 2004; Liang et al, 2006;
Toutanova and Galley, 2011; Riley and Gildea,
2012; Vogel et al, 1996)).
Excluding IBM Model 1, the IBM translation
models, and practically all variants proposed in the
literature, have relied on the optimization of like-
lihood functions or similar functions that are non-
convex. Unfortunately, non-convex objective func-
tions have multiple local optima, and finding a
global optimum of a non-convex function is typi-
cally a computationally intractible problem. Typi-
cally, an EM algorithm is used, which often runs in
a reasonable amount of time, but with no guarantees
of finding a global optima (or for that matter, even a
near-optimal solution).
In this paper we make the following contributions:
? We introduce a convex relaxation of IBM
Model 2. At a very high level, the relaxation
is derived by replacing the product t(fj |ei) ?
d(i|j) with a relaxation that is commonly used
in the linear programming literature (e.g., see
(Bertsimas, 1997; Bertsimas and Tsitsiklis,
1997; Martins et al, 2010)). (Here t(f |e) are
the translation parameters of the model, and
d(i|j) are the distortion parameters; the prod-
uct is non-linear, effectively introducing non-
convexity into the problem.)
? We describe an optimization algorithm for
the relaxed objective, based on a combina-
tion of stochastic subgradient methods with the
exponentiated-gradient (EG) algorithm (Kivi-
nen and Warmuth, 1997; Beck and Teboulle,
2003).
? We describe experiments with the method on
standard alignment datasets, showing that the
EG algorithm converges in only a few passes
over the data, and that our method achieves ac-
curacies that are very similar to those of IBM
Model 2.
Framing the unsupervised learning of alignment
models as a convex optimization problem, with
guaranteed convergence to a global optimum, has
several clear advantages. First, the method is eas-
ier to analyze, as the objective function is being
truly maximized. Second, there is no need for ini-
tialization heuristics with the approach, given that
the method will always converge to a global op-
timum. Finally, we expect that our convexity-
based approach may facilitate the further develop-
ment of more convex models. There has been a rich
1574
interplay between convex and non-convex meth-
ods in machine learning: as one example consider
the literature on classification problems, with early
work on the perceptron (linear/convex), then work
on neural networks with back-propagation (non-
linear/non-convex), then the introduction of support
vector machines (non-linear/convex), and finally re-
cent work on deep belief networks (non-linear/non-
convex). In view of these developments, the lack
of convex methods in translation alignment models
has been noticeable, and we hope that our work will
open up new directions and lead to further progress
in this area.
Notation. Throughout this paper, for any integer
N , we use [N ] to denote {1 . . . N} and [N ]0 to de-
note {0 . . . N}.
2 Related Work
(Brown et al, 1993) introduced IBM Models 1
through 5, and optimization methods for these mod-
els based on the EM algorithm. While the models
were originally introduced for full translation, they
are now mainly used to derive alignments which are
then used by phrase-based and other modern SMT
systems. Since the original IBM models were in-
troduced, many variants have been introduced in the
literature. (Vogel et al, 1996) introduced a model,
sometimes referred to as IBM 2.5, which uses a pa-
rameterization that is similar to a hidden Markov
model, and which allows the value of each alignment
variable to be conditioned on a previous alignment
variable. (Liang et al, 2006) describe a method that
explicitly incorporates agreement preferences dur-
ing training. (Och and Ney, 2003) give a systematic
comparison of several alignment models in the liter-
ature. (Moore, 2004) gives a detailed study of IBM
Model 1, showing various steps that can be used to
improve its performance. (Ganchev et al, 2010)
describes a method based on posterior regulariza-
tion that incorporates additional constraints within
the EM algorithm for estimation of IBM models.
All of these approaches are unsupervised, in that
they do not require labeled alignment data; however
several authors have considered supervised models
(e.g., see (Lacoste-Julien et al, 2006; Taskar et al,
2005; Haghighi et al, 2009)). The focus of the cur-
rent paper is on unsupervised learning; the unsuper-
vised variants described above all make use of non-
convex objective functions during training, with the
usual problems with multiple local maxima.
3 The IBM Model 1 and Model 2
Optimization Problems
In this section we give a brief review of IBM Models
1 and 2, and the optimization problems arising from
these models. The standard approach for optimiza-
tion within these models is the EM algorithm.
Throughout this section, and the remainder of the
paper, we assume that our set of training examples
is (e(k), f (k)) for k = 1 . . . n, where e(k) is the k?th
English sentence and f (k) is the k?th French sen-
tence. Following standard convention, we assume
the task is to translate from French (the ?source?
language) into English (the ?target? language). We
use E to denote the English vocabulary (set of pos-
sible English words), and F to denote the French
vocabulary. The k?th English sentence is a sequence
of words e(k)1 . . . e
(k)
lk
where lk is the length of the
k?th English sentence, and each e(k)i ? E; similarly
the k?th French sentence is a sequence f (k)1 . . . f
(k)
mk
where each f (k)j ? F . We define e
(k)
0 for k = 1 . . . n
to be a special NULL word (note that E contains the
NULL word). Finally, we define L = maxnk=1 lk
and M = maxnk=1mk.
For each English word e ? E, we will assume
that D(e) is a dictionary specifying the set of possi-
ble French words that can be translations of e. The
set D(e) is a subset of F . In practice, D(e) can be
derived in various ways; in our experiments we sim-
ply define D(e) to include all French words f such
that e and f are seen in a translation pair.
Given these definitions, the IBM model 2 opti-
mization problem is given in Figure 1. The parame-
ters in this problem are t(f |e) and d(i|j). The t(f |e)
parameters are translation parameters specifying the
probability of English word e being translated as
French word f . The distortion parameters d(i|j)
specify the probability of the j?th French word in a
sentence being aligned to the i?th English word. We
use a variant of IBM Model 2 where the distortion
variables are shared across all sentence lengths (sim-
ilar variants have been used in (Liang et al, 2006)
and (Koehn, 2008)). The objective function is then
1575
Input: DefineE, F , L,M , (e(k), f (k), lk,mk) for
k = 1 . . . n, D(e) for e ? E as in Section 3.
Parameters:
? A parameter t(f |e) for each e ? E, f ? D(e).
? A parameter d(i|j) for each i ? [L]0, j ? [M ].
Constraints:
?e ? E, f ? D(e), t(f |e) ? 0 (1)
?e ? E,
?
f?D(e)
t(f |e) = 1 (2)
?i ? [L]0, j ? [M ], d(i|j) ? 0 (3)
?j ? [M ],
?
i?[L]0
d(i|j) = 1 (4)
Objective: Maximize
1
n
n?
k=1
mk?
j=1
log
lk?
i=0
t(f (k)j |e
(k)
i )d(i|j) (5)
with respect to the t(f |e) and d(i|j) parameters.
Figure 1: The IBM Model 2 Optimization Problem.
the log-likelihood of the training data (see Eq. 5):
1
n
n?
k=1
mk?
j=1
log p(f (k)j |e
(k)) ,
where
p(f (k)j |e
(k)) =
lk?
i=0
t(f (k)j |e
(k)
i )d(i|j) .
Crucially, while the constraints in the IBM Model
2 optimization problem are linear, the objective
function in Eq. 5 is non-convex. Therefore, opti-
mization methods for IBM Model 2, in particular
the EM algorithm, are typically only guaranteed to
reach a local maximum of the objective function.
For completeness, Figure 2 shows the optimiza-
tion problem for IBM Model 1. In IBM Model 1
the distortion parameters d(i|j) are all fixed to be
the uniform distribution (i.e., 1/(L + 1)). The ob-
jective function for IBM Model 1 is actually convex,
so the EM algorithm will converge to a global max-
imum. However IBM Model 1 is much weaker than
model 2, and typically gives far worse performance.
Input: DefineE, F , L,M , (e(k), f (k), lk,mk) for
k = 1 . . . n, D(e) for e ? E as in Section 3.
Parameters:
? A parameter t(f |e) for each e ? E, f ? D(e).
Constraints:
?e ? E, f ? D(e), t(f |e) ? 0 (6)
?e ? E,
?
f?D(e)
t(f |e) = 1 (7)
Objective: Maximize
1
n
n?
k=1
mk?
j=1
log
lk?
i=0
t(f (k)j |e
(k)
i )
(L+ 1)
(8)
with respect to the t(f |e) parameters.
Figure 2: The IBM Model 1 Optimization Problem.
A common heuristic is to initialize the t(f |e) param-
eters in EM optimization of IBM Model 2 using the
output from IBM Model 1. The intuition behind this
heuristic is that the IBM Model 1 values for t(f |e)
will be a reasonable starting point, and the EM al-
gorithm will climb to a ?good? local optimum. We
are not aware of any guarantees for this initialization
heuristic, however.
4 A Convex Relaxation of IBM Model 2
We now introduce a convex optimization problem,
the I2CR (IBM 2 Convex Relaxation) problem.
As its name suggests, this optimization problem is
closely related to IBM Model 2, but is convex. Be-
cause of this it will be relatively easy to derive an op-
timization algorithm that is guaranteed to converge
to a global optimum. Our experiments show that
the relaxation gives very similar performance to the
original IBM 2 optimization problem, as described
in the previous section.
We first describe an optimization problem,
I2CR-1, that illustrates the basic idea behind the
convex relaxation. We then describe a refined re-
laxation, I2CR-2, that introduces a couple of modi-
fications, and which performs well in experiments.
1576
Input: DefineE, F , L,M , (e(k), f (k), lk,mk) for
k = 1 . . . n, D(e) for e ? E as in Section 3.
Parameters:
? A parameter t(f |e) for each e ? E, f ? D(e).
? A parameter d(i|j) for each i ? [L]0, j ? [M ].
?A parameter q(i, j, k) for each k ? [n], i ? [lk]0,
j ? [mk].
Constraints:
?e ? E, f ? D(e), t(f |e) ? 0 (9)
?e ? E,
?
f?D(e)
t(f |e) = 1 (10)
?i ? [L]0, j ? [M ], d(i|j) ? 0 (11)
?j ? [M ],
?
i?[L]0
d(i|j) = 1 (12)
?i, j, k, q(i, j, k) ? 0 (13)
?i, j, k, q(i, j, k) ? d(i|j) (14)
?i, j, k, q(i, j, k) ? t(f (k)j |e
(k)
i ) (15)
Objective: Maximize
1
n
n?
k=1
mk?
j=1
log
lk?
i=0
q(i, j, k) (16)
with respect to the q(i, j, k), t(f |e) and d(i|j) pa-
rameters.
Figure 3: The I2CR-1 (IBM 2 Convex Relaxation) Prob-
lem, version 1.
4.1 The I2CR-1 Problem
The I2CR-1 problem is shown in Figure 3. A first
key idea is to introduce a new variable q(i, j, k) for
each k ? [n], i ? [lk]0, j ? [mk]: that is, a new
variable for each triple (i, j, k) specifying a sen-
tence pair, and a specific English and French posi-
tion in that sentence. Each q variable must satisfy
the constraints in Eqs. 13-15, repeated here for con-
venience:
?i, j, k, q(i, j, k) ? 0 ,
?i, j, k, q(i, j, k) ? d(i|j) ,
?i, j, k, q(i, j, k) ? t(f (k)j |e
(k)
i ) .
The objective function is
1
n
n?
k=1
mk?
j=1
log
lk?
i=0
q(i, j, k)
which is similar to the objective function in Figure 1,
but where t(f (k)j |e
(k)
i )?d(i|j) has been replaced by
q(i, j, k). The intuition behind the new problem is as
follows. If, instead of the constraints in Eqs. 13-15,
we had the constraint
q(i, j, k) = t(f (k)j |e
(k)
i )? d(i|j) , (17)
then the I2CR-1 problem would clearly be identi-
cal to the IBM Model 2 optimization problem. We
have used a standard relaxation of the non-linear
constraint x = y ? z where x, y, z are all variables
in the range [0, 1], namely
x ? y ,
x ? z ,
x ? y + z ? 1 .
These inequalites are a relaxation in the sense that
any (x, y, z) triple that satisfies x = y ? z also sat-
isfies these constraints. Applying this relaxation to
Eq. 17 gives
q(i, j, k) ? t(f (k)j |e
(k)
i ) ,
q(i, j, k) ? d(i|j) ,
q(i, j, k) ? t(f (k)j |e
(k)
i ) + d(i|j)? 1 . (18)
The final thing to note is that the constraint in
Eq. 18 can be omitted in the I2CR-1 problem. This
is because the task is to maximize the objective
with respect to the q variables and the objective
is strictly increasing as the q values increase?thus
lower bounds on their values are redundant in the
I2CR-1 problem.
It is easily verified that the constraints in the
I2CR-1 problem are linear, and that the objective
function is convex. In Section 5 of this paper we
describe an optimization method for the problem.
Note that because the objective function is being
maximized, and the objective increases monotoni-
cally as the q values increase, at the global optimum1
1More precisely, at any global optimum: the objective func-
tion may not be strictly convex, in which case there will be mul-
tiple global optima.
1577
Input: Same as in I2CR-1 (Figure 4).
Parameters: Same as in I2CR-1 (Figure 4).
Constraints: Same as in I2CR-1 (Figure 4).
Objective: Maximize
1
2n
n?
k=1
mk?
j=1
log?
lk?
i=0
q(i, j, k)
+
1
2n
n?
k=1
mk?
j=1
log?
lk?
i=0
t(f (k)j |e
(k)
i )
(L+ 1)
with respect to the q(i, j, k), t(f |e) and d(i|j) pa-
rameters.
Figure 4: The I2CR-2 (IBM 2 Convex Relaxation) Prob-
lem, version 2. The problem is identical to the I2CR-1
problem, but it also includes a term in the objective func-
tion that is identical to the IBM Model 1 objective. We
define log?(z) = log(z + ?) where ? is a small positive
constant.
we have
q(i, j, k) = min{t(f (k)j |e
(k)
i ), d(i|j)} ,
where min{x, y} returns the minimum of the two
values x and y. Thus, we could actually eliminate
the q variables and write an optimization problem
that is identical to the IBM Model 2 optimization
problem, but with the objective function
1
n
n?
k=1
mk?
j=1
log
lk?
i=0
min{t(f (k)j |e
(k)
i ), d(i|j)} .
It will turn out that both views of the I2CR-1
problem?with and without the q variables?are
helpful, so we have included both in this paper.
4.2 The I2CR-2 Problem
Figure 4 shows the refined optimization problem,
which we call I2CR-2. The problem incorporates
two modifications. First, we modify the objective
function to be
1
2n
n?
k=1
mk?
j=1
log?
lk?
i=0
q(i, j, k)
+
1
2n
n?
k=1
mk?
j=1
log?
lk?
i=0
t(f (k)j |e
(k)
i )
(L+ 1)
.
Thus the objective function includes a second term
that is identical to the objective function for IBM
Model 1 (see Figure 2). In preliminary experiments
with the I2CR-1 optimization problem, we found
that the I2CR-1 objective was not sufficiently depen-
dent on the t parameters: intuitively, if the d param-
eters achieve the min on many training examples,
the values for the t variables become unimportant.
The addition of the IBM Model 1 objective fixed this
problem by introducing a term that depends on the t
values alone.
Second, we replace log by log?, where log?(z) =
log(z + ?), and ? is a small positive constant (in
our experiments we used ? = 0.001). Under this
definition the derivatives of log? are upper-bounded
by 1/?, in contrast to log, where the derivatives
can diverge to infinity. The optimization methods
we use are gradient-based methods (or more pre-
cisely, subgradient-based methods), and we have
found them to be considerably more stable when the
values for gradients do not diverge to infinity.
The modified objective remains convex.
5 A Stochastic Exponentiated-Gradient
Algorithm for Optimization
We now describe an algorithm for optimizing the
I2CR-2 problem in Figure 4. The algorithm is
closely related to stochastic gradient ascent, but with
two modifications:
? First, because the t(f |e) and d(i|j) parame-
ters have simplex constraints (see Figure 1),
we use exponentiated gradient (EG) updates.
EG algorithms are gradient-based methods that
maintain simplex constraints; see for exam-
ple: (Kivinen and Warmuth, 1997; Beck and
Teboulle, 2003; Collins et al, 2008).
? Second, the objective function in the I2CR-
2 problem is convex, but is not differentiable
(the gradient may not exist at all points). For
this reason we use subgradients in the place of
gradients. In spite of the non-differentiability
of the objective function, subgradient meth-
ods still have strong convergence guarantees
when combined with EG updates (e.g., the con-
vergence proofs in (Beck and Teboulle, 2003)
1578
go through with minor modifications; see also
(Bertsekas, 1999)).
To derive the updates, recall that we are maximiz-
ing the following objective function:
h(t, d)
=
1
2|T |
?
k?T
mk?
j=1
log?
lk?
i=0
min
{
t(f (k)j |e
(k)
i ), d(i|j)
}
+
1
2|T |
?
k?T
mk?
j=1
log?
lk?
i=0
t(f (k)j |e
(k)
i )
(L+ 1)
. (19)
Here we use T to denote the set {1 . . . n}; we will
see shortly why this notation is convenient. We use
t and d to refer to the full set of t and d parameters
respectively; h(t, d) is the function to be maximized.
Recall that log?(z) = log(z + ?) where ? is a small
positive parameter.
Given a concave function f(x) where x ? Rd, a
subgradient of f(x) at x is any vector g(x) ? Rd
such that for any y ? Rd,
f(y) ? f(x) + g(x) ? (y ? x) ,
where u?v is the inner product between vectors u and
v. Subgradients are similar to gradients for differ-
entiable concave functions, in that gradients satisfy
the above property. Subgradients can be used in the
place of gradients in many optimization algorithms
(see for example (Bertsekas, 1999)).
The subgradients for the objective function in
Eq. 19 take a simple form. First, define
R(j, k) = ?+
lk?
i=0
t(f (k)j |e
(k)
i ) ,
Q(j, k) = ?+
lk?
i=0
min{t(f (k)j |e
(k)
i ), d(i|j)} ,
and
I(i, j, k) =
{
1 if t(f (k)j |e
(k)
i ) ? d(i|j)
0 otherwise .
Then the subgradients2 are
?t(f |e) =
1
2|T |
?
i,j,k:
f
(k)
j =f
e(k)i =e
(
1
R(j, k)
+
I(i, j, k)
Q(j, k)
)
2We set ?t(f |e) and ?d(i|j) as the subgradients for the
objective function in Eq. 19 with respect to t(f |e) and d(i|j)
respectively.
and
?d(i|j) =
1
2|T |
?
k:i?lk,j?mk
1? I(i, j, k)
Q(j, k)
.
Exponentiated-gradient updates then take the fol-
lowing form:
t(f |e)?
t(f |e)? exp{? ??t(f |e)}
?
f t(f |e)? exp{? ??t(f |e)}
(20)
and
d(i|j)?
d(i|j)? exp{? ??d(i|j)}
?
i d(i|j)? exp{? ??d(i|j)}
, (21)
where ? > 0 is a constant step size in the algorithm.
Note that the EG updates make use of subgradients,
but maintain the simplex constraints on the t and d
variables.
The method just described is a batch gradient
method, where the entire training set T = {1 . . . n}
is used to derive the subgradients before the updates
in Eqs. 20 and 21 are made. Many results in ma-
chine learning and NLP have shown that stochastic
gradient methods, where a subset of the training ex-
amples is used before each gradient-based update,
can converge much more quickly than batch gradi-
ent methods. In our notation, this simply involves
replacing T by some subset T ? of the training exam-
ples in the above definitions, where |T ?| is typically
much smaller than |T |.
Figure 5 shows our final algorithm, a stochastic
version of the exponentiated-gradient method. The
method takes S passes over the data. For each pass,
it randomly partitions the training set into mini-
batches T1 . . . TK of size B, where B is an integer
specifying the size of each mini-batch (in our exper-
iments we used B = 125 or B = 250). The al-
gorithm then performs EG updates using each mini-
batch T1 . . . TK in turn. As can be seen in Table 3,
our experiments show that the algorithm makes very
significant progress in the first pass over the data,
and takes very few iterations to converge to a good
solution even though we initialized with uniform pa-
rameter values.
6 Experiments
In this section we describe experiments using the
I2CR-2 optimization problem combined with the
1579
1: Input: Define E, F , L, M , (e(k), f (k), lk,mk)
for k = 1 . . . n, D(e) for e ? E as in Section 3.
An integer B specifying the batch size. An inte-
ger S specifying the number of passes over the
data. A step size ? > 0. A parameter ? > 0
used in the definition of log? .
2: Parameters:
?A parameter t(f |e) for each e ? E, f ? D(e).
?A parameter d(i|j) for each i ? [L]0, j ? [M ].
3: Definitions:
R(j, k) = ?+
lk?
i=0
t(f (k)j |e
(k)
i )
Q(j, k) = ?+
lk?
i=0
min{t(f (k)j |e
(k)
i ), d(i|j)}
4: Initialization:
? ?e ? E, f ? D(e), t(f |e) = 1/|D(e)|
? ?j ? [M ], i ? [L]0, d(i|j) = 1/(L+ 1)
5: Algorithm:
6: for all s = 1 to S do
7: Randomly partition [n] into subsets T1 . . . TK of
size B where K = n/B.
8: for all b = 1 to K do
9: ?e ? E, f ? D(e), ?(e, f) = 0
10: ?j ? [M ], i ? [L]0, ?(i, j) = 0
11: for all k ? Tb do
12: for all j = 1 to mk do
13: for all i = 0 to lk do
14: ?(e(k)i , f
(k)
j ) += 1/(2R(j, k))
15: if t(f (k)j |e
(k)
i ) ? d(i|j) then
16: ?(e(k)i , f
(k)
j ) += 1/(2Q(j, k))
17: else
18: ?(i, j) += 1/(2Q(j, k))
19: ?e, f, t(f |e) = t(f |e) exp (? ? ?(e, f)/B)
20: ?i, j, d(i|j) = d(i|j) exp (? ? ?(i, j)/B)
21: Renormalize t and d parameters to satisfy
?
f t(f |e) = 1 and
?
i d(i|j) = 1.
22: Output: t and d parameters.
Figure 5: The stochastic exponentiated-gradient algo-
rithm for optimization of I2CR-2.
stochastic EG algorithm for parameter estimation.
We first describe the data sets we use, and then de-
scribe experiments with the method, comparing our
approach to results from IBM Model 2. We com-
pare the various algorithms in terms of their accu-
racy in recovering alignments, using metrics such as
F-measure and AER.
6.1 Data Sets
We use data from the bilingual word alignment
workshop held at HLT-NAACL 2003 (Michalcea
and Pederson, 2003). As a first dataset, we use the
Canadian Hansards bilingual corpus, with 247,878
English-French sentence pairs as training data, 37
sentences of development data, and 447 sentences
of test data (note that we use a randomly chosen
subset of the original training set of 1.1 million sen-
tences, similar to the setting used in (Moore, 2004)).
The development and test data have been manually
aligned at the word level, annotating alignments be-
tween source and target words in the corpus as ei-
ther ?sure? (S) or ?possible? (P ) alignments, as de-
scribed in (Och and Ney, 2003).
As a second data set, we used the Romanian-
English data from the HLT-NAACL 2003 workshop.
This consisted of a training set of 48,706 Romanian-
English sentence-pairs, a development set of 17 sen-
tence pairs, and a test set of 248 sentence pairs.
6.2 Methodology
For each of the models?IBM Model 1, IBM Model
2, and I2CR-2?we follow convention in applying
the following methodology: first, we estimate the
t and d parameters using models in both source-
target and target-source directions; second, we find
the most likely alignment for each development or
test data sentence in each direction; third, we take
the intersection of the two alignments as the final
output from the model.
For the EG algorithm we use a batch size B =
250 and step size ? = 0.5 on the Hansards data, and
B = 125 and ? = 0.5 for the Romanian-English
data.
We report the performance of the models in terms
of Precision, Recall, AER, and F-Measure as defined
by (Och and Ney, 2003). If A is the set of align-
ments produced by an algorithm, S is the set of sure
alignments as annotated in test data, and P is the
set of possible alignments, then these quantities are
defined as
Recall =
|A ? S|
|S|
,
1580
Precision =
|A ? S|
|A|
,
AER = 1?
|A ? S|+ |A ? P |
|A|+ |S|
,
F-Measure =
1
.5
Recall +
.5
Precision
.
Note that we report results in both AER and
F-measure; however there is evidence (Fraser and
Marcu, 2004) that F-measure is better correlated
with translation quality when the alignments are
used in a full system.
In training IBM Model 1 we follow (Moore,
2004) in running EM for 15 iterations. In training
IBM Model 2 we first train IBM Model 1 for 15
iterations to initialize the t parameters, then train
IBM Model 2 for a further 10 iterations. For the
EG algorithm, we use 10 iterations over the training
data for the Hansards data, and 15 iterations on the
Romanian-English data (on the latter dataset results
on the trial data showed that the method took slightly
longer to converge). We report F-measure and AER
results for each of the iterations under the IBM
Model 2 and I2CR-2 models. See Table 1 for the re-
sults on the Hansards data, and Table 2 for the results
on the English-Romanian dataset. It can be seen that
both I2CR-2 and IBM Model 2 converge to a fairly
stable result after 2-3 iterations. The two models
give very similar levels of performance, for example
after 10 iterations on the Hansard data IBM Model
2 gives 14.22 AER and 0.7516 F-Measure versus
14.60 AER and 0.7506 F-Measure for I2CR-2.
On the right, Table 3 shows the values of the ob-
jective function at each iteration when using the EG
algorithm to optimize the I2CR-2 objective. The
method makes a large amount of progress on the first
iteration and then continues to improve. Finally, we
note that the memory requirements for I2CR-2 and
IBM2 are about the same, but that the time for one
iteration of I2CR-2 on the Hansards data is approxi-
mately one hour, while the time for one iteration of
IBM2 was approximately 10 minutes.
7 Conclusions and Future Work
We have introduced the first convex model for un-
supervised learning of alignments in statistical ma-
chine translation with performance comparable to
Iteration IBM2 I2CR-2 IBM2 I2CR-2
AER AER F-Measure F-Measure
Test Set Statistics
1 0.1491 0.1556 0.7530 0.7369
2 0.1477 0.1489 0.7519 0.7456
3 0.1451 0.1476 0.7527 0.7467
4 0.1426 0.1488 0.7536 0.7449
5 0.1422 0.1495 0.7535 0.7472
6 0.1431 0.1476 0.7511 0.7478
7 0.1434 0.1506 0.7506 0.7456
8 0.1437 0.1495 0.7501 0.7470
9 0.1434 0.1494 0.7501 0.7468
10 0.1422 0.1460 0.7516 0.7506
Development Set Statistics
1 0.1871 0.1971 0.6823 .6676
2 0.1896 0.1760 0.6758 .6827
3 0.1964 0.1860 0.6648 .6739
4 0.1912 0.1835 0.6713 .6775
5 0.1884 0.1813 0.6740 .06773
6 0.1836 0.1851 0.6767 0.6811
7 0.1831 0.1806 0.6749 0.6765
8 0.1842 0.1843 0.6739 0.6775
9 0.1864 0.1928 0.6694 0.6640
10 0.1845 0.1829 0.6703 .6721
Table 1: Results on the Hansards data for IBM Model 2
and the I2CR-2 method.
Iteration IBM2 I2CR-2 IBM2 I2CR-2
AER AER F-Measure F-Measure
Test Set Statistics
1 0.4041 0.5354 0.5959 0.4646
2 0.4010 0.4764 0.5990 0.5256
3 0.4020 0.4543 0.5980 0.5457
4 0.4012 0.4384 0.5988 0.5617
5 0.4003 0.4277 0.5997 0.5723
6 0.3990 0.4266 0.6010 0.5834
7 0.4000 0.4162 0.6000 0.5838
8 0.4023 0.4114 0.5977 0.5886
9 0.4022 0.4081 0.5978 0.5919
10 0.4027 0.4043 0.5973 0.5957
11 0.4031 0.4040 0.5969 0.5960
12 0.4042 0.4027 0.5958 0.5973
13 0.4043 0.4021 0.5957 0.5979
14 0.4062 0.4007 0.5938 0.5993
15 0.4057 0.4014 0.5943 0.5986
Development Set Statistics
1 0.4074 0.5841 0.5926 0.4159
2 0.3911 0.4938 0.6089 0.5062
3 0.3888 0.4673 0.6112 0.5327
4 0.3904 0.4596 0.6096 0.5404
5 0.3881 0.4463 0.6119 0.5537
6 0.3904 0.4306 0.6096 0.5694
7 0.3936 0.4175 0.6094 0.5826
8 0.3897 0.4060 0.6103 0.5940
9 0.3961 0.4014 0.6039 0.5986
10 0.3970 0.4072 0.6030 0.5928
11 0.4018 0.3956 0.5982 0.6044
12 0.4035 0.3931 0.5965 0.6069
13 0.4035 0.3862 0.5965 0.6138
14 0.4014 0.3908 0.5986 0.6092
15 0.4063 0.3858 0.5937 0.6142
Table 2: Results on the English-Romanian data for IBM
Model 2 and the I2CR-2 method.
1581
Iteration EF Objective FE Objective
0 -99.6053 -79.5566
1 -32.4528 -27.4925
2 -31.1641 -26.262
3 -30.6311 -25.7093
4 -30.3367 -25.3714
5 -30.1428 -25.1456
6 -30.0000 -24.992
7 -29.8736 -24.8605
8 -29.8093 -24.7551
9 -29.7326 -24.684
10 -29.6771 -24.6099
Table 3: Objective values for the EG algorithm opti-
mization of I2CR-2 at each iteration. ?EF Objective?
corresponds to training a model with t(e|f) parameters,
?FE Objective? corresponds to the reverse direction, with
t(f |e) parameters. Iteration 0 corresponds to the objec-
tive value under the initial, uniform parameter values.
the commonly-used IBM Model 2. We believe
that introducing convexity without sacrificing per-
formance will open the door to further improve-
ments in this area. Future work will consider ways to
speed up our algorithm and extensions of the method
to more complex alignment models.
Acknowledgments
Michael Collins is partly supported by NSF grant
IIS-1161814. Cliff Stein is partly supported by NSF
grant CCF-0915681. The authors thank Sasha Rush
for his help with implementation questions. We
also thank the anonymous reviewers for many use-
ful comments; we hope to pursue the comments we
were not able to address in a followup paper.
References
Peter L. Bartlett, Ben Taskar, Michael Collins and David
Mcallester. 2004. Exponentiated Gradient Algorithms
for Large-Margin Structured Classification. In Pro-
ceedings of NIPS.
Amir Beck and Marc Teboulle. 2003. Mirror Descent and
Nonlinear Projected Subgradient Methods for Convex
Optimization. Operations Research Letters, 31:167-
175.
Dimitris Bertsimas and John N. Tsitsiklis. 1997. Intro-
duction to Linear Programming. Athena Scientific.
Dimitris Bertsimas. 2005. Optimization Over Integers.
Dynamic Ideas.
Dimitri P. Bertsekas. 1999. Nonlinear Optimization.
Athena Press.
Steven Boyd and Lieven Vandenberghe. 2004. Convex
Optimization. Cambridge University Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19:263-311.
David Chiang. 2005. A Hierarchical Phrase-Based Model
for Statistical Machine Translation. In Proceedings of
the ACL.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras and Peter L. Bartlett. 2008. Exponentiated
Gradient Algorithms for Conditional Random Fields
and Max-Margin Markov Networks. Journal Machine
Learning, 9(Aug): 1775-1822.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood From Incomplete Data via the
EM Algorithm. Journal of the royal statistical society,
series B, 39(1):1-38.
Alexander Fraser and Daniel Marcu. 2007. Measur-
ing Word Alignment Quality for Statistical Ma-
chine Translation. Journal Computational Linguistics,
33(3): 293-303.
Kuzman Ganchev, Joao V. Graca, Jennifer Gillenwater,
Ben Taskar. 2010. Posterior Regularization for Struc-
tured Latent Variable Models. Journal of Machine
Learning, 11(July): 2001-2049.
Joao V. Graca, Kuzman Ganchev and Ben Taskar. 2007.
Expectation Maximization and Posterior Constraints.
In Proceedings of NIPS.
Aria Haghighi, John Blitzer, John DeNero and Dan Klein.
2009. Better Word Alignments with Supervised ITG
Models. In Proceedings of the ACL.
Darcey Riley and Daniel Gildea. 2012. Improving the
IBM Alignment Models Using Variational Bayes. In
Proceedings of the ACL.
Yuhong Guo and Dale Schuurmans. 2007. Convex Relax-
ations of Latent Variable Training. In NIPS.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2008. Word Alignment via Quadratic
Assignment. In Proceedings of the HLT-NAACL.
Phillip Koehn. 2008. Statistical Machine Translation.
Cambridge University Press.
Kivinen, J., Warmuth, M. 1997. Exponentiated Gradient
Versus Gradient Descent for Linear Predictors. Infor-
mation and Computation, 132, 1-63.
Percy Liang, Ben Taskar and Dan Klein. 2006. Alignment
by Agreement. In Proceedings of NAACL.
Daniel Marcu, Wei Wang, Abdessamad Echihabi,
and Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phrases. In Proceedings of the EMNLP.
1582
Andre F. T. Martins, Noah A. Smith and Eric P. Xing.
2010. Turbo Parsers: Dependency Parsing by Ap-
proximate Variational Inference. In Proceedings of the
EMNLP.
Rada Michalcea and Ted Pederson. 2003. An Evalua-
tion Exercise in Word Alignment. HLT-NAACL 2003:
Workshop in building and using Parallel Texts: Data
Driven Machine Translation and Beyond.
Robert C. Moore. 2004. Improving IBM Word-
Alignment Model 1. In Proceedings of the ACL.
Stephan Vogel, Hermann Ney and Christoph Tillman.
1996. HMM-Based Word Alignment in Statistical
Translation. In Proceedings of COLING.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational-Linguistics, 29(1): 19-52.
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A
New String-to-Dependency Machine Translation Al-
gorithm with a Target Dependency Language Model.
In Proceedings of the ACL-HLT.
Ben Taskar, Simon Lacoste-Julien and Dan Klein. 2005.
A Discriminative Matching Approach to Word Align-
ment. In Proceedings of the EMNLP.
Kristina Toutanova and Michel Galley. 2011. Why Ini-
tialization Matters for IBM Model 1: Multiple Optima
and Non-Strict Convexity. In Proceedings of the ACL.
Kenji Yamada and Kevin Knight. 2001. A Syntax-Based
Statistical Translation Model. In Proceedings of the
ACL.
Kenji Yamada and Kevin Knight. 2002. A Decoder for
Syntax-Based Statistical Machine Translation. In Pro-
ceedings of the ACL.
Ashish Vaswani, Liang Huang and David Chiang. 2012.
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the L0-norm. In
Proceedings of the ACL.
1583
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 452?461,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Learning Dictionaries for Named Entity Recognition using Minimal
Supervision
Arvind Neelakantan
Department of Computer Science
University of Massachusetts, Amherst
Amherst, MA, 01003
arvind@cs.umass.edu
Michael Collins
Department of Computer Science
Columbia University
New-York, NY 10027, USA
mcollins@cs.columbia.edu
Abstract
This paper describes an approach for au-
tomatic construction of dictionaries for
Named Entity Recognition (NER) using
large amounts of unlabeled data and a few
seed examples. We use Canonical Cor-
relation Analysis (CCA) to obtain lower
dimensional embeddings (representations)
for candidate phrases and classify these
phrases using a small number of labeled
examples. Our method achieves 16.5%
and 11.3% F-1 score improvement over
co-training on disease and virus NER re-
spectively. We also show that by adding
candidate phrase embeddings as features
in a sequence tagger gives better perfor-
mance compared to using word embed-
dings.
1 Introduction
Several works (e.g., Ratinov and Roth, 2009; Co-
hen and Sarawagi, 2004) have shown that inject-
ing dictionary matches as features in a sequence
tagger results in significant gains in NER perfor-
mance. However, building these dictionaries re-
quires a huge amount of human effort and it is of-
ten difficult to get good coverage for many named
entity types. The problem is more severe when we
consider named entity types such as gene, virus
and disease, because of the large (and growing)
number of names in use, the fact that the names are
heavily abbreviated and multiple names are used
to refer to the same entity (Leaman et al., 2010;
Dogan and Lu, 2012). Also, these dictionaries can
only be built by domain experts, making the pro-
cess very expensive.
This paper describes an approach for automatic
construction of dictionaries for NER using large
amounts of unlabeled data and a small number
of seed examples. Our approach consists of two
steps. First, we collect a high recall, low preci-
sion list of candidate phrases from the large unla-
beled data collection for every named entity type
using simple rules. In the second step, we con-
struct an accurate dictionary of named entities by
removing the noisy candidates from the list ob-
tained in the first step. This is done by learning a
classifier using the lower dimensional, real-valued
CCA (Hotelling, 1935) embeddings of the can-
didate phrases as features and training it using a
small number of labeled examples. The classifier
we use is a binary SVM which predicts whether a
candidate phrase is a named entity or not.
We compare our method to a widely used semi-
supervised algorithm based on co-training (Blum
and Mitchell, 1998). The dictionaries are first
evaluated on virus (GENIA, 2003) and disease
(Dogan and Lu, 2012) NER by using them directly
in dictionary based taggers. We also give results
comparing the dictionaries produced by the two
semi-supervised approaches with dictionaries that
are compiled manually. The effectiveness of the
dictionaries are also measured by injecting dictio-
nary matches as features in a Conditional Random
Field (CRF) based tagger. The results indicate
that our approach with minimal supervision pro-
duces dictionaries that are comparable to dictio-
naries compiled manually. Finally, we also com-
pare the quality of the candidate phrase embed-
dings with word embeddings (Dhillon et al., 2011)
by adding them as features in a CRF based se-
quence tagger.
2 Background
We first give background on Canonical Correla-
tion Analysis (CCA), and then give background on
452
CRFs for the NER problem.
2.1 Canonical Correlation Analysis (CCA)
The input to CCA consists of n paired observa-
tions (x
1
, z
1
), . . . , (x
n
, z
n
) where x
i
? R
d
1
, z
i
?
R
d
2
(?i ? {1, 2, . . . , n}) are the feature represen-
tations for the two views of a data point. CCA
simultaneously learns projection matrices ?
1
?
R
d
1
?k
,?
2
? R
d
2
?k
(k is a small number) which
are used to obtain the lower dimensional represen-
tations (x?
1
, z?
1
), . . . , (x?
n
, z?
n
) where x?
i
= ?
T
1
x
i
?
R
k
, z?
i
= ?
T
2
z
i
? R
k
, ?i ? {1, 2, . . . , n}. ?
1
,?
2
are chosen to maximize the correlation between x?
i
and z?
i
, ?i ? {1, 2, . . . , n}.
Consider the setting where we have a label for
the data point along with it?s two views and ei-
ther view is sufficient to make accurate predic-
tions. Kakade and Foster (2007) and Sridharan
and Kakade (2008) give strong theoretical guaran-
tees when the lower dimensional embeddings from
CCA are used for predicting the label of the data
point. This setting is similar to the one considered
in co-training (Collins and Singer, 1999) but there
is no assumption of independence between the two
views of the data point. Also, it is an exact al-
gorithm unlike the algorithm given in Collins and
Singer (1999). Since we are using lower dimen-
sional embeddings of the data point for prediction,
we can learn a predictor with fewer labeled exam-
ples.
2.2 CRFs for Named Entity Recognition
CRF based sequence taggers have been used for
a number of NER tasks (e.g., McCallum and Li,
2003) and in particular for biomedical NER (e.g.,
McDonald and Pereira, 2005; Burr Settles, 2004)
because they allow a great deal of flexibility in the
features which can be included. The input to a
CRF tagger is a sentence (w
1
, w
2
, . . . , w
n
) where
w
i
, ?i ? {1, 2, . . . , n} are words in the sentence.
The output is a sequence of tags y
1
, y
2
, . . . , y
n
where y
i
? {B, I, O}, ?i ? {1, 2, . . . , n}. B
is the tag given to the first word in a named entity,
I is the tag given to all words except the first word
in a named entity and O is the tag given to all other
words. We used the standard NER baseline fea-
tures (e.g., Dhillon et al., 2011; Ratinov and Roth,
2009) which include:
? Current Word w
i
and its lexical features
which include whether the word is capital-
ized and whether all the characters are cap-
italized. Prefix and suffixes of the word w
i
were also added.
? Word tokens in window of size two
around the current word which include
w
i?2
, w
i?1
, w
i+1
, w
i+2
and also the capital-
ization pattern in the window.
? Previous two predictions y
i?1
and y
i?2
.
The effectiveness of the dictionaries are evaluated
by adding dictionary matches as features along
with the baseline features (Ratinov and Roth,
2009; Cohen and Sarawagi, 2004) in the CRF tag-
ger. We also compared the quality of the candi-
date phrase embeddings with the word-level em-
beddings by adding them as features (Dhillon et
al., 2011) along with the baseline features in the
CRF tagger.
3 Method
This section describes the two steps in our ap-
proach: obtaining candidate phrases and classify-
ing them.
3.1 Obtaining Candidate Phrases
We used the full text of 110,369 biomedical pub-
lications in the BioMed Central corpus
1
to get the
high recall, low precision list of candidate phrases.
The advantages of using this huge collection of
publications are obvious: almost all (including
rare) named entities related to the biomedical do-
main will be mentioned and contains more re-
cent developments than a structured resource like
Wikipedia. The challenge however is that these
publications are unstructured and hence it is a dif-
ficult task to construct accurate dictionaries using
them with minimal supervision.
The list of virus candidate phrases were ob-
tained by extracting phrases that occur between
?the? and ?virus? in the simple pattern ?the ...
virus? during a single pass over the unlabeled doc-
ument collection. This noisy list had a lot of virus
names such as influenza, human immunodeficiency
and Epstein-Barr along with phrases that are not
virus names, like mutant, same, new, and so on.
A similar rule like ?the ... disease? did not give
a good coverage of disease names since it is not
the common way of how diseases are mentioned
in publications. So we took a different approach
1
The corpus can be downloaded at
http://www.biomedcentral.com/about/datamining
453
to obtain the noisy list of disease names. We col-
lected every sentence in the unlabeled data col-
lection that has the word ?disease? in it and ex-
tracted noun phrases
2
following the patterns ?dis-
eases like ....?, ?diseases such as ....? , ?diseases in-
cluding ....? , ?diagnosed with ....?, ?patients with
....? and ?suffering from ....?.
3.2 Classification of Candidate Phrases
Having found the list of candidate phrases, we
now describe how noisy words are filtered out
from them. We gather (spelling, context) pairs for
every instance of a candidate phrase in the unla-
beled data collection. spelling refers to the can-
didate phrase itself while context includes three
words each to the left and the right of the candidate
phrase in the sentence. The spelling and the con-
text of the candidate phrase provide a natural split
into two views which multi-view algorithms like
co-training and CCA can exploit. The only super-
vision in our method is to provide a few spelling
seed examples (10 in the case of virus, 18 in the
case of disease), for example, human immunodefi-
ciency is a virus and mutant is not a virus.
3.2.1 Approach using CCA embeddings
We use CCA described in the previous section
to obtain lower dimensional embeddings for the
candidate phrases using the (spelling, context)
views. Unlike previous works such as Dhillon et
al. (2011) and Dhillon et al. (2012), we use CCA to
learn embeddings for candidate phrases instead of
all words in the vocabulary so that we don?t miss
named entities which have two or more words.
Let the number of (spelling, context) pairs be n
(sum of total number of instances of every can-
didate phrase in the unlabeled data collection).
First, we map the spelling and context to high-
dimensional feature vectors. For the spelling view,
we define a feature for every candidate phrase and
also a boolean feature which indicates whether the
phrase is capitalized or not. For the context view,
we use features similar to Dhillon et al. (2011)
where a feature for every word in the context in
conjunction with its position is defined. Each
of the n (spelling, context) pairs are mapped to
a pair of high-dimensional feature vectors to get
n paired observations (x
1
, z
1
), . . . , (x
n
, z
n
) with
x
i
? R
d
1
, z
i
? R
d
2
, ?i ? {1, 2, . . . , n} (d
1
, d
2
are the feature space dimensions of the spelling
2
Noun phrases were obtained using
http://www.umiacs.umd.edu/ hal/TagChunk/
and context view respectively). Using CCA
3
, we
learn the projection matrices ?
1
? R
d
1
?k
,?
2
?
R
d
2
?k
(k << d
1
and k << d
2
) and obtain
spelling view projections x?
i
= ?
T
1
x
i
? R
k
, ?i ?
{1, 2, . . . , n}. The k-dimensional spelling view
projection of any instance of a candidate phrase
is used as it?s embedding
4
.
The k-dimensional candidate phrase embed-
dings are used as features to learn a binary SVM
with the seed spelling examples given in figure 1
as training data. The binary SVM predicts whether
a candidate phrase is a named entity or not. Since
the value of k is small, a small number of labeled
examples are sufficient to train an accurate clas-
sifier. The learned SVM is used to filter out the
noisy phrases from the list of candidate phrases
obtained in the previous step.
To summarize, our approach for classifying
candidate phrases has the following steps:
? Input: n (spelling, context) pairs, spelling
seed examples.
? Each of the n (spelling, context) pairs are
mapped to a pair of high-dimensional fea-
ture vectors to get n paired observations
(x
1
, z
1
), . . . , (x
n
, z
n
) with x
i
? R
d
1
, z
i
?
R
d
2
, ?i ? {1, 2, . . . , n}.
? Using CCA, we learn the projection matri-
ces ?
1
? R
d
1
?k
,?
2
? R
d
2
?k
and ob-
tain spelling view projections x?
i
= ?
T
1
x
i
?
R
k
,?i ? {1, 2, . . . , n}.
? The embedding of a candidate phrase is given
by the k-dimensional spelling view projec-
tion of any instance of the candidate phrase.
? We learn a binary SVM with the candi-
date phrase embeddings as features and the
spelling seed examples given in figure 1 as
training data. Using this SVM, we predict
whether a candidate phrase is a named entity
or not.
3.2.2 Approach based on Co-training
We discuss here briefly the DL-CoTrain algorithm
(Collins and Singer, 1999) which is based on co-
training (Blum and Mitchell, 1998), to classify
3
Similar to Dhillon et al. (2012) we used the method given
in Halko et al. (2011) to perform the SVD computation in
CCA for practical considerations.
4
Note that a candidate phrase gets the same spelling view
projection across it?s different instances since the spelling
features of a candidate phrase are identical across it?s in-
stances.
454
? Virus seed spelling examples
? Virus Names: human immunodeficiency, hepatitis C, influenza, Epstein-Barr, hepatitis B
? Non-virus Names: mutant, same, wild type, parental, recombinant
? Disease seed spelling examples
? Disease Names: tumor, malaria, breast cancer, cancer, IDDM, DM, A-T, tumors, VHL
? Non-disease Names: cells, patients, study, data, expression, breast, BRCA1, protein, mutant
1
Figure 1: Seed spelling examples
candidate phrases. We compare our approach us-
ing CCA embeddings with this approach. Here,
two decision list of rules are learned simultane-
ously one using the spelling view and the other
using the context view. The rules using the
spelling view are of the form: full-string=human
immunodeficiency?Virus, full-string=mutant?
Not a virus and so on. In the context view, we
used bigram
5
rules where we considered all pos-
sible bigrams using the context. The rules are of
two types: one which gives a positive label, for
example, full-string=human immunodeficiency?
Virus and the other which gives a negative label,
for example, full-string=mutant ? Not a virus.
The DL-CoTrain algorithm is as follows:
? Input: (spelling, context) pairs for every in-
stance of a candidate phrase in the corpus, m
specifying the number of rules to be added in
every iteration, precision threshold , spelling
seed examples.
? Algorithm:
1. Initialize the spelling decision list using
the spelling seed examples given in fig-
ure 1 and set i = 1.
2. Label the entire input collection using the
learned decision list of spelling rules.
3. Add i ? m new context rules of each
type to the decision list of context rules
using the current labeled data. The
rules are added using the same criterion
as given in Collins and Singer (1999),
i.e., among the rules whose strength is
greater than the precision threshold ,
the ones which are seen more often with
the corresponding label in the input data
collection are added.
5
We tried using unigram rules but they were very weak
predictors and the performance of the algorithm was poor
when they were considered.
4. Label the entire input collection using the
learned decision list of context rules.
5. Add i ? m new spelling rules of each
type to the decision list of spelling rules
using the current labeled data. The rules
are added using the same criterion as in
step 3. Set i = i+1. If rules were added
in the previous iteration, return to step 2.
The algorithm is run until no new rules are left to
be added. The spelling decision list along with
its strength (Collins and Singer, 1999) is used to
construct the dictionaries. The phrases present in
the spelling rules which give a positive label and
whose strength is greater than the precision thresh-
old, were added to the dictionary of named enti-
ties. We found the parameters m and  difficult
to tune and they could significantly affect the per-
formance of the algorithm. We give more details
regarding this in the experiments section.
4 Related Work
Previously, Collins and Singer (1999) introduced
a multi-view, semi-supervised algorithm based on
co-training (Blum and Mitchell, 1998) for collect-
ing names of people, organizations and locations.
This algorithm makes a strong independence as-
sumption about the data and employs many heuris-
tics to greedily optimize an objective function.
This greedy approach also introduces new param-
eters that are often difficult to tune.
In other works such as Toral and Mu?noz (2006)
and Kazama and Torisawa (2007) external struc-
tured resources like Wikipedia have been used to
construct dictionaries. Even though these meth-
ods are fairly successful they suffer from a num-
ber of drawbacks especially in the biomedical do-
main. The main drawback of these approaches is
that it is very difficult to accurately disambiguate
ambiguous entities especially when the entities are
455
abbreviations (Kazama and Torisawa, 2007). For
example, DM is the abbreviation for the disease
Diabetes Mellitus and the disambiguation page for
DM in Wikipedia associates it to more than 50 cat-
egories since DM can be expanded to Doctor of
Management, Dichroic mirror, and so on, each of
it belonging to a different category. Due to the
rapid growth of Wikipedia, the number of enti-
ties that have disambiguation pages is growing fast
and it is increasingly difficult to retrieve the article
we want. Also, it is tough to understand these ap-
proaches from a theoretical standpoint.
Dhillon et al. (2011) used CCA to learn word
embeddings and added them as features in a se-
quence tagger. They show that CCA learns bet-
ter word embeddings than CW embeddings (Col-
lobert and Weston , 2008), Hierarchical log-linear
(HLBL) embeddings (Mnih and Hinton, 2007)
and embeddings learned from many other tech-
niques for NER and chunking. Unlike PCA, a
widely used dimensionality reduction technique,
CCA is invariant to linear transformations of the
data. Our approach is motivated by the theoreti-
cal result in Kakade and Foster (2007) which is
developed in the co-training setting. We directly
use the CCA embeddings to predict the label of
a data point instead of using them as features in
a sequence tagger. Also, we learn CCA embed-
dings for candidate phrases instead of all words in
the vocabulary since named entities often contain
more than one word. Dhillon et al. (2012) learn
a multi-class SVM using the CCA word embed-
dings to predict the POS tag of a word type. We
extend this technique to NER by learning a binary
SVM using the CCA embeddings of a high recall,
low precision list of candidate phrases to predict
whether a candidate phrase is a named entity or
not.
5 Experiments
In this section, we give experimental results on
virus and disease NER.
5.1 Data
The noisy lists of both virus and disease names
were obtained from the BioMed Central corpus.
This corpus was also used to get the collection of
(spelling, context) pairs which are the input to the
CCA procedure and the DL-CoTrain algorithm de-
scribed in the previous section. We obtained CCA
embeddings for the 100, 000 most frequently oc-
curring word types in this collection along with
every word type present in the training and de-
velopment data of the virus and the disease NER
dataset. These word embeddings are similar to the
ones described in Dhillon et al. (2011) and Dhillon
et al. (2012).
We used the virus annotations in the GE-
NIA corpus (GENIA, 2003) for our experiments.
The dataset contains 18,546 annotated sentences.
We randomly selected 8,546 sentences for train-
ing and the remaining sentences were randomly
split equally into development and testing sen-
tences. The training sentences are used only for
experiments with the sequence taggers. Previ-
ously, Zhang et al. (2004) tested their HMM-based
named entity recognizer on this data. For disease
NER, we used the recent disease corpus (Dogan
and Lu, 2012) and used the same training, devel-
opment and test data split given by them. We used
a sentence segmenter
6
to get sentence segmented
data and Stanford Tokenizer
7
to tokenize the data.
Similar to Dogan and Lu (2012), all the different
disease categories were flattened into one single
category of disease mentions. The development
data was used to tune the hyperparameters and the
methods were evaluated on the test data.
5.2 Results using a dictionary-based tagger
First, we compare the dictionaries compiled us-
ing different methods by using them directly in
a dictionary-based tagger. This is a simple and
informative way to understand the quality of the
dictionaries before using them in a CRF-tagger.
Since these taggers can be trained using a hand-
ful of training examples, we can use them to build
NER systems even when there are no labeled sen-
tences to train. The input to a dictionary tagger is
a list of named entities and a sentence. If there is
an exact match between a phrase in the input list
to the words in the given sentence then it is tagged
as a named entity. All other words are labeled as
non-entities. We evaluated the performance of the
following methods for building dictionaries:
? Candidate List: This dictionary contains all
the candidate phrases that were obtained us-
ing the method described in Section 3.1. The
noisy list of virus candidates and disease can-
didates had 3,100 and 60,080 entries respec-
tively.
6
https://pypi.python.org/pypi/text-sentence/0.13
7
http://nlp.stanford.edu/software/tokenizer.shtml
456
Method
Virus NER Disease NER
Precision Recall F-1 Score Precision Recall F-1 Score
Candidate List 2.20 69.58 4.27 4.86 60.32 8.99
Manual 42.69 68.75 52.67 51.39 45.08 48.03
Co-Training 48.33 66.46 55.96 58.87 23.17 33.26
CCA 57.24 68.33 62.30 38.34 44.55 41.21
Table 1: Precision, recall, F- 1 scores of dictionary-based taggers
? Manual: Manually constructed dictionaries,
which requires a large amount of human ef-
fort, are employed for the task. We used the
list of virus names given in Wikipedia
8
. Un-
fortunately, abbreviations of virus names are
not present in this list and we could not find
any other more complete list of virus names.
Hence, we constructed abbreviations by con-
catenating the first letters of all the strings in
a virus name, for every virus name given in
the Wikipedia list.
For diseases, we used the list of disease
names given in the Unified Medical Lan-
guage System (UMLS) Metathesaurus. This
dictionary has been widely used in disease
NER (e.g., Dogan and Lu, 2012; Leaman et
al., 2010)
9
.
? Co-Training: The dictionaries are con-
structed using the DL-CoTrain algorithm de-
scribed previously. The parameters used
were m = 5 and  = 0.95 as given in Collins
and Singer (1999). The phrases present in
the spelling rules which give a positive label
and whose strength is greater than the preci-
sion threshold, were added to the dictionary
of named entities.
In our experiment to construct a dictionary
of virus names, the algorithm stopped after
just 12 iterations and hence the dictionary had
only 390 virus names. This was because there
were no spelling rules with strength greater
than 0.95 to be added. We tried varying
both the parameters but in all cases, the algo-
rithm did not progress after a few iterations.
We adopted a simple heuristic to increase the
coverage of virus names by using the strength
of the spelling rules obtained after the 12
th
it-
eration. All spelling rules that give a positive
8
http://en.wikipedia.org/wiki/List of viruses
9
The list of disease names from UMLS can be found at
https://sites.google.com/site/fmchowdhury2/bioenex .
label and which has a strength greater than
? were added to the decision list of spelling
rules. The phrases present in these rules are
added to the dictionary. We picked the ? pa-
rameter from the set [0.1, 0.2, 0.3, 0.4, 0.5,
0.6, 0.7, 0.8, 0.9] using the development data.
The co-training algorithm for constructing
the dictionary of disease names ran for close
to 50 iterations and hence we obtained bet-
ter coverage for disease names. We still used
the same heuristic of adding more named en-
tities using the strength of the rule since it
performed better.
? CCA: Using the CCA embeddings of the
candidate phrases
10
as features we learned a
binary SVM
11
to predict whether a candidate
phrase is a named entity or not. We consid-
ered using 10 to 30 dimensions of candidate
phrase embeddings and the regularizer was
picked from the set [0.0001, 0.001, 0.01, 0.1,
1, 10, 100]. Both the regularizer and the num-
ber of dimensions to be used were tuned us-
ing the development data.
Table 1 gives the results of the dictionary based
taggers using the different methods described
above. As expected, when the noisy list of candi-
date phrases are used as dictionaries the recall of
the system is quite high but the precision is very
low. The low precision of the Wikipedia virus
lists was due to the heuristic used to obtain ab-
breviations which produced a few noisy abbrevia-
tions but this heuristic was crucial to get a high re-
call. The list of disease names from UMLS gives
a low recall because the list does not contain many
disease abbreviations and composite disease men-
tions such as breast and ovarian cancer. The pres-
10
The performance of the dictionaries learned from word
embeddings was very poor and we do not report it?s perfor-
mance here.
11
we used LIBSVM (http://www.csie.ntu.edu.tw/ cjlin/libsvm/)
in our SVM experiments
457
0 1000 2000 3000 4000 5000 6000 7000 8000 90000.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
Number of Training Sentences
F?1
 Sco
re
Virus NER
 
 
baseline
manual
co?training
cca
0 1000 2000 3000 4000 5000 60000.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
F?1
 Sco
re
Number of Training Sentences
Disease NER
 
 
baseline
manual
co?training
cca
1
Figure 2: Virus and Disease NER F-1 scores for varying training data size when dictionaries obtained
from different methods are injected
ence of ambiguous abbreviations affected the ac-
curacy of this dictionary.
The virus dictionary constructed using the CCA
embeddings was very accurate and the false pos-
itives were mainly due to ambiguous phrases,
for example, in the phrase HIV replication, HIV
which usually refers to the name of a virus is
tagged as a RNA molecule. The accuracy of the
disease dictionary produced using CCA embed-
dings was mainly affected by noisy abbreviations.
We can see that the dictionaries obtained us-
ing CCA embeddings perform better than the dic-
tionaries obtained from co-training on both dis-
ease and virus NER even after improving the co-
training algorithm?s coverage using the heuristic
described in this section. It is important to note
that the dictionaries constructed using the CCA
embeddings and a small number of labeled exam-
ples performs competitively with dictionaries that
are entirely built by domain experts. These re-
sults show that by using the CCA based approach
we can build NER systems that give reasonable
performance even for difficult named entity types
with almost no supervision.
5.3 Results using a CRF tagger
We did two sets of experiments using a CRF tag-
ger. In the first experiment, we add dictionary fea-
tures to the CRF tagger while in the second ex-
periment we add the embeddings as features to the
CRF tagger. The same baseline model is used in
both the experiments whose features are described
in Section 2.2. For both the CRF
12
experiments
the regularizers from the set [0.0001, 0.001, 0.01,
0.1, 1.0, 10.0] were considered and it was tuned
on the development set.
5.3.1 Dictionary Features
Here, we inject dictionary matches as features
(e.g., Ratinov and Roth, 2009; Cohen and
Sarawagi, 2004) in the CRF tagger. Given a dic-
tionary of named entities, every word in the input
sentence has a dictionary feature associated with
it. When there is an exact match between a phrase
in the dictionary with the words in the input sen-
tence, the dictionary feature of the first word in
the named entity is set to B and the dictionary fea-
ture of the remaining words in the named entity
is set to I. The dictionary feature of all the other
words in the input sentence which are not part of
any named entity in the dictionary is set to O. The
effectiveness of the dictionaries constructed from
various methods are compared by adding dictio-
nary match features to the CRF tagger. These dic-
tionary match features were added along with the
baseline features.
Figure 2 indicates that the dictionary features in
general are helpful to the CRF model. We can see
that the dictionaries produced from our approach
using CCA are much more helpful than the dictio-
naries produced from co-training especially when
there are fewer labeled sentences to train. Simi-
lar to the dictionary tagger experiments discussed
12
We used CRFsuite (www.chokkan.org/software/crfsuite/)
for our experiments with CRFs.
458
0 1000 2000 3000 4000 5000 6000 7000 8000 90000.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
F?1
 Sco
re
Number of Training Sentences
Virus NER
 
 
baseline
cca?word
cca?phrase
0 1000 2000 3000 4000 5000 60000.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
Number of Training Sentences
F?1
 Sco
re
Disease NER
 
 
baseline
cca?word
cca?phrase
1
Figure 3: Virus and Disease NER F-1 scores for varying training data size when embeddings obtained
from different methods are used as features
previously, the dictionaries produced from our ap-
proach performs competitively with dictionaries
that are entirely built by domain experts.
5.3.2 Embedding Features
The quality of the candidate phrase embeddings
are compared with word embeddings by adding
the embeddings as features in the CRF tagger.
Along with the baseline features, CCA-word
model adds word embeddings as features while the
CCA-phrase model adds candidate phrase em-
beddings as features. CCA-word model is similar
to the one used in Dhillon et al. (2011).
We considered adding 10, 20, 30, 40 and 50 di-
mensional word embeddings as features for every
training data size and the best performing model
on the development data was picked for the exper-
iments on the test data. For candidate phrase em-
beddings we used the same number of dimensions
that was used for training the SVMs to construct
the best dictionary.
When candidate phrase embeddings are ob-
tained using CCA, we do not have embeddings
for words which are not in the list of candidate
phrases. Also, a candidate phrase having more
than one word has a joint representation, i.e., the
phrase ?human immunodeficiency? has a lower
dimensional representation while the words ?hu-
man? and ?immunodeficiency? do not have their
own lower dimensional representations (assuming
they are not part of the candidate list). To over-
come this issue, we used a simple technique to dif-
ferentiate between candidate phrases and the rest
of the words. Let x be the highest real valued can-
didate phrase embedding and the candidate phrase
embedding be a d dimensional real valued vector.
If a candidate phrase occurs in a sentence, the em-
beddings of that candidate phrase are added as fea-
tures to the first word of that candidate phrase. If
the candidate phrase has more than one word, the
other words in the candidate phrase are given an
embedding of dimension d with each dimension
having the value 2 ? x. All the other words are
given an embedding of dimension d with each di-
mension having the value 4? x.
Figure 3 shows that almost always the candi-
date phrase embeddings help the CRF model. It is
also interesting to note that sometimes the word-
level embeddings have an adverse affect on the
performance of the CRF model. The CCA-phrase
model performs significantly better than the other
two models when there are fewer labeled sen-
tences to train and the separation of the candidate
phrases from the other words seems to have helped
the CRF model.
6 Conclusion
We described an approach for automatic construc-
tion of dictionaries for NER using minimal super-
vision. Compared to the previous approaches, our
method is free from overly-stringent assumptions
about the data, uses SVD that can be solved ex-
actly and achieves better empirical performance.
Our approach which uses a small number of seed
examples performs competitively with dictionar-
ies that are compiled manually.
459
Acknowledgments
We are grateful to Alexander Rush, Alexandre
Passos and the anonymous reviewers for their
useful feedback. This work was supported by
the Intelligence Advanced Research Projects Ac-
tivity (IARPA) via Department of Interior Na-
tional Business Center (DoI/NBC) contract num-
ber D11PC20153. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copy-
right annotation thereon. The views and conclu-
sions contained herein are those of the authors and
should not be interpreted as necessarily represent-
ing the official policies or endorsements, either ex-
pressed or implied, of IARPA, DoI/NBC, or the
U.S. Government.
References
Andrew McCallum and Wei Li. Early Results for
Named Entity Recognition with Conditional Ran-
dom Fields, Feature Induction and Web-Enhanced
Lexicons. 2003. Conference on Natural Language
Learning (CoNLL).
Andriy Mnih and Geoffrey Hinton. Three New Graph-
ical Models for Statistical Language Modelling.
2007. International Conference on Machine learn-
ing (ICML).
Antonio Toral and Rafael Mu?noz. A proposal to auto-
matically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia. 2006.
Workshop On New Text Wikis And Blogs And
Other Dynamic Text Sources.
Avrin Blum and Tom M. Mitchell. Combining Labeled
and Unlabeled Data with Co-Training. 1998. Con-
ference on Learning Theory (COLT).
Burr Settles. Biomedical Named Entity Recognition
Using Conditional Random Fields and Rich Feature
Sets. 2004. International Joint Workshop on Natural
Language Processing in Biomedicine and its Appli-
cations (NLPBA).
H. Hotelling. Canonical correlation analysis (cca)
1935. Journal of Educational Psychology.
Jie Zhang, Dan Shen, Guodong Zhou, Jian Su and
Chew-Lim Tan. Enhancing HMM-based Biomed-
ical Named Entity Recognition by Studying Special
Phenomena. 2004. Journal of Biomedical Informat-
ics.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi and
Jun?ichi Tsujii. GENIA corpus - a semantically an-
notated corpus for bio-textmining. 2003. ISMB.
Junichi Kazama and Kentaro Torisawa. Exploiting
Wikipedia as External Knowledge for Named Entity
Recognition. 2007. Association for Computational
Linguistics (ACL).
Karthik Sridharan and Sham M. Kakade. An Informa-
tion Theoretic Framework for Multi-view Learning.
2008. Conference on Learning Theory (COLT).
Lev Ratinov and Dan Roth. Design Challenges
and Misconceptions in Named Entity Recognition.
2009. Conference on Natural Language Learning
(CoNLL).
Michael Collins and Yoram Singer. Unsupervised
Models for Named Entity Classification. 1999. In
Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora.
Nathan Halko, Per-Gunnar Martinsson, Joel A. Tropp.
Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix de-
compositions. 2011. Society for Industrial and Ap-
plied Mathematics.
Paramveer S. Dhillon, Dean Foster and Lyle Ungar.
Multi-View Learning of Word Embeddings via CCA.
2011. Advances in Neural Information Processing
Systems (NIPS).
Paramveer Dhillon, Jordan Rodu, Dean Foster and Lyle
Ungar. Two Step CCA: A new spectral method for
estimating vector models of words. 2012. Interna-
tional Conference on Machine learning (ICML).
Rezarta Islamaj Dogan and Zhiyong Lu. An improved
corpus of disease mentions in PubMed citations.
2012. Workshop on Biomedical Natural Language
Processing, Association for Computational Linguis-
tics (ACL).
Robert Leaman, Christopher Miller and Graciela Gon-
zalez. Enabling Recognition of Diseases in Biomed-
ical Text with Machine Learning: Corpus and
Benchmark. 2010. Workshop on Biomedical Nat-
ural Language Processing, Association for Compu-
tational Linguistics (ACL).
Ronan Collobert and Jason Weston. A unified architec-
ture for natural language processing: deep neural
networks with multitask learning. 2008. Interna-
tional Conference on Machine learning (ICML).
Ryan McDonald and Fernando Pereira. Identifying
Gene and Protein Mentions in Text Using Condi-
tional Random Fields. 2005. BMC Bioinformatics.
Sham M. Kakade and Dean P. Foster. Multi-view re-
gression via canonical correlation analysis. 2007.
Conference on Learning Theory (COLT).
William W. Cohen and Sunita Sarawagi. Exploiting
Dictionaries in Named Entity Extraction: Combin-
ing Semi-Markov Extraction Processes and Data In-
tegration Methods. 2004. Semi-Markov Extraction
460
Processes and Data Integration Methods, Proceed-
ings of KDD.
461
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 180?184,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Some Experiments with a Convex IBM Model 2
Andrei Simion
Columbia University
IEOR Department
New York, NY, 10027
aas2148@columbia.edu
Michael Collins
Columbia University
Computer Science
New York, NY, 10027
mc3354@columbia.edu
Clifford Stein
Columbia University
IEOR Department
New York, NY, 10027
cs2035@columbia.edu
Abstract
Using a recent convex formulation of IBM
Model 2, we propose a new initialization
scheme which has some favorable compar-
isons to the standard method of initializing
IBM Model 2 with IBM Model 1. Addition-
ally, we derive the Viterbi alignment for the
convex relaxation of IBM Model 2 and show
that it leads to better F-Measure scores than
those of IBM Model 2.
1 Introduction
The IBM translation models are widely used in
modern statistical translation systems. Unfortu-
nately, apart from Model 1, the IBM models lead
to non-convex objective functions, leading to meth-
ods (such as EM) which are not guaranteed to reach
the global maximum of the log-likelihood function.
In a recent paper, Simion et al. introduced a con-
vex relaxation of IBM Model 2, I2CR-2, and showed
that it has performance on par with the standard IBM
Model 2 (Simion et al., 2013).
In this paper we make the following contributions:
? We explore some applications of I2CR-2. In
particular, we show how this model can be
used to seed IBM Model 2 and compare the
speed/performance gains of our initialization
under various settings. We show that initializ-
ing IBM Model 2 with a version of I2CR-2 that
uses large batch size yields a method that has
similar run time to IBM Model 1 initialization
and at times has better performance.
? We derive the Viterbi alignment for I2CR-2 and
compare it directly with that of IBM Model
2. Previously, Simion et al. (2013) had com-
pared IBM Model 2 and I2CR-2 by using IBM
Model 2?s Viterbi alignment rule, which is not
necessarily the optimal alignment for I2CR-2.
We show that by comparing I2CR-2 with IBM
Model 2 by using each model?s optimal Viterbi
alignment the convex model consistently has a
higher F-Measure. F-Measure is an important
metric because it has been shown to be corre-
lated with BLEU scores (Marcu et al., 2006).
Notation. We adopt the notation introduced in
(Och and Ney, 2003) of having 1
m
2
n
denote the
training scheme of m IBM Model 1 EM iterations
followed by initializing Model 2 with these parame-
ters and running n IBM Model 2 EM iterations. The
notation EG
m
B
2
n
means that we run m iterations of
I2CR-2?s EG algorithm (Simion et al., 2013) with
batch size of B, initialize IBM Model 2 with I2CR-
2?s parameters, and then run n iterations of Model
2?s EM.
2 The IBM Model 1 and 2 Optimization
Problems
In this section we give a brief review of IBM Mod-
els 1 and 2 and the convex relaxation of Model 2,
I2CR-2 (Simion et al., 2013). The standard ap-
proach in training parameters for Models 1 and 2 is
EM, whereas for I2CR-2 an exponentiated-gradient
(EG) algorithm was developed (Simion et al., 2013).
We assume that our set of training examples is
(e
(k)
, f
(k)
) for k = 1 . . . n, where e
(k)
is the k?th
English sentence and f
(k)
is the k?th French sen-
tence. The k?th English sentence is a sequence of
words e
(k)
1
. . . e
(k)
l
k
where l
k
is the length of the k?th
English sentence, and each e
(k)
i
? E; similarly
the k?th French sentence is a sequence f
(k)
1
. . . f
(k)
m
k
where each f
(k)
j
? F . We define e
(k)
0
for k = 1 . . . n
to be a special NULL word (note that E contains the
NULL word). IBM Model 2 is detailed in several
sources such as (Simion et al., 2013) and (Koehn,
2004).
The convex and non-convex objectives of respec-
tively IBM Model 1 and 2 can be found in (Simion
180
et al., 2013). For I2CR-2, the convex relaxation of
IBM Model 2, the objective is given by
1
2n
n
?
k=1
m
k
?
j=1
log
?
l
k
?
i=0
t(f
(k)
j
|e
(k)
i
)
(L+ 1)
+
1
2n
n
?
k=1
m
k
?
j=1
log
?
l
k
?
i=0
min{t(f
(k)
j
|e
(k)
i
), d(i|j)} .
For smoothness reasons, Simion et al. (2013) de-
fined log
?
(z) = log(z + ?) where ? = .001 is a
small positive constant. The I2CR-2 objective is a
convex combination of the convex IBM Model 1 ob-
jective and a direct (convex) relaxation of the IBM2
Model 2 objective, and hence is itself convex.
3 The Viterbi Alignment for I2CR-2
Alignment models have been compared using meth-
ods other than Viterbi comparisons; for example,
Simion et al. (2013) use IBM Model 2?s optimal
rule given by (see below) Eq. 2 to compare mod-
els while Liang et al. (2006) use posterior de-
coding. Here, we derive and use I2CR-2?s Viterbi
alignment. To get the Viterbi alignment of a pair
(e
(k)
, f
(k)
) using I2CR-2 we need to find a
(k)
=
(a
(k)
1
, . . . , a
(k)
m
k
) which yields the highest probability
p(f
(k)
, a
(k)
|e
(k)
).Referring to the I2CR-2 objective,
this corresponds to finding a
(k)
that maximizes
log
?
m
k
j=1
t(f
(k)
j
|e
(k)
a
(k)
j
)
2
+
log
?
m
k
j=1
min {t(f
(k)
j
|e
(k)
a
(k)
j
), d(a
(k)
j
|j)}
2
.
Putting the above terms together and using the
monotonicity of the logarithm, the above reduces to
finding the vector a
(k)
which maximizes
m
k
?
j=1
t(f
(k)
j
|e
(k)
a
(k)
j
)min {t(f
(k)
j
|e
(k)
a
(k)
j
), d(a
(k)
j
|j)}.
As with IBM Models 1 and 2, we can find the vector
a
(k)
by splitting the maximization over the compo-
nents of a
(k)
and focusing on finding a
(k)
j
given by
argmax
a
(t(f
(k)
j
|e
(k)
a
)min {t(f
(k)
j
|e
(k)
a
), d(a|j)}) . (1)
In previous experiments, Simion et al. (Simion et
al., 2013) were comparing I2CR-2 and IBM Model
2 using the standard alignment formula derived in a
similar fashion from IBM Model 2:
a
(k)
j
= argmax
a
(t(f
(k)
j
|e
(k)
a
)d(a|j)) . (2)
4 Experiments
In this section we describe experiments using the
I2CR-2 optimization problem combined with the
stochastic EG algorithm (Simion et al., 2013) for pa-
rameter estimation. The experiments conducted here
use a similar setup to those in (Simion et al., 2013).
We first describe the data we use, and then describe
the experiments we ran.
4.1 Data Sets
We use data from the bilingual word alignment
workshop held at HLT-NAACL 2003 (Michalcea
and Pederson, 2003). We use the Canadian Hansards
bilingual corpus, with 247,878 English-French sen-
tence pairs as training data, 37 sentences of devel-
opment data, and 447 sentences of test data (note
that we use a randomly chosen subset of the orig-
inal training set of 1.1 million sentences, similar to
the setting used in (Moore, 2004)). The development
and test data have been manually aligned at the word
level, annotating alignments between source and tar-
get words in the corpus as either ?sure? (S) or ?pos-
sible? (P ) alignments, as described in (Och and Ney,
2003).
As a second data set, we used the Romanian-
English data from the HLT-NAACL 2003 workshop
consisting of a training set of 48,706 Romanian-
English sentence-pairs, a development set of 17 sen-
tence pairs, and a test set of 248 sentence pairs.
We carried out our analysis on this data set as
well, but because of space we only report the de-
tails on the Hansards data set. The results on the
Romanian data were similar, but the magnitude of
improvement was smaller.
4.2 Methodology
Our experiments make use of either standard train-
ing or intersection training (Och and Ney, 2003).
For standard training, we run a model in the source-
target direction and then derive the alignments on
the test or development data. For each of the
181
Training 2
10
1
5
2
10
EG
1
125
2
10
EG
1
1250
2
10
Iteration Objective
0 -224.0919 -144.2978 -91.2418 -101.2250
1 -110.6285 -85.6757 -83.3255 -85.5847
2 -91.7091 -82.5312 -81.3845 -82.1499
3 -84.8166 -81.3380 -80.6120 -80.9610
4 -82.0957 -80.7305 -80.2319 -80.4041
5 -80.9103 -80.3798 -80.0173 -80-1009
6 -80.3620 -80.1585 -79.8830 -79.9196
7 -80.0858 -80.0080 -79.7911 -79.8048
8 -79.9294 -79.9015 -79.7247 -79.7284
9 -79.8319 -79.8240 -79.6764 -79.6751
10 -79.7670 -79.7659 -79.6403 -79.6354
Table 1: Objective results for the English? French IBM
Model 2 seeded with either uniform parameters, IBM
Model 1 ran for 5 EM iterations, or I2CR-2 ran for 1 iter-
ation with either B = 125 or 1250. Iteration 0 denotes the
starting IBM 2 objective depending on the initialization.
models?IBM Model 1, IBM Model 2, and I2CR-
2? we apply the conventional methodology to in-
tersect alignments: first, we estimate the t and d
parameters using models in both source-target and
target-source directions; second, we find the most
likely alignment for each development or test data
sentence in each direction; third, we take the in-
tersection of the two alignments as the final output
from the model. For the I2CR-2 EG (Simion et al.,
2013) training, we use batch sizes of eitherB = 125
or B = 1250 and a step size of ? = 0.5 throughout.
We measure the performance of the models in
terms of Precision, Recall, F-Measure, and AER us-
ing only sure alignments in the definitions of the first
three metrics and sure and possible alignments in the
definition of AER, as in (Simion et al., 2013) and
(Marcu et al., 2006). For our experiments, we report
results in both AER (lower is better) and F-Measure
(higher is better).
4.3 Initialization and Timing Experiments
We first report the summary statistics on the test set
using a model trained only in the English-French di-
rection. In these experiments we seeded IBM Model
2?s parameters either with those of IBM Model 1 run
for 5, 10 or 15 EM iterations or I2CR-2 run for 1 it-
eration of EG with a batch size of either B = 125 or
1250. For uniform comparison, all of our implemen-
tations were written in C++ using STL/Boost con-
tainers.
There are several takeaways from our experi-
ments, which are presented in Table 2. We first note
that with B = 1250 we get higher F-Measure and
lower AER even though we use less training time: 5
iterations of IBM Model 1 EM training takes about
3.3 minutes, which is about the time it takes for 1 it-
eration of EG with a batch size of 125 (4.1 minutes);
on the other hand, using B = 1250 takes EG 1.7
minutes and produces the best results across almost
all iterations. Additionally, we note that the initial
solution given to IBM Model 2 by running I2CR-2
for 1 iteration with B = 1250 is fairly strong and
allows for further progress: IBM2 EM training im-
proves upon this solution during the first few iter-
ations. We also note that this behavior is global:
no IBM 1 initialization scheme produced subsequent
solutions for IBM 2 with as low in AER or high in
F-Measure. Finally, comparing Table 1 which lists
objective values with Table 2 which lists alignment
statistics, we see that although the objective progres-
sion is similar throughout, the alignment quality is
different.
To complement the above, we also ran inter-
section experiments. Seeding IBM Model 2 by
Model 1 and intersecting the alignments produced
by the English-French and French-English models
gave both AER and F-Measure which were better
than those that we obtained by any seeding of IBM
Model 2 with I2CR-2. However, there are still rea-
sons why I2CR-2 would be useful in this context. In
particular, we note that I2CR-2 takes roughly half
the time to progress to a better solution than IBM
Model 1 run for 5 EM iterations. Second, a possible
remedy to the above loss in marginal improvement
when taking intersections would be to use a more re-
fined method for obtaining the joint alignment of the
English-French and French-English models, such as
?grow-diagonal? (Och and Ney, 2003).
4.4 Viterbi Comparisons
For the decoding experiments, we used IBM Model
1 as a seed to Model 2. To train IBM Model 1, we
follow (Moore, 2004) and (Och and Ney, 2003) in
running EM for 5, 10 or 15 iterations. For the EG al-
gorithm, we initialize all parameters uniformly and
use 10 iterations of EG with a batch size of 125.
Given the lack of development data for the align-
ment data sets, for both IBM Model 2 and the I2CR-
2 method, we report test set F-Measure and AER re-
sults for each of the 10 iterations, rather than picking
the results from a single iteration.
182
Training 2
10
1
5
2
10
1
10
2
10
1
15
2
10
EG
1
125
2
10
EG
1
1250
2
10
Iteration AER
0 0.8713 0.3175 0.3177 0.3160 0.2329 0.2662
1 0.4491 0.2547 0.2507 0.2475 0.2351 0.2259
2 0.2938 0.2428 0.2399 0.2378 0.2321 0.2180
3 0.2593 0.2351 0.2338 0.2341 0.2309 0.2176
4 0.2464 0.2298 0.2305 0.2310 0.2283 0.2168
5 0.2383 0.2293 0.2299 0.2290 0.2268 0.2188
6 0.2350 0.2273 0.2285 0.2289 0.2274 0.2205
7 0.2320 0.2271 0.2265 0.2286 0.2274 0.2213
8 0.2393 0.2261 0.2251 0.2276 0.2278 0.2223
9 0.2293 0.2253 0.2246 0.2258 0.2284 0.2217
10 0.2288 0.2248 0.2249 0.2246 0.2275 0.2223
Iteration F-Measure
0 0.0427 0.5500 0.5468 0.5471 0.6072 0.5977
1 0.4088 0.5846 0.5876 0.5914 0.6005 0.6220
2 0.5480 0.5892 0.5916 0.5938 0.5981 0.6215
3 0.5750 0.5920 0.5938 0.5947 0.5960 0.6165
4 0.5814 0.5934 0.5839 0.5952 0.5955 0.6129
5 0.5860 0.5930 0.5933 0.5947 0.5945 0.6080
6 0.5873 0.5939 0.5936 0.5940 0.5924 0.6051
7 0.5884 0.5931 0.5955 0.5941 0.5913 0.6024
8 0.5899 0.5932 0.5961 0.5942 0.5906 0.6000
9 0.5899 0.5933 0.5961 0.5958 0.5906 0.5996
10 0.5897 0.5936 0.5954 0.5966 0.5910 0.5986
Table 2: Results on the Hansards data for English ?
French IBM Model 2 seeded using different methods.
The first three columns are for a model seeded with IBM
Model 1 ran for 5, 10 or 15 EM iterations. The fourth
and fifth columns show results when we seed with I2CR-
2 ran for 1 iteration either withB = 125 or 1250. Iteration
0 denotes the starting statistics.
Training 1
5
2
10
1
10
2
10
1
15
2
10
EG
10
125
EG
10
125
Viterbi Rule t? d t? d t? d t? d t?min{t? d}
Iteration AER
0 0.2141 0.2159 0.2146 0.9273 0.9273
1 0.1609 0.1566 0.1513 0.1530 0.1551
2 0.1531 0.1507 0.1493 0.1479 0.1463
3 0.1477 0.1471 0.1470 0.1473 0.1465
4 0.1458 0.1444 0.1449 0.1510 0.1482
5 0.1455 0.1438 0.1435 0.1501 0.1482
6 0.1436 0.1444 0.1429 0.1495 0.1481
7 0.1436 0.1426 0.1435 0.1494 0.1468
8 0.1449 0.1427 0.1437 0.1508 0.1489
9 0.1454 0.1426 0.1430 0.1509 0.1481
10 0.1451 0.1430 0.1423 0.1530 0.1484
Iteration F-Measure
0 0.7043 0.7012 0.7021 0.0482 0.0482
1 0.7424 0.7477 0.7534 0.7395 0.7507
2 0.7468 0.7499 0.7514 0.7448 0.7583
3 0.7489 0.7514 0.7520 0.7455 0.7585
4 0.7501 0.7520 0.7516 0.7418 0.7560
5 0.7495 0.7513 0.7522 0.7444 0.7567
6 0.7501 0.7501 0.7517 0.7452 0.7574
7 0.7493 0.7517 0.7507 0.7452 0.7580
8 0.7480 0.7520 0.7504 0.7452 0.7563
9 0.7473 0.7511 0.7513 0.7450 0.7590
10 0.7474 0.7505 0.7520 0.7430 0.7568
Table 3: Intersected results on the English-French data
for IBM Model 2 and I2CR-2 using either IBM Model 1
trained to 5, 10, or 15 EM iterations to seed IBM2 and us-
ing either the IBM2 or I2CR-2 Viterbi formula for I2CR-
2.
In Table 3 we report F-Measure and AER results
for each of the iterations under IBM Model 2 and
I2CR-2 models using either the Model 2 Viterbi rule
of Eq. 2 or I2CR-2?s Viterbi rule in Eq. 1. We
note that unlike in the previous experiments pre-
sented in (Simion et al., 2013), we are directly test-
ing the quality of the alignments produced by I2CR-
2 and IBM Model 2 since we are getting the Viterbi
alignment for each model (for completeness, we also
have included in the fourth column the Viterbi align-
ments we get by using the IBM Model 2 Viterbi for-
mula with the I2CR-2 parameters as Simion et al.
(2013) had done previously). For these experiments
we report intersection statistics. Under its proper
decoding formula, I2CR-2 model yields a higher F-
Measure than any setting of IBM Model 2. Since
AER and BLEU correlation is arguably known to be
weak while F-Measure is at times strongly related
with BLEU (Marcu et al., 2006), the above results
favor the convex model.
We close this section by pointing out that the main
difference between the IBM Model 2 Viterbi rule of
Eq. 2 and the I2CR-2 Viterbi rule in Eq. 1 is that
the Eq. 1 yield fewer alignments when doing inter-
section training. Even though there are fewer align-
ments produced, the quality in terms of F-Measure
is better.
5 Conclusions and Future Work
In this paper we have explored some of the details of
a convex formulation of IBM Model 2 and showed
it may have an application either as a new initial-
ization technique for IBM Model 2 or as a model
in its own right, especially if the F-Measure is the
target metric. Other possible topics of interest in-
clude performing efficient sensitivity analysis on the
I2CR-2 model, analyzing the balance between the
IBM Model 1 and I2CR-1 (Simion et al., 2013) com-
ponents of the I2CR-2 objective, studying I2CR-
2?s intersection training performance using methods
such as ?grow diagonal? or ?agreement? (Liang et
al., 2006), and integrating it into the GIZA++ open
source library so we can see how much it affects the
downstream system.
Acknowledgments
Michael Collins and Andrei Simion are partly sup-
ported by NSF grant IIS-1161814. Cliff Stein is
183
partly supported by NSF grants CCF-0915681 and
CCF-1349602. We thank Professor Paul Blaer and
Systems Engineer Radu Sadeanu for their help set-
ting up some of the hardware used for these experi-
ments. We also thank the anonymous reviewers for
many useful comments; we hope to pursue the com-
ments we were not able to address in a followup pa-
per.
References
Peter L. Bartlett, Ben Taskar, Michael Collins and David
Mcallester. 2004. Exponentiated Gradient Algorithms
for Large-Margin Structured Classification. In Pro-
ceedings of NIPS.
Steven Boyd and Lieven Vandenberghe. 2004. Convex
Optimization. Cambridge University Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19:263-311.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras and Peter L. Bartlett. 2008. Exponentiated
Gradient Algorithms for Conditional Random Fields
and Max-Margin Markov Networks. Journal Machine
Learning, 9(Aug): 1775-1822.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood From Incomplete Data via the
EM Algorithm. Journal of the royal statistical society,
series B, 39(1):1-38.
Alexander Fraser and Daniel Marcu. 2007. Measur-
ing Word Alignment Quality for Statistical Ma-
chine Translation. Journal Computational Linguistics,
33(3): 293-303.
Joao V. Graca, Kuzman Ganchev and Ben Taskar. 2007.
Expectation Maximization and Posterior Constraints.
In Proceedings of NIPS.
Yuhong Guo and Dale Schuurmans. 2007. Convex Re-
laxations of Latent Variable Training. In Proceedings
of NIPS.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2008. Word Alignment via Quadratic
Assignment. In Proceedings of the HLT-NAACL.
Phillip Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of the
EMNLP.
Phillip Koehn. 2008. Statistical Machine Translation.
Cambridge University Press.
Kivinen, J., Warmuth, M. 1997. Exponentiated Gradient
Versus Gradient Descent for Linear Predictors. Infor-
mation and Computation, 132, 1-63.
Percy Liang, Ben Taskar and Dan Klein. 2006. Alignment
by Agreement. In Proceedings of NAACL.
Daniel Marcu, Wei Wang, Abdessamad Echihabi,
and Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phrases. In Proceedings of the EMNLP.
Rada Michalcea and Ted Pederson. 2003. An Evalua-
tion Exercise in Word Alignment. HLT-NAACL 2003:
Workshop in building and using Parallel Texts: Data
Driven Machine Translation and Beyond.
Robert C. Moore. 2004. Improving IBM Word-
Alignment Model 1. In Proceedings of the ACL.
Stephan Vogel, Hermann Ney and Christoph Tillman.
1996. HMM-Based Word Alignment in Statistical
Translation. In Proceedings of COLING.
Franz Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational-Linguistics, 29(1): 19-52.
Andrei Simion, Michael Collins and Cliff Stein. 2013. A
Convex Alternative to IBM Model 2. In Proceedings
of the EMNLP.
Kristina Toutanova and Michel Galley. 2011. Why Ini-
tialization Matters for IBM Model 1: Multiple Optima
and Non-Strict Convexity. In Proceedings of the ACL.
Ashish Vaswani, Liang Huang and David Chiang. 2012.
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the L0-norm. In
Proceedings of the ACL.
184
Proceedings of NAACL-HLT 2013, pages 148?157,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Experiments with Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar3
1Dept. of Computer Science, Columbia University
2Dept. of Statistics/3Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
Abstract
Latent-variable PCFGs (L-PCFGs) are a
highly successful model for natural language
parsing. Recent work (Cohen et al, 2012)
has introduced a spectral algorithm for param-
eter estimation of L-PCFGs, which?unlike
the EM algorithm?is guaranteed to give con-
sistent parameter estimates (it has PAC-style
guarantees of sample complexity). This paper
describes experiments using the spectral algo-
rithm. We show that the algorithm provides
models with the same accuracy as EM, but is
an order of magnitude more efficient. We de-
scribe a number of key steps used to obtain
this level of performance; these should be rel-
evant to other work on the application of spec-
tral learning algorithms. We view our results
as strong empirical evidence for the viability
of spectral methods as an alternative to EM.
1 Introduction
Latent-variable PCFGS (L-PCFGs) are a highly suc-
cessful model for natural language parsing (Mat-
suzaki et al, 2005; Petrov et al, 2006). Recent
work (Cohen et al, 2012) has introduced a spectral
learning algorithm for L-PCFGs. A crucial prop-
erty of the algorithm is that it is guaranteed to pro-
vide consistent parameter estimates?in fact it has
PAC-style guarantees of sample complexity.1 This
is in contrast to the EM algorithm, the usual method
for parameter estimation in L-PCFGs, which has the
weaker guarantee of reaching a local maximum of
the likelihood function. The spectral algorithm is
relatively simple and efficient, relying on a singular
value decomposition of the training examples, fol-
lowed by a single pass over the data where parame-
ter values are calculated.
Cohen et al (2012) describe the algorithm, and
the theory behind it, but as yet no experimental re-
sults have been reported for the method. This paper
1under assumptions on certain singular values in the model;
see section 2.3.1.
describes experiments on natural language parsing
using the spectral algorithm for parameter estima-
tion. The algorithm provides models with slightly
higher accuracy than EM (88.05% F-measure on test
data for the spectral algorithm, vs 87.76% for EM),
but is an order of magnitude more efficient (9h52m
for training, compared to 187h12m, a speed-up of
19 times).
We describe a number of key steps in obtain-
ing this level of performance. A simple backed-off
smoothing method is used to estimate the large num-
ber of parameters in the model. The spectral algo-
rithm requires functions mapping inside and outside
trees to feature vectors?we make use of features
corresponding to single level rules, and larger tree
fragments composed of two or three levels of rules.
We show that it is important to scale features by their
inverse variance, in a manner that is closely related
to methods used in canonical correlation analysis.
Negative values can cause issues in spectral algo-
rithms, but we describe a solution to these problems.
In recent work there has been a series of results in
spectral learning algorithms for latent-variable mod-
els (Vempala and Wang, 2004; Hsu et al, 2009;
Bailly et al, 2010; Siddiqi et al, 2010; Parikh et
al., 2011; Balle et al, 2011; Arora et al, 2012;
Dhillon et al, 2012; Anandkumar et al, 2012). Most
of these results are theoretical (although see Luque
et al (2012) for empirical results of spectral learn-
ing for dependency parsing). While the focus of
our experiments is on parsing, our findings should
be relevant to the application of spectral methods to
other latent-variable models. We view our results as
strong empirical evidence for the viability of spec-
tral methods as an alternative to EM.
2 Background
In this section we first give basic definitions for L-
PCFGs, and then describe the spectral learning algo-
rithm of Cohen et al (2012).
148
2.1 L-PCFGs: Basic Definitions
We follow the definition in Cohen et al (2012)
of L-PCFGs. An L-PCFG is an 8-tuple
(N , I,P,m, n, pi, t, q) where:
? N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We as-
sume thatN = I?P , and I?P = ?. Hence we
have partitioned the set of non-terminals into
two subsets.
? [m] is the set of possible hidden states.2
? [n] is the set of possible words.
? For all a ? I, b, c ? N , h1, h2, h3 ?
[m], we have a context-free rule a(h1) ?
b(h2) c(h3). The rule has an associated pa-
rameter t(a? b c, h2, h3|a, h1).
? For all a ? P , h ? [m], x ? [n], we have a
context-free rule a(h) ? x. The rule has an
associated parameter q(a? x|a, h).
? For all a ? I, h ? [m], pi(a, h) is a parameter
specifying the probability of a(h) being at the
root of a tree.
A skeletal tree (s-tree) is a sequence of rules
r1 . . . rN where each ri is either of the form a? b c
or a? x. The rule sequence forms a top-down, left-
most derivation under a CFG with skeletal rules.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
For a given skeletal tree r1 . . . rN , define ai to be
the non-terminal on the left-hand-side of rule ri. For
any i ? [N ] such that ri is of the form a? b c, de-
fine h(2)i and h
(3)
i as the hidden state value of the left
and right child respectively. The model then defines
a probability mass function (PMF) as
p(r1 . . . rN , h1 . . . hN ) =
pi(a1, h1)
?
i:ai?I
t(ri, h
(2)
i , h
(3)
i |ai, hi)
?
i:ai?P
q(ri|ai, hi)
The PMF over skeletal trees is p(r1 . . . rN ) =?
h1...hN
p(r1 . . . rN , h1 . . . hN ).
2For any integer n, we use [n] to denote the set {1, 2, . . . n}.
The parsing problem is to take a sentence as in-
put, and produce a skeletal tree as output. A stan-
dard method for parsing with L-PCFGs is as follows.
First, for a given input sentence x1 . . . xn, for any
triple (a, i, j) such that a ? N and 1 ? i ? j ? n,
the marginal ?(a, i, j) is defined as
?(a, i, j) =
?
t:(a,i,j)?t
p(t) (1)
where the sum is over all skeletal trees t for
x1 . . . xn that include non-terminal a spanning
words xi . . . xj . A variant of the inside-outside
algorithm can be used to calculate marginals.
Once marginals have been computed, Good-
man?s algorithm (Goodman, 1996) is used to find
arg maxt
?
(a,i,j)?t ?(a, i, j).
3
2.2 The Spectral Learning Algorithm
We now give a sketch of the spectral learning algo-
rithm. The training data for the algorithm is a set
of skeletal trees. The output from the algorithm is a
set of parameter estimates for t, q and pi (more pre-
cisely, the estimates are estimates of linearly trans-
formed parameters; see Cohen et al (2012) and sec-
tion 2.3.1 for more details).
The algorithm takes two inputs in addition to the
set of skeletal trees. The first is an integer m, speci-
fying the number of latent state values in the model.
Typically m is a relatively small number; in our ex-
periments we test values such as m = 8, 16 or 32.
The second is a pair of functions ? and ?, that re-
spectively map inside and outside trees to feature
vectors in Rd and Rd
?
, where d and d? are integers.
Each non-terminal in a skeletal tree has an associ-
ated inside and outside tree. The inside tree for a
node contains the entire subtree below that node; the
outside tree contains everything in the tree excluding
the inside tree. We will refer to the node above the
inside tree that has been removed as the ?foot? of the
outside tree. See figure 1 for an example.
Section 3.1 gives definitions of ?(t) and ?(o)
used in our experiments. The definitions of ?(t) and
3In fact, in our implementation we calculate marginals
?(a? b c, i, k, j) for a, b, c ? N and 1 ? i ? k < j, and
?(a, i, i) for a ? N , 1 ? i ? n, then apply the CKY algorithm
to find the parse tree that maximizes the sum of the marginals.
For simplicity of presentation we will refer to marginals of the
form ?(a, i, j) in the remainder of this paper.
149
VP
V
saw
NP
D
the
N
dog
S
NP
D
the
N
cat
VP
Figure 1: The inside tree (shown left) and out-
side tree (shown right) for the non-terminal VP
in the parse tree [S [NP [D the ] [N cat]]
[VP [V saw] [NP [D the] [N dog]]]]
?(o) are typically high-dimensional, sparse feature
vectors, similar to those in log-linear models. For
example ? might track the rule immediately below
the root of the inside tree, or larger tree fragments;
? might include similar features tracking rules or
larger rule fragments above the relevant node.
The spectral learning algorithm proceeds in two
steps. In step 1, we learn an m-dimensional rep-
resentation of inside and outside trees, using the
functions ? and ? in combination with a projection
step defined through singular value decomposition
(SVD). In step 2, we derive parameter estimates di-
rectly from training examples.
2.2.1 Step 1: An SVD-Based Projection
For a given non-terminal a ? N , each instance of
a in the training data has an associated outside tree,
and an associated inside tree. We define Oa to be
the set of pairs of inside/outside trees seen with a in
the training data: each member of Oa is a pair (o, t)
where o is an outside tree, and t is an inside tree.
Step 1 of the algorithm is then as follows:
1. For each a ? N calculate ??a ? Rd?d
?
as
[??a]i,j =
1
|Oa|
?
(o,t)?Oa
?i(t)?j(o)
2. Perform an SVD on ??a. Define Ua ? Rd?m
(V a ? Rd
??m) to be a matrix containing the
m left (right) singular vectors corresponding
to the m largest singular values; define ?a ?
Rm?m to be the diagonal matrix with the m
largest singular values on its diagonal.
3. For each inside tree in the corpus with root la-
bel a, define
Y (t) = (Ua)>?(t)
For each outside tree with a foot node labeled
a, define
Z(o) = (?a)?1(V a)>?(o)
Note that Y (t) and Z(o) are both m-dimensional
vectors; thus we have used SVD to project inside
and outside trees to m-dimensional vectors.
2.3 Step 2: Parameter Estimation
We now describe how the functions Y (t) and Z(o)
are used in estimating parameters of the model.
First, consider the t(a? b c, h2, h3|a, h1) parame-
ters. Each instance of a given rule a? b c in the
training corpus has an outside tree o associated with
the parent labeled a, and inside trees t2 and t3 as-
sociated with the children labeled b and c. For any
rule a? b cwe defineQa?b c to be the set of triples
(o, t(2), t(3)) occurring with that rule in the corpus.
The parameter estimate is then
c?(a? b c, j, k|a, i) =
count(a? b c)
count(a)
? Ea?b ci,j,k
(2)
where
Ea?b ci,j,k =
?
(o,t(2),t(3))
?Qa?b c
Zi(o)? Yj(t(2))? Yk(t(3))
|Qa?b c|
Here we use count(a? b c) and count(a) to refer
to the count of the rule a? b c and the non-terminal
a in the corpus. Note that once the SVD step has
been used to compute representations Y (t) andZ(o)
for each inside and outside tree in the corpus, calcu-
lating the parameter value c?(a? b c, j, k|a, i) is a
very simple operation.
Similarly, for any rule a ? x, define Qa?x to
be the set of outside trees seen with that rule in the
training corpus. The parameter estimate is then
c?(a? x|a, i) =
count(a? x)
count(a)
? Ea?xi (3)
where Ea?xi =
?
o?Qa?x Zi(o)/|Q
a?x|.
A similar method is used for estimating parame-
ters c?(a, i) that play the role of the pi parameters (de-
tails omitted for brevity; see Cohen et al (2012)).
2.3.1 Guarantees for the Algorithm
Once the c?(a? b c, j, k|a, i), c?(a? x|a, i) and
c?(a, i) parameters have been estimated from the
150
training corpus, they can be used in place of the t,
q and pi parameters in the inside-outside algorithm
for computing marginals (see Eq. 1). Call the re-
sulting marginals ??(a, i, j). The guarantees for the
parameter estimation method are as follows:
? Define ?a = E[?(T )(?(O))>|A = a] where
A,O, T are random variables corresponding to
the non-terminal label at a node, the outside
tree, and the inside tree (see Cohen et al (2012)
for a precise definition). Note that ??a, as de-
fined above, is an estimate of ?a. Then if ?a
has rank m, the marginals ?? will converge to
the true values ? as the number of training ex-
amples goes to infinity, assuming that the train-
ing samples are i.i.d. samples from an L-PCFG.
? Define ? to be the m?th largest singular value
of ?a. Then the number of samples required
for ?? to be -close to ? with probability at least
1? ? is polynomial in 1/, 1/?, and 1/?.
Under the first assumption, (Cohen et al,
2012) show that the c? parameters converge to
values that are linear transforms of the orig-
inal parameters in the L-PCFG. For example,
define c(a? b c, j, k|a, i) to be the value that
c?(a? b c, j, k|a, i) converges to in the limit of infi-
nite data. Then there exist invertible matrices Ga ?
Rm?m for all a ? N such that for any a? b c, for
any h1, h2, h3 ? Rm,
t(a? b c, h2, h3|a, h1) =
?
i,j,k
[Ga]i,h1 [(G
b)?1]j,h2 [(G
c)?1]k,h3c(a? b c, j, k|a, i)
The transforms defined by the Ga matrices are be-
nign, in that they cancel in the inside-outside algo-
rithm when marginals ?(a, i, j) are calculated. Sim-
ilar relationships hold for the pi and q parameters.
3 Implementation of the Algorithm
Cohen et al (2012) introduced the spectral learning
algorithm, but did not perform experiments, leaving
several choices open in how the algorithm is imple-
mented in practice. This section describes a number
of key choices made in our implementation of the
algorithm. In brief, they are as follows:
The choice of functions ? and ?. We will de-
scribe basic features used in ? and ? (single-level
rules, larger tree fragments, etc.). We will also de-
scribe a method for scaling different features in ?
and ? by their variance, which turns out to be im-
portant for empirical results.
Estimation ofEa?b ci,j,k andE
a?x
i . There are a very
large number of parameters in the model, lead-
ing to challenges in estimation. The estimates in
Eqs. 2 and 3 are unsmoothed. We describe a simple
backed-off smoothing method that leads to signifi-
cant improvements in performance of the method.
Handling positive and negative values. As de-
fined, the c? parameters may be positive or negative;
as a result, the ?? values may also be positive or neg-
ative. We find that negative values can be a signif-
icant problem if not handled correctly; but with a
very simple fix to the algorithm, it performs well.
We now turn to these three issues in more detail.
Section 4 will describe experiments measuring the
impact of the different choices.
3.1 The Choice of Functions ? and ?
Cohen et al (2012) show that the choice of feature
definitions ? and ? is crucial in two respects. First,
for all non-terminals a ? N , the matrix ?a must
be of rank m: otherwise the parameter-estimation
algorithm will not be consistent. Second, the num-
ber of samples required for learning is polynomial
in 1/?, where ? = mina?N ?m(?a), and ?m(?a)
is the m?th smallest singular value of ?a. (Note that
the second condition is stronger than the first; ? > 0
implies that ?a is of rank m for all a.) The choice
of ? and ? has a direct impact on the value for ?:
roughly speaking, the value for ? can be thought of
as a measure of how informative the functions ? and
? are about the hidden state values.
With this in mind, our goal is to define a rel-
atively simple set of features, which nevertheless
provide significant information about hidden-state
values, and hence provide high accuracy under the
model. The inside-tree feature function ?(t) makes
use of the following indicator features (throughout
these definitions assume that a? b c is at the root
of the inside tree t):
? The pair of nonterminals (a, b). E.g., for the in-
side tree in figure 1 this would be the pair (VP, V).
151
? The pair (a, c). E.g., (VP, NP).
? The rule a? b c. E.g., VP ? V NP.
? The rule a? b c paired with the rule at the
root of t(i,2). E.g., for the inside tree in fig-
ure 1 this would correspond to the tree fragment
(VP (V saw) NP).
? The rule a? b c paired with the rule at
the root of t(i,3). E.g., the tree fragment
(VP V (NP D N)).
? The head part-of-speech of t(i,1) paired with a.4
E.g., the pair (VP, V).
? The number of words dominated by t(i,1) paired
with a (this is an integer valued feature).
In the case of an inside tree consisting of a single
rule a? x the feature vector simply indicates the
identity of that rule.
To illustrate the function ?, it will be useful to
make use of the following example outside tree:
S
NP
D
the
N
cat
VP
V
saw
NP
D N
dog
Note that in this example the foot node of the out-
side tree is labeled D. The features are as follows:
? The rule above the foot node. We take care
to mark which non-terminal is the foot, using a
* symbol. In the above example this feature is
NP ? D? N.
? The two-level and three-level rule fragments
above the foot node. In the above example these fea-
tures would be
VP
V NP
D? N
S
NP VP
V NP
D? N
? The label of the foot node, together with the
label of its parent. In the above example this is
(D, NP).
? The label of the foot node, together with the la-
bel of its parent and grandparent. In the above ex-
ample this is (D, NP, VP).
? The part of speech of the first head word along
the path from the foot of the outside tree to the root
of the tree which is different from the head node of
4We use the English head rules from the Stanford parser
(Klein and Manning, 2003).
the foot node. In the above example this is N.
? The width of the span to the left of the foot node,
paired with the label of the foot node.
? The width of the span to the right of the foot
node, paired with the label of the foot node.
Scaling of features. The features defined above
are almost all binary valued features. We scale the
features in the following way. For each feature ?i(t),
define count(i) to be the number of times the feature
is equal to 1, and M to be the number of training
examples. The feature is then redefined to be
?i(t)?
?
M
count(i) + ?
where ? is a smoothing term (the method is rela-
tively insensitive to the choice of ?; we set ? = 5 in
our experiments). A similar process is applied to the
? features. The method has the effect of decreasing
the importance of more frequent features in the SVD
step of the algorithm.
The SVD-based step of the algorithm is very
closely related to previous work on CCA (Hotelling,
1936; Hardoon et al, 2004; Kakade and Foster,
2009); and the scaling step is derived from previ-
ous work on CCA (Dhillon et al, 2011). In CCA
the ? and ? vectors are ?whitened? in a preprocess-
ing step, before an SVD is applied. This whiten-
ing process involves calculating covariance matrices
Cx = E[??>] and Cy = E[??>], and replacing ?
by (Cx)?1/2? and ? by (Cy)?1/2?. The exact cal-
culation of (Cx)?1/2 and (Cy)?1/2 is challenging in
high dimensions, however, as these matrices will not
be sparse; the transformation described above can
be considered an approximation where off-diagonal
members of Cx and Cy are set to zero. We will see
that empirically this scaling gives much improved
accuracy.
3.2 Estimation of Ea?b ci,j,k and E
a?x
i
The number of Ea?b ci,j,k parameters is very large,
and the estimation method described in Eqs. 2?3 is
unsmoothed. We have found significant improve-
ments in performance using a relatively simple back-
off smoothing method. The intuition behind this
method is as follows: given two random variablesX
and Y , under the assumption that the random vari-
ables are independent, E[XY ] = E[X] ? E[Y ]. It
152
makes sense to define ?backed off? estimates which
make increasingly strong independence assumptions
of this form.
Smoothing of binary rules For any rule a? b c
and indices i, j ? [m] we can define a second-order
moment as follows:
Ea?b ci,j,? =
?
(o,t(2),t(3))
?Qa?b c
Zi(o)? Yj(t(2))
|Qa?b c|
The definitions ofEa?b ci,?,k andE
a?b c
?,j,k are analogous.
We can define a first-order estimate as follows:
Ea?b c?,?,k =
?
(o,t(2),t(3))
?Qa?b c
Yk(t(3))
|Qa?b c|
Again, we have analogous definitions of Ea?b ci,?,? and
Ea?b c?,j,? . Different levels of smoothed estimate can
be derived from these different terms. The first is
E2,a?b ci,j,k =
Ea?b ci,j,? ? E
a?b c
?,?,k + E
a?b c
i,?,k ? E
a?b c
?,j,? + E
a?b c
?,j,k ? E
a?b c
i,?,?
3
Note that we give an equal weight of 1/3 to each of
the three backed-off estimates seen in the numerator.
A second smoothed estimate is
E3,a?b ci,j,k = E
a?b c
i,?,? ? E
a?b c
?,j,? ? E
a?b c
?,?,k
Using the definition of Oa given in section 2.2.1, we
also define
F ai =
?
(o,t)?Oa Yi(t)
|Oa|
Hai =
?
(o,t)?Oa Zi(o)
|Oa|
and our next smoothed estimate asE4,a?b ci,j,k = H
a
i ?
F bj ? F
c
k .
Our final estimate is
?Ea?b ci,j,k + (1? ?)
(
?E2,a?b ci,j,k + (1? ?)K
a?b c
i,j,k
)
where Ka?b ci,j,k = ?E
3,a?b c
i,j,k + (1? ?)E
4,a?b c
i,j,k .
Here ? ? [0, 1] is a smoothing parameter, set to?
|Qa?b c|/(C +
?
|Qa?b c|) in our experiments,
where C is a parameter that is chosen by optimiza-
tion of accuracy on a held-out set of data.
Smoothing lexical rules We define a similar
method for the Ea?xi parameters. Define
Eai =
?
x?
?
o?Qa?x? Zi(o)
?
x? |Q
a?x? |
hence Eai ignores the identity of x in making its es-
timate. The smoothed estimate is then defined as
?Ea?xi +(1??)E
a
i . Here, ? is a value in [0, 1] which
is tuned on a development set. We only smooth lex-
ical rules which appear in the data less than a fixed
number of times. Unlike binary rules, for which the
estimation depends on a high order moment (third
moment), the lexical rules use first-order moments,
and therefore it is not required to smooth rules with
a relatively high count. The maximal count for this
kind of smoothing is set using a development set.
3.3 Handling Positive and Negative Values
As described before, the parameter estimates may
be positive or negative, and as a result the
marginals computed by the algorithm may in some
cases themselves be negative. In early exper-
iments we found this to be a signficant prob-
lem, with some parses having a very large num-
ber of negatives, and being extremely poor in qual-
ity. Our fix is to define the output of the parser
to be arg maxt
?
(a,i,j)?t |?(a, i, j)| rather than
arg maxt
?
(a,i,j)?t ?(a, i, j) as defined in Good-
man?s algorithm. Thus if a marginal value ?(a, i, j)
is negative, we simply replace it with its absolute
value. This step was derived after inspection of the
parsing charts for bad parses, where we saw evi-
dence that in these cases the entire set of marginal
values had been negated (and hence decoding under
Eq. 1 actually leads to the lowest probability parse
being output under the model). We suspect that this
is because in some cases a dominant parameter has
had its sign flipped due to sampling error; more the-
oretical and empirical work is required in fully un-
derstanding this issue.
4 Experiments
In this section we describe parsing experiments us-
ing the L-PCFG estimation method. We give com-
parisons to the EM algorithm, considering both
speed of training, and accuracy of the resulting
model; we also give experiments investigating the
various choices described in the previous section.
153
We use the Penn WSJ treebank (Marcus et al,
1993) for our experiments. Sections 2?21 were
used as training data, and sections 0 and 22 were
used as development data. Section 23 is used as
the final test set. We binarize the trees in train-
ing data using the same method as that described in
Petrov et al (2006). For example, the non-binary
rule VP ? V NP PP SBAR would be converted
to the structure [VP [@VP [@VP V NP] PP]
SBAR] where @VP is a new symbol in the grammar.
Unary rules are removed by collapsing non-terminal
chains: for example the unary rule S ? VP would
be replaced by a single non-terminal S|VP.
For the EM algorithm we use the initialization
method described in Matsuzaki et al (2005). For ef-
ficiency, we use a coarse-to-fine algorithm for pars-
ing with either the EM or spectral derived gram-
mar: a PCFG without latent states is used to calcu-
late marginals, and dynamic programming items are
removed if their marginal probability is lower than
some threshold (0.00005 in our experiments).
For simplicity the parser takes part-of-speech
tagged sentences as input. We use automatically
tagged data from Turbo Tagger (Martins et al,
2010). The tagger is used to tag both the devel-
opment data and the test data. The tagger was re-
trained on sections 2?21. We use the F1 measure
according to the Parseval metric (Black et al, 1991).
For the spectral algorithm, we tuned the smoothing
parameters using section 0 of the treebank.
4.1 Comparison to EM: Accuracy
We compare models trained using EM and the spec-
tral algorithm using values form in {8, 16, 24, 32}.5
For EM, we found that it was important to use de-
velopment data to choose the number of iterations
of training. We train the models for 100 iterations,
then test accuracy of the model on section 22 (devel-
opment data) at different iteration numbers. Table 1
shows that a peak level of accuracy is reached for all
values of m, other than m = 8, at iteration 20?30,
with sometimes substantial overtraining beyond that
point.
The performance of a regular PCFG model, esti-
mated using maximum likelihood and with no latent
5Lower values of m, such as 2 or 4, lead to substantially
lower performance for both models.
section 22 section 23
EM spectral EM spectral
m = 8 86.87 85.60 ? ?
m = 16 88.32 87.77 ? ?
m = 24 88.35 88.53 ? ?
m = 32 88.56 88.82 87.76 88.05
Table 2: Results on the development data (section 22,
with machine-generated POS tags) and test data (section
23, with machine-generated POS tags).
states, is 68.62%.
Table 2 gives results for the EM-trained models
and spectral-trained models. The spectral models
give very similar accuracy to the EM-trained model
on the test set. Results on the development set with
varying m show that the EM-based models perform
better for m = 8, but that the spectral algorithm
quickly catches up as m increases.
4.2 Comparison to EM: Training Speed
Table 3 gives training times for the EM algorithm
and the spectral algorithm for m ? {8, 16, 24, 32}.
All timing experiments were done on a single Intel
Xeon 2.67GHz CPU. The implementations for the
EM algorithm and the spectral algorithm were writ-
ten in Java. The spectral algorithm also made use
of Matlab for several matrix calculations such as the
SVD calculation.
For EM we show the time to train a single iter-
ation, and also the time to train the optimal model
(time for 30 iterations of training for m = 8, 16, 24,
and time for 20 iterations for m = 32). Note that
this latter time is optimistic, as it assumes an oracle
specifying exactly when it is possible to terminate
EM training with no loss in performance. The spec-
tral method is considerably faster than EM: for ex-
ample, for m = 32 the time for training the spectral
model is just under 10 hours, compared to 187 hours
for EM, a factor of almost 19 times faster.6
The reason for these speed ups is as follows.
Step 1 of the spectral algorithm (feature calculation,
transfer + scaling, and SVD) is not required by EM,
but takes a relatively small amount of time (about
1.2 hours for all values of m). Once step 1 has been
completed, step 2 of the spectral algorithm takes a
6In practice, in order to overcome the speed issue with EM
training, we parallelized the E-step on multiple cores. The spec-
tral algorithm can be similarly parallelized, computing statistics
and parameters for each nonterminal separately.
154
10 20 30 40 50 60 70 80 90 100
m = 8 83.51 86.45 86.68 86.69 86.63 86.67 86.70 86.82 86.87 86.83
m = 16 85.18 87.94 88.32 88.21 88.10 87.86 87.70 87.46 87.34 87.24
m = 24 83.62 88.19 88.35 88.25 87.73 87.41 87.35 87.26 87.02 86.80
m = 32 83.23 88.56 88.52 87.82 87.06 86.47 86.38 85.85 85.75 85.57
Table 1: Results on section 22 for the EM algorithm, varying the number of iterations used. Best results in each row
are in boldface.
single EM spectral algorithm
EM iter. best model total feature transfer + scaling SVD a? b c a? x
m = 8 6m 3h 3h32m
22m 49m
36m 1h34m 10m
m = 16 52m 26h6m 5h19m 34m 3h13m 19m
m = 24 3h7m 93h36m 7h15m 36m 4h54m 28m
m = 32 9h21m 187h12m 9h52m 35m 7h16m 41m
Table 3: Running time for the EM algorithm and the various stages in the spectral algorithm. For EM we show the
time for a single iteration, and the time to train the optimal model (time for 30 iterations of training for m = 8, 16, 24,
time for 20 iterations of training for m = 32). For the spectral method we show the following: ?total? is the total
training time; ?feature? is the time to compute the ? and ? vectors for all data points; ?transfer + scaling? is time
to transfer the data from Java to Matlab, combined with the time for scaling of the features; ?SVD? is the time for
the SVD computation; a? b c is the time to compute the c?(a? b c, h2, h3|a, h1) parameters; a? x is the time to
compute the c?(a? x, h|a, h) parameters. Note that ?feature? and ?transfer + scaling? are the same step for all values
of m, so we quote a single runtime for these steps.
single pass over the data: in contrast, EM requires
a few tens of passes (certainly more than 10 passes,
from the results in table 1). The computations per-
formed by the spectral algorithm in its single pass
are relatively cheap. In contrast to EM, the inside-
outside algorithm is not required; however various
operations such as calculating smoothing terms in
the spectral method add some overhead. The net re-
sult is that form = 32 the time for training the spec-
tral method takes a very similar amount of time to a
single pass of the EM algorithm.
4.3 Smoothing, Features, and Negatives
We now describe experiments demonstrating the im-
pact of various components described in section 3.
The effect of smoothing (section 3.2) Without
smoothing, results on section 22 are 85.05% (m =
8, ?1.82), 86.84% (m = 16, ?1.48), 86.47%
(m = 24, ?1.88), 86.47% (m = 32, ?2.09) (in
each case we show the decrease in performance from
the results in table 2). Smoothing is clearly impor-
tant.
Scaling of features (section 3.1) Without scaling
of features, the accuracy on section 22 with m = 32
is 84.40%, a very significant drop from the 88.82%
accuracy achieved with scaling.
Handling negative values (section 3.3) Replac-
ing marginal values ?(a, i, j) with their absolute
values is also important: without this step, accu-
racy on section 22 decreases to 80.61% (m = 32).
319 sentences out of 1700 examples have different
parses when this step is implemented, implying that
the problem with negative values described in sec-
tion 3.3 occurs on around 18% of all sentences.
The effect of feature functions To test the effect
of features on accuracy, we experimented with a
simpler set of features than those described in sec-
tion 3.1. This simple set just includes an indicator
for the rule below a nonterminal (for inside trees)
and the rule above a nonterminal (for outside trees).
Even this simpler set of features achieves relatively
high accuracy (m = 8: 86.44 , m = 16: 86.86,
m = 24: 87.24 , m = 32: 88.07 ).
This set of features is reminiscent of a PCFG
model where the nonterminals are augmented their
parents (vertical Markovization of order 2) and bina-
rization is done while retaining sibling information
(horizontal Markovization of order 1). See Klein
and Manning (2003) for more information. The per-
155
formance of this Markovized PCFG model lags be-
hind the spectral model: it is 82.59%. This is prob-
ably due to the complexity of the grammar which
causes ovefitting. Condensing the sibling and parent
information using latent states as done in the spectral
model leads to better generalization.
It is important to note that the results for both
EM and the spectral algorithm are comparable to
state of the art, but there are other results previ-
ously reported in the literature which are higher.
For example, Hiroyuki et al (2012) report an ac-
curacy of 92.4 F1 on section 23 of the Penn WSJ
treebank using a Bayesian tree substitution gram-
mar; Charniak and Johnson (2005) report accuracy
of 91.4 using a discriminative reranking model; Car-
reras et al (2008) report 91.1 F1 accuracy for a dis-
criminative, perceptron-trained model; Petrov and
Klein (2007) report an accuracy of 90.1 F1, using
L-PCFGs, but with a split-merge training procedure.
Collins (2003) reports an accuracy of 88.2 F1, which
is comparable to the results in this paper.
5 Conclusion
The spectral learning algorithm gives the same level
of accuracy as EM in our experiments, but has sig-
nificantly faster training times. There are several ar-
eas for future work. There are a large number of pa-
rameters in the model, and we suspect that more so-
phisticated regularization methods than the smooth-
ing method we have described may improve perfor-
mance. Future work should also investigate other
choices for the functions ? and ?. There are natu-
ral ways to extend the approach to semi-supervised
learning; for example the SVD step, where repre-
sentations of outside and inside trees are learned,
could be applied to unlabeled data parsed by a first-
pass parser. Finally, the methods we have described
should be applicable to spectral learning for other
latent variable models.
Acknowledgements
Columbia University gratefully acknowledges the
support of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program un-
der Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author(s)
and do not necessarily reflect the view of DARPA,
AFRL, or the US government. Shay Cohen was
supported by the National Science Foundation un-
der Grant #1136996 to the Computing Research As-
sociation for the CIFellows Project. Dean Foster
was supported by National Science Foundation grant
1106743.
References
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and
M. Telgarsky. 2012. Tensor decompositions for learn-
ing latent-variable models. arXiv:1210.7559.
S. Arora, R. Se, and A. Moitra. 2012. Learning topic
models - going beyond SVD. In Proceedings of
FOCS.
R. Bailly, A. Habrar, and F. Denis. 2010. A spectral
approach for probabilistic grammatical inference on
trees. In Proceedings of ALT.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, Dy-
namic Programming, and the Perceptron for Efficient,
Feature-rich Parsing. In Proceedings of CoNLL, pages
9?16.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
M. Collins. 2003. Head-driven statistical models for nat-
ural language processing. Computational Linguistics,
29:589?637.
P. Dhillon, D. P. Foster, and L. H. Ungar. 2011. Multi-
view learning of word embeddings via CCA. In Pro-
ceedings of NIPS.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.
Ungar. 2012. Spectral dependency parsing with latent
variables. In Proceedings of EMNLP.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
156
D. Hardoon, S. Szedmak, and J. Shawe-Taylor. 2004.
Canonical correlation analysis: An overview with ap-
plication to learning methods. Neural Computation,
16(12):2639?2664.
S. Hiroyuki, M. Yusuke, F. Akinori, and N. Masaaki.
2012. Bayesian symbol-refined tree substitution gram-
mars for syntactic parsing. In Proceedings of ACL,
pages 440?448.
H. Hotelling. 1936. Relations between two sets of vari-
ants. Biometrika, 28:321?377.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
S. M. Kakade and D. P. Foster. 2009. Multi-view regres-
sion via canonical correlation analysis. In COLT.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL, pages 423?430.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. T.
Figueiredo, and M. Q. Aguiar. 2010. TurboParsers:
Dependency parsing by approximate variational infer-
ence. In Proceedings of EMNLP.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of COLING-ACL.
S. Siddiqi, B. Boots, and G. Gordon. 2010. Reduced-
rank hidden markov models. JMLR, 9:741?748.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841?860.
157
Proceedings of NAACL-HLT 2013, pages 487?496,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Approximate PCFG Parsing Using Tensor Decomposition
Shay B. Cohen
Department of Computer Science
Columbia University, USA
scohen@cs.columbia.edu
Giorgio Satta
Department of Information Engineering
University of Padua, Italy
satta@dei.unipd.it
Michael Collins
Department of Computer Science
Columbia University, USA
mcollins@cs.columbia.edu
Abstract
We provide an approximation algorithm for
PCFG parsing, which asymptotically im-
proves time complexity with respect to the in-
put grammar size, and prove upper bounds on
the approximation quality. We test our algo-
rithm on two treebanks, and get significant im-
provements in parsing speed.
1 Introduction
The problem of speeding-up parsing algorithms
based on probabilistic context-free grammars
(PCFGs) has received considerable attention in
recent years. Several strategies have been proposed,
including beam-search, best-first and A?. In this
paper we focus on the standard approach of approx-
imating the source PCFG in such a way that parsing
accuracy is traded for efficiency.
Nederhof (2000) gives a thorough presentation
of old and novel ideas for approximating non-
probabilistic CFGs by means of finite automata,
on the basis of specialized preprocessing of self-
embedding structures. In the probabilistic domain,
approximation by means of regular grammars is also
exploited by Eisner and Smith (2005), who filter
long-distance dependencies on-the-fly.
Beyond finite automata approximation, Charniak
et al (2006) propose a coarse-to-fine approach in
which an approximated (not necessarily regular)
PCFG is used to construct a parse forest for the in-
put sentence. Some statistical parameters are then
computed on such a structure, and exploited to filter
parsing with the non-approximated grammar. The
approach can also be iterated at several levels. In
the non-probabilistic setting, a similar filtering ap-
proach was also proposed by Boullier (2003), called
?guided parsing.?
In this paper we rely on an algebraic formulation
of the inside-outside algorithm for PCFGs, based on
a tensor formulation developed for latent-variable
PCFGs in Cohen et al (2012). We combine the
method with known techniques for tensor decompo-
sition to approximate the source PCFG, and develop
a novel algorithm for approximate PCFG parsing.
We obtain improved time upper bounds with respect
to the input grammar size for PCFG parsing, and
provide error upper bounds on the PCFG approxi-
mation, in contrast with existing heuristic methods.
2 Preliminaries
This section introduces the special representation for
probabilistic context-free grammars that we adopt in
this paper, along with the decoding algorithm that
we investigate. For an integer i ? 1, we let [i] =
{1, 2, . . . , i}.
2.1 Probabilistic Context-Free Grammars
We consider context-free grammars (CFGs) in
Chomsky normal form, and denote them as
(N ,L,R) where:
? N is the finite set of nonterminal symbols, with
m = |N |, and L is the finite set of words (lexi-
cal tokens), with L?N = ? and with n = |L|.
? R is a set of rules having the form a? b c,
a, b, c ? N , or the form a? x, a ? N and
x ? L.
A probabilistic CFG (PCFG) is a CFG associated
with a set of parameters defined as follows:
? For each (a? b c) ? R, we have a parameter
p(a? b c | a).
487
? For each (a? x) ? R, we have a parameter
p(a? x | a).
? For each a ? N , we have a parameter pia,
which is the probability of a being the root
symbol of a derivation.
The parameters above satisfy the following nor-
malization conditions:
?
(a?b c)?R
p(a? b c | a) +
?
(a?x)?R
p(a? x | a) = 1,
for each a ? N , and
?
a?N pia = 1.
The probability of a tree ? deriving a sentence in
the language, written p(?), is calculated as the prod-
uct of the probabilities of all rule occurrences in ? ,
times the parameter pia where a is the symbol at the
root of ? .
2.2 Tensor Form of PCFGs
A three-dimensional tensor C ? R(m?m?m) is a
set of m3 parameters Ci,j,k for i, j, k ? [m]. In what
follows, we associate with each tensor three func-
tions, each mapping a pair of vectors in Rm into a
vector in Rm.
Definition 1 Let C ? R(m?m?m) be a tensor.
Given two vectors y1, y2 ? Rm, we let C(y1, y2)
be them-dimensional row vector with components:
[C(y1, y2)]i =
?
j?[m],k?[m]
Ci,j,ky
1
j y
2
k .
We also let C(1,2)(y1, y2) be them-dimensional col-
umn vector with components:
[C(1,2)(y
1, y2)]k =
?
i?[m],j?[m]
Ci,j,ky
1
i y
2
j .
Finally, we let C(1,3)(y1, y2) be the m-dimensional
column vector with components:
[C(1,3)(y
1, y2)]j =
?
i?[m],k?[m]
Ci,j,ky
1
i y
2
k .
For two vectors x, y ? Rm we denote by x y ?
Rm the Hadamard product of x and y, i.e., [xy]i =
xiyi. Finally, for vectors x, y, z ? Rm, xy>z> is the
tensor D ? Rm?m?m where Di,j,k = xiyjzk (this
is analogous to the outer product: [xy>]i,j = xiyj).
We extend the parameter set of our PCFG such
that p(a? b c | a) = 0 for all a? b c not in R,
and p(a? x | a) = 0 for all a? x not in R. We
also represent each a ? N by a unique index in [m],
and we represent each x ? L by a unique index in
[n]: it will always be clear from the context whether
these indices refer to a nonterminal inN or else to a
word in L.
In this paper we assume a tensor representation
for the parameters p(a? b c | a), and we denote by
T ? Rm?m?m a tensor such that:
Ta,b,c , p(a? b c | a).
Similarly, we denote by Q ? Rm?n a matrix such
that:
Qa,x , p(a? x | a).
The root probabilities are denoted using a vector pi ?
Rm?1 such that pia is defined as before.
2.3 Minimum Bayes-Risk Decoding
Let z = x1 ? ? ?xN be some input sentence; we write
T (z) to denote the set of all possible trees for z. It
is often the case that parsing aims to find the high-
est scoring tree ?? for z according to the underlying
PCFG, also called the ?Viterbi parse:?
?? = argmax
??T (z)
p(?)
Goodman (1996) noted that Viterbi parsers do not
optimize the same metric that is usually used for
parsing evaluation (Black et al, 1991). He sug-
gested an alternative algorithm, which he called the
?Labelled Recall Algorithm,? which aims to fix this
issue.
Goodman?s algorithm has two phases. In the first
phase it computes, for each a ? N and for each sub-
string xi ? ? ?xj of z, the marginal ?(a, i, j) defined
as:
?(a, i, j) =
?
??T (z) : (a,i,j)??
p(?).
Here we write (a, i, j) ? ? if nonterminal a spans
words xi ? ? ?xj in the parse tree ? .
488
Inputs: Sentence x1 ? ? ?xN , PCFG (N ,L,R), pa-
rameters T ? R(m?m?m), Q ? R(m?n), pi ?
R(m?1).
Data structures:
? Each ?(a, i, j) ? R for a ? N , i, j ? [N ],
i ? j, is a marginal probability.
? Each ?i,j ? R for i, j ? [N ], i ? j, is the high-
est score for a tree spanning substring xi ? ? ?xj .
Algorithm:
(Marginals) ?a ? N ,?i, j ? [N ], i ? j, compute
the marginals ?(a, i, j) using the inside-outside
algorithm.
(Base case) ?i ? [N ],
?i,i = max
(a?xi)?R
?(a, i, i)
(Maximize Labelled Recall) ?i, j ? [N ], i < j,
?i,j = max
a?N
?(a, i, j) + max
i?k<j
(
?i,k + ?k+1,j
)
Figure 1: The labelled recall algorithm from Goodman
(1996). The algorithm in this figure finds the highest
score for a tree which maximizes labelled recall. The ac-
tual parsing algorithm would use backtrack pointers in
the score computation to return a tree. These are omitted
for simplicity.
The second phase includes a dynamic program-
ming algorithm which finds the tree ?? that maxi-
mizes the sum over marginals in that tree:
?? = argmax
??T (z)
?
(a,i,j)??
?(a, i, j).
Goodman?s algorithm is described in Figure 1.
As Goodman notes, the complexity of the second
phase (?Maximize Labelled Recall,? which is also
referred to as ?minimum Bayes risk decoding?) is
O(N3 +mN2). There are two nested outer loops,
each of order N , and inside these, there are two sep-
arate loops, one of order m and one of order N ,
yielding this computational complexity. The reason
for the linear dependence on the number of nonter-
minals is the lack of dependence on the actual gram-
mar rules, once the marginals are computed.
In its original form, Goodman?s algorithm does
not enforce that the output parse trees are included in
the tree language of the PCFG, that is, certain com-
binations of children and parent nonterminals may
violate the rules in the grammar. In our experiments
we departed from this, and changed Goodman?s al-
gorithm by incorporating the grammar into the dy-
namic programming algorithm in Figure 1. The rea-
son this is important for our experiments is that we
binarize the grammar prior to parsing, and we need
to enforce the links between the split nonterminals
(in the binarized grammar) that refer to the same
syntactic category. See Matsuzaki et al (2005) for
more details about the binarization scheme we used.
This step changes the dynamic programming equa-
tion of Goodman to be linear in the size of the gram-
mar (figure 1). However, empirically, it is the inside-
outside algorithm which takes most of the time to
compute with Goodman?s algorithm. In this paper
we aim to asymptotically reduce the time complex-
ity of the calculation of the inside-outside probabili-
ties using an approximation algorithm.
3 Tensor Formulation of the
Inside-Outside Algorithm
At the core of our approach lies the observation that
there is a (multi)linear algebraic formulation of the
inside-outside algorithm. It can be represented as a
series of tensor, matrix and vector products. A sim-
ilar observation has been made for latent-variable
PCFGs (Cohen et al, 2012) and hidden Markov
models, where only matrix multiplication is required
(Jaeger, 2000). Cohen and Collins (2012) use this
observation together with tensor decomposition to
improve the speed of latent-variable PCFG parsing.
The representation of the inside-outside algorithm
in tensor form is given in Figure 2. For example,
if we consider the recursive equation for the inside
probabilities (where ?i,j is a vector varying over the
nonterminals in the grammar, describing the inside
probability for each nonterminal spanning words i
to j):
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
489
Inputs: Sentence x1 ? ? ?xN , PCFG (N ,L,R), pa-
rameters T ? R(m?m?m), Q ? R(m?n), pi ?
R(m?1).
Data structures:
? Each ?i,j ? R1?m, i, j ? [N ], i ? j, is a row
vector of inside terms ranging over a ? N .
? Each ?i,j ? Rm?1, i, j ? [N ], i ? j, is a
column vector of outside terms ranging over
a ? N .
? Each ?(a, i, j) ? R for a ? N , i, j ? [N ],
i ? j, is a marginal probability.
Algorithm:
(Inside base case) ?i ? [N ], ?(a? xi) ? R,
[?i,i]a = Qa,x
(Inside recursion) ?i, j ? [N ], i < j,
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
(Outside base case) ?a ? N ,
[?1,N ]a = pia
(Outside recursion) ?i, j ? [N ], i ? j,
?i,j =
i?1?
k=1
T(1,2)(?
k,j , ?k,i?1)+
N?
k=j+1
T(1,3)(?
i,k, ?j+1,k)
(Marginals) ?a ? N ,?i, j ? [N ], i ? j,
?(a, i, j) = [?i,j ]a ? [?i,j ]a
Figure 2: The tensor form of the inside-outside algorithm,
for calculation of marginal terms ?(a, i, j).
and then apply the tensor product from Definition 1
to this equation, we get that coordinate a in ?i,j is
defined recursively as follows:
[?i,j ]a =
j?1?
k=i
?
b,c
Ta,b,c ? ?
i,k
b ? ?
k+1,j
c
=
j?1?
k=i
?
b,c
p(a? b c | a)? ?i,kb ? ?
k+1,j
c ,
which is exactly the recursive definition of the inside
algorithm. The correctness of the outside recursive
equations follows very similarly.
The time complexity of the algorithm in this case
is O(m3N3). To see this, observe that each tensor
application takes timeO(m3). Furthermore, the ten-
sor T is applied O(N) times in the computation of
each vector ?i,j and ?i,j . Finally, we need to com-
pute a total ofO(N2) inside and outside vectors, one
for each substring of the input sentence.
4 Tensor Decomposition for the
Inside-Outside Algorithm
In this section, we detail our approach to approxi-
mate parsing using tensor decomposition.
4.1 Tensor Decomposition
In the formulation of the inside-outside algorithm
based on tensor T , each vector ?i,j and ?i,j consists
of m elements, where computation of each element
requires timeO(m2). Therefore, the algorithm has a
O(m3) multiplicative factor in its time complexity,
which we aim to reduce by means of an approximate
algorithm.
Our approximate method relies on a simple ob-
servation. Given an integer r ? 1, assume that
the tensor T has the following special form, called
?Kruskal form:?
T =
r?
i=1
?iuiv>i w
>
i . (1)
In words, T is the sum of r tensors, where each
tensor is obtained as the product of three vectors
ui, vi and wi, together with a scalar ?i. Exact
Kruskal decomposition of a tensor is not necessarily
unique. See Kolda and Bader (2009) for discussion
of uniqueness of tensor decomposition.
490
Consider now two vectors y1, y2 ? Rm, associ-
ated with the inside probabilities for the left (y1) and
right child (y2) of a given node in a parse tree. Let
us introduce auxiliary arrays U, V,W ? Rr?m, with
the i-th row being ui, vi and wi, respectively. Let
also ? = (?1, . . . , ?r). Using the decomposition in
Eq. (1) within Definition 1 we can express the array
T (y1, y2) as:
T (y1, y2) =
[
r?
i=1
?iuiv>i w
>
i
]
(y1, y2) =
r?
i=1
?iui(v>i y
1)(w>i y
2) =
(
U>(? V y1 Wy2)
)
. (2)
The total complexity of the computation in Eq. (2)
is nowO(rm). It is well-known that an exact tensor
decomposition for T can be achieved with r = m2
(Kruskal, 1989). In this case, there is no computa-
tional gain in using Eq. (2) for the inside calculation.
The minimal r required for an exact tensor decom-
position can be smaller than m2. However, identify-
ing that minimal r is NP-hard (H?astad, 1990).
In this section we focused on the computa-
tion of the inside probabilities through vectors
T (?i,k, ?k+1,j). Nonetheless, the steps above can
be easily adapted for the computation of the outside
probabilities through vectors T(1,2)(?k,j , ?k,i?1)
and T(1,3)(?i,k, ?j+1,k).
4.2 Approximate Tensor Decomposition
The PCFG tensor T will not necessarily have the ex-
act decomposed form in Eq. (1). We suggest to ap-
proximate the tensor T by finding the closest tensor
according to some norm over Rm?m?m.
An example of such an approximate decom-
position is the canonical polyadic decomposition
(CPD), also known as CANDECOMP/PARAFAC
decomposition (Carroll and Chang, 1970; Harsh-
man, 1970; Kolda and Bader, 2009). Given an in-
teger r, least squares CPD aims to find the nearest
tensor in Kruskal form, minimizing squared error.
More formally, for a given tensor D ? Rm?m?m,
let ||D||F =
??
i,j,kD
2
i,j,k. Let the set of tensors in
Kruskal form Cr be:
Cr ={C ? Rm?m?m | C =
r?
i=1
?iuiv>i w
>
i
s.t. ?i ? R, ui, vi, wi ? Rm,
||ui||2 = ||vi||2 = ||wi||2 = 1}.
The least squares CPD of C is a tensor C? such
that C? ? argminC??Cr ||C ? C?||F . Here, we treat
the argmin as a set because there could be multiple
solutions which achieve the same accuracy.
There are various algorithms to perform CPD,
such as alternating least squares, direct linear de-
composition, alternating trilinear decomposition and
pseudo alternating least squares (Faber et al, 2003)
and even algorithms designed for sparse tensors (Chi
and Kolda, 2011). Most of these algorithms treat
the problem of identifying the approximate tensor as
an optimization problem. Generally speaking, these
optimization problems are hard to solve, but they
work quite well in practice.
4.3 Parsing with Decomposed Tensors
Equipped with the notion of tensor decomposition,
we can now proceed with approximate tensor pars-
ing in two steps. The first is approximating the ten-
sor using a CPD algorithm, and the second is apply-
ing the algorithms in Figure 1 and Figure 2 to do
parsing, while substituting all tensor product com-
putations with the approximate O(rm) operation of
tensor product.
This is not sufficient to get a significant speed-up
in parsing time. Re-visiting Eq. (2) shows that there
are additional ways to speed-up the tensor applica-
tion T in the context of the inside-outside algorithm.
The first thing to note is that the projections V y1
and Wy2 in Eq. (2) can be cached, and do not have
to be re-calculated every time the tensor is applied.
Here, y1 and y2 will always refer to an outside or
an inside probability vector over the nonterminals in
the grammar. Caching these projections means that
after each computation of an inside or outside proba-
bility, we can immediately project it to the necessary
r-dimensional space, and then re-use this computa-
tion in subsequent application of the tensor.
The second thing to note is that the U projection
in T can be delayed, because of rule of distributiv-
ity. For example, the step in Figure 2 that computes
491
the inside probability ?i,j can be re-formulated as
follows (assuming an exact decomposition of T ):
?i,j =
j?1?
k=i
T (?i,k, ?k+1,j)
=
j?1?
k=1
U>(? V ?i,k W?k+1,j)
= U>
(j?1?
k=1
(? V ?i,k W?k+1,j)
)
. (3)
This means that projection through U can be done
outside of the loop over splitting points in the sen-
tence. Similar reliance on distributivity can be used
to speed-up the outside calculations as well.
The caching speed-up and the delayed projection
speed-up make the approximate inside-outside com-
putation asymptotically faster. While na??ve applica-
tion of the tensor yields an inside algorithm which
runs in time O(rmN3), the improved algorithm
runs in time O(rN3 + rmN2).
5 Quality of Approximate Tensor Parsing
In this section, we give the main approximation re-
sult, that shows that the probability distribution in-
duced by the approximate tensor is close to the orig-
inal probability distribution, if the distance between
the approximate tensor and the rule probabilities is
not too large.
Denote by T (N) the set of trees in the tree lan-
guage of the PCFG with N words (any nontermi-
nal can be the root of the tree). Let T (N) be the
set of pairs of trees ? = (?1, ?2) such that the to-
tal number of binary rules combined in ?1 and ?2 is
N ? 2 (this means that the total number of words
combined is N ). Let T? be the approximate ten-
sor for T . Denote the probability distribution in-
duced by T? by p?.1 Define the vector ?(?) such that
[?(?)]a = Ta,b,c ? p(?1 | b) ? p(?2 | c) where the root
?1 is nonterminal b and the root of ?2 is c. Similarly,
define [??(?)]a = T?a,b,c ? p?(?1 | b) ? p?(?2 | c).
Define Z(a,N) =
?
??T (N)[??(?)]a. In addition,
define D(a,N) =
?
??T (N) |[??(?)]a ? [?(?)]a|
1Here, p? does not have to be a distribution, because T? could
have negative values, in principle, and its slices do not have to
normalize to 1. However, we just treat p? as a function that maps
trees to products of values according to T? .
and define F (a,N) = D(a,N)/Z(a,N). De-
fine ? = ||T? ? T ||F . Last, define ? =
min(a?b c)?R p(a? b c | a). Then, the following
lemma holds:
Lemma 1 For any a and any N , it holds:
D(a,N) ? Z(a,N)
(
(1 + ?/?)N ? 1
)
Proof sketch: The proof is by induction on N .
Assuming that 1 + F (b, k) ? (1 + ?/?)k and
1 + F (c,N ? k ? 1) ? (1 + ?/?)N?k?1 for F
defined as above (this is the induction hypothesis), it
can be shown that the lemma holds. 
Lemma 2 The following holds for any N :
?
??T (N)
|p?(?)? p(?)| ? m
(
(1 + ?/?)N ? 1
)
Proof sketch: Using Ho?lder?s inequality and
Lemma 1 and the fact that Z(a,N) ? 1, it follows
that:
?
??T (N)
|p?(?)? p(?)| ?
?
??T (N),a
|[?(?)]a ? [??(?)]a|
?
(
?
a
Z(a,N)
)
(
(1 + ?/?)N ? 1
)
? m
(
(1 + ?/?)N ? 1
)

Then, the following is a result that explains how
accuracy changes as a function of the quality of the
tensor approximation:
Theorem 1 For anyN , and  < 1/4, it holds that if
? ?
?
2Nm
, then:
?
??T (N)
|p?(?)? p(?)| ? 
Proof sketch: This is the result of applying Lemma 2
together with the inequality (1 + y/t)t? 1 ? 2y for
any t > 0 and y ? 1/2. 
492
We note that Theorem 1 also implicitly bounds
the difference between a marginal ?(a, i, j) and its
approximate version. A marginal corresponds to a
sum over a subset of summands in Eq. (1).
A question that remains at this point is to decide
whether for a given grammar, the optimal ? that can
be achieved is large or small. We define:
??r = min
T??Cr
||T ? T? ||F (4)
The following theorem gives an upper bound on
the value of ??r based on intrinsic property of the
grammar, or more specifically T . It relies on the
fact that for three-dimensional tensors, where each
dimension is of length m, there exists an exact de-
composition of T using m2 components.
Theorem 2 Let:
T =
m2?
i=1
??iu
?
i (v
?
i )
>(w?i )
>
be an exact Kruskal decomposition of T such that
||u?i ||2 = ||v
?
i ||2 = ||w
?
i || = 1 and ?
?
i ? ?
?
i+1 for
i ? [m2 ? 1]. Then, for a given r, it holds:
??r ?
m2?
i=r+1
|??i |
Proof: Let T? be a tensor that achieves the minimum
in Eq. (4). Define:
T ?r =
r?
i=1
??iu
?
i (v
?
i )
>(w?i )
>
Then, noting that ??r is a minimizer of the norm
difference between T and T? and then applying the
triangle inequality and then Cauchy-Schwartz in-
equality leads to the following chain of inequalities:
??r = ||T ? T? ||F ? ||T ? T
?
r||F
= ||
m2?
i=r+1
??iu
?
i (v
?
i )
>(w?i )
>||F
?
m2?
i=r+1
|??i | ? ||u
?
i (v
?
i )
>(w?i )
>||F =
m2?
i=r+1
|??i |
as required. 
6 Experiments
In this section, we describe experiments that demon-
strate the trade-off between the accuracy of the ten-
sor approximation (and as a consequence, the accu-
racy of the approximate parsing algorithm) and pars-
ing time.
Experimental Setting We compare the tensor ap-
proximation parsing algorithm versus the vanilla
Goodman algorithm. Both algorithms were imple-
mented in Java, and the code for both is almost iden-
tical, except for the set of instructions which com-
putes the dynamic programming equation for prop-
agating the beliefs up in the tree. This makes the
clocktime comparison reliable for drawing conclu-
sions about the speed of the algorithms. Our im-
plementation of the vanilla parsing algorithm is lin-
ear in the size of the grammar (and not cubic in the
number of nonterminals, which would give a worse
running time).
In our experiments, we use the method described
in Chi and Kolda (2011) for tensor decomposition.2
This method is fast, even for large tensors, as long
as they are sparse. Such is the case with the tensors
for our grammars.
We use two treebanks for our comparison: the
Penn treebank (Marcus et al, 1993) and the Arabic
treebank (Maamouri et al, 2004). With the Penn
treebank, we use sections 2?21 for training a max-
imum likelihood model and section 22 for parsing,
while for the Arabic treebank we divide the data into
two sets, of size 80% and 20%, one is used for train-
ing a maximum likelihood model and the other is
used for parsing.
The number of binary rules in the treebank gram-
mar is 7,240. The number of nonterminals is 112
and the number of preterminals is 2593Unary rules
are removed by collapsing non-terminal chains. This
increased the number of preterminals. The number
of binary rules in the Arabic treebank is significantly
smaller and consists of 232 rules. We run all parsing
experiments on sentences of length ? 40. The num-
ber of nonterminals is 48 and the number of preter-
2We use the implementation given in Sandia?s Mat-
lab Tensor Toolbox, which can be downloaded at http:
//www.sandia.gov/?tgkolda/TensorToolbox/
index-2.5.html.
3.
493
rank (r) baseline 20 60 100 140 180 220 260 300 340
Ara
bic speed 0.57 0.04 0.06 0.1 0.12 0.16 0.19 0.22 0.26 0.28
F1 63.78 51.80 58.39 63.63 63.77 63.88 63.82 63.84 63.80 63.88
Eng
lish speed 3.89 0.15 0.21 0.30 0.37 0.44 0.52 0.60 0.70 0.79
F1 71.07 57.83 61.67 68.28 69.63 70.30 70.82 71.42 71.28 71.13
Table 1: Results for the Arabic and English treebank of parsing using a vanilla PCFG with and without tensor decom-
position. Speed is given in seconds per sentence.
minals is 81.
Results Table 1 describes the results of compar-
ing the tensor decomposition algorithm to the vanilla
PCFG parsing algorithm.
The first thing to note is that the running time of
the parsing algorithm is linear in r. This indeed
validates the asymptotic complexity of the inside-
outside component in Goodman?s algorithm with the
approximate tensors. It also shows that most of the
time during parsing is spent on the inside-outside al-
gorithm, and not on the dynamic programming algo-
rithm which follows it.
In addition, compared to the baseline which uses
a vanilla CKY algorithm (linear in the number of
rules), we get a speed up of a factor of 4.75 for
Arabic (r = 140) and 6.5 for English (r = 260)
while retaining similar performance. Perhaps more
surprising is that using the tensor approximation ac-
tually improves performance in several cases. We
hypothesize that the cause of this is that the tensor
decomposition requires less parameters to express
the rule probabilities in the grammar, and therefore
leads to better generalization than a vanilla maxi-
mum likelihood estimate.
We include results for a more complex model for
Arabic, which uses horizontal Markovization of or-
der 1 and vertical Markovization of order 2 (Klein
and Manning, 2003). This grammar includes 2,188
binary rules. Parsing exhaustively using this gram-
mar takes 1.30 seconds per sentence (on average)
with an F1 measure of 64.43. Parsing with tensor
decomposition for r = 280 takes 0.62 seconds per
sentence (on average) with an F1 measure of 64.05.
7 Discussion
In this section, we briefly touch on several other top-
ics related to tensor approximation.
7.1 Approximating the Probability of a String
The probability of a sentence z under a PCFG is de-
fined as p(z) =
?
??T (z) p(?), and can be approx-
imated using the algorithm in Section 4.3, running
in time O(rN3 + rmN2). Of theoretical interest,
we discuss here a time O(rN3 + r2N2) algorithm,
which is more convenient when r < m.
Observe that in Eq. (3) vector ?i,j always appears
within one of the two terms V ?i,j and W?i,j in
Rr?1, whose dimensions are independent of m.
We can therefore use Eq. (3) to compute V ?i,j as
V ?i,j = V U>
(?j?1
k=1(? V ?
i,k W?k+1,j)
)
,
where V U> is a Rr?r matrix that can be
computed off-line, i.e., independently of
z. A symmetrical relation can be used
to compute W?i,j . Finally, we can write
p(z) = pi>U
(?N?1
k=1 (? V ?
1,k W?k+1,N )
)
,
where pi>U is a R1?r vector that can again be
computed off-line. This algorithm then runs in time
O(rN3 + r2N2).
7.2 Applications to Dynamic Programming
The approximation method presented in this paper is
not limited to PCFG parsing. A similar approxima-
tion method has been used for latent-variable PCFGs
(Cohen and Collins, 2012), and in general, ten-
sor approximation can be used to speed-up inside-
outside algorithms for general dynamic program-
ming algorithms or weighted logic programs (Eisner
et al, 2004; Cohen et al, 2011). In the general case,
the dimension of the tensors will not be necessarily
just three (corresponding to binary rules), but can be
of a higher dimension, and therefore the speed gain
can be even greater. In addition, tensor approxima-
tion can be used for computing marginals of latent
variables in graphical models.
For example, the complexity of the forward-
494
backward algorithm for HMMs can be reduced to
be linear in the number of states (as opposed to
quadratic) and linear in the rank used in an approxi-
mate singular-value decomposition (instead of ten-
sor decomposition) of the transition and emission
matrices.
7.3 Tighter (but Slower) Approximation Using
Singular Value Decomposition
The accuracy of the algorithm depends on the ability
of the tensor decomposition algorithm to decompose
the tensor with a small reconstruction error. The de-
composition algorithm is performed on the tensor T
which includes all rules in the grammar.
Instead, one can approach the approximation by
doing a decomposition for each slice of T separately
using singular value decomposition. This will lead
to a more accurate approximation, but will also lead
to an extra factor of m during parsing. This factor
is added because now there is not a single U , V and
W , but instead there are such matrices for each non-
terminal in the grammar.
8 Conclusion
We described an approximation algorithm for prob-
abilistic context-free parsing. The approximation al-
gorithm is based on tensor decomposition performed
on the underlying rule table of the CFG grammar.
The approximation algorithm leads to significant
speed-up in PCFG parsing, with minimal loss in per-
formance.
References
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of DARPA Work-
shop on Speech and Natural Language.
P. Boullier. 2003. Guided earley parsing. In 8th In-
ternational Workshop on Parsing Technologies, pages
43?54.
J. D. Carroll and J. J. Chang. 1970. Analysis of indi-
vidual differences in multidimensional scaling via an
N-way generalization of Eckart-Young decomposition.
Psychometrika, 35:283?319.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil,
D. Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore,
M. Pozar, and T. Vu. 2006. Multilevel coarse-to-fine
pcfg parsing. In Proceedings of HLT-NAACL.
E. C. Chi and T. G. Kolda. 2011. On tensors, spar-
sity, and nonnegative factorizations. arXiv:1112.2414
[math.NA], December.
S. B. Cohen and M. Collins. 2012. Tensor decomposi-
tion for fast latent-variable PCFG parsing. In Proceed-
ings of NIPS.
S. B. Cohen, R. J. Simmons, and N. A. Smith. 2011.
Products of weighted logic programs. Theory and
Practice of Logic Programming, 11(2?3):263?296.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
J. Eisner and N. A. Smith. 2005. Parsing with soft and
hard constraints on dependency length. In Proceed-
ings of IWPT, Parsing ?05.
J. Eisner, E. Goldlust, and N. A. Smith. 2004. Dyna: A
declarative language for implementing dynamic pro-
grams. In Proc. of ACL (companion volume).
N. M. Faber, R. Bro, and P. Hopke. 2003. Recent devel-
opments in CANDECOMP/PARAFAC algorithms: a
critical review. Chemometrics and Intelligent Labora-
tory Systems, 65(1):119?137.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of ACL.
R. A. Harshman. 1970. Foundations of the PARAFAC
procedure: Models and conditions for an ?explana-
tory? multi-modal factor analysis. UCLA working pa-
pers in phoentics, 16:1?84.
J. H?astad. 1990. Tensor rank is NP-complete. Algo-
rithms, 11:644?654.
H. Jaeger. 2000. Observable operator models for dis-
crete stochastic time series. Neural Computation,
12(6):1371?1398.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of ACL.
T. G. Kolda and B. W. Bader. 2009. Tensor decomposi-
tions and applications. SIAM Rev., 51:455?500.
J. B. Kruskal. 1989. Rank, decomposition, and unique-
ness for 3-way and N-way arrays. In R. Coppi and
S. Bolasco, editors, Multiway Data Analysis, pages 7?
18.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a large-
scale annotated Arabic corpus. In Proceedings NEM-
LAR.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19(2):313?330.
495
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
M.-J. Nederhof. 2000. Practical experiments with regu-
lar approximation of context-free languages. Compu-
tational Linguistics, 26(1):17?44.
496
Tutorials, NAACL-HLT 2013, pages 13?15,
Atlanta, Georgia, June 9 2013. c?2013 Association for Computational Linguistics
Spectral Learning Algorithms for
Natural Language Processing
Shay Cohen?, Michael Collins?, Dean P. Foster?, Karl Stratos?, Lyle Ungar?
?Columbia University
?University of Pennsylvania
scohen,mcollins,stratos@cs.columbia.edu
dean@foster.net
ungar@cis.upenn.edu
1 Introduction
Recent work in machine learning and NLP has developed spectral algorithms for
many learning tasks involving latent variables. Spectral algorithms rely on sin-
gular value decomposition as a basic operation, usually followed by some simple
estimation method based on the method of moments. From a theoretical point
of view, these methods are appealing in that they offer consistent estimators (and
PAC-style guarantees of sample complexity) for several important latent-variable
models. This is in contrast to the EM algorithm, which is an extremely success-
ful approach, but which only has guarantees of reaching a local maximum of the
likelihood function.
From a practical point of view, the methods (unlike EM) have no need for
careful initialization, and have recently been shown to be highly efficient (as one
example, in work under submission by the authors on learning of latent-variable
PCFGs, a spectral algorithm performs at identical accuracy to EM, but is around
20 times faster).
2 Outline
In this tutorial we will aim to give a broad overview of spectral methods, describing
theoretical guarantees, as well as practical issues. We will start by covering the
basics of singular value decomposition and describe efficient methods for doing
singular value decomposition. The SVD operation is at the core of most spectral
algorithms that have been developed.
13
We will then continue to cover canonical correlation analysis (CCA). CCA is an
early method from statistics for dimensionality reduction. With CCA, two or more
views of the data are created, and they are all projected into a lower dimensional
space which maximizes the correlation between the views. We will review the
basic algorithms underlying CCA, give some formal results giving guarantees for
latent-variable models and also describe how they have been applied recently to
learning lexical representations from large quantities of unlabeled data. This idea
of learning lexical representations can be extended further, where unlabeled data is
used to learn underlying representations which are subsequently used as additional
information for supervised training.
We will also cover how spectral algorithms can be used for structured predic-
tion problems with sequences and parse trees. A striking recent result by Hsu,
Kakade and Zhang (2009) shows that HMMs can be learned efficiently using a
spectral algorithm. HMMs are widely used in NLP and speech, and previous al-
gorithms (typically based on EM) were guaranteed to only reach a local maximum
of the likelihood function, so this is a crucial result. We will review the basic me-
chanics of the HMM learning algorithm, describe its formal guarantees, and also
cover practical issues.
Last, we will cover work about spectral algorithms in the context of natural
language parsing. We will show how spectral algorithms can be used to estimate
the parameter models of latent-variable PCFGs, a model which serves as the base
for state-of-the-art parsing models such as the one of Petrov et al (2007). We will
show what are the practical steps that are needed to be taken in order to make spec-
tral algorithms for L-PCFGs (or other models in general) practical and comparable
to state of the art.
3 Speaker Bios
Shay Cohen1 is a postdoctoral research scientist in the Department of Computer
Science at Columbia University. He is a computing innovation fellow. His re-
search interests span a range of topics in natural language processing and machine
learning. He is especially interested in developing efficient and scalable parsing
algorithms as well as learning algorithms for probabilistic grammars.
Michael Collins2 is the Vikram S. Pandit Professor of computer science at
Columbia University. His research is focused on topics including statistical pars-
ing, structured prediction problems in machine learning, and applications including
machine translation, dialog systems, and speech recognition. His awards include a
1http://www.cs.columbia.edu/?scohen/
2http://www.cs.columbia.edu/?mcollins/
14
Sloan fellowship, an NSF career award, and best paper awards at EMNLP (2002,
2004, and 2010), UAI (2004 and 2005), and CoNLL 2008.
Dean P. Foster3 is currently the Marie and Joseph Melone Professor of Statis-
tics at the Wharton School of the University of Pennsylvania. His current research
interests are machine learning, stepwise regression and computational linguistics.
He has been searching for new methods of finding useful features in big data sets.
His current set of hammers revolve around fast matrix methods (which decompose
2nd moments) and tensor methods for decomposing 3rd moments.
Karl Stratos4 is a Ph.D. student in the Department of Computer Science at
Columbia. His research is focused on machine learning and natural language pro-
cessing. His current research efforts are focused on spectral learning of latent-
variable models, or more generally, uncovering latent structure from data.
Lyle Ungar5 is a professor at the Computer and Information Science Depart-
ment at the University of Pennsylvania. His research group develops scalable ma-
chine learning and text mining methods, including clustering, feature selection,
and semi-supervised and multi-task learning for natural language, psychology, and
medical research. Example projects include spectral learning of language models,
multi-view learning for gene expression and MRI data, and mining social media to
better understand personality and well-being.
3http://gosset.wharton.upenn.edu/?foster/index.pl
4http://www.cs.columbia.edu/?stratos/
5http://www.cis.upenn.edu/?ungar/
15
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1?11,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Third-order Dependency Parsers
Terry Koo and Michael Collins
MIT CSAIL, Cambridge, MA, 02139, USA
{maestro,mcollins}@csail.mit.edu
Abstract
We present algorithms for higher-order de-
pendency parsing that are ?third-order?
in the sense that they can evaluate sub-
structures containing three dependencies,
and ?efficient? in the sense that they re-
quire only O(n4) time. Importantly, our
new parsers can utilize both sibling-style
and grandchild-style interactions. We
evaluate our parsers on the Penn Tree-
bank and Prague Dependency Treebank,
achieving unlabeled attachment scores of
93.04% and 87.38%, respectively.
1 Introduction
Dependency grammar has proven to be a very use-
ful syntactic formalism, due in no small part to the
development of efficient parsing algorithms (Eis-
ner, 2000; McDonald et al, 2005b; McDonald
and Pereira, 2006; Carreras, 2007), which can be
leveraged for a wide variety of learning methods,
such as feature-rich discriminative models (Laf-
ferty et al, 2001; Collins, 2002; Taskar et al,
2003). These parsing algorithms share an impor-
tant characteristic: they factor dependency trees
into sets of parts that have limited interactions. By
exploiting the additional constraints arising from
the factorization, maximizations or summations
over the set of possible dependency trees can be
performed efficiently and exactly.
A crucial limitation of factored parsing algo-
rithms is that the associated parts are typically
quite small, losing much of the contextual in-
formation within the dependency tree. For the
purposes of improving parsing performance, it is
desirable to increase the size and variety of the
parts used by the factorization.1 At the same
time, the need for more expressive factorizations
1For examples of how performance varies with the degree
of the parser?s factorization see, e.g., McDonald and Pereira
(2006, Tables 1 and 2), Carreras (2007, Table 2), Koo et al
(2008, Tables 2 and 4), or Suzuki et al (2009, Tables 3?6).
must be balanced against any resulting increase in
the computational cost of the parsing algorithm.
Consequently, recent work in dependency pars-
ing has been restricted to applications of second-
order parsers, the most powerful of which (Car-
reras, 2007) requires O(n4) time and O(n3) space,
while being limited to second-order parts.
In this paper, we present new third-order pars-
ing algorithms that increase both the size and vari-
ety of the parts participating in the factorization,
while simultaneously maintaining computational
requirements of O(n4) time and O(n3) space. We
evaluate our parsers on the Penn WSJ Treebank
(Marcus et al, 1993) and Prague Dependency
Treebank (Hajic? et al, 2001), achieving unlabeled
attachment scores of 93.04% and 87.38%. In sum-
mary, we make three main contributions:
1. Efficient new third-order parsing algorithms.
2. Empirical evaluations of these parsers.
3. A free distribution of our implementation.2
The remainder of this paper is divided as follows:
Sections 2 and 3 give background, Sections 4 and
5 describe our new parsing algorithms, Section 6
discusses related work, Section 7 presents our ex-
perimental results, and Section 8 concludes.
2 Dependency parsing
In dependency grammar, syntactic relationships
are represented as head-modifier dependencies:
directed arcs between a head, which is the more
?essential? word in the relationship, and a modi-
fier, which supplements the meaning of the head.
For example, Figure 1 contains a dependency be-
tween the verb ?report? (the head) and its object
?sales? (the modifier). A complete analysis of a
sentence is given by a dependency tree: a set of de-
pendencies that forms a rooted, directed tree span-
ning the words of the sentence. Every dependency
tree is rooted at a special ?*? token, allowing the
2http://groups.csail.mit.edu/nlp/dpo3/
1
Insiders must report purchases and immediatelysales
 * 
Figure 1: An example dependency structure.
selection of the sentential head to be modeled as if
it were a dependency.
For a sentence x, we define dependency parsing
as a search for the highest-scoring analysis of x:
y
?
(x) = argmax
y?Y(x)
SCORE(x, y) (1)
Here, Y(x) is the set of all trees compatible with
x and SCORE(x, y) evaluates the event that tree y
is the analysis of sentence x. Since the cardinal-
ity of Y(x) grows exponentially with the length of
the sentence, directly solving Eq. 1 is impractical.
A common strategy, and one which forms the fo-
cus of this paper, is to factor each dependency tree
into small parts, which can be scored in isolation.
Factored parsing can be formalized as follows:
SCORE(x, y) =
?
p?y
SCOREPART(x, p)
That is, we treat the dependency tree y as a set
of parts p, each of which makes a separate contri-
bution to the score of y. For certain factorizations,
efficient parsing algorithms exist for solving Eq. 1.
We define the order of a part according to the
number of dependencies it contains, with analo-
gous terminology for factorizations and parsing al-
gorithms. In the remainder of this paper, we focus
on factorizations utilizing the following parts:
g
g
hh
h h h
mm
m mm
ss
s
t
dependency sibling grandchild
tri-siblinggrand-sibling
Specifically, Sections 4.1, 4.2, and 4.3 describe
parsers that, respectively, factor trees into grand-
child parts, grand-sibling parts, and a mixture of
grand-sibling and tri-sibling parts.
3 Existing parsing algorithms
Our new third-order dependency parsers build on
ideas from existing parsing algorithms. In this
section, we provide background on two relevant
parsers from previous work.
(a) +=
h h mm ee
(b) +=
h h mm r r+1
Figure 2: The dynamic-programming structures
and derivations of the Eisner (2000) algorithm.
Complete spans are depicted as triangles and in-
complete spans as trapezoids. For brevity, we elide
the symmetric right-headed versions.
3.1 First-order factorization
The first type of parser we describe uses a ?first-
order? factorization, which decomposes a depen-
dency tree into its individual dependencies. Eis-
ner (2000) introduced a widely-used dynamic-
programming algorithm for first-order parsing; as
it is the basis for many parsers, including our new
algorithms, we summarize its design here.
The Eisner (2000) algorithm is based on two
interrelated types of dynamic-programming struc-
tures: complete spans, which consist of a head-
word and its descendents on one side, and incom-
plete spans, which consist of a dependency and the
region between the head and modifier.
Formally, we denote a complete span as Ch,e
where h and e are the indices of the span?s head-
word and endpoint. An incomplete span is de-
noted as Ih,m where h and m are the index of the
head and modifier of a dependency. Intuitively,
a complete span represents a ?half-constituent?
headed by h, whereas an incomplete span is only
a partial half-constituent, since the constituent can
be extended by adding more modifiers to m.
Each type of span is created by recursively
combining two smaller, adjacent spans; the con-
structions are specified graphically in Figure 2.
An incomplete span is constructed from a pair
of complete spans, indicating the division of the
range [h,m] into constituents headed by h and
m. A complete span is created by ?complet-
ing? an incomplete span with the other half of
m?s constituent. The point of concatenation in
each construction?m in Figure 2(a) or r in Fig-
ure 2(b)?is the split point, a free index that must
be enumerated to find the optimal construction.
In order to parse a sentence x, it suffices to
find optimal constructions for all complete and
incomplete spans defined on x. This can be
2
(a) +=
h h mm ee
(b) +=
h h mm ss
(c) +=
mms s r r+1
Figure 3: The dynamic-programming structures
and derivations of the second-order sibling parser;
sibling spans are depicted as boxes. For brevity,
we elide the right-headed versions.
accomplished by adapting standard chart-parsing
techniques (Cocke and Schwartz, 1970; Younger,
1967; Kasami, 1965) to the recursive derivations
defined in Figure 2. Since each derivation is de-
fined by two fixed indices (the boundaries of the
span) and a third free index (the split point), the
parsing algorithm requires O(n3) time and O(n2)
space (Eisner, 1996; McAllester, 1999).
3.2 Second-order sibling factorization
As remarked by Eisner (1996) and McDonald
and Pereira (2006), it is possible to rearrange the
dynamic-programming structures to conform to an
improved factorization that decomposes each tree
into sibling parts?pairs of dependencies with a
shared head. Specifically, a sibling part consists
of a triple of indices (h,m, s) where (h,m) and
(h, s) are dependencies, and where s and m are
successive modifiers to the same side of h.
In order to parse this factorization, the second-
order parser introduces a third type of dynamic-
programming structure: sibling spans, which rep-
resent the region between successive modifiers of
some head. Formally, we denote a sibling span
as Ss,m where s and m are a pair of modifiers in-
volved in a sibling relationship. Modified versions
of sibling spans will play an important role in the
new parsing algorithms described in Section 4.
Figure 3 provides a graphical specification of
the second-order parsing algorithm. Note that in-
complete spans are constructed in a new way: the
second-order parser combines a smaller incom-
plete span, representing the next-innermost depen-
dency, with a sibling span that covers the region
between the two modifiers. Sibling parts (h,m, s)
can thus be obtained from Figure 3(b). Despite
the use of second-order parts, each derivation is
(a) = +
gg hhh mm ee
(b) = +
g gh h hm mr r+1
(c) = +
gg hh hm me e
(d) = +
gg hh hm mr r+1
Figure 4: The dynamic-programming structures
and derivations of Model 0. For brevity, we elide
the right-headed versions. Note that (c) and (d)
differ from (a) and (b) only in the position of g.
still defined by a span and split point, so the parser
requires O(n3) time and O(n2) space.
4 New third-order parsing algorithms
In this section we describe our new third-order de-
pendency parsing algorithms. Our overall method
is characterized by the augmentation of each span
with a ?grandparent? index: an index external to
the span whose role will be made clear below. This
section presents three parsing algorithms based on
this idea: Model 0, a second-order parser, and
Models 1 and 2, which are third-order parsers.
4.1 Model 0: all grandchildren
The first parser, Model 0, factors each dependency
tree into a set of grandchild parts?pairs of de-
pendencies connected head-to-tail. Specifically,
a grandchild part is a triple of indices (g, h,m)
where (g, h) and (h,m) are dependencies.3
In order to parse this factorization, we augment
both complete and incomplete spans with grand-
parent indices; for brevity, we refer to these aug-
mented structures as g-spans. Formally, we denote
a complete g-span as Cgh,e, where Ch,e is a normal
complete span and g is an index lying outside the
range [h, e], with the implication that (g, h) is a
dependency. Incomplete g-spans are defined anal-
ogously and are denoted as Igh,m.
Figure 4 depicts complete and incomplete g-
spans and provides a graphical specification of the
3The Carreras (2007) parser also uses grandchild parts but
only in restricted cases; see Section 6 for details.
3
OPTIMIZEALLSPANS(x)
1. ? g, i Cgi,i = 0 / base case
2. for w = 1 . . . (n? 1) / span width
3. for i = 1 . . . (n? w) / span start index
4. j = i + w / span end index
5. for g < i or g > j / grandparent index
6. Igi,j = max i?r<j {C
g
i,r + C
i
j,r+1} +
SCOREG(x, g, i, j)
7. Igj,i = max i?r<j {C
g
j,r+1 + C
j
i,r} +
SCOREG(x, g, j, i)
8. Cgi,j = max i<m?j {I
g
i,m + C
i
m,j}
9. Cgj,i = max i?m<j {I
g
j,m + C
j
m,i}
10. endfor
11. endfor
12. endfor
Figure 5: A bottom-up chart parser for Model 0.
SCOREG is the scoring function for grandchild
parts. We use the g-span identities as shorthand
for their chart entries (e.g., Igi,j refers to the entry
containing the maximum score of that g-span).
Model 0 dynamic-programming algorithm. The
algorithm resembles the first-order parser, except
that every recursive construction must also set the
grandparent indices of the smaller g-spans; for-
tunately, this can be done deterministically in all
cases. For example, Figure 4(a) depicts the de-
composition of Cgh,e into an incomplete half and
a complete half. The grandparent of the incom-
plete half is copied from Cgh,e while the grandpar-
ent of the complete half is set to h, the head of m
as defined by the construction. Clearly, grandchild
parts (g, h,m) can be read off of the incomplete
g-spans in Figure 4(b,d). Moreover, since each
derivation copies the grandparent index g into suc-
cessively smaller g-spans, grandchild parts will be
produced for all grandchildren of g.
Model 0 can be parsed by adapting standard
top-down or bottom-up chart parsing techniques.
For concreteness, Figure 5 provides a pseudocode
sketch of a bottom-up chart parser for Model 0;
although the sketch omits many details, it suf-
fices for the purposes of illustration. The algo-
rithm progresses from small widths to large in
the usual manner, but after defining the endpoints
(i, j) there is an additional loop that enumerates
all possible grandparents. Since each derivation is
defined by three fixed indices (the g-span) and one
free index (the split point), the complexity of the
algorithm is O(n4) time and O(n3) space.
Note that the grandparent indices cause each g-
(a) = +
gg hhh mm ee
(b) = +
g gh h hm mss
(c) = +
hh hm mss r r+1
Figure 6: The dynamic-programming structures
and derivations of Model 1. Right-headed and
right-grandparented versions are omitted.
span to have non-contiguous structure. For ex-
ample, in Figure 4(a) the words between g and h
will be controlled by some other g-span. Due to
these discontinuities, the correctness of the Model
0 dynamic-programming algorithm may not be
immediately obvious. While a full proof of cor-
rectness is beyond the scope of this paper, we note
that each structure on the right-hand side of Fig-
ure 4 lies completely within the structure on the
left-hand side. This nesting of structures implies,
in turn, that the usual properties required to ensure
the correctness of dynamic programming hold.
4.2 Model 1: all grand-siblings
We now describe our first third-order parsing al-
gorithm. Model 1 decomposes each tree into a
set of grand-sibling parts?combinations of sib-
ling parts and grandchild parts. Specifically, a
grand-sibling is a 4-tuple of indices (g, h,m, s)
where (h,m, s) is a sibling part and (g, h,m) and
(g, h, s) are grandchild parts. For example, in Fig-
ure 1, the words ?must,? ?report,? ?sales,? and
?immediately? form a grand-sibling part.
In order to parse this factorization, we intro-
duce sibling g-spans Shm,s, which are composed of
a normal sibling span Sm,s and an external index
h, with the implication that (h,m, s) forms a valid
sibling part. Figure 6 provides a graphical specifi-
cation of the dynamic-programming algorithm for
Model 1. The overall structure of the algorithm re-
sembles the second-order sibling parser, with the
addition of grandparent indices; as in Model 0, the
grandparent indices can be set deterministically in
all cases. Note that the sibling g-spans are crucial:
they allow grand-sibling parts (g, h,m, s) to be
read off of Figure 6(b), while simultaneously prop-
agating grandparent indices to smaller g-spans.
4
(a) = +
gg hhh mm ee
(b) =
g hh mm s
(c) = +
hh hm ms sst
(d) = +
hh hm mss r r+1
Figure 7: The dynamic-programming structures
and derivations of Model 2. Right-headed and
right-grandparented versions are omitted.
Like Model 0, Model 1 can be parsed via adap-
tations of standard chart-parsing techniques; we
omit the details for brevity. Despite the move to
third-order parts, each derivation is still defined by
a g-span and a split point, so that parsing requires
only O(n4) time and O(n3) space.
4.3 Model 2: grand-siblings and tri-siblings
Higher-order parsing algorithms have been pro-
posed which extend the second-order sibling fac-
torization to parts containing multiple siblings
(McDonald and Pereira, 2006, also see Section 6
for discussion). In this section, we show how our
g-span-based techniques can be combined with a
third-order sibling parser, resulting in a parser that
captures both grand-sibling parts and tri-sibling
parts?4-tuples of indices (h,m, s, t) such that
both (h,m, s) and (h, s, t) are sibling parts.
In order to parse this factorization, we intro-
duce a new type of dynamic-programming struc-
ture: sibling-augmented spans, or s-spans. For-
mally, we denote an incomplete s-span as Ih,m,s
where Ih,m is a normal incomplete span and s is an
index lying in the strict interior of the range [h,m],
such that (h,m, s) forms a valid sibling part.
Figure 7 provides a graphical specification of
the Model 2 parsing algorithm. An incomplete
s-span is constructed by combining a smaller in-
complete s-span, representing the next-innermost
pair of modifiers, with a sibling g-span, covering
the region between the outer two modifiers. As
in Model 1, sibling g-spans are crucial for propa-
gating grandparent indices, while allowing the re-
covery of tri-sibling parts (h,m, s, t). Figure 7(b)
shows how an incomplete s-span can be converted
into an incomplete g-span by exchanging the in-
ternal sibling index for an external grandparent in-
dex; in the process, grand-sibling parts (g, h,m, s)
are enumerated. Since every derivation is defined
by an augmented span and a split point, Model 2
can be parsed in O(n4) time and O(n3) space.
It should be noted that unlike Model 1, Model
2 produces grand-sibling parts only for the outer-
most pair of grandchildren,4 similar to the behav-
ior of the Carreras (2007) parser. In fact, the re-
semblance is more than passing, as Model 2 can
emulate the Carreras (2007) algorithm by ?demot-
ing? each third-order part into a second-order part:
SCOREGS(x, g, h,m, s) = SCOREG(x, g, h,m)
SCORETS(x, h,m, s, t) = SCORES(x, h,m, s)
where SCOREG, SCORES, SCOREGS and
SCORETS are the scoring functions for grand-
children, siblings, grand-siblings and tri-siblings,
respectively. The emulated version has the same
computational complexity as the original, so there
is no practical reason to prefer it over the original.
Nevertheless, the relationship illustrated above
highlights the efficiency of our approach: we
are able to recover third-order parts in place of
second-order parts, at no additional cost.
4.4 Discussion
The technique of grandparent-index augmentation
has proven fruitful, as it allows us to parse ex-
pressive third-order factorizations while retaining
an efficient O(n4) runtime. In fact, our third-
order parsing algorithms are ?optimally? efficient
in an asymptotic sense. Since each third-order part
is composed of four separate indices, there are
?(n
4
) distinct parts. Any third-order parsing al-
gorithm must at least consider the score of each
part, hence third-order parsing is ?(n4) and it fol-
lows that the asymptotic complexity of Models 1
and 2 cannot be improved.
The key to the efficiency of our approach is a
fundamental asymmetry in the structure of a di-
rected tree: a head can have any number of mod-
ifiers, while a modifier always has exactly one
head. Factorizations like that of Carreras (2007)
obtain grandchild parts by augmenting spans with
the indices of modifiers, leading to limitations on
4The reason for the restriction is that in Model 2, grand-
siblings can only be derived via Figure 7(b), which does not
recursively copy the grandparent index for reuse in smaller
g-spans as Model 1 does in Figure 6(b).
5
the grandchildren that can participate in the fac-
torization. Our method, by ?inverting? the modi-
fier indices into grandparent indices, exploits the
structural asymmetry.
As a final note, the parsing algorithms described
in this section fall into the category of projective
dependency parsers, which forbid crossing depen-
dencies. If crossing dependencies are allowed, it
is possible to parse a first-order factorization by
finding the maximum directed spanning tree (Chu
and Liu, 1965; Edmonds, 1967; McDonald et al,
2005b). Unfortunately, designing efficient higher-
order non-projective parsers is likely to be chal-
lenging, based on recent hardness results (McDon-
ald and Pereira, 2006; McDonald and Satta, 2007).
5 Extensions
We briefly outline a few extensions to our algo-
rithms; we hope to explore these in future work.
5.1 Probabilistic inference
Many statistical modeling techniques are based on
partition functions and marginals?summations
over the set of possible trees Y(x). Straightfor-
ward adaptations of the inside-outside algorithm
(Baker, 1979) to our dynamic-programming struc-
tures would suffice to compute these quantities.
5.2 Labeled parsing
Our parsers are easily extended to labeled depen-
dencies. Direct integration of labels into Models 1
and 2 would result in third-order parts composed
of three labeled dependencies, at the cost of in-
creasing the time and space complexities by fac-
tors of O(L3) and O(L2), respectively, where L
bounds the number of labels per dependency.
5.3 Word senses
If each word in x has a set of possible ?senses,?
our parsers can be modified to recover the best
joint assignment of syntax and senses for x, by
adapting methods in Eisner (2000). Complex-
ity would increase by factors of O(S4) time and
O(S
3
) space, where S bounds the number of
senses per word.
5.4 Increased context
If more vertical context is desired, the dynamic-
programming structures can be extended with ad-
ditional ancestor indices, resulting in a ?spine? of
ancestors above each span. Each additional an-
cestor lengthens the vertical scope of the factor-
ization (e.g., from grand-siblings to ?great-grand-
siblings?), while increasing complexity by a factor
of O(n). Horizontal context can also be increased
by adding internal sibling indices; each additional
sibling widens the scope of the factorization (e.g.,
from grand-siblings to ?grand-tri-siblings?), while
increasing complexity by a factor of O(n).
6 Related work
Our method augments each span with the index
of the head that governs that span, in a manner
superficially similar to parent annotation in CFGs
(Johnson, 1998). However, parent annotation is
a grammar transformation that is independent of
any particular sentence, whereas our method an-
notates spans with indices into the current sen-
tence. These indices allow the use of arbitrary fea-
tures predicated on the position of the grandparent
(e.g., word identity, POS tag, contextual POS tags)
without affecting the asymptotic complexity of the
parsing algorithm. Efficiently encoding this kind
of information into a sentence-independent gram-
mar transformation would be challenging at best.
Eisner (2000) defines dependency parsing mod-
els where each word has a set of possible ?senses?
and the parser recovers the best joint assignment
of syntax and senses. Our new parsing algorithms
could be implemented by defining the ?sense? of
each word as the index of its head. However, when
parsing with senses, the complexity of the Eisner
(2000) parser increases by factors of O(S3) time
and O(S2) space (ibid., Section 4.2). Since each
word has n potential heads, a direct application
of the word-sense parser leads to time and space
complexities of O(n6) and O(n4), respectively, in
contrast to our O(n4) and O(n3).5
Eisner (2000) also uses head automata to score
or recognize the dependents of each head. An in-
teresting question is whether these automata could
be coerced into modeling the grandparent indices
used in our parsing algorithms. However, note
that the head automata are defined in a sentence-
independent manner, with two automata per word
in the vocabulary (ibid., Section 2). The automata
are thus analogous to the rules of a CFG and at-
5In brief, the reason for the inefficiency is that the word-
sense parser is unable to exploit certain constraints, such as
the fact that the endpoints of a sibling g-span must have the
same head. The word-sense parser would needlessly enumer-
ate all possible pairs of heads in this case.
6
tempts to use them to model grandparent indices
would face difficulties similar to those already de-
scribed for grammar transformations in CFGs.
It should be noted that third-order parsers
have previously been proposed by McDonald and
Pereira (2006), who remarked that their second-
order sibling parser (see Figure 3) could easily
be extended to capture m > 1 successive modi-
fiers in O(nm+1) time (ibid., Section 2.2). To our
knowledge, however, Models 1 and 2 are the first
third-order parsing algorithms capable of model-
ing grandchild parts. In our experiments, we find
that grandchild interactions make important con-
tributions to parsing performance (see Table 3).
Carreras (2007) presents a second-order parser
that can score both sibling and grandchild parts,
with complexities of O(n4) time and O(n3) space.
An important limitation of the parser?s factoriza-
tion is that it only defines grandchild parts for
outermost grandchildren: (g, h,m) is scored only
when m is the outermost modifier of h in some di-
rection. Note that Models 1 and 2 have the same
complexity as Carreras (2007), but strictly greater
expressiveness: for each sibling or grandchild part
used in the Carreras (2007) factorization, Model 1
defines an enclosing grand-sibling, while Model 2
defines an enclosing tri-sibling or grand-sibling.
The factored parsing approach we focus on is
sometimes referred to as ?graph-based? parsing;
a popular alternative is ?transition-based? parsing,
in which trees are constructed by making a se-
ries of incremental decisions (Yamada and Mat-
sumoto, 2003; Attardi, 2006; Nivre et al, 2006;
McDonald and Nivre, 2007). Transition-based
parsers do not impose factorizations, so they can
define arbitrary features on the tree as it is being
built. As a result, however, they rely on greedy or
approximate search algorithms to solve Eq. 1.
7 Parsing experiments
In order to evaluate the effectiveness of our parsers
in practice, we apply them to the Penn WSJ Tree-
bank (Marcus et al, 1993) and the Prague De-
pendency Treebank (Hajic? et al, 2001; Hajic?,
1998).6 We use standard training, validation, and
test splits7 to facilitate comparisons. Accuracy is
6For English, we extracted dependencies using Joakim
Nivre?s Penn2Malt tool with standard head rules (Yamada
and Matsumoto, 2003); for Czech, we ?projectivized? the
training data by finding best-match projective trees.
7For Czech, the PDT has a predefined split; for English,
we split the Sections as: 2?21 training, 22 validation, 23 test.
measured with unlabeled attachment score (UAS):
the percentage of words with the correct head.8
7.1 Features for third-order parsing
Our parsing algorithms can be applied to scores
originating from any source, but in our experi-
ments we chose to use the framework of structured
linear models, deriving our scores as:
SCOREPART(x, p) = w ? f(x, p)
Here, f is a feature-vector mapping and w is a
vector of associated parameters. Following stan-
dard practice for higher-order dependency parsing
(McDonald and Pereira, 2006; Carreras, 2007),
Models 1 and 2 evaluate not only the relevant
third-order parts, but also the lower-order parts
that are implicit in their third-order factoriza-
tions. For example, Model 1 defines feature map-
pings for dependencies, siblings, grandchildren,
and grand-siblings, so that the score of a depen-
dency parse is given by:
MODEL1SCORE(x, y) =
?
(h,m)?y
wdep ? fdep(x, h,m)
?
(h,m,s)?y
wsib ? fsib(x, h,m, s)
?
(g,h,m)?y
wgch ? fgch(x, g, h,m)
?
(g,h,m,s)?y
wgsib ? fgsib(x, g, h,m, s)
Above, y is simultaneously decomposed into sev-
eral different types of parts; trivial modifications
to the Model 1 parser allow it to evaluate all of
the necessary parts in an interleaved fashion. A
similar treatment of Model 2 yields five feature
mappings: the four above plus ftsib(x, h,m, s, t),
which represents tri-sibling parts.
The lower-order feature mappings fdep, fsib, and
fgch are based on feature sets from previous work
(McDonald et al, 2005a; McDonald and Pereira,
2006; Carreras, 2007), to which we added lexical-
ized versions of several features. For example, fdep
contains lexicalized ?in-between? features that de-
pend on the head and modifier words as well as a
word lying in between the two; in contrast, pre-
vious work has generally defined in-between fea-
tures for POS tags only. As another example, our
8As in previous work, English evaluation ignores any to-
ken whose gold-standard POS tag is one of {?? ?? : , .}.
7
second-order mappings fsib and fgch define lexical
trigram features, while previous work has gener-
ally used POS trigrams only.
Our third-order feature mappings fgsib and ftsib
consist of four types of features. First, we define
4-gram features that characterize the four relevant
indices using words and POS tags; examples in-
clude POS 4-grams and mixed 4-grams with one
word and three POS tags. Second, we define 4-
gram context features consisting of POS 4-grams
augmented with adjacent POS tags: for exam-
ple, fgsib(x, g, h,m, s) includes POS 7-grams for
the tags at positions (g, h,m, s, g+1, h+1,m+1).
Third, we define backed-off features that track bi-
gram and trigram interactions which are absent
in the lower-order feature mappings: for exam-
ple, ftsib(x, h,m, s, t) contains features predicated
on the trigram (m, s, t) and the bigram (m, t),
neither of which exist in any lower-order part.
Fourth, noting that coordinations are typically an-
notated as grand-siblings (e.g., ?report purchases
and sales? in Figure 1), we define coordination
features for certain grand-sibling parts. For exam-
ple, fgsib(x, g, h,m, s) contains features examin-
ing the implicit head-modifier relationship (g,m)
that are only activated when the POS tag of s is a
coordinating conjunction.
Finally, we make two brief remarks regarding
the use of POS tags. First, we assume that input
sentences have been automatically tagged in a pre-
processing step.9 Second, for any feature that de-
pends on POS tags, we include two copies of the
feature: one using normal POS tags and another
using coarsened versions10 of the POS tags.
7.2 Averaged perceptron training
There are a wide variety of parameter estima-
tion methods for structured linear models, such
as log-linear models (Lafferty et al, 2001) and
max-margin models (Taskar et al, 2003). We
chose the averaged structured perceptron (Freund
and Schapire, 1999; Collins, 2002) as it combines
highly competitive performance with fast training
times, typically converging in 5?10 iterations. We
train each parser for 10 iterations and select pa-
9For Czech, the PDT provides automatic tags; for English,
we used MXPOST (Ratnaparkhi, 1996) to tag validation and
test data, with 10-fold cross-validation on the training set.
Note that the reliance on POS-tagged input can be relaxed
slightly by treating POS tags as word senses; see Section 5.3
and McDonald (2006, Table 6.1).
10For Czech, we used the first character of the tag; for En-
glish, we used the first two characters, except PRP and PRP$.
Beam Pass Orac Acc1 Acc2 Time1 Time2
0.0001 26.5 99.92 93.49 93.49 49.6m 73.5m
0.001 16.7 99.72 93.37 93.29 25.9m 24.2m
0.01 9.1 99.19 93.26 93.16 6.7m 7.9m
Table 1: Effect of the marginal-probability beam
on English parsing. For each beam value, parsers
were trained on the English training set and evalu-
ated on the English validation set; the same beam
value was applied to both training and validation
data. Pass = %dependencies surviving the beam in
training data, Orac = maximum achievable UAS
on validation data, Acc1/Acc2 = UAS of Models
1/2 on validation data, and Time1/Time2 = min-
utes per perceptron training iteration for Models
1/2, averaged over all 10 iterations. For perspec-
tive, the English training set has a total of 39,832
sentences and 950,028 words. A beam of 0.0001
was used in all experiments outside this table.
rameters from the iteration that achieves the best
score on the validation set.
7.3 Coarse-to-fine pruning
In order to decrease training times, we follow
Carreras et al (2008) and eliminate unlikely de-
pendencies using a form of coarse-to-fine pruning
(Charniak and Johnson, 2005; Petrov and Klein,
2007). In brief, we train a log-linear first-order
parser11 and for every sentence x in training, val-
idation, and test data we compute the marginal
probability P (h,m |x) of each dependency. Our
parsers are then modified to ignore any depen-
dency (h,m) whose marginal probability is below
0.0001?maxh? P (h?,m |x). Table 1 provides in-
formation on the behavior of the pruning method.
7.4 Main results
Table 2 lists the accuracy of Models 1 and 2 on the
English and Czech test sets, together with some
relevant results from related work.12 The mod-
els marked ??? are not directly comparable to our
work as they depend on additional sources of in-
formation that our models are trained without?
unlabeled data in the case of Koo et al (2008) and
11For English, we generate marginals using a projective
parser (Baker, 1979; Eisner, 2000); for Czech, we generate
marginals using a non-projective parser (Smith and Smith,
2007; McDonald and Satta, 2007; Koo et al, 2007). Param-
eters for these models are obtained by running exponentiated
gradient training for 10 iterations (Collins et al, 2008).
12Model 0 was not tested as its factorization is a strict sub-
set of the factorization of Model 1.
8
Parser Eng Cze
McDonald et al (2005a,2005b) 90.9 84.4
McDonald and Pereira (2006) 91.5 85.2
Koo et al (2008), standard 92.02 86.13
Model 1 93.04 87.38
Model 2 92.93 87.37
Koo et al (2008), semi-sup? 93.16 87.13
Suzuki et al (2009)? 93.79 88.05
Carreras et al (2008)? 93.5
Table 2: UAS of Models 1 and 2 on test data, with
relevant results from related work. Note that Koo
et al (2008) is listed with standard features and
semi-supervised features. ?: see main text.
Suzuki et al (2009) and phrase-structure annota-
tions in the case of Carreras et al (2008). All three
of the ??? models are based on versions of the Car-
reras (2007) parser, so modifying these methods to
work with our new third-order parsing algorithms
would be an interesting topic for future research.
For example, Models 1 and 2 obtain results com-
parable to the semi-supervised parsers of Koo et
al. (2008), and additive gains might be realized by
applying their cluster-based feature sets to our en-
riched factorizations.
7.5 Ablation studies
In order to better understand the contributions of
the various feature types, we ran additional abla-
tion experiments; the results are listed in Table 3,
in addition to the scores of Model 0 and the emu-
lated Carreras (2007) parser (see Section 4.3). In-
terestingly, grandchild interactions appear to pro-
vide important information: for example, when
Model 2 is used without grandchild-based features
(?Model 2, no-G? in Table 3), its accuracy suffers
noticeably. In addition, it seems that grandchild
interactions are particularly useful in Czech, while
sibling interactions are less important: consider
that Model 0, a second-order grandchild parser
with no sibling-based features, can easily outper-
form ?Model 2, no-G,? a third-order sibling parser
with no grandchild-based features.
8 Conclusion
We have presented new parsing algorithms that are
capable of efficiently parsing third-order factoriza-
tions, including both grandchild and sibling inter-
actions. Due to space restrictions, we have been
necessarily brief at some points in this paper; some
additional details can be found in Koo (2010).
Parser Eng Cze
Model 0 93.07 87.39
Carreras (2007) emulation 93.14 87.25
Model 1 93.49 87.64
Model 1, no-3rd 93.17 87.57
Model 2 93.49 87.46
Model 2, no-3rd 93.20 87.43
Model 2, no-G 92.92 86.76
Table 3: UAS for modified versions of our parsers
on validation data. The term no-3rd indicates a
parser that was trained and tested with the third-
order feature mappings fgsib and ftsib deactivated,
though lower-order features were retained; note
that ?Model 2, no-3rd? is not identical to the Car-
reras (2007) parser as it defines grandchild parts
for the pair of grandchildren. The term no-G indi-
cates a parser that was trained and tested with the
grandchild-based feature mappings fgch and fgsib
deactivated; note that ?Model 2, no-G? emulates
the third-order sibling parser proposed by McDon-
ald and Pereira (2006).
There are several possibilities for further re-
search involving our third-order parsing algo-
rithms. One idea would be to consider extensions
and modifications of our parsers, some of which
have been suggested in Sections 5 and 7.4. A sec-
ond area for future work lies in applications of de-
pendency parsing. While we have evaluated our
new algorithms on standard parsing benchmarks,
there are a wide variety of tasks that may bene-
fit from the extended context offered by our third-
order factorizations; for example, the 4-gram sub-
structures enabled by our approach may be useful
for dependency-based language modeling in ma-
chine translation (Shen et al, 2008). Finally, in
the hopes that others in the NLP community may
find our parsers useful, we provide a free distribu-
tion of our implementation.2
Acknowledgments
We would like to thank the anonymous review-
ers for their helpful comments and suggestions.
We also thank Regina Barzilay and Alexander
Rush for their much-appreciated input during the
writing process. The authors gratefully acknowl-
edge the following sources of support: Terry
Koo and Michael Collins were both funded by
a DARPA subcontract under SRI (#27-001343),
and Michael Collins was additionally supported
by NTT (Agmt. dtd. 06/21/98).
9
References
Giuseppe Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Pro-
ceedings of the 10th CoNLL, pages 166?170. Asso-
ciation for Computational Linguistics.
James Baker. 1979. Trainable Grammars for Speech
Recognition. In Proceedings of the 97th meeting of
the Acoustical Society of America.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, Dynamic Programming, and the Per-
ceptron for Efficient, Feature-rich Parsing. In Pro-
ceedings of the 12th CoNLL, pages 9?16. Associa-
tion for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL, pages 957?961. Association for Computa-
tional Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine N -best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd ACL.
Y.J. Chu and T.H. Liu. 1965. On the Shortest Ar-
borescence of a Directed Graph. Science Sinica,
14:1396?1400.
John Cocke and Jacob T. Schwartz. 1970. Program-
ming Languages and Their Compilers: Preliminary
Notes. Technical report, New York University.
Michael Collins, Amir Globerson, Terry Koo, Xavier
Carreras, and Peter L. Bartlett. 2008. Exponenti-
ated Gradient Algorithms for Conditional Random
Fields and Max-Margin Markov Networks. Journal
of Machine Learning Research, 9:1775?1822, Aug.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the 7th EMNLP, pages 1?8. Association for Com-
putational Linguistics.
Jack R. Edmonds. 1967. Optimum Branchings. Jour-
nal of Research of the National Bureau of Standards,
71B:233?240.
Jason Eisner. 1996. Three New Probabilistic Models
for Dependency Parsing: An Exploration. In Pro-
ceedings of the 16th COLING, pages 340?345. As-
sociation for Computational Linguistics.
Jason Eisner. 2000. Bilexical Grammars and Their
Cubic-Time Parsing Algorithms. In Harry Bunt
and Anton Nijholt, editors, Advances in Probabilis-
tic and Other Parsing Technologies, pages 29?62.
Kluwer Academic Publishers.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. Machine Learning, 37(3):277?296.
Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila Panevova,
and Petr Sgall. 2001. The Prague Dependency Tree-
bank 1.0, LDC No. LDC2001T10. Linguistics Data
Consortium.
Jan Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevova?, pages 12?19.
Mark Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613?632.
Tadao Kasami. 1965. An Efficient Recognition and
Syntax-analysis Algorithm for Context-free Lan-
guages. Technical Report AFCRL-65-758, Air
Force Cambridge Research Lab.
Terry Koo, Amir Globerson, Xavier Carreras, and
Michael Collins. 2007. Structured Prediction Mod-
els via the Matrix-Tree Theorem. In Proceedings
of EMNLP-CoNLL, pages 141?150. Association for
Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of the 46th ACL, pages 595?603.
Association for Computational Linguistics.
Terry Koo. 2010. Advances in Discriminative Depen-
dency Parsing. Ph.D. thesis, Massachusetts Institute
of Technology, Cambridge, MA, USA, June.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the 18th ICML,
pages 282?289. Morgan Kaufmann.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David A. McAllester. 1999. On the Complexity
Analysis of Static Analyses. In Proceedings of
the 6th Static Analysis Symposium, pages 312?329.
Springer-Verlag.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsers.
In Proceedings of EMNLP-CoNLL, pages 122?131.
Association for Computational Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Al-
gorithms. In Proceedings of the 11th EACL, pages
81?88. Association for Computational Linguistics.
Ryan McDonald and Giorgio Satta. 2007. On the
Complexity of Non-Projective Data-Driven Depen-
dency Parsing. In Proceedings of IWPT.
10
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online Large-Margin Training of
Dependency Parsers. In Proceedings of the 43rd
ACL, pages 91?98. Association for Computational
Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-Projective Dependency
Parsing using Spanning Tree Algorithms. In Pro-
ceedings of HLT-EMNLP, pages 523?530. Associa-
tion for Computational Linguistics.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA, USA, July.
Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en
Eryig?it, and Svetoslav Marinov. 2006. Labeled
Pseudo-Projective Dependency Parsing with Sup-
port Vector Machines. In Proceedings of the 10th
CoNLL, pages 221?225. Association for Computa-
tional Linguistics.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proceedings of HLT-
NAACL, pages 404?411. Association for Computa-
tional Linguistics.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proceedings
of the 1st EMNLP, pages 133?142. Association for
Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th ACL, pages 577?
585. Association for Computational Linguistics.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic Models of Nonprojective Dependency Trees.
In Proceedings of EMNLP-CoNLL, pages 132?140.
Association for Computational Linguistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An Empirical Study of
Semi-supervised Structured Conditional Models for
Dependency Parsing. In Proceedings of EMNLP,
pages 551?560. Association for Computational Lin-
guistics.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max margin markov networks. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard Scho?lkopf, editors,
NIPS. MIT Press.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector Ma-
chines. In Proceedings of the 8th IWPT, pages 195?
206. Association for Computational Linguistics.
David H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
11
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 72?82,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Exact Decoding of Syntactic Translation Models
through Lagrangian Relaxation
Alexander M. Rush
MIT CSAIL,
Cambridge, MA 02139, USA
srush@csail.mit.edu
Michael Collins
Department of Computer Science,
Columbia University,
New York, NY 10027, USA
mcollins@cs.columbia.edu
Abstract
We describe an exact decoding algorithm for
syntax-based statistical translation. The ap-
proach uses Lagrangian relaxation to decom-
pose the decoding problem into tractable sub-
problems, thereby avoiding exhaustive dy-
namic programming. The method recovers ex-
act solutions, with certificates of optimality,
on over 97% of test examples; it has compa-
rable speed to state-of-the-art decoders.
1 Introduction
Recent work has seen widespread use of syn-
chronous probabilistic grammars in statistical ma-
chine translation (SMT). The decoding problem for
a broad range of these systems (e.g., (Chiang, 2005;
Marcu et al, 2006; Shen et al, 2008)) corresponds
to the intersection of a (weighted) hypergraph with
an n-gram language model.1 The hypergraph rep-
resents a large set of possible translations, and is
created by applying a synchronous grammar to the
source language string. The language model is then
used to rescore the translations in the hypergraph.
Decoding with these models is challenging,
largely because of the cost of integrating an n-gram
language model into the search process. Exact dy-
namic programming algorithms for the problem are
well known (Bar-Hillel et al, 1964), but are too ex-
pensive to be used in practice.2 Previous work on
decoding for syntax-based SMT has therefore been
focused primarily on approximate search methods.
This paper describes an efficient algorithm for ex-
act decoding of synchronous grammar models for
translation. We avoid the construction of (Bar-Hillel
1This problem is also relevant to other areas of statistical
NLP, for example NL generation (Langkilde, 2000).
2E.g., with a trigram language model they run in O(|E|w6)
time, where |E| is the number of edges in the hypergraph, and
w is the number of distinct lexical items in the hypergraph.
et al, 1964) by using Lagrangian relaxation to de-
compose the decoding problem into the following
sub-problems:
1. Dynamic programming over the weighted hy-
pergraph. This step does not require language
model integration, and hence is highly efficient.
2. Application of an all-pairs shortest path al-
gorithm to a directed graph derived from the
weighted hypergraph. The size of the derived
directed graph is linear in the size of the hyper-
graph, hence this step is again efficient.
Informally, the first decoding algorithm incorporates
the weights and hard constraints on translations from
the synchronous grammar, while the second decod-
ing algorithm is used to integrate language model
scores. Lagrange multipliers are used to enforce
agreement between the structures produced by the
two decoding algorithms.
In this paper we first give background on hyper-
graphs and the decoding problem. We then describe
our decoding algorithm. The algorithm uses a sub-
gradient method to minimize a dual function. The
dual corresponds to a particular linear programming
(LP) relaxation of the original decoding problem.
The method will recover an exact solution, with a
certificate of optimality, if the underlying LP relax-
ation has an integral solution. In some cases, how-
ever, the underlying LP will have a fractional solu-
tion, in which case the method will not be exact. The
second technical contribution of this paper is to de-
scribe a method that iteratively tightens the underly-
ing LP relaxation until an exact solution is produced.
We do this by gradually introducing constraints to
step 1 (dynamic programming over the hypergraph),
while still maintaining efficiency.
72
We report experiments using the tree-to-string
model of (Huang and Mi, 2010). Our method gives
exact solutions on over 97% of test examples. The
method is comparable in speed to state-of-the-art de-
coding algorithms; for example, over 70% of the test
examples are decoded in 2 seconds or less. We com-
pare our method to cube pruning (Chiang, 2007),
and find that our method gives improved model
scores on a significant number of examples. One
consequence of our work is that we give accurate
estimates of the number of search errors for cube
pruning.
2 Related Work
A variety of approximate decoding algorithms have
been explored for syntax-based translation systems,
including cube-pruning (Chiang, 2007; Huang and
Chiang, 2007), left-to-right decoding with beam
search (Watanabe et al, 2006; Huang and Mi, 2010),
and coarse-to-fine methods (Petrov et al, 2008).
Recent work has developed decoding algorithms
based on finite state transducers (FSTs). Iglesias et
al. (2009) show that exact FST decoding is feasible
for a phrase-based system with limited reordering
(the MJ1 model (Kumar and Byrne, 2005)), and de
Gispert et al (2010) show that exact FST decoding
is feasible for a specific class of hierarchical gram-
mars (shallow-1 grammars). Approximate search
methods are used for more complex reordering mod-
els or grammars. The FST algorithms are shown to
produce higher scoring solutions than cube-pruning
on a large proportion of examples.
Lagrangian relaxation is a classical technique
in combinatorial optimization (Korte and Vygen,
2008). Lagrange multipliers are used to add lin-
ear constraints to an existing problem that can be
solved using a combinatorial algorithm; the result-
ing dual function is then minimized, for example
using subgradient methods. In recent work, dual
decomposition?a special case of Lagrangian relax-
ation, where the linear constraints enforce agree-
ment between two or more models?has been ap-
plied to inference in Markov random fields (Wain-
wright et al, 2005; Komodakis et al, 2007; Sontag
et al, 2008), and also to inference problems in NLP
(Rush et al, 2010; Koo et al, 2010). There are close
connections between dual decomposition and work
on belief propagation (Smith and Eisner, 2008).
3 Background: Hypergraphs
Translation with many syntax-based systems (e.g.,
(Chiang, 2005; Marcu et al, 2006; Shen et al, 2008;
Huang and Mi, 2010)) can be implemented as a
two-step process. The first step is to take an in-
put sentence in the source language, and from this
to create a hypergraph (sometimes called a transla-
tion forest) that represents the set of possible trans-
lations (strings in the target language) and deriva-
tions under the grammar. The second step is to
integrate an n-gram language model with this hy-
pergraph. For example, in the system of (Chiang,
2005), the hypergraph is created as follows: first, the
source side of the synchronous grammar is used to
create a parse forest over the source language string.
Second, transduction operations derived from syn-
chronous rules in the grammar are used to create the
target-language hypergraph. Chiang?s method uses
a synchronous context-free grammar, but the hyper-
graph formalism is applicable to a broad range of
other grammatical formalisms, for example depen-
dency grammars (e.g., (Shen et al, 2008)).
A hypergraph is a pair (V,E) where V =
{1, 2, . . . , |V |} is a set of vertices, and E is a set of
hyperedges. A single distinguished vertex is taken
as the root of the hypergraph; without loss of gener-
ality we take this vertex to be v = 1. Each hyper-
edge e ? E is a tuple ??v1, v2, . . . , vk?, v0? where
v0 ? V , and vi ? {2 . . . |V |} for i = 1 . . . k. The
vertex v0 is referred to as the head of the edge. The
ordered sequence ?v1, v2, . . . , vk? is referred to as
the tail of the edge; in addition, we sometimes refer
to v1, v2, . . . vk as the children in the edge. The num-
ber of children k may vary across different edges,
but k ? 1 for all edges (i.e., each edge has at least
one child). We will use h(e) to refer to the head of
an edge e, and t(e) to refer to the tail.
We will assume that the hypergraph is acyclic: in-
tuitively this will mean that no derivation (as defined
below) contains the same vertex more than once (see
(Martin et al, 1990) for a formal definition).
Each vertex v ? V is either a non-terminal in the
hypergraph, or a leaf. The set of non-terminals is
VN = {v ? V : ?e ? E such that h(e) = v}
Conversely, the set of leaves is defined as
VL = {v ? V : 6 ?e ? E such that h(e) = v}
73
Finally, we assume that each v ? V has a label
l(v). The labels for leaves will be words, and will
be important in defining strings and language model
scores for those strings. The labels for non-terminal
nodes will not be important for results in this paper.3
We now turn to derivations. Define an index set
I = V ? E. A derivation is represented by a vector
y = {yr : r ? I} where yv = 1 if vertex v is used in
the derivation, yv = 0 otherwise (similarly ye = 1 if
edge e is used in the derivation, ye = 0 otherwise).
Thus y is a vector in {0, 1}|I|. A valid derivation
satisfies the following constraints:
? y1 = 1 (the root must be in the derivation).
? For all v ? VN , yv =
?
e:h(e)=v ye.
? For all v ? 2 . . . |V |, yv =
?
e:v?t(e) ye.
We use Y to refer to the set of valid derivations.
The set Y is a subset of {0, 1}|I| (not all members of
{0, 1}|I| will correspond to valid derivations).
Each derivation y in the hypergraph will imply an
ordered sequence of leaves v1 . . . vn. We use s(y) to
refer to this sequence. The sentence associated with
the derivation is then l(v1) . . . l(vn).
In a weighted hypergraph problem, we assume a
parameter vector ? = {?r : r ? I}. The score for
any derivation is f(y) = ? ? y = ?r?I ?ryr. Sim-
ple bottom-up dynamic programming?essentially
the CKY algorithm?can be used to find y? =
argmaxy?Y f(y) under these definitions.
The focus of this paper will be to solve problems
involving the integration of a k?th order language
model with a hypergraph. In these problems, the
score for a derivation is modified to be
f(y) =
?
r?I
?ryr +
n
?
i=k
?(vi?k+1, vi?k+2, . . . , vi) (1)
where v1 . . . vn = s(y). The ?(vi?k+1, . . . , vi)
parameters score n-grams of length k. These
parameters are typically defined by a language
model, for example with k = 3 we would have
?(vi?2, vi?1, vi) = log p(l(vi)|l(vi?2), l(vi?1)).
The problem is then to find y? = argmaxy?Y f(y)
under this definition.
Throughout this paper we make the following as-
sumption when using a bigram language model:
3They might for example be non-terminal symbols from the
grammar used to generate the hypergraph.
Assumption 3.1 (Bigram start/end assump-
tion.) For any derivation y, with leaves
s(y) = v1, v2, . . . , vn, it is the case that: (1)
v1 = 2 and vn = 3; (2) the leaves 2 and 3 cannot
appear at any other position in the strings s(y) for
y ? Y; (3) l(2) = <s> where <s> is the start
symbol in the language model; (4) l(3) = </s>
where </s> is the end symbol.
This assumption allows us to incorporate lan-
guage model terms that depend on the start and end
symbols. It also allows a clean solution for boundary
conditions (the start/end of strings).4
4 A Simple Lagrangian Relaxation
Algorithm
We now give a Lagrangian relaxation algorithm for
integration of a hypergraph with a bigram language
model, in cases where the hypergraph satisfies the
following simplifying assumption:
Assumption 4.1 (The strict ordering assumption.)
For any two leaves v and w, it is either the case
that: 1) for all derivations y such that v and w are
both in the sequence l(y), v precedes w; or 2) for all
derivations y such that v and w are both in l(y), w
precedes v.
Thus under this assumption, the relative ordering
of any two leaves is fixed. This assumption is overly
restrictive:5 the next section describes an algorithm
that does not require this assumption. However de-
riving the simple algorithm will be useful in devel-
oping intuition, and will lead directly to the algo-
rithm for the unrestricted case.
4.1 A Sketch of the Algorithm
At a high level, the algorithm is as follows. We in-
troduce Lagrange multipliers u(v) for all v ? VL,
with initial values set to zero. The algorithm then
involves the following steps: (1) For each leaf v,
find the previous leaf w that maximizes the score
?(w, v) ? u(w) (call this leaf ??(v), and define
?v = ?(??(v), v) ? u(??(v))). (2) find the high-
est scoring derivation using dynamic programming
4The assumption generalizes in the obvious way to k?th or-
der language models: e.g., for trigram models we assume that
v1 = 2, v2 = 3, vn = 4, l(2) = l(3) = <s>, l(4) = </s>.
5It is easy to come up with examples that violate this as-
sumption: for example a hypergraph with edges ??4, 5?, 1? and
??5, 4?, 1? violates the assumption. The hypergraphs found in
translation frequently contain alternative orderings such as this.
74
over the original (non-intersected) hypergraph, with
leaf nodes having weights ?v + ?v + u(v). (3) If
the output derivation from step 2 has the same set of
bigrams as those from step 1, then we have an exact
solution to the problem. Otherwise, the Lagrange
multipliers u(v) are modified in a way that encour-
ages agreement of the two steps, and we return to
step 1.
Steps 1 and 2 can be performed efficiently; in par-
ticular, we avoid the classical dynamic programming
intersection, instead relying on dynamic program-
ming over the original, simple hypergraph.
4.2 A Formal Description
We now give a formal description of the algorithm.
Define B ? VL?VL to be the set of all ordered pairs
?v, w? such that there is at least one derivation y with
v directly preceding w in s(y). Extend the bit-vector
y to include variables y(v, w) for ?v, w? ? B where
y(v, w) = 1 if leaf v is followed by w in s(y), 0
otherwise. We redefine the index set to be I = V ?
E ? B, and define Y ? {0, 1}|I| to be the set of all
possible derivations. Under assumptions 3.1 and 4.1
above, Y = {y : y satisfies constraints C0, C1, C2}
where the constraint definitions are:
? (C0) The yv and ye variables form a derivation
in the hypergraph, as defined in section 3.
? (C1) For all v ? VL such that v 6= 2, yv =
?
w:?w,v??B y(w, v).
? (C2) For all v ? VL such that v 6= 3, yv =
?
w:?v,w??B y(v, w).
C1 states that each leaf in a derivation has exactly
one in-coming bigram, and that each leaf not in the
derivation has 0 incoming bigrams; C2 states that
each leaf in a derivation has exactly one out-going
bigram, and that each leaf not in the derivation has 0
outgoing bigrams.6
The score of a derivation is now f(y) = ? ? y, i.e.,
f(y) =
?
v
?vyv+
?
e
?eye+
?
?v,w??B
?(v, w)y(v, w)
where ?(v, w) are scores from the language model.
Our goal is to compute y? = argmaxy?Y f(y).
6Recall that according to the bigram start/end assumption
the leaves 2/3 are reserved for the start/end of the sequence
s(y), and hence do not have an incoming/outgoing bigram.
Initialization: Set u0(v) = 0 for all v ? VL
Algorithm: For t = 1 . . . T :
? yt = argmaxy?Y? L(ut?1, y)
? If yt satisfies constraints C2, return yt,
Else ?v ? VL, ut(v) =
ut?1(v)? ?t
(
yt(v)??w:?v,w??B yt(v, w)
)
.
Figure 1: A simple Lagrangian relaxation algorithm.
?t > 0 is the step size at iteration t.
Next, define Y ? as
Y ? = {y : y satisfies constraints C0 and C1}
In this definition we have dropped the C2 con-
straints. To incorporate these constraints, we use
Lagrangian relaxation, with one Lagrange multiplier
u(v) for each constraint in C2. The Lagrangian is
L(u, y) = f(y) +
?
v
u(v)(y(v)?
?
w:?v,w??B
y(v, w))
= ? ? y
where ?v = ?v + u(v), ?e = ?e, and ?(v, w) =
?(v, w)? u(v).
The dual problem is to find minu L(u) where
L(u) = max
y?Y ?
L(u, y)
Figure 1 shows a subgradient method for solving
this problem. At each point the algorithm finds
yt = argmaxy?Y ? L(ut?1, y), where ut?1 are the
Lagrange multipliers from the previous iteration. If
yt satisfies the C2 constraints in addition to C0 and
C1, then it is returned as the output from the algo-
rithm. Otherwise, the multipliers u(v) are updated.
Intuitively, these updates encourage the values of yv
and
?
w:?v,w??B y(v, w) to be equal; formally, these
updates correspond to subgradient steps.
The main computational step at each iteration is to
compute argmaxy?Y ? L(ut?1, y) This step is easily
solved, as follows (we again use ?v, ?e and ?(v1, v2)
to refer to the parameter values that incorporate La-
grange multipliers):
? For all v ? VL, define ??(v) =
argmaxw:?w,v??B ?(w, v) and ?v =
?(??(v), v). For all v ? VN define ?v = 0.
75
? Using dynamic programming, find values for
the yv and ye variables that form a valid deriva-
tion, and that maximize
f ?(y) =
?
v(?v + ?v)yv +
?
e ?eye.
? Set y(v, w) = 1 iff y(w) = 1 and ??(w) = v.
The critical point here is that through our definition
of Y ?, which ignores the C2 constraints, we are able
to do efficient search as just described. In the first
step we compute the highest scoring incoming bi-
gram for each leaf v. In the second step we use
conventional dynamic programming over the hyper-
graph to find an optimal derivation that incorporates
weights from the first step. Finally, we fill in the
y(v, w) values. Each iteration of the algorithm runs
in O(|E|+ |B|) time.
There are close connections between Lagrangian
relaxation and linear programming relaxations. The
most important formal results are: 1) for any value
of u, L(u) ? f(y?) (hence the dual value provides
an upper bound on the optimal primal value); 2) un-
der an appropriate choice of the step sizes ?t, the
subgradient algorithm is guaranteed to converge to
the minimum of L(u) (i.e., we will minimize the
upper bound, making it as tight as possible); 3) if
at any point the algorithm in figure 1 finds a yt that
satisfies the C2 constraints, then this is guaranteed
to be the optimal primal solution.
Unfortunately, this algorithm may fail to produce
a good solution for hypergraphs where the strict or-
dering constraint does not hold. In this case it is
possible to find derivations y that satisfy constraints
C0, C1, C2, but which are invalid. As one exam-
ple, consider a derivation with s(y) = 2, 4, 5, 3 and
y(2, 3) = y(4, 5) = y(5, 4) = 1. The constraints
are all satisfied in this case, but the bigram variables
are invalid (e.g., they contain a cycle).
5 The Full Algorithm
We now describe our full algorithm, which does not
require the strict ordering constraint. In addition, the
full algorithm allows a trigram language model. We
first give a sketch, and then give a formal definition.
5.1 A Sketch of the Algorithm
A crucial idea in the new algorithm is that of
paths between leaves in hypergraph derivations.
Previously, for each derivation y, we had de-
fined s(y) = v1, v2, . . . , vn to be the sequence
of leaves in y. In addition, we will define
g(y) = p0, v1, p1, v2, p2, v3, p3, . . . , pn?1, vn, pn
where each pi is a path in the derivation between
leaves vi and vi+1. The path traces through the non-
terminals that are between the two leaves in the tree.
As an example, consider the following derivation
(with hyperedges ??2, 5?, 1? and ??3, 4?, 2?):
1
2
3 4
5
For this example g(y) is ?1 ?, 2 ?? ?2 ?, 3 ??
?3 ??, 3, ?3 ?? ?3 ?, 4 ?? ?4 ??, 4, ?4 ?? ?4 ?, 2 ??
?2 ?, 5 ?? ?5 ??, 5, ?5 ?? ?5 ?, 1 ??. States of the
form ?a ?? and ?a ?? where a is a leaf appear in
the paths respectively before/after the leaf a. States
of the form ?a, b? correspond to the steps taken in a
top-down, left-to-right, traversal of the tree, where
down and up arrows indicate whether a node is be-
ing visited for the first or second time (the traversal
in this case would be 1, 2, 3, 4, 2, 5, 1).
The mapping from a derivation y to a path g(y)
can be performed using the algorithm in figure 2.
For a given derivation y, define E(y) = {y : ye =
1}, and use E(y) as the set of input edges to this
algorithm. The output from the algorithm will be a
set of states S, and a set of directed edges T , which
together fully define the path g(y).
In the simple algorithm, the first step was to
predict the previous leaf for each leaf v, under
a score that combined a language model score
with a Lagrange multiplier score (i.e., compute
argmaxw ?(w, v) where ?(w, v) = ?(w, v) +
u(w)). In this section we describe an algorithm that
for each leaf v again predicts the previous leaf, but in
addition predicts the full path back to that leaf. For
example, rather than making a prediction for leaf 5
that it should be preceded by leaf 4, we would also
predict the path ?4 ???4 ?, 2 ?? ?2 ?, 5 ???5 ?? be-
tween these two leaves. Lagrange multipliers will
be used to enforce consistency between these pre-
dictions (both paths and previous words) and a valid
derivation.
76
Input: A set E of hyperedges. Output: A directed graph
S, T where S is a set of vertices, and T is a set of edges.
Step 1: Creating S: Define S = ?e?ES(e) where S(e)
is defined as follows. Assume e = ??v1, v2, . . . , vk?, v0?.
Include the following states in S(e): (1) ?v0 ?, v1 ?? and
?vk?, v0??. (2) ?vj ?, vj+1?? for j = 1 . . . k ? 1 (if k = 1
then there are no such states). (3) In addition, for any vj
for j = 1 . . . k such that vj ? VL, add the states ?vj ??
and ?vj ??.
Step 2: Creating T : T is formed by including the fol-
lowing directed arcs: (1) Add an arc from ?a, b? ? S
to ?c, d? ? S whenever b = c. (2) Add an arc from
?a, b ?? ? S to ?c ?? ? S whenever b = c. (3) Add
an arc from ?a ?? ? S to ?b ?, c? ? S whenever a = b.
Figure 2: Algorithm for constructing a directed graph
(S, T ) from a set of hyperedges E.
5.2 A Formal Description
We first use the algorithm in figure 2 with the en-
tire set of hyperedges, E, as its input. The result
is a directed graph (S, T ) that contains all possible
paths for valid derivations in V,E (it also contains
additional, ill-formed paths). We then introduce the
following definition:
Definition 5.1 A trigram path p is p =
?v1, p1, v2, p2, v3? where: a) v1, v2, v3 ? VL;
b) p1 is a path (sequence of states) between nodes
?v1 ?? and ?v2 ?? in the graph (S, T ); c) p2 is a
path between nodes ?v2 ?? and ?v3 ?? in the graph
(S, T ). We define P to be the set of all trigram paths
in (S, T ).
The set P of trigram paths plays an analogous role
to the set B of bigrams in our previous algorithm.
We use v1(p), p1(p), v2(p), p2(p), v3(p) to refer
to the individual components of a path p. In addi-
tion, define SN to be the set of states in S of the
form ?a, b? (as opposed to the form ?c ?? or ?c ??
where c ? VL).
We now define a new index set, I = V ? E ?
SN ? P , adding variables ys for s ? SN , and yp for
p ? P . If we take Y ? {0, 1}|I| to be the set of
valid derivations, the optimization problem is to find
y? = argmaxy?Y f(y), where f(y) = ? ? y, that is,
f(y) =
?
v
?vyv +
?
e
?eye +
?
s
?sys +
?
p
?pyp
In particular, we might define ?s = 0 for all s,
and ?p = log p(l(v3(p))|l(v1(p)), l(v2(p))) where
? D0. The yv and ye variables form a valid derivation
in the original hypergraph.
? D1. For all s ? SN , ys =
?
e:s?S(e) ye (see figure 2
for the definition of S(e)).
? D2. For all v ? VL, yv =
?
p:v3(p)=v yp
? D3. For all v ? VL, yv =
?
p:v2(p)=v yp
? D4. For all v ? VL, yv =
?
p:v1(p)=v yp
? D5. For all s ? SN , ys =
?
p:s?p1(p) yp
? D6. For all s ? SN , ys =
?
p:s?p2(p) yp
? Lagrangian with Lagrange multipliers for D3?D6:
L(y, ?, ?, u, v) = ? ? y
+
?
v ?v
(
yv ?
?
p:v2(p)=v yp
)
+
?
v ?v
(
yv ?
?
p:v1(p)=v yp
)
+
?
s us
(
ys ?
?
p:s?p1(p) yp
)
+
?
s vs
(
ys ?
?
p:s?p2(p) yp
)
.
Figure 3: Constraints D0?D6, and the Lagrangian.
p(w3|w1, w2) is a trigram probability.
The set P is large (typically exponential in size):
however, we will see that we do not need to represent
the yp variables explicitly. Instead we will be able
to leverage the underlying structure of a path as a
sequence of states.
The set of valid derivations is Y = {y :
y satisfies constraints D0?D6} where the constraints
are shown in figure 3. D1 simply states that ys = 1
iff there is exactly one edge e in the derivation such
that s ? S(e). Constraints D2?D4 enforce consis-
tency between leaves in the trigram paths, and the yv
values. Constraints D5 and D6 enforce consistency
between states seen in the paths, and the ys values.
The Lagrangian relaxation algorithm is then de-
rived in a similar way to before. Define
Y ? = {y : y satisfies constraints D0?D2}
We have dropped the D3?D6 constraints, but these
will be introduced using Lagrange multipliers. The
resulting Lagrangian is shown in figure 3, and can
be written as L(y, ?, ?, u, v) = ? ? y where ?v =
?v+?v+?v, ?s = ?s+us+vs, ?p = ?p??(v2(p))?
?(v1(p))?
?
s?p1(p) u(s)?
?
s?p2(p) v(s).
The dual is L(?, ?, u, v) =
maxy?Y ? L(y, ?, ?, u, v); figure 4 shows a sub-
gradient method that minimizes this dual. The key
step in the algorithm at each iteration is to compute
77
Initialization: Set ?0 = 0, ?0 = 0, u0 = 0, v0 = 0
Algorithm: For t = 1 . . . T :
? yt = argmaxy?Y? L(y, ?t?1, ?t?1, ut?1, vt?1)
? If yt satisfies the constraints D3?D6, return yt, else:
- ?v ? VL, ?tv = ?t?1v ? ?t(ytv ?
?
p:v2(p)=v y
t
p)
- ?v ? VL, ?tv = ?t?1v ? ?t(ytv ?
?
p:v1(p)=v y
t
p)
- ?s ? SN , uts = ut?1s ? ?t(yts ?
?
p:s?p1(p) y
t
p)
- ?s ? SN , vts = vt?1s ? ?t(yts ?
?
p:s?p2(p) y
t
p)
Figure 4: The full Lagrangian relaxation algortihm. ?t >
0 is the step size at iteration t.
argmaxy?Y ? L(y, ?, ?, u, v) = argmaxy?Y ? ? ? y
where ? is defined above. Again, our definition
of Y ? allows this maximization to be performed
efficiently, as follows:
1. For each v ? VL, define ??v =
argmaxp:v3(p)=v ?(p), and ?v = ?(??v).
(i.e., for each v, compute the highest scoring
trigram path ending in v.)
2. Find values for the yv, ye and ys variables that
form a valid derivation, and that maximize
f ?(y) =
?
v(?v+?v)yv+
?
e ?eye+
?
s ?sys
3. Set yp = 1 iff yv3(p) = 1 and p = ??v3(p).
The first step involves finding the highest scoring in-
coming trigram path for each leaf v. This step can be
performed efficiently using the Floyd-Warshall all-
pairs shortest path algorithm (Floyd, 1962) over the
graph (S, T ); the details are given in the appendix.
The second step involves simple dynamic program-
ming over the hypergraph (V,E) (it is simple to in-
tegrate the ?s terms into this algorithm). In the third
step, the path variables yp are filled in.
5.3 Properties
We now describe some important properties of the
algorithm:
Efficiency. The main steps of the algorithm are:
1) construction of the graph (S, T ); 2) at each it-
eration, dynamic programming over the hypergraph
(V,E); 3) at each iteration, all-pairs shortest path al-
gorithms over the graph (S, T ). Each of these steps
is vastly more efficient than computing an exact in-
tersection of the hypergraph with a language model.
Exact solutions. By usual guarantees for La-
grangian relaxation, if at any point the algorithm re-
turns a solution yt that satisfies constraints D3?D6,
then yt exactly solves the problem in Eq. 1.
Upper bounds. At each point in the algorithm,
L(?t, ?t, ut, vt) is an upper bound on the score of
the optimal primal solution, f(y?). Upper bounds
can be useful in evaluating the quality of primal so-
lutions from either our algorithm or other methods
such as cube pruning.
Simplicity of implementation. Construction of
the (S, T ) graph is straightforward. The other
steps?hypergraph dynamic programming, and all-
pairs shortest path?are widely known algorithms
that are simple to implement.
6 Tightening the Relaxation
The algorithm that we have described minimizes
the dual function L(?, ?, u, v). By usual results for
Lagrangian relaxation (e.g., see (Korte and Vygen,
2008)), L is the dual function for a particular LP re-
laxation arising from the definition of Y ? and the ad-
ditional constaints D3?D6. In some cases the LP
relaxation has an integral solution, in which case
the algorithm will return an optimal solution yt.7
In other cases, when the LP relaxation has a frac-
tional solution, the subgradient algorithm will still
converge to the minimum of L, but the primal solu-
tions yt will move between a number of solutions.
We now describe a method that incrementally
adds hard constraints to the set Y ?, until the method
returns an exact solution. For a given y ? Y ?,
for any v with yv = 1, we can recover the previ-
ous two leaves (the trigram ending in v) from ei-
ther the path variables yp, or the hypergraph vari-
ables ye. Specifically, define v?1(v, y) to be the leaf
preceding v in the trigram path p with yp = 1 and
v3(p) = v, and v?2(v, y) to be the leaf two posi-
tions before v in the trigram path p with yp = 1 and
v3(p) = v. Similarly, define v??1(v, y) and v??2(v, y)
to be the preceding two leaves under the ye vari-
ables. If the method has not converged, these two
trigram definitions may not be consistent. For a con-
7Provided that the algorithm is run for enough iterations for
convergence.
78
sistent solution, we require v?1(v, y) = v??1(v, y)
and v?2(v, y) = v??2(v, y) for all v with yv = 1.
Unfortunately, explicitly enforcing all of these con-
straints would require exhaustive dynamic program-
ming over the hypergraph using the (Bar-Hillel et
al., 1964) method, something we wish to avoid.
Instead, we enforce a weaker set of constraints,
which require far less computation. Assume some
function pi : VL ? {1, 2, . . . q} that partitions the
set of leaves into q different partitions. Then we will
add the following constraints to Y ?:
pi(v?1(v, y)) = pi(v??1(v, y))
pi(v?2(v, y)) = pi(v??2(v, y))
for all v such that yv = 1. Finding argmaxy?Y ? ? ?
y under this new definition of Y ? can be performed
using the construction of (Bar-Hillel et al, 1964),
with q different lexical items (for brevity we omit
the details). This is efficient if q is small.8
The remaining question concerns how to choose
a partition pi that is effective in tightening the relax-
ation. To do this we implement the following steps:
1) run the subgradient algorithm until L is close to
convergence; 2) then run the subgradient algorithm
for m further iterations, keeping track of all pairs
of leaf nodes that violate the constraints (i.e., pairs
a = v?1(v, y)/b = v??1(v, y) or a = v?2(v, y)/b =
v??2(v, y) such that a 6= b); 3) use a graph color-
ing algorithm to find a small partition that places all
pairs ?a, b? into separate partitions; 4) continue run-
ning Lagrangian relaxation, with the new constraints
added. We expand pi at each iteration to take into ac-
count new pairs ?a, b? that violate the constraints.
In related work, Sontag et al (2008) describe
a method for inference in Markov random fields
where additional constraints are chosen to tighten
an underlying relaxation. Other relevant work in
NLP includes (Tromble and Eisner, 2006; Riedel
and Clarke, 2006). Our use of partitions pi is related
to previous work on coarse-to-fine inference for ma-
chine translation (Petrov et al, 2008).
7 Experiments
We report experiments on translation from Chinese
to English, using the tree-to-string model described
8In fact in our experiments we use the original hypergraph
to compute admissible outside scores for an exact A* search
algorithm for this problem. We have found the resulting search
algorithm to be very efficient.
Time %age %age %age %age
(LR) (DP) (ILP) (LP)
0.5s 37.5 10.2 8.8 21.0
1.0s 57.0 11.6 13.9 31.1
2.0s 72.2 15.1 21.1 45.9
4.0s 82.5 20.7 30.7 63.7
8.0s 88.9 25.2 41.8 78.3
16.0s 94.4 33.3 54.6 88.9
32.0s 97.8 42.8 68.5 95.2
Median time 0.79s 77.5s 12.1s 2.4s
Figure 5: Results showing percentage of examples that are de-
coded in less than t seconds, for t = 0.5, 1.0, 2.0, . . . , 32.0. LR
= Lagrangian relaxation; DP = exhaustive dynamic program-
ming; ILP = integer linear programming; LP = linear program-
ming (LP does not recover an exact solution). The (I)LP ex-
periments were carried out using Gurobi, a high-performance
commercial-grade solver.
in (Huang and Mi, 2010). We use an identical
model, and identical development and test data, to
that used by Huang and Mi.9 The translation model
is trained on 1.5M sentence pairs of Chinese-English
data; a trigram language model is used. The de-
velopment data is the newswire portion of the 2006
NIST MT evaluation test set (616 sentences). The
test set is the newswire portion of the 2008 NIST
MT evaluation test set (691 sentences).
We ran the full algorithm with the tightening
method described in section 6. We ran the method
for a limit of 200 iterations, hence some exam-
ples may not terminate with an exact solution. Our
method gives exact solutions on 598/616 develop-
ment set sentences (97.1%), and 675/691 test set
sentences (97.7%).
In cases where the method does not converge
within 200 iterations, we can return the best primal
solution yt found by the algorithm during those it-
erations. We can also get an upper bound on the
difference f(y?)?f(yt) using mint L(ut) as an up-
per bound on f(y?). Of the examples that did not
converge, the worst example had a bound that was
1.4% of f(yt) (more specifically, f(yt) was -24.74,
and the upper bound on f(y?)? f(yt) was 0.34).
Figure 5 gives information on decoding time for
our method and two other exact decoding methods:
integer linear programming (using constraints D0?
D6), and exhaustive dynamic programming using
the construction of (Bar-Hillel et al, 1964). Our
9We thank Liang Huang and Haitao Mi for providing us with
their model and data.
79
method is clearly the most efficient, and is compara-
ble in speed to state-of-the-art decoding algorithms.
We also compare our method to cube pruning
(Chiang, 2007; Huang and Chiang, 2007). We reim-
plemented cube pruning in C++, to give a fair com-
parison to our method. Cube pruning has a parame-
ter, b, dictating the maximum number of items stored
at each chart entry. With b = 50, our decoder
finds higher scoring solutions on 50.5% of all exam-
ples (349 examples), the cube-pruning method gets a
strictly higher score on only 1 example (this was one
of the examples that did not converge within 200 it-
erations). With b = 500, our decoder finds better so-
lutions on 18.5% of the examples (128 cases), cube-
pruning finds a better solution on 3 examples. The
median decoding time for our method is 0.79 sec-
onds; the median times for cube pruning with b = 50
and b = 500 are 0.06 and 1.2 seconds respectively.
Our results give a very good estimate of the per-
centage of search errors for cube pruning. A natural
question is how large b must be before exact solu-
tions are returned on almost all examples. Even at
b = 1000, we find that our method gives a better
solution on 95 test examples (13.7%).
Figure 5 also gives a speed comparison of our
method to a linear programming (LP) solver that
solves the LP relaxation defined by constraints D0?
D6. We still see speed-ups, in spite of the fact
that our method is solving a harder problem (it pro-
vides integral solutions). The Lagrangian relaxation
method, when run without the tightening method
of section 6, is solving a dual of the problem be-
ing solved by the LP solver. Hence we can mea-
sure how often the tightening procedure is abso-
lutely necessary, by seeing how often the LP solver
provides a fractional solution. We find that this is
the case on 54.0% of the test examples: the tighten-
ing procedure is clearly important. Inspection of the
tightening procedure shows that the number of par-
titions required (the parameter q) is generally quite
small: 59% of examples that require tightening re-
quire q ? 6; 97.2% require q ? 10.
8 Conclusion
We have described a Lagrangian relaxation algo-
rithm for exact decoding of syntactic translation
models, and shown that it is significantly more effi-
cient than other exact algorithms for decoding tree-
to-string models. There are a number of possible
ways to extend this work. Our experiments have
focused on tree-to-string models, but the method
should also apply to Hiero-style syntactic transla-
tion models (Chiang, 2007). Additionally, our ex-
periments used a trigram language model, however
the constraints in figure 3 generalize to higher-order
language models. Finally, our algorithm recovers
the 1-best translation for a given input sentence; it
should be possible to extend the method to find k-
best solutions.
A Computing the Optimal Trigram Paths
For each v ? VL, define ?v = maxp:v3(p)=v ?(p), where
?(p) = h(v1(p), v2(p), v3(p))??1(v1(p))??2(v2(p))?
?
s?p1(p) u(s)?
?
s?p2(p) v(s). Here h is a function that
computes language model scores, and the other terms in-
volve Lagrange mulipliers. Our task is to compute ??v for
all v ? VL.
It is straightforward to show that the S, T graph is
acyclic. This will allow us to apply shortest path algo-
rithms to the graph, even though the weights u(s) and
v(s) can be positive or negative.
For any pair v1, v2 ? VL, define P(v1, v2) to be the
set of paths between ?v1 ?? and ?v2 ?? in the graph S, T .
Each path p gets a score scoreu(p) = ?
?
s?p u(s).
Next, define p?u(v1, v2) = argmaxp?P(v1,v2) scoreu(p),
and score?u(v1, v2) = scoreu(p?). We assume similar
definitions for p?v(v1, v2) and score?v(v1, v2). The p?u and
score?u values can be calculated using an all-pairs short-
est path algorithm, with weights u(s) on nodes in the
graph. Similarly, p?v and score?v can be computed using
all-pairs shortest path with weights v(s) on the nodes.
Having calculated these values, define T (v) for any
leaf v to be the set of trigrams (x, y, v) such that: 1)
x, y ? VL; 2) there is a path from ?x ?? to ?y ?? and from
?y ?? to ?v ?? in the graph S, T . Then we can calculate
?v = max
(x,y,v)?T (v)
(h(x, y, v)? ?1(x)? ?2(y)
+p?u(x, y) + p?v(y, v))
in O(|T (v)|) time, by brute force search through the set
T (v).
Acknowledgments Alexander Rush and Michael
Collins were supported under the GALE program of the
Defense Advanced Research Projects Agency, Contract
No. HR0011-06-C-0022. Michael Collins was also sup-
ported by NSF grant IIS-0915176. We also thank the
anonymous reviewers for very helpful comments; we
hope to fully address these in an extended version of the
paper.
80
References
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 263?270. Association for
Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
computational linguistics, 33(2):201?228.
Adria de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hierar-
chical Phrase-Based Translation with Weighted Finite-
State Transducers and Shallow-n Grammars. In Com-
putational linguistics, volume 36, pages 505?533.
Robert W. Floyd. 1962. Algorithm 97: Shortest path.
Commun. ACM, 5:345.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273?283, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 380?388, Athens, Greece,
March. Association for Computational Linguistics.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In International Conference on
Computer Vision.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1288?1298, Cambridge, MA, October. Association for
Computational Linguistics.
B.H. Korte and J. Vygen. 2008. Combinatorial optimiza-
tion: theory and algorithms. Springer Verlag.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 161?168, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
I. Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 170?177. Morgan Kaufmann
Publishers Inc.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 44?
52, Sydney, Australia, July. Association for Computa-
tional Linguistics.
R.K. Martin, R.L. Rardin, and B.A. Campbell. 1990.
Polyhedral characterization of discrete dynamic pro-
gramming. Operations research, 38(1):127?138.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 108?116, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?06, pages 129?137, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1?11, Cambridge, MA, October. Association for
Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. EMNLP, pages 145?156.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proc. UAI.
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Proceedings of
81
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, HLT-NAACL
?06, pages 423?430, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697?3717.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 777?784,
Morristown, NJ, USA. Association for Computational
Linguistics.
82
Dual Decomposition
for Natural Language Processing
Alexander M. Rush and Michael Collins
Decoding complexity
focus: decoding problem for natural language tasks
y
?
= arg max
y
f (y)
motivation:
? richer model structure often leads to improved accuracy
? exact decoding for complex models tends to be intractable
Decoding tasks
many common problems are intractable to decode exactly
high complexity
? combined parsing and part-of-speech tagging (Rush et al,
2010)
? ?loopy? HMM part-of-speech tagging
? syntactic machine translation (Rush and Collins, 2011)
NP-Hard
? symmetric HMM alignment (DeNero and Macherey, 2011)
? phrase-based translation
? higher-order non-projective dependency parsing (Koo et al,
2010)
in practice:
? approximate decoding methods (coarse-to-fine, beam search,
cube pruning, gibbs sampling, belief propagation)
? approximate models (mean field, variational models)
Motivation
cannot hope to find exact algorithms (particularly when NP-Hard)
aim: develop decoding algorithms with formal guarantees
method:
? derive fast algorithms that provide certificates of optimality
? show that for practical instances, these algorithms often yield
exact solutions
? provide strategies for improving solutions or finding
approximate solutions when no certificate is found
dual decomposition helps us develop algorithms of this form
Dual Decomposition (Komodakis et al, 2010; Lemare?chal, 2001)
goal: solve complicated optimization problem
y
?
= arg max
y
f (y)
method: decompose into subproblems, solve iteratively
benefit: can choose decomposition to provide ?easy? subproblems
aim for simple and efficient combinatorial algorithms
? dynamic programming
? minimum spanning tree
? shortest path
? min-cut
? bipartite match
? etc.
Related work
there are related methods used NLP with similar motivation
related methods:
? belief propagation (particularly max-product) (Smith and
Eisner, 2008)
? factored A* search (Klein and Manning, 2003)
? exact coarse-to-fine (Raphael, 2001)
aim to find exact solutions without exploring the full search space
Tutorial outline
focus:
? developing dual decomposition algorithms for new NLP tasks
? understanding formal guarantees of the algorithms
? extensions to improve exactness and select solutions
outline:
1. worked algorithm for combined parsing and tagging
2. important theorems and formal derivation
3. more examples from parsing, sequence labeling, MT
4. practical considerations for implementing dual decomposition
5. relationship to linear programming relaxations
6. further variations and advanced examples
1. Worked example
aim: walk through a dual decomposition algorithm for combined
parsing and part-of-speech tagging
? introduce formal notation for parsing and tagging
? give assumptions necessary for decoding
? step through a run of the dual decomposition algorithm
Combined parsing and part-of-speech tagging
S
NP
N
United
VP
V
flies
NP
D
some
A
large
N
jet
goal: find parse tree that optimizes
score(S ? NP VP) + score(VP ? V NP) +
...+ score(United1,N) + score(V,N) + ...
Constituency parsing
notation:
? Y is set of constituency parses for input
? y ? Y is a valid parse
? f (y) scores a parse tree
goal:
arg max
y?Y
f (y)
example: a context-free grammar for constituency parsing
S
NP
N
United
VP
V
flies
NP
D
some
A
large
N
jet
Part-of-speech tagging
notation:
? Z is set of tag sequences for input
? z ? Z is a valid tag sequence
? g(z) scores of a tag sequence
goal:
arg max
z?Z
g(z)
example: an HMM for part-of speech tagging
United1 flies2 some3 large4 jet5
N V D A N
Identifying tags
notation: identify the tag labels selected by each model
? y(i , t) = 1 when parse y selects tag t at position i
? z(i , t) = 1 when tag sequence z selects tag t at position i
example: a parse and tagging with y(4,A) = 1 and z(4,A) = 1
S
NP
N
United
VP
V
flies
NP
D
some
A
large
N
jet
y
United1 flies2 some3 large4 jet5
N V D A N
z
Combined optimization
goal:
arg max
y?Y,z?Z
f (y) + g(z)
such that for all i = 1 . . . n, t ? T ,
y(i , t) = z(i , t)
i.e. find the best parse and tagging pair that agree on tag labels
equivalent formulation:
arg max
y?Y
f (y) + g(l(y))
where l : Y ? Z extracts the tag sequence from a parse tree
Dynamic programming intersection
can solve by solving the product of the two models
example:
? parsing model is a context-free grammar
? tagging model is a first-order HMM
? can solve as CFG and finite-state automata intersection
replace S ? NP VP with
SN,N ? NPN,V VPV ,N
S
NP
N
United
VP
V
flies
NP
D
some
A
large
N
jet
Parsing assumption
the structure of Y is open (could be CFG, TAG, etc.)
assumption: optimization with u can be solved efficiently
arg max
y?Y
f (y) +
?
i ,t
u(i , t)y(i , t)
generally benign since u can be incorporated into the structure of f
example: CFG with rule scoring function h
f (y) =
?
X?Y Z?y
h(X ? Y Z ) +
?
(i ,X )?y
h(X ? wi )
where
arg maxy?Y f (y) +
?
i ,t
u(i , t)y(i , t) =
arg maxy?Y
?
X?Y Z?y
h(X ? Y Z ) +
?
(i ,X )?y
(h(X ? wi ) + u(i ,X ))
Tagging assumption
we make a similar assumption for the set Z
assumption: optimization with u can be solved efficiently
arg max
z?Z
g(z)?
?
i ,t
u(i , t)z(i , t)
example: HMM with scores for transitions T and observations O
g(z) =
?
t?t??z
T (t ? t ?) +
?
(i ,t)?z
O(t ? wi )
where
arg maxz?Z g(z)?
?
i ,t
u(i , t)z(i , t) =
arg maxz?Z
?
t?t??z
T (t ? t ?) +
?
(i ,t)?z
(O(t ? wi )? u(i , t))
Dual decomposition algorithm
Set u
(1)
(i , t) = 0 for all i , t ? T
For k = 1 to K
y
(k) ? arg max
y?Y
f (y) +
?
i ,t
u
(k)
(i , t)y(i , t) [Parsing]
z
(k) ? arg max
z?Z
g(z)?
?
i ,t
u
(k)
(i , t)z(i , t) [Tagging]
If y (k)(i , t) = z(k)(i , t) for all i , t Return (y (k), z(k))
Else u(k+1)(i , t)? u(k)(i , t)? ?k(y
(k)
(i , t)? z(k)(i , t))
Algorithm step-by-step
[Animation]
Main theorem
theorem: if at any iteration, for all i , t ? T
y
(k)
(i , t) = z(k)(i , t)
then (y
(k), z(k)) is the global optimum
proof: focus of the next section
2. Formal properties
aim: formal derivation of the algorithm given in the previous
section
? derive Lagrangian dual
? prove three properties
I upper bound
I convergence
I optimality
? describe subgradient method
Lagrangian
goal:
arg max
y?Y,z?Z
f (y) + g(z) such that y(i , t) = z(i , t)
Lagrangian:
L(u, y , z) = f (y) + g(z) +
?
i ,t
u(i , t) (y(i , t)? z(i , t))
redistribute terms
L(u, y , z) =
?
?f (y) +
?
i ,t
u(i , t)y(i , t)
?
?+
?
?g(z)?
?
i ,t
u(i , t)z(i , t)
?
?
Lagrangian dual
Lagrangian:
L(u, y , z) =
?
?f (y) +
?
i ,t
u(i , t)y(i , t)
?
?+
?
?g(z)?
?
i ,t
u(i , t)z(i , t)
?
?
Lagrangian dual:
L(u) = max
y?Y,z?Z
L(u, y , z)
= max
y?Y
?
?f (y) +
?
i ,t
u(i , t)y(i , t)
?
?+
max
z?Z
?
?g(z)?
?
i ,t
u(i , t)z(i , t)
?
?
Theorem 1. Upper bound
define:
? y
?, z? is the optimal combined parsing and tagging solution
with y
?
(i , t) = z?(i , t) for all i , t
theorem: for any value of u
L(u) ? f (y?) + g(z?)
L(u) provides an upper bound on the score of the optimal solution
note: upper bound may be useful as input to branch and bound or
A* search
Theorem 1. Upper bound (proof)
theorem: for any value of u, L(u) ? f (y?) + g(z?)
proof:
L(u) = max
y?Y,z?Z
L(u, y , z) (1)
? max
y?Y,z?Z:y=z
L(u, y , z) (2)
= max
y?Y,z?Z:y=z
f (y) + g(z) (3)
= f (y
?
) + g(z
?
) (4)
Formal algorithm (reminder)
Set u
(1)
(i , t) = 0 for all i , t ? T
For k = 1 to K
y
(k) ? arg max
y?Y
f (y) +
?
i ,t
u
(k)
(i , t)y(i , t) [Parsing]
z
(k) ? arg max
z?Z
g(z)?
?
i ,t
u
(k)
(i , t)z(i , t) [Tagging]
If y (k)(i , t) = z(k)(i , t) for all i , t Return (y (k), z(k))
Else u(k+1)(i , t)? u(k)(i , t)? ?k(y
(k)
(i , t)? z(k)(i , t))
Theorem 2. Convergence
notation:
? u
(k+1)
(i , t)? u(k)(i , t) + ?k(y
(k)
(i , t)? z(k)(i , t)) is update
? u
(k)
is the penalty vector at iteration k
? ?k is the update rate at iteration k
theorem: for any sequence ?1, ?2, ?3, . . . such that
lim
t??
?t = 0 and
??
t=1
?t =?,
we have
lim
t??
L(u
t
) = min
u
L(u)
i.e. the algorithm converges to the tightest possible upper bound
proof: by subgradient convergence (next section)
Dual solutions
define:
? for any value of u
yu = arg max
y?Y
?
?f (y) +
?
i ,t
u(i , t)y(i , t)
?
?
and
zu = arg max
z?Z
?
?g(z)?
?
i ,t
u(i , t)z(i , t)
?
?
? yu and zu are the dual solutions for a given u
Theorem 3. Optimality
theorem: if there exists u such that
yu(i , t) = zu(i , t)
for all i , t then
f (yu) + g(zu) = f (y
?
) + g(z
?
)
i.e. if the dual solutions agree, we have an optimal solution
(yu, zu)
Theorem 3. Optimality (proof)
theorem: if u such that yu(i , t) = zu(i , t) for all i , t then
f (yu) + g(zu) = f (y
?
) + g(z
?
)
proof: by the definitions of yu and zu
L(u) = f (yu) + g(zu) +
?
i ,t
u(i , t)(yu(i , t)? zu(i , t))
= f (yu) + g(zu)
since L(u) ? f (y?) + g(z?) for all values of u
f (yu) + g(zu) ? f (y
?
) + g(z
?
)
but y
?
and z
?
are optimal
f (yu) + g(zu) ? f (y
?
) + g(z
?
)
Dual optimization
Lagrangian dual:
L(u) = max
y?Y,z?Z
L(u, y , z)
= max
y?Y
?
?f (y) +
?
i ,t
u(i , t)y(i , t)
?
?+
max
z?Z
?
?g(z)?
?
i ,t
u(i , t)z(i , t)
?
?
goal: dual problem is to find the tightest upper bound
min
u
L(u)
Dual subgradient
L(u) = max
y?Y
?
?f (y) +
?
i,t
u(i , t)y(i , t)
?
?+ max
z?Z
?
?g(z)?
?
i,t
u(i , t)z(i , t)
?
?
properties:
? L(u) is convex in u (no local minima)
? L(u) is not differentiable (because of max operator)
handle non-differentiability by using subgradient descent
define: a subgradient of L(u) at u is a vector gu such that for all v
L(v) ? L(u) + gu ? (v ? u)
Subgradient algorithm
L(u) = max
y?Y
?
?f (y) +
?
i,t
u(i , t)y(i , t)
?
?+ max
z?Z
?
?g(z)?
?
i,j
u(i , t)z(i , t)
?
?
recall, yu and zu are the argmax?s of the two terms
subgradient:
gu(i , t) = yu(i , t)? zu(i , t)
subgradient descent: move along the subgradient
u
?
(i , t) = u(i , t)? ? (yu(i , t)? zu(i , t))
guaranteed to find a minimum with conditions given earlier for ?
3. More examples
aim: demonstrate similar algorithms that can be applied to other
decoding applications
? context-free parsing combined with dependency parsing
? corpus-level part-of-speech tagging
? combined translation alignment
Combined constituency and dependency parsing
setup: assume separate models trained for constituency and
dependency parsing
problem: find constituency parse that maximizes the sum of the
two models
example:
? combine lexicalized CFG with second-order dependency parser
Lexicalized constituency parsing
notation:
? Y is set of lexicalized constituency parses for input
? y ? Y is a valid parse
? f (y) scores a parse tree
goal:
arg max
y?Y
f (y)
example: a lexicalized context-free grammar
S(flies)
NP(United)
N
United
VP(flies)
V
flies
NP(jet)
D
some
A
large
N
jet
Dependency parsing
define:
? Z is set of dependency parses for input
? z ? Z is a valid dependency parse
? g(z) scores a dependency parse
example:
*0 United1 flies2 some3 large4 jet5
Identifying dependencies
notation: identify the dependencies selected by each model
? y(i , j) = 1 when constituency parse y selects word i as a
modifier of word j
? z(i , j) = 1 when dependency parse z selects word i as a
modifier of word j
example: a constituency and dependency parse with y(3, 5) = 1
and z(3, 5) = 1
S(flies)
NP(United)
N
United
VP(flies)
V
flies
NP(jet)
D
some
A
large
N
jet
y
*0 United1 flies2 some3 large4 jet5
z
Combined optimization
goal:
arg max
y?Y,z?Z
f (y) + g(z)
such that for all i = 1 . . . n, j = 0 . . . n,
y(i , j) = z(i , j)
Algorithm step-by-step
[Animation]
Corpus-level tagging
setup: given a corpus of sentences and a trained sentence-level
tagging model
problem: find best tagging for each sentence, while at the same
time enforcing inter-sentence soft constraints
example:
? test-time decoding with a trigram tagger
? constraint that each word type prefer a single POS tag
Corpus-level tagging
full model for corpus-level tagging
He saw an American man
The smart man stood outside
Man is the best measure
N
Sentence-level decoding
notation:
? Yi is set of tag sequences for input sentence i
? Y = Y1? . . .?Ym is set of tag sequences for the input corpus
? Y ? Y is a valid tag sequence for the corpus
? F (Y ) =
?
i
f (Yi ) is the score for tagging the whole corpus
goal:
arg max
Y?Y
F (Y )
example: decode each sentence with a trigram tagger
He
P
saw
V
an
D
American
A
man
N
The
D
smart
A
man
N
stood
V
outside
R
Inter-sentence constraints
notation:
? Z is set of possible assignments of tags to word types
? z ? Z is a valid tag assignment
? g(z) is a scoring function for assignments to word types
(e.g. a hard constraint - all word types only have one tag)
example: an MRF model that encourages words of the same type
to choose the same tag
z1
man
N
man
N
man
N
N
z2
man
N
man
N
man
A
N
g(z1) > g(z2)
Identifying word tags
notation: identify the tag labels selected by each model
? Ys(i , t) = 1 when the tagger for sentence s at position i
selects tag t
? z(s, i , t) = 1 when the constraint assigns at sentence s
position i the tag t
example: a parse and tagging with Y1(5,N) = 1 and
z(1, 5,N) = 1
He saw an American man
The smart man stood outside
Y
man man man
z
Combined optimization
goal:
arg max
Y?Y,z?Z
F (Y ) + g(z)
such that for all s = 1 . . .m, i = 1 . . . n, t ? T ,
Ys(i , t) = z(s, i , t)
Algorithm step-by-step
[Animation]
Combined alignment (DeNero and Macherey, 2011)
setup: assume separate models trained for English-to-French and
French-to-English alignment
problem: find an alignment that maximizes the score of both
models with soft agreement
example:
? HMM models for both directional alignments (assume correct
alignment is one-to-one for simplicity)
English-to-French alignment
define:
? Y is set of all possible English-to-French alignments
? y ? Y is a valid alignment
? f (y) scores of the alignment
example: HMM alignment
The1 ugly2 dog3 has4 red5 fur6
1 3 2 4 6 5
French-to-English alignment
define:
? Z is set of all possible French-to-English alignments
? z ? Z is a valid alignment
? g(z) scores of an alignment
example: HMM alignment
Le1 chien2 laid3 a4 fourrure5 rouge6
1 2 3 4 6 5
Identifying word alignments
notation: identify the tag labels selected by each model
? y(i , j) = 1 when e-to-f alignment y selects French word i to
align with English word j
? z(i , j) = 1 when f-to-e alignment z selects French word i to
align with English word j
example: two HMM alignment models with y(6, 5) = 1 and
z(6, 5) = 1
The1 ugly2 dog3 has4 red5 fur6
1 3 2 4 6 5
y
Le1 chien2 laid3 a4 fourrure5 rouge6
1 2 3 4 6 5
z
Combined optimization
goal:
arg max
y?Y,z?Z
f (y) + g(z)
such that for all i = 1 . . . n, j = 1 . . . n,
y(i , j) = z(i , j)
Algorithm step-by-step
[Animation]
4. Practical issues
aim: overview of practical dual decomposition techniques
? tracking the progress of the algorithm
? extracting solutions if algorithm does not converge
? lazy update of dual solutions
Tracking progress
at each stage of the algorithm there are several useful values
track:
? y
(k)
, z
(k)
are current dual solutions
? L(u
(k)
) is the current dual value
? y
(k)
, l(y
(k)
) is a potential primal feasible solution
? f (y
(k)
) + g(l(y
(k)
)) is the potential primal value
useful signals:
? L(u
(k)
)? L(u(k?1)) is the dual change (may be positive)
? min
k
L(u
(k)
) is the best dual value (tightest upper bound)
? max
k
f (y
(k)
) + g(l(y
(k)
)) is the best primal value
the optimal value must be between the best dual and primal values
Approximate solution
upon agreement the solution is exact, but this may not occur
otherwise, there is an easy way to find an approximate solution
choose: the structure y (k
?)
where
k
?
= arg max
k
f (y
(k)
) + g(l(y
(k)
))
is the iteration with the best primal score
guarantee: the solution yk
?
is non-optimal by at most
(min
t
L(u
t
))? (f (y (k
?)
) + g(l(y
(k ?)
)))
there are other methods to estimate solutions, for instance by
averaging solutions (see Nedic? and Ozdaglar (2009))
Lazy decoding
idea: don?t recompute y (k) or z(k) from scratch each iteration
lazy decoding: if subgradient u(k) is sparse, then y (k) may be
very easy to compute from y
(k?1)
use:
? very helpful if y or z factors naturally into several parts
? decompositions with this property are very fast in practice
example:
? in corpus-level tagging, only need to recompute sentences
with a word type that received an update
5. Linear programming
aim: explore the connections between dual decomposition and
linear programming
? basic optimization over the simplex
? formal properties of linear programming
? full example with fractional optimal solutions
? tightening linear program relaxations
Simplex
define:
? ?y is the simplex over Y where ? ? ?y implies
?y ? 0 and
?
y
?y = 1
? ?z is the simplex over Z
? ?y : Y ? ?y maps elements to the simplex
example:
Y = {y1, y2, y3}
vertices
? ?y (y1) = (1, 0, 0)
? ?y (y2) = (0, 1, 0)
? ?y (y3) = (0, 0, 1)
?y (y1)
?y (y2) ?y (y3)
?y
Linear programming
optimize over the simplices ?y and ?z instead of the discrete sets
Y and Z
goal: optimize linear program
max
???y ,???z
?
y
?y f (y) +
?
z
?zg(z)
such that for all i , t
?
y
?yy(i , t) =
?
z
?zz(i , t)
Lagrangian
Lagrangian:
M(u, ?, ?) =
?
y
?y f (y) +
?
z
?zg(z) +
?
i,t
u(i , t)
(
?
y
?yy(i , t)?
?
z
?zz(i , t)
)
=
(
?
y
?y f (y) +
?
i,t
u(i , t)
?
y
?yy(i , t)
)
+
(
?
z
?zg(z)?
?
i,t
u(i , t)
?
z
?zz(i , t)
)
Lagrangian dual:
M(u) = max
???y ,???z
M(u, ?, ?)
Strong duality
define:
? ??, ?? is the optimal assignment to ?, ? in the linear program
theorem:
min
u
M(u) =
?
y
??y f (y) +
?
z
??zg(z)
proof: by linear programming duality
Dual relationship
theorem: for any value of u,
M(u) = L(u)
note: solving the original Lagrangian dual also solves dual of the
linear program
Primal relationship
define:
? Q ? ?y ??z corresponds to feasible solutions of the original
problem
Q = {(?y (y), ?z(z)): y ? Y, z ? Z,
y(i , t) = z(i , t) for all (i , t)}
? Q? ? ?y ??z is the set of feasible solutions to the LP
Q? = {(?, ?): ? ? ?Y , ? ? ?Z ,
?
y ?yy(i , t) =
?
z ?zz(i , t) for all (i , t)}
? Q ? Q?
solutions:
max
q?Q
h(q) ? max
q?Q?
h(q) for any h
Concrete example
? Y = {y1, y2, y3}
? Z = {z1, z2, z3}
? ?y ? R
3
, ?z ? R
3
Y
x
a
He
a
is
y1
x
b
He
b
is
y2
x
c
He
c
is
y3
Z a
He
b
is
z1
b
He
a
is
z2
c
He
c
is
z3
Simple solution
Y
x
a
He
a
is
y1
x
b
He
b
is
y2
x
c
He
c
is
y3
Z a
He
b
is
z1
b
He
a
is
z2
c
He
c
is
z3
choose:
? ?(1) = (0, 0, 1) ? ?y is representation of y3
? ?(1) = (0, 0, 1) ? ?z is representation of z3
confirm: ?
y
?(1)y y(i , t) =
?
z
?(1)z z(i , t)
?(1) and ?(1) satisfy agreement constraint
Fractional solution
Y
x
a
He
a
is
y1
x
b
He
b
is
y2
x
c
He
c
is
y3
Z a
He
b
is
z1
b
He
a
is
z2
c
He
c
is
z3
choose:
? ?(2) = (0.5, 0.5, 0) ? ?y is combination of y1 and y2
? ?(2) = (0.5, 0.5, 0) ? ?z is combination of z1 and z2
confirm: ?
y
?(2)y y(i , t) =
?
z
?(2)z z(i , t)
?(2) and ?(2) satisfy agreement constraint, but not integral
Optimal solution
weights:
? the choice of f and g determines the optimal solution
? if (f , g) favors (?(2), ?(2)), the optimal solution is fractional
example: f = [1 1 2] and g = [1 1 ? 2]
? f ? ?(1) + g ? ?(1) = 0 vs f ? ?(2) + g ? ?(2) = 2
? ?(2), ?(2) is optimal, even though it is fractional
Algorithm run
[Animation]
Tightening (Sherali and Adams, 1994; Sontag et al, 2008)
modify:
? extend Y, Z to identify bigrams of part-of-speech tags
? y(i , t1, t2) = 1 ? y(i , t1) = 1 and y(i + 1, t2) = 1
? z(i , t1, t2) = 1 ? z(i , t1) = 1 and z(i + 1, t2) = 1
all bigram constraints: valid to add for all i , t1, t2 ? T
?
y
?yy(i , t1, t2) =
?
z
?zz(i , t1, t2)
however this would make decoding expensive
single bigram constraint: cheaper to implement
?
y
?yy(1, a, b) =
?
z
?zz(1, a, b)
the solution ?(1), ?(1) trivially passes this constraint, while
?(2), ?(2) violates it
Dual decomposition with tightening
tightened decomposition includes an additional Lagrange multiplier
yu,v = arg max
y?Y
f (y) +
?
i ,t
u(i , t)y(i , t) + v(1, a, b)y(1, a, b)
zu,v = arg max
z?Z
g(z)?
?
i ,t
u(i , t)z(i , t)? v(1, a, b)z(1, a, b)
in general, this term can make the decoding problem more difficult
example:
? for small examples, these penalties are easy to compute
? for CFG parsing, need to include extra states that maintain
tag bigrams (still faster than full intersection)
Tightening step-by-step
[Animation]
6. Advanced examples
aim: demonstrate some different relaxation techniques
? higher-order non-projective dependency parsing
? syntactic machine translation
Higher-order non-projective dependency parsing
setup: given a model for higher-order non-projective dependency
parsing (sibling features)
problem: find non-projective dependency parse that maximizes the
score of this model
difficulty:
? model is NP-hard to decode
? complexity of the model comes from enforcing combinatorial
constraints
strategy: design a decomposition that separates combinatorial
constraints from direct implementation of the scoring function
Non-projective dependency parsing
structure:
? starts at the root symbol *
? each word has a exactly one parent word
? produces a tree structure (no cycles)
? dependencies can cross
example:
*0 John1 saw2 a3 movie4 today5 that6 he7 liked8
*0 John1 saw2 a3 movie4 today5 that6 he7 liked8
Arc-Factored
*0 John1 saw2 a3 movie4 today5 that6 he7 liked8
f (y) = score(head =?0,mod =saw2) +score(saw2, John1)
+score(saw2,movie4) +score(saw2, today5)
+score(movie4, a3) + ...
e.g. score(?0, saw2) = log p(saw2|?0) (generative model)
or score(?0, saw2) = w ? ?(saw2, ?0) (CRF/perceptron model)
y
?
= arg max
y
f (y) ? Minimum Spanning Tree Algorithm
Sibling models
*0 John1 saw2 a3 movie4 today5 that6 he7 liked8
f (y) = score(head = ?0, prev = NULL,mod = saw2)
+score(saw2,NULL, John1)+score(saw2,NULL,movie4)
+score(saw2,movie4, today5) + ...
e.g. score(saw2,movie4, today5) = log p(today5|saw2,movie4)
or score(saw2,movie4, today5) = w ? ?(saw2,movie4, today5)
y
?
= arg max
y
f (y) ? NP-Hard
Thought experiment: individual decoding
*0 John1 saw2 a3 movie4 today5 that6 he7 liked8
score(saw2,NULL, John1) + score(saw2,NULL,movie4)
+score(saw2,movie4, today5)
score(saw2,NULL, John1) + score(saw2,NULL, that6)
score(saw2,NULL, a3) + score(saw2, a3,he7)
2
n?1
possibilities
under sibling model, can solve for each word with Viterbi decoding.
Thought experiment continued
*0 John1 saw2 a3 movie4 today5 that6 he7 liked8
idea: do individual decoding for each head word using dynamic
programming
if we?re lucky, we?ll end up with a valid final tree
but we might violate some constraints
Dual decomposition structure
goal:
y
?
= arg max
y?Y
f (y)
rewrite:
arg max
y? Y z? Z,
f (y) + g(z)
such that for all i , j
y(i , j) = z(i , j)
Algorithm step-by-step
[Animation]
Syntactic translation decoding
setup: assume a trained model for syntactic machine translation
problem: find best derivation that maximizes the score of this
model
difficulty:
? need to incorporate language model in decoding
? empirically, relaxation is often not tight, so dual
decomposition does not always converge
strategy:
? use a different relaxation to handle language model
? incrementally add constraints to find exact solution
Syntactic translation example
[Animation]
Summary
presented dual decomposition as a method for decoding in NLP
formal guarantees
? gives certificate or approximate solution
? can improve approximate solutions by tightening relaxation
efficient algorithms
? uses fast combinatorial algorithms
? can improve speed with lazy decoding
widely applicable
? demonstrated algorithms for a wide range of NLP tasks
(parsing, tagging, alignment, mt decoding)
References I
J. DeNero and K. Macherey. Model-Based Aligner Combination
Using Dual Decomposition. In Proc. ACL, 2011.
D. Klein and C.D. Manning. Factored A* Search for Models over
Sequences and Trees. In Proc IJCAI, volume 18, pages
1246?1251. Citeseer, 2003.
N. Komodakis, N. Paragios, and G. Tziritas. Mrf energy
minimization and beyond via dual decomposition. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
2010. ISSN 0162-8828.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola,
and David Sontag. Dual decomposition for parsing with
non-projective head automata. In EMNLP, 2010. URL
http://www.aclweb.org/anthology/D10-1125.
B.H. Korte and J. Vygen. Combinatorial Optimization: Theory and
Algorithms. Springer Verlag, 2008.
References II
C. Lemare?chal. Lagrangian Relaxation. In Computational
Combinatorial Optimization, Optimal or Provably Near-Optimal
Solutions [based on a Spring School], pages 112?156, London,
UK, 2001. Springer-Verlag. ISBN 3-540-42877-1.
Angelia Nedic? and Asuman Ozdaglar. Approximate primal
solutions and rate analysis for dual subgradient methods. SIAM
Journal on Optimization, 19(4):1757?1780, 2009.
Christopher Raphael. Coarse-to-fine dynamic programming. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 23:
1379?1390, 2001.
A.M. Rush and M. Collins. Exact Decoding of Syntactic
Translation Models through Lagrangian Relaxation. In Proc.
ACL, 2011.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakkola. On Dual
Decomposition and Linear Programming Relaxations for Natural
Language Processing. In Proc. EMNLP, 2010.
References III
Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations
and convex hull characterizations for mixed-integer zero?one
programming problems. Discrete Applied Mathematics, 52(1):83
? 106, 1994.
D.A. Smith and J. Eisner. Dependency Parsing by Belief
Propagation. In Proc. EMNLP, pages 145?156, 2008. URL
http://www.aclweb.org/anthology/D08-1016.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss.
Tightening LP relaxations for MAP using message passing. In
Proc. UAI, 2008.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223?231,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Spectral Learning of Latent-Variable PCFGs
Shay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar3
1Dept. of Computer Science, Columbia University
2Dept. of Statistics/3Dept. of Computer and Information Science, University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.edu
Abstract
We introduce a spectral learning algorithm for
latent-variable PCFGs (Petrov et al, 2006).
Under a separability (singular value) condi-
tion, we prove that the method provides con-
sistent parameter estimates.
1 Introduction
Statistical models with hidden or latent variables are
of great importance in natural language processing,
speech, and many other fields. The EM algorithm is
a remarkably successful method for parameter esti-
mation within these models: it is simple, it is often
relatively efficient, and it has well understood formal
properties. It does, however, have a major limitation:
it has no guarantee of finding the global optimum of
the likelihood function. From a theoretical perspec-
tive, this means that the EM algorithm is not guar-
anteed to give consistent parameter estimates. From
a practical perspective, problems with local optima
can be difficult to deal with.
Recent work has introduced polynomial-time
learning algorithms (and consistent estimation meth-
ods) for two important cases of hidden-variable
models: Gaussian mixture models (Dasgupta, 1999;
Vempala and Wang, 2004) and hidden Markov mod-
els (Hsu et al, 2009). These algorithms use spec-
tral methods: that is, algorithms based on eigen-
vector decompositions of linear systems, in particu-
lar singular value decomposition (SVD). In the gen-
eral case, learning of HMMs or GMMs is intractable
(e.g., see Terwijn, 2002). Spectral methods finesse
the problem of intractibility by assuming separabil-
ity conditions. For example, the algorithm of Hsu
et al (2009) has a sample complexity that is polyno-
mial in 1/?, where ? is the minimum singular value
of an underlying decomposition. These methods are
not susceptible to problems with local maxima, and
give consistent parameter estimates.
In this paper we derive a spectral algorithm
for learning of latent-variable PCFGs (L-PCFGs)
(Petrov et al, 2006; Matsuzaki et al, 2005). Our
method involves a significant extension of the tech-
niques from Hsu et al (2009). L-PCFGs have been
shown to be a very effective model for natural lan-
guage parsing. Under a separation (singular value)
condition, our algorithm provides consistent param-
eter estimates; this is in contrast with previous work,
which has used the EM algorithm for parameter es-
timation, with the usual problems of local optima.
The parameter estimation algorithm (see figure 4)
is simple and efficient. The first step is to take
an SVD of the training examples, followed by a
projection of the training examples down to a low-
dimensional space. In a second step, empirical av-
erages are calculated on the training example, fol-
lowed by standard matrix operations. On test ex-
amples, simple (tensor-based) variants of the inside-
outside algorithm (figures 2 and 3) can be used to
calculate probabilities and marginals of interest.
Our method depends on the following results:
? Tensor form of the inside-outside algorithm.
Section 5 shows that the inside-outside algorithm for
L-PCFGs can be written using tensors. Theorem 1
gives conditions under which the tensor form calcu-
lates inside and outside terms correctly.
? Observable representations. Section 6 shows
that under a singular-value condition, there is an ob-
servable form for the tensors required by the inside-
outside algorithm. By an observable form, we fol-
low the terminology of Hsu et al (2009) in referring
to quantities that can be estimated directly from data
where values for latent variables are unobserved.
Theorem 2 shows that tensors derived from the ob-
servable form satisfy the conditions of theorem 1.
? Estimating the model. Section 7 gives an al-
gorithm for estimating parameters of the observable
representation from training data. Theorem 3 gives a
sample complexity result, showing that the estimates
converge to the true distribution at a rate of 1/
?
M
where M is the number of training examples.
The algorithm is strikingly different from the EM
algorithm for L-PCFGs, both in its basic form, and
in its consistency guarantees. The techniques de-
223
veloped in this paper are quite general, and should
be relevant to the development of spectral methods
for estimation in other models in NLP, for exam-
ple alignment models for translation, synchronous
PCFGs, and so on. The tensor form of the inside-
outside algorithm gives a new view of basic calcula-
tions in PCFGs, and may itself lead to new models.
2 Related Work
For work on L-PCFGs using the EM algorithm, see
Petrov et al (2006), Matsuzaki et al (2005), Pereira
and Schabes (1992). Our work builds on meth-
ods for learning of HMMs (Hsu et al, 2009; Fos-
ter et al, 2012; Jaeger, 2000), but involves sev-
eral extensions: in particular in the tensor form of
the inside-outside algorithm, and observable repre-
sentations for the tensor form. Balle et al (2011)
consider spectral learning of finite-state transducers;
Lugue et al (2012) considers spectral learning of
head automata for dependency parsing. Parikh et al
(2011) consider spectral learning algorithms of tree-
structured directed bayes nets.
3 Notation
Given a matrix A or a vector v, we write A? or v?
for the associated transpose. For any integer n ? 1,
we use [n] to denote the set {1, 2, . . . n}. For any
row or column vector y ? Rm, we use diag(y) to
refer to the (m?m) matrix with diagonal elements
equal to yh for h = 1 . . . m, and off-diagonal ele-
ments equal to 0. For any statement ?, we use [[?]]
to refer to the indicator function that is 1 if ? is true,
and 0 if ? is false. For a random variable X, we use
E[X] to denote its expected value.
We will make (quite limited) use of tensors:
Definition 1 A tensor C ? R(m?m?m) is a set of
m3 parameters Ci,j,k for i, j, k ? [m]. Given a ten-
sor C , and a vector y ? Rm, we define C(y) to be
the (m ? m) matrix with components [C(y)]i,j =
?
k?[m]Ci,j,kyk. Hence C can be interpreted as a
function C : Rm ? R(m?m) that maps a vector
y ? Rm to a matrix C(y) of dimension (m?m).
In addition, we define the tensor C? ? R(m?m?m)
for any tensor C ? R(m?m?m) to have values
[C?]i,j,k = Ck,j,i
Finally, for vectors x, y, z ? Rm, xy?z? is the
tensor D ? Rm?m?m where Dj,k,l = xjykzl (this
is analogous to the outer product: [xy?]j,k = xjyk).
4 L-PCFGs: Basic Definitions
This section gives a definition of the L-PCFG for-
malism used in this paper. An L-PCFG is a 5-tuple
(N ,I,P,m, n) where:
? N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We assume
that N = I ? P, and I ? P = ?. Hence we have
partitioned the set of non-terminals into two subsets.
? [m] is the set of possible hidden states.
? [n] is the set of possible words.
? For all a ? I , b ? N , c ? N , h1, h2, h3 ? [m],
we have a context-free rule a(h1) ? b(h2) c(h3).
? For all a ? P, h ? [m], x ? [n], we have a
context-free rule a(h) ? x.
Hence each in-terminal a ? I is always the left-
hand-side of a binary rule a ? b c; and each pre-
terminal a ? P is always the left-hand-side of a
rule a ? x. Assuming that the non-terminals in
the grammar can be partitioned this way is relatively
benign, and makes the estimation problem cleaner.
We define the set of possible ?skeletal rules? as
R = {a ? b c : a ? I, b ? N , c ? N}. The
parameters of the model are as follows:
? For each a? b c ? R, and h ? [m], we have
a parameter q(a ? b c|h, a). For each a ? P,
x ? [n], and h ? [m], we have a parameter
q(a ? x|h, a). For each a ? b c ? R, and
h, h? ? [m], we have parameters s(h?|h, a ? b c)
and t(h?|h, a? b c).
These definitions give a PCFG, with rule proba-
bilities
p(a(h1) ? b(h2) c(h3)|a(h1)) =
q(a? b c|h1, a)? s(h2|h1, a? b c)? t(h3|h1, a? b c)
and p(a(h) ? x|a(h)) = q(a? x|h, a).
In addition, for each a ? I , for each h ? [m], we
have a parameter ?(a, h) which is the probability of
non-terminal a paired with hidden variable h being
at the root of the tree.
An L-PCFG defines a distribution over parse trees
as follows. A skeletal tree (s-tree) is a sequence of
rules r1 . . . rN where each ri is either of the form
a ? b c or a ? x. The rule sequence forms
a top-down, left-most derivation under a CFG with
skeletal rules. See figure 1 for an example.
A full tree consists of an s-tree r1 . . . rN , together
with values h1 . . . hN . Each hi is the value for
224
S1
NP2
D3
the
N4
dog
VP5
V6
saw
P7
him
r1 = S ? NP VP
r2 = NP ? D N
r3 = D ? the
r4 = N ? dog
r5 = VP ? V P
r6 = V ? saw
r7 = P ? him
Figure 1: An s-tree, and its sequence of rules. (For con-
venience we have numbered the nodes in the tree.)
the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [m].
Define ai to be the non-terminal on the left-hand-
side of rule ri. For any i ? {2 . . . N} define pa(i)
to be the index of the rule above node i in the tree.
Define L ? [N ] to be the set of nodes in the tree
which are the left-child of some parent, and R ?
[N ] to be the set of nodes which are the right-child of
some parent. The probability mass function (PMF)
over full trees is then
p(r1 . . . rN , h1 . . . hN ) = ?(a1, h1)
?
N
?
i=1
q(ri|hi, ai)?
?
i?L
s(hi|hpa(i), rpa(i))
?
?
i?R
t(hi|hpa(i), rpa(i)) (1)
The PMF over s-trees is p(r1 . . . rN ) =
?
h1...hN p(r1 . . . rN , h1 . . . hN ).
In the remainder of this paper, we make use of ma-
trix form of parameters of an L-PCFG, as follows:
? For each a? b c ? R, we define Qa?b c ?
Rm?m to be the matrix with values q(a ? b c|h, a)
for h = 1, 2, . . . m on its diagonal, and 0 values for
its off-diagonal elements. Similarly, for each a ? P,
x ? [n], we define Qa?x ? Rm?m to be the matrix
with values q(a ? x|h, a) for h = 1, 2, . . . m on its
diagonal, and 0 values for its off-diagonal elements.
? For each a ? b c ? R, we define Sa?b c ?
Rm?m where [Sa?b c]h?,h = s(h?|h, a? b c).
? For each a ? b c ? R, we define T a?b c ?
Rm?m where [T a?b c]h?,h = t(h?|h, a? b c).
? For each a ? I , we define the vector ?a ? Rm
where [?a]h = ?(a, h).
5 Tensor Form of the Inside-Outside
Algorithm
Given an L-PCFG, two calculations are central:
Inputs: s-tree r1 . . . rN , L-PCFG (N , I,P ,m, n), parameters
? Ca?b c ? R(m?m?m) for all a? b c ? R
? c?a?x ? R(1?m) for all a ? P , x ? [n]
? c1a ? R(m?1) for all a ? I.
Algorithm: (calculate the f i terms bottom-up in the tree)
? For all i ? [N ] such that ai ? P , f i = c?ri
? For all i ? [N ] such that ai ? I, f i = f?Cri(f?) where
? is the index of the left child of node i in the tree, and ?
is the index of the right child.
Return: f1c1a1 = p(r1 . . . rN)
Figure 2: The tensor form for calculation of p(r1 . . . rN ).
1. For a given s-tree r1 . . . rN , calculate
p(r1 . . . rN ).
2. For a given input sentence x = x1 . . . xN , cal-
culate the marginal probabilities
?(a, i, j) =
?
??T (x):(a,i,j)??
p(?)
for each non-terminal a ? N , for each (i, j)
such that 1 ? i ? j ? N .
Here T (x) denotes the set of all possible s-trees for
the sentence x, and we write (a, i, j) ? ? if non-
terminal a spans words xi . . . xj in the parse tree ? .
The marginal probabilities have a number of uses.
Perhaps most importantly, for a given sentence x =
x1 . . . xN , the parsing algorithm of Goodman (1996)
can be used to find
arg max
??T (x)
?
(a,i,j)??
?(a, i, j)
This is the parsing algorithm used by Petrov et al
(2006), for example. In addition, we can calcu-
late the probability for an input sentence, p(x) =
?
??T (x) p(?), as p(x) =
?
a?I ?(a, 1, N).
Variants of the inside-outside algorithm can be
used for problems 1 and 2. This section introduces a
novel form of these algorithms, using tensors. This
is the first step in deriving the spectral estimation
method.
The algorithms are shown in figures 2 and 3. Each
algorithm takes the following inputs:
1. A tensor Ca?b c ? R(m?m?m) for each rule
a? b c.
2. A vector c?a?x ? R(1?m) for each rule a? x.
225
3. A vector c1a ? R(m?1) for each a ? I .
The following theorem gives conditions under
which the algorithms are correct:
Theorem 1 Assume that we have an L-PCFG with
parameters Qa?x, Qa?b c, T a?b c, Sa?b c, ?a, and
that there exist matrices Ga ? R(m?m) for all a ?
N such that each Ga is invertible, and such that:
1. For all rules a? b c, Ca?b c(y) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1
2. For all rules a? x, c?a?x = 1?Qa?x(Ga)?1
3. For all a ? I , c1a = Ga?a
Then: 1) The algorithm in figure 2 correctly com-
putes p(r1 . . . rN ) under the L-PCFG. 2) The algo-
rithm in figure 3 correctly computes the marginals
?(a, i, j) under the L-PCFG.
Proof: See section 9.1.
6 Estimating the Tensor Model
A crucial result is that it is possible to directly esti-
mate parameters Ca?b c, c?a?x and c1a that satisfy the
conditions in theorem 1, from a training sample con-
sisting of s-trees (i.e., trees where hidden variables
are unobserved). We first describe random variables
underlying the approach, then describe observable
representations based on these random variables.
6.1 Random Variables Underlying the Approach
Each s-tree with N rules r1 . . . rN has N nodes. We
will use the s-tree in figure 1 as a running example.
Each node has an associated rule: for example,
node 2 in the tree in figure 1 has the rule NP? D N.
If the rule at a node is of the form a? b c, then there
are left and right inside trees below the left child and
right child of the rule. For example, for node 2 we
have a left inside tree rooted at node 3, and a right
inside tree rooted at node 4 (in this case the left and
right inside trees both contain only a single rule pro-
duction, of the form a ? x; however in the general
case they might be arbitrary subtrees).
In addition, each node has an outside tree. For
node 2, the outside tree is
S
NP VP
V
saw
P
him
Inputs: Sentence x1 . . . xN , L-PCFG (N , I,P ,m, n), param-
eters Ca?b c ? R(m?m?m) for all a? b c ? R, c?a?x ?
R(1?m) for all a ? P , x ? [n], c1a ? R(m?1) for all a ? I.
Data structures:
? Each ?a,i,j ? R1?m for a ? N , 1 ? i ? j ? N is a
row vector of inside terms.
? Each ?a,i,j ? Rm?1 for a ? N , 1 ? i ? j ? N is a
column vector of outside terms.
? Each ?(a, i, j) ? R for a ? N , 1 ? i ? j ? N is a
marginal probability.
Algorithm:
(Inside base case) ?a ? P , i ? [N ], ?a,i,i = c?a?xi
(Inside recursion) ?a ? I, 1 ? i < j ? N,
?a,i,j =
j?1
?
k=i
?
a?b c
?c,k+1,jCa?b c(?b,i,k)
(Outside base case) ?a ? I, ?a,1,n = c1a
(Outside recursion) ?a ? N , 1 ? i ? j ? N,
?a,i,j =
i?1
?
k=1
?
b?c a
Cb?c a(?c,k,i?1)?b,k,j
+
N
?
k=j+1
?
b?a c
Cb?a c? (?c,j+1,k)?b,i,k
(Marginals) ?a ? N , 1 ? i ? j ? N,
?(a, i, j) = ?a,i,j?a,i,j =
?
h?[m]
?a,i,jh ?
a,i,j
h
Figure 3: The tensor form of the inside-outside algorithm,
for calculation of marginal terms ?(a, i, j).
The outside tree contains everything in the s-tree
r1 . . . rN , excluding the subtree below node i.
Our random variables are defined as follows.
First, we select a random internal node, from a ran-
dom tree, as follows:
? Sample an s-tree r1 . . . rN from the PMF
p(r1 . . . rN ). Choose a node i uniformly at ran-
dom from [N ].
If the rule ri for the node i is of the form a? b c,
we define random variables as follows:
? R1 is equal to the rule ri (e.g., NP ? D N).
? T1 is the inside tree rooted at node i. T2 is the
inside tree rooted at the left child of node i, and T3
is the inside tree rooted at the right child of node i.
? H1,H2,H3 are the hidden variables associated
with node i, the left child of node i, and the right
child of node i respectively.
226
? A1, A2, A3 are the labels for node i, the left
child of node i, and the right child of node i respec-
tively. (E.g., A1 = NP, A2 = D, A3 = N.)
? O is the outside tree at node i.
? B is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
If the rule ri for the selected node i is of
the form a ? x, we have random vari-
ables R1, T1,H1, A1, O,B as defined above, but
H2,H3, T2, T3, A2, and A3 are not defined.
We assume a function ? that maps outside trees o
to feature vectors ?(o) ? Rd? . For example, the fea-
ture vector might track the rule directly above the
node in question, the word following the node in
question, and so on. We also assume a function ?
that maps inside trees t to feature vectors ?(t) ? Rd.
As one example, the function ? might be an indica-
tor function tracking the rule production at the root
of the inside tree. Later we give formal criteria for
what makes good definitions of ?(o) of ?(t). One
requirement is that d? ? m and d ? m.
In tandem with these definitions, we assume pro-
jection matices Ua ? R(d?m) and V a ? R(d??m)
for all a ? N . We then define additional random
variables Y1, Y2, Y3, Z as
Y1 = (Ua1)??(T1) Z = (V a1)??(O)
Y2 = (Ua2)??(T2) Y3 = (Ua3)??(T3)
where ai is the value of the random variable Ai.
Note that Y1, Y2, Y3, Z are all in Rm.
6.2 Observable Representations
Given the definitions in the previous section, our
representation is based on the following matrix, ten-
sor and vector quantities, defined for all a ? N , for
all rules of the form a? b c, and for all rules of the
form a? x respectively:
?a = E[Y1Z?|A1 = a]
Da?b c = E
[
[[R1 = a? b c]]Y3Z?Y ?2 |A1 = a
]
d?a?x = E
[
[[R1 = a? x]]Z?|A1 = a
]
Assuming access to functions ? and ?, and projec-
tion matrices Ua and V a, these quantities can be es-
timated directly from training data consisting of a
set of s-trees (see section 7).
Our observable representation then consists of:
Ca?b c(y) = Da?b c(y)(?a)?1 (2)
c?a?x = d?a?x(?a)?1 (3)
c1a = E [[[A1 = a]]Y1|B = 1] (4)
We next introduce conditions under which these
quantities satisfy the conditions in theorem 1.
The following definition will be important:
Definition 2 For all a ? N , we define the matrices
Ia ? R(d?m) and Ja ? R(d??m) as
[Ia]i,h = E[?i(T1) | H1 = h,A1 = a]
[Ja]i,h = E[?i(O) | H1 = h,A1 = a]
In addition, for any a ? N , we use ?a ? Rm to
denote the vector with ?ah = P (H1 = h|A1 = a).
The correctness of the representation will rely on
the following conditions being satisfied (these are
parallel to conditions 1 and 2 in Hsu et al (2009)):
Condition 1 ?a ? N , the matrices Ia and Ja are
of full rank (i.e., they have rank m). For all a ? N ,
for all h ? [m], ?ah > 0.
Condition 2 ?a ? N , the matrices Ua ? R(d?m)
and V a ? R(d??m) are such that the matrices Ga =
(Ua)?Ia and Ka = (V a)?Ja are invertible.
The following lemma justifies the use of an SVD
calculation as one method for finding values for Ua
and V a that satisfy condition 2:
Lemma 1 Assume that condition 1 holds, and for
all a ? N define
?a = E[?(T1) (?(O))? |A1 = a] (5)
Then if Ua is a matrix of the m left singular vec-
tors of ?a corresponding to non-zero singular val-
ues, and V a is a matrix of the m right singular vec-
tors of ?a corresponding to non-zero singular val-
ues, then condition 2 is satisfied.
Proof sketch: It can be shown that ?a =
Iadiag(?a)(Ja)?. The remainder is similar to the
proof of lemma 2 in Hsu et al (2009).
The matrices ?a can be estimated directly from a
training set consisting of s-trees, assuming that we
have access to the functions ? and ?.
We can now state the following theorem:
227
Theorem 2 Assume conditions 1 and 2 are satisfied.
For all a ? N , define Ga = (Ua)?Ia. Then under
the definitions in Eqs. 2-4:
1. For all rules a? b c, Ca?b c(y) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1
2. For all rules a? x, c?a?x = 1?Qa?x(Ga)?1.
3. For all a ? N , c1a = Ga?a
Proof: The following identities hold (see sec-
tion 9.2):
Da?b c(y) = (6)
GcT a?b cdiag(yGbSa?b c)Qa?b cdiag(?a)(Ka)?
d?a?x = 1?Qa?xdiag(?a)(Ka)? (7)
?a = Gadiag(?a)(Ka)? (8)
c1a = Gapia (9)
Under conditions 1 and 2, ?a is invertible, and
(?a)?1 = ((Ka)?)?1(diag(?a))?1(Ga)?1. The
identities in the theorem follow immediately.
7 Deriving Empirical Estimates
Figure 4 shows an algorithm that derives esti-
mates of the quantities in Eqs 2, 3, and 4. As
input, the algorithm takes a sequence of tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i ? [M ].
These tuples can be derived from a training set
consisting of s-trees ?1 . . . ?M as follows:
? ?i ? [M ], choose a single node ji uniformly at
random from the nodes in ?i. Define r(i,1) to be the
rule at node ji. t(i,1) is the inside tree rooted at node
ji. If r(i,1) is of the form a? b c, then t(i,2) is the
inside tree under the left child of node ji, and t(i,3)
is the inside tree under the right child of node ji. If
r(i,1) is of the form a ? x, then t(i,2) = t(i,3) =
NULL. o(i) is the outside tree at node ji. b(i) is 1 if
node ji is at the root of the tree, 0 otherwise.
Under this process, assuming that the s-trees
?1 . . . ?M are i.i.d. draws from the distribution
p(?) over s-trees under an L-PCFG, the tuples
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) are i.i.d. draws
from the joint distribution over the random variables
R1, T1, T2, T3, O,B defined in the previous section.
The algorithm first computes estimates of the pro-
jection matrices Ua and V a: following lemma 1,
this is done by first deriving estimates of ?a,
and then taking SVDs of each ?a. The matrices
are then used to project inside and outside trees
t(i,1), t(i,2), t(i,3), o(i) down to m-dimensional vec-
tors y(i,1), y(i,2), y(i,3), z(i); these vectors are used to
derive the estimates of Ca?b c, c?a?x, and c1a.
We now state a PAC-style theorem for the learning
algorithm. First, for a given L-PCFG, we need a
couple of definitions:
? ? is the minimum absolute value of any element
of the vectors/matrices/tensors c1a, d?a?x, Da?b c,
(?a)?1. (Note that ? is a function of the projec-
tion matrices Ua and V a as well as the underlying
L-PCFG.)
? For each a ? N , ?a is the value of the m?th
largest singular value of ?a. Define ? = mina ?a.
We then have the following theorem:
Theorem 3 Assume that the inputs to the algorithm
in figure 4 are i.i.d. draws from the joint distribution
over the random variables R1, T1, T2, T3, O,B, un-
der an L-PCFG with distribution p(r1 . . . rN ) over
s-trees. Define m to be the number of latent states
in the L-PCFG. Assume that the algorithm in fig-
ure 4 has projection matrices U?a and V? a derived as
left and right singular vectors of ?a, as defined in
Eq. 5. Assume that the L-PCFG, together with U?a
and V? a, has coefficients ? > 0 and ? > 0. In addi-
tion, assume that all elements in c1a, d?a?x, Da?b c,
and ?a are in [?1,+1]. For any s-tree r1 . . . rN de-
fine p?(r1 . . . rN ) to be the value calculated by the
algorithm in figure 3 with inputs c?1a, c??a?x, C?a?b c
derived from the algorithm in figure 4. Define R to
be the total number of rules in the grammar of the
form a? b c or a ? x. Define Ma to be the num-
ber of training examples in the input to the algorithm
in figure 4 where ri,1 has non-terminal a on its left-
hand-side. Under these assumptions, if for all a
Ma ?
128m2
( 2N+1?1 + ?? 1
)2 ?2?4
log
(2mR
?
)
Then
1? ? ?
?
?
?
?
p?(r1 . . . rN )
p(r1 . . . rN )
?
?
?
?
? 1 + ?
A similar theorem (omitted for space) states that
1? ? ?
?
?
?
??(a,i,j)
?(a,i,j)
?
?
?
? 1 + ? for the marginals.
The condition that U?a and V? a are derived from
?a, as opposed to the sample estimate ??a, follows
Foster et al (2012). As these authors note, similar
techniques to those of Hsu et al (2009) should be
228
applicable in deriving results for the case where ??a
is used in place of ?a.
Proof sketch: The proof is similar to that of Foster
et al (2012). The basic idea is to first show that
under the assumptions of the theorem, the estimates
c?1a, d??a?x, D?a?b c, ??a are all close to the underlying
values being estimated. The second step is to show
that this ensures that p?(r1...rN? )p(r1...rN? ) is close to 1.
The method described of selecting a single tuple
(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for each s-tree en-
sures that the samples are i.i.d., and simplifies the
analysis underlying theorem 3. In practice, an im-
plementation should most likely use all nodes in all
trees in training data; by Rao-Blackwellization we
know such an algorithm would be better than the
one presented, but the analysis of how much better
would be challenging. It would almost certainly lead
to a faster rate of convergence of p? to p.
8 Discussion
There are several potential applications of the
method. The most obvious is parsing with L-
PCFGs.1 The approach should be applicable in other
cases where EM has traditionally been used, for ex-
ample in semi-supervised learning. Latent-variable
HMMs for sequence labeling can be derived as spe-
cial case of our approach, by converting tagged se-
quences to right-branching skeletal trees.
The sample complexity of the method depends on
the minimum singular values of ?a; these singular
values are a measure of how well correlated ? and
? are with the unobserved hidden variable H1. Ex-
perimental work is required to find a good choice of
values for ? and ? for parsing.
9 Proofs
This section gives proofs of theorems 1 and 2. Due
to space limitations we cannot give full proofs; in-
stead we provide proofs of some key lemmas. A
long version of this paper will give the full proofs.
9.1 Proof of Theorem 1
First, the following lemma leads directly to the cor-
rectness of the algorithm in figure 2:
1Parameters can be estimated using the algorithm in
figure 4; for a test sentence x1 . . . xN we can first
use the algorithm in figure 3 to calculate marginals
?(a, i, j), then use the algorithm of Goodman (1996) to find
argmax??T (x)
?
(a,i,j)?? ?(a, i, j).
Inputs: Training examples (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i))
for i ? {1 . . .M}, where r(i,1) is a context free rule; t(i,1),
t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; and
b(i) = 1 if the rule is at the root of tree, 0 otherwise. A function
? that maps inside trees t to feature-vectors ?(t) ? Rd. A func-
tion ? that maps outside trees o to feature-vectors ?(o) ? Rd? .
Algorithm:
Define ai to be the non-terminal on the left-hand side of rule
r(i,1). If r(i,1) is of the form a? b c, define bi to be the non-
terminal for the left-child of r(i,1), and ci to be the non-terminal
for the right-child.
(Step 0: Singular Value Decompositions)
? Use the algorithm in figure 5 to calculate matrices U?a ?
R(d?m) and V? a ? R(d??m) for each a ? N .
(Step 1: Projection)
? For all i ? [M ], compute y(i,1) = (U?ai)??(t(i,1)).
? For all i ? [M ] such that r(i,1) is of the form
a? b c, compute y(i,2) = (U?bi)??(t(i,2)) and y(i,3) =
(U?ci)??(t(i,3)).
? For all i ? [M ], compute z(i) = (V? ai)??(o(i)).
(Step 2: Calculate Correlations)
? For each a ? N , define ?a = 1/
?M
i=1[[ai = a]]
? For each rule a? b c, compute D?a?b c = ?a ?
?M
i=1[[r(i,1) = a? b c]]y(i,3)(z(i))?(y(i,2))?
? For each rule a ? x, compute d??a?x = ?a ?
?M
i=1[[r(i,1) = a? x]](z(i))?
? For each a ? N , compute ??a = ?a ?
?M
i=1[[ai = a]]y(i,1)(z(i))?
(Step 3: Compute Final Parameters)
? For all a? b c, C?a?b c(y) = D?a?b c(y)(??a)?1
? For all a? x, c??a?x = d??a?x(??a)?1
? For all a ? I, c?1a =
?M
i=1[[ai=a and b(i)=1]]y(i,1)
?M
i=1[[b(i)=1]]
Figure 4: The spectral learning algorithm.
Inputs: Identical to algorithm in figure 4.
Algorithm:
? For each a ? N , compute ??a ? R(d??d) as
??a =
?M
i=1[[ai = a]]?(t(i,1))(?(o(i)))?
?M
i=1[[ai = a]]
and calculate a singular value decomposition of ??a.
? For each a ? N , define U?a ? Rm?d to be a matrix of the left
singular vectors of ??a corresponding to the m largest singular
values. Define V? a ? Rm?d? to be a matrix of the right singular
vectors of ??a corresponding to the m largest singular values.
Figure 5: Singular value decompositions.
229
Lemma 2 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 2 is an s-tree r1 . . . rN . Define ai for i ? [N ]
to be the non-terminal on the left-hand-side of rule
ri, and ti for i ? [N ] to be the s-tree with rule ri
at its root. Finally, for all i ? [N ], define the row
vector bi ? R(1?m) to have components
bih = P (Ti = ti|Hi = h,Ai = ai)
for h ? [m]. Then for all i ? [N ], f i = bi(G(ai))?1.
It follows immediately that
f1c1a1 = b
1(G(a1))?1Ga1?a1 = p(r1 . . . rN )
This lemma shows a direct link between the vec-
tors f i calculated in the algorithm, and the terms bih,
which are terms calculated by the conventional in-
side algorithm: each f i is a linear transformation
(through Gai) of the corresponding vector bi.
Proof: The proof is by induction.
First consider the base case. For any leaf?i.e., for
any i such that ai ? P?we have bih = q(ri|h, ai),
and it is easily verified that f i = bi(G(ai))?1.
The inductive case is as follows. For all i ? [N ]
such that ai ? I , by the definition in the algorithm,
f i = f?Cri(f?)
= f?Ga?T ridiag(f?Ga?Sri)Qri(Gai)?1
Assuming by induction that f? = b?(G(a? ))?1 and
f? = b?(G(a?))?1, this simplifies to
f i = ?rdiag(?l)Qri(Gai)?1 (10)
where ?r = b?T ri , and ?l = b?Sri . ?r is a row
vector with components ?rh =
?
h??[m] b
?
h?T
ri
h?,h =
?
h??[m] b
?
h?t(h?|h, ri). Similarly, ?l is a row vector
with components equal to ?lh =
?
h??[m] b
?
h?S
ri
h?,h =
?
h??[m] b
?
h?s(h?|h, ri). It can then be verified that
?rdiag(?l)Qri is a row vector with components
equal to ?rh?lhq(ri|h, ai).
But bih = q(ri|h, ai)?
(
?
h??[m] b
?
h?t(h?|h, ri)
)
?
(
?
h??[m] b
?
h?s(h?|h, ri)
)
= q(ri|h, ai)?rh?lh, hence
?rdiag(?l)Qri = bi and the inductive case follows
immediately from Eq. 10.
Next, we give a similar lemma, which implies the
correctness of the algorithm in figure 3:
Lemma 3 Assume that conditions 1-3 of theorem 1
are satisfied, and that the input to the algorithm in
figure 3 is a sentence x1 . . . xN . For any a ? N , for
any 1 ? i ? j ? N , define ??a,i,j ? R(1?m) to have
components ??a,i,jh = p(xi . . . xj|h, a) for h ? [m].
In addition, define ??a,i,j ? R(m?1) to have compo-
nents ??a,i,jh = p(x1 . . . xi?1, a(h), xj+1 . . . xN ) for
h ? [m]. Then for all i ? [N ], ?a,i,j = ??a,i,j(Ga)?1
and ?a,i,j = Ga??a,i,j . It follows that for all (a, i, j),
?(a, i, j) = ??a,i,j(Ga)?1Ga??a,i,j = ??a,i,j ??a,i,j
=
?
h
??a,i,jh ??
a,i,j
h =
?
??T (x):(a,i,j)??
p(?)
Thus the vectors ?a,i,j and ?a,i,j are linearly re-
lated to the vectors ??a,i,j and ??a,i,j , which are the
inside and outside terms calculated by the conven-
tional form of the inside-outside algorithm.
The proof is by induction, and is similar to the
proof of lemma 2; for reasons of space it is omitted.
9.2 Proof of the Identity in Eq. 6
We now prove the identity in Eq. 6, used in the proof
of theorem 2. For reasons of space, we do not give
the proofs of identities 7-9: the proofs are similar.
The following identities can be verified:
P (R1 = a? b c|H1 = h,A1 = a) = q(a? b c|h, a)
E [Y3,j|H1 = h,R1 = a? b c] = Ea?b cj,h
E [Zk|H1 = h,R1 = a? b c] = Kak,h
E [Y2,l|H1 = h,R1 = a? b c] = F a?b cl,h
where Ea?b c = GcT a?b c, F a?b c = GbSa?b c.
Y3, Z and Y2 are independent when conditioned
on H1, R1 (this follows from the independence as-
sumptions in the L-PCFG), hence
E [[[R1 = a? b c]]Y3,jZkY2,l | H1 = h,A1 = a]
= q(a? b c|h, a)Ea?b cj,h Kak,hF a?b cl,h
Hence (recall that ?ah = P (H1 = h|A1 = a)),
Da?b cj,k,l = E [[[R1 = a? b c]]Y3,jZkY2,l | A1 = a]
=
?
h
?ahE [[[R1 = a? b c]]Y3,jZkY2,l | H1 = h,A1 = a]
=
?
h
?ahq(a? b c|h, a)Ea?b cj,h Kak,hF a?b cl,h (11)
from which Eq. 6 follows.
230
Acknowledgements: Columbia University gratefully ac-
knowledges the support of the Defense Advanced Re-
search Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions,
findings, and conclusions or recommendations expressed
in this material are those of the author(s) and do not nec-
essarily reflect the view of DARPA, AFRL, or the US
government. Shay Cohen was supported by the National
Science Foundation under Grant #1136996 to the Com-
puting Research Association for the CIFellows Project.
Dean Foster was supported by National Science Founda-
tion grant 1106743.
References
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
Proceedings of FOCS.
Dean P. Foster, Jordan Rodu, and Lyle H. Ungar.
2012. Spectral dimensionality reduction for hmms.
arXiv:1203.6130v1.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the 34th annual meeting on Associ-
ation for Computational Linguistics, pages 177?183.
Association for Computational Linguistics.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spec-
tral algorithm for learning hidden Markov models. In
Proceedings of COLT.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6).
F. M. Lugue, A. Quattoni, B. Balle, and X. Carreras.
2012. Spectral learning for non-deterministic depen-
dency parsing. In Proceedings of EACL.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 75?82. Association for
Computational Linguistics.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral al-
gorithm for latent tree graphical models. In Proceed-
ings of The 28th International Conference on Machine
Learningy (ICML 2011).
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from partially bracketed corpora. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 128?135, Newark,
Delaware, USA, June. Association for Computational
Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 433?440, Sydney, Australia, July.
Association for Computational Linguistics.
S. A. Terwijn. 2002. On the learnability of hidden
markov models. In Grammatical Inference: Algo-
rithms and Applications (Amsterdam, 2002), volume
2484 of Lecture Notes in Artificial Intelligence, pages
261?268, Berlin. Springer.
S. Vempala and G. Wang. 2004. A spectral algorithm for
learning mixtures of distributions. Journal of Com-
puter and System Sciences, 68(4):841?860.
231
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052?1061,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Provably Correct Learning Algorithm for Latent-Variable PCFGs
Shay B. Cohen
School of Informatics
University of Edinburgh
scohen@inf.ed.ac.uk
Michael Collins
Department of Computer Science
Columbia University
mcollins@cs.columbia.edu
Abstract
We introduce a provably correct learning
algorithm for latent-variable PCFGs. The
algorithm relies on two steps: first, the use
of a matrix-decomposition algorithm ap-
plied to a co-occurrence matrix estimated
from the parse trees in a training sample;
second, the use of EM applied to a convex
objective derived from the training sam-
ples in combination with the output from
the matrix decomposition. Experiments on
parsing and a language modeling problem
show that the algorithm is efficient and ef-
fective in practice.
1 Introduction
Latent-variable PCFGs (L-PCFGs) (Matsuzaki et
al., 2005; Petrov et al, 2006) give state-of-the-art
performance on parsing problems. The standard
approach to parameter estimation in L-PCFGs is
the EM algorithm (Dempster et al, 1977), which
has the usual problems with local optima. Re-
cent work (Cohen et al, 2012) has introduced an
alternative algorithm, based on spectral methods,
which has provable guarantees. Unfortunately this
algorithm does not return parameter estimates for
the underlying L-PCFG, instead returning the pa-
rameter values up to an (unknown) linear trans-
form. In practice, this is a limitation.
We describe an algorithm that, like EM, re-
turns estimates of the original parameters of an L-
PCFG, but, unlike EM, does not suffer from prob-
lems of local optima. The algorithm relies on two
key ideas:
1) A matrix decomposition algorithm (sec-
tion 5) which is applicable to matrices Q of the
form Q
f,g
=
?
h
p(h)p(f | h)p(g | h) where
p(h), p(f | h) and p(g | h) are multinomial dis-
tributions. This matrix form has clear relevance
to latent variable models. We apply the matrix
decomposition algorithm to a co-occurrence ma-
trix that can be estimated directly from a training
set consisting of parse trees without latent anno-
tations. The resulting parameter estimates give us
significant leverage over the learning problem.
2) Optimization of a convex objective function
using EM. We show that once the matrix decom-
position step has been applied, parameter estima-
tion of the L-PCFG can be reduced to a convex
optimization problem that is easily solved by EM.
The algorithm provably learns the parameters of
an L-PCFG (theorem 1), under an assumption that
each latent state has at least one ?pivot? feature.
This assumption is similar to the ?pivot word? as-
sumption used by Arora et al (2013) and Arora et
al. (2012) in the context of learning topic models.
We describe experiments on learning of L-
PCFGs, and also on learning of the latent-variable
language model of Saul and Pereira (1997). A hy-
brid method, which uses our algorithm as an ini-
tializer for EM, performs at the same accuracy as
EM, but requires significantly fewer iterations for
convergence: for example in our L-PCFG exper-
iments, it typically requires 2 EM iterations for
convergence, as opposed to 20-40 EM iterations
for initializers used in previous work.
While this paper?s focus is on L-PCFGs, the
techniques we describe are likely to be applicable
to many other latent-variable models used in NLP.
2 Related Work
Recently a number of researchers have developed
provably correct algorithms for parameter esti-
mation in latent variable models such as hidden
Markov models, topic models, directed graphical
models with latent variables, and so on (Hsu et
al., 2009; Bailly et al, 2010; Siddiqi et al, 2010;
Parikh et al, 2011; Balle et al, 2011; Arora et
al., 2013; Dhillon et al, 2012; Anandkumar et
al., 2012; Arora et al, 2012; Arora et al, 2013).
Many of these algorithms have their roots in spec-
tral methods such as canonical correlation analy-
sis (CCA) (Hotelling, 1936), or higher-order ten-
sor decompositions. Previous work (Cohen et al,
2012; Cohen et al, 2013) has developed a spec-
tral method for learning of L-PCFGs; this method
learns parameters of the model up to an unknown
1052
linear transformation, which cancels in the inside-
outside calculations for marginalization over la-
tent states in the L-PCFG. The lack of direct pa-
rameter estimates from this method leads to prob-
lems with negative or unnormalized probablities;
the method does not give parameters that are in-
terpretable, or that can be used in conjunction with
other algorithms, for example as an initializer for
EM steps that refine the model.
Our work is most directly related to the algo-
rithm for parameter estimation in topic models de-
scribed by Arora et al (2013). This algorithm
forms the core of the matrix decomposition algo-
rithm described in section 5.
3 Background
This section gives definitions and notation for L-
PCFGs, taken from (Cohen et al, 2012).
3.1 L-PCFGs: Basic Definitions
An L-PCFG is an 8-tuple (N , I,P,m, n, pi, t, q)
where: N is the set of non-terminal symbols in the
grammar. I ? N is a finite set of in-terminals.
P ? N is a finite set of pre-terminals. We as-
sume that N = I ? P , and I ? P = ?. Hence
we have partitioned the set of non-terminals into
two subsets. [m] is the set of possible hidden
states.
1
[n] is the set of possible words. For
all (a, b, c) ? I ? N ? N , and (h
1
, h
2
, h
3
) ?
[m] ? [m] ? [m], we have a context-free rule
a(h
1
) ? b(h
2
) c(h
3
). The rule has an associ-
ated parameter t(a? b c, h
2
, h
3
| a, h
1
). For all
a ? P , h ? [m], x ? [n], we have a context-free
rule a(h)? x. The rule has an associated param-
eter q(a ? x | a, h). For all a ? I, h ? [m],
pi(a, h) is a parameter specifying the probability
of a(h) being at the root of a tree.
A skeletal tree (s-tree) is a sequence of rules
r
1
. . . r
N
where each r
i
is either of the form a ?
b c or a ? x. The rule sequence forms a
top-down, left-most derivation under a CFG with
skeletal rules.
A full tree consists of an s-tree r
1
. . . r
N
, to-
gether with values h
1
. . . h
N
. Each h
i
is the value
for the hidden variable for the left-hand-side of
rule r
i
. Each h
i
can take any value in [m].
For a given skeletal tree r
1
. . . r
N
, define a
i
to
be the non-terminal on the left-hand-side of rule
r
i
. For any i ? [N ] such that r
i
is of the form
a? b c, define h
(2)
i
and h
(3)
i
as the hidden state
1
For any integer n, we use [n] to denote the set
{1, 2, . . . n}.
value of the left and right child respectively. The
model then defines a distribution as
p(r
1
. . . r
N
, h
1
. . . h
N
) =
pi(a
1
, h
1
)
?
i:a
i
?I
t(r
i
, h
(2)
i
, h
(3)
i
| a
i
, h
i
)
?
i:a
i
?P
q(r
i
| a
i
, h
i
)
The distribution over skeletal trees is
p(r
1
. . . r
N
) =
?
h
1
...h
N
p(r
1
. . . r
N
, h
1
. . . h
N
).
3.2 Definition of Random Variables
Throughout this paper we will make reference
to random variables derived from the distribution
over full trees from an L-PCFG. These random
variables are defined as follows. First, we select
a random internal node, from a random tree, as
follows: 1) Sample a full tree r
1
. . . r
N
, h
1
. . . h
N
from the PMF p(r
1
. . . r
N
, h
1
. . . h
N
); 2) Choose
a node i uniformly at random from [N ]. We then
give the following definition:
Definition 1 (Random Variables). If the rule r
i
for
the node i is of the form a? b c, we define ran-
dom variables as follows: R
1
is equal to the rule r
i
(e.g., NP? D N). A,B,C are the labels for node i,
the left child of node i, and the right child of node
i respectively. (E.g., A = NP, B = D, C = N.) T
1
is the inside tree rooted at node i. T
2
is the inside
tree rooted at the left child of node i, and T
3
is the
inside tree rooted at the right child of node i. O is
the outside tree at node i. H
1
, H
2
, H
3
are the hid-
den variables associated with node i, the left child
of node i, and the right child of node i respectively.
E is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
If the rule r
i
for the selected node i is
of the form a ? x, we have random vari-
ables R
1
, T
1
, H
1
, A
1
, O,E as defined above, but
H
2
, H
3
, T
2
, T
3
, B, and C are not defined.
4 The Learning Algorithm for L-PCFGs
Our goal is to design a learning algorithm for L-
PCFGs. The input to the algorithm will be a train-
ing set consisting of skeletal trees, assumed to be
sampled from some underlying L-PCFG. The out-
put of the algorithm will be estimates for the pi,
t, and q parameters. The training set does not
include values for the latent variables; this is the
main challenge in learning.
This section focuses on an algorithm for recov-
ery of the t parameters. A description of the al-
gorithms for recovery of the pi and q parameters
is deferred until section 6.1 of this paper; these
1053
steps are straightforward once we have derived the
method for the t parameters.
We describe an algorithm that correctly recov-
ers the parameters of an L-PCFG as the size of the
training set goes to infinity (this statement is made
more precise in section 4.2). The algorithm relies
on an assumption?the ?pivot? assumption?that
we now describe.
4.1 Features, and the Pivot Assumption
We assume a function ? from inside trees to a fi-
nite set F , and a function ? that maps outside trees
to a finite set G. The function ?(t) (?(o)) can be
thought of as a function that maps an inside tree
t (outside tree o) to an underlying feature. As
one example, the function ?(t) might return the
context-free rule at the root of the inside tree t;
in this case the set F would be equal to the set
of all context-free rules in the grammar. As an-
other example, the function ?(o) might return the
context-free rule at the foot of the outside tree o.
In the more general case, we might have K sep-
arate functions ?
(k)
(t) for k = 1 . . .K mapping
inside trees to K separate features, and similarly
we might have multiple features for outside trees.
Cohen et al (2013) describe one such feature def-
inition, where features track single context-free
rules as well as larger fragments such as two or
three-level sub-trees. For simplicity of presenta-
tion we describe the case of single features ?(t)
and ?(o) for the majority of this paper. The exten-
sion to multiple features is straightforward, and is
discussed in section 6.2; the flexibility allowed by
multiple features is important, and we use multiple
features in our experiments.
Given functions ? and ?, we define additional
random variables: F = ?(T
1
), F
2
= ?(T
2
), F
3
=
?(T
3
), and G = ?(O).
We can now give the following assumption:
Assumption 1 (The Pivot Assumption). Under
the L-PCFG being learned, there exist values ? >
0 and ? > 0 such that for each non-terminal a,
for each hidden state h ? [m], the following state-
ments are true: 1) ?f ? F such that P (F =
f | H
1
= h,A = a) > ? and for all h
?
6= h,
P (F = f | H
1
= h
?
, A = a) = 0; 2) ?g ? G
such that P (G = g | H
1
= h,A = a) > ? and
for all h
?
6= h, P (G = g | H
1
= h
?
, A = a) = 0.
This assumption is very similar to the assump-
tion made by Arora et al (2012) in the con-
text of learning topic models. It implies that for
each (a, h) pair, there are inside and outside tree
features?which following Arora et al (2012) we
refer to as pivot features?that occur only
2
in the
presence of latent-state value h. As in (Arora et
al., 2012), the pivot features will give us consider-
able leverage in learning of the model.
4.2 The Learning Algorithm
Figure 1 shows the learning algorithm for L-
PCFGs. The algorithm consists of the following
steps:
Step 0: Calculate estimates p?(a? b c | a),
p?(g, f
2
, f
3
| a? b c) and p?(f, g | a). These
estimates are easily calculated using counts taken
from the training examples.
Step 1: Calculate values r?(f | h, a) and s?(g |
h, a); these are estimates of p(f | h
1
, a) and
p(g | h
1
, a) respectively. This step is achieved us-
ing a matrix decomposition algorithm, described
in section 5 of this paper, on the matrix
?
Q
a
with
entries [
?
Q
a
]
f,g
= p?(f, g | a).
Step 2: Use the EM algorithm to find
?
t values
that maximize the objective function in Eq. 1 (see
figure 1). Crucially, this is a convex optimization
problem, and the EM algorithm will converge to
the global maximum of this likelihood function.
Step 3: Rule estimates are calculated using an
application of the laws of probability.
Before giving a theorem concerning correctness
of the algorithm we introduce two assumptions:
Assumption 2 (Strict Convexity). If we have the
equalities s?(g | h
1
, a) = P (G = g | H
1
=
h
1
, A = a), r?(f
2
| h
2
, b) = P (F
2
= f
2
| H
2
=
h
2
, B = b) and r?(f
3
| h
3
, c) = P (F
3
= f
3
|
H
2
= h
3
, C = c), then the function in Eq. 1 (fig-
ure 1) is strictly concave.
The function in Eq. 1 is always concave; this
assumption adds the restriction that the function
must be strictly concave?that is, it has a unique
global maximum?in the case that the r? and s? es-
timates are exact estimates.
Assumption 3 (Infinite Data). After running Step
0 of the algorithm we have
p?(a? b c | a) = p(a? b c | a)
p?(g, f
2
, f
3
| a? b c) = p(g, f
2
, f
3
| a? b c)
p?(f, g | a) = p(f, g | a)
where p(. . .) is the probability under the underly-
ing L-PCFG.
2
The requirements P (F = f | H
1
= h
?
, A = a) = 0
and P (G = g | H
1
= h
?
, A = a) = 0 are almost certainly
overly strict; in theory and practice these probabilities should
be able to take small but strictly positive values.
1054
We use the term ?infinite data? because under
standard arguments, p?(. . .) converges to p(. . .) as
M goes to?.
The theorem is then as follows:
Theorem 1. Consider the algorithm in figure 1.
Assume that assumptions 1-3 (the pivot, strong
convexity, and infinite data assumptions) hold for
the underlying L-PCFG. Then there is some per-
mutation ? : [m] ? [m] such that for all
a? b c, h
1
, h
2
, h
3
,
?
t(a? b c, h
2
, h
3
| a? b c, h
1
)
= t(a? b c, ?(h
2
), ?(h
3
) | a? b c, ?(h
1
))
where
?
t are the parameters in the output, and t are
the parameters of the underlying L-PCFG.
This theorem states that under assumptions 1-
3, the algorithm correctly learns the t parameters
of an L-PCFG, up to a permutation over the la-
tent states defined by ?. Given the assumptions we
have made, it is not possible to do better than re-
covering the correct parameter values up to a per-
mutation, due to symmetries in the model. As-
suming that the pi and q parameters are recovered
in addition to the t parameters (see section 6.1),
the resulting model will define exactly the same
distribution over full trees as the underlying L-
PCFG up to this permutation, and will define ex-
actly the same distribution over skeletal trees, so
in this sense the permutation is benign.
Proof of theorem 1: Under the assumptions of
the theorem,
?
Q
a
f,g
= p(f, g | a) =
?
h
p(h |
a)p(f | h, a)p(g | h, a). Under the pivot assump-
tion, and theorem 2 of section 5, step 1 (the matrix
decomposition step) will therefore recover values
r? and s? such that r?(f | h, a) = p(f | ?(h), a) and
s?(g | h, a) = p(g | ?(h), a) for some permuta-
tion ? : [m] ? [m]. For simplicity, assume that
?(j) = j for all j ? [m] (the argument for other
permutations involves a straightforward extension
of the following argument). Under the assump-
tions of the theorem, p?(g, f
2
, f
3
| a? b c) =
p(g, f
2
, f
3
| a? b c), hence the function being
optimized in Eq. 1 is equal to
?
g,f
2
,f
3
p(g, f
2
, f
3
| a? b c) log ?(g, f
2
, f
3
)
where
?(g, f
2
, f
3
) =
?
h
1
,h
2
,h
3
(
?
t(h
1
, h
2
, h
3
| a? b c)
?p(g | h
1
, a)p(f
2
| h
2
, b)p(f
3
| h
3
, c))
Now consider the optimization problem in Eq. 1.
By standard results for cross entropy, the maxi-
mum of the function
?
g,f
2
,f
3
p(g, f
2
, f
3
| a? b c) log q(g, f
2
, f
3
| a? b c)
with respect to the q values is achieved at
q(g, f
2
, f
3
| a? b c) = p(g, f
2
, f
3
| a? b c). In
addition, under the assumptions of the L-PCFG,
p(g, f
2
, f
3
| a? b c)
=
?
h
1
,h
2
,h
3
(p(h
1
, h
2
, h
3
| a? b c)
?p(g | h
1
, a)p(f
2
| h
2
, b)p(f
3
| h
3
, c))
Hence the maximum of Eq. 1 is achieved at
?
t(h
1
, h
2
, h
3
| a? b c) = p(h
1
, h
2
, h
3
| a? b c)
(2)
because this gives ?(g, f
2
, f
3
) = p(g, f
2
, f
3
|
a? b c). Under the strict convexity assump-
tion the maximum of Eq. 1 is unique, hence the
?
t values must satisfy Eq. 2. Finally, it follows
from Eq. 2, and the equality p?(a? b c | a) =
p(a? b c | a), that Step 3 of the algorithm gives
?
t(a? b c, h
2
, h
3
| a, h
1
) = t(a? b c, h
2
, h
3
|
a, h
1
).
We can now see how the strict convexity as-
sumption is needed. Without this assumption,
there may be multiple settings for
?
t that achieve
?(g, f
2
, f
3
) = p(g, f
2
, f
3
| a? b c); the values
?
t(h
1
, h
2
, h
3
| a? b c) = p(h
1
, h
2
, h
3
| a? b c)
will be included in this set of solutions, but other,
inconsistent solutions will also be included.
As an extreme example of the failure of the
strict convexity assumption, consider a feature-
vector definition with |F| = |G| = 1. In
this case the function in Eq. 1 reduces to
log
?
h
1
,h
2
,h
3
?
t(h
1
, h
2
, h
3
| a? b c). This func-
tion has a maximum value of 0, achieved at all val-
ues of
?
t. Intuitively, this definition of inside and
outside tree features loses all information about
the latent states, and does not allow successful
learning of the underlying L-PCFG. More gener-
ally, it is clear that the strict convexity assumption
will depend directly on the choice of feature func-
tions ?(t) and ?(o).
Remark: The infinite data assumption, and
sample complexity. The infinite data assump-
tion deserves more discussion. It is clearly a
strong assumption that there is sufficient data for
1055
Input: A set ofM skeletal trees sampled from some underlying L-PCFG. The count[. . .] function counts the number of times
that event . . . occurs in the training sample. For example, count[A = a] is the number of times random variableA takes value
a in the training sample.
Step 0: Calculate the following estimates from the training samples:
? p?(a? b c | a) = count[R
1
= a? b c]/count[A = a]
? p?(g, f
2
, f
3
| a? b c) = count[G = g, F
2
= f
2
, F
3
= f
3
, R
1
= a? b c]/count[R
1
= a? b c]
? p?(f, g | a) = count[F = f,G = g,A = a]/count[A = a]
? ?a ? I, define a matrix
?
Q
a
? R
d?d
?
where d = |F| and d
?
= |G| as [
?
Q
a
]
f,g
= p?(f, g | a).
Step 1: ?a ? I, use the algorithm in figure 2 with input
?
Q
a
to derive estimates r?(f | h, a) and s?(g | h, a).
Remark: These quantities are estimates of P (F
1
= f | H
1
= h,A = a) and P (G = g | H = h,A = a) respectively. Note
that under the independence assumptions of the L-PCFG,
P (F
1
= f | H
1
= h,A = a) = P (F
2
= f | H
2
= h,A
2
= a) = P (F
3
= f | H
3
= h,A
3
= a).
Step 2: For each rule a? b c, find
?
t(h
1
, h
2
, h
3
| a? b c) values that maximize
?
g,f
2
,f
3
p?(g, f
2
, f
3
| a? b c) log
?
h
1
,h
2
,h
3
?
t(h
1
, h
2
, h
3
| a? b c)s?(g | h
1
, a)r?(f
2
| h
2
, b)r?(f
3
| h
3
, c) (1)
under the constraints
?
t(h
1
, h
2
, h
3
| a? b c) ? 0, and
?
h
1
,h
2
,h
3
?
t(h
1
, h
2
, h
3
| a? b c) = 1.
Remark: the function in Eq. 1 is concave in the values
?
t being optimized over. We use the EM algorithm, which converges to
a global optimum.
Step 3: ?a? b c, h
1
, h
2
, h
3
, calculate rule parameters as follows:
?
t(a? b c, h
2
, h
3
| a, h
1
) =
?
t(a? b c, h
1
, h
2
, h
3
| a)/
?
b,c,h
2
,h
3
?
t(a? b c, h
1
, h
2
, h
3
| a)
where
?
t(a? b c, h
1
, h
2
, h
3
| a) = p?(a? b c | a)?
?
t(h
1
, h
2
, h
3
| a? b c).
Output: Parameter estimates
?
t(a? b c, h
2
, h
3
| a, h
1
) for all rules a? b c, for all (h
1
, h
2
, h
3
) ? [m]? [m]? [m].
Figure 1: The learning algorithm for the t(a? b c, h
1
, h
2
, h
3
| a) parameters of an L-PCFG.
the estimates p? in assumption 3 to have converged
to the correct underlying values. A more detailed
analysis of the algorithm would derive sample
complexity results, giving guarantees on the sam-
ple size M required to reach a level of accuracy
 in the estimates, with probability at least 1 ? ?,
as a function of , ?, and other relevant quantities
such as n, d, d
?
,m, ?, ? and so on.
In spite of the strength of the infinite data as-
sumption, we stress the importance of this result
as a guarantee for the algorithm. First, a guar-
antee of correct parameter values in the limit of
infinite data is typically the starting point for a
sample complexity result (see for example (Hsu
et al, 2009; Anandkumar et al, 2012)). Sec-
ond, our sense is that a sample complexity result
can be derived for our algorithm using standard
methods: specifically, the analysis in (Arora et
al., 2012) gives one set of guarantees; the remain-
ing optimization problems we solve are convex
maximum-likelihood problems, which are also
relatively easy to analyze. Note that several pieces
of previous work on spectral methods for latent-
variable models focus on algorithms that are cor-
rect under the infinite data assumption.
5 The Matrix Decomposition Algorithm
This section describes the matrix decomposition
algorithm used in Step 1 of the learning algorithm.
5.1 Problem Setting
Our goal will be to solve the following matrix de-
composition problem:
Matrix Decomposition Problem (MDP) 1. De-
sign an algorithm with the following inputs, as-
sumptions, and outputs:
1056
Inputs: Integers m, d and d
?
, and a matrix Q ?
R
d?d
?
such that Q
f,g
=
?
m
h=1
pi(h)r(f | h)s(g |
h) for some unknown parameters pi(h), r(f | h)
and s(g | h) satisfying:
1) pi(h) ? 0,
?
m
h=1
pi(h) = 1;
2) r(f | h) ? 0,
?
d
f=1
r(f | h) = 1;
3) s(g | h) ? 0,
?
d
?
g=1
s(g | h) = 1.
Assumptions: There are values ? > 0 and ? >
0 such that the r parameters of the model are ?-
separable, and the s parameters of the model are
?-separable.
Outputs: Estimates p?i(h), r?(f | h) and s?(g | h)
such that there is some permutation ? : [m]? [m]
such that ?h, p?i(h) = pi(?(h)), ?f, h, r?(f |
h) = r(f | ?(h)), and ?g, h, s?(g | h) = s(g |
?(h)).
The definition of ?-separability is as follows (?-
separability for s(g | h) is analogous):
Definition 2 (?-separability). The parameters
r(f | h) are ?-separable if for all h ? [m], there
is some j ? [d] such that: 1) r(j | h) ? ?; and 2)
r(j | h
?
) = 0 for h
?
6= h.
This matrix decomposition problem has clear
relevance to problems in learning of latent-
variable models, and in particular is a core step of
the algorithm in figure 1. When given a matrix
?
Q
a
with entries
?
Q
a
f,g
=
?
h
p(h | a)p(f | h, a)p(g |
h, a), where p(. . .) refers to a distribution derived
from an underlying L-PCFG which satisfies the
pivot assumption, the method will recover the val-
ues for p(h | a), p(f | h, a) and p(g | h, a) up to a
permutation over the latent states.
5.2 The Algorithm of Arora et al (2013)
This section describes a variant of the algorithm of
Arora et al (2013), which is used as a component
of our algorithm for MDP 1. One of the proper-
ties of this algorithm is that it solves the following
problem:
Matrix Decomposition Problem (MDP) 2. De-
sign an algorithm with the following inputs, as-
sumptions, and outputs:
Inputs: Same as matrix decomposition problem 1.
Assumptions: The parameters r(f | h) of the
model are ?-separable for some value ? > 0.
Outputs: Estimates p?i(h) and r?(f | h) such that
?? : [m] ? [m] such that ?h, p?i(h) = pi(?(h)),
?f, h, r?(f | h) = r(f | ?(h)).
This is identical to Matrix Decomposition Prob-
lem 1, but without the requirement that the values
s(g | h) are returned by the algorithm. Thus an
algorithm that solves MDP 2 in some sense solves
?one half? of MDP 1.
For completeness we give a sketch of the algo-
rithm that we use; it is inspired by the algorithm
of Arora et al (2012), but has some important dif-
ferences. The algorithm is as follows:
Step 1: Derive a function ? : [d
?
] ? R
l
that
maps each integer g ? [d
?
] to a representation
?(g) ? R
l
. The integer l is typically much smaller
than d
?
, implying that the representation is of low
dimension. Arora et al (2012) derive ? as a ran-
dom projection with a carefully chosen dimension
l. In our experiments, we use canonical correlation
analysis (CCA) on the matrixQ to give a represen-
tation ?(g) ? R
l
where l = m.
Step 2: For each f ? [d], calculate
v
f
= E[?(g) | f ] =
d
?
?
g=1
p(g | f)?(g)
where p(g | f) = Q
f,g
/
?
g
Q
f,g
. It follows that
v
f
=
d
?
?
g=1
m
?
h=1
p(h | f)p(g | h)?(g) =
m
?
h=1
p(h | f)w
h
where w
h
? R
l
is equal to
?
d
?
g=1
p(g | h)?(g).
Hence the v
f
vectors lie in the convex hull of a
set of vectors w
1
. . . w
m
? R
l
. Crucially, for any
pivot word f for latent state h, we have p(h | f) =
1, hence v
f
= w
h
. Thus by the pivot assump-
tion, the set of points v
1
. . . v
d
includes the ver-
tices of the convex hull. Each point v
j
is a convex
combination of the vertices w
1
. . . w
m
, where the
weights in this combination are equal to p(h | j).
Step 3: Use the FastAnchorWords algo-
rithm of (Arora et al, 2012) to identify m vectors
v
s
1
, v
s
2
, . . . v
s
m
. The FastAnchorWords algo-
rithm has the guarantee that there is a permutation
? : [m]? [m] such that v
s
i
= w
?(i)
for all i. This
algorithm recovers the vertices of the convex hull
described in step 2, using a method that greedily
picks points that are as far as possible from the
subspace spanned by previously picked points.
Step 4: For each f ? [d] solve the problem
arg min
?
1
,?
2
,...,?
m
||
?
h
?
h
v
s
h
? v
f
||
2
subject to ?
h
? 0 and
?
h
?
h
= 1. We use the
algorithm of (Frank and Wolfe, 1956; Clarkson,
2010) for this purpose. Set q(h | f) = ?
h
.
1057
Return the final quantities:
p?i(h) =
?
f
p(f)q(h|f) r?(f |h) =
p(f)q(h|f)
?
f
p(f)q(h|f)
where p(f) =
?
g
Q
f,g
.
5.3 An Algorithm for MDP 1
Figure 2 shows an algorithm that solves MDP 1.
In steps 1 and 2 of the algorithm, the algorithm
of section 5.2 is used to recover estimates r?(f |
h) and s?(g | h). These distributions are equal to
p(f | h) and p(g | h) up to permutations ? and
?
?
of the latent states respectively; unfortunately
there is no guarantee that ? and ?
?
are the same
permutation. Step 3 estimates parameters t(h
?
|
h) that effectively map the permutation implied by
r?(f | h) to the permutation implied by s?(g | h);
the latter distribution is recalculated as
?
h
?
?
t(h
?
|
h)s?(g | h
?
).
We now state the following theorem:
Theorem 2. The algorithm in figure 2 solves Ma-
trix Decomposition Problem 1.
Proof: See the supplementary material.
Remark: A natural alternative to the algorithm
presented would be to run Step 1 of the original
algorithm, but to replace steps 2 and 3 with a step
that finds s?(g | h) values that maximize
?
f,g
Q
f,g
log
?
h
r?(h | f)s?(g | h)
This is again a convex optimization problem. We
may explore this algorithm in future work.
6 Additional Details of the Algorithm
6.1 Recovery of the pi and q Parameters
The recovery of the pi and q parameters relies on
the following additional (but benign) assumptions
on the functions ? and ?:
1) For any inside tree t such that t is a unary
rule of the form a ? x, the function ? is defined
as ?(t) = t.
3
2) The set of outside tree features G contains a
special symbol 2, and g(o) = 2 if and only if the
outside tree o is derived from a non-terminal node
at the root of a skeletal tree.
3
Note that if other features on unary rules are desired,
we can use multiple feature functions ?
1
(t) . . . ?
K
(t), where
?
1
(t) = t for inside trees, and the functions ?
2
(t) . . . ?
K
(t)
define other features.
Inputs: As in Matrix Decomposition Problem 1.
Assumptions: As in Matrix Decomposition Problem 1.
Algorithm:
Step 1. Run the algorithm of section 5.2 on the matrix Q
to derive estimates r?(f | h) and p?i(h). Note that under
the guarantees of the algorithm, there is some permutation
? such that r?(f | h) = r(f | ?(h)). Define
r?(h | f) =
r?(f | h)p?i(h)
?
h
r?(f | h)p?i(h)
Step 2. Run the algorithm of section 5.2 on the matrix Q
>
to derive estimates s?(g | h). Under the guarantees of the
algorithm, there is some permutation ?
?
such that s?(g | h) =
s(g | ?
?
(h)). Note however that it is not necessarily the case
that ? = ?
?
.
Step 3. Find
?
t(h
?
| h) for all h, h
?
? [m] that maximize
?
f,g
Q
f,g
log
?
h,h
?
r?(h | f)
?
t(h
?
| h)s?(g | h
?
) (3)
subject to
?
t(h
?
| h) ? 0, and ?h,
?
h
?
?
t(h
?
| h) = 1.
Remark: the function in Eq. 3 is concave in the
?
t parame-
ters. We use the EM algorithm to find a global optimum.
Step 4. Return the following values:
? p?i(h) for all h, as an estimate of pi(?(h)) for some
permutation ?.
? r?(f | h) for all f, h as an estimate of r(f | ?(h)) for
the same permutation ?.
?
?
h
?
?
t(h
?
| h)s?(g | h
?
) as an estimate of s(f | ?(h))
for the same permutation ?.
Figure 2: The algorithm for Matrix Decomposition Problem 1
Under these assumptions, the algorithm in fig-
ure 1 recovers estimates p?i(a, h) and q?(a ? x |
a, h). Simply set
q?(a? x | a, h) = r?(f | h, a) where f = a? x
and p?i(a, h) = p?(2, h, a)/
?
h,a
p?(2, h, a) where
p?(2, h, a) = g?(2 | h, a)p?(h | a)p?(a). Note that
p?(h | a) can be derived from the matrix decompo-
sition step when applied to
?
Q
a
, and p?(a) is easily
recovered from the training examples.
6.2 Extension to Include Multiple Features
We now describe an extension to allowK separate
functions ?
(k)
(t) for k = 1 . . .K mapping inside
trees to features, and L feature functions ?
(l)
(o)
for l = 1 . . . L over outside trees.
The algorithm in figure 1 can be extended as
follows. First, Step 1 of the algorithm (the matrix
1058
decomposition step) can be extended to provide
estimates r?
(k)
(f
(k)
| h, a) and s?
(l)
(g
(l)
| h, a).
In brief, this involves running CCA on a matrix
E[?(T )(?(O))
>
| A = a] where ? and ? are in-
side and outside binary feature vectors derived di-
rectly from the inside and outside features, using
a one-hot representation. CCA results in a low-
dimensional representation that can be used in the
steps described in section 5.2; the remainder of the
algorithm is the same. In practice, the addition of
multiple features may lead to better CCA repre-
sentations.
Next, we modify the objective function in Eq. 1
to be the following:
?
i,j,k
?
g
i
,f
j
2
,f
k
3
p(g
i
, f
j
2
, f
k
3
| a? b c) log ?
i,j,k
(g
i
, f
j
2
, f
k
3
)
where
?
i,j,k
(g
i
, f
j
2
, f
k
3
)
=
?
h
1
,h
2
,h
3
(
?
t(h
1
, h
2
, h
3
| a? b c)
?s?
i
(g
i
| h
1
, a)r?
j
(f
j
2
| h
2
, b)r?
k
(f
k
3
| h
3
, c)
)
Thus the new objective function consists of a sum
ofL?M
2
terms, each corresponding to a different
combination of inside and outside features. The
function remains concave.
6.3 Use as an Initializer for EM
The learning algorithm for L-PCFGs can be used
as an initializer for the EM algorithm for L-
PCFGs. Two-step estimation methods such as
these are well known in statistics; there are guar-
antees for example that if the first estimator is con-
sistent, and the second step finds the closest local
maxima of the likelihood function, then the result-
ing estimator is both consistent and efficient (in
terms of number of samples required). See for
example page 453 or Theorem 4.3 (page 454) of
(Lehmann and Casella, 1998).
7 Experiments on Parsing
This section describes parsing experiments using
the learning algorithm for L-PCFGs. We use the
Penn WSJ treebank (Marcus et al, 1993) for our
experiments. Sections 2?21 were used as training
data, and sections 0 and 22 were used as develop-
ment data. Section 23 was used as the test set.
The experimental setup is the same as described
by Cohen et al (2013). The trees are bina-
rized (Petrov et al, 2006) and for the EM algo-
rithm we use the initialization method described
sec. 22 sec. 23
m 8 16 24 32
EM
86.69
40
88.32
30
88.35
30
88.56
20
87.76
Spectral 85.60 87.77 88.53 88.82 88.05
Pivot 83.56 86.00 86.87 86.40 85.83
Pivot+EM
86.83
2
88.14
6
88.64
2
88.55
2
88.03
Table 1: Results on the development data (section 22) and
test data (section 23) for various learning algorithms for L-
PCFGs. For EM and pivot+EM experiments, the second line
denotes the number of iterations required to reach the given
optimal performance on development data. Results for sec-
tion 23 are used with the best model for section 22 in the cor-
responding row. The results for EM and spectral are reported
from Cohen et al (2013).
in Matsuzaki et al (2005). For the pivot algo-
rithm we use multiple features ?
1
(t) . . . ?
K
(t) and
?
1
(o) . . . ?
L
(o) over inside and outside trees, us-
ing the features described by Cohen et al (2013).
Table 1 gives the F1 accuracy on the develop-
ment and test sets for the following methods:
EM: The EM algorithm as used by Matsuzaki et
al. (2005) and Petrov et al (2006).
Spectral: The spectral algorithm of Cohen et al
(2012) and Cohen et al (2013).
Pivot: The algorithm described in this paper.
Pivot+EM: The algorithm described in this pa-
per, followed by 1 or more iterations of the
EM algorithm with parameters initialized by the
pivot algorithm. (See section 6.3.)
For the EM and Pivot+EM algorithms, we give
the number of iterations of EM required to reach
optimal performance on the development data.
The results show that the EM, Spectral, and
Pivot+EM algorithms all perform at a very similar
level of accuracy. The Pivot+EM results show that
very few EM iterations?just 2 iterations in most
conditions?are required to reach optimal perfor-
mance when the Pivot model is used as an ini-
tializer for EM. The Pivot results lag behind the
Pivot+EM results by around 2-3%, but they are
close enough to optimality to require very few EM
iterations when used as an initializer.
8 Experiments on the Saul and Pereira
(1997) Model for Language Modeling
We now describe a second set of experiments, on
the Saul and Pereira (1997) model for language
modeling. Define V to be the set of words in the
vocabulary. For any w
1
, w
2
? V , the Saul and
Pereira (1997) model then defines p(w
2
| w
1
) =
?
m
h=1
r(h | w
1
)s(w
2
| h) where r(h | w
1
) and
1059
Brown NYT
m 2 4 8 16 32 128 256 test 2 4 8 16 32 128 256 test
EM
737
14
599
14
488
19
468
12
430
10
388
9
365
8
364
926
36
733
39
562
42
420
33
361
38
284
35
265
32
267
bi-KN +int. 408 415 271 279
tri-KN+int. 386 394 150 158
pivot 852 718 605 559 537 426 597 560 1227 1264 896 717 738 782 886 715
pivot+EM
758
2
582
3
502
2
425
1
374
1
310
1
327
1
357
898
20
754
14
553
13
441
15
394
10
279
19
292
12
281
Table 2: Language model perplexity with the Brown corpus and the Gigaword corpus (New York Times portion) for the second
half of the development set, and the test set. With EM and Pivot+EM, the number of iterations for EM to reach convergence is
given below the perplexity. The best result for each column (for each m value) is in bold. The ?test? column gives perplexity
results on the test set. Each perplexity calculation on the test set is done using the best model on the development set. bi-KN+int
and tri-KN+int are bigram and trigram Kneser-Ney interpolated models (Kneser and Ney, 1995), using the SRILM toolkit.
s(w
2
| h) are parameters of the approach. The
conventional approach to estimation of the param-
eters r(h | w
1
) and s(w
2
| h) from a corpus is
to use the EM algorithm. In this section we com-
pare the EM algorithm to a pivot-based method.
It is straightforward to represent this model as an
L-PCFG, and hence to use our implementation for
estimation.
In this special case, the L-PCFG learning al-
gorithm is equivalent to a simple algorithm, with
the following steps: 1) define the matrix Q
with entries Q
w
1
,w
2
= count(w
1
, w
2
)/N where
count(w
1
, w
2
) is the number of times that bi-
gram (w
1
, w
2
) is seen in the data, and N =
?
w
1
,w
2
count(w
1
, w
2
). Run the algorithm of sec-
tion 5.2 on Q to recover estimates s?(w
2
| h); 2)
estimate r?(h | w
1
) using the EM algorithm to op-
timize the function
?
w
1
,w
2
Q
w
1
,w
2
log
?
h
r?(h |
w
1
)s?(w
2
| h) with respect to the r? parameters;
this function is concave in these parameters.
We performed the language modeling experi-
ments for a number of reasons. First, because in
this case the L-PCFG algorithm reduces to a sim-
ple algorithm, it allows us to evaluate the core
ideas in the method very directly. Second, it al-
lows us to test the pivot method on the very large
datasets that are available for language modeling.
We use two corpora for our experiments. The
first is the Brown corpus, as used by Bengio et
al. (2006) in language modeling experiments. Fol-
lowing Bengio et al (2006), we use the first 800K
words for training (and replace all words that ap-
pear once with an UNK token), the next 200K
words for development, and the remaining data
(165,171 tokens) as a test set. The size of the
vocabulary is 24,488 words. The second corpus
we use is the New York Times portion of the Gi-
gaword corpus. Here, the training set consists of
1.31 billion tokens. We use 159 million tokens for
development set and 156 million tokens for test.
All words that appeared less than 20 times in the
training set were replaced with the UNK token.
The size of the vocabulary is 235,223 words. Un-
known words in test data are ignored when calcu-
lating perplexity (this is the standard set-up in the
SRILM toolkit).
In our experiments we use the first half of each
development set to optimize the number of itera-
tions of the EM or Pivot+EM algorithms. As be-
fore, Pivot+EM uses 1 or more EM steps with pa-
rameter initialization from the Pivot method.
Table 2 gives perplexity results for the differ-
ent algorithms. As in the parsing experiments, the
Pivot method alone performs worse than EM, but
the Pivot+EM method gives results that are com-
petitive with EM. The Pivot+EM method requires
fewer iterations of EM than the EM algorithm.
On the Brown corpus the difference is quite dra-
matic, with only 1 or 2 iterations required, as op-
posed to 10 or more for EM. For the NYT cor-
pus the Pivot+EM method requires more iterations
(around 10 or 20), but still requires significantly
fewer iterations than the EM algorithm.
On the Gigaword corpus, with m = 256, EM
takes 12h57m (32 iterations at 24m18s per itera-
tion) compared to 1h50m for the Pivot method. On
Brown, EM takes 1m47s (8 iterations) compared
to 5m44s for the Pivot method. Both the EM and
pivot algorithm implementations were highly op-
timized, and written in Matlab. Results at other
values of m are similar. From these results the
Pivot method appears to become more competitive
speed-wise as the data size increases (the Giga-
word corpus is more than 1,300 times larger than
the Brown corpus).
9 Conclusion
We have described a new algorithm for parameter
estimation in L-PCFGs. The algorithm is provably
correct, and performs well in practice when used
in conjunction with EM.
1060
References
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and
M. Telgarsky. 2012. Tensor decompositions for
learning latent-variable models. arXiv:1210.7559.
S. Arora, R. Ge, and A. Moitra. 2012. Learning
topic models?going beyond SVD. In Proceedings
of FOCS.
S. Arora, R. Ge, Y. Halpern, D. M. Mimno, A. Moitra,
D. Sontag, Y. Wu, and M. Zhu. 2013. A practical
algorithm for topic modeling with provable guaran-
tees. In Proceedings of ICML.
R. Bailly, A. Habrar, and F. Denis. 2010. A spectral
approach for probabilistic grammatical inference on
trees. In Proceedings of ALT.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
Y. Bengio, H. Schwenk, J.-S. Sen?ecal, F. Morin, and
J.-L. Gauvain. 2006. Neural probabilistic language
models. In Innovations in Machine Learning, pages
137?186. Springer.
K. L. Clarkson. 2010. Coresets, sparse greedy ap-
proximation, and the Frank-Wolfe algorithm. ACM
Transactions on Algorithms (TALG), 6(4):63.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and
L. Ungar. 2013. Experiments with spectral learn-
ing of latent-variable PCFGs. In Proceedings of
NAACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maxi-
mum likelihood estimation from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society B, 39:1?38.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.
Ungar. 2012. Spectral dependency parsing with la-
tent variables. In Proceedings of EMNLP.
M. Frank and P. Wolfe. 1956. An algorithm for
quadratic programming. Naval research logistics
quarterly, 3(1-2):95?110.
H. Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321?377.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spectral
algorithm for learning hidden Markov models. In
Proceedings of COLT.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
ICASSP.
E. L. Lehmann and G. Casella. 1998. Theory of Point
Estimation (Second edition). Springer.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
of ACL.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral
algorithm for latent tree graphical models. In Pro-
ceedings of ICML.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of COLING-ACL.
L. Saul and F. Pereira. 1997. Aggregate and mixed-
order markov models for statistical language pro-
cessing. In Proceedings of EMNLP.
S. Siddiqi, B. Boots, and G. Gordon. 2010. Reduced-
rank hidden markov models. Journal of Machine
Learning Research, 9:741?748.
1061
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1481?1490,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Constrained Viterbi Relaxation for Bidirectional Word Alignment
Yin-Wen Chang Alexander M. Rush
MIT CSAIL,
Cambridge, MA 02139
{yinwen,srush}@
csail.mit.edu
John DeNero
UC Berkeley,
Berkeley, CA 94720
denero@
cs.berkeley.edu
Michael Collins
Columbia University,
New York, NY 10027
mcollins@
cs.columbia.edu
Abstract
Bidirectional models of word alignment
are an appealing alternative to post-hoc
combinations of directional word align-
ers. Unfortunately, most bidirectional
formulations are NP-Hard to solve, and
a previous attempt to use a relaxation-
based decoder yielded few exact solu-
tions (6%). We present a novel relax-
ation for decoding the bidirectional model
of DeNero and Macherey (2011). The
relaxation can be solved with a mod-
ified version of the Viterbi algorithm.
To find optimal solutions on difficult
instances, we alternate between incre-
mentally adding constraints and applying
optimality-preserving coarse-to-fine prun-
ing. The algorithm finds provably ex-
act solutions on 86% of sentence pairs
and shows improvements over directional
models.
1 Introduction
Word alignment is a critical first step for build-
ing statistical machine translation systems. In or-
der to ensure accurate word alignments, most sys-
tems employ a post-hoc symmetrization step to
combine directional word aligners, such as IBM
Model 4 (Brown et al, 1993) or hidden Markov
model (HMM) based aligners (Vogel et al, 1996).
Several authors have proposed bidirectional mod-
els that incorporate this step directly, but decoding
under many bidirectional models is NP-Hard and
finding exact solutions has proven difficult.
In this paper, we describe a novel Lagrangian-
relaxation based decoder for the bidirectional
model proposed by DeNero and Macherey (2011),
with the goal of improving search accuracy.
In that work, the authors implement a dual
decomposition-based decoder for the problem, but
are only able to find exact solutions for around 6%
of instances.
Our decoder uses a simple variant of the Viterbi
algorithm for solving a relaxed version of this
model. The algorithm makes it easy to re-
introduce constraints for difficult instances, at the
cost of increasing run-time complexity. To offset
this cost, we employ optimality-preserving coarse-
to-fine pruning to reduce the search space. The
pruning method utilizes lower bounds on the cost
of valid bidirectional alignments, which we obtain
from a fast, greedy decoder.
The method has the following properties:
? It is based on a novel relaxation for the model
of DeNero and Macherey (2011), solvable
with a variant of the Viterbi algorithm.
? To find optimal solutions, it employs an effi-
cient strategy that alternates between adding
constraints and applying pruning.
? Empirically, it is able to find exact solutions
on 86% of sentence pairs and is significantly
faster than general-purpose solvers.
We begin in Section 2 by formally describing
the directional word alignment problem. Section 3
describes a preliminary bidirectional model us-
ing full agreement constraints and a Lagrangian
relaxation-based solver. Section 4 modifies this
model to include adjacency constraints. Section 5
describes an extension to the relaxed algorithm to
explicitly enforce constraints, and Section 6 gives
a pruning method for improving the efficiency of
the algorithm.
Experiments compare the search error and accu-
racy of the new bidirectional algorithm to several
directional combiners and other bidirectional al-
gorithms. Results show that the new relaxation is
much more effective at finding exact solutions and
is able to produce comparable alignment accuracy.
1481
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
Figure 1: An example e?f directional alignment for the sen-
tences let us see the documents and montrez -
nous les documents, with I = 5 and J = 5. The in-
dices i ? [I]
0
are rows, and the indices j ? [J ]
0
are columns.
The HMM alignment shown has transitions x(0, 1, 1) =
x(1, 2, 3) = x(3, 3, 1) = x(1, 4, 4) = x(4, 5, 5) = 1.
Notation We use lower- and upper-case letters
for scalars and vectors, and script-case for sets
e.g. X . For vectors, such as v ? {0, 1}
(I?J )?J
,
where I and J are finite sets, we use the notation
v(i, j) and v(j) to represent elements of the vec-
tor. Define d = ?(i) to be the indicator vector with
d(i) = 1 and d(i
?
) = 0 for all i
?
6= i. Finally de-
fine the notation [J ] to refer to {1 . . . J} and [J ]
0
to refer to {0 . . . J}.
2 Background
The focus of this work is on the word alignment
decoding problem. Given a sentence e of length
|e| = I and a sentence f of length |f | = J , our
goal is to find the best bidirectional alignment be-
tween the two sentences under a given objective
function. Before turning to the model of interest,
we first introduce directional word alignment.
2.1 Word Alignment
In the e?f word alignment problem, each word
in e is aligned to a word in f or to the null word .
This alignment is a mapping from each index i ?
[I] to an index j ? [J ]
0
(where j = 0 represents
alignment to ). We refer to a single word align-
ment as a link.
A first-order HMM alignment model (Vogel et
al., 1996) is an HMM of length I + 1 where the
hidden state at position i ? [I]
0
is the aligned in-
dex j ? [J ]
0
, and the transition score takes into
account the previously aligned index j
?
? [J ]
0
.
1
Formally, define the set of possible HMM align-
ments as X ? {0, 1}
([I]
0
?[J ]
0
)?([I]?[J ]
0
?[J ]
0
)
with
1
Our definition differs slightly from other HMM-based
aligners in that it does not track the last  alignment.
X =
?
?
?
?
?
?
?
?
?
x : x(0, 0) = 1,
x(i, j) =
J?
j
?
=0
x(j
?
, i, j) ?i ? [I], j ? [J ]
0
,
x(i, j) =
J?
j
?
=0
x(j, i+ 1, j
?
) ?i ? [I ? 1]
0
, j ? [J ]
0
where x(i, j) = 1 indicates that there is a link
between index i and index j, and x(j
?
, i, j) = 1
indicates that index i ? 1 aligns to index j
?
and
index i aligns to j. Figure 1 shows an example
member of X .
The constraints of X enforce backward and for-
ward consistency respectively. If x(i, j) = 1,
backward consistency enforces that there is a tran-
sition from (i? 1, j
?
) to (i, j) for some j
?
? [J ]
0
,
whereas forward consistency enforces a transition
from (i, j) to (i+ 1, j
?
) for some j
?
? [J ]
0
. Infor-
mally the constraints ?chain? together the links.
The HMM objective function f : X ? R can
be written as a linear function of x
f(x; ?) =
I
?
i=1
J
?
j=0
J
?
j
?
=0
?(j
?
, i, j)x(j
?
, i, j)
where the vector ? ? R
[I]?[J ]
0
?[J ]
0
includes the
transition and alignment scores. For a generative
model of alignment, we might define ?(j
?
, i, j) =
log(p(e
i
|f
j
)p(j|j
?
)). For a discriminative model
of alignment, we might define ?(j
?
, i, j) = w ?
?(i, j
?
, j, f , e) for a feature function ? and weights
w (Moore, 2005; Lacoste-Julien et al, 2006).
Now reverse the direction of the model and
consider the f?e alignment problem. An f?e
alignment is a binary vector y ? Y where for
each j ? [J ], y(i, j) = 1 for exactly one i ?
[I]
0
. Define the set of HMM alignments Y ?
{0, 1}
([I]
0
?[J ]
0
)?([I]
0
?[I]
0
?[J ])
as
Y =
?
?
?
?
?
?
?
?
?
y : y(0, 0) = 1,
y(i, j) =
I?
i
?
=0
y(i
?
, i, j) ?i ? [I]
0
, j ? [J ],
y(i, j) =
I?
i
?
=0
y(i, i
?
, j + 1) ?i ? [I]
0
, j ? [J ? 1]
0
Similarly define the objective function
g(y;?) =
J
?
j=1
I
?
i=0
I
?
i
?
=0
?(i
?
, i, j)y(i
?
, i, j)
with vector ? ? R
[I]
0
?[I]
0
?[J ]
.
1482
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
(a)
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
(b)
Figure 2: (a) An example alignment pair (x, y) satisfying the
full agreement conditions. The x alignment is represented
with circles and the y alignment with triangles. (b) An exam-
ple f?e alignment y ? Y
?
with relaxed forward constraints.
Note that unlike an alignment from Y multiple words may
be aligned in a column and words may transition from non-
aligned positions.
Note that for both of these models we can solve
the optimization problem exactly using the stan-
dard Viterbi algorithm for HMM decoding. The
first can be solved in O(IJ
2
) time and the second
in O(I
2
J) time.
3 Bidirectional Alignment
The directional bias of the e?f and f?e align-
ment models may cause them to produce differing
alignments. To obtain the best single alignment,
it is common practice to use a post-hoc algorithm
to merge these directional alignments (Och et al,
1999). First, a directional alignment is found from
each word in e to a word f . Next an alignment is
produced in the reverse direction from f to e. Fi-
nally, these alignments are merged, either through
intersection, union, or with an interpolation algo-
rithm such as grow-diag-final (Koehn et al, 2003).
In this work, we instead consider a bidirectional
alignment model that jointly considers both direc-
tional models. We begin in this section by in-
troducing a simple bidirectional model that en-
forces full agreement between directional models
and giving a relaxation for decoding. Section 4
loosens this model to adjacent agreement.
3.1 Enforcing Full Agreement
Perhaps the simplest post-hoc merging strategy is
to retain the intersection of the two directional
models. The analogous bidirectional model en-
forces full agreement to ensure the two alignments
select the same non-null links i.e.
x
?
, y
?
= argmax
x?X ,y?Y
f(x) + g(y) s.t.
x(i, j) = y(i, j) ?i ? [I], j ? [J ]
We refer to the optimal alignments for this prob-
lem as x
?
and y
?
.
Unfortunately this bidirectional decoding
model is NP-Hard (a proof is given in Ap-
pendix A). As it is common for alignment pairs to
have |f | or |e| over 40, exact decoding algorithms
are intractable in the worst-case.
Instead we will use Lagrangian relaxation for
this model. At a high level, we will remove a
subset of the constraints from the original problem
and replace them with Lagrange multipliers. If we
can solve this new problem efficiently, we may be
able to get optimal solutions to the original prob-
lem. (See the tutorial by Rush and Collins (2012)
describing the method.)
There are many possible subsets of constraints
to consider relaxing. The relaxation we use pre-
serves the agreement constraints while relaxing
the Markov structure of the f?e alignment. This
relaxation will make it simple to later re-introduce
constraints in Section 5.
We relax the forward constraints of set Y . With-
out these constraints the y links are no longer
chained together. This has two consequences: (1)
for index j there may be any number of indices i,
such that y(i, j) = 1, (2) if y(i
?
, i, j) = 1 it is no
longer required that y(i
?
, j ? 1) = 1. This gives a
set Y
?
which is a superset of Y
Y
?
=
{
y : y(0, 0) = 1,
y(i, j) =
?
I
i
?
=0
y(i
?
, i, j) ?i ? [I]
0
, j ? [J ]
Figure 2(b) shows a possible y ? Y
?
and a valid
unchained structure.
To form the Lagrangian dual with relaxed for-
ward constraints, we introduce a vector of La-
grange multipliers, ? ? R
[I?1]
0
?[J ]
0
, with one
multiplier for each original constraint. The La-
grangian dual L(?) is defined as
max
x?X ,y?Y
?
,
x(i,j)=y(i,j)
f(x) +
I?
i=1
J?
j=0
I?
i
?
=0
y(i
?
, i, j)?(i
?
, i, j) (1)
?
I?
i=0
J?1?
j=0
?(i, j)
(
y(i, j)?
I?
i
?
=0
y(i, i
?
, j + 1)
)
= max
x?X ,y?Y
?
,
x(i,j)=y(i,j)
f(x) +
I?
i=1
J?
j=0
I?
i
?
=0
y(i
?
, i, j)?
?
(i
?
, i, j)(2)
= max
x?X ,y?Y
?
,
x(i,j)=y(i,j)
f(x) +
I?
i=1
J?
j=0
y(i, j) max
i
?
?[I]
0
?
?
(i
?
, i, j)(3)
= max
x?X ,y?Y
?
,
x(i,j)=y(i,j)
f(x) + g
?
(y;?, ?) (4)
1483
Line 2 distributes the ??s and introduces a modi-
fied potential vector ?
?
defined as
?
?
(i
?
, i, j) = ?(i
?
, i, j)? ?(i, j) + ?(i
?
, j ? 1)
for all i
?
? [I]
0
, i ? [I]
0
, j ? [J ]. Line 3 uti-
lizes the relaxed set Y
?
which allows each y(i, j)
to select the best possible previous link (i
?
, j ? 1).
Line 4 introduces the modified directional objec-
tive
g
?
(y;?, ?) =
I
?
i=1
J
?
j=0
y(i, j) max
i
?
?[I]
0
?
?
(i
?
, i, j)
The Lagrangian dual is guaranteed to be an up-
per bound on the optimal solution, i.e. for all ?,
L(?) ? f(x
?
) + g(y
?
). Lagrangian relaxation
attempts to find the tighest possible upper bound
by minimizing the Lagrangian dual, min
?
L(?),
using subgradient descent. Briefly, subgradient
descent is an iterative algorithm, with two steps.
Starting with ? = 0, we iteratively
1. Set (x, y) to the argmax of L(?).
2. Update ?(i, j) for all i ? [I ? 1]
0
, j ? [J ]
0
,
?(i, j)? ?(i, j)? ?
t
(
y(i, j)?
I?
i
?
=0
y(i, i
?
, j + 1)
)
.
where ?
t
> 0 is a step size for the t?th update. If
at any iteration of the algorithm the forward con-
straints are satisfied for (x, y), then f(x)+g(y) =
f(x
?
) + g(x
?
) and we say this gives a certificate
of optimality for the underlying problem.
To run this algorithm, we need to be able to effi-
ciently compute the (x, y) pair that is the argmax
of L(?) for any value of ?. Fortunately, since the y
alignments are no longer constrained to valid tran-
sitions, we can compute these alignments by first
picking the best f?e transitions for each possible
link, and then running an e?f Viterbi-style algo-
rithm to find the bidirectional alignment.
The max version of this algorithm is shown in
Figure 3. It consists of two steps. We first compute
the score for each y(i, j) variable. We then use the
standard Viterbi update for computing the x vari-
ables, adding in the score of the y(i, j) necessary
to satisfy the constraints.
procedure VITERBIFULL(?, ?
?
)
Let pi, ? be dynamic programming charts.
?[i, j]? max
i
?
?[I]
0
?
?
(i
?
, i, j) ? i ? [I], j ? [J ]
0
pi[0, 0]?
?
J
j=1
max{0, ?[0, j]}
for i ? [I], j ? [J ]
0
in order do
pi[i, j]? max
j
?
?[J]
0
?(j
?
, i, j) + pi[i? 1, j
?
]
if j 6= 0 then pi[i, j]? pi[i, j] + ?[i, j]
return max
j?[J]
0
pi[I, j]
Figure 3: Viterbi-style algorithm for computing L(?). For
simplicity the algorithm shows the max version of the algo-
rithm, argmax can be computed with back-pointers.
4 Adjacent Agreement
Enforcing full agreement can be too strict an align-
ment criteria. DeNero and Macherey (2011) in-
stead propose a model that allows near matches,
which we call adjacent agreement. Adjacent
agreement allows links from one direction to agree
with adjacent links from the reverse alignment for
a small penalty. Figure 4(a) shows an example
of a valid bidirectional alignment under adjacent
agreement.
In this section we formally introduce adjacent
agreement, and propose a relaxation algorithm for
this model. The key algorithmic idea is to extend
the Viterbi algorithm in order to consider possible
adjacent links in the reverse direction.
4.1 Enforcing Adjacency
Define the adjacency set K = {?1, 0, 1}. A bidi-
rectional alignment satisfies adjacency if for all
i ? [I], j ? [J ],
? If x(i, j) = 1, it is required that y(i+k, j) =
1 for exactly one k ? K (i.e. either above,
center, or below). We indicate which position
with variables z
l
i,j
? {0, 1}
K
? If x(i, j) = 1, it is allowed that y(i, j + k) =
1 for any k ? K (i.e. either left, center, or
right) and all other y(i, j
?
) = 0. We indicate
which positions with variables z
?
i,j
? {0, 1}
K
Formally for x ? X and y ? Y , the pair (x, y) is
feasible if there exists a z from the set Z(x, y) ?
{0, 1}
K
2
?[I]?[J ]
defined as
Z(x, y) =
?
?
?
?
?
?
?
?
?
z : ?i ? [I], j ? [J ]
z
l
i,j
? {0, 1}
K
, z
?
i,j
? {0, 1}
K
x(i, j) =
?
k?K
z
l
i,j
(k),
?
k?K
z
?
i,j
(k) = y(i, j),
z
l
i,j
(k) ? y(i+ k, j) ?k ? K : i+ k > 0,
x(i, j) ? z
?
i,j?k
(k) ?k ? K : j + k > 0
1484
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
(a)
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
(b)
Figure 4: (a) An alignment satisfying the adjacency con-
straints. Note that x(2, 1) = 1 is allowed because of
y(1, 1) = 1, x(4, 3) = 1 because of y(3, 3), and y(3, 1)
because of x(3, 2). (b) An adjacent bidirectional alignment
in progress. Currently x(2, 2) = 1 with z
l
(?1) = 1 and
z
?
(?1) = 1. The last transition was from x(1, 3) with
z
??
(?1) = 1, z
??
(0) = 1, z
l?
(0) = 1.
Additionally adjacent, non-overlapping
matches are assessed a penalty ? calculated as
h(z) =
I
?
i=1
J
?
j=1
?
k?K
?|k|(z
l
i,j
(k) + z
?
i,j
(k))
where ? ? 0 is a parameter of the model. The
example in Figure 4(a) includes a 3? penalty.
Adding these penalties gives the complete adja-
cent agreement problem
argmax
z?Z(x,y)
x?X ,y?Y
f(x) + g(y) + h(z)
Next, apply the same relaxation from Sec-
tion 3.1, i.e. we relax the forward constraints of
the f?e set. This yields the following Lagrangian
dual
L(?) = max
z?Z(x,y)
x?X ,y?Y
?
f(x) + g
?
(y;?, ?) + h(z)
Despite the new constraints, we can still com-
pute L(?) in O(IJ(I + J)) time using a variant
of the Viterbi algorithm. The main idea will be to
consider possible adjacent settings for each link.
Since each z
l
i,j
and z
?
i,j
only have a constant num-
ber of settings, this does not increase the asymp-
totic complexity of the algorithm.
Figure 5 shows the algorithm for computing
L(?). The main loop of the algorithm is similar to
Figure 3. It proceeds row-by-row, picking the best
alignment x(i, j) = 1. The major change is that
the chart pi also stores a value z ? {0, 1}
K?K
rep-
resenting a possible z
l
i,j
, z
?
i,j
pair. Since we have
procedure VITERBIADJ(?, ?
?
)
?[i, j]? max
i
?
?[I]
0
?
?
(i
?
, i, j) ? i ? [I], j ? [J ]
0
pi[0, 0]?
?
J
j=1
max{0, ?[0, j]}
for i ? [I], j ? [J ]
0
, z
l
, z
?
? {0, 1}
|K|
do
pi[i, j, z]?
max
j
?
?[J]
0
,
z
?
?N (z,j?j
?
)
?(j
?
, i, j) + pi[i? 1, j
?
, z
?
]
+
?
k?K
z
?
(k)(?[i, j + k] + ?|k|)
+z
l
(k)?|k|
return max
j?[J]
0
,z?{0,1}
|K?K|
pi[I, j, z]
Figure 5: Modified Viterbi algorithm for computing the adja-
cent agreement L(?).
the proposed z
i,j
in the inner loop, we can include
the scores of the adjacent y alignments that are
in neighboring columns, as well as the possible
penalty for matching x(i, j) to a y(i + k, j) in a
different row. Figure 4(b) gives an example set-
ting of z.
In the dynamic program, we need to ensure that
the transitions between the z?s are consistent. The
vector z
?
indicates the y links adjacent to x(i ?
1, j
?
). If j
?
is near to j, z
?
may overlap with z
and vice-versa. The transition setN ensures these
indicators match up
N (z, k
?
) =
?
?
?
z
?
: (z
l
(?1) ? k
?
? K)? z
??
(k
?
),
(z
l?
(1) ? k
?
? K)? z
?
(?k
?
),
?
k?K
z
l
(k) = 1
5 Adding Back Constraints
In general, it can be shown that Lagrangian relax-
ation is only guaranteed to solve a linear program-
ming relaxation of the underlying combinatorial
problem. For difficult instances, we will see that
this relaxation often does not yield provably exact
solutions. However, it is possible to ?tighten? the
relaxation by re-introducing constraints from the
original problem.
In this section, we extend the algorithm to al-
low incrementally re-introducing constraints. In
particular we track which constraints are most of-
ten violated in order to explicitly enforce them in
the algorithm.
Define a binary vector p ? {0, 1}
[I?1]
0
?[J ]
0
where p(i, j) = 1 indicates a previously re-
laxed constraint on link y(i, j) that should be re-
introduced into the problem. Let the new partially
1485
constrained Lagrangian dual be defined as
L(?; p) = max
z?Z(x,y)
x?X ,y?Y
?
f(x) + g
?
(y;?, ?) + h(z)
y(i, j) =
?
i
?
y(i, i
?
, j + 1) ?i, j : p(i, j) = 1
If p =
~
1, the problem includes all of the original
constraints, whereas p =
~
0 gives our original La-
grangian dual. In between we have progressively
more constrained variants.
In order to compute the argmax of this op-
timization problem, we need to satisfy the con-
straints within the Viterbi algorithm. We augment
the Viterbi chart with a count vector d ? D where
D ? Z
||p||
1
and d(i, j) is a count for the (i, j)?th
constraint, i.e. d(i, j) = y(i, j) ?
?
i
?
y(i
?
, i, j).
Only solutions with count 0 at the final position
satisfy the active constraints. Additionally de-
fine a helper function [?]
D
as the projection from
Z
[I?1]
0
?[J ]
? D, which truncates dimensions
without constraints.
Figure 6 shows this constrained Viterbi relax-
ation approach. It takes p as an argument and en-
forces the active constraints. For simplicity, we
show the full agreement version, but the adjacent
agreement version is similar. The main new addi-
tion is that the inner loop of the algorithm ensures
that the count vector d is the sum of the counts of
its children d
?
and d? d
?
.
Since each additional constraint adds a dimen-
sion to d, adding constraints has a multiplicative
impact on running time. Asymptotically the new
algorithm requires O(2
||p||
1
IJ(I + J)) time. This
is a problem in practice as even adding a few con-
straints can make the problem intractable. We ad-
dress this issue in the next section.
6 Pruning
Re-introducing constraints can lead to an expo-
nential blow-up in the search space of the Viterbi
algorithm. In practice though, many alignments
in this space are far from optimal, e.g. align-
ing a common word like the to nous instead
of les. Since Lagrangian relaxation re-computes
the alignment many times, it would be preferable
to skip these links in later rounds, particularly after
re-introducing constraints.
In this section we describe an optimality pre-
serving coarse-to-fine algorithm for pruning. Ap-
proximate coarse-to-fine pruning algorithms are
procedure CONSVITERBIFULL(?, ?
?
, p)
for i ? [I], j ? [J ]
0
, i
?
? [I] do
d? |?(i, j)? ?(i
?
, j ? 1)|
D
?[i, j, d]? ?
?
(i
?
, i, j)
for j ? [J ], d ? D do
pi[0, 0, d]? max
d
?
?D
pi[0, 0, d
?
] + ?[0, j, d? d
?
]
for i ? [I], j ? [J ]
0
, d ? D do
if j = 0 then
pi[i, j, d]? max
j
?
?[J]
0
?(j
?
, i, j) + pi[i? 1, j
?
, d]
else
pi[i, j, d]?
max
j
?
?[J]
0
,d
?
?D
?(j
?
, i, j) + pi[i? 1, j
?
, d
?
]
+?[i, j, d? d
?
]
return max
j?[J]
0
pi[I, j,0]
Figure 6: Constrained Viterbi algorithm for finding partially-
constrained, full-agreement alignments. The argument p in-
dicates which constraints to enforce.
widely used within NLP, but exact pruning is
less common. Our method differs in that it
only eliminates non-optimal transitions based on
a lower-bound score. After introducing the prun-
ing method, we present an algorithm to make this
method effective in practice by producing high-
scoring lower bounds for adjacent agreement.
6.1 Thresholding Max-Marginals
Our pruning method is based on removing transi-
tions with low max-marginal values. Define the
max-marginal value of an e?f transition in our
Lagrangian dual as
M(j
?
, i, j;?) = max
z?Z(x,y),
x?X ,y?Y
?
f(x) + g
?
(y;?) + h(z)
s.t. x(j
?
, i, j) = 1
where M gives the value of the best dual align-
ment that transitions from (i ? 1, j
?
) to (i, j).
These max-marginals can be computed by running
a forward-backward variant of any of the algo-
rithms described thus far.
We make the following claim about max-
marginal values and any lower-bound score
Lemma 1 (Safe Pruning). For any valid con-
strained alignment x ? X , y ? Y, z ? Z(x, y)
and for any dual vector ? ? R
[I?1]
0
?[J ]
0
, if there
exists a transition j
?
, i, j with max-marginal value
M(j
?
, i, j;?) < f(x)+g(y)+h(z) then the tran-
sition will not be in the optimal alignment, i.e.
x
?
(j
?
, i, j) = 0.
This lemma tells us that we can prune transi-
tions whose dual max-marginal value falls below
1486
a threshold without pruning possibly optimal tran-
sitions. Pruning these transitions can speed up La-
grangian relaxation without altering its properties.
Furthermore, the threshold is determined by any
feasible lower bound on the optimal score, which
means that better bounds can lead to more pruning.
6.2 Finding Lower Bounds
Since the effectiveness of pruning is dependent on
the lower bound, it is crucial to be able to produce
high-scoring alignments that satisfy the agreement
constraints. Unfortunately, this problem is non-
trivial. For instance, taking the union of direc-
tional alignments does not guarantee a feasible so-
lution; whereas taking the intersection is trivially
feasible but often not high-scoring.
To produce higher-scoring feasible bidirectional
alignments we introduce a greedy heuristic al-
gorithm. The algorithm starts with any feasible
alignment (x, y, z). It runs the following greedy
loop:
1. Repeat until there exists no x(i, 0) = 1 or
y(0, j) = 1, or there is no score increase.
(a) For each i ? [I], j ? [J ]
0
, k ? K :
x(i, 0) = 1, check if x(i, j) ? 1 and
y(i, j + k) ? 1 is feasible, remember
score.
(b) For each i ? [I]
0
, j ? [J ], k ? K :
y(0, j) = 1, check if y(i, j) ? 1 and
x(i + k, j) ? 1 is feasible, remember
score.
(c) Let (x, y, z) be the highest-scoring fea-
sible solution produced.
This algorithm produces feasible alignments with
monotonically increasing score, starting from the
intersection of the alignments. It has run-time of
O(IJ(I + J)) since each inner loop enumerates
IJ possible updates and assigns at least one index
a non-zero value, limiting the outer loop to I + J
iterations.
In practice we initialize the heuristic based on
the intersection of x and y at the current round
of Lagrangian relaxation. Experiments show that
running this algorithm significantly improves the
lower bound compared to just taking the intersec-
tion, and consequently helps pruning significantly.
7 Related Work
The most common techniques for bidirectional
alignment are post-hoc combinations, such as
union or intersection, of directional models, (Och
et al, 1999), or more complex heuristic combiners
such as grow-diag-final (Koehn et al, 2003).
Several authors have explored explicit bidirec-
tional models in the literature. Cromieres and
Kurohashi (2009) use belief propagation on a fac-
tor graph to train and decode a one-to-one word
alignment problem. Qualitatively this method is
similar to ours, although the model and decoding
algorithm are different, and their method is not
able to provide certificates of optimality.
A series of papers by Ganchev et al (2010),
Graca et al (2008), and Ganchev et al (2008) use
posterior regularization to constrain the posterior
probability of the word alignment problem to be
symmetric and bijective. This work acheives state-
of-the-art performance for alignment. Instead of
utilizing posteriors our model tries to decode a sin-
gle best one-to-one word alignment.
A different approach is to use constraints at
training time to obtain models that favor bidi-
rectional properties. Liang et al (2006) propose
agreement-based learning, which jointly learns
probabilities by maximizing a combination of
likelihood and agreement between two directional
models.
General linear programming approaches have
also been applied to word alignment problems.
Lacoste-Julien et al (2006) formulate the word
alignment problem as quadratic assignment prob-
lem and solve it using an integer linear program-
ming solver.
Our work is most similar to DeNero and
Macherey (2011), which uses dual decomposition
to encourage agreement between two directional
HMM aligners during decoding time.
8 Experiments
Our experimental results compare the accuracy
and optimality of our decoding algorithm to direc-
tional alignment models and previous work on this
bidirectional model.
Data and Setup The experimental setup is iden-
tical to DeNero and Macherey (2011). Evalu-
ation is performed on a hand-aligned subset of
the NIST 2002 Chinese-English dataset (Ayan and
Dorr, 2006). Following past work, the first 150
sentence pairs of the training section are used for
evaluation. The potential parameters ? and ? are
set based on unsupervised HMM models trained
on the LDC FBIS corpus (6.2 million words).
1487
1-20 (28%) 21-40 (45%) 41-60 (27%) all
time cert exact time cert exact time cert exact time cert exact
ILP 15.12 100.0 100.0 364.94 100.0 100.0 2,829.64 100.0 100.0 924.24 100.0 100.0
LR 0.55 97.6 97.6 4.76 55.9 55.9 15.06 7.5 7.5 6.33 54.7 54.7
CONS 0.43 100.0 100.0 9.86 95.6 95.6 61.86 55.0 62.5 21.08 86.0 88.0
D&M - 6.2 - - 0.0 - - 0.0 - - 6.2 -
Table 1: Experimental results for model accuracy of bilingual alignment. Column time is the mean time per sentence pair in
seconds; cert is the percentage of sentence pairs solved with a certificate of optimality; exact is the percentage of sentence pairs
solved exactly. Results are grouped by sentence length. The percentage of sentence pairs in each group is shown in parentheses.
Training is performed using the agreement-based
learning method which encourages the directional
models to overlap (Liang et al, 2006). This direc-
tional model has been shown produce state-of-the-
art results with this setup (Haghighi et al, 2009).
Baselines We compare the algorithm described
in this paper with several baseline methods. DIR
includes post-hoc combinations of the e?f and
f?e HMM-based aligners. Variants include
union, intersection, and grow-diag-final. D&M is
the dual decomposition algorithm for bidirectional
alignment as presented by DeNero and Macherey
(2011) with different final combinations. LR is the
Lagrangian relaxation algorithm applied to the ad-
jacent agreement problem without the additional
constraints described in Section 5. CONS is our
full Lagrangian relaxation algorithm including in-
cremental constraint addition. ILP uses a highly-
optimized general-purpose integer linear program-
ming solver to solve the lattice with the constraints
described (Gurobi Optimization, 2013).
Implementation The main task of the decoder
is to repeatedly compute the argmax of L(?).
To speed up decoding, our implementation fully
instantiates the Viterbi lattice for a problem in-
stance. This approach has several benefits: each
iteration can reuse the same lattice structure; max-
marginals can be easily computed with a gen-
eral forward-backward algorithm; pruning corre-
sponds to removing lattice edges; and adding con-
straints can be done through lattice intersection.
For consistency, we implement each baseline (ex-
cept for D&M) through the same lattice.
Parameter Settings We run 400 iterations of
the subgradient algorithm using the rate schedule
?
t
= 0.95
t
?
where t
?
is the count of updates for
which the dual value did not improve. Every 10
iterations we run the greedy decoder to compute
a lower bound. If the gap between our current
dual value L(?) and the lower bound improves
significantly we run coarse-to-fine pruning as de-
scribed in Section 6 with the best lower bound. For
Model Combiner
alignment phrase pair
Prec Rec AER Prec Rec F1
DIR
union 57.6 80.0 33.4 75.1 33.5 46.3
intersection 86.2 62.9 27.0 64.3 43.5 51.9
grow-diag 59.7 79.5 32.1 70.1 36.9 48.4
D&M
union 63.3 81.5 29.1 63.2 44.9 52.5
intersection 77.5 75.1 23.6 57.1 53.6 55.3
grow-diag 65.6 80.6 28.0 60.2 47.4 53.0
CONS 72.5 74.9 26.4 53.0 52.4 52.7
Table 2: Alignment accuracy and phrase pair extraction ac-
curacy for directional and bidirectional models. Prec is the
precision. Rec is the recall. AER is alignment error rate and
F1 is the phrase pair extraction F1 score.
CONS, if the algorithm does not find an optimal
solution we run 400 more iterations and incremen-
tally add the 5 most violated constraints every 25
iterations.
Results Our first set of experiments looks at the
model accuracy and the decoding time of various
methods that can produce optimal solutions. Re-
sults are shown in Table 1. D&M is only able to
find the optimal solution with certificate on 6% of
instances. The relaxation algorithm used in this
work is able to increase that number to 54.7%.
With incremental constraints and pruning, we are
able to solve over 86% of sentence pairs includ-
ing many longer and more difficult pairs. Addi-
tionally the method finds these solutions with only
a small increase in running time over Lagrangian
relaxation, and is significantly faster than using an
ILP solver.
Next we compare the models in terms of align-
ment accuracy. Table 2 shows the precision, recall
and alignment error rate (AER) for word align-
ment. We consider union, intersection and grow-
diag-final as combination procedures. The com-
bination procedures are applied to D&M in the
case when the algorithm does not converge. For
CONS, we use the optimal solution for the 86%
of instances that converge and the highest-scoring
greedy solution for those that do not. The pro-
posed method has an AER of 26.4, which outper-
forms each of the directional models. However,
although CONS achieves a higher model score
than D&M, it performs worse in accuracy. Ta-
1488
1-20 21-40 41-60 all
# cons. 20.0 32.1 39.5 35.9
Table 3: The average number of constraints added for sen-
tence pairs where Lagrangian relaxation is not able to find an
exact solution.
ble 2 also compares the models in terms of phrase-
extraction accuracy (Ayan and Dorr, 2006). We
use the phrase extraction algorithm described by
DeNero and Klein (2010), accounting for possi-
ble links and  alignments. CONS performs bet-
ter than each of the directional models, but worse
than the best D&M model.
Finally we consider the impact of constraint ad-
dition, pruning, and use of a lower bound. Table 3
gives the average number of constraints added for
sentence pairs for which Lagrangian relaxation
alone does not produce a certificate. Figure 7(a)
shows the average over all sentence pairs of the
best dual and best primal scores. The graph com-
pares the use of the greedy algorithm from Sec-
tion 6.2 with the simple intersection of x and y.
The difference between these curves illustrates the
benefit of the greedy algorithm. This is reflected
in Figure 7(b) which shows the effectiveness of
coarse-to-fine pruning over time. On average, the
pruning reduces the search space of each sentence
pair to 20% of the initial search space after 200
iterations.
9 Conclusion
We have introduced a novel Lagrangian relaxation
algorithm for a bidirectional alignment model that
uses incremental constraint addition and coarse-
to-fine pruning to find exact solutions. The algo-
rithm increases the number of exact solution found
on the model of DeNero and Macherey (2011)
from 6% to 86%.
Unfortunately despite achieving higher model
score, this approach does not produce more accu-
rate alignments than the previous algorithm. This
suggests that the adjacent agreement model may
still be too constrained for this underlying task.
Implicitly, an approach with fewer exact solu-
tions may allow for useful violations of these con-
straints. In future work, we hope to explore bidi-
rectional models with soft-penalties to explicitly
permit these violations.
A Proof of NP-Hardness
We can show that the bidirectional alignment
problem is NP-hard by reduction from the trav-
0 50 100 150 200 250 300 350 400iteration
100
50
0
50
100
sco
re 
rel
ati
ve 
to 
opt
ima
l best dualbest primal
intersection
(a) The best dual and the best primal score, relative to the
optimal score, averaged over all sentence pairs. The best
primal curve uses a feasible greedy algorithm, whereas the
intersection curve is calculated by taking the intersec-
tion of x and y.
0 50 100 150 200 250 300 350 400number of iterations
0.0
0.2
0.4
0.6
0.8
1.0
rel
ati
ve 
sea
rch
 sp
ace
 siz
e
(b) A graph showing the effectiveness of coarse-to-fine prun-
ing. Relative search space size is the size of the pruned lattice
compared to the initial size. The plot shows an average over
all sentence pairs.
Figure 7
eling salesman problem (TSP). A TSP instance
with N cities has distance c(i
?
, i) for each (i
?
, i) ?
[N ]
2
. We can construct a sentence pair in which
I = J = N and -alignments have infinite cost.
?(i
?
, i, j) = ?c(i
?
, i) ?i
?
? [N ]
0
, i ? [N ], j ? [N ]
?(j
?
, i, j) = 0 ?j
?
? [N ]
0
, i ? [N ], j ? [N ]
?(i
?
, 0, j) = ?? ?i
?
? [N ]
0
, j ? [N ]
?(j
?
, i, 0) = ?? ?j
?
? [N ]
0
, i ? [N ]
Every bidirectional alignment with finite objec-
tive score must align exactly one word in e to each
word in f, encoding a permutation a. Moreover,
each possible permutation has a finite score: the
negation of the total distance to traverse the N
cities in order a under distance c. Therefore, solv-
ing such a bidirectional alignment problem would
find a minimal Hamiltonian path of the TSP en-
coded in this way, concluding the reduction.
Acknowledgments Alexander Rush, Yin-Wen
Chang and Michael Collins were all supported
by NSF grant IIS-1161814. Alexander Rush was
partially supported by an NSF Graduate Research
Fellowship.
1489
References
Necip Fazil Ayan and Bonnie J Dorr. 2006. Going
beyond aer: An extensive analysis of word align-
ments and their impact on mt. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 9?16.
Association for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Fabien Cromieres and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 166?174. Association for Computational Lin-
guistics.
John DeNero and Dan Klein. 2010. Discriminative
modeling of extraction sets for machine translation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1453?1463. Association for Computational Linguis-
tics.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In ACL, pages 420?429.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior Regularization for Structured La-
tent Variable Models. Journal of Machine Learning
Research, 11:2001?2049.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20, pages 569?576. MIT Press, Cambridge,
MA.
Inc. Gurobi Optimization. 2013. Gurobi optimizer ref-
erence manual.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with su-
pervised itg models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 923?931. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 112?
119. Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
Robert C Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88. Association for Computational
Linguistics.
Franz Josef Och, Christoph Tillmann, Hermann Ney,
et al 1999. Improved alignment models for statis-
tical machine translation. In Proc. of the Joint SIG-
DAT Conf. on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages
20?28.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305?362.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841. Association for Computational Linguistics.
1490
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 9?16
Manchester, August 2008
TAG, Dynamic Programming, and the Perceptron
for Efficient, Feature-rich Parsing
Xavier Carreras Michael Collins Terry Koo
MIT CSAIL, Cambridge, MA 02139, USA
{carreras,mcollins,maestro}@csail.mit.edu
Abstract
We describe a parsing approach that makes use
of the perceptron algorithm, in conjunction with
dynamic programming methods, to recover full
constituent-based parse trees. The formalism allows
a rich set of parse-tree features, including PCFG-
based features, bigram and trigram dependency fea-
tures, and surface features. A severe challenge in
applying such an approach to full syntactic pars-
ing is the efficiency of the parsing algorithms in-
volved. We show that efficient training is feasi-
ble, using a Tree Adjoining Grammar (TAG) based
parsing formalism. A lower-order dependency pars-
ing model is used to restrict the search space of the
full model, thereby making it efficient. Experiments
on the Penn WSJ treebank show that the model
achieves state-of-the-art performance, for both con-
stituent and dependency accuracy.
1 Introduction
In global linear models (GLMs) for structured pre-
diction, (e.g., (Johnson et al, 1999; Lafferty et al,
2001; Collins, 2002; Altun et al, 2003; Taskar et
al., 2004)), the optimal label y? for an input x is
y
?
= arg max
y?Y(x)
w ? f(x, y) (1)
where Y(x) is the set of possible labels for the in-
put x; f(x, y) ? Rd is a feature vector that rep-
resents the pair (x, y); and w is a parameter vec-
tor. This paper describes a GLM for natural lan-
guage parsing, trained using the averaged percep-
tron. The parser we describe recovers full syntac-
tic representations, similar to those derived by a
probabilistic context-free grammar (PCFG). A key
motivation for the use of GLMs in parsing is that
they allow a great deal of flexibility in the features
which can be included in the definition of f(x, y).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
A critical problem when training a GLM for
parsing is the computational complexity of the
inference problem. The averaged perceptron re-
quires the training set to be repeatedly decoded
under the model; under even a simple PCFG rep-
resentation, finding the argmax in Eq. 1 requires
O(n
3
G) time, where n is the length of the sen-
tence, and G is a grammar constant. The average
sentence length in the data set we use (the Penn
WSJ treebank) is over 23 words; the grammar con-
stant G can easily take a value of 1000 or greater.
These factors make exact inference algorithms vir-
tually intractable for training or decoding GLMs
for full syntactic parsing.
As a result, in spite of the potential advantages
of these methods, there has been very little previ-
ous work on applying GLMs for full parsing with-
out the use of fairly severe restrictions or approxi-
mations. For example, the model in (Taskar et al,
2004) is trained on only sentences of 15 words or
less; reranking models (Collins, 2000; Charniak
and Johnson, 2005) restrict Y(x) to be a small set
of parses from a first-pass parser; see section 1.1
for discussion of other related work.
The following ideas are central to our approach:
(1) A TAG-based, splittable grammar. We
describe a novel, TAG-based parsing formalism
that allows full constituent-based trees to be recov-
ered. A driving motivation for our approach comes
from the flexibility of the feature-vector represen-
tations f(x, y) that can be used in the model. The
formalism that we describe allows the incorpora-
tion of: (1) basic PCFG-style features; (2) the
use of features that are sensitive to bigram depen-
dencies between pairs of words; and (3) features
that are sensitive to trigram dependencies. Any
of these feature types can be combined with sur-
face features of the sentence x, in a similar way
9
to the use of surface features in conditional ran-
dom fields (Lafferty et al, 2001). Crucially, in
spite of these relatively rich representations, the
formalism can be parsed efficiently (in O(n4G)
time) using dynamic-programming algorithms de-
scribed by Eisner (2000) (unlike many other TAG-
related approaches, our formalism is ?splittable?
in the sense described by Eisner, leading to more
efficient parsing algorithms).
(2) Use of a lower-order model for pruning.
The O(n4G) running time of the TAG parser is
still too expensive for efficient training with the
perceptron. We describe a method that leverages
a simple, first-order dependency parser to restrict
the search space of the TAG parser in training and
testing. The lower-order parser runs in O(n3H)
time where H ? G; experiments show that it is
remarkably effective in pruning the search space
of the full TAG parser.
Experiments on the Penn WSJ treebank show
that the model recovers constituent structures with
higher accuracy than the approaches of (Charniak,
2000; Collins, 2000; Petrov and Klein, 2007),
and with a similar level of performance to the
reranking parser of (Charniak and Johnson, 2005).
The model also recovers dependencies with sig-
nificantly higher accuracy than state-of-the-art de-
pendency parsers such as (Koo et al, 2008; Mc-
Donald and Pereira, 2006).
1.1 Related Work
Previous work has made use of various restrictions
or approximations that allow efficient training of
GLMs for parsing. This section describes the rela-
tionship between our work and this previous work.
In reranking approaches, a first-pass parser
is used to enumerate a small set of candidate
parses for an input sentence; the reranking model,
which is a GLM, is used to select between these
parses (e.g., (Ratnaparkhi et al, 1994; Johnson et
al., 1999; Collins, 2000; Charniak and Johnson,
2005)). A crucial advantage of our approach is that
it considers a very large set of alternatives in Y(x),
and can thereby avoid search errors that may be
made in the first-pass parser.1
Another approach that allows efficient training
of GLMs is to use simpler syntactic representa-
tions, in particular dependency structures (McDon-
1Some features used within reranking approaches may be
difficult to incorporate within dynamic programming, but it
is nevertheless useful to make use of GLMs in the dynamic-
programming stage of parsing. Our parser could, of course,
be used as the first-stage parser in a reranking approach.
ald et al, 2005). Dependency parsing can be
implemented in O(n3) time using the algorithms
of Eisner (2000). In this case there is no gram-
mar constant, and parsing is therefore efficient. A
disadvantage of these approaches is that they do
not recover full, constituent-based syntactic struc-
tures; the increased linguistic detail in full syntac-
tic structures may be useful in NLP applications,
or may improve dependency parsing accuracy, as
is the case in our experiments.2
There has been some previous work on GLM
approaches for full syntactic parsing that make use
of dynamic programming. Taskar et al (2004)
describe a max-margin approach; however, in this
work training sentences were limited to be of 15
words or less. Clark and Curran (2004) describe
a log-linear GLM for CCG parsing, trained on the
Penn treebank. This method makes use of paral-
lelization across an 18 node cluster, together with
up to 25GB of memory used for storage of dy-
namic programming structures for training data.
Clark and Curran (2007) describe a perceptron-
based approach for CCG parsing which is consid-
erably more efficient, and makes use of a super-
tagging model to prune the search space of the full
parsing model. Recent work (Petrov et al, 2007;
Finkel et al, 2008) describes log-linear GLMs ap-
plied to PCFG representations, but does not make
use of dependency features.
2 The TAG-Based Parsing Model
2.1 Derivations
This section describes the idea of derivations in
our parsing formalism. As in context-free gram-
mars or TAGs, a derivation in our approach is a
data structure that specifies the sequence of opera-
tions used in combining basic (elementary) struc-
tures in a grammar, to form a full parse tree. The
parsing formalism we use is related to the tree ad-
joining grammar (TAG) formalisms described in
(Chiang, 2003; Shen and Joshi, 2005). However,
an important difference of our work from this pre-
vious work is that our formalism is defined to be
?splittable?, allowing use of the efficient parsing
algorithms of Eisner (2000).
A derivation in our model is a pair ?E,D? where
E is a set of spines, and D is a set of dependencies
2Note however that the lower-order parser that we use to
restrict the search space of the TAG-based parser is based on
the work of McDonald et al (2005). See also (Sagae et al,
2007) for a method that uses a dependency parser to restrict
the search space of a more complex HPSG parser.
10
(a) S
VP
VBD
ate
NP
NN
cake
(b) S
VP
VP
VBD
ate
NP
NN
cake
Figure 1: Two example trees.
specifying how the spines are combined to form
a parse tree. The spines are similar to elementary
trees in TAG. Some examples are as follows:
NP
NNP
John
S
VP
VBD
ate
NP
NN
cake
ADVP
RB
quickly
ADVP
RB
luckily
These structures do not have substitution nodes, as
is common in TAGs.3 Instead, the spines consist
of a lexical anchor together with a series of unary
projections, which usually correspond to different
X-bar levels associated with the anchor.
The operations used to combine spines are sim-
ilar to the TAG operations of adjunction and sis-
ter adjunction. We will call these operations regu-
lar adjunction (r-adjunction) and sister adjunction
(s-adjunction). As one example, the cake spine
shown above can be s-adjoined into the VP node of
the ate spine, to form the tree shown in figure 1(a).
In contrast, if we use the r-adjunction operation to
adjoin the cake tree into the VP node, we get a dif-
ferent structure, which has an additional VP level
created by the r-adjunction operation: the resulting
tree is shown in figure 1(b). The r-adjunction op-
eration is similar to the usual adjunction operation
in TAGs, but has some differences that allow our
grammars to be splittable; see section 2.3 for more
discussion.
We now give formal definitions of the sets E and
D. Take x to be a sentence consisting of n + 1
words, x
0
. . . x
n
, where x
0
is a special root sym-
bol, which we will denote as ?. A derivation for the
input sentence x consists of a pair ?E,D?, where:
? E is a set of (n + 1) tuples of the form ?i, ??,
where i ? {0 . . . n} is an index of a word in the
sentence, and ? is the spine associated with the
word x
i
. The set E specifies one spine for each
of the (n + 1) words in the sentence. Where it is
3It would be straightforward to extend the approach to in-
clude substitution nodes, and a substitution operation.
clear from context, we will use ?
i
to refer to the
spine in E corresponding to the i?th word.
? D is a set of n dependencies. Each depen-
dency is a tuple ?h,m, l?. Here h is the index of
the head-word of the dependency, corresponding
to the spine ?
h
which contains a node that is being
adjoined into. m is the index of the modifier-word
of the dependency, corresponding to the spine ?
m
which is being adjoined into ?
h
. l is a label.
The label l is a tuple ?POS, A, ?
h
, ?
m
, L?. ?
h
and
?
m
are the head and modifier spines that are be-
ing combined. POS specifies which node in ?
h
is
being adjoined into. A is a binary flag specifying
whether the combination operation being used is s-
adjunction or r-adjunction. L is a binary flag spec-
ifying whether or not any ?previous? modifier has
been r-adjoined into the position POS in ?
h
. By a
previous modifier, we mean a modifier m? that was
adjoined from the same direction as m (i.e., such
that h < m? < m or m < m? < h).
It would be sufficient to define l to be the pair
?POS, A??the inclusion of ?
h
, ?
m
and L adds re-
dundant information that can be recovered from
the set E, and other dependencies in D?but it
will be convenient to include this information in
the label. In particular, it is important that given
this definition of l, it is possible to define a func-
tion GRM(l) that maps a label l to a triple of non-
terminals that represents the grammatical relation
between m and h in the dependency structure. For
example, in the tree shown in figure 1(a), the gram-
matical relation between cake and ate is the triple
GRM(l) = ?VP VBD NP?. In the tree shown in
figure 1(b), the grammatical relation between cake
and ate is the triple GRM(l) = ?VP VP NP?.
The conditions under which a pair ?E,D? forms
a valid derivation for a sentence x are similar to
those in conventional LTAGs. Each ?i, ?? ? E
must be such that ? is an elementary tree whose
anchor is the word x
i
. The dependencies D must
form a directed, projective tree spanning words
0 . . . n, with ? at the root of this tree, as is also
the case in previous work on discriminative ap-
proches to dependency parsing (McDonald et al,
2005). We allow any modifier tree ?
m
to adjoin
into any position in any head tree ?
h
, but the de-
pendencies D must nevertheless be coherent?for
example they must be consistent with the spines in
E, and they must be nested correctly.4 We will al-
4For example, closer modifiers to a particular head must
adjoin in at the same or a lower spine position than modifiers
11
low multiple modifier spines to s-adjoin or r-adjoin
into the same node in a head spine; see section 2.3
for more details.
2.2 A Global Linear Model
The model used for parsing with this approach is
a global linear model. For a given sentence x, we
define Y(x) to be the set of valid derivations for x,
where each y ? Y(x) is a pair ?E,D? as described
in the previous section. A function f maps (x, y)
pairs to feature-vectors f(x, y) ? Rd. The param-
eter vector w is also a vector in Rd. Given these
definitions, the optimal derivation for an input sen-
tence x is y? = argmax
y?Y(x)
w ? f(x, y).
We now come to how the feature-vector f(x, y)
is defined in our approach. A simple ?first-order?
model would define
f(x, y) =
?
?i,???E(y)
e(x, ?i, ??) +
?
?h,m,l??D(y)
d(x, ?h,m, l?) (2)
Here we use E(y) and D(y) to respectively refer
to the set of spines and dependencies in y. The
function e maps a sentence x paired with a spine
?i, ?? to a feature vector. The function d maps de-
pendencies within y to feature vectors. This de-
composition is similar to the first-order model of
McDonald et al (2005), but with the addition of
the e features.
We will extend our model to include higher-
order features, in particular features based on sib-
ling dependencies (McDonald and Pereira, 2006),
and grandparent dependencies, as in (Carreras,
2007). If y = ?E,D? is a derivation, then:
? S(y) is a set of sibling dependencies. Each
sibling dependency is a tuple ?h,m, l, s?. For each
?h,m, l, s? ? S the tuple ?h,m, l? is an element of
D; there is one member of S for each member of
D. The index s is the index of the word that was
adjoined to the spine for h immediately before m
(or the NULL symbol if no previous adjunction has
taken place).
? G(y) is a set of grandparent dependencies of
type 1. Each type 1 grandparent dependency is a
tuple ?h,m, l, g?. There is one member of G for
every member of D. The additional information,
the index g, is the index of the word that is the first
modifier to the right of the spine for m.
that are further from the head.
(a) S
VP
VP
VP
VBD
ate
NP
NN
cake
NP
NN
today
ADVP
RB
quickly
(b) S
NP
NNP
John
VP
ADVP
RB
luckily
VP
VBD
ate
Figure 2: Two Example Trees
S
NP
NNP
John
VP
VP
ADVP
RB
luckily
VP
VBD
ate
NP
NN
cake
NP
NN
today
ADVP
RB
quickly
Figure 3: An example tree, formed by a combina-
tion of the two structures in figure 2.
? Q(y) is an additional set of grandparent de-
pendencies, of type 2. Each of these dependencies
is a tuple ?h,m, l, q?. Again, there is one member
of Q for every member of D. The additional infor-
mation, the index q, is the index of the word that is
the first modifier to the left of the spine for m.
The feature-vector definition then becomes:
f(x, y) =
X
?i,???E(y)
e(x, ?i, ??) +
X
?h,m,l??D(y)
d(x, ?h,m, l?) +
X
?h,m,l,s??S(y)
s(x, ?h,m, l, s?) +
X
?h,m,l,g??G(y)
g(x, ?h,m, l, g?) +
X
?h,m,l,q??Q(y)
q(x, ?h,m, l, q?)
(3)
where s, g and q are feature vectors corresponding
to the new, higher-order elements.5
2.3 Recovering Parse Trees from Derivations
As in TAG approaches, there is a mapping from
derivations ?E,D? to parse trees (i.e., the type of
trees generated by a context-free grammar). In our
case, we map a spine and its dependencies to a con-
stituent structure by first handling the dependen-
5We also added constituent-boundary features to the
model, which is a simple change that led to small improve-
ments on validation data; for brevity we omit the details.
12
cies on each side separately and then combining
the left and right sides.
First, it is straightforward to build the con-
stituent structure resulting from multiple adjunc-
tions on the same side of a spine. As one exam-
ple, the structure in figure 2(a) is formed by first
s-adjoining the spine with anchor cake into the VP
node of the spine for ate, then r-adjoining spines
anchored by today and quickly into the same node,
where all three modifier words are to the right of
the head word. Notice that each r-adjunction op-
eration creates a new VP level in the tree, whereas
s-adjunctions do not create a new level. Now con-
sider a tree formed by first r-adjoining a spine for
luckily into the VP node for ate, followed by s-
adjoining the spine for John into the S node, in
both cases where the modifiers are to the left of
the head. In this case the structure that would be
formed is shown in figure 2(b).
Next, consider combining the left and right
structures of a spine. The main issue is how to
handle multiple r-adjunctions or s-adjunctions on
both sides of a node in a spine, because our deriva-
tions do not specify how adjunctions from different
sides embed with each other. In our approach, the
combination operation preserves the height of the
different modifiers from the left and right direc-
tions. To illustrate this, figure 3 shows the result
of combining the two structures in figure 2. The
combination of the left and right modifier struc-
tures has led to flat structures, for example the rule
VP? ADVP VP NP in the above tree.
Note that our r-adjunction operation is different
from the usual adjunction operation in TAGs, in
that ?wrapping? adjunctions are not possible, and
r-adjunctions from the left and right directions are
independent from each other; because of this our
grammars are splittable.
3 Parsing Algorithms
3.1 Use of Eisner?s Algorithms
This section describes the algorithm for finding
y
?
= argmax
y?Y(x)
w ? f(x, y) where f(x, y) is
defined through either the first-order model (Eq. 2)
or the second-order model (Eq. 3).
For the first-order model, the methods described
in (Eisner, 2000) can be used for the parsing algo-
rithm. In Eisner?s algorithms for dependency pars-
ing each word in the input has left and right finite-
state (weighted) automata, which generate the left
and right modifiers of the word in question. We
make use of this idea of automata, and also make
direct use of the method described in section 4.2 of
(Eisner, 2000) that allows a set of possible senses
for each word in the input string. In our use of
the algorithm, each possible sense for a word cor-
responds to a different possible spine that can be
associated with that word. The left and right au-
tomata are used to keep track of the last position
in the spine that was adjoined into on the left/right
of the head respectively. We can make use of sep-
arate left and right automata?i.e., the grammar is
splittable?because left and right modifiers are ad-
joined independently of each other in the tree. The
extension of Eisner?s algorithm to the second-order
model is similar to the algorithm described in (Car-
reras, 2007), but again with explicit use of word
senses and left/right automata. The resulting algo-
rithms run in O(Gn3) and O(Hn4) time for the
first-order and second-order models respectively,
where G and H are grammar constants.
3.2 Efficient Parsing
The efficiency of the parsing algorithm is impor-
tant in applying the parsing model to test sen-
tences, and also when training the model using dis-
criminative methods. The grammar constants G
and H introduced in the previous section are poly-
nomial in factors such as the number of possible
spines in the model, and the number of possible
states in the finite-state automata implicit in the
parsing algorithm. These constants are large, mak-
ing exhaustive parsing very expensive.
To deal with this problem, we use a simple ini-
tial model to prune the search space of the more
complex model. The first-stage model we use
is a first-order dependency model, with labeled
dependencies, as described in (McDonald et al,
2005). As described shortly, we will use this model
to compute marginal scores for dependencies in
both training and test sentences. A marginal score
?(x, h,m, l) is a value between 0 and 1 that re-
flects the plausibility of a dependency for sentence
x with head-word x
h
, modifier word x
m
, and la-
bel l. In the first-stage pruning model the labels l
are triples of non-terminals representing grammat-
ical relations, as described in section 2.1 of this
paper?for example, one possible label would be
?VP VBD NP?, and in general any triple of non-
terminals is possible.
Given a sentence x, and an index m of a word
in that sentence, we define DMAX(x,m) to be the
13
highest scoring dependency with m as a modifier:
DMAX(x,m) = max
h,l
?(x, h,m, l)
For a sentence x, we then define the set of allow-
able dependencies to be
pi(x) = {?h,m, l? : ?(x, h,m, l) ? ?DMAX(x,m)}
where ? is a constant dictating the beam size that
is used (in our experiments we used ? = 10?6).
The set pi(x) is used to restrict the set of pos-
sible parses under the full TAG-based model. In
section 2.1 we described how the TAG model has
dependency labels of the form ?POS, A, ?
h
, ?
m
, L?,
and that there is a function GRM that maps labels
of this form to triples of non-terminals. The ba-
sic idea of the pruned search is to only allow de-
pendencies of the form ?h,m, ?POS, A, ?
h
, ?
m
, L??
if the tuple ?h,m, GRM(?POS, A, ?
h
, ?
m
, L?)? is a
member of pi(x), thus reducing the search space
for the parser.
We now turn to how the marginals ?(x, h,m, l)
are defined and computed. A simple approach
would be to use a conditional log-linear model
(Lafferty et al, 2001), with features as defined by
McDonald et al (2005), to define a distribution
P (y|x) where the parse structures y are depen-
dency structures with labels that are triples of non-
terminals. In this case we could define
?(x, h,m, l) =
?
y:(h,m,l)?y
P (y|x)
which can be computed with inside-outside style
algorithms, applied to the data structures from
(Eisner, 2000). The complexity of training and ap-
plying such a model is again O(Gn3), where G is
the number of possible labels, and the number of
possible labels (triples of non-terminals) is around
G = 1000 in the case of treebank parsing; this
value for G is still too large for the method to be ef-
ficient. Instead, we train three separate models ?
1
,
?
2
, and ?
3
for the three different positions in the
non-terminal triples. We then take ?(x, h,m, l) to
be a product of these three models, for example we
would calculate
?(x, h,m, ?VP VBD NP?) =
?
1
(x, h,m, ?VP?)? ?
2
(x, h,m, ?VBD?)
??
3
(x, h,m, ?NP?)
Training the three models, and calculating the
marginals, now has a grammar constant equal
to the number of non-terminals in the grammar,
which is far more manageable. We use the algo-
rithm described in (Globerson et al, 2007) to train
the conditional log-linear model; this method was
found to converge to a good model after 10 itera-
tions over the training data.
4 Implementation Details
4.1 Features
Section 2.2 described the use of feature vectors
associated with spines used in a derivation, to-
gether with first-order, sibling, and grandparent
dependencies. The dependency features used in
our experiments are closely related to the features
described in (Carreras, 2007), which are an ex-
tension of the McDonald and Pereira (2006) fea-
tures to cover grandparent dependencies in addi-
tion to first-order and sibling dependencies. The
features take into account the identity of the la-
bels l used in the derivations. The features could
potentially look at any information in the la-
bels, which are of the form ?POS, A, ?
h
, ?
m
, L?,
but in our experiments, we map labels to a pair
(GRM(?POS, A, ?
h
, ?
m
, L?), A). Thus the label fea-
tures are sensitive only to the triple of non-
terminals corresponding to the grammatical rela-
tion involved in an adjunction, and a binary flag
specifiying whether the operation is s-adjunction
or r-adjunction.
For the spine features e(x, ?i, ??), we use fea-
ture templates that are sensitive to the identity of
the spine ?, together with contextual features of
the string x. These features consider the iden-
tity of the words and part-of-speech tags in a win-
dow that is centered on x
i
and spans the range
x
(i?2)
. . . x
(i+2)
.
4.2 Extracting Derivations from Parse Trees
In the experiments in this paper, the following
three-step process was used: (1) derivations were
extracted from a training set drawn from the Penn
WSJ treebank, and then used to train a parsing
model; (2) the test data was parsed using the re-
sulting model, giving a derivation for each test
data sentence; (3) the resulting test-data deriva-
tions were mapped back to Penn-treebank style
trees, using the method described in section 2.1.
To achieve step (1), we first apply a set of head-
finding rules which are similar to those described
in (Collins, 1997). Once the head-finding rules
have been applied, it is straightforward to extract
14
precision recall F
1
PPK07 ? ? 88.3
FKM08 88.2 87.8 88.0
CH2000 89.5 89.6 89.6
CO2000 89.9 89.6 89.8
PK07 90.2 89.9 90.1
this paper 91.4 90.7 91.1
CJ05 ? ? 91.4
H08 ? ? 91.7
CO2000(s24) 89.6 88.6 89.1
this paper (s24) 91.1 89.9 90.5
Table 1: Results for different methods. PPK07, FKM08,
CH2000, CO2000, PK07, CJ05 and H08 are results on section
23 of the Penn WSJ treebank, for the models of Petrov et al
(2007), Finkel et al (2008), Charniak (2000), Collins (2000),
Petrov and Klein (2007), Charniak and Johnson (2005), and
Huang (2008). (CJ05 is the performance of an updated
model at http://www.cog.brown.edu/mj/software.htm.) ?s24?
denotes results on section 24 of the treebank.
s23 s24
KCC08 unlabeled 92.0 91.0
KCC08 labeled 92.5 91.7
this paper 93.5 92.5
Table 2: Table showing unlabeled dependency accuracy for
sections 23 and 24 of the treebank, using the method of (Ya-
mada and Matsumoto, 2003) to extract dependencies from
parse trees from our model. KCC08 unlabeled is from (Koo
et al, 2008), a model that has previously been shown to have
higher accuracy than (McDonald and Pereira, 2006). KCC08
labeled is the labeled dependency parser from (Koo et al,
2008); here we only evaluate the unlabeled accuracy.
derivations from the Penn treebank trees.
Note that the mapping from parse trees to
derivations is many-to-one: for example, the ex-
ample trees in section 2.3 have structures that are
as ?flat? (have as few levels) as is possible, given
the set D that is involved. Other similar trees,
but with more VP levels, will give the same set
D. However, this issue appears to be benign in the
Penn WSJ treebank. For example, on section 22 of
the treebank, if derivations are first extracted using
the method described in this section, then mapped
back to parse trees using the method described in
section 2.3, the resulting parse trees score 100%
precision and 99.81% recall in labeled constituent
accuracy, indicating that very little information is
lost in this process.
4.3 Part-of-Speech Tags, and Spines
Sentences in training, test, and development data
are assumed to have part-of-speech (POS) tags.
POS tags are used for two purposes: (1) in the
features described above; and (2) to limit the set
of allowable spines for each word during parsing.
Specifically, for each POS tag we create a separate
1st stage 2nd stage
? active coverage oracle F
1
speed F
1
10
?4 0.07 97.7 97.0 5:15 91.1
10
?5 0.16 98.5 97.9 11:45 91.6
10
?6 0.34 99.0 98.5 21:50 92.0
Table 3: Effect of the beam size, controlled by ?, on the
performance of the parser on the development set (1,699 sen-
tences). In each case ? refers to the beam size used in both
training and testing the model. ?active?: percentage of de-
pendencies that remain in the beam out of the total number of
labeled dependencies (1,000 triple labels times 1,138,167 un-
labeled dependencies); ?coverage?: percentage of correct de-
pendencies in the beam out of the total number of correct de-
pendencies. ?oracle F
1
?: maximum achievable score of con-
stituents, given the beam. ?speed?: parsing time in min:sec
for the TAG-based model (this figure does not include the time
taken to calculate the marginals using the lower-order model);
?F
1
?: score of predicted constituents.
dictionary listing the spines that have been seen
with this POS tag in training data; during parsing
we only allow spines that are compatible with this
dictionary. (For test or development data, we used
the part-of-speech tags generated by the parser of
(Collins, 1997). Future work should consider in-
corporating the tagging step within the model; it is
not challenging to extend the model in this way.)
5 Experiments
Sections 2-21 of the Penn Wall Street Journal tree-
bank were used as training data in our experiments,
and section 22 was used as a development set. Sec-
tions 23 and 24 were used as test sets. The model
was trained for 20 epochs with the averaged per-
ceptron algorithm, with the development data per-
formance being used to choose the best epoch. Ta-
ble 1 shows the results for the method.
Our experiments show an improvement in per-
formance over the results in (Collins, 2000; Char-
niak, 2000). We would argue that the Collins
(2000) method is considerably more complex than
ours, requiring a first-stage generative model, to-
gether with a reranking approach. The Char-
niak (2000) model is also arguably more com-
plex, again using a carefully constructed genera-
tive model. The accuracy of our approach also
shows some improvement over results in (Petrov
and Klein, 2007). This work makes use of a
PCFG with latent variables that is trained using
a split/merge procedure together with the EM al-
gorithm. This work is in many ways comple-
mentary to ours?for example, it does not make
use of GLMs, dependency features, or of repre-
sentations that go beyond PCFG productions?and
15
some combination of the two methods may give
further gains.
Charniak and Johnson (2005), and Huang
(2008), describe approaches that make use of non-
local features in conjunction with the Charniak
(2000) model; future work may consider extend-
ing our approach to include non-local features.
Finally, other recent work (Petrov et al, 2007;
Finkel et al, 2008) has had a similar goal of scal-
ing GLMs to full syntactic parsing. These mod-
els make use of PCFG representations, but do not
explicitly model bigram or trigram dependencies.
The results in this work (88.3%/88.0% F
1
) are
lower than our F
1
score of 91.1%; this is evidence
of the benefits of the richer representations enabled
by our approach.
Table 2 shows the accuracy of the model in
recovering unlabeled dependencies. The method
shows improvements over the method described
in (Koo et al, 2008), which is a state-of-the-art
second-order dependency parser similar to that of
(McDonald and Pereira, 2006), suggesting that the
incorporation of constituent structure can improve
dependency accuracy.
Table 3 shows the effect of the beam-size on the
accuracy and speed of the parser on the develop-
ment set. With the beam setting used in our exper-
iments (? = 10?6), only 0.34% of possible depen-
dencies are considered by the TAG-based model,
but 99% of all correct dependencies are included.
At this beam size the best possible F
1
constituent
score is 98.5. Tighter beams lead to faster parsing
times, with slight drops in accuracy.
6 Conclusions
We have described an efficient and accurate parser
for constituent parsing. A key to the approach has
been to use a splittable grammar that allows effi-
cient dynamic programming algorithms, in com-
bination with pruning using a lower-order model.
The method allows relatively easy incorporation of
features; future work should leverage this in pro-
ducing more accurate parsers, and in applying the
parser to different languages or domains.
Acknowledgments X. Carreras was supported by the
Catalan Ministry of Innovation, Universities and Enterprise,
by the GALE program of DARPA, Contract No. HR0011-06-
C-0022, and by a grant from NTT, Agmt. Dtd. 6/21/1998.
T. Koo was funded by NSF grant IIS-0415030. M. Collins
was funded by NSF grant IIS-0347631 and DARPA contract
No. HR0011-06-C-0022. Thanks to Jenny Rose Finkel for
suggesting that we evaluate dependency parsing accuracies.
References
Altun, Y., I. Tsochantaridis, and T. Hofmann. 2003. Hidden
markov support vector machines. In ICML.
Carreras, X. 2007. Experiments with a higher-order projec-
tive dependency parser. In Proc. EMNLP-CoNLL Shared
Task.
Charniak, E. and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
ACL.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proc. NAACL.
Chiang, D. 2003. Statistical parsing with an automatically
extracted tree adjoining grammar. In Bod, R., R. Scha, and
K. Sima?an, editors, Data Oriented Parsing, pages 299?
316. CSLI Publications.
Clark, S. and J. R. Curran. 2004. Parsing the wsj using ccg
and log-linear models. In Proc. ACL.
Clark, Stephen and James R. Curran. 2007. Perceptron train-
ing for a wide-coverage lexicalized-grammar parser. In
Proc. ACL Workshop on Deep Linguistic Processing.
Collins, M. 1997. Three generative, lexicalised models for
statistical parsing. In Proc. ACL.
Collins, M. 2000. Discriminative reranking for natural lan-
guage parsing. In Proc. ICML.
Collins, M. 2002. Discriminative training methods for hid-
den markov models: Theory and experiments with percep-
tron algorithms. In Proc. EMNLP.
Eisner, J. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Bunt, H. C. and A. Nijholt, editors,
New Developments in Natural Language Parsing, pages
29?62. Kluwer Academic Publishers.
Finkel, J. R., A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing. In
Proc. ACL/HLT.
Globerson, A., T. Koo, X. Carreras, and M. Collins. 2007.
Exponentiated gradient algorithms for log-linear struc-
tured prediction. In Proc. ICML.
Huang, L. 2008. Forest reranking: Discriminative parsing
with non-local features. In Proc. ACL/HLT.
Johnson, M., S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999. Estimators for stochastic unification-based gram-
mars. In Proc. ACL.
Koo, Terry, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
ACL/HLT.
Lafferty, J., A. McCallum, and F. Pereira. 2001. Conditonal
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proc. ICML.
McDonald, R. and F. Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In Proc. EACL.
McDonald, R., K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In Proc.
ACL.
Petrov, S. and D. Klein. 2007. Improved inference for unlex-
icalized parsing. In Proc. of HLT-NAACL.
Petrov, S., A. Pauls, and D. Klein. 2007. Discriminative log-
linear grammars with latent variables. In Proc. NIPS.
Ratnaparkhi, A., S. Roukos, and R. Ward. 1994. A maximum
entropy model for parsing. In Proc. ICSLP.
Sagae, Kenji, Yusuke Miyao, and Jun?ichi Tsujii. 2007. Hpsg
parsing with shallow dependency constraints. In Proc.
ACL, pages 624?631.
Shen, L. and A.K. Joshi. 2005. Incremental ltag parsing. In
Proc HLT-EMNLP.
Taskar, B., D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proceedings of the
EMNLP-2004.
Yamada, H. and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. IWPT.
16
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 56?64,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Spectral Learning of Refinement HMMs
Karl Stratos1 Alexander M. Rush2 Shay B. Cohen1 Michael Collins1
1Department of Computer Science, Columbia University, New-York, NY 10027, USA
2MIT CSAIL, Cambridge, MA, 02139, USA
{stratos,scohen,mcollins}@cs.columbia.edu, srush@csail.mit.edu
Abstract
We derive a spectral algorithm for learn-
ing the parameters of a refinement HMM.
This method is simple, efficient, and can
be applied to a wide range of supervised
sequence labeling tasks. Like other spec-
tral methods, it avoids the problem of lo-
cal optima and provides a consistent esti-
mate of the parameters. Our experiments
on a phoneme recognition task show that
when equipped with informative feature
functions, it performs significantly better
than a supervised HMM and competitively
with EM.
1 Introduction
Consider the task of supervised sequence label-
ing. We are given a training set where the j?th
training example consists of a sequence of ob-
servations x(j)1 ...x(j)N paired with a sequence of
labels a(j)1 ...a(j)N and asked to predict the cor-
rect labels on a test set of observations. A
common approach is to learn a joint distribu-
tion over sequences p(a1 . . . aN , x1 . . . xN ) as a
hidden Markov model (HMM). The downside of
HMMs is that they assume each label ai is inde-
pendent of labels before the previous label ai?1.
This independence assumption can be limiting,
particularly when the label space is small. To re-
lax this assumption we can refine each label ai
with a hidden state hi, which is not observed in
the training data, and model the joint distribu-
tion p(a1 . . . aN , x1 . . . xN , h1 . . . hN ). This re-
finement HMM (R-HMM), illustrated in figure 1,
is able to propagate information forward through
the hidden state as well as the label.
Unfortunately, estimating the parameters of an
R-HMM is complicated by the unobserved hid-
den variables. A standard approach is to use the
expectation-maximization (EM) algorithm which
a1, h1 a2, h2 aN , hN
x1 x2 xN
(a)
a1 a2 aN
h1 h2 hN
x1 x2 xN
(b)
Figure 1: (a) An R-HMM chain. (b) An equivalent
representation where labels and hidden states are
intertwined.
has no guarantee of finding the global optimum of
its objective function. The problem of local op-
tima prevents EM from yielding statistically con-
sistent parameter estimates: even with very large
amounts of data, EM is not guaranteed to estimate
parameters which are close to the ?correct? model
parameters.
In this paper, we derive a spectral algorithm for
learning the parameters of R-HMMs. Unlike EM,
this technique is guaranteed to find the true param-
eters of the underlying model under mild condi-
tions on the singular values of the model. The al-
gorithm we derive is simple and efficient, relying
on singular value decomposition followed by stan-
dard matrix operations.
We also describe the connection of R-HMMs
to L-PCFGs. Cohen et al (2012) present a spec-
tral algorithm for L-PCFG estimation, but the
na??ve transformation of the L-PCFG model and
its spectral algorithm to R-HMMs is awkward and
opaque. We therefore work through the non-trivial
derivation the spectral algorithm for R-HMMs.
We note that much of the prior work on spec-
tral algorithms for discrete structures in NLP has
shown limited experimental success for this fam-
ily of algorithms (see, for example, Luque et al,
2012). Our experiments demonstrate empirical
56
success for the R-HMM spectral algorithm. The
spectral algorithm performs competitively with
EM on a phoneme recognition task, and is more
stable with respect to the number of hidden states.
Cohen et al (2013) present experiments with a
parsing algorithm and also demonstrate it is com-
petitive with EM. Our set of experiments comes as
an additional piece of evidence that spectral algo-
rithms can function as a viable, efficient and more
principled alternative to the EM algorithm.
2 Related Work
Recently, there has been a surge of interest in spec-
tral methods for learning HMMs (Hsu et al, 2012;
Foster et al, 2012; Jaeger, 2000; Siddiqi et al,
2010; Song et al, 2010). Like these previous
works, our method produces consistent parameter
estimates; however, we estimate parameters for a
supervised learning task. Balle et al (2011) also
consider a supervised problem, but our model is
quite different since we estimate a joint distribu-
tion p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) as opposed
to a conditional distribution and use feature func-
tions over both the labels and observations of the
training data. These feature functions also go be-
yond those previously employed in other spectral
work (Siddiqi et al, 2010; Song et al, 2010). Ex-
periments show that features of this type are cru-
cial for performance.
Spectral learning has been applied to related
models beyond HMMs including: head automata
for dependency parsing (Luque et al, 2012),
tree-structured directed Bayes nets (Parikh et al,
2011), finite-state transducers (Balle et al, 2011),
and mixture models (Anandkumar et al, 2012a;
Anandkumar et al, 2012b).
Of special interest is Cohen et al (2012), who
describe a derivation for a spectral algorithm for
L-PCFGs. This derivation is the main driving
force behind the derivation of our R-HMM spec-
tral algorithm. For work on L-PCFGs estimated
with EM, see Petrov et al (2006), Matsuzaki et al
(2005), and Pereira and Schabes (1992). Petrov
et al (2007) proposes a split-merge EM procedure
for phoneme recognition analogous to that used in
latent-variable parsing.
3 The R-HMM Model
We decribe in this section the notation used
throughout the paper and the formal details of R-
HMMs.
3.1 Notation
We distinguish row vectors from column vectors
when such distinction is necessary. We use a
superscript > to denote the transpose operation.
We write [n] to denote the set {1, 2, . . . , n} for
any integer n ? 1. For any vector v ? Rm,
diag(v) ? Rm?m is a diagonal matrix with en-
tries v1 . . . vm. For any statement S , we use [[S]]
to refer to the indicator function that returns 1 if S
is true and 0 otherwise. For a random variable X ,
we use E[X] to denote its expected value.
A tensor C ? Rm?m?m is a set of m3 val-
ues Ci,j,k for i, j, k ? [m]. Given a vector v ?
Rm, we define C(v) to be the m ? m matrix
with [C(v)]i,j = ?k?[m]Ci,j,kvk. Given vectors
x, y, z ? Rm, C = xy>z> is anm?m?m tensor
with [C]i,j,k = xiyjzk.
3.2 Definition of an R-HMM
An R-HMM is a 7-tuple ?l,m, n, pi, o, t, f? for in-
tegers l,m, n ? 1 and functions pi, o, t, f where
? [l] is a set of labels.
? [m] is a set of hidden states.
? [n] is a set of observations.
? pi(a, h) is the probability of generating a ?
[l] and h ? [m] in the first position in the
labeled sequence.
? o(x|a, h) is the probability of generating x ?
[n], given a ? [l] and h ? [m].
? t(b, h?|a, h) is the probability of generating
b ? [l] and h? ? [m], given a ? [l] and
h ? [m].
? f(?|a, h) is the probability of generating the
stop symbol ?, given a ? [l] and h ? [m].
See figure 1(b) for an illustration. At any time step
of a sequence, a label a is associated with a hidden
state h. By convention, the end of an R-HMM
sequence is signaled by the symbol ?.
For the subsequent illustration, let N be the
length of the sequence we consider. A full se-
quence consists of labels a1 . . . aN , observations
x1 . . . xN , and hidden states h1 . . . hN . The model
assumes
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ) = pi(a1, h1)?
N?
i=1
o(xi|ai, hi)?
N?1?
i=1
t(ai+1, hi+1|ai, hi)? f(?|aN , hN )
57
Input: a sequence of observations x1 . . . xN ; operators?
Cb|a, C?|a, c1a, cax
?
Output: ?(a, i) for all a ? [l] and i ? [N ]
[Forward case]
? ?1a ? c1a for all a ? [l].
? For i = 1 . . . N ? 1
?i+1b ?
?
a?[l]
Cb|a(caxi)? ?
i
a for all b ? [l]
[Backward case]
? ?N+1a ? C?|a(caxN ) for all a ? [l]
? For i = N . . . 1
?ia ?
?
b?[l]
?i+1b ? Cb|a(caxi) for all a ? [l]
[Marginals]
? ?(a, i)? ?ia ? ?ia for all a ? [l], i ? [N ]
Figure 2: The forward-backward algorithm
A skeletal sequence consists of labels a1 . . . aN
and observations x1 . . . xN without hidden states.
Under the model, it has probability
p(a1 . . . aN , x1 . . . xN )
=
?
h1...hN
p(a1 . . . aN , x1 . . . xN , h1 . . . hN )
An equivalent definition of an R-HMM is
given by organizing the parameters in matrix
form. Specifically, an R-HMM has parameters?
pia, oax, T b|a, fa
? where pia ? Rm is a column
vector, oax is a row vector, T b|a ? Rm?m is a ma-
trix, and fa ? Rm is a row vector, defined for all
a, b ? [l] and x ? [n]. Their entries are set to
? [pia]h = pi(a, h) for h ? [m]
? [oax]h = o(x|a, h) for h ? [m]
? [T b|a]h?,h = t(b, h?|a, h) for h, h? ? [m]
? [fa]h = f(?|a, h) for h ? [m]
4 The Forward-Backward Algorithm
Given an observation sequence x1 . . . xN , we want
to infer the associated sequence of labels under
an R-HMM. This can be done by computing the
marginals of x1 . . . xN
?(a, i) =
?
a1...aN : ai=a
p(a1 . . . aN , x1 . . . xN )
for all labels a ? [l] and positions i ? [N ]. Then
the most likely label at each position i is given by
a?i = arg maxa?[l] ?(a, i)
The marginals can be computed using a tensor
variant of the forward-backward algorithm, shown
in figure 2. The algorithm takes additional quanti-
ties ?Cb|a, C?|a, c1a, cax
? called the operators:
? Tensors Cb|a ? Rm?m?m for a, b ? [l]
? Tensors C?|a ? R1?m?m for a ? [l]
? Column vectors c1a ? Rm for a ? [l]
? Row vectors cax ? Rm for a ? [l] and x ? [n]
The following proposition states that these opera-
tors can be defined in terms of the R-HMM param-
eters to guarantee the correctness of the algorithm.
Proposition 4.1. Given an R-HMM with param-
eters ?pia, oax, T b|a, fa
?, for any vector v ? Rm
define the operators:
Cb|a(v) = T b|adiag(v) c1a = pia
C?|a(v) = fadiag(v) cax = oax
Then the algorithm in figure 2 correctly computes
marginals ?(a, i) under the R-HMM.
The proof is an algebraic verification and deferred
to the appendix. Note that the running time of the
algorithm as written is O(l2m3N).1
Proposition 4.1 can be generalized to the fol-
lowing theorem. This theorem implies that the op-
erators can be linearly transformed by some invert-
ible matrices as long as the transformation leaves
the embedded R-HMM parameters intact. This
observation is central to the derivation of the spec-
tral algorithm which estimates the linearly trans-
formed operators but not the actual R-HMM pa-
rameters.
Theorem 4.1. Given an R-HMM with parameters?
pia, oax, T b|a, fa
?, assume that for each a ? [l] we
have invertible m ?m matrices Ga and Ha. For
any vector v ? Rm define the operators:
Cb|a(v) = GbT b|adiag(vHa)(Ga)?1 c1a = Gapia
C?|a(v) = fadiag(vHa)(Ga)?1 cax = oax(Ha)?1
Then the algorithm in figure 2 correctly computes
marginals ?(a, i) under the R-HMM.
The proof is similar to that of Cohen et al (2012).
1We can reduce the complexity to O(l2m2N) by pre-
computing the matricesCb|a(cax) for all a, b ? [l] and x ? [n]
after parameter estimation.
58
5 Spectral Estimation of R-HMMs
In this section, we derive a consistent estimator for
the operators ?Cb|a, C?|a, c1a, cax
? in theorem 4.1
through the use of singular-value decomposition
(SVD) followed by the method of moments.
Section 5.1 describes the decomposition of the
R-HMM model into random variables which are
used in the final algorithm. Section 5.2 can be
skimmed through on the first reading, especially
if the reader is familiar with other spectral algo-
rithms. It includes a detailed account of the deriva-
tion of the R-HMM algorithm.
For a first reading, note that an R-HMM se-
quence can be seen as a right-branching L-PCFG
tree. Thus, in principle, one can convert a se-
quence into a tree and run the inside-outside algo-
rithm of Cohen et al (2012) to learn the parame-
ters of an R-HMM. However, projecting this trans-
formation into the spectral algorithm for L-PCFGs
is cumbersome and unintuitive. This is analo-
gous to the case of the Baum-Welch algorithm for
HMMs (Rabiner, 1989), which is a special case of
the inside-outside algorithm for PCFGs (Lari and
Young, 1990).
5.1 Random Variables
We first introduce the random variables un-
derlying the approach then describe the opera-
tors based on these random variables. From
p(a1 . . . aN , x1 . . . xN , h1 . . . hN ), we draw an R-
HMM sequence (a1 . . . aN , x1 . . . xN , h1 . . . hN )
and choose a time step i uniformly at random from
[N ]. The random variables are then defined as
X = xi
A1 = ai and A2 = ai+1 (if i = N , A2 = ?)
H1 = hi and H2 = hi+1
F1 = (ai . . . aN , xi . . . xN ) (future)
F2 = (ai+1 . . . aN , xi+1 . . . xN ) (skip-future)
P = (a1 . . . ai, x1 . . . xi?1) (past)
R = (ai, xi) (present)
D = (a1 . . . aN , x1 . . . xi?1, xi+1 . . . xN ) (destiny)
B = [[i = 1]]
Figure 3 shows the relationship between the ran-
dom variables. They are defined in such a way
that the future is independent of the past and the
present is independent of the destiny conditioning
on the current node?s label and hidden state.
Next, we require a set of feature functions over
the random variables.
? ? maps F1, F2 to ?(F1), ?(F2) ? Rd1 .
a1 ai?1 ai ai+1 aN
x1 xi?1 xi xi+1 xN
P
F1
F2
(a)
a1 ai?1 ai ai+1 aN
x1 xi?1 xi xi+1 xN
D R
(b)
Figure 3: Given an R-HMM sequence, we define
random variables over observed quantities so that
conditioning on the current node, (a) the future F1
is independent of the past P and (b) the present R
is independent of the density D.
? ? maps P to ?(P ) ? Rd2 .
? ? maps R to ?(R) ? Rd3 .
? ? maps D to ?(D) ? Rd4 .
We will see that the feature functions should be
chosen to capture the influence of the hidden
states. For instance, they might track the next la-
bel, the previous observation, or important combi-
nations of labels and observations.
Finally, we require projection matrices
?a ? Rm?d1 ?a ? Rm?d2
?a ? Rm?d3 ?a ? Rm?d4
defined for all labels a ? [l]. These matrices
will project the feature vectors of ?, ?, ?, and ?
from (d1, d2, d3, d4)-dimensional spaces to an m-
dimensional space. We refer to this reduced di-
mensional representation by the following random
variables:
F 1 = ?A1?(F1) (projected future)
F 2 = ?A2?(F2) (projected skip-future: if i = N , F 2 = 1)
P = ?A1?(P ) (projected past)
R = ?A1?(R) (projected present)
D = ?A1?(D) (projected destiny)
Note that they are all vectors in Rm.
59
5.2 Estimation of the Operators
Since F 1, F 2, P , R, and D do not involve hid-
den variables, the following quantities can be di-
rectly estimated from the training data of skeletal
sequences. For this reason, they are called observ-
able blocks:
?a = E[F 1P>|A1 = a] ?a ? [l]
?a = E[R D>|A1 = a] ?a ? [l]
Db|a = E[[[A2 = b]]F 2P>R>|A1 = a] ?a, b ? [l]
dax = E[[[X = x]]D>|A1 = a] ?a ? [l], x ? [n]
The main result of this paper is that under cer-
tain conditions, matrices ?a and ?a are invert-
ible and the operators ?Cb|a, C?|a, c1a, cax
? in the-
orem 4.1 can be expressed in terms of these ob-
servable blocks.
Cb|a(v) = Db|a(v)(?a)?1 (1)
C?|a(v) = D?|a(v)(?a)?1 (2)
cax = dax(?a)?1 (3)
c1a = E[[[A1 = a]]F 1|B = 1] (4)
To derive this result, we use the following defini-
tion to help specify the conditions on the expecta-
tions of the feature functions.
Definition. For each a ? [l], define matrices
Ia ? Rd1?m, Ja ? Rd2?m, Ka ? Rd3?m,W a ?
Rd4?m by
[Ia]k,h = E[[?(F1)]k|A1 = a,H1 = h]
[Ja]k,h = E[[?(P )]k|A1 = a,H1 = h]
[Ka]k,h = E[[?(R)]k|A1 = a,H1 = h]
[W a]k,h = E[[?(D)]k|A1 = a,H1 = h]
In addition, let ?a ? Rm?m be a diagonal matrix
with [?a]h,h = P (H1 = h|A1 = a).
We now state the conditions for the correctness of
Eq. (1-4). For each label a ? [l], we require that
Condition 6.1 Ia, Ja,Ka,W a have rank m.
Condition 6.2 [?a]h,h > 0 for all h ? [m].
The conditions lead to the following proposition.
Proposition 5.1. Assume Condition 6.1 and 6.2
hold. For all a ? [l], define matrices
?a1 = E[?(F1)?(P )>|A1 = a] ? Rd1?d2
?a2 = E[?(R)?(D)>|A1 = a] ? Rd3?d4
Let ua1 . . . uam ? Rd1 and va1 . . . vam ? Rd2 be the
top m left and right singular vectors of ?a. Sim-
ilarly, let la1 . . . lam ? Rd3 and ra1 . . . ram ? Rd4 be
the top m left and right singular vectors of ?a.
Define projection matrices
?a = [ua1 . . . uam]> ?a = [va1 . . . vam]>
?a = [la1 . . . lam]> ?a = [ra1 . . . ram]>
Then the following m?m matrices
Ga = ?aIa Ga = ?aJa
Ha = ?aKa Ha = ?aW a
are invertible.
The proof resembles that of lemma 2 of Hsu et al
(2012). Finally, we state the main result that shows?
Cb|a, C?|a, c1a, cax
? in Eq. (1-4) using the projec-
tions from proposition 5.1 satisfy theorem 4.1. A
sketch of the proof is deferred to the appendix.
Theorem 5.1. Assume conditions 6.1 and 6.2
hold. Let ??a,?a,?a,?a? be the projection ma-
trices from proposition 5.1. Then the operators in
Eq. (1-4) satisfy theorem 4.1.
In summary, these results show that with the
proper selection of feature functions, we can con-
struct projection matrices ??a,?a,?a,?a? to ob-
tain operators ?Cb|a, C?|a, c1a, cax
? which satisfy
the conditions of theorem 4.1.
6 The Spectral Estimation Algorithm
In this section, we give an algorithm to estimate
the operators ?Cb|a, C?|a, c1a, cax
? from samples of
skeletal sequences. Suppose the training set con-
sists of M skeletal sequences (a(j), x(j)) for j ?
[M ]. ThenM samples of the random variables can
be derived from this training set as follows
? At each j ? [M ], choose a position
ij uniformly at random from the positions
in (a(j), x(j)). Sample the random vari-
ables (X,A1, A2, F1, F2, P,R,D,B) using
the procedure defined in section 5.1.
This process yields M samples
(x(j), a(j)1 , a
(j)
2 , f
(j)
1 , f
(j)
2 , p(j), r(j), d(j), b(j)) for j ? [M ]
Assuming (a(j), x(j)) are i.i.d. draws from
the PMF p(a1 . . . aN , x1 . . . xN ) over skeletal se-
quences under an R-HMM, the tuples obtained
through this process are i.i.d. draws from the joint
PMF over (X,A1, A2, F1, F2, P,R,D,B).
60
Input: samples of (X,A1, A2, F1, F2, P,R,D,B); feature
functions ?, ?, ?, and ?; number of hidden states m
Output: estimates
?
C?b|a, C??|a, c?1a, c?ax
?
of the operators
used in algorithm 2
[Singular Value Decomposition]
? For each label a ? [l], compute empirical estimates of
?a1 = E[?(F1)?(P )>|A1 = a]
?a2 = E[?(R)?(D)>|A1 = a]
and obtain their singular vectors via an SVD. Use
the top m singular vectors to construct projections?
??a, ??a, ??a, ??a
?
.
[Sample Projection]
? Project (d1, d2, d3, d4)-dimensional samples of
(?(F1), ?(F2), ?(P ), ?(R), ?(D))
with matrices
?
??a, ??a, ??a, ??a
?
to obtain m-
dimensional samples of
(F 1, F 2, P ,R,D)
[Method of Moments]
? For each a, b ? [l] and x ? [n], compute empirical
estimates
?
??a, ??a, D?b|a, d?ax
?
of the observable blocks
?a = E[F 1P>|A1 = a]
?a = E[R D>|A1 = a]
Db|a = E[[[A2 = b]]F 2P>R>|A1 = a]
dax = E[[[X = x]]D>|A1 = a]
and also c?1a = E[[[A1 = a]]F 1|B = 1]. Finally, set
C?b|a(v)? D?b|a(v)(??a)?1
C??|a(v)? D??|a(v)(??a)?1
c?ax ? d?ax(??a)?1
Figure 4: The spectral estimation algorithm
The algorithm in figure 4 shows how to derive
estimates of the observable representations from
these samples. It first computes the projection
matrices ??a,?a,?a,?a? for each label a ? [l]
by computing empirical estimates of ?a1 and ?a2
in proposition 5.1, calculating their singular vec-
tors via an SVD, and setting the projections in
terms of these singular vectors. These projection
matrices are then used to project (d1, d2, d3, d4)-
0 5 10 15 20 25 30hidden states (m)54.0
54.555.0
55.556.0
56.557.0
57.5
accur
acy
SpectralEM
Figure 5: Accuracy of the spectral algorithm and
EM on TIMIT development data for varying num-
bers of hidden states m. For EM, the highest scor-
ing iteration is shown.
dimensional feature vectors
(
?(f (j)1 ), ?(f
(j)
2 ), ?(p(j)), ?(r(j)), ?(d(j))
)
down to m-dimensional vectors
(
f (j)1 , f
(j)
2 , p
(j), r(j), d(j)
)
for all j ? [M ]. It then computes correlation
between these vectors in this lower dimensional
space to estimate the observable blocks which are
used to obtain the operators as in Eq. (1-4). These
operators can be used in algorithm 2 to compute
marginals.
As in other spectral methods, this estimation al-
gorithm is consistent, i.e., the marginals ??(a, i)
computed with the estimated operators approach
the true marginal values given more data. For
details, see Cohen et al (2012) and Foster et al
(2012).
7 Experiments
We apply the spectral algorithm for learning
R-HMMs to the task of phoneme recognition.
The goal is to predict the correct sequence of
phonemes a1 . . . aN for a given a set of speech
frames x1 . . . xN . Phoneme recognition is often
modeled with a fixed-structure HMM trained with
EM, which makes it a natural application for spec-
tral training.
We train and test on the TIMIT corpus of spoken
language utterances (Garofolo and others, 1988).
The label set consists of l = 39 English phonemes
following a standard phoneme set (Lee and Hon,
1989). For training, we use the sx and si utter-
ances of the TIMIT training section made up of
61
?(F1) ai+1 ? xi, ai+1, xi, np(ai . . . aN )
?(P ) (ai?1, xi?1), ai?1, xi?1, pp(a1 . . . ai)
?(R) xi
?(D) ai?1 ? xi?1, ai?1, xi?1, pp(a1 . . . ai),
pos(a1 . . . aN )
iy r r r r r r ow . . .. . .
pp b m e np
Figure 6: The feature templates for phoneme
recognition. The simplest features look only at the
current label and observation. Other features in-
dicate the previous phoneme type used before ai
(pp), the next phoneme type used after ai (np),
and the relative position (beginning, middle, or
end) of ai within the current phoneme (pos). The
figure gives a typical segment of the phoneme se-
quence a1 . . . aN
M = 3696 utterances. The parameter estimate is
smoothed using the method of Cohen et al (2013).
Each utterance consists of a speech signal
aligned with phoneme labels. As preprocessing,
we divide the signal into a sequence of N over-
lapping frames, 25ms in length with a 10ms step
size. Each frame is converted to a feature repre-
sentation using MFCC with its first and second
derivatives for a total of 39 continuous features.
To discretize the problem, we apply vector quanti-
zation using euclidean k-means to map each frame
into n = 10000 observation classes. After pre-
processing, we have 3696 skeletal sequence with
a1 . . . aN as the frame-aligned phoneme labels and
x1 . . . xN as the observation classes.
For testing, we use the core test portion of
TIMIT, consisting of 192 utterances, and for de-
velopment we use 200 additional utterances. Ac-
curacy is measured by the percentage of frames
labeled with the correct phoneme. During infer-
ence, we calculate marginals ? for each label at
each position i and choose the one with the highest
marginal probability, a?i = arg maxa?[l] ?(a, i).
The spectral method requires defining feature
functions ?, ?, ?, and ?. We use binary-valued
feature vectors which we specify through features
templates, for instance the template ai ? xi corre-
sponds to binary values for each possible label and
output pair (ln binary dimensions).
Figure 6 gives the full set of templates. These
feature functions are specially for the phoneme
labeling task. We note that the HTK baseline
explicitly models the position within the current
Method Accuracy
EM(4) 56.80
EM(24) 56.23
SPECTRAL(24), no np, pp, pos 55.45
SPECTRAL(24), no pos 56.56
SPECTRAL(24) 56.94
Figure 7: Feature ablation experiments on TIMIT
development data for the best spectral model (m =
24) with comparisons to the best EM model (m =
4) and EM with m = 24.
Method Accuracy
UNIGRAM 48.04
HMM 54.08
EM(4) 55.49
SPECTRAL(24) 55.82
HTK 55.70
Figure 8: Performance of baselines and spectral
R-HMM on TIMIT test data. Number of hidden
states m optimized on development data (see fig-
ure 5). The improvement of the spectral method
over the EM baseline is significant at the p ? 0.05
level (and very close to significant at p ? 0.01,
with a precise value of p ? 0.0104).
phoneme as part of the HMM structure. The spec-
tral method is able to encode similar information
naturally through the feature functions.
We implement several baseline for phoneme
recognition: UNIGRAM chooses the most likely
label, arg maxa?[l] p(a|xi), at each position;
HMM is a standard HMM trained with maximum-
likelihood estimation; EM(m) is an R-HMM
with m hidden states estimated using EM; and
SPECTRAL(m) is an R-HMM with m hidden
states estimated with the spectral method de-
scribed in this paper. We also compare to HTK,
a fixed-structure HMM with three segments per
phoneme estimated using EM with the HTK
speech toolkit. See Young et al (2006) for more
details on this method.
An important consideration for both EM and the
spectral method is the number of hidden states m
in the R-HMM. More states allow for greater label
refinement, with the downside of possible overfit-
ting and, in the case of EM, more local optima.
To determine the best number of hidden states, we
optimize both methods on the development set for
a range of m values between 1 to 32. For EM,
62
we run 200 training iterations on each value of m
and choose the iteration that scores best on the de-
velopment set. As the spectral algorithm is non-
iterative, we only need to evaluate the develop-
ment set once per m value. Figure 5 shows the
development accuracy of the two method as we
adjust the value of m. EM accuracy peaks at 4
hidden states and then starts degrading, whereas
the spectral method continues to improve until 24
hidden states.
Another important consideration for the spectral
method is the feature functions. The analysis sug-
gests that the best feature functions are highly in-
formative of the underlying hidden states. To test
this empirically we run spectral estimation with a
reduced set of features by ablating the templates
indicating adjacent phonemes and relative posi-
tion. Figure 7 shows that removing these features
does have a significant effect on development ac-
curacy. Without either type of feature, develop-
ment accuracy drops by 1.5%.
We can interpret the effect of the features in
a more principled manner. Informative features
yield greater singular values for the matrices ?a1
and ?a2, and these singular values directly affect
the sample complexity of the algorithm; see Cohen
et al (2012) for details. In sum, good feature func-
tions lead to well-conditioned ?a1 and ?a2, which in
turn require fewer samples for convergence.
Figure 8 gives the final performance for the
baselines and the spectral method on the TIMIT
test set. For EM and the spectral method, we
use best performing model from the develop-
ment data, 4 hidden states for EM and 24 for
the spectral method. The experiments show that
R-HMM models score significantly better than a
standard HMM and comparatively to the fixed-
structure HMM. In training the R-HMM models,
the spectral method performs competitively with
EM while avoiding the problems of local optima.
8 Conclusion
This paper derives a spectral algorithm for the
task of supervised sequence labeling using an R-
HMM. Unlike EM, the spectral method is guar-
anteed to provide a consistent estimate of the pa-
rameters of the model. In addition, the algorithm
is simple to implement, requiring only an SVD
of the observed counts and other standard ma-
trix operations. We show empirically that when
equipped with informative feature functions, the
spectral method performs competitively with EM
on the task of phoneme recognition.
Appendix
Proof of proposition 4.1. At any time step i ? [N ] in the al-
gorithm in figure 2, for all label a ? [l] we have a column
vector ?ia ? Rm and a row vector ?ia ? Rm. The value of
these vectors at each index h ? [m] can be verified as
[?ia]h =
?
a1...ai,h1...hi:
ai=a,hi=h
p(a1 . . . ai, x1 . . . xi?1, h1 . . . hi)
[?ia]h =?
ai...aN ,hi...hN :
ai=a,hi=h
p(ai+1 . . . aN , xi . . . xN , hi+1 . . . hN |ai, hi)
Thus ?ia?ia is a scalar equal to
?
a1...aN ,h1...hN :ai=a
p(a1 . . . aN , x1 . . . xN , h1 . . . hN )
which is the value of the marginal ?(a, i).
Proof of theorem 5.1. It can be verified that c1a = Gapia. For
the others, under the conditional independence illustrated in
figure 3 we can decompose the observable blocks in terms of
the R-HMM parameters and invertible matrices
?a = Ga?a(Ga)> ?a = Ha?a(Ha)>
Db|a(v) = GbT b|adiag(vHa)?a(Ga)>
D?|a(v) = fadiag(vHa)?a(Ga)> dax = oax?a(Ha)>
using techniques similar to those sketched in Cohen et al
(2012). By proposition 5.1, ?a and ?a are invertible, and
these observable blocks yield the operators that satisfy theo-
rem 4.1 when placed in Eq. (1-3).
References
A. Anandkumar, D. P. Foster, D. Hsu, S.M. Kakade, and Y.K.
Liu. 2012a. Two svds suffice: Spectral decompositions
for probabilistic topic modeling and latent dirichlet alo-
cation. Arxiv preprint arXiv:1204.6703.
A. Anandkumar, D. Hsu, and S.M. Kakade. 2012b. A
method of moments for mixture models and hidden
markov models. Arxiv preprint arXiv:1203.0683.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral
learning algorithm for finite state transducers. Machine
Learning and Knowledge Discovery in Databases, pages
156?171.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-
gar. 2012. Spectral learning of latent-variable PCFGs. In
Proceedings of the 50th Annual Meeting of the Association
for Computational Linguistics. Association for Computa-
tional Linguistics.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Un-
gar. 2013. Experiments with spectral learning of latent-
variable pcfgs. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies.
63
D. P. Foster, J. Rodu, and L.H. Ungar. 2012. Spec-
tral dimensionality reduction for hmms. Arxiv preprint
arXiv:1203.6130.
J. S. Garofolo et al 1988. Getting started with the darpa
timit cd-rom: An acoustic phonetic continuous speech
database. National Institute of Standards and Technology
(NIST), Gaithersburgh, MD, 107.
D. Hsu, S.M. Kakade, and T. Zhang. 2012. A spectral al-
gorithm for learning hidden markov models. Journal of
Computer and System Sciences.
H. Jaeger. 2000. Observable operator models for discrete
stochastic time series. Neural Computation, 12(6):1371?
1398.
K. Lari and S. J. Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algorithm.
Computer speech & language, 4(1):35?56.
K.F. Lee and H.W. Hon. 1989. Speaker-independent phone
recognition using hidden markov models. Acoustics,
Speech and Signal Processing, IEEE Transactions on,
37(11):1641?1648.
F. M. Luque, A. Quattoni, B. Balle, and X. Carreras. 2012.
Spectral learning for non-deterministic dependency pars-
ing. In EACL, pages 409?419.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic cfg
with latent annotations. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Linguis-
tics, pages 75?82. Association for Computational Linguis-
tics.
A. Parikh, L. Song, and E.P. Xing. 2011. A spectral algo-
rithm for latent tree graphical models. In Proceedings of
the 28th International Conference on Machine Learning.
F. Pereira and Y. Schabes. 1992. Inside-outside reestima-
tion from partially bracketed corpora. In Proceedings
of the 30th annual meeting on Association for Computa-
tional Linguistics, pages 128?135. Association for Com-
putational Linguistics.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learn-
ing accurate, compact, and interpretable tree annotation.
In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages
433?440. Association for Computational Linguistics.
Slav Petrov, Adam Pauls, and Dan Klein. 2007. Learn-
ing structured models for phone recognition. In Proc. of
EMNLP-CoNLL.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Proceed-
ings of the IEEE, 77(2):257?286.
S. Siddiqi, B. Boots, and G. J. Gordon. 2010. Reduced-
rank hidden Markov models. In Proceedings of the Thir-
teenth International Conference on Artificial Intelligence
and Statistics (AISTATS-2010).
L. Song, B. Boots, S. Siddiqi, G. Gordon, and A. Smola.
2010. Hilbert space embeddings of hidden markov mod-
els. In Proceedings of the 27th International Conference
on Machine Learning. Citeseer.
S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw,
XA Liu, G. Moore, J. Odell, D. Ollason, D. Povey, et al
2006. The htk book (for htk version 3.4).
64
