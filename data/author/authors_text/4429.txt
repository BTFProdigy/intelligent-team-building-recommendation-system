Proceedings of NAACL HLT 2007, Companion Volume, pages 9?12,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Conquest ? an Open-Source Dialog System for Conferences 
 
Dan Bohus, Sergio Grau Puerto, David Huggins-Daines, Venkatesh Keri,  
Gopala Krishna,  Rohit Kumar, Antoine Raux, Stefanie Tomko 
School of Computer Science 
Carnegie Mellon University 
{ dbohus, sgrau, dhuggins, vkeri, gopalakr, rohitk, antoine, stef }@ cs.cmu.edu 
 
 
  
Abstract 
We describe ConQuest, an open-source, 
reusable spoken dialog system that pro-
vides technical program information dur-
ing conferences. The system uses a 
transparent, modular and open infrastruc-
ture, and aims to enable applied research 
in spoken language interfaces. The con-
ference domain is a good platform for ap-
plied research since it permits periodical 
redeployments and evaluations with a real 
user-base. In this paper, we describe the 
system?s functionality, overall architec-
ture, and we discuss two initial deploy-
ments.  
1 Introduction  
Conducting applied spoken language interface re-
search is generally a costly endeavor. Developing, 
deploying and maintaining real-world spoken lan-
guage interfaces requires an existing infrastructure, 
a significant amount of engineering effort, and can 
greatly benefit from the availability of certain re-
sources such as transcribed in-domain data.  
In an effort to enable applied research and to 
lower this high cost of entry, we have developed 
ConQuest (Conference Questions) an open-source 
spoken dialog system that provides access to 
schedule and technical program information during 
conferences. We believe the conference domain 
has a number of good properties for applied re-
search: it includes a number of tasks of different 
complexities, it provides regular access to a real-
world user population; it permits periodical rede-
ployments and evaluations and therefore can pro-
vide a natural common evaluation task for the 
spoken language interfaces community.  
The ConQuest system is constructed on top of 
the open, transparent and modular Olympus dialog 
system framework (2007), and can be easily reused 
across different conferences. To date, the system 
has been deployed in two conferences: InterSpeech 
2006 and IJCAI 2007. Together with corpora col-
lected from these deployments, the system is freely 
available for download (Conquest, 2007).  
We begin by describing the ConQuest function-
ality in the next section. Then, in section 3 we pro-
vide an overview of the system architecture and 
discuss the development process. In section 4 we 
briefly discuss the two deployment efforts. Finally, 
in section 5 we discuss related work and draw a 
number of conclusions.   
2 Functionality 
As Figure 1 illustrates, ConQuest is a mixed-
initiative spoken dialog system that provides ac-
cess to schedule and technical program information 
during conferences.  
Users can browse the schedule and find details 
about various papers or sessions of interest by pro-
viding identifying information, such as topics, ses-
sion names, special events, paper titles, author 
names, specific dates and times, specific locations, 
or a combination thereof (e.g. turns 2, 4, 14). Addi-
tionally, the system also allows uses to listen to 
current announcements and to cast their votes for a 
Best Paper Award (e.g. turns 10-17).  
The interaction is mixed-initiative; at any point, 
the user may switch the current focus of the con-
versation (e.g. turn 8). ConQuest employs an adap-
tive information presentation strategy that allows 
9
users to easily navigate the schedule (see turns 3, 5 
and 15). The system uses a rich repertoire of error 
recovery strategies to handle potential errors, in-
cluding several fall-back strategies (e.g. turn 13).  
3 System Architecture  
The ConQuest system was built using RavenClaw/ 
Olympus (2007), an open-source framework that 
facilitates research and development in task ori-
ented conversational spoken language interfaces. 
Olympus consists of a collection of components 
for recognition, language understanding, dialog 
management, language generation, speech synthe-
sis, etc., and the corresponding communication 
infrastructure. To date, Olympus has been used to 
develop and deploy a number of other systems 
spanning different domains and interaction types 
(Bohus and Rudnicky, 2003).  
A key characteristic of the Olympus framework 
is a clear separation between the domain independ-
ent programs (or components) and domain specific 
resources. This decoupling promotes reusability 
and significantly lessens the system development 
effort. In ConQuest, the authoring effort was fo-
cused on developing resources such as the lexicon, 
language model, grammar, dialog task specifica-
tion, etc. Some interesting, unanticipated engineer-
ing challenges we faced during development were 
dealing with foreign names and accented charac-
ters and performing text normalization on various 
fields (e.g. Alex Smith and Alexander Smith are 
the same author), while at the same time ensuring 
consistency between these various resources. Be-
low, we briefly comment of each component and 
the corresponding resources. Figure 2 provides a 
top-level architectural view. 
Speech Recognition. ConQuest uses a recogni-
tion server coupled to a set of parallel recognition 
engines: two SPHINX-II decoders (Huang et al, 
1992) that use gender-specific acoustic models, 
and a DTMF (touch-tone decoder). Each recogni-
tion engine uses class-based (e.g. paper titles, au-
thor names, etc.), state-specific trigram-language 
models. We started with an initial language model 
built using data collected with an early text-only 
prototype. We then internally deployed a speech 
based system, collected more data, transcribed it, 
and used it to retrain the language models. The 
Figure 1. A sample interaction with ConQuest 
1 S: Welcome to ConQuest [?] I can give you information about papers, sessions, special events and announcements at Inter-
Speech 2006. You can also cast a vote for the best paper award. How can I help you? 
2 U: are there any papers on spoken dialog systems today 
3 S:  There are 23 papers on spoken dialog systems today. These papers are being presented in 3 sessions: Dialog and Discourse, 
Language Modeling for Spoken Dialog Systems and Spoken Dialog Systems I. To browse this list of papers, please tell me the 
author name, the paper title, the time or the venue you?re looking for. If you?re not interested in any of these papers say ?quit?. 
4 U: which ones are in the afternoon 
5 S:  There will be 11 papers on spoken dialog systems this afternoon. Would you like to hear information about all these papers? 
6 U: yes 
7 S:  The list of papers on spoken dialog systems this afternoon is: Paper 1 - ?Dynamic Extension of a Grammar-based Dialogue 
System: Constructing an All-Recipes Knowing Robot?, Paper 2 ?  [?] Would you like more information about any of these? 
8 U: tell me more about the third one 
9 S:  Paper 3 is titled ?System- versus user-initiative dialog strategy for driver information systems?, and was presented yesterday at 2 
pm in the poster session in Allegheny Ballroom Foyer. This paper was written by Chantal Ackermann, and Marion Libossek. 
Would you like more information about any of these? 
8 U: no when is the banquet 
9 S:  The Banquet will be held Wednesday from 7pm to 9:30 pm in the Heintz History Center. The Heinz History Center is located on 
Smallman street between 12th and 13th streets. The History Center is a museum that relates and interprets 250 years of history 
in the Western Pennsylvania area. Is there anything else I can do for you? 
10 U: yes I?d like to cast a vote 
11 S:  Please tell me the title of the paper you?d like to vote for.. 
12 U: I don?t really know the title but I think it had Diane as one of the main authors ? [non-understanding] 
13 S:  Sorry, I didn?t catch that. Please dial the first six letters from the title of the paper you would like to vote for.  
14 U: I don?t know it it?s a paper by Diane Litman 
15 S:  There are 3 papers by Diane Litman. Paper 1 ? ?Discourse Structure and Speech Recognition Problems?, Paper 2 ? [?], Paper 
3 ? [?] Would you like to vote for any of these? 
16 U: Discourse Structure and Speech Recognition Problems 
17 S:  Okay, I?ve recorded your vote for the paper entitled ?Discourse Structure and Speech Recognition Problems? What else can I do 
for you?  
18 U: [?] 
10
final language models used during the InterSpeech 
deployment were trained from on a corpus of 6350 
utterances. The system operated with a lexicon of 
4795 words, which included 659 lexicalized (con-
catenated) paper titles, and 1492 lexicalized author 
names, and 78 lexicalized session names. The pro-
nunciations were generated using CMU Dictionary 
and later manually corrected.  
Language understanding. The system uses the 
Phoenix (Ward and Issar, 1994) robust parser to 
extract concepts from the recognition results. A 
domain-specific shallow semantic grammar was 
developed and concatenated with a domain-
independent grammar for generic expressions like 
[Yes], [No], [Date], [Time], etc.  
Dialog management. ConQuest uses a Raven-
Claw-based dialog manager (Bohus and Rudnicky, 
2003). We developed a dialog task specification 
for the conference schedule domain, expressed as a 
hierarchical plan for the interaction, which the 
RavenClaw engine uses to drive the dialog. In the 
process, the RavenClaw engine automatically pro-
vides additional generic conversational skills such 
as error recovery strategies and support for various 
universal dialog mechanisms (e.g. repeat, start-
over, what-can-I-say, etc.)  
Backend/Database. A backend agent looks up 
schedule information from the database (stored as 
a flat text file). The backend agent also performs 
domain specific pre-lookup normalization (e.g. 
mapping author names to their canonical forms), 
and post-lookup processing of the returned records 
(e.g. clustering papers by sessions). The database 
file serves as starting point for constructing a 
number of other system resources (e.g. language 
model classes, lexicon, etc.)  
Figure 2. The Olympus dialog system reference architecture (a typical system) 
Temporal reference resolution agent. Apart 
from the database agent, the dialog manager also 
communicates with an agent that resolves temporal 
expressions (e.g. tomorrow at four p.m.) into ca-
nonical forms.  
Language generation. ConQuest uses Rosetta, 
a template-based language generation component. 
The authoring effort at this level consisted of writ-
ing various templates for the different system ques-
tions and information presentation prompts.  
Speech synthesis. ConQuest uses the Cepstral 
(2005) speech synthesis engine, configured with an 
open-domain unit selection voice. We manually 
checked and corrected pronunciations for author 
names, various technical terms and abbreviations.  
4 Development and Deployment 
The first development of ConQuest system was 
done for the Interspeech 2006 conference held in 
Pittsburgh, PA. The iterative development process 
involved regular interaction with potential users 
i.e. researchers who regularly attend conferences. 
Seven developers working half time participated in 
this development for about three months. An esti-
mated one man-year of effort was spent. This esti-
mate does not include the effort involved in 
transcribing the data collected after the conference. 
Two systems were deployed at the Interspeech 
2006 conference: a desktop system using a close-
talking microphone placed by the registration desk, 
and a telephone-based system. Throughout the 
conference we collected a corpus of 174 sessions. 
We have orthographically transcribed the user ut-
11
terances and are currently analyzing the data; we 
plan to soon release it to the community, together 
with detailed statistics, the full system logs as well 
as the full system source code (Conquest, 2007). 
Following Interspeech 2006, ConQuest was re-
deployed at IJCAI 2007 conference held in Hy-
derabad, India. The second deployment took an 
estimated two man-months: three developers work-
ing half-time for over a month. The significant 
parts of the second deployment involved incorpo-
rating scheduling data for the IJCAI 2007 and im-
plementing two new requirements i.e. support for 
workshops and Indian English speech recognition. 
The IJCAI development had fewer iterations than 
the first effort. The two desktop systems set up at 
the conference venue collected 129 sessions of 
data. This data is currently being transcribed and 
will soon be released to the community through the 
Conquest website (Conquest, 2007). 
Through these two deployments of ConQuest 
the system specifications have been refined and we 
expect the development time to asymptote to less 
than a month after a few more deployments. 
5 Discussion and Conclusion  
Our primary goal in developing ConQuest was to 
enable research by constructing and releasing an 
open-source, full-fledged dialog system, as well as 
an initial corpus collected with this system. The 
system is built on top of an open, transparent and 
modular infrastructure that facilitates research in 
spoken language interfaces (Olympus, 2007).  
There have been a number of other efforts to 
collect and publish dialog corpora, for instance 
within the DARPA Communicator project. A more 
recent project, that operates in a domain similar to 
ConQuest is DiSCoH, a Dialog System for Confer-
ence Help developed by researchers at AT&T, 
ICSI and Edinburgh University, and deployed dur-
ing the SLT-2006 workshop (Adreani et al, 2006). 
While their goals are similar, i.e. to enable re-
search, DiSCoH and ConQuest differ in a number 
of dimensions. Functionality-wise, DiSCoH offers 
general conference information about the venue, 
accommodation options and costs, paper submis-
sion, etc., while ConQuest provides access to the 
technical schedule and allows participants to vote 
for a best paper award. DiSCoH is built using 
AT&T technology and a call-routing approach; 
ConQuest relies on a plan-based dialog manage-
ment framework (RavenClaw) and an open-source 
infrastructure (Olympus). Finally, the DiSCoH ef-
fort aims to develop a richly annotated dialog cor-
pus to be used for research; ConQuest?s aim is to 
provide both the full system and an initial tran-
scribed and annotated corpus to the community. 
The conference domain is interesting in that it 
allows for frequent redeployment and in theory 
provides regular access to a certain user-base. It 
should therefore facilitate research and periodical 
evaluations. Unfortunately, the dialog corpora col-
lected so far using DiSCoH and ConQuest have 
been somewhat smaller than our initial expecta-
tions. We believe this is largely due to the fact that 
the systems provide information that is already 
accessible to users by other means (paper confer-
ence program, web-sites, etc.). Perhaps combining 
the functionalities of these two systems, and ex-
panding into directions where the system provides 
otherwise hard-to-access information (e.g. local 
restaurants, transportation, etc.) would lead to in-
creased traffic.  
References  
Adreani, G., Di Fabbrizio, G., Gilbert, M., Gillick, D., 
Hakkani-Tur, D., and Lemon, O., 2006 Let?s DiS-
CoH: Collecting an Annotated Open Corpus with 
Dialogue Acts and Reward Signals for Natural Lan-
guage Helpdesk, in Proceedings of IEEE SLT-2006 
Workshop, Aruba Beach, Aruba. 
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog 
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proceedings of 
Eurospeech 2003, Geneva, Switzerland. 
Cepstral, LLC, 2005, SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com. 
Conquest, 2007, http://www.conquest-dialog.org. 
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, 
K.-F. and Rosenfeld, R., 1992. The SPHINX-II 
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992. 
Olympus/RavenClaw web page, as of January 2007: 
http://www.ravenclaw-olympus.org/. 
Ward, W., and Issar, S., 1994. Recent improvements in 
the CMU spoken language understanding system, in 
Proceedings of the ARPA Human Language Tech-
nology Workshop, pages 213?216, Plainsboro, NJ. 
12
Proceedings of NAACL HLT 2009: Demonstrations, pages 5?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Building Conversational Agents with Basilica 
Rohit Kumar Carolyn P. Ros? 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
rohitk@cs.cmu.edu cprose@cs.cmu.edu 
 
 
Abstract 
Basilica is an event-driven software architec-
ture for creating conversational agents as a 
collection of reusable components. Software 
engineers and computer scientists can use this 
general architecture to create increasingly so-
phisticated conversational agents. We have 
developed agents based on Basilica that have 
been used in various application scenarios and 
foresee that agents build on Basilica can cater 
to a wider variety of interactive situations as 
we continue to add functionality to our archi-
tecture. 
1 Introduction 
Conversational Interfaces apply the metaphor of 
agent to an interface which allows the user to con-
versationally interact with the machine using natu-
ral language through speech or text. The current 
state of the art in the area of conversational inter-
faces is largely dominated by spoken dialog sys-
tems (SDS).  These SDS are most often used for 
the purpose of accessing information from a data-
base over the telephone. Other common applica-
tions of conversational agents include computer 
aided instruction (CAI) and human-robot interac-
tion (HRI). 
Conversational Agents in most of today?s SDS, 
CAI and HRI are designed to work within the 
scope of specific task domains which allows the 
scientists and engineers working on such systems 
to ensure satisfactory and relevant interaction with 
the user most of the time. Within the task domain, 
such agents can display intelligent interactive be-
havior like helping the user use the interface, ask-
ing remedial questions (Bohus and Rudnicky, 
2005), shaping the user behavior (Tomko and Ro-
senfeld, 2004) by using alternative phrasing of ut-
terances, responding to user affect (D?Mello et al, 
2008) through text, voice and gesture, engaging the 
user through the display of presence via backchan-
nels (Ward, 1996) and embodiment (Cassell et al, 
1999). 
As more and more of these intelligent interac-
tive agents get built for many task domains (Raux 
et al, 2005; Bohus et al, 2007; Gockley et al, 
2005; Amtrak Julie; ?) that surround our every-
day life, we observe a gradual transition in the use 
of the conversational agent technology to be a form 
of situated interaction. One of the characteristic 
requirements of this transition towards ubiquity of 
such interactive agents is the capability to sense 
and trigger behavior in a context sensitive way. 
In most conversational interfaces today, the on-
ly trigger used by the agents is that of initiation of 
conversation usually by sensing user presence 
through a telephone call, proximity detection or 
user login into a virtual environment. The initiation 
event is followed by a scripted task-oriented con-
versation with the agent. These scripts could be 
fairly complex depending on the representational 
formalism underlying the script. Most of the com-
mon software architectures/platforms used to 
create conversational agents like TellMe Studio, 
Voxeo Prophecy, Olympus (Bohus et al, 2007), 
DIPPER (Bos and Oka, 2003), etc. use one or more 
of these presence sensing techniques and one of the 
many existing scripting languages including 
VoiceXML, SALT, TuTalk (Jordan et al, 2007) 
and Ravenclaw (Bohus and Rudnicky, 2003) task 
specification language among others.  
However, in our recent work on building con-
versational agents situated in collaborative learning 
5
environments, we have discovered the need for a 
software architecture for creating agents that pers-
ist in an interactive environment in which human 
users interact with these agents as well as with 
each other. In this situation, the agents need to be 
able to sense many kinds of triggers at many points 
of time and choose to respond to some of those 
triggers through a variety of modalities including 
conversation. This observation was the motivation 
for creating Basilica which is our architecture for 
building conversational agents. In section 2, we 
talk more about the intricacies of Basilica and 
agents built on this architecture. Section 3 de-
scribes some of application scenarios in which we 
are using Conversational Agents based on Basilica. 
2 Basilica Architecture 
In order to meet the need for an architecture that 
enables development of Conversational Agents as 
a collection of behavioral components that can 
sense triggers and respond to those appropriately, 
we created the Basilica architecture. 
In this architecture, we model sensing and res-
ponding as two types of components that make up 
conversational agents. The sensing components 
referred to as Filters observe stimuli from various 
kinds of input sources and other components. They 
can also generate stimuli for other components. On 
the other hand, Actor components generate respon-
sive behavior that may be observed the user(s) and 
other components. Basilica provides the software 
elements required to tie Filters and Actors together 
through Connections that carry Events over them. 
We think that many of the state of the art intelli-
gent behaviors listed in section 1 can be imple-
mented as dyads of filter and actor components. 
The minimal set of behavioral component 
classes listed above can easily be extended. For 
example, certain agent designs may need memory 
components and coordination components which 
bridge across multiple actors or filters that do not 
necessarily share events with each others. Timer 
components may be used to generate regulated 
stimuli. Besides belonging to one of these classes 
of components, certain components may act as 
wrappers to external systems. For example, we use 
wrapper components to integrate TuTalk dialog 
management system (Jordan et al, 2007) for some 
of the instructive behavior exhibited by our agents. 
Also, certain components act as wrappers to the 
environment in which the agent is present. These 
wrappers help in easily integrating the same agent 
with multiple environments without having to 
change any underlying components except the 
wrappers to the environment.  
We believe that fairly intelligent conversational 
agents can be built for situated interaction applica-
tions by incrementally building a large number of 
behavioral components. Each of these components 
represent a decomposition of the agent?s perceptive 
and cognitive capabilities. Among the agents we 
have built using Basilica, we observe that some of 
these capabilities are common across agents. 
Hence the corresponding behavioral components 
get re-used in many cases. Some instances of com-
ponent re-use are mentioned in Section 3. 
Note that recently there has been other work on 
modeling conversational agents as a decomposition 
of components. Jaspis (Turunen and Hakulinen, 
2003) models the agent as a collection of manag-
ers, agents and evaluators which synchronize with 
each other through transactions. RIME (Nakano et 
al., 2008) distributes cognitive capabilities across a 
collection of experts of two types. However, eva-
luators and agents are configured as a pile of com-
ponents whereas our filters and actors are 
configured as a network. Hence, designing conver-
sational agents with Basilica gives the flexibility to 
change the network topology. Also, while Jaspis 
agents are stateless, actors in our architecture need 
not be stateless. In other work on event-based mul-
ti-layered architectures (Raux and Eskenazi, 2007), 
events are used for communication between layers 
as a mean to provide higher reactive compared to 
pipeline architectures. While we share this motiva-
tion, definition of events is extended here as events 
are used for all kinds of communication, coordina-
tion and control in Basilica. 
3 Current Application Scenarios 
In 2008, we built three conversational agents to 
support learners in collaborative learning environ-
ments. Also, we are currently using Basilica to de-
velop a cross-lingual assistive agent to support 
non-Spanish speaking 911 dispatchers in the 
southern states of the US. In this section, we will 
discuss these four conversational agents briefly. 
CycleTalk is an intelligent tutoring system that 
helps college sophomores studying Thermodynam-
ics learn about principles of designing Steam 
6
cycles. In our recent experiments, we have studied 
the effectiveness of conversational agents in this 
intelligent tutoring system (Kumar et al, 2007; 
Chaudhuri et al, 2008). Student use the system 
both individually and in pairs. The conversational 
agent monitors student interaction in a chat room 
as the students work on solving a design problem. 
The tutor provides the students with hints to help 
touch upon all the underlying concepts while the 
students work on the design exercise. Also the 
agent brings up reflective dialogs when it detects a 
relevant topic in the students conversation. One of 
the problems we observed over the years with the 
use of instructional dialogs in collaborative envi-
ronments is that the students tend to ignore the tu-
toring agent if it interrupts the students when they 
are talking to each other. Basilica helped us in re-
solving this problem by implementing a compo-
nent that tells that student that help is available on 
the topic they are talking about and they can ask 
for the dialog support when they are ready. Basili-
ca gives the flexibility to change the intervention 
strategy used by the agent when it is speaking with 
more than one student. 
In another version of this system, the tutoring 
agent prompted the students with some motiva-
tional prompts occasionally as we observed that 
many of the students found the design exercise 
very demanding to complete in the time permitted 
for this lab exercise. We found that the use of mo-
tivational prompts improved the student?s attitude 
towards the automated agent. 
We developed another agent to help college 
level mathematics students working on problem 
solving. This agent operates in a collaborative en-
vironment which includes a whiteboard. As in the 
case with the CycleTalk agent, the agent used here 
also helps the students with hints and dialogs. The 
component required for those behaviors were re-
used as-is with modifications only their configura-
tion files. Besides these behaviors, the agent coor-
dinates the problem solving sessions for the team 
by presenting the team with problems as images 
placed on the whiteboard and helping the students 
stay on track by answering questions about the 
amount of time left in the problem solving session. 
Recently, we modified the environment wrap-
per components of our CycleTalk agent and inte-
grated them with a SecondLife application 
(Weusijana et al, 2008). This integration helps 
developers of conversational agents create interac-
tive agents in the SecondLife virtual environment. 
Finally, in a currently ongoing project, we are 
building an agent that would interpret Spanish ut-
terances from a distressed 9-1-1 caller and work 
with a human dispatcher who does not know Span-
ish to attend to the call. We model the agent in this 
scenario after a human translator who does not just 
translate the caller?s input to English and vice ver-
sa. Instead the translator partners with the dis-
patcher to provide service to the caller. Partnering 
conversational agents with a human user to help 
another human user in a different role is a novel 
application of interactive agents. 
4 Building Agents using Basilica 
 
Figure 1. Components of the CycleTalk Agent 
 
Building conversational agents using Basilica in-
volves the process of representing the desired 
agent as a decomposition of components. Figure 1 
above shows the components that make up the 
CycleTalk conversational agent we mentioned in 
Section 3. The rectangles represent Filters and the 
parallelograms represent Actors. Connections are 
shown as solid lines. In a detailed design, these 
lines are annotated with the events they carry. 
Once an agent is designed, the agents and filters 
required for the implementation of the agent can be 
either re-used from the pre-existing components of 
Basilica or implemented as Java objects that ex-
tend the corresponding component class. Often the 
programming task is limited to implementing han-
dlers and generators for the events received and 
sent out by the component. Theoretically, the va-
lidity of a component can be verified if it can han-
dle and generate all the events as specified in the 
design diagram. 
As we continue to develop more conversational 
agents on this architecture, we intend to create de-
velopment tools which would easily translate a 
7
design like Figure 1 to the implementation and fa-
cilitate validation and debugging of the agent. 
5 Demonstration Outline 
The demonstration of our architecture will give the 
audience an opportunity to interact with the agents 
we have described in section 3 and discuss how we 
can design such agents using Basilica. We will 
have a poster to aid the discussion along with abili-
ty to probe into the code underlying the design of 
these agents. Attendees will be able to understand 
the process involved in building agents with Basi-
lica and assess the effort required. Additionally, if 
we have any specialized development tools to au-
tomatically map agent design as described in Sec-
tion 4 to Java code, we will demonstrate those 
tools. Up to date information about Basilica can be 
found at http://basilica.rohitkumar.net/wiki/ 
Acknowledgements 
 
This work is supported by NSF REESE/REC grant 
number 0723580. 
References 
 
Dan Bohus and Alex Rudnicky, 2005. Error Handling 
in the RavenClaw dialog management architecture, 
HLT-EMNLP-2005, Vancouver 
Stefanie Tomko and Roni Rosenfeld, 2004. Shaping 
Spoken Input in User-Initiative Systems. Interspeech 
2004, Jeju, Korea 
Antoine Raux, Brian Langner, Dan Bohus, Alan Black, 
and Maxine Eskenazi, 2005. Let's Go Public! Taking 
a Spoken Dialog System to the Real World, Inters-
peech 2005, Lisbon, Portugal 
Dan Bohus, Sergio Grau, David Huggins-Daines, Ven-
katesh Keri, Gopala Krishna A., Rohit Kumar, An-
toine Raux, and Stefanie Tomko, 2007.  Conquest - 
an Open-Source Dialog System for Conferences, 
HLT-NAACL 2007, Rochester, NY 
Amtrack Julie, http://www.networkworld.com/news/ 
2003/0619julie.html 
Justin Cassell, Timothy Bickmore, Billinghurst, M., 
Campbell, L., Chang, K., Vilhj?lmsson, H. and Yan, 
H., 1999. Embodiment in Conversational Interfaces: 
Rea, CHI'99, Pittsburgh, PA 
Nigel Ward, 1996. Using Prosodic Clues to decide 
when to produce Back-channel Utterances, ICSLP 96 
Sidney D' Mello, Tanner Jackson, Scotty Craig, Brent 
Morgan, Patrick Chipman, Holly White, Natalie Per-
son, Barry Kort, Rana el Kaliouby, Rosalid W. Pi-
card and Arthur Graesser, 2008, AutoTutor Detects 
and Responds to Learners Affective and Cognitive 
States, Workshop on Emotional and Cognitive Is-
sues, ITS 2008, Montreal 
Rachel Gockley, Allison Bruce, Jodi Forlizzi, Marek 
Michalowski, Anne Mundell, Stephanie Rosenthal, 
Brennan Sellner, Reid Simmons, Kevin Snipes, Alan 
C. Schultz and Jue Wang, 2005. Designing Robots 
for Long-Term Social Interaction, IROS 2005 
Dan Bohus, Antoine Raux, Thomas Harris, Maxine 
Eskenazi and Alex Rudnicky, 2007. Olympus: an 
open-source framework for conversational spoken 
language interface research HLT-NAACL 2007 
Workshop on Bridging the Gap: Academic and In-
dustrial Research in Dialog Technology, Rochester, 
NY  
Johan Bos and Tetsushi Oka, 2003. Building Spoken 
Dialogue Systems for Believable Characters, 7th 
workshop on the semantics & pragmatics of dialogue 
TellMe, https://studio.tellme.com/ 
Voxeo Prophecy, http://www.voxeo.com/products/ 
Pamela Jordan, Brian Hall, Michael Ringenberg, Yue 
Cui, Carolyn P. Ros?, 2007.  Tools for Authoring a 
Dialogue Agent that Participates in Learning Stu-
dies, AIED 2007 
Dan Bohus and Alex Rudnicky, 2003. RavenClaw: Di-
alog Management Using Hierarchical Task Decom-
position and an Expectation Agenda, Eurospeech 
2003, Geneva, Switzerland 
Markku Turunen, Jaakko Hakulinen, 2003. Jaspis - An 
Architecture for Supporting Distributed Spoken Di-
alogues, Eurospeech? 2003, Geneva, Switzerland 
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, Hi-
roshi Tsujino, 2008. A Framework for Building Con-
versational Agents Based on a Multi-Expert Model, 
9th SigDial Workshop on Discourse and Dialog, Co-
lumbus, Ohio 
Antoine Raux and Maxine Eskenazi, 2007. A Multi-
Layer Architecture for Semi-Synchronous Event-
Driven Dialogue Management, ASRU 2007, Kyoto 
Rohit Kumar, Carolyn Rose, Mahesh Joshi, Yi-Chia 
Wang, Yue Cui, Allen Robinson, Tutorial Dialogue 
as Adaptive Collaborative Learning Support, 13th 
AIED 2007, Los Angeles, California 
Sourish Chaudhuri, Rohit Kumar, Carolyn P. Rose, 
2008. It?s not easy being green - Supporting Colla-
borative Green Design Learning, ITS 2008, Montreal 
Baba Kofi A. Weusijana, Rohit Kumar, Carolyn P. 
Rose, 2008. MultiTalker: Building Conversational 
Agents in Second Life using Basilica, Second Life 
Education Community Convention, Purple Strand: 
Educational Tools and Products, 2008, Tampa, FL  
8
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 677?680,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Engaging learning groups using Social Interaction Strategies 
Rohit Kumar Carolyn P. Ros? 
Language Technologies Institute 
Carnegie Mellon University, Pittsburgh, PA, 15213 
rohitk@cs.cmu.edu cprose@cs.cmu.edu 
 
 
Abstract 
Conversational Agents have been shown to be 
effective tutors in a wide range of educational 
domains. However, these agents are often ig-
nored and abused in collaborative learning 
scenarios involving multiple students. In our 
work presented here, we design and evaluate 
interaction strategies motivated from prior re-
search in small group communication. We 
will discuss how such strategies can be im-
plemented in agents. As a first step towards 
evaluating agents that can interact socially, we 
report results showing that human tutors em-
ploying these strategies are able to cover more 
concepts with the students besides being rated 
as better integrated, likeable and friendlier. 
1 Introduction 
Conversational Agents (CAs) are autonomous in-
terfaces that interact with users via spoken or writ-
ten conversation. One of the applications of CAs is 
tutoring. Various research groups have developed 
tutoring agents in domains like reading, algebra, 
geometry, calculus, physics, computer literacy, 
programming, foreign languages, research methods 
and thermodynamics. Many of the evaluations 
show that CAs can be effective tutors (Arnott et. 
al., 2008; Kumar et. al., 2007; Graesser et. al., 
2005). 
Most systems that use CAs as tutors have been 
built for learning scenarios involving one student. 
Evaluation of learning technologies involving stu-
dents working in groups with interactive agents has 
shown that learners are helped both by learning as 
a group and receiving tutorials from agents (Kumar 
et. al., 2007). However, some previous studies 
have reported that students learning in groups ig-
nore the tutor?s messages, unlike the case where 
students are individually tutored. Groups are more 
likely to abuse tutors than individual students.  
We reason that the presence of other students in 
collaborative learning scenarios causes the agents 
to compete for the attention of the students. Since 
the agents are not adept at performing social inter-
active behavior, which makes up the bulk of for-
mative communication in a group, they are quickly 
pushed to the periphery of the group. 
Research on small group communication has 
identified twelve interaction categories that are 
commonly observed in small groups (Bales, 1950). 
These categories are broadly classified into task 
and social-emotional categories. Content presented 
by most current CAs mostly classifies under the 
task categories. In section 2, we will list the con-
versational strategies motivated from the three pos-
itive social-emotional interaction categories. 
Thereafter, the implementation and evaluation of a 
CA that interleaves these social interaction strate-
gies while executing a task plan will be described. 
2 Social Interaction Strategies 
Balesian methodology (Bales, 1950) identifies 
three positive social-emotional interaction catego-
ries: showing solidarity, showing tension release 
and agreeing. Participants contribute turns of these 
categories to address the problems of re-
integration, tension release and decision respec-
tively. We have mapped these categories to practi-
cally implementable conversational strategies. This 
mapping is shown in table 1 ahead. 
Each strategy is implemented as an instantiation 
of a conversational behavior. Most of the strategies 
listed in Table 1 are realized as prompts, triggered 
by rules based on agent plan, discourse and context 
features. For example, strategy 1e is triggered 
677
when one or more students in the group are found 
to be inactive for over 5 minutes. In this event, the 
tutor chooses to raise the status of the inactive stu-
dents by eliciting contributions from them through 
a prompt like: Do you have any suggestions Mike? 
More implementation details of these strategies 
and triggers are discussed in the following section. 
 
1. Showing Solidarity 
Raises other's status, gives help, reward 
1a. Do Introductions 
Introduce and ask names of all participants 
1b. Be Protective & Nurturing 
Discourage teasing 
1c. Give Re-assurance 
When student is discontent, asking for help 
1d. Complement / Praise 
To acknowledge student contributions 
1e. Encourage 
When group or members are inactive 
1f. Conclude Socially 
2. Showing Tension Release 
Jokes, laughs, shows satisfaction 
2a. Expression of feeling better 
After periods of tension, work pressure 
2b. Be cheerful 
2c. Express enthusiasm, elation, satisfaction 
On completing significant steps of the task 
3. Agreeing 
Shows passive acceptance, understands, 
concurs, complies 
3a. Show attention 
To student ideas as encouragement 
3b. Show comprehension / approval 
To student opinions and orientations 
Table 1. Social Interaction Strategies for three  
social-emotional interaction categories 
3 WrenchTalker: Implementation 
WrenchTalker is a CA we have built to employ the 
social interaction strategies listed in section 2. It 
helps teams of engineering students learn and ap-
ply basic concepts of mechanical stress while they 
participate in a freshmen lab project to design an 
aluminum wrench. Students can interact with this 
agent using a text-based chat environment. 
The agent is built using the Basilica architecture 
(Kumar and Ros?, 2009). Under this architecture, 
CAs are modeled as a network of behavioral 
components. There are three types of components: 
actors (actuators / performers), filters (perceptors / 
annotators / cordinators) and memories. Figure 1 
below shows a simplified depiction of the 
WrenchTalker component network. 
 
 
Figure 1. Component Network of WrenchTalker 
Three of the actor and filter components 
correspond to three observable behaviors of the 
tutor, i.e., Introducing (ai, fi), Prompting (ap, fp) and 
Tutoring (at, ft). Most of the other filter 
components form a sub-network that annotates 
turns with applicable semantic categories, 
accumulates them to identify inactive students and 
generates events that regulate the controllers. 
The plan controller (fplan) is responsible for 
executing the agent?s interaction plan, which is 
comprised of 37 steps. The plan is executed largely 
sequentially; however the plan controller can 
choose to skip some steps in the interest of time. In 
the experiment described in section 5, the same 
plan controller is used in all three conditions. The 
social controller (fsocial) implements the 12 
strategies listed earlier. The strategies are triggered 
by rules based on combinations of three 
conditions: the last executed plan step, semantic 
categories associated with the most recent student 
turns and the ratio of tutor turns generated by fsocial 
to fplan. The first two conditions attempt to ensure 
that social behavior is suitable in the current 
conversational context and the third condition 
regulates the amount of social behavior by the CA. 
The plan and social controllers are connected so 
that they regulate each other. For instance, when 
the plan controller is working, it blocks fsocial. Upon 
completion of the blocking step, fsocial is given 
control, which can then choose to perform a 
strategy by blocking fplan before it progresses to the 
next step. Reflex strategies like 1b are not blocked. 
Once the controllers determine a step or a strat-
egy that is to be generated, the actors generate their 
turns. For example, strategy 1a is generated by ac-
tor ai after it is triggered by the social controller. 
We note that Basilica provides the flexibility to 
build complicated pipelines, as demonstrated in 
this case by the use of two controllers. 
678
4 Related Work 
To contextualize our research with other work on 
CAs, we classify agents with the social interaction 
strategies listed in Table 1 as social interfaces fol-
lowing the taxonomy proposed by Isbister (2002). 
Within this class of CAs, researchers have investi-
gated the technical challenges and effects of con-
versational behavior that are similar in motivation 
to the ones we are exploring. Bickmore et. al. 
(2009) report that users found agents with autobio-
graphies, i.e., back stories in first person more en-
joyable and they completed more conversations 
with such agents. Dybala et. al. (2009) found that 
agents equipped with humor were evaluated as 
more human-like, funny and likeable. In a multi-
party conversational scenario, Dohsaka et. al. 
(2009) found that an agent?s use of emphatic ex-
pressions improved user satisfaction and user rat-
ing of the agent. We note that use of CAs as social 
interfaces has been found to have effects on both 
performance and perception metrics. 
5 Experimental Design 
In order to evaluate the effect of social interaction 
strategies listed in Table 1, we designed an expe-
riment with three conditions. In the experimental 
condition (Social), students interacted with an 
agent that was equipped with our social interaction 
strategies, unlike the control condition (Task). In 
the third condition, a human tutor was allowed to 
intervene while the students interacted with a Task 
agent. In all three conditions, students go through 
the same task plan. However, the degree of social 
performance is varied from minimal (Task) to ideal 
(Human). We hypothesize that the human and so-
cial agents will be rated better than the Task agent. 
We conducted a between subjects experiment 
during a freshmen computer aided engineering lab. 
98 students participated in the experiment, which 
was held over six sessions spread evenly between 
two days. The two days of the experiment were 
separated by two weeks. Students were grouped 
into teams of three to four individuals. Students 
were grouped so that no two members of the same 
team sat next to each other during the lab, to en-
sure all communication was recorded. The teams 
were distributed between the three conditions. 
Each session started with a follow-along tutori-
al of computer-aided analysis where the students 
analyzed a wrench they had designed earlier. The 
experimental manipulation happened during a col-
laborative design competition after the tutorial. 
Students were asked to work as a team to design a 
better wrench considering three aspects: ease of 
use, cost and safety. Students were instructed to 
make three new designs and calculate success 
measures of each of the three considerations. They 
were also told that a tutor will help them with two 
designs so that they are well-prepared to do the 
final design. No additional details about the tutor 
were given. The students communicated with each 
other and with the tutors using ConcertChat, an on-
line environment that provides text-based instant 
messaging and workspace sharing facilities. 
After spending 30-35 minutes on the design 
competition, each student filled out a question-
naire. It was comprised of eighteen questions on a 
seven point Likert-scale ranging from Strongly 
Disagree (1) to Strongly Agree (7). The questions 
were designed to elicit four types of ratings. 
? Ratings about the tutor 
? Ratings about the other team members 
? Ratings about the design task 
? Ratings about the team functioning 
The questions in the first two classes elicited 
perceived liking and integration and checked 
whether the students noticed the tutor?s display of 
the social interaction strategies. Task related ques-
tions asked about satisfaction, perceived legitimacy 
and discussion quality. 
6 Results 
Table 2 below shows the mean values for ques-
tionnaire categories apart from ratings about team 
members, since there were no significant effects 
related to those questions. 
 
  D1 D2  T S H 
Integration 3.85 3.94 3.03 3.94 4.77 
Liking 3.68 3.63 2.78 3.53 4.73 
Friendly 5.13 5.43 4.47 5.56 5.83 
T.Releasing 4.49 4.63 3.84 4.61 5.27 
Agreeing 4.30 4.45 3.97 4.44 4.73 
Satisfaction 4.66 5.77 5.09 4.75 5.97 
Table 2. Mean outcomes per condition ((T)ask,(S)ocial, 
(H)uman) and per day (Day1, and Day2) 
The means are highlighted appropriately 
(p<0.001, p<0.05, p<0.08) to indicate significant 
679
differences from Day1 to Day2 and between the 
Task condition and each of the other two using a 
pairwise Tukey comparison. 
First of all, we note that there is a significant 
difference in task satisfaction between the two 
days. We fine-tuned the timing parameters of the 
plan controller after day 1 so that the students had 
sufficient time to follow along with each of the 
steps. This was particularly useful for the task con-
dition where the steps would be executed rapidly 
due to lack of regulation by the social controller. 
On the right side of Table 2, we notice that the 
human tutors (H) were rated higher on being part 
of the team (Integration), being more liked, being 
friendlier and keeping the group more socially 
comfortable (T.Releasing). On the other hand, the 
social tutors (S) were rated to be friendlier and 
were only marginally better at being seen as part of 
the team. 
 
 Strategy Social Human 
Introducing 1a 2.67 3.80 
Friendly 1b-1e 5.61 8.10 
Concluding 1f 0.97 1.80 
T.Releasing 2a-2c 5.81 1.77 
Agreeing 3a-3b 1.78 4.90 
Sum  16.83 22.17 
Table 3. Mean counts of social turns by tutor 
Note that human tutors were restricted to exhi-
bit only social behaviors, which were displayed in 
addition to the same task related content given to 
students in the other two conditions. Clearly, the 
human tutors were better at employing the social 
interaction strategies. To further investigate this, 
we compare the number of turns corresponding to 
the broad categories of strategies in Table 3. Hu-
man tutors performed significantly more (p<0.001) 
social turns than the automated tutors in all strate-
gies except showing tension release. 
7 Conclusions 
In order to make CAs that can participate in multi-
party conversational scenarios, the agents must be 
able to employ Social Interaction Strategies. Here 
we have shown that the human tutors that use these 
strategies are better integrated into the group, and 
are considered more likeable and friendlier. These 
tutors also cover more steps and concepts and take 
less time to tutor the concepts, suggesting that the 
students are more engaged and responsive to them. 
On the other hand, automated tutors that employ 
these strategies in our current implementation do 
not show significant differences compared to task 
tutor. 
We note a contrast between the performance of 
the human and the automated tutors with respect to 
the frequency with which they employ these strat-
egies. Besides the frequent use of these strategies, 
we believe human tutors were better at identifying 
opportunities for employing these strategies, and 
they are able to customize the prompt to better suit 
the discourse context. 
Acknowledgments 
The research was supported by NSF grant number 
DUE 837661 
References 
Elizabeth Arnott, Peter Hastings and David Allbritton, 
2008, Research Methods Tutor: Evaluation of a di-
alogue-based tutoring system in the classroom, Beha-
vior Research Methods, 40 (3), 694-698 
Robert F. Bales, 1950, Interaction process analysis: A 
method for the study of small groups, Addison-
Wesley, Cambridge, MA 
Timothy Bickmore, Daniel Schulman and Langxuan 
Yin, Engagement vs. Deceit: Virtual Humans with 
Human Autobiographies, 2009, IVA, Amsterdam 
Kohji Dohsaka, Ryoto Asai, Ryichiro Higashinaka, Ya-
suhiro Minami and Eisaku Maeda, Effects of Con-
versational Agents on Human Communication in 
Though Evoking Multi-Party dialogues, 2009, 10th 
Annual SigDial, London, UK 
Pawel Dybala, Michal Ptaszynski, Rafal Rzepka and 
Kenji Araki, Humoroids: Conversational Agents that 
induce positive emotions with humor, 2009, AAMAS, 
Budapest, Hungary 
Arthur C. Graesser, Patrick Chipman, Brian C. Haynes, 
and Andrew Olney, 2005, AutoTutor: An Intelligent 
Tutoring System with Mixed-initiative Dialogue, 
IEEE Transactions in Education, 48, 612-618 
Katherine Isbister and Patrick Doyle, Design and Evalu-
ation of Embodied Conversational Agents: A Pro-
posed Taxonomy, 2002, AAMAS Workshop: 
Embodied Conversational Agents, Bologna, Italy 
Rohit Kumar, Carolyn Ros?, Mahesh Joshi, Yi-Chia 
Wang, Yue Cui and Allen Robinson, Tutorial Dialo-
gue as Adaptive Collaborative Learning Support, 
13th AIED 2007, Los Angeles, California 
Rohit Kumar, Carolyn Ros?, Building Conversational 
Agents with Basilica, 2009, NAACL, Boulder, CO 
680
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 227?238,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Comparing Triggering Policies for Social Behaviors 
 
 
Rohit Kumar, Carolyn P. Ros? 
Language Technologies Institute, Carnegie Mellon University 
Gates Hillman Center, 5000 Forbes Avenue, 
Pittsburgh, PA, USA 15213 
rohitk , cprose @ cs.cmu.edu 
 
 
 
 
 
 
Abstract 
Instructional efficacy of automated Con-
versational Agents designed to help small 
groups of students achieve higher learning 
outcomes can be improved by the use of 
social interaction strategies. These strate-
gies help the tutor agent manage the atten-
tion of the students while delivering useful 
instructional content. Two technical chal-
lenges involving the use of social interac-
tion strategies include determining the 
appropriate policy for triggering these 
strategies and regulating the amount of so-
cial behavior performed by the tutor. In this 
paper, a comparison of six different trigger-
ing policies is presented. We find that a 
triggering policy learnt from human beha-
vior in combination with a filter that keeps 
the amount of social behavior comparable 
to that performed by human tutors offers 
the most effective solution to the these 
challenges. 
1 Introduction 
While Conversational Agents have been shown to 
be an effective technology for delivering instruc-
tional content to students in a variety of learning 
domains and situations (Grasser et. al., 2005; Ku-
mar et. al., 2007; Arnott et. al., 2008), it has been 
observed that students are more likely to ignore 
and abuse the tutor in a collaborative learning set-
ting (with 2 or more students) compared to the case 
of one-on-one tutoring (Bhatt et. al., 2004; Kumar 
et. al., 2007). In our prior work (Kumar et. al., 
2010a), we have addressed this problem by em-
ploying agents that are capable of performing both 
instructional behavior as well as social behavior. In 
our initial implementation, the social behavior per-
formed by these agents was composed of eleven 
social interaction strategies that were triggered by 
a set of hand crafted rules (Kumar and Ros?, 
2010b). Section 2 provides additional details about 
these strategies. 
Comparison between the social behavior trig-
gered by our hand crafted rules and that triggered 
by a human tutor revealed significant perception 
benefits (more likeable, higher task satisfaction, 
etc.) for the human triggering policy. Also, the stu-
dents in a wizard-of-oz condition who interacted 
with the tutors whose social behaviors were trig-
gered by humans had better learning outcomes 
(0.93?) with respect to a No social behavior base-
line. The condition where students interacted with 
the rule-based automated tutors was also signifi-
cantly better (0.71?) than the No social behavior 
baseline in terms of learning outcomes. While the 
learning outcomes of the rule-based tutors was not 
significantly worse than the human tutor, in com-
bination with the perception outcomes, we see the 
potential for further improvement of conversation-
al agents by employing a better triggering policy. 
Building on these prior results, in this paper we 
explore a way to improve the effectiveness of so-
cially capable tutor agents that uses a triggering 
policy learnt from a corpus of human behavior. 
The underlying hypothesis of this approach is that 
a human-like triggering policy would lead to im-
provements in the agent?s performance and percep-
227
tion ratings compared to a rule-based triggering 
policy. As a first step towards verifying this hypo-
thesis, we learnt a collection of triggering policies 
from a corpus of human behavior. While the focus 
of this paper is to evaluate the most human-like 
triggering policy learnt from data in terms of its 
perception benefits and learning outcomes, Section 
4 summarizes our efforts on learning triggering 
policies. 
Before we discuss the details of the evaluation 
we conducted, Section 3 presents an analysis of 
mediating factors that provides insights into the 
reasons behind the effectiveness of social behavior. 
The design and procedure of the user study we 
conducted to evaluate the learnt triggering policies 
is described in Section 5. Finally, Section 6 dis-
cusses the results of this evaluation. 
2 Social Interaction Strategies 
In our prior work (Kumar et. al., 2010; Ai et. al., 
2010; Kumar et. al., 2011), we have developed and 
evaluated automated tutors for two different educa-
tional domains equipped with eleven social interac-
tion strategies. These strategies, listed in Table 1, 
correspond to three positive socio-emotional inte-
raction categories identified by Bales (1950): 
Showing Solidarity, Showing Tension Release and 
Agreeing. 
Appendix A shows excerpts of an interaction 
between three students and a tutor during a college 
freshmen mechanical engineering learning activity.  
The shaded turns demonstrate realizations of some 
of the eleven social interaction strategies. 
Turns 7-12 shows the tutor initiating and partic-
ipating in group formation using Strategy 1a (Do 
Introductions) by greeting the students and asking 
for their names. In turn 53, the tutor is employing 
Strategy 3b (Show Comprehension / Approval) in 
response to a student opinion expressed in turn 52. 
When one of the students becomes inactive in the 
interaction, the tutor uses strategy 1e (Encourage) 
realized as a targeted prompt shown in turn 122 to 
elicit a response from the inactive student. Turn 
148 demonstrates Strategy 1d (Complement / 
Praise) to appreciate student participation in a con-
ceptual tutoring episode that concluded at turn 147. 
Finally, turn 152 shows a realization of Strategy 2c 
(Express Enthusiasm, Elation, Satisfaction) which 
is tied to either the start or the end of lengthy prob-
lem solving steps in the learning activity such as 
calculating the outcome of certain design choices 
made by the students during the learning activity. 
 
1. Showing Solidarity 
Raises other's status, gives help, reward 
1a. Do Introductions 
Introduce and ask names of all participants 
1b. Be Protective & Nurturing 
Discourage teasing 
1c. Give Re-assurance 
When student is discontent, asking for help 
1d. Complement / Praise 
To acknowledge student contributions 
1e. Encourage 
When group or members are inactive 
1f. Conclude Socially 
 
2. Showing Tension Release 
Jokes, laughs, shows satisfaction 
2a. Expression of feeling better 
After periods of tension, work pressure 
2b. Be cheerful 
2c. Express Enthusiasm, Elation, Satisfaction 
On completing significant steps of the task 
 
3. Agreeing 
Shows passive acceptance, understands, 
concurs, complies 
3a. Show Attention 
To student ideas as encouragement 
3b. Show Comprehension / Approval 
To student opinions and orientations 
Table 1. Social Interaction Strategies for three  
social-emotional interaction categories 
3 Effect of Social Behaviors 
We have observed the the use of social behavior in 
collaborative learning settings can help in improv-
ing learning outcomes (Kumar et. al., 2010a). This 
observation is not necessarily consistent with intui-
tion. On the one hand, we could attribute this to the 
ability to manage the user?s (student) attention to-
wards the agent (tutor) by displaying the social 
behavior. At the same time, we could argue against 
the use of social behavior by considering it as a 
potential distraction from the task. In an earlier 
experiment (Kumar et. al., 2010c), we found that it 
is important to keep the amount of social behavior 
at an optimal level in order to observe the learning 
228
benefits of the social behavior and to avoid its dis-
traction costs. 
 
1 Tutor Are you paying attention to this? 
2 S006 of course 
3 S001 yes 
4 Tutor 
What would happen if the stress in 
our Wrench is too high? 
5 S006 fail 
6 S001 it'd fail 
7 S014 it would break. 
8 Tutor That's right. The wrench can break! 
9 Tutor 
A wrench that can break in normal 
use is not safe! 
10 S006 :( 
11 Tutor 
In general, to keep a wrench safe, 
we want to keep the stress in the 
wrench high or low? 
12 S006 lowwww 
13 S001 low 
14 Tutor Right, stress should be kept low. 
15 Tutor 
Which material property deter-
mines when the stress is too high? 
16 S001 yield strength? 
 
Table 2. Excerpt of a Conceptual Tutoring Episode 
 
In this section, we present a corpus analysis to 
discover an underlying model of how the social 
behavior affects learning outcomes. We used a 
corpus of collaborative learning interactions be-
tween teams of three or more students and a tutor. 
The interactions involve the teams performing a 
learning activity which lasts approximately 40 mi-
nutes. During this time, the tutors provide direc-
tions, hints and conceptual tutoring. Each team 
undergoes up to four conceptual tutoring episodes. 
An episode is a system-initiated conversation dur-
ing which the tutor leads the students through a 
directed line of reasoning to help them reflect upon 
a concept related to the learning activity. An ex-
cerpt of a tutoring episode discussing the relation-
ship between stress and safety is shown in Table 2. 
3.1 Coding Tutoring Episodes 
Each turn in all the tutoring episodes of the 32 inte-
ractions between a team of students and an auto-
mated tutor were annotated using a coding scheme 
described here. The tutor turns were categorized as 
either Respondable (TR) if the students were ex-
pected to the respond to that tutor turn or Not Res-
pondable (TU) otherwise. In Table 2, all the 
shaded turns are labeled as Respondable. 
 
 
Figure 1. Venn Diagram of Episode Turn Annotations 
 
Student Turns are categorized into one of three 
categories. Good turns (SG) identifies turns where 
the students are showing attention to a respondable 
tutor turn (e.g. Turn 2 & 3 in Table 2) or the stu-
dents are giving a correct or an incorrect response 
to a direct question by the tutor (e.g. Turns 5, 6, 7, 
12, 13 & 16). Counterproductive (Bad) student 
turns (SB) include students abusing the tutor or 
ignoring the tutor (e.g. talking to another student 
when the students are expected to respond to a tu-
tor turn). Student turns that are not categorized as 
Good or Bad are labeled as Other (SO). Turn 10 is 
an example of SO because it is a response to a tu-
tor turn (9) where no student response is expected. 
Figure 1 shows a Venn diagram of the different 
annotations. All five categories are mutually exclu-
sive. 
3.2 Structural Equation Modeling 
In order to discover an underlying model of how 
the use of social behavior affects student learning, 
we used a structural equation modeling (SEM) 
technique (Scheines et. al., 1994). 
Data: To measure learning outcomes, our data 
comprised of scores from pre-test and post-test 
administered to 88 students who were part of the 
32 teams whose data was annotated for this analy-
sis. We normalized the number of Good (SG) and 
Bad (SB) student turns by the number of Respond-
able (TR) tutor turns and included normalized SG 
(nSG) and normalized SB (nSB) as measures of 
interaction characteristics of each student in our 
dataset. Total number of social turns performed by 
the tutor in each interaction was included as a cha-
racteristic of social behavior displayed by the tutor. 
Finally, the total amount of time (in seconds) that 
229
the students spent on the tutoring episodes was 
included as a characteristic of the interaction quali-
ty during the tutoring episodes. 
Prior Knowledge: The only prior knowledge 
input to the model stated that the pre-test occurs 
before the post-test. 
Discovered Models: We used Tetrad IV to dis-
cover a structural equation model in the data com-
prising of 6 fields (PreTest, PostTest, nSG, nSB, 
SocialTurns, EpisodeDuration) for each of the 88 
students. Figure 2 shows the structural equation 
model discovered by Tetrad using the dataset de-
scribed above. p-Value of 0.46 for this model con-
firms the hypothesis used by Tetrad for its 
statistical analysis i.e. the model was not discov-
ered randomly. Note that unlike other statistical 
tests, SEM models built using Tetrad are evaluated 
as significant if the p-Value is greater than 0.05. 
The numbers on the arrows are correlation coeffi-
cients and the numbers on the boxes indicate mean 
values for each variable. 
 
 
 
Figure 2. SEM discovered using all 6 variables in our 
dataset 
 
Besides the obvious causal effect of PreTest 
score on PostTest score, we find that as the dura-
tion of the tutoring episodes (EpisodeDuration) 
increases, the learning outcomes deteriorate. We 
notice that an increase in the normalized number of 
Bad student turns increases EpisodeDuration indi-
cating that students who abuse or ignore the tutor 
are likely to not pay attention to the learning con-
tent presented during the tutoring episodes, hence 
prolonging the tutoring episode as the tutor tries to 
get the students through the instructional content. 
Furthermore, we observe that social behavior helps 
in counteracting the negative learning effect of Bad 
interaction behaviors of the students. Tutors that 
perform social behavior are capable of managing 
the student?s attention and get the students through 
the tutoring episode faster. 
3.3 Discussion 
The SEM analysis discussed in the previous sec-
tion helps us better understand the relationship be-
tween the use of social behavior and student 
learning in a collaborative learning setting. Let?s 
consider the duration of the tutoring episodes as an 
indicator of the students? attention to the tutor 
(higher duration lower attention). We see that 
social behavior helps in managing the students? 
attention, which may be affected negatively by 
counterproductive/bad interaction behavior from 
the students. 
Besides suggesting that social behavior could be 
a useful strategy for directing student attention, it 
also suggests that social behavior may not serve 
this function where counterproductive student be-
havior is not present or where it does not occur 
enough to negatively impact task behavior. This is 
because a minimum amount of time needs to be 
spent on each tutoring episode to deliver the in-
structional of the concept being discussed. In the 
absence of counterproductive student behavior, 
episode duration may be close to that minimum. 
Also, in an earlier analysis (Hua et. al., 2010) in 
a different learning domain where the social beha-
viors described in Section 2 were employed, we 
have observed that the number of abusive/negative 
comment made by the students about the tutor dur-
ing the interaction were significantly higher in a 
condition where the tutors performed a high 
amount of social behavior. This suggests that the 
relationship between the SocialTurns and Episo-
deDuration variables may not be linear in extreme 
cases and emphasizes the importance of perform-
ing an optimal amount of social behavior. 
4 Triggering Social Behavior 
Aside from designing, implementing and regulat-
ing the amount of social behavior performed by 
automated tutors, one of the challenges involved in 
the appropriate use of social interaction strategies 
is that of triggering these strategies only at the 
most appropriate moments during the interaction. 
Our initial implementation of these strategies 
(Kumar & Ros?, 2010b) achieved this using a set 
230
of hand crafted rules that used features such as re-
cent student turns, state of the tutoring plan, etc. 
Here we will summarize our efforts on building 
a better triggering policy using a data-driven ap-
proach that models the behavior of human tutors at 
triggering the social interaction strategies listed in 
Table 1. Using a corpus of 10 interactions between 
a group of students and partially automated tutors 
whose social behaviors were triggered by human 
tutors, we attempt to learn a triggering policy that 
predicts when the human tutors will trigger a social 
strategies. Currently, we focus on only learning a 
triggering policy that determines if a social beha-
vior should be performed. The choice of which 
behavior is performed when triggered by the policy 
is still based on the rules used in our earlier im-
plementation as discussed in Section 5.3. 
In order to compare the triggers generated by a 
policy, we use a binary sequence comparison me-
tric called kKappa (Neikrasz & Moore, 2010) de-
veloped for evaluating discourse segmentation 
approaches. The metric allows a soft penalty for 
misplacing a trigger (or a segment boundary) with-
in a window of k turns. 
We developed a large margin learning algo-
rithm following McDonald et. al. (2005) that itera-
tively learns the coefficients of a linear function in 
the feature space that separates turns where human 
tutors decided to trigger a social behavior from the 
rest of the turns. Instead of using an instance-based 
objective function (like square-loss), our algorithm 
maximizes the kKappa metric over a provided 
training set. The function learnt this way can be 
used as a triggering policy by using it at every turn 
during an interaction to predict if a human tutor 
would trigger a social behavior. We used a collec-
tion of automatically extractable features that 
represent the lexical and semantic content of recent 
student and tutor turns, current discourse state and 
activity levels of the students. 
While details of the objective evaluation of the 
various learnt triggering policies is beyond the 
scope of this paper, we found that the best per-
forming strategy (k-? = 0.13) was significantly bet-
ter than a random baseline (k-? = 0.01) as well as 
the rule based triggering policy (k-? = -0.09) used 
in our initial implementation. Also, the policy 
learnt by our algorithm outperformed policies 
learnt by algorithms such as Linear Regression (k-
? = 0.00) and Logistic Regression (k-? = 0.05) that 
use instance-based loss metrics (Hall et. al., 2009). 
5 User Study 
Here we will present an experiment we conducted 
to evaluate the effectiveness of various ways to 
trigger social behavior discussed in Section 4. This 
experiment is a step towards verifying the hypo-
thesis that a human-like triggering policy could 
outperform a rule-based triggering policy that was 
used in our earlier experiments (Kumar et. al., 
2010a). We use the same interactive situation for 
the experiment presented here as in our earlier 
work. Freshmen mechanical engineering students 
enrolled at an American university participate in a 
computer-aided engineering lab that is divided into 
three parts, i.e., Computer-Aided Design (CAD), 
Computer-Aided Analysis (CAA) and Computer-
Aided Manufacturing (CAM). Students practice 
the use of various engineering software packages 
for all three parts as they design, analyze and man-
ufacture an Aluminum wrench. Our experiment is 
conducted during the second part (CAA) of the lab. 
5.1 Procedure & Materials 
The Computer-Aided Analysis lab comprises of 
two activities. The first activity involves analyzing 
a wrench design given to the students by specify-
ing certain loading conditions and simulating the 
stresses and deformations in the wrench. Students 
are led by a teaching assistant during this activity. 
They spend approximately 25 minutes performing 
this activity. At the end of the analysis activity, the 
students see a simulation of the stress distribution 
in the body of the wrench. 
After the analysis activity, a pre-test is adminis-
tered. Each student spends 10 minutes working on 
the pre-test individually. The pre-test comprises of 
11 questions, 8 of which are multiple-choice ques-
tions and the other 3 are short essay type questions. 
The second activity of the CAA lab is a colla-
borative design activity. During this activity, stu-
dents work in teams of three. Student in the same 
team are seated in separate parts of the lab and can 
only communicate using a text-based chatroom 
application (M?hlpfordt and Wessner, 2005). The 
chatroom application also provides a shared work-
space in the form of a whiteboard. 
After the pre-test, students are given written in-
structions describing the collaborative design ac-
tivity. The instructions ask the students to design a 
better wrench in terms of ease of use, cost of mate-
rials and safety compared to the wrench they ana-
231
lyzed earlier. The students are expected to come up 
with three new designs in 40 minutes by varying 
parameters like dimensions and materials of the 
wrench. The instructions also include various for-
mulae and data that the students might need to use 
for their designs. Besides course credit, the instruc-
tions mention an additional giftcard for the team 
that comes up with the best design ($10 for each 
member of the winning team). 
Students are asked to log in to their respective 
team?s chatroom. They spend the next 40 minutes 
working on the collaborative design activity. Be-
sides the three students, the chatroom for each 
team includes an automated tutor. The tutor guides 
the students through the first two designs suggest-
ing potential choices for dimension and materials 
for each design. As the design activity progresses, 
the tutor initiates four conceptual tutoring episodes 
to help the students reflect upon underlying me-
chanical engineering concepts like stress, force, 
moment, safety, etc., that are relevant to the design 
activity. 
Our experimental manipulation happens during 
this 40 minute segment. The tutor in each team?s 
chatroom is configured to perform social behavior 
using different triggering policies as specified by 
the condition assigned to the team. The conditions 
are discussed in the next section. Irrespective of 
the condition, each team receives the 4 conceptual 
tutoring episodes. Every student performs all the 
steps of this procedure like all other students. 
At the end of the collaborative design activity, a 
post-test and a survey are administered. Students 
are asked to spend 15 minutes to first complete the 
test and then the survey. The post-test is the same 
test used for pre-test. The survey comprises of 15 
items shown in Appendix B. The students are 
asked to rate each item on a 7-point Likert scale 
ranging from Strongly Disagree (1) to Strongly 
Agree (7). The 15 items on the survey include 11 
items eliciting perception of the tutor. 9 of the 11 
items state positive aspects of the tutor (e.g. ?tutor 
was friendly?). The other 2 items stated negative 
aspects about the tutor (e.g. ?tutor?s responses got 
in the way?). Besides the items about the tutor, 2 
items elicited the student?s rating about the colla-
borative design activity. The last 2 items were 
about the student?s satisfaction with their perfor-
mance on the design task. 
In total, both the activities that are part of the 
CAA lab take approximately 1 hour 40 minutes. 
5.2 Experimental Design 
The teams participating in the experiment de-
scribed here were divided into six conditions. 
These conditions determined the triggering policy 
and the amount of social behavior performed by 
the automated tutors. Tutors in the None condition 
did not perform any social behavior. Tutors in the 
Rules condition used the same hand crafted rule-
based triggering policy employed in our earlier 
experiment (Kumar et. al., 2010a). Following the 
results from another experiment (Kumar & Ros?, 
2010c), the automated tutors in the Rules condition 
performed a moderate amount of social behavior 
(atmost 20% of all tutor turns). On average, the 
Rules policy triggered 25 social turns per interac-
tion. 
The RandomLow and RandomHigh condi-
tions used a random triggering policy with a social 
ratio filter to regulate the amount of social beha-
vior. In both the random conditions, the tutor 
would trigger social behavior using a random 
number generator to generate the confidence of 
triggering a social behavior after every turn (by a 
student or a tutor). In the RandomLow condition, a 
behavior would be triggered if the confidence was 
above 0.91. In the RandomHigh condition, a beha-
vior would be triggered if the confidence was 
above 0.85. On average, the RandomLow condi-
tion had 23 behaviors triggered per interaction. 
About 37 behaviors were triggered in the Ran-
domHigh condition. 
The LearntLow and LearntHigh conditions 
used the best triggering policy learnt from a corpus 
of human triggering of social behavior as discussed 
in Section 4. The same social ratio filter used in the 
random conditions was used in these two condi-
tions also. As in the case with RandomLow and 
RandomHigh, different values of a confidence pa-
rameter were used for the LearntLow and Learn-
tHigh conditions to control the number of social 
behaviors triggered. On average, the LearntLow 
condition had 22 triggers and the LearntHigh con-
dition had 28 triggers. 
5.3 Generating Behaviors 
The various triggering policies described above for 
each of our experimental conditions only deter-
mine when a tutor agent will perform a social be-
havior. In order to perform the social behavior in 
actual use, the agent must not only determine when 
232
a behavior should be triggered, but also determine 
which behavior should be performed when a trig-
ger is received. Our implementation of the tutor 
agent used in this experiment provides a conti-
nuous stream of scores for each of the eleven so-
cial interaction strategies that the tutor can 
perform. The scores are computed using hand-
crafted functions that use the same features used in 
our rule-based triggering policy (Kumar et. al., 
2010b). When a social behavior is triggered, a rou-
lette wheel selection is used to determine the strat-
egy to be performed. The circumference of the 
wheel assigned to each strategy is proportional to 
the score of each strategy. If the score of all the 
strategies is zero, a generic social prompt is per-
formed. 
6 Results 
126 students enrolled in an introductory mechani-
cal engineering course at an American university 
participated in the experiment described in this 
paper. The experiment was conducted on two sepa-
rate days separated by one week. On each day, four 
sessions of the Computer-Aided Analysis lab were 
conducted, and students attended only one as-
signed session. Session assignment was made 
based on an alphabetic split. The 126 students were 
divided into 42 teams. 20 teams participated on the 
first day of the experiment. They were evenly split 
into four conditions (None, Rules, RandomHigh & 
LearntHigh). The remaining 22 teams participated 
on the second day. Out of these, 5 teams each were 
assigned to the None and RandomLow condition. 6 
teams each were assigned to the Rules and 
LearntLow conditions. 
The rest of this section presents detailed results 
and analysis of this experiment. To summarize, we 
found that out of the six evaluated policies only the 
LearntLow policy that uses a triggering model 
learnt from human triggering data and generates a 
moderate amount of social behavior is consistently 
better than the other policies in terms of both per-
formance as well as perception outcomes. Also, the 
LearntLow policy is found to be most efficient at 
delivering the instructional content as indicated by 
the smallest EpisodeDuration in Table 5. 
6.1 Learning Outcomes 
The learning outcomes analysis presented here 
shows the advantage of using a triggering policy 
learnt from a corpus of human triggering behavior 
along with a filtering technique that regulates the 
amount of social behavior as shown in Table 3. 
We first verified that there was no significant 
difference between the six conditions on the pre-
test scores. As in the case of previous experiments 
using this learning activity, we saw that the learn-
ing activity was pedagogically beneficial to the 
students irrespective of the condition. There was a 
significant improvement in test scores between 
pre-test and post-test { p < 0.0001, F(1,250) = 
26.01, effect-size = 0.58? }. 
There was no significant effect of the condition 
assigned to each team on the total test scores. 
However, there was a significant effect on the test 
scores of short-essay type questions using the pre-
test score as a covariate and the condition as a fac-
tor { p < 0.05, F(5, 119) = 2.88 }. The adjusted 
post test scores for the short essay type questions 
and their standard deviations are shown in Table 3. 
Post-hoc analysis showed that the LearntLow con-
dition was significantly better than LearntHigh 
condition { effect-size = 0.65? }. Also, Random-
Low condition was marginally better than Learn-
tHigh condition { p < 0.07, effect-size = 0.62? }. 
 
 
Mean St.Dev. 
LearntLow 5.12 0.54 
RandomLow 5.06 0.67 
None 4.75 1.13 
RandomHigh 4.59 1.09 
Rules 4.38 0.89 
LearntHigh 3.98 1.74 
 
Table 3. Mean and Standard Deviation of Adjusted Post 
Test Scores for Short Essay Type Questions 
 
This result further supports the observation 
from our earlier experiment (Kumar & Ros?, 
2010c) which demonstrated that importance of per-
forming the right amount of social behavior. Both 
RandomLow and LearntLow conditions employ 
the non-linear social ratio filter which keeps the 
amount of allowed social behavior at a level com-
parable to the amount of social behavior performed 
by human tutors. 
Since the primary objective of the experiment 
described here was to evaluate a learnt triggering 
policy with respect to a rule-based triggering poli-
cy, we repeated the ANCOVA for the short essay 
type question using data from only the Rules, 
233
LearntLow and LearntHigh conditions. We found a 
significant effect of condition on the post-test score 
using pre-test score as a covariate { p = 0.01, 
F(2,62) = 4.98 }. A post-hoc analysis showed that 
the LearntLow condition was significantly better 
than the LearntHigh condition as above and  the 
LearntLow condition was marginally better than 
the Rules condition { p ? 0.08, effect-size = 0.84? 
}. We observe that a triggering policy learnt from 
human triggering behavior can achieve a marginal 
improvement on learning outcomes compared to 
our existing rule-based triggering policy. This is 
consistent with our hypothesis. 
6.2 Perception Ratings 
We averaged the student?s rating for the 11 items 
about the tutor into a single tutor rating measure 
used here. Rating on the two negative statements 
about the tutor were inverted (7?1, 6?2, and so 
on) for this calculation.   
 
 
Mean St.Dev. 
Rules 4.74 1.45 
LearntLow 4.56 1.58 
None 4.42 1.49 
RandomHigh 3.74 1.63 
LearntHigh 3.55 1.26 
RandomLow 3.18 0.91 
 
Table 4. Mean and Standard Deviation of Tutor Ratings 
 
We found a significant effect of condition on 
the tutor ratings { p < 0.01, F(5,120) = 3.83 }. Ta-
ble 4 shows the mean and standard deviations of 
tutor ratings for each condition. Post-hoc analysis 
showed that only the Rules condition was signifi-
cantly better than the RandomLow condition. Also, 
we found that Rules was marginally better than 
LearntHigh condition { p < 0.08 } and both Learnt-
Low and None conditions was marginally better 
than RandomLow condition { p < 0.08 }. 
While we did not see a significant improvement 
in perception due the use of a learnt triggering pol-
icy when compared to a rule-based triggering poli-
cy, we find an advantage over using a random 
triggering policy (RandomLow) which was as 
good as a learnt policy on the learning outcomes. 
The results from the tutor?s perception ratings fur-
ther support the importance of timing and regulat-
ing the amount of social behavior. 
We did not find any significant effect of condi-
tion on the ratings about the design activity or stu-
dent?s task satisfaction. 
6.3 Analysis of Tutoring Episodes 
In order to understand the results from the experi-
ment presented in this paper, we applied the struc-
tural equation model discussed earlier (Figure 2) to 
the data collected from our current experiment. 
Figure 3 shows the model for our current experi-
ment (p=0.4492). Only four variables were used 
because the annotations of good and bad student 
behavior are not available at this time. 
 
 
 
Figure 3. SEM applied to data from this experiment 
 
 
Mean St.Dev. 
RandomHigh 540.80 49.50 
LearntHigh 534.80 61.00 
None 523.88 41.54 
Rules 519.80 102.70 
RandomLow 519.20 74.40 
LearntLow 484.00 69.80 
 
Table 5. Mean and Standard Deviation of Duration 
of Tutoring Episodes 
 
We see that most of the model parameters (p-
Value, means & correlations) are similar to para-
meters for the model shown in Figure 2. However, 
the correlation between SocialTurns and Episode-
Duration is much smaller. Also, note that the mean 
of EpisodeDuration is smaller compared to that in 
Figure 2 which indicates that lesser counterproduc-
tive behavior was displayed by the students in this 
experiment. The conceptual tutoring episodes are 
operating closer to the minimum episode duration 
which leaves a smaller room for improvement by 
234
the use of social interaction strategies. As dis-
cussed in Section 3.3, this explains the smaller cor-
relation between SocialTurns and EpisodeDuration 
in Figure 3. 
Table 5 shows the mean and standard deviations 
of the duration of tutoring episodes for each condi-
tion. Even though the differences are not signifi-
cant, the LearntLow policy has the lowest duration 
indicating higher student attention than the other 
conditions. 
7 Discussion 
Prior work in the field of human-human interaction 
and human-machine interaction in the form of di-
alog systems has emphasized the importance of 
timing the display of behavior to achieve natural 
and/or productive interactions. In general, timing 
of interactive behaviors (verbal as well as non-
verbal) has been studied in the context of joint ac-
tivities being performed by the participants. Beha-
viors are timed to achieve and maintain 
coordination between the participants (Clark, 
2005). Specifically, among other topics, timing of 
low-level (signal) interaction like turn-taking has 
been the subject of several investigations (Raux & 
Eskenazi, 2008; Takeuchi et. al., 2004). 
On the other hand, the use of social behavior by 
conversational agents to support students has been 
proposed (Veletsianos et. al., 2009; Gulz et. al., 
2010). Work in the area of affective computing and 
its application to tutorial dialog has focused on 
identification of student?s emotional states and us-
ing those to improve choice of behavior performed 
by tutors (D?Mello et. al., 2005). Our prior work 
(Kumar et. al., 2010; Kumar et. al., 2007) has 
shown that social behavior motivated from empiri-
cal research in small group communication (Bales, 
1950) can help in effectively supporting students in 
collaborative learning settings. Use of social inte-
raction in other applications of conversational 
agents besides education has been investigated 
(Bickmore et. al., 2009; Dybala et. al., 2009; Doh-
saka et. al., 2009). 
The experiments presented here bridges these 
two tracks of research specifically proposing a so-
lution to the challenge of timing social behavior in 
the context of a supporting collaborative learning. 
Compared to the work on timing signal-level joint 
activities like turn-taking, this work focuses on the 
timing of joint activities at the conversation level. 
The success of our algorithm at learning a model 
of timing conversational behaviors in the context 
of an interactive task could potentially offer a gen-
eral approach for realizing such behaviors in other 
conversational agents. 
8 Conclusion  
In this paper, we presented an experiment that 
compared the effectiveness of several social beha-
vior triggering policies. Specifically, we compared 
a triggering policy learnt from a corpus of human 
triggering behavior to a rule-based policy which 
has previously been shown to be successful at trig-
gering effective social behavior in a collaborative 
learning activity. 
The presented experiment provides further evi-
dence in support of the intuition that timing of so-
cial behavior and regulating the amount of social 
behavior are critical to improving performance and 
perception outcomes. A triggering policy based on 
human-like timing in combination with a filter that 
attempts to keep amount of social behavior at the 
same level as human tutors was shown to be mar-
ginally better than the rule-based policy on learn-
ing outcomes. Also, on perception measures, we 
found that the human-like policy is marginally bet-
ter than a random triggering policy which uses the 
same filter to control the amount of social beha-
vior.  Only the learned model provides a win both 
on learning and on perception measures. 
In order to better understand the effect of use of 
social behavior by automated tutors on student?s 
learning outcomes, we presented a structured mod-
el which suggests that social behavior helps in 
achieving higher learning outcomes by allowing 
the tutor to better manage the student?s attention. 
Following this model, we saw that a human-like 
triggering policy is able to achieve higher student 
attention as indicated by the smaller duration of 
tutoring episodes. 
We found a significant negative correlation { 
coefficient = -0.20, p < 0.05 } between the tutor?s 
perception rating and number of social behaviors 
triggered when none of the social interaction strat-
egies were applicable. As next steps, our best trig-
gering policy could be potentially further refined 
by achieving a closer integration of the triggering 
model with the social behavior generation mechan-
ism to prevent triggering when none of the eleven 
strategies could be generated. 
235
References  
Hua Ai, Rohit Kumar, Dong Nguyen, Amrut Nagasund-
er and Carolyn P. Ros?, 2010, Exploring the Effec-
tiveness of Social Capabilities and Goal Alignment in 
Computer Supported Collaborative Learning, Intelli-
gent Tutoring Systems, Pittsburgh, PA 
Elizabeth Arnott, Peter Hastings and David Allbritton, 
2008, Research Methods Tutor: Evaluation of a di-
alogue-based tutoring system in the classroom, Be-
havior Research Methods, 40 (3), 694-698 
Robert F. Bales, 1950, Interaction process analysis: A 
method for the study of small groups, Addison-
Wesley, Cambridge, MA 
Khelan Bhatt, Martha Evens, Shlomo Argamon, 2004, 
Hedged responses and expressions of affect in hu-
man/human and human/computer tutorial interac-
tions, CogSci, Chicago, IL 
Timothy Bickmore, Daniel Schulman and Langxuan 
Yin, 2009, Engagement vs. Deceit: Virtual Humans 
with Human Autobiographies, Proc. of Intelligent 
Virtual Agents, Amsterdam, Netherlands 
Herbert H. Clark, 2005, Coordinating with each other in 
a material world, Discourse Studies, 7 (4-5), 507-525 
Sidney K. D?Mello, Scotty D. Craig, Barry Gholson, 
Stan Frankin, Rosalind Picard, Arthur C. Graesser, 
2005, Integrating Affect Sensors in an Intelligent Tu-
toring System, Wksp on Affective Interactions: The 
Computer in the Affective Loop, IUI, San Diego, CA 
Pawel Dybala, Michal Ptaszynski, Rafal Rzepka and 
Kenji Araki, 2009, Humoroids: Conversational 
Agents that induce positive emotions with humor, 
AAMAS, Budapest, Hungary 
Kohji Dohsaka, Ryoto Asai, Ryichiro Higashinaka, Ya-
suhiro Minami and Eisaku Maeda, 2009, Effects of 
Conversational Agents on Human Communication in 
Though Evoking Multi-Party dialogues, SIGDial 
2009, London, UK 
Agneta Gulz, Annika Silvervarg and Bj?rn Sj?d?n, 
2010, Design for off-task interaction - Rethinking 
pedagogy in technology enhanced learning, Intl. 
Conf. on Advanced Learning Technologies, Tunisia 
Arthur C. Graesser, Patrick Chipman, Brian C. Haynes, 
and Andrew Olney, 2005, AutoTutor: An Intelligent 
Tutoring System with Mixed-initiative Dialogue, 
IEEE Transactions in Education, 48, 612-618 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann and Ian H. Witten, 
2009, The WEKA Data Mining Software: An Up-
date; SIGKDD Explorations, Volume 11, Issue 1 
Rohit Kumar, Jack L. Beuth and Carolyn P. Ros?, 2011, 
Conversational Strategies that Support Idea Genera-
tion Productivity in Groups, 9th Intl. Conf. on Com-
puter Supported Collaborative Learning, Hong Kong 
Rohit Kumar, Hua Ai, Jack Beuth and Carolyn P. Ros?, 
2010a, Socially-capable Conversational Tutors can 
be Effective in Collaborative-Learning situations, In-
telligent Tutoring Systems, Pittsburgh, PA 
Rohit Kumar and Carolyn P. Ros?, 2010b, Engaging 
learning groups using Social Interaction Strategies, 
NAACL-HLT, Los Angeles, CA 
Rohit Kumar and Carolyn P. Ros?, 2010c, Conversa-
tional Tutors with Rich Interactive Behaviors that 
support Collaborative Learning, Workshop on Op-
portunities for intelligent and adaptive behavior in 
collaborative learning systems, ITS 2010, Pittsburgh, 
PA 
Rohit Kumar, Gahgene Gweon, Mahesh Joshi, Yue Cui 
and Carolyn Ros?, Supporting students working to-
gether on Math with Social Dialogue, Workshop on 
Speech and Language Technology in Education, 
Farmington, PA, 2007 
Ryan McDonald, Koby Crammer and Fernando Pereira, 
2005a, Online large-margin training of dependency 
parsers, Proc. of ACL, 91-98, Ann Arbor, MI 
Martin M?hlpfordt and Martin Wessner, 2005, Explicit 
referencing in chat supports collaborative learning, 
Proc. Computer Support for Collaborative Learning 
(CSCL) 
John Niekrasz and Johanna D. Moore, 2010, Unbiased 
Discourse Segmentation Evaluation, Proc. of SLT, 
Berkeley, CA 
Antoine Raux and Maxine Eskenazi, 2008, Optimizing 
Endpointing Thresholds using Dialogue Features in a 
Spoken Dialogue System, SIGDial 2008, Columbus, 
OH 
Richard Scheines, Peter Spirtes, Clark Glymour, and 
Christopher Meek, 1994, TETRAD II: Tools for Dis-
covery, Lawrence Erlbaum Associates, Hillsdale, NJ 
 Masashi Takeuchi, Norihide Kitaoka and Seiichi Naka-
gawaM, 2004, Timing detection for realtime dialog 
systems using prosodic and linguistic information, 
Intl Conf. on Speech Prosody 2004, Nara, Japan 
Tetrad IV, 
http://www.phil.cmu.edu/projects/tetrad/tetrad4.html 
George Veletsianos, Charles Miller and Aaron Doering, 
Veletsianos, 2009, EnALI: A Research and Design 
Framework for Virtual Characters and Pedagogical 
Agents, Journal of Educational Computing Research, 
41(2), 171-194 
236
Appendix A. Excerpts of an interaction between three students and an automated tutor demonstrating 
some of the social interaction strategies listed in Table 1. 
 
7 Tutor Hi, I am your tutor for today's lab. 
8 Tutor Lets introduce ourselves briefly. I am Avis. 
9 S083 Hey Avis! I'm _name1_ 
10 S073 Im _name2_ 
11 S089 i'm _name3_ 
12 Tutor Its nice to meet you all. :) 
13 Tutor Let's get started by reviewing the base design in your worksheet. 
 
Strategy 1a: Do Introductions 
 
48 S083 R we using the same material? 
49 S073 I assume so 
50 S073 just changing the length 
51 S089 yeah we have multiple design steps 
52 S089 so probably for now 
53 Tutor cool :) 
54 S083 O ok 
 
Strategy 3b: Show Comprehension / Approval 
 
119 Tutor Is this a safe wrench? 
120 S073 and then is the same for design 2 
121 S073 so yes the wrench is safe 
122 Tutor _name1_ ... any thoughts you'd like to contribute? 
123 S083 its a safer wrench if its in steel 
 
Strategy 1e: Encourage (inactive members) 
 
143 S073 high 
144 Tutor Right, higher yield stress is better. 
145 S089 so steel or titanium 
146 S089 but have fun paying for that... 
147 Tutor All other things being equal, you want to choose a stronger material. 
148 Tutor It's good to have your contributions to the discussion :-) 
149 S073 yay 
 
Strategy 1d: Complement / Praise 
 
150 S073 :) 
151 Tutor Let's improve design 1 by using Steel for our 2nd _truncated_ 
152 Tutor I am happy to be working with our team 
153 S083 thanks :-) 
154 Tutor You can start calculating the fields in the worksheet _truncated_ 
155 S089 woo... 
 
Strategy 2c: Express Enthusiasm, Elation, Satisfaction 
237
Appendix B. Survey administered to the participants at the end of the Collaborative Design Activity 
 
Using the following scale, Indicate to what extent you agree with each of the following items. 
 
1 2 3 4 5 6 7 
Strongly 
Disagree 
Mostly 
Disagree 
Somewhat 
Disagree 
Neutral Somewhat 
Agree 
Mostly 
Agree 
Strongly 
Agree 
 
 
 
The tutor was part of my team. 1 2 3 4 5 6 7 
The tutor provided good ideas for the discussion. 1 2 3 4 5 6 7 
The tutor received my contributions positively. 1 2 3 4 5 6 7 
The tutor was friendly during the discussion. 1 2 3 4 5 6 7 
The tutor responded to my contributions. 1 2 3 4 5 6 7 
The tutor helped in lowering the tension in my group. 1 2 3 4 5 6 7 
The tutor was paying attention to our conversation. 1 2 3 4 5 6 7 
Overall, I liked the tutor very much. 1 2 3 4 5 6 7 
I think the tutor was as good as a human tutor. 1 2 3 4 5 6 7 
I often ignored what the tutor was saying. 1 2 3 4 5 6 7 
The tutor's responses got in the way of our conversation. 1 2 3 4 5 6 7 
The design challenge was exciting. 1 2 3 4 5 6 7 
I did my best to come up with good designs. 1 2 3 4 5 6 7 
I am happy with the discussion I had with my group. 1 2 3 4 5 6 7 
Overall, we were successful at meeting our goals during the design challenge. 1 2 3 4 5 6 7 
 
 
238
Proceedings of the SIGDIAL 2013 Conference, pages 142?144,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Interactive Error Resolution Strategies for  
Speech-to-Speech Translation Systems 
 
 
Rohit Kumar, Matthew Roy, Sankaranarayanan Ananthakrishnan,  
Sanjika Hewavitharana, Frederick Choi 
Speech, Language and Multimedia Business Unit 
Raytheon BBN Technologies 
Cambridge, MA, USA 
{rkumar, mroy, sanantha, shewavit, fchoi}@bbn.com 
 
  
 
Abstract1 
In this demonstration, we will showcase 
BBN?s Speech-to-Speech (S2S) transla-
tion system that employs novel interac-
tion strategies to resolve errors through 
user-friendly dialog with the speaker. 
The system performs a series of analysis 
on input utterances to detect out-of-
vocabulary (OOV) named-entities and 
terms, sense ambiguities, homophones, 
idioms and ill-formed inputs. This analy-
sis is used to identify potential errors and 
select an appropriate resolution strategy. 
Our evaluation shows a 34% (absolute) 
improvement in cross-lingual transfer of 
erroneous concepts in our English to Ira-
qi-Arabic S2S system. 
1 Introduction 
Great strides have been made in Speech-to-
Speech (S2S) translation systems that facilitate 
cross-lingual spoken communication (Stallard et. 
al., 2011). However, in order to achieve broad 
domain coverage and unrestricted dialog capabil-
ity, S2S systems need to be transformed from 
passive conduits of information to active partici-
pants in cross-lingual dialogs. These active par-
ticipants must detect key causes of communica-
tion failures and recover from them in an effi-
cient, user-friendly manner. 
                                                 
Disclaimer: This paper is based upon work supported by the 
DARPA BOLT Program. The views expressed are those of 
the authors and do not reflect the official policy or position 
of the Department of Defense or the U.S. Government. 
 
Distribution Statement A (Approved for Public Release, 
Distribution Unlimited) 
Our ongoing work on eyes-free S2S systems is 
focused on detecting three types of errors that 
affect S2S systems. First, out-of-vocabulary 
(OOV) words are misrecognized as phonetically 
similar words that do not convey the intended 
concept. Second, ambiguous words such as hom-
ophones and homographs often lead to recogni-
tion and translation errors. Also, unseen idioms 
produce erroneous literal translations. Third, user 
errors such as mispronunciations and incomplete 
utterances lead to ASR errors. We will demon-
strate our interactive error resolution strategies to 
recover from each of these error types. 
Section 2 presents our system architecture. 
Section 3 describes nine interactive error resolu-
tion strategies that are the focus of this demon-
stration. An evaluation of our English to Iraqi-
Arabic S2S system is summarized in Section 4. 
2 System Architecture 
 
Figure 1 shows the architecture of our two-way 
 
 
Figure 1: BBN S2S System with Error Recovery 
in English to Iraqi-Arabic direction 
142
English to Iraqi-Arabic S2S translation system. 
In the English to Iraqi direction, the initial Eng-
lish ASR hypothesis and its corresponding trans-
lation are processed through a series of analysis 
(e.g. parsing, sense disambiguation) and error 
detection (e.g. ASR/MT confidence, Homo-
phone/Idiom/Named-Entity detection) modules. 
A detailed discussion on the various error detec-
tion modules can be found in Prasad et. al. 
(2012). A novel Inference Bridge data structure 
supports storage of these analyses in an intercon-
nected and retraceable manner. The potential 
erroneous spans are identified and ranked in an 
order of severity using this data structure. 
Based on the top ranked error, one of nine er-
ror resolution strategies (discussed in Section 3), 
is selected and executed. Each strategy is com-
posed of a sequence of steps which include ac-
tions such as TTS output, user input processing, 
translation (unconstrained or constrained) and 
other error type specific operations. This se-
quence is hand-crafted to efficiently recover 
from an error. Following a multi-expert design 
(Turunen and Hakulinen, 2003), each strategy 
represents an error-specific expert. 
3 Error Resolution Strategies 
Figure 2 illustrates the sequence of steps for the 
nine interaction strategies used by our system.  
The OOV Name and ASR Error strategies are 
designed to interactively resolve errors caused by 
OOV words (names and non-names) as well as 
other generic ASR and MT errors. When a span 
of words is identified as an OOV named-entity, 
the user is asked to confirm whether the audio 
segment corresponding to those words is a name. 
Upon user confirmation, the audio segment is 
spliced into the output target language utterance. 
This is based on the principle that audio seg-
ments containing names are understandable 
across languages. 
In the case where a generic erroneous span is 
detected, the user is asked to rephrase the utter-
ance. This strategy is suitable for handling multi-
ple error types including OOVs, mispronuncia-
tions, and generic ASR/MT errors. Additionally, 
the ASR Errors strategy has been designed to 
 
Figure 2. Interaction Strategies for Error Resolution 
143
capture a large fraction of the OOV name false 
negatives (i.e. missed detections) by allowing the 
user to indicate if the identified erroneous span is 
a name. Because of the confusability between the 
errors handled by these two strategies, we have 
found it beneficial to maintain reciprocity be-
tween them to recover from all the errors handled 
by each of these strategies. 
The four Word Sense (WS) disambiguation 
strategies resolve sense ambiguity errors. The 
underlying principle behind these strategies is 
that the sense of an ambiguous word must be 
confirmed by at least two of four possible inde-
pendent sources of evidence. These four sources 
include (a) the translation system (sense lookup 
corresponding to phrase pair associated with the 
ambiguous word), (b) a list of source-language 
contextual keywords that disambiguate a word, 
(c) the sense predicted by a sense-disambiguation 
model and (d) sense specified by the user. Be-
sides the objective to minimize user effort, these 
multiple sources are necessary because not all of 
them may be available for every ambiguous 
word. Case 1: No Mismatch strategy corresponds 
to the case where sources (a) and (c) agree. Case 
2: Filtered strategy corresponds to the case 
where (a) and (b) agree. In both of these cases, 
the system proceeds to present the translation to 
the Arabic speaker without performing any error 
resolution. If these three sources are unable to 
resolve the sense of a word, the user is asked to 
confirm the sense identified by source (a) as il-
lustrated in Case 3: Mismatch strategy. If the 
user rejects that sense, a list of senses is present-
ed to the user (Case 4: Backoff strategy). The 
user-specified sense then drives constrained de-
coding to obtain an accurate translation. 
Albeit simpler, the two homophone resolution 
strategies mimic the word sense disambiguation 
strategies in principle and design. The observed 
homophone variant produced by the ASR must 
be confirmed either by a homophone disambigu-
ation model (Case 1: No Mismatch) or by the 
user (Case 2: Mismatch). The input utterance is 
modified (if needed) by substituting the resolved 
homophone variant in the ASR output which is 
then translated and presented to the Arabic 
speaker. 
Strategies for resolving errors associated with 
idioms and incomplete utterances primarily rely 
on informing the user about these errors and elic-
iting a rephrasal. For idioms, the user is also giv-
en the choice to force a literal translation when 
appropriate. 
Following a mixed-initiative design, at all 
times, the user has the ability to rephrase their 
utterance as well as to force the system to pro-
ceed with the current translation. This allows the 
user to override system false alarms whenever 
suitable. The interface also allows the user to 
repeat the last system message which is helpful 
for comprehension of some of the synthesized 
system prompts for unfamiliar users. 
4 Summary of Evaluation 
Our S2S system equipped with the error resolu-
tion strategies discussed in the previous section 
was evaluated on 103 English utterances (25 
unique utterances repeated by multiple speakers). 
Each utterance was designed to elicit one of the 
error types listed in Section 1. 
The ASR word error rate for these utterances 
was 23%. The error detection components were 
able to identify 59% of these errors and the cor-
responding error resolution strategies were cor-
rectly triggered. 
The erroneous concepts in 13 of the 103 utter-
ances (12.6%) were translated without any error. 
Using the error resolution strategies, an addition-
al 34% of the erroneous concepts were accurate-
ly translated. This increased precision is 
achieved at the cost of user effort. On average, 
the strategies needed 1.4 clarifications turns per 
utterance. 
Besides focusing on improving the error de-
tection and resolution capabilities, we are cur-
rently working on extending these capabilities to 
two-way S2S systems. Specifically, we are de-
signing interactive strategies that engage both 
users in eyes-free cross-lingual communication. 
References 
David Stallard, Rohit Prasad, Prem Natarajan, Fred 
Choi, Shirin Saleem, Ralf Meermeier, Kriste 
Krstovski, Shankar Ananthakrishnan, and Jacob 
Devlin. 2011. The BBN TransTalk Speech-to-
Speech Translation System. Speech and Language 
Technologies, InTech, 31-52 
Rohit Prasad, Rohit Kumar, Sankaranarayanan Anan-
thakrishnan, Wei Chen, Sanjika Hewavitharana, 
Matthew Roy, Frederick Choi, Aaron Challenner, 
Enoch Kan, Arvind Neelakantan, and Premkumar 
Natarajan. 2012.  Active Error Detection and Reso-
lution for Speech-to-Speech Translation. Intl. 
Workshop on Spoken Language Translation 
(IWSLT), Hong Kong 
Markku Turunen, and Jaakko Hakulinen, 2003. Jaspis 
- An Architecture for Supporting Distributed Spo-
ken Dialogues. Proc. of Eurospeech, Geneva, Swit-
zerland 
144
