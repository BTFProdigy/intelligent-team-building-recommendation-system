Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770?779,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Graph-based Semi-Supervised Model for Joint Chinese Word
Segmentation and Part-of-Speech Tagging
Xiaodong Zeng? Derek F. Wong? Lidia S. Chao? Isabel Trancoso?
?Department of Computer and Information Science, University of Macau
?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
Abstract
This paper introduces a graph-based semi-
supervised joint model of Chinese word
segmentation and part-of-speech tagging.
The proposed approach is based on a
graph-based label propagation technique.
One constructs a nearest-neighbor simi-
larity graph over all trigrams of labeled
and unlabeled data for propagating syn-
tactic information, i.e., label distribution-
s. The derived label distributions are re-
garded as virtual evidences to regular-
ize the learning of linear conditional ran-
dom fields (CRFs) on unlabeled data. An
inductive character-based joint model is
obtained eventually. Empirical results on
Chinese tree bank (CTB-7) and Microsoft
Research corpora (MSR) reveal that the
proposed model can yield better result-
s than the supervised baselines and other
competitive semi-supervised CRFs in this
task.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are two critical and necessary initial proce-
dures with respect to the majority of high-level
Chinese language processing tasks such as syn-
tax parsing, information extraction and machine
translation. The traditional way of segmentation
and tagging is performed in a pipeline approach,
first segmenting a sentence into words, and then
assigning each word a POS tag. The pipeline ap-
proach is very simple to implement, but frequently
causes error propagation, given that wrong seg-
mentations in the earlier stage harm the subse-
quent POS tagging (Ng and Low, 2004). The join-
t approaches of word segmentation and POS tag-
ging (joint S&T) are proposed to resolve these t-
wo tasks simultaneously. They effectively allevi-
ate the error propagation, because segmentation
and tagging have strong interaction, given that
most segmentation ambiguities cannot be resolved
without considering the surrounding grammatical
constructions encoded in a POS sequence (Qian
and Liu, 2012).
In the past years, several proposed supervised
joint models (Ng and Low, 2004; Zhang and
Clark, 2008; Jiang et al, 2009; Zhang and Clark,
2010) achieved reasonably accurate results, but the
outstanding problem among these models is that
they rely heavily on a large amount of labeled data,
i.e., segmented texts with POS tags. However, the
production of such labeled data is extremely time-
consuming and expensive (Jiao et al, 2006; Jiang
et al, 2009). Therefore, semi-supervised join-
t S&T appears to be a natural solution for easily in-
corporating accessible unlabeled data to improve
the joint S&T model. This study focuses on using
a graph-based label propagation method to build
a semi-supervised joint S&T model. Graph-based
label propagation methods have recently shown
they can outperform the state-of-the-art in sever-
al natural language processing (NLP) tasks, e.g.,
POS tagging (Subramanya et al, 2010), knowl-
edge acquisition (Talukdar et al, 2008), shallow
semantic parsing for unknown predicate (Das and
Smith, 2011). As far as we know, however, these
methods have not yet been applied to resolve
the problem of joint Chinese word segmentation
(CWS) and POS tagging.
Motivated by the works in (Subramanya et al,
2010; Das and Smith, 2011), for structured prob-
lems, graph-based label propagation can be em-
ployed to infer valuable syntactic information (n-
gram-level label distributions) from labeled data
to unlabeled data. This study extends this intui-
tion to construct a similarity graph for propagating
trigram-level label distributions. The derived label
distributions are regarded as prior knowledge to
regularize the learning of a sequential model, con-
ditional random fields (CRFs) in this case, on both
770
labeled and unlabeled data to achieve the semi-
supervised learning. The approach performs the
incorporation of the derived labeled distributions
by manipulating a ?virtual evidence? function as
described in (Li, 2009). Experiments on the da-
ta from the Chinese tree bank (CTB-7) and Mi-
crosoft Research (MSR) show that the proposed
model results in significant improvement over oth-
er comparative candidates in terms of F-score and
out-of-vocabulary (OOV) recall.
This paper is structured as follows: Section
2 points out the main differences with the re-
lated work of this study. Section 3 reviews the
background, including supervised character-based
joint S&T model based on CRFs and graph-based
label propagation. Section 4 presents the details of
the proposed approach. Section 5 reports the ex-
periment results. The conclusion is drawn in Sec-
tion 6.
2 Related Work
Prior supervised joint S&T models present ap-
proximate 0.2% - 1.3% improvement in F-score
over supervised pipeline ones. The state-of-the-
art joint models include reranking approaches (Shi
and Wang, 2007), hybrid approaches (Nakagawa
and Uchimoto, 2007; Jiang et al, 2008; Sun,
2011), and single-model approaches (Ng and Low,
2004; Zhang and Clark, 2008; Kruengkrai et al,
2009; Zhang and Clark, 2010). The proposed ap-
proach in this paper belongs to the single-model
type.
There are few explorations of semi-supervised
approaches for CWS or POS tagging in previ-
ous works. Xu et al (2008) described a Bayesian
semi-supervised CWS model by considering the
segmentation as the hidden variable in machine
translation. Unlike this model, the proposed ap-
proach is targeted at a general model, instead of
one oriented to machine translation task. Sun and
Xu (2011) enhanced a CWS model by interpolat-
ing statistical features of unlabeled data into the
CRFs model. Wang et al (2011) proposed a semi-
supervised pipeline S&T model by incorporating
n-gram and lexicon features derived from unla-
beled data. Different from their concern, our em-
phasis is to learn the semi-supervised model by
injecting the label information from a similarity
graph constructed from labeled and unlabeled da-
ta.
The induction method of the proposed approach
also differs from other semi-supervised CRFs al-
gorithms. Jiao et al (2006), extended by Mann
and McCallum (2007), reported a semi-supervised
CRFs model which aims to guide the learning
by minimizing the conditional entropy of unla-
beled data. The proposed approach regularizes the
CRFs by the graph information. Subramanya et
al. (2010) proposed a graph-based self-train style
semi-supervised CRFs algorithm. In the proposed
approach, an analogous way of graph construction
intuition is applied. But overall, our approach dif-
fers in three important aspects: first, novel feature
templates are defined for measuring the similari-
ty between vertices. Second, the critical property,
i.e., sparsity, is considered among label propaga-
tion. And third, the derived label information from
the graph is smoothed into the model by optimiz-
ing a modified objective function.
3 Background
3.1 Supervised Character-based Model
The character-based joint S&T approach is oper-
ated as a sequence labeling fashion that each Chi-
nese character, i.e., hanzi, in the sequence is as-
signed with a tag. To perform segmentation and
tagging simultaneously in a uniform framework,
according to Ng and Low (2004), the tag is com-
posed of a word boundary part, and a POS part,
e.g., ?B NN? refers to the first character in a word
with POS tag ?NN?. In this paper, 4 word bound-
ary tags are employed: B (beginning of a word),
M (middle part of a word), E (end of a word) and
S (single character). As for the POS tag, we shal-
l use the 33 tags in the Chinese tree bank. Thus,
the potential composite tags of joint S&T consist
of 132 (4?33) classes.
The first-order CRFs model (Lafferty et al,
2001) has been the most common one in this
task. Given a set of labeled examples Dl =
{(xi, yi)}li=1, where xi = x1ix2i ...xNi is the se-
quence of characters in the ith sentence, and yi =
y1i y2i ...yNi is the corresponding label sequence.
The goal is to learn a CRFs model in the form,
p(yi|xi; ?) =
1
Z(xi; ?)
exp{
N?
j=1
K?
k=1
?kfk(yj?1i , y
j
i , xi, j)}
(1)
where Z(xi; ?) is the partition function that nor-
malizes the exponential form to be a probability
distribution, and fk(yj?1i , yji , xi, j). In this study,
771
the baseline feature templates of joint S&T are
the ones used in (Ng and Low, 2004; Jiang et al,
2008), as shown in Table 1. ? = {?1?2...?K} ?
RK are the weight parameters to be learned. In su-
pervised training, the aim is to estimate the ? that
maximizes the conditional likelihood of the train-
ing data while regularizing model parameters:
L(?) =
l?
i=1
log p(yi|xi; ?)?R(?) (2)
R(?) can be any standard regularizer on parame-
ters, e.g., R(?) =? ? ? /2?2, to limit overfitting
on rare features and avoid degeneracy in the case
of correlated features. This objective function can
be optimized by the stochastic gradient method or
other numerical optimization methods.
Type Font Size
Unigram Cn(n = ?2,?1, 0, 1, 2)
Bigram CnCn+1(n = ?2,?1, 0, 1)
Date, Digit and
Alphabetic Letter
T (C?2)T (C?1)T (C0)
T (C1)T (C2)
Table 1: The feature templates of joint S&T.
3.2 Graph-based Label Propagation
Graph-based label propagation, a critical subclass
of semi-supervised learning (SSL), has been wide-
ly used and shown to outperform other SSL meth-
ods (Chapelle et al, 2006). Most of these algo-
rithms are transductive in nature, so they cannot
be used to predict an unseen test example in the fu-
ture (Belkin et al, 2006). Typically, graph-based
label propagation algorithms are run in two main
steps: graph construction and label propagation.
The graph construction provides a natural way to
represent data in a variety of target domains. One
constructs a graph whose vertices consist of la-
beled and unlabeled examples. Pairs of vertices
are connected by weighted edges which encode
the degree to which they are expected to have the
same label (Zhu et al, 2003). Popular graph con-
struction methods include k-nearest neighbors (k-
NN) (Bentley, 1980; Beygelzimer et al, 2006),
b-matching (Jebara et al, 2009) and local recon-
struction (Daitch et al, 2009). Label propaga-
tion operates on the constructed graph. The pri-
mary objective is to propagate labels from a few
labeled vertices to the entire graph by optimiz-
ing a loss function based on the constraints or
properties derived from the graph, e.g., smooth-
ness (Zhu et al, 2003; Subramanya et al, 2010;
Talukdar et al, 2008), or sparsity (Das and Smith,
2012). State-of-the-art label propagation algo-
rithms include LP-ZGL (Zhu et al, 2003), Ad-
sorption (Baluja et al, 2008), MAD (Talukdar
and Crammer, 2009) and Sparse Inducing Penal-
ties (Das and Smith, 2012).
4 Method
The emphasis of this work is on building a joint
S&T model based on two different kinds of data
sources, labeled and unlabeled data. In essence,
this learning problem can be treated as incorporat-
ing certain gainful information, e.g., prior knowl-
edge or label constraints, of unlabeled data into
the supervised model. The proposed approach em-
ploys a transductive graph-based label propagation
method to acquire such gainful information, i.e.,
label distributions from a similarity graph con-
structed over labeled and unlabeled data. Then,
the derived label distributions are injected as vir-
tual evidences for guiding the learning of CRFs.
Algorithm 1 semi-supervised joint S&T induction
Input:
Dl = {(xi, yi)}li=1 labeled sentences
Du = {(xi)}l+ui=l+1 unlabeled sentencesOutput:
?: a set of feature weights
1: Begin
2: {G} = construct graph (Dl,Du)
3: {q0} = init labelDist ({G})
4: {q} = propagate label ({G}, {q0})
5: {?} = train crf (Dl ? Du, {q})
6: End
The model induction includes the following
steps (see Algorithm 1): firstly, given labeled
and unlabeled data, i.e., Dl = {(xi, yi)}li=1
with l labeled sentences and Du = {(xi)}l+ui=l+1with u unlabeled sentences, a specific similarity
graph G representing Dl and Du is constructed
(construct graph). The vertices (Section 4.1) in
the constructed graph consist of all trigrams that
occur in labeled and unlabeled sentences, and edge
weights between vertices are computed using the
cosine distance between pointwise mutual infor-
mation (PMI) statistics. Afterwards, the estimated
label distributions q0 of vertices in the graph G are
randomly initialized (init labelDist). Subsequently,
772
the label propagation procedure (propagate label)
is conducted for projecting label distributions q
from labeled vertices to the entire graph, using
the algorithm of Sparse-Inducing Penalties (Das
and Smith, 2012) (Section 4.2). The final step
(train crf) of the induction is incorporating the in-
ferred trigram-level label distributions q into CRFs
model (Section 4.3).
4.1 Graph Construction
In most graph-based label propagation tasks, the
final effect depends heavily on the quality of
the graph. Graph construction thus plays a cen-
tral role in graph-based label propagation (Zhu et
al., 2003). For character-based joint S&T, unlike
the unstructured learning problem whose vertices
are formed directly by labeled and unlabeled in-
stances, the graph construction is non-trivial. Das
and Petrov (2011) mentioned that taking individu-
al characters as the vertices would result in various
ambiguities, whereas the similarity measurement
is still challenging if vertices corresponding to en-
tire sentences.
This study follows the intuitions of graph con-
struction from Subramanya et al (2010) in which
vertices are represented by character trigrams oc-
curring in labeled and unlabeled sentences. For-
mally, given a set of labeled sentences Dl, and un-
labeled onesDu, whereD , {Dl,Du}, the goal is
to form an undirected weighted graph G = (V,E),
where V is defined as the set of vertices which
covers all trigrams extracted from Dl and Du.
Here, V = Vl ? Vu, where Vl refers to trigrams
that occurs at least once in labeled sentences and
Vu refers to trigrams that occur only in unlabeled
sentences. The edges E ? Vl ? Vu, connect all
the vertices. This study makes use of a symmet-
ric k-NN graph (k = 5) and the edge weights are
measured by a symmetric similarity function (E-
quation (3)):
wi,j =
{
sim(xi, xj) if j ? K(i) or i ? K(j)
0 otherwise
(3)
where K(i) is the set of the k nearest neighbors of
xi(|K(i) = k, ?i|) and sim(xi, xj) is a similari-
ty measure between two vertices. The similarity
is computed based on the co-occurrence statistic-
s over the features in Table 2. Most features we
adopted are selected from those of (Subramanya
et al, 2010). Note that a novel feature in the last
row encodes the classes of surrounding character-
s, where four types are defined: number, punctu-
ation, alphabetic letter and other. It is especially
helpful for the graph to make connections with tri-
grams that may not have been seen in labeled data
but have similar label information. The pointwise
mutual information values between the trigram-
s and each feature instantiation that they have in
common are summed to sparse vectors, and their
cosine distances are computed as the similarities.
Description Feature
Trigram + Context x1x2x3x4x5
Trigram x2x3x4
Left Context x1x2
Right Context x4x5
Center Word x3
Trigram - Center Word x2x4
Left Word + Right Context x2x4x5
Right Word + Left Context x1x2x3
Type of Trigram: number,
punctuation, alphabetic letter
and other
t(x2)t(x3)t(x4)
Table 2: Features employed to measure the sim-
ilarity between two vertices, in a given tex-
t ?x1x2x3x4x5?, where the trigram is ?x2x3x4?.
The nature of the similarity graph enforces that
the connected trigrams with high weight appearing
in different texts should have similar syntax con-
figurations. Thus, the constructed graph is expect-
ed to provide additional information that cannot
be expressed directly in a sequence model (Subra-
manya et al, 2010). One primary benefit of this
property is on enriching vocabulary coverage. In
other words, the new features of various trigram-
s only occurring in unlabeled data can be discov-
ered. As the excerpt in Figure 1 shows, the trigram
????? (Tianjin port) has no any label informa-
tion, as it only occurs in unlabeled data, but for-
tunately its neighborhoods with similar syntax in-
formation, e.g., ????? (Shanghai port), ???
?? (Guangzhou port), can assist to infer the cor-
rect tag ?M NN?.
4.2 Label Propagation
In order to induce trigram-level label distributions
from the graph constructed by the previous step,
a label propagation algorithm, Sparsity-Inducing
Penalties, proposed by Das and Smith (2012), is
employed. This algorithm is used because it cap-
tures the property of sparsity that only a few labels
773
Figure 1: An excerpt from the similarity graph
over trigrams on labeled and unlabeled data.
are typically associated with a given instance. In
fact, the sparsity is also a common phenomenon
among character-based CWS and POS tagging.
The following convex objective is optimized on
the similarity graph in this case:
argmin
q
l?
j=1
? qj ? rj ?2
+?
l+u?
i=1,k?N (i)
wik ? qi ? qk ?2 +?
l+u?
i=1
? qi ?2
s.t. qi ? 0, ?i ? V
(4)
where rj denotes empirical label distributions of
labeled vertices, and qi denotes unnormalized es-
timate measures in every vertex. The wik refers to
the similarity between the ith trigram and the kth
trigram, and N (i) is a set of neighbors of the ith
trigram. ? and ? are two hyperparameters whose
values are discussed in Section 5. The squared-
loss criterion1 is used to formulate the objective
function. The first term in Equation (4) is the seed
match loss which penalizes the estimated label dis-
tributions qj , if they go too far away from the em-
pirical labeled distributions rj . The second term
is the edge smoothness loss that requires qi should
be smooth with respect to the graph, such that two
vertices connected by an edge with high weight
should be assigned similar labels. The final term
is a regularizer to incorporate the prior knowledge,
e.g., uniform distributions used in (Talukdar et al,
2008; Das and Smith, 2011). This study applies
the squared norm of q to encourage sparsity per
vertex. Note that the estimated label distribution
1It can be seen as a multi-class extension of quadratic cost
criterion (Bengio et al, 2006) or as a variant of the objective
in (Zhu et al, 2003). An entropic distance measure could also
be used, e.g., KL-divergence (Subramanya et al, 2010; Das
and Smith, 2012).
qi in Equation (4) is relaxed to be unnormalized,
which simplifies the optimization. Thus, the objec-
tive function can be optimized by L-BFGS-B (Zhu
et al, 1997), a generic quasi-Newton gradient-
based optimizer. The partial derivatives of Equa-
tion (4) are computed for each parameter of q and
then passed on to the optimizer that updates them
such that Equation (4) is maximized.
4.3 Semi-Supervised CRFs Training
The trigram-level label distributions inferred in the
propagation step can be viewed as a kind of valu-
able ?prior knowledge? to regularize the learning
on unlabeled data. The final step of the induc-
tion is thus to incorporate such prior knowledge
into CRFs. Li (2009) generalizes the use of vir-
tual evidence to undirected graphical models and,
in particular, to CRFs for incorporating external
knowledge. By extending the similar intuition, as
illustrated in Figure 2, we modify the structure of
a regular linear-chain CRFs on unlabeled data for
smoothing the derived label distributions, where
virtual evidences, i.e., q in our case, are donated
by {v1, v2, . . . , vT }, in parallel with the state vari-
ables {y1, y2, . . . , yT }. The modified CRFs model
allows us to flexibly define the interaction between
estimated state values and virtual evidences by po-
tential functions. Therefore, given labeled and un-
labeled data, the learning objective is defined as
follows:
L(?) +
l+u?
i=l+1
Ep(yi|xi,vi;?g)[log p(yi, vi|xi; ?)]
(5)
where the conditional probability in the second
term is denoted as
p(yi, vi|xi; ?) =
1
Z ?(xi; ?)
exp{
N?
j=1
K?
k=1
?kfk(yj?1i , y
j
i , xi, j)
+?
N?
t=1
s(yti , vti)}
(6)
The first term in Equation (5) is the same as E-
quation (2), which is the traditional CRFs learn-
ing objective function on the labeled data. The
second term is the expected conditional likelihood
of unlabeled data. It is directed to maximize the
conditional likelihood of hidden states with the
derived label distributions on unlabeled data, i.e.,
p(y, v|x), where y and v are jointly modeled but
774
the probability is still conditional on x. Here,
Z ?(x; ?) is the partition function of normalization
that is achieved by summing the numerator over
both y and v. A virtual evidence feature function
of s(yti , vti) with pre-defined weight ? is defined
to regularize the conditional distributions of states
over the derived label distributions. The learning
is impacted by the derived label distributions as E-
quation (7): firstly, if the trigram xt?1i xtixt+1i at
current position does have no corresponding de-
rived label distributions (vti = null), the value of
zero is assigned to all state hypotheses so that the
posteriors would not affected by the derived infor-
mation. Secondly, if it does have a derived label
distribution, since the virtual evidence in this case
is a distribution instead of a specific label, the la-
bel probability in the distribution under the current
state hypothesis is assigned. This means that the
values of state variables are constrained to agree
with the derived distributions.
s(yti , vti) =
{
qxt?1i xtixt+1i (y
t
i) if vti 6= null
0 else
(7)
The second term in Equation (5) can be op-
timized by using the expectation maximization
(EM) algorithm in the same fashion as in the
generative approach, following (Li, 2009). One
can iteratively optimize the Q function Q(?) =?
y p(yi|xi; ?g) log p(yi, vi|xi; ?), in which ?g is
the model estimated from the previous iteration.
Here the gradient of the Q function can be mea-
sured by:
?Q(?)
??k
=
?
t
?
yt?1i ,yti
fk(yt?1i , yti , xi, t).
(p(yt?1i , yti |xi, vi; ?)? p(yt?1i , yti |xi; ?))
(8)
The forward-backward algorithm is used to mea-
sure p(yt?1i , yti |xi, vi; ?) and p(yt?1i , yti |xi; ?).
Thus, the objective function Equation (5) is op-
timized as follows: for the instances i = 1, 2, ..., l,
the parameters ? are learned as the supervised
manner; for the instances i = l+1, l+2, ..., u+ l,
in the E-step, the expected value of Q function is
computed, based on the current model ?g. In the
M-step, the posteriors are fixed and updated ? that
maximizes Equation (5).
Figure 2: Modified linear-chain CRFs integrating
virtual evidences on unlabeled data.
5 Experiment
5.1 Setting
The experimental data are mainly taken from the
Chinese tree bank (CTB-7) and Microsoft Re-
search (MSR)2. CTB-7 consists of over one mil-
lion words of annotated and parsed text from Chi-
nese newswire, magazine news, various broadcast
news and broadcast conversation programs, web
newsgroups and weblogs. It is a segmented, POS
tagged3 and fully bracketed corpus. The train, de-
velopment and test sets4 from CTB-7 and their
corresponding statistics are reported in Table 3.
To satisfy the characteristic of the semi-supervised
learning problem, the train set, i.e., the labeled da-
ta, is formed by a relatively small amount of an-
notated texts sampled from CTB-7. For the un-
labeled data in this experiment, a greater amount
of texts is extracted from CTB-7 and MSR, which
contains 53,108 sentences with 2,418,690 charac-
ters.
The performance measurement indicators for
word segmentation and POS tagging (joint S&T)
are balance F-score, F = 2PR/(P+R), the harmon-
ic mean of precision (P) and recall (R), and out-
of-vocabulary recall (OOV-R). For segmentation,
a token is regarded to be correct if its boundaries
match the ones of a word in the gold standard.
For the POS tagging, it is correct only if both the
boundaries and the POS tags are perfect matches.
The experimental platform is implemented
based on two toolkits: Mallet (McCallum and
Kachites, 2002) and Junto (Talukdar and Pereira,
2010). Mallet is a java-based package for s-
tatistical natural language processing, which in-
cludes the CRFs implementation. Junto is a graph-
2It can be download at: www.sighan.org/bakeoff2005.
3There is a total of 33 POS tags in CTB-7.
4The extracted sentences in train, development and test set
were assigned with the composite tags as described in Section
3.1.
775
based label propagation toolkit that provides sev-
eral state-of-the-art algorithms.
Data #Sent #Word #Char #OOV
Train 17,968 374,697 596,360
Develop 1,659 46,637 79,283 0.074
Test 2,037 65,219 104,502 0.089
Table 3: Training, development and testing data.
5.2 Baseline and Proposed Models
In the experiment, the baseline supervised pipeline
and joint S&T models are built only on the train
data. The proposed model will also be compared
with the semi-supervised pipeline S&T model de-
scribed in (Wang et al, 2011). In addition, two
state-of-the-art semi-supervised CRFs algorithms,
Jiao?s CRFs (Jiao et al, 2006) and Subramanya?s
CRFs (Subramanya et al, 2010), are also used to
build joint S&T models. The corresponding set-
tings of the above candidates are listed below:
? Baseline I: a supervised CRFs pipeline S&T
model. The feature templates are from Zhao
et al (2006) and Wu et al (2008).
? Wang?s model: a semi-supervised CRFs
pipeline S&T model. The same feature tem-
plates in (Wang et al, 2011) are used, i.e.,
?+n-gram+cluster+lexicon?.
? Baseline II: a supervised CRFs joint S&T
model. The feature templates introduced in
Section 3.1 are used.
? Jiao?s model: a semi-supervised CRFs joint
S&T model trained using the entropy regular-
ization (ER) criteria (Jiao et al, 2006). The
optimization method proposed by Mann and
McCallum (2007) is applied.
? Subramanya?s model: a self-train style
semi-supervised CRFs joint S&T model
based on the same parameters used in (Sub-
ramanya et al, 2010).
? Our model: several parameters in our model
are needed to tune based on the development
set, e.g., ?, ? and ?.
In all the CRFs models above, the Gaussian reg-
ularizer and stochastic gradient descent method
are employed.
5.3 Main Results
This experiment yielded a similarity graph that
consists of 462,962 trigrams from labeled and un-
labeled data. The majority (317,677 trigrams) oc-
curred only in unlabeled data. Based on the de-
velopment data, the hyperparameters of our mod-
el were tuned among the following settings: for
the graph propagation, ? ? {0.2, 0.5, 0.8} and
? ? {0.1, 0.3, 0.5, 0.8}; for the CRFs training,
? ? {0.1, 0.3, 0.5, 0.7, 0.9}. The best performed
joint settings are ? = 0.5, ? = 0.3 and ? = 0.7.
With the chosen set of hyperparameters, the test
data was used to measure the final performance.
Model Segmentation POS TaggingF1 OOV-R F1 OOV-R
Baseline I 94.27 60.12 91.08 51.72
Wang?s 95.17 63.10 91.64 53.29
Baseline II 95.14 61.52 91.61 52.29
Jiao?s 95.58 63.05 92.11 53.27
Subramanya?s 96.30 67.12 92.46 57.15
Our model 96.85 68.09 92.89 58.36
Table 4: The performance of segmentation and
POS tagging on testing data.
Table 4 summarizes the performance of seg-
mentation and POS tagging on the test data, in
comparison with the other five models. First-
ly, as expected, for the two supervised baselines,
the joint model outperforms the pipeline one, e-
specially on segmentation. It obtains 0.92% and
2.32% increase in terms of F-score and OOV-R
respectively. This outcome verifies the commonly
accepted fact that the joint model can substantially
improve the pipeline one, since POS tags provide
additional information to word segmentation (Ng
and Low, 2004). Secondly, it is also noticed that
all four semi-supervised models are able to benefit
from unlabeled data and greatly improve the re-
sults with respect to the baselines. On the whole,
for segmentation, they achieve average improve-
ments of 1.02% and 6.8% in F-score and OOV-R;
whereas for POS tagging, the average increments
of F-sore and OOV-R are 0.87% and 6.45%. An
interesting phenomenon is found among the com-
parisons with baselines that the supervised joint
model (Baseline II) is even competitive with semi-
supervised pipeline one (Wang et al, 2011). This
illustrates the effects of error propagation in the
pipeline approach. Thirdly, in what concerns the
semi-supervised approaches, the three joint S&T
models, i.e., Jiao?s, Subramanya?s and our mod-
el, are superior to the pipeline model, i.e., Wang?s
776
model. Moreover, the two graph-based approach-
es, i.e., Subramanya?s and our model, outperform
the others. Most importantly, the boldface num-
bers in the last row illustrate that our model does
achieve the best performance. Overall, for word
segmentation, it obtains average improvements of
1.43% and 8.09% in F-score and OOV-R over oth-
ers; for POS tagging, it achieves average improve-
ments of 1.09% and 7.73%.
0 10,000 20,000 30,000 40,000 50,000
94.0
94.5
95.0
95.5
96.0
96.5
97.0
97.5
 Wang's
 Jiao's
 Subramanya's
 Our
F
-
s
c
o
r
e
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
91.0
91.5
92.0
92.5
93.0
93.5
 Wang's
 Jiao's
 Subramanya's
 Our
F
-
s
c
o
r
e
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
60.0
62.5
65.0
67.5
70.0
 Wang's
 Jiao's
 Subramanya's
 Our
O
O
V
-
R
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
51.0
52.5
54.0
55.5
57.0
58.5
 Wang's
 Jiao's
 Subramanya's
 Our
O
O
V
-
R
Number of unlabeled sentences
Figure 3: The learning curves of semi-supervised
models on unlabeled data, where left graphs are
segmentation and the right ones are tagging.
5.4 Learning Curve
An additional experiment was conducted to inves-
tigate the impact of unlabeled data for the four
semi-supervised models. Figure 3 illustrates the
curves of F-score and OOV-R for segmentation
and tagging respectively, as the unlabeled data
size is progressively increased in steps of 6,000
sentences. It can be clearly observed that al-
l curves of our model are able to mount up steadi-
ly and achieve better gains over others consistent-
ly. The most competitive performance of the oth-
er three candidates is achieved by Subramanya?s
model. This strongly reveals that the knowledge
derived from the similarity graph does effectively
strengthen the model. But in Subramanya?s mod-
el, when the unlabeled size ascends to approxi-
mately 30,000 sentences the curves become nearly
asymptotic. The semi-supervised pipeline model,
Wang?s model, presents a much slower growth on
all curves over the others and also begins to over-
fit with large unlabeled data sizes (>25,000 sen-
tences). The figure also shows an erratic fluctu-
ation of Jiao?s model. Since this approach aims
at minimizing conditional entropy over unlabeled
data and encourages finding putative labelings for
unlabeled data, it results in a data-sensitive mod-
el (Li et al, 2009).
5.5 Analysis & Discussion
A statistical analysis of the segmentation and tag-
ging results of the supervised joint model (Base-
line II) and our model is carried out to comprehend
the influence of the graph-based semi-supervised
behavior. For word segmentation, the most signif-
icant improvement of our model is mainly concen-
trated on two kinds of words which are known for
their difficulties in terms of CWS: a) named enti-
ties (NE), e.g., ????? (Tianjin port) and ???
?? (free tax zone); and b) Chinese numbers (CN),
e.g., ?????? (eight hundred and fifty million)
and ???????? (seventy two percent). Very
often, these words do not exist in the labeled data,
so the supervised model is hard to learn their fea-
tures. Part of these words, however, may occur in
the unlabeled data. The proposed semi-supervised
approach is able to discover their label information
with the help of a similarity graph. Specifically, it
learns the label distributions from similar words
(neighborhoods), e.g., ????? (Shanghai port),
????? (protection zone), ?????? (nine
hundred and seventy million). The statistics in Ta-
ble 5 demonstrate significant error reductions of
50.44% and 48.74% on test data, corresponding to
NE and CN respectively.
Type #word #baErr #gbErr ErrDec%
NE 471 226 112 50.44
CN 181 119 61 48.74
Table 5: The statistics of segmentation error for
named entities (NE) and Chinese numbers (CN)
in test data. #baErr and #gbErr denote the count
of segmentations by Baseline II and our model;
ErrDec% denotes the error reduction.
On the other hand, to better understand the tag-
ging results, we summarize the increase and de-
crease of the top five common tagging error pat-
terns of our model over Baseline II for the cor-
rectly segmented words, as shown in Table 6. The
error pattern is defined by ?A?B? that refers the
true tag of ?A? is annotated by a tag of ?B?. The
obvious improvement brought by our model oc-
curs with the tags ?NN?, ?CD?, ?NR?, ?JJ? and
?NR?, where errors are reduced 60.74% on aver-
777
Pattern #baErr ? Pattern #baErr ?
NN?VV 58 38 NN?NR 13 6
CD?NN 41 27 IJ?ON 9 5
NR?VV 29 17 VV?NN 4 3
JJ?NN 18 11 NR?NN 1 3
NR?VA 19 10 JJ?AD 1 2
Table 6: The statistics of POS tagging error pat-
terns in test data. #baErr denote the count of tag-
ging error by Baseline II, while ? and ? denotes
the number of error reduced or increased by our
model.
age. More impressively, there is a large portion of
fixed error pattern instances stemming from OOV
words. Meanwhile, it is also observed that the dis-
ambiguation of error patterns in the right portion
of the table slightly suffers from our approach. In
reality, it is impossible and unrealistic to request
a model to be ?no harms but only benefits? under
whatever circumstances.
6 Conclusion
This study introduces a novel semi-supervised ap-
proach for joint Chinese word segmentation and
POS tagging. The approach performs the semi-
supervised learning in the way that the trigram-
level distributions inferred from a similarity graph
are used to regularize the learning of CRFs model
on labeled and unlabeled data. The empirical re-
sults indicate that the similarity graph information
and the incorporation manner of virtual evidences
present a positive effect to the model induction.
Acknowledgments
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau
for the funding support for our research, un-
der the reference No. 017/2009/A and RG060/09-
10S/CS/FST. The authors also wish to thank the
anonymous reviewers for many helpful comments.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak Ravich,
and Mohamed Aly. 2008. Video suggestion and
discovery for youtube: taking random walks through
the view graph. In Proceedings of WWW, pages 895-
904, Beijing, China.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization. Journal of machine
learning research, 7:2399?2434.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le
Roux. 2006. Label propogation and quadratic crite-
rion. MIT Press.
Jon Louis Bentley. 1980. Multidimensional divide-and
-conquer. Communications of the ACM, 23(4):214 -
229.
Alina Beygelzimer, Sham Kakade, and John Langford.
2006. Cover trees for nearest neighbor. In Proceed-
ings of ICML, pages 97-104, New York, USA
Olivier Chapelle, Bernhard Scho? lkopf, and Alexander
Zien. 2006. Semi-supervised learning. MIT Press.
Samuel I. Daitch, Jonathan A. Kelner, and Daniel A.
Spielman. 2009. Fitting a graph to vector data. In
Proceedings of ICML, 201-208, NY, USA.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised framesemantic parsing for unknown
predicates. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
Part-of-Speech Tagging with Bilingual Graph-based
Projections. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of NAACL, pages 677-687, Montre?al,
Canada.
Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009.
Graph construction and b-matching for semi-
supervised learning. In Proceedings of ICML, 441-
448, New York, USA.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging ? A Case
Study. In Proceedings of he ACL and the 4th IJC-
NLP of the AFNLP, pages 522?530, Suntec, Singa-
pore.
Feng Jiao, Shaojun Wang, and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In In
Proceedings of ACL, pages 209?216, Sydney, Aus-
tralia.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL and IJCNLP
of the AFNLP, pages 513- 521, Suntec, Singapore
August.
778
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Xiao Li. 2009. On the use of virtual evidence in con-
ditional random fields. In Proceedings of EMNLP,
pages 1289-1297, Singapore.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields In Pro-
ceedings of ACM SIGIR, pages 572-579, Boston,
USA.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Proceed-
ings of NAACL, pages 109-112, New York, USA.
McCallum and Andrew Kachites. 2002. MALLET: A
Machine Learning for Language Toolkit. Software
at http://mallet.cs.umass.edu.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and POS tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, pages 217?220, Prague, Czech Republic.
Hwee Tou Ng and Jin Kiat Low 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, Barcelona, Spain.
Xian Qian and Yang Liu. 2012. Joint Chinese Word
Segmentation, POS Tagging and Parsing. In Pro-
ceedings of EMNLP-CoNLL, pages 501-511, Jeju Is-
land, Korea.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRF based joint decoding method for cascade seg-
mentation and labelling tasks. In Proceedings of IJ-
CAI, Hyderabad, India.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of EMNLP, pages 167-176, Mas-
sachusetts, USA.
Weiwei Sun. 2011. A Stacked Sub-Word Model
for Joint Chinese Word Segmentation and Part-of-
Speech Tagging. In Proceedings of ACL, pages
1385?1394, Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas-
ca, Deepak Ravichandran, Rahul Bhagat, and Fer-
nando Pereira. 2008. Weakly Supervised Acquisi-
tion of Labeled Class Instances using Graph Ran-
dom Walks. In Proceedings of EMNLP, pages 582-
590, Hawaii, USA.
Partha Pratim Talukdar and Koby Crammer. 2009.
New Regularized Algorithms for Transductive
Learning. In Proceedings of ECML-PKDD, pages
442 - 457, Bled, Slovenia.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learn-
ing methods for class-instance acquisition. In Pro-
ceedings of ACL, pages 1473-1481, Uppsala, Swe-
den.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309?317, Chiang Mai, Thailand.
Yu-Chieh Wu Jie-Chi Yang, and Yue-Shi Lee. 2008.
Description of the NCU Chinese Word Segmenta-
tion and Part-of-Speech Tagging for SIGHAN Bake-
off. In Proceedings of the SIGHAN Workshop on
Chinese Language Processing, pages 161-166, Hy-
derabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Herman-
n Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of EMNLP, pages 888-896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
ICML, pages 912?919, Washington DC, USA.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
779
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171?176,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Co-regularizing character-based and word-based models for
semi-supervised Chinese word segmentation
Xiaodong Zeng? Derek F. Wong? Lidia S. Chao? Isabel Trancoso?
?Department of Computer and Information Science, University of Macau
?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
Abstract
This paper presents a semi-supervised
Chinese word segmentation (CWS) ap-
proach that co-regularizes character-based
and word-based models. Similarly to
multi-view learning, the ?segmentation
agreements? between the two differen-
t types of view are used to overcome the
scarcity of the label information on unla-
beled data. The proposed approach train-
s a character-based and word-based mod-
el on labeled data, respectively, as the ini-
tial models. Then, the two models are con-
stantly updated using unlabeled examples,
where the learning objective is maximiz-
ing their segmentation agreements. The a-
greements are regarded as a set of valuable
constraints for regularizing the learning of
both models on unlabeled data. The seg-
mentation for an input sentence is decod-
ed by using a joint scoring function com-
bining the two induced models. The e-
valuation on the Chinese tree bank reveals
that our model results in better gains over
the state-of-the-art semi-supervised mod-
els reported in the literature.
1 Introduction
Chinese word segmentation (CWS) is a critical
and a necessary initial procedure with respect to
the majority of high-level Chinese language pro-
cessing tasks such as syntax parsing, informa-
tion extraction and machine translation, since Chi-
nese scripts are written in continuous characters
without explicit word boundaries. Although su-
pervised CWS models (Xue, 2003; Zhao et al,
2006; Zhang and Clark, 2007; Sun, 2011) pro-
posed in the past years showed some reasonably
accurate results, the outstanding problem is that
they rely heavily on a large amount of labeled da-
ta. However, the production of segmented Chi-
nese texts is time-consuming and expensive, since
hand-labeling individual words and word bound-
aries is very hard (Jiao et al, 2006). So, one can-
not rely only on the manually segmented data to
build an everlasting model. This naturally pro-
vides motivation for using easily accessible raw
texts to enhance supervised CWS models, in semi-
supervised approaches. In the past years, however,
few semi-supervised CWS models have been pro-
posed. Xu et al (2008) described a Bayesian semi-
supervised model by considering the segmentation
as the hidden variable in machine translation. Sun
and Xu (2011) enhanced the segmentation result-
s by interpolating the statistics-based features de-
rived from unlabeled data to a CRFs model. An-
other similar trial via ?feature engineering? was
conducted by Wang et al (2011).
The crux of solving semi-supervised learning
problem is the learning on unlabeled data. In-
spired by multi-view learning that exploits redun-
dant views of the same input data (Ganchev et
al., 2008), this paper proposes a semi-supervised
CWS model of co-regularizing from two dif-
ferent views (intrinsically two different models),
character-based and word-based, on unlabeled da-
ta. The motivation comes from that the two types
of model exhibit different strengths and they are
mutually complementary (Sun, 2010; Wang et al,
2010). The proposed approach begins by train-
ing a character-based and word-based model on
labeled data respectively, and then both models
are regularized from each view by their segmen-
tation agreements, i.e., the identical outputs, of
unlabeled data. This paper introduces segmenta-
tion agreements as gainful knowledge for guiding
the learning on the texts without label information.
Moreover, in order to better combine the strengths
of the two models, the proposed approach uses a
joint scoring function in a log-linear combination
form for the decoding in the segmentation phase.
171
2 Segmentation Models
There are two classes of CWS models: character-
based and word-based. This section briefly re-
views two supervised models in these categories,
a character-based CRFs model, and a word-based
Perceptrons model, which are used in our ap-
proach.
2.1 Character-based CRFs Model
Character-based models treat word segmentation
as a sequence labeling problem, assigning label-
s to the characters in a sentence indicating their
positions in a word. A 4 tag-set is used in this
paper: B (beginning), M (middle), E (end) and
S (single character). Xue (2003) first proposed
the use of CRFs model (Lafferty et al, 2001) in
character-based CWS. Let x = (x1x2...x|x|) ? X
denote a sentence, where each character and y =
(y1y2...y|y|) ? Y denote a tag sequence, yi ? T
being the tag assigned to xi. The goal is to achieve
a label sequence with the best score in the form,
p?c(y|x) =
1
Z(x; ?c)
exp{f(x, y) ? ?c} (1)
where Z(x; ?c) is a partition function that normal-
izes the exponential form to be a probability distri-
bution, and f(x, y) are arbitrary feature functions.
The aim of CRFs is to estimate the weight param-
eters ?c that maximizes the conditional likelihood
of the training data:
??c = argmax
?c
l?
i=1
log p?c(yi|xi)? ???c?22 (2)
where ???c?22 is a regularizer on parameters to
limit overfitting on rare features and avoid degen-
eracy in the case of correlated features. In this
paper, this objective function is optimized by s-
tochastic gradient method. For the decoding, the
Viterbi algorithm is employed.
2.2 Word-based Perceptrons Model
Word-based models read a input sentence from left
to right and predict whether the current piece of
continuous characters is a word. After one word
is identified, the method moves on and searches
for a next possible word. Zhang and Clark (2007)
first proposed a word-based segmentation mod-
el using a discriminative Perceptrons algorithm.
Given a sentence x, let us denote a possible seg-
mented sentence as w ? w, and the function that
enumerates a set of segmentation candidates as
GEN:w = GEN(x) for x. The objective is to
maximize the following problem for all sentences:
??w = argmax
w=GEN(x)
|w|?
i=1
?(x,wi) ? ?w (3)
where it maps the segmented sentencew to a glob-
al feature vector ? and denotes ?w as its cor-
responding weight parameters. The parameter-
s ?w can be estimated by using the Perceptron-
s method (Collins, 2002) or other online learning
algorithms, e.g., Passive Aggressive (Crammer et
al., 2006). For the decoding, a beam search decod-
ing method (Zhang and Clark, 2007) is used.
2.3 Comparison Between Both Models
Character-based and word-based models present
different behaviors and each one has its own
strengths and weakness. Sun (2010) carried out a
thorough survey that includes theoretical and em-
pirical comparisons from four aspects. Here, two
critical properties of the two models supporting
the co-regularization in this study are highlight-
ed. Character-based models present better predic-
tion ability for new words, since they lay more
emphasis on the internal structure of a word and
thereby express more nonlinearity. On the oth-
er side, it is easier to define the word-level fea-
tures in word-based models. Hence, these models
have a greater representational power and conse-
quently better recognition performance for in-of-
vocabulary (IV) words.
3 Semi-supervised Learning via
Co-regularizing Both Models
As mentioned earlier, the primary challenge of
semi-supervised CWS concentrates on the unla-
beled data. Obviously, the learning on unlabeled
data does not come for ?free?. Very often, it is
necessary to discover certain gainful information,
e.g., label constraints of unlabeled data, that is in-
corporated to guide the learner toward a desired
solution. In our approach, we believe that the seg-
mentation agreements (? 3.1) from two differen-
t views, character-based and word-based models,
can be such gainful information. Since each of the
models has its own merits, their consensuses signi-
fy high confidence segmentations. This naturally
leads to a new learning objective that maximizes
segmentation agreements between two models on
unlabeled data.
172
This study proposes a co-regularized CWS
model based on character-based and word-based
models, built on a small amount of segmented sen-
tences (labeled data) and a large amount of raw
sentences (unlabeled data). The model induction
process is described in Algorithm 1: given labeled
dataset Dl and unlabeled dataset Du, the first t-
wo steps are training a CRFs (character-based) and
Perceptrons (word-based) model on the labeled
data Dl , respectively. Then, the parameters of
both models are continually updated using unla-
beled examples in a learning cycle. At each iter-
ation, the raw sentences in Du are segmented by
current character-based model ?c and word-based
model ?w. Meanwhile, all the segmentation agree-
ments A are collected (? 3.1). Afterwards, the
agreements A are used as a set of constraints to
bias the learning of CRFs (? 3.2) and Perceptron
(? 3.3) on the unlabeled data. The convergence
criterion is the occurrence of a reduction of seg-
mentation agreements or reaching the maximum
number of learning iterations. In the final segmen-
tation phase, given a raw sentence, the decoding
requires both induced models (? 3.4) in measuring
a segmentation score.
Algorithm 1 Co-regularized CWS model induction
Require: n labeled sentencesDl;m unlabeled sentencesDu
Ensure: ?c and ?w
1: ?0c ? crf train(Dl)
2: ?0w ? perceptron train(Dl)
3: for t = 1...Tmax do
4: At ? agree(Du, ?t?1c , ?t?1w )
5: ?tc ? crf train constraints(Du,At, ?t?1c )
6: ?tw ? perceptron train constraints(Du,At, ?t?1w )
7: end for
3.1 Agreements Between Two Models
Given a raw sentence, e.g., ?????????
?????(I am watching the opening ceremony
of the Olympics in Beijing.)?, the two segmenta-
tions shown in Figure 1 are the predictions from
a character-based and word-based model. The
segmentation agreements between the two mod-
els correspond to the identical words. In this ex-
ample, the five words, i.e. ?? (I)?, ??? (Bei-
jing)?, ?? (watch)?, ???? (opening ceremony)?
and ??(.)?, are the agreements.
3.2 CRFs with Constraints
For the character-based model, this paper fol-
lows (Ta?ckstro?m et al, 2013) to incorporate the
segmentation agreements into CRFs. The main
idea is to constrain the size of the tag sequence
lattice according to the agreements for achieving
simplified learning. Figure 2 demonstrates an ex-
ample of the constrained lattice, where the bold
node represents that a definitive tag derived from
the agreements is assigned to the current charac-
ter, e.g., ?? (I)? has only one possible tag ?S?
because both models segmented it to a word with
a single character. Here, if the lattice of all admis-
sible tag sequences for the sentence x is denoted
as Y(x), the constrained lattice can be defined by
Y?(x, y?), where y? refers to tags inferred from the
agreements. Thus, the objective function on unla-
beled data is modeled as:
???c = argmax
?c
m?
i=1
log p?c(Y?(xi, y?i)|xi)? ???c?22
(4)
It is a marginal conditional probability given by
the total probability of all tag sequences consistent
with the constrained lattice Y?(x, y?). This objec-
tive can be optimized by using LBFGS-B (Zhu et
al., 1997), a generic quasi-Newton gradient-based
optimizer.
Figure 1: The segmentations given by character-
based and word-based model, where the words in
?2? refer to the segmentation agreements.
Figure 2: The constrained lattice representation
for a given sentence, ????????????
???.
3.3 Perceptrons with Constraints
For the word-based model, this study incorporates
segmentation agreements by a modified parame-
ter update criterion in Perceptrons online training,
as shown in Algorithm 2. Because there are no
?gold segmentations? for unlabeled sentences, the
output sentence predicted by the current model is
compared with the agreements instead of the ?an-
swers? in the supervised case. At each parameter
173
update iteration k, each raw sentence xu is decod-
ed with the current model into a segmentation zu.
If the words in output zu do not match the agree-
ments A(xu) of the current sentence xu, the pa-
rameters are updated by adding the global feature
vector of the current training example with the a-
greements and subtracting the global feature vec-
tor of the decoder output, as described in lines 3
and 4 of Algorithm 2.
Algorithm 2 Parameter update in word-based model
1: for k = 1...K, u = 1...m do
2: calculate zu = argmax
w=GEN(x)
?|w|
i=1 ?(xu, wi) ? ?k?1w
3: if zu 6= A(xu)
4: ?kw = ?k?1w + ?(A(xu))? ?(zu)
5: end for
3.4 The Joint Score Function for Decoding
There are two co-regularized models as results of
the previous induction steps. An intuitive idea is
that both induced models are combined to conduct
the segmentation, for the sake of integrating their
strengths. This paper employs a log-linear inter-
polation combination (Bishop, 2006) to formulate
a joint scoring function based on character-based
and word-based models in the decoding:
Score(w) = ? ? log(p?c(y|x))
+(1? ?) ? log(?(x,w) ? ?w) (5)
where the two terms of the logarithm are the s-
cores of character-based and word-based model-
s, respectively, for a given segmentation w. This
composite function uses a parameter ? to weight
the contributions of the two models. The ? value
is tuned using the development data.
4 Experiment
4.1 Setting
The experimental data is taken from the Chinese
tree bank (CTB). In order to make a fair compar-
ison with the state-of-the-art results, the versions
of CTB-5, CTB-6, and CTB-7 are used for the e-
valuation. The training, development and testing
sets are defined according to the previous works.
For CTB-5, the data split from (Jiang et al, 2008)
is employed. For CTB-6, the same data split as
recommended in the CTB-6 official document is
used. For CTB-7, the datasets are formed accord-
ing to the way in (Wang et al, 2011). The cor-
responding statistic information on these data s-
plits is reported in Table 1. The unlabeled data in
our experiments is from the XIN CMN portion of
Chinese Gigaword 2.0. The articles published in
1991-1993 and 1999-2004 are used as unlabeled
data, with 204 million words.
The feature templates in (Zhao et al, 2006)
and (Zhang and Clark, 2007) are used in train-ing
the CRFs model and Perceptrons model, respec-
tively. The experimental platform is implement-
ed based on two popular toolkits: CRF++ (Kudo,
2005) and Zpar (Zhang and Clark, 2011).
Data #Sent-train
#Sent-
dev
#Sent-
test
OOV-
dev
OOV-
test
CTB-5 18,089 350 348 0.0811 0.0347
CTB-6 23,420 2,079 2,796 0.0545 0.0557
CTB-7 31,131 10,136 10,180 0.0549 0.0521
Table 1: Statistics of CTB-5, CTB-6 and CTB-7
data.
4.2 Main Results
The development sets are mainly used to tune the
values of the weight factor ? in Equation 5. We
evaluated the performance (F-score) of our model
on the three development sets by using differen-
t ? values, where ? is progressively increased in
steps of 0.1 (0 < ? < 1.0). The best performed
settings of ? for CTB-5, CTB-6 and CTB-7 on de-
velopment data are 0.7, 0.6 and 0.6, respectively.
With the chosen parameters, the test data is used
to measure the final performance.
Table 2 shows the F-score results of word seg-
mentation on CTB-5, CTB-6 and CTB-7 testing
sets. The line of ?ours? reports the performance
of our semi-supervised model with the tuned pa-
rameters. We first compare it with the supervised
?baseline? method which joints character-based
and word-based model trained only on the training
set1. It can be observed that our semi-supervised
model is able to benefit from unlabeled data and
greatly improves the results over the supervised
baseline. We also compare our model with two
state-of-the-art semi-supervised methods of Wang
?11 (Wang et al, 2011) and Sun ?11 (Sun and X-
u, 2011). The performance scores of Wang ?11 are
directly taken from their paper, while the results of
Sun ?11 are obtained, using the program provided
by the author, on the same experimental data. The
1The ?baseline? uses a different training configuration so
that the ? values in the decoding are also need to be tuned on
the development sets. The tuned ? values are {0.6, 0.6, 0.5}
for CTB-5, CTB-6 and CTB-7.
174
bold scores indicate that our model does achieve
significant gains over these two semi-supervised
models. This outcome can further reveal that us-
ing the agreements from these two views to regu-
larize the learning can effectively guide the mod-
el toward a better solution. The third compari-
son candidate is Hatori ?12 (Hatori et al, 2012)
which reported the best performance in the litera-
ture on these three testing sets. It is a supervised
joint model of word segmentation, POS tagging
and dependency parsing. Impressively, our model
still outperforms Hatori ?12 on all three datasets.
Although there is only a 0.01 increase on CTB-5,
it can be seen as a significant improvement when
considering Hatori ?12 employs much richer train-
ing resources, i.e., sentences tagged with syntactic
information.
Method CTB-5 CTB-6 CTB-7
Ours 98.27 96.33 96.72
Baseline 97.58 94.71 94.87
Wang ?11 98.11 95.79 95.65
Sun ?11 98.04 95.44 95.34
Hatori ?12 98.26 96.18 96.07
Table 2: F-score (%) results of five CWS models
on CTB-5, CTB-6 and CTB-7.
5 Conclusion
This paper proposed an alternative semi-
supervised CWS model that co-regularizes a
character- and word-based model by using their
segmentation agreements on unlabeled data. We
perform the agreements as valuable knowledge
for the regularization. The experiment results
reveal that this learning mechanism results in a
positive effect to the segmentation performance.
Acknowledgments
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau for
the funding support for our research, under the ref-
erence No. 017/2009/A and MYRG076(Y1-L2)-
FST13-WF. The authors also wish to thank the
anonymous reviewers for many helpful comments.
References
Christopher M. Bishop. 2006. Pattern recognition and
machine learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1-8, Philadelphia, USA.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of ma-chine
learning research, 7:551-585.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-View Learning over Struc-
tured and Non-Identical Outputs. In Proceedings of
CUAI, pages 204-211, Helsinki, Finland.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2012. Incremental Joint Approach
to Word Segmentation, POS Tagging, and Depen-
dency Parsing in Chinese. In Proceedings of ACL,
pages 1045-1053, Jeju, Republic of Korea.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging - A Case
Study. In Proceedings of ACL and the 4th IJCNLP
of the AFNLP, pages 522-530, Suntec, Singapore.
Feng Jiao, Shaojun Wang and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Pro-
ceedings of ACL and the 4th IJCNLP of the AFNLP,
pages 209-216, Strouds-burg, PA, USA.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Software available at http://crfpp.sourceforge. net.
John Lafferty, Andrew McCallum, and Fernando Pe-
reira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Weiwei Sun. 2001. Word-based and character-based
word segmentation models: comparison and com-
bination. In Proceedings of COLING, pages 1211-
1219, Bejing, China.
Weiwei Sun. 2011. A stacked sub-word model for
joint Chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL, pages 1385-1394,
Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan M-
cDonald, and Joakim Nivre. 2013. Token and Type
Constraints for Cross-Lingual Part-of-Speech Tag-
ging. In Transactions of the Association for Compu-
tational Linguistics, 1:1-12.
175
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A Character-Based Joint Model for Chinese Word
Segmentation. In Proceedings of COLING, pages
1173-1181, Bejing, China.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving Chinese word segmentation and
POS tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309-317, Hyderabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29-48.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation using a word-based perceptron algorithm. In
Proceedings of ACL, pages 840-847, Prague, Czech
Republic.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105-151.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 2006. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
176
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1360?1369,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Toward Better Chinese Word Segmentation for SMT via Bilingual
Constraints
Xiaodong Zeng
?
Lidia S. Chao
?
Derek F. Wong
?
Isabel Trancoso
?
Liang Tian
?
?
NLP
2
CT Lab / Department of Computer and Information Science, University of Macau
?
INESC-ID / Instituto Superior T?enico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {lidiasc, derekfw}@umac.mo,
isabel.trancoso@inesc-id.pt,tianliang0123@gmail.com
Abstract
This study investigates on building a
better Chinese word segmentation mod-
el for statistical machine translation. It
aims at leveraging word boundary infor-
mation, automatically learned by bilin-
gual character-based alignments, to induce
a preferable segmentation model. We
propose dealing with the induced word
boundaries as soft constraints to bias the
continuous learning of a supervised CRF-
s model, trained by the treebank data (la-
beled), on the bilingual data (unlabeled).
The induced word boundary information
is encoded as a graph propagation con-
straint. The constrained model induction
is accomplished by using posterior reg-
ularization algorithm. The experiments
on a Chinese-to-English machine transla-
tion task reveal that the proposed model
can bring positive segmentation effects to
translation quality.
1 Introduction
Word segmentation is regarded as a critical pro-
cedure for high-level Chinese language process-
ing tasks, since Chinese scripts are written in con-
tinuous characters without explicit word bound-
aries (e.g., space in English). The empirical works
show that word segmentation can be beneficial to
Chinese-to-English statistical machine translation
(SMT) (Xu et al, 2005; Chang et al, 2008; Zhao
et al, 2013). In fact most current SMT models
assume that parallel bilingual sentences should be
segmented into sequences of tokens that are meant
to be ?words? (Ma and Way, 2009). The practice
in state-of-the-art MT systems is that Chinese sen-
tences are tokenized by a monolingual supervised
word segmentation model trained on the hand-
annotated treebank data, e.g., Chinese treebank
(CTB) (Xue et al, 2005). These models are con-
ducive to MT to some extent, since they common-
ly have relatively good aggregate performance and
segmentation consistency (Chang et al, 2008).
But one outstanding problem is that these mod-
els may leave out some crucial segmentation fea-
tures for SMT, since the output words conform to
the treebank segmentation standard designed for
monolingually linguistic intuition, rather than spe-
cific to the SMT task.
In recent years, a number of works (Xu et al,
2005; Chang et al, 2008; Ma and Way, 2009;
Xi et al, 2012) attempted to build segmentation
models for SMT based on bilingual unsegment-
ed data, instead of monolingual segmented data.
They proposed to learn gainful bilingual knowl-
edge as golden-standard segmentation supervi-
sions for training a bilingual unsupervised mod-
el. Frequently, the bilingual knowledge refers to
the mappings of an individual English word to one
or more consecutive Chinese characters, generat-
ed via statistical character-based alignment. They
leverage such mappings to either constitute a Chi-
nese word dictionary for maximum-matching seg-
mentation (Xu et al, 2004), or form labeled data
for training a sequence labeling model (Paul et al,
2011). The prior works showed that these models
help to find some segmentations tailored for SMT,
since the bilingual word occurrence feature can be
captured by the character-based alignment (Och
and Ney, 2003). However, these models tend to
miss out other linguistic segmentation patterns as
monolingual supervised models, and suffer from
the negative effects of erroneously alignments to
word segmentation.
This paper proposes an alternative Chinese
Word Segmentation (CWS) model adapted to the
SMT task, which seeks not only to maintain the
advantages of a monolingual supervised model,
having hand-annotated linguistic knowledge, but
also to assimilate the relevant bilingual segmenta-
1360
tion nature. We propose leveraging the bilingual
knowledge to form learning constraints that guide
a supervised segmentation model toward a better
solution for SMT. Besides the bilingual motivat-
ed models, character-based alignment is also em-
ployed to achieve the mappings of the successive
Chinese characters and the target language word-
s. Instead of directly merging the characters in-
to concrete segmentations, this work attempts to
extract word boundary distributions for character-
level trigrams (types) from the ?chars-to-word?
mappings. Furthermore, these word boundaries
are encoded into a graph propagation (GP) expres-
sion, in order to widen the influence of the induced
bilingual knowledge among Chinese texts. The G-
P expression constrains similar types having ap-
proximated word boundary distributions. Crucial-
ly, the GP expression with the bilingual knowledge
is then used as side information to regularize a
CRFs (conditional random fields) model?s learn-
ing over treebank and bitext data, based on the
posterior regularization (PR) framework (Ganchev
et al, 2010). This constrained learning amounts to
a jointly coupling of GP and CRFs, i.e., integrating
GP into the estimation of a parametric structural
model.
This paper is structured as follows: Section 2
points out the main differences with the related
works of this study. Section 3 presents the de-
tails of the proposed segmentation model. Section
4 reports the experimental results of the proposed
model for a Chinese-to-English MT task. The con-
clusion is drawn in Section 5.
2 Related Work
In the literature, many approaches have been pro-
posed to learn CWS models for SMT. They can
be put into two categories, monolingual-motivated
and bilingual-motivated. The former primarily op-
timizes monolingual supervised models according
to some predefined segmentation properties that
are manually summarized from empirical MT e-
valuations. Chang et al (2008) enhanced a CRF-
s segmentation model in MT tasks by tuning the
word granularity and improving the segmentation
consistence. Zhang et al (2008) produced a bet-
ter segmentation model for SMT by concatenat-
ing various corpora regardless of their differen-
t specifications. Distinct from their behaviors,
this work uses automatically learned constraints
instead of manually defined ones. Most impor-
tantly, the constraints have a better learning guid-
ance since they originate from the bilingual texts.
On the other hand, the bilingual-motivated CWS
models typically rely on character-based align-
ments to generate segmentation supervisions. Xu
et al (2004) proposed to employ ?chars-to-word?
alignments to generate a word dictionary for max-
imum matching segmentation in SMT task. The
works in (Ma and Way, 2009; Zhao et al, 2013)
extended the dictionary extraction strategy. Ma
and Way (2009) adopted co-occurrence frequency
metric to iteratively optimize ?candidate words?
extract from the alignments. Zhao et al (2013) at-
tempted to find an optimal subset of the dictionary
learned by the character-based alignment to maxi-
mize the MT performance. Paul et al (2011) used
the words learned from ?chars-to-word? align-
ments to train a maximum entropy segmentation
model. Rather than playing the ?hard? uses of
the bilingual segmentation knowledge, i.e., direct-
ly merging ?char-to-word? alignments to words
as supervisions, this study extracts word bound-
ary information of characters from the alignments
as soft constraints to regularize a CRFs model?s
learning.
The graph propagation (GP) technique provides
a natural way to represent data in a variety of tar-
get domains (Belkin et al, 2006). In this tech-
nique, the constructed graph has vertices consist-
ing of labeled and unlabeled examples. Pairs of
vertices are connected by weighted edges encod-
ing the degree to which they are expected to have
the same label (Zhu et al, 2003). Many recent
works, such as by Subramanya et al (2010), Das
and Petrov (2011), Zeng et al (2013; 2014) and
Zhu et al (2014), proposed GP for inferring the la-
bel information of unlabeled data, and then lever-
age these GP outcomes to learn a semi-supervised
scalable model (e.g., CRFs). These approaches are
referred to as pipelined learning with GP. This s-
tudy also works with a similarity graph, encoding
the learned bilingual knowledge. But, unlike the
prior pipelined approaches, this study performs a
joint learning behavior in which GP is used as a
learning constraint to interact with the CRFs mod-
el estimation.
One of our main objectives is to bias CRF-
s model?s learning on unlabeled data, under a
non-linear GP constraint encoding the bilingual
knowledge. This is accomplished by the poste-
rior regularization (PR) framework (Ganchev et
1361
al., 2010). PR performs regularization on poste-
riors, so that the learned model itself remains sim-
ple and tractable, while during learning it is driven
to obey the constraints through setting appropriate
parameters. The closest prior study is constrained
learning, or learning with prior knowledge. Chang
et al (2008) described constraint driven learning
(CODL) that augments model learning on unla-
beled data by adding a cost for violating expec-
tations of constraint features designed by domain
knowledge. Mann and McCallum (2008) and M-
cCallum et al (2007) proposed to employ gener-
alized expectation criteria (GE) to specify prefer-
ences about model expectations in the form of lin-
ear constraints on some feature expectations.
3 Methodology
This work aims at building a CWS model adapted
to the SMT task. The model induction is shown in
Algorithm 1. The input data requires two type-
s of training resources, segmented Chinese sen-
tences from treebank D
c
l
and parallel unsegment-
ed sentences of Chinese and foreign language D
c
u
and D
f
u
. The first step is to conduct character-
based alignment over bitexts D
c
u
and D
f
u
, where
every Chinese character is an alignment target.
Here, we are interested on n-to-1 alignment pat-
terns, i.e., one target word is aligned to one or
more source Chinese characters. The second step
aims to collect word boundary distributions for al-
l types, i.e., character-level trigrams, according to
the n-to-1 mappings (Section 3.1). The third step
is to encode the induced word boundary informa-
tion into a k-nearest-neighbors (k-NN) similarity
graph constructed over the entire set of types from
D
c
l
and D
c
u
(Section 3.2). The final step trains a
discriminative sequential labeling model, condi-
tional random fields, on D
c
l
and D
c
u
under bilin-
gual constraints in a graph propagation expression
(Section 3.3). This constrained learning is carried
out based on posterior regularization (PR) frame-
work (Ganchev et al, 2010).
3.1 Word Boundaries Learned from
Character-based Alignments
The gainful supervisions toward a better segmen-
tation solution for SMT are naturally extracted
from MT training resources, i.e., bilingual parallel
data. This study employs an approximated method
introduced in (Xu et al, 2004; Ma and Way, 2009;
Chung and Gildea, 2009) to learn bilingual seg-
Algorithm 1 CWS model induction with bilingual
constraints
Require:
Segmented Chinese sentences from treebank
D
c
l
; Parallel sentences of Chinese and foreign
language D
c
u
and D
f
u
Ensure:
?: the CRFs model parameters
1: D
c?f
? char align bitext (D
c
u
,D
f
u
)
2: r ? learn word bound (D
c?f
)
3: G ? encode graph constraint (D
c
l
,D
c
u
, r)
4: ? ? pr crf graph (D
c
l
,D
c
u
,G)
mentation knowledge. This relies on statistical
character-based alignment: first, every Chinese
character in the bitexts is divided by a white s-
pace so that individual characters are regarded as
special ?words? or alignment targets, and second,
they are connected with English words by using
a statistical word aligner, e.g., GIZA++ (Och and
Ney, 2003). Note that the aligner is restricted to
use an n-to-1 alignment pattern. The primary idea
is that consecutive Chinese characters are grouped
to a candidate word, if they are aligned to the same
foreign word. It is worth mentioning that prior
works presented a straightforward usage for can-
didate words, treating them as golden segmenta-
tions, either dictionary units or labeled resources.
But this study treats the induced candidate word-
s in a different way. We propose to extract the
word boundary distributions
1
for character-level
trigrams (type)
2
, as shown in Figure 1, instead of
the very specific words. There are two main rea-
sons to do so. First, it is a more general expression
which can reduce the impact amplification of er-
roneous character alignments. Second, boundary
distributions can play more flexible roles as con-
straints over labelings to bias the model learning.
The type-level word boundary extraction is for-
mally described as follows. Given the ith sen-
tence pair ?x
c
i
, x
f
i
,A
c?f
i
? of the aligned bilin-
gual corpus D
c?f
, the Chinese sentence x
c
i
con-
sisting of m characters {x
c
i,1
, x
c
i,2
, ..., x
c
i,m
}, and
the foreign language sentence x
f
i
, consisting of
1
The distribution is on four word boundary labels indi-
cating the character positions in a word, i.e., B (begin), M
(middle), E (end) and S (single character).
2
A word boundary distribution corresponds to the center
character of a type. In fact, it aims at reducing label ambi-
guities to collect boundary information of character trigrams,
rather than individual characters (Altun et al, 2006).
1362
n words {x
f
i,1
, x
f
i,2
, ..., x
f
i,n
}, A
c?f
i
represents a
set of alignment pairs a
j
= ?C
j
, x
f
i,j
? that de-
fines connections between a few Chinese char-
acters C
j
= {x
c
i,j
1
, x
c
i,j
2
, ..., x
c
i,j
k
} and a sin-
gle foreign word x
f
i,j
. For an alignment a
j
=
?C
j
, x
f
i,j
?, only the sequence of characters C
j
=
{x
c
i,j
1
, x
c
i,j
2
, ..., x
c
i,j
k
} ?d ? [1, k?1], j
d+1
? j
d
=
1 constitutes a valid candidate word. For the w-
hole bilingual corpus, we assign each character
in the candidate words with a word boundary tag
T ? {B,M,E, S}, and then count across the en-
tire corpus to collect the tag distributions r
i
=
{r
i,t
; t ? T} for each type x
c
i,j?1
x
c
i,j
x
c
i,j+1
.
???
??
Beijin
g   Ol
ympu
s
Chara
cter-b
ased a
lignm
ent
???
??
BE 
   B   
M   E
Beijin
g   Ol
ympu
s
Word
 boun
daries
??? ??? ? Type-level W
ord 
bound
ary di
stribu
tions
BeiP
ing S
hi ???
BeiJi
ng Re
n ??? BeiJing
 Di ???
Quan
Yun H
ui ???
BeiJi
ng Sh
i ???
0.8 0
.6
0.3
0.2
0.9
AoYu
n Hui ??? 0.2
Figure 1: An example of similarity graph over
character-level trigrams (types).
3.2 Constraints Encoded by Graph
Propagation Expression
The previous step contributes to generate bilingual
segmentation supervisions, i.e., type-level word
boundary distributions. An intuitive manner is to
directly leverage the induced boundary distribu-
tions as label constraints to regularize segmenta-
tion model learning, based on a constrained learn-
ing algorithm. This study, however, makes further
efforts to elevate the positive effects of the bilin-
gual knowledge via the graph propagation tech-
nique. We adopt a similarity graph to encode
the learned type-level word boundary distribution-
s. The GP expression will be defined as a PR con-
straint in Section 3.3 that reflects the interactions
between the graph and the CRFs model. In other
words, GP is integrated with estimation of para-
metric structural model. This is greatly different
from the prior pipelined approaches (Subramanya
et al, 2010; Das and Petrov, 2011; Zeng et al,
2013), where GP is run first and its propagated
outcomes are then used to bias the structural mod-
el. This work seeks to capture the GP benefits dur-
ing the modeling of sequential correlations.
In what follows, the graph setting and propa-
gation expression are introduced. As in conven-
tional GP examples (Das and Smith, 2012), a sim-
ilarity graph G = (V,E) is constructed over N
types extracted from Chinese training data, includ-
ing treebank D
c
l
and bitexts D
c
u
. Each vertex V
i
has a |T |-dimensional estimated measure v
i
=
{v
i,t
; t ? T} representing a probability distribu-
tion on word boundary tags. The induced type-
level word boundary distributions r
i
= {r
i,t
; t ?
T} are empirical measures for the corresponding
M graph vertices. The edges E ? V
i
?V
j
connect
all the vertices. Scores between pairs of graph ver-
tices (types), w
ij
, refer to the similarities of their
syntactic environment, which are computed fol-
lowing the method in (Subramanya et al, 2010;
Das and Petrov, 2011; Zeng et al, 2013). The
similarities are measured based on co-occurrence
statistics over a set of predefined features (intro-
duced in Section 4.1). Specifically, the point-wise
mutual information (PMI) values, between ver-
tices and each feature instantiation that they have
in common, are summed to sparse vectors, and
their cosine distances are computed as the sim-
ilarities. The nature of this similarity graph en-
forces that the connected types with high weight-
s appearing in different texts should have similar
word boundary distributions.
The quality (smoothness) of the similarity graph
can be estimated by using a standard propagation
function, as shown in Equation 1. The square-loss
criterion (Zhu et al, 2003; Bengio et al, 2006) is
used to formulate this function:
P(v) =
T
?
t=1
(
M
?
i=1
(v
i,t
? r
i,t
)
2
+?
N
?
j=1
N
?
i=1
w
ij
(v
i,t
? v
j,t
)
2
+ ?
N
?
i=1
(v
i,t
)
2
)
(1)
The first term in this equation refers to seed match-
es that compute the distances between the estimat-
ed measure v
i
and the empirical probabilities r
i
.
The second term refers to edge smoothness that
measures how vertices v
i
are smoothed with re-
spect to the graph. Two types connected by an
edge with high weight should be assigned similar
word boundary distributions. The third term, a `
2
norm, evaluates the distribution sparsity (Das and
1363
Smith, 2012) per vertex. Typically, the GP process
amounts to an optimization process with respect
to parameter v such that Equation 1 is minimized.
This propagation function can be used to reflect
the graph smoothness, where the higher the score,
the lower the smoothness.
3.3 PR Learning with GP Constraint
Our learning problem belongs to semi-supervised
learning (SSL), as the training is done on treebank
labeled data (X
L
,Y
L
) = {(x
1
, y
1
), ..., (x
l
, y
l
)},
and bilingual unlabeled data (X
U
) = {x
1
, ..., x
u
}
where x
i
= {x
1
, ..., x
m
} is an input word se-
quence and y
i
= {y
1
, ..., y
m
}, y ? T is its corre-
sponding label sequence. Supervised linear-chain
CRFs can be modeled in a standard conditional
log-likelihood objective with a Gaussian prior:
L(?) = p
?
(y
i
|x
i
)?
???
2
2?
(2)
The conditional probabilities p
?
are expressed as a
log-linear form:
p
?
(y
i
|x
i
) =
exp(
m
?
k=1
?
T
f(y
k?1
i
, y
k
i
, x
i
))
Z
?
(x
i
)
(3)
Where Z
?
(x
i
) is a partition function that normal-
izes the exponential form to be a probability dis-
tribution, and f(y
k?1
i
, y
k
i
, x
i
) are arbitrary feature
functions.
In our setting, the CRFs model is required
to learn from unlabeled data. This work em-
ploys the posterior regularization (PR) frame-
work
3
(Ganchev et al, 2010) to bias the CRFs
model?s learning on unlabeled data, under a con-
straint encoded by the graph propagation expres-
sion. It is expected that similar types in the graph
should have approximated expected taggings un-
der the CRFs model. We follow the approach in-
troduced by (He et al, 2013) to set up a penalty-
based PR objective with GP: the CRFs likelihood
is modified by adding a regularization term, as
shown in Equation 4, representing the constraints:
R
U
(?, q) = KL(q||p
?
) + ?P(v) (4)
Rather than regularize CRFs model?s posteriors
p
?
(Y|x
i
) directly, our model uses an auxiliary
distribution q(Y|x
i
) over the possible labelings
3
The readers are refered to the original paper of Ganchev
et al (2010).
Y for x
i
, and penalizes the CRFs marginal log-
likelihood by a KL-divergence term
4
, represent-
ing the distance between the estimated posteriors
p and the desired posteriors q, as well as a penal-
ty term, formed by the GP function. The hy-
perparameter ? is used to control the impacts of
the penalty term. Note that the penalty is fired
if the graph score computed based on the expect-
ed taggings given by the current CRFs model is
increased vis-a-vis the previous training iteration.
This nature requires that the penalty term P(v)
should be formed as a function of posteriors q over
CRFs model predictions
5
, i.e., P(q). To state this,
a mappingM : ({1, ..., u}, {1, ...,m})? V from
words in the corpus to vertices in the graph is de-
fined. We can thus decompose v
i,t
into a function
of q as follows:
v
i,t
=
u?
a=1
m?
b=1;
M(a,b)=V
i
T?
c=1
?
y?Y
1(y
b
= t, y
b?1
= c)q(y|x
a
)
u?
a=1
m?
b=1
1(M(a, b) = V
i
)
(5)
The final learning objective combines the CRF-
s likelihood with the PR regularization term:
J (?, q) = L(?) + R
U
(?, q). This joint objec-
tive, over ? and q, can be optimized by an expecta-
tion maximization (EM) style algorithm as report-
ed in (Ganchev et al, 2010). We start from ini-
tial parameters ?
0
, estimated by supervised CRFs
model training on treebank data. The E-step is to
minimize R
U
(?, q) over the posteriors q that are
constrained to the probability simplex. Since the
penalty term P(v) is a non-linear form, the opti-
mization method in (Ganchev et al, 2010) via pro-
jected gradient descent on the dual is inefficient
6
.
This study follows the optimization method (He et
al., 2013) that uses exponentiated gradient descent
(EGD) algorithm. It allows that the variable up-
date expression, as shown in Equation 6, takes a
multiplicative rather than an additive form.
q
(w+1)
(y|x
i
) = q
(w)
(y|x
i
) exp(??
?R
?q
(w)
(y|x
i
)
)
(6)
where the parameter ? controls the optimization
rate in the E-step. With the contributions from
4
The form of KL term: KL(q||p) =
?
q?Y
q(y) log
q(y)
p(y)
.
5
The original PR setting also requires that the penalty ter-
m should be a linear (Ganchev et al, 2010) or non-linear (He
et al, 2013) function on q.
6
According to (He et al, 2013), the dual of quadratic pro-
gram implies an expensive matrix inverse.
1364
the E-step that further encourage q and p to agree,
the M-step aims to optimize the objective J (?, q)
with respect to ?. The M-step is similar to the stan-
dard CRFs parameter estimation, where the gradi-
ent ascent approach still works. This EM-style ap-
proach monotonically increases J (?, q) and thus
is guaranteed to converge to a local optimum.
E-step: q
(t+1)
= argmin
q
R
U
(?
(t)
, q
(t)
)
M-step: ?
(t+1)
= argmax
?
L(?)
+?
u
?
i=1
?
y?Y
q
(t+1)
(y|x
i
) log p
?
(y|x
i
)
(7)
4 Experiments
4.1 Data and Setup
The experiments in this study evaluated the per-
formances of various CWS models in a Chinese-
to-English translation task. The influence of
the word segmentation on the final translation
is our main investigation. We adopted three
state-of-the-art metrics, BLEU (Papineni et al,
2002), NIST (Doddington et al, 2000) and ME-
TEOR (Banerjee and Lavie, 2005), to evaluate the
translation quality.
The monolingual segmented data, train
TB
, is
extracted from the Penn Chinese Treebank (CTB-
7) (Xue et al, 2005), containing 51,447 sentences.
The bilingual training data, train
MT
, is formed
by a large in-house Chinese-English parallel cor-
pus (Tian et al, 2014). There are in total 2,244,319
Chinese-English sentence pairs crawled from on-
line resources, concentrated in 5 different domains
including laws, novels, spoken, news and miscel-
laneous
7
. This in-house bilingual corpus is the
MT training data as well. The target-side lan-
guage model is built on over 35 million mono-
lingual English sentences, train
LM
, crawled from
online resources. The NIST evaluation campaign
data, MT-03 and MT-05, are selected to comprise
the MT development data, dev
MT
, and testing da-
ta, test
MT
, respectively.
For the settings of our model, we adopted the
standard feature templates introduced by Zhao et
al. (2006) for CRFs. The character-based align-
ment for achieving the ?chars-to-word? mappings
is accomplished by GIZA++ aligner (Och and
Ney, 2003). For the GP, a 10-NNs similarity graph
7
The in-house corpus has been manually validated, in a
long process that exceeded 500 hours.
was constructed
8
. Following (Subramanya et al,
2010; Zeng et al, 2013), the features used to
compute similarities between vertices were (Sup-
pose given a type ?w
2
w
3
w
4
? surrounding contexts
?w
1
w
2
w
3
w
4
w
5
?): unigram (w
3
), bigram (w
1
w
2
,
w
4
w
5
, w
2
w
4
), trigram (w
2
w
3
w
4
, w
2
w
4
w
5
,
w
1
w
2
w
4
), trigram+context (w
1
w
2
w
3
w
4
w
5
) and
character classes in number, punctuation, alpha-
betic letter and other (t(w
2
)t(w
3
)t(w
4
)). There
are four hyperparameters in our model to be tuned
by using the development data (dev
MT
) among
the following settings: for the graph propagation,
? ? {0.2, 0.5, 0.8} and ? ? {0.1, 0.3, 0.5, 0.8};
for the PR learning, ? ? {0 ? ?
i
? 1} and ? ?
{0 ? ?
i
? 1} where the step is 0.1. The best per-
formed joint settings, ? = 0.5, ? = 0.5, ? = 0.9
and ? = 0.8, were used to measure the final per-
formance.
The MT experiment was conducted based on
a standard log-linear phrase-based SMT model.
The GIZA++ aligner was also adopted to obtain
word alignments (Och and Ney, 2003) over the
segmented bitexts. The heuristic strategy of grow-
diag-final-and (Koehn et al, 2007) was used to
combine the bidirectional alignments for extract-
ing phrase translations and reordering tables. A
5-gram language model with Kneser-Ney smooth-
ing was trained with SRILM (Stolcke, 2002) on
monolingual English data. Moses (Koehn et al,
2007) was used as decoder. The Minimum Error
Rate Training (MERT) (Och, 2003) was used to
tune the feature parameters on development data.
4.2 Various Segmentation Models
To provide a thorough analysis, the MT experi-
ments in this study evaluated three baseline seg-
mentation models and two off-the-shelf models,
in addition to four variant models that also employ
the bilingual constraints. We start from three base-
line models:
? Character Segmenter (CS): this model sim-
ply divides Chinese sentences into sequences
of characters.
? Supervised Monolingual Segmenter (SM-
S): this model is trained by CRFs on treebank
training data (train
TB
). The same feature
templates (Zhao et al, 2006) are used. The
standard four-tags (B, M, E and S) were used
8
We evaluated graphs with top k (from 3 to 20) nearest
neighbors on development data, and found that the perfor-
mance converged beyond 10-NNs.
1365
as the labels. The stochastic gradient descent
is adopted to optimize the parameters.
? Unsupervised Bilingual Segmenter (UBS):
this model is trained on the bitexts (trainMT)
following the approach introduced in (Ma
and Way, 2009). The optimal set of the mod-
el parameter values was found on dev
MT
to
be k = 3, t
AC
= 0.0 and t
COOC
= 15.
The comparison candidates also involve two pop-
ular off-the-shelf segmentation models:
? Stanford Segmenter: this model, trained by
Chang et al (2008), treats CWS as a binary
word boundary decision task. It covers sev-
eral features specific to the MT task, e.g., ex-
ternal lexicons and proper noun features.
? ICTCLAS Segmenter: this model, trained
by Zhang et al (2003), is a hierarchical
HMM segmenter that incorporates parts-of-
speech (POS) information into the probabili-
ty models and generates multiple HMM mod-
els for solving segmentation ambiguities.
This work also evaluated four variant models
9
that perform alternative ways to incorporate the
bilingual constraints based on two state-of-the-art
graph-based SSL approaches.
? Self-training Segmenters (STS): two vari-
ant models were defined by the approach re-
ported in (Subramanya et al, 2010) that us-
es the supervised CRFs model?s decodings,
incorporating empirical and constraint infor-
mation, for unlabeled examples as additional
labeled data to retrain a CRFs model. One
variant (STS-NO-GP) skips the GP step, di-
rectly decoding with type-level word bound-
ary probabilities induced from bitexts, while
the other (STS-GP-PL) runs the GP at first
and then decodes with GP outcomes. The
optimal hyperparameter values were found to
be: STS-NO-GP (? = 0.8) and ? = 0.6) and
STS-GP-PL (? = 0.5, ? = 0.3, ? = 0.8 and
? = 0.6).
? Virtual Evidences Segmenters (VES): T-
wo variant models based on the approach
in (Zeng et al, 2013) were defined. The type-
level word boundary distributions, induced
9
Note that there are two variant models working with GP.
To be fair, the same similarity graph settings introduced in
this paper were used.
by the character-based alignment (VES-NO-
GP), and the graph propagation (VES-GP-
PL), are regarded as virtual evidences to bias
CRFs model?s learning on the unlabeled da-
ta. The optimal hyperparameter values were
found to be: VES-NO-GP (? = 0.7) and
VES-GP-PL (? = 0.5, ? = 0.3 and ? = 0.7).
4.3 Main Results
Table 1 summarizes the final MT performance on
the MT-05 test data, evaluated with ten different
CWS models. In what follows, we summarized
four major observations from the results. First-
ly, as expected, having word segmentation does
help Chinese-to-English MT. All other nine CWS
models outperforms the CS baseline which does
not try to identify Chinese words at all. Second-
ly, the other two baselines, SMS and UBS, are on
a par with each other, showing less than 0.36 av-
erage performance differences on the three eval-
uation metrics. This outcome validated that the
models, trained by either the treebank or the bilin-
gual data, performed reasonably well. But they
only capture partial segmentation features so that
less gains for SMT are achieved when compar-
ing to other sophisticated models. Thirdly, we no-
tice that the two off-the-shelf models, Stanford and
ICTCLAS, just brought minor improvements over
the SMS baseline, although they are trained us-
ing richer supervisions. This behaviour illustrates
that the conventional optimizations to the mono-
lingual supervised model, e.g., accumulating more
supervised data or predefined segmentation prop-
erties, are insufficient to help model for achiev-
ing better segmentations for SMT. Finally, high-
lighting the five models working with the bilingual
constraints, most of them can achieve significant
gains over the other ones without using the bilin-
gual constraints. This strongly demonstrates that
bilingually-learned segmentation knowledge does
helps CWS for SMT. The models working with G-
P, STS-GP-PL, VES-GP-PL and ours outperform
all others. We attribute this to the role of GP in
assisting the spread of bilingual knowledge on the
Chinese side. Importantly, it can be observed that
our model outperforms STS-GP, VES-GP, which
greatly supports that joint learning of CRFs and
GP can alleviate the error transfer by the pipelined
models. This is one of the most crucial findings
in this study. Overall, the boldface numbers in the
last row illustrate that our model obtains average
improvements of 1.89, 1.76 and 1.61 on BLEU,
1366
NIST and METEOR over others.
Models BLEU NIST METEOR
CS 29.38 59.85 54.07
SMS 30.05 61.33 55.95
UBS 30.15 61.56 55.39
Stanford 30.40 61.94 56.01
ICTCLAS 30.29 61.26 55.72
STS-NO-GP 31.47 62.35 56.12
STS-GP-PL 31.94 63.20 57.09
VES-NO-GP 31.98 62.63 56.59
VES-GP-PL 32.04 63.49 57.34
Our Model 32.75 63.72 57.64
Table 1: Translation performances (%) on MT-05
testing data by using ten different CWS models.
4.4 Analysis & Discussion
This section aims to further analyze the three pri-
mary observations concluded in Section 4.3: i)
word segmentation is useful to SMT; ii) the tree-
bank and the bilingual segmentation knowledge
are helpful, performing segmentation of differen-
t nature; and iii) the bilingual constraints lead to
learn segmentations better tailored for SMT.
The first observation derives from the compar-
isons between the CS baseline and other model-
s. Our results, showing the significant CWS ben-
efits to SMT, are consistent with the works re-
ported in the literature (Xu et al, 2004; Chang
et al, 2008). In our experiment, two additional
evidences found in the translation model are pro-
vided to further support that NO tokenization of
Chinese (i.e., the CS model?s output) could har-
m the MT system. First, the SMT phrase extrac-
tion, i.e., building ?phrases? on top of the char-
acter sequences, cannot fully capture all meaning-
ful segmentations produced by the CS model. The
character based model leads to missing some use-
ful longer phrases, and to generate many meaning-
less or redundant translations in the phrase table.
Moreover, it is affected by translation ambiguities,
caused by the cases where a Chinese character has
very different meanings in different contextual en-
vironments.
The second observation shifts the emphasis to
SMS and UBS, based on the treebank and the
bilingual segmentation, respectively. Our result-
s show that both segmentation patterns can bring
positive effects to MT. Through analyzing both
models? segmentations for train
MT
and test
MT
,
we attempted to get a closer inspection on the seg-
mentation preferences and their influence on MT.
Our first finding is that the segmentation consen-
suses between SMS and UBS are positive to MT.
There have about 35% identical segmentations
produced by the two models. If these identical
segmentations are removed, and the experiments
are rerun, the translation scores decrease (on av-
erage) by 0.50, 0.85 and 0.70 on BLEU, NIST
and METEOR, respectively. Our second finding
is that SMS exhibits better segmentation consis-
tency than UBS. One representative example is the
segmentations for ???? (lonely)?. All the out-
puts of SMS were ?????, while UBS generat-
ed three ambiguous segmentations, ??(alone) ?
?(double zero)?, ???(lonely) ?(zero)? and
??(alone) ?(zero) ?(zero)?. The segmentation
consistency of SMS rests on the high-quality tree-
bank data and the robust CRFs tagging mod-
el. On the other hand, the advantage of UB-
S is to capture the segmentations matching the
aligned target words. For example, UBS grouped
??(country) ?(border) ?(between)? to a word
????(international)?, rather than two word-
s ???(international) ?(between)? (as given by
SMS), since these three characters are aligned to
a single English word ?international?. The above
analysis shows that SMS and UBS have their own
merits and combining the knowledge derived from
both segmentations is highly encouraged.
The third observation concerns the great im-
pact of the bilingual constraints to the segmenta-
tion models in the MT task. The use of the bilin-
gual constraints is the prime objective of this s-
tudy. Our first contribution for this purpose is
on using the word boundary distributions to cap-
ture the bilingual segmentation supervisions. This
representation contributes to reduce the negative
impacts of erroneous ?chars-to-word? alignments.
The ambiguous types (having relatively uniform
boundary distribution), caused by alignment er-
rors, cannot directly bias the model tagging pref-
erences. Furthermore, the word boundary distri-
butions are convenient to make up the learning
constraints over the labelings among various con-
strained learning approaches. They have success-
fully played in three types of constraints for our
experiments: PR penalty (Our model), decoding
constraints in self-training (STS) and virtual evi-
dences (VES). The second contribution is the use
of GP, illustrated by STS-GP-PL, VES-GP-PL and
1367
Our model. The major effect is to multiply the im-
pacts of the bilingual knowledge through the sim-
ilarity graph. The graph vertices (types)
10
, with-
out any supervisions, can learn the word bound-
ary information from their similar types (neigh-
borhoods) having the empirical boundary prob-
abilities. The segmentations given by the three
GP models show about 70% positive segmenta-
tion changes, affected by the unlabeled graph ver-
tices, with respect to the ones given by the NO-
GP models, STS-NO-GP and VES-NO-GP. In our
opinion, the learning mechanism of our approach,
joint coupling of GP and CRFs, rather than the
pipelined one as the other two models, contributes
to maximizing the graph smoothness effects to the
CRFs estimation so that the error propagation of
the pipelined approaches is alleviated.
5 Conclusion
This paper proposed a novel CWS model for the
SMT task. This model aims to maintain the lin-
guistic segmentation supervisions from treebank
data and simultaneously integrate useful bilingual
segmentations induced from the bitexts. This ob-
jective is accomplished by three main steps: 1)
learn word boundaries from character-based align-
ments; 2) encode the learned word boundaries into
a GP constraint; and 3) training a CRFs model, un-
der the GP constraint, by using the PR framework.
The empirical results indicate that the proposed
model can yield better segmentations for SMT.
Acknowledgments
The authors are grateful to the Science and
Technology Development Fund of Macau and
the Research Committee of the University of
Macau (Grant No. MYRG076 (Y1-L2)-FST13-
WF and MYRG070 (Y1-L2)-FST12-CS) for the
funding support for our research. The work of
Isabel Trancoso was supported by national funds
through FCT-Fundac??ao para a Ci?ecia e a Tecnolo-
gia, under project PEst-OE/EEI/LA0021/2013.
The authors also wish to thank the anonymous re-
viewers for many helpful comments.
10
This experiment yielded a similarity graph that consists
of 11,909,620 types from train
TB
and train
MT
, where there
have 8,593,220 (72.15%) types without any empirical bound-
ary distributions.
References
Yasemin Altun, David McAllester, and Mikhail Belkin.
2006. Maximum margin semi-supervised learning
for structured variables. Advances in Neural Infor-
mation Processing Systems, 18:33.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved
correlation with human judgments. In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic E-
valuation Measures for Machine Translation and/or
Summarization, pages 65?72. Association for Com-
putational Linguistics.
Yoshua Bengio, Olivier Delalleau, and Nicolas
Le Roux. 2006. Label propagation and quadrat-
ic criterion. Semi-Supervised Learning, pages 193?
216.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of WMT, pages 224?232. Association for
Computational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of EMNLP, pages 718?726. Association
for Computational Linguistics.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL, pages 600?609.
Association for Computational Linguistics.
Dipanjan Das and Noah A Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of NAACL, pages 677?687. Associa-
tion for Computational Linguistics.
George R. Doddington, Mark A. Przybocki, Alvin F.
Martin, and Douglas A. Reynolds. 2000. The nist
speaker recognition evaluation?overview, methodol-
ogy, systems, results, perspective. Speech Commu-
nication, 31(2):225?254.
Kuzman Ganchev, J?oao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 11:2001?2049.
Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of CoNLL, page 38. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL on Interactive Poster and Demon-
stration Sessions, pages 177?180. Association for
Computational Linguistics.
1368
Yanjun Ma and Andy Way. 2009. Bilingually motivat-
ed domain-adapted word segmentation for statistical
machine translation. In Proceedings of EACL, pages
549?557. Association for Computational Linguistic-
s.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL, pages 870?878. Association for Com-
putational Linguistics.
Andrew McCallum, Gideon Mann, and Gregory
Druck. 2007. Generalized expectation criteri-
a. Computer Science Technical Note, University of
Massachusetts, Amherst, MA.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of A-
CL, pages 160?167. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic e-
valuation of machine translation. In Proceedings of
ACL, pages 311?318. Association for Computation-
al Linguistics.
Michael Paul, Finch Andrew, and Sumita Eiichiro.
2011. Integration of multiple bilingually-trained
segmentation schemes into statistical machine trans-
lation. IEICE Transactions on Information and Sys-
tems, 94(3):690?697.
Andreas Stolcke. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of Inter-
speech.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models. In
Proceedings of EMNLP, pages 167?176. Associa-
tion for Computational Linguistics.
Liang Tian, Derek F. Wong, Lidia S. Chao, Paulo
Quaresma, Francisco Oliveira, Shuo Li, Yiming
Wang, and Yi Lu. 2014. UM-Corpus: A large
English-Chinese parallel corpus for statistical ma-
chine translation. In Proceedings of LREC. Euro-
pean Language Resources Association.
Ning Xi, Guangchao Tang, Xinyu Dai, Shujian Huang,
and Jiajun Chen. 2012. Enhancing statistical ma-
chine translation with character alignment. In Pro-
ceedings of ACL, pages 285?290. Association for
Computational Linguistics.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need Chinese word segmentation for statistical
machine translation? In Proceedings of the Third
SIGHAN Workshop on Chinese Language Learning,
pages 122?128. Association for Computational Lin-
guistics.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation
in statistical machine translation. In Proceedings of
IWSLT, pages 216?223. Association for Computa-
tional Linguistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and Is-
abel Trancoso. 2013. Graph-based semi-supervised
model for joint Chinese word segmentation and part-
of-speech tagging. In Proceedings of ACL, pages
770?779. Association for Computational Linguistic-
s.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, Is-
abel Trancoso, Liangye He, and Qiuping Huang.
2014. Lexicon expansion for latent variable gram-
mars. Pattern Recognition Letters, 42:47?55.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. HHMM-based Chinese lexical analyzer
ICTCLAS. In Proceedings of the Second SIGHAN
Workshop on Chinese Language Processing, pages
184?187. Association for Computational Linguistic-
s.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceed-
ings of WMT, pages 216?223. Association for Com-
putational Linguistics.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing. Association for Computational Linguistics.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for Chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248?263. Springer.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of
ICML, volume 3, pages 912?919.
Ling Zhu, Derek F. Wong, and Lidia S. Chao. 2014.
Unsupervised chunking based on graph propagation
from bilingual corpus. The Scientific World Journal,
2014(401943):10.
1369
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
UM-Checker: A Hybrid System for English Grammatical Error Cor-
rection 
 
 
Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
nlp2ct.{vincent, anson}@gmail.com,  
{derekfw, lidiasc}@umac.mo, nlp2ct.samuel@gmail.com 
 
  
 
Abstract 
This paper describes the NLP2CT Grammati-
cal Error Detection and Correction system for 
the CoNLL 2013 shared task, with a focus on 
the errors of article or determiner (ArtOrDet), 
noun number (Nn), preposition (Prep), verb 
form (Vform) and subject-verb agreement 
(SVA). A hybrid model is adopted for this spe-
cial task. The process starts with spell-
checking as a preprocessing step to correct any 
possible erroneous word. We used a Maxi-
mum Entropy classifier together with manual-
ly rule-based filters to detect the grammatical 
errors in English. A language model based on 
the Google N-gram corpus was employed to 
select the best correction candidate from a 
confusion matrix. We also explored a graph-
based label propagation approach to overcome 
the sparsity problem in training the model. Fi-
nally, a number of deterministic rules were 
used to increase the precision and recall. The 
proposed model was evaluated on the test set 
consisting of 50 essays and with about 500 
words in each essay. Our system achieves the 
5
th
 and 3
rd
 F1 scores on official test set among 
all 17 participating teams based on gold-
standard edits before and after revision, re-
spectively.  
1 Introduction 
With the increasing number of people all over 
the world who study English as their second lan-
guage1, grammatical errors in writing often oc-
curs due to cultural diversity, language habits, 
education background, etc. Thus, there is a sub-
stantial and increasing need of using computer 
                                                 
    1 A well-known fact is that the most popular language 
chosen as a first foreign language is English. 
techniques to improve the writing ability for sec-
ond language learners. Grammatical error correc-
tion is the task of automatically detecting and 
correction erroneous word usage and ill-formed 
grammatical constructions in text (Dahlmeier et 
al., 2012). 
In recent decades, this special task has gained 
more attention by some organizations such as the 
Helping Our Own (HOO) challenge (Dale and 
Kilgarriff, 2010; Dale et al, 2012). Although the 
performance of grammatical error correction sys-
tems has been improved, it is still mostly limited 
to dealing with the determiner and preposition 
error types with a very low recall and precision. 
This year, the CoNLL-2013 shared task extends 
to include a more comprehensive list of error 
types, as shown in Table 1. 
To take on this challenge, this paper proposes 
pipe-line architecture in combination with sever-
al error detection and correction models based on 
a hybrid approach. As a preprocessing step we 
firstly employ a spelling correction to correct the 
misspelled words. To correct the grammatical 
errors, a hybrid system is designed that integrat-
ed with Maximum Entropy (ME) classifier, de-
terministic filter and N-gram language model 
scorer, each of which is constructed as an indi-
vidual model. According to the phenomena of 
the problems, we use different combinations of 
the models trained on specific data to tackle the 
corresponding types of errors. For instance, Prep 
and Nn have a strong inter-relation with the 
words (surface) that are preceding and following 
the active word. This can be detected and recov-
ered by using a language model. On the other 
hand, SVA is more complicated and it is more 
effective to determine the mistakes by using the 
linguistic and grammatical rules. The correction
34
Error Type Description Example 
Vform 
Replacement The solution can be obtain (obtained) by using technology. 
Insertion 
However, the world has always beyond our imagination and ? (has) 
never let us down. 
Deletion It also indicates that the economy has been (?) dramatically grown. 
SVA 
Subject-verb-
Agreement 
My brothers is (are) nutritionists. 
ArtOrDet 
Replacement 
The leakage of these (this) confidential information can be a sensitive 
issue to personal, violation of freedom and breakdown of safety. 
Insertion The survey was done by ? (the) United Nations. 
Deletion 
The air cargo of the (?) Valujet plane was on fire after the plane had 
taken off. 
Nn Noun number He receives two letter (letters). 
Prep 
Replacement They work under (in) a conductive environment. 
Insertion 
Definitely, there are point of view that agree ? (with) the technology 
but also the voices of objection. 
Deletion 
Today, the surveillance technology has become almost manifest to (?) 
wherever we go. 
 
Table 1: The error types with descriptions and examples. 
 
components are combined into a pipeline of cor-
rection steps to form an end-to-end correction 
system. Different types of corrections may inter-
act with each other. Therefore, only for each fo-
cus word in a sentence will pass the filter and 
predict by the system. 
Take the sentence for example, ?The patent 
applications do not need to be censored.?, if the 
word ?applications? is changed to ?application? 
(Nn error) by a correction module, then the fol-
lowing auxiliary verb ?do? should be revised to 
?does? (SVA error) accordingly. That is, if a mis-
take is introduced by a component in the prior 
step, subsequent analyses are most likely affect-
ed negatively. To avoid the errors propagated 
into further components, we proposed to deploy 
the analytical (pipelined) components in the or-
der of Nn, ArtOrDet, Vform, SVA and Prep. 
For non-native language learners, over 90% 
usage of prepositions and articles are correctly 
used, which makes the errors very sparse (Ro-
zovskaya and Roth, 2010c) in a text, and about 
10% error is not ?sparse? by the way. This factor 
severely restricts the improvement of data-driven 
systems. Different from the previous methods to 
overcome error sparsity, we explored a graph-
based label propagation method that makes use 
of the prediction on large amount of unlabeled 
data. The predicted data are then used to 
resample our training data. This semi-supervised 
method may fix a skewed label distribution in the 
training set and is helpful to enhance the models.  
The paper is organized as follows. We firstly 
review and discuss the related work. The data 
used to construct the models is described in Sec-
tion 3. Section 4 discusses the proposed model 
based on semi-supervised learning, and the over-
all hybrid system is given in Section 5. The 
methods of grammatical error detection and cor-
rection are detailed in Section 6, followed by an 
evaluation, discussion and a conclusion to end 
the paper. 
2 Related Work 
The issues of grammatical error correction have 
been discussed from different perspectives for 
several decades. In this section, we briefly re-
view some related methods. 
The use of machine learning methods to tackle 
this problem has shown a promising perfor-
mance. These methods are normally created 
based on a large corpus of well-formed native 
English texts (Tetreault and Chodorow 2008; 
Tetreault et al, 2010) or annotated non-native 
data (Gamon, 2010; Han et al, 2010). Although 
the manually error-tagged text is much more ex-
pensive, it has shown improvements over the 
models trained solely on well-formed native text 
(Kochmar et al, 2012). Additionally, both gener-
ative and discriminative classifiers were widely 
used. Among them, Maximum Entropy was gen-
erally used (Rozovskaya and Roth, 2011; 
Sakaguchi et al, 2012; Quan et al, 2012) and 
obtained a good result for preposition and article 
correction using a large feature set. Naive Bayes 
35
were also applied to recognize or correct the er-
rors in speech or texts (Lynch et al, 2012). How-
ever, only using classifiers always cannot give a 
satisfied performance. Thus, grammar rules and 
probabilistic language model can be used as a 
simple but effective assistant for correction of 
spelling (Kantrowitz et al 2003) and grammati-
cal errors (Dahlmeier et al, 2012; Lynch et al, 
2012; Quan et al, 2012; Rozovskaya et al, 
2012). 
3 Data Set 
The training data is the NUS Corpus of Learner 
English (NUCLE) that provided by the National 
University of Singapore (Dahlmeier et al, 2013). 
The NUCLE contains more than one million 
words (1,400 essays) and has been annotated 
with error-tags and correction-labels. There are 
27 categories of errors, with 45,106 errors in to-
tal. In this CoNLL-2013 shared task, five types 
of errors (around 32% of the total errors) are 
concerned. Figure 1 shows the statistics infor-
mation of error types. 
 
 
 
Figure 1. The distribution of different error types in 
the training set. 
 
As the distribution of different errors respects 
the real environment, there is a serious problem 
hidden in it. Roughly estimated, the ratio be-
tween the correct and error classes in NUCLE is 
around 100:1, or even more. The imbalance 
problem may be heavily harmful to machine 
learning methods. Therefore, researchers (Ro-
zovskaya et al, 2012; Dahlmeier et al, 2012) 
provided several approaches such as reducing 
correct instances to deal with error sparsity. In-
stead of downsampling the data, we try to up-
sample error instances. Different from UI system 
(Rozovskaya et al, 2012) which simulates learn-
ers to make mistakes artificially, we propose a 
semi-supervised learning method that makes use 
of a large amount of unlabeled data which is easy 
to collect. In practice, semi-supervised learning 
requires less human effort and gives higher accu-
racy in creating a model.  
4 Error Examples Expansion Using 
Graph-Based Label Propagation  
As mentioned before, the corpus contains a low 
amount of error examples, which results in a 
high sparsity in the label distribution. In reality, 
the balance between the error and correct data is 
crucial for training a robust grammar detection 
models. Our experiment results demonstrate that 
too many correct data lead to unfavorable error 
detection rate. In order to resolve this obstacle, 
this paper introduces to using external data 
sources, i.e., a large amount of easily accessible 
raw texts, to automatically achieve more labeled 
example for training a stronger model. This pa-
per employs transductive graph-based semi-
supervised learning approach. 
4.1 Graph-Based Label Propagation 
Graph-based label propagation is one of the criti-
cal subclasses of SSL. Graph-based label propa-
gation methods have recently shown they can 
outperform the state-of-the-art in several natural 
language processing (NLP) tasks, e.g., POS tag-
ging (Subramanya et al, 2010), knowledge ac-
quisition (Talukdar et al, 2008), shallow seman-
tic parsing for unknown predicate (Das and 
Smith, 2011).  This study uses graph SSL to en-
rich training data, mainly the examples with in-
correct tag, from raw texts.  
This approach constructs a k nearest-neighbor 
(k-nn) similarity graph over the labeled and un-
labeled data in the first step. The vertices in the 
constructed graph consist of all instances (feature 
vector) that occur in labeled and unlabeled text, 
and edge weights between vertices are computed 
using their Euclidean distance. Pairs of vertices 
are connected by weighted edges which encode 
the degree to which they are expected to have the 
same label (Zhu, 2003). In the second step, label 
propagation operates on the constructed graph. 
The primary objective is to propagate labels from 
a few labeled vertices to the unlabeled ones by 
optimizing a loss function based on the con-
straints or properties derived from the graph, e.g. 
smoothness (Zhu et al, 2003; Subramanya and 
Bilmes, 2008; Talukdar et al, 2009), or sparsity 
(Das and Smith, 2012). This paper uses propaga-
tion method (MAD) in (Talukdar et al, 2009).  
Vform
9%
SVA
10%
ArtOrDet
42%
Nn
24%
P ep
15%
36
  
 
Figure 2. Workflow of our proposed system. 
4.2 Implementation 
In this paper, the labeled data is taken from NU-
CLE corpus. They are regarded as the ?seed? 
data, including 93,000 correct and 1,200 incor-
rect instances. The unlabeled data is collected 
from the English side of news magazine corpus 
(LDC2005T10). Based on that, a 5-NN similarity 
graph is constructed. With the graph and the 
properties of the labeled data derived from the 
NUCLE, the MAD algorithm is used to propa-
gate the error-tag (label) from labeled vertices to 
the unlabeled vertices. Afterwards, the unlabeled 
examples with incorrect tag are added into the 
original training data for training. 
5 System Description 
This section describes the details of our system, 
including preprocessing of training set, confusion 
set generating, classifier training and language 
models building. The grammatical error correc-
tion procedure is shown in Figure 2. 
5.1 Preprocessing 
As mentioned in Section 3, there is a large 
amount (68%) of other error types which may 
result in new errors or confuse the system with 
wrong information in correction. In order to 
make the best use of the corpus, it needs to filter 
all errors not covered by the CoNLL 2013 shared 
task, and then generate a separate corpus for each 
error type. Therefore, we recovered other irrele-
vant errors accordingly. For each error type, we 
also recover other 4 types of errors, and then we 
got a pure training data set which only includes 
one error type.  
For the misspelled problem, we used an open 
source toolkit (JMySpell2) which allows us to 
use the dictionaries form OpenOffice. JMySpell 
                                                 
    2 Available at https://kenai.com/projects/jmyspell. 
gives a list of suggestion candidate words, and 
we select the first one to replace the original 
word.  
5.2 Confusion Set Generating 
Confusion sets include the correction candidates 
which are used to modify the wrong places of a 
sentence. We generated a confusion set for each 
type of error correction component.  
The confusion set for Nn, Vform and SVA was 
built on Penn Treebank3. The format can be de-
scribed as that each prototype word follows all 
possible combinations with Part-Of-Speech (POS) 
and variants. For instance, the format of the word 
?look? in confusion set should looks like ?look 
look#VB look#VBP looking#VBG looks#VBZ 
looked#VBN look#NN looks#NNS?. The proto-
type ?look? and POS are the constraints for 
choosing the correct candidate. In order to quick-
ly find the candidates according to each detected 
error place, we indexed the confusion set in Lu-
cene4 which is another open source toolkit with a 
high-performance, full-featured text search en-
gine library. 
For ArtOrDet and Prep, the confusion sets are 
manually created because the possible modifica-
tions are not so many which are discussed in 
Section 6.1 and 6.2. 
5.3 Maximum Entropy Classifier 
The machine learning algorithm we used to train 
the detection models is Maximum Entropy (ME), 
which can classify the data by giving a probabil-
ity distribution. It is similar to multiclass logistic 
regression models, but much more profitable 
with sparse explanatory feature vectors. For ME 
classifier, the feature of text data is suitable for 
training the model, so we choose it as our detec-
tion classifier.  
                                                 
    3 Available at http://www.cis.upenn.edu/~treebank/. 
    4 Available at http://lucene.apache.org/. 
Source
Text
Rule-based 
Filter
ArtOrDet
LM Scorer
Nn
ME
classifier
LM Scorer
Vform
SVA
ME
classifier
Rule-based 
Filter
Rule-based 
Filter
Rule-based 
Filter
Prep
LM Scorer
Hybrid System
Correct
Text
37
We employed Stanford Classifier5 which is a 
Java implementation of maximum entropy 
(Manning & Klein, 2003).  
5.4 N-gram Language Model 
The probabilistic language model is constructed 
on Google Web 1T 5-gram corpus (Brants and 
Franz, 2006) by using the SRILM toolkit 
(Stolcke, 2002). All generated modification can-
didates are scored by it and only candidates that 
strictly increase than a threshold can be kept.  
The normalized language model score is de-
fined as 
1 log Pr( )lmscore ss?
                (1) 
in which s is the corrected sentence and |s| is the 
sentence length in tokens (Dahlmeier et al, 
2012). 
6 Grammatical Error Correction 
6.1 Article and Determiner 
The component for ArtOrDet task integrates with 
the language model and rule-based techniques. 
Language models are constructed to select the 
best candidate from a confusion set of possible 
article choices {a, the, an, ?}, given the pre-
corrected sentence. Each Noun Phrase (NP) in 
the test sentence will be pre-corrected as correc-
tion candidates. However, only using a language 
model to determine the best correction will often 
result in a low precision, because a certain 
amount of correct usages of ArtOrDet are mis-
judged. 
In order to avoid this problem, we proposed a 
voting method based on multiple language mod-
els. We integrated two separate language models: 
one was converted from the large Google corpus 
(general LM) and the other one was constructed 
from a small in-domain corpus (in-domain LM). 
Additionally, the in-domain corpus involves two 
parts. One is the training data which has been 
totally corrected according to the gold answer. 
The other one includes the sentences which are 
similar to the test set. We extracted them from 
some well-formed native English corpora such as 
English News Magazine of LDC2005T106 using 
term frequency-inverse document frequency (TF-
IDF) as the similarity score. Each document Di is 
                                                 
    5 Available at 
http://nlp.stanford.edu/software/classifier.shtml. 
    
6
 Available at http://www.ldc.upenn.edu/Catalog/catalog 
Entry.jsp?catalogId=LDC2005T10. 
represented as a vector (wi1, wi2,?, win), and n is 
the size of the vocabulary. So wij is calculated as 
follows: 
 )log( jijij idftfw ??  (2) 
where tfij is term frequency (TF) of the j-th word 
in the vocabulary in the document Di, and idfj is 
the is the inverse document frequency (IDF) of 
the j-th word calculated. The similarity between 
two sentences is then defined as the cosine of the 
angle between two vectors.  
Each candidate sentence will be scored by 
these two LMs and compared with a threshold. 
Only if both of the LMs agree, the modification 
will be kept. We believe this method could filter 
a lot of wrong modification and improve the pre-
cision. 
6.2 Preposition 
For Prep error type, we used the same method as 
ArtOrDet. The only difference is confusion ma-
trix. Our system corrects the unnecessary, miss-
ing and unwanted errors for the five most fre-
quently prepositions which are in, for, to, of and 
on. While developing our system, we found that 
adding more prepositions did not increase per-
formance in our experiments. Thus the confusion 
set is {in, for, to, of, on, ?}. 
6.3 Noun Number 
A single noun in the sentence that is hard to dis-
tinguish whether it is singular or plural, so we 
treat a noun phrase as a observe subject. Our 
strategy of correcting noun number error is to use 
a filter contains rule-based and machine learning 
method. It can filter a part of nouns that absolute-
ly right, and the rest of nouns will be detected by 
the language model generated by SRILM7. 
The rule-based filter of our system contains 
several criteria. It can detect the noun phrase by 
article, i.e. it can simply find out that the noun is 
singular which with an article of ?a? or ?an?. 
The determiner and cardinal number also will be 
taken into consider by the rule-based model such 
as ?I have three apple.?, then system can find out 
the ?apple? should be ?apples?. The correct noun 
will keep the original one, and the incorrect noun 
will be replaced with a new candidate. 
After the first level filtering by the rules, the 
rest of noun phrases are indeterminacy by system. 
Therefore, we use a ME classifier for further fil-
tering. We use lexical, POS and dependency 
                                                 
    7 http://www.speech.sri.com/projects/srilm/. 
38
parse information as features. The features are 
listed in Table 2.  
In previous steps, most of the error can be de-
tected, but also it may give a lot of wrong sug-
gests, in order to reduce this situation, we use N-
gram language model scorer to evaluate on the 
candidates and choose the highest probability 
one. 
 
Feature Example 
Observer word 
Word (w0) resource 
POS (p0) NN 
First word in NP 
Word (wNP-1st) a 
POS (pNP-1st) DT 
Dependency Relation det 
Previous word before observed word 
Word (w-1) good 
POS (p-1) JJ 
Word after observed word 
Word (w1) and 
POS (p1) CC 
Head word of observed word 
Word (whead) water 
POS (phead) NN 
Dependency relation rcomd 
Word Combination 
w0 + wNP-1st resource + a 
w0 + w-1 resource + good 
w0 + w1 resource + and 
w0 + whead resource + water 
wNP-1st + whead a + water 
POS Combination 
p0 + pNP-1st NN + DT 
p0 + p-1 NN + JJ 
p0 + p1 NN + CC 
p0 + phead NN + NN 
pNP-1st + phead DT + NN 
 
Table 2: Features for Nn and the example: ?An exam-
ple is water which is a good resource and is plentiful.? 
6.4 Verb Form 
Determining the correct form of a verb in Eng-
lish is complex, involving a relatively wide range 
of choices. A verb can have many forms, such as 
base, gerund, preterite, past participle and so on. 
To detect the tense of verb error is much more 
related to the semantics level than syntax level. 
Therefore, it is hard to extract a common feature 
for training model. We chose to separate it into 
several problems and use rule-based model to do 
the Vform correction. 
For auxiliary verbs, there are three categories, 
one is modal verbs (do, can, may, will, might, 
should, must, need and dare), the other is the 
form of ?be? and ?have?. In a verb phrase, nor-
mally modals precede ?have? and ?be?, and 
?have? proceed ?be?, then we can get the order-
ing like this: Modal, Have, Be. Auxiliary verbs 
can incorporate with other verbs, and have dif-
ferent combination. Based on the previous study 
of the core language engine (Alshawi, 1992), we 
define the rules that contain the type of verb, 
which tense of verbs can be used with, and their 
entries in the lexicon. For example: 
 
(can (aux (modal) (vform pres)  (COMPFORM bare)) 
 
This means ?can? is a modal verb, it can be 
used with a verb that in the present tense, when 
?can? used alone with the main verb should as 
complement the base (bare) form. In here, the 
COMPFORM attribute is the entry condition in 
the grammar.  
6.5 Subject-Verb Agreement 
The basic principle of Subject-Verb Agreement 
is singular subjects need singular verbs; plural 
subjects need plural verbs, such as following sen-
tences: 
My brother is a nutritionist. 
My sisters are dancers. 
Therefore, the subject of the sentence is the 
key point. To decide whether the verb is singular 
or plural should look into the context and find 
out the POS of the subject. We utilize the exist-
ing information given by NUCLE to extract the 
subject of the verb. For example, the sentence 
?Statistics show that the number are continuing 
to grow with the existing population explosion.? 
Figure 3 shows the parse tree of this sentence. 
 
Figure 3. Parse tree of the example sentence. 
Root
S1
NP1
VP1
VBP1NNPS
SBAR
IN1
S2
NP2 VP2
?DT2 NN2 VBP2
arenumberthe
that
showStatistics
.
.
?
39
Through Figure 3, the observed words are 
?show? and ?are?, the subjects are ?statistics? 
and ?number? respectively that we can conclude 
?statistics? should use plural verb and ?number? 
should use singular verb ?is? instead of ?are?. 
The other features extracted for training are 
listed in Table 3. 
 
Feature Example 
Observer word 
Word (w0) are 
POS (p0) VBP 
Subject NP 
First word (wNP-1st) the 
POS of first word (pNP-1st) DT 
Head word (wNP-head) number 
POS of head word (pNP-head) NN 
Previous word before observed word 
Word (w-1) number 
POS (p-1) NN 
NP after observed word 
First word (wNPa-1st) the 
POS of first word (pNPa-1st) DT 
Head word (wNPa-head) explosion 
POS of head word (pNPa-head) NN 
Word combination 
w0 + wNP-1st are + the 
w0 + wNP-head are + number 
w0 + w-1 are + number 
w0 + wNPa-1st are + the 
w0 + wNPa-head are + explosion 
POS combination 
p0 + pNP-1st VBP + DT 
p0 + pNP-head VBP + NN 
p0 + p-1 VBP + NN 
p0 + pNPa-1st VBP + DT 
p0 + pNPa-head VBP + NN 
 
Table 3: Features for SVA and the example: ?Statis-
tics show that the number are continuing to grow with 
the existing population explosion.? 
 
The purpose of extracting the noun phrase af-
ter the observed word is in the situation of the 
subject is after the verb, such as ?Where are my 
scissors??, ?scissors? is the subject of this sen-
tence. 
7 Evaluation and Discussion 
The evaluation is provided by the organizer and 
generated by M2 scorer (Dahlmeier & Ng, 2012). 
The result consists of precision, recall and F-
score. Our grammatical error correction system 
has proposed 1,011 edits. The evaluation result 
of our system output for the CoNLL-2013 test 
data is shown in Table 4. 
 
Results Precision Recall F-score 
Before 
Revision 
0.2849 0.1753 0.2170 
After  
Revision 
0.3712 0.2366 0.2890 
 
Table 4: Evaluation result of Precision, Recall and F-
score. 
 
Error Type Error # Correct # % 
ArtOrDet 690 145 21.01 
Nn 396 92 23.23 
Vform 122 8 6.55 
SVA 124 37 29.83 
Prep 311 6 1.93 
 
Table 5: Detail information of evaluation result (Be-
fore Revision). 
 
Error Type Error # Correct # % 
ArtOrDet 725 177 24.42 
Nn 484 132 27.27 
Vform 151 16 10.60 
SVA 138 47 34.06 
Prep 325 9 2.77 
 
Table 6: Detail information of evaluation result (After 
Revision). 
 
The data in table 5 and 6 are the detailed in-
formation for each error type which was calcu-
lated by us, the table 5 is the data before revision, 
and the table 6 is that after revision. Second col-
umn is the amount of the gold edits, and the third 
column is the amount of our correct edits, and 
the last column is the percentage of correct edits. 
We analyzed the results in detail, and found sev-
eral critical reasons of causing low recall. Firstly, 
the five error types are associated relatively, if 
one is modified, it may cause a chain reaction, 
such as the article will affect the noun number, 
and the noun number will cause the SVA errors. 
Some Nn errors still cannot be detected or given 
a wrong correction by our system, which de-
creases the precision and recall of SVA. Another 
reason is our system does not perform well in 
Vform and Prep error correction. In our output, 
just a few errors have been revised. This means 
the quantity of correction rules is not enough that 
cannot cover all the linguistic phenomena. For 
40
instance, the situation of missing verb or unnec-
essary verb cannot be detected. On the other 
hand, the hybrid method of our system has fil-
tered some wrong suggestion candidates that im-
prove the precision. 
8 Conclusion 
We have presented the hybrid system for English 
grammatical error correction. It achieves a 28.9% 
F1-score on the official test set. We believe that if 
we find more appropriate features, our system 
can still be improved and achieve a better per-
formance. 
 
Acknowledgments 
 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for our research, 
under the reference No. 017/2009/A and 
MYRG076(Y1-L2)-FST13-WF. The authors also 
wish to thank the anonymous reviewers for many 
helpful comments as well as Liangye He, Yuchu 
Lin and Jiaji Zhou who give us a lot of help. 
References  
Hiyan Alshawi. 1992. The core language engine. The 
MIT Press. 
Jon Louis Bentley. 1980. Multidimensional divide-
and-conquer. Communications of the ACM, 
23:214?229. 
Alina Beygelzimer, Sham Kakade, and John Lang-
ford. 2006. Cover trees for nearest neighbor. In: 
Proceedings of the 23rd International Confer-
ence on Machine Learning, pp. 97?104. 
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1. Linguistic Data Consortium, 
Philadelohia, PA. 
Olivier Chapelle, Bernhard Sch?lkopf, Alexander 
Zien, and others. 2006. Semi-supervised learning. 
MIT press Cambridge. 
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng 
Ng. 2012. NUS at the HOO 2012 Shared Task. In: 
Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 216?224. 
Daniel Dahlmeier & Hwee Tou Ng, and Siew Mei 
Wu (2013). Building a Large Annotated Corpus of 
Learner English: The NUS Corpus of Learner Eng-
lish. To appear in Proceedings of the 8th Work-
shop on Innovative Use of NLP for Building 
Educational Applications (BEA 2013). Atlanta, 
Georgia, USA. 
Daniel Dahlmeier, and Hwee Tou Ng (2012). Better 
Evaluation for Grammatical Error Correction. 
Proceedings of the 2012 Conference of the 
North American Chapter of the Association 
for Computational Linguistics (NAACL 2012), 
pp. 568 ? 572. 
Robert Dale, Ilya Anisimoff, and George Narroway. 
2012. HOO 2012: A report on the preposition and 
determiner error correction shared task. In: Pro-
ceedings of the Seventh Workshop on Building 
Educational Applications Using NLP, pp. 54?
62. 
Robert Dale and Adam Kilgarriff. 2011. Helping our 
own: The HOO 2011 pilot shared task. In: 
Proceedings of the 13th European Workshop 
on Natural Language Generation, pp. 242?249. 
Dipanjan Das and Noah A. Smith 2012. Graph-based 
lexicon expansion with sparsity-inducing penalties. 
In: Proceedings of the 2012 Conference of the 
North American Chapter of the Association 
for Computational Linguistics: Human Lan-
guage Technologies, pp. 677?687. 
Michael Gamon. 2010. Using mostly native data to 
correct errors in learners? writing: a meta-classifier 
approach. In: Human Language Technologies: 
The 2010 Annual Conference of the North 
American Chapter of the Association for 
Computational Linguistics, pp. 163?171. 
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing 
stars when there aren?t many stars: graph-based 
semi-supervised learning for sentiment categoriza-
tion. In: Proceedings of the First Workshop on 
Graph Based Methods for Natural Language 
Processing, pp. 45?52. 
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner 
corpus to develop an ESL/EFL error correction 
system. In: Proceedings of LREC, pp. 763?770. 
Mark Kantrowitz. 2003. Method and apparatus for 
analyzing affect and emotion in text. U.S. Patent 
No. 6,622,140. 
Ekaterina Kochmar. 2011. Identification of a writer?s 
native language by error analysis. Master?s thesis, 
University of Cambridge. 
Gerard Lynch, Erwan Moreau, and Carl Vogel. 2012. 
A Naive Bayes classifier for automatic correction 
of preposition and determiner errors in ESL text. 
In: Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 257?262. 
41
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, Maxent Models, and Conditional Estimation 
without Magic. Tutorial at HLT-NAACL 2003 
and ACL 2003. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian 
Hadiwinoto, and Joel Tetreault (2013). The 
CoNLL-2013 Shared Task on Grammatical Error 
Correction. To appear in Proceedings of the Sev-
enteenth Conference on Computational Natu-
ral Language Learning. 
Li Quan, Oleksandr Kolomiyets, and Marie-Francine 
Moens. 2012. KU Leuven at HOO-2012: a hybrid 
approach to detection and correction of determiner 
and preposition errors in non-native English text. 
In: Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 263?271. 
Juan Ramos. 2003. Using tf-idf to determine word 
relevance in document queries. In: Proceedings of 
the First Instructional Conference on Machine 
Learning. 
Alla Rozovskaya and Dan Roth. 2010. Training para-
digms for correcting errors in grammar and usage. 
In: Human Language Technologies: The 2010 
Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics, pp. 154?162. 
Alla Rozovskaya, Mark Sammons, and Dan Roth. 
2012. The UI system in the HOO 2012 shared task 
on error correction. In: Proceedings of the Sev-
enth Workshop on Building Educational Ap-
plications Using NLP, pp. 272?280. 
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo, 
Lis Kanashiro, Tomoya Mizumoto, Mamoru Ko-
machi, and Yuji Matsumoto. 2012. NAIST at the 
HOO 2012 Shared Task. In: Proceedings of the 
Seventh Workshop on Building Educational 
Applications Using NLP, pp. 281?288. 
Andreas Stolcke and others. 2002. SRILM-an exten-
sible language modeling toolkit. In: Proceedings 
of the International Conference on Spoken 
Language Processing, pp. 901?904. 
Partha Pratim Talukdar and Koby Crammer. 2009. 
New regularized algorithms for transductive learn-
ing. In: Machine Learning and Knowledge 
Discovery in Databases. Springer, pp. 442?457. 
Joel Tetreault, Jennifer Foster, and Martin Chodorow. 
2010. Using parse features for preposition selection 
and error detection. In: Proceedings of the Acl 
2010 Conference Short Papers, pp. 353?358. 
Joel R. Tetreault and Martin Chodorow. 2008. The 
ups and downs of preposition error detection in 
ESL writing. In: Proceedings of the 22nd Inter-
national Conference on Computational Lin-
guistics Volume 1, pp. 865?872. 
 
42
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 83?90,
Baltimore, Maryland, 26-27 July 2014. c?2014 Association for Computational Linguistics
Factored Statistical Machine Translation for Grammatical Error 
Correction 
 
 
Yiming Wang, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng, Yi Lu 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
{wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, 
lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo 
 
  
 
Abstract 
This paper describes our ongoing work on 
grammatical error correction (GEC). Focusing 
on all possible error types in a real-life 
environment, we propose a factored statistical 
machine translation (SMT) model for this task. 
We consider error correction as a series of 
language translation problems guided by 
various linguistic information, as factors that 
influence translation results. Factors included 
in our study are morphological information, i.e. 
word stem, prefix, suffix, and Part-of-Speech 
(PoS) information. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase-based and 
factor-based, trained on various datasets to 
boost the overall performance. Empirical 
results show that the proposed model yields an 
improvement of 32.54% over a baseline 
phrase-based SMT model. The system 
participated in the CoNLL 2014 shared task 
and achieved the 7
th
 and 5
th
 F0.5 scores
1 on the 
official test set among the thirteen 
participating teams. 
 
1 Introduction 
The task of grammatical error detection and 
correction (GEC) is to make use of 
computational methods to fix the mistakes in a 
written text. It is useful in two aspects. For a 
non-native English learner it may help to 
improve the grammatical quality of the written 
text. For a native speaker the tool may help to 
remedy mistakes automatically. Automatic 
                                                          
1  These two rankings are based on gold-standard edits 
without and with alternative answers, respectively. 
correction of grammatical errors is an active 
research topic, aiming at improving the writing 
process with the help of artificial intelligent 
techniques. Second language learning is a user 
group of particular interest. 
Recently, Helping Our Own (HOO) and 
CoNLL held a number of shared tasks on this 
topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 
2014). Previous studies based on rules (Sidorov 
et al., 2013), data-driven methods (Berend et al., 
2013, Yi et al., 2013) and hybrid methods (Putra 
and Szab?, 2013, Xing et al., 2013) have shown 
substantial gains for some frequent error types 
over baseline methods. Most proposed methods 
share the commonality that a sub-model is built 
for a specific type of error, on top of which a 
strategy is applied to combine a number of these 
individual models. Also, detection and correction 
are often split into two steps. For example, Xing 
et al. (2013) presented the UM-Checker for five 
error types in the CoNLL 2013 shared task. The 
system implements a cascade of five individual 
detection-and-correction models for different 
types of error. Given an input sentence, errors are 
detected and corrected one-by-one by each sub-
model at the level of its corresponding error type.  
The specifics of an error type are fully 
considered in each sub-model, which is easier to 
realize for a single error type than for multiple 
types in a single model. In addition, dividing the 
error detection and correction into two steps 
alleviates the application of machine learning 
classifiers. However, an approach that considers 
error types individually may have negative 
effects: 
? This approach assumes independence 
between each error type. It ignores the 
interaction of neighboring errors. Results 
(Xing et al., 2013) have shown that 
83
consecutive errors of multiple types tend to 
hinder solving these errors individually. 
? As the number of error types increases, the 
complexities of analyzing, designing, and 
implementing the model increase, in 
particular when combinatorial errors are 
taken into account. 
? Looking for an optimal model combination 
becomes complex. A simple pipeline 
approach would result in interference and the 
generation of new errors, and hence to 
propagating those errors to the subsequent 
processes. 
? Separating the detection and correction tasks 
may result in more errors. For instance, once 
a candidate is misidentified as an error, it 
would be further revised and turned into an 
error by the correction model. In this 
scenario the model risks losing precision. 
In the shared task of this year (Ng et la., 
2014), two novelties are introduced: 1) all types 
of errors present in an essay are to be detected 
and corrected (i.e., there is no restriction on the 
five error types of the 2013 shared task); 2) the 
official evaluation metric of this year adopts F0.5, 
weighting precision twice as much as recall. This 
requires us to explore an alternative universal 
joint model that can tackle various kinds of 
grammatical errors as well as join the detection 
and correction processes together. Regarding 
grammatical error correction as a process of 
translation has been shown to be effective (Ehsan 
and Faili, 2013, Mizumoto et al., 2011, 
Yoshimoto et al., 2013, Yuan and Felice, 2013). 
We treat the problematic sentences and golden 
sentences as pairs of source and target sentences. 
In SMT, a translation model is trained on a 
parallel corpus that consists of the source 
sentences (i.e. sentences that may contain 
grammatical errors) and the targeted translations 
(i.e. the grammatically well-formed sentences). 
The challenge is that we need a large amount of 
these parallel sentences for constructing such a 
data-driven SMT system. Some researches 
(Brockett et al., 2006, Yuan and Felice, 2013) 
explore generating artificial errors to resolve this 
sparsity problem. Other studies (Ehsan and Faili, 
2013, Yoshimoto et al., 2013, Yuan and Felice, 
2013) focus on using syntactic information (such 
as PoS or tree structure) to enhance the SMT 
models.  
In this paper, we propose a factored SMT 
model by taking into account not only the surface 
information contained in the sentence, but also 
morphological and syntactic clues (i.e., word 
stem, prefix, suffix and finer PoS information). 
To counter the sparsity problem we do not use 
artificial or manual approaches to enrich the 
training data. Instead we apply factored and 
transductive learning techniques to enhance the 
model on a small dataset. In addition, we also 
experimented with different combinations of 
translation models (TM), phrase- and factor-
based, that are trained on different datasets to 
boost the overall performance. Empirical results 
show that the proposed model yields an 
improvement of 32.54% over a baseline phrase-
based SMT model. 
The remainder of this paper is organized as 
follows: Section 2 describes our proposed 
methods. Section 3 reports on the design of our 
experiments. We discuss the result, including the 
official shared task results, in Section 4,. We 
summarize our conclusions in Section 5. 
2 Methodology 
In contrast with phrase-based translation models, 
factored models make use of additional linguistic 
clues to guide the system such that it generates 
translated sentences in which morphological and 
syntactic constraints are met (Koehn and Hoang, 
2007). The linguistic clues are taken as factors in 
a factored model; words are represented as 
vectors of factors rather than as a single token. 
This requires us to pre-process the training data 
to factorize all words. In this study, we explore 
the use of various types of morphological 
information and PoS as factors. For each possible 
factor we build an individual translation model. 
The effectiveness of all factors is analyzed by 
comparing the performance of the corresponding 
models on the grammatical error correction task. 
Furthermore, two approaches are proposed to 
combine those models. One adopts the model 
cascading method based on transductive learning. 
The second approach relies on learning and 
decoding multiple factors learning. The details of 
each approach are discussed in the following 
sub-sections. 
2.1 Data Preparation 
In order to construct a SMT model, we convert 
the training data into a parallel corpus where the 
problematic sentences that ought to be corrected 
are regarded as source sentences, while the 
reference sentences are treated as the 
corresponding target translations. We discovered 
that a number of sentences is absent at the target 
side due to incorrect annotations in the golden 
84
data. We removed these unparalleled sentences 
from the data. Secondly, the initial 
capitalizations of sentences are converted to their 
most probable casing using the Moses truecaser2. 
URLs are quite common in the corpus, but they 
are not useful for learning and even may cause 
the model to apply unnecessary correction on it. 
Thus, we mark all of the ULRs with XML 
markups, signaling the SMT decoder not to 
analyze an URL and output it as is.  
2.2 Model Construction 
In this study we explore four different factors: 
prefix, suffix, stem, and PoS. This linguistic 
information not only helps to capture the local 
constraints of word morphologies and the 
interaction of adjacent words, but also helps to 
prevent data sparsity caused by inflected word 
variants and insufficient training data.  
Word stem: Instead of lemmas, we prefer  
word stemming as one of the factors, considering 
that stemming does not requires deep 
morphological analysis and is easier to obtain. 
Second, during the whole error detection and 
correction process, stemming information is used 
as auxiliary information in addition to the 
original word form. Third, for grammatical error 
correction using word lemmas or word stems in 
factored translation model shows no significant 
difference. This is because we are translating text 
of the same language, and the translation of this 
factor, stem or lemma, is straightforwardly 
captured by the model. Hence, we do not rely on 
the word lemma. In this work, we use the 
English Porter stemmer (Porter, 1980) for 
generating word stems.  
Prefix: The second type of morphological 
information we explored is the word prefix. 
Although a prefix does not present strong 
evidence to be useful to the grammatical error 
correction, we include it in our study in order to 
fully investigate all types of morphological 
information. We believe the prefix can be an 
important factor in the correction of initial 
capitalization, e.g. ?In this era, engineering 
designs?? should be changed to ?In this era, 
engineering designs?? In model construction, 
we take the first three letters of a word as its 
prefix. If the length of a word is less than three, 
we use the word as the prefix factor. 
Suffix: Suffix, one of the important factors, 
helps to capture the grammatical agreements 
between predicates and arguments within a 
                                                          
2 After decoding, we will de-truecase all these words. 
sentence. Particularly the endings of plural nouns 
and inflected verb variants are useful for the 
detection of agreement violations that shown up 
in word morphologies. Similar to how we 
represent the prefix, we are interested in the last 
three characters of a word.  
 Examples 
Sentence 
this card contains biometric data to 
add security and reduce the risk of 
falsification 
Original 
POS 
DT NN BVZ JJ NNS TO VB NN 
CC VB DT NN IN NN 
Specific 
POS 
DT NN VBZ JJ NNS TO_to VB 
NN CC VB DT_the NN IN_of 
NN 
Table 1: Example of modified PoS. 
According to the description of factors, Figure 
1 illustrates the forms of various factors 
extracted from a given example sentence.  
Surface 
constantly combining ideas will 
result in better solutions being 
formulated 
Prefix con com ide wil res in bet sol bei for 
Suffix tly ing eas ill ult in ter ons ing ted 
Stem 
constantli combin idea will result in 
better solut be formul 
Specific 
POS 
RB VBG NNS MD VB IN JJR NNS 
VBG VBN 
Figure 1: The factorized sentence. 
PoS: Part-of-Speech tags denote the morpho-
syntactic category of a word. The use of PoS 
sequences enables us to some extent to recover 
missing determiners, articles, prepositions, as 
well as the modal verb in a sentence. Empirical 
studies (Yuan and Felice, 2013) have 
demonstrated that the use of this information can 
greatly improve the accuracy of the grammatical 
error correction. To obtain the PoS, we adopt the 
Penn Treebank tag set (Marcus et al., 1993), 
which contains 45 PoS tags. The Stanford parser 
(Klein and Manning, 2002) is used to extract the 
PoS information. Inspired by Yuan and Felice 
(2013), who used preposition-specific tags to fix 
the problem of being unable to distinguish 
between prepositions and obtained good 
performance, we create specific tags both for 
determiners (i.e., a, an, the) and prepositions. 
Table 1 provides an example of this modification, 
where prepositions, TO and IN, and determiner, 
85
DT, are revised to TO_to, IN_of and DT_the, 
respectively. 
2.3 Model Combination 
In addition to the design of different factored 
translation models, two model combination 
strategies are designed to treat grammatical error 
correction problem as a series of translation 
processes, where an incorrect sentence is 
translated into the correct one. In both 
approaches we pipeline two translation models, 
    and    . In the first approach, we derive 
four combinations of different models that 
trained on different sources.  
? In case I,    
  and    
  are both factored 
models but trained on different factors, e.g. 
for     
 training on ?surface + factori? and 
    
  on ?surface + factori?j?. Both models 
use the same training sentences, but different 
factors.  
? In case II,     
  is trained on sentences that 
paired with the output from the previous 
model,     
 , and the golden correct sentences. 
We want to create a second model that can 
also tackle the new errors introduced by the 
first model. 
? In case III, similar to case II, the second 
translation model,    
  is replaced by a 
phrase-based translation model.  
? In case IV, the quality of training data is 
considered vital to the construction of a good 
translation model. The present training dataset 
is not large enough. To complement this, the 
second model,     
 , is trained on an enlarged 
data set, by combining the training data of 
both models, i.e. the original parallel data 
(official incorrect and correct sentence pairs) 
and the supplementary parallel data 
(sentences output from the first model,     
 , 
and the correct sentences). Note that we do 
not de-duplicate sentences.  
In all cases, the testing process is carried out 
as follows. The test set is translated by the first 
translation model,     
 . The output from the first 
model is then fed into the second translation 
model,     
 . The output of the second model is 
used as the final corrections. 
The second combination approach is to make 
use of multiple factors for model construction. 
The question is whether multiple factors when 
used together may improve the correction results. 
In this setting we combine two factors together 
with the word surface form to build a multi-
factored translation model. All pairs of factors 
are used, e.g. stem and PoS. The decoding 
sequence is as follows: translate the input stems 
into target stems; translate the PoS; and generate 
the surface form given the factors of stem and 
PoS. 
3 Experiment Setup  
3.1 Dataset 
We pre-process the NUCLE corpus (Dahlmeier 
et al., 2013) as described in Section 2 for training 
different translation models. We use both the 
official golden sentences and additional 
WMT2014 English monolingual data3 to train an 
in-domain and a general-domain language model 
(LM), respectively. These language models are 
linearly interpolated in the decoding phase. We 
also randomly select a number of sentence pairs 
from the parallel corpus as a development set and 
a test set, disjoint from the training data. Table 2 
summarizes the statistics of all the datasets.  
Corpus Sentences Tokens 
Parallel 
Corpus 
55,503 
1,124,521 / 
1,114,040 
Additional 
Monolingual 
85,254,788 2,033,096,800 
Dev. Set 500 10,532 / 10,438 
Test Set 900 18,032 / 17,906 
Table 2: Statistics of used corpora. 
The experiments were carried out with 
MOSES 1.04 (Philipp Koehn et al., 2007). The 
translation and the re-ordering model utilizes the 
?grow-diag-final? symmetrized word-to-word 
alignments created with GIZA++5 (Och and Ney, 
2003) and the training scripts of MOSES. A 5-
gram LM was trained using the SRILM toolkit6 
(Stolcke et al., 2002), exploiting the improved 
modified Kneser-Ney smoothing (Kneser and 
Ney, 1995), and quantizing both probabilities 
and back-off weights. For the log-linear model 
training, we take minimum-error-rate training 
(MERT) method as described in (Och, 2003). 
The result is evaluated by M2 Scorer (Dahlmeier 
and Ng, 2012) computing precision, recall and 
F0.5.  
                                                          
3 http://www.statmt.org/wmt14/translation-task.html. 
4 http://www.statmt.org/moses/. 
5 http://code.google.com/p/giza-pp/. 
6 http://www.speech.sri.com/projects/srilm/. 
86
In total, one baseline system, five individual 
systems, and four combination systems are 
evaluated in this study. The baseline system 
(Baseline) is trained on the words-only corpus 
using a phrase-based translation model. For the 
individual systems we adopt the factored 
translation model that are trained respectively on 
1) surface and stem factors (Sys+stem), 2) surface 
and suffix factors (Sys+suf), 3) surface and prefix 
factors (Sys+pref), 4) surface and PoS factors 
(Sys+PoS), and 5) surface and modified-PoS 
factors (Sys+MPoS). The combination systems 
include: 1) the combination of ?factored + 
phrase-based? and ?factored + factored? for 
models cascading; and 2) the factors of surface, 
stem and modified-PoS (Sys+stem+MPoS) are 
combined for constructing a correction system 
based on a multi-factor model. 
4 Results and Discussions 
We report our results in terms of the precision, 
recall and F0.5 obtained by each of the individual 
models and combined models.  
4.1 Individual Model 
Table 3 shows the absolute measures for the 
baseline system, while the other individual 
models are listed with values relative to the 
baseline.  
Model Precision  Recall  F0.5 
Baseline 25.58 3.53 11.37 
Sys+stem -14.84 +13.00 +0.18 
Sys+suf -14.57 +14.77 +0.60 
Sys+pref -15.74 +12.20 -0.77 
Sys+PoS -11.63 +9.79 +2.45 
Sys+MPoS -10.25 +10.60 +3.70 
Table 3: Performance of various models. 
The baseline system has the highest precision 
score but the lowest recall. Nearly all individual 
models except Sys+pref show improvements in the 
correction result (F0.5) over the baseline. Overall, 
Sys+MPoS achieves the best result for the 
grammatical error correction task. It shows a 
significant improvement over the other models 
and outperforms the baseline model by 3.7 F0.5 
score. The Sys+stem and Sys+suf models obtain an 
improvement of 0.18 and 0.60 in F0.5 scores, 
respectively, compared to the baseline. Although 
the differences are not significant, it confirms our 
hypothesis that morphological clues do help to 
improve error correction. The F0.5 score of 
Sys+pref is the lowest among the models including 
the baseline, showing a drop of 0.77 in F0.5 score 
against the baseline. One possible reason is that 
few errors (in the training corpus) involve word 
prefixes. Thus, the prefix does not seem to be a 
suitable factor for tackling the GEC problem. 
Type 
Sys+stem 
(%) 
Sys+suf 
(%) 
Sys+MPoS 
(%) 
Error 
Num. 
Vt 17.07 12.20 12.20 41 
ArtOrDet 37.65 36.47 29.41 85 
Nn 33.33 19.61 23.53 51 
Prep 10.26 10.26 12.82 39 
Wci 9.10 10.61 6.10 66 
Rloc- 15.20 13.92 10.13 79 
Table 4: The capacity of different models in 
handling six frequent error types. 
We analyze the capacities of the models on 
different types of errors. Sys+PoS and Sys+MPoS are 
built by using the PoS and modified PoS. Both of 
them yield an improvement in F0.5 score. Overall, 
Sys+MPoS produces more accurate results than 
Sys+pref. Therefore, we specifically compare and 
evaluate the best three models, Sys+stem, Sys+suf 
and Sys+MPoS. Table 4 presents evaluation scores 
of these models for the six most frequent error 
types, which take up a large part of the training 
and test data. Among them, Sys+stem displays a 
powerful capacity to handle determiner and 
noun/number agreement errors, up to 37.65% 
and 33.33%. Sys+suf shows the ability to correct 
determiner errors at 36.47%; Sys+MPoS yields a 
similar performance to Sys+suf. All three 
individual models exhibit a relatively high 
capacity to handle determiner errors. The likely 
reason is that this mistake constitutes the largest 
portion in training data and test set, giving the 
learning models many examples to capture this 
problem well. In the case of preposition errors, 
Sys+MPoS demonstrates a better performance. This, 
once again, confirms the result (Yuan and Felice, 
2013) that the modified PoS factor is effective 
for every preposition word. For these six error 
types, the individual models show a weak 
capacity to handle the word collocation or idiom 
error category (Wci). Although Sys+MPoS 
achieves the highest F0.5 score in the overall 
evaluation, it only achieves 6.10% in handling 
this error type. The likely reason is that idioms 
are not frequent in the training data, and also that 
in most of the cases they contain out-of-
vocabulary words never seen in training data. 
4.2 Model Combination 
We intend to further boost the overall 
performance of the correction system by 
87
combining the strengths of individual models 
through model combination, and compare against 
the baseline. The systems compared here cover 
three pipelined models and a multi-factored 
model, as described earlier in Section 3. The 
combined systems include: 1) CSyssuf+phrase: the 
combination of Sys+suf and the baseline phrase-
based translation model; 2) CSyssuf+suf: we 
combine two similar factored models with suffix 
factors, Sys+suf, which is trained on the same 
corpus; and 3) TSyssuf+phrase: similar to 
CSyssuf+phrase, but the training data for the second 
phrase-based model is augmented by adding the 
output sentences from the previous model (paired 
with the correct sentences). Our intention is to 
enlarge the size of the training data. The 
evaluation results are presented in Table 5. 
Model Precision Recall F0.5 
Baseline 25.58 3.53 11.37 
CSyssuf+phrase -14.70 +14.61 +0.45 
CSyssuf+suf -15.04 +14.13 +0.09 
TSyssuf+phrase -14.76 +14.61 +0.40 
Sys+stem+MPoS -15.87 +11.72 -0.90 
Table 5: Evaluation results of combined models. 
In Table 5 we observe that Sys+stem+MPoS hurts 
performance and shows a drop of 0.9% in F0.5 
score. Both the CSyssuf+phrase and CSyssuf+suf 
show minor improvements over the baseline 
system. Even when we enrich the training data 
for the second model in TSyssuf+phrase, it cannot 
help in boosting the overall performance of the 
system. One of the problems we observe is that, 
with this combination structure, new incorrect 
sentences are introduced by the model at each 
step. The errors are propagated and accumulated 
to the final result. Although CSyssuf+phrase and 
CSyssuf+suf produce a better F0.5 score over the 
baseline, they are not as good as the individual 
models, Sys+PoS and Sys+MPoS, which are trained 
on PoS and modified-PoS, respectively. 
4.3 The Official Result 
After fully evaluating the designed individual 
models as well as the integrated ones, we adopt 
Sys+MPoS as our designated system for this 
grammatical error correction task. The official 
test set consists of 50 essays, and 2,203 errors. 
Table 6 shows the final result obtained by our 
submitted system.  
Table 7 details the correction rate of the five 
most frequent error types obtained by our system. 
The result suggests that the proposed system has 
a better ability in handling the verb, article and 
determiner error than other error types. 
Criteria Result Alt. Result 
P 0.3127 0.4317 
R 0.1446 0.1972 
F0.5 0.2537 0.3488 
Table 6: The official correction results of our 
submitted system. 
Type Error Correct % 
Vt 203/201 21/22 10.34/10.94 
V0 57/54 9/9 15.79/16.67 
Vform 156/169 11/18 7.05/10.65 
ArtOrDet 569/656 84/131 14.76/19.97 
Nn 319/285 31/42 9.72/10.91 
Table 7: Detailed error information of evaluation 
system (with alternative result). 
5 Conclusion 
This paper describes our proposed grammatical 
error detection and correction system based on a 
factored statistical machine translation approach. 
We have investigated the effectiveness of models 
trained with different linguistic information 
sources, namely morphological clues and 
syntactic PoS information. In addition, we also 
explore some ways to combine different models 
in the system to tackle the correction problem. 
The constructed models are compared against the 
baseline model, a phrase-based translation model. 
Results show that PoS information is a very 
effective factor, and the model trained with this 
factor outperforms the others. One difficulty of 
this year?s shared task is that participants have to 
tackle all 28 types of errors, which is five times 
more than last year. From the results, it is 
obvious there are still many rooms for improving 
the current system. 
Acknowledgements 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for their research, 
under the Reference nos. MYRG076 (Y1-L2)-
FST13-WF and MYRG070 (Y1-L2)-FST12-CS. 
The authors also wish to thank the anonymous 
reviewers for many helpful comments with 
special thanks to Antal van den Bosch for his 
generous help on this manuscript. 
  
88
References  
 G?bor Berend, Veronika Vincze, Sina Zarriess, 
and Rich?rd Farkas. 2013. LFG-based 
Features for Noun Number and Article 
Grammatical Errors. CoNLL-2013. 
Chris Brockett, William B. Dolan, and Michael 
Gamon. 2006. Correcting ESL errors using 
phrasal SMT techniques. Proceedings of the 
21st International Conference on 
Computational Linguistics and the 44th 
annual meeting of the Association for 
Computational Linguistics pages 249?256. 
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei 
Wu. 2013. Building a Large Annotated 
Corpus of Learner English: The NUS Corpus 
of Learner English. Proceedings of the Eighth 
Workshop on Innovative Use of NLP for 
Building Educational Applications. pages 22-
31. 
Robert Dale, Ilya Anisimoff, and George 
Narroway. 2012. HOO 2012: A report on the 
preposition and determiner error correction 
shared task. Proceedings of the Seventh 
Workshop on Building Educational 
Applications Using NLP pages 54?62. 
Nava Ehsan, and Heshaam Faili. 2013. 
Grammatical and context-sensitive error 
correction using a statistical machine 
translation framework. Software: Practice and 
Experience. Wiley Online Library. 
D. Klein, and C. D. Manning. 2002. Fast exact 
inference with a factored model for natural 
language parsing. Advances in neural 
information processing systems. 
Reinhard Kneser, and Hermann Ney. 1995. 
Improved backing-off for m-gram language 
modeling. Acoustics, Speech, and Signal 
Processing, 1995. ICASSP-95., 1995 
International Conference on Vol. 1, pages 
181?184. 
P. Koehn, and H. Hoang. 2007. Factored 
translation models. Proceedings of the Joint 
Conference on Empirical Methods in Natural 
Language Processing and Computational 
Natural Language Learning (EMNLP-CoNLL) 
Vol. 868, pages 876?876. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, 
Nicola Bertoldi, Brooke Cowan, et al. 2007. 
Moses: Open source toolkit for statistical 
machine translation. Proceedings of the 45th 
Annual Meeting of the ACL on Interactive 
Poster and Demonstration Sessions pages 
177?180. 
M. P. Marcus, M. A. Marcinkiewicz, and B. 
Santorini. 1993. Building a large annotated 
corpus of English: The Penn Treebank. 
Computational linguistics. MIT Press. 
Tomoya Mizumoto, Mamoru Komachi, Masaaki 
Nagata, and Yuji Matsumoto. 2011. Mining 
Revision Log of Language Learning SNS for 
Automated Japanese Error Correction of 
Second Language Learners. IJCNLP pages 
147?155. 
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, 
Christian Hadiwinoto, Raymond Hendy 
Susanto, and Bryant Christopher. 2014. The 
conll-2014 shared task on grammatical error 
correction. Proceedings of CoNLL. Baltimore, 
Maryland, USA. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, 
Christian Hadiwinoto, and Joel Tetreault. 
2013. The conll-2013 shared task on 
grammatical error correction. Proceedings of 
CoNLL. 
Franz Josef Och. 2003. Minimum Error Rate 
Training in Statistical Machine Translation, 
160?167. 
Franz Josef Och, and Hermann Ney. 2003. A 
systematic comparison of various statistical 
alignment models. Computational linguistics. 
MIT Press. 
Martin F. Porter. 1980. An algorithm for suffix 
stripping. Program: electronic library and 
information systems. MCB UP Ltd. 
Desmond Darma Putra, and Lili Szab?. 2013. 
UdS at the CoNLL 2013 Shared Task. 
CoNLL-2013. 
Grigori Sidorov, Anubhav Gupta, Martin Tozer, 
Dolors Catala, Angels Catena, and Sandrine 
Fuentes. 2013. Rule-based System for 
Automatic Grammar Correction Using 
Syntactic N-grams for English Language 
Learning (L2). CoNLL-2013. 
Andreas Stolcke, and others. 2002. SRILM-an 
extensible language modeling toolkit. 
INTERSPEECH. 
Junwen Xing, Longyue Wang, Derek F. Wong, 
Lidia S. Chao, and Xiaodong Zeng. 2013. 
UM-Checker: A Hybrid System for English 
Grammatical Error Correction. Proceedings of 
the Seventeenth Conference on Computational 
Natural Language Learning: Shared Task, 
34?42. Sofia, Bulgaria: Association for 
Computational Linguistics. Retrieved from 
http://www.aclweb.org/anthology/W13-3605 
Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang 
Rim. 2013. KUNLP Grammatical Error 
Correction System For CoNLL-2013 Shared 
89
Task. CoNLL-2013. 
Ippei Yoshimoto, Tomoya Kose, Kensuke 
Mitsuzawa, Keisuke Sakaguchi, Tomoya 
Mizumoto, Yuta Hayashibe, Mamoru 
Komachi, et al. 2013. NAIST at 2013 CoNLL 
grammatical error correction shared task. 
CoNLL-2013. 
Zheng Yuan, and Mariano Felice. 2013. 
Constrained grammatical error correction 
using Statistical Machine Translation. 
CoNLL-2013. 
  
90
