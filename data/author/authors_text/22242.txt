Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14?25,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Translation Modeling with Bidirectional Recurrent Neural Networks
Martin Sundermeyer
1
, Tamer Alkhouli
1
, Joern Wuebker
1
, and Hermann Ney
1,2
1
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
2
Spoken Language Processing Group
Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France
{surname}@cs.rwth-aachen.de
Abstract
This work presents two different trans-
lation models using recurrent neural net-
works. The first one is a word-based ap-
proach using word alignments. Second,
we present phrase-based translation mod-
els that are more consistent with phrase-
based decoding. Moreover, we introduce
bidirectional recurrent neural models to
the problem of machine translation, allow-
ing us to use the full source sentence in our
models, which is also of theoretical inter-
est. We demonstrate that our translation
models are capable of improving strong
baselines already including recurrent neu-
ral language models on three tasks:
IWSLT 2013 German?English, BOLT
Arabic?English and Chinese?English.
We obtain gains up to 1.6% BLEU
and 1.7% TER by rescoring 1000-best
lists.
1 Introduction
Neural network models have recently experienced
unprecedented attention in research on statistical
machine translation (SMT). Several groups have
reported strong improvements over state-of-the-art
baselines using feedforward neural network-based
language models (Schwenk et al., 2006; Vaswani
et al., 2013), as well as translation models (Le et
al., 2012; Schwenk, 2012; Devlin et al., 2014).
Different from the feedforward design, recurrent
neural networks (RNNs) have the advantage of be-
ing able to take into account an unbounded his-
tory of previous observations. In theory, this en-
ables them to model long-distance dependencies
of arbitrary length. However, while previous work
on translation modeling with recurrent neural net-
works shows its effectiveness on standard base-
lines, so far no notable gains have been presented
on top of recurrent language models (Auli et al.,
2013; Kalchbrenner and Blunsom, 2013; Hu et al.,
2014).
In this work, we present two novel approaches
to recurrent neural translation modeling: word-
based and phrase-based. The word-based ap-
proach assumes one-to-one aligned source and
target sentences. We evaluate different ways of
resolving alignment ambiguities to obtain such
alignments. The phrase-based RNN approach is
more closely tied to the underlying translation
paradigm. It models actual phrasal translation
probabilities while avoiding sparsity issues by us-
ing single words as input and output units. Fur-
thermore, in addition to the unidirectional formu-
lation, we are the first to propose a bidirectional
architecture which can take the full source sen-
tence into account for all predictions. Our ex-
periments show that these models can improve
state-of-the-art baselines containing a recurrent
language model on three tasks. For our compet-
itive IWSLT 2013 German?English system, we
observe gains of up to 1.6% BLEU and 1.7% TER.
Improvements are also demonstrated on top of our
evaluation systems for BOLT Arabic?English
and Chinese?English, which also include recur-
rent neural language models.
The rest of this paper is structured as follows. In
Section 2 we review related work and in Section 3
an overview of long short-term memory (LSTM)
neural networks, a special type of recurrent neural
networks we make use of in this work, is given.
Section 4 describes our novel translation models.
Finally, experiments are presented in Section 5
and we conclude with Section 6.
14
2 Related Work
In this Section we contrast previous work to ours,
where we design RNNs to model bilingual depen-
dencies, which are applied to rerank n-best lists
after decoding.
To the best of our knowledge, the earliest at-
tempts to apply neural networks in machine trans-
lation (MT) are presented in (Casta?no et al.,
1997; Casta?no and Casacuberta, 1997; Casta?no
and Casacuberta, 1999), where they were used for
example-based MT.
Recently, Le et al. (2012) presented translation
models using an output layer with classes and
a shortlist for rescoring using feedforward net-
works. They compare between word-factored and
tuple-factored n-gram models, obtaining their best
results using the word-factored approach, which is
less amenable to data sparsity issues. Both of our
word-based and phrase-based models eventually
work on the word level. Kalchbrenner and Blun-
som (2013) use recurrent neural networks with
full source sentence representations. The continu-
ous representations are obtained by applying a se-
quence of convolutions, and the result is fed into
the hidden layer of a recurrent language model.
Rescoring results indicate no improvements over
the state of the art. Auli et al. (2013) also in-
clude source sentence representations built either
using Latent Semantic Analysis or by concatenat-
ing word embeddings. This approach produced
no notable gain over systems using a recurrent
language model. On the other hand, our pro-
posed bidirectional models include the full source
sentence relying on recurrency, yielding improve-
ments over competitive baselines already includ-
ing a recurrent language model.
RNNs were also used with minimum translation
units (Hu et al., 2014), which are phrase pairs un-
dergoing certain constraints. At the input layer,
each of the source and target phrases are mod-
eled as a bag of words, while the output phrase
is predicted word-by-word assuming conditional
independence. The approach seeks to alleviate
data sparsity problems that would arise if phrases
were to be uniquely distinguished. Our proposed
phrase-based models maintain word order within
phrases, but the phrases are processed in a word-
pair manner, while the phrase boundaries remain
implicitly encoded in the way the words are pre-
sented to the network. Schwenk (2012) proposed
a feedforward network that predicts phrases of a
fixed maximum length, such that all phrase words
are predicted at once. The prediction is condi-
tioned on the source phrase. Since our phrase-
based model predicts one word at a time, it does
not assume any phrase length. Moreover, our
model?s predictions go beyond phrase boundaries
and cover unbounded history and future contexts.
Using neural networks during decoding re-
quires tackling the costly output normalization
step. Vaswani et al. (2013) avoid this step by
training feedforward neural language models us-
ing noise contrastive estimation, while Devlin et
al. (2014) augment the training objective function
to produce approximately normalized scores di-
rectly. The latter work makes use of translation
and joint models, and pre-computes the first hid-
den layer beforehand, resulting in large speedups.
They report major improvements over strong base-
lines. The speedups achieved by both works al-
lowed to integrate feedforward neural networks
into the decoder.
3 LSTM Recurrent Neural Networks
Our work is based on recurrent neural networks.
In related fields like e. g. language modeling, this
type of neural network has been shown to perform
considerably better than standard feedforward ar-
chitectures (Mikolov et al., 2011; Arisoy et al.,
2012; Sundermeyer et al., 2013; Liu et al., 2014).
Most commonly, recurrent neural networks are
trained with stochastic gradient descent (SGD),
where the gradient of the training criterion is com-
puted with the backpropagation through time al-
gorithm (Rumelhart et al., 1986; Werbos, 1990;
Williams and Zipser, 1995). However, the combi-
nation of RNN networks with conventional back-
propagation training leads to conceptual difficul-
ties which are known as the vanishing (or explod-
ing) gradient problem, described e. g. in (Bengio
et al., 1994). To remedy this problem, in (Hochre-
iter and Schmidhuber, 1997) it was suggested to
modify the architecture of a standard RNN in such
a way that vanishing and exploding gradients are
avoided during backpropagation. In particular, no
modification of the training algorithm is necessary.
The resulting architecture is referred to as long
short-term memory (LSTM) neural network.
Bidirectional recurrent neural networks
(BRNNs) were first proposed in (Schuster and
Paliwal, 1997) and applied to speech recognition
tasks. They have been since applied to different
15
Surfers
,
for
example
,
know
this
incredibly
.
S
u
r
f
e
r
z
u
m
B
e
i
s
p
i
e
l
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
(a) Original
Surfers
,
for
example
,
know
this

unaligned
incredibly
.
S
u
r
f
e
r

a
l
i
g
n
e
d
z
u
m
B
e
i
s
p
i
e
l

u
n
a
l
i
g
n
e
d
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
(b) One-to-one alignment
Figure 1: Example sentence from the German?English IWSLT data. The one-to-one alignment is
created by introducing 
aligned
and 
unaligned
tokens.
tasks like parsing (Henderson, 2004) and spoken
language understanding (Mesnil et al., 2013).
Bidirectional long short-term memory (BLSTM)
networks are BRNNs using LSTM hidden layers
(Graves and Schmidhuber, 2005). This work
introduces BLSTMs to the problem of machine
translation, allowing powerful models that employ
unlimited history and future information to make
predictions.
While the proposed models do not make any as-
sumptions about the type of RNN used, all of our
experiments make use of recurrent LSTM neural
networks, where we include later LSTM exten-
sions proposed in (Gers et al., 2000; Gers et al.,
2003). The cross-entropy error criterion is used
for training. Further details on LSTM neural net-
works can be found in (Graves and Schmidhuber,
2005; Sundermeyer et al., 2012).
4 Translation Modeling with RNNs
In the following we describe our word- and
phrase-based translation models in detail. We also
show how bidirectional RNNs can enable such
models to include full source information.
4.1 Resolving Alignment Ambiguities
Our word-based recurrent models are only de-
fined for one-to-one-aligned source-target sen-
tence pairs. In this work, we always evaluate the
model in the order of the target sentence. How-
ever, we experiment with several different ways
to resolve ambiguities due to unaligned or mul-
tiply aligned words. To that end, we introduce
two additional tokens, 
aligned
and 
unaligned
. Un-
dev test
BLEU TER BLEU TER
baseline 33.5 45.8 30.9 48.4
w/o  34.2 45.3 31.8 47.7
w/o 
unaligned
34.4 44.8 31.7 47.4
source identity 34.5 45.0 31.9 47.5
target identity 34.5 44.6 31.9 47.0
all  34.6 44.5 32.0 47.1
Table 1: Comparison of including different sets
of  tokens into the one-to-one alignment on the
IWSLT 2013 German?English task using the uni-
directional RNN translation model.
aligned words are either removed or aligned to an
extra 
unaligned
token on the opposite side. If an

unaligned
is introduced on the target side, its posi-
tion is determined by the aligned source word that
is closest to the unaligned source word in question,
preferring left to right. To resolve one-to-many
alignments, we use an IBM-1 translation table to
decide for one of the alignment connections to be
kept. The remaining words are also either deleted
or aligned to additionally introduced 
aligned
to-
kens on the opposite side. Fig. 1 shows an ex-
ample sentence from the IWSLT data, where all 
tokens are introduced.
In a short experiment, we evaluated 5 differ-
ent setups with our unidirectional RNN translation
model (cf. next Section): without any  tokens,
without 
unaligned
, source identity, target identity
and using all  tokens. Source identity means we
16
introduce no  tokens on source side, but all on
target side. Target identity is defined analogously.
The results can be found in Tab. 1. We use the
setup with all  tokens in all following experi-
ments, which showed the best BLEU performance.
4.2 Word-based RNN Models
Given a pair of source sequence f
I
1
= f
1
. . . f
I
and target sequence e
I
1
= e
1
. . . e
I
, where we as-
sume a direct correspondence between f
i
and e
i
,
we define the posterior translation probability by
factorizing on the target words:
p(e
I
1
|f
I
1
) =
I
?
i=1
p(e
i
|e
i?1
1
, f
I
1
) (1)
?
I
?
i=1
p(e
i
|e
i?1
1
, f
i+d
1
) (2)
?
I
?
i=1
p(e
i
|f
i+d
1
). (3)
We denote the formulation (1) as the bidirectional
joint model (BJM). This model can be simplified
by several independence assumptions. First, we
drop the dependency on the future source infor-
mation, receiving what we denote as the unidirec-
tional joint model (JM) in (2). Here, d ? N
0
is
a delay parameter, which is set to d = 0 for all
experiments, except for the comparative results re-
ported in Fig. 7. Finally, assuming conditional in-
dependence from the previous target sequence, we
receive the unidirectional translation model (TM)
in (3). Analogously, we can define a bidirectional
translation model (BTM) by keeping the depen-
dency on the full source sentence f
I
1
, but dropping
the previous target sequence e
i?1
1
:
p(e
I
1
|f
I
1
) ?
I
?
i=1
p(e
i
|f
I
1
). (4)
Fig. 2 shows the dependencies of the word-
based neural translation and joint models. The
alignment points are traversed in target order and
at each time step one target word is predicted.
The pure translation model (TM) takes only source
words as input, while the joint model (JM) takes
the preceding target words as an additional input.
A delay of d > 0 is implemented by shifting the
target sequence by d time steps and filling the first
d target positions and the last d source positions
with a dedicated 
padding
symbol. The RNN archi-
tecture for the unidirectional word-based models
j
o
i
n
t
m
o
d
e
l
all models bidirectional
Surfers
,
for
example
,
know
this

unaligned
incredibly
.
S
u
r
f
e
r

a
l
i
g
n
e
d
z
u
m
B
e
i
s
p
i
e
l

u
n
a
l
i
g
n
e
d
k
e
n
n
e
n
d
a
s
z
u
r
G
e
n
?
u
g
e
.
Figure 2: Dependencies modeled within the word-
based RNN models when predicting the target
word ?know?. Directly processed information is
depicted with solid rectangles, and information
available through recurrent connections is marked
with dashed rectangles.
is illustrated in Fig. 3, which corresponds to the
following set of equations:
y
i
= A
1
?
f
i
+A
2
e?
i?1
z
i
= ?(y
i
;A
3
, y
i?1
1
)
p
(
c(e
i
)|e
i?1
1
, f
i
1
)
= ?
c(e
i
)
(A
4
z
i
)
p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
= ?
e
i
(A
c(e
i
)
z
i
)
p(e
i
|e
i?1
1
, f
i
1
) = p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
?
p
(
c(e
i
)|e
i?1
1
, f
i
1
)
Here, by
?
f
i
and e?
i?1
we denote the one-hot en-
coded vector representations of the source and
target words f
i
and e
i?1
. The outgoing activa-
tion values of the projection layer and the LSTM
layer are y
i
and z
i
, respectively. The matrices A
j
contain the weights of the neural network layers.
By ?(? ;A
3
, y
i?1
1
) we denote the LSTM formalism
that we plug in at the third layer. As the LSTM
layer is recurrent, we explicitly include the de-
pendence on the previous layer activations y
i?1
1
.
Finally, ? is the widely-used softmax function to
obtain normalized probabilities, and c denotes a
word class mapping from any target word to its
unique word class. For the bidirectional model,
the equations can be defined analogously.
Due to the use of word classes, the output
layer consists of two parts. The class probabil-
ity p
(
c(e
i
)|e
i?1
1
, f
i
1
)
is computed first, and then
17
p(
c(e
i
)|ei?11 , f i+d1
)
p
(
e
i
|c(e
i
), ei?11 , f i+d1
)
class layer output layer
LSTM layer
projection layer
input layer
e
i?1f
i+d)
Figure 3: Architecture of a recurrent unidirec-
tional translation model. By including the dashed
parts, a joint model is obtained.
the word probability p
(
e
i
|c(e
i
), e
i?1
1
, f
i
1
)
is ob-
tained given the word class. This trick helps avoid-
ing the otherwise computationally expensive nor-
malization sum, which would be carried out over
all words in the target vocabulary. In a class-
factorized output layer where each word belongs
to a single class, the normalization is carried out
over all classes, whose number is typically much
less than the vocabulary size. The other normal-
ization sum needed to produce the word probabil-
ity is limited to the words belonging to the same
class (Goodman, 2001; Morin and Bengio, 2005).
4.3 Phrase-based RNN Models
One of the conceptual disadvantages of word-
based modeling as introduced in the previous sec-
tion is that there is a mismatch between train-
ing and testing conditions: During neural network
training, the vocabulary has to be extended by ad-
ditional  tokens, and a one-to-one alignment is
used which does not reflect the situation in decod-
ing. In phrase-based machine translation, more
complex alignments in terms of multiple words
on both the source and the target sides are used,
which allow the decoder to make use of richer
short-distance dependencies and are crucial for the
performance of the resulting system.
From this perspective, it seems interesting to
standardize the alignments used in decoding, and
in training the neural network. However, it is dif-
ficult to use the phrases themselves as the vocab-
ulary of the RNN. Usually, the huge number of
potential phrases in comparison to the relatively
small amount of training data makes the learn-
ing of continuous phrase representations difficult
Surfer
Surfers
zum Beispiel
, for example ,
kennen
know
zur Gen?ge
incredibly
.
.
das
this
Figure 4: Example phrase alignment for a sen-
tence from the IWSLT training data.
due to data sparsity. This is confirmed by results
presented in (Le et al., 2012), which show that a
word-factored translation model outperforms the
phrase-factored version. Therefore, in this work
we continue relying on source and target word vo-
cabularies for building our phrase representations.
However, we no longer use a direct correspon-
dence between a source and a target word, as en-
forced in our word-based models.
Fig. 4 shows an example phrase alignment,
where a sequence of source words
?
f
i
is directly
mapped to a sequence of target words e?
i
for 1 ?
i ?
?
I . By
?
I , we denote the number of phrases in
the alignment. We decompose the target sentence
posterior probability in the following way:
p(e
I
1
|f
J
1
) =
?
I
?
i=1
p(e?
i
|e?
i?1
1
,
?
f
?
I
1
) (5)
?
?
I
?
i=1
p(e?
i
|e?
i?1
1
,
?
f
i
1
) (6)
where the joint model in Eq. 5 would correspond
to a bidirectional RNN, and Eq. 6 only requires a
unidirectional RNN. By leaving out the condition-
ing on the target side, we obtain a phrase-based
translation model.
As there is no one-to-one correspondence be-
tween the words within a phrase, the basic idea of
our phrase-based approach is to let the neural net-
work learn the dependencies itself, and present the
full source side of the phrase to the network be-
fore letting it predict target side words. Then the
probability for the target side of a phrase can be
computed, in case of Eq. 6, by:
p(e?
i
|e?
i?1
1
,
?
f
?
I
1
) =
|e?
i
|
?
j=1
p
(
(e?
i
)
j
|(e?
i
)
j?1
1
, e?
i?1
1
,
?
f
i
1
)
,
and analogously for the case of Eq. 5. Here, (e?
i
)
j
denotes the j-th word of the i-th aligned target
phrase.
We feed the source side of a phrase into the neu-
ral network one word at a time. Only when the
18
output layer
LSTM layer
projection layer
input layer
Surfers ,, for example know this incredibly .??
?s? Surfers
,, for example
know
this
incredibly
.Surfer
zum
B
eispiel
kennen
das zur Genu?ge
?? ???
Figure 5: A recurrent phrase-based joint translation model, unfolded over time. Source words are printed
in normal face, while target words are printed in bold face. Dashed lines indicate phrases from the
example sentence. For brevity, we omit the precise handling of sentence begin and end tokens.
presentation of the source side is finished we start
estimating probabilities for the target side. There-
fore, we do not let the neural network learn a target
distribution until the very last source word is con-
sidered. In this way, we break up the conventional
RNN training scheme where an input sample is di-
rectly followed by its corresponding teacher sig-
nal. Similarly, the presentation of the source side
of the next phrase only starts after the prediction
of the current target side is completed.
To this end, we introduce a no-operation token,
denoted by ?, which is not part of the vocabulary
(which means it cannot be input to or predicted by
the RNN). When the ? token occurs as input, it in-
dicates that no input needs to be processed by the
RNN. When the ? token occurs as a teacher signal
for the RNN, the output layer distribution is ig-
nored, and does not even have to be computed. In
both cases, all the other layers are still processed
during forward and backward passes such that the
RNN state can be advanced even without addi-
tional input or output.
Fig. 5 depicts the evaluation of a phrase-based
joint model for the example alignment from Fig. 4.
For a source phrase
?
f
i
, we include (|e?
i
|?1) many ?
symbols at the end of the phrase. Conversely, for
a target phrase e?
i
, we include (|
?
f
i
| ? 1) many ?
symbols at the beginning of the phrase.
E. g., in the figure, the second dashed rectan-
gle from the left depicts the training of the English
phrase ?, for example ,? and its German transla-
tion ?zum Beispiel?. At the input layer, we feed in
the source words one at a time, while we present
? tokens at the target side input layer and the out-
put layer (with the exception of the very first time
step, where we still have the last target word from
the previous phrase as input instead of ?). With
the last word of the source phrase ?Beispiel? being
presented to the network, the full source phrase is
stored in the hidden layer, and the neural network
is then trained to predict the target phrase words
at the output layer. Subsequently, the source input
is ?, and the target input is the most recent target
side history word.
To obtain a phrase-aligned training sequence for
the phrase-based RNN models, we force-align the
training data with the application of leave-one-out
as described in (Wuebker et al., 2010).
4.4 Bidirectional RNN Architecture
While the unidirectional RNNs include an un-
bounded sentence history, they are still limited in
the number of future source words they include.
Bidirectional models provide a flexible means to
also include an unbounded future context, which,
unlike the delayed unidirectional models, require
no tuning to determine the amount of delay.
Fig. 6 illustrates the bidirectional model archi-
tecture, which is an extension of the unidirectional
model of Fig. 3. First, an additional recurrent
hidden layer is added in parallel to the existing
one. This layer will be referred to as the back-
ward layer, since it processes information in back-
ward time direction. This hidden layer receives
source word input only, while target words in the
case of a joint model are fed to the forward layer
as in the unidirectional case. Due to the backward
recurrency, the backward layer will make the in-
formation f
I
i
available when predicting the target
word e
i
, while the forward layer takes care of the
source history f
i
1
. Jointly, the forward and back-
ward branches include the full source sentence f
I
1
,
as indicated in Fig. 2. Fig. 6 shows the ?deep?
variant of the bidirectional model, where the for-
19
p(
c(e
i
)|ei?11 , fI1
)
p
(
e
i
|c(e
i
), ei?11 , fI1
)
class layer output layer
2nd LSTM layer
1st LSTM layer
projection layer
input layer
e
i?1fi
(+)
(+)
(?)
Figure 6: Architecture of a recurrent bidirectional
translation model. By (+) and (?), we indicate
a processing in forward and backward time direc-
tions, respectively. The inclusion of the dashed
parts leads to a bidirectional joint model. One
source projection matrix is used for the forward
and backward branches.
ward and backward layers converge into a hidden
layer. A shallow variant can be obtained if the
parallel layers converge into the output layer di-
rectly
1
.
Due to the full dependence on the source se-
quence, evaluating bidirectional networks requires
computing the forward pass of the forward and
backward layers for the full sequence, before be-
ing able to evaluate the next layers. In the back-
ward pass of backpropagation, the forward and
backward recurrent layers are processed in de-
creasing and increasing time order, respectively.
5 Experiments
5.1 Setup
All translation experiments are performed with the
Jane toolkit (Vilar et al., 2010; Wuebker et al.,
2012). The largest part of our experiments is car-
ried out on the IWSLT 2013 German?English
shared translation task.
2
The baseline system is
trained on all available bilingual data, 4.3M sen-
tence pairs in total, and uses a 4-gram LM with
modified Kneser-Ney smoothing (Kneser and Ney,
1995; Chen and Goodman, 1998), trained with
the SRILM toolkit (Stolcke, 2002). As additional
1
In our implementation, the forward and backward layers
converge into an intermediate identity layer, and the aggre-
gate is weighted and fed to the next layer.
2
http://www.iwslt2013.org
data sources for the LM we selected parts of the
Shuffled News and LDC English Gigaword cor-
pora based on cross-entropy difference (Moore
and Lewis, 2010), resulting in a total of 1.7 bil-
lion running words for LM training. The state-of-
the-art baseline is a standard phrase-based SMT
system (Koehn et al., 2003) tuned with MERT
(Och, 2003). It contains a hierarchical reorder-
ing model (Galley and Manning, 2008) and a 7-
gram word cluster language model (Wuebker et
al., 2013). Here, we also compare against a feed-
forward joint model as described by Devlin et al.
(2014), with a source window of 11 words and a
target history of three words, which we denote as
BBN-JM. Instead of POS tags, we predict word
classes trained with mkcls. We use a shortlist
of size 16K and 1000 classes for the remaining
words. All neural networks are trained on the TED
portion of the data (138K segments) and are ap-
plied in a rescoring step on 1000-best lists.
To confirm our results, we run additional
experiments on the Arabic?English and
Chinese?English tasks of the DARPA BOLT
project. In both cases, the neural network models
are added on top of our most competitive eval-
uation system. On Chinese?English, we use a
hierarchical phrase-based system trained on 3.7M
segments with 22 dense features, including an ad-
vanced orientation model (Huck et al., 2013). For
the neural network training, we selected a subset
of 9M running words. The Arabic?English
system is a standard phrase-based decoder trained
on 6.6M segments, using 17 dense features. The
neural network training was performed using a
selection amounting to 15.5M running words.
For both tasks we apply the neural networks by
rescoring 1000-best lists and evaluate results on
two data sets from the ?discussion forum? domain,
test1 and test2. The sizes of the data sets
for the Arabic?English system are: 1219 (dev),
1510 (test1), and 1137 (test2) segments, and
for the Chinese?English system are: 5074 (dev),
1844 (test1), and 1124 (test2) segments. All
results are measured in case-insensitive BLEU [%]
(Papineni et al., 2002) and TER [%] (Snover et al.,
2006) on a single reference.
5.2 Results
Our results on the IWSLT German?English task
are summarized in Tab. 2. At this point, we
do not include a recurrent neural network lan-
20
dev test
BLEU TER BLEU TER
baseline 33.5 45.8 30.9 48.4
TM 34.6 44.5 32.0 47.1
JM 34.7 44.7 31.8 47.4
BTM 34.7 44.9 32.3 47.0
BTM (deep) 34.8 44.3 32.5 46.7
BJM 34.7 44.5 32.1 47.0
BJM (deep) 34.9 44.1 32.2 46.6
PTM 34.3 44.9 32.1 47.5
PJM 34.3 45.0 32.0 47.5
PJM (10-best) 34.4 44.8 32.0 47.3
PJM (deep) 34.6 44.7 32.0 47.6
PBJM (deep) 34.8 44.9 31.9 47.5
BBN-JM 34.4 44.9 31.9 47.6
Table 2: Results for the IWSLT 2013
German?English task with different RNN
models. T: translation, J: joint, B: bidirectional,
P: phrase-based.
guage model yet. Here, the delay parameter d
from Equations 2 and 3 is set to zero. We ob-
serve that for all recurrent translation models, we
achieve substantial improvements over the base-
line on the test data, ranging from 0.9 BLEU
up to 1.6 BLEU. These results are also consistent
with the improvements in terms of TER, where we
achieve reductions by 0.8 TER up to 1.8 TER.
These numbers can be directly compared to the
case of feedforward neural network-based transla-
tion modeling as proposed in (Devlin et al., 2014)
which we include in the very last row of the table.
Nearly all of our recurrent models outperform the
feedforward approach, where the RNN model per-
forming best on the dev data is better on test
by 0.3 BLEU and 1.0 TER.
Interestingly, for the recurrent word-based mod-
els, on the test data it can be seen that TMs per-
form better than JMs, even though TMs do not
take advantage of the target side history words.
However, exploiting this extra information does
not always need to result in a better model, as the
target side words are only derived from the given
source side, which is available to both TMs and
JMs. On the other hand, including future source
words in a bidirectional model clearly improves
the performance further. By adding another LSTM
 
31
 
31
.5
 
32
 
32
.5
 
33
0
1
2
3
4
BLEU[%]
De
layRN
N-
TM
RN
N-
BT
M
Figure 7: BLEU scores on the IWSLT test set
with different delays for the unidirectional RNN-
TM and the bidirectional RNN-BTM.
layer that combines forward and backward time
directions (indicated as ?deep? in the table), we ob-
tain our overall best model.
In Fig. 7 we compare the word-based bidirec-
tional TM with a unidirectional TM that uses dif-
ferent time delays d = 0, . . . , 4. For a delay d =
2, the same performance is obtained as with the
bidirectional model, but this comes at the price of
tuning the delay parameter.
In comparison to the unidirectional word-based
models, phrase-based models perform similarly.
In the tables, we include those phrase-based vari-
ants which perform best on the dev data, where
phrase-based JMs always are at least as good or
better than the corresponding TMs in terms of
BLEU. Therefore, we mainly report JM results
for the phrase-based networks. A phrase-based
model can also be trained on multiple variants for
the phrase alignment. For our experiments, we
tested 10-best alignments against the single best
alignment, which resulted in a small improvement
of 0.2 TER on both dev and test. We did not ob-
serve consistent gains by using an additional hid-
den layer or bidirectional models. To some ex-
tent, future information is already considered in
unidirectional phrase-based models by feeding the
complete source side before predicting the target
side.
Tab. 3 shows different model combination re-
sults for the IWSLT task, where a recurrent lan-
guage model is included in the baseline. Adding
a deep bidirectional TM or JM to the recur-
rent language model improves the RNN-LM base-
line by 1.2 BLEU or 1.1 BLEU, respectively. A
phrase-based model substantially improves over
21
dev eval11 test
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
BLEU
[%]
TER
[%]
baseline (w/ RNN-LM) 34.3 44.8 36.4 42.9 31.5 47.8
BTM (deep) 34.9 43.7 37.6 41.5 32.7 46.1
BJM (deep) 35.0 44.4 37.4 41.9 32.6 46.5
PBJM (deep) 34.8 44.6 36.9 42.6 32.3 47.2
4 RNN models 35.2 43.4 38.0 41.2 32.7 46.0
Table 3: Results for the IWSLT 2013 German?English task with different RNN models. All results
include a recurrent language model. T: translation, J: joint, B: bidirectional, P: phrase-based.
the RNN-LM baseline, but performs not as good
as its word-based counterparts. By adding four
different translation models, including models in
reverse word order and reverse translation direc-
tion, we are able to improve these numbers even
further. However, especially on the test data, the
gains from model combination saturate quickly.
Apart from the IWSLT track, we also ana-
lyze the performance of our translation models on
the BOLT Chinese?English and Arabic?English
translation tasks. Due to the large amount of train-
ing data, we concentrate on models of high perfor-
mance in the IWSLT experiments. The results can
be found in Tab. 4 and 5. In both cases, we see
consistent improvements over the recurrent neural
network language model baseline, improving the
Arabic?English system by 0.6 BLEU and 0.5 TER
on test1. This can be compared to the rescoring
results for the same task reported by (Devlin et al.,
2014), where they achieved 0.3 BLEU, despite the
fact that they used multiple references for scoring,
whereas in our experiments we rely on a single
reference only. The models are also able to im-
prove the Chinese?English system by 0.5 BLEU
and 0.5 TER on test2.
5.3 Analysis
To investigate whether bidirectional models ben-
efit from future source information, we compare
the single-best output of a system reranked with a
unidirectional model to the output reranked with
a bidirectional model. We choose the models
to be translation models in both cases, as they
predict target words independent of previous
predictions, given the source information (cf. Eqs.
(3, 4)). This makes it easier to detect the effect
of including future source information or the lack
thereof. The examples are taken from the IWSLT
test1 test2
BLEU TER BLEU TER
baseline 25.2 57.4 26.8 57.3
BTM (deep) 25.6 56.6 26.8 56.7
BJM (deep) 25.9 56.9 27.4 56.7
RNN-LM 25.6 57.1 27.5 56.7
+ BTM (deep) 25.9 56.7 27.3 56.8
+ BJM (deep) 26.2 56.6 27.9 56.5
Table 4: Results for the BOLT Arabic?English
task with different RNN models. The ?+? sign in
the last two rows indicates that either of the corre-
sponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, J: joint, B: bidirectional.
task, where we include the one-to-one source
information, reordered according to the target
side.
source: nicht so wie ich
reference: not like me
Hypothesis 1:
1-to-1 source: so ich  nicht wie
1-to-1 target: so I do n?t like
Hypothesis 2:
1-to-1 source: nicht so wie ich
1-to-1 target: not  like me
In this example, the German phrase ?so wie?
translates to ?like? in English. The bidirectional
model prefers hypothesis 2, making use of the
future word ?wie? when translating the German
word ?so? to , because it has future insight that
this move will pay off later when translating
22
BLEU TER BLEU TER
baseline 18.3 63.6 16.7 63.0
BTM (deep) 18.7 63.3 17.1 62.6
BJM (deep) 18.5 63.1 17.2 62.3
RNN-LM 18.8 63.3 17.2 62.8
+ BTM (deep) 18.9 63.1 17.7 62.3
+ BJM (deep) 18.8 63.3 17.5 62.5
Table 5: Results for the BOLT Chinese?English
task with different RNN models. The ?+? sign in
the last two rows indicates that either of the corre-
sponding deep models (BTM and BJM) are added
to the baseline including the recurrent language
model (i.e. they are not applied at the same time).
T: translation, B: bidirectional.
the rest of the sentence. This information is
not available to the unidirectional model, which
prefers hypothesis 1 instead.
source: das taten wir dann auch und verschafften uns
so eine Zeit lang einen Wettbewerbs Vorteil .
reference: and we actually did that and it gave us a
competitive advantage for a while .
Hypothesis 1:
1-to-1 source: das    wir dann auch taten und
verschafften uns so eine Zeit lang einen Wettbewerbs
Vorteil .
1-to-1 target: that ?s just what we   did and gave us 
a time , a competitive advantage .
Hypothesis 2:
1-to-1 source: das    wir dann auch taten und
verschafften uns so einen Wettbewerbs Vorteil  eine
Zeit lang .
1-to-1 target: that ?s just what we   did and gave us 
a competitive advantage for a  while .
Here, the German phrase ?eine Zeit lang? trans-
lates to ?for a while? in English. Bidirectional
scoring favors hypothesis 2, while unidirectional
scoring favors hypothesis 1. It seems that the uni-
directional model translates ?Zeit? to ?time? as the
object of the verb ?give? in hypothesis 1, being
blind to the remaining part ?lang? of the phrase
which changes the meaning. The bidirectional
model, to its advantage, has the full source infor-
mation, allowing it to make the correct prediction.
6 Conclusion
We developed word- and phrase-based RNN trans-
lation models. The former is simple and performs
well in practice, while the latter is more consistent
with the phrase-based paradigm. The approach in-
herently evades data sparsity problems as it works
on words in its lowest level of processing. Our
experiments show the models are able to achieve
notable improvements over baselines containing a
recurrent LM.
In addition, and for the first time in statistical
machine translation, we proposed a bidirectional
neural architecture that allows modeling past and
future dependencies of any length. Besides its
good performance in practice, the bidirectional ar-
chitecture is of theoretical interest as it allows the
exact modeling of posterior probabilities.
Acknowledgments
This material is partially based upon work sup-
ported by the DARPA BOLT project under Con-
tract No. HR0011- 12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
The research leading to these results has also re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements n
o
287658 and n
o
287755.
Experiments were performed with computing re-
sources granted by JARA-HPC from RWTH
Aachen University under project ?jara0085?. We
would like to thank Jan-Thorsten Peter for provid-
ing the BBN-JM system.
References
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep neural network
language models. In Proceedings of the NAACL-
HLT 2012 Workshop: Will We Ever Really Replace
the N-gram Model? On the Future of Language
Modeling for HLT, pages 20?28. Association for
Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Con-
ference on Empirical Methods in Natural Language
Processing, pages 1044?1054, Seattle, USA, Octo-
ber.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
23
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Maria Asunci?on Casta?no and Francisco Casacuberta.
1997. A connectionist approach to machine trans-
lation. In 5th International Conference on Speech
Communication and Technology (EUROSPEECH-
97), Rhodes, Greece.
Maria Asunci?on Casta?no and Francisco Casacuberta.
1999. Text-to-text machine translation using the
RECONTRA connectionist model. In Lecture Notes
in Computer Science (IWANN 99), volume 1607,
pages 683?692, Alicante, Spain.
Maria Asunci?on Casta?no, Francisco Casacuberta, and
Enrique Vidal. 1997. Machine translation using
neural networks and finite-state models. In 7th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation. TMI?97,
pages 160?167, Santa Fe, USA.
Stanley F. Chen and Joshua Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University, Cam-
bridge, MA, August.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, page to appear, Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 848?856, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Felix A. Gers, J?urgen Schmidhuber, and Fred Cum-
mins. 2000. Learning to forget: Contin-
ual prediction with LSTM. Neural computation,
12(10):2451?2471.
Felix A. Gers, Nicol N. Schraudolph, and J?urgen
Schmidhuber. 2003. Learning precise timing with
lstm recurrent networks. The Journal of Machine
Learning Research, 3:115?143.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Acoustics, Speech, and Signal
Processing, 2001. Proceedings.(ICASSP?01). 2001
IEEE International Conference on, volume 1, pages
561?564. IEEE.
Alex Graves and J?urgen Schmidhuber. 2005. Frame-
wise phoneme classification with bidirectional
LSTM and other neural network architectures. Neu-
ral Networks, 18(5):602?610.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 95. Association for Com-
putational Linguistics.
Sepp Hochreiter and J?urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735?1780.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A phrase orientation model for hi-
erarchical machine translation. In ACL 2013 Eighth
Workshop on Statistical Machine Translation, pages
452?463, Sofia, Bulgaria, August.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for M-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processingw, volume 1,
pages 181?184, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Meeting of the North Ameri-
can chapter of the Association for Computational
Linguistics (NAACL-03), pages 127?133, Edmon-
ton, Alberta.
Hai Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
39?48, Montreal, Canada, June.
Xunying Liu, Yongqiang Wang, Xie Chen, Mark J. F.
Gales, and Phil C. Woodland. 2014. Efficient lattice
rescoring using recurrent neural network language
models. In Acoustics, Speech and Signal Processing
(ICASSP), 2014 IEEE International Conference on,
pages 4941?4945. IEEE.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. In Interspeech, pages
3771?3775.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
24
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Robert C. Moore and William Lewis. 2010. Intelligent
Selection of Language Model Training Data. In ACL
(Short Papers), pages 220?224, Uppsala, Sweden,
July.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA, July.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In: J. L. McClelland, D. E.
Rumelhart, and The PDP Research Group: ?Paral-
lel Distributed Processing, Volume 1: Foundations?.
The MIT Press.
Mike Schuster and Kuldip K. Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Transactions
on Signal Processing, 45(11):2673?2681.
Holger Schwenk, Daniel D?echelotte, and Jean-Luc
Gauvain. 2006. Continuous Space Language Mod-
els for Statistical Machine Translation. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 723?730, Sydney, Australia,
July.
Holger Schwenk. 2012. Continuous Space Translation
Models for Phrase-Based Statistical Machine Trans-
lation. In 25th International Conference on Compu-
tational Linguistics (COLING), pages 1071?1080,
Mumbai, India, December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
Martin Sundermeyer, Ralf Schl?uter, and Hermann Ney.
2012. LSTM neural networks for language model-
ing. In Interspeech, Portland, OR, USA, September.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl?uter, and Hermann Ney.
2013. Comparison of feedforward and recurrent
neural network language models. In IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, pages 8430?8434, Vancouver, Canada,
May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Paul J. Werbos. 1990. Backpropagation through time:
what it does and how to do it. Proceedings of the
IEEE, 78(10):1550?1560.
Ronald J. Williams and David Zipser. 1995. Gradient-
Based Learning Algorithms for Recurrent Net-
works and Their Computational Complexity. In:
Yves Chauvain and David E. Rumelhart: ?Back-
Propagation: Theory, Architectures and Applica-
tions?. Lawrence Erlbaum Publishers.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Assoc. for Computational Linguistics,
pages 475?484, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving statistical machine
translation with word class models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377?1381, Seattle, USA, October.
25
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 1?10,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Vector Space Models for Phrase-based Machine Translation
Tamer Alkhouli
1
, Andreas Guta
1
, and Hermann Ney
1,2
1
Human Language Technology and Pattern Recognition Group
RWTH Aachen University, Aachen, Germany
2
Spoken Language Processing Group
Univ. Paris-Sud, France and LIMSI/CNRS, Orsay, France
{surname}@cs.rwth-aachen.de
Abstract
This paper investigates the application
of vector space models (VSMs) to the
standard phrase-based machine translation
pipeline. VSMs are models based on
continuous word representations embed-
ded in a vector space. We exploit word
vectors to augment the phrase table with
new inferred phrase pairs. This helps
reduce out-of-vocabulary (OOV) words.
In addition, we present a simple way to
learn bilingually-constrained phrase vec-
tors. The phrase vectors are then used to
provide additional scoring of phrase pairs,
which fits into the standard log-linear
framework of phrase-based statistical ma-
chine translation. Both methods result
in significant improvements over a com-
petitive in-domain baseline applied to the
Arabic-to-English task of IWSLT 2013.
1 Introduction
Categorical word representation has been widely
used in many natural language processing (NLP)
applications including statistical machine transla-
tion (SMT), where words are treated as discrete
random variables. Continuous word representa-
tions, on the other hand, have been applied suc-
cessfully in many NLP areas (Manning et al.,
2008; Collobert and Weston, 2008). However,
their application to machine translation is still an
open research question. Several works tried to ad-
dress the question recently (Mikolov et al., 2013b;
Zhang et al., 2014; Zou et al., 2013), and this work
is but another step in that direction.
While categorical representations do not encode
any information about word identities, continuous
representations embed words in a vector space, re-
sulting in geometric arrangements that reflect in-
formation about the represented words. Such em-
beddings open the potential for applying informa-
tion retrieval approaches where it becomes possi-
ble to define and compute similarity between dif-
ferent words. We focus on continuous represen-
tations whose training is influenced by the sur-
rounding context of the token being represented.
One motivation for such representations is to cap-
ture word semantics (Turney et al., 2010). This
is based on the distributional hypothesis (Harris,
1954) which says that words that occur in similar
contexts tend to have similar meanings.
We make use of continuous vectors learned
using simple neural networks. Neural networks
have been gaining increasing attention recently,
where they have been able to enhance strong SMT
baselines (Devlin et al., 2014; Sundermeyer et
al., 2014). While neural language and transla-
tion modeling make intermediate use of continu-
ous representations, there have been also attempts
at explicit learning of continuous representations
to improve translation (Zhang et al., 2014; Gao et
al., 2013).
This work explores the potential of word se-
mantics based on continuous vector representa-
tions to enhance the performance of phrase-based
machine translation. We present a greedy algo-
rithm that employs the phrase table to identify
phrases in a training corpus. The phrase table
serves to bilingually restrict the phrases spotted
in the monolingual corpus. The algorithm is ap-
plied separately to the source and target sides of
the training data, resulting in source and target cor-
pora of phrases (instead of words). The phrase
corpus is used to learn phrase vectors using the
same methods that produce word vectors. The
vectors are then used to provide semantic scor-
ing of phrase pairs. We also learn word vectors
and employ them to augment the phrase table with
paraphrased entries. This leads to a reduction in
1
the OOV rate which translates to improved BLEU
and and TER scores. We apply the two methods on
the IWSLT 2013 Arabic-to-English task and show
significant improvements over a strong in-domain
baseline.
The rest of the paper is structured as follows.
Section 2 presents a background on word and
phrase vectors. The construction of the phrase
corpus is discussed in Section 3, while Section 4
demonstrates how to use word and phrase vectors
in the standard phrase-based SMT pipeline. Ex-
periments are presented in Section 5, followed by
an overview of the related word in Section 6, and
finally Section 7 concludes the work.
2 Vector Space Models
One way to obtain context-based word vectors is
through a neural network (Bengio et al., 2003;
Schwenk, 2007). With a vocabulary size V , one-
hot encoding of V -dimensional vectors is used to
represent input words, effectively associating each
word with a D-dimensional vector in the V ?D
input weight matrix, where D is the size of the
hidden layer. Similarly, one-hot encoding on the
output layer associates words with vectors in the
output weight matrix.
Alternatively, a count-based V-dimensional
word co-occurrence vector can serve as a word
representation (Lund and Burgess, 1996; Lan-
dauer and Dumais, 1997). Such representations
are sparse and high-dimensional, which might re-
quire an additional dimensionality reduction step
(e.g. using SVD). In contrast, learning word rep-
resentations via neural models results directly in
relatively low-dimensional, dense vectors. In this
work, we follow the neural network approach to
extract the feature vectors. Whether word vectors
are extracted by means of a neural network or co-
occurrence counts, the context surrounding a word
influences its final representation by design. Such
context-based representations can be used to de-
termine semantic similarities.
The construction of phrase representations, on
the other hand, can be done in different ways.
The compositional approach constructs the vector
representation of a phrase by resorting to its con-
stituent words (or sub-phrases) (Gao et al., 2013;
Chen et al., 2010). Kalchbrenner and Blunsom
(2013) obtain continuous sentence representations
by applying a sequence of convolutions, starting
with word representations.
Another approach for phrase representation
considers phrases as atomic units that can not be
divided further. The representations are learned
directly in this case (Mikolov et al., 2013b; Hu et
al., 2014).
In this work, we follow the second approach to
obtain phrase vectors. To this end, we apply the
same methods that yield word vectors, with the
difference that phrases are used instead of words.
In the case of neural word representations, a neural
network that is presented with words at the input
layer is presented with phrases instead. The result-
ing vocabulary size in this case would be the num-
ber of distinct phrases observed during training.
Although learning phrase embeddings directly is
amenable to data sparsity issues, it provides us
with a simple means to build phrase vectors mak-
ing use of tools already developed for word vec-
tors, focussing the effort on preprocessing the data
as will be discussed in the next section.
3 Phrase Corpus
When training word vectors using neural net-
works, the network is presented with a corpus.
To build phrase vectors, we first identify phrases
in the corpus and generate a phrase corpus. The
phrase corpus is similar to the original corpus ex-
cept that its words are joined to make up phrases.
The new corpus is then used to train the neural net-
work. The columns of the resulting input weight
matrix of the network are the phrase vectors corre-
sponding to the phrases encountered during train-
ing.
Mikolov et al. (2013b) identify phrases using a
monolingual point-wise mutual information crite-
rion with discounting. Since our end goal is to
generate phrase vectors that are helpful for trans-
lation, we follow a different approach: we con-
strain the phrases by the conventional phrase table
of phrase-based machine translation. This is done
by limiting the phrases identified in the corpus to
high quality phrases occurring in the phrase table.
The quality is determined using bilingual scores
of phrase pairs. While the phrase vectors of a lan-
guage are eventually obtained by training the neu-
ral network on the monolingual phrase corpus of
that language, the reliance on bilingual scores to
2
Algorithm 1 Phrase Corpus Construction
1: p? 1
2: for p? numPasses do
3: i? 2
4: for i? corpus.size?1 do
5: w?? join(t
i
, t
i+1
) . create a phrase using the current and next tokens
6: v?? join(t
i?1
, t
i
) . create a phrase using the previous and current tokens
7: joinForward? score(w?)
8: joinBackward? score(v?)
9: if joinForward ? joinBackward and joinForward ? ? then
10: t
i
? w?
11: remove t
i+1
12: i? i+2 . newly created phrase not available for further merge during current pass
13: else
14: if joinBackward > joinForward and joinBackward ? ? then
15: t
i?1
? v?
16: remove t
i
17: i? i+2 . newly created phrase not available for further merge during current pass
18: else
19: i? i+1
20: end if
21: end if
22: end for
23: p? p+1
24: end for
construct the monolingual phrase corpus encodes
bilingual information in the corpus, namely, the
corpus will include phrases that having a match-
ing phrase in the other language, which is in line
with the purpose for which the phrases are con-
structed, that is, their use in the phrase-based ma-
chine translation pipeline which is explained in the
next section. In addition, the aforementioned scor-
ing serves to exclude noisy phrase-pair entries dur-
ing the construction of the phrase corpus. Next, we
explain the details of the construction algorithm.
3.1 Phrase Spotting
We propose Algorithm 1 as a greedy approach for
phrase corpus construction. It is a multi-pass algo-
rithm where each pass can extend tokens obtained
during the previous pass by a single token at most.
Before the first pass, all tokens are words. During
the passes the tokens might remain as words or can
be extended to become phrases. Given a token t
i
at position i, a scoring function is used to score
the phrase (t
i
, t
i+1
) and the phrase (t
i?1
, t
i
). The
phrase having a higher score is adopted as long as
its score exceeds a predefined threshold ? . The
scoring function used in lines 7 and 8 is based on
the phrase table. If the phrase does not belong to
the phrase table it is given a score ?
?
< ? . If the
phrase exists, a bilingual score is computed using
the phrase table fields as follows:
score(
?
f ) = max
e?
{
L
?
i=1
w
i
g
i
(
?
f , e?)
}
(1)
where g
i
(
?
f , e?) is the ith feature of the bilingual
phrase pair (
?
f , e?). The maximization is carried out
over all phrases e? of the other language. The score
is the weighted sum of the phrase pair features.
Throughout our experiments, we use 2 phrasal and
2 lexical features for scoring, with manual tuning
of the weights w
i
.
The resulting corpus is then used to train phrase
vectors following the same procedure of training
word vectors.
4 End-to-end Translation
In this section we will show how to employ phrase
vectors in the phrase-based statistical machine
translation pipeline.
3
4.1 Phrase-based Machine Translation
The phrase-based decoder consists of a search us-
ing a log-linear framework (Och and Ney, 2002)
as follows:
e?
?
I
1
= argmax
I,e
I
1
{
max
K,s
K
1
M
?
m=1
?
m
h
m
(e
I
1
,s
K
1
, f
J
1
)
}
(2)
where e
I
1
= e
1
...e
I
is the target sentence, f
J
1
=
f
1
... f
J
is the source sentence, s
K
1
= s
1
...s
K
is
the hidden alignment or derivation. The mod-
els h
m
(e
I
1
,s
K
1
, f
J
1
) are weighted by the weights ?
m
which are tuned using minimum error rate train-
ing (MERT) (Och, 2003). The rest of the section
presents two ways to integrate vector representa-
tions into the system described above.
4.2 Semantic Phrase Feature
Words that occur in similar contexts tend to have
similar meanings. This idea is known as the dis-
tributional hypothesis (Harris, 1954), and it moti-
vates the use of word context to learn word repre-
sentations that capture word semantics (Turney et
al., 2010). Extending this notion to phrases, phrase
vectors that are learned based on the surrounding
context encode phrase semantics. Since we will
use phrase vectors to compute a feature of a phrase
pair in the following, we refer to the feature as a
semantic phrase feature.
Given a phrase pair (
?
f , e?), we can use the phrase
vectors of the source and target phrases to compute
a semantic phrase feature as follows:
h
M+1
(
?
f , e?) = sim(Wx
?
f
,z
e?
) (3)
where sim is a similarity function, x
?
f
and z
e?
are the
S-dimensional source and T -dimensional target
vectors respectively corresponding to the source
phrase
?
f and target phrase e?. W is an S?T linear
projection matrix that maps the source space to the
target space (Mikolov et al., 2013a). The matrix
is estimated by optimizing the following criterion
with stochastic gradient descent:
min
W
N
?
i=1
||Wx
i
? z
i
||
2
(4)
where the training data consists of the pairs
{(x
1
,z
1
), ...,(x
N
,z
N
)} corresponding to the source
and target vectors.
Since the source and target phrase vectors are
learned separately, we do not have an immedi-
ate mapping between them. As such mapping is
needed for the training of the projection matrix,
we resort to the phrase table to obtain it. A source
and a target phrase vectors are paired if there is a
corresponding phrase pair entry in the phrase table
whose score exceeds a certain threshold. Scoring
is computed using Eq. 1. Similarly, word vectors
are paired using IBM 1 p(e| f ) and p( f |e) lexica.
Noisy entries are assumed to have a probability
less than a certain threshold and are not used to
pair word vectors.
4.3 Paraphrasing
While the standard phrase table is extracted using
parallel training data, we propose to extend it and
infer new entries relying on continuous representa-
tions. With a similarity measure (e.g. cosine sim-
ilarity) that computes the similarity between two
phrases, a new phrase pair can be generated by re-
placing either or both of its constituent phrases by
similar phrases. The new phrase is referred to as a
paraphrase of the phrase it replaces. This enables
a richer use of the bilingual data, as a source para-
phrase can be borrowed from a sentence that is not
aligned to a sentence containing the target side of
the phrase pair. It also enables the use of monolin-
gual data, as the source and target paraphrases do
not have to occur in the parallel data. The cross-
interaction between sentences in the parallel data
and the inclusion of the monolingual data to ex-
tend the phrase table are potentially capable of re-
ducing the out-of-vocabulary (OOV) rate.
In order to generate a new phrase rule, we en-
sure that noisy rules do not contribute to the gener-
ation process, depending on the score of the phrase
pair (cf. Eq. 1). High scoring entries are para-
phrased as follows. To paraphrase the source side,
we perform a k-nearest neighbor search over the
source phrase vectors. The top-k similar entries
are considered paraphrases of the given phrase.
The same can be done for the target side. We as-
sign the newly generated phrase pair the same fea-
ture values of the pair used to induce it. However,
two extra phrase features are added: one measur-
ing the similarity between the source phrase and
its paraphrase, and another for the target phrase
and its paraphrase. The new feature values for
the original non-paraphrased entries are set to the
4
highest similarity value.
We focus on a certain setting that avoids in-
terference with original phrase rules, by extend-
ing the phrase table to cover OOVs only. That
is, source-side paraphrasing is performed only if
the source paraphrase does not already occur in
the phrase table. This ensures that original entries
are not interfered with and only OOVs are affected
during translation. Reducing OOVs by extending
the phrase table has the advantage of exploiting
the full decoding capabilities (e.g. LM scoring),
as opposed to post-decoding translation of OOVs,
which would not exhibit any decoding benefits.
The k-nearest neighbor (k-NN) approach is
computationally prohibitive for large phrase tables
and large number of vectors. This can be allevi-
ated by resorting to approximate k-NN search (e.g.
locality sensitive hashing). Note that this search
is performed during training time to generate ad-
ditional phrase table entries, and does not affect
decoding time, except through the increase of the
phrase table size. In our experiments, the train-
ing time using exact k-NN search was acceptable,
therefore no search approximations were made.
5 Experiments
In the following we first provide an analysis of the
word vectors that are later used for translation ex-
periments. We use word vectors (as opposed to
phrase vectors) for phrase table paraphrasing to
reduce the OOV rate. Next, we present end-to-
end translation results using the proposed seman-
tic feature and our OOV reduction method.
The experiments are based on vectors trained
using the word2vec
1
toolkit, setting vector dimen-
sionality to 800 for Arabic and 200 for English
vectors. We used the skip-gram model with a max-
imum skip length of 10. The phrase corpus was
constructed using 5 passes, with scores computed
according to Eq. 1 using 2 phrasal and 2 lexical
features. The phrasal and lexical weights were set
to 1 and 0.5 respectively, with all features being
negative log-probabilities, and the scoring thresh-
old ? was set to 10. All translation experiments
are performed with the Jane toolkit (Vilar et al.,
2010; Wuebker et al., 2012).
1
https://code.google.com/p/word2vec/
5.1 Baseline System
Our phrase-based baseline system consists of two
phrasal and two lexical translation models, trained
using a word-aligned bilingual training corpus.
Word alignment is automatically generated by
GIZA
++
(Och and Ney, 2003) given a sentence-
aligned bilingual corpus. We also include bi-
nary count features and bidirectional hierarchical
reordering models (Galley and Manning, 2008),
with three orientation classes per direction result-
ing in six reordering models. The baseline also in-
cludes word penalty, phrase penalty and a simple
distance-based distortion model.
The language model (LM) is a 4-gram mix-
ture LM trained on several data sets using mod-
ified Kneser-Ney discounting with interpolation,
and combined with weights tuned to achieve the
lowest perplexity on a development set using the
SRILM toolkit (Stolcke, 2002). Data selection
is performed using cross-entropy filtering (Moore
and Lewis, 2010).
5.2 Word Vectors
Here we analyze the quality of word vectors used
in the OOV reduction experiments. The vectors
are trained using an unaltered word corpus. We
build a lexicon using source and target word vec-
tors together with the projection matrix using the
similarity score sim(Wx
f
,z
e
)), where the projec-
tion matrix W is used to project the source word
vector x
f
, corresponding to the source word f , to
the target vector space. The similarity between the
projection result Wx
f
and the target word vector
z
e
is computed. In the following we will refer to
these scores computed using vector representation
as VSM-based scores.
The resulting lexicon is compared to the IBM
1 lexicon
2
. Given a source word, we select the
the best target word according to the VSM-based
score. This is compared to the best translation
based on the IBM 1 probability. If both transla-
tions coincide, we refer to this as a 1-best match.
We also check whether the best translation accord-
ing to IBM 1 matches any of the top-5 translations
based on the VSM model. A match in this case is
referred to as a 5-best match.
2
We assume for the purpose of this experiment that the
IBM 1 lexicon provides perfect translations, which is not nec-
essarily the case in practice.
5
corpus Lang. # tokens # segments
WIT Ar 3,185,357 147,256
UN Ar 228,302,244 7,884,752
arGiga3 Ar 782,638,101 27,190,387
WIT En 2,951,851 147,256
UN En 226,280,918 7,884,752
news En 1,129,871,814 45,240,651
Table 1: Arabic and English corpora statistics.
The vectors are trained on a mixture of in-
domain data (WIT) which correspond to TED
talks, and out-of-domain data (UN). These sets are
provided as part of the IWSLT 2013 evaluation
campaign. We include the LDC2007T40 Arabic
Gigaword v3 (arGiga3) and English news crawl ar-
ticles (2007 through 2012) to experiment with the
effect of increasing the size of the training corpus
on the quality of the word vectors. Table 1 shows
the corpora statistics obtained after preprocessing.
The fractions of the 1- and 5-best matches are
shown in table 2. The table is split into two halves.
The upper part investigates the effect of increasing
the amount of Arabic data while keeping the En-
glish data fixed (2nd row), the effect of increasing
the amount of the English data while keeping the
Arabic data fixed (3rd row), and the effect of using
more data on both sides (4th row). The projection
is done on the representation of the Arabic word f ,
and the similarity is computed between the projec-
tion and the representation of the English word e.
In the lower half of the table, the same effects are
explored, except that the projection is performed
on the English side instead. The results indicate
that the accuracy increases when increasing the
amount of data only on the side being projected.
More data on the corresponding side (i.e. the side
being projected to) decreases the accuracy. The
same behavior is observed whether the projected
side is Arabic (upper half) or English (lower half).
All in all, the accuracy values are low. The accu-
racy increases about three times when looking at
the 5-best instead of the 1-best accuracy. While the
accuracies 32.2% and 33.1% are low, they reflect
that the word representations are encoding some
information about the words, although this infor-
mation might not be good enough to build a word-
to-word lexicon. However, using this information
for OOV reduction might still yield improvements
as we will see in the translation results.
Arabic English
word corpus size 231M 229M
phrase corpus size 126M 115M
word corpus vocab. size 467K 421K
phrase corpus vocab. size 5.8M 5.3M
# phrase vectors 934K 913K
Table 3: Phrase vectors statistics.
5.3 Phrase Vectors
Translation experiments pertaining to the pro-
posed semantic feature are presented here. The
feature is based on phrase vectors which are built
with the word2vec toolkit in a similar way word
vectors are trained, except that the training cor-
pus is the phrase corpus containing phrases con-
structed as described in section 3. Once trained, a
new feature is added to the phrase table. The fea-
ture is computed for each phrase pair using phrase
vectors as described in Eq. 3.
Table 3 shows statistics about the phrase corpus
and the original word corpus it is based on. Al-
gorithm 1 is used to build the phrase corpus using
5 passes. The number of phrase vectors trained
using the phrase corpus are also shown. Note that
the tool used does not produce vectors for all 5.8M
Arabic and 5.3M English phrases in the vocab-
ulary. Rather, noisy phrases are excluded from
training, eventually leading to 934K Arabic and
913K English phrase embeddings.
We perform two experiments on the IWSLT
2013 Arabic-to-English evaluation data set. In the
first experiment, we examine how the semantic
feature affects a small phrase table (2.3M phrase
pairs) trained on the in-domain data (WIT). The
second experiment deals with a larger phrase table
(34M phrase pairs), constructed by a linear inter-
polation between in- and out-of-domain phrase ta-
bles including UN data, resulting in a competitive
baseline. The two baselines have hierarchical re-
ordering models (HRMs) and a tuned mixture LM,
in addition to the standard models, as described in
section 5.1. The results are shown in table 4.
In the small experiment, the semantic phrase
feature improves TER by 0.7%, and BLEU by
0.4% on the test set eval13. The translation seems
to benefit from the contextual information en-
coded in the phrase vectors during training. This
is in contrast to the training of the standard phrase
6
Arabic Data English Data 1-best
Match %
5-best
Matches %
WIT+UN WIT+UN 8.0 26.1
WIT+UN+arGiga3 WIT+UN 10.9 32.2
WIT+UN WIT+UN+news 4.9 17.9
WIT+UN+arGiga3 WIT+UN+news 7.5 25.7
WIT+UN WIT+UN 8.4 27.2
WIT+UN WIT+UN+news 10.9 33.1
WIT+UN+arGiga3 WIT+UN 5.7 18.9
WIT+UN+arGiga3 WIT+UN+news 8.3 25.2
Table 2: The effect of increasing the amount of data on the quality of word vectors. VSM-based scores are
compared to IBM model 1 p(e| f ) (upper half) and p( f |e) (lower half), effectively regarding the IBM 1
models as the true probability distributions. In the upper part, the projection is done on the representation
of the Arabic word f , and the similarity is computed between the projection and the representation of the
English word e. In the lower half of the table, the role of f and e is interchanged, where the English side
in this case will be projected.
system dev2010 eval2013
BLEU TER BLEU TER
WIT 29.1 50.5 28.9 52.5
+ feature 29.1 ?50.1 ?29.3 ?51.8
+ paraph. 29.2 ?50.2 ?29.5 ?51.8
+ both 29.2 50.2 ?29.4 ?51.8
WIT+UN 29.7 49.3 30.5 50.5
+ feature 29.8 49.2 30.2 50.7
Table 4: Semantic feature and paraphrasing re-
sults. The symbol ? indicates statistical signifi-
cance with p < 0.01.
features, which disregards context. As for the hi-
erarchical reordering models which are part of the
baseline, they do not capture lexical information
about the context. They are only limited to the or-
dering information. The skip-gram-based phrase
vectors used for the semantic feature, on the other
hand, discard ordering information, but uses con-
textual lexical information for phrase representa-
tion. In this sense, HRMs and the semantic feature
can be said to complement each other. Using the
semantic feature for the large phrase table did not
yield improvements. The difference compared to
the baseline in this case is not statistically signifi-
cant.
All reported results are averages of 3 MERT op-
timizer runs. Statistical significance is computed
using the Approximate Randomization (AR) test.
We used the multeval toolkit (Clark et al., 2011)
for evaluation.
5.4 Paraphrasing and OOV Reduction
The next set of experiments investigates the re-
duction of the OOV rate through paraphrasing,
and its impact on translation. Paraphrasing is per-
formed employing the cosine similarity, and the k-
NN search is done on the source side, with k = 3.
The nearest neighbors are required to satisfy a ra-
dius threshold r > 0.3, i.e., neighbors with a simi-
larity value less or equal to r are rejected. Training
the projection matrices is performed using a small
amount of training data amounting to less than 30k
translation pairs.
To examine the effect of OOV reduction, we
perform paraphrasing on a resource-limited sys-
tem, where a small amount of parallel data ex-
ists, but a larger amount of monolingual data is
available. Such a system is simulated by train-
ing word vectors on the WIT+UN data monolin-
gually , while extracting the phrase table using the
much smaller in-domain WIT data set only. Table
5 shows the change in the number of OOV words
after introducing the paraphrased rules to the WIT-
based phrase table. 19% and 30% of the original
OOVs are eliminated in the dev and eval13 sets,
respectively. This reduction translates to an im-
provement of 0.6% BLEU and 0.7% TER as indi-
cated in table 4.
Since BLEU or TER are based on word iden-
tities and do not detect semantic similarities, we
make a comparison between the reference transla-
tions and translations of the system that employed
7
# OOV
phrase table dev eval13
WIT 185 254
WIT+paraph. 150 183
Vocab. size 3,714 4,734
Table 5: OOV change due to paraphrasing. Vocab-
ulary refers to the number of unique tokens in the
Arabic dev and test sets.
OOV VSM-based
Translation
Reference

I
	
?

??

K found unfolded

??


Qk interested keen
?


	
?m
.

?
jail imprisoned
	
?CK
.
claim report

??
.

J?? confusing confounding

I

Jk encourage rallied for
AK


?Q

? villagers redneck
Table 6: Examples of OOV words that were trans-
lated due to paraphrasing. The examples are
extracted from the translation hypotheses of the
small experiment.
OOV reduction. Examples are shown in Table 6.
Although the reference words are not matched ex-
actly, the VSM translations are semantically close
to them, suggesting that OOV reduction in these
cases was somewhat successful, although not re-
warded by either of the scoring measures used.
6 Related Work
Bilingually-constrained phrase embeddings were
developed in (Zhang et al., 2014). Initial embed-
dings were trained in an unsupervised manner, fol-
lowed by fine-tuning using bilingual knowledge to
minimize the semantic distance between transla-
tion equivalents, and maximizing the distance be-
tween non-translation pairs. The embeddings are
learned using recursive neural networks by de-
composing phrases to their constituents. While
our work includes bilingual constraints to learn
phrase vectors, the constraints are implicit in the
phrase corpus. Our approach is simple, focusing
on the preprocessing step of preparing the phrase
corpus, and therefore it can be used with different
existing frameworks that were developed for word
vectors.
Zou et al. (2013) learn bilingual word embed-
dings by designing an objective function that com-
bines unsupervised training with bilingual con-
straints based on word alignments. Similar to
our work, they compute an additional feature for
phrase pairs using cosine similarity. Word vec-
tors are averaged to obtain phrase representations.
In contrast, our approach learns phrase representa-
tions directly.
Recurrent neural networks were used with min-
imum translation units (Hu et al., 2014), which are
phrase pairs undergoing certain constraints. At the
input layer, each of the source and target phrases
are modeled as a bag of words, while the output
phrase is predicted word-by-word assuming con-
ditional independence. The approach seeks to al-
leviate data sparsity problems that would arise if
phrases were to be uniquely distinguished. Our
approach does not break phrases down to words,
but learns phrase embeddings directly.
Chen et al. (2010) represent a rule in the hierar-
chical phrase table using a bag-of-words approach.
Instead, we learn phrase vectors directly without
resorting to their constituent words. Moreover,
they apply a count-based approach and employ
IBM model 1 probabilities to project the target
space to the source space. In contrast, our map-
ping is similar to that of Mikolov et al. (2013a)
and is learned directly from a small set of bilin-
gual data.
Mikolov et al. (2013a) proposed an efficient
method to learn word vectors through feed-
forward neural networks by eliminating the hid-
den layer. They do not report end-to-end sentence
translation results as we do in this work.
Mikolov et al. (2013b) learn direct representa-
tions of phrases after joining a training corpus us-
ing a simple monolingual point-wise mutual in-
formation criterion with discounting. Our work
exploits the rich bilingual knowledge provided by
the phrase table to join the corpus instead.
Gao et al. (2013) learn shared space mappings
using a feed-forward neural network and represent
a phrase vector as a bag-of-words vector. The vec-
tors are learned aiming to optimize an expected
BLEU criterion. Our work is different in that we
learn two separate source and target mappings.
8
We also do not follow their bag-of-words phrase
model approach.
Marton et al. (2009) proposed to eliminate
OOVs by looking for similar words using distri-
butional vectors, but they prune the search space
limiting it to candidates observed in the same con-
text as that of the OOV. We do not employ such a
heuristic. Instead, we perform a k-nearest neigh-
bor search spanning the full phrase table to para-
phrase its rules and generate new entries.
Estimating phrase table scores using monolin-
gual data was investigated in (Klementiev et al.,
2012), by building co-occurrence context vectors
and using a small dictionary to induce new scores
for existing phrase rules. Our work explores the
use of distributional vectors extracted from neu-
ral networks, moreover, we induce new phrase
rules to extend the phrase table. New phrase rules
were also generated in (Irvine and Callison-Burch,
2014), where new phrases were produced as a
composition of unigram translations.
7 Conclusion
In this work we adapted vector space models to
provide the state-of-the-art phrase-based statisti-
cal machine translation system with semantic in-
formation. We leveraged the bilingual knowledge
of the phrase table to construct source and target
phrase corpora to learn phrase vectors, which were
used to provide semantic scoring of phrase pairs.
Word vectors allowed to extend the phrase table
and eliminate OOVs. Both methods proved bene-
ficial for low-resource tasks.
Future work would investigate decoder inte-
gration of semantic scoring that extends beyond
phrase boundaries to provide semantically coher-
ent translations.
Acknowledgments
This material is partially based upon work sup-
ported by the DARPA BOLT project under Con-
tract No. HR0011- 12-C-0015. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
The research leading to these results has also re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement n
o
287658.
References
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Boxing Chen, George Foster, and Roland Kuhn. 2010.
Bilingual sense similarity for statistical machine
translation. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 834?843.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis test-
ing for statistical machine translation: Controlling
for optimizer instability. In 49th Annual Meet-
ing of the Association for Computational Linguis-
tics:shortpapers, pages 176?181, Portland, Oregon,
June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and Robust Neural Network Joint Models for
Statistical Machine Translation. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics, Baltimore, MD, USA, June.
Michel Galley and Christopher D. Manning. 2008.
A simple and effective hierarchical phrase reorder-
ing model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?08, pages 848?856, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Zellig S Harris. 1954. Distributional structure. Word.
Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014. Minimum translation modeling with recur-
rent neural networks. In Proceedings of the 14th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 20?29,
Gothenburg, Sweden, April. Association for Com-
putational Linguistics.
Ann Irvine and Chris Callison-Burch. 2014. Hal-
lucinating phrase translations for low resource mt.
In Proceedings of the Conference on Computational
Natural Language Learning (CoNLL).
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1700?1709, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
9
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 130?140. Association for Computa-
tional Linguistics.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
104(2):211.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, & Computers, 28(2):203?208.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to informa-
tion retrieval, volume 1. Cambridge university press
Cambridge.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1-Volume 1, pages 381?390. Association for Com-
putational Linguistics.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among lan-
guages for machine translation. arXiv preprint
arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111?3119.
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short
Papers), pages 220?224, Uppsala, Sweden, July.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native Training and Maximum Entropy Models for
Statistical Machine Translation. In Proc. of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 295?302, Philadel-
phia, PA, July.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160?167, Sap-
poro, Japan, July.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech & Language, 21(3):492?
518.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation Modeling
with Bidirectional Recurrent Neural Networks. In
Proceedings of the Conference on Empirical Meth-
ods on Natural Language Processing, October.
Peter D Turney, Patrick Pantel, et al. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open source hierarchi-
cal translation, extended with reordering and lexi-
con models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262?270, Uppsala, Sweden, July.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2: Open
source phrase-based and hierarchical statistical ma-
chine translation. In International Conference on
Computational Linguistics, pages 483?491, Mum-
bai, India, December.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52th Annual Meeting on Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
EMNLP, pages 1393?1398.
10
