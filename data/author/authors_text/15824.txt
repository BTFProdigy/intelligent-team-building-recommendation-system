Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 109?119,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Adaptation of Statistical Machine Translation Model for Cross-Lingual
Information Retrieval in a Service Context
Vassilina Nikoulina
Xerox Research Center Europe
vassilina.nikoulina@xrce.xerox.com
Bogomil Kovachev
Informatics Institute
University of Amsterdam
B.K.Kovachev@uva.nl
Nikolaos Lagos
Xerox Research Center Europe
nikolaos.lagos@xrce.xerox.com
Christof Monz
Informatics Institute
University of Amsterdam
C.Monz@uva.nl
Abstract
This work proposes to adapt an existing
general SMT model for the task of translat-
ing queries that are subsequently going to
be used to retrieve information from a tar-
get language collection. In the scenario that
we focus on access to the document collec-
tion itself is not available and changes to
the IR model are not possible. We propose
two ways to achieve the adaptation effect
and both of them are aimed at tuning pa-
rameter weights on a set of parallel queries.
The first approach is via a standard tuning
procedure optimizing for BLEU score and
the second one is via a reranking approach
optimizing for MAP score. We also extend
the second approach by using syntax-based
features. Our experiments show improve-
ments of 1-2.5 in terms of MAP score over
the retrieval with the non-adapted transla-
tion. We show that these improvements are
due both to the integration of the adapta-
tion and syntax-features for the query trans-
lation task.
1 Introduction
Cross Lingual Information Retrieval (CLIR) is an
important feature for any digital content provider
in today?s multilingual environment. However,
many of the content providers are not willing to
change existing well-established document index-
ing and search tools, nor to provide access to
their document collection by a third-party exter-
nal service. The work presented in this paper as-
sumes such a context of use, where a query trans-
lation service allows translating queries posed to
the search engine of a content provider into sev-
eral target languages, without requiring changes
to the undelying IR system used and without ac-
cessing, at translation time, the content provider?s
document set. Keeping in mind these constraints,
we present two approaches on query translation
optimisation.
One of the important observations done dur-
ing the CLEF 2009 campaign (Ferro and Peters,
2009) related to CLIR was that the usage of Sta-
tistical Machine Translation (SMT) systems (eg.
Google Translate) for query translation led to
important improvements in the cross-lingual re-
trieval performance (the best CLIR performance
increased from ?55% of the monolingual baseline
in 2008 to more than 90% in 2009 for French
and German target languages). However, general-
purpose SMT systems are not necessarily adapted
for query translation. That is because SMT sys-
tems trained on a corpus of standard parallel
phrases take into account the phrase structure im-
plicitly. The structure of queries is very differ-
ent from the standard phrase structure: queries are
very short and the word order might be different
than the typical full phrase one. This problem can
be seen as a problem of genre adaptation for SMT,
where the genre is ?query?.
To our knowledge, no suitable corpora of par-
allel queries is available to train an adapted SMT
system. Small corpora of parallel queries1 how-
ever can be obtained (eg. CLEF tracks) or man-
ually created. We suggest to use such corpora
in order to adapt the SMT model parameters for
query translation. In our approach the parameters
of the SMT models are optimized on the basis of
the parallel queries set. This is achieved either di-
rectly in the SMT system using the MERT (Mini-
mum Error Rate Training) algorithm and optimiz-
1Insufficient for a full SMT system training (?500 entries)
109
ing according to the BLEU2(Papineni et al 2001)
score, or via reranking the Nbest translation can-
didates generated by a baseline system based on
new parameters (and possibly new features) that
aim to optimize a retrieval metric.
It is important to note that both of the pro-
posed approaches allow keeping the MT system
independent of the document collection and in-
dexing, and thus suitable for a query translation
service. These two approaches can also be com-
bined by using the model produced with the first
approach as a baseline that produces the Nbest list
of translations that is then given to the reranking
approach.
The remainder of this paper is organized as fol-
lows. We first present related work addressing the
problem of query translation. We then describe
two approaches towards adapting an SMT system
to the query-genre: tuning the SMT system on a
parallel set of queries (Section 3.1) and adapting
machine translation via the reranking framework
(Section 3.2). We then present our experimental
settings and results (Section 4) and conclude in
section 5.
2 Related work
We may distinguish two main groups of ap-
proaches to CLIR: document translation and
query translation. We concentrate on the second
group which is more relevant to our settings. The
standard query translation methods use different
translation resources such as bilingual dictionar-
ies, parallel corpora and/or machine translation.
The aspect of disambiguation is important for the
first two techniques.
Different methods were proposed to deal with
disambiguation issues, often relying on the docu-
ment collection or embedding the translation step
directly into the retrieval model (Hiemstra and
Jong, 1999; Berger et al 1999; Kraaij et al
2003). Other methods rely on external resources
like query logs (Gao et al 2010), Wikipedia (Ja-
didinejad and Mahmoudi, 2009) or the web (Nie
and Chen, 2002; Hu et al 2008). (Gao et al
2006) proposes syntax-based translation models
to deal with the disambiguation issues (NP-based,
dependency-based). The candidate translations
proposed by these models are then reranked with
the model learned to minimize the translation er-
2Standard MT evaluation metric
ror on the training data.
To our knowledge, existing work that use MT-
based techniques for query translation use an out-
of-the-box MT system, without adapting it for
query translation in particular (Jones et al 1999;
Wu et al 2008) (although some query expan-
sion techniques might be applied to the produced
translation afterwards (Wu and He, 2010)).
There is a number of works done for do-
main adaptation in Statistical Machine Transla-
tion. However, we want to distinguish between
genre and domain adaptation in this work. Gen-
erally, genre can be seen as a sub-problem of do-
main. Thus, we consider genre to be the general
style of the text e.g. conversation, news, blog,
query (responsible mostly for the text structure)
while the domain reflects more what the text is
about ? eg. social science, healthcare, history, so
domain adaptation involves lexical disambigua-
tion and extra lexical coverage problems. To our
knowledge, there is not much work addressing ex-
plicitly the problem of genre adaptation for SMT.
Some work done on domain adaptation could be
applied to genre adaptation, such as incorporating
available in-domain corpora in the SMT model:
either monolingual (Bertoldi and Federico, 2009;
Wu et al 2008; Zhao et al 2004; Koehn and
Schroeder, 2007), or small parallel data used for
tuning the SMT parameters (Zheng et al 2010;
Pecina et al 2011).
3 Our approach
This work is based on the hypothesis that the
general-purpose SMT system needs to be adapted
for query translation. Although in (Ferro and
Peters, 2009) it has been mentioned that using
Google translate (general-purpose MT) for query
translation allowed to CLEF participants to obtain
the best CLIR performance, there is still 10% gap
between monolingual and cross-lingual IR. We
believe that, as in (Clinchant and Renders, 2007),
more adapted query translation, possibly further
combined with query expansion techniques, can
lead to improved retrieval.
The problem of the SMT adaptation for query-
genre translation has different quality aspects.
On the one hand, we want our model to pro-
duce a ?good? translation (well-formed and trans-
mitting the information contained in the source
query) of an input query. On the other hand, we
want to obtain good retrieval performance using
110
the proposed translation. These two aspects are
not necessarily correlated: a bag-of-word transla-
tion can lead to good retrieval performance, even
though it won?t be syntactically well-formed; at
the same time a well-formed translation can lead
to worse retrieval if the wrong lexical choice is
done. Moreover, often the retrieval demands some
linguistic preprocessing (eg. lemmatisation, PoS
tagging) which in interaction with badly-formed
translations might bring some noise.
A couple of works studied the correlation be-
tween the standard MT evaluation metrics and
the retrieval precision. Thus, (Fujii et al 2009)
showed a good correlation of the BLEU scores
with the MAP scores for Cross-Lingual Patent
Retrieval. However, the topics in patent search
(long and well structured) are very different from
standard queries. (Kettunen, 2009) also found a
pretty high correlation ( 0.8 ? 0.9) between stan-
dard MT evaluation metrics (METEOR(Banerjee
and Lavie, 2005), BLEU, NIST(Doddington,
2002)) and retrieval precision for long queries.
However, the same work shows that the correla-
tion decreases ( 0.6? 0.7) for short queries.
In this paper we propose two approaches to
SMT adaptation for queries. The first one op-
timizes BLEU, while the second one optimizes
Mean Average Precision (MAP), a standard met-
ric in information retrieval. We?ll address the is-
sue of the correlation between BLEU and MAP in
Section 4.
Both of the proposed approaches rely on the
phrase-based SMT (PBMT) model (Koehn et al
2003) implemented in the Open Source SMT
toolkit MOSES (Koehn et al 2007).
3.1 Tuning for genre adaptation
First, we propose to adapt the PBMT model by
tuning the model?s weights on a parallel set of
queries. This approach addresses the first as-
pect of the problem, which is producing a ?good?
translation. The PBMT model combines differ-
ent types of features via a log-linear model. The
standard features include (Koehn, 2010, Chapter
5): language model, word penalty, distortion, dif-
ferent translation models, etc. The weights of
these features are learned during the tuning step
with the MERT (Och, 2003) algorithm. Roughly
the MERT algorithm tunes feature weights one by
one and optimizes them according to the BLEU
score obtained.
Our hypothesis is that the impact of different
features should be different depending on whether
we translate a full sentence, or a query-genre en-
try. Thus, one would expect that in the case
of query-genre the language model or the distor-
tion features should get less importance than in
the case of the full-sentence translation. MERT
tuning on a genre-adapted parallel corpus should
leverage this information from the data, adapting
the SMT model to the query-genre. We would
also like to note that the tuning approach (pro-
posed for domain adaptation by (Zheng et al
2010)) seems to be more appropriate for genre
adaptation than for domain adaptation where the
problem of lexical ambiguity is encoded in the
translation model and re-weighting the main fea-
tures might not be sufficient.
We use the MERT implementation provided
with the Moses toolkit with default settings. Our
assumption is that this procedure although not ex-
plicitly aimed at improving retrieval performance
will nevertheless lead to ?better? query transla-
tions when compared to the baseline. The results
of this apporach allow us also to observe whether
and to what extent changes in BLEU scores are
correlated to changes in MAP scores.
3.2 Reranking framework for query
translation
The second approach addresses the retrieval qual-
ity problem. An SMT system is usually trained to
optimize the quality of the translation (eg. BLEU
score for SMT), which is not necessarily corre-
lated with the retrieval quality (especially for the
short queries). Thus, for example, the word or-
der which is crucial for translation quality (and is
taken into account by most MT evaluation met-
rics) is often ignored by IR models. Our second
approach follows (Nie, 2010, pp.106) argument
that ?the translation problem is an integral part
of the whole CLIR problem, and unified CLIR
models integrating translation should be defined?.
We propose integrating the IR metric (MAP) into
the translation model optimisation step via the
reranking framework.
Previous attempts to apply the reranking ap-
proach to SMT did not show significant improve-
ments in terms of MT evaluation metrics (Och
et al 2003; Nikoulina and Dymetman, 2008).
One of the reasons being the poor diversity of the
Nbest list of the translations. However, we be-
111
lieve that this approach has more potential in the
context of query translation.
First of all the average query length is ?5 words,
which means that the Nbest list of the translations
is more diverse than in the case of general phrase
translation (average length 25-30 words).
Moreover, the retrieval precision is more natu-
rally integrated into the reranking framework than
standard MT evaluation metrics such as BLEU.
The main reason is that the notion of Average Re-
trieval Precision is well defined for a single query
translation, while BLEU is defined on the corpus
level and correlates poorly with human quality
judgements for the individual translations (Specia
et al 2009; Callison-Burch et al 2009).
Finally, the reranking framework allows a lot
of flexibility. Thus, it allows enriching the base-
line translation model with new complex features
which might be difficult to introduce into the
translation model directly.
Other works applied the reranking framework
to different NLP tasks such as Named Entities
Extraction (Collins, 2001), parsing (Collins and
Roark, 2004), and language modelling (Roark et
al., 2004). Most of these works used the reranking
framework to combine generative and discrimina-
tive methods when both approaches aim at solv-
ing the same problem: the generative model pro-
duces a set of hypotheses, and the best hypoth-
esis is chosen afterwards via the discriminative
reranking model, which allows to enrich the base-
line model with the new complex and heteroge-
neous features. We suggest using the reranking
framework to combine two different tasks: Ma-
chine Translation and Cross-lingual Information
Retrieval. In this context the reranking framework
doesn?t only allow enriching the baseline transla-
tion model but also performing training using a
more appropriate evaluation metric.
3.2.1 Reranking training
Generally, the reranking framework can be re-
sumed in the following steps :
1. The baseline (generic-purpose) MT system
generates a list of candidate translations
GEN(q) for each query q;
2. A vector of features F (t) is assigned to each
translation t ? GEN(q);
3. The best translation t? is chosen as the one
maximizing the translation score, which is
defined as a weighted linear combination of
features: t?(?) = argmaxt?GEN(q) ??F (t)
As shown above the best translation is selected ac-
cording to features? weights ?. In order to learn
the weights ? maximizing the retrieval perfor-
mance, an appropriate annotated training set has
to be created. We use the CLEF tracks to create
the training set. The retrieval scores annotations
are based on the document relevance annotations
performed by human annotators during the CLEF
campaign.
The annotated training set is created out of
queries {q1, ..., qK} with an Nbest list of trans-
lations GEN(qi) of each query qi, i ? {1..K} as
follows:
? A list of N (we take N = 1000) translations
(GEN(qi)) is produced by the baseline MT
model for each query qi, i = 1..K.
? Each translation t ? GEN(qi) is used
to perform a retrieval from a target docu-
ment collection, and an Average Precision
score (AP (t)) is computed for each t ?
GEN(qi) by comparing its retrieval to the
relevance annotations done during the CLEF
campaign.
The weights ? are learned with the objective of
maximizing MAP for all the queries of the train-
ing set, and, therefore, are optimized for retrieval
quality.
The weights optimization is done with
the Margin Infused Relaxed Algorithm
(MIRA)(Crammer and Singer, 2003), which
was applied to SMT by (Watanabe et al 2007;
Chiang et al 2008). MIRA is an online learning
algorithm where each weights update is done to
keep the new weights as close as possible to the
old weights (first term), and score oracle trans-
lation (the translation giving the best retrieval
score : t?i = argmaxtAP (t)) higher than each
non-oracle translation (tij) by a margin at least as
wide as the loss lij (second term):
? = min??
1
2??
?
? ??2 +
C
?K
i=1 maxj=1..N
(
lij ? ?
?
? (F (t?i )? F (tij)
)
The loss lij is defined as the difference in the re-
trieval average precision between the oracle and
non-oracle translations: lij = AP (t?i )?AP (tij).
C is the regularization parameter which is chosen
via 5-fold cross-validation.
112
3.2.2 Features
One of the advantages of the reranking frame-
work is that new complex features can be easily
integrated. We suggest to enrich the reranking
model with different syntax-based features, such
as:
? features relying on dependency structures:
called therein coupling features (proposed by
(Nikoulina and Dymetman, 2008));
? features relying on Part of Speech Tagging:
called therein PoS mapping features.
By integrating the syntax-based features we
have a double goal: showing the potential of
the reranking framework with more complex fea-
tures, and examining whether the integration of
syntactic information could be useful for query
translation.
Coupling features. The goal of the coupling
features is to measure the similarity between
source and target dependency structures. The ini-
tial hypothesis is that a better translation should
have a dependency structure closer to the one of
the source query.
In this work we experiment with two dif-
ferent coupling variants proposed in (Nikoulina
and Dymetman, 2008), namely, Lexicalised and
Label-specific coupling features.
The generic coupling features are based on
the notion of ?rectangles? that are of the follow-
ing type : ((s1, ds12, s2), (t1, dt12, t2)), where
ds12 is an edge between source words s1 and s2,
dt12 is an edge between target words t1 and t2,
s1 is aligned with t1 and s2 is aligned with t2.
Lexicalised features take into account the qual-
ity of lexical alignment, by weighting each rect-
angle (s1, s2, t1, t2) by a probability of align-
ing s1 to t1 and s2 to t2 (eg. p(s1|t1)p(s2|t2) or
p(t1|s1)p(t2|s2)).
The Label-Specific features take into account
the nature of the aligned dependencies. Thus, the
rectangles of the form ((s1, subj, s2), (t1, subj,
t2)) will get more weight than a rectangle ((s1,
subj, s2), (t1, nmod, t2)). The importance of
each ?rectangle? is learned on the parallel anno-
tated corpus by introducing a collection of Label-
Specific coupling features, each for a specific pair
of source label and target label.
PoS mapping features. The goal of the PoS
mapping features is to control the correspondence
of Part Of Speech Tags between an input query
and its translation. As the coupling features, the
PoS mapping features rely on the word align-
ments between the source sentence and its trans-
lation3. A vector of sparse features is introduced
where each component corresponds to a pair of
PoS tags aligned in the training data. We intro-
duce a generic PoS map variant, which counts a
number of occurrences of a specific pair of PoS
tags, and lexical PoS map variant, which weights
down these pairs by a lexical alignment score
(p(s|t) or p(t|s)).
4 Experiments
4.1 Experimental basis
4.1.1 Data
To simulate parallel query data we used trans-
lation equivalent CLEF topics. The data set used
for the first approach consists of the CLEF topic
data from the following years and tasks: AdHoc-
main track from 2000 to 2008; CLEF AdHoc-
TEL track 2008; Domain Specific tracks from
2000 to 2008; CLEF robust tracks 2007 and 2008;
GeoCLEf tracks 2005-2007. To avoid the issue of
overlapping topics we removed duplicates. The
created parallel queries set contained 500 ? 700
parallel entries (depending on the language pair,
Table 1) and was used for Moses parameters tun-
ing.
In order to create the training set for the rerank-
ing approach, we need to have access to the rele-
vance judgements. We didn?t have access to all
relevance judgements of the previously desribed
tracks. Thus we used only a subset of the previ-
ously extracted parallel set, which includes CLEF
2000-2008 topics from the AdHoc-main, AdHoc-
TEL and GeoCLEF tracks.
The number of queries obtained altogether is
shown in (Table 1).
4.1.2 Baseline
We tested our approaches on the CLEF AdHoc-
TEL 2009 task (50 topics). This task dealt
with monolingual and cross-lingual search in a
library catalog. The monolingual retrieval is
3This alignment can be either produced by a toolkit like
GIZA++(Och and Ney, 2003) or obtained directly by a sys-
tem that produced the Nbest list of the translations (Moses).
113
Language pair Number of queries
Total queries
En - Fr, Fr - En 470
En - De, De - En 714
Annotated queries
En - Fr, Fr - En 400
En - De, De - En 350
Table 1: Top: total number of parallel queries gathered
from all the CLEF tasks (size of the tuning set). Bot-
tom: number of queries extracted from the tasks for
which the human relevance judgements were availble
(size of the reranking training set).
performed with the lemur4 toolkit (Ogilvie and
Callan, 2001). The preprocessing includes lem-
matisation (with the Xerox Incremental Parser-
XIP (A??t-Mokhtar et al 2002)) and filtering out
the function words (based on XIP PoS tagging).
Table 2 shows the performance of the monolin-
gual retrieval model for each collection. The
monolingual retrieval results are comparable to
the CLEF AdHoc-TEL 2009 participants (Ferro
and Peters, 2009). Let us note here that it is not
the case for our CLIR results since we didn?t ex-
ploit the fact that each of the collections could ac-
tually contain the entries in a language other than
the official language of the collection.
The cross-lingual retrieval is performed as fol-
lows :
? the input query (eg. in English) is first trans-
lated into the language of the collection (eg.
German);
? this translation is used to search the target
collection (eg. Austrian National Library for
German ) .
The baseline translation is produced with
Moses trained on Europarl. Table 2 reports the
baseline performance both in terms of MT evalu-
ation metrics (BLEU) and Information Retrieval
evaluation metric MAP (Mean Average Preci-
sion).
The 1best MAP score corresponds to the case
when the single translation is proposed for the
retrieval by the query translation model. 5best
MAP score corresponds to the case when the 5
top translations proposed by the translation ser-
vice are concatenated and used for the retrieval.
4http://www.lemurproject.org/
The 5best retrieval can be seen as a sort of query
expansion, without accessing the document col-
lection or any external resources.
Given that the query length is shorter than for a
standard sentence, the 4-gramm BLEU (used for
standart MT evaluation) might not be able to cap-
ture the difference between the translations (eg.
English-German 4-gramm BLEU is equal to 0 for
our task). For that reason we report both 3- and
4-gramm BLEU scores.
Note, that the French-English baseline retrieval
quality is much better than the German-English.
This is probably due to the fact that our German-
English translation system doesn?t use any de-
coumpounding, which results into many non-
translated words.
4.2 Results
We performed the query-genre adaptation ex-
periments for English-French, French-English,
German-English and English-German language
pairs.
Ideally, we would have liked to combine the
two approaches we proposed: use the query-
genre-tuned model to produce the Nbest list
which is then reranked to optimize the MAP
score. However, it was not possible in our exper-
imental settings due to the small amount of train-
ing data available. We thus simply compare these
two approaches to a baseline approach and com-
ment on their respective performance.
4.2.1 Query-genre tuning approach
For the CLEF-tuning experiments we used the
same translation model and language model as for
the baseline (Europarl-based). The weights were
then tuned on the CLEF topics described in sec-
tion 4.1.1. We then tested the system obtained on
50 parallel queries from the CLEF AdHoc-TEL
2009 task.
Table 3 describes the results of the evalua-
tion. We observe consistent 1-best MAP improve-
ments, but unstable BLEU (3-gramm) (improve-
ments for English-German, and degradation for
other language pairs), although one would have
expected BLEU to be improved in this experi-
mental setting given that BLEU was the objective
function for MERT. These results, on one side,
confirm the remark of (Kettunen, 2009) that there
is a correlation (although low) between BLEU
and MAP scores. The unstable BLEU scores
114
MAP
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Monolingual IR Bilingual IR
English 0.3159
French-English 0.1828 0.2186 0.1199 0.1568
German-English 0.0941 0.0942 0.2351 0.2923
French 0.2386 English-French 0.1504 0.1543 0.2863 0.3423
German 0.2162 English-German 0.1009 0.1157 0.0000 0.1218
Table 2: Baseline MAP scores for monolingual and bilingual CLEF AdHoc TEL 2009 task.
MAP MAP BLEU BLEU
1-best 5-best 4-gramm 3-gramm
Fr-En 0.1954 0.2229 0.1062 0.1489
De-En 0.1018 0.1078 0.2240 0.2486
En-Fr 0.1611 0.1516 0.2072 0.2908
En-De 0.1062 0.1132 0.0000 0.1924
Table 3: BLEU and MAP performance on CLEF AdHoc TEL 2009 task for the genre-tuned model.
might also be explained by the small size of the
test set (compared to a standard test set of 1000
full-sentences).
Secondly, we looked at the weights of the fea-
tures both in the baseline model (Europarl-tuned)
and in the adapted model (CLEF-tuned), shown in
Table 4. We are unsure how suitable the sizes of
the CLEF tuning sets are, especially for the pairs
involving English and French. Nevertheless we
do observe and comment on some patterns.
For the pairs involving English and German
the distortion weight is much higher when tuning
with CLEF data compared to tuning with Europarl
data. The picture is reversed when looking at the
two pairs involving English and French. This is
to be expected if we interpret a high distortion
weight as follows: ?it is not encouraged to place
source words that are near to each other far away
from each other in the translation?. Indeed, the lo-
cal reorderings are much more frequent between
English and French (e.g. white house = maison
blanche), while the long-distance reorderings are
more typcal between English and German.
The word penalty is consistenly higher over all
pairs when tuning with CLEF data compared to
tuning with Europarl data. We could see an ex-
planation for this pattern in the smaller size of
the CLEF sentences if we interpret higher word
penalty as a preference for shorter translations.
This can be explained both with the smaller aver-
age size of the queries and with the specific query
structure: mostly content words and fewer func-
tion words when compared to the full sentence.
The language model weight is consistently
though not drastically smaller when tuning with
CLEF data. We suppose that this is due to the
fact that a Europarl-base language model is not
the best choice for translating query data.
4.2.2 Reranking approach
The reranking experiments include different
features combinations. First, we experiment with
the Moses features only in order to make this ap-
proach comparable with the first one. Secondly,
we compare different syntax-based features com-
binations, as described in section 3.2.2. Thus, we
compare the following reranking models (defined
by the feature set): moses, lex (lexical coupling
+ moses features), lab (label-specific coupling +
moses features), posmaplex (lexical PoS mapping
+ moses features ), lab-lex (label-specific cou-
pling + lexical coupling + moses features), lab-
lex-posmap (label-specific coupling + lexical cou-
pling features + generic PoS mapping). To reduce
the size of feature-functions vectors we take only
the 20 most frequent features in the training data
for Label-specific coupling and PoS mapping fea-
tures. The computation of the syntax features is
based on the rule-based XIP parser, where some
heuristics specific to query processing have been
integrated into English and French (but not Ger-
man) grammars (Brun et al 2012).
The results of these experiments are illustrated
115
Lng pair Tune set DW LM ?(f |e) lex(f |e) ?(e|f) lex(e|f) PP WP
Fr-En
Europarl 0.0801 0.1397 0.0431 0.0625 0.1463 0.0638 -0.0670 -0.3975
CLEF 0.0015 0.0795 -0.0046 0.0348 0.1977 0.0208 -0.2904 0.3707
De-En
Europarl 0.0588 0.1341 0.0380 0.0181 0.1382 0.0398 -0.0904 -0.4822
CLEF 0.3568 0.1151 0.1168 0.0549 0.0932 0.0805 0.0391 -0.1434
En-Fr
Europarl 0.0789 0.1373 0.0002 0.0766 0.1798 0.0293 -0.0978 -0.4002
CLEF 0.0322 0.1251 0.0350 0.1023 0.0534 0.0365 -0.3182 -0.2972
En-De
Europarl 0.0584 0.1396 0.0092 0.0821 0.1823 0.0437 -0.1613 -0.3233
CLEF 0.3451 0.1001 0.0248 0.0872 0.2629 0.0153 -0.0431 0.1214
Table 4: Feature weights for the query-genre tuned model. Abbreviations: DW - distortion weight, LM - language
model weight, PP - phrase penalty, WP - word penalty, ?-phrase translation probability, lex-lexical weighting.
Query Example MAP bleu1
Src1 Weibliche Ma?rtyrer
Ref Female Martyrs
T1 female martyrs 0.07 1
T2 Women martyr 0.4 0
Src 2 Genmanipulation am
Menschen
Ref Human Gene Manipula-
tion
T1 On the genetic manipula-
tion of people
0.044 0.167
T2 genetic manipulation of
the human being
0.069 0.286
Src 3 Arbeitsrecht in der Eu-
ropa?ischen Union
Ref European Union Labour
Laws
T1 Labour law in the Euro-
pean Union
0.015 0.5
T2 labour legislation in the
European Union
0.036 0.5
Table 5: Some examples of queries translations (T1:
baseline, T2: after reranking with lab-lex), MAP and
1-gramm BLEU scores for German-English.
in Figure 1. To keep the figure more readable,
we report only on 3-gramm BLEU scores. When
computing the 5best MAP score, the order in the
Nbest list is defined by a corresponding reranking
model. Each reranking model is illustrated by a
single horizontal red bar. We compare the rerank-
ing results to the baseline model (vertical line) and
also to the results of the first approach (yellow bar
labelled MERT:moses) on the same figure.
First, we remark that the adapted models
(query-genre tuning and reranking) outperform
the baseline in terms of MAP (1best and 5 best)
for French-English and German-English transla-
tions for most of the models. The only exception
is posmaplex model (based on PoS tagging) for
German which can be explained by the fact that
the German grammar used for query processing
was not adapted for queries as opposed to English
and French grammars. However, we do not ob-
serve the same tendency for BLEU score, where
only a few of the adapted models outperform the
baseline, which confirms the hypothesis of the
low correlation between BLEU and MAP scores
in these settings. Table 5 gives some examples of
the queries translations before (T1) and after (T2)
reranking. These examples also illustrate differ-
ent types of disagreement between MAP and 1-
gramm BLEU5 score.
The results for English-German and English-
French look more confusing. This can be partly
due to the more rich morphology of the target lan-
guages which may create more noise in the syn-
tax structure. Reranking however improves over
the 1-best MAP baseline for English-German, and
5-best MAP is also improved excluding the mod-
els involving PoS tagging for German (posmap,
posmaplex, lab-lex-posmap). The results for
English-French are more difficult to interpret. To
find out the reason of such a behavior, we looked
at the translations. We observed the following to-
kenization problem for French: the apostrophe is
systematically separated, e.g. ?d ? aujourd ? hui?.
This leads to both noisy pre-retrieval preprocess-
ing (eg. d is tagged as a NOUN) and noisy syntax-
based feature values, which might explain the un-
stable results.
Finally, we can see that the syntax-based fea-
tures can be beneficial for the final retrieval qual-
ity: the models with syntax features can outper-
form the model basd on the moses features only.
The syntax-based features leading to the most sta-
5The higher order BLEU scores are equal to 0 for most
of the individual translations.
116
Figure 1: Reranking results. The vertical line corresponds to the baseline scores. The lowest bar (MERT:moses,
in yellow): the results of the tuning approach, other bars(in red): the results of the reranking approach.
ble results seem to be lab-lex (combination of lex-
ical and label-specific coupling): it leads to the
best gains over 1-best and 5-best MAP for all lan-
guage pairs excluding English-French. This is a
surprising result given the fact that the underlying
IR model doesn?t take syntax into account in any
way. In our opinion, this is probably due to the
interaction between the pre-retrieval preprocess-
ing (lemmatisation, PoS tagging) done with the
linguistic tools which might produce noisy results
when applied to the SMT outputs. The rerank-
ing with syntax-based features allows to choose
a better-formed query for which the PoS tagging
and lemmatisation tools produce less noise which
leads to a better retrieval.
5 Conclusion
In this work we proposed two methods for query-
genre adaptation of an SMT model: the first
method addressing the translation quality aspect
and the second one the retrieval precision aspect.
We have shown that CLIR performance in terms
of MAP is improved between 1-2.5 points. We
believe that the combination of these two meth-
ods would be the most beneficial setting, although
we were not able to prove this experimentally
(due to the lack of training data). None of these
methods require access to the document collec-
tion at test time, and can be used in the context
of a query translation service. The combination
of our adapted SMT model with other state-of-the
art CLIR techniques (eg. query expansion with
PRF) will be explored in future work.
Acknowledgements
This research was supported by the European
Union?s ICT Policy Support Programme as part of
the Competitiveness and Innovation Framework
Programme, CIP ICT-PSP under grant agreement
nr 250430 (Project GALATEAS).
References
Salah A??t-Mokhtar, Jean-Pierre Chanod, and Claude
Roux. 2002. Robustness beyond shallowness: in-
117
cremental deep parsing. Natural Language Engi-
neering, 8:121?144, June.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
an automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65?72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Adam Berger, John Lafferty, and John La Erty. 1999.
The weaver system for document retrieval. In In
Proceedings of the Eighth Text REtrieval Confer-
ence (TREC-8, pages 163?174.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 182?189. Association for Computa-
tional Linguistics.
Caroline Brun, Vassilina Nikoulina, and Nikolaos La-
gos. 2012. Linguistically-adapted structural query
annotation for digital libraries in the social sciences.
In Proceedings of the 6th EACL Workshop on Lan-
guage Technology for Cultural Heritage, Social Sci-
ences, and Humanities, Avignon, France, April.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 224?233. Association
for Computational Linguistics.
Ste?phane Clinchant and Jean-Michel Renders. 2007.
Query translation through dictionary adaptation. In
CLEF?07, pages 182?187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ?04:
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics.
Michael Collins. 2001. Ranking algorithms for
named-entity extraction: boosting and the voted
perceptron. In ACL?02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 489?496, Philadelphia, Pennsyl-
vania. Association for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
George Doddington. 2002. Automatic evaluation
of Machine Translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, pages 138?145, San Diego,
California. Morgan Kaufmann Publishers Inc.
Nicola Ferro and Carol Peters. 2009. CLEF 2009
ad hoc track overview: TEL and persian tasks.
In Working Notes for the CLEF 2009 Workshop,
Corfu, Greece.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, and
Takehito Utsuro. 2009. Evaluating effects of ma-
chine translation accuracy on cross-lingual patent
retrieval. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, SIGIR ?09, pages
674?675.
Jianfeng Gao, Jian-Yun Nie, and Ming Zhou. 2006.
Statistical query translation models for cross-
language information retrieval. 5:323?359, Decem-
ber.
Wei Gao, Cheng Niu, Jian-Yun Nie, Ming Zhou, Kam-
Fai Wong, and Hsiao-Wuen Hon. 2010. Exploit-
ing query logs for cross-lingual query suggestions.
ACM Trans. Inf. Syst., 28(2).
Djoerd Hiemstra and Franciska de Jong. 1999. Dis-
ambiguation strategies for cross-language informa-
tion retrieval. In Proceedings of the Third European
Conference on Research and Advanced Technology
for Digital Libraries, pages 274?293.
Rong Hu, Weizhu Chen, Peng Bai, Yansheng Lu,
Zheng Chen, and Qiang Yang. 2008. Web query
translation via web log mining. In Proceedings of
the 31st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?08, pages 749?750. ACM.
Amir Hossein Jadidinejad and Fariborz Mahmoudi.
2009. Cross-language information retrieval us-
ing meta-language index construction and structural
queries. In Proceedings of the 10th cross-language
evaluation forum conference on Multilingual in-
formation access evaluation: text retrieval experi-
ments, CLEF?09, pages 70?77, Berlin, Heidelberg.
Springer-Verlag.
Gareth Jones, Sakai Tetsuya, Nigel Collier, Akira Ku-
mano, and Kazuo Sumita. 1999. Exploring the
use of machine translation resources for english-
japanese cross-language information retrieval. In In
Proceedings of MT Summit VII Workshop on Ma-
chine Translation for Cross Language Information
Retrieval, pages 181?188.
Kimmo Kettunen. 2009. Choosing the best mt pro-
grams for clir purposes ? can mt metrics be help-
ful? In Proceedings of the 31th European Confer-
ence on IR Research on Advances in Information
Retrieval, ECIR ?09, pages 706?712, Berlin, Hei-
delberg. Springer-Verlag.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT
118
?07, pages 224?227. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In ACL ?07: Proceedings of the 45th
Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, pages 177?180. As-
sociation for Computational Linguistics.
Philip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Wessel Kraaij, Jian-Yun Nie, and Michel Simard.
2003. Embedding web-based statistical trans-
lation models in cross-language information re-
trieval. Computational Linguistiques, 29:381?419,
September.
Jian-yun Nie and Jiang Chen. 2002. Exploiting the
web as parallel corpora for cross-language informa-
tion retrieval. Web Intelligence, pages 218?239.
Jian-Yun Nie. 2010. Cross-Language Information Re-
trieval. Morgan & Claypool Publishers.
Vassilina Nikoulina and Marc Dymetman. 2008. Ex-
periments in discriminating phrase-based transla-
tions on the basis of syntactic coupling features. In
Proceedings of the ACL-08: HLT Second Workshop
on Syntax and Structure in Statistical Translation
(SSST-2), pages 55?60. Association for Computa-
tional Linguistics, June.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2003.
Syntax for Statistical Machine Translation: Final
report of John Hopkins 2003 Summer Workshop.
Technical report, John Hopkins University.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Paul Ogilvie and James P. Callan. 2001. Experiments
using the lemur toolkit. In TREC.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation.
Pavel Pecina, Antonio Toral, Andy Way, Vassilis Pa-
pavassiliou, Prokopis Prokopidis, and Maria Gi-
agkou. 2011. Towards using web-crawled data for
domain adaptation in statistical machine translation.
In Proceedings of the 15th Annual Conference of
the European Associtation for Machine Translation,
pages 297?304, Leuven, Belgium. European Asso-
ciation for Machine Translation.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?04), July.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Confer-
ence of the EAMT, page 28?35, Barcelona, Spain.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin train-
ing for statistical machine translation. In Proceed-
ings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 764?773, Prague, Czech Republic.
Association for Computational Linguistics.
Dan Wu and Daqing He. 2010. A study of query
translation using google machine translation sys-
tem. Computational Intelligence and Software En-
gineering (CiSE).
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics (Col-
ing2008), pages 993?100.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Pro-
ceedings of the 20th international conference on
Computational Linguistics, COLING ?04. Associ-
ation for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and
Hao Yu. 2010. Domain adaptation for statisti-
cal machine translation in development corpus se-
lection. In Universal Communication Symposium
(IUCS), 2010 4th International, pages 2?7. IEEE.
119
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 87?91,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Query log analysis with GALATEAS LangLog
Marco Trevisan and Luca Dini
CELI
trevisan@celi.it
dini@celi.it
Eduard Barbu
Universita` di Trento
eduard.barbu@unitn.it
Igor Barsanti
Gonetwork
i.barsanti@gonetwork.it
Nikolaos Lagos
Xerox Research Centre Europe
Nikolaos.Lagos@xrce.xerox.com
Fre?de?rique Segond and Mathieu Rhulmann
Objet Direct
fsegond@objetdirect.com
mruhlmann@objetdirect.com
Ed Vald
Bridgeman Art Library
ed.vald@bridgemanart.co.uk
Abstract
This article describes GALATEAS
LangLog, a system performing Search Log
Analysis. LangLog illustrates how NLP
technologies can be a powerful support
tool for market research even when the
source of information is a collection of
queries each one consisting of few words.
We push the standard Search Log Analysis
forward taking into account the semantics
of the queries. The main innovation of
LangLog is the implementation of two
highly customizable components that
cluster and classify the queries in the log.
1 Introduction
Transaction logs become increasingly important
for studying the user interaction with systems
likeWeb Searching Engines, Digital Libraries, In-
tranet Servers and others (Jansen, 2006). Var-
ious service providers keep log files recording
the user interaction with the searching engines.
Transaction logs are useful to understand the user
search strategy but also to improve query sugges-
tions (Wen and Zhang, 2003) and to enhance
the retrieval quality of search engines (Joachims,
2002). The process of analyzing the transaction
logs to understand the user behaviour and to as-
sess the system performance is known as Transac-
tion Log Analysis (TLA). Transaction Log Anal-
ysis is concerned with the analysis of both brows-
ing and searching activity inside a website. The
analysis of transaction logs that focuses on search
activity only is known as Search Log Analysis
(SLA). According to Jansen (2008) both TLA
and SLA have three stages: data collection, data
preparation and data analysis. In the data collec-
tion stage one collects data describing the user
interaction with the system. Data preparation is
the process of loading the collected data in a re-
lational database. The data loaded in the database
gives a transaction log representation independent
of the particular log syntax. In the final stage
the data prepared at the previous step is analyzed.
One may notice that the traditional three levels
log analyses give a syntactic view of the infor-
mation in the logs. Counting terms, measuring
the logical complexity of queries or the simple
procedures that associate queries with the ses-
sions in no way accesses the semantics of queries.
LangLog system addreses the semantic problem
performing clustering and classification for real
query logs. Clustering the queries in the logs al-
lows the identification of meaningful groups of
queries. Classifying the queries according to a
relevant list of categories permits the assessment
of how well the searching engine meets the user
needs. In addition the LangLog system address
problems like automatic language identification,
Name Entity Recognition, and automatic query
translation. The rest of the paper is organized
as follows: the next section briefly reviews some
systems performing SLA. Then we present the
data sources the architecture and the analysis pro-
cess of the LangLog system. The conclusion sec-
tion concludes the article summarizing the work
and presenting some new possible enhancements
of the LangLog.
87
2 Related work
The information in the log files is useful in many
ways, but its extraction raises many challenges
and issues. Facca and Lanzi (2005) offer a sur-
vey of the topic. There are several commercial
systems to extract and analyze this information,
such as Adobe web analytics1, SAS Web Analyt-
ics2, Infor Epiphany3, IBM SPSS4. These prod-
ucts are often part of a customer relation manage-
ment (CRM) system. None of those showcases
include any form of linguistic processing. On the
other hand, Web queries have been the subject
of linguistic analysis, to improve the performance
of information retrieval systems. For example, a
study (Monz and de Rijke, 2002) experimented
with shallow morphological analysis, another (Li
et al 2006) analyzed queries to remove spelling
mistakes. These works encourage our belief that
linguistic analysis could be beneficial for Web log
analysis systems.
3 Data sources
LangLog requires the following information from
the Web logs: the time of the interaction, the
query, click-through information and possibly
more. LangLog processes log files which con-
form to the W3C extended log format. No other
formats are supported. The system prototype is
based on query logs spanning one month of inter-
actions recorded at the Bridgeman Art Library5.
Bridgeman Art library contains a large repository
of images coming from 8000 collections and rep-
resenting more than 29.000 artists.
4 Analyses
LangLog organizes the search log data into units
called queries and hits. In a typical search-
ing scenario a user submits a query to the con-
tent provider?s site-searching engine and clicks
on some (or none) of the search results. From
now on we will refer to a clicked item as a hit,
and we will refer to the text typed by the user as
the query. This information alone is valuable to
the content provider because it allows to discover
1http://www.omniture.com/en/products/analytics
2http://www.sas.com/solutions/webanalytics/index.html
3http://www.infor.com
4http://www-01.ibm.com/software/analytics/spss/
5http://www.bridgemanart.com
which queries were served with results that satis-
fied the user, and which queries were not.
LangLog extracts queries and hits from the log
files, and performs the following analyses on the
queries:
? language identification
? tokenization and lemmatization
? named entity recognition
? classification
? cluster analysis
Language information may help the content
provider decide whether to translate the content
into new languages.
Lemmatization is especially important in lan-
guages like German and Italian that have a rich
morphology. Frequency statistics of keywords
help understand what users want, but they are bi-
ased towards items associated with words with
lesser ortographic and morpho-syntactic varia-
tion. For example, two thousand queries for
?trousers?, one thousand queries for ?handbag?
and another thousand queries for ?handbags?
means that handbags are twice as popular as
trousers, although statistics based on raw words
would say otherwise.
Named entities extraction helps the content
provider for the same reasons lemmatization does.
Named entities are especially important because
they identify real-world items that the content
provider can relate to, while lemmas less often do
so. The name entities and the most important con-
cepts can be linked afterwards with resources like
Wikipedia which offer a rich specification of their
properties.
Both classification and clustering allow the
content provider to understand what kind of the
users look for and how this information is targeted
by means of queries.
Classification consists of classifying queries
into categories drawn from a classification
schema. When the schema used to classify
is different from the schema used in the con-
tent provider?s website, classification may provide
hints as to what kind of queries are not matched
by items in the website. In a similar way, cluster
analysis can be used to identify new market seg-
ments or new trends in the user?s behaviour. Clus-
88
ter analysis provide more flexybility than classifi-
cation, but the information it produces is less pre-
cise. Many trials and errors may be necessary be-
fore finding interesting results. One hopes that the
final clustering solution will give insights into the
patterns of users? searches. For example an on-
line book store may discover that one cluster con-
tains many software-related terms, altough none
of those terms is popular enough to be noticeable
in the statistics.
5 Architecture
LangLog consists of three subsystems: log ac-
quisition, log analysis, log disclosure. Periodi-
cally the log acquisition subsystem gathers new
data which it passes to the log analyses compo-
nent. The results of the analyses are then available
through the log disclosure subsystem.
Log acquisition deals with the acquisition and
normalization and anonymization of the data con-
tained in the content provider?s log files. The
data flows from the content provider?s servers to
LangLog?s central database. This process is car-
ried out by a series of Pentaho Data Integration6
procedures.
Log analysis deals with the anaysis of the data.
The analyses proper are executed by NLP systems
provided by third parties and accessible as Web
services. LangLog uses NLP Web services for
language identification, morpho-syntactic analy-
sis, named entity recognition, classification and
clustering. The analyses are stored in the database
along with the original data.
Log disclosure is actually a collection of inde-
pendent systems that allow the content providers
to access their information and the analyses. Log
disclosure systems are also concerned with access
control and protection of privacy. The content
provider can access the output of LangLog using
AWStats, QlikView, or JPivot.
? AWStats7 is a widely used log analysis sys-
tem for websites. The logs gathered from the
websites are parsed by AWStats, which gen-
erates a complete report about visitors, vis-
its duration, visitor?s countries and other data
to disclose useful information about the visi-
tor?s behavior.
6http://kettle.pentaho.com
7http://awstats.sourceforge.net
? QlikView8 is a business intelligence (BI)
platform. A BI platform provides histori-
cal, current, and predictive views of busi-
ness operations. Usually such tools are used
by companies to have a clear view of their
business over time. In LangLog, QlickView
does not display sales or costs evolution over
time. Instead, it displays queries on the con-
tent provider?s website over time. A dash-
board with many elements (input selections,
tables, charts, etc.) provides a wide range of
tools to visualize the data.
? JPivot9 is a front-end for Mondrian. Mon-
drian10 is an Online Analytical Processing
(OLAP) engine, a system capable of han-
dling and analyzing large quantities of data.
JPivot allows the user to explore the output
of LangLog, by slicing the data along many
dimensions. JPivot allows the user to display
charts, export results to Microsoft Excel or
CSV, and use custom OLAP MDX queries.
Log analysis deals with the anaysis of the data.
The analyses proper are executed by NLP systems
provided by third parties and accessible as Web
services. LangLog uses NLP Web services for
language identification, morpho-syntactic analy-
sis, named entity recognition, classification and
clustering. The analyses are stored in the database
along with the original data.
5.1 Language Identification
The system uses a language identification sys-
tem (Bosca and Dini, 2010) which offers language
identification for English, French, Italian, Span-
ish, Polish and German. The system uses four
different strategies:
? N-gram character models: uses the distance
between the character based models of the
input and of a reference corpus for the lan-
guage (Wikipedia).
? Word frequency: looks up the frequency of
the words in the query with respect to a ref-
erence corpus for the language.
? Function words: searches for particles
highly connoting a specific language (such
as prepositions, conjunctions).
8http://www.qlikview.com
9http://jpivot.sourceforge.net
10http://mondrian.pentaho.com
89
? Prior knowledge: provides a default guess
based on a set of hypothesis and heuristics
like region/browser language.
5.2 Lemmatization
To perform lemmatization, Langlog uses general-
purpose morpho-syntactic analysers based on the
Xerox Incremental Parser (XIP), a deep robust
syntactic parser (Ait-Mokhtar et al 2002). The
system has been adapted with domain-specific
part of speech disambiguation grammar rules, ac-
cording to the results a linguistic study of the de-
velopment corpus.
5.3 Named entity recognition
LangLog uses the Xerox named entity recogni-
tion web service (Brun and Ehrmann, 2009) for
English and French. XIP includes also a named
entity detection component, based on a combina-
tion of lexical information and hand-crafted con-
textual rules. For example, the named entity
recognition system was adapted to handle titles
of portraits, which were frequent in our dataset.
While for other NLP tasks LangLog uses the same
system for every content provider, named entity
recognition is a task that produces better analyses
when it is tailored to the domain of the content.
Because LangLog uses a NER Web service, it is
easy to replace the default NER system with a dif-
ferent one. So if the content provider is interested
in the development of a NER system tailored for
a specific domain, LangLog can accomodate this.
5.4 Clustering
We developed two clustering systems: one per-
forms hierarchical clustering, another performs
soft clustering.
? CLUTO: the hierarchical clustering system
relies on CLUTO411, a clustering toolkit.
To understand the main ideas CLUTO is
based on one might consult Zhao and
Karypis (2002). The clustering process pro-
ceeds as follows. First, the set of queries to
be clustered is partitioned in k groups where
k is the number of desired clusters. To do
so, the system uses a partitional clustering
algorithm which finds the k-way clustering
solution making repeated bisections. Then
11http://glaros.dtc.umn.edu/gkhome/views/cluto
the system arranges the clusters in a hierar-
chy by successively merging the most similar
clusters in a tree.
? MALLET: the soft clustering system we
developed relies on MALLET (McCallum,
2002), a Latent Dirichlet Allocation (LDA)
toolkit (Steyvers and Griffiths, 2007).
Our MALLET-based system considers that
each query is a document and builds a topic
model describing the documents. The result-
ing topics are the clusters. Each query is as-
sociated with each topic according to a cer-
tain strenght. Unlike the system based on
CLUTO, this system produces soft clusters,
i.e. each query may belong to more than one
cluster.
5.5 Classification
LangLog allows the same query to be classified
many times using different classification schemas
and different classification strategies. The result
of the classification of an input query is always a
map that assigns each category a weight, where
the higher the weight, the more likely the query
belongs to the category. If NER performs bet-
ter when tailored to a specific domain, classifi-
cation is a task that is hardly useful without any
customization. We need a different classification
schema for each content provider. We developed
two classification system: an unsupervised sys-
tem and a supervised one.
? Unsupervised: this system does not require
any training data nor any domain-specific
corpus. The output weight of each category
is computed as the cosine similarity between
the vector models of the most representa-
tive Wikipedia article for the category and
the collection of Wikipedia articles most rel-
evant to the input query. Our evaluation in
the KDD-Cup 2005 dataset results in 19.14
precision and 22.22 F-measure. For com-
parison, the state of the art in the competi-
tion achieved a 46.1 F-measure. Our system
could not achieve a similar score because it
is unsupervised, and therefore it cannot make
use of the KDD-Cup training dataset. In ad-
dition, it uses only the query to perform clas-
sification, whereas KDD-Cup systems were
also able to access the result sets associated
to the queries.
90
? Supervised: this system is based on the
Weka framework. Therefore it can use any
machine learning algorithm implemented in
Weka. It uses features derived from the
queries and from Bridgeman metadata. We
trained a Naive Bayes classifier on a set of
15.000 queries annotated with 55 categories
and hits and obtained a F-measure of 0.26.
The results obtained for the classification
are encouraging but not yet at the level of
the state of the art. The main reason for
this is the use of only in-house meta-data in
the feature computation. In the future we
will improve both components by providing
them with features from large resources like
Wikipedia or exploiting the results returned
by Web Searching engines.
6 Demonstration
Our demonstration presents:
? The setting of our case study: the Bridgeman
Art Library website, a typical user search,
and what is recorded in the log file.
? The conceptual model of the results of the
analyses: search episodes, queries, lemmas,
named entities, classification, clustering.
? The data flow across the parts of the system,
from content provider?s servers to the front-
end through databases, NLP Web services
and data marts.
? The result of the analyses via QlikView.
7 Conclusion
In this paper we presented the LangLog system,
a customizable system for analyzing query logs.
The LangLog performs language identification,
lemmatization, NER, classification and clustering
for query logs. We tested the LangLog system on
queries in Bridgeman Library Art. In the future
we will test the system on query logs in differ-
ent domains (e.g. pharmaceutical, hardware and
software, etc.) thus increasing the coverage and
the significance of the results. Moreover we will
incorporate in our system the session information
which should increase the precision of both clus-
tering and classification components.
References
Salah Ait-Mokhtar, Jean-Pierre Chanod and Claude
Roux 2002. Robustness Beyond Shallowness: In-
cremental Deep Parsing. Journal of Natural Lan-
guage Engineering 8, 2-3, 121-144.
Alessio Bosca and Luca Dini. 2010. Language Identi-
fication Strategies for Cross Language Information
Retrieval. CLEF 2010 Working Notes.
C. Brun and M. Ehrmann. 2007. Adaptation of
a Named Entity Recognition System for the ES-
TER 2 Evaluation Campaign. In proceedings of
the IEEE International Conference on Natural Lan-
guage Processing and Knowledge Engineering.
F. M. Facca and P. L. Lanzi. 2005. Mining interesting
knowledge from weblogs: a survey. Data Knowl.
Eng. 53(3):225241.
Jansen, B. J. 2006. Search log analysis: What is it;
what?s been done; how to do it. Library and Infor-
mation Science Research 28(3):407-432.
Jansen, B. J. 2008. The methodology of search log
analysis. In B. J. Jansen, A. Spink and I. Taksa (eds)
Handbook of Web log analysis 100-123. Hershey,
PA: IGI.
Joachims T. 2002. Optimizing search engines us-
ing clickthrough data. In proceedings of the 8th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining 133-142.
M. Li, Y. Zhang, M. Zhu, and M. Zhou. 2006. Ex-
ploring distributional similarity based models for
query spelling correction. In proceedings of In ACL
06: the 21st International Conference on Computa-
tional Linguistics and the 44th annual meeting of
the ACL 10251032, 2006.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
C. Monz and M. de Rijke. 2002. Shallow Morpholog-
ical Analysis in Monolingual Information Retrieval
for Dutch, German and Italian. In Proceedings of
CLEF 2001. Springer
M. Steyvers and T. Griffiths. 2007. Probabilistic
Topic Models. In T. Landauer, D McNamara, S.
Dennis and W. Kintsch (eds), Handbook of Latent
Semantic Analysis, Psychology Press.
J. R. Wen and H.J. Zhang 2003. Query Clustering
in the Web Context. In Wu, Xiong and Shekhar
(eds) Information Retrieval and Clustering 195-
226. Kluwer Academic Publishers.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets.
In proceedings of the ACM Conference on Informa-
tion and Knowledge Management.
91
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Linguistically-Adapted Structural Query Annotation for Digital Li-
braries in the Social Sciences 
 Carol ine Brun Vassilina Nikoulina Nikolaos Lagos 
Xerox Research Centre Europe 
6, chemin de Maupertuis 
38240, Meylan France 
{firstname.lastname}@xrce.xerox.com 
 
Abstrac t 
Query processing is an essential part of a  
range of applications in the social sciences 
and cultural heritage domain. However, out-
of-the-box natural language processing tools 
originally developed for full phrase analysis 
are inappropriate for query analysis. In this  
paper, we propose an approach to solving 
this problem by adapting a complete and in-
tegrated chain of NLP tools, to make it  suit-
able for queries analysis. Using as a case 
study the automatic translation of queries 
posed to the Europeana library, we demon-
strate that adapted linguistic processing can 
lead to improvements in translation quality. 
1 Introduction 
Query processing tools are essential components 
of digital libraries and content aggregators. Their 
operation varies from simple stop word removal 
and stemming to advanced parsing, that treats 
queries as a collection of phrases rather than sin-
gle terms (Mothe and Tanguy, 2007). They are 
used in a range of applications, from information 
retrieval (via search engines that provide access 
to the digital collections) to query analysis.  
Current query processing solutions tend to 
use out-of-the-box Natural Language Processing 
(NLP) tools that were originally developed for 
full phrase analysis, being inappropriate for 
query analysis. 
Correct query annotation and interpretation is 
even more important in the cultural heritage or 
social sciences domain, as a lot of the content 
can be in multimedia form and only metadata 
(most of the times in the form of tags) is exploit-
able by traditional text-oriented information re-
trieval and analysis techniques. 
Furthermore, as recent studies of user query-
ing behavior mention, queries in these domains 
are not only very short but are also quite specific 
in terms of content: they refer to artist names, 
titles, dates, and objects (Koolen and Kamps, 
2010; Ireson and Oomen, 2007). Take the exam-
ple of a query like ?coupe apollon? (?bowl apol-
lon?). While in standard analysis ?coupe? would 
be identified as a verb (?couper?, i.e. ?to cut?), in 
the context of a query it should be actually 
tagged as a noun, which refers to an object. Such 
a difference may lead to different preprocessing 
and worse retrieval. 
In this paper, we propose an approach to solv-
ing this problem by adapting a complete and in-
tegrated chain of NLP tools, based on the Xerox 
Incremental Parser (XIP), to make it suitable for 
queries? analysis. The adaptation includes recapi-
talization, adapted Part of Speech (PoS) tagging, 
adapted chunking and Named Entities (NE) rec-
ognition. We claim that several heuristics espe-
cially important for queries? analysis, such as 
favoring nominal interpretations, result in im-
proved linguistic structures, which can have an 
impact in a wide range of further applications 
(e.g. information retrieval, query translation, in-
formation extraction, query reformulation etc.). 
2 Prior art 
The problem of adapted query processing, often 
referred to as structural query annotation, in-
cludes capitalization, NEs detection, PoS tagging 
and query segmentation. Most of the existing 
works treat each of these steps independently and 
address only one of the above issues. 
Many works address the problem of query 
segmentation. According to Tan and Peng 
(2008), query segmentation is a problem which is 
close to the chunking problem, but the chunking 
problem is directly linked to the PoS tagging re-
sults, which are often noisy for the queries. Thus, 
most of the works on query segmentation are 
based on the statistical interaction between a pair 
of query words to identify the border between the 
segments in the query (Jones et al, 2006; Guo et 
55
al., 2008). Tan and Peng (2008) propose a gen-
erative language model enriched with Wikipedia 
to identify ?concepts? rather than simply ?fre-
quency-based? patterns. The segmentation pro-
posed by Bergsma and Wang (2007) is closer to 
the notion of NP chunking. They propose a ma-
chine-learned query segmentation system trained 
on manually annotated set of 500 AOL queries. 
However, in this work PoS tagging is used as one 
of the features in query segmentation and is done 
with a generic PoS tagger, non adapted for que-
ries. 
PoS tagging is an important part of query 
processing and used in many information ana-
lytics tasks (query reformulation, query segmen-
tation, etc.). However very few works address 
query-oriented PoS tagging. Allan and Raghavan 
(2002) consider that PoS tagging might be am-
biguous for short queries and propose to interact 
with the user for disambiguation. Barr et al 
(2008) produce a set of manually annotated que-
ries, and then train a Brill tagger on this set in 
order to create an adapted PoS tagger for search 
queries. 
A notable work is the one by Bendersky et al 
(2010), which addresses the capitalization, PoS 
tagging and query segmentation in the same pa-
per. However, this approach proposes for each of 
the above steps a probabilistic model that relies 
on the document corpus rather on the query it-
self. Such an approach is not applicable for most 
digital content providers who would reluctantly 
give access to their document collection. More-
over, the query expansion, which is the central 
idea of the described approach, is not possible for 
most of digital libraries that are organized in a 
database. Secondly, Bendersky et al (2010) pro-
poses adapting each processing step independ-
ently. Although this is not mentioned in the 
paper, these three steps can be applied in a se-
quence, where PoS tagging can profit from the 
recapitalization, and chunking from the PoS tag-
ging step. However, once the recapitalization is 
done, it can not be changed in the following 
steps. This work doesn?t address the adaptation 
of the NE recognition component, as we do, and 
which might change the final chunking and PoS 
tagging in certain cases. 
In our approach, part of the recapitalization is 
done during the PoS tagging, in interaction with 
the NE recognition, which allows us to consider 
these two steps as interleaved. Moreover, the 
linguistic processing we propose is generic: cor-
pus-independent (at least most of its parts except 
for NE recognition) and doesn?t require access to 
the document collection. 
3 Data 
This work is based on search logs from Euro-
peana 1
4 Motivation 
. These are real users? queries, where 
Named Entities are often lowercased and the 
structures are very different from normal phrase 
structure. Thus, this data is well adapted to dem-
onstrate the impact of adapted linguistic process-
ing. 
We show the importance of the adapted linguistic 
query processing using as example the task of 
query translation, a real need for today?s digital 
content providers operating in a multilingual en-
vironment. We took a sample of Europeana que-
ries and translated them with different MT 
systems: in-house (purely statistical) or available 
online (rule-based). Some examples of problem-
atic translations are shown in the Table 1. 
 
 Input query Automatic 
Translation 
Human 
translation 
 French-English 
1 
 
journal pano-
rama paris  
newspaper 
panorama bets 
newspaper 
panorama 
paris   
2 saint jean de 
luz 
saint jean of 
luz 
saint jean 
de luz 
3 vie et mort 
de l?image 
life and died of 
the image 
life and 
death of 
image 
4 langue et 
r?alit? 
and the reality 
of language 
language 
and reality 
 English-French 
5 maps europe trace l?Europe cartes de 
l?Europe 
6 17th century 
saw 
Du 17?me 
si?cle a vu 
scie du 
17?me 
si?cle 
7 chopin 
george sand 
george sable 
chopin soit 
chopin 
george 
sand 
Table 1: Examples of the problematic query 
translations 
 
                                                                 
1 A portal that acts as an interface to millions of digitized 
records, allowing users to explore Europe?s cultural heri-
tage. For more information please visit 
http://www.europeana.eu/portal/ 
56
Although in general, the errors done by statis-
tical and rule-based models are pretty different, 
there are some common errors done in the case 
of the query translation. Both models, being de-
signed for full-sentence translation, find the 
query structure very unnatural and tend to repro-
duce the full sentence in the output (ex. 1, 3, 4, 5, 
6). The errors may come either from a wrong 
PoS tagging (for rule-based systems), or from the 
wrong word order (statistical-based systems), or 
from the choice of the wrong translation (both 
types of systems). 
One might think that the word order problem 
is not crucial for queries, because most of the IR 
models use the bag of words models, which ig-
nore the order of words. However, it might mat-
ter in some cases: for example, if and/or are 
interpreted as a logical operator, it is important to 
place them correctly in the sentence (examples3, 
4).  
Errors also may happen when translating NEs 
(ex.1, 2, 7). The case information, which is often 
missing in the real-life queries, helps to deal with 
the NEs translation. 
The examples mentioned above illustrate that 
adapted query processing is important for a task 
such as query translation, both in the case of 
rule-based and empirical models. Although the 
empirical models can be adapted if an appropri-
ately sized corpus exists, such a corpus is not 
always available.  
Thus we propose adapting the linguistic proc-
essing prior to query translation (which is further 
integrated in the SMT model). We demonstrate 
the feasibility and impact of our approach based 
on the difference in translation quality but the 
adaptations can be useful in a number of other 
tasks involving query processing (e.g. question 
answering, query logs analysis, etc.). 
5 Linguistic Processing Adaptation 
As said before, queries have specific linguistic 
properties that make their analysis difficult for 
standard NLP tools. This section describes the 
approach we have designed to improve query 
chunking. Following a study of the corpus of 
query logs, we rely on the specific linguistic 
properties of the queries to adapt different steps 
of linguistic analysis, from preprocessing to 
chunking.  
These adaptations consist in the following 
very general processes, for both English and 
French: 
Recapitalization: we recapitalize, in a preproc-
essing step, some uncapitalized words in queries 
that can be proper nouns when they start with a 
capital letter. 
Part of Speech disambiguation
? the part of speech tagging favors nominal 
interpretation (whereas standard part of 
speech taggers are designed to find a verb in 
the input, as PoS tagging generally applies 
on complete sentences); 
:  
? the recapitalization information transmitted 
from the previous step is used to change the 
PoS interpretation in some contexts. 
Chunking
? considering that a full NE is a chunk, which 
is not the case in standard text processing, 
where a NE can perfectly be just a part of a 
chunk; 
:  the chunking is improved by: 
? grouping coordinated NEs of the same type; 
? performing PP and AP attachment with the 
closest antecedent that is morphologically 
compatible 
These processes are very general and may ap-
ply to queries in different application domains, 
with maybe some domain-dependent adaptations 
(for example, NEs may change across domains). 
These adaptations have been implemented 
within the XIP engine, for the French and Eng-
lish grammars. The XIP framework allows inte-
grating the adaptations of different steps of query 
processing into a unified framework, where the 
changes from one step can influence the result of 
the next step: the information performed at a 
given step is transmitted to the next step by XIP 
through linguistic features. 
5.1 Preprocessing 
Queries are often written with misspelling errors, 
in particular for accents and capital letters of 
NEs. See the following query examples extracted 
from our query log corpus: 
 
lafont Robert (French query) 
henry de forge et jean maucl?re 
(French query) 
muse prado madrid (French query) 
carpaccio queen cornaro (English 
query) 
man ray (English query) 
 
This might be quite a problem for linguistic 
treatments, like PoS tagging and of course NE 
57
recognition, which often use capital letter infor-
mation as a triggering feature.  
Recapitalizing these words at the preprocess-
ing step of a linguistic analysis, i.e. during the 
morphological analysis, is technically relatively 
easy, however it would be an important generator 
of spurious ambiguities in the context of full sen-
tence parsing (standard context of linguistic pars-
ing). Indeed, considering that all lower case 
words that can be proper nouns with a capital 
letter should also have capitalized interpretation, 
such as price, jean, read, us, bush, lay, etc., in 
English or pierre, m?decin, ? in French) would 
be problematic for a PoS tagger as well as for a 
NE recognizer. That?s why it is not performed in 
a standard analysis context, considering also that 
misspelling errors are not frequent in ?standard? 
texts. In the case of queries however, they are 
frequent, and since queries are far shorter in av-
erage than full sentences the tagging can be 
adapted to this context (see next section), we can 
afford to perform recapitalization using the fol-
lowing methodology, combining lexical informa-
tion and contextual rules: 
1. The preprocessing lexicon integrates all 
words starting with a lower case letter 
which can be first name (henry, jean, 
isaac ?), family and celebrity name 
(chirac, picasso...) and  place names 
(paris, saint p?tersbourg, ?) when capi-
talized. 
2. When an unknown word starting with a 
lower case letter is preceded by a first 
name and eventually by a particle (de, 
van, von ?), it is analyzed as a last 
name, in order to be able to trigger stan-
dard NE recognition. This is one exam-
ple of interleaving of the processes: here 
part-of-speech interpretation is condi-
tioned by the recapitalization steps which 
transmits information about recapitaliza-
tion (via features within XIP) that trig-
gers query-specific pos disambiguation 
rules. 
The recapitalization (1) has been imple-
mented within the preprocessing components of 
XIP within finite state transducers (see (Kart-
tunen, 2000)). The second point (2) is done di-
rectly within XIP in the part-of-speech tagging 
process, with a contextual rule.  For example, the 
analysis of the input query ?jean maucl?re? gets 
the following structure and dependency output 
with the standard French grammar. 
 
Query: jean maucl?re 
NMOD(jean, maucl?re) 
0>GROUP[NP[jean] AP[maucl?re]] 
 
Because jean is a common noun and maucl?re 
is an unknown word which has been guessed as 
an adjective by the lexical guesser. 
It gets the following analysis with the pre-
processing adaptations described above: 
 
NMOD(jean,maucl?re) 
PERSON_HUM(jean maucl?re) 
FIRSTNAME(jean,jean maucl?re) 
LASTNAME(maucl?re,jean maucl?re) 
0>GROUP[NP[NOUN[jean maucl?re]]] 
 
Because jean has been recognized as a first 
name and consequently the unknown word after 
has been inferred has a proper noun (last name) 
by the pos tagging contextual rule; the recapitali-
zation process and part-of-speech interpretation 
are therefore interleaved. 
5.2 Part of speech disambiguation 
In the context of query analysis, part-of-speech 
tagging has to be adapted also, since standard 
part-of-speech disambiguation strategies aim 
generally at disambiguating in the context of full 
sentences. But queries are very different from 
full sentences: they are mostly nominal with 
sometimes infinitive, past participial, or gerun-
dive insertions, e.g.: 
 
statuettes hommes jouant avec un 
chien (French query) 
coupe apollon (French query) 
architecture musique (French 
query) 
statue haut relief grecque du 5 
siecle (French query)  
david playing harp fpr saul (Eng-
lish query) 
stained glass angel (English 
query) 
 
Standard techniques for part-of-speech tag-
ging include rule based methods and statistical 
methods, mainly based on hidden Markov mod-
els (see for example (Chanod and Tapanainen, 
1995)). In this case, it would be possible to re-
compute the probabilities on a corpus of queries 
manually annotated. However, the correction of 
part-of-speech tags in the context of queries is 
easy to develop with a small set of rules. We fo-
cus on English and French, and in queries, the 
main problems come from the ambiguity be-
58
tween noun and verbs, which has to be solved 
differently than in the context of a standard sen-
tence.   
The approach we adopt to correct the tagging 
with the main following contextual rules: 
? If there is a noun/verb ambiguity: 
? If the ambiguity is on the first word of 
the query (e.g. ?coupe apollon?, ?oil
? If the ambiguity is on the second word of 
the query, prefer the noun interpretation 
if the query starts with an adjective or a 
noun (e.g. in ?young 
 
flask?), select the noun interpretation; 
people
? Select noun interpretation if there is no 
person agreement with one of the previ-
ous nouns (e.g. ?les fr?res 
 social com-
petences?, select the noun interpretation 
for people, instead of verb) 
bissons
? For a verb which is neither at the past 
participle form nor the infinitive form, 
select the noun interpretation if it is not 
followed by a determiner (e.g. ?tremble-
ment 
?, 
fr?res belongs to the 3rd person but bis-
sons to the 1st one of the verb ?bisser?) 
terre
? Choose the noun interpretation if the 
word is followed by a conjunction and a 
noun or preceded by a noun and a con-
junction (e.g. in ?gauguin 
 lisbonne?, terre is disambigu-
ated as a noun?)) 
moon and 
earth?, choose the noun interpretation 
for moon, instead of verb2
? In case of ambiguity between adjective and 
past participle verb, select the adjective in-
terpretation if the word is followed by a 
noun (e.g.  ?stained glass angel?, stained is 
disambiguated as an adjective instead of a 
past participle verb) 
). 
5.3 Chunking 
The goal of chunking is to assign a partial 
structure to a sentence and focuses on easy to 
parse pieces in order to avoid ambiguity and re-
cursion. In the very specific context of query 
analysis, and once again since queries have spe-
cific linguistic properties (they are not sentences 
but mostly nominal sequences), chunking can be 
improved along several heuristics. We propose 
here some adaptations to improve query chunk-
                                                                 
2To moon about 
ing to deal with AP and PP attachment, and co-
ordination, using also NE information to guide 
the chunking strategy. 
AP and PP attachment 
In standard cases of chunking, AP and PP at-
tachment is not considered, because of attach-
ment ambiguity problems that cannot be solved 
at this stage of linguistic analysis.  
Considering the shortness of queries and the 
fact that they are mostly nominal, some of these 
attachments can be solved however in this con-
text. 
For the adjectival attachment in French, we 
attach the post modifier adjectival phrases to the 
first previous noun with which there is agreement 
in number and gender. For example, the chunk-
ing structure for the query ?Biblioth?que eu-
ropeenne numerique? is: 
 
NP[ [Biblioth?que AP[europeenne] 
AP[numerique] ] 
 
while it is  
 
NP[Biblioth?que] AP[europeenne] 
AP[numerique] 
 
with our standard French grammar.  
For PP attachment, we simply consider that 
the PP attaches systematically to the previous 
noun. For example, the chunking structure for 
?The history of the University of Oxford? is: 
 
NP[the history PP[of the University 
PP[of Oxford] ] ]  
 
instead of: 
 
NP[The history] PP[of the Univer-
sity] PP[of  Oxford ] 
Coordination 
Some cases of coordination, usually very com-
plex, can be solved in the query context, in par-
ticular when NEs are involved. For both English 
and French, we attach coordinates when they 
belong to the same entity type (person conj per-
son, date conj date, place conj place, etc.), for 
example, ?vase achilles et priam? :  
 
NP[vase] NP[Achille et Priam]  
 
instead of: 
 
NP[vase] NP[Achille] et NP[Priam] 
59
 
We also attach coordinates when the second 
is introduced by a reflexive pronoun, such as in: 
?[Le laboureur et ses enfants] La Fontaine? and 
attach coordinates within a PP when they are in-
troduced by the preposition ?entre? in French 
and ?between? in English. 
Use of NE information to guide the chunking 
strategy 
We also use information about NEs present in 
the queries to guide the query chunking strategy. 
In standard analysis, NEs are generally part of 
larger chunking units. In queries, however, be-
cause of their strong semantic, they can be iso-
lated as separate chunking units. We have 
adapted our chunking strategy using this infor-
mation: when the parser detects a NE (including 
a date), it chunks it as a separate NP. The follow-
ing examples show the chunking results for this 
adapted strategy versus the analysis of standard 
grammar: 
 
? ?Anglo Saxon 11th century? (English) 
Adapted chunking:  
NP[Anglo Saxon] NP[ 11th century] 
 
Standard chunking:  
NP[Anglo Saxon 11th century ] 
 
? ?Alexandre le Grand Persepolis?  (French) 
Adapted chunking:  
NP[Alexandre le Grand] NP[Perspolis] 
 
Standard chunking: 
NP[Alexandre le Grand Perspolis] 
 
The whole process is illustrated in Figure 1.  
 
When applying the full chain on an example 
query like ?gauguin moon and earth?, we have 
the following steps and result: 
Preprocessing: gauguin is recognized as Gauguin 
(proper noun of celebrity); 
Part of speech tagging:  moon is disambiguated 
as a noun instead of a verb); 
Chunking:
So we get the following structure:  
 moon and earth are grouped together 
in a coordination chunk, gauguin is a NE 
chunked separately.  
 
NP[Gauguin] NP[moon and earth] 
 
and gauguin is recognized as a person name, 
instead of  
 
SC 3 [NP[gauguin] FV 4
 
Input query 
Query 
Preprocessing 
Query POS 
Disambiguation 
Query  
Chunking 
Adapted lexical 
Resources combined 
with contextual rules 
Improved query structure 
Adapted strategy for 
POS tagging 
(Contextual rules) 
Adapted Chunking 
strategy: contextual 
rules + named entities  
[moon]] and 
NP[earth],  
 
gauguin remaining unknown,  with the standard 
English grammar. 
 
Fig 1: Linguistic processing adaptation for que-
ries 
 
5.4 Examples of query structures 
The following table shows the differences of 
query structures obtained with the standard lin-
guistic processing and with the adapted linguistic 
processing.  
 
1. Albert Camus la peste 
Standard LP: NP {albert}  AP {camus}  
NP {la peste}   
Adapted LP: NP {albert camus}  NP {la 
peste}   
2. dieux ou h?ros grec 
Standard LP: NP {dieux}  COORD {ou}  
NP {h?ros}  AP {grec}   
                                                                 
3 SC: chunk tag for sentential clause 
4 FV: finite verb chunk 
60
Adapted LP: NP {dieux}  COORD {ou}  
NP {h?ros grec}   
3. pierre berg? 
Standard LP: NP {pierre}  VERB {berg?}   
Adapted LP: NP {pierre berg?} 
Table 2:  Some examples of query structure produced 
by standard and adapted linguistic processing. 
 
The evaluation of this customization is done 
indirectly through query translation, and is de-
scribed in the next section 
6 Experiments 
6.1 Experimental settings 
In our experiments we tried to enrich our base-
line SMT system with an adapted linguistic proc-
essing in order to improve the query translation. 
These experiments have double goal. First, to 
show that the adapted linguistic processing al-
lows to improve query translation compared to a 
standard linguistic processing, and second, to 
show that enriching an SMT model with a lin-
guistic processing (adapted) is helpful for the 
translation.  
We use an open source toolkit Moses (trained 
on Europarl) as a baseline model for query trans-
lations. Based on the examples from the section 
5, we choose to integrate the chunking and NE 
information in the translation. We integrate this 
knowledge in the following way: 
? Chunking: We check whether the query 
matches one of the following patterns: ?NP1 
and NP2?, ?NP1 or NP2?, ?NP1 NP2?, 
?NP1, NP2?, etc. If it is the case, the NPs are 
translated independently.  Thus, we make 
sure that the output query will preserve the 
logical structure, if ?and/or? are treated as 
logical operators. Also, translating NPs inde-
pendently might result at different (hopefully 
better) lexical choices.  
? Named entities: We introduce XML tags for 
person names where we propose a possible 
translation. During the translation process the 
proposed translation competes with the pos-
sible translations from a bi-phrase library. 
The translation maximizing internal transla-
tion score is chosen. In these experiments we 
propose not to translate an NE at all, how-
ever in more general case we could imagine 
having an adapted NE dictionary.  
6.2 Evaluation 
We have translated the totality of available 
Europeana French logs to English (8870 distinct 
queries), with the following translation models:  
? Moses trained on Europarl (Baseline 
MT) 
? Baseline MT model enriched with lin-
guistic processing (as defined in 6.1) 
based on basic grammar (Baseline MT + 
basic grammar) 
? Baseline MT enriched with linguistic 
processing based on adapted grammar 
(Baseline MT + adapted grammar) 
Our approach brings two new aspects com-
pared to simple SMT system. First, an SMT sys-
tem is enriched with linguistic processing as 
opposed to system without linguistic processing 
(baseline system), second: usage of an adapted 
linguistic processing as opposed to standard lin-
guistic processing. Thus, we evaluate: 
1. The impact of linguistic processing on 
the final query translations; 
2. The impact of grammar adaptation 
(adapted linguistic processing) in the 
context of query translation. 
First, we measure the overall impact of each 
of the two aspects mentioned above. Table 3 re-
flects the general impact of linguistic enrichment 
and grammar adaptation on query structure and 
translation. 
First, we note that the linguistic processing as 
defined in 6.1 won?t be applied to all queries. 
Thus, we count an amount of queries out of our 
test set to which this processing can actually be 
applied. This corresponds to the first line of the 
Table 3 (26% of all queries). 
Second, we compare the queries translation 
with and without linguistic processing. This is 
shown in the second line of the Table 3: the 
amount of queries for which the linguistic proc-
essing lead to different translation (25% of que-
ries for which the linguistic processing was 
applied). 
The second part of the table shows the differ-
ence between the standard linguistic processing 
and an adapted linguistic processing. First, we 
check how many queries get different structure 
after grammar adaptation (Section 5) (~42%) and 
second, we check how many of these queries 
61
actually get different translation (~16% queries 
with new structure obtained after adaptation get 
different translations).  
These numbers show that the linguistic 
knowledge that we integrated into the SMT 
framework may impact a limited portion of que-
ries? translations. However, we believe that this 
is due, to some extent, to the way the linguistic 
knowledge was integrated in SMT, which ex-
plores only a small portion of the actual linguis-
tic information that is available. We carried out 
these experiments as a proof of concept for the 
adapted linguistic processing, but we believe that 
a deeper integration of the linguistic knowledge 
into the SMT framework will lead to more sig-
nificant results. For example, integrating such an 
adapted linguistic processing in a rule-based MT 
system will be straightforward and beneficial, 
since the linguistic information is explored di-
rectly by a translation model (e.g. in the example 
6 in Table 1 tagging "saw" as a noun will defi-
nitely lead to a better translation). 
Next, we define 2 evaluation tasks, where the 
goal of each task is to compare 2 translation 
models. We compare:  
1. Baseline MT versus linguistically en-
riched translation model (Baseline MT+adapted 
adapted linguistic processing). This task evalu-
ates the impact of linguistic enrichment in the 
query translation task with SMT.  
2. Translation model using standard lin-
guistic processing versus translation model using 
adapted linguistic processing. This task evalu-
ates the impact of the adapted linguistic process-
ing in the query translation task.  
For each evaluation task we have randomly 
selected a sample of 200 translations (excluding 
previously the identical translations for the 2 
models compared) and we perform a pairwise 
evaluation for each evaluation task. Thus, for the 
first evaluation task, a baseline translation (per-
formed by standard Moses without linguistic 
processing) is compared to the translation done 
by Moses + adapted linguistic processing. In the 
second evaluation task, the translation performed 
by Moses + standard linguistic processing is 
compared to the translation performed by Moses 
+ adapted linguistic processing. 
The evaluation has been performed by 3 
evaluators. However, no overlapping evaluations 
have been performed to calculate intra-evaluators 
agreement. We could observe, however, the simi-
lar tendency for improvement in each on the 
evaluated sample (similar to the one shown in the 
Table 2).  
We evaluate the overall translation perform-
ance, independently of the task in which the 
translations are going to be used afterwards (text  
Table 3: Impact of linguistic processing and 
grammar adaptation for query translation 
 
understanding, text analytics, cross-lingual in-
formation retrieval etc.) 
The difference between slight improvements 
and important improvements as in the examples 
below has been done during the evaluation. 
 
src1: max weber 
t1:max mr weber 
t2:max weber (slight improvement)
   
src2: albert camus la peste 
t1:albert camus fever 
t2:albert camus the plague (impor-
tant improvement) 
 
Thus, each pair of translations (t1, t2) re-
ceives a score from the scale [-2, 2] which can 
be: 
? 2, if t2 is much better than t1,  
? 1, if t2 is better than t1,  
? 0, if t2 is equivalent to t1,  
? -1, if t1 is better than t2,  
? -2, if t1 is much better than t2,  
Linguistic enrichment 
Nb of  queries to which the adapted 
linguistic processing was applied 
before translation.  
2311 
(26% of 
8870) 
Nb of translations which differ 
between baseline Moses and Moses 
with adapted linguistic processing.  
582  
(25% of 
2311) 
Grammar adaptation 
Nb of queries which get different 
structures between standard linguistic 
processing and adapted linguistic 
processing. 
3756  
(42% of 
8870) 
Nb of translations which differ 
between Moses+standard linguistic 
processing and Moses+adapted 
linguistic processing 
638   
(16 %  of 
3756) 
62
Table 4 presents the results of translation 
evaluation. 
 
Note, that a part of slight decreases can be 
corrected by introducing an adapted named enti-
ties dictionary to the translation system. For ex-
ample, for the source query ?romeo et juliette?, 
keeping NEs untranslated results at the following 
translation: ?romeo and juliette?, which is con-
sidered as a slight decrease in comparison to a 
baseline translation: ?romeo and juliet?. Creating 
an adapted NEs dictionary, either by crawling 
Wikipedia, or other parallel resources, might be 
helpful for such cases. 
Often, the cases of significantly better transla-
tions could potentially lead to the better retrieval. 
For example, a better lexical choice (don juan 
moliere vs. donation juan moliere, the plague vs. 
fever) often judged as significant improvement 
may lead to a better retrieval. 
Based on this observation one may hope that 
the adapted linguistic processing can indeed be 
useful in the query translation task in CLIR con-
text, but also in general query analysis context. 
7 Conclusion 
Queries posed to digital library search engines in 
the cultural heritage and social sciences domain 
tend to be very short, referring mostly to artist 
names, objects, titles, and dates. As we have il-
lustrated with several examples, taken from the 
logs of the Europeana portal, standard NLP 
analysis is not well adapted to treat that domain. 
In this work we have proposed adapting a com-
plete chain of linguistic processing tools for 
query processing, instead of using out-of-the-box 
tools designed to analyze full sentences. 
Focusing on the cultural heritage domain, we 
translated queries from the Europeana portal us-
ing a state-of-the-art machine translation system 
and evaluated translation quality before and after 
applying the adaptations. The impact of the lin-
guistic adaptations is quite significant, as in 42% 
of the queries the resulting structure changes. 
Subsequently, 16% of the query translations are 
also different. The positive impact of the adapted 
linguistic processing on the translation quality is 
evident, as for 99 queries the translation (out of 
200 sample evaluated) is improved when com-
pared to having no linguistic processing. We ob-
serve also that 78 queries are better translated 
after adapting the linguistic processing compo-
nents. 
Our results show that customizing the linguis-
tic processing of queries can lead to important  
 
improvements in translation (and eventually to 
multilingual information retrieval and data min-
ing). A lot of the differences are related to the 
ability of properly identifying and treating do-
main-specific named entities. We plan to further 
research this aspect in future works. 
 
Acknowledge ments  
This research was supported by the European 
Union?s ICT Policy Support Programme as part 
of the Competitiveness and Innovation Frame-
work Programme, CIP ICT-PSP under grant 
agreement nr 250430 (Project GALATEAS). 
References 
Bin Tan and Fuchun Peng. 2008. Unsupervised query 
segmentation using generative language models 
and wikipedia. In Proceedings of the 17th interna-
tional conference on World Wide Web (WWW 
'08). ACM, New York, NY, USA, 347-356. 
Cory Barr, Rosie Jones, Moira Regelson. 2008. The 
Linguistic Structure of EnglishWeb-Search Que-
ries, Proceedings of ENMLP'08, pp 1021?1030, 
Octobre 2008, Honolulu. 
James Allan and Hema Raghavan. 2002. Using part-
of-speech patterns to reduce query ambiguity. In  
Proceedings of the 25th annual international ACM 
SIGIR conference on Research and development in  
informat ion retrieval (SIGIR '02). ACM, New 
York, NY, USA, 307-314. 
Jeann-Pierre Chanod, Pasi Tapanainen. 1995.  Tag-
ging French - comparing a statistical and a con-
straint-based method. Proc. From Texts To Tags: 
 Impor
tant 
++ 
 Total 
nb+ 
Impo
rtant 
- - 
Total  
nb - 
Overall 
impact   
Moses<
Moses+
adapted  
35 87 4 19 99 
Moses+
basic<
Moses+
adapted  
28 66 2 12 80 
Table 4: Translation evaluation. Total nb+ (-): total 
number of improvements (decreases), not distinguish-
ing whether it is slight or important; important ++ (--): 
the number of important improvements (decreases). 
Overall impact = (Total nb+) + (Importan++ ) ? (Total 
nb-) ? (Important --) 
63
Issues In Multilingual Language Analysis, EACL 
SIGDAT workshop. Dublin, 1995. 
Jiafeng Guo, Gu Xu, Hang Li, Xueqi Cheng. 2008. A 
Unified and Discriminative Model for Query Re-
finement. Proc. SIGIR?08, July 20?24, 2008, Sin-
gapore. 
Josiane Mothe and Ludovic Tanguy. 2007. Linguistic 
Analysis of Users' Queries: towards an adaptive In-
formation Retrieval System. International Confer-
ence on Signal-Image Technology & Internet?
Based Systems, Shangai, China, 2007. 
http://halshs.archives-ouvertes.fr/halshs-
00287776/fr/ [Last accessed March 3, 2011]  
Lauri Karttunen. 2000. Applications of Finite-State 
Transducers in Natural Language Processing. Pro-
ceedings of CIAA-2000. Lecture Notes in Com-
puter Science. Springer Verlag. 
Marijn Koolen and Jaap Kamps. 2010. Searching cul-
tural heritage data: does structure help expert  
searchers?. In Adaptivity, Personalizat ion and Fu-
sion of Heterogeneous Information (RIAO '10). Le 
centre des hautes etudes internationals 
d?informat ique documentaire, Paris, France, 152-
155. 
Michael Bendersky, W. Bruce Croft and David A. 
Smith. 2010. Structural Annotation of Search Que-
ries Using Pseudo-Relevance Feedback. Proceed-
ings of CIKM'10, October 26-29, 2010, Toronto, 
Ontario, Canada 
Neil Ireson and Johan Oomen. 2007. Capturing e-
Culture: Metadata in MultiMatch., J. In Proc. 
DELOS-MultiMatch workshop, February 2007, 
Tirrenia, Italy. 
Rosie Jones, Ben jamin Rey, Omid Madani, and Wiley 
Greiner. 2006. Generating query substitutions. In 
Proceedings of the 15th international conference on 
World Wide Web (WWW '06). ACM, New York, 
NY, USA, 387-396. 
Shane Bergsma and Qin Iris Wang. 2007. Learning 
Noun Phrase Query Segmentation, Proceedings of 
the 2007 Jo int Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning, pp. 819?826, Prague, 
June 2007. 
64
