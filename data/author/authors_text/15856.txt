Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2008?2018,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Brighter than Gold: Figurative Language in User Generated Comparisons
Vlad Niculae and Cristian Danescu-Niculescu-Mizil
MPI-SWS
Cornell University
vniculae@mpi-sws.org, cristian@mpi-sws.org
Abstract
Comparisons are common linguistic de-
vices used to indicate the likeness of two
things. Often, this likeness is not meant
in the literal sense?for example, ?I slept
like a log? does not imply that logs ac-
tually sleep. In this paper we propose a
computational study of figurative compar-
isons, or similes. Our starting point is a
new large dataset of comparisons extracted
from product reviews and annotated for
figurativeness. We use this dataset to char-
acterize figurative language in naturally
occurring comparisons and reveal linguis-
tic patterns indicative of this phenomenon.
We operationalize these insights and ap-
ply them to a new task with high relevance
to text understanding: distinguishing be-
tween figurative and literal comparisons.
Finally, we apply this framework to ex-
plore the social context in which figurative
language is produced, showing that simi-
les are more likely to accompany opinions
showing extreme sentiment, and that they
are uncommon in reviews deemed helpful.
1 Introduction
In argument similes are like songs in love; they
describe much, but prove nothing.
? Franz Kafka
Comparisons are fundamental linguistic devices
that express the likeness of two things?be it en-
tities, concepts or ideas. Given that their work-
ing principle is to emphasize the relation between
the shared properties of two arguments (Bredin,
1998), comparisons can synthesize important se-
mantic knowledge.
Often, comparisons are not meant to be under-
stood literally. Figurative comparisons are an im-
portant figure of speech called simile. Consider the
following two examples paraphrased from Ama-
zon product reviews:
(1) Sterling is much cheaper than gold.
(2) Her voice makes this song shine brighter than gold.
In (1) the comparison draws on the relation be-
tween the price property shared by the two metals,
sterling and gold. While (2) also draws on a com-
mon property (brightness), the polysemantic use
(vocal timbre vs. light reflection) makes the com-
parison figurative.
Importantly, there is no general rule separating
literal from figurative comparisons. More gen-
erally, the distinction between figurative and lit-
eral language is blurred and subjective (Hanks,
2006). Multiple criteria for delimiting the two
have been proposed in the linguistic and philo-
sophical literature?for a comprehensive review,
see Shutova (2010)?but they are not without ex-
ceptions, and are often hard to operationalize in a
computational framework. When considering the
specific case of comparisons, such criteria cannot
be directly applied.
Recently, the simile has received increasing at-
tention from linguists and lexicographers (Moon,
2008; Moon, 2011; Hanks, 2013) as it became
clearer that similes need to be treated separately
from metaphors since they operate on funda-
mentally different principles (Bethlehem, 1996).
Metaphors are linguistically simple structures hid-
ing a complex mapping between two domains,
through which many properties are transferred.
For example the conceptual metaphor of life as
a journey can be instantiated in many particular
ways: being at a fork in the road, reaching the end
of the line (Lakoff and Johnson, 1980). In contrast,
the semantic context of similes tends to be very
shallow, transferring a single property (Hanks,
2013). Their more explicit syntactic structure al-
lows, in exchange, for more lexical creativity. As
Hanks (2013) puts it, similes ?tend to license all
2008
sorts of logical mayhem.? Moreover, the over-
lap between the expressive range of similes and
metaphors is now known to be only partial: there
are similes that cannot be rephrased as metaphors,
and the other way around (Israel et al., 2004). This
suggests that figurativeness in similes should be
modeled differently than in metaphors. To further
underline the necessity of a computational model
for similes, we give the first estimate of their fre-
quency in the wild: over 30% of comparisons are
figurative.
1
We also confirm that a state of the art
metaphor detection system performs poorly when
applied directly to the task of detecting similes.
In this work we propose a computational study
of figurative language in comparisons. To this end,
we build the first large collection of naturally oc-
curring comparisons with figurativeness annota-
tion, which we make publicly available. Using
this resource we explore the linguistic patterns that
characterize similes, and group them in two con-
ceptually distinctive classes. The first class con-
tains cues that are agnostic of the context in which
the comparison appears (domain-agnostic cues).
For example, we find that the higher the seman-
tic similarity between the two arguments, the less
likely it is for the comparison to be figurative?in
the examples above, sterling is semantically very
similar to gold, both being metals, but song and
gold are semantically dissimilar. The second type
of cues are domain-specific, drawing on the in-
tuition that the domain in which a comparison is
used is a factor in determining its figurativeness.
We find, for instance, that the less specific a com-
parison is to the domain in which it appears, the
more likely it is to be used in a figurative sense
(e.g., in example (2), gold is very unexpected in
the musical domain).
We successfully exploit these insights in a new
prediction task relevant to text understanding: dis-
criminating figurative comparisons from literal
ones. Encouraged by the high accuracy of our
system?which is within 10% of that obtained by
human annotators?we automatically extend the
figurativeness labels to 80,000 comparisons occur-
ring in product reviews. This enables us to conduct
a fine-grained analysis of how comparison usage
interacts with their social context, opening up a
research direction with applications in sentiment
analysis and opinion mining. In particular we find
1
This estimate is based on the set of noun-noun compar-
isons with non-identical arguments collected for this study
from Amazon.com product reviews.
that figurative comparisons are more likely to ac-
company reviews showing extreme sentiment, and
that they are uncommon in opinions deemed as be-
ing helpful. To the best of our knowledge, this is
the first time figurative language is tied to the so-
cial context in which it appears.
To summarize, the main contributions of this
work are as follows:
? it introduces the first large dataset of compar-
isons with figurativeness annotations (Sec-
tion 3);
? it unveils new linguistic patterns characteriz-
ing figurative comparisons (Section 4);
? it introduces the task of distinguishing figura-
tive from literal comparisons (Section 5);
? it establishes the relation between figurative
language and the social context in which it
appears (Section 6).
2 Further Related Work
Corpus studies on figurative language in compar-
isons are scarce, and none directly address the
distinction between figurative and literal compar-
isons. Roncero et al. (2006) observed, by search-
ing the web for several stereotypical comparisons
(e.g., education is like a stairway), that similes
are more likely to be accompanied by explana-
tions than equivalent metaphors (e.g., education
is a stairway). Related to figurativeness is irony,
which Veale (2012a) finds to often be lexically
marked. By using a similar insight to filter out
ironic comparisons, and by assuming that the rest
are literal, Veale and Hao (2008) learn stereotyp-
ical knowledge about the world from frequently
compared terms. A similar process has been ap-
plied to both English and Chinese by Li et al.
(2012), thereby encouraging the idea that the trope
behaves similarly in different languages. A related
system is the Jigsaw Bard (Veale and Hao, 2011),
a thesaurus driven by figurative conventional sim-
iles extracted from the Google N-grams. This sys-
tem aims to build and generate canned expressions
by using items frequently associated with the sim-
ile pattern above. An extension of the principles of
the Jigsaw Bard is found in Thesaurus Rex (Veale
and Li, 2013), a data-driven partition of words into
ad-hoc categories. Thesaurus Rex is constructed
using simple comparison and hypernym patterns
2009
and is able to provide weighted lists of categories
for given words.
In text understanding systems, literal compar-
isons are used to detect analogies between related
geographical places (Lofi et al., 2014). Tandon et
al. (2014) use relative comparative patterns (e.g.,
X is heavier than Y) to enrich a common-sense
knowledge base. Jindal and Liu (2006) extract
graded comparisons from various sources, with
the objective of mining consumer opinion about
products. They note that identifying objective vs.
subjective comparisons?related to literality?is
an important future direction. Given that many
comparisons are figurative, a system that discrim-
inates literal from figurative comparisons is essen-
tial for such text understanding and information
retrieval systems.
The vast majority of previous work on figu-
rative language focused on metaphor detection.
Tsvetkov et al. (2014a) propose a cross-lingual
system based on word-level conceptual features
and they evaluate it on Subject-Verb-Object triples
and Adjective-Noun pairs. Their features include
and extend the idea of abstractness used by Turney
et al. (2011) for Adjective-Noun metaphors. Hovy
et al. (2013) contribute an unrestricted metaphor
corpus and propose a method based on tree ker-
nels. Bridging the gap between metaphor identifi-
cation and interpretation, Shutova and Sun (2013)
proposed an unsupervised system to learn source-
target domain mappings. The system fits concep-
tual metaphor theory (Lakoff and Johnson, 1980)
well, at the cost of not being able to tackle figu-
rative language in general, and similes in particu-
lar, as similes do not map entire domains to one
another. Since similes operate on fundamentally
different principles than metaphors, our work pro-
poses a computational approach tailored specifi-
cally for comparisons.
3 Background and Data
3.1 Structure of a comparison
Unlike metaphors, which are generally unre-
stricted, comparisons are more structured but also
more lexically and semantically varied. This en-
ables a more structured computational representa-
tion of which we take advantage. The constituents
of a comparison according to Hanks (2012) are:
? the TOPIC, sometimes called tenor: it is usu-
ally a noun phrase and acts as logical subject;
? the VEHICLE: it is the object of the compari-
son and is also usually a noun phrase;
? the shared PROPERTY or ground: it expresses
what the two entities have in common?it can
be explicit but is often implicit, left for the
reader to infer;
? the EVENT (eventuality or state): usually a
verb, it sets the frame for the observation of
the common property;
? the COMPARATOR: commonly a preposition
(like) or part of an adjectival phrase (better
than), it is the trigger word or phrase that
marks the presence of a comparison.
The literal example (1) would be segmented as:
[Sterling /TOPIC] [is /EVENT] much [cheaper
/PROPERTY] [than /COMPARATOR] [gold /VE-
HICLE]
3.2 Annotation
People resort to comparisons often when mak-
ing descriptions, as they are a powerful way of
expressing properties by example. For this rea-
son we collect a dataset of user-generated compar-
isons in Amazon product reviews (McAuley and
Leskovec, 2013), where users have to be descrip-
tive and precise, but also to express personal opin-
ion. We supplement the data with a smaller set of
comparisons from WaCky and WaCkypedia (Ba-
roni et al., 2009) to cover more genres. In pre-
liminary work, we experimented with dependency
parse tree patterns for extracting comparisons and
labeling their parts (Niculae, 2013). We use the
same approach, but with an improved set of pat-
terns, to extract comparisons with the COMPARA-
TORS like, as and than.
2
We keep only the matches
where the TOPIC and the VEHICLE are nouns, and
the PROPERTY, if present, is an adjective, which
is the typical case. Also, the head words of the
constituents are constrained to occur in the distri-
butional resources used (Baroni and Lenci, 2010;
Faruqui and Dyer, 2014).
3
2
We process the review corpus with part-of-speech tag-
ging using the IRC model for TweetNLP (Owoputi et al.,
2013; Forsyth and Martell, 2007) and dependency parsing
using the TurboParser standard model (Martins et al., 2010).
3
Due to the strong tendency of comparisons with the same
TOPIC and VEHICLE to be trivially literal in the WaCky
examples, we filtered out such examples from the Amazon
product reviews. We also filtered proper nouns using a capi-
talization heuristic.
2010
We proceed to validate and annotate for figu-
rativeness a random sample of the comparisons
extracted using the automated process described
above. The annotation is performed using crowd-
sourcing on the Amazon Mechanical Turk plat-
form, in two steps. First, the annotators are asked
to determine whether a displayed sentence is in-
deed a comparison between the highlighted words
(TOPIC and VEHICLE). Sentences qualified by
two out of three annotators as comparisons are
used in the second round, where the task is to
rate how metaphorical a comparison is. We use
a scale of 1 to 4 following Turney et al. (2011),
and then binarize to consider scores of 1?2 as lit-
eral and 3?4 as figurative. Finally, in this work we
only consider comparisons where all three annota-
tors agree on this binary notion of figurativeness.
For both tasks, we provide guidelines mostly in
the form of examples and intuition, motivated on
one hand by the annotators not having specialized
knowledge, and on the other hand by the observa-
tion that the literal-figurative distinction is subjec-
tive. All annotators have the master worker qual-
ification, reside in the U.S. and completed a lin-
guistic background questionnaire that verifies their
experience with English. In both tasks, control
sentences with confidently known labels are used
to filter low quality answers; in addition, we test
annotators with a simple paraphrasing task shown
to be effective for eliciting and verifying linguis-
tic attention (Munro et al., 2010). Both tasks
seem relatively difficult for humans, with inter-
annotator agreement given by Fleiss? k of 0.48
for the comparison identification task and of 0.54
for the figurativeness annotation after binarization.
This is comparable to 0.57 reported by Hovy et al.
(2013) for general metaphor labeling. We show
some statistics about the collected data in Table 1.
Overall, this is a costly process: out of 2400 auto-
matically extracted comparison candidates, about
60% were deemed by the annotators to be actual
comparisons and only 12% end up being selected
confidently enough as figurative comparisons.
Our dataset of human-filtered comparisons,
with the scores given by the three annotators,
is made publicly available to encourage further
work.
4
This also includes about 400 comparisons
where the annotators do not agree perfectly on bi-
nary figurativeness. Such cases can be interest-
ing to other analyses, even if we don?t consider
4
http://vene.ro/figurative-comparisons/
Domain fig. lit. % fig.
Books 177 313 36%
Music 45 68 40%
Electronics 23 105 18%
Jewelery 9 126 7%
WaCky 19 79 19%
Total 273 609 31%
Table 1: Figurativeness annotation results. Only
comparisons where all three annotators agree are
considered.
them in our experiments. It is worth noting that
the existing corpora annotated for metaphor can-
not be directly used to study comparisons. For ex-
ample, in TroFi (Birke and Sarkar, 2006), a cor-
pus of 6436 sentences annotated for figurative-
ness, we only find 42 noun-noun comparisons with
sentence-level (thus noisy) figurativeness labels.
4 Linguistic Insights
We now proceed to exploring the linguistic pat-
terns that discriminate figurative from literal com-
parisons. We consider two broad classes of cues,
which we discuss next.
4.1 Domain-specific cues
Figurative language is often used for striking ef-
fects, and comparisons are used to describe new
things in terms of something given (Hanks, 2013).
Since the norms that define what is surprising and
what is well-known vary across domains, we ex-
pect that such contextual information should play
an important role in figurative language detection.
This is a previously unexplored dimension of figu-
rative language, and Amazon product reviews of-
fer a convenient testbed for this intuition since cat-
egory information is provided.
Specificity To estimate whether a compari-
son can be considered striking in a particular
domain?whether it references images or ideas
that are unexpected in its context?we employ a
simple measure of word specificity with respect to
a domain: the ratio of the word frequency within
the domain and the word frequency in all domains
being considered.
5
It should be noted that speci-
ficity is not purely a function of the word, but
5
We measure specificity for the VEHICLE, PROPERTY
and EVENT.
2011
(a) VEHICLE specificity. (b) TOPIC-VEHICLE similarity. (c) Imageability of the PROPERTY.
Figure 1: Distribution of some of the features we use, across literal and figurative comparisons in the test
set. The profile of the plot is a kernel density estimation of the distribution, and the markers indicate the
median and the first and third quartiles.
of the word and the context in which it appears.
A comparison in the music domain that involves
melodies is not surprising:
But the title song really feels like a pretty bland
vocal melody [...]
But the same word can play a very different role
in another context, for example, book reviews:
Her books are like sweet melodies that flow
through your head.
Indeed, the word melody has a specificity of 96%
in the music domain and only of 3% in the books
domain.
An analysis on the labeled data confirms that
literal comparisons do indeed tend to have more
domain-specific VEHICLES (Mann-Whitney U
test, p < 0.01) than figurative ones. Further-
more, the distribution of specificity across both
types of comparisons, as shown in Figure 1a, has
the appearance of a mixture model of general and
specific words. Figurative comparison VEHICLES
largely exhibit only the general component of the
mixture.
6
Domain label An analysis of the annotation re-
sults reveals that the percentage of comparisons
that are figurative differs widely across domains,
as indicated in the last column in Table 1. This
suggests that simply knowing the domain of a
text can serve to adjust some prior expectation
about figurative language presence and therefore
improve detection. We test this hypothesis using
6
The mass around 0.25 in Figure 1a is largely explained
by generic words such as thing, others, nothing, average and
barely specific words like veil, reputation, dream, garbage.
a Z-test comparing all Amazon categories. With
the exception of books and music reviews, that
have similar ratios, all other pairs of categories
show significantly different proportions of figura-
tive comparisons (p < 0.01).
4.2 Domain-agnostic cues
Linguistic studies of figurative language suggest
that there is a fundamental generic notion of fig-
urativeness. We attempt to capture this notion in
the context of comparisons using syntactic and se-
mantic information.
Topic-Vehicle similarity The default role of lit-
eral comparisons is to assert similarity of things.
Therefore, we expect that a high semantic simi-
larity between the TOPIC and the VEHICLE of a
comparison is a sign of literal usage, as we pre-
viously hypothesized in preliminary work (Nicu-
lae, 2013). To test this hypothesis, we compute
TOPIC-VEHICLE similarity using Distributional
Memory (Baroni and Lenci, 2010), a freely avail-
able distributional semantics resource that cap-
tures word relationships through grammatical role
co-occurrence.
By applying this measure to our data, we find
that there is indeed an important difference be-
tween the distributions of TOPIC-VEHICLE simi-
larity in figurative and literal comparisons (shown
in Figure 1b); the means of the two distribu-
tions are significantly different (Mann-Whitney
p < 0.01).
Metaphor-inspired features We also seek to
understand to what extent insights provided by
computational work on metaphor detection can be
2012
more concrete less concrete
more imageable cinnamon, kiss devil, happiness
less imageable casque, pugilist aspect, however
Table 2: Examples of words with high and low
concreteness and imageability scores from the
MRC Psycholinguistic Database.
applied in the context of comparisons. To that end
we consider features shown to provide state of the
art performance in the task of metaphor detection
(Tsvetkov et al., 2014a): abstractness, imageabil-
ity and supersenses.
Abstractness and imageability features are de-
rived from the MRC Psycholinguistic Database
(Coltheart, 1981), a dictionary based on manually
annotated datasets of psycholinguistic norms. Im-
ageability is the property of a word to arouse a
mental image, be it in the form of a mental pic-
ture, sound or any other sense. Concreteness is
defined as ?any word that refers to objects, materi-
als or persons,? while abstractness, at the other end
of the spectrum, is represented by words that can-
not be usually experienced by the senses (Paivio
et al., 1968). Table 2 shows a few examples of
words with high and low concreteness and image-
ability scores. Supersenses are a very coarse form
of meaning representation. Tsvetkov et al. (2014a)
used WordNet (Miller, 1995) semantic classes
for nouns and verbs, for example noun.body,
noun.animal, verb.consumption, or verb.motion.
For adjectives, Tsvetkov et al. (2014b) developed
and made available a novel classification in the
same spirit.
7
We compute abstractness, image-
ability and supersenses for the TOPIC, VEHICLE,
EVENT, and PROPERTY.
8
We concatenate these
features with the raw vector representations of the
constituents, following Tsvetkov et al. (2014a).
We find that such features relate to figurative
comparisons in a meaningful way. For example,
out of all comparisons with explicit properties, fig-
urative comparisons tend to have properties that
7
Following Tsvetkov et al. (2014a) we train a classifier to
predict these features from a vector space representation of a
word. We use the same cross-lingually optimized represen-
tation from Faruqui and Dyer (2014) and a simpler classifier,
a logistic regression, which we find to perform as well as the
random forests used in Tsvetkov et al. (2014a). We treat su-
persense prediction as a multi-label problem and apply a one-
versus-all transformation, effectively learning a linear classi-
fier for each supersense.
8
If the PROPERTY is implicit, all corresponding features
are set to zero. An extra binary feature indicates whether the
PROPERTY is explicit or implicit.
are more imageable (Mann-Whitney p < 0.01), as
illustrated by Figure 1c. This is in agreement with
Hanks (2005), who observed that similes are char-
acterized by their appeal to sensory imagination.
Definiteness We introduce another simple but
effective syntactic cue that relates to concreteness:
the presence of a definite article versus an indefi-
nite one (or none at all). We search for the indefi-
nite articles a and an and the definite article the in
each component of a comparison.
We find that similes tend to have indefinite arti-
cles in the VEHICLE more often and definite arti-
cles less often (Mann-Whitney p < 0.01). In par-
ticular, 59% of comparisons where the VEHICLE
has a indefinite article are figurative, as opposed
to 13% of the comparisons where VEHICLE has a
definite article.
5 Prediction Task
We now turn to the task of predicting whether a
comparison is figurative or literal. Not only does
this task allow us to assess and compare the effi-
ciency of the linguistic cues we discussed, but it is
also highly relevant in the context of natural lan-
guage understanding systems.
We conduct a logistic regression analysis, and
compare the efficiency of the features derived
from our analysis to a bag of words baseline.
In addition to the features inspired by the pre-
viously described linguistic insights, we also try
to computationally capture the lexical usage pat-
terns of comparisons using a version of bag of
words adapted to the comparison structure. In this
slotted bag of words system, features correspond
to occurrence of words within constituents (e.g.,
bright ? PROPERTY).
We perform a stratified split of our compari-
son dataset into equal train and test sets (each set
containing 408 comparisons, out of which 134 are
figurative),
9
and use a 5-fold stratified cross vali-
dation over the training set to choose the optimal
value for the logistic regression regularization pa-
rameter and the type of regularization (?
1
or ?
2
) for
each feature set.
10
9
The entire analysis described in Section 4 is only con-
ducted on the training set. Also, in order to ensure that we are
assessing the performance of the classifier on unseen com-
parisons, we discard from our dataset all those with the same
TOPIC and VEHICLE pair.
10
We use the logistic regression implementation
of liblinear (Fan et al., 2008) wrapped by the
scikit-learn library (Pedregosa et al., 2011).
2013
Model # features Acc. P R F
1
AUC
Bag of words 1970 0.79 0.63 0.84 0.72 0.87
Slotted bag of words 1840 0.80 0.64 0.90 0.75 0.89
Domain-agnostic cues 357 0.81 0.70 0.74 0.72 0.90
only metaphor inspired 345 0.75 0.60 0.72 0.65 0.84
Domain-specific cues 8 0.69 0.51 0.81 0.63 0.76
All linguistic insight cues 365 0.86 0.76 0.83 0.79 0.92
Full 2202 0.88 0.80 0.84 0.82 0.94
Human - 0.96 0.92 0.96 0.94 -
Table 3: Classification performance on the test set for the different sets of features we considered; human
performance is shown for reference.
Classifier performance The performance on
the classification task is summarized in Table 3.
We note that the bag of words baseline is remark-
ably strong, because of common idiomatic simi-
les that can be captured through keywords. Our
full system (which relies on our linguistically in-
spired cues discussed in Section 4 in addition to
slotted bag of words) significantly outperforms the
bag of words baseline and the slotted bag of words
system in terms of accuracy, F
1
score and AUC
(p < 0.05),
11
suggesting that linguistic insights
complement idiomatic simile matching. Impor-
tantly, a system using only our linguistic insight
cues also significantly improves over the baseline
in terms of accuracy and AUC and it is not signif-
icantly different from the full system in terms of
performance, in spite of having about an order of
magnitude fewer features. It is also worth noting
that the domain-specific cues play an important
role in bringing the performance to this level by
capturing a different aspect of what it means for a
comparison to be figurative.
The features used by the state of the art
metaphor detection system of Tsvetkov et al.
(2014a), adapted to the comparison structure, per-
form poorly by themselves and do not improve
significantly over the baseline. This is consis-
tent with the theoretical motivation that figura-
tiveness in comparisons requires special compu-
tational treatment, as discussed in Section 1. Fur-
thermore, the linguistic insight features not only
significantly outperform the metaphor inspired
features (p < 0.05), but are also better at exploit-
ing larger amounts of data, as shown in Figure 2.
11
All statistical significance results in this paragraph are
obtained from 5000 bootstrap samples.
Figure 2: Learning curves. Each point is obtained
by fitting a model on 10 random subsets of the
training set. Error bars show 95% confidence in-
tervals.
Comparison to human performance To gauge
how well humans would perform at the classifica-
tion task on the actual test data, we perform an-
other Amazon Mechanical Turk evaluation on 140
examples from the test set. For the evaluation,
we use majority voting between the three anno-
tators,
12
and compare to the agreed labels in the
dataset. Estimated human accuracy is 96%, plac-
ing our full systemwithin 10% of human accuracy.
Feature analysis The predictive analysis we
perform allows us to investigate to what extent the
features inspired by our linguistic insights have
discriminative power, and whether they actually
cover different aspects of figurativeness.
12
Majority voting helps account for the noise inherent to
crowdsourced annotation, which is less accurate than profes-
sional annotation. Taking the less optimistic invididual turker
answers, human performance is on the same level as our full
system.
2014
Feature Coef. Example where the feature is positively activated
TOPIC-VEHICLE similarity ?11.3 the older man was wiser and stronger than the boy
VEHICLE specificity ?5.8 the cord is more durable than the adapter [Electronics]
VEHICLE imageability 4.9 the explanations are as clear as mud
VEHICLE communication supersense ?4.6 the book reads like six short articles
VEHICLE indefiniteness 4.0 his fame drew foreigners to him like a magnet
life ? VEHICLE 7.1 the hero is truly larger than life: godlike, yet flawed
picture ? VEHICLE ?6.0 the necklace looks just like the picture
other ? VEHICLE ?5.9 this one is just as nice as the other
others ? VEHICLE ?5.5 some songs are more memorable than others
crap ? VEHICLE 4.7 the headphones sounded like crap
Table 4: Top 5 linguistic insight features (top) and slotted bag of words features (bottom) in the full model
and their logistic regression coefficients. A positive coefficient means the feature indicates figurativeness.
Table 4 shows the best linguistic insight and
slotted bag of words features selected by the full
model. The strongest feature by far is the seman-
tic similarity between the TOPIC and the VEHI-
CLE. By itself, this feature gets 70% accuracy and
61% F
1
score.
The rest of the top features involve mostly the
VEHICLE. This suggests that the VEHICLE is the
most informative element of a comparison when it
comes to figurativeness. Features involving other
constituents also get selected, but with slightly
lower weights, not making it to the top.
VEHICLE specificity is one of the strongest fea-
tures, with positive values indicating literal com-
parisons. This confirms our intuition that domain
information is important to discriminate figurative
from literal language.
Of the adapted metaphor features, the noun
communication supersense and the imageability
of the VEHICLE make it to the top. Nouns with
low communication rating occurring in the train-
ing set include puddles, arrangements, carbohy-
drates while nouns with high communication rat-
ing include languages and subjects.
Presence of an indefinite article in the VEHICLE
is a strong indicator of figurativeness. By them-
selves, the definiteness and indefiniteness features
perform quite well, attaining 78% accuracy and
67% F
1
score.
The salient bag of words features correspond to
specific types of comparisons. The words other
and others in the VEHICLE indicate comparisons
between the same kind of arguments, for exam-
ple some songs are more memorable than others,
and these are likely to be literal. The word pic-
ture is specific to the review setting, as products
are accompanied by photos, and for certain kinds
of products, the resemblance of the product with
the image is an important factor for potential buy-
ers.
13
The bag of words systems are furthermore
able to learn idiomatic comparisons by identify-
ing common figurative VEHICLES such as life and
crap, corresponding to fixed expressions such as
larger than life.
Error analysis Many of the errors made by our
full system involve indirect semantic mechanisms
such as metonymy. For example, the false pos-
itive the typeface was larger than most books
really means larger than the typefaces found in
most books, but without the implicit expansion the
meaning can appear figurative. A similar kind of
ellipsis makes the example a lot [of songs] are
even better than sugar be wrongly classified as
literal. Another source of error is polysemy. Ex-
amples like the rejuvelac formula is about 10 times
better than yogurt are misclassified because of the
multiple meanings of the word formula, one being
closely related to yogurt and food, but the more
common ones being general and abstract, suggest-
ing figurativeness.
6 Social Correlates
The advantage of studying comparisons situated
in a social context is that we can understand how
their usage interacts with internal and external hu-
man factors. An internal factor is the sentiment of
13
This feature is highly correlated with the domain: it ap-
pears 25 times in the training set, 24 of which in the jewelery
domain and once in book reviews.
2015
(a) Figurative comparisons are more likely to be found in re-
views with strongly polarized sentiment.
(b) Helpful reviews are less likely to contain figurative compar-
isons.
Figure 3: Interaction between figurative language and social context aspects. Error bars show 95%
confidence intervals. The dashed horizontal line marks the average proportion of figurative comparisons.
In Figure 3b the average proportion is different because we only consider reviews rated by at least 10
readers.
the user towards the reviewed product, indicated
by the star rating of the review. An external factor
present in the data is how helpful the review is per-
ceived by other users. In this section we analyze
how these factors interact with figurative language
in comparisons.
To gain insight about fine grained interactions
with human factors at larger scale, we use our clas-
sifier to find over 80,000 figurative and literal com-
parisons from the same four categories. The trends
we reveal also hold significantly on the manually
annotated data.
Sentiment While it was previously noted that
similes often transmit strong affect (Hanks, 2005;
Veale, 2012a; Veale, 2012b), the connection be-
tween figurativeness and sentiment was never em-
pirically validated. The setting of product reviews
is convenient for investigating this issue, since
the star ratings associated with the reviews can
be used as sentiment labels. We find that com-
parisons are indeed significantly more likely to
be figurative when the users express strong opin-
ions, i.e., in one-star or five-star reviews (Mann-
Whitney p < 0.02 on the manually annotated
data). Figure 3a shows how the proportion of fig-
urative comparisons varies with the polarity of the
review.
Helpfulness It is also interesting to understand
to what extent figurative language relates to the
external perception of the content in which it ap-
pears. We find that comparisons in helpful re-
views
14
are less likely to be figurative. Figure 3b
shows a near-constant high ratio of figurative com-
parisons among unhelpful and average reviews; as
helpfulness increases, figurative comparisons be-
come less frequent. We further validate that this
effect is not a confound of the distribution of help-
fulness ratings across reviews of different polarity
by controlling for the star rating: given a fixed star
rating, the proportion of figurative comparisons is
still lower in helpful (helpfulness over 50%) than
in unhelpful (helpfulness under 50%) reviews; this
difference is significant (Mann-Whitney p < 0.01)
for all classes of ratings except one-star. The
size of the manually annotated data does not al-
low for star rating stratification, but the overall dif-
ference is statistically significant (Mann-Whitney
p < 0.01). This result encourages further exper-
imentation to determine whether there is a causal
link between the use of figurative language in user
generated content and its external perception.
7 Conclusions and Future Work
This work proposes a computational study of fig-
urative language in comparisons. Starting from
a new dataset of naturally occurring comparisons
with figurativeness annotation (which we make
publicly available) we explore linguistic patterns
that are indicative of similes. We show that these
14
In order to have reliable helpfulness scores, we only con-
sider reviews that have been rated by at least by ten readers.
2016
insights can be successfully operationalized in a
new prediction task: distinguishing literal from
figurative comparisons. Our system reaches ac-
curacy that is within 10% of human performance,
and is outperforming a state of the art metaphor
detection system, thus confirming the need for
a computational approach tailored specifically to
comparisons. While we take a data-driven ap-
proach, our annotated dataset can be useful for
more theoretical studies of the kinds of compar-
isons and similes people use.
We discover that domain knowledge is an im-
portant factor in identifying similes. This suggests
that future work on automatic detection of figura-
tive language should consider contextual parame-
ters such as the topic and community where the
content appears.
Furthermore, we are the first to tie figurative
language to the social context in which it is pro-
duced and show its relation to internal and exter-
nal human factors such as opinion sentiment and
helpfulness. Future investigation into the causal
effects of these interactions could lead to a better
understanding of the role of figurative language in
persuasion and rhetorics.
In our work, we consider common noun TOP-
ICS and VEHICLES and adjectival PROPERTIES.
This is the most typical case, but supporting other
parts of speech?such as proper nouns, pronouns,
and adverbs?can make a difference in many ap-
plications. Capturing compositional interaction
between the parts of the comparison could lead to
more flexible models that give less weight to the
VEHICLE.
This study is also the first to estimate how
prevalent similes are in the wild, and reports that
about one third of the comparisons we consider are
figurative. This is suggestive of the need to build
systems that can properly process figurative com-
parisons in order to correctly harness the semantic
information encapsulated in comparisons.
Acknowledgements
We would like to thank Yulia Tsvetkov for con-
structive discussion about figurative language and
about her and her co-authors? work. We are grate-
ful for the suggestions of Patrick Hanks, Con-
stantin Or?asan, Sylviane Cardey, Izabella Thomas,
Ekaterina Shutova, Tony Veale, and Niket Tan-
don. We extend our gratitude to Julian McAuley
for preparing and sharing the Amazon review
dataset. We are thankful to the anonymous review-
ers, whose comments were like a breath of fresh
air. We acknowledge the help of the Amazon Me-
chanical Turk annotators and of the MPI-SWS stu-
dents involved in pilot experiments.
Vlad Niculae was supported in part by the
European Commission, Education & Training,
Erasmus Mundus: EMMC 2008-0083, Erasmus
Mundus Masters in NLP & HLT.
References
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky wide web:
A collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209?226.
Louise Shabat Bethlehem. 1996. Simile and figurative
language. Poetics Today, 17(2):203?240.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In Proceedings of EACL.
Hugh Bredin. 1998. Comparisons and similes. Lin-
gua, 105(1):67?78.
Max Coltheart. 1981. The MRC psycholinguistic
database. The Quarterly Journal of Experimental
Psychology, 33(4):497?505.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of EACL.
Eric N Forsyth and Craig H Martell. 2007. Lexical and
discourse analysis of online chat dialog. In Proceed-
ings of ICSC.
Patrick Hanks. 2005. Similes and sets: The English
preposition ?like?. In R. Blatn?a and V. Petkevic, ed-
itors, Languages and Linguistics: Festschrift for Fr.
Cermak. Charles University, Prague.
Patrick Hanks. 2006. Metaphoricity is grad-
able. Trends in Linguistic Studies and Monographs,
171:17.
Patrick Hanks. 2012. The roles and structure of com-
parisons, similes, and metaphors in natural language
(an analogical system). Presented at the Stockholm
Metaphor Festival.
2017
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. MIT Press.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Pro-
ceedings of the NAACL Workshop on Metaphors for
NLP.
M. Israel, J.R. Harding, and V. Tobin. 2004. On simile.
Language, Culture, and Mind. CSLI Publications.
Nitin Jindal and Bing Liu. 2006. Identifying compara-
tive sentences in text documents. In Proceedings of
SIGIR.
George Lakoff and Mark Johnson. 1980. Metaphors
we live by. University of Chicago Press.
Bin Li, Jiajun Chen, and Yingjie Zhang. 2012. Web
based collection and comparison of cognitive prop-
erties in English and Chinese. In Proceedings of the
Joint Workshop on Automatic Knowledge Base Con-
struction and Web-scale Knowledge Extraction.
Christoph Lofi, Christian Nieke, and Nigel Collier.
2014. Discriminating rhetorical analogies in social
media. In Proceedings of EACL.
Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-
dro MQ Aguiar, and M?ario AT Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of EMNLP.
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: Understanding rating dimen-
sions with review text. In Proceedings of RecSys.
George A Miller. 1995. WordNet: a lexical
database for English. Communications of the ACM,
38(11):39?41.
Rosamund Moon. 2008. Conventionalized as-similes
in English: A problem case. International Journal
of Corpus Linguistics, 13(1):3?37.
Rosamund Moon. 2011. Simile and dissimilarity.
Journal of Literary Semantics, 40(2):133?157.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher
Potts, Tyler Schnoebelen, and Harry Tily. 2010.
Crowdsourcing and language studies: The new gen-
eration of linguistic data. In Proceedings of the
NAACL Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk.
Vlad Niculae. 2013. Comparison pattern matching and
creative simile recognition. In Proceedings of the
Joint Symposium on Semantic Processing.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT.
Allan Paivio, John C Yuille, and Stephen A Madigan.
1968. Concreteness, imagery, and meaningfulness
values for 925 nouns. Journal of Experimental Psy-
chology, 76(1p2):1.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in Python. Journal of Machine
Learning Research, 12:2825?2830.
Carlos Roncero, John M Kennedy, and Ron Smyth.
2006. Similes on the internet have explanations.
Psychonomic Bulletin & Review, 13(1):74?77.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised
metaphor identification using hierarchical graph fac-
torization clustering. In Proceedings of NAACL-
HLT.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of ACL.
Niket Tandon, Gerard de Melo, and Gerhard Weikum.
2014. Smarter than you think: Acquiring compara-
tive commonsense knowledge from the web. In Pro-
ceedings of AAAI.
Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman,
Eric Nyberg, and Chris Dyer. 2014a. Metaphor de-
tection with cross-lingual model transfer. In Pro-
ceedings of ACL.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014b.
Augmenting English adjective senses with super-
senses. In Proceedings of LREC.
Peter D Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of EMNLP.
Tony Veale and Yanfen Hao. 2008. A context-sensitive
framework for lexical ontologies. Knowledge Engi-
neering Review, 23(1):101?115.
Tony Veale and Yanfen Hao. 2011. Exploiting ready-
mades in linguistic creativity: A system demonstra-
tion of the Jigsaw Bard. In Proceedings of ACL (Sys-
tem Demonstrations).
Tony Veale and Guofu Li. 2013. Creating similarity:
Lateral thinking for vertical similarity judgments. In
Proceedings of ACL.
Tony Veale. 2012a. A computational exploration of
creative similes. Metaphor in Use: Context, Cul-
ture, and Communication, 38:329.
Tony Veale. 2012b. A context-sensitive, multi-faceted
model of lexico-conceptual affect. In Proceedings
of ACL.
2018
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 524?528,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Learning How to Conjugate the Romanian Verb. Rules for Regular and
Partially Irregular Verbs
Liviu P. Dinu
Faculty of Mathematics
and Computer Science
University of Bucharest
ldinu@fmi.unibuc.ro
Vlad Niculae
Faculty of Mathematics
and Computer Science
University of Bucharest
vlad@vene.ro
Octavia-Maria S, ulea
Faculty of Foreign Languages
and Literatures
Faculty of Mathematics
and Computer Science
University of Bucharest
mary.octavia@gmail.com
Abstract
In this paper we extend our work described
in (Dinu et al 2011) by adding more con-
jugational rules to the labelling system in-
troduced there, in an attempt to capture
the entire dataset of Romanian verbs ex-
tracted from (Barbu, 2007), and we em-
ploy machine learning techniques to predict
a verb?s correct label (which says what con-
jugational pattern it follows) when only the
infinitive form is given.
1 Introduction
Using only a restricted group of verbs, in (Dinu
et al 2011) we validated the hypothesis that pat-
terns can be identified in the conjugation of the
Romanian (partially irregular) verb and that these
patterns can be learnt automatically so that, given
the infinitive of a verb, its correct conjugation
for the indicative present tense can be produced.
In this paper, we extend our investigation to the
whole dataset described in (Barbu, 2008) and at-
tempt to capture, beside the general ending pat-
terns during conjugation, as much of the phono-
logical alternations occuring in the stem of verbs
(apophony) from the dataset as we can.
Traditionally, Romanian has received a Latin-
inspired classification of verbs into 4 (or some-
times 5) conjugational classes based on the ending
of their infinitival form alone (Costanzo, 2011).
However, this infinitive-based classification has
proved itself inadequate due to its inability to ac-
count for the behavior of partially irregular verbs
(whose stems have a smaller number of allo-
morphs than the completely irregular) during their
conjugation.
There have been, thus, numerous attempts
throughout the history of Romanian Linguistics
to give other conjugational classifications based
on the way the verb actually conjugates. Lom-
bard (1955), looking at a corpus of 667 verbs,
combined the traditional 4 classes with the way in
which the biggest two subgroups conjugate (one
using the suffix ?ez?, the other ?esc?) and ar-
rived at 6 classes. Ciompec (Ciompec et. al.,
1985 in Costanzo, 2011) proposed 10 conjuga-
tional classes, while Felix (1964) proposed 12,
both of them looking at the inflection of the verbs
and number of allomorphs of the stem. Romalo
(1968, p. 5-203) produced a list of 38 verb types,
which she eventually reduced to 10.
For the purpose of machine translation, Moisil
(1960) proposed 5 regrouped classes of verbs,
with numerous subgroups, and introduced the
method of letters with variable values, while Pa-
pastergiou et al(2007) have recently developed
a classification from a (second) language acquisi-
tion point of view, dividing the 1st and 4th tradi-
tional classes into 3 and respectively 5 subclasses,
each with a different conjugational pattern, and
offering rules for alternations in the stem.
Of the more extensive classifications, Barbu
(2007) distinguished 41 conjugational classes for
all tenses and 30 for the indicative present alone,
covering a whole corpus of more that 7000 con-
temporary Romanian verbs, a corpus which was
also used in the present paper. However, her
classes were developed on the basis of the suf-
fixes each verb receives during conjugation, and
the classification system did not take into account
the alternations occuring in the stem of irregular
and partially irregular verbs. The system of rules
presented below took into account both the end-
ings pattern and the type of stem alternation for
each verb.
In what follows we describe our method for la-
beling the dataset and finding a model able to pre-
524
dict the labels.
2 Approach
The problem which we are aiming to solve is to
determine how to conjugate a verb, given its in-
finitive form. The traditional infinitive-based clas-
sification taught in school does not take one all the
way to solving this problem. Many conjugational
patterns exist within each of these four classes.
2.1 Labeling the dataset
Following our own observations, the alternations
identified in (Papastergiou et al 2007) and the
classes of suffix patterns given in (Barbu, 2007),
we developed a number of conjugational rules
which were narrowed down to the 30 most pro-
ductive in relation to the dataset. Each of these
30 rules (or patterns) contains 6 regular expres-
sions through which the rule models how a (dif-
ferent) type of Romanian verb conjugates in the
indicative present. They each consist of 6 reg-
ular expressions because there are three persons
(first, second, and third) times two numbers (sin-
gular and plural).
Rule 10, for example, models, as stated in
the list that follows, how verbs of the type
?a ca?nta? (to sing) conjugate in the indicative
present, by having the first regular expression
model the first person singular form ?(eu) ca?nt?
(in regular expression format: ?(.+)$), the sec-
ond, model the second person singular form ?(tu)
ca?nt?i? (?(.+)t?i$), the third, model the third per-
son singular form ?(ei) ca?nta?? (?(.+)a?$), and so
forth. Thus, rule 10 catches the alternation t?t?
for the 2nd person singular, while modelling a
particular type of verb class with a particular set
of suffixes. Note that the dot accepts any letter
in the Romanian alphabet and that, for each of
the six forms, the value of the capturing groups
(those between brackets) remains constant, in this
case ca?n. These groups correspond to all parts of
the stem that remain unchanged and ensure that,
given the infinitive and the regular expressions,
one can work backwards and produce the correct
conjugation.
For a clearer understanding of one such rule,
Table 1 shows an example of how the verb ?a
tresa?lta? is modeled by rule 14.
Below, we list all the rules used, with the stem
alternations they capture and an example of a verb
Person Regexp Example
1st singular ?(.+)a(.+)t$ tresalt
2nd singular ?(.+)a(.+)t?i$ tresalt?i
3rd singular ?(.+)a(.+)ta?$ tresalta?
1st plural ?(.+)a?(.+)ta?m$ tresa?lta?m
2nd plural ?(.+)a?(.+)tat?i$ tresa?ltat?i
3rd plural ?(.+)a(.+)ta?$ tresalta?
Table 1: Rule 14 modelling ?a tresa?lta?
that they model. Note that, when we say (no) al-
ternation, we mean (no) alternation in the stem.
So the difference between rules 1, 20, 22, and the
sort lies in the suffix that is added to the stem
for each verb form. They may share some suf-
fixes, but not all and/or not for the same person
and number.
1. no alternation; ?a spera? (to hope);
2. alternation: a??e for the 2nd person singular;
?a numa?ra? (to count);
3. no alternation; ?a intra? (to enter), stem ends
in ?tr?, ?pl?, ?bl? or ?fl? which determines
the addition of ?u? at the end of the 1st per-
son singular form;
4. alternation: it lacks t?t? for the 2nd person
singular, which otherwise normally occurs;
?a mis?ca? (to move), stem ends in ?s?ca?;
5. no alternation; ?a ta?ia? (to cut), ends in ?ia?
and has a vowel before;
6. no alternation; ?a speria? (to scare), ends in
?ia? and has a consonant before;
7. no alternation; ?a dansa? (to dance), conju-
gated with the suffix ?ez?;
8. no alternation; ?a copia? (to copy), conju-
gated with a modified ?ez? due to the stem
ending in ?ia?;
9. altenation c?ch(e) or g?gh(e); ?a parca?
(to park), conjugated with ?ez?, ending in
?ca? or ?ga?;
10. alternation: t?t? for the 2nd person singular;
?a ca?nta? (to sing);
11. alternation: s?s? which replaces the usual
t?t? for the 2nd person singular; ?a exista?
(to exist);
525
12. alternation: a?ea for the 3rd person singular
and plural, t?t? for the 2nd person singular;
?a des?tepta? (to awake/arouse);
13. alternation: e?ea for the 3rd person singular
and plural, t?t? for the 2nd person singular;
?a des?erta? (to empty);
14. alternation: a??a for all the forms except the
1st and 2nd person plural; ?a tresa?lta? (to
start, to take fright);
15. alternation: a??a in the 3rd person singular
and plural, a??e in the 2nd person singular;
?a desfa?ta? (to delight);
16. alternation: a??a for all the forms except for
the 1st and 2nd person plural; ?a pa?rea? (to
seem);
17. alternation: d?z for the 2nd person singu-
lar due to palatalization, along with a??e; ?a
vedea? (to see), stem ends in ?d?;
18. alternation: a??a for all forms except the 1st
and 2nd person plural, d?z for the 2nd per-
son singular due to palatalization; ?a ca?dea?
(to fall);
19. no alternation; ?a veghea? (to watch over),
conjugates with another type of ?ez? ending
pattern;
20. no alternations; ?a merge? (to walk), receives
the typical ending pattern for the third conju-
gational class;
21. alternation: t?t? for the 2nd person singular;
?a promite? (to promise);
22. no alternation; ?a scrie? (to write);
23. alternations: s?t?sc for the 1st person singu-
lar and 3rd person plural; ?a nas?te? (to give
birth), ends in ?s?te?;
24. alternation: ?n? is deleted from the stem in
the 2nd person singular; ?a pune? (to put),
ends in ?ne?;
25. alternation: d?z in the 2nd person singular
due to palatalization; ?a crede? (to believe),
stem ends in ?d?;
26. no alternation; ?a sui? (to climb), ends in
?ui?, ?a?i?, or ?a?i?;
27. no alternation; ?a citi? (to read), conjugates
with the suffix ?esc? ;
28. this type preserves the ?i? from the infinitive;
?a locui? (to reside), ends in ?a?i?, ?oi?, or ui?
and conjugates with ?esc?;
29. alternation: o?oa in the 3rd person singular
and plural; end in ????, ?a omor??? (to kill);
30. no alternation; ?a hota?r??? (to decide), ends in
???? and conjugates with ?a?sc?, a variant of
?esc?
2.2 Classifiers and features
Each infinitive in the dataset received a label cor-
responding to the first rule that correctly produces
a conjugation for it. This was implemented in
order to reduce the ambiguity of the data, which
was due to some verbs having alternate conjuga-
tion patterns. The unlabeled verbs were thrown
out, while the labeled ones were used to train and
evaluate a classifier.
The context sensitive nature of the alternations
leads to the idea that n-gram character windows
are useful. In the preprocessing step, the list of in-
finitives is transformed to a sparse matrix whose
lines correspond to samples, and whose features
are the occurence or the frequency of a specific n-
gram. This feature extraction step has three free
parameters: the maximum n-gram length, the op-
tional binarization of the features (taking only bi-
nary occurences instead of counts), and the op-
tional appending of a terminator character. The
terminator character allows the classifier to iden-
tify and assign a different weight to the n-grams
that overlap with the suffix of the string.
For example, consider the English infinitive to
walk. We will assume the following illustrative
values for the parameters: n-gram size of 3 and
appending the terminator character. Firstly, a ter-
minator is appended to the end, yielding the string
walk$. Subsequently, the string is broken into 1, 2
and 3-grams: w, a, l, k, $, wa, al, lk, k$, wal, alk,
lk$. Next, this list is turned into a vector using a
standard process. We have first built a dictionary
of all the n-grams from the whole dataset. These,
in order, encode the features. The verb (to) walk
is therefore encoded as a row vector with ones in
the columns corresponding to the features w, a,
etc. and zeros in the rest. In this particular case,
there is no difference between binary and count
526
rule no. verbs
1 547
2 8
3 18
4 5
5 8
6 16
7 3330
8 273
9 89
10 4
11 5
12 4
13 106
14 13
15 5
rule no. verbs
16 13
17 6
18 4
19 14
20 124
21 25
22 15
23 7
24 41
25 51
26 185
27 1554
28 486
29 5
30 27
Table 2: Number of verbs captured by each of our rules
features because all of the n-grams of this short
verb occur only once. But for a verb such as (to)
tantalize, the feature corresponding to the 2-gram
ta would get a value of 2 in a count reprezentation,
but only a value of 1 in a binary one.
The system was put together using the scikit-
learn machine learning library for Python (Pe-
dregosa et al 2011), which provides a fast, scal-
able implementation of linear support vector ma-
chines based on liblinear (Fan et al 2008), along
with n-gram extraction and grid search function-
ality.
3 Results
Tabel 2 shows how well the rules fitted the dataset.
Out of 7,295 verbs in the dataset, 349 were uncap-
tured by our rules. As expected, the rule capturing
the most verbs (3,330) is the one modelling those
from the 1st conjugational class (whose infinitives
end in ?a?) which conjugate with the ?ez? suffix
and are regular, namely rule 7, created for verbs
like ?a dansa?. The second largest class, also as
expected, is the one belonging to verbs from the
4th conjugational group (whose infinitives end in
?i?), which are regular, meaning no alternation in
the stem, and conjugate with the ?esc? suffix. This
class is modeled by rule number 27.
The support vector classifier was evaluated
using a 10-fold cross-validation. The multi-
class problem is treated using the one-versus-all
scheme. The parameters chosen by grid search are
a maximum n-gram length of 5, with appended
terminator and with non-binarized (count) fea-
tures. The estimated correct classification rate is
90.64%, with a weighted averaged precision of
80.90%, recall of 90.64% andF1 score of 89.89%.
Appending the artificial terminator character ?$?
consistently improves accuracy by around 0.7%.
Because each word was represented as a bag of
character n-grams instead of a continuous string,
and because, by its nature, a SVM yields sparse
solutions, combined with the evaluation using
cross-validation, we can safely say that the model
does not overfit and indeed learns useful decision
boundaries.
4 Conclusions and Future Works
Our results show that the labelling system based
on the verb conjugation model we developed can
be learned with reasonable accuracy. In the future,
we plan to develop a multiple tiered labelling sys-
tem that will allow for general alternations, such
as the ones occuring as a result of palatalization,
to be defined only once for all verbs that have
them, taking cues from the idea of letters with
multiple values. This, we feel, will highly im-
prove the acuracy of the classifier.
5 Acknowledgements
The authors would like to thank the anonymous
reviewers for their helpful comments. All authors
contributed equally to this work. The research of
Liviu P. Dinu was supported by the CNCS, IDEI
- PCE project 311/2011, ?The Structure and In-
terpretation of the Romanian Nominal Phrase in
Discourse Representation Theory: the Determin-
ers.?
References
Ana-Maria Barbu. Conjugarea verbelor roma?-
nes?ti. Dict?ionar: 7500 de verbe roma?nes?ti gru-
pate pe clase de conjugare. Bucharest: Coresi,
2007. 4th edition, revised. (In Romanian.) (263
pp.).
Ana-Maria Barbu. Romanian lexical databases:
Inflected and syllabic forms dictionaries. In
Sixth International Language Resources and
Evaluation (LREC?08), 2008.
Angelo Roth Costanzo. Romance Conjugational
Classes: Learning from the Peripheries. PhD
thesis, Ohio State University, 2011.
527
Figure 1: 10-fold cross validation scores for various combination of parameters. Only the values corresponding
to the best C regularization parameters are shown.
Liviu P. Dinu, Emil Ionescu, Vlad Niculae, and
Octavia-Maria S?ulea. Can alternations be
learned? a machine learning approach to verb
alternations. In Recent Advances in Natural
Language Processing 2011, September 2011.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. Liblinear:
A library for large linear classification. Journal
of Machine Learning Research, 9:1871?1874,
June 2008. ISSN 1532-4435.
Jir?i Felix. Classification des verbes roumains, vol-
ume VII. Philosophica Pragensia, 1964.
Alf Lombard. Le verbe roumain. Etude mor-
phologique, volume 1. Lund, C. W. K. Gleerup,
1955.
Grigore C. Moisil. Probleme puse de traduc-
erea automata?. conjugarea verbelor ??n limba
roma?na?. Studii si cerceta?ri lingvistice, XI(1):
7?29, 1960.
I. Papastergiou, N. Papastergiou, and L. Man-
deki. Verbul roma?nesc - reguli pentru ??nlesnirea
??nsus?irii indicativului prezent. In Romanian
National Symposium ?Directions in Roma-
nian Philological Research?, 7th Edition, May
2007.
F. Pedregosa, G. Varoquaux, A. Gramfort,
V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Jour-
nal of Machine Learning Research, 12:2825?
2830, Oct 2011.
Valeria Gut?u Romalo. Morfologie Structurala? a
limbii roma?ne. Editura Academiei Republicii
Socialiste Roma?nia, 1968.
528
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 17?21,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Temporal Text Ranking and Automatic Dating of Texts
Vlad Niculae
1
, Marcos Zampieri
2
, Liviu P. Dinu
3
, Alina Maria Ciobanu
3
Max Planck Institute for Software Systems, Germany
1
Saarland University, Germany
2
Center for Computational Linguistics, University of Bucharest, Romania
3
vniculae@mpi-sws.org, marcos.zampieri@uni-saarland.de,
ldinu@fmi.unibuc.ro, alina.ciobanu@my.fmi.unibuc.ro
Abstract
This paper presents a novel approach to
the task of temporal text classification
combining text ranking and probability for
the automatic dating of historical texts.
The method was applied to three histor-
ical corpora: an English, a Portuguese
and a Romanian corpus. It obtained per-
formance ranging from 83% to 93% ac-
curacy, using a fully automated approach
with very basic features.
1 Introduction
Temporal text classification is an underexplored
problem in NLP, which has been tackled as a
multi-class problem, with classes defined as time
intervals such as months, years, decades or cen-
turies. This approach has the drawback of having
to arbitrarily delimit the intervals, and often leads
to a model that is not informative for texts written
within such a window. If the predefined window is
too large, the output is not useful for most systems;
if the window is too small, learning is impractical
because of the large number of classes. Particu-
larly for the problem of historical datasets (as the
one we propose here), learning a year-level classi-
fier would not work, because each class would be
represented by a single document.
Our paper explores a solution to this drawback
by using a ranking approach. Ranking amounts to
ordering a set of inputs with respect to some mea-
sure. For example, a search engine ranks returned
documents by relevance. We use a formalization
of ranking that comes from ordinal regression, the
class of problems where samples belong to inher-
ently ordered classes.
This study is of interest to scholars who deal
with text classification and NLP in general; his-
torical linguists and philologists who investigate
language change; and finally scholars in the dig-
ital humanities who often deal with historical
manuscripts and might take advantage of temporal
text classification applications in their research.
2 Related Work
Modelling temporal information in text is a rele-
vant task for a number of NLP tasks. For example,
in Information Retrieval (IR) research has been
concentrated on investigating time-sensitivity doc-
ument ranking (Dakka and Gravana, 2010). Even
so, as stated before, temporal text classification
methods were not substantially explored as other
text classification tasks.
One of the first studies to model temporal infor-
mation for the automatic dating of documents is
the work of de Jong et al. (2005). In these exper-
iments, authors used unigram language models to
classify Dutch texts spanning from January 1999
to February 2005 using normalised log-likelihood
ratio (NLLR) (Kraaij, 2004). As to the features
used, a number of approaches proposed to auto-
matic date take into account lexical features (Dalli
and Wilks, 2006; Abe and Tsumoto, 2010; Ku-
mar et al., 2011) and a few use external linguistic
knowledge (Kanhabua and N?rv?ag, 2009).
A couple of approaches try to classify texts not
only regarding the time span in which the texts
were written, but also their geographical location
such as (Mokhov, 2010) for French and, more re-
cently, (Trieschnigg et al., 2012) for Dutch. At the
word level, two studies aim to model and under-
stand how word usage and meaning change over
time (Wijaya and Yeniterzi, 2011), (Mihalcea and
Nastase, 2012).
The most recent studies in temporal text classifi-
cation to our knowledge are (Ciobanu et al., 2013)
for Romanian using lexical features and (
?
Stajner
and Zampieri, 2013) for Portuguese using stylistic
and readability features.
17
3 Methods
3.1 Corpora
To evaluate the method proposed here we used
three historical corpora. An English historical
corpus entitled Corpus of Late Modern English
Texts (CLMET)
1
(de Smet, 2005), a Portuguese
historical corpus entitled Colonia
2
(Zampieri and
Becker, 2013) and a Romanian historical corpus
(Ciobanu et al., 2013).
CLMET is a collection of English texts derived
from the Project Gutenberg and from the Oxford
Text Archive. It contains around 10 million to-
kens, divided over three sub-periods of 70 years.
The corpus is available for download as raw text
or annotated with POS annotation.
For Portuguese, the aforementioned Colonia
(Zampieri and Becker, 2013) is a diachronic col-
lection containing a total of 5.1 million tokens and
100 texts ranging from the 16
th
to the early 20
th
century. The texts in Colonia are balanced be-
tween European and Brazilian Portuguese (it con-
tains 52 Brazilian texts and 48 European texts) and
the corpus is annotated with lemma and POS in-
formation. According to the authors, some texts
presented edited orthography prior to their com-
pilation but systematic spelling normalisation was
not carried out.
The Romanian corpus was compiled to portrait
different stages in the evolution of the Romanian
language, from the 16
th
to the 20
th
century in a
total of 26 complete texts. The methodology be-
hind corpus compilation and the date assignment
are described in (Ciobanu et al., 2013).
3.2 Temporal classification as ranking
We propose a temporal model that learns a linear
function g(x) = w ? x to preserve the temporal or-
dering of the texts, i.e. if document
3
x
i
predates
document x
j
, which we will henceforth denote as
x
i
? x
j
, then g(x
i
) < g(x
j
). Such a problem is
often called ranking or learning to rank. When the
goal is to recover contiguous intervals that corre-
spond to ordered classes, the problem is known as
ordinal regression.
We use a pairwise approach to ranking that re-
duces the problem to binary classification using a
1
https://perswww.kuleuven.be/
?
u0044428/clmet
2
http://corporavm.uni-koeln.de/
colonia/
3
For brevity, we use x
i
to denote both the document itself
and its representation as a feature vector.
linear model. The method is to convert a dataset
of the form D = {(x, y) : x ? R
d
, y ? Y} into a
pairwise dataset:
D
p
= {((x
i
, x
j
), I[y
i
< y
j
]) :
(x
i
, y
i
), (x
j
, y
j
) ? D}
Since the ordinal classes only induce a partial or-
dering, as elements from the same class are not
comparable, D
p
will only consist of the compara-
ble pairs.
The problem can be turned into a linear classifi-
cation problem by noting that:
w ? x
i
< w ? x
j
?? w ? (x
i
? x
j
) < 0
In order to obtain probability values for the or-
dering, we use logistic regression as the linear
model. It therefore holds that:
P(x
i
? x
j
;w) =
1
1 + exp(?w ? (x
i
? x
j
))
While logistic regression usually fits an inter-
cept term, in our case, because the samples consist
of differences of points, the model operates in an
affine space and therefore gains an extra effective
degree of freedom. The intercept is therefore not
needed.
The relationship between pairwise ranking and
predicting the class from an ordered set {r
1
, ...r
k
}
is given by assigning to a document x the class r
i
such that
?(r
i?1
) ? g(x) < ?(r
i
) (1)
where ? is an increasing function that does not
need to be linear. (Pedregosa et al., 2012), who
used the pairwise approach to ordinal regression
on neuroimaging prediction tasks, showed using
artificial data that ? can be accurately recovered
using non-parametric regression. In this work, we
use a parametric estimation of ? that can be used
in a probabilistic interpretation to identify the most
likely period when a text was written, as described
in section 3.3.
3.3 Probabilistic dating of uncertain texts
The ranking model described in the previous sec-
tion learns a direction along which the temporal
order of texts is preserved as much as possible.
This direction is connected to the chronological
axis through the ? function. For the years t for
18
which we have an unique attested document x
t
,
we have that
x ? x
t
?? g(x) < g(x
t
) < ?(t)
This can be explained by seeing that equation 2
gives ?(t) as an upper bound for the projections of
all texts written in year t, and by transitivity for all
previous texts as well.
Assuming we can estimate the function ? with
another function
?
?, the cumulative densitiy func-
tion of the distribution of the time when an unseen
document was written can be expressed.
P (x ? t) ?
1
1 + exp(w ? x?
?
?(t))
(2)
Setting the probability to
1
2
provides a point es-
timate of the time when x was written, and confi-
dence intervals can be found by setting it to p and
1? p.
3.4 Features
Our ranking and estimation model can work with
any kind of numerical features. For simplicity
we used lexical and naive morphological features,
pruned using ?
2
feature selection with tunable
granularity.
The lexical features are occurrence counts of all
words that appear in at least p
lex
documents. The
morphological features are counts of character n-
grams of length up to w
mph
in final positions of
words, filtered to occur in at least n
mph
documents.
Subsequently, a non-linear transformation ? is
optionally applied to the numerical features. This
is one of ?
sqrt
(z) =
?
z, ?
log
(z) = log(z) or
?
id
(z) = z (no transformation).
The feature selection step is applied before gen-
erating the pairs for classification, in order for the
?
2
scoring to be applicable. The raw target val-
ues used are year labels, but to avoid separating
almost every document in its own class, we in-
troduce a granularity level that transforms the la-
bels into groups of n
gran
years. For example, if
n
gran
= 10 then the features will be scored ac-
cording to how well they predict the decade a doc-
ument was written in. The features in the top p
fsel
percentile are kept. Finally, C is the regulariza-
tion parameter of the logistic regression classifier,
as defined in liblinear (Fan et al., 2008).
0.2 0.4 0.6 0.8 1.00.72
0.74
0.76
0.78
0.80
0.82
0.84
RidgeRanking
0.6 0.7 0.8 0.9 1.00.78
0.79
0.80
0.81
0.82
0.83
RidgeRanking
Figure 1: Learning curves for English (top) and
Portuguese (bottom). Proportion of training set
used versus score.
4 Results
Each corpus is split randomly into training and test
sets with equal number of documents. The best
feature set is chosen by 3-fold cross-validated ran-
dom search over a large grid of possible configu-
rations. We use random search to allow for a more
efficient exploration of the parameter space, given
that some parameters have much less impact to the
final score than others.
The evaluation metric we used is the percentage
of non-inverted (correctly ordered) pairs, follow-
ing (Pedregosa et al., 2012).
We compare the pairwise logistic approach to
a ridge regression on the same feature set, and
two multiclass SVMs, at century and decade level.
While the results are comparable with a slight ad-
vantage in favour of ranking, the pairwise ranking
system has several advantages. On the one hand, it
provides the probabilistic interpretation described
in section 3.3. On the other hand, the model can
naturally handle noisy, uncertain or wide-range la-
bels, because annotating whether a text was writ-
ten before another can be done even when the texts
do not correspond to punctual moments in time.
While we do not exploit this advantage, it can lead
to more robust models of temporal evolution. The
learning curves in Figure 1 further show that the
pairwise approach can better exploit more data and
nonlinearity.
The implementation is based on the scikit-learn
machine learning library for Python (Pedregosa et
al., 2011) with logistic regression solver from (Fan
et al., 2008). The source code will be available.
4.1 Uncertain texts
We present an example of using the method from
Section 3.3 to estimate the date of uncertain, held-
out texts of historical interest. Figure 2 shows the
process used for estimating ? as a linear, and in
the case of Portuguese, quadratic function. The
19
size p
lex
n
mph
w
mph
? n
gran
p
fsel
C score ridge century decade MAE
en 293 0.9 0 3 ?
log
100 0.15 2
9
0.838 0.837 0.751 0.813 22.8
pt 87 0.9 25 4 ?
sqrt
5 0.25 2
?5
0.829 0.819 0.712 0.620 58.7
ro 42 0.8 0 4 ?
log
5 0.10 2
28
0.929 0.924 0.855 0.792 28.8
Table 1: Test results of the system on the three datasets. The score is the proportion of pairs of docu-
ments ranked correctly. The column ridge is a linear regression model used for ranking, while century
and decade are linear SVMs used to predict the century and the decade of each text, but scored as pair-
wise ranking, for comparability. Chance level is 0.5. MAE is the mean absolute error in years. The
hyperparameters are described in section 3.4.
1650 1700 1750 1800 1850 1900 1950Year
300
200
100
0
100
200
w?x
Linear (33.54)TrainTest
1400 1500 1600 1700 1800 1900 2000 2100Year
40
20
0
20
40
60
w?x
Linear (17.27)Quadratic (15.44)TrainTest
1400 1500 1600 1700 1800 1900 2000 2100Year
100
50
0
50
100
w?x
Linear (1.87)TrainTest
Figure 2: Estimating the function ? that defines the relationship between years and projections of docu-
ments to the direction of the model, for English, Portuguese and Romanian (left to right). In parantheses,
the normalized residual of the least squares fit is reported on the test set.
154
0
156
0
158
0
160
0
162
0
164
0
166
0
168
0
170
0
172
0
174
0
176
0
178
0
180
0
182
0
184
0
186
0
188
0
190
0
192
0
194
0
196
0
198
0
200
0
202
00.2
0.00.2
0.40.6
0.81.0
1.2
Figure 3: Visualisation of the probability esti-
mation for the dating of C. Cantacuzino?s Isto-
ria T
,
?arii Rum?anes
,
ti. The horizontal axis is the
time, the points are known texts with a height
equal to the probability predicted by the classifier.
The dashed line is the estimated probability from
Equation 2.
estimation is refit on all certain documents prior to
plugging into the probability estimation.
The document we use to demonstrate the pro-
cess is Romanian nobleman and historian Con-
stantin Cantacuzino?s Istoria T
,
?arii Rum?anes
,
ti.
The work is believed to be written in 1716, the
year of the author?s death, and published in sev-
eral editions over a century later (Stahl, 2001).
This is an example of the system being reasonably
close to the hypothesis, thus providing linguistic
support to it. Our system gives an estimated dat-
ing of 1744.7 with a 90% confidence interval of
1736.2 ? 1753.2. As publications were signifi-
cantly later, the lexical pull towards the end of 18
th
century that can be observed in Figure 3 could be
driven by possible editing of the original text.
5 Conclusion
We propose a ranking approach to temporal mod-
elling of historical texts. We show how the model
can be used to produce reasonable probabilistic
estimates of the linguistic age of a text, using a
very basic, fully-automatic feature extraction step
and no linguistic or historical knowledge injected,
apart from the labels, which are possibly noisy.
Label noise can be atenuated by replacing un-
certain dates with intervals that are more certain,
and only generating training pairs out of non-
overlapping intervals. This can lead to a more
robust model and can use more data than would
be possible with a regression or classification ap-
proach. The problem of potential edits that a text
has suffered still remains open.
Finally, better engineered and linguistically-
motivated features, such as syntactic, morphologi-
cal or phonetic patterns that are known or believed
to mark epochs in the evolution of a language, can
be plugged in with no change to the fundamental
method.
20
References
H. Abe and S. Tsumoto. 2010. Text categorization
with considering temporal patterns of term usages.
In Proceedings of ICDM Workshops, pages 800?
807. IEEE.
A. Ciobanu, A. Dinu, L. Dinu, V. Niculae, and
O. Sulea. 2013. Temporal text classification for
romanian novels set in the past. In Proceedings of
RANLP2013, Hissar, Bulgaria.
W. Dakka and C. Gravana. 2010. Answering gen-
eral time-sensitive queries. IEEE Transactions on
Knowledge and Data Engineering.
A. Dalli and Y. Wilks. 2006. Automatic dating of doc-
uments and temporal text classification. In Proceed-
ings of the Workshop on Annotating and Reasoning
about Time and Events, pages 17?22, Sidney, Aus-
tralia.
F. de Jong, H. Rode, and D. Hiemstra. 2005. Temporal
language models for the disclosure of historical text.
In Proceedings of AHC 2005 (History and Comput-
ing).
H. de Smet. 2005. A corpus of late modern english.
ICAME-Journal.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
N. Kanhabua and P. N?rv?ag. 2009. Using tem-
poral language models for document dating. In
ECML/PKDD, pages 738?741.
W. Kraaij. 2004. Variations on language modeling
for information retrieval. Ph.D. thesis, University
of Twente.
A. Kumar, M. Lease, and J. Baldridge. 2011. Super-
vised language modelling for temporal resolution of
texts. In Proceedings of CIKM11 of the 20th ACM
international conference on Information and knowl-
edge management, pages 2069?2072.
R. Mihalcea and V. Nastase. 2012. Word epoch dis-
ambiguation: Finding how words change over time.
In Proceedings of ACL, pages 259?263. Association
for Computational Linguistics.
S. Mokhov. 2010. A marf approach to deft2010. In
Proceedings of TALN2010, Montreal, Canada.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Fabian Pedregosa, Alexandre Gramfort, Ga?el Varo-
quaux, Elodie Cauvet, Christophe Pallier, and
Bertrand Thirion. 2012. Learning to rank from
medical imaging data. CoRR, abs/1207.3598.
H.H. Stahl. 2001. G?anditori s?i curente de istorie
social?a rom?aneasc?a. Biblioteca Institutului Social
Rom?an. Ed. Univ. din Bucures?ti.
S.
?
Stajner and M. Zampieri. 2013. Stylistic changes
for temporal text classification. In Proceedings of
the 16th International Conference on Text Speech
and Dialogue (TSD2013), Lecture Notes in Artificial
Intelligence (LNAI), pages 519?526, Pilsen, Czech
Republic. Springer.
D. Trieschnigg, D. Hiemstra, M. Theune, F. de Jong,
and T. Meder. 2012. An exploration of lan-
guage identification techniques for the dutch folktale
database. In Proceedings of LREC2012.
D. Wijaya and R. Yeniterzi. 2011. Understanding se-
mantic change of words over centuries. In Proc. of
the Workshop on Detecting and Exploiting Cultural
Diversity on the Social Web (DETECT).
M. Zampieri and M. Becker. 2013. Colonia: Corpus of
historical portuguese. ZSM Studien, Special Volume
on Non-Standard Data Sources in Corpus-Based Re-
search, 5.
21
Proceedings of the ACL Student Research Workshop, pages 89?95,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Computational considerations of comparisons and similes
Vlad Niculae
University of Wolverhampton
vlad@vene.ro
Victoria Yaneva
University of Wolverhampton
v.yaneva@wlv.ac.uk
Abstract
This paper presents work in progress to-
wards automatic recognition and classifi-
cation of comparisons and similes.
Among possible applications, we discuss
the place of this task in text simplifica-
tion for readers with Autism Spectrum
Disorders (ASD), who are known to have
deficits in comprehending figurative lan-
guage.
We propose an approach to comparison
recognition through the use of syntactic
patterns. Keeping in mind the require-
ments of autistic readers, we discuss the
properties relevant for distinguishing se-
mantic criteria like figurativeness and ab-
stractness.
1 Introduction
Comparisons are phrases that express the likeness
of two entities. They rely on specific patterns that
make them recognisable. The most obvious pat-
tern, be like , is illustrated by the following
example, but many subtler ways of building com-
parisons exist:
?He was like his father, except he had
a crooked nose and his ears were a little
lopsided.? (In ?Black cat? by Alex Krill)
Similes are a subset of comparisons. The simile
is a figure of speech that builds on a comparison in
order to exploit certain attributes of an entity in a
striking manner. According to the Oxford English
Dictionary, what sets a simile apart from a com-
parison is that it compares ?one thing with another
thing of a different kind?1.
1?simile, n. a figure of speech involving the comparison
of one thing with another thing of a different kind, used to
make a description more emphatic or vivid (e.g. as brave as a
lion)? OED Online. June 2004. Oxford University Press. 06
February 2013 http://dictionary.oed.com/.
A popular example by Charles Dickens is:
?Mrs. Cratchit entered: flushed, but
smiling proudly: with the pudding, like
a speckled cannon-ball, so hard and
firm, (...)? (In ?A Christmas Carol? by
Charles Dickens)
The comparison between a Christmas pudding
and a cannon-ball is so unexpected, as delicious
deserts are not conventionally associated with
cannon-balls (or any kind of metal objects), that
the author needs to clarify the resemblance by
adding ?so hard and firm? right after the sim-
ile. Intuitively, the OED definition is confirmed
by these two examples: a Christmas pudding
and a cannon-ball are things of different kinds,
whereas he and his father are things of the same
kind (namely, human males). As we shall see,
the borderline which divides some similes and
fixed expressions is the degree of conventional-
ity. Many other phrases used by Dickens in ?A
Christmas Carol? also link two notions of differ-
ent kinds: Old Marley was ?as dead as a doornail?
and Scrooge was ?as hard as flint? and ?as soli-
tary as an oyster?. In these cases, however, the
link between the two entities is a pattern repeated
so many times that it has consequently lost its in-
novativeness and turned into a dead metaphor (?as
dead as a doornail?) or a conventional simile (sec-
tions 4.1, 5.4.2).
The scholarly discussion of the simile has been
controversial, especially with respect to its rela-
tive, the metaphor. The two were regarded as very
close by Aristotle?s Rhetoric: ?The simile, also, is
a metaphor, the difference is but slight? (Aristote-
les and Cooper, 1932). However, modern research
has largely focused on metaphor, while the sim-
ile suffered a defiguration, described and argued
against by Bethlehem (1996): in order to support
the idea that the metaphor embodies the essence of
figurativeness, the simile was gradually stripped of
89
its status as figure of speech.
Metaphor is defined as ?a word or phrase ap-
plied to an object or action to which it is not liter-
ally applicable?2.
In other words, a metaphor links features of ob-
jects or events from two different, often incompat-
ible domains, thus being a ?realization of a cross-
domain conceptual mapping? (Deignan, 2005).
We are interested in the parallel between similes
and metaphors insofar as it points to an overlap.
There are types of similes that can be transformed
into equivalent metaphors, and certain metaphors
can be rewritten as similes, but neither set is in-
cluded in the other. This view is supported by
corpus evidence (Hanks, 2012) and contradicts re-
ductionist defiguration point of view, in a way that
Israel et al (2004) suggest: some metaphors ex-
press things that cannot be expressed by similes,
and vice versa.
In computational linguistics, similes have been
neglected in favour of metaphor even more than
in linguistics3 , despite the fact that comparisons
have a structure that makes them rather amenable
to automated processing. In sections 2 we discuss
one motivation for studying comparisons and sim-
iles: their simplification to language better suited
for people with ASD. Section 3 reviews related
work on figurative language in NLP. In section 4
we present the structure of comparisons and some
associated patterns, emphasising the difficulties
posed by the flexibility of language. Section 5
describes computational approaches to the tasks,
along with results from preliminary experiments
supporting our ideas. The study is wrapped up and
future work is presented in section 6.
2 Autism and simile comprehension
2.1 Autism and figurative language
Highly abstract or figurative metaphors and sim-
iles may be problematic for certain groups of
language users amongst which are people with
different types of acquired language disorders
(aphasias) or developmental ones like ASD. As
a result of impairment in communication, social
interaction and behaviour, ASD are characterised
2?metaphor, n.? OED Online. June 2004. Oxford Univer-
sity Press. 06 February 2013 http://dictionary.oed.com/
3A Google Scholar search for papers containing the word
linguistic have the word metaphor in the title approximately
5000 times, but simile only around 645 times. In the ACL
anthology, metaphor occurs around 1070 times while simile
occurs 52 times.
by atypical information processing in diverse areas
of cognition (Skoyles, 2011). People with autism,
especially if they are children, experience disturb-
ing confusion when confronted with figurative lan-
guage. Happe? (1995) describes:
A request to ?Stick your coat down over
there? is met by a serious request for
glue. Ask if she will ?give you a hand?,
and she will answer that she needs to
keep both hands and cannot cut one off
to give to you. Tell him that his sister
is ?crying her eyes out? and he will look
anxiously on the floor for her eye-balls...
The decreased ability of autistic people to un-
derstand metaphors and figurative language as a
whole (Rundblad and Annaz, 2010; MacKay and
Shaw, 2004; Happe?, 1995), could be seen as an ob-
stacle in communication, given that we all ?think
in metaphors? and a language system is ?figura-
tive in its nature? (Lakoff and Johson, 1980). The
growing demand to overcome this barrier has led
to the investigation of possible ways in which NLP
can detect and simplify non-literal expressions in
a text.
2.2 Comprehending similes
People with ASD4 show almost no impairment
in comprehending those similes which have lit-
eral meaning (Happe?, 1995). This relative ease in
processing is probably due to the fact that similes
contain explicit markers (e.g. like and as), which
evoke comparison between two things in a certain
aspect.
With regard to understanding figurative similes,
Hobson (2012) describes in the case of fifteen-
year-old L.: ?He could neither grasp nor formulate
similarities, differences or absurdities, nor could
he understand metaphor?.
Theoretically, one of the most obvious markers
of similes, the word like, could be a source of a
lot of misinterpretations. For example, like could
be a verb, a noun, or a preposition, depending on
the context. Given that autistic people have prob-
lems understanding context (Skoyles, 2011), how
would an autistic reader perceive the role of like
in a more elaborate and ambiguous comparison?
Another possible linguistic reason for the impaired
understanding of similes might be that like is used
4With level of cognitive ability corresponding to at least
first level of Theory of Mind (Baron-Cohen et al, 1985)
90
ambiguously in many expressions which are nei-
ther similes nor comparisons, such as I feel like an
ice cream or I feel like something is wrong.
Even if the expression does not include such
an ambiguous use of like, there are other cases
in which a person with autism might be misled.
For example, if the simile is highly figurative or
abstract, it may be completely incomprehensible
for people with ASD (e.g. the conventional Love is
like a flame). A step forward towards the simpli-
fication of such expressions is their identification
and filtering of the ones that are not problematic.
Through manipulations, the difficult aspects such
as abstractness, figurativeness, and ambiguity can
be attenuated.
3 Relevant literature
Comprehensive theoretical investigations into the
expressive power of similes can be found in
(Bethlehem, 1996) and (Israel et al, 2004).
Weiner (1984) applies ontologies to discriminate
simple literal and figurative comparisons (loosely
using the term metaphor to refer to what we call
the intersection of similes and metaphors).
Most of the recent computational linguistics re-
search involving similes comes from Veale. In
(Veale and Hao, 2008), the pattern as as is ex-
ploited to mine salient and stereotypical properties
of entities using the Google search engine. A sim-
ilar process has been applied to both English and
Chinese by Li et al (2012). The Metaphor Mag-
net system presented in (Veale and Li, 2012) sup-
ports queries against a rich ontology of metaphor-
ical meanings and affects using the same simple
simile patterns. The Jigsaw Bard (Veale and Hao,
2011) is a thesaurus driven by figurative conven-
tional similes extracted from the Google Ngram
corpus.
The role played by figurative language in the
field of text simplification has not been extensively
studied outside of a few recent publications (Tem-
nikova, 2012; S?tajner et al, 2012).
4 Anatomy of a comparison
4.1 Conventionality: norms and exploitations
The theory of norms and exploitations (Hanks,
2013) describes language norms as ?a pattern of
ordinary usage in everyday language with which
a particular meaning or implicature is associated?
and argues that norms can be exploited in differ-
ent ways in order to ?say new things or to say old
things in new and interesting ways?. This distinc-
tion can be applied to similes: as slow as a snail
is a conventional simile that evokes strong asso-
ciation between slowness and snails. On the con-
trary, in she looked like a cross between a Christ-
mas tree and an American footballer (example
adapted from the British National Corpus, hence-
forth BNC) a person (the topic) is not convention-
ally associated with a Christmas tree (the vehicle),
let alne if it is crossed with a football player. In
this example the vehicle is not merely unexpected,
it also does not exist as a common pattern, and can,
by itself, create amazement.
Though figures of speech are good ways to ex-
ploit norms, figurative language can become con-
ventional, and an exploitation can be literal (e.g.
word creation, ellipsis).
The border between conventionality and cre-
ativeness is fuzzy and heuristics such as the ones
proposed in (Deignan, 2005) can only approxi-
mate it. Possible alternative methods are discussed
in section 5.4.2.
4.2 Syntactic structure
The breadth of comparisons and similes hasn?t
been extensively studied, so there is no surprise
in the small amount of coverage in computational
linguistics research on the subject. In order to de-
velop a solid foundation for working with complex
comparisons, we will follow and argue for the ter-
minology from (Hanks, 2012), where the structure
of a simile is analysed. The same structure applies
to comparisons, since as we have said, all simi-
les are comparisons and they are indistinguishable
syntactically. The constituents of a comparison
are:
? T : the topic, sometimes called tenor: it is
usually a noun phrase and acts as logical sub-
ject.
? E: the eventuality (event or state): usually a
verb, it sets the frame for the observation of
the common property.
? P : the shared property or ground: it ex-
presses what the two entities have in com-
mon.
? C: the comparator: commonly a preposition
(like or part of an adjectival phrase (better
than), it is the trigger word or phrase that
marks the presence of a comparison.
91
VG
E = VB
HE
AD
T
SBJ
PP
IN
C ? { like, as }
HE
AD
V
OBJ
COMP/ADV
(a) Basic comparison pattern. Matches he
eats like a pig and it is seen as a release.
VG
E = VB
HE
AD
T
SBJ
ADJP
IN
C = as
ADV
JJ
P
HEAD
SBAR
IN
C = as
SU
BO
RD
V = S
SENT
DGCOMP
PRD
(b) Explicit comparison with double as.
Matches expressions like it?s as easy as pie.
Figure 1: GLARF-style representation of two basic comparison patterns.
? V : the vehicle: it is the object of the compar-
ison and is also usually a noun phrase.
An example (adapted from the BNC) of a simile
involving all of the above would be:
[He T ] [looked E] [like C] [a broiled
frog V ], [hunched P ] over his desk, grin-
ning and satisfied.
The order of the elements is flexible.
Fishelov (1993) attributes this reordering to
poetic simile, along with other deviations from
the norm that he defines as non-poetic simile.
We note, in agreement with Bethlehem (1996),
that the distinction is rendered less useful when
the focus is on the vague notion of poeticality.
Fishelov even suggested that poetic similes can be
found outside of poetic text, and vice versa. We
will therefore focus on exploitations that change
the meaning.
More often than not, the property is left for the
reader to deduce:
[His mouth T ] [tasted E] [like C] [the
bottom of a parrot?s cage V ]
But even when all elements appear, the compar-
ison may be ambiguous, as lexical choice in P and
in E lead to various degrees of specificity. For
example replacing the word tasted, which forms
the E in the example above, with the more gen-
eral predicator is, results in a simile that might
have the same meaning, but is more difficult to
decode. On the other hand, the whole V phrase
the bottom of a parrot?s cage, which is an eu-
phemistic metonymy, could be substituted with
its concrete, literal meaning thus transforming the
creative simile into what might be a conventional
pattern. Nested figures of speech can also occur at
this level, for example the insertion of a metaphor-
ical and synesthetic P : it tasted [dirty P ], like a
parrot?s cage.
We consider the eventuality E as the syntac-
tic core of the comparison structure. Despite the
apparently superior importance of the comparator,
which acts as a trigger word, the event acts as a
predicator, attracting to it the entire structure in
the form of a set of arguments. This observation
is missing from the work of Fishelov (1993) and
Bethlehem (1996), who lump the event together
with either P or T . In terms of meaning, the two
constituents are of course tightly connected, but
to computationally identify the components, their
separation is important.
Roncero (2006) pointed out that for certain
common similes (e.g. love is like a rose) found
on the Internet, it is likely that an explanation of
the shared property follows, whereas for all topic-
vehicle pairs studied, the corresponding metaphor
is less often explained. However, these simpler
similes form a special case, as most similes can-
not be made into metaphors (Hanks, 2012).
92
4.3 Comparisons without like
Hanks (2012) observes that there are plenty of
other ways to make a simile in addition to using
like or as. Most definitions of similes indeed claim
that there are more possible comparators, but ex-
amples are elusive.
Israel et al (2004) point out that any construc-
tion that can make a comparison can be used to
make a simile. This is a crucial point given the
amount of flexibility available for such construc-
tions. An example they give is:
[The retirement of Yves Saint Laurent
T ] [is E] [the fashion equivalent C] of
[the breakup of the Beatles V ]. (heard
on the National Public Radio)
We can see that it is possible for the comparator
to be informative and not just an empty marker, in
this case marking the domain (fashion) to which
the topic refers to.
5 Approaches proposed
5.1 Overview
Simplifying creative language involves under-
standing. The task of understanding similes may
be hard to achieve. We will not just write about
the components we have already developed (the
pattern matching), but also present a broader plan.
At a coarse scale, the process breaks down into a
syntactic recognition step and a semantic step that
could be called entailment. The goal is to find out
what is being said about the topic. Often similes
claim that a property is present or absent, but this
is not always the case.
5.2 Dataset
At the moment there is no available dataset for
comparison and simile recognition and classifica-
tion. We have begun our investigation and de-
veloped the patterns on a toy dataset consisting
of the examples from (Hanks, 2005), which are
comparisons, similes and other ambiguous uses
of the preposition like extracted from the BNC.
We also evaluated the system on around 500 sen-
tences containing like and as from the BNC and
the VUAMC5. The latter features some marking
of trigger words, but we chose to score manually
in order to assess the relevance of the annotation.
5VU Amsterdam Metaphor Corpus (Steen et al, 2010),
available at www.metaphorlab.vu.nl
5.3 Recognizing comparisons and similes
5.3.1 Comparison pattern matching
We have seen that similes are a subset of compar-
isons and follow comparison structures. A good
consequence is that they follow syntactic patterns
that can be recognised. We have used GLARF
(Meyers et al, 2001), an argument representation
framework built on the output of the BLLIP parser.
It enhances the constituency-based parse tree with
additional roles and arguments by applying rules
and resources like Propbank. The like and as com-
parators form the GLARF-style patterns shown in
figure 1. The matching process iterates over all
nodes with arguments, principally verbs and nom-
inalisations. If the subtree rooted under it matches
certain filters, then we assign to the root the role
of E and the arguments can fill the other slots.
We evaluated the process on the small devel-
opment set as well as on the larger set of lexi-
cal matches described above. The results are pre-
sented in table 1. The mistakes on the develop-
ment set, as well as many on the other corpus, are
caused by slightly different patterns (e.g. he didn?t
look much like a doctor). This can be addressed
by adjustment or through automatic discovery of
patterns. Expressions like in hold your hands like
this are mistaken as comparisons. Ad hoc set
constructions are mostly correctly unmatched (e.g.
big earners like doctors and airline pilots but in-
correctly matches semantically ambiguous uses of
feel like.
On the lexical matches of as, the behaviour is
different as the word seems much less likely to be
a trigger. Most errors are therefore returning spu-
rious matches, as opposed to like, where most er-
rors are omissions This suggests that each trigger
word behaves differently, and therefore robustness
across patterns is important.
Overall, our method handles typical compar-
isons in short sentences rather well. Complex or
long sentences sometimes cause T and V to be in-
completely identified, or sometimes the parse to
fail. This suggests that deep syntactic parsing is a
limitation of the approach.
5.3.2 Discovering new patterns
Using a seed-based semi-supervised iterative pro-
cess, we plan to identify most of the frequent
structures used to build conventional comparisons.
We expect that, in addition to idiomatic expres-
sions, some T -V pairs often compared to each
93
full part none
comparison 24 5 4
not comparison 1 1 5
(a) Counts of 40 examples with like from the
development set in (Hanks, 2005). Partial match
P = 94%, R = 88%.
full part none
0.17 0.07 0.33
0.05 0.05 0.33
(b) Proportions of 410
examples with like from BNC
and VUAMC. Partial match
P = 70.5%, R = 41.7%
full part none
0.11 0.05 0.09
0.26 0.11 0.39
(c) Proportions of 376
examples with as from BNC
and VUAMC. Partial match
P = 29.6%, R = 64.8%
Table 1: Confusion matrices and precision/recall scores for comparison identification. Full matching is
when the heads of T,E, V and C are correctly identified, while partial is if only some of them are.
other with the like pattern will occur in other syn-
tactical patterns or lexical collocations.
5.4 Semantic aspects
5.4.1 Classifying comparisons
The phrases that match patterns like the ones de-
scribed are not necessarily comparisons. Due to
ambiguities, sentences such as I feel like an ice
cream are indistinguishable from comparisons in
our model.
Another aspect we would like to distinguish is
whether an instance of a pattern is a simile or not.
We plan to tackle this using machine learning. Se-
mantic features from an ontology like the one used
in PDEV6, or a more comprehensive work such as
WordNet7, can carry the information whether T
and V belong to similar semantic categories. We
expect other information, such as distributional
and distributed word vector representations, to be
of use.
5.4.2 Conventional similes
It may also be of interest to decide whether an in-
stance is conventional or creative. This can be im-
plemented by measuring corpus frequencies. In-
stead of looking for perfect matches, patterns can
be applied to simply count how many times some-
thing is compared to a V , regardless of the specific
syntax used8.
5.4.3 Simplification
The goal of text simplification is to generate syn-
tactically well-formed language9 that is easier to
6http://deb.fi.muni.cz/pdev/
7http://wordnet.princeton.edu/
8Care must be taken to avoid contradictions from ex-
ploitations: The aircraft is like a rock or is built like a rock
seems like a conventional simile, but The aircraft would gen-
tly skip like a rock and then settle down on the surface of the
ocean (Example from the BNC) is unconventional.
9Especially for ASD readers, who are very sensitive to
language mistakes to the point that it completely distracts
them from the meaning.
understand than the original phrase.
A comparison can be formalized as predicate
E(T ;P ). We can think of his mouth tasted like
the bottom of a parrot?s cage as a way to express
taste(his mouth; very bad). There is more than
one way to build such an encoding.
The task reduces to the generation a simple
phrase of the form T ?E?P ?, by simplifying the
elements of the representation above. Useful re-
sources are corpus occurrence counts of related
phrases, word similarity and relatedness, and con-
ventional associations.
6 Conclusions and future work
The problem of automatic identification of similes
has its place in the paradigm of text simplification
for people with language impairments. In particu-
lar, people with ASD have difficulties understand-
ing figurative language.
We applied the idea of comparison patterns to
match subtrees of an enhanced parse tree to eas-
ily match comparison structures and their con-
stituents. This lead us to investigate corpus-driven
mining of new comparison patterns, to go beyond
like and as.
We are working on semi-automatically develop-
ing a dataset of comparisons and ambiguous non-
comparisons, labelled with the interesting proper-
ties and with a focus on pattern variety and am-
biguous cases. This will be useful for evaluat-
ing our system at a proper scale. We plan to per-
form extrinsic evaluation with respect to tasks like
text simplification, textual entailment and machine
translation.
Acknowledgements
The research described in this paper was partially
funded by the European Commission through the
FIRST project (FP7-287607) and partially by the
BCROCE project.
94
References
Aristoteles and Lane Cooper. 1932. The rhetoric of
Aristotle. Appleton.
Simon Baron-Cohen, Alan M Leslie, and Uta Frith.
1985. Does the autistic child have a ?theory of
mind?? Cognition, 21(1):37?46.
Louise Shabat Bethlehem. 1996. Simile and figurative
language. Poetics Today, v17(n2):p203(38). table.
Alice Deignan. 2005. Metaphor and Corpus Linguis-
tics. Converging Evidence in Language and Com-
munication Research Series. John Benjamins.
David Fishelov. 1993. Poetic and non-poetic simile:
Structure, semantics, rhetoric. Poetics Today, pages
1?23.
Patrick Hanks. 2005. Similes and Sets: The English
Preposition ?like?. In R. Blatna? and V. Petkevic, ed-
itors, Languages and Linguistics: Festschrift for Fr.
Cermak.
Patrick Hanks. 2012. The Roles and Structure of Com-
parisons, Similes, and Metaphors in Natural Lan-
guage (An Analogical System). In Presented at the
Stockholm Metaphor Festival, September 6-8.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Francesca G. E. Happe?. 1995. Understanding minds
and metaphors: Insights from the study of figurative
language in autism. Metaphor and Symbolic Activ-
ity, 10(4):275?295.
R. Peter Hobson. 2012. Autism, literal language and
concrete thinking: Some developmental considera-
tions. Metaphor and Symbol, 27(1):4?21.
Michael Israel, Jennifer Riddle Harding, and Vera To-
bin. 2004. On simile. Language, Culture, and
Mind. CSLI Publications.
George Lakoff and Mark Johson. 1980. Metaphors We
Live By. Ca?tedra.
Bin Li, Jiajun Chen, and Yingjie Zhang. 2012. Web
based collection and comparison of cognitive prop-
erties in english and chinese. In Proceedings of
the Joint Workshop on Automatic Knowledge Base
Construction and Web-scale Knowledge Extraction,
AKBC-WEKEX ?12, pages 31?34, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Gilbert MacKay and Adrienne Shaw. 2004. A com-
parative study of figurative language in children with
autistic spectrum disorders. Child Language Teach-
ing and Therapy, 20(1):13?32.
Adam Meyers, Ralph Grishman, Michiko Kosaka, and
Shubin Zhao. 2001. Covering treebanks with glarf.
In Proceedings of the ACL 2001 Workshop on Shar-
ing Tools and Resources - Volume 15, STAR ?01,
pages 51?58, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Carlos Roncero, John M. Kennedy, and Ron Smyth.
2006. Similes on the internet have explanations.
Psychonomic Bulletin & Review, 13:74?77.
Gabriella Rundblad and Dagmara Annaz. 2010. The
atypical development of metaphor and metonymy
comprehension in children with autism. Autism,
14(1):29?46.
John R Skoyles. 2011. Autism, context/noncontext
information processing, and atypical development.
Autism research and treatment, 2011.
G.J. Steen, A.G. Dorst, and J.B. Herrmann. 2010.
A Method for Linguistic Metaphor Identification:
From Mip to Mipvu. Converging evidence in lan-
guage and communication research. Benjamins.
Irina Temnikova. 2012. Text Complexity and Text Sim-
plification in the Crisis Management domain. Ph.D.
thesis, University of Wolverhampton, Wolverhamp-
ton, UK, May.
Tony Veale and Yanfen Hao. 2008. A context-sensitive
framework for lexical ontologies. Knowledge Eng.
Review, 23(1):101?115.
Tony Veale and Yanfen Hao. 2011. Exploiting ready-
mades in linguistic creativity: A system demonstra-
tion of the jigsaw bard. In ACL (System Demonstra-
tions), pages 14?19. The Association for Computer
Linguistics.
Tony Veale and Guofu Li. 2012. Specifying viewpoint
and information need with affective metaphors: A
system demonstration of the metaphor-magnet web
app/service. In ACL (System Demonstrations),
pages 7?12. The Association for Computer Linguis-
tics.
Sanja S?tajner, Richard Evans, Constantin Orasan, and
Ruslan Mitkov. 2012. What can readability mea-
sures really tell us about text complexity? In
Luz Rello and Horacio Saggion, editors, Proceed-
ings of the LREC?12 Workshop: Natural Lan-
guage Processing for Improving Textual Accessibil-
ity (NLP4ITA), page 14, Istanbul, Turkey, may. Eu-
ropean Language Resources Association (ELRA).
E. Judith Weiner. 1984. A knowledge representation
approach to understanding metaphors. Comput. Lin-
guist., 10(1):1?14, January.
95
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 72?77,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Pastiche detection based on stopword rankings. Exposing impersonators
of a Romanian writer
Liviu P. Dinu
Faculty of Mathematics
and Computer Science
University of Bucharest
ldinu@fmi.unibuc.ro
Vlad Niculae
Faculty of Mathematics
and Computer Science
University of Bucharest
vlad@vene.ro
Octavia-Maria S, ulea
Faculty of Foreign Languages
and Literatures
Faculty of Mathematics
and Computer Science
University of Bucharest
mary.octavia@gmail.com
Abstract
We applied hierarchical clustering using
Rank distance, previously used in compu-
tational stylometry, on literary texts written
by Mateiu Caragiale and a number of dif-
ferent authors who attempted to imperson-
ate Caragiale after his death, or simply to
mimic his style. Their pastiches were con-
sistently clustered opposite to the original
work, thereby confirming the performance
of the method and proposing an extension
of the method from simple authorship attri-
bution to the more complicated problem of
pastiche detection.
The novelty of our work is the use of fre-
quency rankings of stopwords as features,
showing that this idea yields good results
for pastiche detection.
1 Introduction
The postulated existence of the human stylome
has been thoroughly studied with encouraging re-
sults. The term stylome, which is currently not in
any English dictionaries, was recently defined as
a linguistic fingerprint which can be measured, is
largely unconscious, and is constant (van Halteren
et al, 2005).
Closely related to the problem of authorship
attribution lies the pastiche detection problem,
where the fundamental question is: Can the hu-
man stylome be faked in order to trick authorship
attribution methods? There are situations where
certain authors or journalists have tried to pass
their own work as written by someone else. A
similar application is in forensics, where an im-
personator is writing letters or messages and sign-
ing with someone else?s name, especially online.
It is important to note that sometimes pastiches
are not intended to deceive, but simply as an ex-
ercise in mimicking another?s style. Even in this
case, the best confirmation that the author of the
pastiche can get is if he manages to fool an au-
thorship attribution algorithm, even if the ground
truth is known and there is no real question about
it.
Marcus (1989) identifies the following four sit-
uation in which text authorship is disputed:
? A text attributed to one author seems non-
homogeneous, lacking unity, which raises
the suspicion that there may be more than
one author. If the text was originally at-
tributed to one author, one must establish
which fragments, if any, do not belong to
him, and who are their real authors.
? A text is anonymous. If the author of a text
is unknown, then based on the location, time
frame and cultural context, we can conjec-
ture who the author may be and test this hy-
pothesis.
? If based on certain circumstances, arising
from literature history, the paternity is dis-
puted between two possibilities, A and B, we
have to decide if A is preferred to B, or the
other way around.
? Based on literary history information, a text
seems to be the result of the collaboration of
two authors, an ulterior analysis should es-
tablish, for each of the two authors, their cor-
responding text fragments.
We situate ourselves in a case similar to the
third, but instead of having to choose between two
authors, we are asking whether a group of texts
were indeed written by the claimed author or by
someone else. Ideally, we would take samples au-
thored by every possible impersonator and run a
72
multi-class classifier in order to estimate the prob-
ability that the disputed work is written by them
or by the asserted author. Such a method can give
results if we know who the impersonator can be,
but most of the time that information is not avail-
able, or the number of possible impersonators is
intractabally large.
In the case of only one impersonator, the prob-
lem can simply be stated as authorship attribu-
tion with a positive or a negative answer. How-
ever, when there are a number of people sepa-
rately writing pastiches of one victim?s style, the
extra information can prove beneficial in an unsu-
pervised learning sense. In this paper we analyze
the structure induced by the Rank Distance metric
using frequencies of stopwords as features, previ-
ously applied for authorship attribution, on such
a sample space. The assumption is that trying
to fake someone else?s stylome will induce some
consistent bias so that new impersonators can be
caught using features from other pastiche authors.
2 The successors of Mateiu Caragiale
Mateiu Caragiale, one of the most important Ro-
manian novelists, died in 1936, at the age of 51,
leaving behind an unfinished novel, Sub pecetea
tainei. Some decades later, in the 70?s, a rumor
agitated the Romanian literary world: it seemed
that the ending of the novel had been found. A
few human experts agreed that the manuscript is
in concordance with Mateiu?s style, and in the
next months almost everybody talked about the
huge finding. However, it was suspicious that
the writer who claimed the discovery, Radu Al-
bala, was considered by the critics to be one of
the closest stylistic followers of Mateiu Caragiale.
When the discussions regarding the mysterious
finding reached a critical mass, Albala publically
put a stop to them, by admitting that he him-
self had written the ending as a challenge - he
wanted to see how well he could deceive the pub-
lic into thinking the text in question was written
by Mateiu himself.
Other authors attempted to write different end-
ings to the novel, but without claiming Caragiale?s
paternity, like Albala did. Around the same time,
Eugen Ba?lan also set to continue the unfinished
novel, as a stylistic exercise. He addressed a sep-
arate storyline than Albala?s. Later, Alexandru
George also attempted to finish the novel, claim-
ing that his ending is the best. Unfortunately
there is only one copy of George?s work, and we
couldn?t obtain it for this study.
In 2008, Ion Iovan published the so-called Last
Notes of Mateiu Caragiale, composed of sections
written from Iovan?s voice, and another section
in the style of a personal diary describing the life
of Mateiu Caragiale, suggesting that this is really
Caragiale?s diary. This was further strengthened
by the fact that a lot of phrases from the diary
were copied word for word from Mateiu Cara-
giale?s novels, therefore pushing the style towards
Caragiale?s. However, this was completely a work
of fiction, the diary having been admittedly imag-
ined and written by Iovan.
Another noteworthy case is the author S?tefan
Agopian. He never attempted to continue Mateiu
Caragiale?s novel, but critics consider him one of
his closest stylistic successors. Even though not
really a pastiche, we considered worth investigat-
ing how such a successor relates to the imperson-
ators.
3 Simple visual comparisons
The pioneering methods of Mendenhall (Menden-
hall, 1901) on the subject of authorship attribu-
tion, even though obsolete by today?s standards,
can be used to quickly examine at a glance the dif-
ferences between the authors, from certain points
of view. The Mendenhall plot, showing frequency
versus word length, does not give an objective cri-
terion to attribute authorship, but as an easy to cal-
culate statistic, it can motivate further research on
a specific attribution problem.
A further critique to Mendenhall?s method is
that different distributions of word length are not
necessary caused by individual stylome but rather
by the genre or the theme of the work. This can
further lead to noisy distributions in case of ver-
satile authors, whereas the stylome is supposed to
be stable.
Even so, the fact that Mateiu Caragiale?s
Mendenhall distribution has its modes consis-
tently in a different position than the others, sug-
gests that the styles are different, but it appears
that Caragiale?s successors have somewhat simi-
lar distributions. This can be seen in figure 3. In
order to evaluate the questions How different, how
similar?, and to make a more objective judgement
on authorship attribution, we resort to pairwise
distance-based methods.
73
(a) Mateiu Caragiale (b) S?tefan Agopian
(c) Radu Albala (d) Ion Iovan
Figure 1: Mendenhall plots: frequency distribution of word lengths, showing similarities between the other
authors, but differences between them and Mateiu Caragiale.
s?i ??n sa? se cu o la nu a ce mai din pe un ca? ca ma? fi care era lui fa?ra? ne pentru el ar dar
??l tot am mi ??nsa? ??ntr cum ca?nd toate al aa dupa? pa?na? deca?t ei nici numai daca? eu avea
fost le sau spre unde unei atunci mea prin ai ata?t au chiar cine iar noi sunt acum ale
are asta cel fie fiind peste aceasta? a cele face fiecare nimeni ??nca? ??ntre aceasta aceea
acest acesta acestei avut ceea ca?t da fa?cut noastra? poate acestui alte celor cineva ca?tre
lor unui alta? at?i dintre doar foarte unor va? aceste astfel avem avet?i cei ci deci este
suntem va vom vor de
Table 1: The 120 stopwords extracted as the most fre-
quent words in the corpus.
In order to speak of distances, we need to rep-
resent the samples (the novels) as points in a met-
ric space. Using the idea that stopword frequen-
cies are a significant component of the stylome,
and one that is difficult to fake (Chung and Pen-
nebaker, 2007), we first represented each work
as a vector of stopword frequencies, where the
stopwords are chosen to be the most frequent
words from all the concatenated documents. The
stopwords can be seen in table 1. Another use-
ful visualisation method is the Principal Compo-
nents Analysis, which gives us a projection from
a high-dimensional space into a low-dimensional
one, in this case in 2D. Using this stopword fre-
quency representation, the first principal compo-
nents plane looks like figure 3.
4 Distances and clustering
In (Popescu and Dinu, 2008), the use of rankings
instead of frequencies is proposed as a smoothing
method and it is shown to give good results for
computational stylometry. A ranking is simply an
ordering of items; in this case, the representation
of each document is the ranking of the stopwords
in that particular document. The fact that a spe-
cific function word has the rank 2 (is the second
most frequent word) in one text and has the rank 4
(is the fourth most frequent word) in another text
can be more directly relevant than the fact that the
respective word appears 349 times in the first text
and only 299 times in the second.
Rank distance (Dinu, 2003) is an ordinal metric
able to compare different rankings of a set of ob-
jects. In the general case, Rank distance works for
74
Figure 2: Principal components plot. Works are colour coded like in figure 3. The cluster on the left consists
only of novels by Mateiu Caragiale. Individual authors seem to form subclusters in the right cluster.
rankings where the support set is different (for ex-
ample, if a stopword would completely be missing
from a text). When this is not the case, we have
the following useful property:
A ranking of a set of n objects is a mapping
? : {1, 2, ..., n} ? {1, 2, ..., n} where ?(i) will
represent the place (rank) of the object indexed as
i such that if ?(q) < ?(p) word q is more frequent
than word p. The Rank distance in this case is
simply the distance induced by L1 norm on the
space of vector representations of permutations:
D(?1, ?2) =
n?
i=1
|?1(i)? ?2(i)| (1)
This is a distance between what is called full rank-
ings. However, in real situations, the problem of
tying arises, when two or more objects claim the
same rank (are ranked equally). For example, two
or more function words can have the same fre-
quency in a text and any ordering of them would
be arbitrary.
The Rank distance allocates to tied objects a
number which is the average of the ranks the tied
objects share. For instance, if two objects claim
the rank 2, then they will share the ranks 2 and 3
and both will receive the rank number (2+3)/2 =
2.5. In general, if k objects will claim the same
rank and the first x ranks are already used by other
objects, then they will share the ranks x + 1, x +
2, . . . , x + k and all of them will receive as rank
the number: (x+1)+(x+2)+...+(x+k)k = x +
k+1
2 .
In this case, a ranking will be no longer a permu-
tation (?(i) can be a non integer value), but the
formula (1) will remain a distance (Dinu, 2003).
Even though computationally the formula (1)
allows us to use the L1 distance we will continue
using the phrase Rank distance to refer to it, in or-
der to emphasize that we are measuring distances
between rankings of stopwords, not L1 distances
between frequency values or anything like that.
Hierarchical clustering (Duda et al, 2001) is a
bottom-up clustering method that starts with the
most specific cluster arrangement (one cluster for
each sample) and keeps joining the nearest clus-
ters, eventually stopping when reaching either a
stopping condition or the most general cluster ar-
rangement possible (one cluster containing all the
samples). When joining two clusters, there are
many possible ways to specify the distance be-
tween them. We used complete linkage: the dis-
tance between the most dissimilar points from the
two clusters. The resulting clustering path, visu-
alised a dendrogram, is shown in figure 4.
The use of clustering techniques in authorship
attribution problems has been shown useful by
Labbe? and Labbe? (2006); Luyckx et al (2006).
Hierarchical clustering with Euclidean distances
75
Figure 3: Dendrogram showing the results of hierarchical clustering using the L2 (euclidean) distance.
has been used for pastiche detection in (Somers
and Tweedie, 2003). The novelty of our work
is the use of rankings as features, and using the
L1 distance (equivalent to the Rank distance for
this particular case). (Somers and Tweedie, 2003)
shows how the Euclidean distance clusters mostly
works by the same author at the finest level, with a
few exceptions. On the data from our problem, we
observed a similar problem. The Euclidean dis-
tance behaves in a less than ideal fasion, joining
some of Agopian?s works with the cluster formed
by the other authors (see figure 3), whereas the
Rank distance always finds works by the same au-
thor the most similar at the leaves level (with the
obvious exception of Eugen Ba?lan?s text, because
it is his only available text).
Reading the dendrogram in the reverse order
(top to bottom), we see that for k = 2 clusters,
one corresponds to Mateiu Caragiale and the other
to all of his successors. In a little finer-grained
spot, there is a clear cluster of S?tefan Agopian?s
work, the (single) text by Eugen Ba?lan, and a joint
cluster with Radu Albala and Ion Iovan, which
also quickly breaks down into the separate au-
thors. The fact that there is no k for which all
authors are clearly separated in clusters can be
attributed to the large stylistic variance exhibited
by S?tefan Agopian and Mateiu Caragiale, whose
clusters break down more quickly.
These results confirm our intuition that rank-
ings of stopwords are more relevant than frequen-
cies, when an appropriate metric is used. Rank
distance is well-suited to this task. This leads us
to believe that if we go back and apply our meth-
ods to the texts studies in (Somers and Tweedie,
2003), an improvement will be seen, and we in-
tend to further look into this.
5 Conclusions
We reiterate that all of the authors used in
the study are considered stylistically similar to
Mateiu Caragiale by the critics. Some of their
works, highlighted on the graph, were either at-
tributed to Caragiale (by Albala and Iovan), or in-
tended as pastiche works continuing Caragiale?s
unfinished novel.
A key result is that with this models, all of these
successors prove to be closer to each other than to
Mateiu Caragiale. Therefore, when faced with a
new problem, we don?t have to seed the system
with many works from the possible authors (note
that we used a single text by Ba?lan): it suffices
to use as seeds texts by one or more authors who
are stylistically and culturally close to the claimed
author (in this case, Mateiu Caragiale). Cluster-
ing with an appropriate distance such as Rank dis-
76
Figure 4: Dendrogram showing the results of hierarchical clustering using L1 distance on stopword rankings
(equivalent to Rank distance).
tance will unmask the pastiche.
References
Cindy Chung and James Pennebaker. The psy-
chological functions of function words. Social
communication: Frontiers of social psychol-
ogy, pages 343?359, 2007.
Liviu Petrisor Dinu. On the classification and ag-
gregation of hierarchies with different consti-
tutive elements. Fundamenta Informaticae, 55
(1):39?50, 2003.
R. O. Duda, P. E. Hart, and D. G. Stork. Pattern
Classification (2nd ed.). Wiley-Interscience
Publication, 2001.
Cyril Labbe? and Dominique Labbe?. A tool for
literary studies: Intertextual distance and tree
classification. Literary and Linguistic Comput-
ing, 21(3):311?326, 2006.
Kim Luyckx, Walter Daelemans, and Edward
Vanhoutte. Stylogenetics: Clustering-based
stylistic analysis of literary corpora. In Pro-
ceedings of LREC-2006, the fifth International
Language Resources and Evaluation Confer-
ence, pages 30?35, 2006.
Solomon Marcus. Inventie si descoperire. Ed.
Cartea Romaneasca, Bucuresti, 1989.
T C Mendenhall. A mechanical solution of a liter-
ary problem. Popular Science Monthly, 60(2):
97?105, 1901.
Marius Popescu and Liviu Petrisor Dinu. Rank
distance as a stylistic similarity. In COLING
(Posters)?08, pages 91?94, 2008.
Harold Somers and Fiona Tweedie. Authorship
attribution and pastiche. Computers and the
Humanities, 37:407?429, 2003. ISSN 0010-
4817. 10.1023/A:1025786724466.
Hans van Halteren, R. Harald Baayen, Fiona J.
Tweedie, Marco Haverkort, and Anneke Neijt.
New machine learning methods demonstrate
the existence of a human stylome. Journal of
Quantitative Linguistics, pages 65?77, 2005.
77
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 102?106,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Temporal classification for historical Romanian texts
Alina Maria Ciobanu
Liviu P. Dinu
Octavia-Maria S, ulea
Faculty of Mathematics and Computer Science
Center for Computational Linguistics
University of Bucharest
alinamaria.ciobanu@yahoo.com
ldinu@fmi.unibuc.ro
mary.octavia@gmail.com
Anca Dinu
Faculty of Foreign Languages
University of Bucharest
anca d dinu@yahoo.com
Vlad Niculae
University of Wolverhampton
vlad@vene.ro
Abstract
In this paper we look at a task at border
of natural language processing, historical
linguistics and the study of language de-
velopment, namely that of identifying the
time when a text was written. We use
machine learning classification using lexi-
cal, word ending and dictionary-based fea-
tures, with linear support vector machines
and random forests. We find that lexical
features are the most helpful.
1 Introduction
Text dating, or determination of the time period
when it was written, proves to be a useful com-
ponent in NLP systems that can deal with such
diachronistically dynamic inputs (Moura?o et al,
2008). Besides this, the models that can perform
such classification can shine light on less than ob-
vious changes of certain features.
The knowledge captured in such systems can
prove useful in transferring modern language re-
sources and tools to historical domains (Meyer,
2011). Automatic translation systems between
and across language stages, as in the corpus in-
troduced by (Magaz, 2006), can benefit from the
identification of feature variation over time.
In this paper we study the problem of super-
vised temporal text classification across genres
and authors. The problem turns out to be solvable
to a very high degree of accuracy.
2 Related Work
The influence of the temporal effects in automatic
document classification is analyzed in (Moura?o et
al., 2008) and (Salles et al, 2010). The authors
state that a major challenge in building text clas-
sification models may be the change which occurs
in the characteristics of the documents and their
classes over time (Moura?o et al, 2008). There-
fore, in order to overcome the difficulties which
arise in automatic classification when dealing with
documents dating from different epochs, identify-
ing and accounting for document characteristics
changing over time (such as class frequency, rela-
tionships between terms and classes and the sim-
ilarity among classes over time (Moura?o et al,
2008)) is essential and can lead to a more accurate
discrimination between classes.
In (Dalli and Wilks, 2006) a method for clas-
sification of texts and documents based on their
predicted time of creation is successfully applied,
proving that accounting for word frequencies and
their variation over time is accurate. In (Kumar
et al, 2012) the authors argue as well for the ca-
pability of this method, of using words alone, to
determine the epoch in which a text was written or
the time period a document refers to.
The effectiveness of using models for individu-
als partitions in a timeline with the purpose of pre-
dicting probabilities over the timeline for new doc-
uments is investigated in (Kumar et al, 2011) and
(Kanhabua and N?rva?g, 2009). This approach,
based on the divergence between the language
model of the test document and those of the time-
line partitions, was successfully employed in pre-
dicting publication dates and in searching for web
pages and web documents.
In (de Jong et al, 2005) the authors raise the
problem of access to historical collections of doc-
uments, which may be difficult due to the differ-
ent historical and modern variants of the text, the
less standardized spelling, words ambiguities and
102
other language changes. Thus, the linking of cur-
rent word forms with their historical equivalents
and accurate dating of texts can help reduce the
temporal effects in this regard.
Recently, in (Mihalcea and Nastase, 2012), the
authors introduced the task of identifying changes
in word usage over time, disambiguating the epoch
at word-level.
3 Approach
3.1 Datasets used
In order to investigate the diachronic changes and
variations in the Romanian lexicon over time, we
used copora from five different stages in the evo-
lution of the Romanian language, from the 16th
to the 20th century. The 16th century represents
the beginning of the Romanian writing. In (Dim-
itrescu, 1994, p. 13) the author states that the mod-
ern Romanian vocabulary cannot be completely
understood without a thorough study of the texts
written in this period, which should be consid-
ered the source of the literary language used to-
day. In the 17th century, some of the most im-
portant cultural events which led to the develop-
ment of the Romanian language are the improve-
ment of the education system and the establish-
ing of several printing houses (Dimitrescu, 1994,
p. 75). According to (Lupu, 1999, p. 29), in
the 18th century a diversification of the philologi-
cal interests in Romania takes place, through writ-
ing the first Romanian-Latin bilingual lexicons,
the draft of the first monolingual dictionary, the
first Romanian grammar and the earliest transla-
tions from French. The transition to the Latin al-
phabet, which was a significant cultural achieve-
ment, is completed in the 19th century. The Cyril-
lic alphabet is maintained in Romanian writing
until around 1850, afterwards being gradually re-
placed with the Latin alphabet (Dimitrescu, 1994,
p. 270). The 19th century is marked by the conflict
(and eventually the compromise) between etymol-
ogism and phonetism in Romanian orthography.
In (Maiorescu, 1866) the author argues for apply-
ing the phonetic principle and several reforms are
enforced for this purpose. To represent this pe-
riod, we chose the journalism texts of the leading
Romanian poet Mihai Eminescu. He had a cru-
cial influence on the Romanian language and his
contribution to modern Romanian development is
highly appreciated. In the 20th century, some vari-
ations regarding the usage of diacritics in Roma-
nian orthography are noticed.
Century Corpus Nwordstype token
16
Codicele Todorescu 3,799 15,421
Codicele Martian 394 920
Coresi, Evanghelia cu ??nva?t?a?tura? 10,361 184,260
Coresi, Lucrul apostolesc 7,311 79,032
Coresi, Psaltirea slavo-roma?na? 4,897 36,172
Coresi, Ta?rgul evangheliilor 6,670 84,002
Coresi, Tetraevanghelul 3,876 36,988
Manuscrisul de la Ieud 1,414 4,362
Palia de la Ora?s?tie 6,596 62,162
Psaltirea Hurmuzaki 4,851 32,046
17
The Bible 15,437 179,639
Miron Costin, Letopiset?ul T?a?rii Moldovei 6,912 70,080
Miron Costin, De neamul moldovenilor 5,499 31,438
Grigore Ureche, Letopiset?ul T?a?rii Moldovei 5,958 55,128
Dosoftei, Viat?a si petreacerea sfint?ilor 23,111 331,363
Varlaam Motoc, Cazania 10,179 154,093
Varlaam Motoc, Ra?spunsul ??mpotriva 2,486 14,122
Catehismului calvinesc
18
Antim Ivireanul, Opere 11,519 123,221
Axinte Uricariul, Letopiset?ul T?a?rii 16,814 147,564
Roma?nesti s?i al T?a?rii Moldovei
Ioan Canta, Letopiset?ul T?a?rii Moldovei
Dimitrie Cantemir, Istoria ieroglifica? 13,972 130,310
Dimitrie Eustatievici Bras?oveanul, 5,859 45,621
Gramatica roma?neasca?
Ion Neculce, O sama? de cuvinte 9,665 137,151
19
Mihai Eminescu, Opere, v. IX 27,641 227,964
Mihai Eminescu, Opere, v. X 30,756 334,516
Mihai Eminescu, Opere, v. XI 27,316 304,526
Mihai Eminescu, Opere, v. XII 28,539 308,518
Mihai Eminescu, Opere, v. XIII 26,242 258,234
20
Eugen Barbu, Groapa 14,461 124,729
Mircea Cartarescu, Orbitor 35,486 306,541
Marin Preda, Cel mai iubit dintre pa?ma?nteni 28,503 388,278
Table 1: Romanian corpora: words
For preprocessing our corpora, we began by re-
moving words that are irrelevant for our investiga-
tion, such as numbers. We handled word bound-
aries and lower-cased all words. We computed,
for each text in our corpora, the number of words
(type and token). The results are listed in Table
1. For identifying words from our corpora in dic-
tionaries, we performed lemmatization. The in-
formation provided by the machine-readable dic-
tionary dexonline 1 regarding inflected forms al-
lowed us to identify lemmas (where no semantic
or part-of-speech ambiguities occurred) and to fur-
ther lookup the words in the dictionaries. In our
investigations based on dexonline we decided to
use the same approach as in (Mihalcea and Nas-
tase, 2012) and to account only for unambiguous
words. For example, the Romanian word ai is
morphologically ambiguous, as we identified two
corresponding lemmas: avea (verb, meaning to
have) and ai (noun, meaning garlic). The word
ama?nare is semantically ambiguous, having two
different associated lemmas, both nouns: ama?nar
(which means flint) and ama?na (which means to
postpone). We do not use the POS information di-
1http://dexonline.ro
103
rectly, but we use dictionary occurrence features
only for unambiguous words.
The database of dexonline aggregates informa-
tion from over 30 Romanian dictionaries from dif-
ferent periods, from 1929 to 2012, enabling us to
investigate the diachronic evolution of the Roma-
nian lexicon. We focused on four different sub-
features:
? words marked as obsolete in dexonline defi-
nitions (we searched for this tag in all dictio-
naries)
? words which occur in the dictionaries of ar-
chaisms (2 dictionaries)
? words which occur in the dictionaries pub-
lished before 1975 (7 dictionaries)
? words which occur in the dictionaries pub-
lished after 1975 (31 dictionaries)
As stated before, we used only unambiguous
words with respect to the part of speech, in order to
be able to uniquely identify lemmas and to extract
the relevant information. The aggregated counts
are presented in table 2.
Sub-feature 16 17 18 19 20
archaism type 1,590 2,539 2,114 1,907 2,140
token 5,652 84,804 56,807 120,257 62,035
obsolete type 5,652 8,087 7,876 9,201 8,465
token 172,367 259,367 199,899 466,489 279,654
< 1975 type 11,421 17,200 16,839 35,383 34,353
token 311,981 464,187 337,026 885,605 512,156
> 1975 type 12,028 18,948 18,945 42,855 41,643
token 323,114 480,857 356,869 943,708 541,258
Table 2: Romanian corpora: dexonline sub-
features
3.2 Classifiers and features
The texts in the corpus were split into chunks of
500 sentences in order to increase the number of
sample entries and have a more robust evaluation.
We evaluated all possible combinations of the four
feature sets available:
? lengths: average sentence length in words,
average word length in letters
? stopwords: frequency of the most common
50 words in all of the training set:
de s, i ??n a la cu au no o sa? ca? se pe
din s ca i lui am este fi l e dar pre ar
va? le al dupa? fost ??ntr ca?nd el daca?
ne n ei sau sunt
Century Precision Recall F1-score texts
16 1.00 1.00 1.00 16
17 1.00 0.88 0.94 17
18 0.88 1.00 0.93 14
19 1.00 1.00 1.00 23
20 1.00 1.00 1.00 21
average/ total 0.98 0.98 0.98 91
Table 4: Random Forest test scores using all fea-
tures and aggregating over 50 trees
? endings: frequency of all word suffixes of
length up to three, that occur at least 5 times
in the training set
? dictionary: proportion of words matching
the dexonline filters described above
The system was put together using the scikit-
learn machine learning library for Python (Pe-
dregosa et al, 2011), which provides an imple-
mentation of linear support vector machines based
on liblinear (Fan et al, 2008), an implementation
of random forests using an optimised version of
the CART algorithm.
4 Results
The hyperparameters (number of trees, in the ran-
dom forest case, and C, for the SVM) were op-
timized using 3 fold cross-validation for each of
the feature sets. For the best feature sets, denoted
with an asterisk in table 3, the test results and hy-
perparameter settings are presented in tables 4 and
5.
The results show that the nonlinear nature of
the random forest classifier is important when us-
ing feature sets so different in nature. However, a
linear SVM can perform comparably, using only
the most important features. The misclassifica-
tions that do occur are not between very distant
centuries.
5 Conclusions
We presented two classification systems, a linear
SVM one and a nonlinear random forest one, for
solving the temporal text classification problem on
Romanian texts. By far the most helpful features
turn out to be lexical, with dictionary-based histor-
ical information less helpful than expected. This is
probably due to inaccuracy and incompleteness of
104
lengths stopwords endings dictionary RF SVM
False False False False 25.38 25.38
False False False True 86.58 79.87
False False True False 98.51 95.16
False False True True 97.76 97.02
False True False False 98.51 96.27
False True False True 98.51 94.78
False True True False 98.88 *98.14
False True True True 98.51 97.77
True False False False 68.27 22.01
True False False True 92.92 23.13
True False True False 98.14 23.89
True False True True 98.50 23.14
True True False False 98.14 23.53
True True False True 98.51 25.00
True True True False 98.88 23.14
True True True True *99.25 22.75
Table 3: Cross-validation accuracies for different feature sets. The score presented is the best one over
all of the hyperparameter settings, averaged over the folds.
Century Precision Recall F1-score texts
16 1.00 1.00 1.00 16
17 1.00 1.00 1.00 17
18 1.00 0.93 0.96 14
19 1.00 1.00 1.00 23
20 0.95 1.00 0.98 21
average/ total 0.99 0.99 0.99 91
Table 5: Linear SVC test scores using only stop-
words and word endings for C = 104.
dictionary digitization, along with ambiguities that
might need to be dealt with better.
We plan to further investigate feature impor-
tances and feature selection for this task to ensure
that the classifiers do not actually fit authorship or
genre latent variables.
Acknowledgements
The authors thank the anonymous reviewers for
their helpful and constructive comments. The con-
tribution of the authors to this paper is equal. Re-
search supported by a grant of the Romanian Na-
tional Authority for Scientific Research, CNCS ?
UEFISCDI, project number PN-II-ID-PCE-2011-
3-0959.
References
Angelo Dalli and Yorick Wilks. 2006. Automatic dat-
ing of documents and temporal text classification.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, Sydney,, pages
17?-22.
Franciska de Jong, Henning Rode, and Djoerd Hiem-
stra. 2005. Temporal language models for the dis-
closure of historical text. In Humanities, computers
and cultural heritage: Proceedings of the XVIth In-
ternational Conference of the Association for His-
tory and Computing.
Florica Dimitrescu. 1994. Dinamica lexicului
roma?nesc - ieri s?i azi. Editura Logos. In Romanian.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874, June.
Nattiya Kanhabua and Kjetil N?rva?g. 2009. Using
temporal language models for document dating. In
ECML/PKDD (2), pages 738?741.
Abhimanu Kumar, Matthew Lease, and Jason
Baldridge. 2011. Supervised language modeling
for temporal resolution of texts. In CIKM, pages
2069?2072.
Abhimanu Kumar, Jason Baldridge, Matthew Lease,
and Joydeep Ghosh. 2012. Dating texts without ex-
plicit temporal cues. CoRR, abs/1211.2290.
Coman Lupu. 1999. Lexicografia roma?neasca? ??n pro-
cesul de occidentalizare latino-romanica? a limbii
roma?ne moderne. Editura Logos. In Romanian.
105
Judit Martinez Magaz. 2006. Tradi imt (xx-xxi):
Recent proposals for the alignment of a diachronic
parallel corpus. International Computer Archive of
Modern and Medieval English Journal, (30).
Titu Maiorescu. 1866. Despre scrierea limbei ruma?ne.
Edit?iunea s?i Imprimeria Societa?t?ei Junimea. In Ro-
manian.
Roland Meyer. 2011. New wine in old wineskins?
tagging old russian via annotation projection from
modern translations. Russian Linguistcs.
Rada Mihalcea and Vivi Nastase. 2012. Word epoch
disambiguation: Finding how words change over
time. In ACL (2), pages 259?263. The Association
for Computer Linguistics.
Fernando Moura?o, Leonardo Rocha, Renata Arau?jo,
Thierson Couto, Marcos Gonc?alves, and Wag-
ner Meira Jr. 2008. Understanding temporal aspects
in document classification. In WSDM ?08 Proceed-
ings of the 2008 International Conference on Web
Search and Data Mining, pages 159?170.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830, Oct.
Thiago Salles, Leonardo Rocha, Fernando Moura?o,
Gisele L. Pappa, Lucas Cunha, Marcos Gonc?alves,
and Wagner Meira Jr. 2010. Automatic document
classification temporally robust. Journal of Infor-
mation and Data Management, 1:199?211, June.
106
