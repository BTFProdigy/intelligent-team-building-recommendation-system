HOW TO INTEGRATE LINGUISTIC INFORMATION
IN FILES AND GENERATE
FEEDBACK FOR GRAMMAR ERRORS
Rodolfo Delmonte, Luminita Chiran, Ciprian Bacalu
Dipartimento di Scienze del Linguaggio
Ca' Garzoni-Moro, San Marco 3417
Universit? "Ca Foscari"
30124 - VENEZIA
Tel. 39-41-2578464/52/19 - Fax. 39-41-5287683
E-mail: delmont@unive.it - website: byron.cgm.unive.it
Abstract
We present three applications which share
some of their linguistic processor. The first
application ?FILES? ? Fully Integrated
Linguistic Environment for Syntactic and
Functional Annotation - is a fully integrated
linguistic environment for syntactic and
functional annotation of corpora currently
being used for the Italian Treebank. The
second application is a shallow parser ? the
same used in FILES ? which has been
endowed with a feedback module in order to
inform students about their grammatical
mistakes, if any, in German. Finally an LFG-
based multilingual parser simulating parsing
strategies with ambiguous sentences. We
shall present the three applications in that
sequence.
1. FILES
FILES has been used to annotate a number of
corpora of Italian within the National Project
currently still work in progress. Input to FILES is
the output of our linguistic modules for the
automatic analysis of Italian, a tokenizer, a
morphological analyser, a tagger equipped with a
statistic and syntactic disambiguator and finally a
shallow parser. All these separate modules
contribute part of the input for the system which is
then used by human annotators to operate at
syntactic level on constituent structure, or at
function level on head-features functional
representation. We don?t have here space to
describe the linguistic processors ? but see [8, 9,
10, 11, 12]. As to tag disambiguation, this is
carried out in a semi-automatic manner by the
human annotator, on the basis of the automatic
redundant morphological tagger. The
disambiguator takes each token and in case of
ambiguity it alerts the annotator to decide which is
the tag to choose: the annotator is presented with
the best candidate computed on the basis both of
syntactic and statistical information. Low level
representations are integrated in a relational
database and shown in the FILES environment
which is an intelligent browser allowing the
annotation to operate changes and create XML
output automatically for each file. Here below is a
snapshot of the six relational databases where all
previously analysed linguistic material has been
inputted. It contains tokens, lemmata, POS
tagging, empty categories, sentences containing
each token, tokens regarded as heads as
separated from tokens regarded as features and
verb subacategorization list.
Fig.1 Relational databases to be used as input for the
Syntactic and Functional Annotation
An interesting part of the browser is the
availability of subcategorization frames for verbs:
these are expressed in a compact format which
are intended to help the annotator in the most
difficult task, i.e. that of deciding whether a given
constituent head must be interpreted as either an
argument or an adjunct; and in case it is an
argument, whether it should be interpreted as
predicative or ?open? in LFG terms, or else as
non-predicative or ?close?. The list of
subcategorization frames contains 17,000 entries.
Of course the annotator can add new entries
either as new lexical items or simply as new
subcategorizations frames, which are encoded in
the current list. Notable features of the browser
are the subdivision into two separate columns of
verbal heads from non verbal ones, whereas the
actual sentence highlights all heads verbal and non
verbal in bold. On the righthand side there is a
scrollable list of relations and the possibility to
move from one sentence to another at will. Finally
the XML button to translate the contents of each
or any number of sentences into xml format.
Fig.2 Browser for Functional Annotation with
Structural representation
2. GRAMMCHECK
The second application is a Grammar Checker for
Italian students of German and English. The one
for students of English is based on GETARUNS
and uses a highly sophisticated grammar which is
however a completely separated system from the
one presented here and requires a lot more space
for its presentation ? see [13, 14]. It is available
under Internet and will be shown as such.
The one for students of German on the contrary,
is based on the shallow parser of Italian used to
produce the syntactic constituency for the
National Treebank. The output of the parser is a
bracketing of the input tagged word sequence
which is then passed to the higher functional
processor. This is an LFG-based c-structure to f-
structure mapping algorithm which has three
tasks: the first one is to compute features from
heads; the second one is to compute agreement.
The third task is to impose LFG?s grammaticality
principles: those of Coherence and Consistency,
i.e. number and type of arguments are constrained
by the lexical form of the governing predicate.
The parser is an RTN which has been endowed
with a grammar and a lexicon of German of about
8K entries. The grammar is written in the usual
arc/transition nodes formalism, well-known in
ATNs. However, the aim of the RTN is that of
producing a structured output both for wellformed
and illformed grammatical sentences of German.
To this end, we allowed the grammar to keep part
of the rules of Italian at the appropriate structural
level, though. Grammar checking is not
accomplished at the constituent structure building
level, but at the f-structure level.
2.1 THE SHALLOW PARSER
The task of the Shallow Parser is that of creating
syntactic structures which are eligible for
Grammatical Function assignment. This task is
made simpler given the fact that the disambiguator
will associate a net/constituency label to each
disambiguated tag. Parsing can then be defined as
a Bottom-Up collection of constituents which
contain either the same label, or which may be
contained in/be member of the same net/higher
constituent. No attachment is performed in order
to avoid being committed to structural decisions
which might then reveal themselves to be wrong.
We prefer to perform some readjustment
operations after structures have been built rather
than introducing errors from the start.
Readjustment operations are in line with LFG
theoretical framework which assumes that f-
structures may be recursively constituted by
subsidiary f-structures, i.e. by complements or
adjuncts of a governing predicate. So the basic
task of the shallow parser is that of building
shallow structures for each safely recognizable
constituent and then pass this information to the
following modules.
2.2 Syntactic Readjustment Rules
Syntactic structure is derived from shallow
structures by a restricted and simple set of
rewriting operations which are of two categories:
deletions, and restructuring. Here are some
examples of both:
a. Deletion
Delete structural labels internally with the same
constituent label that appears at the beginning as
in Noun Phrases, whenever a determiner is taken
in front of the head noun;
b. Restructuring
As explained above, we want to follow a policy of
noncommittal as to attachment of constituents:
nonetheless, there are a number of restructuring
operations which can be safely executed in order
to simplify the output without running the risk of
taking decisions which shall have later to be
modified.
Restructuring is executed taking advantage of
agreement information which in languages like
Italian or German, i.e. in morphologically rich
languages, can be fruitfully used to that aim. In
particular, predicative constituents may belong to
different levels of attachment from the adjacent
one. More Restructuring is done at sentence level,
in case the current sentence is a coordinate or
subordinate sentence.
3 FROM C-STRUCTURE TO F-
STRUCTURE
Before working at the Functional level we
collected 2500 grammatical mistakes taken from
real student final tests. We decided to keep trace
of the following typical grammatical mistakes:
- Lack of Agreement NP internally;
- Wrong position of Argument Clitic;
- Wrong Subject-Verb Agreement;
- Wrong position of finite Verbs both in Main,
Subordinate and Dependent clauses;
- Wrong case assignment.
Example 1. Heute willst ich mich eine bunte
Krawatte umbinden.
cp-[
savv-[avv-[heute]],
vsec-[vsupp-[willst],
       fvsec-[sogg2-[sn-[pers-[ich]]],
ogg-[sn-[clitdat-[mich]]],
ogg1-[snsempl-[art-[eine],ag-[bunte],
 n-[krawatte]]],
ibar2-[vit-[umbinden]]]
], punto-[.]]
The parser will issue two error messages:
The first one is relative to Case assignment,
?mich? is in the accusative while dative is
required. The second one is relative to Subject-
Verb agreement, ?willst? is second person
singular while the subject ?ich? is first person
singular.
As to the use of f-structure for grammar checking
the implementation we made in GETARUN ? a
complete system for text understanding, is a case
where parsing strategies are used.
This is a web-based multilingual parser which is
based mainly on LFG theory and partly on
Chomskian theories, incorporating a number of
Parsing Strategies which allow the student to
parse ambiguous sentences using the appropriate
strategy in order to obtain an adequate
grammatical output. The underlying idea was that
of stimulating the students to ascertain and test by
themselves linguistic hypotheses with a given
linguistically motivated system architecture. The
parser builds c-structure and f-structure and
computer anaphoric binding at sentence level; it
also has provision for quantifier raising and
temporal local interpretation. Predicates are
provided for all lexical categories, noun, verb,
adjective and adverb and their description is a
lexical form in the sense of LFG. It is composed
both of functional and semantic specifications for
each argument of the predicate: semantic
selection is operated by means both of thematic
role and inherent semantic features or selectional
restrictions. Moreover, in order to select adjuncts
appropriately  at each level of constituency,
semantic classes are added to more traditional
syntactic ones like transitive, unaccusative,
reflexive and so on. Semantic classes are of two
kinds: the first class is related to extensionality vs
intensionality, and is used to build discourse
relations mainly; the second class is meant to
capture aspectual restrictions which decide the
appropriateness and adequacy of adjuncts, so that
inappropriate ones are attached at a higher level.
SYSTEM ARCHITECTURE I?
Top-Down
DGC-based
Grammar Rules
Lexical Look-Up
Or 
Full Morphological
Analysis
Deterministic
Policy:
Look-ahead
WFST 
Verb Guidance From 
Subcategorization
Frames
Semantic Consistency Check
for every
Syntactic Constituent
Starting from CP level
Phrase Structure Rules
==> F-structure
check for Completeness
Coherence, UniquenessTense, Aspect and
Time Reference:
Time Relations and
Reference Interval
Quantifier Raising
Pronominal Binding at f-structure level
TABLE 1. GETARUNS PARSER
3.1 Parsing Strategies
Ambiguities dealt with by the parser go from
different binding solution of a pronoun contained in
a subordinate clause by two possible antecedents,
chosen according to semantic and pragmatic
strategies based on semantic roles and meaning
associated to the subordinating conjunction, as in
the following examples:
i.The authorities refused permission to the
demonstrators because they feared violence
ii.The authorities refused permission to the
demonstrators because they supported the
revolution
iii.The cop shot the thief because he was escaping
iv.Mario criticized Luigi because he is
hypercritical
v.Mario criticized Luigi because he ruined his
party
vi.Mario called Luigi because he needed the file
vii.The thieves stole the paintings in the museum
viii.The thieves stole the painting in the night
The underlying mechanisms for ambiguity
resolution takes one analysis as default in case it
is grammatical and the other/s plausible
interpretations are obtained by activating one of
the available strategies which are linguistically and
psychologically grounded.
From our perspective, it would seem that parsing
strategies should be differentiated according to
whether there are argument requirements or
simply semantic compatibily evaluation for
adjuncts. As soon as the main predicate or head is
parsed, it makes available all lexical information in
order to predict if possible the complement
structure, or to guide the following analysis
accordingly. As an additional remark, note that not
all possible syntactic structure can lead to
ambiguous interpretations: in other words, we
need to consider only cases which are factually
relevant also from the point of view of language
dependent ambiguities. To cope with this problem,
we built up a comprehensive taxonomy from a
syntactic point of view which takes into account
language dependent ambiguities
A. Omissibility of Complementator
? NP vs S complement
? S complement vs relative clause
B. Different levels of attachment for
Adjuncts
? VP vs NP attachment of pp
? Low vs high attachment of relative clause
C. Alternation of Lexical Forms
? NP complement vs main clause subject
D. Ambiguity at the level of lexical category
? Main clause vs reduced relative clause
? NP vs S conjunction
E. Ambiguities due to language specific
structural proprieties
? Preposition stranding
? Double Object
? Prenominal Modifiers
? Demonstrative-Complementizer Ambiguity
? Personal vs Possessive Pronoun
Here below is a snapshot of the output of the
parser for the sentence: ?The doctor called in the
son of the pretty nurse who hurt herself/himself?.
The c-structure is followed by the f-structure
representation where binding has taken place and
relative clause attachment is consequently realized
with the higher or lower NP head according to the
different agreement requirements imposed by the
two reflexive pronouns herself/himself either with
?the nurse? or with ?the son?.
From a theoretical point of view this phenomenon
is dubbed Short Binding, and is dealt with at the
same level of Grammaticality Principles, rather
than as a case of Anaphoric Binding. In this way
a failure is imposed to the parser by agreement
constraints between the reflexive pronoun and its
binder.
References
[1] P. Tapanainen and Voutilainen A.(1994),
Tagging accurately - don't guess if you know,
Proc. of ANLP '94,  pp.47-52, Stuttgart,
Germany.
[2] Brants T. & C.Samuelsson(1995), Tagging the
Teleman Corpus, in Proc.10th Nordic
Conference of Computational Linguistics,
Helsinki, 1-12.
[3] Lecomte J.(1998), Le Categoriseur Brill14-
JL5 / WinBrill-0.3, INaLF/CNRS,
[4] Chanod J.P., P.Tapanainen (1995), Tagging
French - comparing a statistical and a constraint-
based method". Proc. EACL'95, pp.149-156.
[5] Brill E. (1992), A Simple Rule-Based Part of
Speech Tagger, in Proc. 3rd Conf. ANLP, Trento,
152-155.
[6] Cutting D., Kupiec J., Pedersen J., Sibun P.,
(1992), A practical part-of-speech tagger, in Proc.
3rd Conf. ANLP, Trento.
[7] Voutilainen A. and P. Tapanainen,(1993),
Ambiguity resolution in a reductionistic parser, in
Sixth Conference of the European Chapter of
the ACL, pp. 394-403. Utrecht.
[8] Delmonte R., E.Pianta(1996), "IMMORTALE
- Analizzatore Morfologico, Tagger e
Lemmatizzatore per l'Italiano", in Atti V
Convegno AI*IA, Napoli, 19-22.
[9] Delmonte R. G.A.Mian, G.Tisato(1986), A
Grammatical Component for a Text-to-Speech
System, Proceedings of the ICASSP'86, IEEE,
Tokyo, 2407-2410.
[10] Delmonte R., R.Dolci(1989), Parsing Italian
with a Context-Free Recognizer, Annali di Ca'
Foscari XXVIII, 1-2,123-161.
[11] Delmonte R., E.Pianta(1999), Tag
Disambiguation in Italian, in Proc.Treebanks
Workshop ATALA, pp.41-49.
[12] Delmonte R.(1999), From Shallow Parsing to
Functional Structure, in Atti del Workshop AI*IA
- IRST Trento,pp.8-19.
[13] Delmonte R.(2000), Parsing with
GETARUN, Proc.TALN2000, 7? conf?rence
annuel sur le TALN,Lausanne, pp.133-146.
[14]  Delmonte R.(2000),  Generating and Parsing
Clitics with GETARUN, Proc. CLIN'99, Utrech,
pp.13-27.
Fig. 3 GETARUN parsing from user window
Text Understanding with GETARUNS for Q/A and Summarization
Rodolfo Delmonte
Department of Language Sciences
Universit? Ca? Foscari
Ca? Garzoni-Moro - San Marco 3417 - 30124 VENEZIA
e-mail: delmont@unive.it website - http://project.cgm.unive.it
Abstract
Summarization and Question Answering need
precise linguistic information with a much higher
coverage than what is being offered by currently
available statistically based systems. We assume
that the starting point of any interesting application
in these fields must necessarily be a good syntactic-
semantic parser. In this paper we present the
system for text understanding called GETARUNS,
General Text and Reference Understanding System
(Delmonte, 2003a). The heart of the system is a
rule-based top-down DCG-style parser, which uses
an LFG oriented grammar organization. The parser
produces an f-structure as a DAG which is then
used to create a Logical Form, the basis for all
further semantic representation. GETARUNS, has
a highly sophisticated linguistically based semantic
module which is used to build up the Discourse
Model. Semantic processing is strongly
modularized and distributed amongst a number of
different submodules which take care of Spatio-
Temporal Reasoning, Discourse Level Anaphora
Resolution.
1. Introduction
GETARUNS, the system for text understanding
developed at the University of Venice, is equipped
with three main modules: a lower module for
parsing where sentence strategies are implemented;
a middle module for semantic interpretation and
discourse model construction which is cast into
Situation Semantics; and a higher module where
reasoning and generation takes place (Delmont &
Bianchi, 2002) .
The system is based on LFG theoretical
framework (Bresnan, 2001) and has a highly
interconnected modular structure. It is a top-down
depth-first DCG-based parser written in Prolog
which uses a strong deterministic policy by means
of a lookahead mechanism with a WFST to help
recovery when failure is unavoidable due to strong
attachment ambiguity.
It is divided up into a pipeline of sequential but
independent modules which realize the subdivision
of a parsing scheme as proposed in LFG theory
where a c-structure is built before the f-structure
can be projected by unification into a DAG. In this
sense we try to apply in a given sequence phrase-
structure rules as they are ordered in the grammar:
whenever a syntactic constituent is successfully
built, it is checked for semantic consistency, both
internally for head-spec agreement, and externally,
in case of a non-substantial head like a preposition
dominating the lower NP constituent. Other
important local semantic consistency checks are
performed with modifiers like attributive and
predicative adjuncts. In case the governing
predicate expects obligatory arguments to be
lexically realized they will be searched and
checked for uniqueness and coherence as LFG
grammaticality principles require (Delmonte,
2002). In other words, syntactic and semantic
information is accessed and used as soon as
possible: in particular, both categorial and
subcategorization information attached to
predicates  in the lexicon is extracted  as soon as
the main predicate is processed, be it adjective,
noun or verb, and is used to subsequently restrict
the number of possible structures to be built.
Adjuncts are computed by semantic cross
compatibility tests on the basis of selectional
restrictions of main predicates and adjuncts heads.
As far as parsing is concerned, we purport the view
that the implementation of sound parsing algorithm
must go hand in hand with sound grammar
construction. Extragrammaticalities can be better
coped with within a solid linguistic framework
rather than without it. Our parser is a rule-based
deterministic parser in the sense that it uses a
lookahead and a Well-Formed Substring Table to
reduce backtracking. It also implements Finite
State Automata in the task of tag disambiguation,
and produces multiwords whenever lexical
information allows it. In our parser we use a
number of parsing strategies and graceful recovery
procedures which follow a strictly parameterized
approach to their definition and implementation.
Recovery procedures are also used to cope with
elliptical structures and uncommon orthographic
and punctuation patterns. A shallow or partial
parser, in the sense of (Abney, 1996), is also
implemented and always activated before the
complete parse takes place, in order to produce the
default baseline output to be used by further
computation in case of total failure. In that case
partial semantic mapping will take place where no
Logical Form is being built and only referring
expressions are asserted in the Discourse Model ?
but see below.
1.2 The Binding Module
The output of grammatical modules is then fed
onto the Binding Module(BM) which activates an
algorithm for anaphoric binding in LFG terms
using f-structures as domains and grammatical
functions as entry points into the structure.
Pronominals are internally decomposed into a
feature matrix which is made visible to the Binding
Algorithm(BA) and allows for the activation of
different search strategies into f-structure domains.
Antecedents for pronouns are ranked according to
grammatical function, semantic role, inherent
features and their position at f-structure. Special
devices are required for empty pronouns contained
in a subordinate clause which have an ambiguous
context, i.e. there are two possible antecedents
available in the main clause. Also split antecedents
trigger special search strategies in order to evaluate
the set of possible antecedents in the appropriate f-
structure domain. Eventually, this information is
added into the original f-structure graph and then
passed on to the Discourse Module(DM). We show
here below the architecture of the parser.
Fig.1 GETARUNS? LFG-Based Parser
1.3 Lexical Information
The grammar is equipped with a lexicon
containing a list of fully specified inflected word
forms where each entry is followed by its lemma
and a list of morphological features, organized in
the form of attribute-value pairs. However,
morphological analysis for English has also been
implemented and used for OOV words. The system
uses a core fully specified lexicon, which contains
approximately 10,000 most frequent entries of
English. In addition to that, there are all lexical
forms provided by a fully revised version of
COMLEX. In order to take into account phrasal
and adverbial verbal compound forms, we also use
lexical entries made available by UPenn and TAG
encoding. Their grammatical verbal syntactic codes
have then been adapted to our formalism and is
used to generate an approximate subcategorization
scheme with an approximate aspectual and
semantic class associated to it. Semantic inherent
features for Out of Vocabulary words , be they
nouns, verbs, adjectives or adverbs, are provided
by a fully revised version of WordNet ? 270,000
lexical entries - in which we used 75 semantic
classes similar to those provided by CoreLex.
Our training corpus which is made up 200,000
words and is organized by a number of texts taken
from different genres, portions of the UPenn WSJ
corpus, test-suits for grammatical relations, and
sentences taken from COMLEX manual.
To test the parser performance we used the
?Greval Corpus? made available by John Carroll
and Ted Briscoe which allows us to measure the
precision and recall against data published in
(Preis, 2003). The results obtained are a 90% F-
measure which is by far the best result obtained on
that corpus by other system, ranging around 75%.
Overall almost the whole text - 98% - is turned into
semantically consistent structures which have
already undergone Pronominal Binding at sentence
level in their DAG structural representation. The
basic difference between the complete and the
partial parser is the ability of the first to ensure
propositional level semantic consistency in almost
every parse, which is not the case with the second.
2. The Upper Module
GETARUNS, has a highly sophisticated
linguistically based semantic module which is used
to build up the Discourse Model. Semantic
processing is strongly modularized and distributed
amongst a number of different submodules which
take care of Spatio-Temporal Reasoning, Discourse
Level Anaphora Resolution, and other subsidiary
processes like Topic Hierarchy which will impinge
on Relevance Scoring when creating semantic
individuals. These are then asserted in the
Discourse Model (hence the DM), which is then
used to solve nominal coreference together with
WordNet. The system uses two resolution
submodules which work in sequence: they
constitute independent modules and allow no
backtracking. The first one is fired whenever a free
sentence external pronoun is spotted; the second
one takes the results of the first submodule and
checks for nominal anaphora. They have access to
all data structures contemporarily and pass the
resolved pair, anaphor-antecedent to the following
modules. Semantic Mapping is performed in two
steps: at first a Logical Form is produced which is a
structural mapping from DAGs onto of unscoped
well-formed formulas. These are then turned into
situational semantics informational units, infons
which may become facts or sits. Each unit has a
relation, a list of arguments which in our case
receive their semantic roles from lower processing
?  a polarity, a temporal and a spatial location
index.
2.1 Logical Form Creation and Semantic
Mapping
In order to produce a semantic interpretation
from the output of the parser we adopt a uniform
meaning representation which is a structured
Logical Form(LF). In other words we map our f-
structures into a linear formalism that can capture
the basic meaning of the structural units of
grammatical representation. We assume that
parsing has made explicit predicate-argument
relations as well as subordination and adjunction in
f-structure representation: no ambiguity has been
left to decide in the semantics, seen that all
constituents have been assigned a preferential
reading.
LF representations are used to generate a
semantic analysis for an utterance: in this sense,
they represents its interpretation in context and
also its truth conditions. In fact, the system
generates a situation semantics mapping directly
from LF, and that is used to update the Discourse
Model with new discourse entities or new
properties of already existing entities.
LF is basically a flat version of f-structure,
where the main verb predicate is raised at the
higher node, and arguments and adjuncts are
stripped off of useless information w.r.t. semantic
mapping. In order to produce a semantic
interpretation of each utterance we proceed as
follows:
A. we start from DAGs(Direct Acyclic Graphs)
available for each utterance, i.e. f-structures, and
perform pronominal binding and anaphora
resolution at discourse level. Our f-structures are
enriched with Semantic Roles which are derived
from our augmented Lexical Forms by a match
with the head Noun inherent features and
selectional restrictions. Semantic match is also
performed for Adjuncts, which require an
intermediate Preposition and Verb semantic
consistency check for all PP adjuncts. Semantic
Roles may undergo a transformation in the
semantic mapping from LF to Infons in case of
idiomatic expressions, and in case of unexpressed
Obligatory Arguments;
B. each CLAUSE in a DAG is turned into a well-
formed-formula with restricted unscoped
quantification, positive literals, no variables except
for those introduced at a syntactic level. The LF
transducer looks for the starting node which is the
propositional node, where mood and tense are
available. All arguments are searched first, by
traversing the DAG looking for grammatical
functions; only semantically referential arguments
are considered, non referential ones are erased
(notice that f-structures containing semantic role
Form (corresponding to ?there? existential subject,
or pleonastic ?it?) are excluded from LF;
C. after argument f-structures are mapped in
appropriate logical terms, i.e. by computing
internal adjuncts and/or arguments, the algorithm
looks for sentence level adjuncts. In LFG, both
arguments and adjuncts may be computed in two
different ways: open or predicative, closed or non-
predicative. These two syntactic constructions
receive a different treatment in the semantics: in
particular, closed adjuncts have only a modifying
import on the Event variable associate to the main
predicate. On the contrary, open adjuncts have
both an Event variable and an argument variable
which they modify: this information is represented
in f-structure by the presence of an internal Subject
variable functionally controlled by the governing
head NP. An example will be reported below and
discussed in details;
D. each wff is an expression of logical form which
is made up of a predicate and a number of
arguments, "p(arg
1
, ..., arg
n
), where 'p' is a constant
and 'arg' may be a complex term. A term is made
up of a quantifier, a variable and a restriction,
"term(quant,var,restr)" where the quantifier may be
a real natural language quantifier existing in a NP
or a time operator like "time"; the variable is a
syntactic index assigned to the phrase in the f-
structure representation by the parser; the
restriction is the structure on which the
quantifier/operator takes scope which might
coincide with the phrase or clause of f-structure
representation or may be a logical expression built
for that aim at logical form level, as happens for
time formulas. In order to reach an adequate
representation for our discourse model we generate
a generic "situation" predicate for each tensed
clause we compute, and we build a complex term
for time-aspect representation.
E. In LF representation we use syntactic indices
derived directly from f-structure. The mapping
onto semantic representation has two effects:
syntactic indices are substituted by semantic ones,
where they already exist ? and this is the case of
anaphora resolution. In case of new entities, new
semantic indices are generated.
F. Each term is enriched with Semantic Role
information. As said above, Semantic Roles may
undergo a transformation in the semantic mapping
from LF to Infons in case of idiomatic expressions,
and in case of unexpressed Obligatory Arguments.
In the former case semantically empty arguments
are assembled together to produce a non
compositional meaning representation (see
THERE_BE, as opposed to the BE predicate). The
latter case regards both agentless passives and the
Receiver or Goal of ditransitive verbs.
The following is the LF for the first utterance:
John went into a restaurant.
wff(situation, [
      wff(go, [term(definite, sn2, wff(isa, [sn2, john])),
                    term(definite, sn5, wff(isa, [sn5, restaurant])),
                    term(event, f5, wff(and, [wff(isa, [f5, ev]),
                             wff(time, [f5, term(definite, t1,
                                 wff(and, [wff(isa, [t1, tloc]),
                                   wff(past, [t1])]))])])) 'term-event'])])
Generic 'isa' relations are introduced into wffs
for NP's and the quantifier is represented by the
translation of the content of the NP's specifier.
Indefinite NP are turned into 'definite' operators in
case no scope ambiguity in the clause may arise
due to the absence of ambiguity inducing
quantifiers. Tense specifications are transformed
into complex terms with a semantic operator that
translates the contents of aspect after the
computations that have transformed the lexical
static value of aspect into its corresponding
dynamic propositional import. We use three
different operators: event, process, state. These
operators then have a complex restriction,
represented by a conjoined number of wffs, where
we indicate both the location in time - tloc - and its
specificity.
This LF representation is then converted into a
situational semantic representation where syntactic
identifiers are turned into semantic identifiers and
all logical predicates are omitted except for the
conjunction 'and'. Semantic identifiers might be
derived from the discourse model in case the
linguistic form represents an entity already existing
or known to the world of the DM. Situation
semantics builds infons for each unit of
information constituting the situation denoted by
the proposition being represented in the formula.
In addition, for each individual or set entity we
record the semantic role already assigned at f-
structure level by the grammar. A generic 'arg' is
associated to arguments of time predicate. Notice
then that a polarity argument has been added at the
end of each expression.
sit(event, id4, go,
     [ind(definite, id3,
         and([infon(att, infon8, isa, [id3, john], [], 1)]), agent),
           ind(indefinite, id2,
                   and([infon(att, infon9, isa,
                     [id2, restaurant], [], 1)]), locat)],
                   and([infon(att, infon10, isa, [id4, ev], [], 1),
                infon(att, infon13, time, [id4,
                 ind(definite, id5,
                 and([infon(att, infon11, isa, [id5, tloc], [], 1),
         infon(att, infon12, past, [id5], [], 1)]), arg)], [], 1)]), 1)
Finally the content of this representation is
asserted in the DM as a set of 'facts' or 'sits' in case
they are not already present. Factuality for
situational types - events, processes and states - is
computed from propositional level informational
and semantic features. Semantic roles inherited
from f-structure representation make explicit, in a
declarative way, semantic relations which are not
computed in the LF.
The final translation in the DM introduces the
objects of our ontology which, as we said above
are made up of the following literals: fact, sit, loc,
ind, set, card, in, class. The structure of each
situation semantic expression is different
according to their semantic role: loc, locations has
no polarity and no spatiotemporal location indices;
ind, in, card, set, class are type denotators and have
no internal structure. Fact and sit have an internal
structure which is made up of the following
arguments:
- an infon ranked number; a relational type
specifier; a list of argument expressed as a feature
role:identifier; a polarity, spatiotemporal indices.
Facts and sits corresponding to main propositional
relations have no infon: in its place they have a
semantic unique identifier.
Fig.2 GETARUNS? Discourse Level Modules
2.2 Building the Discourse Model
In Situation Semantics where reality is
represented in Situations which are collections of
Facts: in turn facts are made up of Infons which
information units characterised as follows:
Infon(Index, Relation(Property),
List of Arguments - with Semantic Roles,
Polarity - 1 affirmative, 0 negation,
Temporal Location Index,
Spatial Location Index)
In addition Arguments have each a semantic
identifier which is unique in the Discourse Model
and is used to individuate the entity uniquely. Also
propositional facts have semantic identifiers
assigned thus constituting second level ontological
objects. They may be ?quantified? over by
temporal representations but also by discourse level
operators, like subordinating conjunctions.
Negation on the contrary is expressed in each fact.
All entities and their properties are asserted in the
DM with the relations in which they are involved;
in turn the relations may have modifiers - sentence
level adjuncts and entities may also have modifiers
or attributes. Each entity has a polarity and a
couple of spatiotemporal indices which are linked
to main temporal and spatial locations if any exists;
else they are linked to presumed time reference
derived from tense and aspect computation.
Entities are mapped into semantic individual with
the following ontology: on first occurrence of a
referring expression it is asserted as an INDividual
if it is a definite or indefinite expression; it is
asserted as a CLASS if it is quantified (depending
on quantifier type) or has no determiner. Special
individuals are ENTs which are associated to
discourse level anaphora which bind relations and
their arguments. Finally, we have LOCs for main
locations, both spatial and temporal. If it has a
cardinality determined by a number, it is plural or it
is quantified (depending on quantifier type) it is
asserted as a SET and the cardinality is simply
inferred in case of naked plural, i.e. in case of
collective nominal expression it is set to 100,
otherwise to 5. On second occurrence of the same
nominal head the semantic index is recovered from
the history list and the system checks whether it is
the same referring expression:
- in case it is definite or indefinite with a
predicative role and no attributes nor modifiers
nothing is done;
- in case it has different number - singular and the
one present in the DM is a set or a class nothing
happens;
- in case it has attributes and modifiers which are
different and the one present in the DM has none,
nothing happens;
- in case it is quantified expression and has no
cardinality, and the one present in the DM is a set
or a class, again nothing happens.
In all other cases a new entity is asserted in the DM
which however is also computed as being included
in (a superset of) or by (a subset of) the previous
entity.
2.3 GETARUNS at work
As said at the beginning, this paper is concerned
with an hybrid approach to text understanding
which is based on the concurrent use of complete
NLP techniques with shallow and partial ones in
heavily linguistically demanding tasks such as the
one posed by summarization and question
answering. This approach should be taken as a
proposal in line with current NLP research in
unrestricted texts that assumes that partial
processing can be more suitable and useful for
better satisfaction of certain requirements. In
particular, morphological analysis is a prerequisite
in order to better cope with Out of Vocabulary
Words(OOW) by means of guessing techniques
based on morphological rules; statistical processing
? or finite state automata as is the case with our
system - is assumed to be essential for tagging
disambiguation. As to syntactic parsing, robust
approaches should be adopted in order to allow for
structure building in the case of local failures.
Eventually, whenever required, partial semantic
interpretation has to be carried out in order to
execute anaphora resolution and a Discourse Model
is built with a limited number of relations and
properties. Partial semantic interpretation means
that not all semantic relations will be detected and
encoded appropriately in a sense better specified
below. Nonetheless, what is captured by partial
analysis can still be useful to carry out such
important tasks as anaphora resolution at discourse
level and a rough evaluation of entity relevance in
order to better grasp what topic has been the most
relevant one.
Consider now a simple sentence like the
following:
1. John went into a restaurant
This might be represented by Ternary Expressions
(Katz, 1997) as follows:
<John go restaurant>
<GO <SUBJ John>, <OBL restaurant>>
GETARUNS represents the same sentence in
different manners according to whether it is
operating in Complete or in Partial modality. In
turn the operating modality is determined by its
ability to compute the current text: in case of
failure the system will switch automatically from
Complete to Partial modality.
The system will produce the following
representations:
loc(infon2, id1, [arg:main_tloc, arg:tr(f1_r01)])
loc(infon3, id2, [arg:main_sloc, arg:restaurant])
ind(infon4, id3)
fact(infon5, inst_of, [ind:id3, class:man], 1, univ, univ)
fact(infon6, name, [john, id3], 1, univ, univ)
ind(infon7, id4)
fact(infon8, isa, [ind:id4, class:restaurant], 1, id1, id2)
fact(infon9, inst_of, [ind:id4, class:place], 1, univ, univ)
fact(id5, go, [agent:id3, locat:id4], 1, tes(f1_r01), id2)
fact(infon12, isa, [arg:id5, arg:ev], 1, tes(f1_r01), id2)
fact(infon13, isa, [arg:id6, arg:tloc], 1, tes(f1_r01), id2)
fact(infon14, past, [arg:id6], 1, tes(f1_r01), id2)
fact(infon15, time, [arg:id5, arg:id6], 1, tes(f1_r01), id2)
So in case of failure at the Complete level, the
system will switch to Partial and the representation
will be deprived of its temporal and spatial location
information as follows:
ind(infon4, id3)
fact(infon5, inst_of, [ind:id3, class:man], 1, univ, univ)
fact(infon6, name, [john, id3], 1, univ, univ)
ind(infon7, id4)
fact(infon8, isa, [ind:id4, class:restaurant], 1, id1, id2)
fact(infon9, inst_of, [ind:id4, class:place], 1, univ, univ)
fact(id5, go, [agent:id3, locat:id4], 1, univ, id2)
In order to test the performance of the system in
text understanding we refer to such application
fields as Question/Answering and Summarization.
They are by far the best benchmark for any system
that aims at showing how good the semantic
mapping has been.
We will show how GETARUNS computes the
DM by presenting the output of the system for the
?Maple Syrup? text made available by Mitre for the
ANLP2000 Workshop(see Hirschmann et al
1999). Here below is the original text which is
followed by the DM only relatively to the linguistic
material needed to answer the five questions,
though.
How Maple Syrup is Made
  Maple syrup comes from sugar maple trees.  At one
time, maple syrup was used to make sugar.  This is why
the tree is called a "sugar" maple tree.
  Sugar maple trees make sap.  Farmers collect the sap.
The best time to collect sap is in February and March.
The nights must be cold and the days warm.
  The farmer drills a few small holes in each tree.  He
puts a spout in each hole.  Then he hangs a bucket on the
end of each spout.  The bucket has a cover to keep rain
and snow out.  The sap drips into the bucket.  About 10
gallons of sap come from each hole.
Discourse Model for sentences 6 and 7
6.  Farmers collect the sap
class(infon100, id28)
fact(infon101, inst_of, [ind:id28, class:man], 1, univ, univ)
fact(infon102, isa, [ind:id28, class:farmer], 1, univ, id8)
fact(id29, collect, [agent:id28, theme_aff:id24], 1, tes(f1_es6), id8)
fact(infon105, isa, [arg:id29, arg:ev], 1, tes(f1_es6), id8)
fact(infon106, isa, [arg:id30, arg:tloc], 1, tes(f1_es6), id8)
fact(infon107, pres, [arg:id30], 1, tes(f1_es6), id8)
during(tes(f1_es6), tes(f1_es5))
includes(tr(f1_es6), univ)
7.  The best time to collect sap is in February and March
ind(infon112, id31)
fact(infon113, inst_of, [ind:id31, class:substance], 1, univ, univ)
fact(infon114, isa, [ind:id31, class:sap], 1, univ, id8)
in(infon115, id31, id24)
ind(infon116, id32)
fact(infon117, best, [ind:id32], 1, univ, id8)
fact(infon118, inst_of, [ind:id32, class:time], 1, univ, univ)
fact(infon119, isa, [ind:id32, class:time], 1, univ, id8)
set(infon120, id33)
card(infon121, 2)
fact(infon122, inst_of, [ind:id33, class:time], 1, univ, univ)
fact(infon123, isa, [ind:id33, class:[march, February]], 1, univ, id8)
fact(id35, collect, [agent:id28, theme_aff:id31], 1, tes(finf1_es7),
id8)
fact(infon126, isa, [arg:id35, arg:ev], 1, tes(finf1_es7), id8)
fact(infon127, isa, [arg:id36, arg:tloc], 1, tes(finf1_es7), id8)
fact(infon128, nil, [arg:id36], 1, tes(finf1_es7), id8)
fact(infon130, [march, February], [arg:id32], 1, univ, id8)
fact(id37, be, [prop:id35, prop:infon130], 1, tes(f1_es7), id8)
fact(infon131, isa, [arg:id37, arg:st], 1, tes(f1_es7), id8)
fact(infon132, isa, [arg:id38, arg:tloc], 1, tes(f1_es7), id8)
fact(infon133, pres, [arg:id38], 1, tes(f1_es7), id8)
during(tes(f1_es7), tes(f1_es6))
includes(tr(f1_es7), univ)
3. Question-Answering
Coming now to Question Answering, the system
accesses the DM looking for relations at first then
for entities : entities are searched according to the
form of the focussed element in the User DataBase
of Question-Facts as shown below with the QDM
for the first question:
User Question-Facts Discourse Model
q_loc(infon3, id1, [arg:main_tloc, arg:tr(f1_free_a)])
q_ent(infon4, id2)
q_fact(infon5, isa, [ind:id2, class:who], 1, id1, univ)
q_fact(infon6, inst_of, [ind:id2, class:man], 1, univ, univ)
q_class(infon7, id3)
q_fact(infon8, inst_of, [ind:id3, class:coll], 1, univ, univ)
q_fact(infon9, isa, [ind:id3, class:sap], 1, id1, univ)
q_fact(infon10, focus, [arg:id2], 1, id1, univ)
q_fact(id4, collect, [agent:id2, theme_aff:id3], 1, tes(f1_free_a),
univ)
q_fact(infon13, isa, [arg:id4, arg:pr], 1, tes(f1_free_a), univ)
q_fact(infon14, isa, [arg:id5, arg:tloc], 1, tes(f1_free_a), univ)
q_fact(infon15, pres, [arg:id5], 1, tes(f1_free_a), univ)
The system knows that the ? focus ? argument is
? who ? with semantic id, id2, and is an entity
belonging to the semantic class of ? man ?, this
latter informantion being derived from the
syntactic structure of the corresponding sentence
where the interrogative pronoun has bound an
empty category in the SUBJect of the verb
? COLLECT ? of the main clause : this in turn has
allowed the parser to pass the selectional
restrictions associated in the lexicon with the
corresponding lexical frame for the verb
? COLLECT ?. Search of the answer is performed
by looking into the DM for the best Infon that
matches the question: at first, the system looks for
the same relation ? collect ?, then it looks for the
entity corresponding to the semantic role of the
Focus in the question, the Agent. If the first action
doesn?t succeed, the well-known ? semantic
bottleneck ? will cause the system to search for
synonyms in the WordNet synset at first, then in a
more generic dictionary (2 million correlations for
some 30,000 entries) of quasi-synonyms or
concepts belonging to the same semantic field.
Then, the system tries to pick up the entity that is
the Agent, which in our case is id28 (as shown in
the DM for sentence 6), by searching the entity
ontological identifiers ? set, ind, ent. When the
corresponding fact is found, the predicate
(FARMER) is passed to the Generator that builds
the reply sentence.
As to the current text, it replies correctly to all
questions. As to question 4, at first the system
takes ? come from ? to be answered exhaustively
by sentence 14 ; however, seen that ? hole ? is not
computed with a ? location ? semantic role, it
searches the DM for a better answer which is the
relation linguistically expressed in sentence 9,
where ? holes ? are drilled ? in each tree ?. The
? tree ? is the Main Location of the whole story
and ? hole ? in sentence 9 is inferentially linked to
? hole ? in sentence 14, by a chain of inferential
inclusions. In fact, come_from does not figure in
WordNet even though it does in our generic
dictionary of synonyms. As to the fifth question,
the system replies correctly.
1. Who collects maple sap?                 (Farmers)
2. What does the farmer hang from a spout? (A bucket)
3. When is sap collected?        (February and March)
4. Where does the maple sap come from?  (Sugar maple
trees)
5. Why is the bucket covered? (to keep rain and snow out)
Another possible ? Why ? question could have
been the following : ? why is the tree called a
"sugar" maple tree ?, which would have received
the appropriate answer seen that the corresponding
sentence has received an appropriate grammatical
and semantic analysis. In particular, the discourse
deictic pronoun ? This ? has been bound to the
previous main relation ? use ? and its arguments,
so that they can be used to answer the ? Why ?
question appropriately.
There is not enough space here to comment in
detail the parse and the semantics (but see
Delmonte 2000d); however, as far as anaphora
resolution is concerned, the Higher Module
computes the appropriate antecedent for the big
Pro, i.e. the empty SUBject of the infinitive in
sentence n. 7, where the collecting action would
have been left without an agent. This resolution of
anaphora is triggered by the parser decision to treat
the big Pro as an arbitrary pronominal and this
information is stored at lexical level in the
subcategorization frame for the name ? time ?.
With question n.4 the text only makes available
information related to ? maple syrup ?. As said
above, we start looking for relations, and the
? come from ? relation has a different linguistic
description as SUBJect/ Theme_Unaffected
argument ? i.e. ? SAP ? -, what we do is to try and
see whether there is some inferential link between
? sap ? and ? syrup ? in WordNet. This fails, seen
that WordNet does not link the two concepts
explicitly. However both are classified as
? substance ? thus allowing the required inference
to be fired ? both are also taken as synonyms in our
generic dictionary. The final question does not
constitute a problem seen that the relation ?cover?
has become a semantic relation and is no longer a
noun or a verb. Also worth noting is the fact that
the question is not a real passive, but a quasi-
passive or an ergative construction, so no agent
should be searched for. Our conclusion is that the
heart of a Q/A system should be a strongly
restrictive pipeline of linguistically based modules
which alone can ensure the adequate information
for the knowledge representation and the reasoning
processes required to answer natural language
queries.
3.1 Answering Generic Question
An important issue in QA is answering generic
questions on the ?aboutness? of the text, questions
which may be answered by producing appropriate
headlines or just a title. In our system, given the
concomitant work of anaphora resolution modules
and the semantic mapping into predicate-argument
structures, this can be made as follows. The system
collapses all entities and their properties, relations
and attributes, after the text has been fully
analysed, by collecting them for each ontological
type under each semantic identifier. At the same
time, each semantic id receives a score for
topichood thus allowing a ranking of the entities.
Here below we list the most relevant entities of the
text reported above:
entity(set,id8,30,facts([
card(infon23, id8, 5),
fact(infon24, sugar_maple, [ind:id8], 1, T, P),
fact(infon25, inst_of, [ind:id8, class:plant_life], 1, T, P),
fact(infon26, isa, [ind:id8, class:tree], 1, T, P),
fact(id11, come, [actor:id2, locat:id8], 1, T, P),
fact(id25, make, [agent:id8, theme_aff:id23, patient:id24], 1, T,
P)])).
entity(class,id30,77,facts([
fact(infon114, inst_of, [ind:id30, class:man], 1, T, P),
fact(infon115, isa, [ind:id30, class:farmer], 1, T, P),
fact(id39, drill, [agent:id30, theme_aff:id38], 1, T, P),
fact(id42, put, [agent:id30, theme_aff:id41, locat:id38], 1, T, P),
fact(id48, hang, [agent:id30, theme_aff:id44], 1, T, P)])).
entity(ind,id13,10,facts([
in(infon48, id13, id9),
fact(infon46, inst_of, [ind:id13, class:substance], 1, T, P),
fact(infon47, isa, [ind:id13, class:sugar], 1, T, P),
fact(id14, make, [agente:id2, tema_aff:id13], 1, T, P),
fact(*, inst_of, [ind:id13, class:maple], 1, T, P),
fact(*, isa, [ind:id13, class:maple], 1, T, P),
fact(*, isa, [ind:id13, class:sugar_maple], 1, T, P),
fact(*, of, [arg:id10, specif:id13], 1, T, P)])).
Where starred facts are inherited by the inclusion
relation specified by the ?in? semantic predicate.
For instance, the fact constituted by a ?specifying?
relation between ?sugar? and ?maple? as
fact(infon34, of, [arg:id10, specif:id9], 1, univ,
univ)
becomes a starred fact inherited by id13 in force of
the inclusion relation,
in(infon48, id13, id9)
In this way, an appropriate answer to the
question ?What is the text about? can be generated
directly from the entity list by picking up relations
and properties of the most relevant individuals,sets
and classes (Delmonte, 2000).
4. The Experiment
We downloaded the only freely available corpus
annotated with anaphoric relations, i.e.
Wolverhampton?s Manual Corpus made available
by Prof. Ruslan Mitkov on his website. The corpus
contains text from Manuals at the following
address,
http://clg.wlv.ac.uk/resources/corpus.html
To compare our results with the SGML
documents we created a Perl script that extracted
all referring expressions and wrote the output into
a separate file. The new representation of the
SGML files looked now like a list of records each
one denoted by an index a dash and the text of the
referring expression. In case of complex referring
expressions we had more than one index available
and so we translated the complex referring
expression into a couple or a triple of records each
one denoted by its index. The final results were
75% F-measure - complete results are published in
(Delmonte, 2003b).
5. Conclusions
Results reported in the experiment above have
been aimed to show the ability of the system to
cope with what has always been regarded as the
toughest task for an NLP system to cope with, that
of reference resolution which is paramount in any
system of Q/A. We have not addressed the problem
of summarization for lack of space: however hints
have been addressed by the issue of answering
Generic Questions.
We are currently experimenting with automatic
ontology building from the DM into a Proteg?
database which is then used to answer queries from
the web (Delmonte, 2003b). By weaving natural
language into the basic fabric of the Semantic Web,
we can begin to create an enormous network of
knowledge easily accessible by both machines and
humans alike. Furthermore, we believe that natural
language querying capabilities will be a key
component of any future Semantic Web system. By
providing ?natural? means for creating and
accessing information on the Semantic Web, we
can dramatically lower the barrier of entry to the
Semantic Web. Natural language support gives
users a whole new way of interacting with any
information system, and from a knowledge
engineering point of view, natural language
technology divorces the majority of users from the
need to understand formal ontologies. As we have
tried to show in the paper, this calls for better NLP
tools where a lot of effort has to be put in order to
allow for complete and shallow techniques to
coalesce smoothly into one single system.
GETARUNS represents such a hybrid system and
its performance is steadily improving.
6. References
Abney, A. 1996. Part-of-Speech Tagging and
Partial Parsing, in Ken Church et al, eds.
Corpus-Based Methods in Language and
Speech, Kluwer Academic Publishers,
Dordrecht.
Bresnan, Joan.  2001.  Lexical-Functional Syntax.
Blackwells.
Delmonte R., 2003a. Getaruns: a hybrid system
for summarization and question answering, in
Proc. Workshop "NLP for Question Answering"
in EACL, Budapest, 21-28.
Delmonte R., D. Bianchi. 2002. From Deep to
Partial Understanding with GETARUNS, Proc.
ROMAND 2002, Universit? Roma2, Roma, 57-
71.
Delmonte R. 2002. GETARUN PARSER - A
parser equipped with Quantifier Raising and
Anaphoric Binding based on LFG, Proc.
LFG2002 Conference, Athens, 130-153, at
http://cslipublications.stanford.edu/hand/miscpubsonline.ht
ml.
Delmonte R. 2000. Generating from a Discourse
Model, Proc. MT-2000, BCS, Exeter, 25-1/10.
Delmonte R., 2003b. The Semantic Web Needs
Anaphora Resolution, Proc.Workshop ARQAS,
2003 International Symposium on Reference
Resolution and Its Applications to Q/A and
Summarization, Venice, Ca' Foscari University,
25-32.
Preis J., 2003. Using Grammatical Relations to
Compare Parsers, in Proc., EACL, Budapest,
291-298.
Hirschman, L. Marc Light, Eric Breck, & J. D.
Buger. 1999. Deep Read: A reading
comprehension system. In Proc. A CL
'99.University of Maryland.
Katz, B. 1997. Annotating the World Wide Web
using natural language. In RIAO ?97.
EVALUATING GETARUNS PARSER WITH
GREVAL TEST SUITE
Rodolfo Delmonte
Ca' Garzoni-Moro, San Marco 3417
Universit? "Ca Foscari"
30124 - VENEZIA
Tel. 39-041-2349464/52/19 - Fax. 39-041-5287683
E-mail: delmont@unive.it - website: project.cgm.unive.it
Abstract
GREVAL, the test suite of 500 English sentences
taken from SUSANNE Corpus and made
available by John Carroll and Ted Briscoe at their
website, has been used to test the performance of
a symbolic linguistically-based parser called
GETARUNS presented in (Delmonte, 2002).
GETARUNS is a symbolic linguistically-based
parser written in Prolog Horn clauses which uses
a strong deterministic policy by means of a
lookahead mechanism and a WFST. The grammar
allows the specification of linguistic rules in a
highly declarative mode: it works topdown and by
making a heavy use of linguistic knowledge may
achieve an almost complete deterministic policy:
in this sense it is equivalent to an LR parser. The
results obtained fare higher that the ones reported
in (Preis, 2003) and this we argue is due basically
to the symbolic rule-based approach: we reach
96% precision (coverage) and 84% recall
(accuracy). We assume that from a
psycholinguistic point of view, parsing requires
setting up a number of disambiguating strategies,
to tell arguments apart from adjuncts and reduce
the effects of backtracking. To do that the system
is based on LFG theoretical framework and uses
Grammatical Functions information to help the
parser cope with syntactic ambiguity. In the paper
we shall comment on some shortcomings of the
Greval corpus annotation and more in general we
shall criticize some aspects of the Dependency
Structure representation.
1. Introduction
In this paper we will present the parser used by the
system GETARUN and discuss its performance with
the test suite called GREVAL set up by Carroll &
Briscoe. We will also discuss the mapping algorithm
from LFG to Dependency Grammatical Relations
(DGRs), which we have been obliged to develop in
order to be able to evaluate our parser. Greval is a
benchmark for parser evaluation based on
Grammatical Relations in a Head Dependency
Structure style output, i.e. a word based Head-
Dependent flat representation enriched with
Grammatical Relation information, where each
relation is represented as follows,
Relation(introducer,head,dependent)
Relation(introducer,head,dependent,deep-relation)
where a deep relation is introduced basically for
passive constructions, dative shift, and potentially
other structures, according to the ?Movement?
approach invoked by chomskians. The annotation
adopted by the authors is a surface level GRs
approach where for instance, in cases of Locative
Inversion as in sentence 284,
(1) Here, in the old days - when they had come
to see the moon or displays of fireworks - sat
the king and his court while priests, soldiers,
and other members of the party lounged in
the smaller alcoves between.
the SUBJect NP ?the king and his court? is assigned
to DOBJ and then receives an additional deep-
relation label as NCSUBJ to indicate its original
deep structure position. However the same relation is
?wrongly? marked as NCSUBJ in a subsequent case
of Locative Inversion (the only other one, sentence
445), which we report below,
(2) In his stead is a milquetoast version known
as the corporation.
where the inverted subject NP ?a milquetoast
version? is annotated straightforwardly as NCSUBJ.
The inconsistency denounced by this case of double
annotation lingers on other types of ambiguous GRs
that we will comment below.
In our experiment, for reasons already explained in
(Crouch et al, 2002) and further commented below,
we restricted our mapping algorithm to all ?pred-
only? f-structures, i.e. only to semantic heads with
primary GRs. This is because we assume that the
most difficult task a parser is faced with when
parsing a sentence is to operate the argument-adjunct
distinction: thus the subset of GRs we will take into
account is the following:
NCSUBJ, DOBJ, IOBJ, ARG_MOD, XCOMP,
CCOMP
Difficulties in building up a comparable version of
our output, include the inconsistencies present in the
non typographical text distributed for the test. We
also had to give up using the internally provided tool
for evaluation because our system builds multiword
expressions which are almost totally absent in the
annotated Gold version. So even though the authors
admit to the need of improving this aspect, its lack
makes it impossible for real systems to use automatic
evaluation tools.
1.1 Multiwords
Multiwords go from obvious cases such as United
States, which is wrongly treated in the annotated
corpus as two separate words ? ?state? modified by
?united? ? to prepositional and adverbial locutions
some of which have been individuated but have been
left with an SGML markup, as shown here,
accord<blank>to
in<blank>favour<blank>of
even<blank>if
no<blank>one
a<blank>few
out<blank>of
in<blank>order<blank>to
at<blank>least
so<blank>that
for<blank>example
along<blank>with
in<blank>front<blank>of
because<blank>of
as<blank>to
e.<blank>g.
In addition to these 15 multiwords, we produced
over 220 nouns, adverbials and adjectives which
contributed in an important manner to disambiguate
both syntactic and semantic processing, as well as to
facilitate tagging.
1.2 Mapping from LFG to DGRs
However the most difficult part of the work was the
mapping itself. The mapping from LFG to DGRs
requires the setting up of a principled distinction
between GRs.
The main difficulties were due to the treatment of
OBLiques vs. PP adjuncts: LFG erases the
preposition of oblique arguments which is no longer
available for comparison, on the contrary, the
preposition is preserved in case it is semantically
needed to identify the semantic role associated to PP
adjuncts. In DGRs instead we have two options
reported below and taken from the online Readme
document,
ncmod(type, head, dependent) is the most common
relation type and is used to encode PP,
adjectival/adverbial modification, nominal modifiers,
and so on. ncmod is also used to indicate a particle
`modifier' in a verb particle combination, e.g. Bill
looked the word up is encoded as ncmod(prt, look,
up); of in consist of is treated as a particle.
iobj(type, head, dependent) is the relation between
a predicate and a non-clausal complement introduced
by a preposition;
iobj(in, arrive, Spain) arrive in Spain
iobj(into, put, box) put the tools into the box
iobj(to, give, poor) give to the poor
The definitions do not respect the actual decisions
taken in the annotation process because only
adjective and verb predicates are assigned
complement labels like nsubj, dobj and iobj. Nouns
are only assigned generic modifiers, apart from 9
cases of ?wrongly? labeled IOBJ complements
which we comment in more detail below. On the
contrary, in our representation, possessor relations
and other agent-like relations in noun modification
are computed as SUBJ. The remaining ?of? headed
PPs are all computed as OBJ in case the noun is a
deverbal predicate and is transitive; other
prepositions introduce OBLiques and ADJuncts
again according to the governing noun.
As to verb and adjective predicates, in order to be
able to sort out NCMODs from IOBJs, all
subcategorization relations must be suitably
encoded, be it obligatory or optional ones, as can be
understood from the examples of IOBJs listed above
and taken from the online accompanying document.
In particular, ARRIVE would be computed as taking
a Locative PP complement at the same level of PUT,
whereas only the latter requires the complement
obligatorily. As to the third example, GIVE, a
ditransitive verb allowing Dative Shift, its indirect
object would be computed as a case of IOBJ thus
collapsing an important linguistic distinction existing
between OBLiques and Indirect Objects.
However, given the generic criteria followed by the
annotators, we still wanted to verify the consistency
of annotation of IOBJs, so we checked with
COMLEX subcategorization frames whether the
relation would be predicted or not. When collecting
the IOBJs of the corpus we soon discovered that 9
IOBJS constitute cases of non verbal
complementation which we report below,
(iobj to akin future) ;;; non-verbal complementation
(iobj to adherence principle)
(iobj to applicability people)
(iobj of independent pressure)
(iobj of independent volume)
(iobj notorious for disregard)
(iobj to atune Whig)
(iobj to key acceptance)
(iobj on ruin country)
then, there is one dubious case of IOBJ: in sentence
316 the ellipsed governing adjective predicate
?atune? is done away with and the IOBJ relation is
assigned to the verb BE,
(3) Indeed, the old Jeffersonians were far more
atune to the Hamilton oriented Whigs than
they were to the Jacksonian Democrats.
(ncsubj be Jeffersonian _)
(xcomp _ be atune)
(iobj to atune Whig)
(ncsubj be they _)
**(iobj to be democrat)
From the search in COMLEX of the remaining 127
predicates governing IOBJ relations, we derived 20
predicates missing the preposition required for the
complement discriminative choice. In more than one
case the choice of complement (iobj) vs. adjunct
(ncmod) is highly disputable. This situation makes
the comparison and evaluation of IOBJs very
uncertain and bound to low scoring as happened with
the parsers included in the test reported under (Preis,
2003). As an additional remark, from the definition it
would appear that OBLiques are treated as DOBJ of
a preposition particle which is in turn treated itself as
ncmod. To better clarify the issue we partially report
the annotation of example 244 from the corpus,
where we italicize the relevant relations,
(4) Meanwhile, the experts speak of wars
triggered by false pre-emption, escalation,
unauthorized behavior and other terms that
will be discussed in this report.
ncsubj(speak, expert, _)
dobj(speak, war, _)
ncsubj(trigger, war, obj)
arg_mod(by, trigger, pre-emption, subj)
arg_mod(by, trigger, escalation, subj)
arg_mod(by, trigger, behaviour, subj)
arg_mod(by, trigger, term, subj)
ncsubj(discuss, term, obj)
ncmod(prt, speak, of)
So we decided that in our evaluation we treat as
IOBJs both those actually produced by our parser
and matched directly with the Gold corpus, as well
as those which appear in the Gold corpus as DOBJ
governed by a preposition NCMOD and also those
that have been computed as NCMODs directly, as
long as the head and the dependent are identical.
We then individuated a number of mismatches in the
Gold annotation which would not receive a suitable
mapping in our output, which were then interpreted
as mistakes by Ted Briscoe (p.c.) in particular cases
of secondary predication for the class of ECM verbs
(consider, believe, term, etc) which were treated as
OBJ2, as well as the relation of DOBJ associated to
complements of verb HAVE, which we compute as
copulative verb.
(5) Sunday he had added, We can love
Eisenhower the man, even if we considered
him a mediocre president... but there is
nothing left of the Republican Party without
his leadership.
ncsubj(add, he, _)
ncsubj(love, we, _)
dobj(love, Eisenhower, _)
ncsubj(consider, we, _)
dobj(consider, he, _)
obj2(consider, president, _)
For this reason we didn?t include data for OBJ2 in
our comparison which required too many changes in
our parser architecture in order to have the
appropriate mapping. As to clause level GRs, we
computed both open and closed sentential (CCOMP)
and small clause (XCOMP) complements, but we
did not compute open adjuncts ? XMODS ? which
again did not seem to be easily comparable in our
mapping. As a matter of fact, these clausal
complements have not been separated in the
published test and only figure as CLAUSAL. For
comparison reasons we had to erase all subject
relations made available by our LFG-based
representation for all open predicative complements
which were however not represented in the Gold
manually annotated GREVAL corpus.
2. Parsing and Robust Techniques
As far as parsing is concerned, we purport the view
that the implementation of sound parsing algorithm
must go hand in hand with sound grammar
construction. Extragrammaticalities can be better
coped with within a solid linguistic framework rather
than without it. Our parser is a rule-based
deterministic parser in the sense that it uses a
lookahead and a Well-Formed Substring Table to
reduce backtracking. It also implements Finite State
Automata in the task of tag disambiguation, and
produces multiwords whenever lexical information
allows it. In our parser we use a number of parsing
strategies and graceful recovery procedures which
follow a strictly parameterized approach to their
definition and implementation. Recovery procedures
are also used to cope with elliptical structures and
uncommon orthographic and punctuation patterns. A
shallow or partial parser, in the sense of (Abney,
1996) is also implemented and always activated
before the complete parse takes place, in order to
produce the default baseline output to be used by
further computation in case of total failure.
The grammar is equipped with a lexicon containing a
list of fully specified inflected word forms where
each entry is followed by its lemma and a list of
morphological features, organized in the form of
attribute-value pairs. However, morphological
analysis for English has also been implemented and
used for OOV words. The system uses a core fully
specified lexicon, which contains approximately
10,000 most frequent entries of English. In addition
to that, there are all lexical forms provided by a fully
revised version of COMLEX. In order to take into
account phrasal and adverbial verbal compound
forms, we also use lexical entries made available by
UPenn and TAG encoding. Their grammatical verbal
syntactic codes have then been adapted to our
formalism and is used to generate an approximate
subcategorization scheme with an approximate
aspectual and semantic class associated to it.
Semantic inherent features for Out of Vocabulary
words , be they nouns, verbs, adjectives or adverbs,
are provided by a fully revised version of WordNet ?
270,000 lexical entries - in which we used 75
semantic classes similar to those provided by
CoreLex.
All parser rules from lexicon to c-structure to f-
structure amount to 7532 rules, thus subdivided:
1. Calls to lexical entries - morphology and lexical
forms: 3865 rules
2. Syntactic and semantic rules in the parser proper:
2617 rules
All syntactic/semantic rules: 6482 rules
4. Semantic Rules for F-Structure
Lexical Rules for Consistency and Control:- 439
rules
F-structure building, F-command:- 170 rules
Quantifier Raising and Anaphoric Control:- 441
rules
All semantic f-structure building rules: 1050 rules
The parser itself is made up of 51,000 lines of code.
This does not take into account the code for the
lexicon ? with fully specified subcategorization
frames - and the dictionary for morphological
decomposition: 6600 entries for the lexicon and
76,000 entries for the dictionary. These are all
consulted at runtime. Eventually the semantics from
the WordNet and other sources derived from the web
make up three hash-tables for 5 Mb overall sitting on
the hard disk and accessed when needed.
Our training corpus for the complete system is made
up 200,000 words and is organized by a number of
texts taken from different genres, portions of the
UPenn WSJ corpus, test-suits for grammatical
relations, narrative texts, and sentences taken from
COMLEX manual.
Fig.1 GETARUNS? LFG-Based Parser
2.1 Lookahead and FSA
One of the important differences we would like to
highlight is the use of topdown lookahead based
parsing strategies. The following list of preterminal
14 symbols is used:
1. v=verb-auxiliary-modal-clitic-cliticized verb
2. n=noun ? common, proper; 
3. c=complementizer
4. s=subordinator; 
5. e=conjunction
6. p=preposition-particle
7. a=adjective; 
8. q=participle/gerund
9. i=interjection
10. g=negation
11.d=article-quantifier-number-intensifier-focalizer
12. r=pronoun
13. av=adverb
14. x=punctuation
Tab. 1: Preterminal symbols used for lookahead
As has been reported in the literature (see
Tapanainen and Voutilainen 1994; Brants and
Samuelsson 1995), English is a language with a high
level of homography: readings per word are around 2
(i.e. each word can be assigned in average two
different tags depending on the tagset). Lookahead in
our system copes with most cases of ambiguity:
however, we also had to introduce a disambiguating
tool before the input string could be safely passed to
the parser. Disambiguation is applied to the
lookahead stack and is operated by means of Finite
State Automata. The reason why we use FSA is
simply due to the fact that for some important
categories, English has unambiguous tags which can
be used as anchoring in the input string, to reduce
ambiguity. I am now referring to the class of
determiners which is used to tell apart words
belonging to the ambiguity class [verb,noun], the
most frequent in occurrence in English. Besides, all
FSA may be augmented by tests related to linguistic
properties needed for disambiguation; some such
tests are,
 check subcategorization frame for current
word
This is used for [n,v], [a,n,v] ambiguity classes
followed by a preposition, or followed by ?that?
 check for gerundive verb form
This is used to check for ?ing endings of words
 check for auxiliary and modals
This is used to disambiguate [n,v], [a,n,v]
ambiguity classes when preceded by an auxiliary
or a modal
 check for noun belonging to factive class
This is used to disambiguate ?that? [a,c,r]
ambiguity class when preceded by a governing
noun
 check for verbs of saying
This is used to disambiguate verbs preceded or
followed by punctuation marks
3. GETARUNS: a Linguistically and
Psychologically Based Parser
The parser is divided up into a pipeline of sequential
but independent modules which realize the
subdivision of a parsing scheme as proposed in LFG
theory where a c-structure is built before the f-
structure can be projected by unification into a DAG.
In this sense we try to apply phrase-structure rules in
a given sequence as they are ordered in the grammar:
whenever a syntactic constituent is successfully
built, it is checked for semantic consistency, both
internally for head-spec agreement, and externally,
in case of a non-substantial head like a preposition
dominates the lower NP constituent; other important
local semantic consistency checks are performed
with modifiers like attributive and predicative
adjuncts. In case the governing predicate expects
obligatory arguments to be lexically realized they
will be searched and checked for uniqueness and
coherence as LFG grammaticality principles require.
We assume that from a psycholinguistic point of
view, parsing requires setting up a number of
disambiguating strategies, basically to tell arguments
apart from adjuncts and reduce the effects of
backtracking. The use of strategies calls for
psychologically related disambiguation processes
which are strictly bound to linguistic parameters. For
instance, English is a language that freely allows
compless (complementizer-less) complement and
relative clauses. Being the sentence the highest
recursive structural level, it is plausible that English
speakers will adopt some strategy in order to avoid
falling in a garden path - thus freezing the parser.
Another peculiar feature of English regards the
inherent ambiguity of Past Tense/Past Participle verb
forms, exception made for irregular verbs which
however only constitute a small subset in the verb
lexicon of English amounting in our case to some
20,000 entries. Seen that Reduced Relative Clauses
are headed by the past participle verb form, and that
Participial Adjuncts may be attached to any NP head
nouns quite consistently; and seen also that is very
hard to apply strict subcategorization tests for
participial SUBJect - or deep OBJect in case of
passives - with good enough confidence we assume
that such tests will only be performed in case the
parser is at the complement level of the SUBJect NP.
The reason for this being that we need to prevent as
much as possible failures at the I_bar level. For this
reason we pass grammatical function information
down into the NP complement level in order to be
used for that purpose.
Whenever a given predicate has expectancies for a
given argument to be realized either optionally or
obligatorily this information will be passed below to
the recursive portion of the parsing: this operation
allows us to implement parsing strategies like
Minimal Attachment, Functional Preference and
other ones (see Delmonte, 2000a; Delmonte, 2000b).
As said above, English allows an empty
Complementizer for finite complement and relative
clauses, two structures which contribute a lot of
indeterminacy to the parsing process. However, in
our system, this can be nicely accomodated by using
linguistic information to prevent the rule to be
entered by the parser. Syntactic and semantic
information is accessed and used as soon as possible:
in particular, both categorial and subcategorization
information attached to predicates in the lexicon is
extracted as soon as the main predicate is processed,
be it adjective, noun or verb, and is used in
association with local lookahead to restrict the
number of possible structures to be built. Adjuncts
are computed by semantic compatibility tests on the
basis of selectional restrictions of main predicates
and adjuncts heads.
The grammar formalism implemented in our system
is not fully compliant with the one suggested by LFG
theory (Bresnan, 2001), in the sense that we do not
use a specific Feature-Based Unification algorithm
but a Prolog-based parsing scheme. On the other
hand, Prolog language gives full control of a
declarative rule-based system, where information is
clearly spelled out and passed on and out to
higher/lower levels of computation. In addition, we
find that topdown parsing policies are better suited to
implement parsing strategies that are essential in
order to cope with attachment ambiguities (but see
below).
We will need here to make clear what we intend for
"LFG-related grammar organization": as said above,
we are not following LFG theory strictly in that the
parser is not organized as would all constraint
unification-based parsers, with a context-free or
context-augmented grammar that produces
constituents which are then passed to a unification
mechanism to check for consistency, uniqueness and
coherence in LFG terms, or simply to check for
feature agreement and subcategorization constraints
satisfaction. In our parser the grammar is organized
by Grammatical Functions which call syntactic
constituents. Each Grammatical Function call passes
functional information to the constituent level which
is paramount to the lookahead mechanism and makes
available syntactic constraints of a higher level than
the constituent itself.
3.1 The Organization of Grammar Rules
The grammar is divided up into five main levels:
- the complex utterance level, where choices are
made for subordinate/coordinate, direct/indirect
discourse markers, or as simple assertion;
- the utterance level, where choices are made for
detecting a question vs. assertion;
- the simple assertion level, where we may have
assertions with a sentential subject; a verbal structure
as a gerundive as subject; a fronted OBJect NP as
focalized constituent or a Locative Inversion
sentence structure.
In case none of the previous structures are detected
the parser enters the CP level of canonical sentences
where Aux-to-Comp may take place.
All sentential fronted constituents are taken at the CP
level. Adjuncts at this level may be of many different
kinds, some of them conflicting with the same role
the constituent may have at a lower level. For
instance, a vocative NP may be present, fronted PP
complements, as well as various types of
parenthetical structures: here again, the parser must
be told at which level of computation it is actually
situated in the grammar. This is done again by
passing down the corresponding grammatical
function. When the parser leaves this level of
computation it will enter the canonical IP level
where the SUBJect NP must be computed, either as a
lexically present nominal head or as a string passed
in the Extraposition variable. Then again a number
of ADJuncts may be present between SUBJect and
verb, and they can be adverbials and parentheticals.
When this level is left the parser enters the I_bar
level where it is expecting a verb in the input string.
This can be a verb complex with a number of
internal constituents, but the first item must be
definitely a verb. In case there is none, a number of
fail-soft and recovery strategies are tried to check
whether the parser has taken a participial as ADJunct
or as reduced relative clause in a previous parse
which is passed down to perusal. Also in case there
was no nominal element available at IP level a
number of recovery strategy are tried to check
whether the parser has taken an Appositive or a
Vocative in a previous parse. So all the previously
parser material is either passed down or is recorded
in a WFST to be used in an upper level in case of
failure at a lower level.
The parser is strictly top-down, depth-first, one-stage
parser with backtracking: differently from most
principle-based parsers presented in (Berwick,
Abney, and Tenny, 1991) which are two-stage
parsers, our parser computes its representations in
one pass. This makes it psychologically more
realistic. The final output of the parsing process is an
f-structure which serves as input to the binding
module and logical form: in other words, it
constitutes the input to the semantic component to
compute logical relations. In turn the binding
module may add information as to pronominal
elements present in the structure by assigning a
controller/binder in case it is available, or else the
pronominal expression will be available for
discourse level anaphora resolution.
What are the main advantages of performing a
topdown lookahead driven parse as compared to
unification procedures applied to a LR parse table as
commented in (Carroll, 2000)? First of all, our
grammar is not a CF grammar seen that CF rules are
multiplied by all different sentence positions at
which they may occur: in order to do that, a NP may
be called by a SUBJect, an OBJect, a
NCOMPlement, an APPosition, a VOCative etc. so
that different properties and constraints may be
associated with each NP realization.
Consider now the wellknown case of
COMPlementizerLess complement clauses in
English as represented by the following example (all
examples are taken from Greval):
(6) A Yale historian, writing a few years ago in
The Yale Review, said We in New England
have long since segregated our children.
And now consider the case in which a sentential
complement is used in subject position, as in
(7) That any sort of duty was owed by his nation
to other nations would have astonished a
nineteenth century statesman.
No rule accessing sentential complement would be
used to look for subject complement clauses which
are only accessed at sentence level as a special case
of sentence structure. In order to take care of
compless complement clauses the parser checks
subcategorization frames, then in case the
complementizer is missing, it activates a check for
the semantic typology of verb predicates which
allow the complementizer to be omitted, which
coincides with bridge verbs or non-factive verbs.
Now consider the case in which the complement
clause follows the object (direct,indirect) NP, as in
(8) He told the committee the measure would
merely provide means of enforcing the
escheat law which has been on the books
since Texas was a republic.
This strategy is organized in a similar way to
checking for the attachment of a PP complement
following a NP. The complementation information is
turned into a ?that? word in case of sentential
complement, and into the whole set of prepositions
subcategorized by the verb with PP complements.
These words are used as Cues-set to prevent the NP
from entering relative clause rules, or any PP headed
with one of the prepositions listed in the Cues-set,
after the head has been taken and the parser is in the
complement block of rules. The Cues-set is passed as
a list from the I_bar level down to the vp level and
into the object NP if any.
Now consider cases in which the parser has to
choose between an OBJect NP/Sentence complement
SUBJect in case the verb is compless as shown in:
(9) Mitchell decried the high rate of
unemployment in the state and said the
Meyner administration and the Republican
controlled State Senate Must share the blame
for this.
Or this sentence where the complement is started by
a comma, and a vocative,
(10) A man must be able to say, Father, I
have sinned, or there is no hope for him.
As said above, we ascertain the verb belongs to the
semantic class of non-factive verbs and then look for
a finite verb ahead before allowing the Sentence
complement rule to be fired. Other similarly difficult
cases that can be adequately treated in our parser are
shown below,
(11) I told him what Liston had said and
he said Liston was a double crosser and said
anything he (Liston) got was through a
keyhole.
where both the complement clause and the following
relative clause are compless. Getting the final results
reported in Table 2. with Greval took us one
month/man work to account for rules and lexical
entries missing in the parser. Some such coverage
problems were caused by sentences like 12 and 13
below,
(12) Wagner replied, Can't you just see
the headline City Hooked for $172,000 ?
(13) Yet, I responded, could not similar
things be said about the art of the past ?
Or an imperative/exhortative followed by a question
as in,
(14) Take Augustine's doctrine of grace
given and grace withheld : have you
pondered the dramatic qualities in this
theology ?
or a subordinate clause followed by a question as in,
(15) If he attaches little importance to
personal liberty, why not make this known
to the world ?
Hard sentences to parse were the following ones,
(16) Battalion Chief Stanton M. Gladden,
42, the central figure in a representation
dispute between the fire fighters association
and the teamsters union, suffered multiple
fractures of both ankles.
(17) He bent down, a black cranelike
figure, and put his mouth to the ground.
where in 16 there are two long parentheticals fairly
hard to process before the main verb comes; in 17
the appositive comes after the verb and not after
head noun, the pronoun ?he?.
New lexical entries had to be added to account for
special multiwords basically in the area of
grammatical function words. We also added some
new lexical multiwords which caused the FSA
disambiguation problems.
4. Evaluation and Discussion
The parser parses 89% of all text top down: then it
parses 9.3% of the remaining linguistic material
bottom up and adds it up to the parsed portion of the
current sentence. That may produce wrong results in
case a list has been partially parsed by the top down
parser. But it produces right results whenever any
additional complete subordinate or coordinate
sentence structure has been left over ? which
constitutes the majority of cases. Overall almost the
whole text ? 98.3% - is turned into semantically
consistent structures which have already undergone
Pronominal Binding at sentence level in their DAG
structural representation.
We find it very important to remark the fact that the
performance of our parser is mainly to be appreciate
for the high coverage. None of the statistically and
stochastically based parser reported under (Preis,
2003)  reached such a high score.
ALL-RELS GOLD VENICE CORRECT %PRECIS
RECALL/
GOLD
RECALL/
GUESS
xcomp 361 344 274 95.29 75.90 79.65
ncsubj 1038 1038 883 100.00 85.06 85.06
iobj 158 126 99 79.74 62.65 78.57
dobj 409 394 331 96.33 80.92 84.01
argmod 38 30 26 78.94 68.42 86.66
ccomp 81 80 62 98.76 76.54 77.5
TOTAL 2085 2012 1675 96.49 80.33 83.25
Table 2: Grammatical Relations produced by GETARUNS with Precision and Recall
For the sake of comparison we also report the main
data taken from the table presented under (Preis,
2003) to allow the reader to appreciate the results of
our parser. In particular, the number of main
Grammatical Relations treated in the previous test is
half the number in comparison with ours. If we look
at the easiest GR to parse, i.e. the NCSUBJ GR, we
see that the number of cases found by the best parser
in the previous test is by far lower that our result.
The highest case of precision is for DOBJ in the BU
parser which reaches 88.42% which is 8 points lower
that our result. In absolute terms, limiting the
comparison to the two most frequent GRs, the best
parser ? BU - has found 361 DOBJs against our 394;
891 NCSUBJs against our 1038.
GF oec
BC -
PRECIS
BC -
RECALL
BU -
PRECIS
BU -
RECALL
CH -
PRECIS
CH -
RECALL
C1 -
PRECIS
C1 -
RECALL
C2 -
PRECIS
C2 -
RECALL
dobj 409 85.83 78.48 88.42 76.53 84.43 75.55 86.16 74.57 84.85 75.31
iobj 158 31.93 67.09 57.75 51.9 27.53 67.09 27.09 69.62 27.01 70.25
ncsubj 1038 81.99 82.47 85.83 72.93 81.8 70.13 79.19 65.99 81.29 69.46
obj2 19 27.45 73.68 46.15 31.58 61.54 42.11 81.82 47.37 61.54 42.11
arg_mod 41 - - 75.68 68.29 78.12 60.98 82.86 70.73 82.86 70.73
clausal 403 43.27 52.61 75.79 71.46 62.19 43.67 50.57 32.75 49.11 27.30
Table 3: GR Precisions and Recalls as derived from (Preis, 2003)
The recall is also accordingly lower in absolute
terms: there are 277 cases of correct DOBJs in BU
against our 331, and 702 cases of correct NCSUBJs
? in this case it is the BC parser that gets best recall ?
against our 883. And 277 and 702 are slightly better
than chance - 67.72  and 67.63 respectively.
The impression one gets from the performance of
statistically and stochastically based parsers is that
they are inherently unable to cope with deep
linguistic information. They are certainly impossible
to undergo substantial improvements. On the
contrary, rule based parsers would benefit from
additional subcategorization frames as in our case:
and for all those constructions which require setting
up of new additional peripheral rules in the grammar,
they would typically increase their coverage, as did
our parser.
As a last comment, we started evaluating subsets of
GREVAL corpus with the online version of
?Connexor? dependency parser, on the assumption
that that version would be identical or even better
than the one commercially available. We did that
because this parser is regarded the best dependency
parser on the market. We tried out a subset of 50
sentences, and on a first perusal of the output we
discovered that only 40 sentences contained correct,
and fully connected representations. The remaining
10 sentences either presented unconnected heads, or
misconnected ones due to wrong attachments. Some
remarks on the possible reasons for that:
 bottom up local parsing techniques are good
at coping with typically hard to parse
structures for a top down parser like
coordinate structures but they are bad at
computing long distance dependencies;
 they are good at computing attachment
whenever it is local, but they make mistakes
when there are extraposed elements;
 dependency parsing does not seem to obey
to generally accepted grammaticality
principles like the obligatoriness of SUBJect
constituents, nor the need to provide some
landing site for extracted wh- elements in
relative and interrogative clauses;
 control structures like small clauses for
predicative complements and adjuncts are all
attached locally, which is not always the
case.
So, even though word-level parsing may be more
effective as to the number of connections
(constituents) safely produced, without leaving off
any fragment or skimmed fragment, it is nonetheless
faced with the hard task of recomposing clause level
control mechanisms which in a top down
constituency-based parser are given for granted.
The F-measure derived from our P and R according
to the usual formula:
2rp
 F1(r,p) =       ---------
r+p
is 89.38%, which is by far higher than the 75%
reported in (Crouch et al, 2002) as being the best
result obtained by linguistic parsers today.
We are currently experimenting with a ?mildly?
topdown/bottomup version of the parser in which
rather than starting from Clause level we search
recursively for Arguments and Adjuncts. In other
words, we look for fully semantically interpreted
constituents in which choice for argumenthood has
already been partially performed. In addition to
collecting Arguments/Adjuncts, the new parser
scatters in the output list punctuation marks and
coordinate/subordinate words which are deemed
responsible to determine clause boundaries. To that
aim, we devised a procedure for clause creation
under the restriction that a main tensed Verb
constituent complex has been found. This can be
iterated on the input list and the procedure may
decide to fuse portion of the output list which have
been left stranded without independent clause status,
and append it to the preceding prospective clause.
Interpretation procedures follows by recovering
subcategorization frames for the main tensed verb
and assignment of grammatical function and
semantic roles takes place. The Clause level
procedure is then followed by an Utterance level
procedure that produces simple utterance or complex
ones ? coordinated or subordinated ? according to
the Clause input list.
We experimented the new version of the parser with
the Greval Corpus and discovered that in some cases
it was much slower than the fully topdown version.
However, we also recovered parsing time in highly
ambiguous and complex sentences, where the
?mildly? bottomup parser actually followed a totally
linear behaviour: no increase in computation time
resulted and the performance is only conditioned by
the number of words/number of arguments-
adjuncts/number of clauses to build. We haven?t
been able to compute this proportion systematically
but will do so in the future.
5. References
Abney, S. 1996. Part-of-Speech Tagging and Partial
Parsing, in Ken Church, Steve Young, and Gerrit
Bloothooft, eds. Corpus-Based Methods in
Language and Speech, Kluwer Academic
Publishers, Dordrecht.
Berwick, R.C., S. Abney, and C. Tenny. 1991.
Principle-Based Parsing: Computation and
Psycholinguistics. New York: Kluwer Academic
Publishers.
Brants T. & C.Samuelsson 1995. Tagging the
Teleman Corpus, in Proc.10th Nordic Conference
of Computational Linguistics, Helsinki, 1-12.
Bresnan, Joan. 2001. Lexical-Functional Syntax.
Blackwells.
Carroll, John A. 2000. Statistical Parsing, in R.Dale,
H.Moisl, H.Somers 2000. Handbook of Natural
Language Processing, Marcel Dekker, New
York, Chapt.22, 525-43.
Delmonte R. 2002. GETARUN PARSER - A parser
equipped with Quantifier Raising and Anaphoric
Binding based on LFG, Proc. LFG2002
Conference, Athens, pp.130-153, at
http://cslipublications.stanford.edu/hand/miscpub
sonline.html.
Delmonte R. 2000a. Parsing Preferences and
Linguistic Strategies, in LDV-Forum - Zeitschrift
fuer Computerlinguistik und Sprachtechnologie -
"Communicating Agents", Band 17, 1,2, (ISSN
0175-1336), pp. 56-73.
Delmonte R. 2000b. Parsing with GETARUN,
Proc.TALN2000, 7? conf?rence annuel sur le
TALN,Lausanne, pp.133-146.
Preis J., 2003. Using Grammatical Relations to
Compare Parsers, in Proc., EACL, Budapest,
pp.291-298.
Richard Crouch, Ronald M. Kaplan, Tracy H. King,
Stefan Riezler 2002. A Comparison of
Evaluation Metrics for a Broad Coverage
Stochastic Parser. In Proceedings of the
Workshop on "Parseval and Beyond", LREC'02,
Las Palmas, Spain, 67-74.
Tapanainen P. and Voutilainen A. 1994. Tagging
accurately - don't guess if you know, Proc. of
ANLP '94,  pp.47-52, Stuttgart, Germany.
Proceedings of the Workshop on How Can Computational Linguistics Improve Information Retrieval?, pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Hybrid Systems for Information Extraction and Question Answering Rodolfo Delmonte Ca' Bembo, San Trovaso 1075 Universit? "Ca Foscari" 30123 - VENEZIA Tel. 39-041-2345717/12 - Fax. 39-041-2345703 E-mail: delmont@unive.it - website: project.cgm.unive.it  Abstract Information Extraction, Summarization and Question Answering all manipulate natural language texts and should benefit from the use of NLP techniques. Statistical techniques have till now outperformed symbolic processing of unrestricted text. However, Information Extraction and Question Answering require by far more accurate results of what is currently produced by Bag-Of-Words approaches. Besides, we see that such tasks as Semantic Evaluation of Text Entailment or Similarity ? as required by the RTE Challenge, impose a much stricter performance in semantic terms to tell true from false pairs. We will speak in favour of a hybrid system, a combination of statistical and symbolic processing with reference to a specific problem, that of Anaphora Resolution which looms large and deep in text processing. 1. Introduction Although full syntactic and semantic analysis of open-domain natural language text is beyond current technology, a number of papers have been recently published [1,2,3] showing that, by using probabilistic or symbolic methods, it is possible to obtain dependency-based representations of unlimited texts with good recall and precision. Consequently, we believe it should be possible to augment the manual-annotation-based approach with automatically built annotations by extracting a limited subset of semantic relations from unstructured text. In short, shallow/partial text understanding on the level of semantic relations, an extended label including Predicate-Argument Structures and other syntactically and semantically derivable head modifiers and adjuncts. This approach is promising because it attempts to address the well-known shortcomings of standard ?bag-of-words? (BOWs) information retrieval/extraction techniques without requiring manual intervention: it develops current NLP technologies which make heavy use of statistically and FSA based approaches to syntactic parsing. GETARUNS [4,5,6], a text understanding system (TUS), developed in collaboration between the University of Venice and the University of Parma,  can perform semantic analysis on the basis of syntactic parsing and, after performing anaphora resolution, builds a quasi 
logical form with flat indexed Augmented Dependency Structures (ADSs). In addition, it uses a centering algorithm to individuate the topics or discourse centers which are weighted on the basis of a relevance score. This logical form can then be used to individuate the best sentence candidates to answer queries or provide appropriate information. This paper is organized as follows: in section 2 below we discuss why deep linguistic processing is needed in Information Retrieval and Information Extraction; in section 3 we present GETARUNS, the NLP system and the Upper Module of GETARUNS; in section 4 we describe two experiments with state-of-the-art benchmark corpora. 2 Ternary Expressions as Predicate-Argument Structures Researchers like Lin, Katz and Litkowski have started to work in the direction of using NLP to populate a database of RDFs, thus creating the premises for the automatic creation of ontologies to be used in the IR/IE tasks. However, in no way RDFs and ternary expressions may constitute a formal tool sufficient to express the complexity of natural language texts. RDFs are assertions about the things (people, Webpages and whatever) they predicate about by asserting that they have certain properties with certain values. If we may agree with the fact that this is natural way of dealing with data handled by computers most frequently, it also a fact that this is not equivalent as being useful for natural language. The misconception seems to be deeply embedded in the nature of RDFs as a whole: they are directly comparable to attribute-value pairs and DAGs which are also the formalism used by most recent linguistic unification-based grammars. From the logical and semantic point of view RDFs also resemble very closely first order predicate logic constructs: but we must remember that FOPL is as such insufficient to describe natural language texts. Ternary expressions(T-expressions), <subject relation object>.  Certain other parameters (adjectives, possessive nouns, prepositional phrases, etc.) are used to create additional T-expressions in which prepositions and several special words may serve as relations. For instance, the following simple sentence   (1) Bill surprised Hillary with his answer  
9
 will produce two T-expressions:   (2) <<Bill surprise Hillary> with answer>      <answer related-to Bill>   In Litkowski?s system the key step in their question-answering prototype was the analysis of the parse trees to extract semantic relation triples and populate the databases used to answer the question. A semantic relation triple consists of a discourse entity, a semantic relation which characterizes the entity's role in the sentence, and a governing word to which the entity stands in the semantic relation. The semantic relations in which entities participate are intended to capture the semantic roles of the entities, as generally understood in linguistics. This includes such roles as agent, theme, location, manner, modifier, purpose, and time. Surrogate place holders included are "SUBJ," "OBJ", "TIME," "NUM," "ADJMOD," and the prepositions heading prepositional phrases. The governing word was generally the word in the sentence that the discourse entity stood in relation to. For "SUBJ," "OBJ," and "TIME," this was generally the main verb of the sentence. For prepositions, the governing word was generally the noun or verb that the prepositional phrase modified. For the adjectives and numbers, the governing word was generally the noun that was modified. 2.1 Ternary Expressions are better than the BOWs approach, but? People working advocating the supremacy of the Tes approach were reacting against the Bag of Words approach of IR/IE in which words were wrongly regarded to be entertaining a meaningful relation simply on the basis of topological criteria: normally the distance criteria or the more or less proximity between the words to be related. Intervening words might have already been discarded from the input text on the basis of stopword filtering. Stopwords list include all grammatical close type words of the language considered useless for the main purpose of IR/IE practitioners seen that they cannot be used to denote concepts. Stopwords constitute what is usually regarded the noisy part of the channel in information theory. However, it is just because the redundancy of the information channel is guaranteed by the presence of grammatical words that the message gets appropriately computed by the subject of the communication process, i.e. human beings. Besides, entropy is not to be computed in terms of number of words or letters of the alphabet, but in number of semantic and syntactic relation entertained by open class words (nouns, verbs, adjectives, adverbials) basically by virtue of closed class words. Redundancy should then be computed on the basis of the ambiguity intervening when enumerating those relations, a very hard task to accomplish which has never been attemped yet, at least to my knowledge. What people working with TEs noted was just the problem of encoding relations appropriately, at least some of these relations. The IR/IE BOWs approach 
suffers (at least) from Reversible Arguments Problem (see [7]) - What do frogs eat? vs  What eats frogs? The verb ?eat? entertains asymmetrical relations with its SUBJect and its OBJect: in one case we talk of the ?eater?, the SUBJect and in another case of the ?eatee?, the OBJect. Other similar problems occur with TEs when the two elements of the relation have the same head, as in: -The president of Russia visited the president of China. Who visited the president? The question will not be properly answered in lack of some clarification dialogue intervening, but the corresponding TEs should have more structure to be able to represent the internal relations of the two presidents. The asymmetry of relation in transitive constructions involving verbs of accomplishments and achievements (or simply world-changing events) is however further complicated by a number of structural problems which are typically found in most languages of the world, the first one and most common being Passive constructions:  i.John killed Tom.  ii.Tom was killed by a man.  Who killed the man? Answer to the question would be answered by ?John? in case the information available was represented by sentence in i., but it would be answered by ?Tom? in case the information available was represented by sentence ii. Obviously this would happen only in lack of sufficient NLP elaboration: a too shallow approach would not be able to capture presence of a passive structure. We are here referring to ?Chunk?-based approaches those in which the object of computation is constituted by the creation of Noun Phrases and no attempt is made to compute clause-level structure. There is a certain number of other similar structure in texts which must be regarded as inducing into the same type of miscomputation: i.e. taking the surface order of NPs as indicating the deep intended meaning. In all of the following constructions the surface subject is on the contrary the deep object thus the Affected Theme or argument that suffers the effects of the action expressed by the governing verb rather than the Agent:  Inchoatized structures; Ergativized structures; Impersonal structures  Other important and typical structures which constitute problematic cases for a surface chunks based TEs approach to text computation are the following ones in which one of the arguments is missing and Control should be applied by a governing NP, they are called in one definition Open Predicative structures and they are  Relative clauses; Fronted Adjectival adjunct clauses; Infinitive clauses; Fronted Participial clauses,; Gerundive Clauses; Elliptical Clauses; Coordinate constructions  In addition to that there is one further problem and is definable as the Factuality Prejudice: by collecting 
10
keywords and TEs people apply a Factuality Presupposition to the text they are mining: they believe that all terms being recovered by the search represent real facts. This is however not true and the problem is related to the possibility to detect in texts the presence of such semantic indicators as those listed here below:  Negation; Quantification; Opaque contexts (wish, want); Future, Subjunctive Mode; Modality; Conditionals  Finally there is a discourse related problem and is the Anaphora Resolution problem which is the hardest to be tackled by NLP: it is a fact that anaphoric relations are the building blocks of cohesiveness and coherence in texts. Whenever an anaphoric link is missed one relation will be assigned to a wrong referring expression thus presumably jeopardising the possibility to answer a related question appropriately. This is we believe the most relevant topic to be put forward in favour of the need to have symbolic computational linguistic processing (besides statistical processing). 3 GETARUNS ? the NLUS  GETARUN, the System for Natural Language Understanding, produces a semantic representation in xml format, in which each sentence of the input text is divided up into predicate-argument structures where arguments and adjuncts are related to their appropriate head. Consider now a simple sentence like the following: (1) John went into a restaurant GETARUNS represents this sentence in different manners according to whether it is operating in Complete or in Shallow modality. In turn the operating modality is determined by its ability to compute the current text: in case of failure the system will switch automatically from Complete to Partial/Shallow modality. The system will produce a representation inspired by Situation Semantics[14] where reality is represented in Situations which are collections of Facts: in turn facts are made up of Infons which are information units characterised as follows:     Infon(Index,  Relation(Property),  List of Arguments - with Semantic Roles,  Polarity - 1 affirmative, 0 negation,  Temporal Location Index,  Spatial Location Index) In addition each Argument has a semantic identifier which is unique in the Discourse Model and is used to individuate the entity uniquely. Also propositional facts have semantic identifiers assigned, thus constituting second level ontological objects. They may be ?quantified? over by temporal representations but also by discourse level operators, like subordinating conjunctions and a performative operator if needed. Negation on the contrary is expressed in each fact. In case of failure at the Complete level, the system will switch to Partial and the representation will be deprived of its temporal and spatial location information. In the current version of the system, we use Complete modality 
for tasks which involve short texts (like the students summaries and text understanding queries), where text analyses may be supervisioned and updates to the grammar and/or the lexicon may be needed. For unlimited text from the web we only use partial modality. Evaluation of the two modalities are reported in a section below. 3.1 The Parser and the Discourse Model As said above, the query building process needs an ontology which is created from the translation of the Discourse Model built by GETARUNS in its Complete/Partial Representation. GETARUNS, is equipped with three main modules: a lower module for parsing where sentence strategies are implemented; a middle module for semantic interpretation and discourse model construction which is cast into Situation Semantics; and a higher module where reasoning and generation takes place. The system works in Italian and English. Our parser is a rule-based deterministic parser in the sense that it uses a lookahead and a Well-Formed Substring Table to reduce backtracking. It also implements Finite State Automata in the task of tag disambiguation, and produces multiwords whenever lexical information allows it. In our parser we use a number of parsing strategies and graceful recovery procedures which follow a strictly parameterized approach to their definition and implementation. A shallow or partial parser is also implemented and always activated before the complete parse takes place, in order to produce the default baseline output to be used by further computation in case of total failure. In that case partial semantic mapping will take place where no Logical Form is being built and only referring expressions are asserted in the Discourse Model ? but see below.  3.2 Lexical Information The output of grammatical modules is then fed onto the Binding Module(BM) which activates an algorithm for anaphoric binding in LFG (see [13]) terms using f-structures as domains and grammatical functions as entry points into the structure. We show here below the architecture of the system. The grammar is equipped with a lexicon containing a list of 30000 wordforms derived from Penn Treebank.  However, morphological analysis for English has also been implemented and used for OOV words. The system uses a core fully specified lexicon, which contains approximately 10,000 most frequent entries of English. In addition to that, there are all lexical forms provided by a fully revised version of COMLEX. In order to take into account phrasal and adverbial verbal compound forms, we also use lexical entries made available by UPenn and TAG encoding. Their grammatical verbal syntactic codes have then been adapted to our formalism and is used to generate an approximate subcategorization scheme with an approximate aspectual class associated to it.  
11
  Fig. 1. GETARUNS? LFG-Based Parser  Fig. 2. GETARUNS? Discourse Level Modules Semantic inherent features for Out of Vocabulary words, be they nouns, verbs, adjectives or adverbs, are provided by a fully revised version of WordNet ? 270,000 lexical entries - in which we used 75 semantic classes similar to those provided by CoreLex. Subcategorization information and Semantic Roles are then derived from a carefully adapted version of FrameNet and VerbNet. Our ?training? corpus is made up of 200,000 words and contains a number of texts taken from different genres, portions of the UPenn Treebank corpus, test-suits for grammatical relations, and sentences taken from COMLEX manual. An evaluation carried out on the Susan Corpus related GREVAL testsuite made of 500 sentences has been reported lately [12] to have achieved 90% F-measure over all major grammatical relations. We achieved a similar result with the shallow cascaded parser, limited though to only SUBJect and OBJect relations on LFG-XEROX 700 corpus. 3.3 The Upper Module GETARUNS, as shown in Fig.2 has a linguistically-based semantic module which is used to build up the Discourse Model. Semantic processing is strongly modularized and distributed amongst a number of different submodules which take care of Spatio-Temporal Reasoning, Discourse Level Anaphora Resolution, and other subsidiary processes like Topic Hierarchy which will impinge on Relevance Scoring when creating semantic individuals. These are then asserted in the Discourse Model (hence the DM), which is then used to solve nominal coreference together with WordNet. Semantic Mapping is performed in two steps: at first a Logical Form is produced which is a structural mapping from DAGs onto of unscoped well-formed formulas. These are then turned into situational semantics informational units, infons which may become facts or sits.  In each infon, Arguments have each a semantic identifier which is unique in the DM and is used to individuate the entity. Also propositional facts have semantic identifiers assigned thus constituting second level ontological objects. They may be ?quantified? over by temporal representations but also by discourse level operators, like subordinating conjunctions. Negation on the contrary is 
expressed in each fact. All entities and their properties are asserted in the DM with the relations in which they are involved; in turn the relations may have modifiers - sentence level adjuncts and entities may also have modifiers or attributes. Each entity has a polarity and a couple of spatiotemporal indices which are linked to main temporal and spatial locations if any exists; else they are linked to presumed time reference derived from tense and aspect computation. Entities are mapped into semantic individuals with the following ontology: on first occurrence of a referring expression it is asserted as an INDividual if it is a definite or indefinite expression; it is asserted as a CLASS if it is quantified (depending on quantifier type) or has no determiner. Special individuals are ENTs which are associated to discourse level anaphora which bind relations and their arguments. Finally, we have LOCs for main locations, both spatial and temporal. Whenever there is cardinality determined by a digit, its number is plural or it is quantified (depending on quantifier type) the referring expression is asserted as a SET. Cardinality is simply inferred in case of naked plural: in case of collective nominal expression it is set to 100, otherwise to 5. On second occurrence of the same nominal head the semantic index is recovered from the history list and the system checks whether it is the same referring expression:  - in case it is definite or indefinite with a predicative role and no attributes nor modifiers, nothing is done; - in case it has different number - singular and the one present in the DM is a set or a class, nothing happens; - in case it has attributes and modifiers which are different and the one present in the DM has none, nothing happens; - in case it is quantified expression and has no cardinality, and the one present in the DM is a set or a class, again nothing happens. In all other cases a new entity is asserted in the DM which however is also computed as being included in (a superset of) or by (a subset of) the previous entity.  The upper module of GETARUNS has been evaluated on the basis of its ability to perform anaphora resolution and to individuate referring expressions, with a corpus of 40,000 words: it achieved 74% F-measure.  
12
4. Two experiments with GETURANS As an example of the shallow system we discuss here below the analysis of a newspaper article which as would usually be the case has a certain number of pronominal expressions, which modify the relevance of lexical descriptions in the overall processing for the search of either ?Named Entities? or simply entities individuated by common nouns. If the count is based solely on lexical lemmata and not on the presence of coreferential pronominal expressions, the results will be heavily biased and certainly wrong. Here is the text:  1.Thursday, 25th June 2001 National Parties and the Internet by Joanna Crawford 2.A survey of how national parties used the internet as a campaigning tool during the election will brand their efforts "bleak and dispiriting" - despite the pre-campaign hype of an "e-election". 3.Researchers from Salford University studied websites from all the major parties during the general election, as well as looking at every site put up by local candidates. 4.Their conclusions - to be presented tomorrow at a special conference organised by the Institute for Public Policy Research - could influence how future political contests, including the forthcoming Euro debate, are carried out on the web. 5.The report finds that none of the major three parties allowed message boards or chat rooms for users to post their opinions on the sites.  6.It states: "Parties were accused of simply engaging in online propaganda with boring content and largely ignoring interactivity." 7.The report concludes: "The new media is a way for them to get closer to the public without necessarily allowing the public to become overly familiar in return. 8.The authors - Rachel Gibson and Stephen Ward - go on to state that this may be because parties still regard the web as an electioneering tool, rather than as a democratic device. 9.They said: "Very few offered original material, or changed their sites noticeably over the course of the campaign.  10.Indeed, a large majority of local sites were really no more than static electronic brochures." 11.They dub this "rather disappointing", but praise the Liberal Democrats as "clearly the most active" with around 150 sites. The report concludes: "Parties, as with the general public, need incentives to use the technology.  12.As yet, there seems more to lose and less to gain if they make mistakes experimenting with the technology."  We highlighted pronominal expressions in bold. In a BOWs approach, the count for most relevant topics is solely based on lexical descriptions and ?party, internet? are computed as the most important key-words. However, after the text has been passed by the partial semantic analysis, ?researcher, author? come up as important topics. We report here below the output of the Anaphora Resolution module: in interaction with the Discourse Model where semantic indices are asserted for each entity. Sentence numbers are taken from the text. We report Anaphora Resolution decisions: in particular in sentences where a 
pronoun is coreferred to an antecedent, the antecedent is set as current Main Topic and its semantic ID is used. 1. state(1, change) topics:  main:party, secondary: internet topics(1, main, id1; secondary, id2; potential, id3) 2. state(2, continue) topics:  main:party, secondary: survey topics(2, main, id1; secondary, id7; potential, id2) 3. state(3, retaining) topics:  main: researcher, secondary: party topic(3, main, id18; secondary, id1; , id19) 4. Anaphora Resolution: their resolved as  researcher state(4, continue) topics:  main: researcher, secondary: contest topics(4, main, id18; secondary, id26; potential, id27) 5. state(5, retaining) topics:  main: report, secondary: researcher topics(5, main, id7; secondary, id18; potential, id1) 6. Anaphora Resolution: it  resolved as  report state(6, continue) topics:  main: report, secondary: party topics(6, main, id7; secondary, id1; potential, id40) 7. state(7, continue) topics:  main: report, secondary: party topics(7, main, id7; secondary, id1; potential, id2) 8. The authors - Rachel Gibson and Stephen Ward - go on to state that this may be because parties still regard the web as an electioneering tool, rather than as a democratic device. Anaphora Resolution: this  resolved as  'discourse bound' state(8, retaining) topics:  main: author, secondary: report topics(8, main, id54; secondary, id7; potential, id55) 9. Anaphora Resolution: they  resolved as  author state(9, continue) topics:  main: author, secondary: material topics(9, main, id54; secondary, id61; potential, id62) 10. state(10, continue) topics:  main: author, secondary: site topics(10, main, id54; secondary, id67; potential, id68) 11. Anaphora Resolution: this  resolved as  'discourse bound'; they  resolved as  author state(11, retaining) topics:  main: author, secondary: active topics(11, main, id54; secondary, id71; potential, id72) 12. Anaphora Resolution: they  resolved as  party state(12, continue) topics:  main: party, secondary: mistake topics(12, main, id1; secondary, id78) 4.1 The First Experiment: Anaphora Resolution in Technical Manuals We downloaded the only freely available corpus annotated with anaphoric relations, i.e. Wolverhampton?s Manual Corpus made available by Prof. Ruslan Mitkov on his website. The corpus contains text from Manuals at the following address, http://clg.wlv.ac.uk/resources/corpus.html 
13
  Text Type Referring Exps Coreferring Exps Total  Words AIWA 1629 716 6818 ACCESS 1862 513 9381 PANASONIC 1263 537 4829 HINARI 673 292 2878 URBAN 453 81 2222 WINHELP 672 206 2935 CDROM 1944 279 10568 Totals 8496 2624 39631 Table 2. General data of Worlverhampton?s coreference annotated corpora   Text Type Referring Exps % W Coreferring Exps % RE AIWA 23.89 43.21 ACCESS 19.84 27.01 PANASONIC 26.15 42.51 HINARI 23.38 29,22 URBAN 20.38 17.88 WINHELP 22.89 27.14 CDROM 18.39 14.24 Means 21.43 30.88 Table 3. Proportion of coreferential expressions to referring expressions  
 Fig. 3. Comparing GETARUNS output to WMC   We reported in Tab. 2 the general data of the Coreference Corpus. As can be easily noted, there is no direct relationship existing between the number of referring expressions and the number of coreferring expressions. We assume that the higher the number of coreferring expressions in a text the higher is the cohesion achieved. Thus the text identified as CDROM has a very small number of coreferring expressions if compared to the total number of referring expressions. The proportion of referring expressions to words and of coreferring 
expressions to referring expressions is reported in percent value in table 3. where the most highly cohesive texts are highlighted in italics; highly non cohesive texts are highlighted in bold: The final results are reported in the following figure where we plot Precision and Recall for each text and then the comprehensive values.   
 Fig. 4. Precision and Recall for the WMC  4.2 GETARUNS approach to WEB-Q/A  Totally shallow approaches when compared to ours will always be lacking sufficient information for semantic processing at propositional level: in other words, as happens with our ?Partial? modality, there will be no possibility of checking for precision in producing predicate-argument structures. Most systems would use some Word Matching algorithm to count the number of words appearing in both question and the sentence being considered after stripping stopwords: usually two words will match if they share the same morphological root after some stemming has taken place. Most QA systems presented in the literature rely on the classification of words into two classes: function and content words. They don't make use of a Discourse Model where input text has been transformed via a rigorous semantic mapping algorithm: they rather access tagged input text in order to sort best matched words, phrases or sentences according to some scoring function. It is an accepted fact that introducing or increasing the amount of linguistic knowledge over crude IR-based systems will contribute substantial improvements. In particular, systems based on simple Named-Entity identification tasks are too rigid to be able to match phrase relations constraints often involved in a natural language query. We raise a number of objections to these approaches: first objection is the impossibility to take into account pronominal expressions, their relations and properties as belonging to the antecedent, if no head transformation has taken place during the analysis process. Another objection comes from the treatment of the Question: it is usually the case that QA systems divide the question to be answered into two parts: the Question 
14
Target represented by the wh- word and the rest of the sentence; otherwise the words making up the yes/no question are taken in their order, and then a match takes place in order to identify most likely answers in relation to the rest/whole of the sentence except for stopwords. However, it is just the semantic relations that need to be captured and not just the words making up the question that matter. Some systems implemented more sophisticated methods (notably [8;9;10]) using syntactic-semantic question analysis. This involves a robust syntactic-semantic parser to analyze the question and candidate answers, and a matcher that combines word- and parse-tree-level information to identify answer passages more precisely. 4.3 A Prototype Q/A system for the web  We experimented our approach over the web using 450 factoid questions from TREC. On a first run the base system only used an off-the-shelf tagger in order to recover main verb from the query. In this way we managed to get 67% correct results, by this meaning that the correct answer was contained in the best five snippets selected by the BOWs system on the output of Google API. However, only 30% of the total correct results had the right snippet ranked in position one. Then we applied GETARUNS shallow on the best five snippets with the intent of improving the automatic ranking of the system and have the best snippet alays position as first possibility. Here below is a figure showing the main components for GETARUNS based analysis.  We will present two examples and discuss them  in some detail. The questions are the following ones: Q: Who was elected president of South Africa in 1994?  A: Nelson Mandela Q: When was Abraham Lincoln born?  A: Lincoln was born February_12_1809 The answers produced by our system are indicated after each question. Now consider the best five snippets as filtered by the BOWs system:  
 Fig. 5. System Architecture for QA  who/WP was/VBD elected/VBN president/NN of/IN south/JJ africa/NN in/IN 1994/CD  Main keywords: president south africa 1994  Verb roots: elect  
Google search: elected president south africa 1994  1.On June 2, 1999, Mbeki, the pragmatic deputy president of South Africa and leader of the African National Congress, was elected president in a landslide, having already assumed many of Mandela's governing responsibilities shortly after Mandela won South Africa's first democratic election in 1994. 2.Washington ? President Bill Clinton announced yesterday a doubling in US assistance South Africa of $600-million (R2 160-million) over three years, and said his wife Hillary would attend Nelson Mandela's inauguration as the country's first black president. 3.Nelson Mandela, President of the African National Congress (ANC), casting the ballot in his country's first all-race elections, in April 1994 at Ohlange High School near Durban, South Africa. 4.Newly-elected President Nelson Mandela addressing the crowd from a balcony of the Town Hall in Pretoria, South Africa on May 10, 1994. 5.The CDF boycotted talks in King William's Town yesterday called by the South African government and the Transitional Executive Council to smooth the way for the peaceful reincorporation of the homeland into South Africa following the resignation of Oupa Gqozo as president.  Notice snippet n.1 where two presidents are present and two dates are reported for each one: however the relation ?president? is only indicated for the wrong one, Mbeki and the system rejects it. The answer is collected from snippet no.4 instead. As a matter of fact, after computing the ADM, the system decides to rerank the snippets and use the contents of snippet 4 for the answer. Now the second question:  when/WRB was/VBD abraham/NN lincoln/NN born/VBN  Main keywords: abraham lincoln  Verb roots: bear  Google search: abraham lincoln born  1. Abraham Lincoln was born in a log cabin in Kentucky to Thomas and Nancy Lincoln. 2. Two months later on February 12, 1809, Abraham Lincoln was born in a one-room log cabin near the Sinking Spring. 3. Abraham Lincoln was born in a log cabin near Hodgenville, Kentucky. 4.Lincoln himself set the date of his birth at feb_ 12, 1809, though some have attempted to disprove that claim .  5. A. Lincoln ( February 12, 1809 April 15, 1865 ) was the 16/th president of the United States of America.  In this case, snippet n.2 is selected by the system as the one containing the required information to answer the question. In both cases, the answer is built from the ADM, so it is not precisely the case that the snippets are selected for the answer: they are nonetheless reranked to make the answer available.  5. System Evaluation  After running with GETARUNS, the 450 questions recovered the whole of the original correct result 67% from first snippet.  The complete system has been tested with a set of texts derived from newspapers, narrative texts, children stories. The performance is 75% correct. However, updating and tuning of the system is required for each 
15
new text whenever a new semantic relation is introduced by the parser and the semantics does not provide the appropriate mapping. For instance, consider the case of the constituent "holes in the tree", where the syntax produces the appropriate structure but the semantics does not map "holes" as being in a LOCATion semantic relation with "tree". In lack of such a semantic role information a dummy "MODal" will be produced which however will not generate the adequate semantic mapping in the DM and the meaning is lost. As to the partial system, it has been used for DUC summarization contest, i.e. it has run over approximately 1 million words, including training and test sets, for a number of sentences totalling over 50K. We tested the "Partial" modality with an additional 90,000 words texts taken from the testset made available by DUC 2002 contest. On a preliminary perusal of samples of the results, we calculated 85% Precision on parsing and 70% on semantic mapping. However evaluating full results requires a manually annotated database in which all linguistic properties have been carefully decided by human annotators. In lack of such a database, we are unable to provide precise performance data. The system has also been used for the RTE Challenge and performance was over 60% correct [11]. 6. Conclusions Results reported in the experiment above have been limited to the ability of the system to cope with what has always been regarded as the toughest task for an NLP system to cope with. We have not addressed the problem of question answering for lack of space. Would it be possible for computers the recognize the layout of a Web page, much in the same manner as a human? Much like the development of the Semantic Web itself, early efforts to integrate natural language technology with the Semantic Web will no doubt be slow and incremental. By weaving natural language into the basic fabric of the Semantic Web, we can begin to create an enormous network of knowledge easily accessible by both machines and humans alike. Furthermore, we believe that natural language querying capabilities will be a key component of any future Semantic Web system. By providing ?natural? means for creating and accessing information on the Semantic Web, we can dramatically lower the barrier of entry to the Semantic Web. Natural language support gives users a whole new way of interacting with any information system, and from a knowledge engineering point of view, natural language technology divorces the majority of users from the need to understand formal ontologies. As we have tried to show in the paper, this calls for better NLP tools where a lot of effort has to be put in order to allow for complete and shallow techniques to coalesce smoothly into one single system. GETARUNS represents such a hybrid system and its performance is steadily improving.  In the future we intend to address the problem of using the database of TEs created by our system in asnswering a more extended set of natural language queries than what has been tried sofar. 
References 1. Dan Klein and Christopher D. Manning: Accurate Unlexicalized Parsing. ACL, (2003) 423-430 2. D. Lin.: Dependency-based evaluation of MINIPAR. In Proceedings of the Workshop on Evaluation of Parsing Systems at LREC 1998. Granada, Spain, (1998) 3. Sleator, Daniel, and Davy Temperley: "Parsing English with a Link  Grammar." Proceedings of IWPT ?93, (1993) 4. Delmonte R.: Parsing Preferences and Linguistic Strategies, in LDV-Forum - Zeitschrift fuer Computerlinguistik und Sprachtechnologie - "Communicating Agents", Band 17, 1,2, (2000) 56-73 5. Delmonte R.: Parsing with GETARUN, Proc.TALN2000, 7? conf?rence annuel sur le TALN, Lausanne, (2000) 133-146 6. Delmonte R., D. Bianchi: From Deep to Partial Understanding with GETARUNS, Proc. ROMAND 2002, Universit? Roma2, Roma, (2002) 57-71  7. Boris Katz, Jimmy J. Lin, Sue Felshin: The START Multimedia Information System: Current Technology and Future Directions, In Proceedings of the International Workshop on Multimedia Information Systems (MIS 2002) 8. Hovy, E., U. Hermjakob, & C. Lin.: The Use of External Knowledge in Factoid QA. In E. M. Voorhees & D. K. Harman (eds.), The Tenth Text Retrieval Conference (TREC 2001). (2002) 644-652 9. Litkowski, K. C.: Syntactic Clues and Lexical Resources in Question-Answering. In E. M. Voorhees & D. K. Harman (eds.), The Ninth Text Retrieval Conference (TREC-9). (2001) 157-166 10. Litkowski, K. C.: CL Research Experiments in TREC-10 Question-Answering. In E. M. Voorhees & D. K. Harman (eds.), The Tenth Text Retrieval Conference (TREC 2001). (2002) 122-131 11. Delmonte R., Sara Tonelli, Marco Aldo Piccolino Boniforti, Antonella Bristot, Emanuele Pianta: VENSES ? a Linguistically-Based System for Semantic Evaluation, RTE Challenge Workshop, Southampton, PASCAL - European Network of Excellence, (2005) 49-52 12. Delmonte R.: Evaluating GETARUNS Parser with GREVAL Test Suite, Proc. ROMAND - 20th International Conference on Computational Linguistics - COLING, University of Geneva, (2004) 32-41. 13. Bresnan J.(ed.): The Mental Representation of  Grammatical Relations, MIT Press, Cambridge Mass., 1982) 14. Barwise J., J.M.Gawron, G.Plotkin, S.Tutiya(eds.): Situation Theory and its Applications, Vol.2, CSLI Lecture Notes No.26, (1991)  
16
Another Evaluation of Anaphora Resolution Algorithms and a
Comparison with GETARUNS? Knowledge Rich Approach
Rodolfo Delmonte, Antonella Bristot, Marco
Aldo Piccolino Boniforti, Sara Tonelli
Department of Language Sciences
Universit? Ca? Foscari ? Ca? Bembo
30120, Venezia, Italy
delmont@unive.it
Abstract
In this paper we will present an evaluation of
current state-of-the-art algorithms for Anaphora
Resolution based on a segment of Susanne
corpus (itself a portion of Brown Corpus), a
much more comparable text type to what is
usually required at an international level for
s u c h a p p l i c a t i o n d o m a i n s a s
Question/Answering, Information Extraction,
Text Understanding, Language Learning. The
portion of text chosen has an adequate size
which lends itself to significant statistical
measurements: it is portion A, counting 35,000
tokens and some 1000 third person pronominal
expressions. The algorithms will then be
compared to our system, GETARUNS, which
incorporates an AR algorithm at the end of a
pipeline of interconnected modules that
instantiate standard architectures for NLP. F-
measure values reached by our system are
significantly higher (75%) than the other ones.
1 Introduction
The problem of anaphora resolution (hence AR)
looms more and more as a prominent one in
unrestricted text processing due to the need to
recover semantically consistent information in most
current NLP applications. This problem does not
lend itself easily to a statistical approach so that
rule-based approaches seem the only viable
solution.
We present a new evaluation of three state-of-the-art
algorithms for anaphora resolution ? GuiTAR,
JavaRAP, MARS ? on the basis of a portion of
Susan Corpus (derived from Brown Corpus) a much
richer testbed than the ones previously used for
evaluation, and in any case a much more
comparable source with such texts as newspaper
articles and stories. Texts used previously ranged
from scientific manuals to descriptive scientific
texts and were generally poor on pronouns and rich
on nominal descriptions. Two of the algorithms ?
GuiTAR and JavaRAP - use Charniak?s parser
output, which contributes to the homogeneity of the
type of knowledge passed to the resolution
procedure. MARS, on the contrary, uses a more
sophisticated input, the one provided by Connexor
FDG-parser. The algorithms will then be compared
to our system, GETARUNS, which incorporated an
AR algorithm at the end of a pipeline of
interconnected modules that instantiate standard
architectures for NLP. The version of the algorithm
presented here is a newly elaborated one, and is
devoted to unrestricted text processing. It is an
upgraded version from the one discussed in
Delmonte (1999;2002a;2002b) and tries to
incorporate as much as possible of the more
sophisticated version implemented in the complete
GETARUN (see Delmonte 1990;1991;1992;1994;
2003;2004).
The paper is organized as follows: in section 2
below we briefly discuss architectures and criteria
for AR of the three algorithms evaluated. In section
3 we present our system. Section 4 is dedicated to a
compared evaluation and a general discussion.
2 The Anaphora Resolution Algorithms
We start by presenting a brief overview of three
state-of-the-art algorithms for anaphora resolution ?
GuiTAR, JavaRAP, MARS.
2.1 JavaRAP
As reported by the authors (Long Qiu, Min-Yen
Kan, Tat-Seng Chua, 2004) of the JAVA
implementation, head-dependent relations required
by RAP are provided by looking into the structural
?argument domain? for arguments and into the
structural ?adjunct domain? for adjuncts. Domain
information is important to establish disjunction
relations, i.e. to tell whether a third person pronoun
can look for antecedents within a certain structural
domain or not. According to Binding Principles,
Anaphors (i.e. reciprocal and reflexive pronouns),
3
must be bound ? search for their binder-antecedent ?
in their same binding domain ? roughly
corresponding to the notion of structural
?argument/adjunct domain?. Within the same
domains, Pronouns must be free. Head-argument or
head-adjunct relation is determined whenever two or
more NPs are sibling of the same VP.
Additional information is related to agreement
features, which in the case of pronominal
expressions are directly derived. As for nominal
expressions, features are expressed in case they are
either available on the verb ? for SUBJect NPs? or
else if they are expressed on the noun and some
other tricks are performed for conjoined nouns.
Gender is looked up in the list of names available on
the web. This list is also used to provide the
semantic feature of animacy.
RAP is also used to find pleonastic pronouns, i.e.
pronouns which have no referents. To detect
conditions for pleonastic pronouns a list of patterns
is indicated, which used both lexical and structural
information.
Salience weight is produced for each candidate
antecedent from a set of salience factors. These
factors include main Grammatical Relations,
Headedness, non Adverbiality, belonging to the
same sentence. The information is computed again
by RAP, directly on the syntactic structure. The
weight computed for each noun phrase is divided by
two in case the distance from the current sentence
increases. Only NPs contained within a distance of
three sentences preceding the anaphor are
considered by JavaRAP.
2.2 GuiTAR
The authors (Poesio, M. and Mijail A. Kabadjov
2004) present their algorithm as an attempt at
providing a domain independent anaphora
resolution module, ?that developers of NLE
applications can pick off the shelf in the way of
tokenizers, POS taggers, parsers, or Named
Entity classifiers?. For these reasons, GuiTAR has
been designed to be as independent as possible from
other modules, and to be as modular as possible,
thus ?allowing for the possibility of replacing
specific components (e.g., the pronoun resolution
component)?.
The authors have also made an attempt at specifying
what they call the Minimal Anaphoric Syntax
(MAS) and have devised a markup language based
on GNOME mark-up scheme. In MAS, Nominal
Expressions constitute the main processing units,
and are identified with the tag NE <ne>, which have
a CAT attribute, specifying the NP type: the-np,
pronoun etc., as well as Person, Number and Gender
attributes for agreement features. Also the internal
structure of the NP is marked with Mod and
NPHead tags.
The pre-processing phase uses a syntactic guesser
which is a chunker of NPs based on heuristics. All
NEs add up to a discourse model ? or better History
List - which is then used as the basic domain where
Discourse Segments are contained. Each Discourse
Segment in turn may be constituted by one or more
Utterances. Each Utterance in turn contains a list of
forward looking centers Cfs.
The Anaphora Resolution algorithm implemented is
the one proposed by MARS which will be
commented below. The authors also implemented a
simple algorithm for resolving Definite Descriptions
on the basis of the History List by a same head
matching approach.
2.3 MARS
The approach is presented as a knowledge poor
anaphora resolution algorithm (Mitkov R.
[1995;1998]), which makes use of POS and NP
chunking, it tries to individuate pleonastic ?it?
occurrences, and assigns animacy. The weighting
algorithm seems to contain the most original
approach. It is organized with a filtering approach
by a series of indicators that are used to boost or
reduce the score for antecedenthood to a given NP.
The indicators are the following ones:
FNP (First NP); INDEF (Indefinite NP); IV
(Indicating Verbs); REI (Lexical Reiteration); SH
(Section Heading Preference); CM (Collocation
Match); PNP (Prepositional Noun Phrases); IR
(Immediate Reference); SI (Sequential Instructions);
RD (Referential Distance); TP (Term Preference),
As the author comments, antecedent indicators
(preferences) play a decisive role in tracking down
the antecedent from a set of possible candidates.
Candidates are assigned a score (-1, 0, 1 or 2) for
each indicator; the candidate with the highest
aggregate score is proposed as the antecedent.
The authors comment is that antecedent indicators
have been identified empirically and are related
to salience (definiteness, givenness, indicating
verbs, lexical reiteration, section heading
preference, "non- prepositional" noun phrases), to
structural matches (collocation, immediate
reference), to referential distance or to preference
of terms. However it is clear that most of the
indicators have been suggested for lack of better
information, in particular no syntactic constituency
was available.
In a more recent paper (Mitkov et al, 2003) MARS
has been fully reimplemented and the indicators
updated. The authors seem to acknowledge the fact
that anaphora resolution is a much more difficult
task than previous work had suggested, In
4
unrestricted text analysis, the tasks involved in the
anaphora resolution process contribute a lot of
uncertainty and errors that may be the cause for low
performance measures.
The actual algorithm uses the output of Connexor?s
FDG Parser, filters instances of ?it? and eliminates
pleonastic cases, then produces a list of potential
antecedents by extracting nominal and pronominal
heads from NPs preceding the pronoun. Constraints
are then applied to this list in order to produce the
?set of competing candidates? to be considered
further, i.e. those candidates that agree in number
and gender with the pronoun, and also obey
syntactic constraints. They also introduced the use
of Genetic Algorithms in the evaluation phase.
The new version of MARS includes three new
indicators which seem more general and applicable
to any text, so we shall comment on them.
Frequent Candidates (FC) ? this is a boosting score
for most frequent three NPs; Syntactic Parallelism
(SP) ? this is a boosting score for NPs with the same
syntactic role as the pronoun, roles provided by the
FDG-Parser; Boost Pronoun (BP) ? pronoun
candidates are given a bonus (no indication of
conditions for such a bonus).
The authors also reimplemented in a significant way
the indicator First NPs which has been renamed,
?Obliqueness (OBL) ? score grammatical functions,
SUBJect > OBJect > IndirectOBJect > Undefined?.
MARS has a procedure for automatically identifying
pleonastic pronouns: the classification is done by
means of 35 features organized into 6 types and are
expressed by a mixture of lexical and grammatical
heuristics. The output should be a fine-grained
characterization of the phenomenon of the use of
pleonastic pronouns which includes, among others,
discourse anaphora, clause level anaphora and
idiomatic cases.
In the same paper, the authors deal with two more
important topics: syntactic constraints and animacy
identification.
3 GETARUNS
In a number of papers (Delmonte 1990;1991;
1992;1994; 2003;2004) and in a book (Delmonte
1992) we described our algorithms and the
theoretical background which inspired it. Whereas
the old version of the system had a limited
vocabulary and was intended to work only in limited
domains with high precision, the current version of
the system has been created to cope with
unrestricted text. In Delmonte (2002), we reported
preliminary results obtained on a corpus of
anaphorically annotated texts made available by
R.Mitkov on his website. Both definite descriptions
and pronominal expressions were considered,
success rate was at 75% F-measure. In those case
we used a very shallow and robust parser which
produced only NP chunks which were then used to
fire anaphoric processes. However the texts making
up the corpus were technical manuals, where the
scope and usage of pronominal expressions is very
limited.
The current algorithm for anaphora resolution works
on the output of a complete deep robust parser
which builds an indexed linear list of dependency
structures where clause boundaries are clearly
indicated; differently from Connexor, our system
elaborates both grammatical relations and semantic
roles information for arguments and adjuncts.
Semantic roles are very important in the weighting
procedures. Our system also produces implicit
grammatical relations which are either controlled
SUBJects of untensed clauses, arguments or
adjuncts of relative clauses.
As to the anaphoric resolution algorithm, it is based
on the original Sidner?s (1983:Chapter 5) and
Webber?s (1983:Chapter 6) intuitions on Focussing
in Discourse. We find distributed, local approaches
to anaphora resolution more efficient than
monolithic, global ones. In particular we believe
that due to the relevance of structural constraints in
the treatment of locally restricted classes of
pronominal expressions, it is more appropriate to
activate different procedures which by dealing
separately with non-locally restricted classes also
afford separate evaluation procedures. There are
also at least two principled reasons for the
separation into two classes.
The first reason is a theoretical one. Linguistic
theory has long since established without any doubt
the existence in most languages of the world of at
least two classes: the class of pronouns which must
be bound locally in a given domain and the class of
pronouns which must be left free in the same
domain ? as a matter of fact, English also has a third
class of pronominals, the so-called long-distance
subject-of-consciousness bound pronouns (see
Zribi-Hertz A., 1989);
The second reason is empirical. Anaphora resolution
is usually carried out by searching antecedents
backward w.r.t. the position of the current anaphoric
expression. In our approach, we proceed in a clause
by clause fashion, weighting each candidate
antecedent w.r.t. that domain, trying to resolve it
locally. Weighting criteria are amenable on the one
hand to linear precedence constraints, with scores
assigned on a functional/semantic basis. On the
other hand, these criteria may be overrun by a
functional ranking of clauses which requires to treat
main clauses differently from secondary clauses,
5
and these two differently from complement clauses.
On the contrary, global algorithms neglect
altogether such requirements: they weight each
referring expression w.r.t. the utterance, linear
precedence is only physically evaluated, no
functional correction is introduced.
3.1 Referential Policies and Algorithms
There are also two general referential policy
assumption that we adopt in our approach: The first
one is related to pronominal expressions, the second
one to referring expressions or entities to be asserted
in the History List, and are expressed as follows:
- no more than two pronominal expressions are
allowed to refer back in the previous discourse
portion;
- at discourse level, referring expressions are
stored in a push-down stack according to
Persistence principles.
Persistence principles respond to psychological
principles and limit the topicality space available to
user w.r.t. a given text. It has a bidimensional
nature: it is determined both in relation to an overall
topicality frequency value and to an utterance
number proximity value.
Only ?persistent? referring expressions are allowed
to build up the History List, where persistence is
established on the basis of the frequency of
topicality for each referring expression which must
be higher than 1. All referring expression asserted as
Topic (Secondary, Potential) only once are
discarded in case they appeared at a distance
measured in 5 previous utterances. Proximate
referring expressions are allowed to be asserted in
the History List.
In particular, if Mitkov considers the paragraph as
the discourse unit most suitable for coreferring and
cospecifying operation at discourse level, we prefer
to adopt a parameterized procedure which is
definable by the user and activated automatically: it
can be fired within a number that can vary from
every 10 up to 50 sentences. Our procedure has the
task to prune the topicality space and reduce the
number of perspective topic for Main and
Secondary Topic. Thus we garbage-collect all non-
relevant entities. This responds to the empirically
validated fact that as the distance between first and
second mention of the same referring expression
increases, people are obliged to repeat the same
linguistic description, using a definite expression or
a bare NP. Indefinites are unallowed and may only
serve as first mention; they can also be used as
bridging expression within opaque propositions. The
first procedure is organized as follows:
A. For each clause,
1. we collect all referential expressions and
weight them (see B below for criteria) ? this
is followed by an automatic ranking;
2. then we subtract pronominal expressions;
3. at clause level, we try to bind personal and
possessive pronouns obeying specific
structural properties; we also bind reflexive
pronouns and reciprocals if any, which must
be bound obligatorily in this domain;
4. when binding a pronoun, we check for
disjointness w.r.t. a previously bound
pronoun if any;
5. all unbound pronouns and all remaining
personal pronouns are asserted as
?externals?, and are passed up to the higher
clause levels;
B. Weighting is carried out by taking into account
the following linguistic properties associated to each
referring expression:
1. Grammatical Function with usual hierarchy
(SUBJ > ARG_MOD > OBJ > OBJ2 > IOBJ >
NCMOD);
2. Semantic Roles, as they have been labelled in
FrameNet, and in our manually produced
frequency lexicon of English;
3. Animacy: we use 75 semantic features derived
from WordNet, and reward Human and
Institution/Company labelled referring
expressions;
4. Functional Clause Type is further used to
introduce penalties associated to those referring
expressions  which don?t belong to main clause.
C. Then we turn at the higher level ? if any -, and
we proceed as in A., in addition
1. we try to bind pronouns passed up by the lower
clause levels
o if successful, this will activate a retract of the
?external? label and a label of
?antecedenthood? for the current pronoun
with a given antecedent;
o the best antecedent is chosen by recursively
trying to match features of the pronoun with
the first available antecedent previously
ranked by weighting;
o here again whenever a pronoun is bound we
check for disjointness at utterance level.
D. This is repeated until all clauses are examined
and all pronouns are scrutinised and bound or left
free.
E. Pronouns left free ? those asserted as externals ?
will be matched tentatively with the best candidates
provided this time by a ?centering-like? algorithm.
Step A. is identical and is recursively repeated until
all clauses are processed.
6
Then, we move to step B. which in this case will use
all referring expressions present in the utterance,
rather than only those available locally.
Fig. 1 GETARUNS AR algorithm
3.2 Focussing Revisited
Our version of the focussing algorithm follows
Sidner?s proposal (Sidner C., 1983; Grosz B., Sidner
C., 1986), to use a Focus Stack, a certain Focus
Algorithm with Focus movements and data
structures to allow for processing simple inferential
relations between different linguistic descriptions
co-specifying or coreferring to a given entity.
Our Focus Algorithm is organized as follows: for
each utterance, we assert three ?centers? that we call
Main, Secondary and the first Potential Topic,
which represent the best three referring expressions
as they have been weighted in the candidate list
used for pronominal binding; then we also keep a
list of Potential Topics for the remaining best
candidates. These three best candidates repositories
are renovated at each new utterance, and are used
both to resolve pronominal and nominal
cospecification and coreference: this is done both in
case of strict identity of linguistic description and of
non-identity. The second case may occur either
when derivational morphological properties allow
the two referring expressions to be matched
successfully, or when a simple hyponym/hypernym
relation is entertained by two terms, one of which is
contained in the list of referring expressions
collected from the current sentence, and the other is
among one of the entities stored in the focus list.
The Main Topic may be regarded the Forward
Looking Center in the centering terminology or the
Current Focus. All entities are stored in the History
List (HL) which is a stack containing their
morphological and semantic features: this is not to
be confused with a Discourse Model - what we did
in the deep complete system anaphora resolution
module ? which is a highly semantically wrought
elaboration of the current text. In the HL every new
entity is assigned a semantic index which identifies
it uniquely. To allow for Persistence evaluation, we
also assert rhetorical properties associated to each
entity, i.e. we store the information of topicality (i.e.
whether it has been evaluated as Main, Secondary or
Potential Topic), together with the semantic ID and
the number of the current utterance. This is
subsequently used to measure the degree of
Persistence in the overall text of a given entity, as
explained below.
In order to decide which entity has to become Main,
Secondary or Potential Topic we proceed as
follows:
- we collect all entities present in the History List
with their semantic identifier and feature list
and proceed to an additional weighting
procedure;
- nominal expressions, they are divided up into
four semantic types: definite, indefinite, bare
NPs, quantified NPs. Both definite and
indefinite NP may be computed as new or old
entity according to contextual conditions as
will be discussed below and are given a
rewarding score;
- we enumerate for each entity its persistence in
the previous text, and keep entities which have
frequency higher than 1, we discard the others;
- we recover entities which have been asserted in
the HL in proximity to the current utterance, up
to four utterances back;
- we use this list to ?resolve? referring
expressions contained in the current utterance;
- if this succeeds, we use the ?resolved? entities
as new Main, Secondary, and Potential Topics
and assert the rest in the Potential Topics stack;
- if this fails ? also partially ? we use the best
candidates in the weighted list of referring
expressions to assert the new Topics. It may be
the case that both resolved and current best
candidates are used, and this is by far the most
common case.
4. Evaluation and General Discussion
Evaluating anaphora resolution systems calls for a
reformulation of the usual parameters of Precision
and Recall as introduced in IR/IE field: in that case,
there are two levels that are used as valuable results;
a first stage where systems are measured for their
7
capacity to retrieve/extract relevant items from the
corpus/web (coverage-recall). Then a second stage
follows in which systems are evaluated for their
capacity to match the content of the query
(accuracy-precision). In the field of IR/IE items to
be matched are usually constituted by words/phrases
and pattern-matching procedures are the norm.
However, for AR systems this is not sufficient and
NLP heavy techniques are used to get valuable
results. As Mitkov also notes, this phase jeopardizes
the capacity of AR systems to reach satisfactory
accuracy scores simply because of its intrinsic
weakness: none of the off-the-shelf parsers currently
available overcomes 90% accuracy.
To clarify these issues, we present here below two
Tables: in the first one we report data related to the
vexed question of whether pleonastic ?it? should be
regarded as part of the task of anaphora resolution
or rather part of a separate classification task ? as
suggested in a number of papers by Mitkov. In the
former case, they should contribute to the overall
anaphora resolution evaluation metrics; in the latter
case they should be compute separately as a case of
classification over all occurrences of ?it? in the
current dataset and discarded from the overall count.
Even though we don?t agree fully with Mitkov?s
position, we find it useful to deal with ?it? separate,
due to its high inherent ambiguity. Besides, it is true
that the AR task is not like any Information
Retrieval task.
In Table 1 below we reported figures for ?it? in
order to evaluate the three algorithms in relation to
the classification task. Then in Table 2. we report
general data where we computed the two types of
accuracy reported in the literature. In Table 1 we
split results for ?it? into Wrong Reference vs Wrong
Classification: following Mitkov, in case we only
computed anaphora related cases and disregarded
those cases of ?it? which were wrongly classified as
expletives. Expletive ?it? present in the text are 189:
so at first we computed coverage and accuracy with
the usual formula that we report below. Then we
subtracted wrongly classified cases from the number
of total ?it? found in one case (following Mitkov
who claims that wrongly classified ?it? found by the
system should not count; in another case, this
number is subtracted from the total number of ?it?
to be found in the text. Only for MARS we then
computed different measures of Coverage and
Accuracy. If we regard this approach worth
pursuing, we come up with two Adjusted Accuracy
measures which are related to the revised total
numbers of anaphors by the two subtractions
indicated above.
We computed manually all third person pronominal
expressions and came up with a figure 982 which is
Table 1. Expletive ?it? compared results
MARS JavaRAP GuiTAR GETARUNS
Coverage 163  (86.2%) 188  (99.5%) 188  (99.5%) 171  (91%)
Accuracy 1 63  (33.3%) 73  (38.6%) 75  (39.7%) 87  (46 %)
Wrong Classification 44
163-44=119
189-44=145
49
189-49=140
64
189-64=125
53
189-53=136
Wrong Reference 56 66 49 32
Accuracy 2 63  (38.6%)
Adjusted Accuracy 2 63  (52.9%)
Adjusted Accuracy 3 63  (43.4%) 73  (52.1%) 75  (60%) 87 (64 %)
only confirmed by one of the three systems
considered: JavaRAP. Pronouns considered are the
following one, lower case and upper case included:
Possessives ? his, its, her, hers, their, theirs
Personals ? he, she, it, they, him, her, it, them
(where ?it? and ?her? have to be disambiguated)
Reflexives ? himself, itself, herself, themselves
There are 16 different wordforms. As can be seen
from the table below, apart from JavaRAP, none of
the other systems considered comes close to 100%
coverage.
Computing general measures for Precision and
Recall we have three quantities (see also Poesio &
Kabadjov):
? total number of anaphors present in the text;
? anaphors identified by the system;
? correctly resolved anaphors.
Formulas related to Accuracy/Success Rate or
Precision are as follows: Accuracy1 = number of
successfully resolved anaphors/number of all
anaphors; Accuracy2 = number of successfully
resolved anaphors/number of anaphors found
(attempted to be resolved). Recall - which should
correspond to Coverage - we come up with formula:
R= number of anaphors found /number of all
anaphors to be resolved (present in the text). Finally
the formula for F-measure is as follows:
2*P*R/(P+R) where P is chosen as Accuracy 2.
8
Table 2. Overall results Coverage/Accuracy
COVERAGE ACCURACY 1 ACCURACY 2 F-measure
MARS 936  (95.3%) 403/982  (41.5%) 403/903 (43%) 59.26%
JavaRAP 981  (100%) 490/982  (49.9%) 490/981 (50%) 66.7%
GUITAR 824  (84.8%) 445/982  (45.8%) 445/824 (54%) 65.98%
GETARUNS 885  (90.1%) 555/982  (56.5%) 555/885 (62.7%) 73.94%
In absolute terms best accuracy figures have been
obtained by GETARUNS, followed by JavaRAP. So
it is still thanks to the classic Recall formula that
this result stands out clearly. We also produced
another table which can however only be worked
out for our system, which uses a distributed
approach. We managed to separate pronominal
expressions in relation to their contribution at the
different levels of anaphora resolution considered:
clause level, utterance level, discourse level. At
clause level, only those pronouns which must be
bound locally are checked, as is the case with
reflexive pronouns, possessives, some cases of
expletive ?it?: both arguments and adjuncts may
contribute the appropriate antecedent. At utterance
level, in case the sentence is complex or there is
more than one clause, also personal subject/object
pronouns may be bound (if only preferentially so).
Eventually, those pronouns which do not find an
antecedent are regarded discourse level pronouns.
We collapsed under CLAUSE all pronouns bound at
clause and utterance level; DISCOURSE contains
only sentence external pronouns. Expletives have
been computed in a separate column.
Table 3. GETARUNS pronouns collapsed at structural level
CLAUSE DISCOURSE EXPLETIVES TOTALS
Pronouns found 410 366 109 885
Correct 266 222 67 555
Errors made 144 144 42 330
As can be noticed easily, the highest percentage of
pronouns found is at Clause level: this is not
however the best performance of the system, which
on the contrary performs better at discourse level.
Expletives contribute by far the highest correct
result. We also found correctly 47 ?there? expletives
and 6 correctly classified pronominal ?there? which
however have been left unbound. The system also
found 48 occurrences of deictic discourse bound
?this? and ?that?, which corresponds to the full
coverage.
Finally, nominal expressions: the History List (HL)
has been incremented up to 2243 new entities. The
system identified 2773 entities from the HL by
matching their linguistic description. The overall
number of resolution actions taken by the Discourse
Level algorithm is 1861: this includes both cases of
nominal and pronominal expressions. However,
since only 366 can be pronouns, the remaining 1500
resolution actions have been carried out on nominal
expressions present in the HL. If we compare these
results to the ones computed by GuiTAR, which
assign semantic indices to NamedEntities
disregarding their status of anaphora, we can see
that the whole text is made up of 12731 NEs.
GuiTAR finds 1585 cases of identity relations
between a NE and an antecedent. However,
GuiTAR introduces always new indices and creates
local antecedent-referring expression chains rather
than repeating the same index of the chain head. In
this way, it is difficult if not impossible to compute
how many times the text corefers/cospecifies to the
same referring expressions. On the contrary, in our
case, this can be easily computed by counting how
many times the same semantic index is being
repeated in a ?resolution? or ?identity? action of the
anaphora resolution algorithm. For instance, the
Jury is coreferred/cospecified 12 times; Price Daniel
also 12 times and so on.
5. Conclusions
The error rate of both Charniak?s and Connexor?s as
reported in the literature, is approximately the same,
20%; this notwithstanding, MARS has a slightly
reduced coverage when compared with JavaRAP,
96%. GuiTAR has the worst coverage, 85%. As to
accuracy, none of the three algorithms overruns
50%: JavaRAP has the best score 49.9%. However
GETARUNS has 63% correct score, with 90%
coverage.
There are at least three reasons why our system has a
better performance: one is the presence of a richer
functional and semantic information as explained
above, which comes with augmented head-
dependent structures. Second reason is the decision
to split the referential process into two and treat
utterance level pronominal expressions separately
from discourse level ones. Third reason is the way
in which discourse level anaphora resolution is
9
organized: our version of the Centering algorithm
hinges on a record of a list of best antecedents
weighted on the basis of their behaviour in History
List and on their intrinsic semantic properties. These
three properties of our AR algorithm can be dubbed
the Knowledge Rich approach.
F-measures approximates very closely what we
obtained in a previous experiment: however, as a
whole it is an insufficient score to insure adequate
confidence in semantic substitution of anaphoric
items by the head of the antecedent. Improvements
need to come from parsing and the lexical
component.
Acknowledgements
Thanks to three anonymous reviewers who helped us
improve the overall layout of the paper.
References
Delmonte R. 1990. Semantic Parsing with an LFG-based
Lexicon and Conceptual Representations, Computers
& the Humanities, 5-6, pp.461-488.
Delmonte R. and D.Bianchi 1991. Binding Pronominals
with an LFG Parser, Proceeding of the Second
International Workshop on Parsing Technologies,
Cancun(Messico), ACL 1991, pp.59-72.
Delmonte R., D.Bianchi 1992. Quantifiers in Discourse,
in Proc. ALLC/ACH'92, Oxford(UK), OUP, pp. 107-
114.
Delmonte R. 1992. Linguistic and Inferential Processing
in Text Analysis by Computer, UP, Padova.
Delmonte R. and D.Bianchi 1994. Computing Discourse
Anaphora from Grammatical Representation, in
D.Ross & D.Brink(eds.), Research in Humanities
Computing 3, Clarendon Press, Oxford, 179-199.
Delmonte R. and D.Bianchi 1999. Determining Essential
Properties of Linguistic Objects for Unrestricted Text
Anaphora Resolution, Proc. Workshop on Procedures
in Discourse, Pisa, pp.10-24.
Delmonte R., L.Chiran, and C.Bacalu, (2000). Towards
An Annotated Database For Anaphora Resolution,
LREC, Atene, pp.63-67.
Delmonte R. 2002a. From Deep to Shallow Anaphora
Resolution: What Do We Lose, What Do We Gain, in
Proc. International Symposium RRNLP, Alicante,
pp.25-34.
Delmonte R. 2002b. From Deep to Shallow Anaphora
Resolution:, in Proc. DAARC2002 , 4th Discourse
Anaphora and Anaphora Resolution Colloquium,
Lisbona, pp.57-62.
Delmonte, R. 2003. Getaruns: a Hybrid System for
Summarization and Question Answering. In Proc.
Natural Language Processing (NLP) for Question-
Answering, EACL, Budapest, pp. 21-28.
Delmonte R. 2004. Evaluating GETARUNS Parser with
GREVAL Test Suite, In Proc. ROMAND - 20th
COLING, University of Geneva, pp. 32-41.
Di Eugenio B. 1990. Centering Theory and the Italian
pronominal system, COLING, Helsinki.
Grosz B. and C. Sidner 1986. Attention, Intentions, and
the Structure of Discourse, Computational Linguistics
12 (3), 175-204.
Kennedy, C. and B. Boguraev, 1996. Anaphora for
everyone: Pronominal anaphora resolution without a
parser. In Proc. of the 16th COLING, Budapest.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua, 2004. A
Public Reference Implementation of the RAP
Anaphora Resolution Algorithm, In Proceedings of the
Language Resources and Evaluation Conference 2004
(LREC 04), Lisbon, Portugal, pp.1-4.
Mitkov R. 1995. Two Engines are better than one:
Generating more power and confidence in the search
for the antecedent, Proceedings of Recent Advances in
Natural Language Processing, Tzigov Chark, 87-94.
Mitkov, R. 1998. Robust Pronoun Resolution with
limited knowledge. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING?98)/ACL?98 Conference, pp.
869-875, Montreal, Canada.
Mitkov, R., R. Evans, and C. Orasan. 2002. A New, Fully
Automatic Version of Mitkov?s Knowledge-Poor
Pronoun Resolution Method, Proceedings of CICLing-
2002, pp.1-19.
Poesio, M. and R. Vieira, 1998. A corpus-based
investigation of definite description use.
Computational  Linguistics, 24(2):183?216.
Poesio, M. and Mijail A. Kabadjov 2004. A General-
Purpose, off-the-shelf Anaphora Resolution Module:
Implementation and Preliminary Evaluation
Proceedings of the Language Resources and
Evaluation Conference 2004 (LREC 04), Lisbon,
Portugal, pp.1-4.
Sidner C. 1983. Focusing in the Comprehension of
Definite Anaphora, in Brady M., Berwick R.(eds.),
Computational Models of Discourse, MIT Press,
Cambridge, MA, 267-330.
Webber B. 1983. So can we Talk about Now?, in Brady
M., Berwick R.(eds.), Computational Models of
Discourse, MIT Press, Cambridge, MA, 331-371.
Webber B. L. 1991. Structure and Ostension in the
Interpretation of Discourse Deixis, in Language and
Cognitive Processes 6 (2):107-135.
Zribi-Hertz A. 1989. Anaphor Binding and Narrative
Point of View: English reflexive pronouns in sentence
and discourse, Language, 65(4):695-727.
10
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 48?53,
Prague, June 2007. c?2007 Association for Computational Linguistics
Entailment and Anaphora Resolution in RTE3 Rodolfo Delmonte, Antonella Bristot, Marco Aldo Piccolino Boniforti, Sara Tonelli Department of Language Sciences Universit? Ca? Foscari ? Ca? Bembo 30123, Venezia, Italy delmont@unive.it
Abstract We present VENSES, a linguistically-based approach for semantic inference which is built around a neat division of labour between two main components: a grammatically-driven subsystem which is responsible for the level of predicate-arguments well-formedness and works on the output of a deep parser that produces augmented head-dependency structures. A second subsystem fires allowed logical and lexical inferences on the basis of different types of structural transformations intended to produce a semantically valid meaning correspondence. In the current challenge, we produced a new version of the system, where we do away with grammatical relations and only use semantic roles to generate weighted scores. We also added a number of additional modules to cope with fine-grained inferential triggers which were not present in previous dataset. Different levels of argumenthood have been devised in order to cope with semantic uncertainty generated by nearly-inferrable Text-Hypothesis pairs where the interpretation needs reasoning. RTE3 has introduced texts of paragraph length: in turn this has prompted us to upgrade VENSES by the addition of a discourse level anaphora resolution module, which is paramount to allow entailment in pairs where the relevant portion of text contains pronominal expressions. We present the system, its relevance to the task at hand and an evaluation. 1 Introduction The system for semantic evaluation VENSES (Venice Semantic Evaluation System) is organized as a pipeline of two subsystems: the first is a 
reduced version of GETARUN, our system for Text Understanding; the second is the semantic evaluator which was previously created for Summary and Question evaluation and has now been thoroughly revised for the new more comprehensive RTE task. The reduced GETARUN is composed of the usual sequence of sub-modules common in Information Extraction systems,  i.e. a tokenizer, a multiword and NE recognition module, a PoS tagger based on finite state automata; then a multilayered cascaded RTN-based parser which produces c-structures and has an additional module to map them into tentative grammatical functions in LFG terms. The functionally labeled constituents are then passed to an interpretation module that uses subcategorization information to choose final grammatical relations and assign semantic roles. Eventually, the system has a pronominal binding module that works at clause level for lexical personal, possessive and reflexive pronouns, which are substituted by the heads of their antecedents - if available. The output of the binding module can contain one or more ?external? pronouns, which need to be bound in the discourse. This output is passed to the Anaphora Resolution module presented in detail in Delmonte (2006) and outlined below. This module works on the so-called History List of entities present in the text so far. In order to make the output of this module usable by the Semantic Evaluator, we decided to produce a flat list of semantic vectors which contain all semantic related items of the current sentence. Inside these vectors, pronominal expressions are substituted by the heads of their antecedents. Basically, the output of the system is elaborated on top of the output of the parser, which produces a flat list of fully indexed augmented head-dependent structures (AHDS) with Grammatical Relations (GRs) and Semantic Roles (SRs) labels. Notable additions to the usual formalism is the presence of a distinguished Negation relation; we 
48
also mark modals and progressive mood, tense and voice (for similar approaches see Bos et al, Raina et al).  The evaluation system uses a cost model with rewards/penalties for T/H pairs where text entailment is interpreted in terms of semantic similarity: the closest the T/H pairs are in semantic terms the more probable is their entailment. Rewards in terms of scores are assigned for each "similar" semantic element; penalties on the contrary can be expressed in terms of scores or they can determine a local failure and a consequent FALSE decision ? more on scoring below. The evaluation system is made up of two main Modules: the first takes care of paraphrases and idiomatic expressions; the second is a sequence of linguistic rule-based sub-modules. Their work is basically a count of how much similar are linguistic elements in the H/T pair. Similarity may range from identical linguistic descriptions, to synonymous or just morphologically derivable ones. As to GRs and SRs, they are scored higher according to whether they belong to the subset of core relations and roles, i.e. obligatory arguments, or not, that is adjuncts. Both Modules go through General Consistency checks which are targeted to high level semantic attributes like presence of modality, negation, and opacity operators. The latter ones are expressed either by the presence of discourse markers of conditionality or by a secondary level relation intervening between the main predicate and a governing higher predicate belonging to the class of non factual verbs, but see below. All rule-based sub-modules are organized into a sequence of syntactic-semantic transformation rules going from those containing axiomatic-like paraphrase HDSs which are ranked higher, to rules stating conditions for similarity according to a scale of argumentality (more below) and are ranked lower. All rules address HDSs and SRs. Propositional level ones have access to vectors of semantic features which encode propositional level information. 2   The Task of Semantic Inference Evaluation As happened in the previous Challenge, this year?s Challenge is also characterized by the presence of a relatively high number (we counted 
more than 100 True and another 100 False pairs, i.e. 25%) of T/H pairs which require two particularly hard NLP processes to set up: - reasoning  - paraphrases (reformulation) In addition to that, we found a significant number of pairs ? about 150 in the development set and some 100 in the test set ? in which pronominal binding and anaphora resolution are essential to the definition of entailment. In particular, in such cases it is only by virtue of the substitution of a pronominal expression with the linguistic description of its antecedent that the appropriate Predicate-Argument relations required in order to fire the inference is made available ? but see below. Setting up rules for Paraphrase evaluation, requires the system to actually use the lemmas ? and in some cases the wordforms - to be matched together in axiomatic-like structures: in other words, in these cases the actual linguistic expressions play a determining role in the task to derive a semantic inference. In order for these rules to be applied by the SE, we surmise it is important to address a combination of Syntactic and Semantic structures, where Lexical Inference also may play a role: the parser in this case has a fundamental task of recovering the heads of antecedents of all possible pronominal and unexpressed Grammatical relations. It is important to remark that besides pronominal-antecedent relations, our system also recovers all those cases defined as Control in LFG theory, where basically the unexpressed subject of an untensed proposition (infinitival, participial, gerundive) either lexically, syntactically or structurally controlled is bound to some argument of the governing predicate.  2.1   Pronominal Binding and Anaphora Resolution This year RTE introduces as a novelty a certain number (117 in the Test set ? 135 in the Dev set) of long Texts, of paragraph length. This move is justified by the need to address more realistic data, and consequently to tune the whole process of semantic evaluation to the problems related to such data. Thus more relevance is given to empirical issues to be tackled, rather than to the theoretical ones, which however don?t disappear but may assume less importance. 
49
When a system has to cope with paragraph length texts, the basic difference with short texts regards the problem of anaphora resolution. In short texts, pronominal expressions constituted a minor 
problem and all referring expressions were specified fully. Not so in long texts, as can be seen from the Table below: 
  He Him His She Her It Its They Their Them Total Test 80 15 91 19 18 91 68 43 63 15 485 Dev. 113 16 136 27 35 123 76 44 64 18 652 Total 193 31 227 46 53 214 144 87 127 33 1137 Table 1. 3rd person pronominal expressions contained in RTE3 data sets  As can be seen from this table, the problem a system is faced with is not just to cope with an ad hoc solution for single cases where the pronoun is placed, for instance, in sentence first position and it might be easy to recover its antecedent by some empirical ad hoc procedure. The problem needs to be addressed fully and this requires a full-fledged system for anaphora resolution. One such system is shown in Fig. 2 below, where we highlight the architecture and main processes undergoing at the anaphora level. First of all, the subdivision of the system into two levels: Clause level ? intrasentential pronominal phenomena ? where all pronominal expressions contained in modifiers, adjuncts or complement clauses receive their antecedent locally. Possessive pronouns, pronouns contained in relative clauses and complement clauses choose preferentially their antecedents from list of higher level referring expressions. Not so for those pronouns contained in matrix clauses. In particular the ones in subject position are to be coreferred in the discourse. This requires the system to be equipped with a History List of all referring expressions to be used when needed. In the system, three levels are indicated: Clause level, i.e. simple sentences; Utterance level, i.e. complex sentences; Discourse level, i.e. intersententially. Our system computes semantic structures in a sentence by sentence fashion and any information useful to carry out anaphoric processes needs to be made available to the following portion of text, and eventually to the Semantic Evaluation that computes entailment. We will comment a number of significant examples to clarify the way in which our system operates. 3. Anaphora Resolution for RTE Why is it important to implement an anaphora resolution algorithm for RTE? I think the reason is quite straightforward: pronominal expressions do 
not allow any inference to be drawn or any otherwise semantic similarity processes to be fired, being inherently referentially poor. In order to be able to use information made available by the verb in the sentence in which a pronoun is used, the antecedent must be recovered and the pronoun substituted by its head. So, very simply, RTE needs anaphora resolution in order to allow the system to use verb predicates where pronouns have been used to corefer to some antecedent in the previous text. In turn that verb predicate is just what is being searched for in the first place, and in our case it is the one contained in the Hypothesis. The current algorithm for anaphora resolution works on the output of the complete deep robust parser which builds an indexed linear list of dependency structures where clause boundaries are clearly indicated. As said above, our system elaborates both grammatical relations and semantic roles information for arguments and adjuncts. Semantic roles are very important in the weighting procedures.  As to the anaphoric resolution algorithm, it is a distributed, local ? clause-based - approach to anaphora resolution which we regard more efficient than monolithic, global ones. Linguistic theory has long since established without any doubt the existence in most languages of the world of at least two classes of pronouns: the class which must be bound locally in a given domain ? roughly the clause, and the class which must be left free in the same domain. In our approach, we proceed in a clause by clause fashion, weighting each candidate antecedent w.r.t. that domain, trying to resolve it locally. Weighting criteria are amenable on the one hand to linear precedence constraints, with scores assigned on a functional/semantic basis. On the other hand, these criteria may be overrun by a functional ranking of clauses which requires to 
50
treat main clauses differently from secondary clauses, and these two differently from complement clauses.  There are also two general referential policy assumption that we adopt in our approach: The first one is related to pronominal expressions, the second one to referring expressions or entities to be asserted in the History List, and are expressed as follows: - no more than two pronominal expressions are allowed to refer back in the previous discourse portion; - at discourse level, referring expressions are stored in a push-down stack according to Persistence principles. Only ?persistent? referring expressions are allowed to build up the History List, where persistence is established on the basis of the frequency of topicality for each referring expression which must be higher than 1. All referring expression asserted as Topic (Main, Secondary, Potential) only once are discarded in case they appeared at a distance measured in 5 previous utterances. Proximate referring expressions are allowed to be asserted in the History List. The first procedure is organized as follows.  A. For each clause, 1. we collect all referential expressions and weight them ? this is followed by an automatic ranking; 2. then we subtract pronominal expressions; 3. at clause level, we try to bind personal and possessive pronouns obeying specific structural properties; we also bind reflexive pronouns and reciprocals if any, which must be bound obligatorily in this domain; 4. when binding a pronoun, we check for disjointness w.r.t. a previously bound pronoun if any; 5. all unbound pronouns and all remaining personal pronouns are asserted as ?externals?, and are passed up to the higher clause levels; B. Then we turn at the higher level ? if any -, and we proceed as in A., in addition we try to bind pronouns passed up by the lower clause levels o if successful, this will activate a retract of the ?external? label and a label of ?antecedenthood? for the current pronoun with a given antecedent; o the best antecedent is chosen by recursively trying to match features of the pronoun with the 
first available antecedent previously ranked by weighting; o here again whenever a pronoun is bound we check for disjointness at utterance level.  
 Fig. 1. Anaphoric Processes in VENSES  C. This is repeated until all clauses are examined and all pronouns are scrutinised and bound or left free. D. Pronouns left free ? those asserted as externals ? will be matched tentatively with the best candidates provided this time by a ?centering-like? algorithm. Step A. is identical and is recursively repeated until all clauses are processed. 3.1 Focussing Revisited Our version of the focussing algorithm follows Sidner?s proposal (Sidner C., 1983; Grosz B., Sidner C., 1986), to use a Focus Stack, a certain Focus Algorithm with Focus movements and data structures to allow for processing simple inferential relations between different linguistic descriptions co-specifying or coreferring to a given entity.  Our Focus Algorithm is organized as follows: for each utterance, we assert three hierarchically ordered ?centers? that we call Main, Secondary and the first Potential Topic, which represent the best three referring expressions as they have been weighted in the candidate list used for pronominal binding; then we also keep a list of Potential Topics for the remaining best candidates. These 
51
three best candidates repositories are renovated at each new utterance, and are used both to resolve pronominal and nominal cospecification and coreference: this is done both in case of strict identity of linguistic description and of non-identity.  The Main Topic may be regarded the Forward Looking Center in the centering terminology or the Current Focus. All entities are stored in the History List (HL) which is a stack containing their morphological and semantic features. In the HL every new entity is assigned a semantic index which identifies it uniquely. To allow for Persistence evaluation, we also assert rhetorical properties associated to each entity, i.e. we store the information of topicality (i.e. whether it has been evaluated as Main, Secondary or Potential Topic), together with the semantic ID and the number of the current utterance. This is subsequently used to measure the degree of Persistence in the overall text of a given entity, as explained below. In order to decide which entity has to become Main, Secondary or Potential Topic we proceed as follows: - we collect all entities present in the History List with their semantic identifier and feature list and proceed to an additional weighting procedure; - nominal expressions, they are divided up into four semantic types: definite, indefinite, bare NPs, quantified NPs. Both definite and indefinite NP may be computed as new or old entity according to contextual conditions as will be discussed below and are given a rewarding score; - we enumerate for each entity its persistence in the previous text, and keep entities which have frequency higher than 1, we discard the others; - we recover entities which have been asserted in the HL in proximity to the current utterance, up to four utterances back; - we use this list to ?resolve? referring expressions contained in the current utterance; - if this succeeds, we use the ?resolved? entities as new Main, Secondary, and Potential Topics and assert the rest in the Potential Topics stack; - if this fails ? also partially ? we use the best candidates in the weighted list of referring expressions to assert the new Topics. It may be the case that both resolved and current best 
candidates are used, and this is by far the most common case. In example n.3 below, the first possessive pronoun ?his? is met at Utterance level ? the first sentence has two clauses: clause 1, headed by the predicate DIVORCE, and clause 2, headed by MARRY. ?His? will look for a masculine antecedent and Chabrol will be chosen, also for weights associated to it, being the higher subject. This will produce the following semantic structure, which is made of a Head, a Semantic Role and an Index, - Chabrol-poss-sn2 which is the output of the substitution of ?his? present in the same structure by means of information made available by the Anaphoric module. Note that the index of a modifier points to the governing head, in this case ?wife?, the apposition associated to ?Agnes?, which in turn is the OBJect of DIVORCE.  T/H pair n. 3 T: Claude Chabrol divorced Agnes, his first wife, to marry the actress St?phane Audran. His third wife is Aurore Paquiss. H: Aurore Paquiss married Chabrol.  When the first sentence is passed to the semantic interpreter, anaphoric processes have already been completed and the information is then transferred to semantic structure which will register the anaphoric relation by the substitution operation. However this specific relation is not the one that really matters in the current T/H pair. When the system passes to the analysis of the following sentence it has another possessive pronoun which is contained in a SUBJect NP. By definition, these pronouns take their antecedent from the discourse level. To have the system do that, the pronoun has to be left free at sentence level, i.e. it must be computed as ?external? to the current sentence, and not bound locally. Discourse level processes will look for antecedents from the History list and from the socalled Topic Hierarchy, our way to compute centering (but see again Delmonte, 2006). This is shown schematically in the output of the Anaphora Resolution module shown here below, which reports the listing of pronouns, Topic Hierarchy, and Anaphora Resolution processes carried out. In this case, every referring expression will have a semantic index (SI) associated which is unique in the History List. In example n.31, here below, the pronominal expressions are two: an Utterance level 
52
possessive pronoun bound to the local SUBJect; and a Discourse level personal pronoun ?He? which receives its antecedent from the History List. In both cases, substitution with their antecedents? head will take place in the semantic interpretation level.     
 T/H pair n. 31 T: Upon receiving his Ph.D., Wetherill became a staff member at Carnegie's Department of Terrestrial Magnetism (DTM) in Washington, D.C. He originated the concept of the Concordia Diagram for the uranium-lead isotopic system. H: Wetherill was the inventor of the concept of the Concordia Diagram.  Clause No. Main Topic Secondary Topic Potential Topics Pronouns + Features Disjoint-ness Pronominal Binding Anaphora  Resolution id_3_1 Text 'Claude Chabrol'  Agnes     id_3_2 Text Main resolved as Claude Chabrol  -  SI=id1 
wife Aurore Paquiss his-[sem:hum, cat:poss, gen:mas, num:sing, pers:3, pred:he, gov_pred:be] 
disj=[sn1-wife] External pronoun (be, his) his  resolved as  'Claude Chabrol'  id_3_3 Hypo-thesis 
Aurore Paquiss  - SI=id4 Claude Chabrol  - SI=id1 
     
Table 2. Output of the Anaphora Resolution Module  4   Results and Discussion Results for the Test set 485 total pronominal expressions amount to 69% accuracy, 92% recall ? this includes computing It-expletives. The F-measure computed is thus 79%. Overall, we evaluated the contribution of the Anaphora Resolution Module as 15% additional correct results. Of course, the impact of using this module would have been different in case all T/H pairs were constituted by long texts. The RTE task is a hard task: in our case 10-15% mistakes are ascribable to the parser or any other analysis tool; another 5-10% mistakes will certainly come from insufficient semantic information.    ACCURACY AVERAGE PRECISION IE 0.5850 0.5992 IR 0.6050 0.5296 QA 0.5900 0.6327 SUMM 0.5700 0.6132 TOTAL 0.5875 0.5830 Table 3. Results for First Run  
References Bos, J., Clark, S., Steedman, M., Curran, J., Hockenmaier, J.: Wide-coverage semantic representations from a ccg parser. In: Proc. of the 20th International Conference on Computational Linguistics. Geneva, Switzerland (2004) Delmonte, R.: Text Understanding with GETARUNS for Q/A and Summarization, Proc. ACL 2004 - 2nd Workshop on Text Meaning & Interpretation, Barcelona, Columbia University (2004) 97-104 Delmonte R., et al Another Evaluation of Anaphora Resolution Algorithms and a Comparison with GETARUNS' Knowledge Rich Approach. In: ROMAND 2006 - 11th EACL. Geneva, (2006) 3-10 Grosz B. and C.  Sidner 1986. Attention, Intentions, and the Structure of Discourse, Computational Linguistics 12 (3), 175-204. Raina, R., et al: Robust Textual Inference using Diverse Knowledge Sources. In: Proc. of the 1st. PASCAL Recognision Textual Entailment Challenge Workshop, Southampton, U.K., (2005) 57-60 Sidner C. 1983. Focusing in the Comprehension of Definite Anaphora, in Brady M., Berwick R.(eds.), Computational Models of Discourse, MIT Press, Cambridge, MA, 267-330.   
53
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 40?41,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Scaling up a NLU system from text to dialogue understanding  R. Delmonte, A. Bristot, G. Voltolina Department of Language Science - Universit? Ca? Foscari - 30123 - VENEZIA delmont@unive.it 
 Vincenzo Pallotta Webster University, Geneva Switzerland pallotta@webster.ch 
Abstract In this paper we will present work carried out to scale up the system for text understanding called GETARUNS, and port it to be used in dialogue understanding. We will present the adjustments we made in order to cope with transcribed spoken dialogues like those produced in the ICSI Berkely project. In a final section we present preliminary evaluation of the system on non-referential pronominals individuation. 1 Introduction Very much like other deep linguistic processing systems (see Allen et al), our system is a generic text/dialogue understanding system that can be used in connection with an ontology ? WordNet - and/or a repository of commonsense knowledge like CONCEPTNET. Word sense disambiguation takes place at the level of semantic interpretation and is represented in the Discourse Model.  Computing semantic representations for spoken dialogues is a particularly hard task which ? when compared to written text processing - requires the following additional information to be made available: - adequate treatment of fragments; - adequate treatment of short turns, in particular one-word turns; - adequate treatment of first person singular and plural pronominal expressions; - adequate treatment of disfluencies, thus including cases of turns made up of just such expressions, or cases when they are found inside the utterance; - adequate treatment of overlaps; - adequate treatment of speaker identity for pronominal coreference; In our system, then, every dialogue turn receives one polarity label, indicating negativity or 
positivity, and this is computed by looking into a dictionary of polarity items. This is subsequently used to decide on argumentative automatic classification.  The Berkeley ICSI dialogues are characterized by the need to argument in a exhaustive manner the topics to be debated which are the theme of each multiparty dialogue. The mean length of utterances/turns in each dialogue we parsed was rather long.  2 The System GETARUNS GETARUNS1, the system for text understanding developed at the University of Venice, is organized as a pipeline which includes two versions of the system: what we call the Partial and the Deep GETARUNS (Delmonte 2007;2009). The Deep version is equipped with three main modules: a lower module for parsing, where sentence strategies are implemented; a middle module for semantic interpretation and discourse model construction which is cast into Situation Semantics; and a higher module where reasoning and generation takes place.  2.1 The Algorithm for Overlaps Overlaps are an important component of all spoken dialogue analysis. In all dialogue transcription, overlaps are treated as a separate turn from the one in which they occur, which usually follows it.  On the contrary, when computing overlaps we set as our first goal that of recovering the temporal order. This is done because overlaps may introduce linguistic elements which influence the local context. Eventually, they may determine the interpretation of the current utterance.                                                  1 The system has been tested in STEP competition, and can be downloaded at, http://project.cgm.unive.it/html/sharedtask/. 
40
For these reasons, they cannot be moved to a separate turn because they must be semantically interpreted where they temporally belong.  The algorithm we built looks at time stamps, and everytime the following turn begins at a time preceding the ending time of current turn it enters a special recursive procedure. It looks for internal interruption in the current turn and splits the utterance where the interruption occurs. Then it parses it split initial portion of current utterance and continues with the overlapping turn. This may be reiterated in case another overlap follows which again begins before the end of current utterance. Eventually, it returns to the analysis of the current turn with the remaining portion of current utterance. 2.2 The Treatment of Fragments and Short Turns Fragments and short turns are filtered by a lexical lookup procedure that searches for specific linguistic elements which are part of a list of backchannels, acknowledgements expressions and other similar speech acts. In case this procedure has success, no further computation takes place. However, this only applies to utterances shorter than 5 words, and should be made up only of such special words. No other linguistic element should be present apart from non-words, that is words which are only partially produced and have been transcribed with a dash at the end. Otherwise we proceed as follows: - graceful failure procedures for ungrammatical sentences, which might be fullfledged utterances but semantically uninterpretable due to the presence of repetitions, false starts and similar disfluency phenomena. Or else they may be just fragments, i.e. partial or incomplete utterances, hence non-interpretable as such; this is done by imposing grammatical constraints of wellformedness in the parser. We implemented a principled treatment of elliptical utterances and contribute one specific speech act. They may express agreement/ disagreement, acknowledgements, assessments, continuers etc. All these items are computed as being complements of abstract verb SAY which is introduced in the analysis, and has as subject, the name of current speaker. 
3 The Experiment We set up an experiment in order to test the new version of the system, that is detecting referential from nonreferential uses of personal pronouns ?you?, ?we? and ?it?.  In order to take decisions as to whether pronouns are to be interpreted as referential or not a recursive procedure checks the type of governing predicate. Referential pronouns are then passed on to the pronominal binding algorithm that looks for local antecedents if any. Otherwise, the pronouns is labeled as having External coreference in the previous discourse stretch. The Anaphora Resolution module will then take care of the antecedent and a suitable semantic identifier will be associated to it. On the contrary, if the pronouns are judged to be referentially empty or generic, no binding takes place. Here below is a table containing total values for pronouns WE/YOU/IT in all the 10 dialogues analysed.   Referential Generic Total WE 1186 706 1892 YOU 1045 742 1787 IT 1593 1008 2601   Total 3824 2456 6280 Table 1. Overall count of pronominal expressions Results for the experiment are as follows   Recall Precision F-Score WE 98.2% 60.59% 74.94% YOU 99.3% 70.99% 82.79% IT 97.6% 64.2% 77.45% Table 2. Results for pronominal expressions  References  Allen, J., M. Dzikovska, M. Manshadi, and M. Swift. 2007. Deep linguistic processing for spoken dialogue systems. In ACL 2007 Workshop on Deep Linguistic Processing, pp. 49?56.  Delmonte R. 2007. Computational Linguistic Text Processing ? Logical Form, Semantic Interpretation, Discourse Relations and Question Answering, Nova Science Publishers, New York. Delmonte R. 2009. Computational Linguistic Text Processing ? Lexicon, Grammar, Parsing and Anaphora Resolution, Nova Science Publishers, New York.  
41
Proceedings of the 8th International Conference on Computational Semantics, pages 277?281,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Computing implicit entities and events
for story understanding
Rodolfo Delmonte, Emanuele Pianta
Universita` Ca? Foscari and IRST, Fondazione Bruno Kessler, Venezia
delmont@unive.it, pianta@itc.it
1 Introduction
In order to show that a system for text understanding has produced a sound
representation of the semantic and pragmatic contents of a story, it should be
able to answer questions about the participants and the events occurring in
the story. This requires processing linguistic descriptions which are lexically
expressed but also unexpressed ones, a task that, in our opinion, can only be
accomplished starting from full-fledged semantic representations. The over-
all task of story understanding requires in addition computing appropriate
coreference and cospecification for entities and events in what is usually re-
ferred to as a Discourse Model. All these tasks have been implemented in
the GETARUNS system, which is subdivided into two main meta-modules
or levels: the Low Level System, containing all modules that operate at sen-
tence level; High Level System, containing all the modules that operate at
discourse level by updating the Discourse Model. The system is divided up
into a pipeline of sequential but independent modules which realize the sub-
division of a parsing scheme as proposed in LFG theory where a c-structure
is built before the f-structure can be projected by unification into a DAG
(Direct Acyclic Graph). In this sense we try to apply phrase-structure rules
in a given sequence as they are ordered in the grammar: whenever a syntac-
tic constituent is successfully built, it is checked for semantic consistency, as
LFG grammaticality principles require [1].
GETARUNS has a highly sophisticated linguistically based semantic
module which is used to build up the Discourse Model. Semantic process-
ing is strongly modularized and distributed amongst a number of differ-
ent submodules which take care of Spatio-Temporal Reasoning, Discourse
Level Anaphora Resolution, and other subsidiary processes like Topic Hier-
archy which cooperate to find the most probable antecedent of coreferring
277
and cospecifying referential expressions when creating semantic individuals.
These are then asserted in the Discourse Model (hence the DM), which is
then the sole knowledge representation used to solve nominal coreference.
Semantic Mapping is performed in two steps: at first a Logical Form is pro-
duced which is a structural mapping from DAGs onto unscoped well-formed
formulas. These are then turned into situational semantics informational
units, infons which may become facts or sits (non factual situations). Each
unit has a relation, a list of arguments which in our case receive their se-
mantic roles from lower processing - a polarity, a temporal and a spatial
location index. Inferences can be drawn on the facts repository as will be
discussed below.
2 Implicit entities and implicatures
Conversational implicatures and implications in general, are based on an
assumption by the addressee that the speaker is obeying the conversational
maxims (see [2]), in particular the cooperative principle. We regard the
mechanism that recovers standard implicatures and conversational implica-
tions in general, as a reasoning process that uses the knowledge contained in
the semantic relations actually expressed in the utterance to recover hidden
or implied relations or events as we call them. This reasoning process can
be partially regarded as a subproduct of an inferential process that takes
spatio-temporal locations as the main component and is triggered by the
need to search for coreferent or cospecifiers to a current definite or indef-
inite NP head. This can be interpreted as bridging referential expression
entertaining some semantic relation with previously mentioned entities. If
we consider a classical example from [5] (A: Can you tell me the time? ; B:
Well, the milkman has come), we see that the request of the current time
is bound to a spatio-temporal location. Using the MILKMAN rather than
a WATCH to answer the question, is relatable to spatio-temporal triggers.
In fact, in order to infer the right approximate time, we need to situate the
COMING event of the milkman in time, given a certain spatial location.
Thus, it is just the ?pragmatic restriction? associated to SPACE and TIME
implied in the answer, that may trigger the inference.
2.1 The restaurant text
To exemplify some of the issues presented above we present a text by [7]. In
this text, entities may be scenario-dependent characters or main characters
independent thereof. Whereas the authors use the text for psychological
278
experimental reasons, we will focus on its computability.
(0) At the restaurant. (1) John went into a restaurant. (2) There was a
table in the corner. (3) The waiter took the order. (4) The atmosphere was
warm and friendly. (5) He began to read his book.
Sentence (1) introduces both JOHN as the Main Topic in the Topic Hier-
archy and RESTAURANT as the Main Location (in the role of LOCATion
argument of the governing verb GO and the preposition INTO). Sentence
(2) can potentially introduce TABLE as new main Topic. This type of sen-
tences is called presentational in the linguistic literature, and has the prag-
matic role of presenting an entity on the scene of the narration in an abrupt
manner, or, as Centering would definite it, with a SHIFT move. However,
the TABLE does not constitute a suitable entity to be presented on the
scene and the underlying import is triggering the inference that ?someone
is SITting at a TABLE?. This inference is guided by the spatio-temporal
component of the system. GETARUNS is equipped with a spatio-temporal
inferential module that asserts Main Spatio-Temporal Locations to anchor
events and facts expressed by situational infons. This happens whenever an
explicit lexical location is present in the text, as in the first sentence (the
RESTAURANT). The second sentence contains another explicit location:
the CORNER. Now, the inferential system will try to establish whether the
new location is either a deictic version of the Main Location, or it is semanti-
cally included in the Main Location, or else it is a new unconnected location
that substitutes the previous one. The CORNER is in a meronymic seman-
tic relation with RESTAURANT and thus it is understood as being a part
of it. This inference triggers the implicature that the TABLE mentioned in
sentence (2) is a metonymy for the SITting event. Consequently, the system
will not assume that the indefinite expression a table has the funciton to
present a new entity TABLE, but that an implicit entity is involved with a
related event. The entity implied is understood as the Main Topic of the
current Topic Hierarchy, i.e. JOHN.
We will now concentrate our attention onto sentence (3). To account
for the fact that whenever a waiter takes an order there is always someone
that makes the order, GETARUNS computes TAKE ORDER as a com-
pound verb with an optional implicit GOAL argument that is the person
ORDERing something. The system then looks for the current Main Topic of
discourse or the Focus as computed by the Topic Hierarchy Algorithm, and
associates the semantic identifier to the implicit entity. This latter procedure
is triggered by the existential dummy quantifier associated to the implicit
279
optional argument. However, another important process has been activated
automatically by the presence of a singular definite NP, ?the WAITER?,
which is searched at first in the Discourse Model of entities and proper-
ties asserted for the previous stretch of text. Failure in equality matching
activates the bridging mechanism for inferences which succeeds in identify-
ing the WAITER as a Social Role in a RESTAURANT, the current Main
Location.
The text includes a sentence (4) that represents a psychological state-
ment, that is it expresses the feelings and is viewed from the point of view of
one of the characters in the story. The relevance of the sentence is its role in
the assignment of the antecedent to the pronominal expressions contained in
the following sentence (5). Without such a sentence the anaphora resolution
module would have no way of computing JOHN as the legitimate antecedent
of ?He/his?. However, in order to capture such information, GETARUNS
computes the Point of View and Discourse Domain on the basis of Informa-
tional Structure and Focus Topic by means of a Topic Hierarchy algorithm
based on [3] and [8].
2.2 Common sense reasoning
GETARUNS is also able to search for unexpressed relations intervening
in the current spatio-temporal location. To solve this problem in a princi-
pled way we needed commonsense knowledge organized in a computationally
tractable way. This is what CONCEPTNET 2.1 ([6]) provides. ConceptNet
- available at www.conceptnet.org - is the largest freely available, machine-
useable commonsense resource. Organized as a network of semi-structured
natural language fragments, ConceptNet consists of over 250,000 elements
of commonsense knowledge. At present it includes instances of 19 semantic
relations, representing categories of, inter alia, temporal, spatial, causal, and
functional knowledge. The representation chosen is semi-structured natu-
ral language using lemmata rather than inflected words. The way in which
concepts are related reminds ?scripts?, where events may be decomposed in
Preconditions, Subevents and so on, and has been inspired by Cyc ([4]).
ConceptNet can be accessed in different ways; we wanted a strongly con-
strained one. We choose a list of relations from this external resource and
combine them with the information available from the processing of the text
to derive Implicit Information. In other words, we assume that what is be-
ing actually said hides additional information which however is implicitely
hinted at. ConceptNet provides the following relations: SubEventOf, First-
SubeventOf, DesiresEvent, Do, CapableOf, FunctionOf, UsedFor, EventRe-
280
quiresObject, LocationOf. Let us see how this information can be exploited
to interpret another classical example from the Pragmatics literature: A:
I?ve just run out of petrol ; B: Oh, there?s a garage just around the corner.
There are a number of missing conceptual links that need to be inferred in
this text, as follows: Inf1 : the CAR has run out of petrol; Inf2 : the CAR
NEEDS petrol; Inf3 : garages SELL PETROL for cars.
In addition, in order to use ConceptNet we need to link petrol and garage
to gas/gasoline and gas station respectively. Now we can query the ontology
and will recover the following facts. The whole process starts from the first
utterance and uses RUN OUT OF GAS: (Do ?car? ?run out of gas?). Then
we can use GAS STATION and CAR to build another query and get (Do
?car? ?get fuel at gas station?), where FUEL and GASoline are in IsA
relation. Eventually we may still get additional information on the reason
why this has to be done: (Do ?person? ?don?t want to run out of gas?),
(SubeventOf ?drive car? ?you run out of gas?), (Do ?car? ?need gas petrol
in order to function?), (Do ?gas station? ?sell fuel for automobile?). These
may all constitute additional commonsense knowledge that may be used to
further explain and clarify the implicature.
References
[1] Joan Bresnan. Lexical-Functional Syntax (Blackwell Textbooks in Linguistics).
Blackwell Publisher, September 2000.
[2] H.P. Grice. Logic and conversation. In P. Cole and J.L. Morgan, editors, Syntax
and Semantics, volume 3. New York Academic Press, 1975.
[3] B. Grosz. Focusing and description in natural language dialogues. Cambridge
University Press, 1981.
[4] Douglas B. Lenat. CYC: A large-scale investment in knowledge infrastructure.
Communications of the ACM, 38(11):33?38, 1995.
[5] Stephen Levinson. Pragmatics. Cambridge University Press, 1983.
[6] Hugo Liu and Push Singh. ConceptNet: A practical commonsense reasoning
toolkit. BT Technology Journal, 22(211?226), 2004.
[7] A.J. Sanford and S.C. Garrod. Thematic subjecthood and cognitive constraints
on discourse structure. Journal of Pragmatics, 12(5-6):519?534, 1988.
[8] C. Sidner. Focusing in the comprehension of definite anaphora. In M. Brady
and R. Berwick, editors, Computational models of discourse, pages 267?330.
MIT Press, 1983.
281
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 73?76,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
SPARSAR: An Expressive Poetry Reader 
 Rodolfo Delmonte & Anton Maria Prati Department of Language Studies & Department of Computer Science Ca? Foscari University - 30123, Venezia, Italy delmont@unive.it  Abstract 
We present SPARSAR, a system for the auto-matic analysis of poetry(and text) style which makes use of NLP tools like tokenizers, sen-tence splitters, NER (Name Entity Recogni-tion) tools, and taggers. In addition the system adds syntactic and semantic structural analysis and prosodic modeling. We do a dependency mapping to analyse the verbal complex and de-termine Discourse Structure. Another impor-tant component of the system is a phonological parser to account for OOVWs, in the process of grapheme to phoneme conversion of the poem. We also measure the prosody of the poem by associating mean durational values in msecs to each syllable from a database of syl-lable durations; to account for missing sylla-bles we built a syllable parser with the aim to evaluate durational values for any possible syl-lable structure. A fundamental component for the production of emotions is the one that per-forms affective and sentiment analysis. This is done on a line by line basis. Lines associated to specific emotions are then marked to be pro-nounced with special care for the final module of the system, which is reponsible for the pro-duction of expressive reading by a TTS modu-le, in our case the one made available by Apple on their computers. Expressive reading is al-lowed by the possibility to interact with the TTS. 1 Introduction We present SPARSAR, a system for poetry (and text) style analysis by means of parameters de-rived from deep poem (and text) analysis. We use our system for deep text understanding called VENSES(XXX,2005) for that aim. SPAR-SAR(XXX,2013a) works on top of the output provided by VENSES and is organized in three main modules which can be used also to analyse similarities between couples of poems by the same or different poet and similarities between collections of poems by a couple of poets. In ad-dition to what is usually needed to compute text 
level semantic and pragmatic features, poetry introduces a number of additional layers of meaning by means of metrical and rhyming de-vices. For these reasons more computation is required in order to assess and evaluate the level of complexity that a poem objectively contains. We use prosodic durational parameters from a database of English syllables we produced for a prosodic speech recognizer (XXX,1990). These parameters are used to evaluate objective pre-sumed syllable and feet prosodic distribution at line level. The sum of all of these data is then used to create a parameterized version of the po-em to be read by a TTS, with an appropriate ex-pressivity. Expressive reading is generated by combining syntactic, semantic, lexical and pro-sodic information. It is a well-known fact that TTS systems are unable to produce utterances with appropriate prosody(van Santen et al.,2003)1. Besides the general problems related to TTS reading normal texts, when a poem is inputted to the TTS the result is worsened by the internal rules which compute stanza boundaries as sentence delimiters. So every time there are continuations or enjambements from one stanza to the next the TTS will not be able to see it, and will produce a long pause. The TTS is also blind to line boundaries. More importantly, the TTS reads every sentence with the same tone, thus contributing an unpleasant repeated overall bor-ing sense which does not correspond to the con-tents read. This is why sentiment analysis can be of help, together with semantic processing at dis-course level. As regards affective or emotional reading, then, the prosody of current TTS systems is neutral, and generally uses flat intonation contours. Pro-ducing ?expressive? prosody will require mo-difying rhythm, stress patterns and intonation as described in section 4(see Kao & Jurafsky,2012).                                                 1 as he puts it, ?The wrong words are emphasized, phrase boundaries are not appropriately indicated, and there is no prosodic structure for longer stretches of speech. As a result, comprehension is difficult and the overall listening expe-rience is disconcerting?? (ibid.,1657). 
73
The paper is organized as follows: here below a subsection contains a short state of the art limited though to latest publications; section 2 shortly presents SPARSAR; section 3 is dedicated to Prosody, Rhyming and Metrical Structure; a short state of the art of expressive reading is pre-sented in section 4, which is devoted to TextTo-Speech and parameters induction from the analy-sis. Eventually we present an evaluation, a con-clusion and work for the future. 
2 PARSAR - Automatic Analysis of Po-etic Structure and Rhythm with Syn-tax, Semantics and Phonology SPARSAR[8] produces a deep analysis of each poem at different levels: it works at sentence le-vel at first, than at line level and finally at stanza level. The structure of the system is organized as follows: at first syntactic, semantic and gramma-tical functions are evaluated. Then the poem is translated into a phonetic form preserving its vi-sual structure and its subdivision into lines and stanzas. Phonetically translated words are asso-ciated to mean duration values taking into ac-count position in the word and stress. Taking into account syntactic and semantic information, we then proceed to ?demote? word stress of depen-dent or functional words. At the end of the analy-sis of the poem, the system can measure the fol-lowing parameters: mean verse length in terms of msec. and in number of feet. The latter is derived by a line and stanza representation of metrical structure. More on this topic below.  Another important component of the analysis of rhythm is constituted by the algorithm that measures and evaluates rhyme schemes at stanza level and then the overall rhyming structure at poem level.  As regards syntax, we build chunks and dependency structures. To complete our work, we introduce semantics at two levels. On the one hand, we isolate verbal complex in order to verify propositional properties, like presence of negation, computing factuality from a crosscheck with modality, aspectuality ? that we derive from our lexica ? and tense. We also clas-sify referring espressions by distinguishing con-crete from abstract nouns, identifying highly am-biguous from singleton concepts (from number of possible meanings from WordNet and other similar repositories). Eventually, we carry out a sentiment analysis of every poem, thus contribu-ting a three-way classification: neutral, negative, positive that can be used as a powerful tool for expressive purposes. 
3 Rhetoric Devices, Metrical and Pro-sodic Structure The second module takes care of rhetorical de-vices, metrical structure and prosodic structure. This time the file is read on a line by line level by simply collecting strings in a sequence and splitting lines at each newline character. In a subsequent loop, whenever two newlines charac-ters are met, a stanza is computed. In order to compute rhetorical and prosodic structure we need to transform each word into its phonetic counterpart, by accessing the transcriptions available in the CMU dictionary. The Carnegie Mellon Pronouncing Dictionary is freely avai-lable online and includes American English pro-nunciation2. We had available a syllable parser which was used to build the VESD database of English syllables (XXX, 1999a) (Venice English Syllable Database) to be used in the Prosodic Module of SLIM, a system for prosodic self-learning activities(XXX,2010), which we use whenever we have a failure of our pronunciation dictionary which covers some 170,000 entries.  Remaining problems to be solved are related to ambiguous homographs like ?import? (verb) and ?import? (noun) and are  treated on the basis of their lexical category derived from previous tag-ging; and Out Of Vocabulary Words (OOVW). If a word is not found in the dictionary, we try dif-ferent capitalizations, as well as breaking apart hyphenated words, and then we check with sim-ple heuristics, differences in spelling determined by British vs. American pronunciation. Then we proceed by morphological decomposition, split-ting at first the word from its prefix and if that still does not work, its derivational suffix. As a last resource, we use an orthographically based version of the same dictionary to try and match the longest possible string in coincidence with our OOVW. Some words we had to reconstruct are: wayfare, gangrened, krog, copperplate, splendor, filmy, seraphic, unstarred, shrive, slip-stream, fossicking, unplotted, corpuscle, thither, wraiths, etc. In some cases, the problem that made the system fail was the syllable which was not available in our database of syllable dura-tions, VESD3. This problem has been coped with                                                 2 It is available online at <http://www.speech.cs.cmu.edu/cgi-bin/cmudict/>. 3 In VESD, syllables have been collected from WSJCAM, the Cambridge version of the continuous speech recognition corpus produced from the Wall Street Journal, distributed by the Linguistic Data Consortium (LDC). We worked on a subset of 4165 sentences, with 70,694 words which consti-
74
by launching the syllable parser and then compu-ting durations from the component phonemes, or from the closest similar syllable available in the database. We only had to add 12 new syllables for a set of approximately 500 poems that we computed to test the system. 3.1 Computing Metrical Structure and Rhyming Scheme Any poem can be characterized by its rhythm which is also revealing of the poet's peculiar style. In turn, the poem's rhythm is based mainly on two elements: meter, that is distribution of stressed and unstressed syllables in the verse, presence of rhyming and other poetic devices like alliteration, assonance, consonance, en-jambements, etc. which contribute to poetic form at stanza level.  We follow Hayward (1991)  to mark a poetic foot by a numerical sequence that is an alterna-tion of 0/1: ?0? for unstressed and ?1? for stres-sed syllables. The sequence of these sings makes up the foot and depending on number of feet one can speak of iambic, trochaic, anapestic, dactylic, etc. poetic style. But then we deepen our analysis by considering stanzas as structural units in which rhyming plays an essential role. Secondly we implement a prosodic acoustic measure to get a precise definition of rhythm. Syllables are not just any combination of sounds, and their internal structure is fundamental to the nature of the poet-ic rhythm that will ensue. The use of duration has allowed our system to produce a model of a poet-ry reader that we implement by speech synthesis. To this aim we assume that syllable acoustic identity changes as a function of three parame-ters: - internal structure in terms of onset and rhyme which is characterized by number consonants, consonant clusters, vowel or diphthong - position in the word, whether beginning, end or middle - primary stress, secondary stress or unstressed 4 TTS and Modeling Poetry Reading The other important part of the work regards us-ing the previous analyses to produce intelligible,                                                                        tute half of the total number of words in the corpus amoun-ting to 133,080. We ended up with 113,282 syllables and 287,734 phones. The final typology is made up of 44 pho-nes, 4393 syllable types and 11,712 word types. From word-level and phoneme-level transcriptions we produced sylla-bles automatically by means of a syllable parser. The result was then checked manually. 
correct, appropriate and possibly pleasant or catchy poetry reading by a TextToSpeech sys-tem. In fact, the intention was more ambitious and was producing an ?expressive? reading of a poem in the sense also intended by work reported in Ovesdotter & Sprout(2005), Ovesdotter(2005), Scherer(2003). In Ovesdotter & Sprout(2005), the authors present work on fairy tales, intended to use positive vs negative classification of sen-tences to produce a better reading. To that aim they used a machine learning approach, based on the manual annotation of some 185 children sto-ries4. They reported accuracy results around 63% and F-score around 70%, which they explain may be due to a very low interannotator agree-ment, and to the fact that the dataset was too small. In Ovesdotter(2005) the author presents work on the perception of emotion based again on fairy tales reading by human readers. The ex-periment had the goal of checking the validity of the association of acoustic parameters to emotion types. Global acoustic features included F0, in-tensity, speech rate in number of words, feet, syllables per minute, fluency, i.e. number of pauses or silences. The results show some con-tradictory data for ANGRY state, but fully com-pliant data for HAPPY5. These data must be re-garded as tendencies and are confirmed by ex-periments reported also in Scherer(2003) and Schr?der(2001). However, it must be underlined that all researchers confirm the importance of semantic content, that is the meaning as a means for transmitting affective states. The TTS we are now referring to is the one freely available under Mac OSX in Apple?s de-vices. In fact, the output of our system can be used to record .wav or .mpeg files that can then be played by any sound player program. The in-formation made available by the system is suffi-ciently deep to allow for Mac TTS interactive program to adapt the text to be read and model it                                                 4 Features used to learn to distinguish ?emotional? from ?neutral? sentences, include (ibid., 582): first sentence in the story; direct speech; thematic story type (animal tale, ordi-nary folk-tale, jokes and anecdotes); interrogative and ex-clamative punctuation marks; sentence length in words; ranges of story progress; percent of semantic words (JJ, N, V, RB); V count in sentence, excluding participles; positive and negative words; WordNet emotion words; interjections and affective words; content BOW: N,V,JJ,RB words by POS. 5 In particular, ?angry? was associated with ?decreased F0? and ?decreased speech rate?, but also an increased ?paus-ing?. On the contrary, ?happy? showed an ?increased F0, intensity, pausing? but a ?decreased speech rate?.  ?Happy? is similar to ?surprised?, while ?angry? is similar to ?sad?. 
75
accurately. We used the internal commands which can modify sensibly the content of the text to be read. The voices now available are pleasant and highly intelligible. We produced a set of rules that take into account a number of essential variables and parameter to be introduced in the file to be read. Parameters that can be modified include: Duration as Speaking Rate; Intonation from first word marked to a Reset mark; Silence introduced as Durational value; Emphasis at word level increasing Pitch; Volume from first word marked to a Reset mark, increasing intensi-ty. We discovered that Apple?s TTS makes mis-takes when reading some specific words, which we then had to input to the system in a phonetic format, using the TUNE modality. The rules address the following information: - the title - the first and last line of the poem - a word is one of the phonetically spelled out words - a word is the last word of a sentence and is followed by an exclamation/interrogative mark - a word is a syntactic head (either at constituency or dependency level) - a word is a quantifier, or marks the beginning of a quantified expression - a word is a SUBJect head  - a word marks the end of a line and is (not) followed by punctuation - a word is the first word of a line and coincides with a new stanza and is preceded by punctuation - a line is part of a sentence which is a frozen or a formulaic expression with specific pragmatic content specifically encoded - a line is part of a sentence that introduces new Top-ic, a Change, Foreground Relevance as computed by semantics and discourse relations - a line is part of a sentence and is dependent in Dis-course Structure and its Move is Down or Same Level - a discourse marker indicates the beginning of a sub-ordinate clause 5 Evaluation, Conclusion and Future Work We have done a manual evaluation by analysing a randomly chosen sample of 50 poems out of the 500 analysed by the system. The evaluation has been made by a secondary school teacher of English literature, expert in poetry6. We asked the teacher to verify the following four levels of analysis: 1. phonetic translation; 2. syllable divi-sion; 3. feet grouping; 4. metrical rhyming struc-ture. Results show a percentage of error which is                                                 6 I here acknowledge the contribution of XXX and thank her for the effort. 
around 5% as a whole, in the four different levels of analysis. A first prototype has been presented in(XXX,2013a), and improvements have been done since then; but more work is needed to tune prosodic parameters for expressivity rendering both at intonational and rhythmic level. The most complex element to control seems to be varia-tions at discourse structure which are responsible for continuation intonational patterns vs. begin-ning of a new contour.  Reference XXX. 1999. "Prosodic Modeling for Syllable Structures from the VESD - Venice English Syllable Database", in Atti 9? Convegno GFS-AIA, Venezia, 161-168.  XXX. 2008. "Speech Synthesis for Language Tutoring Sys-tems", in V.Melissa Holland & F.Pete Fisher(eds.), (2008), The Path of Speech Technologies in Computer Assisted Language Learning, Routledge - Taylor and Francis Group-, New York, 123-150. XXX,  2010. "Prosodic tools for language learning", Inter-national Journal of Speech Technology. 12(4):161-184. XXX, 2013a. SPARSAR: a System for Poetry Automatic Rhythm and Style AnalyzeR, SLATE 2013, Demon-stration Track.  XXX. 2005. "VENSES ? a Linguistically-Based System for Semantic Evaluation", in J. Qui?onero-Candela et al.(eds.), 2005. Machine Learning Challenges. LNCS, Springer, Berlin, 344-371. M. Hayward. 1991. "A connectionist model of poetic me-ter". Poetics, 20, 303-317.  Justine Kao and Dan Jurafsky. 2012. "A Computational Analysis of Style, Affect, and Imagery in Contempo-rary Poetry". in Proc. NAACL Workshop on Computa-tional Linguistics for Literature. Cecilia Ovesdotter Alm, Richard Sproat, 2005. "Emotional sequencing and development in fairy tales", In Procee-dings of the First International Conference on Affective Computing and Intelligent Interaction, ACII ?05. Cecilia Ovesdotter Alm, 2005. "Emotions from text: Ma-chine learning for text-based emotion prediction", In Proceedings of HLT/EMNLP, 347-354. Jan van Santen, Lois Black, Gilead Cohen, Alexander Kain, Esther Klabbers,Taniya Mishra, Jacques de Villiers, and Xiaochuan Niu. 2003. "Applications of Computer Gene-rated Expressive Speech for Communication Disor-ders", in Proc. Eurospeech, Geneva, 1657-1660. K. R. Scherer. 2003. ?Vocal communication of emotions: a review of research paradigms?, Speech Communication, 40(1-2):227-256. 
76
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 296?299,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
VENSES++: Adapting a deep semantic processing system to the identification of null instantiations 
Sara Tonelli Fondazione Bruno Kessler Trento, Italy. satonelli@fbk.eu 
Rodolfo Delmonte Universit? Ca? Foscari Venezia, Italy. delmont@unive.it    Abstract The system to spot INIs, DNIs and their anteced-ents is an adaptation of VENSES, a system for semantic evaluation that has been used for RTE challenges in the last 6 years. In the following we will briefly describe the system and then the ad-ditions we made to cope with the new task. In particular, we will discuss how we mapped the VENSES analysis to the representation of frame information in order to identify null instantia-tions in the text. 1 Introduction The SemEval-2010 task for linking events and their participants in discourse (Ruppenhofer et al, 2009) introduced a new issue w.r.t. the Se-mEval-2007 task ?Frame Semantic Structure Ex-traction? (Baker et al, 2007), in that it focused on linking local semantic argument structures across sentence boundaries. Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al, 1998), then the identifica-tion of locally uninstantiated roles (NIs). If these roles are indefinite (INI), they have to be marked as such and no antecedent has to be found. On the contrary, if they are definite (DNI), their coreferents have to be found in the wider dis-course context. The challenge comprised two tasks, namely the full task (semantic role recog-nition and labelling + NI linking) and the NIs only task, i.e. the identification of null instantia-tions and their referents given a test set with gold standard local semantic argument structure. We took part to the NIs only task by modify-ing the VENSES system for deep semantic pro-cessing and entailment recognition (Delmonte et al, 2005). In our approach, we assume that the identification of null instantiations is a complex task requiring different levels of semantic know-ledge and several processing steps. For this rea-
son, we believe that the rich analysis performed by the pipeline architecture of VENSES is par-ticularly suitable for the task, also due to the small amount of training data available and the heterogeneity of NI phenomena.  2 The VENSES system VENSES is a reduced version of GETARUNS (Delmonte, 2008), a complete system for text understanding, whose backbone is LFG theory in its original version (Bresnan, 1982 and 2000). The system produces different levels of analysis, from syntax to discourse. However, three of them contribute most to the NI identification task: the lexico-semantic, the anaphora resolution and the deep semantic module. 2.1 The syntactic and lexico-semantic   module The system produces a c(onstituent)-structure representation by means of a cascade of aug-mented FSA, then it uses this output to map lexi-cal information from a number of different lexica which however contain similar information re-lated to verb/adjective and noun subcategoriza-tion. The mapping is done by splitting sentences into main and subordinate clauses. Other clauses are computed in their embedded position and can be either complement or relative clauses.  The system output is an Augmented Head Dependent Structure (AHDS), which is a fully indexed logical form, with Grammatical Rela-tions and Semantic Roles. The inventory of se-mantic roles we use is however very small ? 35, even though it is partly overlapping the one pro-posed in the first FrameNet project. We prefer to use generic roles rather than specific Frame Ele-ments (FEs) because sense disambiguation at this stage of computation may not be effective.  
296
2.2 The anaphora resolution module The AHDS structure is passed to and used by a full-fledged module for pronominal and ana-phora resolution, which is in turn split into two submodules. The resolution procedure takes care only of third person pronouns of all kinds ? re-ciprocals, reflexives, possessive and personal. Its mechanisms are quite complex, as described in (Delmonte et al, 2006). The first submodule basically treats all pronouns at sentence level ? that is, taking into account their position ? and if they are left free, they receive the annotation ?external?. If they are bound, they are associated to an antecedent?s index; else they might also be interpreted as expletives, i.e. they receive a label that prevents the following submodule to con-sider them for further computation. The second submodule receives as input the ex-ternal pronouns, and tries to find an antecedent in the previous stretch of text or discourse. To do that, the systems computes a topic hierarchy that is built following suggestions by (Sidner and Grosz, 1986) and is used in a centering-like manner.  2.3 The semantic module The output of the anaphora resolution module is used by the semantic module to substitute the pronoun?s head with the antecedent?s head. After this operation, the module produces Predicate-Argument Structures or PAS on the basis of a previously produced Logical Form. PAS are pro-duced for each clause and they separate obliga-tory from non-obligatory arguments, and these from adjuncts and modifiers. Some adjuncts, like spatiotemporal locations, are only bound at propositional level.  3 From VENSES output to NIs identifi-cation and binding After computing PAS information for each sen-tence, we first map the test set gold standard an-notation of frame information to VENSES out-put. Starting from the PAS with frames and FE labels attached to the predicates and the argu-ments, we run a module for DNI/INI spotting and DNI binding. It is composed by two different submodules, one for verbal predicates and one for nominal ones.  3.1 NIs identification and binding with ver-bal predicates As pointed out in (Ruppenhofer et al, 2009), the identification of DNI/INIs includes three main 
steps: i) recognizing that a core role is missing ii) ascertaining if it has a definite interpretation and iii) if yes, finding a role filler for it.  For verbal predicates, the two first steps are ac-complished starting from the PAS structure pro-duced by VENSES and trying to map them with the valence patterns in FrameNet. To this pur-pose, we take into account the list of all valence patterns extracted for every LU and every frame from FrameNet 1.4 and from the training data, in which all possible sequences of FEs (both overtly expressed and null instantiated) are listed with their grammatical functions, coreness status and frequencies. For example, the predicate ?barbecue.v? in the APPLY_HEAT frame is char-acterized by two patterns, both occurring once. In the first, Food is the subject (ext) and Cook is constructionally not instantiated (cni). In the sec-ond, the peripheral FE Time is also present:  ssr(barbecue-v,apply_heat,[[[[food-  c,np,ext],[cook-c,cni,null]],1],[[[time-p,pp,dep],[food-c,np,ext],[cook-c,cni,null]],1]]).  The first step in our computation is selecting for the current predicate those patterns or templates that contain the same number of core arguments of the clause under analysis plus one. This is due to the fact that NIs are always core FEs. For ex-ample, if a test sentence contains the ?barbe-cue.v? lexical unit labelled with the AP-PLY_HEAT frame and only the Food FE is overtly annotated, we look in the template list for all pat-terns in which ?barbecue.v? appears with the Food FE and another implicit core FE (either INI or DNI). If ?barbecue.v? is not present in the template list, we consider the templates of the other verbal lexical units in the same frame. The second step is assessing the licensor of the omission, whether lexical or constructional. Here we only distinguish complement governing predicates and passive constructions. For exam-ple, if ?barbecue.v? is attested in the template list both with an indefinite and with a definite instan-tiation of the Cook FE, we check if it occurs in the passive form in the test sentence. If yes, we infer that Cook has to be labelled as an indefinite null instantiation (INI). Another licensor of the omission could be the imperative form of the verb, which however has not been considered yet by our system. If we assess that the null instantiation is not indefinite, we look for an antecedent of the NI and, if we find it, we label it as a DNI. Other-wise, we don?t encode any information about 
297
omitted roles. The strategy devised for searching for possible referential expressions is as follows:  1. Given the current PAS (with frame labels), look in the previous sentence(s) for compa-rable PAS. Comparable means that the predi-cate is the same or semantically related based on WordNet synsets. 2. If a comparable PAS is found, check if they share at least one argument slot ? typically they should share the subject role. 3. If yes, look for the best head available in that PAS by semantic matching with the FE label as a referent for the DNI label in the current sentence. In case that does not produce any matching, we look into the list of all heads in FrameNet associated to the FE label and se-lect the one present in the PAS that matches. 3.2 NIs identification and binding with nominal predicates In order to identify DNI/INIs of nominal predi-cates, we take into account the History List pro-duced by VENSES in the AHDS analysis, where all nominal heads describing Events, Spatial and Temporal Locations and Body Parts in the document are collected together with their cur-rent sentence ID. Such list is derived from WordNet general nouns.  Based on a computational lexicon of Com-mon Sense Reasoning relations made available with ConceptNet 2.0 by MIT AI Lab (Liu and Singh, 2004), we first process the history list in order to identify the relations between nominal heads in different sentences. Such relations in-clude inheritance and inferences. For instance, if the current sentence contains the nominal heads ?door? or ?window?, they are connected to the ?house? head, if it is present in the History List as a spatial location occurring in a previous sen-tence. For instance, sentence 42 of the test document n. 13 contains the noun ?wall? as lexi-cal unit of the ARCHITECTURAL_PART frame. In the History List, it is classified as a place. Also the noun ?house? in sentence n. 7 (token 7) is classified as a place in the History List. Since ConceptNet alows us to infer a meronymy rela-tion between ?wall? and ?house?, we can derive the following information, saying that ?place? in sent. 45, token 25, is related to ?house?, in sent. 7, token 7:  loc(42-25, place, wall, house-[7-7]). Starting from this information, we then check which core FEs are overtly expressed in the test sentence for the ?wall? lexical unit. As encoded in the FrameNet database, the ARCHITEC-
TURAL_PART frame has two core FEs, namely Part and Whole. Since Part is already present in sentence n. 45, we assume that Whole could be a candidate DNI. After looking up the relations between nominal heads identified in the previous step, we make the hypothesis that ?house? be the antecedent of the Whole DNI. We then check if ?house? appears as a head of the Whole FE either in the FrameNet database or in the training data of the SemEval task in order to perform some semantic verification. If this hypothesis is con-firmed, we finally take the syntactic node headed by the antecedent as the best DNI referent. In our example, ?house? is the head of the node 501, so we generate the following output, in which the Whole FE is identified with the node 501 (headed by ?house?) in sentence 7:      <fe id="s42_f5_e2" name="Whole">   <fenode idref="s7_501"/>  <flag name="Definite_Interpretation">  Note that, in case the antecedent does not appear as the head of the candidate FE, it is discarded and no information about NIs is generated. This is clearly a limit of our approach, because nomi-nal predicates are never assigned an INI label. 4 System output and evaluation The SemEval test data comprise two annotated documents extracted from Conan Doyle?s novels. We report some statistics about the test data with gold standard annotation and a comparison with our system output in Table 1.   Text 1 Text 2 N. of sentences 249 276 Gold standard data N. of DNIs 158 191 N. of INIs 115 245 System output N. of DNIs 35 30 N. of INIs 16 20 F-score 0.0121 Table 1: Comparison between gold standard and   system output  The amount of NIs detected by our system is much lower than the gold standard one, particu-larly for INIs. This depends partly on the fact that no specific strategy for INI detection with nominal predicates has been devised so far, as described in Section 3.2. Another problem is that a lot of DNIs in the gold standard don?t get re-solved, while our system always looks for a re-
298
ferent in case of DNIs and if it is not found, the procedure fails.  The issue of detecting which DNIs are liable not to have an explicit antecedent remains an open problem. In general, Ruppenhofer et al (2009) suggest to treat the DNI identification and binding as a coreference resolution task. How-ever, the only information available is in fact the label of the missing FE. The authors propose to obtain information about the likely fillers of a missing FE from annotated data sets, but the task showed that this procedure could be successful only in case all FE labels are semantically well identifiable: in fact many FE labels are devoid of any specific associated meaning. Furthermore, lexical fillers of a given semantic role in the Fra-meNet data sets can be as diverse as possible. For example, a complete search in the FrameNet database for the FE Charges will reveal heads like ?possession, innocent, actions?, where the significant portion of text addressed by the FE would be in the specification - i.e. "possession of a gun" etc. Only in case of highly specialized FEs there will be some help in the semantic characterization of a possible antecedent. An-other open issue is the notion of context where the antecedent should be searched for, which is lacking an appropriate definition. If we take into account our system results on Text 1, we notice that only 3 DNIs have been identified and linked to the correct antecedent, while the overall amount of exact matches in-cluding INIs is 7. However, in 21 other cases the system correctly identifies a null instantiated role and assigns the right FE label, but it either de-tects an INI instead of a DNI (and vice-versa), or it finds the wrong antecedent for the DNI. A similar performance is achieved on Text 2: no DNI has been linked to the correct antecedent, and in only 8 cases there is an exact match be-tween the INIs identified by the system and those in the gold standard. However, in 18 cases a null instantiation is detected and assigned the correct FE label, even if either the referent or the defi-niteness label is wrong. Some evaluation metrics taking into account the different information lay-ers conveyed by the system would help high-lighting such differences and pointing out the NI identification steps that need to be consolidated. 5 Conclusions In this paper, we have introduced VENSES++, a modified version of the VENSES system for deep semantic processing and entailment detection. 
We described two strategies for the identification of null instantiations in a text, depending on the predicate class (either nominal or verbal).     The system took part to the SemEval task for NIs identification and binding. Even if the pre-liminary results are far from satisfactory, we were able to devise a general strategy for dealing with the task. Only 2 teams took part to the competition, and the first ranked system achieved F1 = 0.0140. This confirms that NI identification is a very challenging issue which can be hardly modeled. Anyway, it deserves further efforts, as various NLP applications could benefit from the effective identification of null instantiated roles, from SRL to coreference resolution and informa-tion extraction.  References  Baker, C., Ellsworth, M. and Erk, K. 2007. Frame Semantic Structure Extraction. In Proceedings of the 4th International Workshop on Semantic Evaluations. Prague, Czech Republic. Baker, C. F., Fillmore, C. J., & Lowe, J. B. 1998. The Berkeley FrameNet project. In Proceedings of COLING-ACL-98, Montreal, Canada. Bresnan, J. 2000. Lexical-functional syntax. Oxford: Blackwell. Bresnan, J. (ed.). 1982. The mental representation of grammatical relations, The MIT Press, Cambridge. Delmonte R., 2008. Computational Linguistic Text Processing ? Lexicon, Grammar, Parsing and Anaphora Resolution, Nova Science, New York. Delmonte, R., Tonelli, S., Piccolino Boniforti, M. A., Bristot, A., and Pianta, E. 2005. VENSES ? A Lin-guistically-based System for Semantic Evaluation. In Proc. of the 1st PASCAL RTE Workshop. Delmonte, R., Bristot, A., Piccolino Boniforti, M.A., and Tonelli, S. 2006. Another Evaluation of Anaphora Resolution Algorithms and a Compari-son with GETARUNS' Knowledge Rich Approach, In Proc. of ROMAND 2006, Trento, pp. 3-10. Grosz, B., and Sidner, C. 1986. Attention, intentions and the structure of discourse. Computational Lin-guistics, 12, 175?204. Liu, H., and Singh, P. 2004. ConceptNet: a practical commonsense reasoning toolkit. At http://web.media.mit.edu/~push/ConceptNet.pdf. Ruppenhofer, J., Sporleder, C., Morante, R., Baker, C. and Palmer, M. 2009. SemEval-2010 Task 10: Linking Events and Their Participants in Dis-course. In Proc. of the HLT-NAACL Workshop on Semantic Evaluations: Recent Achievements and Future Directions. Boulder, Colorado. 
299
Answering Why-Questions in
Closed Domains from a
Discourse Model
Rodolfo Delmonte
University of Venice ?Ca? Foscari? (Italy)
email: delmont@unive.it
Emanuele Pianta
Fondazione Bruno Kessler ? FBK (Italy)
email: pianta@fbk.eu
Abstract
In this paper we will present a system for Question Answering called
GETARUNS, in its deep version applicable to closed domains, that is to
say domains for which the lexical semantics is fully specified and does
not have to be induced. In addition, no ontology is needed: semantic re-
lations are derived from linguistic relations encoded in the syntax. The
main tenet of the system is that it is possible to produce consistent seman-
tic representations using a strict linguistic approach without resorting to
extralinguistic knowledge sources. The paper will briefly present the low
level component which is responsible for pronominal binding, quantifier
raising and temporal interpretation. Then it will discuss in more detail
the high level component where a Discourse Model is created from text.
The system has been evaluated on a wide variety of texts from closed
domains, producing full and accurate parsing, semantics and anaphora
resolution for all sentences.
103
104 Delmonte and Pianta
1 Introduction
In this paper we will present the system for Question Answering called GETARUNS,
in its deep version applicable to closed domains, that is to say domains for which the
lexical semantic is fully specified and does not have to be induced. GETARUNS is
a GEneral multilingual Text And Reference UNderstander which follows a linguisti-
cally based approach to text understanding and embodies a number of general strate-
gies on how to implement linguistic principles in a running system. The system ad-
dresses one main issue: how to restrict access to extralinguistic knowledge of the
world by contextual reasoning, i.e. reasoning from linguistically available cues.
Another important issue addressed by the system is multilinguality. In GETARUNS
the user may switch from one language to another by simply unloading the current lex-
icon and uploading the lexicon for the new language: at present Italian, German and
English are implemented. Multilinguality has been implemented to support the theo-
retical linguistic subdivision of Universal Grammar into a Core and a Peripheral set of
rules. The system is organized around another fundamental assumption: the architec-
ture of such a system must be modular thus requiring a pipeline of sequential feeding
processes of information, each module providing one chunk of knowledge, backtrack-
ing being allowed only within each single module. The architecture of the system
is organized in such a way as to allow for feedback into the parser from Anaphoric
Binding: however, when pronominals have been finally bound or left free, no more
changes are allowed on the f-structure output of the parser.
Thus, we can think of the system as being subdivided into two main meta-modules
or levels: Low Level System, containing all modules that operate at Sentence Level;
High Level System, containing all the modules that operate at Discourse and Text
Level by updating the Discourse Model. The deep and complete version of the sys-
tem that we present here can be used with strictly closed domains and does not need
any supporting ontology. However, it has also been used in one such context with a
different architecture, which had OWL and RDFs as final external knowledge repre-
sentation formalisms. Ontologies and Knowledge Sources should be used as Word
Sense Disambiguation tools (we have not produced results on this however).
Texts belonging to what we define as closed domains are characterized by the fact
that the system has all the semantic information which is needed process then; and
most importantly, sentences making up the texts can be fully parsed without failures.
In practice, these texts are relatively short and the length of sentences is below a certain
threshold, typically 25 words. They are used for text understanding practice in a lan-
guage learning environment. In this context, question answering is used to validate the
appropriateness of the user?s answer. Some such texts will be presented below. One
will be the text used by Mitre in 2000 to organize the Workshop on Reading Compre-
hension Tests as Evaluation for Computer-Based Language Understanding Systems
(Brill et al, 2000). The system has been evaluated on a wide variety of such texts
and has parsed fully and accurately all sentences with the appropriate Semantics and
Anaphora Resolution (Delmonte, 2007).
Answering Why-Questions in Closed Domains from a Discourse Model 105
2 The Low Level System
Even though we assume that the output of the Low Level System is mandatory for the
creation of the semantic representation needed to create a consistent Discourse Model
we will not be able comment it in depth for lack of space. We will simply show the
internal components or modules it encompasses and add a few comments. However
we stress the paramount importance of a deep linguistic analysis of the input text.
When each sentence is parsed, tense, aspect and temporal adjuncts are used to
build the basic temporal interpretation to be used by the temporal reasoner. Every
constituent is checked for semantic consistency and semantic features are added to
each semantic head in the form of generic concepts taken from WordNet and other
similar semantic lexical resources.
Eventually two important modules are implemented: Quantifier Raising and Pro-
nominal Binding. Quantifier Raising is computed on f-structure which is represented
internally as a DAG (Direct Acyclic Graph). It may introduce a pair of functional
components: an operator where the quantifier can be raised, and a pool containing the
associated variable where the quantifier is actually placed in the f-structure represen-
tation. This information may then be used by the following Higher System to inspect
quantifier scope.
Pronominal Binding is carried out at first at sentence internal level. DAGs will
be searched for binding domains and antecedents matched to the pronouns if any to
produce a list of possible bindings. Best candidates will then be chosen.
3 The Discourse Model
Informally, a DiscourseModel (DM)may be described as the set of entities "specified"
in a discourse, linked together by the relations they participate in. They are called dis-
course entities, but may also be regarded as discourse referents or cognitive elements.
A discourse entity (DE) inhabits a speaker?s discourse model and represents some-
thing the speaker has referred to. A speaker refers to something by utterances that
either evoke (if first reference) or access (if subsequent reference) its corresponding
discourse entity.
As soon as a DE is evoked, it gets a description. The initial description ID that
tags a newly evoked DE might have a special status, because it is the only information
about an entity that can be assumed to be shared (though not necessarily believed) by
both speaker and listener alike. However certain types of DE must be derived from
other ones inferentially.
Definite descriptions can be used like definite pronouns to access entities which are
presumably in the listener?s DM, or they can be used to evoke new entities into that
model.
Building a DM is clearly only a part of the overall process of understanding which
makes heavy use of background mutual knowledge on the side of the addressee in
order to carry out the complex inferences required. In order to build an adequate Dis-
course Model we rely on a version of Situation Semantics which takes perspectives or
point of view as the higher node in a hierarchical scheme in which there is a bifurca-
tion between factual and non-factual situations. Partially following Burke (1991) we
assume that the notion of perspectives is significant in situation theory insofar as the
106 Delmonte and Pianta
very same situations can be viewed by an agent (or by different agents) from different
perspectives, hence situations may support different and perhaps conflicting kinds of
information (ibid., p.134). Situations are characterized in terms of infons, or better
the infons that they support. In turn we distinguish between facts and concepts where
the former have to do with concrete ostensive entities which yield information that is
referential, in that they explicitly involve objects in the world relative to a given per-
spective. On the contrary concepts constitutes a piece of general information about
the world relative to a given perspective, which does not directly refer to any particular
entity or object, nor is it specific to particular ostensive entities.
Infons are built according to situation theory: a basic infon consists of a relation, its
argument roles , a polarity, and a couple of indices anchoring the event/state/processe
to a given spatiotemporal location.
In our system, facts may describe information relative to a subjective or an objec-
tive discourse domain: subjective facts are thus computable as situations viewed from
the perspective of a given agent?s mind, in our case also corresponding to the Main
Topic of discourse. On the contrary, objective facts are reported from the perspective
of the text?s author. However, to highlight the difference existing between subjective
and objective information in the model, we decided to call facts only objective infons;
subjective infons are called sit. Also generic facts are treated as sits.
These main constituents of situations are further described by taking as primitives
individuals, relations and locations and by using as logical notation set theory. Thus,
individuals and inferences on individuals are wrought out in set theory notation: we
use ind for a unique individual, set for a collection of individuals which can be indi-
viduated by means of membership, card for the cardinality of any set with a numerical
or indefinite quantified value, in to indicate membership, class for generic sets which
can be made up of an indefinite quantity however big enough to encompass sets, sub-
sets, classes or individuals. Each entity is assigned a constant value or id and an infon
which are uniquely individuated by a number.
Infons may express or contain a main relation: relations may be properties, so-
cial or relational roles, events or states, locational modifiers or specifiers ? that is
attributes, etc.. Simplex properties predicate some property of a semantic identifier;
complex properties take individuals and propositions as their arguments and in this
case individuals are usually associated to a semantic role. Semantic roles are inherited
from the lexical form associated to a given predicate in the lexicon and transferred
into the f-structure of the utterance under analysis. Semantic roles are paramount in
the choice and construction of questions and answers.
Inferences are produced every time a given property is reintroduced in the story in
order to ascertain whether the same property was already present in the model and
should not be reasserted, or whether it should be added to it. Properties may be an-
chored to a given location or be universally anchored: a name, is a rigid designator in
that it is computed as a property associated to a given individual and has a universal
locational anchoring, meaning that the same individual will always be individuated
by that name in the story. The same would apply to permanent properties like the
substance or matter constituting an object, like a house, or other such properties. Per-
sistence may then be computed both for entities, properties, relations and locations;
also, a Relevance Score is computed by a separate module that analyzes information
Answering Why-Questions in Closed Domains from a Discourse Model 107
structure for each simplex utterance.
4 Semantic Rules
After collecting all modifier heads, if any, of the current predicate, the rule for the
creation of semantic individuals separates previously resolved pronouns/nouns from
non resolved ones. In both cases it uses some sort of equational reasoning in order
to ascribe properties to already asserted semantic identifiers, by taking advantage of
linguistic information encoded in Function/Role, according to a linguistically well-
defined hierarchy which treats arguments and adjuncts as semantically different. New
semantic individuals are added when needed.
The module handling semantic individuals treats new individuals to be asserted in
the DM separately from already asserted ones ? in which case, the semantic index
should be inherited from properties belonging to previously asserted individuals. In
addition, quantified expressions should be treated differently from individuals or sets,
be they singleton sets, or sets with a given cardinality.
Semantic attributes are collected in the f-structure representation and come from the
SPEC subsidiary function. We use the following attributes to separate semantic types:
definiteness, partitivity and class. Definiteness applies to nominal expressions: these
may be definite (+def), indefinite (-def), or zero definite (def0), which applies both
to bare NPs and to proper nouns; partitivity is an attribute which gets a value only
in case of quantified NPs. Finally the class attribute is used to differentiate proper
nouns (-class) from common nouns (+class) which may undergo quantification,
and quantified pronouns (+me).
5 Question Answering from a Discourse Model
In order to show how the system behaves we report and focus only on one small text.
New texts are usually fully parsed: some intervention may be required to introduce
contextual classes for tag disambiguation purposes. Here below is the text and the
questions proposed for the Workshop on Text Understanding quoted above:
How Maple Syrup is Made
Maple syrup comes from sugar maple trees. At one time, maple syrup
was used to make sugar. This is why the tree is called a "sugar" maple
tree. Sugar maple trees make sap. Farmers collect the sap. The best time
to collect sap is in February and March. The nights must be cold and the
days warm. The farmer drills a few small holes in each tree. He puts
a spout in each hole. Then he hangs a bucket on the end of each spout.
The bucket has a cover to keep rain and snow out. The sap drips into the
bucket. About 10 gallons of sap come from each hole.
1. Who collects maple sap? (Farmers)
2. What does the farmer hang from a spout? (A bucket)
3. When is sap collected? (February and March)
4. Where does the maple sap come from? (Sugar maple trees)
5. Why is the bucket covered? (to keep rain and snow out)
As far as we gathered from the proceedings of the conference, none of the participants
was able to answer all the questions (Brill et al, 2000).
108 Delmonte and Pianta
This is howwe organize the system. We first compute the DM of the target question
(hereafter QDM), the whole process is carried out on the basis of the facts contained
in the question ad text DMs. Questions are classified into three types: partial or wh-
questions, why questions and complete or yes/no questions.
Recovering the answer from the DM is essentially done in four steps:
? extracting question word or question type for yes/no questions
? extracting the main predicates from the question, which are then used to
? search for identical/similar predicates in the text DM
? extraction of the argument matching the answer
As commented in the sections above, the semantic representation contained in a
DM can be basically defined as Predicate-Argument Structures or PAS, with a polarity
and pair of spatiotemporal indices. Given a short text and a question about the text,
the QA system will build a semantic model of the text where each distinct entity
is assigned a unique semantic identifier, and is represented as a pool of properties,
relations and attributes. Whenever possible, the system will also draw the necessary
inferences to assign relation and attributes of sets to the individuals composing those
sets.
Then it will completely parse the input question and produce a QDM for it, where
facts are represented as q_fact terms. Afterwards, the first move consists in recovering
the question word in the QDM by the following conjunction of queries
q_fact(K,focus,[arg:Id],1,_,_),
q_fact(_,isa,[_:Id,_:Focus],1,A,B)
where the variable Id, associated to the property "focus", is used to recover the ac-
tual Focus in the associated "isa" fact. This Focus is constituted by the question word
used to formulate the query. This is used by the system to activate specialized proce-
dures that will address specific semantic structures. As said above, why questions are
processed separately from other wh- questions. The next query fired is
get_focus_arg(Focus,Pred,Args,Answer,True-NewFact),
which will give back the contents of the answer in the variable Answer and the govern-
ing predicate in Pred. These are then used to generate the actual surface form of the
answer. Args and True-NewFact are used in case the question is a complete or yes/no
question. In order to generate the answer, tense and mood are searched in the DM;
then a logical form is build as required by the generator, and the build_reply procedure
is fired:
get_focus_tense(T,M), Form=[Pred,T,M,P,[D]],
build_reply(Out,Focus,Form), !.
We will present general wh- questions at first. They include all types of factoid
questions and also how questions. The main predicate looks for an appropriate lin-
guistic description to substitute the wh- word argument position in the appropriate
PAS. Here follows the full definition of the get_focus_arg procedure for the ?who?
case.
Answering Why-Questions in Closed Domains from a Discourse Model 109
get_focus_arg(who,Pred,Ind,D1,NewP):-
q_getevents(A,Pred),
q_fact(X,Pred,Args,1,_,L),
q_role(Y,X,Z,Role),
answer_buildarg(Role,Pred,[Idx:Z],D,Facts),
select_from_pred1(Pred,Role,Facts,NewP,D1), !.
We use a different procedure in case the question governing predicate is a copu-
lative verb, because we have to search for the associated property in the QDM, as
follows:
copulative(Pred),
q_fact(X,Pred,[prop:Y],1,_,_),
q_fact(Y,Prop,[_:K,Role:Type],1,_,_)
q_fact(_,inst_of,[_:K,_:Z],P,T,S),
q_get_ind_des(K,Propp,Ty),
Copulative predicates have a proposition as their argument and the verb itself is not
useful, being semantically empty. The predicate corresponding to the proposition is
searched through the infon Y identifying the fact. When we have recovered the Role
and the linguistic description of the property Propp indicated by the wh- question, we
pass them to the following predicate and search the associated individual in the DM:
answer_buildarg(Role,Pred,[Idx:Propp],Answer,Facts)
Suppose the wh-question is a where question with a copulative verb; then the role
will be a location and the Propp will be "in". How copulative questions will search for
class properties, i.e. not for names or individuals:
q_fact(X,how,[_:Y],1,_,_),
q_fact(Q,isa,[_:K,class:Pred],1,_,_),
q_fact(_,inst_of,[_:K,_:Z],P,T,S)
Semantic roles are irrelevant in this latter case: the only indication we use for the
search is a dummy prop role. On the contrary, when lexical verbs are governing predi-
cates, we need to use the PAS and the semantic role associated to the missing argument
to recover the appropriate answer in all other cases. Here we should also use a differ-
ent semantic strategy in case an argument is questioned and there is another argument
expressed in the question ? what, whom, who. Or else an adjunct is questioned ?
where, when, how, etc. ? or the predicate is intransitive, an argument is questioned
and there is no additional information available.
Now consider a typical search for the answer argument:
answer_buildarg(Role,Pred,Tops,Answer,Facts):-
on(Ind:Prop,Tops),
entity(Type,Id,Score,facts(Facts)),
extract_properties(Type,Ind,Facts,Def,Num,NProp,Cat),
select_allrole_facts(Role,Ind,Facts,Pred,PropLoc),
Answer=[Def,nil,Num,NProp,Cat,PropLoc], !.
110 Delmonte and Pianta
Here, extract_properties checks for the appropriate semantic type and property by
picking one entity and its properties at the time. When it succeeds, the choice is
further checked and completed by the call to select_allrole_facts. This is what ex-
tract_properties does:
extract_properties(Type, Ind, Facts, Def, Num, NProp, Gend):-
( Sclass=prop,
extrfacts(Facts,Ind,Gend,Sclass,Prop), Num=sing
; Sclass=class,
extrfacts(Facts,Ind,CGend,Sclass,Prop),
select_gend(Prop,CGend,Gend) ),
topichood_stack(Prop,Def),
( Type=ind, Num=sing
; Type=set, Num=plur ),
set_def(Sclass, Ind, Prop, Role, Def),
confirm_head(Def, Gend, Prop, NProp), !.
The procedure searches for individuals or sets filling a given semantic role in the
predicate-argument structure associated to the governing predicate. In addition, it has
the important task of setting functional and semantic features for the generator, like
gender and number. This is paramount when a pronoun has to be generated instead
of the actual basic linguistic description associated to a given semantic identifier. In
particular, gender may be already explicitly associated in the DM to the linguistic
description of a given entity or it may be derived from WordNet or other linguistic
resources handling derivational morphology. The call topichood_stack looks for static
definiteness information associated to the linguistic description in the DM. Proper
names are always "definite". On the contrary, common nouns may be used in definite
or indefinite ways. This information may be modified by the dialogue intervening
between user and system and be recorded in the user model. The decision is ulti-
mately taken by the set_def procedure which looks into the question-answering user
model knowledge base where previous mentions of the same entity might have been
recorded. Or else it does it ? by means of update_user_model? to be used in further
user-system interactions. If the entity semantic identifier is already present Def will
be set to "definite", otherwise it will remain as it has been originally set in the DM.
set_def(Def,Id,Prop,Role,Def1):-
( tknow(Id,Role1), swap_def(Def,Def1)
; tknow(Prop,Role1), swap_def(Def,Def1)
; update_user_model(Id,Role), assign_def(Def,Def1) ).
6 Computing Answers to WHY questions
Why question are usually answered by events, i.e. complete propositions. They would
in general constitute cases of rhetorical clause pairs labelled either as a Motivation-
Effect or a Cause-Result. In Delmonte et al (2007), causal relations are further de-
composed into the following finer-grained subprocesses:
? Cause-Result
? Rationale-Effect
Answering Why-Questions in Closed Domains from a Discourse Model 111
? Purpose-Outcome
? Circumstance-Outcome
? Means-Outcome
Furthermore, rationale clauses have been shown to be constituted structurally by un-
tensed Infinitival Adjuncts: on the contrary, Cause-Result pairs are usually constituted
by tensed propositions.
Consider now the pieces of knowledge needed to build the appropriate answer to the
question "Why is the tree called sugar maple tree?". Sentences involved to reconstruct
the answer are:
Maple syrup comes from sugar maple trees.
At one time, maple syrup was used to make sugar.
This is why the tree is called a "sugar" maple tree.
In other words, in order to build the appropriate answer, the system should be able
to build an adequate semantic representation for the discourse anaphora "This", which
is used to essentially relate the current sentence to the event chain of the previous
sentence. This is a fairly common way of expressing this kind of causal relation, that
we then would like to assume as a paradigmatic one. Eventually, the correct answer
would be:
Because maple syrup was used to make sugar
which as can be easily gathered is the content of the previous complex sentence. Here
below is the portion of the DM representation needed to reconstruct the answer:
ind(infon19, id8)
fact(infon20,inst_of,[ind:id8,class:edible_animal],1,univ, univ)
fact(infon21, isa,[ind:id8,class:[maple_syrup]],1, id1, id7)
set(infon23, id9)
card(infon24, id9, 5)
fact(infon25, sugar_maple, [ind:id10], 1, id1, id7)
fact(infon26, of, [arg:id10, specif:id9], 1, univ, univ)
fact(infon27,inst_of,[ind:id9,class:plant_life],1,univ, univ)
fact(infon28, isa, [ind:id9, class:tree], 1, id1, id7)
class(infon43, id13)
fact(infon44,inst_of,[ind:id13,class:substance],1,univ, univ)
fact(infon45, isa, [ind:id13, class:sugar], 1, id1, id7)
fact(id14,make,[agent:id8,theme_aff:id13],1,tes(finf_m3), id7)
fact(infon48,isa,[arg:id14,arg:ev],1,tes(finf_m3), id7)
fact(infon49, isa, [arg:id15, arg:tloc], 1, tes(finf_m3), id7)
fact(infon50, pres, [arg:id15], 1, tes(finf_m3), id7)
fact(infon51,time,[arg:id14,arg:id15], 1, tes(finf_m3), id7)
fact(id16,use,[theme_unaff:id8,prop:id14], 1, tes(sn5_m3), id7)
fact(id21,call,[actor:id9, theme_bound:id9], 1, tes(f1_m4), id7)
ent(infon61, id18)
fact(infon62,prop,[arg:id18,
disc_set:[id16:use:[theme_unaff:id8, prop:id14]]],
1, id1, id7)
ind(infon63, id19)
fact(infon66, inst_of, [ind:id19, class:abstract], 1, univ, univ)
fact(infon67, isa, [ind:id19, class:reason], 1, id1, id7)
fact(infon81, in, [arg:id21, nil:id19], 1, tes(f1_m4), id7)
fact(infon83, reason, [nil:id18, arg:id19], 1, id1, id7)
fact(id23, be, [prop:infon83], 1, tes(sn10_m4), id7)
112 Delmonte and Pianta
These three pieces of knowledge representation are built respectively when the
three sentences above are processed. When the second sentence is processed, the
semantic identifier id8 is simply inherited. Also, notice that it is transferred from USE
predicate to MAKE by means of controlling equations which are part of LFG syntactic
representations.
The system will at first look for a REASON semantic predicate associated to a
CALL predicate, as derived from the question semantic representation, which we re-
port here below:
q_loc(infon3, id1, [arg:main_tloc, arg:tr(f2_q6)])
q_ind(infon4, id2)
q_fact(infon5, tree, [nil:id2], 1, id1, univ)
q_fact(infon6, maple, [ind:id2], 1, id1, univ)
q_fact(infon7, sugar, [ind:id2], 1, id1, univ)
q_fact(infon8, of, [arg:id2, specif:id2], 1, univ, univ)
q_fact(infon9, why, [ind:id2], 1, id1, univ)
q_fact(infon10, inst_of, [ind:id2, class:plant_life], 1, univ, univ)
q_fact(infon11, isa, [ind:id2, class:tree], 1, id1, univ)
q_class(infon12, id3)
q_fact(infon13, inst_of, [ind:id3, class:substance], 1, univ, univ)
q_fact(infon14, isa, [ind:id3, class:sugar], 1, id1, univ)
q_class(infon15, id4)
q_fact(infon16, inst_of, [ind:id4, class:plant_life], 1, univ, univ)
q_fact(infon17, isa, [ind:id4, class:maple], 1, id1, univ)
q_fact(infon22, tree, [nil:id2, arg:id2], 1, id1, univ)
q_fact(id5, call, [prop:infon22], 1, tes(f2_q6), univ)
q_fact(infon23, isa, [arg:id5, arg:ev], 1, tes(f2_q6), univ)
q_fact(infon24, isa, [arg:id6, arg:tloc], 1, tes(f2_q6), univ)
q_fact(infon25, pres, [arg:id6], 1, tes(f2_q6), univ)
q_fact(infon26, time, [arg:id5, arg:id6], 1, tes(f2_q6), univ)
q_fact(infon27, focus, [arg:id7], 1, tes(f2_q6), univ)
q_fact(infon28, isa, [arg:id7, arg:why], 1, tes(f2_q6), univ)
q_fact(infon29, for, [arg:id5, motiv:id7], 1, tes(f2_q6), univ)
q_fact(infon35, perf, [arg:id8, ask:id5], 1, id1, univ)
The final part of the answer building process is constituted by the search of the
actual linguistic description to associate to the original predicate. This is done in the
pool of facts associated to the current entity which has been chosen from the inventory
of entities of the world associated to the original text.
answer_buildarg(Role,Pred,Tops,Answer,Facts,[]):-
on(Ind:Prop,Tops),
entity(Type,Id,Score,facts(Facts)),
extract_properties(Type,Ind,Facts,Def,Num,NProp,Cat),
select_allrole_facts(Role,Ind,Facts,Pred,PropLoc),
Answer=[Def,nil,Num,NProp,Cat,PropLoc],!.
select_allrole_facts(Role,Ind,Facts,Pred,PropLoc):-
selarf(Pred,Fact,Args,Pol,Id),
on(Fact,Facts),
isa_role_fatto(Args),
ind_role(Args,Inds),
on(Prop-Role1,Inds),
belongsrole(Role,Role1), !.
For instance, when searching the answer to the question "who collects the sap?",
the answer is searched in the following pool associated to the entity FARMER:
Answering Why-Questions in Closed Domains from a Discourse Model 113
entity(set,id32,28,facts([
card(infon117,id32,5),
fact(infon118,inst_of,[ind:id32,class:man],1,univ,univ),
fact(infon119,isa,[ind:id32,class:farmer],1,id31,id8),
fact(id33,collect,[agent:id32,theme_aff:id28],1,tes(f1_m6),id8),
fact(id58,drill,[agent:id32,theme_aff:id56],1,tes(f1_m9),id8),
fact(id63,put,[agent:id32,theme_aff:id61,loc_direct:id56],1,tes(f1_m10),id8),
fact(id69,hang,[agent:id32,theme_aff:id66,loc_direct:id67],1,tes(f1_m11),id8)])).
This is reached from the COLLECT and SAP entities pools, which are cross-
checked to verify that the same predicates are available.
entity(class,id28,7,facts([
fact(infon102,inst_of,[ind:id28,class:substance],1,univ,univ),
fact(infon103,isa,[ind:id28,class:sap],1,id27,id8),
fact(id29,make,[actor:id9,theme_aff:id28],1,tes(f1_m5),id8),
fact(id33,collect,[agent:id32,theme_aff:id28],1,tes(f1_m6),id8),
fact(id41,collect,[agent:id39,theme_aff:id28],1,tes(finf_m7),id8),
fact(id82,drip,[agent:id28, modal:id66],1,tes(f1_m13),id8),
fact(infon343,has,[arg:id88,theme:id28],1,id84,id85)])).
Then belongsrole checks to verify that the Role belongs to the appropriate set of
roles adequate for that slot in the PAS. In the ?why? case it has to search recursively
for events. This is the case represented by discourse anaphora of the type "this is
why/that is why", where the reason is a complex event structure:
extract_properties(Role,Ind,Facts,NewProp):-
Fact =.. [fact,Id,Pred,Args,Pol,Time,Place],
on(Fact,Fa),
on(_:Ind,Args),
on(disc_set:Disc,Args),
Disc=[Ind1:Pre:[Ro1:Id1, Ro2:Id2]],
buildarg2(Ro2,NewP,[Id1:Prop],FirstProp,Facts,MDs),
FirstProp=[Def1,nil,Num,NProp,Cat,PropLoc],
Fact1 =.. [fact,Id2,NewP,Args1,Pol1,Time1,Place1],
on(Fact1,Facts),
on(Ro3:Ind2,Args1),
Ind2$\backslash$=Id1,
buildarg2(Ro3,What,[Ind2:Prop],SecProp,Facts1,MDs),
SecondProp=[Def2,nil,Num2, NProp2,Cat2,PropLoc2],
Prop_Why=[to,What,NProp2],
NewProp=[Pre,[Def1,nil,Num,NProp,Cat,Prop_Why]], !.
Here below is the relevant DM representation of the other WHY question, the one
requesting for a Motivation through Rational clauses: ?why is the bucket covered??
loc(infon288, id73, [arg:main_tloc, arg:tes(sn7_m11)])
ind(infon289, id74)
fact(infon290, inst_of, [ind:id74, class:event], 1, univ, univ)
fact(infon291, isa, [ind:id74, class:rain], 1, univ, univ)
ind(infon292, id75)
fact(infon293, inst_of, [ind:id75, class:event], 1, univ, univ)
fact(infon294, isa, [ind:id75, class:snow], 1, univ, univ)
ind(infon295, id76)
fact(infon296, isa, [ind:id76, class:cover], 1, id73, id8)
fact(infon297, inst_of, [ind:id76, class:legal], 1, univ, univ)
fact(infon301, cover, [nil:id69, arg:id76], 1, id73, id8)
fact(id77,have,[owner:id69,prop:infon301],1,tes(sn10_m12),id8)
114 Delmonte and Pianta
fact(infon302, isa, [arg:id77, arg:st], 1, tes(sn10_m12), id8)
fact(infon303, isa, [arg:id78, arg:tloc], 1, tes(sn10_m12), id8)
fact(infon304, pres, [arg:id78], 1, tes(sn10_m12), id8)
fact(infon305, time, [arg:id77, arg:id78], 1, tes(sn10_m12), id8)
in(infon322, id74, id79)
in(infon323, id75, id79)
fact(id80,keep_out,[actor:id69,theme_aff:id79],1,tes(finf1_m12), id8)
fact(infon308, isa, [arg:id80, arg:pr], 1, tes(finf1_m12), id8)
fact(infon309, isa, [arg:id81, arg:tloc], 1, tes(finf1_m12), id8)
fact(infon310, pres, [arg:id81], 1, tes(finf1_m12), id8)
fact(infon311, time, [arg:id80, arg:id81], 1, tes(finf1_m12), id8)
fact(infon312, result, [arg:id77, arg:id80], 1, tes(sn10_m12), id8)
during(tes(sn10_m12), tes(sn7_m11))
includes(tr(sn10_m12), id73)
The relevant information is expressed as a semantic role RESULT, and is the one
connecting the two predicates, HAVE/KEEP_OUT. This is the piece of information
that will be used to answer the question.
7 Conclusions
In the paper we have shows that one can actually implement systems using deep lin-
guistic and semantic analysis to answer hard questions. Our systems employs repre-
sentations derived from Situation Semantics paradigm (Burke, 1991) and LFG syn-
tactic theory (Bresnan, 2001). We have exemplified its performance on a series of
factoid questions and we also added ?why? questions. GETARUNS has been able to
answer all questions proposed in the Mitre Workshop and also the additional seman-
tically and syntactically hard discourse bound Why question based on the recurrent
formulaic copulative expression ?this/that is why?. For a complete presentation of the
system please refer to Delmonte (2007).
References
Bresnan, J. (2001). Lexical-Functional Syntax. Oxford: Blackwell.
Brill, E., E. Charniak, M. Harper, M. Light, E. Riloff, and E. Voorhees (Eds.) (2000,
May). Reading Comprehension Tests as Evaluation for Computer-Based Language
Understanding Sytems, Seattle, Washington. ANLP-NAACL.
Burke, T. (1991). Peirce on truth and partiality. In J. Barwise, J. M. Gawron,
G. Plotkin, and S. Tutiya (Eds.), Situation Theory and its Applications. Stanford:
CSLI Publications.
Delmonte, R. (2007). Computational Linguistic Text Processing: Logical Form, Se-
mantic Interpretation, Discourse Relations and Question Answering. New York:
Nova Science Publishers.
Delmonte, R., G. Nicolae, S. Harabagiu, and C. Nicolae (2007). A linguistically-
based approach to discourse relations recognition. In B. Sharp and M. Zock (Eds.),
Natural Language Processing and Cognitive Science: Proc. of 4th NLPCS (Fun-
chal, Portugal), pp. 81?91. INSTICC PRESS.
Semantic and Pragmatic
Computing with GETARUNS
Rodolfo Delmonte
University of Venice "Ca? Foscari" (Italy)
email: delmont@unive.it
Abstract
We present a system for text understanding called GETARUNS, in its
deep version applicable only to Closed Domains. We will present the low
level component organized according to LFG theory. The system also
does pronominal binding, quantifier raising and temporal interpretation.
Then we will introduce the high level component where the Discourse
Model is created from a text. Texts belonging to closed domains are char-
acterized by the fact that their semantics is controlled or under command
of the system; and most importantly, sentences making up the texts are
fully parsed without failures. In practice, these texts are short and sen-
tences are also below a certain threshold, typically less than 25 words.
For longer sentences the system switches from the topdown to the bot-
tomup system. In case of failure it will backoff to the partial system which
produces a very lean and shallow semantics with no inference rules. The
small text we will present contains what is called a ?psychological state-
ment? sentence which contributes an important bias as to the linking of
the free pronominal expression contained in the last sentence.
287
288 Delmonte
1 The System GETARUNS
GETARUNS, the system for text understanding developed at the University of Venice,
is equipped with three main modules: a lower module for parsing where sentence
strategies are implemented; a middle module for semantic interpretation and discourse
model construction which is cast into Situation Semantics; and a higher module where
reasoning and generation takes place.
The system is based on LFG theoretical framework (Bresnan, 2001) and has a
highly interconnected modular structure. The Closed Domain version of the system is
a top-down depth-first DCG-based parser written in Prolog Horn Clauses, which uses
a strong deterministic policy by means of a lookahead mechanism with a WFST to
help recovery when failure is unavoidable due to strong attachment ambiguity.
It is divided up into a pipeline of sequential but independent modules which realize
the subdivision of a parsing scheme as proposed in LFG theory where a c-structure is
built before the f-structure can be projected by unification into a DAG (Direct Acyclic
Graph). In this sense we try to apply in a given sequence phrase-structure rules as they
are ordered in the grammar: whenever a syntactic constituent is successfully built, it is
checked for semantic consistency. In case the governing predicate expects obligatory
arguments to be lexically realized they will be searched and checked for uniqueness
and coherence as LFG grammaticality principles require.
Syntactic and semantic information is accessed and used as soon as possible: in
particular, both categorial and subcategorization information attached to predicates in
the lexicon is extracted as soon as the main predicate is processed, be it adjective,
noun or verb, and is used to subsequently restrict the number of possible structures
to be built. Adjuncts are computed by semantic compatibility tests on the basis of
selectional restrictions of main predicates and adjuncts heads.
The output of grammatical modules is fed then onto the Binding Module (BM)
which activates an algorithm for anaphoric binding. Antecedents for pronouns are
ranked according to grammatical function, semantic role, inherent features and their
position at f-structure. Eventually, this information is added into the original f-structure
graph and then passed on to the Discourse Module (DM).
The grammar is equipped with a core lexicon containing most frequent 5,000 fully
specified inflected word forms where each entry is followed by its lemma and a list of
morphological features, organised in the form of attribute-value pairs. However, mor-
phological analysers for English are also available with big root dictionaries (25,000
for English) which only provide for syntactic subcategorization, though. In addition
to that there are all lexical form provided by a fully revised version of COMLEX, and
in order to take into account phrasal and adverbial verbal compound forms, we also
use lexical entries made available by UPenn and TAG encoding. Their grammatical
verbal syntactic codes have then been adapted to our formalism and are used to gener-
ate a subcategorization schemes with an aspectual and semantic class associated to it
? however no restrictions can reasonably be formulated on arguments of predicates.
Semantic inherent features for Out of Vocabulary Words, be they nouns, verbs, adjec-
tives or adverbs, are provided by a fully revised version of WordNet (Fellbaum, 1998)
? plus EuroWordnet, with a number of additions coming from computer, economics,
and advertising semantic fields ? in which we used 75 semantic classes similar to
those provided by CoreLex (Buitelaar, 1998).
Semantic and Pragmatic Computing with GETARUNS 289
When each sentence is parsed, tense aspect and temporal adjuncts are accessed to
build the basic temporal interpretation to be used by the temporal reasoner. Eventually
two important modules are fired: Quantifier Raising and Pronominal Binding. QR is
computed on f-structure which is represented internally as a DAG. It may introduce
a pair of functional components: an operator where the quantifier can be raised, and
a pool containing the associated variable where the quantifier is actually placed in
the f-structure representation. This information may then be used by the following
higher system to inspect quantifier scope. Pronominal binding is carried out at first at
sentence internal level. DAGs will be searched for binding domains and antecedents
matched to the pronouns if any to produce a list of possible bindings. Best candidates
will then be chosen.
2 The Upper Module
GETARUNS has a highly sophisticated linguistically based semantic module which is
used to build up the Discourse Model. Semantic processing is strongly modularized
and distributed amongst a number of different submodules which take care of Spatio-
Temporal Reasoning, Discourse Level Anaphora Resolution, and other subsidiary pro-
cesses like Topic Hierarchy which cooperate to find the most probable antecedent of
coreferring and cospecifying referential expressions when creating semantic individ-
uals. These are then asserted in the Discourse Model (hence the DM), which is then
the sole knowledge representation used to solve nominal coreference.
The system uses two resolution submodules which work in sequence: they consti-
tute independent modules and allow no backtracking. The first one is fired whenever
a free sentence external pronoun is spotted; the second one takes the results of the
first submodule and checks for nominal anaphora. They have access to all data struc-
tures contemporarily and pass the resolved pair, anaphor-antecedent to the following
modules.
Semantic Mapping is performed in two steps: at first a Logical Form is produced
which is a structuralmapping fromDAGs onto unscopedwell-formed formulas. These
are then turned into situational semantics informational units, infons which may be-
come facts or sits. Each unit has a relation, a list of arguments which in our case
receive their semantic roles from lower processing ? a polarity, a temporal and a
spatial location index.
3 The Text
The text we present for the shared task (Bos, 2008) is a ?psychological statement?
text, i.e. it includes a sentence (namely sentence 4) that represents a psychological
statement, i.e. it expresses the feelings and is viewed from the point of view of one of
the participants in the story. The relevance of the sentence is its role in the assignment
of the antecedent to the pronominal expressions contained in the following sentence.
Without such a sentence the anaphora resolution module would have no way of com-
puting ?John? as the legitimate antecedent of ?He/his?. On the contrary, in a system
like ours that computes Point of View and Discourse Domain on the basis of Informa-
tional Structure and Centering information, it will be possible to make available the
appropriate antecedent to the anaphora resolution module.
290 Delmonte
We will discuss mainly semantic information processing. In so doing we shall
have to devote some space to LFG grammatical representation, to Logical Form and
eventually the Discourse Model. However, since this is meant to be a short paper, we
will only be able to show some fragments of the overall representation, highlighting
the most important features and disregarding the rest. So first of all, consider the
sentences making up the text:
1. John went into a restaurant.
2. There was a table in the corner.
3. The waiter took the order.
4. The atmosphere was warm and friendly.
5. He began to read his book.
Wewill be able to present an almost complete sequence of representations as produced
by GETARUNS only for one sentence, and then we will comment on the rest.
1. John went into a restaurant
index:f1
pred:go
lex_form:[np/subj/agente/[human, object],
sp/obl/locat/[to, in, into]/[object, place]]
voice:active; mood:ind; tense:pres; cat:risultato
subj/agent:index:sn4
cat:[human]; pred:?John?
gen:mas; num:sing; pers:3; spec:def:?0?
tab_ref:[+ref, -pro, -ana, -class]
obl/locat:index:sn5
cat:[place]; pred:restaurant
num:sing; pers:3; spec:def:-
tab_ref:[+ref, -pro, -ana, +class]; qmark:q1
aspect:achiev_tr
rel1:[td(f1_res2)=tr(f1_res2)]
rel2:[included(tr(f1_res2), tes(f1_res2))]
specificity:-; ref_int:[tr(f1_res2)]
qops:qop:q(q1, indefinite)
?Centering and Topic Hierarchy?
state(1, retaining) topic(1, main, id5) topic(1, potential, id1)
INFORMATIONAL STRUCTURE
CLAUSE IDENTIFIER: 2-n1
CLAUSE TYPE: main/prop
FACTUALITY: factive
CHANGE IN THE WORLD: null
RELEVANCE: background
TEMP_RELATION: undef(tes(f1_res2), nil)
DISCOURSE FOCUS: tes(f1_res2)
DISCOURSE RELATION: narration
DISCOURSE DOMAIN: objective
POINT OF VIEW: narrator
Semantic and Pragmatic Computing with GETARUNS 291
LOGICAL FORM
wff(situation,
wff(go,
< entity : sn4 : wff(isa, sn4, John) >,
< indefinite : sn5 : wff(isa, sn5, restaurant) >,
< event : f1 :
wff(and, wff(isa, f1, ev),
wff(time, f1, < definite : t2 :
wff(and, wff(isa, t2, tloc),
wff(pres, t2)) >)) >))
DISCOURSE MODEL 2
/*** There was a table in the corner. ***/
loc(infon13, id4, [arg:main_sloc, arg:restaurant])
ind(infon14, id5)
fact(infon15, inst_of, [ind:id5, class:man], 1, univ, univ)
fact(infon16, name, [?John?, id5], 1, univ, univ)
fact(id6, go, [agente:id5, locat:id1], 1, tes(f1_res2), id4)
fact(infon19, isa, [arg:id6, arg:ev], 1, tes(f1_res2), id4)
fact(infon20, isa, [arg:id7, arg:tloc], 1, tes(f1_res2), id4)
fact(infon21, pres, [arg:id7], 1, tes(f1_res2), id4)
fact(infon22, time, [arg:id6, arg:id7], 1, tes(f1_res2), id4)
includes(tr(f1_res2), univ)
Sentence 2, is a presentational structure, where the subject form ?there? is recov-
ered as being part of the meaning of the main predicate in the semantics. The location
?in the corner? is computed as a adjunct and it is understood as a entertaining a meron-
imic relation with the main location, ?the restaurant?, again in the semantics. When
building the Discourse Model it is possible to fire inferences to recover pragmatic
unexpressed implicatures, as for instance, the fact that introducing a ?table? with a
presentational structure and an indefinite NP but accompanied by a definite location
induces the reader to produce such implicit information as indicated below, i.e, the
fact that the main topic and only current participant to the discourse is supposed to
be sitting at the table in the corner. This inference is fired by inferential rules that
look for relations intevening between main location and current location; also presen-
tational structure contributes by introducing an indefinite ?table? which is the trigger
of the SITTING event.
DISCOURSE MODEL 3
/*** The waiter took the order. ***/
loc(infon26, id8, [arg:main_tloc, arg:tes(f1_res2)])
ent(infon27, id9)
fact(infon28, inst_of, [ind:id9, class:place], 1, univ, univ)
fact(infon29, isa, [ind:id9, class:table], 1, id8, id4)
in(infon30, id9, id4)
fact(id10, sit, [actor:id5, locat:id9], 1, tes(f5_id10), id4)
fact(infon31, isa, [arg:id10, arg:ev], 1, tes(f5_id10), id4)
fact(infon32, isa, [arg:id11, arg:tloc], 1, tes(f5_id10), id4)
fact(infon33, isa, [arg:id11], 1, tes(f5_id10), id4)
ind(infon34, id12)
fact(infon35, inst_of, [ind:id12, class:place], 1, univ, univ)
292 Delmonte
fact(infon36, isa, [ind:id12, class:corner], 1, id8, id4)
fact(infon37, part_of, [restaurant, id12, id1], 1, id8, id4)
fact(id13, there_be, [prop:id9], 1, tes(f4_res3), id4)
This sentence is computed as containing an idiomatic predicate ?take_order? which
in turn has a BENEFICIARY/GOAL of the same event. In turn the Goal is com-
puted as if it were an obligatory semantic role like the missing Agent of passivized
structures. The semantics is then responsible for checking consistency of predicate-
argument structures. The Goal induces the presence of an Oblique which is filled with
an ?exist? dummy predicate. This predicate is then linked to the only other available
participant in the topic structure organized by the Centering Algorithm, John with
semantic Id = id5.
index:f1
pred:take
lex_form:[np/subj/agent/[human], idioms/obj/form/[order],
pp/obl/goal/from/[human]]
voice:active; mood:ind; tense:pres; cat:activity
subj/agent:index:sn3
cat:[human, social]; pred:waiter
gen:mas; num:sing; pers:3; spec:def:+
tab_ref:[+ref, -pro, -ana, +class]
ogg/form:index:sn4
cat:[activity, event]; pred:order
num:sing; pers:3; spec:def:+
tab_ref:[+ref, -pro, -ana, +class]
obj2/goal:index:sn5
cat:[human, animate]; pred:exist
spec:def:-; part:+
tab_ref:[+ref, -pro, -ana, +me]
aspect:activity
rel1:[td(f1_res4)=tr(f1_res4)]
rel2:[included(tr(f1_res4), tes(f1_res4))]
specificity:+; ref_int:[tr(f1_res4)]
?Centering and Topic Hierarchy?
state(4, continue) topic(4, main, id5) topic(4, potential, id16)
DISCOURSE MODEL 4
/*** The atmosphere was warm and friendly. ***/
loc(infon49, id15, [arg:main_tloc, arg:tes(f4_res3)])
ind(infon50, id16)
fact(infon51, inst_of, [ind:id16, class:social_role], 1, univ, univ)
fact(infon52, isa, [ind:id16, class:waiter], 1, id15, id4)
fact(infon53, role, [waiter, id4, id16], 1, id15, id4)
fact(infon55, isa, [arg:id5, arg:exist], 1, id15, id4)
fact(id18, take_order, [agent:id16, goal:id5], 1, tes(f1_res4), id4)
Sentence 4, is the psychological statement, where the Centering Algorithm uses the
information made available by the computational called Informational Structure that
we report here below.
?Centering and Topic Hierarchy?
state(4, continue) topic(4, main, id5) topic(4, potential, id21)
Semantic and Pragmatic Computing with GETARUNS 293
INFORMATIONAL STRUCTURE
CLAUSE IDENTIFIER: 5-n1
CLAUSE TYPE: main/prop
FACTUALITY: factive
CHANGE IN THE WORLD: null
RELEVANCE: background
TEMP_RELATION: during(tes(f1_res5), tes(f1_res4))
DISCOURSE FOCUS: tes(f1_res5)
DISCOURSE RELATION: explanation
DISCOURSE DOMAIN: subjective
POINT OF VIEW: John
As can be noticed, the system has computed the Discourse Domain as ?subjective?,
and the Point of View as belonging to one of the participants, the one referred by with
a proper name. In fact, it is just the use of a definite expression ?the waiter? that tells
the system to underrate the importance in the Topic Hierarchy automatically built by
the Centering Algorithm.
DISCOURSE MODEL
loc(infon77, id24, [arg:main_tloc, arg:tes(f1_res5)])
fact(infon78, poss, [?John?, id5, id25], 1, id24, id4)
ind(infon79, id25)
fact(infon80, inst_of, [ind:id25, class:thing], 1, univ, univ)
fact(infon81, isa, [ind:id25, class:book], 1, id24, id4)
fact(id26, read, [agent:id5, theme_aff:id25], 1, tes(finf1_res6), id4)
fact(infon85, isa, [arg:id26, arg:ev], 1, tes(finf1_res6), id4)
fact(infon86, isa, [arg:id27, arg:tloc], 1, tes(finf1_res6), id4)
fact(infon87, pres, [arg:id27], 1, tes(finf1_res6), id4)
fact(infon88, time, [arg:id26, arg:id27], 1, tes(finf1_res6), id4)
fact(id28, begin, [actor:id5, prop:id26], 1, tes(f1_res6), id4).
4 Performance on the Shared Task Texts
If we try to grade the seven texts of the shared task (Bos, 2008), from the point of view
of their intrinsic semantic complexity we should get the following picture:
(a) Texts 6, 7 (scientific texts)
(b) Texts 4, 5 (newswire articles)
(c) Texts 1, 2, 3 (made up texts, schoolbook texts)
Overall, the system performed better with category (c). texts and worse with scien-
tific texts, category (a). I take Text 6 and 7 to be in need of a specific domain ontology
in order to have semantic inferences fired when needed. In addition, in our case, these
two texts have sentences exceeding the maximum length for topdown parsing, which
is the modality that better guarantees a full parse. Text 6 has sentences respectively
31, 38 and 49. In fact Text 1 represents an easy to understand scientific text and is
much easier to parse ? even though there are mistakes in Adjuncts attachment.
Apart from Texts 6 and 7, which lack in semantic relations due to the lack of se-
mantic information, the remaining texts abound in semantically relevant syntactic in-
formation which can be used to assert facts in the Discourse Model which create a
294 Delmonte
network of meaningful associations. PAs, that is Predicate Argument structures, to-
gether with implicit optional and obligatory arguments are mostly recovered ? more
on this in the following sections.
The system has failed in finding antecedents for the pronoun IT. The current version
of the complete system is not equipped with an algorithm that tells expletive IT cases
from referential ones. On the contrary, one such algorithm has been successfully
experimented with the partial system. Other pronouns are almost all correctly bound.
As for nominal expressions, problems arise with scientific texts in case a different
linguistic description is used to corefer or cospecify to the same entity.
For every text we will list pieces of what we call the Discourse Model World of
Entities participating in the events described in the text. This file is produced at the
end of the analysis and contains all entities recorded with a semantic Identifier by the
system during the analysis of the text. The file is produced by a procedure that re-
cursively searches the dynamic database of FACTS or Infons in Situation Semantics
terms, associated to each entity semantic identifier. These Infons may register prop-
erties, attributes or participation in events. Eventually, Infons may also be inherited
in case one of the entity is semantically included in another entity ? see the case
of CANCER being included in the more general notion of CANCERS at the end of
Text 2.
The procedure produces a score that is derived from the relevance in terms of top-
ichood ? being Main, Secondary or Potential Topic ? as asserted by the Centering
algorithm. Entities and their associated infons are thus graded according to relevance.
They are listed on the basis of their ontological status: INDividuals, SETs, CLASSes.
4.1 Text One
The main topic is the OBJECT. As can be gathered from the question posed to the
system at the end of the parse, the main relations are all captured throughout the text.
They can also be recovered from the Inherited Discourse World of Entities:
entity(ind,id2,9,facts([
fact(infon111, coincide, [arg:id24, arg:id29], 1, tes(sn59_t13), id20),
fact(infon4, isa, [ind:id2, class:object], 1, id1, univ),
fact(infon5, inst_of, [ind:id2, class:thing], 1, univ, univ),
fact(id9, throw, [tema_nonaff:id2, agente:id8], 1, tes(sn42_t11), univ),
fact(id17, fall, [actor:id2, modale:id16], 1, tes(f1_t12), univ),
fact(id29, take, [actor:id26, theme_aff:id2], 1, tes(finf1_t13), id20)])).
THROW is understood as being an event that takes place from a CLIFF and with a
SPEED. However the SPEED is HORIZONTAL but the CLIFF is not HIGH ? this
relation has been missed. The OBJECT falls from a height of the same CLIFF. The
one but last sentence is only partially represented. On the contrary, the final question
is perfectly understood.
4.2 Text Two
The main topic is CANCER. From the Discourse World we know that:
entity(class,id3,2,facts([
fact(infon7, inst_of, [ind:id3, class:stato], 1, univ, univ),
fact(infon8, isa, [ind:id3, class:cancer], 1, id1, univ),
Semantic and Pragmatic Computing with GETARUNS 295
fact(id4, cause, [theme_aff:id3, agent:id2], 1, tes(f2_t21), univ),
fact(infon81, isa, [arg:id3, arg:cancer], 1, id25, id26),
fact(id31, look, [actor:id27, locat:id3], 1, tes(f3_t23), id26)])).
CANCER is CAUSED by a VIRUS and that RESEARCHERs have been LOOKing
for other CANCERs which receive a different semantic identifier but inherit all the
properties:
entity(class,id28,2,facts([ in(infon79, id28, id3),
fact(infon75, cause, [ind:id28], 1, id25, id26),
fact(infon76, of, [arg:id28, specif:id28], 1, univ, univ),
fact(infon77, inst_of, [ind:id28, class:stato], 1, univ, univ),
fact(infon78, isa, [ind:id28, class:cancer], 1, id25, id26),
fact(*, inst_of, [ind:id28, class:stato], 1, univ, univ),
fact(*, isa, [ind:id28, class:cancer], 1, id1, univ),
fact(*, cause, [theme_aff:id28, agent:id2], 1, tes(f2_t21), univ),
fact(*, isa, [arg:id28, arg:cancer], 1, id25, id26),
fact(*, look, [actor:id27, locat:id28], 1, tes(f3_t23), id26)])).
The VIRUS is understood as the AGENT.
entity(ind,id2,11,facts([
fact(infon4, isa, [ind:id2, class:virus], 1, id1, univ),
fact(infon5, inst_of, [ind:id2, class:animal], 1, univ, univ),
fact(id4, cause, [theme_aff:id3, agent:id2], 1, tes(f2_t21), univ),
fact(infon82, isa, [arg:id2, arg:virus], 1, id25, id26),
fact(id29, cause, [agent:id2], 1, tes(f2_t23), id26)])).
The system also understands that those EVENTs, were KNOWn for some time, as
shown by the ID8 which is bound in the discourse by means of THAT to the event id4
listed above,
entity(ind,id8,1,facts([
fact(infon21, prop, [arg:id8,
disc_set:[id4:cause:
[theme_aff:id3, agent:id2]]],
1, id6, id7),
fact(infon31, isa, [arg:id8, arg:that], 1, id6, id7),
fact(id12, know, [tema_nonaff:id8, actor:id11], 1, tes(f2_t22), id7)])).
However the system has not bound IT to THAT so we do not know what LEADs to a
vaccine, nor do we know what prevents from what. All IT are unbound.
4.3 Text Three
This is the text that we proposed for the shared task and is already completely and
consistently semantically and pragmatically represented. It has already been presented
above.
4.4 Text Four
The text is not completely and consistently represented but most of the relations are
fully understood. In particular consider THEY in the third sentence which is rightly
bound to the SET of two trainers asserted in the DiscourseWorld. The school is always
coindixed. The last sentence contains a first plural pronoun WE which is interpreted
as being coindexed with the narrator, but also wrongly with the location of the text.
296 Delmonte
4.5 Text Five
The text is not completely and consistently represented but most of the relations are
fully understood. We still know a lot about the main Entities, the PROPELLANT and
NITROCELLULOSE which is composed in CHUNKs.
entity(ind,id19,8,facts([
fact(infon42, inst_of, [ind:id19, class:sub], 1, univ, univ),
fact(infon43, isa, [ind:id19, class:propellant], 1, id18, nil),
fact(infon44, isa, [arg:id19, arg:propellant], 1, id18, univ),
fact(id20, explode, [agent:id19], 1, tes(f1_t53), univ),
fact(infon108, isa, [arg:id19, arg:propellant], 1, id30, univ),
fact(id38, use, [theme_aff:id19, actor:id37], 1, tes(f2_t55), univ),
fact(id41, make, [theme_aff:id19, actor:id40, loc_origin:id31],
1, tes(sn32_t55), univ),
fact(id20, explode, [agent:id19], 1, tes(f1_t53), univ),
fact(infon50, sub, [prop:id20], 1, id18, univ)])).
entity(ind,id32,1.2,facts([ in(infon91, id32, id31),
fact(infon89, inst_of, [ind:id32, class:sub], 1, univ, univ),
fact(infon90, isa, [ind:id32, class:nitrocellulose], 1, id30, nil),
fact(*, nitrocellulose, [ind:id32], 1, id30, nil),
fact(*, produce, [ind:id32], 1, id30, nil),
fact(*, repackage, [ind:id32], 1, id30, nil),
fact(*, of, [arg:id32, specif:id31], 1, univ, univ),
fact(*, of, [arg:id32, specif:id31], 1, univ, univ),
fact(*, of, [arg:id32, specif:id31], 1, univ, univ),
fact(*, inst_of, [ind:id32, class:col], 1, univ, univ),
fact(*, isa, [ind:id32, class:chunk], 1, id30, nil),
fact(*, make, [theme_aff:id19, actor:id40, loc_origin:id32],
1, tes(sn32_t55), univ)])).
entity(set,id31,1,facts([ card(infon79, id31, 5),
fact(infon80, nitrocellulose, [ind:id31], 1, id30, nil),
fact(infon81, produce, [ind:id31], 1, id30, nil),
fact(infon82, repackage, [ind:id31], 1, id30, nil),
fact(infon83, of, [arg:id31, specif:id31], 1, univ, univ),
fact(infon86, inst_of, [ind:id31, class:col], 1, univ, univ),
fact(infon87, isa, [ind:id31, class:chunk], 1, id30, nil),
fact(id41, make, [theme_aff:id19, actor:id40, loc_origin:id31],
1, tes(sn32_t55), univ)])).
The relation intervening between CHUNKS and NITROCELLULOSE endows tran-
sitivity to the EVENTS taking place so that both are involved in REPACKAGE, PRO-
DUCE, MAKE. We also know that a CREWMAN was OPERATING at a center and
that the GUN CREW was KILLed, by an unknown AGENT, id26.
entity(class,id23,6,facts([
fact(infon55, of, [arg:id23, specif:id8], 1, univ, univ),
fact(infon56, inst_of, [ind:id23, class:institution], 1, univ, univ),
fact(infon57, isa, [ind:id23, class:crew], 1, id22, nil),
fact(id27, kill, [theme_aff:id23, agent:id26], 1, tes(f2_t54), univ)])).
We know that EVENTS happened during WORLD_WAR_II. Also notice that IT
SUBJect of SUSPECT is correctly computed as an expletive.
Semantic and Pragmatic Computing with GETARUNS 297
4.6 Text Six
Two of the sentences are parsed by the partial system, but the main relations are well
understood. The FARM and the COMMUNITY provide FOOD and EARNs a REV-
ENUE.
entity(ind,id13,3,facts([
fact(infon30, inst_of, [ind:id13, class:informa], 1, univ, univ),
fact(infon31, isa, [ind:id13, class:farm], 1, univ, univ),
fact(id17, provide, [goal:id8,tema_nonaff:id7,actor:id13],1,univ,univ),
fact(infon85, isa, [arg:id13, arg:farm], 1, id41, univ),
fact(id43, earn, [agent:id13, theme_aff:id42], 1, tes(sn59_t63),univ)])).
entity(ind,id7,0,facts([
fact(infon10, inst_of, [ind:id7, class:any], 1, univ, univ),
fact(infon11, isa, [ind:id7, class:food], 1, univ, univ),
fact(id17, provide,[goal:id8,tema_nonaff:id7,actor:id13],1,univ,univ)])).
entity(ind,id42,2,facts([
fact(infon83, inst_of, [ind:id42, class:legal], 1, univ, univ),
fact(infon84, isa, [ind:id42, class:revenue], 1, id41, nil),
fact(id43, earn, [agent:id13, theme_aff:id42], 1, tes(sn59_t63), univ)])).
The COMMUNITY LACK the FOOD
entity(ind,id8,0,facts([
fact(infon13, inst_of, [ind:id8, class:luogo], 1, univ, univ),
fact(infon14, isa, [ind:id8, class:community], 1, univ, univ),
fact(id17, provide, [goal:id8,tema_nonaff:id7,actor:id13],1,univ,univ),
fact(id14, lack, [theme_aff:id9, actor:id8, purpose:cl5, result:id14],
1, univ, univ)])).
Most of the sentences are parsed by the partial system. However questions can be
asked and get a reply, even though the generator does not handle uncountable nouns
like MONEY properly.
4.7 Text Seven
The most difficult text is fully parsed but not satisfactorily semantically represented.
We only know few things, and they are all unrelated. There is no way to related WIND
to TURBINE and to ENERGY in a continuous way.
entity(set,id61,4,facts([ card(infon253, id61, 5),
fact(infon254, power, [nil:id61], 1, id60, id20),
fact(infon255, maximum, [ind:id61], 1, id60, id20),
fact(infon256, of, [arg:id61, specif:id61], 1, univ, univ),
fact(infon257, wind_turbine, [ind:id61], 1, id60, id20),
fact(infon258, inst_of, [ind:id61, class:thing], 1, univ, univ),
fact(infon259, isa, [ind:id61, class:[wind, turbine]], 1, id60, id20),
fact(infon264, of, [arg:id63, specif:id61], 1, univ, univ),
fact(infon267, isa, [arg:id61, arg:wind_turbine], 1, id60, id20),
fact(infon268, isa, [arg:id61, arg:power], 1, id60, id20),
fact(infon269, typical, [arg:id61], 1, id60, id20),
fact(infon271, power, [nil:id61, arg:id61], 1, id60, id20)])).
298 Delmonte
entity(ind,id14,2,facts([
fact(infon52, inst_of, [ind:id14, class:abstract_state], 1, univ, univ),
fact(infon53, inst_of, [ind:id14, class:energy], 1, univ, univ),
fact(infon54, isa, [ind:id14, class:energy], 1, univ, univ),
fact(infon55, isa, [ind:id14, class:wind_energy], 1, univ, univ),
fact(infon58, of, [arg:id15, specif:id14], 1, univ, univ)])).
entity(ind,id22,1,facts([ in(infon90, id22, id15),
fact(infon88, inst_of, [ind:id22, class:thing], 1, univ, univ),
fact(infon89, isa, [ind:id22, class:wind], 1, id19, id20),
fact(*, isa, [ind:id22, class:wind], 1, univ, univ),
fact(*, of, [arg:id22, specif:id14], 1, univ, univ)])).
We know that WIND and ENERGY are related, and also that there is one such tech-
nology, but is semantically set apart, due to orthography.
entity(class,id11,1,facts([
fact(infon39, ?wind-energy?, [ind:id11], 1, id1, univ),
fact(infon44, of, [arg:id11, specif:id12], 1, univ, univ),
fact(infon45, inst_of, [ind:id11, class:abstract_state], 1, univ, univ),
fact(infon46, isa, [ind:id11, class:technology], 1, id1, univ)])).
entity(class,id12,0,facts([
fact(infon41, inst_of, [ind:id12, class:astratto], 1, univ, univ),
fact(infon42, isa, [ind:id12, class:energy], 1, univ, univ),
fact(infon44, of, [arg:id11, specif:id12], 1, univ, univ),
fact(infon103, has, [arg:id26, tema:id12], 1, id19, id20),
fact(infon109, of, [arg:id26, specif:id12], 1, univ, univ)])).
I assume that scientific language requires a different setup of semantic rules of infer-
ence, which can only be appropriately specified in a domain ontology.
References
Bos, J. (2008). Introduction to the Shared Task on Comparing Semantic Representa-
tions. In J. Bos and R. Delmonte (Eds.), Semantics in Text Processing. STEP 2008
Conference Proceedings, Volume 1 of Research in Computational Semantics, pp.
257?261. College Publications.
Bresnan, J. (2001). Lexical-Functional Syntax. Oxford: Blackwell.
Buitelaar, P. (1998). CoreLex: Systematic Polysemy and Underspecification. Ph. D.
thesis, Brandeis University.
Delmonte, R. (2007). Computational Linguistic Text Processing: Logical Form, Se-
mantic Interpretation, Discourse Relations and Question Answering. New York:
Nova Science Publishers.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. Cambridge (MA):
MIT Press.
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 54?62,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Desperately Seeking Implicit Arguments in Text
Sara Tonelli
Fondazione Bruno Kessler / Trento, Italy
satonelli@fbk.eu
Rodolfo Delmonte
Universit Ca? Foscari / Venezia, Italy
delmont@unive.it
Abstract
In this paper, we address the issue of automat-
ically identifying null instantiated arguments
in text. We refer to Fillmore?s theory of prag-
matically controlled zero anaphora (Fillmore,
1986), which accounts for the phenomenon of
omissible arguments using a lexically-based
approach, and we propose a strategy for iden-
tifying implicit arguments in a text and finding
their antecedents, given the overtly expressed
semantic roles in the form of frame elements.
To this purpose, we primarily rely on linguis-
tic knowledge enriched with role frequency
information collected from a training corpus.
We evaluate our approach using the test set
developed for the SemEval task 10 and we
highlight some issues of our approach. Be-
sides, we also point out some open problems
related to the task definition and to the general
phenomenon of null instantiated arguments,
which needs to be better investigated and de-
scribed in order to be captured from a compu-
tational point of view.
1 Introduction
In natural language, lexically unexpressed linguistic
items are very frequent and indirectly weaken any
attempt at computing the meaning of a text or dis-
course. However, the need to address semantic in-
terpretation is strongly felt in current advanced NLP
tasks, in particular, the issue of transforming a text
or discourse into a set of explicitly interconnected
predicate-argument/adjunct structures (hence PAS).
The aim of this task would be to unambiguously
identify events and participants and their association
to spatiotemporal locations. However, in order to do
that, symbolic and statistical approaches should be
based on the output representation of a deep parser,
which is currently almost never the case. Current
NLP technologies usually address the surface level
linguistic information with good approximation in
dependency or constituency structures, but miss im-
plicit entities (IEs) altogether. The difficulties to
deal with lexically unexpressed items or implicit en-
tities are related on the one hand to recall problems,
i.e. the problem of deciding whether an item is im-
plicit or not, and on the other hand to precision prob-
lems, i.e. if an implicit entity is accessible to the
reader from the discourse or its context, an appropri-
ate antecedent has to be found. However, a system
able to derive the presence of IEs may be a deter-
mining factor in improving performance of QA sys-
tems and, in general, in Informations Retrieval and
Extraction systems.
The current computational scene has witnessed an
increased interest in the creation and use of semanti-
cally annotated computational lexica and their asso-
ciated annotated corpora, like PropBank (Palmer et
al., 2005), FrameNet (Baker et al, 1998) and Nom-
Bank (Meyers, 2007), where the proposed annota-
tion scheme has been applied in real contexts. In all
these cases, what has been addressed is a basic se-
mantic issue, i.e. labeling PAS associated to seman-
tic predicates like adjectives, verbs and nouns. How-
ever, what these corpora have not made available is
information related to IEs. For example, in the case
of eventive deverbal nominals, information about the
subject/object of the nominal predicate is often im-
plicit and has to be understood from the previous
54
discourse or text, e.g. ?the development of a pro-
totype [? implicit subject]?. As reported by Gerber
and Chai (2010), introducing implicit arguments to
nominal predicates in NomBank would increase the
resource coverage of 65%.
Other IEs can be found in agentless passive con-
structions ( e.g.?Our little problem will soon be
solved ? [? unexpressed Agent ]?1) or as unex-
pressed arguments such as addressee with verbs of
commitment, for example ?I can promise ? that one
of you will be troubled [? unexpressed Addressee]?
and ?I dare swear ? that before tomorrow night he
will be fluttering in our net [? unexpressed Ad-
dressee]?.
In this paper we discuss the issues related to the
identification of implicit entities in text, focussing in
particular on omissions of core arguments of pred-
icates. We investigate the topic from the perspec-
tive proposed by (Fillmore, 1986) and base our ob-
servations on null instantiated arguments annotated
for the SemEval 2010 Task 10, ?Linking Events and
Their Participants in Discourse? (Ruppenhofer et al,
2010)2. The paper is structured as follows: in Sec-
tion 2 we detail the task of identifying null instan-
tiated arguments from a theoretical perspective and
describe related work. In Section 3 we briefly in-
troduce the SemEval task 10 for identifying implicit
arguments in text, while in Section 4 we detail our
proposal for NI identification and binding. In Sec-
tion 5 we give a thorough description of the types of
null instantiations annotated in the SemEval data set
and we explain the behavior of our algorithm w.r.t.
such cases. We also compare our results with the
output of the systems participating to the SemEval
task. Finally, we draw some conclusions in Section
6.
2 Related work
In this work, we focus on null complements, also
called pragmatically controlled zero anaphora (Fill-
more, 1986), understood arguments or linguistically
1Unless otherwise specified, the following examples are
taken from the data sets made available in the SemEval 2010
task ?Linking Events and Their Participants in Discourse?.
Some of them have been slightly simplified for purposes of ex-
position.
2http://semeval2.fbk.eu/semeval2.php?
location=tasks&taskid=9
unrealized arguments. We focus on Fillmore?s the-
ory because his approach represents the backbone of
the FrameNet project, which in turn inspired the Se-
mEval task we will describe below. Fillmore (1986)
shows that in English and many other languages
some verbs allow null complements and some oth-
ers don?t. The latter require that, when they ap-
pear in a sentence, all core semantic roles related
to the predicate are expressed. For example, sen-
tences like ?Mary locked ??? or ?John guaranteed
??? are not grammatically well-formed, because they
both require two mandatory linguistically inherent
participants. Fillmore tries to explain why seman-
tic roles can sometimes be left unspoken and what
constraints help the interpreter recover the missing
roles. He introduces different factors that can in-
fluence the licensing of null complements. These
can be lexically-based, (semantically close predi-
cates like ?promise? and ?guarantee? can license the
omission of the theme argument in different cases),
motivated by the interpretation of the predicate (?I
was eating ?? licenses a null object because it has
an existential interpretation) and depending on the
context (see for example the use of impress in an
episodic context like ?She impressed the audience?,
where the null complement is not allowed, compared
to ?She impresses ? every time? in habitual interpre-
tation; examples from Ruppenhofer and Michaelis
(2009)).
The fact that Fillmore explains the phenomenon
of omissible arguments with a lexically-based ap-
proach implies that from his perspective neither a
purely pragmatic nor a purely semantic approach
can account for the behavior of omissible arguments.
For example, he argues that some verbs, such as to
lock will never license a null complement, no matter
in which pragmatic context they are used. Besides,
there are synonymous verbs which behave differ-
ently as regards null complementation, which Fill-
more sees as evidence against a purely semantic ex-
planation of implicit arguments.
Another relevant distinction drawn in Fillmore
(1986) is the typology of omitted arguments, which
depends on the type of licensor and on the interpre-
tation of the null complement. Fillmore claims that
with some verbs the missing complement can be re-
trieved from the context, i.e. it is possible to find a
referent previously mentioned in the text / discourse
55
and bearing a definite, precise meaning. These cases
are labeled as definite null complements or instantia-
tions (DNI) and are lexically specific in that they ap-
ply only to some predicates. We report an example
of DNI in (1), taken from the SemEval task 10 data
set (see Section 3). The predicate ?waiting? has an
omitted object, which we understand from the dis-
course context to refer to ?I?.
(1) I saw him rejoin his guest, and I crept quietly
back to where my companions were waiting ?
to tell them what I had seen.
DNIs can also occur with nominal predicates, as
reported in (2), where the person having a thought,
the baronet, is mentioned in the preceding sentence:
(2) Stapleton was talking with animation, but the
baronet looked pale and distrait. Perhaps the
thought of that lonely walk across the
ill-omened moor was weighing heavily upon
his mind.
In contrast to DNIs, Fillmore claims that with
some verbs and in some interpretations, a core ar-
gument can be omitted without having a referent
expressing the meaning of the null argument. The
identity of the missing argument can be left un-
known or indefinite. These cases are labeled as in-
definite null complements or instantiations (INI) and
are constructionally licensed in that they apply to
any predicate in a particular grammatical construc-
tion. See for example the following cases, where the
omission of the agent is licensed by the passive con-
struction:
(3) One of them was suddenly shut off ?.
(4) I am reckoned fleet of foot ?.
Cases of INI were annotated by the organizers of
the SemEval task 10 also with nominal predicates,
as shown in the example below, where the perceiver
of the odour is left unspecified:
(5) Rank reeds and lush, slimy water-plants sent
an odour ? of decay and a heavy miasmatic
vapour.
Few attempts have been done so far to automati-
cally deal with the recovery of implicit information
in text. One of the earliest systems for identifying
extra-sentential arguments is PUNDIT by Palmer et
al. (1986). This Prolog-based system comprises a
syntactic component for parsing, a semantic compo-
nent, which decomposes predicates into component
meanings and fills their semantic roles with syntactic
constituents based on a domain-specific model, and
a reference resolution component, which is called
both for explicit constituents and for obligatory im-
plicit constituents. The reference resolution process
is based on a focus list with all potential pronominal
referents identified by the semantic component. The
approach, however, has not been evaluated on a data
set, so we cannot directly compare its performance
with other approaches. Furthermore, it is strongly
domain-dependent.
In a case study, Burchardt et al (2005) propose
to identify implicit arguments exploiting contex-
tual relations from deep-parsing and lexico-semantic
frame relations encoded in FrameNet. In particu-
lar, they suggest converting a text into a network of
lexico-semantic predicate-argument relations con-
nected through frame-to-frame relations and recur-
rent anaphoric linking patterns. However, the au-
thors do not implement and evaluate this approach.
Most recently, Gerber and Chai (2010) have pre-
sented a supervised classification model for the re-
covery of implicit arguments of nominal predicates
in NomBank. The model features are quite different
from those usually considered in standard SRL tasks
and include among others information from Verb-
Net classes, pointwise mutual information between
semantic arguments, collocation and frequency in-
formation about the predicates, information about
parent nodes and siblings of the predicates and dis-
course information. The authors show the feasibility
of their approach, which however relies on a selected
group of nominal predicates with a large number of
annotated instances.
The first attempt to evaluate implicit argument
identification over a common test set and consider-
ing different kinds of predicates was made by Rup-
penhofer et al (2010). Further details are given in
the following section.
56
Data set Sentences Frame inst. Frame types Overt FEs DNIs (resolved) INIs
Train 438 1,370 317 2,526 303 (245) 277
Test 525 1,703 452 3,141 349 (259) 361
Table 1: Data set statistics from SemEval task 10
3 SemEval 2010 task 10
The SemEval-2010 task for linking events and their
participants in discourse (Ruppenhofer et al, 2010)
introduced a new issue w.r.t. the SemEval-2007
task ?Frame Semantic Structure Extraction? (Baker
et al, 2007), in that it focused on linking local se-
mantic argument structures across sentence bound-
aries. Specifically, the task included first the identi-
fication of frames and frame elements in a text fol-
lowing the FrameNet paradigm (Baker et al, 1998),
then the identification of locally uninstantiated roles
(NIs). If these roles are indefinite (INI), they have
to be marked as such and no antecedent has to be
found. On the contrary, if they are definite (DNI),
their coreferents have to be found in the wider dis-
course context. The challenge comprised two tasks,
namely the full task (semantic role recognition and
labeling + NI linking) and the NIs only task, i.e. the
identification of null instantiations and their refer-
ents given a test set with gold standard local seman-
tic argument structure. In this work, we focus on the
latter task.
The data provided to the participants included a
training and a test set. The training data comprised
438 sentences from Arthur Conan Doyle?s novel
?The Adventure of Wisteria Lodge?, manually an-
notated with frame and INI/DNI information. The
test set included 2 chapters of the Sherlock Holmes
story ?The Hound of Baskervilles? with a total of
525 sentences, provided with gold standard frame
information. The participants had to i) assess if a lo-
cal argument is implicit; ii) decide whether it is an
INI or a DNI and iii) in the second case, find the
antecedent of the implicit argument. We report in
Table 1 some statistics about the provided data sets
from Ruppenhofer et al (2010). Note that overt FEs
are the explicit frame elements annotated in the data
set.
Although 26 teams downloaded the data sets,
there were only two submissions, probably depend-
ing on the intrinsic difficulties of the task (see dis-
cussion in Section 5). The best performing system
(Chen et al, 2010) is based on a supervised learn-
ing approach using, among others, distributional se-
mantic similarity between the heads of candidate
referents and role fillers in the training data, but
its performance is strongly affected by data sparse-
ness. Indeed, only 438 sentences with annotated
NIs were made available in the training set, which
is clearly insufficient to capture such a multifaceted
phenomenon with a supervised approach. The sec-
ond system participating in the task (Tonelli and
Delmonte, 2010) was an adaptation of an exist-
ing LFG-based system for deep semantic analysis
(Delmonte, 2009), whose output was mapped to
FrameNet-style annotation. In this case, the major
challenge was to cope with the classification of some
NI phenomena which are very much dependent on
frame specific information, and can hardly be gener-
alized in the LFG framework.
4 A linguistically motivated proposal for
NI identification and binding
In this section, we describe our proposal for dealing
with INI/DNI identification and evaluate our output
against SemEval gold standard data. As discussed in
the previous section, existing systems dealing with
this task suffer on the one hand from a lack of train-
ing data and on the other hand from the dependence
of the task on frame annotation, which makes it diffi-
cult to adapt existing unsupervised approaches. We
show that, given this state of the art, better results
can be achieved in the task by simply developing an
algorithm that reflects as much as possible the lin-
guistic motivations behind NI identification in the
FrameNet paradigm. Our approach is divided into
two subtasks: i) identify INIs/DNIs and ii) for each
DNI, find the corresponding referent in text.
We develop an algorithm that incorporates the fol-
lowing linguistic information:
FE coreness status Null instantiated arguments as
defined in FrameNet are always core arguments, i.e.
57
they are central to the meaning of a frame. Since
the coreness status of the arguments is encoded in
FrameNet, we limit our search for an NI only if a
core frame element is not overtly expressed in the
text.
Incorporated FEs Although all lexical units be-
longing to the same frame in the FrameNet database
are characterized by the same set of core FEs, a fur-
ther distinction should be introduced when dealing
with NIs identification. For example, in PERCEP-
TION ACTIVE, several predicates are listed, which
however have a different behavior w.r.t. the core
Body part FE. ?Feel.v?, for instance, is underspec-
ified as regards the body part perceiving the sensa-
tion, so we can assume that when it is not overtly
expressed, we have a case of null instantiation. For
other verbs in the same frame, such as ?glance.v? or
?listen.v?, the coreness status of Body part seems to
be more questionable, because the perceiving organ
is already implied by the verb meaning. For this rea-
son, we argue that if Body part is not expressed with
?glance.v? or ?listen.v?, it is not a case of null instan-
tiation. Such FEs are defined as incorporated in the
lexical unit and are encoded as such in FrameNet.
Excludes and Includes relation In FrameNet,
some information about the relationship between
certain FEs is encoded. In particular, some FEs are
connected by the Excludes relation, which means
that they cannot occur together, and others by the
Requires relation, which means that if a given FE
is present, then also the other must be overtly or
implicitly present. An example of Excludes is the
relationship between the FE Entity 1 / Entity 2 and
Entities, because if Entity 1 and Entity 2 are both
present in a sentence, then Entities cannot be co-
present. Conversely, Entity 1 and Entity 2 stand in a
Requires relationship, because the first cannot occur
without the second. This kind of information can
clearly be helpful in case we have to automatically
decide whether an argument is implicit or is just not
present because it is not required.
INI/DNI preference Ruppenhofer and Michaelis
(2009) suggest that omissible arguments in particu-
lar frames tend to be always interpreted as definite or
indefinite. For example, they report that in a sample
from the British National Corpus, the interpretation
for a null instantiated Goal argument is definite in
97.5% of the observed cases. We take this feature
into account by considering the frequency of an im-
plicit argument being annotated as definite/indefinite
in the training set.
The algorithm incorporating all this linguistic in-
formation is detailed in the following subsection.
4.1 INI/DNI identification
In a preliminary step, we collect for each frame the
list of arguments being annotated as DNI/INI with
the corresponding frequency in the training set. For
example, in the CALENDRIC UNIT frame, the Whole
argument has been annotated 11 times as INI and
5 times as DNI. Some implicit frame elements oc-
cur only as INI or DNI, for example Goal, which is
annotated 14 times as DNI and never as INI in the
ARRIVING frame. This frequency list (FreqList)
is collected in order to decide if candidate null in-
stantiations have to be classified as DNI or INI.
We consider each sentence in the test data pro-
vided with FrameNet annotation, and for each pred-
icate p annotated with a set of overt frame elements
FEs, we run the first module for DNI/INI identi-
fication. The steps followed are reported in Algo-
rithm 1. We first check if the annotated FEs con-
tain all core frame elements C listed in FrameNet for
p. If the two sets are identical, we conclude that no
core frame element can be implicit and we return an
empty set both for DNI and INI . For example, in
the test sentence (6), the BODY MOVEMENT frame
appears in the sentence with its two core frame el-
ements, i.e. Body part and Agent. Therefore, no
implicit argument can be postulated.
(6) Finally [she]Agent openedBODY MOVEMENT [her
eyes]Body part again.
If the core FEs in C are not all overtly expressed
in FEs, we run two routines to check if the miss-
ing FEs CandNIs are likely to be null instantiated
elements. First, we discard all candidate NIs that ap-
pear as incorporated FEs for the given p. Second, we
discard as well candidate NIs if they are excluded by
the overtly annotated FEs.
The last steps of the algorithm are devoted to de-
ciding if the candidate null instantiation is definite
or indefinite. For this step, we rely on the observa-
tions collected in FreqList. In particular, for each
58
candidate c we check if it was already present as INI
or DNI in the training set. If yes, we label c accord-
ingly. In case c was observed both as INI and as
DNI, the most probable label is assigned based on
its frequency in the training set.
Input: TestSet with annotated core FEs;
FreqList
Output: INI and DNI for p
foreach p ? TestSet do
extract annotated core FEs;
extract set C of core FEs for p in FrameNet;
if C ? FEs then
DNI = ?;
INI = ?;
else
C \ FEs = CandNIs;
foreach c ? CandNIs do
if c is incorporated FE of p then
delete c
foreach fe ? FEs do
if fe excludes c then
delete c
end
foreach nip ? FreqListp do
if c = nip then
if nip is only dnip then
c ? DNI
if nip is only inip then
c ? INI
if nip is inip and nip is dnip
then
if Freq(inip) >
Freq(dnip) then
c ? INI
else
c ? DNI
end
end
return(INI);
return(DNI);
end
Algorithm 1: DNI/INI identification
4.2 DNI binding
Given that both the supervised approach exploited
by Chen et al (2010) and the methodology pro-
posed in Tonelli and Delmonte (2010) based on
deep-semantic parsing achieved quite poor results
in the DNI-binding task, we devise a third approach
that relies on the observed heads of each FE in the
training set and assigns a relevance score to each
candidate antecedent.
We first collect for each FE the list of heads
Htrain assigned to FE in the training set, and we ex-
tract for each head htrain ? Htrain the correspond-
ing frequency fhtrain . Then, for each dni ? DNI
identified with Algorithm 1 in the test set, we collect
all nominal heads Htest occurring in a window of
(plus/minus) 5 sentences and we assign to each can-
didate head htest ? Htest a relevance score relhtest
w.r.t. dni computed as follows:
relhtest =
fhtrain
dist(sentdni, senthtest)
(7)
where fhtrain is the number of times h has been
observed in the training set with a FE label, and
dist(sentdni, senthtest) is the distance between the
sentence where the dni has been detected and the
sentence where the candidate head htest occurs (0 ?
dist(sentdni, senthtest) ? 5).
The best candidate head for dni is the one with
the highest relhtest , given that it is (higher) than 0.
The way we compute the relevance score is based on
the intuition that, if a head was frequently observed
for FE in the training set, it is likely that it is a good
candidate. However, the more distant it occurs from
dni, then less probable it is as antecedent.
5 Evaluation and error analysis
We present here an evaluation of the system output
on test data. We further comment on some difficult
aspects of the task and suggest some solutions.
5.1 Results
Evaluation consists of different layers, which we
consider separately. The first task was to decide
whether an argument is implicit or not. We were
able to identify 53.8% of all null instantiated ar-
guments in text, which is lower than the recall of
63.4% achieved by SEMAFOR (Chen et al, 2010),
the best performing system in the challenge. How-
ever, in the following subtask of deciding whether an
implicit argument is an INI or a DNI, we achieved
an accuracy of 74.6% (vs. 54.7% of SEMAFOR,
59
even if our result is based on fewer proposed clas-
sifications). Note that the majority-class accuracy
reported by Ruppenhofer et al (2010) is 50.8%.
In Table 2 we further report precision, recall and
F1 scores computed separately on all DNIs and all
INIs automatically detected. Precision corresponds
to the percentage of null instantiations found (either
INI or DNI) that are correctly labeled as such, while
recall indicates the amount of INI or DNI that were
correctly identified compared to the gold standard
ones. Our approach does not show significant dif-
ferences between the result obtained with INIs and
DNIs, while the evaluation of SEMAFOR (between
parenthesis) shows that its performance suffers from
low recall as regards DNIs and low precision as re-
gards INIs.
P R F1
DNI 0.39 (0.57) 0.43 (0.03) 0.41 (0.06)
INI 0.46 (0.20) 0.38 (0.61) 0.42 (0.30)
Table 2: Evaluation of INI/DNI identification.
SEMAFOR performance between parenthesis.
Another evaluation step concerns the binding of
DNIs with the corresponding antecedents by apply-
ing the equation reported in Section 4.2. Results are
shown in Table 3:
P R F1
DNI 0.13 (0.25) 0.06 (0.01) 0.08 (0.02)
Table 3: Evaluation of DNI resolution. SEMAFOR per-
formance between parenthesis.
Although the binding quality still needs to be im-
proved, two main factors have a negative impact on
our performance, which do not depend on our al-
gorithm: first, 9% of the DNIs we bound to an an-
tecedent don?t have a referent in the gold standard.
Second, 26% of the wrong assignments concern an-
tecedents found for the Topic frame element in test
sentences where the STATEMENT frame has been
annotated together with the overtly expressed core
FE Message. In all these gold cases, Topic is not
considered null instantiated if the Message FE is ex-
plicit in the clause. Therefore, we can conclude that
the mistake done by our algorithm depends on the
missing Excludes relation between Topic and Mes-
sage, i.e. a rule should be introduced saying that
one of the two roles is redundant (and not null in-
stantiated) if the other is overtly expressed.
5.2 Open issues related to our approach
Even if with a small set of rules our approach
achieved state-of-the-art results in the SemEval task,
our performance clearly requires further improve-
ments. Indeed, we currently rely only on the back-
ground knowledge about core FEs from FrameNet,
combined with statistical observations about role
fillers acquired from the training set. Additional
morphological, syntactic, semantic and discourse in-
formation could be exploited in different ways. For
example, since the passive voice of a verb can con-
structionally license INIs, this kind of information
would greatly improve our performance with verbal
predicates (i.e. 46% of all annotated predicates in
the test set).
As for nominal predicates, consider for example
sentence (8) extracted from the test set:
(8) ?Excuse the admirationJUDGMENT [of a
connoisseur]Evaluee,? said [he]Cognizer.
In this case, ?admiration? is a nominal predicate
with two explicit FEs, namely Evaluee and Cog-
nizer. The JUDGMENT frame includes also the Rea-
son core FE, which can be a candidate for a null in-
stantiation. In fact, it is annotated as INI in the gold
standard data, because in the previous sentences a
reason for such admiration is not mentioned. How-
ever, this could have been annotated as DNI as well,
if only some specific quality of the person had been
previously introduced. This shows that the current
sentence does not present any inherent characteris-
tic motivating the presence of a definite instantia-
tion. In this case, a strategy based on some kind of
history list may be very helpful. This could con-
tain, for example, all subjects and direct objects pre-
viously mentioned in text and selected according to
some relevance criteria, as in (Tonelli and Delmonte,
2010). A further improvement may derive from the
integration of an anaphora resolution step, as first
proposed by Palmer et al (1986) and more recently
by Gerber and Chai (2010).
60
5.3 Open issues related to the task
Other open issues are related to the specification of
the task and to the nature of implicit entities, which
make it difficult to account for this phenomenon
from a computational point of view. We report be-
low the main issues that need to be tackled:
INI Linking: Table 1 shows that 28% of DNIs
in the test set are not linked to any referent. This
puts into question one of the main assumptions of
the task, that is the connection between a definite
instantiation and a referent. In the test set, there are
also 14 cases of indefinite null instantiations (out of
361) that are provided with a referent. Consider for
example the following sentence with gold standard
annotation, in which the INI label Path is actually
instantiated and refers to ?we?:
(9) (We)Path allowed [him]Theme to passTRAVERSING
before we had recovered our nerve.
This again may be a controversial annotation choice,
since the annotation guidelines of the task reported
that ?in cases of indefinite omission, there need not
be any overt mention of an indefinite NP in the lin-
guistic context nor does there have to be a referent
of the kind denoted by the omitted argument in the
physical discourse setting? (Ruppenhofer, 2010).
Position of referent: Although we suggested that
the History List may represent a good starting point
for finding antecedents to DNIs, searching only in
the context preceding the current predicate is not
enough because the referent can occur after such
predicate. Also, the predicate with a DNI and the
referent can be divided by a very large text span. In
the test data, 38% of the DNIs referent occur in the
same sentence of the predicate, while 14% are men-
tioned after that (in a text span of max. 4 sentences).
Another 38% of DNIs are resolved in a text span
preceding the current predicate of max. 5 sentences,
while the rest has a very far antecedent (up to 116
sentences before the current predicate). The notion
of context where the antecedent should be searched
for is clearly lacking an appropriate definition.
Diversity of lexical fillers: In general, it is pos-
sible to successfully obtain information about the
likely fillers of a missing FE from annotated data
sets only in case all FE labels are semantically well
identifiable: in fact many FE labels are devoid of
any specific associated meaning. Furthermore, lex-
ical fillers of a given semantic role in the FrameNet
data sets can be as diverse as possible. For exam-
ple, a complete search in the FrameNet database for
the FE Charges will reveal heads like ?possession,
innocent, actions?, where the significant portion of
text addressed by the FE would be in the specifica-
tion - i.e. ?possession of a gun? etc. Only in case of
highly specialized FEs there will be some help in the
semantic characterization of a possible antecedent.
6 Conclusions
In this paper, we have described the phenomenon
of null instantiated arguments according to the
FrameNet paradigm and we have proposed a strat-
egy for identifying implicit arguments and find-
ing their antecedents, if any, using a linguistically-
motivated approach. We have evaluated our system
using the test set developed for the SemEval task
10 and we have discussed some problems in our ap-
proach affecting its performance. Besides, we have
also pointed out some issues related to the task defi-
nition and to the general phenomenon of null instan-
tiated arguments that make the identification task
challenging from a computational point of view. We
have shed some light on the syntactic, semantic and
discourse information that we believe are necessary
to successfully handle the task.
In the future, we plan to improve on our binding
approach by making our model more flexible. More
specifically, we currently treat DNI referents occur-
ring before and after the sentence containing the
predicate as equally probable. Instead, we should
penalize less those preceding the predicate because
they are more frequent in the training set. For this
reason, the number of observations for the candi-
date head and the distance should be represented
as different weighted features. Another direction to
explore is to extend the training set to the whole
FrameNet resource and not just to the SemEval
data set. However, our approach based on the ob-
servations of lexical fillers is very much domain-
dependent, and a larger training set may introduce
too much variability in the heads. An approach ex-
ploiting some kind of generalization, for example by
linking the fillers to WordNet synsets as proposed by
(Gerber and Chai, 2010), may be more appropriate.
61
References
Collin F. Baker, Charles J. Fillmore, and J. B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of Coling/ACL, Montreal, Quebec, Canada.
C. F. Baker, M. Ellsworth, and K. Erk. 2007. Semeval-
2007 task 10: Frame semantic structure extraction. In
Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 99?104,
Prague, CZ, June.
Aljoscha Burchardt, Annette Frank, and Manfred Pinkal.
2005. Building text meaning representations from
contextually related frames - a case study. In Proceed-
ings of the Sixth International Workshop on Computa-
tional Semantics, Tilburg, NL.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. SEMAFOR: Frame Argument
Resolution with Log-Linear Models. In Proceedings
of SemEval-2010: 5th International Workshop on Se-
mantic Evaluations, pages 264?267, Uppsala, Swe-
den. Association for Computational Linguistics.
Rodolfo Delmonte. 2009. Understanding Implicit Enti-
ties and Events with Getaruns. In Proceedings of the
IEEE International Conference on Semantic Comput-
ing, pages 25?32, Berkeley, California.
Charles J. Fillmore. 1986. Pragmatically Controlled
Zero Anaphora. In V. Nikiforidou, M. Vanllay,
M. Niepokuj, and D. Felder, editors, Proceedings of
the XII Annual Meeting of the Berkeley Linguistics So-
ciety, Berkeley, California. BLS.
Matthew Gerber and Joyce Y. Chai. 2010. Beyond Nom-
Bank: A Study of Implicit Arguments for Nominal
Predicates. In Proceedings of the 48th annual meet-
ing of the Association for Computational Linguistics
(ACL-10), pages 1583?1592, Uppsala, Sweden. Asso-
ciation for Computational Linguistics.
Adam Meyers. 2007. Annotation guidelines for Nom-
Bank - noun argument structure for PropBank. Tech-
nical report, New York University.
M. Palmer, D. Dahl, R. Passonneau, L. Hirschman,
M. Linebarger, and J. Dowding. 1986. Recovering im-
plicit information. In Proceedings of ACL 1986, pages
96?113.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. Computational Linguistics, 31.
Josef Ruppenhofer and Laura A. Michaelis. 2009.
Frames predict the interpretation of lexical omissions.
Submitted.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin F. Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking Events and Their Participants
in Discourse. In Proceedings of SemEval-2010: 5th
International Workshop on Semantic Evaluations.
Josef Ruppenhofer, 2010. Annotation guidelines used for
Semeval task 10 - Linking Events and Their Partici-
pants in Discourse. (manuscript).
Sara Tonelli and Rodolfo Delmonte. 2010. VENSES++:
Adapting a deep semantic processing system to the
identification of null instantiations. In Proceedings
of SemEval-2010: 5th International Workshop on Se-
mantic Evaluations, Uppsala, Sweden.
62
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 1?10,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Coping With Implicit Arguments And Events Coreference Rodolfo Delmonte Department of Language Studies & Department of Computer Science Ca? Foscari University - 30123, Venezia, Italy delmont@unive.it  Abstract In this paper we present ongoing work for the creation of a linguistically-based system for event coreference. We assume that this task requires deep understanding of text and that statistically-based methods, both supervised and unsupervised are inadequate. The reason for this choice is due to the fact that event coreference can only take place whenever argumenthood is properly computed. It is a fact that in many cases, arguments of predicates are implicit and thus linguistically unexpressed. This prevents training to produce sensible results. We also assume that spatiotemporal locations need to be taken into account and this is also very often left implicit. We used GETARUNS system to develop the coreference system which works on the basis of the discourse model and the automatically annotated markables. We present data from the analysis, both on unexpressed implicit arguments and the description of the coreference algorithm. 1 Introduction NLP processing is more and more oriented towards semantic processing which in turn requires deep understanding of texts. We assume that this is only possible if unexpressed implicit linguistic elements and semantically deficient items are taken into consideration (Delmonte 2009a, 2009b). One of the first problem in the analysis of any text is accounting for implicit or linguistically unexpressed information. This kind of information is not available in dependecy-based current annotated corpora or is only partially available ? as in Penn Treebank ? but it cannot possibly be learnt. The problem of null and pronominal elements is paramount in the recovery of Predicate-Argument Structures which constitutes the fundamental element onto which propositional semantics is made to work. However, applying machine 
learning techniques on available treebanks is of no help. State of the art systems are using more and more dependency representations which have lately shown great resiliency, robustness, scalability and great adaptability for semantic enrichment and processing. However, by far the majority of systems available off the shelf don?t support a fully semantically consistent representation and lack Null Elements or Antecedents for pronominal ones. If we limit ourselves to Null Elements, and to PennTreebank (hence PT), we may note that Marcus (?94) referred explicitly to Predicate-Argument Structures (hence PASs) and to the need to address this level of annotation. He mentions explicitly that ?we intend to automatically extract a bank of PASs intended at the very least for parser evaluation from the resulting annotated corpus? and further on ?the notation should make it easy to automatically recover PAS? (ibid. 121). He also mentions the need to allow for a clear and concise distinction between verb ARGUMENTs and ADJUNCTs, which he asserts to be very difficult to make, consistently. This happens to be true: the final version of PT II does not include coindexing in controversial cases and has coindexing for null SBJ only in a percentage of the cases. PT contains 36862 cases of null elements (including traces, expletives, gapping and ambiguity) as listed in Johansson(2007), over 93532 simple clauses and 55600 utterances, for a percentage of 66.3%. Of course this number does not include pronominal arguments which need to be bound ? and are not bound in PT - to an antecedent in order to become semantically consistent. As to PT, the difficulty of the task is testified by the presence of non coindexed Null Elements: in particular we see that they are 8416, that is 22.83%. If we exclude all traces of WH and topicalization and limit ourselves to the category OTHER TRACES which includes all unexpressed SUBJects of infinitivals and gerundives, we come up with 12172 cases of Null non-coindexed 
1
elements, 33% of all cases. We should note that for how much large this number may seem, this still represents a small percentage when compared to the number of null elements in languages like Chinese or Romance languages like Italian, which allow for free null subjects insertion in tensed clauses.  Current statistically dependency parsers have made improvements in enriching their structural output representation (Gabbard et al 2006; Sagae and Tsujii, 2008;	 ?Choi & Palmer, 2010;	 ?Cai et al 2011). However, coindexation is not always performed: when it is, its performance is computed separately because it is lower than accuracy for labeled/unlabeled tasks. In particular, Schmid reports 84% F-score for empty elements prediction and 77% for coindexation on PT. However, other parsers have much worse results, with Johnson(2001) being the worst, with 68% F-score. The presence of additional difficulties to predict empty categories is the cause of a bad drop in performance in Chinese - no more than 50% accuracy reported by Cai et al (2011) compared to 74/77% of the labeled/unlabeled task. Results reported by Yang & Xue (2010) on recovering labeled empty elements in an experiment carried on a small subset of the Penn Chinese Treebank 6.0 reach an average of 60.5% of F-measure. As to recovery of specific items, we note that over a total number of 290 little_pro items recall fares around 50%. Of course the phenomenon is very much language dependent, as discussed above. If we consider a language like Italian ? which we described fully from structures annotated in the treebank called VIT (Delmonte 2004) ? we can see that in addition to untensed sentences also simple clauses with tensed verbs show the same problem. In fact, over 66.5% (9634 over 15874) of all simple clauses are subjectless, they have an omitted or unexpressed subject which is marked in linguistics with a little_pro and the agreement coming from morphology of the main verb. Of the remaining lexically expressed subjects, only 64% (6166 over 9634) are in canonical position, that is in preverbal position and adjacent to the inflected verb. The remaining 36% of lexically expresses subjects are positioned to the right or are separated from the verb by other constituents. 2 Events and Null Elements 
We will now try to describe events in terms of the contribution of Null Elements. Events are mainly characterized by their meaning which is defined in a gloss or by one or more semantic categories, or even by a synset of synonym concepts. In addition to that, events may be regarded as being composed of two other elements: - the participants to the event, which are arguments and adjuncts or circumstantials - the spatiotemporal location of the event Both components may be linguistically expressed or be left implicit and thus should be inferred from previous discourse. In fact the spatiotemporal location of the event is usually indicated explicitly only if needed and is mostly left unexpressed. Participants on the contrary are mostly explicitly expressed before they can be left implicit. However, in some case, participants are linguistically unexpressed for structural reasons or else expressed by a pronoun. Both cases require a deep system or a deep parser together with a pronominal binding algorithm to be in place, in order to find the appropriate antecedent and bind the empty arguments. There are exceptions to these rules and they are constituted by utterances of generic or arbitrary reference, something intended in utterances such as, (1) Doing regular physical exercise is strongly recommended at a certain age. where no participant is explicitly indicated, but it is clearly understood by inferences determined by knowledge of the world. Events may be coreferred or may be queried: in both cases, we are also dealing with semantic relations at discourse structure level. The need to corefer to a previous event derives from conversational or argumentative strategies. Generally speaking, it is due to the need of expanding concepts and facts reported in the previous mention. At a discourse level, this is usually called ELABORATION or EXPLANATION. Other possible cases of event coreference at discourse structure level can be due to the need of enriching the previous description of additional facts cooccuring with the previously mentioned event: in this case we may have an hypernymic or an hyponymic relation intervening 
2
between the two facts or concepts. Let?s look at some examples taken from the demo text made available by the organizers. After the title, we have a first event description, reported by a newspaper, which is a violent event followed by an adjunct clause describing the effects or caused consequences: we capitalize event naming words and then indicate the semantic discourse relation:  ?A Kurdish newspaper said Wednesday that Iraqi members of an Al Qaeda-linked group, a Kurd and an Arab, BLEW themselves up in northern Iraq on February 1, KILLING at least 105 people.?  CAUSE ?? BLOW, KILL The idea in this case is that the two events are linked by a semantic relation rather than simply the first event being coreferred by the second. The text continues by expanding the event introducing some comment that elaborates on the previous sentence:  ?The twin suicide bombing WAS the deadliest attack in post-war Iraq and WAS SUSPECTED TO HAVE BEEN CARRIED OUT by foreign fighters, possibly linked to Osama bin Laden's Al-Qaeda network.? ELABORATION ?? BOMBING, ATTACK CAUSE ?? BOMBING, BLOW EXPLANATION ?? SUSPECT(CARRY_OUT), BLOW Discourse relations are triggered by event coreference which in this sentence is achieved by two nominalizations: in fact only definite expressions are taken into consideration, in particular if singular in number. The first one is TWIN SUICIDE BOMBING which we understand to be a new enriched mention of BLOW at first by a causal relation intervening between BOMB and BLOW. This semantic relation is not available from WordNet but from Sumo-Milo, where the verbs BOMB, BLAST, ATTACK, KILL, and FIGHT all share one semantic class, VIOLENT_CONTEST and/or DESTRUCTION with BLOW. The causal relation is derived from commonsense knowledge available in "ConceptNet" by the AI Laboratory of MIT. 
Searching for relations intervening between BOMB and BLOW_UP, this is what you can find - represented in an appropriate Prolog-like format: cpn(udf,bomb,[blow,something,up]). cpn(udf,bomb,[blow,things,up]). cpn(udf,bomb,[blow,up,buildings]). cpn(udf,bomb,[blow,up,stuff]). cpn(udf,bomb,blow). cpn(do,person,[don_t,want,be,blow,up,by,bomb]). cpn(dof,person,[not,be,blow,up,by,bomb]).  They are also all classified as NEGATIVE polarity items and are part of the same Lexical Field in Roget's Thesaurus. Then the additional contribution of its arguments, where ?blowing themselves up? implies a SUICIDE took place. At the same time, the use of ?twin? is coreferring with ?members? a plural noun, better specified as being composed of two individuals ?a Kurd and an Arab? in an apposition to it. Thus, the nominalization does not add any new information that could not be understood from previous mention, but certainly clarifies previous information thus respecting Grice?s maxims.  The copulative structure headed by WAS, is used to assign a property to the coreferred event thus contributing new information. We now know that the newspaper reports the event as being ?the deadliest attack in post-war Iraq?. We also learn that the two fighters identity was suspected to be not Iraqi but possibly ?foreign?, deemed to belong to bin Laden?s network. All of this new information can be labeled as ?Explanation?. The news story continues by elaborating on the two fighters by expanding on their identity, and then explaining the way in which the bombing was organized in the following two sentences.  ?The pair were named respectively as Abu Bakr Hawleri and Kazem Al-Juburi, alias Abu Turab, by independent newspaper Hawlani, which said they belonged to the Army of Ansar al-Sunna. The Kurd blew himself up in the offices of the Patriotic Union of Kurdistan (PUK) and the Arab in the offices of the Kurdistan Democratic Party (KDP), both in the Kurdish city of Arbil, said the newspaper. 
3
Each one carried a belt packed with four kilograms (8.8 pounds) of TNT mixed with phosphorus, a highly flammable material, the newspaper said.? The use of a definite singular expression is highly indicative of the coreference mechanism being activated. This applies to THE PAIR, coreferring with "Iraqi members" and also with "twin". The same can be said of "The Kurd" coreferring with the previous mention and also the use of the same predicate BLOW UP. In the following stretch of discourse, the story corefers to the ?Army of Ansar al-Sunna?, to explain the role that the organization had in the bombing: ?Ansar al-Sunna last week claimed the twin bombings in a statement posted on an Islamist website. The newspaper said the motive of the attack was to "punish" the two Kurdish secular groups, which control Iraqi Kurdistan, for their alliance with the US-led coalition. The newspaper said Ansar al-Sunna broke away from the Ansar al-Islam group last October and was led by an Arab whose alias is Abu Abdullah Hasan bin Mahmud. Ansar al-Sunna is more extreme, said the newspaper.? The first coreference link is expressed by the sentence ?Ansar claimed the TWIN BOMBINGs? which is used to expand on the role of the organization of the original event. Additional events are the STATEMENT, a nominal event, and the MOTIVE OF THE ATTACK which introduces the MOTIVATION for the event. This causal link is connected to actual causal event: PUNISHing the Kurdish group controlling Iraqi Kurdistan. In turn, the action of PUNISHing is explained by another eventive nominalization, the ALLIANCE of the group (the possessive THEIR corefers with it), with the US-led coalition. Additional explanation is reported in the final sentence (longer than 40 tokens!!) where the relation intervening between the motive the attack as contained in the statement and previously occurring facts is further clarified:  ?The newspaper added that bin Mahmud is the brother of man whose alias is Abdullah Al-Shami, an Ansar al-Islam leader who was killed last year while fighting a US-backed onslaught by the PUK 
that forced the group out of its enclave near the Iranian border at the end of March last year.? The sentence contains additional coreferring nominalizations like ONSLAUGHT, which reminds of the bombing and of the previous attack. The overall events description is rich in temporal and spatial locations which contribute to the understanding and the overall discourse structure. In particular we start out by a spatial location, NORTHERN IRAQ, and a temporal location, FEBRUARY 1st. Both locations remain the same in the following sentences until we reach a change in topics and locations. This happens when ?Ansar al-Sunna? is introduced as SUBJect of CLAIM, an event location in time, LAST WEEK.  Additional information the Ansar al-Islam group takes us back to LAST OCTOBER. Eventually, in the final sentence, we have been told that the current bombing event may have relation with the killing of another Ansar al-Islam leader, during an ONSLAUGHT that took place LAST YEAR, in a different location, NEAR THE IRANIAN BORDER. The generic location LAST_YEAR is further specified as being END OF MARCH.  3. GETARUNS : a system for text understanding  GETARUNS1, the system for text understanding developed at the University of Venice, is organized as a pipeline which includes two versions of the system: what we call the Partial and the Deep GETARUNS and they work in a backoff policy. There are in fact three parsers interconnected and they are activated in order to prevent failure to take place. The system has a middle module for semantic interpretation and discourse model construction which is cast into Situation Semantics; and a higher module where reasoning and generation takes place.  The system is based on LFG theoretical 	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?	 ?1 The system has been tested in STEP competition (see Delmonte 2008), and can be downloaded in two separate places. The partial system called VENSES in its stand-alone version is available at http://www.aclweb.org/aclwiki/ index.php?title=Textual_Entailment_Resource_Pool. The complete deep system is available both at http://www.sigsem.org/wiki/ STEP_2008_shared_task:_comparing_semantic_representations, and at, http://project.cgm.unive.it/html/sharedtask/. 
4
framework and has a highly interconnected modular structure.  The output of grammatical modules is fed then onto the Binding Module which activates an algorithm for anaphoric binding. Antecedents for pronouns are ranked according to grammatical function, semantic role, inherent features and their position at f-structure. Eventually, this information is added into the original f-structure graph and then passed on to the Discourse Module (hence DM). GETARUNS, has a linguistically based semantic module which is used to build up the DM. Semantic processing is strongly modularized and distributed amongst a number of different submodules which take care of Spatio-Temporal Reasoning, Discourse Level Anaphora Resolution, and other subsidiary processes like Topic Hierarchy which cooperate to find the most probable antecedent of coreferring and cospecifying referential expressions when creating semantic individuals. These are then asserted in the DM, which is then the sole knowledge representation used to solve nominal coreference. The system uses two resolution submodules which work in sequence: the first one is fired whenever a free sentence external pronoun is spotted; the second one takes the results of the first submodule and checks for nominal anaphora. They have access to all data structures contemporarily and pass the resolved pair, anaphor-antecedent to the following modules. Semantic Mapping is performed in two steps: at first a Logical Form is produced which is a structural mapping from DAGs onto unscoped well-formed formulas. These are then turned into situational semantics informational units, infons which may become facts or sits. Each unit has a relation, a list of arguments which in our case receive their semantic roles from lower processing ? a polarity, a temporal and a spatial location index. All entities and their properties are asserted in the DM with the relations in which they are involved; in turn the relations may have modifiers - sentence level adjuncts, and entities may also have modifiers and attributes. Each entity has a polarity and a couple of spatiotemporal indices which are linked to main temporal and spatial locations if any exists; else they are linked to presumed time reference derived from tense and aspect computation. On second occurrence of the same nominal head the semantic index is recovered from 
the  history list and the system checks whether it is the same referring expression and has non-conflicting attributes or properties. In all other cases a new entity is asserted in the DM which however is also computed as being included in (a superset of) or by (a subset of) the previous entity. 
4. A System For Event Marking And Event Coreference I will now go through the text above indicating places where the system has been able to locate and identify missing arguments. In order to clarify the working of the system I will use the output of the discourse model, which contains fully coreferred empty or linguistically unexpressed elements which have gone through pronominal binding process as well as coreference analysis.  The first unexpressed element is the subject of the adjunct gerundive headed by KILL in the first sentence:  (1) A Kurdish newspaper said Wednesday that Iraqi members of an Al Qaeda-linked group, a Kurd and an Arab, blew themselves up in northern Iraq on February 1, killing at least 105 people.  The second unexpressed argument is contained in sentence 2, in the infinitival governed by SUSPECT and headed by CARRY_OUT, which is contained in the coordinate structure headed by SUSPECT,  (2) The twin suicide bombing was the deadliest attack in post-war Iraq and was suspected to have been carried out by foreign fighters, possibly linked to Osama bin Laden's Al-Qaeda network.  where the suicide_bombing is predicated as the "deadliest attack" in the previous main sentence. The main spatial location now becomes Iraq. Another unexpressed argument is the subject of POSTED, a participial modifying STATEMENT,  (3) Ansar al-Sunna last week claimed the twin bombings in a statement posted on an Islamist website.  where we still want to know who posted the ?statement?, and the information is passed by the 
5
main clause as the subject of CLAIM, i.e. Ansar al-Sunna.  Then we have another infinitival lacking subject argument information, in the following sentence,  (4) The newspaper said the motive of the attack was to "punish" the two Kurdish secular groups, which control Iraqi Kurdistan, for their alliance with the US-led coalition.  This is a copulative structure where the subject MOTIVE is predicated by the infinitival headed by PUNISH. In fact, this verb is lacking a referential subject simply because the predication prevents it from having a specific one. The same GROUP is coreferred in the final sentence that contains the most important sequence of unexpressed but yet essential arguments:  (5) The newspaper added that bin Mahmud is the brother of man whose alias is Abdullah Al-Shami, an Ansar al-Islam leader who was killed last year while fighting a US-backed onslaught by the PUK that forced the group out of its enclave near the Iranian border at the end of March last year.  The gerundive headed by FIGHT has LEADER as SUBJect and as OBJect the GROUP we found in the previous sentence. It is important to notice that this mention of GROUP is NOT coreferent with the one appearing at the beginning of the text. This non coreference is clearly apparent from attributes accompanying the head: the first one is expressed as "an Al Qaeda-linked group", whereas the second as "the two Kurdish secular groups". In the final computation, the system produces a set of entity pools, that is a set of all referents to a given semantic index - be they properties, entities or relations. In particular, the referent to the LEADER coincides by virtue of a predication, with Abdullah Al-Shami and has the property of being associated to Ansar al-Islam: from the pool, we now know that he was KILLED, being associated to the THEME_AFFected role.  4.1 The Experiment and an Evaluation We tested the coreference module with the sample text and produced the following output that we comment in this section. For each event we have two vectors of information that we then use to 
evaluate its relevance and its possible coreference in the previous text. The categories used are fully explained in Delmonte (2007; 2009) and here we limit ourselves to a short description. The event may be a verb and be related to a propositional analysis or be a noun. Nouns classified as activity or events are selected as markables: this classification is partially derived from NomBank associated information about eventive nominals. Coreference links are activated by synonymity or just similarity, measured by WordNet synset, a Thesaurus or sharing identical semantic classes as indicated in SUMO-MILO or other similar computational lexica. The certainty value varies accordingly: from more certain, say .9, to less certain .4. Obviously, copulative predications are marked with certainty equal to 1 being properties predicated in the syntax of the subject.  4.1.1 Coreference links We present here briefly the addition to the system GETARUNS that have been produced for this task. The annotation of each text is shown in an xml file which has been obtained in the following steps: a. the system GETARUNS produces a deep analysis of each text on a sentence by sentence basis. At the end of the analysis of each sentence, markables are collected and all semantic information is attached to each word of the sentence. We collected all verbs and also eventive nominals and possible eventive modifiers. This is done in two steps.  b. at the end of parsing each word of the sentence is associated to its lemma and general semantic categories are also collected from the analysis.  c. The system produces then the steps required for the Discourse Model which is where entities, relations and properties are asserted with their attributes. Semantic indices are assigned to each new entity and previous mentions receive previously assigned indices. At this point the contents of the discourse model are associated to each word of the sentence.  d. At the end of the analysis of the text the system collects all markables, which are internally made of four elements: an markable index, a word, a lemma, a semantic index (from the discourse model) or a generic indicator of eventuality for all verbs.  
6
e. Then the complete discourse model is searched to produce a list of all entities, relations and properties with their spatiotemporal relations and polarity, as documented in situation semantics. Additional information is derived in this phase from WordNet, FrameNet or SumoMilo ontology and is made available to the coreference algorithm. Another component that is activated at the end of the analysis is sentiment analysis that computes an affective label associated to each markable - if possible - and classifies each markable into three different classes: positive, negative and neutral. d. The coreference algorithm works as follows: for each markable it check all possible coreference links, at first on the basis only of inherent semantic features, which are: wordform and lemma identity; then semantic similarity measured on the basis of a number of similarity criteria which are lexically based (no statistical measure is used). We search WordNet synsets and assign a score according to whether the markables are directly contained in the same synset or not. A different score is assigned if their relation can be inferred from the hierarchy. Other computational lexical resources we use are those documented in our work on Text Entailment Recognition (Delmonte et al 2005; 2006; 2007; 2008), and include FrameNet and Frames hierarchy; SumoMilo and its semantic classification.  f. After collecting all possible coreferential relations between semantically validated markables, we then proceed to filter out those links that are inconsistent or incompatible according to three criteria: - first criterion: diverse sentiment polarity - second criterion: different argument structure  - third criterion: non related spatiotemporal relations Both argument structure and spatiotemporal relations are collected in the discourse structure which also contains dependence relations expressed by discourse relations in discourse structures; temporal logical relations as computed from an adaptation of Allen's algorithm; and a point of view computed on the basis of presence of ?reportive? verbs, or direct speech, reported speech, reported indirect speech.  Another criterion we adopt is the nature of semantic similarity computed by the system. Values below a certain threshold indicate the coreference has been chosen on the basis of weak 
similarity, as may apply to semantic lexical fields. These are based on thesauri classification. Some examples below. As said above, event coreference links require sentiment match, argument identity or semantic similarity. In particular consider such cases as   <MARK  ID=m34> claimed  </MARK>.  is semantically computed as a communication verb on a par with SAY, but coreference is prevented by the fact that arguments don't coincide. SAY in all its various forms is used to report what the newspaper Hawlani said. Here CLAIM is related to different arguments as shown in the discourse structure entry, ds(to(7-17),7-18,claim([id86:[ansar,sunna,al],id4:suspect,id87:statement],1,id71),during(tes(sn19evs7),tes(sn31evs6)), narration,'ansar_al-sunna') The same applies to the use of KILL in the last sentence (11) whose argument structure prevents a coreference link with the previous occurrence of an identical verb form in sentence (2). Here below are the two discourse structures containing argument structures for the verb KILL in the two sentences:  ds(down(11-28),11-29,kill([id140:[[abdullah,shami,al],leader],id145:exist],1,id71), after(tes(f562evs11),tes(f772evs11)), narration,narrator), ds(to(2-3),2-4,kill([id16:member,id18:people],1,univ),after(tes(f4_evs_2),tes(f2_evs_1)),result,narrator), Discourse Structures also contain temporal logical relations, Discourse relation and Point of View. If we consider all computed markables, which are in our system 67, we come up with 47 possible coreference links. However only 17 have been regarded admissible and consistent and are listed here below.   1.coref-ident m1 m7 hypothetical_certainty 1 2.coref-ident m3 m17 hypothetical_certainty 1 3.coref-simil m2 m20 hypothetical_certainty 0.9 4.coref-simil m4 m14 hypothetical_certainty 0.9 5.coref-ident m7 m25 hypothetical_certainty 1 6.coref-simil m10 m21 hypothetical_certainty 0.9 7.coref-ident m6 m28 hypothetical_certainty 1 
7
8.coref-ident m7 m29 hypothetical_certainty 1 9.coref-ident m7 m33 hypothetical_certainty 1 10.coref-simil m1 m36 hypothetical_certainty 0.9 11.coref-simil m11 m35 hypothetical_certainty 0.9 12.coref-simil m15 m37 hypothetical_certainty 0.9 13.coref-ident m6 m43 hypothetical_certainty 1 14.coref-ident m7 m38 hypothetical_certainty 1 15.coref-ident m14 m40 hypothetical_certainty 1 16.coref-simil m32 m47 hypothetical_certainty 0.9 17.coref-ident m7 m48 hypothetical_certainty 1  Markables M1, M7, M25, M33, M38, M48 all refer to verb SAY and have as SUBJect the newspaper; in one case M1 is wrongly coreferred to M36, STATEMENT. M3 is attached to the noun SUSPECT and is made to corefer to M17, the verb SUSPECT which share arguments with the noun. M2 is ?Al_Qaeda-linked? and is coreferred to M20, ?linked?. M4 is BLASTS and is coreferred to M14, ATTACK. M10 is TWIN and is coreferred to M21, PAIR. M6 is ?Kurdish? coreferred to M28 again Kurdish, but also M43. M11 is ?suicide_bombing? which corefers to M35, ?bombings?. M15 is ?post-war? and is wrongly coreferred to M37 ?posted?. M14 ATTACK is coreferred to M40 again ATTACK. M32 MIXED is wrongly coreferred to M47 COALITION. There are three errors over 17. Omitted links include the following one coref- (m4- (blasts-blast-id5))- (m8- (blew-blow_up-id26))-5 where coreference between BLAST and BLOW_UP is established and the score assigned is 0.5. This score is regarded too low and is filtered out, even though a causal link was clearly inferrable.  5. Conclusion and Future Work  We show here below in Table 1. total counts for the 13 texts distributed with the Event Coreference Task. The system computed automatically Controllers and Antecedents: the first are referred to syntactically controlled Null Elements of Relative and Interrogative Clauses. The second are referred to SUBJects of infinitivals, and other  predicative structures both argumental and non-argumental. The table also includes counts of Markables and Coreferent Links, again computed automatically. There is no evaluation yet available. What we wanted to show is the proportion of NEs 
with respect to sentences, which is 1.6 per sentence, that is there are three NEs every two sentences.   LI/Rounds Round1 Round2 Round3 Total Markables 334 372 325 1031 Corefs 72 79 37 188 Controllers 69 57 55 181 Antecedents 60 57 53 170 Sentences 69 78 72 219 Total Null  Elementss  129  114  108  351 Table 1. Null Elements, Markables and Coreferents automatically computed by Getaruns on the 13 texts of the Task  In this paper we presented ongoing work to produce a system for event coreference that uses a linguistically-based approach and the output of a deep system for the representation of a text in a situation semantics framework. The output of the system on the sample text has been fairly consistent in particular for what concerns the computation of implicit information which we regard paramount for a successful performance in the task at hand.  Semantic relations have been built taking into due account all attributes and modifiers of the semantic head. This process has allowed preventing coreference to take place on the basis of simple concept matching procedures. Some inferential processes have been fired using commonsense knowledge stored in the publicly available resource, ConceptNet.  Besides, the computation of temporal relations based on a revised version of Allen's algorithm has allowed to control inclusion relations intervening between event structures. The output of the system includes a discourse structure which shows coordination and subordination links between discourse stretches defined by propositional level analysis. Structural inclusion is allowed again only in presence of same TOPIC and same spatiotemporal relation checking. Both NEW topic and NEW spatiotemporal relation will cause the structure to jump up to any possible previous node that may be used to provide a cohesion link in the text. This notion of coreference has not been explored yet and will be the topic of further study.    
8
REFERENCES Alshawi, H., Pi-Chuan Chang, M. Ringgaard. (2011). Deterministic Statistical Mapping of Sentences to Underspecified Semantics, in Johan Bos and Stephen Pulman (editors), Proceedings of the 9th International Conference on Computational Semantics, IWCS,15-24. Bender, E.M. and D.Flickinger (2005). Rapid prototyping of scalable grammars: Towards modularity in extensions to a language-independent core. in Proc. 2nd IJCNLP-05, Jeju Island, Korea. Bender, E.M., D.Flickinger, and S.Oepen (2002). The Grammar Matrix: An open-source starter-kit for the rapid development of cross-linguistically consistent broad-coverage precision grammars. In J.Carroll et al(Eds.), Proc. Workshop Grammar Engineering and Evaluation at COLING19, Taipei, Taiwan, 8-14. Bresnan, J., 2000. Lexical-Functional Syntax, Blackwell. Cai, Shu, David Chiang, Yoav Goldberg, 2011. Language-Independent Parsing with Empty Elements, in Proceedings of the 49th Annual Meeting of the ACL, 212?216.  Choi, Jinho D., Martha Palmer, 2010. Robust Constituent-to-Dependency Conversion for English, in Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories (TLT'9), 55-66, Tartu, Estonia.  Clark P., C. Fellbaum, J. Hobbs, P. Harrison, W.R.Murray, J. Thompson, 2008. Augmenting WordNet for Deep Understanding of Text, in J.Bos & R.Delmonte(eds.), 2008. ACL-SigSem, STEP (Semantics in Text Processing), College Publications, London, p.45-58. Copestake, Ann. 2004/2006. Robust Minimal Recursion Semantics, Unpublished draft (downloadable from http://www.cl.cam.ac.uk/~aac10/papers.html). Copestake, Ann, (2009). Invited Talk: Slacker Semantics: Why Superficiality, Dependency and Avoidance of Commitment can be the Right Way to Go. In: Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 1-9. Athens, Greece, 2009. Copestake, A., D.Flickinger, C.Pollard, and I.Sag (2005). Minimal recursion semantics: An introduction. Research on Language and Computations 3(4), 281-332. CoreLex:- http://www.cs.brandeis.edu/~paulb/CoreLex/ corelex.html EuroWordNet:- http://www.illc.uva.nl/EuroWordNet/ Delmonte R.(1990), Semantic Parsing with an LFG-based Lexicon and Conceptual Representations, Computers & the Humanities, 5-6, pp.461-488. Delmonte R., D.Bianchi(1991), Binding Pronominals with an LFG Parser, Proceeding of the Second International Workshop on Parsing Technologies, 
Cancun(Messico), ACL 1991, pp.59-72. Delmonte R.(1995), Lexical Representations: Syntax-Semantics interface and World Knowledge, in Rivista dell'AI*IA (Associazione Italiana di Intelligenza Artificiale), Roma, pp.11-16. Delmonte R.(1996),  Lexical Representations, Event Structure and Quantification, Quaderni Patavini di Linguistica 15, 39-93. Bianchi D., Delmonte R. (1996),  Temporal Logic in Sentence and Discourse, in Atti SIMAI'96, pp.226-228. Dario Bianchi, Rodolfo Delmonte(1999), Reasoning with A Discourse Model and Conceptual Representations, Proc. VEXTAL, Unipress, pp. 401-411. Delmonte R.(2002),  From Deep to Shallow Anaphora Resolution:, in Proc. DAARC2002 , 4th Discourse Anaphora and Anaphora Resolution Colloquium, Lisbona, pp.57-62. Delmonte R.(2002), GETARUN PARSER - A parser equipped with Quantifier Raising and Anaphoric Binding based on LFG, Proc. LFG2002 Conference, Athens, pp.130-153, at http://cslipublications. stanford.edu/hand/miscpubsonline.html.  Delmonte R., Sara Tonelli, Marco Aldo Piccolino Boniforti, Antonella Bristot, Emanuele Pianta (2005), VENSES ? a Linguistically-Based System for Semantic Evaluation, in Joaquin Qui?onero-Candela, Ido Dagan, Bernardo Magnini, Florence d?Alch?-Buc, 2005, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment.: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, 344-371. Delmonte, R., Antonella Bristot, Marco Aldo Piccolino Boniforti and Sara Tonelli, 2006. Another Evaluation of Anaphora Resolution Algorithms and a Comparison with GETARUNS' Knowledge Rich Approach, ROMAND 2006, 11th EACL, Trento, Association for Computational Linguistics, 3-10. Delmonte, R., A. Bristot, M.A.Piccolino Boniforti and S. Tonelli, 2006. Coping with semantic uncertainty with VENSES, in Bernardo Magnini, Ido Dagan(eds.), Proceedings of the Challenges Workshop - The 2nd PASCAL Recognizing Textual Entailment Challenge, 86-91, Universit? Ca' Foscari, Venezia. Delmonte R., (2007), Computational Linguistic Text Processing ? Logical Form, Semantic Interpretation, Discourse Relations and Question Answering, Nova Science Publishers, New York. Delmonte R., A. Bristot, M.A.Piccolino Boniforti, S.Tonelli (2007), Entailment and Anaphora Resolution in RTE3, in Proc. ACL Workshop on Text Entailment and Paraphrasing, Prague, ACL 
9
Madison, USA, pp. 48-53. Bos Johan & Rodolfo Delmonte (eds.), 2008. Semantics in Text Processing (STEP), Research in Computational Semantics, Vol.1, College Publications, London. Delmonte R., 2009. Computational Linguistic Text Processing ? Lexicon, Grammar, Parsing and Anaphora Resolution, Nova Science Publishers, New York. Delmonte R., G. Nicolae, S. Harabagiu, C.Nicolae (2007), A Linguistically-based Approach to Discourse Relations Recognition, in B.Sharp & M.Zock(eds.), Natural Language Processing and Cognitive Science, Proc. 4th NLPCS, Funchal, Portugal, INSTICC PRESS, pp. 81-91. Delmonte R., G. Nicolae, S. Harabagiu (2007), A Linguistically-based Approach to Detect Causality Relations in Unrestricted Text, in Proc. MICAI-2007, IEEE Publications, 173-185. Delmonte R., 2008. Semantic and Pragmatic Computing with GETARUNS, in Bos & Delmonte (eds.), Semantics in Text Processing (STEP), Research in Computational Semantics, Vol.1, College Publications, London, pp. 287-298. Delmonte R., E. Pianta, (2009), Computing Implicit Entities and Events for Story Understanding, in H.Bunt, V.Petukhova and S.Wubben(eds.), Proc. Eighth International Conference on Computational Semantics IWCS-8, Tilburg University Press, pp. 277-281.  Delmonte R., (2009), A computational approach to implicit entities and events in text and discourse, in International Journal of Speech Technology (IJST), Springer, pp. 1-14. Gabbard, Ryan, Mitchell Marcus, Seth Kulick, 2006. Fully Parsing the Penn Treebank, in Proceedings of the HLT Conference of the North American Chapter of the ACL, 184?191. Grice, H. P., 1975. Logic and Conversation. in P. Cole & J. L. Morgan, Syntax and Semantics, Vol. 3: Speech Acts. New York : Academic Press, 41-58. Harabagiu, S.M., Miller, G.A., Moldovan, D.I.: eXtended WordNet - A Morphologically and 
Semantically Enhanced Resource (2003), http://xwn.hlt.utdallas.edu, 1-8. Hobbs, J. (2005). Toward a useful notion of causality for lexical semantics. Journal of Semantics 22, 181?209. Hobbs, J. (2008). Encoding commonsense knowledge. Technical report, USC/ISI. http://www.isi.edu/?hobbs/csk.html. Johansson, R. and P. Nugues. 2007. Extended Constituent-to-dependency Conversion for English. In Proceedings of NODALIDA 2007, Tartu, Estonia. Johnson. M., 2001. Joint and conditional estimation of tagging and parsing models. In ACL 2001, pages 322?329. Liu, H., Singh, P. (2004). ConceptNet: A Practical Commonsense Reasoning Toolkit. Marcus, M., G. Kim, M. Ann Marcinkiewicz, R. Macintyre, A. Bies, M. Ferguson, K. Katz, B. Schasberger, (1994). The Penn Treebank: Annotating Predicate Argument Structure, In ARPA Human Language Technology Workshop, 114-119. Sagae, K. and Tsujii, J. 2008. Shift-Reduce Dependency DAG Parsing. Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). Manchester, UK. Yang, Yaqin and Nianwen Xue. 2010. Chasing the ghost: recovering empty categories in the Chinese Treebank. In Proc. COLING.Schubert, L. and C. Hwang (1993). Episodic logic: A situational logic for NLP. In Situation Theory and Its Applications, pp. 303?337. Schubert, L. and C.Hwang (1993). Episodic logic: A situational logic for NLP. In Peter Aczel, David Israel, Yasuhiro Katagiri, and Stanley Peters, (Eds.), Situation Theory and its Applications, vol.3, 303-337. Tonelli, S. & R. Delmonte, 2011. "Desperately seeking Implicit arguments in text", in RELMS'2011, Workshop on Relational Models of Semantics at ACL 2011 Portland, USA. pp.54-62. At:http://web.media.mit.edu/~push/ConceptNet.pdf. 
 
10
