Learning Chinese Bracketing Knowledge Based on  
a Bilingual Language Model 
Yajuan L?, Sheng Li, Tiejun Zhao, Muyun Yang  
School of Computer Science & Engineering, Harbin Institute of Technology 
Harbin, China, 150001 
Email: {lyj,lish,tjzhao,ymy}@mtlab.hit.edu.cn 
 
Abstract  
This paper proposes a new method for 
automatic acquisition of Chinese bracketing 
knowledge from English-Chinese sentence- 
aligned bilingual corpora. Bilingual sentence 
pairs are first aligned in syntactic structure by 
combining English parse trees with a 
statistical bilingual language model. Chinese 
bracketing knowledge is then extracted 
automatically. The preliminary experiments 
show automatically learned knowledge 
accords well with manually annotated 
brackets. The proposed method is 
particularly useful to acquire bracketing 
knowledge for a less studied language that 
lacks tools and resources found in a second 
language more studied. Although this paper 
discusses experiments with Chinese and 
English, the method is also applicable to 
other language pairs. 
Introduction 
The past few years have seen a great success in 
automatic acquisition of monolingual parsing 
knowledge and grammars. The availability of 
large tagged and syntactically bracketed corpora, 
such as Penn Tree bank, makes it possible to 
extract syntactic structure and grammar rules 
automatically (Marcus 1993). Substantial 
improvements have been made to parse western 
language such as English, and many powerful 
models have been proposed (Brill 1993, Collins 
1997). However, very limited progress has been 
achieved in Chinese. 
      Knowledge acquisition is a bottleneck for 
real appication of Chinese parsing. While some 
methods have been proposed to learn syntactic 
knowledge from annotated Chinese corpus, most 
of the methods depended on the annotated or 
partial annotated data(Zhou 1997, Streiter 2000). 
Due to the limited availbility of Chinese 
annotated corpus, tests of these methods are still 
small in scale. Although some institutions and 
universities currently are engaged in building 
Chinese tree bank, no large scale annotated 
corpus has been published until now because the 
complexity in Chinese syntatic sturcture and the 
difficulty in corpus annotation (Chen 1996).  
This paper proposes a novel method to 
facilitate the Chinese tree bank construction. 
Based on English-Chinese bilingual corpora and 
better English parsing, this method obtains 
Chinese bracketing information automatically via 
a bilingual model and word alignment results. 
The main idea of the method is that we may 
acquire knowledge for a language lacking a rich 
collection of resources and tools from a second 
language that is full of them.  
The rest of this paper is organized as 
follows : In the next section, a bilingual language 
model is introduced. Then, a bilingual parsing 
method supervised by English parsing is 
proposed in section 2. Based on the bilingual 
parsing, Chinese bracketing knowlege is 
extracted in section 3. The evaluation and 
discussion are given in section 4. We conclude 
with discussion of future work. 
1 A bilingual language model ? ITG 
Wu (1997) has proposed a bilingual language 
model called Inversion Transduction Grammar 
(ITG), which can be used to parse bilingual 
sentence pairs simultaneously. We will give a 
brief description here. For details please refer to 
(Wu 1995, Wu 1997).  
The Inversion Transduction Grammar is a 
bilingual context-free grammar that generates 
two matched output languages (referred to as L1 
and L2). It also differs from standard context-free 
grammars in that the ITG allows right-hand side 
production in two directions: straight or inverted. 
The following examples are two ITG 
productions: 
C -> [A B] 
C -> <A B> 
Each nonterminal symbol stands for a pair of 
matched strings. For example, the nonterminal A 
stands for the string-pair (A1, A2). A1 is a 
sub-string in L1, and A2 is A1?s corresponding 
translation in L2. Similarly, (B1, B2) denotes the 
string-pair generated by B. The operator [ ] 
performs the usual concatenation, so that C -> [A 
B] yields the string-pair (C1, C2), where C1=A1B1 
and C2=A2B2. On the other hand, the operator <> 
performs the straight concatenation for language 
1 but the reversing concatenation for language 2, 
so that C -> <A B> yields C1=A1B1, but C2=B2A2. 
The inverted concatenation operator permits the 
extra flexibility needed to accommodate many 
kinds of word-order variation between source 
and target languages (Wu 1995). 
There are also lexical productions of the 
following form in ITG: 
A -> x/y 
This means that a symbol x in language L1 is 
translated by the symbol y in language L2.  x or y 
may be a null symbol e, which means there may 
be no counterpart string on other side of the 
bitext.  
ITG based parsing matches constituents for 
an input sentence-pair. For example, Figure 1 
shows an ITG parsing tree for an 
English-Chinese sentence-pair. The inverted 
production is indicated by a horizontal line in the 
parsing tree. The English text is read in the usual 
depth-first left to right order, but for the Chinese 
text, a horizontal line means the right sub-tree is 
traversed before the left. The generated parsing 
results are: 
(1) [[[Mr. Wu]BNP [[plays basketball]VP [on 
Sunday ]PP ]VP ]S . ]S  
(2) [[[ ] [ [ 	]]] 
] 
We can also represent the common structure 
of the two sentences more clearly and compactly 
with the aid of <> notation: 
(3)  [[<Mr./ Wu/>BNP < [plays/ basketball/	]VP 
[on/e Sunday/]PP >VP ]S ./
]S 
where the horizontal line from Figure 1 
corresponds to the <> level of bracketing. 
. 
S 
BNP 
VP 
PP 
VP 
Mr./ Wu/ 
plays/ basketball/

 
on/e Sunday/	 
S 
./ 
Figure 1  Inversion transduction Grammar parsing
      Any ITG can be converted to a normal form, 
where all productions are either lexical 
productions or binary-fanout nonterminal 
productions(Wu 1997). If probability is 
associated with each production, the ITG is 
called the Stochastic Inversion Transduction 
Grammar (SITG). 
2 English parsing supervised bilingual 
bracketing 
Because of the difficulty in finding a suitable 
bilingual syntactic grammar for Chinese and 
English, a practical ITG is the generic Bracketing 
Inversion Transduction Grammar (BTG)(Wu 
1995). BTG is a simplified ITG that has only one 
nonterminal and does not use any syntactic 
grammar. A Statistical BTG (SBTG) grammar is 
as follows: 
j
b
i
b
ji
baa
veAeuA
vuAAAAAAA
ejie
ij
/    ;/ 
   ; /    ;    ];[ 
??????
???><??????
       SBTG employs only one nonterminal 
symbol A that can be used recursively. Here, ?a? 
denotes the probability of syntactic rules. 
However, since those constituent categories are 
not differentiated in BTG, it has no practical 
effect here and can be set to an arbitrary constant. 
The remaining productions are all lexical. bij is 
the translation probability that source word ui 
translates into target word vj. bij can be obtained 
using a statistical word-translation model 
(Melamed 2000) or word alignment(L? 2001a). 
The last two productions denote that the word in 
one language has no counterpart on other side of 
the bitext. A small constant can be chosen for the 
probabilities bie and bej.   
In BTG, no language specific syntactic 
grammar is used. The maximum-likelihood 
parser selects the parse tree that best satisfies the 
combined lexical translation preferences, as 
expressed by the bij probabilities. Because the 
expressiveness characteristics of ITG naturally 
constrain the space of possible matching in a 
highly appropriate fashion, BTG achieves 
encouraging results for bilingual bracketing 
using a word-translation lexicon alone (Wu 
1997). 
Since no syntactic knowledge is used in 
SBTG, output grammaticality can not be 
guaranteed. In particular, if the corresponding 
constituents appear in the same order in both 
languages, both straight and inverted, then lexical 
matching does not provide the discriminative 
leverage needed to identify the sub-constituent 
boundaries. For example, consider an 
English-Chinese sentence pair: 
(4) English: That old teacher is our adviser. 
Chinese: 	
 
Using SBTG, the bilingual bracketing result is : 
(5) [[[[[[The/ old/] teacher/] is/] our/	] 
adviser/
] ./] 
The result is not consistent with the 
expected syntactic structure. In this case, 
grammatical information about one or both of the 
languages can be very helpful. For example, if we 
know the English parsing result shown in (6), 
then the bilingual bracketing can be determined 
easily; the result should be (7).  
(6) [[That old teacher]BNP [is [our adviser]BNP ]VP .]S 
(7) [[That/ old/ teacher/] [is/ [our/	 
adviser/
] ] ./] 
From the example, we can see that if one 
language parser is available, the induced 
bilingual bracketing result would be more 
accurate. English parsing methods have been 
well studied and many powerful models have 
been proposed. It will be helpful to make use of 
English parsing results. In the following, we will 
propose a method of bilingual bracketing 
supervised by English parsing.  
Here, English parsing supervised BTG 
means using an English parser?s bracketing 
information as a boundary restriction in the BTG 
language model. But this does not necessitate 
parsing Chinese completely according to the 
same parsing boundary of English. If the English 
parsing structure is totally fixed, it is possible that 
the structure is not linguistically valid for 
Chinese under the formalism of Inversion 
Transduction Grammar. To illustrate this, see the 
example shown in Figure 2.  
If you want to lose weight, you had better eat less bread . 
    	
 
 

 
  
 
eat 
less bread 
VP 
BNP 

   

     

 
 (a) 
VP 
eat/ less/ 
bread/

 
X 
 (b) 
Figure 2  A example of mismatch subtree 
VP 
eat less bread 
 (c) 
 
 
 
        The sub-tree for blacked underlined part of 
English and corresponding Chinese are shown in 
Figure 2(a). We can see that the Chinese 
constituents do not match the English 
counterparts in the English structure. In this case, 
our solution is that: the whole English constituent 
of ?VP? is aligned with the whole Chinese 
correspondence; i.e., ?eat less bread? is matched 
with ?? shown in Figure 2(b). At the 
same time, we give the inner structure matching 
according to ITG regardless of the English 
parsing constraint. An ?X? tag is introduced to 
indicate that the sub-bilingual-parsing-tree is not 
consistent with the given English sub-tree. Our 
result can also be understood as a flattened 
bilingual parsing tree as shown in Figure 2(c). 
This means that when the bilingual constituents 
couldn?t match in the small syntactic structure, 
we will match them in a larger structure. 
        The main idea is that the given English 
parser is only used as a boundary constraint for 
bilingual parsing. When the constraint is 
incompatible with the bilingual model ITG, we 
use ITG as the default result. This process 
enables parsing to go on regardless of some 
failures in matching. 
We heuristically define a constraint function 
Fe(s, t) to denote the English boundary constraint, 
where s is the beginning position and t is the end. 
There are three cases of structure matching: 
violate match, exact match and inside match. 
Violate match means the bilingual parsing 
conflicts with the given English bracketing 
boundary. For example, given the following 
English bracketing result (8), (1,2), (1,3), (2,3), 
(2,4) etc. are Violate matches. We assign a 
minimum Fe(s, t) (0.0001 at present) to prevent 
the structure match from being chosen when an 
alternative match is available. Exact match 
means the match falls exactly on the English 
parsing boundary, and we assign a high Fe(s, t) 
value (10 at present) to emphasize it. (1,6), (2,5), 
(3,5) are examples. (3,4), (4,5) are examples of 
inside match, and the value 1 is assigned to these 
Fe(s, t) functions. 
(8) [She/1 [is/2 [a/3 lovely/4 girl/5] ] ./6]    
Let the input English and Chinese sentences 
be Tee ,...1  and Vcc ,...1 . As an abbreviation we 
write tse ...  for the sequence of words 
tss eee ..., ,21 ++ , and similarly write vuc ... . The local 
optimization function =),,,( vuts?  
]/[max
.... vuts ceP denotes the maximum probability 
of sub-parsing-tree of node q and that both the 
sub-string tse ...  and vuc ...  derive from node q. 
Thus, the best parser has the 
probability ),0,,0( VT? . ),,,( vuts? is calculated as 
the maximum probability combination of all 
possible sub-tree combinations(Wu 1995). To 
insert English parsing constraints in bilingual 
parsing, we integrate the constraint function Fe(s, 
t) into the local optimization function.   
Computation of the local optimization function is 
then modified as given below:  
.),,,(),,,(),(max),,,(
,),,,(),,,(),(max),,,(
,)],,,(),,,,(max[),,,(
0))(())((
0))(())((
[]
[]
UutSvUSstsFvuts
vUtSUuSstsFvuts
vutsvutsvuts
e
UvuUStsS
vUu
tSs
e
UvuUStsS
vUu
tSs
???
???
???
???+??
??
??
<>
???+??
??
??
<>
=
=
=
 
    Initialization is as follows : 
V1,1),/(
V1,1),/(
V1,1),/(
,1,,
,,,1
,1,,1
????=
????=
????=
?
?
??
vTtceb
vTteeb
vTtceb
vvvtt
tvvtt
vtvvtt
?
?
?
     
where, T ,V is the length of English and Chinese 
sentence respectively. )/( vt ceb is the probability 
of translating English word te  into Chinese word 
vc . A minimal probability can be assigned to 
empty word alignment b( eet / ) and b( vce / ). 
The optimal bilingual parsing tree for a 
given sentence-pair can be computed using  
dynamic programming (DP) algorithm(Wu 1997). 
Using the standard SBTG local optimization 
fuction, the obtained bilingual parsing result for 
the given sentence-pair(4) is shown as example 
(5); when using the above modified local 
optimization function, the parsing result is that 
shown as example (7). Comparing the two results, 
we can see that by intergrating English parsing 
constraints into BTG, the bilingual parsing 
becomes more grammatical. Our experiments 
showed that this English parsing supervised BTG 
would improve the accuracy of bilingual 
bracketing by nearly 20% (L? 2001b). 
The obtained bilingual parsing tree is in the 
normal form of ITG, that is each node in the tree 
is either a lexical node or a binary-fanout 
nonterminal node. We can combine the subtree to 
restore the fanout flexibility using the production 
characters [[AA]A]=[A[AA]]=[AAA] and 
<<AA>A>= <A<AA>>=<AAA>. The combining 
operation could not cross the given English 
parisng boundary.  
3 Chinese bracketing knowledge extraction 
Table 1 shows some bilingual bracketing 
examples obtained using the above method. To 
understand easily, we give the tree form of the 
first example in Figure 3(a). The leaf node is the 
aligned words of the two languages and their 
POS tag categories. These POS tags are 
generated from an English and a Chinese POS 
tagger respectively. The English POS tag and 
phrase tag set are the same as those of the Penn 
Tree Bank (Marcus 1993) and the Chinse POS 
tag set please refer to the web site: 
http://mtlab.hit.edu.cn. The nonterminal node are 
labeled using English sub-tree tags. 
Based on the bilingual parsing result, it is 
easy to extract the Chinese bracketing structure 
according to the Inversion Transduction 
Grammar. For the normal node, the Chinese text 
is traversed in depth-first left to right order, but 
for an inverted node (indicated by a horizontal 
line in the parsing tree or indicated by a <> 
notation in bracketing expression), the right 
sub-tree is traversed before the left. Thus, the 
Chinese parsing tree corresponding to Figure 3(a) 
is shown in Figure 3(b). The nonterminal labels 
are derived from the English sub-tree. The 
extracted Chinese bracketing results from Table1  
Table 1  Bilingual bracketing examples 
1. [<Mr.(NNP)/(nc) Chen(NNP)/(nx) >BNP [is (VBZ) /(vx) < [the(ART)/e representative(NN)/(ng)]BNP 
<of (IN) /(usde) [our (PRP$)/	(r) company(NN)/
(ng)]BNP >PP >NP ]VP .(.)/(wj) ]S 
2. [Spring(NN)/(t) [is(VBZ)/(vx) <[the(ART)/e first(JJ)/(m) e/(q) season(NN)/(ng) ]BNP <in(IN)/
(f) [a(ART)/(m) year(NN)/(q) ]BNP >PP >X ]VP .(.)/(wj) ]S 
3. [[The(ART)/e window(NN)/(ng)]BNP [is/e/VBZ <[e/(d) narrower(JJR)/An Automatic Evaluation Method for Localization Oriented 
Lexicalised EBMT System 
 
Jianmin Yao+, Ming Zhou++, Tiejun Zhao+, Hao Yu+, Sheng Li+ 
  +School of Computer Science and Technology
Harbin Institute of Technology,  
Harbin, China, 150001 
{james, tjzhao, yu, shengli}@mtlab.hit.edu.cn
++Natural Language Computing Group 
Microsoft Research Asia 
Beijing, China, 100080 
Mingzhou@microsoft.com 
 
Abstract  
To help developing a localization oriented 
EBMT system, an automatic machine 
translation evaluation method is 
implemented which adopts edit distance, 
cosine correlation and Dice coefficient as 
criteria. Experiment shows that the 
evaluation method distinguishes well 
between ?good? translations and ?bad? ones. 
To prove that the method is consistent with 
human evaluation, 6 MT systems are scored 
and compared. Theoretical analysis is made 
to validate the experimental results. 
Correlation coefficient and significance tests 
at 0.01 level are made to ensure the 
reliability of the results. Linear regression 
equations are calculated to map the 
automatic scoring results to human scorings. 
Introduction 
Machine translation evaluation has always been 
a key and open problem. Various evaluation 
methods exist to answer either of the two 
questions (Bohan 2000): (1) How can you tell if 
a machine translation system is ?good?? And (2) 
How can you tell which of two machine 
translation systems is ?better?? Since manual 
evaluation is time consuming and inconsistent, 
automatic methods are broadly studied and 
implemented using different heuristics. Jones 
(2000) utilises linguistic information such as 
balance of parse trees, N-grams, semantic 
co-occurrence and so on as indicators of 
translation quality. Brew C (1994) compares 
human rankings and automatic measures to 
decide the translation quality, whose criteria 
involve word frequency, POS tagging 
distribution and other text features. Another type 
of evaluation method involves comparison of the 
translation result with human translations. 
Yokoyama (2001) proposed a two-way MT 
based evaluation method, which compares 
output Japanese sentences with the original 
Japanese sentence for the word identification, 
the correctness of the modification, the syntactic 
dependency and the parataxis. Yasuda (2001) 
evaluates the translation output by measuring the 
similarity between the translation output and 
translation answer candidates from a parallel 
corpus. Akiba (2001) uses multiple edit 
distances to automatically rank machine 
translation output by translation examples. 
Another path of machine translation evaluation 
is based on test suites. Yu (1993) designs a test 
suite consisting of sentences with various test 
points. Guessoum (2001) proposes a 
semi-automatic evaluation method of the 
grammatical coverage machine translation 
systems via a database of unfolded grammatical 
structures. Koh (2001) describes their test suite 
constructed on the basis of fine-grained 
classification of linguistic phenomena. 
There are many other valuable reports on 
automatic evaluation. All the evaluation 
methods show the wisdom of authors in their 
utilisation of available tools and resources for 
automatic evaluation tasks. For our 
localization-oriented lexicalised EBMT system 
an automatic evaluation module is implemented. 
Some string similarity criteria are taken as 
heuristics. Experimental results show that this 
method is useful in quality feedback in 
development of the EBMT system. Six machine 
translation systems are utilised to test the 
consistency between the automatic method and 
human evaluation. To avoid stochastic errors, 
significance test and linear correlation are 
calculated. Compared with previous works, ours 
is special in the following ways: 1) It is 
developed for localisation-oriented EBMT, 
which demands higher translation quality. 2) 
Statistical measures are introduced to verify the 
significance of the experiments. Linear 
regression provides a bridge over human and 
automatic scoring for systems. 
The paper is organised as follows: First the 
localization-oriented lexicalised EBMT system 
is introduced as the background of evaluation 
task. Second the automatic evaluation method is 
further described. Both theoretical and 
implementation of the evaluation method are 
fully discussed. Then six systems are evaluated 
both manually and with our automatic method. 
Consistency between the two methods is 
analysed. At last before the conclusion, linear 
correlation and significance test validate the 
result and exclude the possibility of random 
consistency. 
1 EBMT Evaluation Solution 
1.1 EBMT System Setup 
From Figure 1 you can get a general overview of 
our EBMT system. 
 
Input sentence 
Transfer 
<Phrase Alignment>
Translation result 
Resources 
(Bilingual and 
monolingual) 
Example Base 
(Software 
manual) Match 
Recombine 
Figure 1. Flowchart of the EBMT System 
The EBMT system is developed for 
localization purpose, which demands the 
translation to be restricted in style and 
expression. This makes it rational to take string 
similarity as criterion for translation quality 
evaluation. The solution is useful because in 
localization, an example based machine 
translation system helps only if it outputs the 
very high quality translation results. 
1.2 Evaluation Criteria 
The criteria we utilise for evaluation include edit 
distance, dice coefficient and cosine correlation 
between (the vectors or word bag sets of) the 
machine translation and the gold standard 
translation. Followed is a detailed description of 
the three criteria. 
The edit distance between two strings s1 
and s2, is defined as the minimum number of 
operations to become the same 
(Levenshtein1965). It gives an indication of how 
`close' or ?similar? two strings are. Denote the 
length of a sentence s as |s|. A two-dimensional 
matrix, m[0...|s1|,0...|s2|] is used to hold the edit 
distance values. The algorithm is as follows 
(Wagner 1974): 
Step 1 Initialization: 
For i=0 to |s1| 
m[i, 0] = i//initializing the columns  
For j=1 to |s2| 
m[0, j] = j //initializing the rows 
Step 2 Iteration: 
For i=1 to |s1| 
For j=1 to |s2| 
 if(s1[i] = s2[j]) 
 { 
    d=m[i-1,j-1] 
 }//equality 
 else 
 { 
d=m[i-1,j-1]+1 
 }//substring 
 m[i, j]=min(m[i-1,j]+1,m[i,j-1]+1,d) 
End For 
End For 
Step 3: Result: 
Return m[i,j] 
Figure 2. Algorithm for Edit Distance 
The time complexity of this algorithm is 
O(|s1|*|s2|). If s1 and s2 have a `similar' length, 
about `n' say, this complexity is O(n2). 
Taking into account the lengths of 
translations, the edit distance is normalised as 
21
)2,1(d2
tDistancenormal_edi  
ss
ss
+
?=
     (1) 
Cosine correlation between the vectors of 
two sentences is often used to compute the 
similarity in information retrieval between a 
document and a query (Manning 1999). In our 
task, it is a similarity criterion defined as 
follows: 
?= ?=
?
?=
?
= n
1i
n
1i
2w2i2w1i
n
1i
2i)(w1i
s2)cos(s1,
w
    (2) 
Where 
w1i = weight of ith term in vector of sentence 
s1, 
w2i = weight of ith term in vector for sentence 
s2, 
n = number of words in sum vector of s1 and s2. 
The cosine correlation reaches maximum value 
of 1 when the two strings s1 and s2 are the same, 
while if none of the elements co-occurs in both 
vectors, the cosine value will reach its minimum 
of 0. 
Another criterion we utilised is the Dice 
coefficient of element sets of strings s1 and s2, 
21
21
2)2,1(
ss
ss
ssDice +?=
I
   (3) 
The Dice coefficient demonstrates the 
intuitive that good translation tends to have 
more common words with standard than bad 
ones. This is especially true for example based 
machine translation for localization purpose. 
1.3 Relationship Among Similarity Criteria 
In this section we analyse the relationship 
between the criteria so that we have a better 
understanding of the experiment results. 
If weight of all words are 1, i.e. each word has 
the uniform importance to translation quality, 
the cosine value becomes very similar to the 
Dice coefficient criterion. if we assume 
??
?=
                                                    else        0 
rsboth vectoin  occurs ith word  theiff     1 
bi
 
??
?=
                                                    else        0 
s1 ofin vector  occurs ith word  theiff     1 
1ib
 
??
?=
                                                    else        0 
s2 ofin vector  occurs ith word  theiff     1 
2ib
 
then 
?= ?=
?
?=
?
= n
1i
n
1i
2w2i2w1i
n
1i
2i)(w1i
s2)cos(s1,
w
 
?= ?=
?
?== n
i
n
i
ibib
n
i
bi
1 1
2221
1
21
21
1 1
21
1
ss
ss
n
i
n
i
ibib
n
i
bi
?=?= ?=
?
?== I
  
Similar to (3), this is also a calculation of the 
number of words in common The Dice 
coefficient and cosine function have common 
characteristics. Especially when two strings are 
of the same length, we have 
)2,1(
21
21
2
1
21
11
21
21
21
1 1
21
1)2,1cos(
                 ssDice
ss
ss
s
ss
ss
ss
ss
ss
n
i
n
i
ibib
n
i
bi
ss
=+?==
?
=
?
=
?= ?=
?
?==
II
II
 
The above equation holds if and only if |s1| 
== |s2|. The experimental results will clearly 
demonstrate the correspondence between cosine 
correlation and Dice coefficient. The two values 
become more similar as the lengths of the two 
strings draw nearer. They become the same 
when the two sentences are of the same length. 
The (normalized) edit distance evaluation 
has a somewhat different variance from the other 
two values. Edit distance cares not only how 
many words there are in common, but also takes 
into account the factor of word order adjustment. 
For example, take two strings of s1 and s2 
composed of words, 
s1 = w1 w2 w3 w4 
s2 = w1 w3 w2 w4 
Then, 
1
44
4221
21
2)2,1( =+?=+?= ss
ss
ssDice
I
 
1
44
4
1 1
2221
1)2,1cos( =
?
=
?
=
?
=
?
?
==
n
i
n
i
ibib
n
i
bi
ss
 
5.0
44
22
21
)2,1(d2 tDistancenormal_edi
2s2)ce(s1,editDistan
=+
?=+
?=
=
ss
ss
 
Edit distance and the other two criteria have 
their respective good aspects and shortcomings. 
So they can complement each other in the 
evaluation work.  
In the EBMT development, we sort the 
translations by a combination of the three factors, 
i.e. first by Dice coefficient in descending order, 
then by cosine correlation in descending order, 
last by normalized edit distance in ascending 
order. This method makes a simple combination 
of the three factors, while no more complexity 
arises from this combination. 
2 Experiments and Results 
2.1 Experimental Setup 
Our evaluation method is designed to help in 
developing the EBMT system. It is supposed to 
sort the translations by quality. Experiments 
show that it works well sorting the sentences by 
order of it?s being good or bad translations. In 
order to justify the effectiveness of the 
evaluation method, we also design experiments 
to compare the automatic evaluation with human 
evaluation. The result shows good compatibility 
between the automatic and human evaluation 
results. Followed are details of the experimental 
setup and results. 
In order to evaluate the performance of our 
EBMT system, a sample from a bilingual corpus 
of Microsoft Software Manual is taken as the 
standard test set. Denote the source sentences in 
the test set as set S, and the target T. Sentences 
in S are fed into the EBMT system. We denote 
the output translation set as R. Every sentence ti 
in T is compared with the corresponding 
sentence ri in R. Evaluation results are got via 
the functions cosine(ti, ri), Dice(ti, ri), and 
normalized edit distance normal_editDistance(ti, 
ri). As discussed in the previous section, good 
translations tend to have higher values of cosine 
correlation, Dice coefficient and lower edit 
distance. After sorting the translations by these 
values, we will see clearly which sentences are 
translated with high quality and which are not. 
Knowledge engineers can obtain much help 
finding the weakness of the EBMT system. 
Some sample sentences and evaluation 
results are attached in the Appendix. In our 
experience, with Dice as example, the 
translations scored above 0.7 are fairly good 
translations with only some minor faults; those 
between 0.5 and 0.7 are faulty ones with some 
good points; while those scored under 0.4 are 
usually very bad translations. From these 
examples, we can see that the three criteria 
really help sorting the good translation from 
those bad ones. This greatly aids the developers 
to find out the key faults in sentence types and 
grammar points. 
2.2 Comparison with Human Evaluation 
In the above descriptions, we have presented our 
theoretical analysis and experimental results of 
our string similarity based evaluation method. 
The evaluation has gained the following 
achievements: 1) It helps distinguishing ?good? 
translations from ?bad? ones in developing the 
EBMT system; 2) The scores give us a clear 
view of the quality of the translations in 
localization based EBMT. In this section we will 
make a direct comparison between human 
evaluation and our automatic machine 
evaluation to test the effectiveness of the string 
similarity evaluation method. To tackle this 
problem, we carry out another experiment, in 
which human scoring of systems are compared 
with the machine scoring. 
The human scoring is carried out with a test 
suite of High School English. Six undergraduate 
students are asked to score the translations 
independent from each other. The average of 
their scoring is taken as human scoring result. 
The method is similar to ALPAC scoring system. 
We score the translations with a 6-point scale 
system. The best translations are scored 1. If it?s 
not so perfect, with small errors, the translation 
gets a score of 0.8. If a fatal error occurs in the 
translation but it?s still understandable, a point 
of 0.6 is scored. The worst translation gets 0 
Table 1. Human Evaluation of 6 Machine Translation Systems 
System# #1 #2 #3 #4 #5 #6 
Error5 5 5% 1 1% 2 2% 4 4% 9 9% 7 7% 
Error4 4 4% 6 6% 4 4% 7 7% 18 18% 21 21%
Error3 7 7% 14 14% 21 21% 23 23% 23 23% 26 26%
Error2 14 14% 15 14% 21 21% 19 19% 18 18% 17 17%
Error1 15 14% 17 17% 33 32% 16 16% 15 15% 8 8% 
Perfect 57 56% 49 48% 21 21% 33 32% 19 19% 23 23%
Good% 70% 65% 43% 48% 34% 31% 
Score 81 78 69 68 55 54 
point of score. Table 1 shows the manual 
evaluation results for 6 general-purpose machine 
translation systems available to us. In table 1, 
Error5 means the worst translation. Error4 to 
Error1 are better when the numbering becomes 
smaller. A translation is labelled ?Perfect? when 
it?s a translation without any fault in it. 
?Good%? is the sum of percent of ?Error1? and 
?Perfect?. Because ?Error1? translations refer to 
those have small imperfections. ?Score? is the 
weighted sum of scores of the 6 kinds of 
translations. E.g. for machine translation system 
MTS1, the score is calculated as follows: 
811578.0156.014                         
4.072.0405)1(
=?+?+?
+?+?+?=MTSscore
 
In table 2, the human scorings and automatic 
scorings of the 6 machine translation systems are 
listed. The translations of system #1 are taken as 
standard for automatic evaluations, i.e. all 
scorings are made on the basis of the result of 
system #1. In principle this will introduce some 
errors, but we suppose it not so great as to 
invalidate the automatic evaluation result. This 
is also why the scorings of system #1 are 100. 
The last row labele AutoAver is the average of 
automatic evaluations. 
Table 2. Scoring of 6 MT Systems 
System# #1 #2 #3 #4 #5 #6 
Human 100 78 69 68 55 54 
Dice 100 70 57 65 48 56 
Cosine 100 75 64 72 55 63 
Edistance 100 78 69 75 63 68 
AutoAver 100 74 63 71 55 62 
Figure 3 presents the scorings of Dice 
coefficient, cosine correlation, edit distance and 
the average of the three automatic criterions in a 
chart, we can clearly see the consistency among 
these parameters. 
4 0
5 0
6 0
7 0
8 0
9 0
1 0 0
1 2 3 4 5 6
D i c e C o s i n e
E d i t D A u t o A v e r
 
Figure 3. Automatic Scoring of 6 MT Systems 
In Figure 3, the numbers on X-axis are the 
numbering of machine translation systems, 
while the Y-axis denotes the evaluation scores. 
40
50
60
70
80
90
100
1 2 3 4 5 6
Human Automatic
 
Figure 4. Scoring of 6 MT Systems 
The human and automatic average scoring 
is shown in Figure 4. The Automatic data refers 
to the average of Dice, cosine correlation and 
edit distance scorings. On the whole, human and 
automatic evaluations tend to present similar 
scores for a specific system, e.g. 78/74 for 
system #2, while 69/63 for system #3. 
3 Result Analysis 
The experimental results and the charts have 
shown some intuitionistic relationship among 
the automatic criteria of Dice coefficient, cosine 
value, edit distance and the human evaluation 
result. A more solid analysis is made in this 
section to verify this relationship. Statistical 
analysis is a useful tool to 1) find the 
relationship between data sets and 2) decide 
whether the relationship is significant enough or 
just for random errors.  
The measure of linear correlation is a way 
of assessing the degree to which a linear 
relationship between two variables is implied by 
observed data. The correlation coefficient 
between variable X and Y is defined as 
YXss
YXCOVYXr ),(),( =
   (7) 
where 
COV(X,Y) is the covariance defined by 
? ???= ))((11),( YYXXnYXCOV ii  (8) 
The symbol meanings are as follows: 
sX: sample standard deviation of variable X 
sY: sample standard deviation of variable Y 
n: sample size 
Xi (Yi) : the ith component of variable X (Y) 
X (Y ): the sample mean of variable X (Y) 
From its definition, we know that the correlation 
coefficient is scale-independent and 11 ??? r . 
After we get the correlation coefficient r, a 
significance test at the level 01.0=?  is made 
to verify whether the correlation is real or just 
due to random errors. Linear regression is used 
to construct a model that specifies the linear 
relationship between the variables X and Y. A 
scatter diagram and regression line will be 
presented for an intuitionistic view of the 
relationship. The results are presented in the 
graphs below. In the graphs, the human 
evaluation results are placed on the X axis, while 
the automatic results are on the Y axis. 
Correlation coefficient and the linear regression 
equation are shown below the graphs. Taking 
into the sample size and the correlation 
coefficient, the significance level is also 
calculated for the statistical analysis. 
 
Figure 5. Human (X) and AutoAver (Y) 
Y=8.0+0.89X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 6. Human (X) and Dice (Y) 
Y=6.9+1.03X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 7. Human (X) and Cosine (Y) 
Y=9.3+0.88X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 8. Human (X) and Edistance (Y) 
Y=23.3+0.74X, P < 0.01 
r = 0.95, P < 0.01 
It is a property of r that it has a value 
domain of [-1,+1]. A positive r implies that the 
X and Y tend to increase/decrease together. A 
minus r implies a tendency for Y to decrease as 
X increases and vice versa. When there is no 
particular relation between X and Y, r tends to 
have a value close to zero. From the above 
analysis, we can see that the Dice coefficient, 
cosine, and average of the automatic values are 
highly correlated with the human evaluation 
results with r=0.96. P < 0.01 shows the two 
variables are strongly correlated with a 
significance level beyond the 99%. While P < 
0.01 for the linear regression equation has the 
same meaning. 
Conclusion 
Our evaluation method is designed for the 
localization oriented EBMT system. This is why 
we take string similarity criteria as basis of the 
evaluation. In our approach, we take edit 
distance, dice coefficient and cosine correlation 
between the machine translation results and the 
standard translation as evaluation criteria. A 
theoretical analysis is first made so that we can 
know clearly the goodness and shortcomings of 
the three factors. The evaluation has been used 
in our development to distinguish bad 
translations from good ones. Significance test at 
0.01 level is made to ensure the reliability of the 
results. Linear regression and correlation 
coefficient are calculated to map the automatic 
scoring results to human scorings. 
Acknowledgements 
This work was done while the author visited 
Microsoft Research Asia. Our thanks go to Wei 
Wang, Jinxia Huang, and Professor Changning 
Huang at Microsoft Research Asia and Jing 
Zhang, Wujiu Huang at Harbin Institute of 
Technology. Their help has contributed much to 
this paper. 
References  
A. Guessoum, R. Zantout, Semi-Automatic 
Evaluation of the Grammatical Coverage of 
Machine Translation Systems, MT Summit? 
conference, Santiago de Compostela, 2001 
Brew C, Thompson H.S, Automatic Evaluation of 
Computer Generated Text: A Progress Report on 
the TextEval Project, Proceedings of the Human 
Language Technology Workshop, 108-113, 1994. 
Christopher D. Manning, Hinrich Schutze, 
Foundations of Statistical Natural Language 
Processing, the MIT Press, 1999, 530-572 
Douglas A. Jones, Gregory M. Rusk, 2000, Toward a 
Scoring Function for Quality-Driven Machine 
Translation, Proceedings of COLING-2000. 
Keiji Yasuda, Fumiaki Sugaya, etc, An Automatic 
Evaluation Method of Translation Quality Using 
Translation Answer Candidates Queried from a 
Parallel Corpus, MT Summit? conference, Santiago 
de Compostela, 2001 
Language and Machines. Computers in Translation 
and Linguistics, (ALPAC report, 1966). National 
Academy of Sciences, 1966 
Niamh Bohan, Elisabeth Breidt, Martin Volk, 2000, 
Evaluating Translation Quality as Input to Product 
Development, 2nd International Conference on 
Language Resources and Evaluation, Athens, 2000. 
Shoichi Yokoyama, Hideki Kashioka, etc., An 
Automatic Evaluation Method for Machine 
Translation using Two-way MT, 8th MT Summit 
conference, Santiago de Compostela, 2001 
Sungryong Koh, Jinee Maeng, etc, A Test Suite for 
Evaluation of English-to-Korean Machine 
Translation Systems, MT Summit? conference, 
Santiago de Compostela, 2001 
Shiwen Yu, Automatic Evaluation of Quality for 
Machine Translation Systems, Machine Translation, 
8: 117-126, 1993, Kluwer Academic Publishers, 
printed in the Netherlands. 
Wagner A.R.  and Fischer M., The string-to-stirng 
correction problem, Journal of the ACM, Vol. 21, 
No. 1, 168-173 
V.I. Levenshtein, Binary codes capable of correcting 
deletions, insertions and reversals. Doklady 
Akademii Nauk SSSR 163(4) 845-848, 1965 
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita, 
Using Multiple Edit Distances to Automatically 
Rank Machine Translation Output, MT Summit? 
conference, Santiago de Compostela, 2001 
Appendix: Automatic Evaluation Results 
cosine      Dice  edistance* `  standard translation&EBMT translation 
0.27273     0.27273      44/6=7     ???????MAPI?? 
                   ????extendedmapi? 
0.43301     0.42857     28/6=4     ???????? 
                             ??mail?? 
0.53452     0.53333 30/7=4      ???????? 
                               ??role??? 
0.62994     0.625     32/4=8      ????????? 
                               ??????? 
0.7     0.7      80/16=5      ???????????????????? 
                               ???????????????????? 
0.72058     0.72      50/11=4      ???????????? 
                               ????????????? 
0.78335     0.78261 46/3=15      ???????????? 
                         ??????????? 
0.81786     0.81633 98/20=4      ?????????????????????????? 
                             ??????????????????????? 
0.8528     0.84211 76/12=6      ?????????????????????? 
                               ???????????????? 
0.86772     0.86486 37/2=18      ?????????? 
                               ????????: 
0.875      0.875  32/1=32   ???????? 
                               ???????? 
0.90889     0.90476 42/2=21      ??????????... 
                               ????????... 
 
*Notes: The data presented in ?edistance? is the reciprocal of the normalized edit distance: the numerator is |s1 + s2| in bytes ; the 
denominator is the edit distance in Chinese characters or English words. 
Subcategorization Acquisition and Evaluation for Chinese Verbs 
Xiwu Han, Tiejun Zhao, Haoliang Qi, Hao Yu 
Department of Computer Science,  
Harbin Institute of Technology, 150001 Harbin, China 
{hxw, tjzhao, qhl, yh}@mtlab.hit.edu.cn 
 
Abstract 
This paper describes the technology and an ex-
periment of subcategorization acquisition for 
Chinese verbs. The SCF hypotheses are gener-
ated by means of linguistic heuristic information 
and filtered via statistical methods. Evaluation 
on the acquisition of 20 multi-pattern verbs 
shows that our experiment achieved the similar 
precision and recall with former researches. Be-
sides, simple application of the acquired lexicon 
to a PCFG parser indicates great potentialities of 
subcategorization information in the fields of 
NLP. 
Credits 
This research is sponsored by National Natural 
Science Foundation (Grant No. 60373101 and 
603750 19), and High-Tech Research and Devel-
opment Program (Grant No. 2002AA117010-09). 
Introduction 
Since (Brent 1991) there have been a consider-
able amount of researches focusing on verb lexi-
cons with respective subcategorization informa-
tion specified both in the field of traditional lin-
guistics and that of computational linguistics. As 
for the former, subcategory theories illustrating 
the syntactic behaviors of verbal predicates are 
now much more systemically improved, e.g. 
(Korhonen 2001). And for auto-acquisition and 
relevant application, researchers have made great 
achievements not only in English, e.g. (Briscoe 
and Carroll 1997), (Korhonen 2003), but also in 
many other languages, such as Germany (Schulte 
im Walde 2002), Czech (Sarkar and Zeman 
2000), and Portuguese (Gamallo et. al 2002). 
However, relevant theoretical researches on 
Chinese verbs are generally limited to case gram-
mar, valency, some semantic computation theo-
ries, and a few papers on manual acquisition or 
prescriptive designment of syntactic patterns. 
Due to irrelevant initial motivations, syntactic 
and semantic generalizabilities of the consequent 
outputs are not in such a harmony that satisfies 
the description granularity for SCF (Han and 
Zhao 2004). The only auto-acquisition work for 
Chinese SCF made by (Han and Zhao 2004) de-
scribes the predefinition of 152 general frames 
for all verbs in Chinese, but that experiment is 
not based on real corpus. After observing and 
analyzing quantity of subcategory phenomena in 
real Chinese corpus in the People?s Daily 
(Jan.~June, 1998), we removed from Han & 
Zhao?s predefinition 15 SCFs that are actually 
similar derivants of others, and then with this 
foundation and linguistic rules from (Zhao 2002) 
as heuristic information we generated SCF hy-
potheses from the corpus of People?s Daily 
(Jan.~June, 1998), and statistically filtered the 
hypotheses into a Chinese verb SCF lexicon. As 
far as we know, this is the first attempt of Chi-
nese SCF auto-acquisition based on real corpus. 
In the rest of this paper, the second section de-
scribes a comprehensive system that builds verb 
SCF lexicons from large real corpus, the respec-
tive operating principles, and the knowledge 
coded in our SCF. The third section analyzed the 
acquired lexicon with two experiments: one 
evaluated the acquisition results of 20 verbs with 
multi syntactic patterns against manual gold 
standard; the other checked the performance of 
the lexicon when applied in a PCFG parser. The 
forth section compares and contrasts this research 
with related works done by others. And at last, 
Section 5 concludes our present achievements, 
disadvantages and possible future focuses. 
 
1   SCF Acquisition 
1.1 The Acquisition Method 
There are generally 4 steps in the process of our 
auto-acquisition experiment. First, the corpus is 
processed with a cascaded HMM parser; second, 
every possible local patterns for verbs are ab-
stracted; and then, the verb patterns are classified 
into SCF hypotheses according to the predefined 
set; at last, hypotheses are filtered statistically 
and the respective frequencies are also recorded. 
The actual application program consists of 6 
parts as shown in the following paragraphs. 
a. Segmenting and tagging: The raw cor-
pus is segmented into words and tagged 
with POS by the comprehensive seg-
menting and tagging processor devel-
oped by MTLAB of Computer 
Department in Harbin Institute of Tech-
nology. The advantage of the POS defi-
nition is that it describes some subsets of 
nouns and verbs in Chinese. 
b. Parsing: The tagged sentences are parsed 
with a cascaded HMM parser1, devel-
oped by MTLAB of HIT, but only the 
intermediate parsing results are used. 
The training set of the parser is 20,000 
sentences in the Chinese Tree Bank2 of 
(Zhao 2002). 
c. Error-driven correction: Some key errors 
occurring in the former two parts are 
corrected according to manually ob-
tained error-driven rules, which are gen-
erally about words or POS in the corpus. 
d. Pattern abstraction: Verbs with largest 
governing ranges are regarded as predi-
cates, then local patterns, previous 
phrases and respective syntactic tags are 
abstracted, and isolated parts are com-
bined, generalized or omitted according 
to basic phrase rules in (Zhao 2002). 
e. Hypothesis generation: Based on lin-
guistic restraining rules, e.g. no more 
than two NP?s occurring in a series and 
no more than three in one pattern, and 
no PP TP MP occurring with NP before 
any predicates (Han and Zhao 2004), the 
patterns are coordinated and classified 
into the predefined SCF groups. In this 
part, about 5% unclassifiable patterns 
are removed. 
                                                           
1 When evaluated on auto-tagged open corpus, the parser?s 
phrase precision if 62.3%, and phrase recall is 60.9% (Meng, 
2003). 
2 A sample of the tree bank or relevant introduction could be 
found at http://mtlab.hit.edu.cn. 
f. Hypothesis filtering: According to the 
statistical reliability of each type of the 
SCF hypotheses and the linguistic prin-
ciple that arguments occur more fre-
quently with predicates than adjuncts do, 
the hypotheses are filtered by means of 
statistical methods, in this paper which 
are binomial hypotheses testing (BHT) 
and maximum likelihood estimation 
(MLE). 
 
Table 1: An Example of Auto-acquisition 
 
No Actions Results 
a) Input ????????????
???????? 
b) Tag and 
parse 
BNP[BMP[?/m ?/q ]?/ng ]
? /p NDE[ ? ? ? /r ?
/usde ]BVP[ ? ? /vg ?
/vq ]BVP[ ? ? /vg ?
/ut ]NP[??/nc ?/usde ??
/ng ]?/wj 
c) Correct 
errors 
BNP[BMP[?/m ?/q ]?/ng ]
?/p NDE[???/r ?/usde ?
?/vg ?/vq ]BVP[??/vg ?
/LE ]NP[??/nc ?/usde ??
/ng ]?/wj 
d) Abstract 
patterns 
BNP PP BVP[vg LE ] NP 
e) Generate 
hypothesis
NP v NP ?01000? 
f) Filter hy-
potheses 
NP v NP {01111}3 
 
In Table 1, for example, when acquiring SCF 
information for ???? (prove) and a related sen-
tence in the corpus is a), our tagger and parser 
will return b), and error-driven correction will 
return c) with errors of NDE and the 1st BVP cor-
rected4. Since the governing range of ???? is 
larger than that of ???? (ask), the other verb in 
this sentence, the program abstracts its local pat-
tern BVP[vg LE] and previous phrase BNP, gen-
                                                           
3  {01000} projects to the Chinese syntactic mor-
phemes {?????????}, 1 means the SCF 
may occur with the respective morpheme, while 0 
may not (Han & Zhao, 2004). 
4 Note that not all errors in this example have been corrected, 
but this doesn?t affect further procession. Also, for defini-
tions of NDE and BVP see (Zhao, 2002). 
eralizes BNP and NDE as NP, combines the sec-
ond NP with isolated part ??/p? into PP, and 
returns d). Then the hypothesis generator returns 
e) as the possible SCF in which the verb may 
occurs. Actually in the corpus there are 621 hy-
pothesis tokens generated, and among them 92 
ones are of same arguments with e), and thus e) 
can pass the hypothesis testing (See also Section 
1.2), so we obtain one SCF for ???? as f). 
1.2 Filtering Methods 
In researches of subcategorization acquisition, 
statistical methods for hypothesis filtering mainly 
include the BHT, the Log Likelihood Ratio 
(LLR), the T-test and the MLE, and the most 
popular one is the BHT. Since (Brent 1993) be-
gan to use the method, most researchers have 
agreed that the BHT results in better precision 
and recall with SCF hypotheses of high, medium 
and low frequencies. Only (Korhonen 2001) re-
ports 11.9% total performance of the MLE better 
than the BHT. Therefore, we applied the two sta-
tistical methods in our present experiment. This 
subsection chiefly illustrates the expressions of 
our methods and definitions of parameters in 
them, while performance comparison of the two 
will be introduced in Section 3. 
When applying the BHT method, it is nec-
essary to determine the probability of the primi-
tive event. As for SCF acquisition, the co-
occurrence of one predefined SCF scfi with one 
verb v is the relevant primitive event, and the 
concerned probability is p(v|scfi) here. However, 
the aim of filtering is to rule out those unreliable 
hypotheses, so it is the probability that one primi-
tive event doesn't occur that is often used for 
SCF hypothesis testing, i.e. the error probability: 
pe(v|scfi) = 1 p(v|scfi). (Brent 1993) estimated pe 
according to the acquisition system?s perform-
ance, while (Briscoe and Carroll 1997) calculated 
pe from the distribution of SCF types in ANLT 
and SCF tokens in Susanne as shown in the fol-
lowing equation. 
 
Brent?s method mainly depends on the related 
corpus and processing program, which may 
cause intolerable errors. Briscoe and Carroll?s 
method draws on both linguistic and statistical 
information thus leading to comparatively stable 
estimation, and therefore has been used by many 
latter researches, e.g. (Korhonen 2001). But there 
is no MRD proper for Chinese SCF description 
so we estimated pe from the 1,775 common verbs 
and SCF tokens in the related corpus of 43,000 
sentences used by (Han and Zhao 2004). We 
formed the equation as follows: 
 
Then the number of all hypotheses about verb 
vj is recorded as n, and the number of those for 
scfi as m. According to Bernoulli theory, the 
probability P that an event with probability p ex-
actly happens m times out of n such trials is:  
 
And the probability that the event happens m or 
more times is: 
 
In turn, P(m+, n, pe) is the probability that scfi 
wrongly occurs m or more times with a verb that 
doesn't match it. Therefore, a threshold of 0.05 
on this probability will yield a 95% confidence 
that a high enough proportion of hypotheses for 
scfi have been observed for the verb legitimately 
to be assigned scfi (Korhonen 2001). 
The MLE method is closely related to the general 
performance of the concerned SCF acquisition 
system. First, we randomly draw from the ap-
plied corpus a training set, which is large enough 
so as to ensure similar SCF frequency distribu-
tion. Then, the frequency of scfi occurring with a 
verb vj is recorded and used to estimate the actual 
probability p(scfi| vj). Thirdly, an empirical 
threshold is determined, such that it ensures 
maximum value of F measure on the training set. 
Finally, the threshold is used to filter out those 
SCF hypotheses with low frequencies from the 
total set. 
2    Experimental Evaluation 
2.1   Acquisition Performance 
 
Using the previously described theory and tech-
nology we have acquired an SCF lexicon for 
3,558 common Chinese verbs from the corpus of 
People?s Daily (Jan.~June, 1998). In the lexicon 
the minimum number of SCF tokens for a verb is 
30, and the maximum is 20,000. In order to check 
the acquisition performance of the used system, 
we evaluated a part of the lexicon against a man-
ual gold standard. The testing set includes 20 
verbs of multi syntactic patterns, and for each 
verb there are 503~2,000 SCF tokens with the 
total number of 18,316 (See Table 2). Table 3 
gives the evaluation results for different filtering 
methods, including non-filtering 5 , BHT, and 
MLE with thresholds of 0.001, 0.005, 0.008 and 
0.01. We calculated the type precision and recall 
by the following expressions as (Korhonen 2001) 
did:  
 
In here, true positives are correct SCF types 
proposed by the system, false positives are incor-
rect SCF types proposed by system, and false 
negatives are correct SCF types not proposed by 
the system. 
 
Table 2: Verbs in the Testing Set6 
 
Verbs English Tokens Verbs English Tokens
? Read 503 ?? Hope 620 
?? Find 529 ? See 645 
?? Reckon 543 ?? Invest 679 
? Pull 544 ?? Know 722 
?? Report 612 ? Send 800 
?? Develop 1,006 ?? Set up 1,186
?? Behave 1,007 ?? Insist 1,200
?? Decide 1,038 ? Think 1,200
?? End 1,140 ?? Require 1,200
?? Begin 1142 ? Write 2,000
 
According to Table 3, all other filtering meth-
ods outperform non-filtering, and MLE is better 
than BHT. Among the four MLE thresholds, 
0.008 achieves the best comprehensive perform-
ance but its F-measure is only 0.74 larger than 
that of 0.01 while its precision drops by 2.4 per-
cent. Hence, we chose 0.01 as the threshold for 
the whole experiment with purpose to meet the 
practical requirement of high precision and to 
avoid possible over-fit phenomena. Finally, with 
a confidence of 95% we can estimate the general 
performance of the acquisition system with preci-
sion of  60.6% +/- 2.39%, and recall of 51.3%+/-
2.45%. 
                                                           
5 Non-filtering means filtering with a zero threshold or not 
filtering at all. This method is used as baseline here.  
6 The English meanings given here are not intended to cover 
the whole semantic range of the respective verbs, on the 
contrary they are just for readers? reference. 
 
Table 3: System Performance for Different 
 Filtering Methods 
 
          Measures
Methods Precision Recall F-measure
Non-filtering 37.43% 85.9% 52.14 
BHT 50% 57.2% 53.36 
0.001 39.2% 85.9% 53.83 
0.005 40.3% 83.33% 54.33 
0.008 58.2% 54.5% 56.3 MLE
0.01 60.6% 51.3% 55.56 
 
2.2    Task-oriented Evaluation 
In order to further analyze the practicability of 
the previously described technology, we per-
formed a simple task-oriented evaluation, apply-
ing the acquired SCF lexicon in a PCFG parser 
helping to choose from the n-best parsing results. 
The concerned parser was trained from 10,000 
manually parsed Chinese sentences7. In this ex-
periment there are 664 verbs and their SCF in-
formation involved. The open testing set consists 
of 1,500 sentences, for each of which the PCFG 
parser outputs 5-best parsing results. Then SCF 
hypotheses are generated for each result by 
means of the formerly mentioned technology. 
Finally, the maximum likelihood between hy-
potheses and those SCF types for the related verb 
in the lexicon is calculated in the following way:  
 
where i ? 5, hi is one of the hypotheses generated 
for the parsing results, and scfj is the jth SCF type 
for the concerned verb. This calculation keeps the 
likelihood between 0 and 1. The parsing result 
                                                           
7 These sentences and the testing corpus mentioned latter are 
all taken from the Chinese Tree Bank developed by MTLAB 
of HIT, and a sample may be downloaded at 
http://mtlab.hit.edu.cn. 
with maximum likelyhood is then regarded as the 
final choice. When two or more hypotheses hold 
the same likelihood, the one with larger or largest 
PCFG probability will be chosen.  
Table 4 shows the phrase-based and sentence-
based evaluation results for the parser without 
and with SCF heuristic information. There are 
three cased included: a) The output is one-best; b) 
The output is 5-best and the best evaluation result 
is recorded; c) The 5-best output is checked again 
for the best syntactic tree by means of SCF in-
formation. The phrased-based evaluation follows 
the popular method for evaluating a parser, while 
the sentence-based depends on the intersection of 
the parsed trees and those in the gold standard. 
Since the PCFG parser output at least one syntac-
tic tree for every sentence in our testing corpus, 
the sentence-based precision and recall are equal 
to each other. 
 
Table 4: Parsing Evaluation 
 
Phrase-based Sentence-
based 
Parsing  
Methods 
Precision Recall Precision  
= Recall 
One-best 57.5% 55% 13.64% 
5-best 65.28% 64.59% 26.2% 
With SCF 62.86% 62.1% 21.66% 
 
Table 4 shows that SCF information remarka-
bly improved the performance of the PCFG 
parser: the phrase-based precision increased by 
5.36% and recall by 7.1%, while the sentence-
based precision and recall both increased by 
8.04%. However, this doesn?t reach the upper 
limit of the 5-best. The possible reasons are: a) 
the our present SCF lexicon remains to be im-
proved; b) our method of applying SCF informa-
tion to the parser is too simple, e.g. probabilities 
of PCFG parsing results haven?t been exploited 
thoroughly. 
 
3 Related Works 
As far as we know, this is the first attempt to 
automatically acquire SCF information from real 
Chinese corpus and the first trial to apply SCF 
lexicon to a Chinese parser. Our research draws a 
lot on related works from international researches, 
and for the purpose of crosslingual processing, 
our research is kept in consistency with SCF 
conventions as much as possible. 
Due to linguistic differences, nevertheless, not 
all theories, methods or experiences could adapt 
to Chinese. Generally, there are four aspects that 
our research differs from those of other lan-
guages. First, the SCF formalization of most 
former researches follows the Levin style, in 
which most SCFs omit NP before predicates, 
while Chinese SCFs need to depict arguments 
occurring before verbs. Second, except (Sarkar 
and Zeman 2000), most former researches are 
based on manual SCF predefinition, while our 
predefined SCF set is statistically acquired (See 
Han and Zhao 2004). Third, involved parsers of 
former researches are mostly better than Chinese 
parsers to some degree. Forth, our SCF informa-
tion also includes 5 syntactic morphemes (See 
also Section 1.1). 
Meanwhile, the basic purpose for Chinese SCF 
acquisition is also to determine the subcategory 
features for a verb via its argument distributions 
and then apply the lexicon to NLP tasks. There-
fore, under similar cases the respective evalua-
tions are comparable. And Table 5 gives the 
comparison between our research and the best 
English results without semantic backoff 8  in 
(Korhonen 2001). 
 
Table 5: Performance Comparison Between 
Chinese and English Researches 
 
                Filtering 
Measures    Non BHT MLE
Ours 37.43% 50% 58.2%Precision Korhonen 24.3% 50.3% 74.8%
Ours 85.9% 57.2% 54.5%Recall Korhonen 83.5% 56.6% 57.8%
Ours 52.14 53.36 56.3F-
measure Korhonen 37.6 53.3 65.2
 
The comparison shows that our nonfiltering re-
sult is better than Korhonen?s, both BHT results 
are similar, while our MLE result is much worse 
                                                           
8 Semantic backoff is a method of generating SCF hypothe-
ses according to the semantic classification of the concerned 
verb. Note that this paper doesn?t involve verb meanings for 
generating hypotheses. Besides, though the evaluation for 
English SCF acquisition is the best, it?s not the newest. For 
the newest, please refer to (Korhonen 2003), in which the 
precision is 71.8% and recall is 34.5%. 
than Korhonen?s. That means our hypothesis 
generator performs well but our filtering method 
remains to be improved. According to the analy-
sis of relevant corpus, we found the main cause 
might be that low frequency SCF types account 
for 32% in our corpus while those in (Korhonen 
2001) sum to nearly 21%. 
Further more, (Briscoe and Carroll 1997) ap-
plied their acquired English SCF lexicon to an 
intermediate parser, and reported a 7% improve-
ment of both phrase-based precision and recall. 
Our application of SCF lexicon to a PCFG parser 
leads to 5.36% improvement for phrase-based 
precision, 7.1% for recall, and 8.04% for sen-
tence-based precision and recall. 
 
4    Conclusion 
 
This paper for the first time describes a largescale 
experiment of automatically acquiring SCF lexi-
con from real Chinese corpus. Perfor mance eva-
luation shows that our technology and acquiring 
program have achieved similar performance 
compared with former researches of other lan-
guages. And the application of the acquired lexi-
con to a PCFG parser indicates great potentiali-
ties of SCF information in the field of NLP. 
However, there is still a large gap between 
Chinese subcategorization works and those of o-
ther languages. Our future work will focus on the 
optimization of linguistic heuristic information 
and filtering methods, the application of semantic 
backoff, and the exploitation of SCF lexicon for 
other NLP tasks. 
References  
Brent, M. R. 1991. Automatic acquisition of subcate-
gorization frames from untagged text. In Proceed-
ings of the 29th Annual Meeting of the Association 
for Computational Linguistics, Berkeley, CA. 209-
214. 
Brent, M. 1993. From Grammar to Lexicon: un-
supervised learning of lexical syntax. Compu-
tational Linguistics 19.3. 243-262. 
Briscoe, Ted and John Carroll, 1997. Automatic ex-
traction of subcategorization from corpora. In Pro-
ceedings of the 5th ACL Conference on Applied 
Natural Language Processing, Washington, DC. 
Dorr, B. J. Gina-Anne Levow, Dekang Lin, and Scott 
Thomas, 2000. Chinese-English Semantic Resource 
Construction, 2nd International Conference on 
Language Resources and Evaluation (LREC2000), 
Athens, Greece, pp. 757--760. 
Gamallo, P., Agustini, A. and Lopes Gabriel P., 2002. 
Using Co-Composition for Acquiring Syntactic 
and Semantic Subcategorisation, ACL-02.  
Han, Xiwu, Tiejun Zhao, 2004. FML-Based SCF Pre-
definition Learning for Chinese Verbs. Interna-
tional Joint Conference of NLP 2004. 
Jin, Guangjin, 2001. Semantic Computations for Mod-
ern Chinese Verbs. Beijing University Press, Bei-
jing. (in Chinese) 
Korhonen, Anna, 2001. Subcategorization Acquistion, 
Dissertation for Ph.D, Trinity Hall University of 
Cambridge. 29-77. 
Korhonen, Anna, 2003. Clustering Polysemic Sub-
categorization Frame Distributions Semantically. 
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, pp. 64-71. 
Meng, Yao, 2003. Research on Global Chinese Pars-
ing Model and Algorithm Based on Maximum En-
tropy. Dissertation for Ph.D. Computer Department, 
HIT. 33-34. 
Sabine Shulte im Walde, 2002. Inducing German Se-
mantic Verb Classes from Purely Syntactic Sub-
categorization Information. Proceedings of the 40st 
ACL, pp. 223-230. 
Sarkar, A. and Zeman, D. 2000. Automatic Ex-
traction of Subcategorization Frames for 
Czech. In Proceedings of the 19th Interna-
tional Conference on Computational Linguis-
tics, aarbrucken, Germany. 
Zhan Weidong, 2000. Valence Based Chinese Seman-
tic Dictionary, Language and Character Applica-
tions, Volume 1. (in Chinese) 
Zhao Tiejun, 2002. Knowledge Engineering Report 
for MTS2000.  
 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1033?1040
Manchester, August 2008
Chinese Term Extraction Using Minimal Resources 
Yuha
School of C
Science and Technology,  
Harbin In
Techn
Harbin 15
1983yang@gmail.com 
Qin L
Department of Computing,  
The Hon
Polytechn
Hong Ko
csluqin@comp.polyu.e
du.
Tieju
School of C
Science and Technology,  
Harbin In
Techn
Harbin 1
tjzhao@mtlab.hit.edu
.
 
ct 
This pap
term extraction nimal resources. 
A term candidate extraction algorithm is 
proposed to i tures of the 
1 
Ter st 
fun  domain. Term 
                                                
ng Yang 
omputer  
stitute of  
ology, 
0001, China 
u 
g Kong  
ic University, 
ng, China 
hk 
n Zhao 
omputer  
stitute of  
ology, 
50001, China 
cn 
Abstra
er presents a new approach for 
using mi
dentify fea
relatively stable and domain independent 
term delimiters rather than that of the 
terms. For term verification, a link 
analysis based method is proposed to 
calculate the relevance between term 
candidates and the sentences in the 
domain specific corpus from which the 
candidates are extracted. The proposed 
approach requires no prior domain 
knowledge, no general corpora, no full 
segmentation and minimal adaptation for 
new domains. Consequently, the method 
can be used in any domain corpus and it 
is especially useful for resource-limited 
domains. Evaluations conducted on two 
different domains for Chinese term 
extraction show quite significant 
improvements over existing techniques 
and also verify the efficiency and relative 
domain independent nature of the 
approach. Experiments on new term 
extraction also indicate that the approach 
is quite effective for identifying new 
terms in a domain making it useful for 
domain knowledge update. 
Introduction 
ms are the lexical units to represent the mo
damental knowledge of a
 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
extraction is 
knowledge acq
an essential task in domain 
uisition which
lexicon update, domain onto
etc. Term extraction involves tw
tes by unithood calculation 
s a valid term. The second 
endent features of domain 
ter
s. 
Ot
 can be used for 
logy construction, 
o steps. The first 
step extracts candida
to qualify a string a
step verifies them through termhood measures 
(Kageura and Umino, 1996) to validate their 
domain specificity.  
Existing techniques extract term candidates 
mainly by two kinds of statistic based measures 
including internal association (e.g. Schone and 
Jurafsky, 2001) and context dependency (e.g. 
Sornlertlamvanich et al, 2000). These techniques 
are also used in Chinese term candidate 
extraction (e.g. Luo and Sun, 2003; Ji and Lu, 
2007). Domain dep
ms are used in a weighted manner to identify 
term boundaries. However, these algorithms 
always face the dilemma that fewer features are 
not enough to identify terms from non-terms 
whereas more features lead to more conflicts 
among selected features in a specific instance.  
Most term verification techniques use features 
on the difference in distribution of a term 
occurred within a domain and across domains, 
such as TF-IDF (Salton and McGill, 1983; Frank, 
1999) and Inter-Domain Entropy (Chang, 2005). 
Limited distribution information on term 
candidates in different documents are far from 
enough to distinguish terms from non-term
her researches attempted to use more direct 
information. The therm verification algorithm, 
TV_ConSem, proposed in (Ji and Lu, 2007) for 
Chinese calculate the percentage of context 
words in a domain lexicon using both frequency 
information and semantic information. However, 
this technique requires a large domain lexicon 
and relies heavily on both the size and the quality 
of the lexicon. Some supervised learning 
1033
approaches have been applied to protein/gene 
name recognition (Zhou et al, 2005) and Chinese 
new word identification (Li et al, 2004) using 
SVM classifiers (Vapnik, 1995) which also 
require large domain corpora and annotations, 
and intensive training is needed for a new domain. 
Current term extraction techniques (e.g. Frank 
et al, 1999; Chang, 2005; Ji and Lu, 2007) suffer 
from three major problems. The first problem is 
that these algorithms cannot identify certain 
kinds of terms such as the ones that have less 
statistical significance. The second problem is 
their dependency on full segmentation for 
Chinese text which is particularly vulnerable to 
ha
idates and the sentences in domain 
sp
s (terms for short) are more likely to 
be domain substantives. Words immediate before 
s, called predecessors and 
es
conne . These predecessors and 
Ch
ndle domain specific data (Huang et al, 2007). 
The third problem is their dependency on some a 
priori domain knowledge such as a domain 
lexicon making it difficult to be applied to a new 
domain.  
In this work, the proposed algorithm extracts 
candidates by identifying the relatively stable and 
domain independent term boundary markers 
instead of looking for features associated with the 
term candidate themselves. Furthermore, a novel 
algorithm for term verification is proposed using 
link analysis to calculate the relevance between 
term cand
ecific corpus to validate their domain 
specificity.  
The rest of the paper is organized as follows. 
Section 2 describes the proposed algorithms. 
Section 3 explains the experiments and the 
performance evaluation. Section 4 is the 
conclusion. 
2 Methodology 
2.1 Delimiters Based Term Candidate 
Extraction 
Generally speaking, sentences are constituted by 
substantives and functional words. Domain 
specific term
and after these term
succ sors of the terms, are likely to be either 
functional words or other general substantives 
cting terms
successors can be considered as markers of terms, 
and are referred to as term delimiters in this 
paper. In contrast to terms, delimiters are 
relatively stable and domain independent. Thus, 
they can be extracted more easily. Instead of 
looking for features associated with terms as in 
other works, this paper looks for features 
associated with term delimiters. That is, term 
delimiters are identified first. Words between 
delimiters are then taken as term candidates.  
The proposed delimiter identification based 
algorithm, referred to as TCE_DI (Term 
Candidate Extraction ? Delimiter Identification), 
extracts term candidates from a domain corpus 
by using a delimiter list, referred to as the DList. 
Given a DList, the algorithm TCE_DI itself is 
straight forward. For a given character string CS 
(CS = C1C2?Cn) shown in Figure 1, where Ci is a 
inese character. Suppose there are two 
delimiters D1 = Ci1?Cil and D2 = Cj1?Cjm in CS 
where D1 ? DList and D2 ? DList. The string CS 
is then segmented to five substrings: C1?Cib, 
Ci1?Cil, Cia?Cjb, Cj1?Cjm, and Cja?Cn. Since 
Ci1?Cil and Cj1?Cjm are delimiters, C1?Cib, 
Cia?Cjb, and Cja?Cn are regarded as term 
candidates as labeled by TC1, TC2 and TC3 in 
Figure 1, respectively. If there is no delimiter 
contained in CS, the whole string C1C2?Cn is 
regarded as one term candidate. 
Figure 1. Paradigm of Term Candidate Extraction 
DList can be obtained either from a delimiter 
training corpus or from a given  o l stop w rd ist. 
Given a delimiter training corpus, CorpusTraining, 
normally a domain specific corpus, and a domain 
lexicon Lexicon, DList can be obtained based on 
the following algorithm, referred to as DList_Ext 
St
withou
a stop  experts or from a 
ge
(DelimiterList Extraction Algorithm).   
ep 1: For each term Ti in Lexicon, mark Ti in 
CorpusTraining as a non-divisible lexical unit.  
Step 2: Segment remaining text in CorpusTraining.  
Step 3: Extracts predecessors and successors of 
all Ti as delimiter candidates. 
Step 4: Remove delimiter candidates that are 
contained in a Ti in Lexicon. 
Step 5: Rank delimiter candidates by frequency 
and the top NDI number of items are 
considered delimiters. 
The DList_Ext algorithm basically use known 
terms in a domain specific Lexicon to find the 
delimiters. It can be shown in the experiments 
later that Lexicon does not need to be 
comprehensive. Even if a small training corpus, 
CorpusTraining, is not available in a language 
t sufficient domain specific NLP resources, 
-word list produced by
neral corpus can serve as DList directly 
without using the DList_Ext algorithm. 
1034
2.2 Link Analysis Based Term Verification 
In a domain corpus, some sentences are domain 
relevant sentences which contain more domain 
specific information whereas others are general 
sentences which contain less domain information. 
A domain specific term is more likely to be 
contained in domain relevant sentences, which 
means that domain relevant sentences and 
ai  
, w(pn)
l numb
dom n specific terms have a mutually
reinforcing relationship. A novel algorithm, 
referred to as TV_LinkA (Term Verification ? 
Link Analysis) based the Hyperlink-Induced 
Topic Search (HITS) algorithm (Kleinberg, 1997) 
originally proposed for information retrieval, is 
proposed using link analysis to calculate the 
relevance between term candidates and the 
sentences in domain specific corpora for term 
verification.   
In TV_LinkA, a node p can either be a sentence 
or a term candidate. If a term candidate TermC is 
contained in a sentence Sen of the corpus 
CorpusExtract where the candidates were extracted, 
there is a directional link from Sen to TermC. This 
way, a graph for the candidates and the sentences 
in CorpusExtract can be constructed and the links 
between them indicate their relationships. A good 
hub  Corpu in sExtract is a sentence that contains 
many good authorities; a good authority is a term 
candidate that is contained in many good hubs. 
Each node p is associated with a non-negative 
authority weight Apw )(  and a non-negative hub 
weight Hpw )( . Link analysis in TV_LinkA 
makes use of the relationship between hubs and 
authorities via an iterative process to maintain 
and update authority/hub weights for each node 
of the graph.  
Let VA denote the authority vector (w(p1)A, 
w(p2)A,?, w(pn)A)  and VH denote the hub vector 
(w(p1)H, w(p2)H,? H), where n is the sum of 
the tota er of sentences and the total 
number of term candidates. Given weights VA and 
VH with a directional link p?q, the I operation(an 
in-pointer to a node) and the O operation(an out-
pointer to a node) update w(q)A and w(p)H as 
follows. 
I operation: ?
??
=
Eqp
HA w(p)w(q)          (1) 
O operation: ?
??
e calculated as follows. 
For i 
Apply the I operation to ( ), 
o . 
factor
=
Eqp
AH w(q)w(p)         (2) 
Let k be the iteration termination parameter and z 
be the vector (1, 1, 1,?, 1) , and VA and VH are 
initialized to AV0  = 
HV0  = z. Hubs and authorities 
can then b
= 1, 2,?, k 
A
iV 1- ,
H
iV 1-
btaining new AiV '
Apply the O operation to ( AiV ' ,
H
iV 1- ), 
obtaining new HiV ' . 
Normalize iV '  by dividing the 
A
normalization  ? 2)'( A(p)w  to 
'  by dividing the 
obtain AiV . 
Normalize V Hi
normalization factor ? 2)'( H(p)w  to 
End 
R
In sExt , term candidates with high 
authority in 
dom terms wh
high uments are m
likel
on this observation, the termhood of each 
candidate term TermC, denoted as TermhoodC, is 
calculated according to formula (3) defined 
be
obtain iV . 
eturn ( AkV , 
H
kV ) 
 Corpu ract
H
a few documents are likely to be 
ain specific ereas candidates with 
 authority in many doc ore 
y to be commonly used general words. Based 
low. 
)log()(
Cj
A
jC DF
D
w(C)Termhood ?=      (3) 
where Ajw(C)  is the authority of TermC in a 
document Dj of CorpusExtract, |D| is the total 
number of documents in CorpusExtract and DFC is 
the total number of documents in which TermC 
occurs. Term s
termhood val
C  are then ranked according to their 
ues TermhoodC, and the top ranked 
NTCList candidates are considered terms. NT
an algorithm parameter to be determined 
entally
e two sets of non-overlapping 
academic papers in the IT domain and 
CorpusIT_Small is identical to the corpus used in 
TV_ConSem(Ji and Lu, 2007). CorpusLegal_Small is 
a complete set of official Chinese criminal law 
articles. CorpusLegal_Large includes the complete set 
CList is 
experim . 
3 Performance Evaluation 
3.1 Data Preparation 
To evaluate the performance of the proposed 
algorithms for Chinese, experiments are 
conducted on four corpora of two different 
domains as listed in Table 1. CorpusIT_Small and 
CorpusIT_Large ar
1035
of ficial Chinese constitutional lof aw articles and 
d
Economics/Finance law articles (http://www.law-
lib.com/). Three domain lexicons used in the 
experiments are detailed in Table 2. LexiconIT is 
obtained according to the term extraction 
algorithm (Ji and Lu, 2007) with manual 
verification. LexiconLegal is extracted from 
CorpusLegal_Small by manual verification too. 
Because legal text covers a lot of different areas 
such finance, science, advertisement, etc., the 
actually legal specific terms are relatively small 
in size. LexiconPKU contains a total of 144K 
manually verified IT terms supplied by the 
Institute of Computational Linguistics, Peking 
University. LexiconPKU, is used as the standard 
term set for evaluation on the IT domain. 
CorpusIT_Small and LexiconIT are used to obtain the 
delimiter list of IT domain, DListIT. 
CorpusLegal_Small and LexiconLegal are used to 
obtain the elimiter list of legal domain, 
DListLegal. CorpusIT_Large and CorpusLegal_Large are 
used as open test data to evaluate the proposed 
algorithms in IT domain and legal domain, 
respectively.  
Corpus Domain Size Text type
CorpusIT_Small IT 77K Academic 
papers 
CorpusIT_Large IT 6.64M Academic 
papers 
Corpus 
Legal_Small
Legal 344K Law  
article 
Corpus 
Legal_Large
Legal 1.04M Law  
article 
Table 1. Different Corpora Used for Experiments 
Lexicon Domain Size Source 
LexiconIT IT 3,337 Corpus 
IT_Small
Lex 3
 
iconLegal Legal 94 Corpus 
Legal_Small
LexiconPKU IT 144K PKU 
Table 2. D rent Le se
xperime
rify that the approach works with a 
op word list without delimiter extraction, 
rd list, , is d a ence 
 the 494 general purpose stop words 
web www n) thou  
. 
 in the IT 
alua e 
follow formula: 
iffe
E
xicons U
nts 
d for 
To ve
simple st
a stop wo DListSW also use s refer
by taking
downloaded from a Chinese NLP resource 
site (
ion
.nlp.org.c wi t any
modificat
The performance of the algorithm
domain is ev ted by precision according to th
TCList
NewLexicon N+
TE N
N
recis =
where tes in 
term candidate xtracted by an 
ev
e verification of all the new terms 
is 
arked them as correct terms. As 
there is no reasonably large standard l
list available, the evaluation of the leg
p ion            (4) 
NTCList is the number of term candida
list TCList e
aluated algorithm, NLexicon denotes the number 
of term candidates in TCList contained in 
LexiconPKU, NNew denotes the number of extracted 
term candidates that are not in LexiconPKU, yet 
are considered correct. Thus, NNew is the number 
of newly discovered terms with respect to 
LexiconPKU. Th
carried out manually by two experts 
independently. A new term is considered correct 
if both experts m
egal term 
al domain 
in terms of precision is conducted manually. No 
evaluation on new term extraction is conducted. 
To evaluate the ability of the algorithms in 
identify new terms in the IT domain, another 
measurement is applied to the IT corpus against 
LexiconPKU based on the following formula: 
TCList
New
NTE N
N
R =                          (5) 
where TCList and NNew are the same as given in 
formula (4). A higher RNTE indicates that more 
extracted terms are outside of LexiconPKU and are 
thus considered new terms. This is similar to the 
measurements of out of vocabulary (OOV) in 
Chinese segmentation. A higher RNTE indicates 
the algorithm can be useful for domain 
knowledge update including lexicon expansion. 
3.2 Evaluation on Term Extraction 
For comparison, a statistical based term 
candidate extraction algorithm, TCE_SEF&CV 
with the best performance in 
using both internal association and external 
e 
; one is a 
(Ji and Lu, 2007) 
strength, is used as the reference algorithm for 
the evaluation of TCE_DI. A statistics based term 
verification algorithm, TV_ConSem (Ji and Lu, 
2007) using semantic information within a 
context window is used for the evaluation of 
TV_LinkA. LexiconPKU is also used in 
TV_ConSem. Two popular methods integrated 
without division of candidate extraction and 
verification steps are used for comparison. Th
first one is based on TF-IDF (Salton and McGill, 
1983 Frank et al, 1999). The second 
supervised learning approach based on a SVM 
classifier, SVMlight (Joachims, 1999). The 
features used by SVMlight are shown in Table 3. 
Two training sets are constructed for the SVM 
classifier. The first one includes 3,337 positive 
examples (LexiconIT) and 5,950 negative 
examples extracted from CorpusIT_Small. The 
second one includes 394 positive examples 
1036
(LexiconLegal) and 28,051 negative examples 
extracted from CorpusLegal_Small.  
No. Feature Explanation 
1 Percentage of the Chinese characters 
occurred in LexiconDomain
2 Frequency in the domain corpus 
3 Frequency in the general corpus 
4 Part of speech 
5 The length of Chinese characters in 
the candidate 
6 The length of non-Chinese 
characters in the candidate 
7 Contextual evidence 
Table 3. Features Used in the SVM Classifier 
perforFigure 2 shows the mance of the 
proposed TCE_ for term 
ex tio s 
for IT do IT
TCE_DIl tracted 
de iter  NDI = 
50 esp SW
word list 
DI and TV_LinkA 
trac n compared to the reference algorithm
main using CorpusIT_Large. TCE_DI  and 
egal indicate TCE_DI using ex
lim  lists DListIT and DListLegal with
I  simply uses the stop 0, r ectively. TCE_D
DListSW. 
0 1000 2000 3000 4000 5000
40
45
50
55
60
65
70
75
80
85
90
95
100
Pr
ec
is
io
n
Extracted Terms (N
TCList
)
 TCE_DI
IT
+TV_LinkA
 TCE_DI
Legal
+TV_LinkA
 TCE_DI +TV_L
SW
inkA
 TCE_SEF&CV+TV_LinkA
 TCE_DI
IT
+TV_ConSem
 TCE_SEF&CV+TV_ConSem
 TF-IDF
 SVM
Figure 2 Performance of Different Algorithms on 
IT Domain 
As shown in Figure 2, term extraction based 
on TCE_DIIT combined with TV_LinkA gives the 
best performance. It achieves 75.4% precision 
when the number of extracted terms NTCList 
reaches 5,000. The performance is 9.6% and 
29.4% higher in precision compared to TF-IDF 
and TCE_SEF&CV combined with TV_ConSem, 
respectively. These translate to improvements of 
ver 14.8% and 63.9%, respectively.  
When applying the same TV_LinkA algorithm 
for term verification, TCE_DI using different 
delimiter lists provide 24% better performance on 
average compared to the TCE_SEF&CV 
algorithm which translates to improvement of 
over 47%. The result from using delimiters of 
legal domain (DListLegal) to data in IT domain (as 
shown in TCE_DIlegal) is better on average than 
using a simple general stop word list. It should be 
ver, that TCE_DISW still performs 
much better than the reference algorithms, which 
means that delimiter based term candidate 
extraction algorithm can improve performance 
even without any domain specific training. When 
applying the same TCE_DIIT algorithm in term 
candidate extraction, TV_LinkA provides 10% 
higher performance compared to the TV_ConSem 
TV_LinkA using o word list without 
an
of
precision of o
noted, howe
algorithm which translates to improvement of 
over 15.3%. It is important to point out that 
nly the stop 
y domain specific knowledge performs better 
than TV_ConSem using a large domain lexicon. 
In other words, delimiter based extraction with 
link analysis use much less resources and still 
improve performance of TV_ConSem. 
The performance of TCE_DIIT or 
TCE_SEF&CV combined with TV_ConSem have 
an upward trend when more terms are extracted 
which seems to be against intuition. The principle 
 the TV_ConSem algorithm is that a candidate 
is considered a valid term if a majority of its 
context words already appear in the domain 
lexicon. General words are more likely to be 
ranked on top because they are commonly used 
which explains the low performance of 
TV_ConSem in the lower range of NTCList. When 
NTCList increases, more domain terms are included. 
Thus, there is an upward trend in precision. But, 
the upward trend reverts at around 4,500 because 
the measurement in percentage is too low to 
distinguish valid terms from non-term candidates.  
It is also interesting to point out that the simple 
TF-IDF algorithm which was rarely used in 
Chinese term extraction performs as well as the 
SVM classifier. The main reason is that the test 
corpus consists of academic papers. So, many 
terms are consistent and repeated a lot of times in 
different documents which accords with the idea 
of TF-IDF. Thus, TF-IDF performs relatively 
well because of the high-quality domain corpus. 
However, TF-IDF, as a statistics based algorithm 
suffers from similar problem as others based on 
1037
statistics. Thus it does not perform as well as the 
proposed TCE_DI and TV_LinkA algorithms. 
ac
Figure 3 shows that the proposed algorithms 
achieve similar performance on the legal domain. 
TCE_DILegal combined with TV_LinkA perform 
the best. The result from using IT domain 
delimiters (DListIT) in legal domain as shown in 
TCE_DIIT is better on average than using the 
general purpose stop list. This further proves that 
extracted delimiter list even from a different 
domain can be more effective than a general stop 
word list. When applying the same TV_LinkA 
algorithm for term verification, TCE_DI using 
different delimiter lists are better than all the 
reference algorithms. Without large lexicon in 
Chinese legal domain, the TV_ConSem algorithm 
does not even work.  TV_LinkA using no prior 
domain knowledge for term verification still 
hieves similar improvement compared to that 
of the IT domain where a comprehensive domain 
lexicon is available. 
70
80
90
100
0 1000 2000 3000 4000 5000
40
50
60
Extracted Terms (N
TCList
)
Pr
ec
is
io
n
 TCE_DI
IT
+TV_LinkA
 TCE_DI
Legal
+TV_LinkA
 TCE_DI
SW
+TV_LinkA
 TCE_SEF&CV+TV_LinkA
 TF-IDF
 SVM
 Figure 3. Performance of Different Algorithms 
on Legal Domain 
There are three main reasons for the 
performance improvements of the proposed 
TCE_DI and TV_LinkA algorithms. Firstly, the 
delimiters which are mainly functional words (e. 
g. ???(at/in), ???(or)) and general substantive 
(e.g. ???(be), ????(adopt)) can be extracted 
easily and are effective term boundary markers 
since they are quite domain independent and 
stable. Secondly, the granularity of domain 
specific terms extracted the proposed algorithm is 
much larger than words obtained by word 
segmentation. This keeps many noisy strings out 
of the term candidate set. Thus, the proposed 
delimiter based algorithm performs much better 
over segmentation based statistical methods. 
Thirdly, the proposed approach is not as sensitive 
to term frequency as other statistical based 
approaches because term candidates are 
identified without regards to the frequencies of 
the candidates. In the TV_LinkA algorithm, terms 
are verified by calculating the relevance between 
candidates and the sentences instead of the 
distributions of terms in different types of 
documents. Terms having low frequencies can be 
identified as long as they are in domain relevant 
sentences whereas in the previous approaches 
including TF-IDF, terms with less statistical 
significance are weeded out. For example, a long 
IT term ????????? (Hierarchical storage 
system) with a low frequency of 6 is extracted 
using the proposed approach. It cannot be 
i  
information is is term cannot 
be extracte
dentified by TF-IDF since the statistical
not significant. Th
d by the segmentation based 
algorithms either because general segmentor split 
long terms into pieces making them difficult to 
be reunited using term extraction techniques.  
It is interesting to know that the proposed 
approach not only achieves the best performance 
for both domains, it also achieves second best 
when using extracted delimiters from a different 
domain. The results confirm that delimiters are 
quite stable across domains and the relevance 
between candidates and sentences are efficient 
for distinguishing terms from non-terms in 
different domains. In fact, the proposed approach 
can be applied to different domains with minimal 
training or no training if resources are limited. 
3.3 Evaluation on New Term Extraction 
As LexiconPKU is the only ready-to-use domain 
lexicon, the evaluation on new term extraction is 
conducted on CorpusIT_Large only. Figure 4 shows 
the evaluation of the proposed algorithms 
compared to the reference algorithms in terms of 
RNTE, the ratio of new terms among all identified 
terms.  
It can be seen that the proposed algorithms 
TCE_DIIT combined with TV_LinkA is basically 
the top performer throughout the range. It can 
identify 4% (with respect to TCE_SEF&CV 
+TV_ConSem) to 27% (with respect to TF-IDF) 
more new terms when NTCList reaches 5,000 which 
translate to improvements of over 9% to 170%, 
respectively. The second best performer is 
TCE_DIlegal combined with TV_LinkA using 
delimiters of legal domain. In fact, it only 
underperforms in the lower range of NTCList 
1038
compared to TCE_DIIT. When NTCList reaches 
5,000, their performance is basically the same. 
However, the TCE_DISW algorithm using s
context words occur in the domain lexicon than 
that of other terms. Thus, new terms are actually 
ranked higher than other terms in TV_ConSem 
which explains its higher ability to identify new 
terms in the low range of NTCList. However, its 
performance drops in the high range of NTCList 
because the influence of context words 
diminishes in terms of percentage in the domain 
lexicon to distinguish terms from non-terms. 
Figure 4 also shows that TF-IDF and SVM 
perform the worst in new term extraction 
compared to other algorithms. TF-IDF has 
relatively low ability to identify new terms since 
new terms are not widely used and they do not 
repeat a lot of times in many documents. As  
SVM  is sensitive to training data, it is naturally 
not adaptive to new terms. 
All current Chinese term extraction algorithms 
rely on segmentation with comprehensive lexical 
knowledge and y
a  
problem. T xtraction 
pa
top 
wo
in 
mi ion 
 a 
rds performs much worse than using extracted 
delimiter lists as shown for TCE_DIIT and 
TCE_DIlegal. In the TCE_DI algorithm, character 
strings are split by delimiters and the remained 
parts are taken as term candidates. Generally 
speaking, if a new term contains a delimiter or a 
stop word as its component, it cannot be 
identified correctly. Consequently, if a new term 
contains a stop word as its component, it cannot 
be extracted correctly using TCE_DISW.  
However, new terms are less likely to conta
deli ters because the delimiter extract
algorithm DList_Ext would not consider
component as a delimiter if it is contained in a 
term in LexiconDomain. Consequently, TCE_DISW 
is less adaptive to domain specific data compared 
to TCE_DIIT and TCE_DIlegal. That is also why 
TCE_DISW picks up new terms much more slowly. 
0 1000 2000 3000 4000 5000
0
10
20
30
40
50
Pe
rc
en
ta
ge
 o
f N
ew
 T
er
m
s
Extracted Terms (N
TCList
)
 TCE_DI +TV_LinkA
IT
 TCE_DI
Legal
+TV_LinkA
 TCE_DI
SW
+TV_LinkA
 TCE_SEF&CV+TV_LinkA
 TCE_DI
IT
+TV_ConSem
 TCE_SEF&CV+TV_ConSem
 TF-IDF
 SVM
Figure 4. Performance of Different Algorithms 
for New Term Extraction 
It is interesting to know that TCE_DIIT 
combined with TV_ConSem identifies more new 
terms in the low range of NTCList. In the 
TV_ConSem algorithm, the major information 
used for term verification is the percentage of the 
context words appear in the domain lexicon. As 
discussed earlier in Section 3.2, TV_ConSem 
ranks commonly used general words higher than 
others which leads to the low precision of 
TV_ConSem for term extraction. A new term 
faces a similar scenario because more of its 
et Chinese segmentation 
lgorithms have the OOV (out of vocabulary)
his makes Chinese term e
rticularly vulnerable to new term extraction. 
The proposed approach, on the other hand, is 
based on delimiters which is more stable, domain 
independent, and OOV independent. Figure 4 
shows that TCE_DI and TV_LinkA using minimal 
training from different domains can extract much 
more new terms than previous techniques. In fact, 
the proposed approach can serve as a much better 
tool to identify new domain terms and can be 
quite effective for domain lexicon expansion. 
4 Conclusion 
In conclusion, this paper presents a robust term 
extraction approach using minimal resources. It 
includes a delimiter based algorithm for term 
candidate extraction and a link analysis based 
algorithm for term verification. The proposed 
approach is not sensitive to term frequency as the 
previous works. It requires no prior domain 
knowledge, no general corpora, no full 
segmentation, and minimal adaptation for new 
domains.  
Experiments for term extraction are conducted 
on IT domain and legal domain, respectively. 
Evaluations indicate that the proposed approach 
has a number of advantages. Firstly, the proposed 
approach can improve precision of term 
extraction quite significantly. Secondly, the fact 
that the proposed approach achieves the best 
performance on two different domains verifies its 
domain independent nature. The proposed 
approach using delimiters extracted from a 
1039
different domain also achieves the second best 
performance which indicates that the delimiters 
are quite stable and domain independent. The 
proposed approach still performs much better 
than the reference algorithms when using a 
general purpose stop word list, which means that 
the proposed approach can improve performance 
well even as a completely unsupervised approach 
without any training. Consequently, the results 
demonstrate that the proposed approach can be 
applied to different domains easily even without 
he proposed approach is 
R
 August 2002. 
a. 2002. A measure of term 
ed on the number of co-
inese Word 
Segmentation: Tokenization, Character 
n, or Wordbreak Identification. In 
Ka K., and B. Umino. 1996. Methods of 
Kl
onment. In Proceedings of the 9th 
Ji 
2 ? 74. 
Lu
 Measures. In 
M
e Identification and Semantic 
Na
Sc rafsky D. 2001. Is Knowledge-free 
So
ord 
Vl
Zh
training. Thirdly, t
particularly good for identifying new terms so 
that it can serve as an effective tool for domain 
lexicon expansion. 
Acknowledgements 
This work was done while the first author was 
working at the Hong Kong Polytechnic 
University supported by CERG Grant B-Q941 
and Central Research Grant: G-U297.
eferences 
Chang Jing-Shin. 2005. Domain Specific Word 
Extraction from Hierarchical Web Documents: A 
First Step toward Building Lexicon Trees from 
Web Corpora. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Learning: 
64-71. 
Chien LF. 1999. Pat-tree-based adaptive keyphrase 
extraction for intelligent Chinese information 
retrieval. Information Processing and Management, 
vol.35: 501-521. 
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. 
Domain-specific Keyphrase Extraction. In 
Proceedings of 16th International Joint Conference 
on Artificial Intelligence IJCAI-99: 668-673. 
Feng Haodi, Kang Chen, Xiaotie Deng , and Weimin 
Zheng, 2004. Accessor variety criteria for Chinese 
word extraction. Computational Linguistics, 
30(1):75-93. 
Hiroshi Nakagawa, and Tatsunori Mori. 2002. A 
simple but powerful automatic term extraction 
method. In COMPUTERM-2002 Proceedings of 
the 2nd International Workshop on Computational 
Term: 29-35. Taiwan,
Hisamitsu T., and Y. Niw
representativeness bas
occurring salient words. In Proceedings of the 19th 
COLING, 2002. 
Huang Chu-Ren, Petr ?Simon, Shu-Kai Hsieh, and 
Laurent Pr?evot. 2007. Rethinking Ch
Classificatio
Proceedings of the ACL 2007 Demo and Poster 
Sessions: 69?72. Joachims T. 2000. Estimating the 
Generalization Performance of a SVM Efficiently. 
In Proceedings of the International Conference on 
Machine Learning, Morgan Kaufman, 2000. 
geura 
automatic term recognition: a review. Term 
3(2):259-289. 
einberg J. 1997. Authoritative sources in a 
hyperlinked envir
ACM-SIAM Symposium on Discrete Algorithms: 
668-677. New Orleans, America, January 1997. 
Luning, and Qin Lu. 2007. Chinese Term Extraction 
Using Window-Based Contextual Information. In 
Proceedings of CICLing 2007, LNCS 4394: 6
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and 
Xiaozhong Fan. The Use of SVM for Chinese New 
Word Identification. In Proceedings of the 1st 
International Joint Conference on Natural 
Language Processing ( IJCNL P2004): 723-732. 
Hainan Island, China, March 2004. 
o Shengfen, and Maosong Sun. 2003. Two-
Character Chinese Word Extraction Based on 
Hybrid of Internal and Contextual
Proceedings of the Second SIGHAN Workshop on 
Chinese Language Processing: 24-30. 
cDonald, David D. 1993. Internal and External 
Evidence in th
Categorization of Proper Names. In Proceedings of 
the Workshop on Acquisition of Lexical 
Knowledge from Text, pages 32--43, Columbus, 
OH, June. Special Interest Group on the Lexicon of 
the Association for Computational Linguistics. 
sreen AbdulJaleel and Yan Qu. 2005. Domain 
Term Extraction and Structuring via Link Analysis. 
In Proceedings of the AAAI '05 Workshop on Link 
Analysis: 39-46. 
Salton, G., and McGill, M.J. (1983). Introduction to 
Modern Information Retrieval. McGraw-Hill. 
hone, P. and Ju
Induction of Multiword Unit Dictionary Headwords 
a solved problem? In Proceedings of EMNLP2001. 
rnlertlamvanich V., Potipiti T., and Charoenporn T. 
2000. Automatic Corpus-based Thai W
Extraction with the C4.5 Learning Algorithm. In 
Proceedings of COLING 2000. 
adimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, 1995. 
ou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. 
Recognition of Protein/Gene Names from Text 
using an Ensemble of Classifiers. BMC 
Bioinformatics 2005, 6(Suppl 1):S7. 
1040
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1121?1128
Manchester, August 2008
Diagnostic Evaluation of Machine Translation Systems Using Auto-
matically Constructed Linguistic Check-Points 
Ming Zhou1, Bo Wang2, Shujie Liu2, Mu Li1, Dongdong Zhang1, Tiejun Zhao2 
1Microsoft Research Asia 
Beijing, China 
{mingzhou,muli,dozhang} 
@microsoft.com 
 
2Harbin Institute of Technology 
Harbin, China 
{bowang,Shujieliu,tjzhao} 
@mtlab.hit.edu.cn 
 
 
?Abstract 
We present a diagnostic evaluation plat-
form which provides multi-factored eval-
uation based on automatically con-
structed check-points. A check-point is a 
linguistically motivated unit (e.g. an am-
biguous word, a noun phrase, a verb~obj 
collocation, a prepositional phrase etc.), 
which are pre-defined in a linguistic tax-
onomy. We present a method that auto-
matically extracts check-points from pa-
rallel sentences. By means of check-
points, our method can monitor a MT 
system in translating important linguistic 
phenomena to provide diagnostic evalua-
tion. The effectiveness of our approach 
for diagnostic evaluation is verified 
through experiments on various types of 
MT systems. 
1 Introduction 
Automatic MT evaluation is a crucial issue for 
MT system developers. The state-of-the-art me-
thods for automatic MT evaluation are using an 
n-gram based metric represented by BLEU (Pa-
pineni et al, 2002) and its variants. Ever since its 
invention, the BLEU score has been a widely 
accepted benchmark for MT system evaluation. 
Nevertheless, the research community has been 
aware of the deficiencies of the BLEU metric 
(Callison-Burch et al, 2006). For instance, 
BLEU fails to sufficiently capture the vitality of 
natural languages: all grams of a sentence are 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
treated equally ignoring their linguistic signific-
ance; only consecutive grams are considered ig-
noring the skipped grams of certain linguistic 
relations; candidate translation gets acknowl-
edged only if it uses exactly the same lexicon as 
the reference ignoring the variation in lexical 
choice. Furthermore, BLEU is useful for opti-
mizing and improving statistical MT systems but 
it has shown to be ineffective in comparing sys-
tems with different architectures (e.g., rule-based 
vs. phrase-based) (Callison-Burch et al,  2006).  
    Another common deficiency of the state-of-
the-art evaluation approaches is that they cannot 
clearly inform MT developers on the detailed 
strengths and flaws of an MT system, and there-
fore there is no way for us to understand the ca-
pability of certain modules of an MT system, and 
the capability of translating certain kinds of lan-
guage phenomena. For the purpose of system 
development, MT developers need a diagnostic 
evaluation approach to provide the feedback on 
the translation ability of an MT system with re-
gard to various important linguistic phenomena.     
    We propose a novel diagnostic evaluation ap-
proach. Instead of assigning a general score to an 
MT system we evaluate the capability of the sys-
tem in handling various important linguistic test 
cases called Check-Points. A check-point is a 
linguistically motivated unit, (e.g. an ambiguous 
word, a noun phrase, a verb~obj collocation, a 
prepositional phrase etc.) which are pre-defined 
in a linguistic taxonomy for diagnostic evalua-
tion. The reference of a check-point is its corres-
ponding part in the target sentence. The evalua-
tion is performed by matching the candidate 
translation corresponding to the references of the 
check-points. The extraction of the check-points 
is an automatic process using word aligner and 
parsers. We control the noise of the word aligner 
and parsers within tolerable scope by selecting 
1121
reliable subset of the check-points and weighting 
the references with confidence.  
    The check-points of various kinds extracted in 
this way have shown to be effective in perform-
ing diagnostic evaluation of MT systems. In ad-
dition, scores of check-points are also approved 
to be useful to improve the ranking of MT sys-
tems as additional features at sentence level and 
document level. 
The rest of the paper is structured in the fol-
lowing way:  Section 2 gives the overview of the 
process of the diagnostic evaluation. Section 3 
introduces the design of check-point taxonomy. 
Section 4 explains the details of construction of 
check-point database and the methods of reduc-
ing the noise of aligner and parsers. Section 5 
explains the matching approach. In Section 6, we 
introduce the experiments on different MT sys-
tems to demonstrate the capability of the diag-
nostic evaluation. In Section 7, we show that the 
check-points can be used to improve the current 
ranking methods of MT systems. Section 8 com-
pares our approach with related evaluation ap-
proaches. We conclude this work in Section 9. 
2 Overview of Diagnostic Evaluation 
In our implementation, we first build a check-
point database encoded in XML by associating a 
test sentence with qualified check-points it con-
tains. This process can be described as following 
steps: 
 
? Collect a large amount of parallel sen-
tences from the web or book collections. 
? Parse the sentences of source language 
and target language. 
? Perform the word alignments between 
each sentence pair. 
? For each category of check-points, extract 
the check-points from the parsed sentence 
pairs. 
? Determine the references of each check-
point in source language based on the 
word alignment.  
 
   With the extracted check-point database, the 
diagnostic evaluation of an MT system is per-
formed with the following steps: 
 
? The test sentences are selected from the 
database based on the selected categories 
of check-points to be evaluated. 
? For each check-point, we calculate the 
number of matched n-grams of the refer-
ences against the translated sentence of 
the MT system.  The credit of the MT sys-
tem in translating this check-point is ob-
tained after necessary normalization. 
? The credit of a category can be obtained 
by summing up the credits of all check-
points of this category. Then the credit of 
an MT system can be obtained by sum-
ming up the credits of all categories. 
? Finally, scores of system, category groups 
(e.g. Words), single category (e.g. Noun), 
and detail information of n-gram matching 
of each check-point are all provided to the 
developers to diagnose the MT system. 
3 Linguistic Check-Point Taxonomy 
The taxonomy of automatic diagnostic evaluation 
should be widely accepted so that the diagnostic 
results can be explained and shared with each 
other. We will also need to remove the sophisti-
cated categories that are out of the capability of 
current NLP tools to recognize.  
In light of this consideration, for Chinese-
English machine translation, we adopted the ma-
nual taxonomy introduced by (Lv, 2000; Liu, 
2002) and removed items that are beyond the 
capability of our parsers. The taxonomy includes 
typical check-pints at word, phrase and sentence 
levels. Some examples of the representative 
check-points at different levels are provided be-
low: 
 
?  Word level check-points: 
    a. Preposition word e.g., ?(in), ?(at) 
    b. Ambiguous word e.g., ?(play) 
    c. New word1 e.g., ??(Punk)  
?  Phrase level check-points: 
    a. Collocation. e.g., ??-??(fired ? food)  
    b. Repetitive word combination. e.g., ??
(have a look) 
    c. Subjective-predicate phrase e.g., ?*?, 
(he*said) 
     ? Sentence level check-points:  
a. ?BA? sentence 2 : ?? (BA)???? . 
(He took away the book.) 
b. ?BEI? sentence3????(BEI)???. 
(The vase was broken.)   
                                                 
1 New words are the terms extracted from web which can be 
a name entity or popular words emerging recently.   
2 In a ?BA? sentence, the object which normally follows the 
verb occurs preverbally, marked by word ?BA?. 
3 ?BEI? sentence is a kind of passive voice in Chinese 
marked by word ?BEI?. 
1122
    Our implementation of Chinese-English 
check-point taxonomy contains 22 categories and 
English-Chinese check-point taxonomy contains 
20 categories. Table 1 and 2 show the two 
taxonomies. In practice, any tag in parsers (e.g. 
NP) can be easily added as new category. 
 
Word level 
Ambiguous word New word Idiom 
Noun Verb Adjective 
Pronoun Adverb Preposition 
Quantifier Repetitive word Collocation 
Phrase level 
Subject-predicate 
phrase 
Predicate-object 
 phrase 
Preposition-
object phrase 
Measure phrase Location phrase  
Sentence level 
BA sentence BEI sentence SHI sentence 
YOU sentence Compound sentence 
Table 1:  Chinese check-point taxonomy 
 
Word level 
Noun Verb (with Tense) Modal verb 
Adjective Adverb Pronoun 
Preposition Ambiguous word Plurality 
Possessive Comparative & Superlative  degree 
Phrase level 
Noun phrase Verb phrase Adjective 
phrase 
Adverb phrase Preposition phrase  
Sentence level 
Attribute clause Adverbial clause Noun clause 
Hyperbaton  
Table 2: English check-point taxonomy 
4 Construction of Check-Point Data-
base 
Given a bilingual corpus with word alignment, 
the construction of check-point database consists 
of following two steps. First, the information of 
pos-tag, dependency structure and constituent 
structure can be identified with parsers. Then 
check-points of different categories are identified. 
Check-points of word-level categories such as 
Chinese idiom and English ambiguous words are 
extracted with human-made dictionaries, and the 
check-points of New-Word are extracted with a 
new word list mined from the web. A set of hu-
man-made rules are employed to extract certain 
categories involving sentence types such as com-
pound sentence.  
    Second, for a check-point, with the word 
alignment information, the corresponding target 
language portion is identified as the reference of 
this check-point. The following example illu-
strates the process of extracting check-points 
from a parallel sentence pair.  
?  A Chinese-English sentence pair: 
  ?????????. 
    They opposed the building of reserve funds. 
?  Word segmentation and pos-tagging: 
  ??/R ??/V ??/V ???/N ./P 
?  Parsing result (e.g.  a dependency result): 
    (SUB, 1/??, 0/??)  (OBJ, 1/??, 2/?
?) (OBJ, 2/??, 3/???) 
?  Word alignment: 
     (1; 1); (2; 2); (3; 4); (4; 6,7);   
?   The check-points in table 3 are extracted: 
 
Table 3: Example of check-point extraction 
 
    To extract the categories of check-points of 
different schema of syntactic analysis such as 
constitute structure and dependency structure, 
three parsers including a Chinese skeleton parser 
(a kind of dependency parser) (Zhou, 2000), 
Stanford statistical parser and Berkeley statistical 
parser (Klein 2003) are used to parse the Chinese 
and English sentences.  As explained in next sec-
tion, these multiple parsers are also used to select 
high confident check-points. To get word align-
ment, an existing tool GIZA++ (Och 2003) is 
used.  
4.1 Reducing the Noise of the Parser 
The reliability of the check-points mainly 
depends on the accuracy of the parsers. We can 
achieve high quality word level check-points 
with the state-of-the-art POS tagger (94% 
precision) and dictionaries of various purposes. 
For sentence level categories, the parser tags and 
manually compiled rules can also achieve 95% 
accuracy. For some kinds of categories at phrase 
level which parsers cannot produce high 
accuracy, we only select the check-points which 
can be identified by multiple parsers, that is, 
adopt the intersection of the parsers results. 
Table 4 shows the improvement brought by this 
approach. Column 1 and 2 shows the precision of 
6 major types of phrases in Stanford and 
Berkeley parser. Column 3 shows the precision 
of intersection and column 4 shows the reduction 
of the number of check-points when adopting the 
intersection results. The test corpus is a part of 
Category Check-point Reference 
New word ??? reserve funds 
Ambiguous word ?? building 
Predicate ? object 
phrase 
????? the building of  
reserve funds 
Subject-predicate 
phrase 
???? They opposed 
1123
Penn Chinese Treebank which is not contained in 
the training corpus of two statistical parsers. 
(Klein 2003).  
 
 Stf% Brk% Inter% Tpts redu% 
NP 87.37 86.03 95.83 17.06 
VP 87.34 82.87 95.23 19.68 
PP 90.60 88.56 96.00 11.50 
QP 98.12 92.90 99.21 6.31 
ADJP 91.95 90.87 96.41 10.20 
ADVP 95.21 94.25 92.64 3.92 
Table 4:  Precision of parsers and their intersec-
tion (Stf is Stanford, Brk is Berkelry) 
 
4.2 Alleviating the Impact of Alignment 
Noise 
Except for sentence level check-points whose 
references are the whole sentences and New 
Word, Idiom check-points whose references are 
extracted from dictionary, the quality of the ref-
erences are impacted by the alignment accuracy. 
To alleviate the noise of aligner we use the lexi-
cal dictionary to check the reliability of refer-
ences. Suppose c is a check-point, for each refer-
ence c.r of c we calculate the dictionary match-
ing degree DM(c.r) with the source side c.s of c: 
 
)1()).(
)).(,.(,1.0().( rcWordCnt
scDicrcCoCntMaxrcDM ?  
 
    Where Dic(x) is a word bag contains all words 
in the dictionary translations of each source word 
in x. CoCnt(x, y) is the count of the common 
words in x and y. WordCnt(x) is the count of 
words in x. Specially, if c.r is not obtained based 
on alignment DM(c.r) will be 1. Because the li-
mitation of dictionary, a zero DM score not al-
ways means the reference is completely wrong, 
so we force the DM score to be not less than a 
minimum value (e.g. 0.1). DM score will further 
be used in evaluation in section 5.  
    To better understand the reliability of the ref-
erences and explore whether increasing the num-
ber of check-points could also alleviate the im-
pact of noise, we built two check-point databases 
from a human-aligned corpus (with 60,000 sen-
tence pairs) and an automatically aligned corpus 
(using GIZA++) respectively and tested 10 dif-
ferent SMT systems4 with them. The Spearman 
correlation is calculated between two ranked lists 
of the 10 evaluation results against the two data-
                                                 
4 These systems are derived from an in-house phrase based 
SMT engine with different parameter sets. 
bases. A higher correlation score means that the 
impact of the mistakes in word alignment is 
weaker. The experiment is repeated on 6 subsets 
of the database with the size from 500 sentences 
to 16K sentences to check the impact of the cor-
pus size. 
    At system level, high correlations are found at 
different corpus sizes. At category level, correla-
tions are found to be low for some categories at 
small size and become higher at larger corpus 
size. The results indicate that the impact of the 
alignment quality can be ignored if the corpus 
size is at large scale. As the check-points can be 
extracted fully automatically, increasing the size 
of check-point database will not bring extra cost 
and efforts. Empirically, the proper scale is set to 
be 2000 or more sentences according to the Ta-
ble 6. 
 
Table 6: Impact of word alignment at different 
sizes of test corpus. 
5 Matching Check-Points for Evalua-
tion 
Evaluation can be carried out at multiple options: 
for certain linguistic category, a group of catego-
ries or entire taxonomy. For instance, in Chinese-
English translation task, if a MT developer 
would like to know the ability to translate idiom, 
then a number of parallel sentences containing 
idiom check-points are selected from the data-
base. Then the system translation sentences are 
matched to the references of the check-points of 
idioms.  
 500 1K 2K 4K 8K 16K 
Ambiguous 
word 
0.98 0.98 0.98 0.98 0.96 0.98 
Noun 0.93 0.99 0.99 0.89 0.8 0.86 
Verb 0.97 0.97 0.99 0.99 0.95 0.92 
Adjective 0.16 0.19 0.57 0.75 0.77 0.97 
Pronoun 0.96 1 0.93 0.99 0.97 0.99 
Adverb 0.38 0.77 0.8 0.96 0.72 0.84 
Preposition 0.56 0.86 0.9 0.9 0.97 0.96 
Quantifier 1 0.46 0.46 0.98 0.85 0.96 
Repetitive 
Word 
0.99 0.99 0.97 0.89 0.73 0.95 
Collocation 0.42 0.77 0.77 0.77 0.73 0.88 
Subject-
predicate 
phrase 
0.06 0.8 0.95 1 0.96 0.84 
Predicate-
object phrase 
0.84 0.96 0.78 0.7 0.78 0.88 
Preposition-
object phrase 
0.51 0.5 0.93 0.95 0.87 0.99 
Measure 
phrase 
0.91 0.67 0.95 0.95 0.87 0.97 
Location 
phrase 
0.62 0.54 0.55 0.55 0.85 0.89 
SYSTEM 0.95 0.95 0.98 0.99 0.97 0.98 
1124
To calculate the credit at different occasions of 
matching, similar to BLEU, we split each refer-
ence of a check-point into a set of n-grams and 
sum up the gains over all grams as the credit of 
this check-point. Especially, if the check-point is 
not consecutive we use a special token (e.g. ?*?) 
to represent a component which can be wildcard 
matched by any word sequence. We use the fol-
lowing examples to demonstrate the splitting and 
matching of grams.  
 
?  Consecutive check-point: 
    Check- point: ??? 
    Reference: playing a drum 
    Candidate translation:  He is playing a drum.  
    Matched n-grams: playing; a; drum; playing a; 
a drum; playing a drum  
 
?   Not consecutive check-point: 
    Check- point: ??*?   
    Reference: They*playing   
    Candidate translation: They are playing cop 
per drum. 
    Matched n-grams: They; playing; They * play-
ing 
    Additionally, to match word inflections, 3 dif-
ferent options of matching granularity are de-
fined as follows.  
?  Normal: matching with exact form. 
?  Lower-case: matching with lowercase. 
?  Stem: matching with the stem of the word. 
 
    For a check-point c and references set R of c, 
we select the r* in R which matches the transla-
tion best based on formula (2).  
 
 
 
    
 
When we calculate the recall of a set of check-
points C (C can be a single check-point, a cate-
gory or a category group), r* of each check-point 
c in C are merged into one reference set R* and 
the recall of C is obtained using formula (3) on 
R*. 
 
 
 
 
 
 
A penalty is also introduced to punish the re-
dundancy of candidate sentences, where length(T) 
is the average length of all translation sentences 
and length(R) is the average length of all refer-
ence sentences. 
 
 
 
 
 
Then, the final score of C will be: 
 
)5()(Re)( PenaltyCCScore ??
 
6 Experiments on MT System Diagnos-
es 
In this section, to demonstrate the ability of our 
approach in the diagnoses of MT systems, we 
apply diagnostic evaluation to 3 statistical MT 
(SMT) systems and a rule-based MT (RMT) sys-
tem respectively. We compare two SMT systems 
to understand the strength and shortcoming of 
each of them, and also compare a SMT system 
with the RMT system. The test corpus is NIST05 
test data with 54852 check-points. 
    First SMT system (system A) is an implemen-
tation of classical phrase based SMT. The second 
SMT system (system B) shares the same decoder 
with system A and introduces a preprocess to 
reorder the long phrases in source sentences ac-
cording to the syntax structure before decoding 
(Chiho Li et al, 2007). The third SMT system 
(system C) is a popular internet service and the 
RMT system (system D) is a popular commercial 
system.  
    In the first experiment, we diagnose the sys-
tem A and B and compare the results as shown in 
table 7. When evaluated using BLEU, system B 
achieved a 0.005 points increase on top of system 
A which is not a very significant difference. The 
diagnostic results in table 7 provide much richer 
information. The results indicate that two sys-
tems perform similar at the word level categories 
while at all phrase level categories, system B 
performs better. This result reflects the benefit 
from the reordering of complex phrases in sys-
tem B. Paired t-statistic score for each pair of 
category scores is also calculated by repeating 
the evaluation on a random copy of the test set 
with replacement (Koehn 2004). An absolute 
score beyond 2.17 of paired t-statistic means the 
difference of the samples is statistically signifi-
cant (above 95%). Table 8 and 9 show an in-
stance of the check-point and its evaluation in 
this experiment. 
)2()
)'(
)(
)((maxarg
'
*
?
?
??
??
? ?
?
??
rgramn
rgramn
Rr gramnCount
gramnMatch
rDMr
)4(
1
)()(
)(
)(
??
?
?
? ?
?
Otherwise
RlengthTlengthif
Tlength
Rlength
Penalty
)3(
))'()((
))()((
)Re(
*
*
' ''
'? ?
? ?
? ??
? ??
??
??
?
Rr rgran
Rr rgramn
gramnCountrDM
gramnMatchrDM
C
1125
 System A System B T score 
WORDs 
Idiom 0.1933 0.2370 13.38 
Adjective 0.5836 0.5577 -17.43 
Pronoun 0.7566 0.7344 -13.49 
Adverb 0.5365 0.5433 7.11 
Preposition 0.6529 0.6456 -6.21 
Repetitive word 0.3363 0.3958 9.86 
PHRASEs 
Subject-predicate 0.5117 0.5206 7.36 
Predicate-object 0.4041 0.4180 15.52 
Predicate-complement 0.4409 0.5125 9.51 
Measure phrase 0.5030 0.5092 3.56 
Location phrase 0.5245 0.5338 2.83 
GROUPs 
WORDs 0.4839 0.4855 2.03 
PHRASEs 0.4744 0.4964 13.97 
SYSTEM (Linguistic) 0.4263 0.4370 16.50 
SYSTEM (BLEU) 0.3564 0.3614 7.91 
Table 7: Diagnose of SMT systems 
 
Source Sentence ????????????????
??????? 
Category Preposition_Object_Phrase 
Check-Point ? ? ?? 
Reference 1 in this country  DM = 0.5 
Reference 2 in his country  DM = 0.5 
System A Translation but the prime minister of thailand Dex-
in vowed to continue in domestic the 
search. 
System B Translation but the prime minister of thailand Dex-
in vowed to continue the search in his 
country. 
Table 8: An instance of the check-point. 
 
 System A System B 
Ref 1: Match/Total 1/6 2/6 
Ref 2: Match/Total 1/6 6/6 
Score 0.17 1 
Table 9: N-gram matching rate and scores. 
 
Table 10: Diagnose of SMT and RMT. 
 
In the second experiment, we diagnose system 
C and D and compare the results. The BLEU 
score of system C is 0.3005 and system D is 
0.2606. Table 10 shows the diagnostic results on 
categories with significant differences. Scores 
calculated with 3 matching options described in 
section 5 are given (?Lower? means Lowercase. 
The scores are listed in the form ?SMT 
score/RMT score?). The diagnostic results indi-
cate that system C performs better on most cate-
gories than system D, but system D performs 
better on categories like idiom, pronoun and pre-
position. This result reveals a key difference be-
tween two types of MT systems: the SMT works 
well on the open categories that can be handled 
by context, while the RMT works well on closed 
categories which are easily translated by linguis-
tic rules. 
    As the results of two experiments demonstrate, 
the diagnostic evaluation provides rich informa-
tion of the capability of translating various im-
portant linguistic categories beyond a single sys-
tem score. It successfully distinguishes the spe-
cific difference between the MT systems whose 
system level performance is similar. It can also 
diagnose the MT system with different architec-
tures. Diagnostic evaluation tells the developers 
about the direction to improve the system. Along 
with the scores of categories, the diagnostic 
evaluation provides the system translation and 
references at every check-point so that the devel-
opers can trace and understand about how the 
MT system works on every single instance. 
7 Experiments on Ranking MT Systems 
Offering a general evaluation at system level is 
the major goal of state-of-the-art evaluation me-
thods including widely accepted n-gram metrics. 
The absence of linguistic knowledge in BLEU 
motivated many work to integrate linguistic fea-
tures into evaluation metric. In (Yang 2007), the 
evaluation of SMT systems is alternately formu-
lated as a ranking problem. Different linguistic 
features are combined with BLEU such as 
matching rate of dependency relations of transla-
tion candidates against the reference sentences. 
The experiments demonstrate that the dependen-
cy matching rate feature can increase the ranking 
accuracy in some cases. Compared to dependen-
cy structure, the linguistic categories in our ap-
proach showcase more extensive features. It 
would be interesting to see whether the linguistic 
categories can be used to further improve the 
ranking of SMT systems.  
    In experiments, we use the scores of linguistic 
categories, dependency matching rate, scores of 
BLEU and other popular metrics as ranking fea-
tures of MT systems and trained by Ranking 
SVM of SVMlight (Joachims, 1998). We per-
formed the ranking experiments on ACL 2005 
workshop data, ranking 7 MT translations with 
three-fold cross-validation both on sentence level 
and document level. The Spearman score is used 
Type Normal Lower Stem 
Ambiguous word 0.49/0.42 0.50/0.42 0.53/0.46 
New word 0.13/0.13 0.37/0.32 0.42/0.35 
Idiom 0.43/0.66 0.46/0.67 0.51/0.71 
Pronoun 0.60/0.68 0.69/0.75 0.66/0.75 
Preposition 0.38/0.42 0.42/0.45 0.43/0.46 
Collocation 0.66/0.54 0.66/0.55 0.70/0.56 
Subject-predicate 
phrase 
0.46/0.30 0.51/0.36 0.58/0.42 
Predicate-object 
phrase 
0.37/0.25 0.37/0.26 0.47/0.29 
Compound sentence 0.22/0.16 0.23/0.16 0.23/0.17 
1126
to calculate the correlation with human assess-
ments. Table 11 and 12 show the results of the 
different feature sets on sentence level and doc-
ument level respectively. 
As shown in experiment results linguistic cat-
egories (LC), when used alone, are better related 
with human assessments than BLEU and GTM. 
When combined with the baseline metrics 
(BLEU & NIST), LC scores further improve the 
correlation score, better than dependence match-
ing rate (DP). LC scores are obtained by match-
ing the exact form of the words as ME-
TEOR(exact) does. NIST+LC combination score 
is better than METEOR(exact) at sentence and 
document level, and also better than ME-
TEOR(exact&syn) (syn means wn_synonymy 
module in METEOR) at document level. This 
results indicate the ability of linguistic features in 
improving the performance of ranking task. 
 
 Mean Correlation 
BLEU 4 0.245 
NIST 5 0.307 
GTM (e=2) 0.251 
METEOR(exact) 0.306 
METEOR(exact&syn) 0.327 
DP 0.246 
LC 0.263 
BLEU+DP 0.270 
BLEU+ LC 0.288 
BLEU+ DP +LC 0.307 
NIST+ LC 0.322 
NIST+ DP +LC 0.333 
 Table11: Sentence level ranking (DP means 
dependency and LC means linguistic categories)  
 
 Mean Correlation 
BLEU 4 0.305 
NIST 5 0.373 
GTM (e=2) 0.327 
METEOR(exact) 0.363 
METEOR(exact&syn) 0.394 
DP 0.323 
LC 0.369 
BLEU+DP 0.325 
BLEU+ LC 0.387 
BLEU+ DP +LC 0.332 
NIST+ LC 0.409 
NIST+ DP +LC 0.359 
Table 12: Document level ranking 
8 Comparison with Related Work 
This work is inspired by (Yu, 1993) with many 
extensions. (Yu, 1993) proposed MTE evaluation 
system based on check-points for English-
Chinese machine translation systems with human 
craft linguistic taxonomy including 3,200 pairs of 
sentences containing 6 classes of check-points. 
Their check-points were manually constructed by 
human experts, therefore it will be costly to build 
new test corpus while the check-points in our 
approach are constructed automatically. Another 
limitation of their work is that only binary score 
is used for credits while we use n-gram matching 
rate which provides a broader coverage of differ-
ent levels of matching.   
    There are many recent work motivated by n-
gram based approach. (Callison-Burch et al, 
2006) criticized the inadequate accuracy of eval-
uation at the sentence level. (Lin and Och, 2004) 
used longest common subsequence and skip-
bigram statistics. (Banerjee and Lavie, 2005) cal-
culated the scores by matching the unigrams on 
the surface forms, stemmed forms and senses. 
(Liu et al, 2005) used syntactic features and un-
labeled head-modifier dependencies to evaluate 
MT quality, outperforming BLEU on sentence 
level correlations with human judgment. (Gime-
nez and Marquez, 2007) showed that linguistic 
features at more abstract levels such as depen-
dency relation may provide more reliable system 
rankings. (Yang et al, 2007) formulates MT 
evaluation as a ranking problems leading to 
greater correlation with human assessment at the 
sentence level.  
There are many differences between these n-
gram based methods and our approach. In n-
gram approach, a sentence is viewed as a collec-
tion of n-grams with different length without dif-
ferentiating the specific linguistic phenomena. In 
our approach, a sentence is viewed as a collec-
tion of check-points with different types and 
depth, conforming to a clear linguistic taxonomy. 
Furthermore, in n-gram approach, only one gen-
eral score at the system level is provided which 
make it not suitable for system diagnoses, while 
in our approach we can give scores of linguistic 
categories and provide much richer information 
to help developers to find the concrete strength 
and flaws of the system, in addition to the gener-
al score. The n-gram based metric is not very 
effective when comparing the systems with dif-
ferent architectures or systems with similar gen-
eral score, while our approach is more effective 
in both cases by digging into the multiple lin-
guistic levels and disclosing the latent differenc-
es of the systems. 
9 Conclusion and Future Work 
This paper presents an automatically diagnostic 
evaluation methods on MT based on linguistic 
check-points automatically constructed. In con-
trast with the metrics which only give a general 
score, our evaluation system can give developers 
1127
feedback about the faults and strength of an MT 
system regarding specific linguistic category or 
category group. Different with the existing work 
based on check-points, our work presents an ap-
proach to automatically generate the check-point 
database. We show that although there is some 
noise brought from word alignment and parsing, 
we can effectively alleviate the problem by refin-
ing the parser results, weighting the reference 
with confidence score and providing large quan-
tity of check-points.  
    The experiments demonstrate that this method 
can uncover the specific difference between MT 
systems with similar architectures and different 
architectures. It is also demonstrated that the lin-
guistic check-points can be used as new features 
to improve the ranking task of MT systems.   
    Although we present the diagnostic evaluation 
method with Chinese-English language pair, our 
approach can be applied to other language pair if 
syntax parser and word aligner are available. 
    The taxonomy used in current proposal is 
based on the human-made linguistic system. An 
interesting problem to be explored in the future is 
whether the taxonomy could be constructed au-
tomatically from the parsing results.  
References 
Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization 2005. 
Chris Callison-Burch, Miles Osborne, Philipp Koehn. 
2006. Re-evaluating the Role of Bleu in Machine 
Translation Research. In Proceedings of the Euro-
pean Chapter of the ACL 2006. 
Martin Chodorow, Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors, 
In 1st Meeting of the North America Chapter of the 
ACL, pp.140?147, 2000. 
Thorsten Joachims. 1998. Making Large-scale Sup-
port Vector Machine Learning Practical, In B. 
Scholkopf, C. Burges, A. Smola. Advances in Ker-
nel Methods: Support VectorMachines, MIT Press, 
Cambridge, MA, December. 
Jesus Gimenez and Llis Marquez. 2007. Linguistic 
features for automatic evaluation of heterogeneous 
MT systems, Workshop of statistical machine trans-
lation in conjunction with 45th ACL, 2007. 
Dan Klein, Christopher Manning. 2003. Accurate 
Unlexicalized Parsing, Proceedings of the 41th 
Meeting of the ACL, pp. 423-430. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of the 
EMNLP, Barcelona, Spain. 
Chiho Li, Minghui Li, Dongdong Zhang, Mu Li, 
Ming Zhou, Yi Guan. 2007. A Probabilistic Ap-
proach to Syntax-based Reor-dering for SMT. In 
Proceedings of the 45th  ACL, 2007. 
Chin-Yew Lin and Franz Josef Och. 2004. Automatic 
evaluation of machine translation quality using 
longest common subsequence and skip-bigram sta-
tistics. In Proceedings of the 42th ACL 2004.  
Ding Liu, Daniel Gildea. 2005. Syntactic Features for 
Evaluation of Machine Translation, ACL Work-
shop on Intrinsic and Extrinsic Evaluation Meas-
ures for Machine Translation and/or Summariza-
tion. 
Shuxin Liu. 2002. Linguistics of Contemporary Chi-
nese Language (in Chinese), Advanced Education 
Publisher. 
Jiping Lv. 2000. Foundation of Mandarin Grammar 
(in Chinese), Shangwu Publisher. 
Franz Josef Och, Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Mod-
els, Computational Linguistics, volume 29, number 
1, pp. 19-51 March 2003. 
Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation, In Proceedings of the 
ACL 2002. 
Shiwen Yu. 1993. Automatic evaluation of output 
quality for machine translation systems, In Pro-
ceedings of the evaluators? forum, April 21-24, 
1991, Les Rasses, Vaud, 1993.  
Yang Ye, Ming Zhou, Chinyew Lin. 2007. Sentence 
level machine translation evaluation as a ranking 
problem: one step aside from BLEU, In Workshop 
of statistical machine translation, in conjunction 
with 45th ACL, 2007. 
Ming Zhou. 2000, A Block-Based Robust Dependency 
Parser for Unrestricted Chinese Text. Proceedings 
of Second Chinese Language Processing Workshop, 
2000, held in conjunction with ACL, 2000.  
 
1128
A Hybrid Chinese Language Model based on a Combination of 
Ontology with Statistical Method 
Dequan Zheng, Tiejun Zhao, Sheng Li and Hao Yu 
MOE-MS Key Laboratory of Natural Language Proceessing and Speech 
Harbin Institute of Technology 
Harbin, China, 150001 
{dqzheng, tjzhao, lisheng, yu}@mtlab.hit.edu.cn 
  
Abstract 
In this paper, we present a hybrid Chi-
nese language model based on a com-
bination of ontology with statistical 
method. In this study, we determined 
the structure of such a Chinese lan-
guage model. This structure is firstly 
comprised of an ontology description 
framework for Chinese words and a 
representation of Chinese lingual on-
tology knowledge. Subsequently, a 
Chinese lingual ontology knowledge 
bank is automatically acquired by de-
termining, for each word, its co-
occurrence with semantic, pragmatics, 
and syntactic information from the 
training corpus and the usage of Chi-
nese words will be gotten from lingual 
ontology knowledge bank for a actual 
document. To evaluate the performance 
of this language model, we completed 
two groups of experiments on texts re-
ordering for Chinese information re-
trieval and texts similarity computing. 
Compared with previous works, the 
proposed method improved the preci-
sion of nature language processing. 
1 Introduction 
Language modeling is a description of natural 
language and a good language model can help to 
improve the performance of the natural language 
processing. 
Traditional statistical language model 
(SLM) is fundamental to many natural language 
applications like automatic speech recognitionP[1]P, 
statistical machine translationP[2]P, and information 
retrievalP[3]P. Different statistical models have 
been proposed in the past, but n-gram models (in 
particular, bi-gram and tri-gram models) still 
dominate SLM research. After that, other ap-
proaches were put forward, such as the 
combination of statistical-based approach and 
rule-based approachP[4,5]P, self-adaptive language 
modelsP[6]P, topic-based model P[7]P and cache-based 
model P[8]P. But when the models are applied, the 
crucial disadvantages are that they can?t repre-
sent and process the semantic information of a 
natural language, so they can?t adapt well to the 
environment with changeful topics. 
Ontology was recognized as a conceptual 
modeling tool, which can descript an informa-
tion system in the semantic level and knowledge 
level. After it was first introduced in the field of 
Artificial IntelligenceP[9]P, it was closed combined 
with natural language processing and are widely 
applied in many field such as knowledge engi-
neering, digital library, information retrieval, 
semantic Web, and etc.  
In this paper, combining with the character-
istic of ontology and statistical method, we pre-
sent a hybrid Chinese language model. In this 
study, we determined the structure of Chinese 
language model and evaluate its performance 
with two groups of experiments on texts reorder-
ing for Chinese information retrieval and texts 
similarity computing. 
The rest of this paper is organized as fol-
lows. In section 2, we describe the Chinese lan-
guage model. In section 3, we evaluate the 
language model by several experiments about 
natural language processing. In section 4, we 
present the conclusion and some future work. 
2 The language model description 
Traditional SLM is make use to estimate the 
likelihood (or probability) of a word string, in 
13
this study, we determined the structure of Chi-
nese language model, first, we gave the ontology 
description framework of Chinese word and the 
representation of Chinese lingual ontology 
knowledge, and then, automatically acquired the 
usage of a word with its co-occurrence of con-
text in using semantic, pragmatics, syntactic, etc 
from the corpus to act as Chinese lingual ontol-
ogy knowledge bank. In actual document, the 
usage of lingual knowledge will be gotten from 
lingual ontology knowledge bank. 
2.1   Ontology description framework 
Traditional ontology mainly emphasizes the 
interrelations between essential concept, domain 
ontology is a public concept set of this do-
main P[10]P. We make use of this to present Chinese 
lingual ontology knowledge bank. 
In practical application, ontology can be 
figured in many waysP[11]P, natural languages, 
frameworks, semantic webs, logical languages, 
etc. Presently, popular models, such as Ontolin-
gua, CycL and Loom, are all based on logical 
language. Though logical language has a strong 
expression, its deduction is very difficult to lin-
gual knowledge. Semantic web and natural lan-
guage are non-formal, which have disadvantages 
in grammar and expression. 
For a Chinese word, we provided a frame-
work structure that can be understood by com-
puter combined with WordNet, HowNet and 
Chinese Thesaurus. This framework includes a 
Chinese word in concept, part of speech (POS), 
semantic, synonyms, English translation. Fig-
ure1 shows the ontology description framework 
of a Chinese word. 
 
 
 
 
 
 
 
Fig. 1. Ontology description framework 
2.2   Lingual ontology knowledge representation 
A word is the basic factor that composes the 
natural language, to acquire lingual ontology 
knowledge, we need to know POS, means and 
semantic of a word in a sentence. For example, 
for a Chinese sentence, the POS, means and 
Semantic label of ??? in HowNet are shown in 
table 1. For the Chinese sentence ??????
??????, after words segmented, POS tag-
ging and semantic tagging, we get a characteris-
tic string. They are shown in table 2. 
Table 1. the usage of ??? in Chinese sentence 
Chinese Sentence POS Means Semantic Num 
??? Verb Weave 525(weave|?? ) 
?? ? Verb Buy 348(buy|? ) 
Table 2. Segmentation, POS and Semantic tagging 
Items Results (???? acts as keyword) 
Chinese sentence ?????????? 
Words segmenta-
tion 
??  ??  ?  ??  ??  ? 
POS tagging ?? nd/ ?? Keyword/? vg/ ?? nd/ ??
vg/ ?wj/ 
Semantic label 
tagging 
?? nd/021243 ?? Keyword/070366?  
vg/017545 ?? nd/021243 ?? vg/092317 ?
wj/-1 
Characteristic string nd/021243 ?? Keyword/070366  vg/017545 
nd/021243 vg/092317 
Explanation of 
Semantic label 
021243 represents ????, 070366 represents 
???, 092317 represents ? ?? ?, ?-1? repre-
sents not to be defined or exist this semantic in 
HowNet. 
 
In order to use and express easily, we gave 
a description for ontology knowledge of every 
Chinese word, which learned from corpus, to be 
shown as expression 1. All of them composed 
the Chinese lingual ontology knowledge bank. 
( ) ( ) ( )???????? == UU
n
r
rrr
m
l
lll CLPOSSemCLPOSSemontologyKeyWord
11
,,,,,,,,
 
Where, KeyWord(ontology) is the ontology 
description of a Chinese word, ( )iii CLPOSSem ,,,  is 
the left co-occurrence knowledge of a Chinese 
word got from its context and ( )iii CLPOSSem ,,,  is 
the right co-occurrence knowledge. Symbol 
?? ? represents the aggregate of all the co-
occurrence with the KeyWord. ( )iii CLPOSSem ,,,  denotes the multi-grams 
from context of a Chinese word, which is com-
posed of semantic information SemBi B, part of 
speech POS Bi B, the position L from the word 
KeyWord to its co-occurrence, the average dis-
tance lC  from the word to its left (or right) i-th 
word. 
( )( )LPOSSemKeyword ii ,,,  denotes a seman-
tic relation pair between the keyword and its co-
occurrence in current context. 
The multi-grams of a Chinese word in con-
text, including the co-occurrence and their posi-
tion will act as the composition of lingual 
ontology knowledge too. In figure 2, the charac-
teristic string WB1 B, WB2 B, ?, WBi B represents POS and 
semantic label, Keyword is keyword itself, l or r 
Keywords  <?>
Concept             <?> 
Part of Speech  <?> 
Ontology   Semantic            <?> 
                    Synonym           <?> 
E-translation     <?> 
14
is the position of word that is left or right co-
occurrence with keyword. 
 
Fig. 2. Co-occurrence and the position information 
2.3   Lingual ontology knowledge acquisition 
According to the course that human being ac-
quires and accumulates knowledge, we propose 
a measurable description for Chinese lingual 
ontology knowledge through automatically 
learning typical corpus. In this approach, we will 
acquire the usage of a Chinese word in semantic, 
pragmatic and syntactic in all documents. We 
combine with the multi-grams in context includ-
ing its co-occurrence, POS, semantic, synonym, 
position. In practical application, we will proc-
ess every Chinese keyword that has the same 
grammar expression, semantic representation 
and syntactic structure with Chinese lingual on-
tology knowledge bank. 
2.3.1   Algorithm of automatic acquisition 
Step 1: corpus pre-processing.  
For any Chinese document DBi B in the docu-
ment set {D}, we treat the sentence that includes 
keyword as a processing unit. First, we have a 
Chinese word segmentation, POS tagging, Se-
mantic label tagging based on HowNet, and then, 
confirm a word to act as the keyword for acquir-
ing its co-occurrence knowledge. We wipe off 
the word that can do little contribution to the 
lingual ontology knowledge, such as preposition, 
conjunction, auxiliary word and etc. 
Step 2: Unify the keyword. 
Making use of the ontology description of 
Chinese word, we make the synonym into uni-
form one. 
Step 3: Calculate the co-occurrence distance. 
In our proposal, first, we treat the sentence 
that includes keyword as a processing unit and 
make POS tagging, semantic label tagging, then, 
we get Characteristic string. We take the key-
word as the center, define the left and right dis-
tance factor B Bl B and B Br B to be shown at formula 1. 
ml
B
??
???
??
?
=
2
11
2
11               
nr
B
??
???
??
?
=
2
11
2
11     (1) 
Where, m and n represent the left and right 
number of word that centered with the keyword. 
In this way, we try to get the language intuition, 
in a word, if the co-occurrence is nearer to the 
keyword, we will get more the co-occurrence 
distant. Final, we respectively get the left-side 
and right-side co-occurrence distant from key-
word to its co-occurrence to be shown as for-
mula 2. 
l
i
li BC
1
2
1 ???
???
?=  (i=1,?,m) 
r
j
rj BC
1
2
1 ???
???
?=  (j=1,?,n)       (2) 
Step4: Calculate the average co-occurrence 
distance. 
For a keyword, in the current sentence of 
document DBi,B we regard the keyword and its co-
occurrence (SemBi B, POS Bi B, L) as semantic relation 
pair, and CBjB is their co-occurrence distance. We 
calculate the average of CBjB that appear in corpus 
and act as the average co-occurrence distance 
lC  
between the keyword and its co-occurrence 
(SemBi B, POS Bi B, L). 
When all of documents are learned, all of 
keyword and their co-occurrence information ( )iii CLPOSSem ,,,  compose the Chinese lingual 
ontology knowledge bank. 
Step 5: Rebuild the index. 
In order to improve the processing speed, 
for acquired lingual ontology knowledge bank, 
we first build an index according to Chinese 
word, and then, we respectively make a sorting 
according to the semantic label SemBi B for every 
Chinese word. 
2.3.2 Lingual ontology knowledge application 
In practical application, we will respectively get 
different evaluation of a document from the lin-
gual ontology knowledge bank. For the natural 
language processing, e.g. documents similarity 
computing, text re-ranking for information re-
trieval, information filtering, the general proc-
essing is as follow. 
Step 1: Pre-processing and unify the key-
word. 
The processing is the same as Step 1 and 
Step 2 in section 2.3.1. 
Step 2: Fetch the average co-occurrence 
distance from lingual ontology knowledge bank. 
We regard a sentence including keyword in 
document D as a processing unit. First, we make 
POS tagging, semantic label tagging and get 
Characteristic string, and then, for every key-
word, if it has the same semantic relation pair as 
lingual ontology knowledge bank, i.e. the key-
word and its co-occurrence (SemBi B, POS Bi B, L) in 
practical document is the same one as lingual 
15
ontology knowledge bank, we add up all the 
average co-occurrence distance 
lC  from Chinese 
lingual ontology knowledge bank acquired in 
section 2.3.1. 
Step 3: Get the evaluation value of a docu-
ment. 
Repeat Step 2 until all keywords be proc-
essed and the accumulation of the average co-
occurrence distance 
lC will act as the evaluation 
value of current document. 
3 Evaluation of language model 
We completed two groups of experiments on 
text re-ranking for information retrieval, text 
similarity computing to verify the performance 
of lingual ontology knowledge. 
3.1   Texts reordering 
Information retrieval is used to retrieve relevant 
documents from a large document set for a user 
query, where the user query can be a simple de-
scription by natural. As a general rule, users 
hope more to acquire relevant information from 
the top ranking documents, so they concern 
more on the precision of top ranking documents 
than the recall. 
We use the Chinese document set CIRB011 
(132,173 documents) and CIRB020 (249,508 
documents) from NTCIR3 CLIR dataset and 
select 36 topics from 50 search topics (see 
http://research.nii.ac.jp/ntcir-ws3/work-en.html 
for more information) to evaluate our method. 
We use the same method to retrieve documents 
mentioned by Yang LingpengP[12]P, i.e. we use 
vector space model to retrieve documents, use 
cosine to calculate the similarity between docu-
ment and user query. We respectively use bi-
grams and words as indexing unitsP[13,14]P, the av-
erage precision of top N ranking documents acts 
as the normal results. In this paper, we used a 
Chinese dictionary that contains about 85,000 
items to segment Chinese document and query. 
To measure the effectiveness of informa-
tion retrieval, we use the same two kinds of 
relevant measures: relax-relevant and rigid-
relevantP[14,15]P. A document is rigid-relevant if it?s 
highly relevant or relevant with user query, and 
a document is relax-relevant if it is high relevant 
or relevant or partially relevant with user query. 
We also use PreAt10 and PreAt100 to represent 
the precision of top 10 ranking documents and 
top 100 ranking documents. 
3.1.1   Strategy of texts reordering 
First, we get some keywords to every topic by 
query description. For example, 
Title: ????? (The birth of a cloned 
calf) 
Description: ????????????
?????????????? (Find Arti-
cles relating to the birth of cloned calves using 
the technique called somatic cell nuclear transfer) 
We extract ???, ???, ??, ???
?? as feature word in this topic. 
Second, acquire lingual ontology knowl-
edge every topic by their feature words. In this 
proposal, we arrange 300 Chinese texts of this 
topic as learning corpus to get lingual ontology 
knowledge bank. 
Third, get the evaluation value of every text 
about this topic, i.e. respectively add up all the 
average co-occurrence distance lC  to the same 
semantic relation pairs in every text from lingual 
ontology knowledge bank.  
If a text has several keywords, repeat step3 
to acquire every evaluation value to these key-
words, and then, add up each evaluation value to 
act as the text evaluation value. 
Final, we reorder the initial retrieval texts 
according to the every text evaluation value of 
every topic. 
3.1.2   Experimental results and analysis 
We calculate the evaluation value of every text 
in each topic to reorder the initial relevant 
documents. 
Table 3 lists the normal results and our re-
sults based on bi-gram indexing, our results are 
acquired based on Chinese lingual ontology 
knowledge to enhance the effectiveness. 
PreAt10 is the average precision of 36 topics in 
precision of top 10 ranking documents, while 
PreAt100 is top 100 ranking documents. 
Table 4 lists the normal results and our re-
sults based on word indexing. Ratio displays an 
increase ratio of our result compared with nor-
mal result. 
Table 3. Precision (bi-gram as indexing unit) 
Items Normal Our method Ratio 
PreAt10 (Relax) 0.3704 0.4389 18.49% 
PreAt100 (Relax) 0.1941 0.2239 15.35% 
PreA10 (Rigid) 0.2625 0.3083 17.45% 
PreAt100 (Rigid) 0.1312 0.1478 12.65% 
16
Table 4. Precision (word as indexing unit) 
Items Normal Our method Ratio 
PreAt10 (Relax) 0.3829 0.4481 17.03% 
PreAt100 (Relax) 0.2022 0.2306 14.05% 
PreAt10 (Rigid) 0.2745 0.3169 15.45% 
PreAt100 (Rigid) 0.1405 0.1573 11.96% 
In table 3, it is shown that compared with 
bi-grams as indexing units, our method respec-
tively increases 18.49% in relax relevant meas-
ure and 17.45% in rigid in PreAt10. In PreAt100 
level, our method respectively increases 15.35% 
in relax relevant and 12.65% in rigid relevant 
measure. Figure 3 displays the PreAt10 values 
of each topic in relax relevant measure based on 
bi-gram indexing where one denotes the preci-
sion enhanced with our method, another denotes 
the normal precision. It is shown the precision of 
each topic is all improved by using our method. 
 
Fig. 3. PreAt10 of all topics in relax judgment 
In table 4, using words as indexing units, 
our method respectively increases 17.03% in 
relax relevant measure and 15.45% in rigid in 
PreAt10. In PreAt100 level, our method respec-
tively increases 14.05% in relax relevant meas-
ure and 11.96% in rigid. 
In our experiments, compared with the two 
Chinese indexing units: bi-gram and words, our 
method increases the average precision of all 
queries in top 10 and top 100 measure levels for 
about 17.1% and 13.5%. What lies behind our 
method is that for each topic, we manually select 
some Chinese corpus to acquire the lingual on-
tology knowledge, and can help us to focus on 
relevant documents. Our experiment also shows 
improper extract and corpus may decrease the 
precision of top documents. So our method de-
pends on right keywords in texts, queries and the 
corpus. 
3.2   Text similarity computing 
Text similarity is a measure for the matching 
degree between two or more texts, the more high 
the similarity degree is, the more the meaning of 
text expressing is closer, vice versa. Some pro-
posal methods include Vector Space Model P[16]P, 
Ontology-based P[17]P, Distributional Semantics 
model P[18]P. 
3.2.1   Strategy of similarity computation 
First, for two Chinese texts DBiB and DBjB, we re-
spectively extract k same feature words, if the 
same feature words in the two texts is less than k, 
we don?t compare their similarity. 
Second, acquire lingual ontology knowl-
edge every text by their feature words. 
Third, get the evaluation value of every text, 
i.e. respectively add up all the average co-
occurrence distance 
lC  to the same semantic 
relation pairs in two texts. 
Final, compute the similarity ratio of every 
two text DBi B and DBj B. The similarity ratio equals to 
the ratio of the similarity evaluation value of 
text DBi B and DBj B, if the ratio is in the threshold ?, 
then we think that text DBi B is similar to text DBj B. 
3.2.2   Experimental results and analysis 
We download four classes of text for testing 
from Sina, Yahoo, Sohu and Tom, which in-
clude 71 current affairs news, 68 sports news, 69 
IT news, 74 education news. 
For the test of current affairs texts, accord-
ing to the strategy of similarity computation, we 
choose five words as feature word. They are ??
?, ??, ??, ??, ???. In the texts, the 
word ???, ??? are all replaced by word ??
?? and other classes are similar. The testing 
result is shown in table 5.  
Table 5. Testing results for text similarity 
0.95<?<1.05 0.85<?<1.15 Items 
Precision Recall FB1 B-measure Precision Recall FB1 B-measure 
Current affairs news 97.14% 97.14% 97.14% 94.60% 100% 97.23% 
Sports News 88.57% 91.18% 89.86% 84.62% 97.06% 90.41% 
IT news 93.75% 96.77% 95.24% 91.18% 100% 95.39% 
Education news 94.74% 97.30% 96.00% 90.24 100% 94.87% 
General results 93.57% 95.62% 94.58% 90.07% 99.27% 94.42% 
 
17
We analyzed all the experimental results to 
find that the results for current affairs texts are 
the best, while the sports texts are lower than 
others. We think it is mainly because some 
sports terms are unprofessional for the lower 
sports texts recognition, such as ????, ??, 
???. Other feature words are more fixed and 
more concentrated. 
4 Conclusion 
In this paper, we presented a hybrid Chinese 
language model based on a combination of on-
tology with statistical method. We discuss the 
modeling and evaluate its performance. In the 
test about texts reordering, our experiences show 
that our method can increase the performance of 
Chinese information retrieval about 17.1% and 
13.5% at top 10 and top 100 documents measure 
level. In another test about texts similarity com-
puting, F1-measure is above 95%. 
On the other hand, in the current disposal 
of our information processing, we only make 
use of some characteristics ontology and use 
some co-occurrence information, such as seman-
tics, POS, context, position, distance, and etc. 
For the further research and experiment, we will 
be on the following: (1) Research on the charac-
teristics of relations between semantics and 
combine with some mature natural language 
processing techniques. (2) Research traditional 
ontology representation to keep up with interna-
tional stand. (3) Apply our key techniques to 
English information retrieval and cross-lingual 
information retrieval systems and study a 
general approach. 
References 
1. Jelinek, F. 1990. Self-organized language model-
ing for speech recognition. In Readings in Speech 
Recognition,A. Waibel and K. F. Lee, eds. Mor-
gan-Kaufmann, San Mateo, CA,1990, 450-506. 
2.  Brown, P., Pietra, S. D., Pietra, V. D., and Mercer, 
R. 1993. The mathematics of statistical machine-
translation: Parameter estimation. Computational 
Linguistics 19, 2 (1993), 269-311. 
3.  Croft, W. B. and Lafferty, J. (EDS.) 2003. Lan-
guage Modeling for Information Retrieval. Kluwer 
Academic,Amsterdam. 
4. Wang Xiaolong, Wang Kaizhu. 1994. Speech in-
put by sentence, Chinese Journal of Computers, 
17(2): 96-103 
5.  Zhou Ming, Huang Changning, Zhang Min, Bai 
Shuanhu, and Wu Sheng. 1994. A Chinese parsing 
model based on corpus, rules and statistics, Com-
puter research and development, 31(2):40-49 
6. R DeMori, M Federico. 1999. Language model 
adaptation. In: Keith Pointing ed. Computational 
Models of Speech Pattern Processing. NATO ASI 
Series. Berlin: Springer Verlag, 102-111 
7. R Kuhn , R D Mori. 1990. A cache-based natural 
language model for speech reproduction. IEEE 
Trans on Pattern Analysis and Machine Intelli-
gence, PAM2-12(6), 570-583 
8.  Daniel Gildea, Thomas Hofmannl. 1999. Topic-
based language models using EM1. In : Proceed-
ing of the 6th European Conf on Speech Commu-
nication and Technology, Budapest, Hungary: 
ESCA, 2167-2170 
9.  Neches R., Fikes R., Finin T., Gruber T., Patil R., 
Senator T., and Swartout W. R.. 1991. Enabling 
Technology for Knowledge Sharing. AI Magazine, 
12(3) :16~36 
10. Gruber, T. R. 1993. Toward principles for the 
design of ontologies used for knowledge sharing. 
International Workshop on Formal Ontology, Pa-
dova, Italy 
11. Uschold M. 1996. Building Ontologies-Towards 
A Unified Methodology. In expert systems 96 
12. Yang Lingpeng, Ji Donghong, TangLi. 2004. 
Document Re-ranking Based on Automatically 
Acquired Key Terms in Chinese Information Re-
trieval. In Proceedings of the COLING'2004, pp. 
480-486 
13. Kwok, K.L. 1997. Comparing Representation in 
Chinese Information Retrieval. In Proceeding of 
the ACM SIGIR-97, pp. 34-4 
14. Nie, J.Y., Gao, J., Zhang, J., Zhou, M. 2000. On 
the Use of Words and N-grams for Chinese Infor-
mation Retrieval. In Proceedings of the IRAL-
2000, pp. 141-148 
15. Robertson, S.E. and Walker, S. 2001. Microsoft 
Cambridge at TREC-9: Filtering track: In Pro-
ceeding of the TREC 2000, pages 361-369 
16. Salton, G., Buckley, C. Term weighting ap-
proaches in automatic text retrieval. Information 
Processing and Management, 1988, 24(5), 
pp.513?523 
17. Vladimir Oleshchuk, Asle Pedersen. Ontology 
Based Semantic Similarity Comparison of Docu-
ments, 14th International Workshop on Database 
and Expert Systems Applications, September, 
2003, pp.735-738 
18. Besancon, R., Rajman, M., Chappelier, J. C. Tex-
tual similarities based on a distributional approach, 
Tenth International Workshop on Database and 
Expert Systems Applications, 1-3 Sept. 1999, 
pp.180-184 
 
18
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 331?336,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Improving English Subcategorization Acquisition with Diathesis Al-
ternations as Heuristic Information 
Xiwu Han 
Institute of Computational 
Linguistics 
Heilongjiang University 
Harbin City 150080 China 
hxw@hlju.edu.cn 
Tiejun Zhao 
School of Computer Science and 
Technology 
Harbin Institute of Technology 
Harbin City 150001 China 
tjzhao@mtlab.hit.edu.cn
Xingshang Fu 
Institute of Computational 
Linguistics 
Heilongjiang University 
Harbin City 150080 China
fxs@hlju.edu.cn 
 
  
 
Abstract 
Automatically acquired lexicons with 
subcategorization information have al-
ready proved accurate and useful enough 
for some purposes but their accuracy still 
shows room for improvement. By means 
of diathesis alternation, this paper pro-
poses a new filtering method, which im-
proved the performance of Korhonen?s 
acquisition system remarkably, with the 
precision increased to 91.18% and recall 
unchanged, making the acquired lexicon 
much more practical for further manual 
proofreading and other NLP uses. 
1 Introduction 
Subcategorization is the process that further clas-
sifies a syntactic category into its subsets. Chom-
sky (1965) defines the function of strict subcate-
gorization features as appointing a set of con-
straints that dominate the selection of verbs and 
other arguments in deep structure. Large sub-
categorized verbal lexicons have proved to be 
crucially important for many tasks of natural 
language processing, such as probabilistic pars-
ers (Korhonen, 2001, 2002) and verb classifica-
tions (Schulte im Walde, 2002; Korhonen, 2003).  
Since Brent (1993) a considerable amount of re-
search focusing on large-scaled automatic acqui-
sition of subcategorization frames (SCF) has met 
with some success not only in English but also in 
many other languages, including German 
(Schulte im Walde, 2002), Spanish (Chrupala, 
2003), Czech (Sarkar and Zeman, 2000), Portu-
guese (Gamallo et. al, 2002), and Chinese (Han 
et al 2004). The general objective of this re-
search is to acquire from a given corpus the SCF 
types and numbers for predicate verbs. Two typi-
cal steps during the process of automatic acquisi-
tion are hypothesis generation and selection. 
Usually based on heuristic rules, the first step 
generates SCF hypotheses for involved verbs; 
and the second selects reliable ones via statistical 
methods, such as BHT (binomial hypothesis test-
ing), LLR (log likelihood ratio) and MLE 
(maximum likelihood estimation). This second 
step is also called statistical filtering and has 
been widely regarded as problematic. English 
researchers have proposed some methods adjust-
ing the corpus hypothesis frequencies before or 
while filtering. These methods are often called 
backoff techniques for SCF acquisition. Some of 
them represent a remarkable improvement in the 
acquisition performance, for example diathesis 
alternation and semantic motivation (Korhonen, 
1998, 2001, 2002). 
For the convenience of comparison between 
performances of different SCF acquisition meth-
ods, we define absolute and relative recall in this 
paper. By absolute recall, we mean the figure 
computed against the background of input corpus, 
while relative recall is against the set of gener-
ated hypotheses.  
At present, automatically acquired verb lexi-
cons with SCF information have already proved 
accurate and useful enough for some NLP pur-
poses (Korhonen, 2001; Han et al 2004). As for 
English, Korhonen (2002) reported that semanti-
cally motivated SCF acquisition achieved a pre-
cision of 87.1%, an absolute recall of 71.2% and 
a relative recall of 85.27%, thus making the ac-
quired lexicon much more accurate and useful. 
However, the accuracy still shows room for im-
provement, especially for those SCF hypotheses 
with low frequencies. Detailed analysis on the 
acquisition system and some resulting data 
shows that three main causes should account for 
the comparatively unsatisfactory performance: a. 
the imperfect hypothesis generator, b. the Zipfian 
331
distribution of syntactic patterns, c. the incom-
plete partition over SCF types of a given verb. 
The first problem mainly comes from the inade-
quate parsing performance and noises existing in 
the corpus, while the other two problems are in-
herent to natural languages and should be solved 
in terms of acquisition techniques particularly 
during the process of hypothesis selection. 
2 Related Work 
The empirical background of this paper is the 
public resource for subcategorization acquisition 
of English verbs, provided by Anna Korhonen 
(2005) in her personal home page. The data in-
clude 30 verbs, as shown in Table 1, and their 
unfiltered SCF hypotheses, which were auto-
matically generated via Briscoe and Carroll?s 
(1997) SCF acquisition system, and the manually 
established standard.  
Precision  + Recall 
2 * Precision * Recall 
|True positives|+|False positives|
|True positives| 
|True positives|+|False negatives|
|True positives| 
Table 1. English Verbs in Use. 
add agree attach 
bring carry carve 
chop cling clip 
fly  cut travel 
drag communicate give 
lend lock marry 
meet mix move 
offer provide visit 
push sail send 
slice supply swing 
For each verb, there is a corpus of 1000 sen-
tences extracted from the BNC, and all together 
42 SCF types are involved in the corpus. The 
framework of Briscoe and Carroll?s system con-
sists of six overall components, which are ap-
plied in sequence to sentences containing a spe-
cific predicate in order to retrieve a set of SCFs 
for that verb: 
z A tagger, a first-order Hidden Markov 
Model POS and punctuation tag disam-
biguator. 
z A lemmatizer, an enhanced version of the 
General Architecture for Text Engineering 
project stemmer. 
z A probabilistic LR parser, trained on a 
tree-bank derived semi-automatically from 
the SUSANNE corpus, returns ranked 
analyses using a feature-based unification 
grammar. 
z A pattern extractor, which extracts 
subcategorization patterns, i.e. local 
syntactic frames, including the syntactic 
frames, including the syntactic categories 
and head lemmas. 
z A pattern classifier, which assigns patterns 
to SCFs or rejects them as unclassifiable. 
z A SCF filter, which evaluates sets of SCFs 
gathered for a predicate verb. 
Nowadays, in most related researches, the per-
formances of subcategorization acquisition sys-
tems are often evaluated in terms of precision, 
recall and F measure of SCF types (Korhonen, 
2001, 2002). Generally, precision is the percent-
age of SCFs that the system proposes correctly, 
while recall is the percentage of SCFs in the gold 
standard that the system proposes: 
 
Precision = 
 
 
Recall =  
 
 
F-measure =  
 
Here, true positives are correct SCF types pro-
posed by the system, false positives are incorrect 
SCF types proposed by system, and false nega-
tives are correct SCF types not proposed by the 
system. 
3 The MLE Filtering Method 
The present SCF acquisition system for English 
verbs employs a MLE filter to test the automati-
cally generated SCF hypotheses. Due to noises 
accumulated while tagging, lemmatizing and 
parsing the corpus, even though correction is im-
plemented for some typical errors when classify-
ing the extracted patterns, the hypothesis genera-
tor does not perform as efficiently as hoped. 
Sampling analysis on the unfiltered hypotheses 
in Korhonen?s evaluation corpus indicates that 
about 74% incorrectly proposed and rejected 
SCF types come from the defects of the MLE 
filtering method. 
Performance of the MLE filter is closely re-
lated to the actual distributions p(scfi|v) over 
predicates and SCF types in the input corpus. 
First, from the overall corpus a training set is 
drawn randomly; it must be large enough to en-
sure a similar distribution. Then, the frequency 
of a subcategorization frame scfi occurring with a 
verb v is recorded and used to estimate the prob-
ability p(scfi|v). Thirdly, an empirical threshold ? 
is determined, which ensures that a maximum 
332
value of the F-measure will result for the training 
set. Finally, the threshold is used to filter out 
from the total set those SCF hypotheses with fre-
quencies lower than ?.  
Therefore, the statistical foundation of this fil-
tering method is the assumption of independence 
among the SCFs that a verb enters, which can be 
probabilistically expressed in two formulas as 
follows: 
0),|(,,, =??? vscfscfpjiji ji ? (1) 
?
=
=
n
i
i vscfp
1
1)|(                          ? (2) 
Here, i and j are natural numbers, scfi and scfj are 
two SCF types that verb v enters, and variables in 
formulas henceforth will hold the same meanings. 
In actual application, the probability p(scfi|v) is 
estimated from the observed frequency f(scfi, v), 
and the conditional probability p(scfi|scfj, v) is 
assumed to be zero. This means any two SCF 
types entered by a given verb are taken for 
granted to be probabilistically independent from 
each other. However, this assumption can some-
times be far from appropriate. 
4 Diathesis Alternations and Filtering 
Much linguistic research focusing on child lan-
guage acquisition has revealed that many chil-
dren are able to produce new grammatical sen-
tences from what they have learned (Peters, 1983; 
Ellis, 1985). This implies that the widely-used 
independence assumption in the field of NLP 
may not be very appropriate, at least for syntactic 
patterns. If this assumption should be removed, a 
possible heuristic could be the information of 
diathesis alternations, which is also another con-
vincing counterargument. Diathesis alternations 
are generally regarded as alternative ways in 
which verbs express their arguments. Examples 
are as follows: 
a. He broke the glass. 
b. The glass broke. 
c. Ta1 chi1 le0 pin2guo3. 
? ? ? ???(         ) 
d. Ta1 ba3 pin2guo3 chi1 le01. 
?? ?? ???(         ) 
In the above examples, the English verb break 
takes the causative-inchoative alternation as 
shown in sentences a and b, while sentences c 
and d indicate that the Chinese verb chi1 ? ( , eat) 
may enter the ba-object-raising alternation where 
the object is shifted forward by the preposition 
ba3 ? ( ) to the location between the subject and 
the predicate, as illustrated in Figure 1. 
                                                 
1 The numbers in sentences c and d, which are pinyin nota-
tions, show tones of the Chinese syllables, and the two sen-
tences, in English, generally mean He ate an apple. 
 
 
 
 
 
 
Figure 1. Ba-object-raising Alternation. 
ba3 
Ta1 chi1 le0 pin2guo3. 
ba-object-raising 
Subcategorization of verbs has much to do 
with diathesis alternations, and most SCF re-
searchers regard information of diathesis alterna-
tion as an indispensable part of subcategorization 
(Korhonen, 2001; McCarthy, 2001). Therefore, 
one may conclude that, for subcategorization 
acquisition, the independence assumption sup-
porting the MLE filter is not as appropriate as 
previously thought.  
For a given verb, the assumption will be ap-
propriate and sufficient if and only if there is no 
diathesis alternation between all the SCFs it en-
ters, and formula (1) and (2) in Section 3 are ef-
ficient enough to serve as a foundation for the 
MLE filtering method. Otherwise, if there are 
diathesis alternations between some of the SCFs 
that a verb enters, then formula (1) and (2) must 
be modified as illustrated in formula (3) and (4). 
In either case, for the sake of convenience, it 
would be better to combine the formulas as 
shown in (5) and (6). 
0),|(,,, >??? vscfscfpjiji ji  ? (3) 
?
=
>
n
i
i vscfp
1
1)|(                           ? (4) 
0),|(,,, ???? vscfscfpjiji ji  ? (5) 
?
=
?
n
i
i vscfp
1
1)|(                            ? (6) 
For English verbs, previous research has 
achieved great progress in diathesis alternation 
and relative applications, such as the work of 
Levin (1993) and McCarthy (2001). Besides, 
Korhonen (1998) has proved that diathesis alter-
nation could be used as heuristic information for 
backoff estimates to improve the general per-
formance of subcategorization acquisition. How-
ever, determining where and how to seed the 
heuristic remains difficult. 
Korhonen (1998) employed diathesis alterna-
tions in Briscoe and Carroll?s system to improve 
the performance of their BHT filter. Although 
the precision rate increased from 61.22% to 
333
69.42% and the recall rate from 44.70% to 
50.81%, the results were still not accurate 
enough for possible practical NLP uses.  
Korhonen obtained her one-way diathesis al-
ternations from the ANLT dictionary (Boguraev 
and Briscoe, 1987), calculated the alternating 
probability p(scfj|scfi) according to the number of 
common verbs that took the alternation 
(scfi?scfj), and used formula (7) and (8), where 
w is an empirical weight, to adjust the previously 
estimated p(scfi|v): 
If p(scfi|scfj, v)>0,  
p(scfi|v) = p(scfi|v)?w(p(scfi|v)? 
 p(scfj| scfi))                  ?(7) 
If p(scfi|v)>0 & p(scfj|v)=0,  
p(scfi|v) = p(scfi|v)+w(p(scfi|v)? 
                p(scfj| scfi))                   ?(8)2 
Following the adjustment, a BHT filter with a 
confidence rate of 95% was used to check the 
SCF hypotheses. 
This method removes the assumption of inde-
pendence among SCF types but establishes an-
other assumption of independence between 
p(scfj|scfi) and certain verbs, which assumes that 
all verbs take each diathesis alternation with the 
same probability. Nevertheless, linguistic knowl-
edge tells us that verbs often enter different dia-
thesis alternations and can be classified accord-
ingly. Consider the following examples: 
e. He broke the glass. / The glass broke. 
f. The police dispersed the crowd.  
/ The crowd dispersed. 
g. Mum cut the bread. / *The bread cut. 
Both of the English verbs ?break? and ?disperse? 
can take the causative-inchoative alternation and, 
hence, may be classified together, while the verb 
?cut? does not take this alternation. Therefore, 
the newly established assumption doesn?t fit the 
actual situation either, and the probability sums 
?ip(scfi|v) and ?i,jp(scfi|scfj, v) neither need or 
can be normalized. 
Based on the above methodology, we formed a 
new filtering method with diathesis alternations 
as heuristic information, which is, in fact, de-
rived from the simple MLE filter and based on 
formula (5) and (6). The algorithm can be briefly 
expressed as shown in Table 2. 
 
                                                 
2 For the sake of consistency in this paper and for the con-
venience of understanding, formulae formats here are modi-
fied. They may look different from those of Korhonen 
(1998), but they are actually the same. 
Table 2. The New Filtering Method. 
For hypotheses of a given verb v, 
1. if p(scfi|v) > ?1,  
accept scfi into the output set S; 
2. else  
if p(scfi|v) > ?2, 
& p(scfi|scfj, v) > 0, 
& scfj?S, 
accept scfi into set S; 
3. Go to step 1 until S doesn?t increase.
In our method, two filters are employed. For 
each verb involved, first a common MLE filter is 
used, but it employs a threshold ?1 that is much 
higher than usual, and those SCF hypotheses that 
satisfy the requirement are accepted. Then, all of 
the remainder of the hypotheses are checked by 
another MLE filter seeded with diathesis alterna-
tions as heuristic information and equipped with 
a much lower threshold ?2. Any hypothesis scfi 
left out by the first filter will be accepted if its 
probability exceeds ?2 and it is an alternative of 
an SCF type scfj that has been accepted by the 
first filter, which means that p(scfi|scfj, v)>0 and 
scfj?S. The filtering process will be performed 
repeatedly for those unaccepted hypotheses until 
no more hypotheses can be accepted for the verb. 
5 Experimental Evaluation 
We implemented an acquisition experiment on 
Korhonen?s evaluation resources with the above-
mentioned filtering method.  
The diathesis alternations in use are also those 
provided by Korhonen, except that we used them 
in a two-way manner (scfi??scfj) instead of 
one-way (scfi?scfj), because the two involved 
SCF types are usually alternative pragmatic for-
mats of the concerned verb, as shown in exam-
ples in Section 3 and 4. 
In the experiment we empirically set ?1= 0.2, 
which is ten times of Korhonen?s threshold for 
her MLE filter; ?2= 0.002, which is one tenth of 
Korhonen?s. Thus, in a token set of hypotheses 
no more than 1000, an SCF type scfi will be ac-
cepted if it occurs two times or more and has a 
diathesis alternative type scfj already accepted for 
the verb. 
The gold standard was the manually analysed 
results by Korhonen. Precision, recall and F-
measure were calculated via expressions given in 
Section 2.  
Table 3 lists the performances of the baseline 
method of non-filtering (No_f), MLE filtering 
with ? = 0.02, and our filtering method on the 
334
evaluation corpus, and also gives the best results 
of Korhonen's method that is using extra seman-
tic information (Kor) to make a comparison. 
Here, Ab_R is the absolute recall ratio, Re_R the 
relative recall ratio, Ab_F the absolute F-
measure that is calculated from Precision and 
Ab_R, and Re_F the relative F-measure that is 
from Precision and Re_R. 
Table 3. Performance Comparison. 
Methods No-f MLE  ours Kor 
P(%) 47.85 67.89 91.18 87.1 
Ab_R(%) 34.62 32.52 32.52 71.2 
Re_R(%) 100 93.93 93.93 85.27
Ab_F 40.17 43.98 47.94 78.35
Re_F 64.73 78.81 92.53 86.18
The evaluation shows that our new filtering 
method improved the acquisition performance 
remarkably: a. Compared with MLE, precision 
increased by 23.29%, recall ratio remained un-
changed, absolute F-measure increased by 3.96, 
and relative F-measure increased by 13.72; b. 
Compared with Korhonen?s best results, preci-
sion, Re_R and Re_F also increased respec-
tively 3 . Thus, the general performance of our 
filtering method makes the acquired lexicon 
much more practical for further manual proof-
reading and other NLP uses. 
What?s more, the data shown in Table 3 im-
plies that there is little room left for improvement 
of the statistical filter, since the absolute recall 
ratio is only 2.1% lower than that of the non-
filtering method. Whereas, detailed analysis of 
the evaluation corpus shows that the hypothesis 
generator accounts for about 95% of those unre-
called and wrongly recalled SCF types, which 
indicates, for the present time, more improve-
ment efforts need to be made on the first step of 
subcategorization acquisition, i.e. hypothesis 
generation. 
6 Conclusion 
Our new filtering method removed the inappro-
priate assumptions and takes much more advan-
                                                 
3 Korhonen (2002) reported the non-filtering absolute recall 
ratio of her experiment was about 83.5%. She didn?t give 
any explanation with her evaluation resources why here 
non-filtering Ab_R was so much lower. Therefore, the 
Ab_R and Ab_F figures are not comparable here. 
tage of what can be observed in the corpus by 
drawing on the alternative relationship between 
SCF hypotheses with higher and lower frequen-
cies. Unlike the semantically motivated method 
(Korhonen, 2001, 2002), which is dependent on 
verb classifications that linguistic resources are 
able to provide, our filter needs no prior knowl-
edge other than reasonable diathesis alternation 
information and may work well for most verbs in 
other languages with sufficient predicative to-
kens. 
Our experimental results suggest that the pro-
posed technique improves the general perform-
ance of the English subcategorization acquisition 
system, and leaves only a little room for further 
improvement in statistical filtering methods. 
However, approaches that are more complicated 
still exist theoretically, for instance, some SCF 
types unseen by the hypothesis generator may be 
recalled by integrating semantic verb-
classification information into the system. 
More essential aspects of our future work, 
however, will focus on improving the perform-
ance of the hypothesis generator, and testing and 
applying the acquired subcategorization lexicons 
in some concrete NLP tasks. 
Acknowledgement This research has been 
jointly sponsored by the NSFC project No. 
60373101 and the post-doctor scholarship of for-
eign linguistics and literature in Heilongjiang 
University. And at the same time, our great 
thanks go to Dr. Anna Korhonen for her public 
evaluation resources, and Dr. Chrys Chrystello 
for his helpful advice on the English writing of 
this paper. 
References 
Boguraev B. K., E. J. Briscoe. Large lexicons for 
natural language processing utilizing the grammar 
coding system of the Longman Dictionary of Con-
temporary English. Computational Linguistics, 
1987: 219-240 
Brent, M., From Grammar to Lexicon: unsupervised 
learning of lexical syntax, Computational Linguis-
tics 19(3) 1993: 243-262. 
Briscoe, Ted and John Carroll, Automatic extraction 
of subcategorization from corpora, Proceedings of 
the 5th ACL Conference on Applied Natural Lan-
guage Processing, Washington, DC, 1997: 356-
363. 
Chomsky, Noam, Aspects of the Theory of Syntax, 
MIT Press, Cambridge, 1965. 
Chrupala, Grzegorz, Acquiring Verb Subcategoriza-
tion from Spanish Corpora, PhD program ?Cogni-
335
tive Science and Language?, Universitat de Barce-
lona, 2003: 67-68. 
Ellis, R?Understanding Second language Acquisi-
tion, Oxford University Press.1985 
Gamallo, P., Agustini, A. and Lopes Gabriel P., Using 
Co-Composition for Acquiring Syntactic and Se-
mantic Subcategorisation, Proceedings of the 
Workshop of the ACL Special Interest Group on 
the Lexicon (SIGLEX), Philadelphia, 2002: 34-41.  
Han, Xiwu, Tiejun Zhao, Haoliang Qi, and Hao Yu, 
Subcategorization Acquisition and Evaluation for 
Chinese Verbs, Proceedings of the COLING 2004, 
2004: 723-728. 
Korhonen, Anna, Automatic Extraction of Subcatego-
rization Frames from Corpora ?Improving Filtering 
with Diathesis Alternations, 1998. Please refer to 
http://www.folli.uva.nl/CD/1998/pdf/keller/korhon
en.pdf 
Korhonen, Anna, Subcategorization Acquisition, Dis-
sertation for PhD, Trinity Hall University of Cam-
bridge, 2001. 
Korhonen, Anna, Subcategorization Acquisition, 
Technical Report Number 530, Trinity Hall Uni-
versity of Cambridge, 2002. 
Korhonen, Anna, Yuval Krymolowski, Zvika Marx, 
Clustering Polysemic Subcategorization Frame 
Distributions Semantically, Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics, 2003: 64-71. 
Korhonen, Anna. Subcategorization Evaluation Re-
sources. http://www.cl.cam.ac.uk/users/alk23/sub-
cat/subcat.html. 2005 
Levin, B., English Verb Classes and Alternations, 
Chicago University Press, Chicago, 1993. 
McCarthy, D., Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Alternations, Sub-
categorization Frames and Selectional Preferences, 
PhD thesis, University of Sussex, 2001. 
Peters, A. The Unit of Language Acquisition, Cam-
bridge University Press. 1983. 
Sarkar, A. and Zeman, D., Automatic Extraction of 
Subcategorization Frames for Czech, Proceedings 
of the 19th International Conference on Computa-
tional Linguistics, Saarbrucken, Germany, 2000. 
Please refer to http://www.sfu.ca/~anoop/papers/ 
pdf/coling0final.pdf 
Shulte im Walde, Sabine, Inducing German Semantic 
Verb Classes from Purely Syntactic Subcategoriza-
tion Information, Proceedings of the 40th Annual 
Meeting of the Association for Computational Lin-
guistics, 2002: 223-230. 
336
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 688?695,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Unified Tagging Approach to Text Normalization 
Conghui Zhu 
Harbin Institute of Technology 
Harbin, China 
chzhu@mtlab.hit.edu.cn 
Jie Tang 
Department of Computer Science
Tsinghua University, China 
jietang@tsinghua.edu.cn
Hang Li 
Microsoft Research Asia  
Beijing, China 
hangli@microsoft.com
Hwee Tou Ng 
Department of Computer Science 
National University of Singapore, Singapore 
nght@comp.nus.edu.sg 
Tiejun Zhao 
Harbin Institute of Technology 
Harbin, China 
tjzhao@mtlab.hit.edu.cn 
 
 
Abstract 
This paper addresses the issue of text nor-
malization, an important yet often over-
looked problem in natural language proc-
essing. By text normalization, we mean 
converting ?informally inputted? text into 
the canonical form, by eliminating ?noises? 
in the text and detecting paragraph and sen-
tence boundaries in the text. Previously, 
text normalization issues were often under-
taken in an ad-hoc fashion or studied sepa-
rately. This paper first gives a formaliza-
tion of the entire problem. It then proposes 
a unified tagging approach to perform the 
task using Conditional Random Fields 
(CRF). The paper shows that with the in-
troduction of a small set of tags, most of 
the text normalization tasks can be per-
formed within the approach. The accuracy 
of the proposed method is high, because 
the subtasks of normalization are interde-
pendent and should be performed together. 
Experimental results on email data cleaning 
show that the proposed method signifi-
cantly outperforms the approach of using 
cascaded models and that of employing in-
dependent models. 
1 Introduction 
More and more ?informally inputted? text data be-
comes available to natural language processing, 
such as raw text data in emails, newsgroups, fo-
rums, and blogs. Consequently, how to effectively 
process the data and make it suitable for natural 
language processing becomes a challenging issue. 
This is because informally inputted text data is 
usually very noisy and is not properly segmented. 
For example, it may contain extra line breaks, extra 
spaces, and extra punctuation marks; and it may 
contain words badly cased. Moreover, the bounda-
ries between paragraphs and the boundaries be-
tween sentences are not clear. 
We have examined 5,000 randomly collected 
emails and found that 98.4% of the emails contain 
noises (based on the definition in Section 5.1). 
In order to perform high quality natural lan-
guage processing, it is necessary to perform ?nor-
malization? on informally inputted data first, spe-
cifically, to remove extra line breaks, segment the 
text into paragraphs, add missing spaces and miss-
ing punctuation marks, eliminate extra spaces and 
extra punctuation marks, delete unnecessary tokens, 
correct misused punctuation marks, restore badly 
cased words, correct misspelled words, and iden-
tify sentence boundaries. 
Traditionally, text normalization is viewed as an 
engineering issue and is conducted in a more or 
less ad-hoc manner. For example, it is done by us-
ing rules or machine learning models at different 
levels. In natural language processing, several is-
sues of text normalization were studied, but were 
only done separately. 
This paper aims to conduct a thorough investiga-
tion on the issue. First, it gives a formalization of 
688
the problem; specifically, it defines the subtasks of 
the problem. Next, it proposes a unified approach 
to the whole task on the basis of tagging. Specifi-
cally, it takes the problem as that of assigning tags 
to the input texts, with a tag representing deletion, 
preservation, or replacement of a token. As the 
tagging model, it employs Conditional Random 
Fields (CRF). The unified model can achieve better 
performances in text normalization, because the 
subtasks of text normalization are often interde-
pendent. Furthermore, there is no need to define 
specialized models and features to conduct differ-
ent types of cleaning; all the cleaning processes 
have been formalized and conducted as assign-
ments of the three types of tags. 
Experimental results indicate that our method 
significantly outperforms the methods using cas-
caded models or independent models on normali-
zation. Our experiments also indicate that with the 
use of the tags defined, we can conduct most of the 
text normalization in the unified framework. 
Our contributions in this paper include: (a) for-
malization of the text normalization problem, (b) 
proposal of a unified tagging approach, and (c) 
empirical verification of the effectiveness of the 
proposed approach. 
The rest of the paper is organized as follows. In 
Section 2, we introduce related work. In Section 3, 
we formalize the text normalization problem. In 
Section 4, we explain our approach to the problem 
and in Section 5 we give the experimental results. 
We conclude the paper in Section 6. 
2 Related Work 
Text normalization is usually viewed as an 
engineering issue and is addressed in an ad-hoc 
manner. Much of the previous work focuses on 
processing texts in clean form, not texts in 
informal form. Also, prior work mostly focuses on 
processing one type or a small number of types of 
errors, whereas this paper deals with many 
different types of errors. 
Clark (2003) has investigated the problem of 
preprocessing noisy texts for natural language 
processing. He proposes identifying token bounda-
ries and sentence boundaries, restoring cases of 
words, and correcting misspelled words by using a 
source channel model. 
Minkov et al (2005) have investigated the prob-
lem of named entity recognition in informally in-
putted texts. They propose improving the perform-
ance of personal name recognition in emails using 
two machine-learning based methods: Conditional 
Random Fields and Perceptron for learning HMMs. 
See also (Carvalho and Cohen, 2004). 
Tang et al (2005) propose a cascaded approach 
for email data cleaning by employing Support Vec-
tor Machines and rules. Their method can detect 
email headers, signatures, program codes, and ex-
tra line breaks in emails. See also (Wong et al, 
2007). 
Palmer and Hearst (1997) propose using a Neu-
ral Network model to determine whether a period 
in a sentence is the ending mark of the sentence, an 
abbreviation, or both. See also (Mikheev, 2000; 
Mikheev, 2002). 
Lita et al (2003) propose employing a language 
modeling approach to address the case restoration 
problem. They define four classes for word casing: 
all letters in lower case, first letter in uppercase, all 
letters in upper case, and mixed case, and formal-
ize the problem as assigning class labels to words 
in natural language texts. Mikheev (2002) proposes 
using not only local information but also global 
information in a document in case restoration. 
Spelling error correction can be formalized as a 
classification problem. Golding and Roth (1996) 
propose using the Winnow algorithm to address 
the issue. The problem can also be formalized as 
that of data conversion using the source channel 
model. The source model can be built as an n-gram 
language model and the channel model can be con-
structed with confusing words measured by edit 
distance. Brill and Moore, Church and Gale, and 
Mayes et al have developed different techniques 
for confusing words calculation (Brill and Moore, 
2000; Church and Gale, 1991; Mays et al, 1991). 
Sproat et al (1999) have investigated normaliza-
tion of non-standard words in texts, including 
numbers, abbreviations, dates, currency amounts, 
and acronyms. They propose a taxonomy of non-
standard words and apply n-gram language models, 
decision trees, and weighted finite-state transduc-
ers to the normalization. 
3 Text Normalization 
In this paper we define text normalization at three 
levels: paragraph, sentence, and word level. The 
subtasks at each level are listed in Table 1. For ex-
ample, at the paragraph level, there are two sub-
689
tasks: extra line-break deletion and paragraph 
boundary detection. Similarly, there are six (three) 
subtasks at the sentence (word) level, as shown in 
Table 1. Unnecessary token deletion refers to dele-
tion of tokens like ?-----? and ?====?, which are 
not needed in natural language processing. Note 
that most of the subtasks conduct ?cleaning? of 
noises, except paragraph boundary detection and 
sentence boundary detection. 
Level Task Percentages of Noises
Extra line break deletion 49.53 
Paragraph 
Paragraph boundary detection  
Extra space deletion 15.58 
Extra punctuation mark deletion 0.71 
Missing space insertion 1.55 
Missing punctuation mark insertion 3.85 
Misused punctuation mark correction 0.64 
Sentence 
Sentence boundary detection  
Case restoration 15.04 
Unnecessary token deletion 9.69 Word 
Misspelled word correction 3.41 
Table 1. Text Normalization Subtasks 
As a result of text normalization, a text is seg-
mented into paragraphs; each paragraph is seg-
mented into sentences with clear boundaries; and 
each word is converted into the canonical form. 
After normalization, most of the natural language 
processing tasks can be performed, for example, 
part-of-speech tagging and parsing. 
We have manually cleaned up some email data 
(cf., Section 5) and found that nearly all the noises 
can be eliminated by performing the subtasks de-
fined above. Table 1 gives the statistics. 
1.  i?m thinking about buying a pocket 
2.  pc    device for my wife this christmas,. 
3.  the worry that i have is that she won?t 
4.  be able to sync it to her outlook express  
5.  contacts? 
Figure 1. An example of informal text 
I?m thinking about buying a Pocket PC device for my 
wife this Christmas.// The worry that I have is that 
she won?t be able to sync it to her Outlook Express 
contacts.// 
Figure 2. Normalized text 
Figure 1 shows an example of informally input-
ted text data. It includes many typical noises. From 
line 1 to line 4, there are four extra line breaks at 
the end of each line. In line 2, there is an extra 
comma after the word ?Christmas?. The first word 
in each sentence and the proper nouns (e.g., 
?Pocket PC? and ?Outlook Express?) should be 
capitalized. The extra spaces between the words 
?PC? and ?device? should be removed. At the end 
of line 2, the line break should be removed and a 
space is needed after the period. The text should be 
segmented into two sentences. 
Figure 2 shows an ideal output of text normali-
zation on the input text in Figure 1. All the noises 
in Figure 1 have been cleaned and paragraph and 
sentence endings have been identified. 
We must note that dependencies (sometimes 
even strong dependencies) exist between different 
types of noises. For example, word case restoration 
needs help from sentence boundary detection, and 
vice versa. An ideal normalization method should 
consider processing all the tasks together. 
4 A Unified Tagging Approach 
4.1 Process 
In this paper, we formalize text normalization as a 
tagging problem and employ a unified approach to 
perform the task (no matter whether the processing 
is at paragraph level, sentence level, or word level). 
There are two steps in the method: preprocess-
ing and tagging. In preprocessing, (A) we separate 
the text into paragraphs (i.e., sequences of tokens), 
(B) we determine tokens in the paragraphs, and (C) 
we assign possible tags to each token. The tokens 
form the basic units and the paragraphs form the 
sequences of units in the tagging problem. In tag-
ging, given a sequence of units, we determine the 
most likely corresponding sequence of tags by us-
ing a trained tagging model. In this paper, as the 
tagging model, we make use of CRF. 
Next we describe the steps (A)-(C) in detail and 
explain why our method can accomplish many of 
the normalization subtasks in Table 1. 
(A). We separate the text into paragraphs by tak-
ing two or more consecutive line breaks as the end-
ings of paragraphs. 
(B). We identify tokens by using heuristics. 
There are five types of tokens: ?standard word?, 
?non-standard word?, punctuation mark, space, and 
line break. Standard words are words in natural 
language. Non-standard words include several 
general ?special words? (Sproat et al, 1999), email 
address, IP address, URL, date, number, money, 
percentage, unnecessary tokens (e.g., ?===? and 
690
?###?), etc. We identify non-standard words by 
using regular expressions. Punctuation marks in-
clude period, question mark, and exclamation mark. 
Words and punctuation marks are separated into 
different tokens if they are joined together. Natural 
spaces and line breaks are also regarded as tokens. 
(C). We assign tags to each token based on the 
type of the token. Table 2 summarizes the types of 
tags defined. 
Token Type Tag Description 
PRV Preserve line break 
RPA Replace line break by space Line break 
DEL Delete line break 
PRV Preserve space 
Space 
DEL Delete space 
PSB Preserve punctuation mark and view it as sentence ending 
PRV Preserve punctuation mark without viewing it as sentence ending 
Punctuation 
mark 
DEL Delete punctuation mark 
AUC Make all characters in uppercase 
ALC Make all characters in lowercase 
FUC Make the first character in uppercase
Word 
AMC Make characters in mixed case 
PRV Preserve the special token 
Special token 
DEL Delete the special token 
Table 2. Types of tags 
 
Figure 3. An example of tagging 
Figure 3 shows an example of the tagging proc-
ess. (The symbol ??? indicates a space). In the fig-
ure, a white circle denotes a token and a gray circle 
denotes a tag. Each token can be assigned several 
possible tags. 
Using the tags, we can perform most of the text 
normalization processing (conducting seven types 
of subtasks defined in Table 1 and cleaning 
90.55% of the noises). 
In this paper, we do not conduct three subtasks, 
although we could do them in principle. These in-
clude missing space insertion, missing punctuation 
mark insertion, and misspelled word correction. In 
our email data, it corresponds to 8.81% of the 
noises. Adding tags for insertions would increase 
the search space dramatically. We did not do that 
due to computation consideration. Misspelled word 
correction can be done in the same framework eas-
ily. We did not do that in this work, because the 
percentage of misspelling in the data is small. 
We do not conduct misused punctuation mark 
correction as well (e.g., correcting ?.? with ???). It 
consists of 0.64% of the noises in the email data. 
To handle it, one might need to parse the sentences. 
4.2 CRF Model 
We employ Conditional Random Fields (CRF) as 
the tagging model. CRF is a conditional probability 
distribution of a sequence of tags given a sequence 
of tokens, represented as P(Y|X) , where X denotes 
the token sequence and Y the tag sequence 
(Lafferty et al, 2001). 
In tagging, the CRF model is used to find the 
sequence of tags Y* having the highest likelihood 
Y* = maxYP(Y|X), with an efficient algorithm (the 
Viterbi algorithm). 
In training, the CRF model is built with labeled 
data and by means of an iterative algorithm based 
on Maximum Likelihood Estimation. 
Transition Features 
yi-1=y?, yi=y 
yi-1=y?, yi=y, wi=w 
yi-1=y?, yi=y, ti=t 
State Features 
wi=w, yi=y 
wi-1=w, yi=y 
wi-2=w, yi=y 
wi-3=w, yi=y 
wi-4=w, yi=y 
wi+1=w, yi=y 
wi+2=w, yi=y 
wi+3=w, yi=y 
wi+4=w, yi=y 
wi-1=w?, wi=w, yi=y
wi+1=w?, wi=w, yi=y 
ti=t, yi=y 
ti-1=t, yi=y 
ti-2=t, yi=y 
ti-3=t, yi=y 
ti-4=t, yi=y 
ti+1=t, yi=y 
ti+2=t, yi=y 
ti+3=t, yi=y 
ti+4=t, yi=y 
ti-2=t??, ti-1=t?, yi=y 
ti-1=t?, ti=t, yi=y 
ti=t, ti+1=t?, yi=y 
ti+1=t?, ti+2=t??, yi=y 
ti-2=t??, ti-1=t?, ti=t, yi=y 
ti-1=t??, ti=t, ti+1=t?, yi=y 
ti=t, ti+1=t?, ti+2=t??, yi=y 
Table 3. Features used in the unified CRF model 
691
4.3 Features 
Two sets of features are defined in the CRF model: 
transition features and state features. Table 3 
shows the features used in the model. 
Suppose that at position i in token sequence x, wi 
is the token, ti the type of token (see Table 2), and 
yi the possible tag. Binary features are defined as 
described in Table 3. For example, the transition 
feature yi-1=y?, yi=y implies that if the current tag is 
y and the previous tag is y?, then the feature value 
is true; otherwise false. The state feature wi=w, 
yi=y implies that if the current token is w and the 
current label is y, then the feature value is true; 
otherwise false. In our experiments, an actual fea-
ture might be the word at position 5 is ?PC? and the 
current tag is AUC. In total, 4,168,723 features 
were used in our experiments. 
4.4 Baseline Methods 
We can consider two baseline methods based on 
previous work, namely cascaded and independent 
approaches. The independent approach performs 
text normalization with several passes on the text. 
All of the processes take the raw text as input and 
output the normalized/cleaned result independently. 
The cascaded approach also performs normaliza-
tion in several passes on the text. Each process car-
ries out cleaning/normalization from the output of 
the previous process. 
4.5 Advantages 
Our method offers some advantages. 
(1) As indicated, the text normalization tasks are 
interdependent. The cascaded approach or the in-
dependent approach cannot simultaneously per-
form the tasks. In contrast, our method can effec-
tively overcome the drawback by employing a uni-
fied framework and achieve more accurate per-
formances. 
(2) There are many specific types of errors one 
must correct in text normalization. As shown in 
Figure 1, there exist four types of errors with each 
type having several correction results. If one de-
fines a specialized model or rule to handle each of 
the cases, the number of needed models will be 
extremely large and thus the text normalization 
processing will be impractical. In contrast, our 
method naturally formalizes all the tasks as as-
signments of different types of tags and trains a 
unified model to tackle all the problems at once. 
5 Experimental Results 
5.1 Experiment Setting 
Data Sets 
We used email data in our experiments. We ran-
domly chose in total 5,000 posts (i.e., emails) from 
12 newsgroups. DC, Ontology, NLP, and ML are 
from newsgroups at Google (http://groups-
beta.google.com/groups). Jena is a newsgroup at Ya-
hoo (http://groups.yahoo.com/group/jena-dev). Weka 
is a newsgroup at Waikato University (https://list. 
scms.waikato.ac.nz). Prot?g? and OWL are from a 
project at Stanford University 
(http://protege.stanford.edu/). Mobility, WinServer, 
Windows, and PSS are email collections from a 
company. 
Five human annotators conducted normalization 
on the emails. A spec was created to guide the an-
notation process. All the errors in the emails were 
labeled and corrected. For disagreements in the 
annotation, we conducted ?majority voting?.  For 
example, extra line breaks, extra spaces, and extra 
punctuation marks in the emails were labeled. Un-
necessary tokens were deleted. Missing spaces and 
missing punctuation marks were added and marked. 
Mistakenly cased words, misspelled words, and 
misused punctuation marks were corrected. Fur-
thermore, paragraph boundaries and sentence 
boundaries were also marked. The noises fell into 
the categories defined in Table 1. 
Table 4 shows the statistics in the data sets. 
From the table, we can see that a large number of 
noises (41,407) exist in the emails. We can also see 
that the major noise types are extra line breaks, 
extra spaces, casing errors, and unnecessary tokens. 
In the experiments, we conducted evaluations in 
terms of precision, recall, F1-measure, and accu-
racy (for definitions of the measures, see for ex-
ample (van Rijsbergen, 1979; Lita et al, 2003)). 
Implementation of Baseline Methods 
We used the cascaded approach and the independ-
ent approach as baselines. 
For the baseline methods, we defined several 
basic prediction subtasks: extra line break detec-
tion, extra space detection, extra punctuation mark 
detection, sentence boundary detection, unneces-
sary token detection, and case restoration. We 
compared the performances of our method with 
those of the baseline methods on the subtasks. 
692
Data Set 
Number 
of 
Email 
Number 
of 
Noises 
Extra 
Line 
Break 
Extra 
Space 
Extra
 Punc.
Missing
Space
Missing
Punc.
Casing
Error
Spelling
Error
Misused 
Punc.
Unnece-
ssary 
Token 
Number of 
Paragraph 
Boundary 
Number of 
Sentence 
Boundary
DC 100 702 476 31 8 3 24 53 14 2 91 457 291 
Ontology 100 2,731 2,132 24 3 10 68 205 79 15 195 677 1,132 
NLP 60 861 623 12 1 3 23 135 13 2 49 244 296 
ML 40 980 868 17 0 2 13 12 7 0 61 240 589 
Jena 700 5,833 3,066 117 42 38 234 888 288 59 1,101 2,999 1,836 
Weka 200 1,721 886 44 0 30 37 295 77 13 339 699 602 
Prot?g? 700 3,306 1,770 127 48 151 136 552 116 9 397 1,645 1,035 
OWL 300 1,232 680 43 24 47 41 152 44 3 198 578 424 
Mobility 400 2,296 1,292 64 22 35 87 495 92 8 201 891 892 
WinServer 400 3,487 2,029 59 26 57 142 822 121 21 210 1,232 1,151 
Windows 1,000 9,293 3,416 3,056 60 116 348 1,309 291 67 630 3,581 2,742 
PSS 1,000 8,965 3,348 2,880 59 153 296 1,331 276 66 556 3,411 2,590 
Total 5,000 41,407 20,586 6,474 293 645 1,449 6,249 1,418 265 4,028 16,654 13,580 
Table 4. Statistics on data sets 
For the case restoration subtask (processing on 
token sequence), we employed the TrueCasing 
method (Lita et al, 2003). The method estimates a 
tri-gram language model using a large data corpus 
with correctly cased words and then makes use of 
the model in case restoration. We also employed 
Conditional Random Fields to perform case 
restoration, for comparison purposes. The CRF 
based casing method estimates a conditional 
probabilistic model using the same data and the 
same tags defined in TrueCasing. 
For unnecessary token deletion, we used rules as 
follows. If a token consists of non-ASCII charac-
ters or consecutive duplicate characters, such as 
?===?, then we identify it as an unnecessary token. 
For each of the other subtasks, we exploited the 
classification approach. For example, in extra line 
break detection, we made use of a classification 
model to identify whether or not a line break is a 
paragraph ending. We employed Support Vector 
Machines (SVM) as the classification model (Vap-
nik, 1998). In the classification model we utilized 
the same features as those in our unified model 
(see Table 3 for details). 
In the cascaded approach, the prediction tasks 
are performed in sequence, where the output of 
each task becomes the input of each immediately 
following task. The order of the prediction tasks is: 
(1) Extra line break detection: Is a line break a 
paragraph ending? It then separates the text into 
paragraphs using the remaining line breaks. (2) 
Extra space detection: Is a space an extra space? (3) 
Extra punctuation mark detection: Is a punctuation 
mark a noise? (4) Sentence boundary detection: Is 
a punctuation mark a sentence boundary? (5) Un-
necessary token deletion: Is a token an unnecessary 
token? (6) Case restoration. Each of steps (1) to (4) 
uses a classification model (SVM), step (5) uses 
rules, whereas step (6) uses either a language 
model (TrueCasing) or a CRF model (CRF). 
In the independent approach, we perform the 
prediction tasks independently. When there is a 
conflict between the outcomes of two classifiers, 
we adopt the result of the latter classifier, as de-
termined by the order of classifiers in the cascaded 
approach. 
To test how dependencies between different 
types of noises affect the performance of normali-
zation, we also conducted experiments using the 
unified model by removing the transition features. 
Implementation of Our Method 
In the implementation of our method, we used the 
tool CRF++, available at http://chasen.org/~taku 
/software/CRF++/. We made use of all the default 
settings of the tool in the experiments. 
5.2 Text Normalization Experiments 
Results 
We evaluated the performances of our method 
(Unified) and the baseline methods (Cascaded and 
Independent) on the 12 data sets. Table 5 shows 
the five-fold cross-validation results. Our method 
outperforms the two baseline methods. 
Table 6 shows the overall performances of text 
normalization by our method and the two baseline 
methods. We see that our method outperforms the 
two baseline methods. It can also be seen that the 
performance of the unified method decreases when 
removing the transition features (Unified w/o 
Transition Features). 
693
We conducted sign tests for each subtask on the 
results, which indicate that all the improvements of 
Unified over Cascaded and Independent are statis-
tically significant (p << 0.01). 
Detection Task Prec. Rec. F1 Acc.
Independent 95.16 91.52 93.30 93.81
Cascaded 95.16 91.52 93.30 93.81Extra Line Break  
Unified 93.87 93.63 93.75 94.53
Independent 91.85 94.64 93.22 99.87
Cascaded 94.54 94.56 94.55 99.89Extra Space 
Unified 95.17 93.98 94.57 99.90
Independent 88.63 82.69 85.56 99.66
Cascaded 87.17 85.37 86.26 99.66
Extra 
 Punctuation 
Mark Unified 90.94 84.84 87.78 99.71
Independent 98.46 99.62 99.04 98.36
Cascaded 98.55 99.20 98.87 98.08Sentence Boundary  
Unified 98.76 99.61 99.18 98.61
Independent 72.51 100.0 84.06 84.27
Cascaded 72.51 100.0 84.06 84.27Unnecessary Token 
Unified 98.06 95.47 96.75 96.18
Independent 27.32 87.44 41.63 96.22Case  
Restoration 
(TrueCasing) Cascaded 28.04 88.21 42.55 96.35
Independent 84.96 62.79 72.21 99.01
Cascaded 85.85 63.99 73.33 99.07
Case  
Restoration 
(CRF) Unified 86.65 67.09 75.63 99.21
Table 5. Performances of text normalization (%) 
Text Normalization Prec. Rec. F1 Acc.
Independent (TrueCasing) 69.54 91.33 78.96 97.90
Independent (CRF) 85.05 92.52 88.63 98.91
Cascaded (TrueCasing) 70.29 92.07 79.72 97.88
Cascaded (CRF) 85.06 92.70 88.72 98.92
Unified w/o Transition 
Features 86.03 93.45 89.59 99.01
Unified 86.46 93.92 90.04 99.05
Table 6. Performances of text normalization (%) 
Discussions 
Our method outperforms the independent method 
and the cascaded method in all the subtasks, espe-
cially in the subtasks that have strong dependen-
cies with each other, for example, sentence bound-
ary detection, extra punctuation mark detection, 
and case restoration. 
The cascaded method suffered from ignorance 
of the dependencies between the subtasks. For ex-
ample, there were 3,314 cases in which sentence 
boundary detection needs to use the results of extra 
line break detection, extra punctuation mark detec-
tion, and case restoration. However, in the cas-
caded method, sentence boundary detection is con-
ducted after extra punctuation mark detection and 
before case restoration, and thus it cannot leverage 
the results of case restoration. Furthermore, errors 
of extra punctuation mark detection can lead to 
errors in sentence boundary detection. 
The independent method also cannot make use 
of dependencies across different subtasks, because 
it conducts all the subtasks from the raw input data. 
This is why for detection of extra space, extra 
punctuation mark, and casing error, the independ-
ent method cannot perform as well as our method. 
Our method benefits from the ability of model-
ing dependencies between subtasks. We see from 
Table 6 that by leveraging the dependencies, our 
method can outperform the method without using 
dependencies (Unified w/o Transition Features) by 
0.62% in terms of F1-measure. 
Here we use the example in Figure 1 to show the 
advantage of our method compared with the inde-
pendent and the cascaded methods. With normali-
zation by the independent method, we obtain: 
I?m thinking about buying a pocket PC   device for my wife 
this Christmas, The worry that I have is that she won?t be able 
to sync it to her outlook express contacts.// 
With normalization by the cascaded method, we 
obtain: 
I?m thinking about buying a pocket PC device for my wife 
this Christmas, the worry that I have is that she won?t be able 
to sync it to her outlook express contacts.// 
With normalization by our method, we obtain: 
I?m thinking about buying a Pocket PC device for my wife 
this Christmas.// The worry that I have is that she won?t be 
able to sync it to her Outlook Express contacts.// 
The independent method can correctly deal with 
some of the errors. For instance, it can capitalize 
the first word in the first and the third line, remove 
extra periods in the fifth line, and remove the four 
extra line breaks. However, it mistakenly removes 
the period in the second line and it cannot restore 
the cases of some words, for example ?pocket? and 
?outlook express?. 
In the cascaded method, each process carries out 
cleaning/normalization from the output of the pre-
vious process and thus can make use of the 
cleaned/normalized results from the previous proc-
ess. However, errors in the previous processes will 
also propagate to the later processes. For example, 
the cascaded method mistakenly removes the pe-
riod in the second line. The error allows case resto-
ration to make the error of keeping the word ?the? 
in lower case. 
694
TrueCasing-based methods for case restoration 
suffer from low precision (27.32% by Independent 
and 28.04% by Cascaded), although their recalls 
are high (87.44% and 88.21% respectively). There 
are two reasons: 1) About 10% of the errors in 
Cascaded are due to errors of sentence boundary 
detection and extra line break detection in previous 
steps; 2) The two baselines tend to restore cases of 
words to the forms having higher probabilities in 
the data set and cannot take advantage of the de-
pendencies with the other normalization subtasks. 
For example, ?outlook? was restored to first letter 
capitalized in both ?Outlook Express? and ?a pleas-
ant outlook?. Our method can take advantage of the 
dependencies with other subtasks and thus correct 
85.01% of the errors that the two baseline methods 
cannot handle. Cascaded and Independent methods 
employing CRF for case restoration improve the 
accuracies somewhat. However, they are still infe-
rior to our method. 
Although we have conducted error analysis on 
the results given by our method, we omit the de-
tails here due to space limitation and will report 
them in a future expanded version of this paper. 
We also compared the speed of our method with 
those of the independent and cascaded methods. 
We tested the three methods on a computer with 
two 2.8G Dual-Core CPUs and three Gigabyte 
memory. On average, it needs about 5 hours for 
training the normalization models using our 
method and 25 seconds for tagging in the cross-
validation experiments. The independent and the 
cascaded methods (with TrueCasing) require less 
time for training (about 2 minutes and 3 minutes 
respectively) and for tagging (several seconds). 
This indicates that the efficiency of our method 
still needs improvement. 
6 Conclusion 
In this paper, we have investigated the problem of 
text normalization, an important issue for natural 
language processing. We have first defined the 
problem as a task consisting of noise elimination 
and boundary detection subtasks. We have then 
proposed a unified tagging approach to perform the 
task, specifically to treat text normalization as as-
signing tags representing deletion, preservation, or 
replacement of the tokens in the text. Experiments 
show that our approach significantly outperforms 
the two baseline methods for text normalization. 
References 
E. Brill and R. C. Moore. 2000. An Improved Error 
Model for Noisy Channel Spelling Correction, Proc. 
of ACL 2000. 
V. R. Carvalho and W. W. Cohen. 2004. Learning to 
Extract Signature and Reply Lines from Email, Proc. 
of CEAS 2004. 
K. Church and W. Gale. 1991. Probability Scoring for 
Spelling Correction, Statistics and Computing, Vol. 1. 
A. Clark. 2003. Pre-processing Very Noisy Text, Proc. 
of Workshop on Shallow Processing of Large Cor-
pora. 
A. R. Golding and D. Roth. 1996. Applying Winnow to 
Context-Sensitive Spelling Correction, Proc. of 
ICML?1996. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data, Proc. of ICML 
2001. 
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 
2003. tRuEcasIng, Proc. of ACL 2003. 
E. Mays, F. J. Damerau, and R. L. Mercer. 1991. Con-
text Based Spelling Correction, Information Process-
ing and Management, Vol. 27, 1991. 
A. Mikheev. 2000. Document Centered Approach to 
Text Normalization, Proc. SIGIR 2000. 
A. Mikheev. 2002. Periods, Capitalized Words, etc. 
Computational Linguistics, Vol. 28, 2002. 
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting Personal Names from Email: Applying 
Named Entity Recognition to Informal Text, Proc. of 
EMNLP/HLT-2005. 
D. D. Palmer and M. A. Hearst. 1997. Adaptive Multi-
lingual Sentence Boundary Disambiguation, Compu-
tational Linguistics, Vol. 23. 
C.J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London. 
R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf, 
and C. Richards. 1999. Normalization of non-
standard words, WS?99 Final Report. 
http://www.clsp.jhu.edu/ws99/projects/normal/. 
J. Tang, H. Li, Y. Cao, and Z. Tang. 2005. Email data 
cleaning, Proc. of SIGKDD?2005. 
V. Vapnik. 1998. Statistical Learning Theory, Springer. 
W. Wong, W. Liu, and M. Bennamoun. 2007. Enhanced 
Integrated Scoring for Cleaning Dirty Texts, Proc. of 
IJCAI-2007 Workshop on Analytics for Noisy Un-
structured Text Data. 
695
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 125?128,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Statistical Machine Translation Model Based on a Synthetic
Synchronous Grammar
Hongfei Jiang, Muyun Yang, Tiejun Zhao, Sheng Li and Bo Wang
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,ymy,tjzhao,lisheng,bowang}@mtlab.hit.edu.cn
Abstract
Recently, various synchronous grammars
are proposed for syntax-based machine
translation, e.g. synchronous context-free
grammar and synchronous tree (sequence)
substitution grammar, either purely for-
mal or linguistically motivated. Aim-
ing at combining the strengths of differ-
ent grammars, we describes a synthetic
synchronous grammar (SSG), which ten-
tatively in this paper, integrates a syn-
chronous context-free grammar (SCFG)
and a synchronous tree sequence substitu-
tion grammar (STSSG) for statistical ma-
chine translation. The experimental re-
sults on NIST MT05 Chinese-to-English
test set show that the SSG based transla-
tion system achieves significant improve-
ment over three baseline systems.
1 Introduction
The use of various synchronous grammar based
formalisms has been a trend for statistical ma-
chine translation (SMT) (Wu, 1997; Eisner, 2003;
Galley et al, 2006; Chiang, 2007; Zhang et al,
2008). The grammar formalism determines the in-
trinsic capacities and computational efficiency of
the SMT systems.
To evaluate the capacity of a grammar formal-
ism, two factors, i.e. generative power and expres-
sive power are usually considered (Su and Chang,
1990). The generative power refers to the abil-
ity to generate the strings of the language, and
the expressive power to the ability to describe the
same language with fewer or no extra ambigui-
ties. For the current synchronous grammars based
SMT, to some extent, the generalization ability of
the grammar rules (the usability of the rules for the
new sentences) can be considered as a kind of the
generative power of the grammar and the disam-
biguition ability to the rule candidates can be con-
sidered as an embodiment of expressive power.
However, the generalization ability and the dis-
ambiguition ability often contradict each other in
practice such that various grammar formalisms
in SMT are actually different trade-off be-
tween them. For instance, in our investiga-
tions for SMT (Section 3.1), the Formally SCFG
based hierarchical phrase-based model (here-
inafter FSCFG) (Chiang, 2007) has a better gen-
eralization capability than a Linguistically moti-
vated STSSG based model (hereinafter LSTSSG)
(Zhang et al, 2008), with 5% rules of the former
matched by NIST05 test set while only 3.5% rules
of the latter matched by the same test set. How-
ever, from expressiveness point of view, the for-
mer usually results in more ambiguities than the
latter.
To combine the strengths of different syn-
chronous grammars, this paper proposes a statisti-
cal machine translation model based on a synthetic
synchronous grammar (SSG) which syncretizes
FSCFG and LSTSSG. Moreover, it is noteworthy
that, from the combination point of view, our pro-
posed scheme can be considered as a novel system
combination method which goes beyond the ex-
isting post-decoding style combination of N -best
hypotheses from different systems.
2 The Translation Model Based on the
Synthetic Synchronous Grammar
2.1 The Synthetic Synchronous Grammar
Formally, the proposed Synthetic Synchronous
Grammar (SSG) is a tuple
G = ??
s
,?
t
, N
s
, N
t
, X,P?
where ?
s
(?
t
) is the alphabet set of source (target)
terminals, namely the vocabulary; N
s
(N
t
) is the
alphabet set of source (target) non-terminals, such
125
? ? ???
Figure 1: A syntax tree pair example. Dotted lines
stands for the word alignments.
as the POS tags and the syntax labels; X repre-
sents the special nonterminal label in FSCFG; and
P is the grammar rule set which is the core part of
a grammar. Every rule r in P is as:
r = ??, ?,A
NT
, A
T
, ???
where ? ? [{X}, N
s
,?
s
]
+
is a sequence of one or
more source words in ?
s
and nonterminals sym-
bols in [{X}, N
s
];? ? [{X}, N
t
,?
t
]
+
is a se-
quence of one or more target words in ?
t
and non-
terminals symbols in [{X}, N
t
]; A
T
is a many-to-
many corresponding set which includes the align-
ments between the terminal leaf nodes from source
and target side, and A
NT
is a one-to-one corre-
sponding set which includes the synchronizing re-
lations between the non-terminal leaf nodes from
source and target side; ?? contains feature values
associated with each rule.
Through this formalization, we can see that
FSCFG rules and LSTSSG rules are both in-
cluded. However, we should point out that the
rules with mixture of X non-terminals and syn-
tactic non-terminals are not included in our cur-
rent implementation despite that they are legal
under the proposed formalism. The rule extrac-
tion in current implementation can be considered
as a combination of the ones in (Chiang, 2007)
and (Zhang et al, 2008). Given the sentence pair
in Figure 1, some SSG rules can be extracted as
illustrated in Figure 2.
2.2 The SSG-based Translation Model
The translation in our SSG-based translation
model can be treated as a SSG derivation. A
derivation consists of a sequence of grammar rule
applications. To model the derivations as a latent
variable, we define the conditional probability dis-
tribution over the target translation e and the cor-
Input: A source parse tree T (f
J
1
)
Output: A target translation e?
for u := 0 to J ? 1 do
for v := 1 to J ? u do
foreach rule r = ??, ?,A
NT
, A
T
, ??? spanning
[v, v + u] do
if A
NT
of r is empty then
Add r into H[v, v + u];
end
else
Substitute the non-terminal leaf node pair
(N
src
, N
tgt
) with the hypotheses in the
hypotheses stack corresponding with N
src
?s
span iteratively.
end
end
end
end
Output the 1-best hypothesis in H[1, J] as the final translation.
Figure 3: The pseudocode for the decoding.
responding derivation d of a given source sentence
f as
(1) p
?
(d, e|f) =
exp
?
k
?
k
H
k
(d, e, f)
?
?
(f)
where H
k
is a feature function ,?
k
is the corre-
sponding feature weight and ?
?
(f) is a normal-
ization factor for each derivation of f. The main
challenge of SSG-based model is how to distin-
guish and weight the different kinds of derivations
. For a simple illustration, using the rules listed in
Figure 2, three derivations can be produced for the
sentence pair in Figure 1 by the proposed model:
d
1
= (R
4
, R
1
, R
2
)
d
2
= (R
6
, R
7
, R
8
)
d
3
= (R
4
, R
7
, R
2
)
All of them are SSG derivations while d
1
is also a
FSCFG derivation, d
2
is also a LSTSSG deriva-
tion. Ideally, the model is supposed to be able
to weight them differently and to prefer the better
derivation, which deserves intensive study. Some
sophisticated features can be designed for this is-
sue. For example, some features related with
structure richness and grammar consistency
1
of a
derivation should be designed to distinguish the
derivations involved various heterogeneous rule
applications. For the page limit and the fair com-
parison, we only adopt the conventional features
as in (Zhang et al, 2008) in our current implemen-
tation.
1
This relates with reviewers? questions: ?can a rule ex-
pecting an NN accept an X?? and ?. . . the interaction between
the two typed of rules . . . ?. In our study in progress, we
would design some features to distinguish the derivation steps
which fulfill the expectation or not, to measure how much
heterogeneous rules are applied in a derivation and so on.
126
R6
1?
BA
VV[2]NN[1]
1
VB[2] NP[1]?
PN
to me
TO PRP
PP
1
R7
penthe
DT NN
NP
??
NN
1
R4 Give 1? 1 X[1] X[2]X[2]? X[1] R5 X[1]X[1] ? 2 the pen 1 to 2me1??
R1 penthe 1?? 1 R3 theGive 2 pen 1? 2?? 1R2 to me 1? 1
R8
?
VV
Give
VB
11
Figure 2: Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure
1. R
1
-R
3
are bilingual phrase rules, R
4
-R
5
are FSCFG rules and R
6
-R
8
are LSTSSG rules.
2.3 Decoding
For efficiency, our model approximately search for
the single ?best? derivation using beam search as
(2) (
?
e,
?
d) = argmax
e,d
{
?
k
?
k
h
k
(d, e, f)
}
.
The major challenge for such a SSG-based de-
coder is how to apply the heterogeneous rules in a
derivation. For example, (Chiang, 2007) adopts a
CKY style span-based decoding while (Liu et al,
2006) applies a linguistically syntax node based
bottom-up decoding, which are difficult to inte-
grate. Fortunately, our current SSG syncretizes
FSCFG and LSTSSG. And the conventional de-
codings of both FSCFG and LSTSSG are span-
based expansion. Thus, it would be a natural way
for our SSG-based decoder to conduct a span-
based beam search. The search procedure is given
by the pseudocode in Figure 3. A hypotheses
stack H[i, j] (similar to the ?chart cell? in CKY
parsing) is arranged for each span [i, j] for stor-
ing the translation hypotheses. The hypotheses
stacks are ordered such that every span is trans-
lated after its possible antecedents: smaller spans
before larger spans. For translating each span
[i, j], the decoder traverses each usable rule r =
??, ?,A
NT
, A
T
, ???. If there is no nonterminal
leaf node in r, the target side ? will be added into
H[i, j] as the candidate hypothesis. Otherwise, the
nonterminal leaf nodes in r should be substituted
iteratively by the corresponding hypotheses until
all nonterminal leaf nodes are processed. The key
feature of our decoder is that the derivations are
based on synthetic grammar, so that one derivation
may consist of applications of heterogeneous rules
(Please see d
3
in Section 2.2 as a simple demon-
stration).
3 Experiments and Discussions
Our system, named HITREE, is implemented in
standard C++ and STL. In this section we report
Extracted(k) Scored(k)(S/E%) Filtered(k)(F/S%)
BP 11,137 4,613(41.4%) 323(0.5%)
LSTSSG 45,580 28,497(62.5%) 984(3.5%)
FSCFG 59,339 25,520(43.0%) 1,266(5.0%)
HITREE 93,782 49,404(52.7%) 1,927(3.9%)
Table 1: The statistics of the counts of the rules in
different phases. ?k? means one thousand.
on experiments with Chinese-to-English transla-
tion base on it. We used FBIS Chinese-to-English
parallel corpora (7.2M+9.2M words) as the train-
ing data. We also used SRI Language Model-
ing Toolkit to train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus(181M words). NIST MT2002 test set is used
as the development set. The NIST MT2005 test
set is used as the test set. The evaluation met-
ric is case-sensitive BLEU4. For significant test,
we used Zhang?s implementation (Zhang et al,
2004)(confidence level of 95%). For comparisons,
we used the following three baseline systems:
LSTSSG An in-house implementation of linguis-
tically motivated STSSG based model similar
to (Zhang et al, 2008).
FSCFG An in-house implementation of purely
formally SCFG based model similar to (Chiang,
2007).
MBR We use an in-house combination system
which is an implementation of a classic sentence
level combination method based on the Minimum
Bayes Risk (MBR) decoding (Kumar and Byrne,
2004).
3.1 Statistics of Rule Numbers in Different
Phases
Table 1 summarizes the statistics of the rules for
different models in three phases: after extrac-
tion (Extracted), after scoring(Scored), and af-
ter filtering (Filtered) (filtered by NIST05 test
set just, similar to the filtering step in phrase-
based SMT system). In Extracted phase, FSCFG
127
ID System BLEU4 #of used rules(k)
1 LSTSSG 0.2659?0.0043 984
2 FSCFG 0.2613?0.0045 1,266
3 HITREE 0.2730?0.0045 1,927
4 MBR(1,2) 0.2685?0.0044 ?
Table 2: The Comparison of LSTSSG, FSCFG
,HITREE and the MBR.
has obvious more rules than LSTSSG. However,
in Scored phase, this situation reverses. Inter-
estingly, the situation reverses again in Filtered
phase. The reasons for these phenomenons are
that FSCFG abstract rules involves high-degree
generalization. Each FSCFG abstract rule aver-
agely have several duplicates
2
in the extracted rule
set. Then, the duplicates will be discarded dur-
ing scoring. However, due to the high-degree gen-
eralization , the FSCFG abstract rules are more
likely to be matched by the test sentences. Con-
trastively, LSTSSG rules have more diversified
structures and thus weaker generalization capabil-
ity than FSCFG rules. From the ratios of two tran-
sition states, Table 1 indicates that HITREE can
be considered as compromise of FSCFG between
LSTSSG.
3.2 Overall Performances
The performance comparison results are presented
in Table 2. The experimental results show that
the SSG-based model (HITREE) achieves signifi-
cant improvements over the models based on the
two isolated grammars: FSCFG and LSTSSG
(both p < 0.001). From combination point of
view, the newly proposed model can be consid-
ered as a novel method going beyond the con-
ventional post-decoding style combination meth-
ods. The baseline Minimum Bayes Risk com-
bination of LSTSSG based model and FSCFG
based model (MBR(1, 2)) obtains significant im-
provements over both candidate models (both p <
0.001). Meanwhile, the experimental results show
that the proposed model outperforms MBR(1, 2)
significantly (p < 0.001). These preliminary re-
sults indicate that the proposed SSG-based model
is rather promising and it may serve as an alterna-
tive, if not superior, to current combination meth-
ods.
4 Conclusions
To combine the strengths of different gram-
mars, this paper proposes a statistical machine
2
Rules with identical source side and target side are du-
plicated.
translation model based on a synthetic syn-
chronous grammar (SSG) which syncretizes a
purely formal synchronous context-free gram-
mar (FSCFG) and a linguistically motivated syn-
chronous tree sequence substitution grammar
(LSTSSG). Experimental results show that SSG-
based model achieves significant improvements
over the FSCFG-based model and LSTSSG-based
model.
In the future work, we would like to verify
the effectiveness of the proposed model on vari-
ous datasets and to design more sophisticated fea-
tures. Furthermore, the integrations of more dif-
ferent kinds of synchronous grammars for statisti-
cal machine translation will be investigated.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In computational linguistics, 33(2).
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL 2003.
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-
rich syntactic translation models In Proceedings of
ACL-COLING.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
04.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine transla-
tion. In Proceedings of ACL-COLING.
Keh-Yin Su and Jing-Shin Chang. 1990. Some key
Issues in Designing Machine Translation Systems.
Machine Translation, 5(4):265-300.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of LREC 2004, pages 2051-2054.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li,
Chew Lim Tan and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT.
128
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 213?216,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Chinese Term Extraction Using Different Types of Relevance 
 
 
Yuhang Yang1, Tiejun Zhao1, Qin Lu2, Dequan Zheng1 and Hao Yu1
1School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin 150001, China 
{yhyang,tjzhao,dqzheng,yu}@mtlab.hit.edu.cn 
2Department of Computing,  
The Hong Kong Polytechnic University, Hong Kong, China 
csluqin@comp.polyu.edu.hk 
 
  
 
Abstract 
This paper presents a new term extraction ap-
proach using relevance between term candi-
dates calculated by a link analysis based 
method. Different types of relevance are used 
separately or jointly for term verification. The 
proposed approach requires no prior domain 
knowledge and no adaptation for new domains. 
Consequently, the method can be used in any 
domain corpus and it is especially useful for 
resource-limited domains. Evaluations con-
ducted on two different domains for Chinese 
term extraction show significant improve-
ments over existing techniques and also verify 
the efficiency and relative domain independent 
nature of the approach. 
1 Introduction 
Terms are the lexical units to represent the most 
fundamental knowledge of a domain. Term ex-
traction is an essential task in domain knowledge 
acquisition which can be used for lexicon update, 
domain ontology construction, etc. Term extrac-
tion involves two steps. The first step extracts 
candidates by unithood calculation to qualify a 
string as a valid term. The second step verifies 
them through termhood measures (Kageura and 
Umino, 1996) to validate their domain specificity.  
Many previous studies are conducted on term 
candidate extraction. Other tasks such as named 
entity recognition, meaningful word extraction 
and unknown word detection, use techniques 
similar to that for term candidate extraction. But, 
their focuses are not on domain specificity. This 
study focuses on the verification of candidates by 
termhood calculation.  
Relevance between term candidates and docu-
ments is the most popular feature used for term 
verification such as TF-IDF (Salton and McGill, 
1983; Frank, 1999) and Inter-Domain Entropy 
(Chang, 2005), which are all based on the hy-
pothesis that ?if a candidate occurs frequently in 
a few documents of a domain, it is likely a term?. 
Limited distribution information of term candi-
dates in different documents often limits the abil-
ity of such algorithms to distinguish terms from 
non-terms. There are also attempts to use prior 
domain specific knowledge and annotated cor-
pora for term verification. TV_ConSem (Ji and 
Lu, 2007) calculates the percentage of context 
words in a domain lexicon using both frequency 
information and semantic information. However, 
this technique requires a domain lexicon whose 
size and quality have great impact on the per-
formance of the algorithm. Some supervised 
learning approaches have been applied to pro-
tein/gene name recognition (Zhou et al, 2005) 
and Chinese new word identification (Li et al, 
2004) using SVM classifiers (Vapnik, 1995) 
which also require large domain corpora and an-
notations. The latest work by Yang (2008) ap-
plied the relevance between term candidates and 
sentences by using the link analysis approach 
based on the HITS algorithm to achieve better 
performance. 
In this work, a new feature on the relevance 
between different term candidates is integrated 
with other features to validate their domain 
specificity. The relevance between candidate 
terms may be useful to identify domain specific 
terms based on two assumptions. First, terms are 
more likely to occur with other terms in order to 
express domain information. Second, term can-
didates extracted from domain corpora are likely 
213
to be domain specific. Previous work by (e.g. Ji 
and Lu, 2007) uses similar information by com-
paring the context to an existing large domain 
lexicon. In this study, the relevance between 
term candidates are iteratively calculated by 
graphs using link analysis algorithm to avoid the 
dependency on prior domain knowledge.  
The rest of the paper is organized as follows. 
Section 2 describes the proposed algorithms. 
Section 3 explains the experiments and the per-
formance evaluation. Section 4 concludes and 
presents the future plans. 
2 Methodology 
This study assumes the availability of term can-
didates since the focus is on term verification by 
termhood calculation. Three types of relevance 
are first calculated including (1) the term candi-
date relevance, CC; (2) the candidate to sentence 
relevance, CS; and the candidates to document 
relevance, CD. Terms are then verified by using 
different types of relevance. 
2.1 Relevance between Term Candidates 
Based on the assumptions that term candidates 
are likely to be used together in order to repre-
sent a particular domain concept, relevance of 
term candidates can be represented by graphs in 
a domain corpus. In this study, CC is defined as 
their co-occurrence in the same sentence of the 
domain corpus. For each document, a graph of 
term candidates is first constructed. In the graph, 
a node is a term candidate. If two term candi-
dates TC1 and TC2 occur in the same sentence, 
two directional links between TC1 to TC2 are 
given to indicate their mutually related. Candi-
dates with overlapped substrings are not removed 
which means long terms can be linked to their 
components if the components are also candi-
dates.  
After graph construction, the term candidate 
relevance, CC, is then iteratively calculated using 
the PageRank algorithm (Page et al 1998) origi-
nally proposed for information retrieval. PageR-
ank assumes that the more a node is connected to 
other nodes, it is more likely to be a salient node. 
The algorithm assigns the significance score to 
each node according to the number of nodes link-
ing to it as well as the significance of the nodes. 
The PageRank calculation PR of a node A is 
shown as follows:  
)
)(
)(
...
)(
)(
)(
)(
()1()(
2
2
1
1
t
t
BC
BPR
BC
BPR
BC
BPR
ddAPR ++++?=
(1) 
where B1, B2,?, Bt are all nodes linked to node A; 
C(Bi) is the number of outgoing links from node 
Bi; d is the factor to avoid loop trap in the 
graphic structure. d is set to 0.85 as suggested in 
(Page et al, 1998). Initially, all PR weights are 
set to 1. The weight score of each node are ob-
tained by (1), iteratively. The significance of 
each term candidate in the domain specific cor-
pus is then derived based on the significance of 
other candidates it co-occurred with. The CC 
weight of term candidate TCi is given by its PR 
value after k iterations, a parameter to be deter-
mined experimentally. 
2.2 Relevance between Term Candidates 
and Sentences 
A domain specific term is more likely to be con-
tained in domain relevant sentences. Relevance 
between term candidate and sentences, referred 
to as CS, is calculated using the TV_HITS (Term 
Verification ? HITS) algorithm proposed in 
(Yang et al, 2008) based on  Hyperlink-Induced 
Topic Search (HITS) algorithm (Kleinberg, 
1997). In TV_HITS, a good hub in the domain 
corpus is a sentence that contains many good 
authorities; a good authority is a term candidate 
that is contained in many good hubs.  
In TV_HITS, a node p can either be a sentence 
or a term candidate. If a term candidate TC is 
contained in a sentence Sen of the domain corpus, 
there is a directional link from Sen to TC. 
TV_HITS then makes use of the relationship be-
tween candidates and sentences via an iterative 
process to update CS weight for each TC.  
Let VA(w(p1)A, w(p2)A,?, w(pn)A) denote the 
authority vector and VH(w(p1)H, w(p2)H,?, w(pn)H) 
denote the hub vector. VA and VH are initialized 
to (1, 1,?, 1). Given weights VA and VH with a 
directional link p?q, w(q)A and w(p)H are up-
dated by using the I operation(an in-pointer to a 
node) and the O operation(an out-pointer to a 
node) shown as follows. The CS weight of term 
candidate TCi is given by its w(q)A value after 
iteration. 
I operation:          (2) ?
??
=
Eqp
HA w(p)w(q)
O operation:         (3) ?
??
=
Eqp
AH w(q)w(p)
2.3 Relevance between Term Candidates 
and Documents 
The relevance between term candidates and 
documents is used in many term extraction algo-
214
rithms. The relevance is measured by the TF-IDF 
value according to the following equations: 
)IDF(TC)TF(TC)TFIDF(TC iii ?=      (4) 
)
)(
log()(
i
i TCDF
D
TCIDF =             (5) 
where TF(TCi) is the number of times term can-
didate TCi occurs in the domain corpus, DF(TCi) 
is the number of documents in which TCi occurs 
at least once, |D| is the total number of docu-
ments in the corpus, IDF(TCi) is the inverse 
document frequency which can be calculated 
from the document frequency. 
2.4 Combination of Relevance 
To evaluate the effective of the different types of 
relevance, they are combined in different ways in 
the evaluation. Term candidates are then ranked 
according to the corresponding termhood values 
Th(TC) and the top ranked candidates are con-
sidered terms.  
For each document Dj in the domain corpus 
where a term candidate TCi occurs, there is CCij 
weight and a CSij weight. When features CC and 
CS are used separately, termhood ThCC(TCi) and 
ThCS(TCi) are calculated by averaging CCij and 
CSij, respectively. Termhood of different combi-
nations are given in formula (6) to (9). R(TCi) 
denotes the ranking position of TCi.  
)(TCR)(TCR
)(TCTh
iCSiCC
iCSCC
11 +=+    (6) 
)log()()(
Cj
ijiCDCC DF
D
CCTCTh ?=+     (7) 
)log()()(
Cj
ijiCDCS DF
D
CSTCTh ?=+     (8) 
)(TCR)(TCR
TCTh
iCDCSiCDCC
iCDCSCC
++
++ += 11)( (9) 
3 Performance Evaluation 
3.1 Data Preparation 
To evaluate the performance of the proposed 
relevance measures for Chinese in different do-
mains, experiments are conducted on two sepa-
rate domain corpora CorpusIT and CorpusLegal., 
respectively. CorpusIT includes academic papers 
of 6.64M in size from Chinese IT journals be-
tween 1998 and 2000. CorpusLegal includes the 
complete set of official Chinese constitutional 
law articles and Economics/Finance law articles 
of 1.04M in size (http://www.law-lib.com/).  
For comparison to previous work, all term 
candidates are extracted from the same domain 
corpora using the delimiter based algorithm 
TCE_DI (Term Candidate Extraction ? Delimiter 
Identification) which is efficient according to 
(Yang et al, 2008). In TCE_DI, term delimiters 
are identified first. Words between delimiters are 
then taken as term candidates. 
The performances are evaluated in terms of 
precision (P), recall (R) and F-value (F). Since 
the corpora are relatively large, sampling is used 
for evaluation based on fixed interval of 1 in 
each 10 ranked results. The verification of all the 
sampled data is carried out manually by two ex-
perts independently. To evaluate the recall, a set 
of correct terms which are manually verified 
from the extracted terms by different methods is 
constructed as the standard answer. The answer 
set is certainly not complete. But it is useful as a 
performance indication for comparison since it is 
fair to all algorithms. 
3.2 Evaluation on Term Extraction 
For comparison, three reference algorithms are 
used in the evaluation. The first algorithm is 
TV_LinkA which takes CS and CD into consid-
eration and performs well (Yang et al, 2008). 
The second one is a supervised learning ap-
proach based on a SVM classifier, SVMlight 
(Joachims, 1999). Internal and external features 
are used by SVMlight. The third algorithm is the 
popular used TF-IDF algorithm. All the refer-
ence algorithms require no training except 
SVMlight. Two training sets containing thousands 
of positive and negative examples from IT do-
main and legal domain are constructed for the 
SVM classifier. The training and testing sets are 
not overlapped. 
Table 1 and Table 2 show the performance of 
the proposed algorithms using different features 
for IT domain and legal domain, respectively. 
The algorithm using CD alone is the same as the 
TF-IDF algorithm. The algorithm using CS and 
CD is the TV_LinkA algorithm.  
Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 63.6 49.5 55.6 
CC 47.1 36.5 41.2 
CS 65.6 51 57.4 
CD(TF-IDF) 64.8 50.4 56.7 
CC+CS 80.4 62.5 70.3 
CC+CD 49 38.1 42.9 
CS+CD 
(TV_LinkA) 
75.4 58.6 66 
CC+CS+CD 82.8 64.4 72.4 
Table 1. Performance on IT Domain 
215
 Algorithms Precision 
(%) 
Recall 
(%) 
F-value 
(%) 
SVM 60.1 54.2 57.3 
CC 45.2 40.3 42.6 
CS 70.5 40.1 51.1 
CD(TF-IDF) 59.4 52.9 56 
CC+CS 64.2 49.9 56.1 
CC+CD 48.4 43.1 45.6 
CS+CD 
(TV_LinkA) 
67.4 60.1 63.5 
CC+CS+CD 70.2 62.6 66.2 
Table 2. Performance on Legal Domain 
Table 1 and Table 2 show that the proposed 
algorithms achieve similar performance on both 
domains. The proposed algorithm using all three 
features (CC+CS+CD) performs the best. The 
results confirm that the proposed approach are 
quite stable across domains and the relevance 
between candidates are efficient for improving 
performance of term extraction in different do-
mains. The algorithm using CC only does not 
achieve good performance. Neither does CC+CS. 
The main reason is that the term candidates used 
in the experiments are extracted using the 
TCE_DI algorithm which can extract candidates 
with low statistical significance. TCE_DI pro-
vides a better compromise between recall and 
precision. CC alone is vulnerable to noisy candi-
dates since it relies on the relevance between 
candidates themselves. However, as an addi-
tional feature to the combined use of CS and CD 
(TV_LinkA), improvement of over 10% on F-
value is obtained for the IT domain, and 5% for 
the legal domain. This is because the noise data 
are eliminated by CS and CD, and CC help to 
identify additional terms that may not be statisti-
cally significant.  
4 Conclusion and Future Work 
In conclusion, this paper exploits the relevance 
between term candidates as an additional feature 
for term extraction approach. The proposed ap-
proach requires no prior domain knowledge and 
no adaptation for new domains. Experiments for 
term extraction are conducted on IT domain and 
legal domain, respectively. Evaluations indicate 
that the proposed algorithm using different types 
of relevance achieves the best performance in 
both domains without training.  
In this work, only co-occurrence in a sentence 
is used as the relevance between term candidates. 
Other features such as syntactic relations can 
also be exploited. The performance may be fur-
ther improved by using more efficient combina-
tion strategies. It would also be interesting to 
apply this approach to other languages such as 
English. 
Acknowledgement: The project is partially sup-
ported by the Hong Kong Polytechnic University 
(PolyU CRG G-U297) 
References 
Chang Jing-Shin. 2005. Domain Specific Word Ex-
traction from Hierarchical Web Documents: A 
First Step toward Building Lexicon Trees from 
Web Corpora. In Proc of the 4th SIGHAN Work-
shop on Chinese Language Learning: 64-71. 
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. Do-
main-specific Keyphrase Extraction. In Proc.of 
16th Int. Joint Conf. on AI,  IJCAI-99: 668-673. 
Joachims T. 2000. Estimating the Generalization Per-
formance of a SVM Efficiently. In Proc. of the Int 
Conf. on Machine Learning, Morgan Kaufman, 
2000. 
Kageura K., and B. Umino. 1996. Methods of auto-
matic term recognition: a review. Term 3(2):259-
289. 
Kleinberg J. 1997. Authoritative sources in a hyper-
linked environment. In Proc. of the 9th ACM-SIAM 
Symposium on Discrete Algorithms: 668-677. New 
Orleans, America, January 1997. 
Ji Luning, and Qin Lu. 2007. Chinese Term Extrac-
tion Using Window-Based Contextual Information. 
In Proc. of CICLing 2007, LNCS 4394: 62 ? 74. 
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and 
Xiaozhong Fan. The Use of SVM for Chinese New 
Word Identification. In Proc. of the 1st Int.Joint 
Conf. on NLP (IJCNLP2004): 723-732. Hainan Is-
land, China, March 2004. 
Salton, G., and McGill, M.J. (1983). Introduction to 
Modern Information Retrieval. McGraw-Hill. 
S. Brin, L. Page. The anatomy of a large-scale hyper-
textual web search engine. The 7th Int. World Wide 
Web Conf, Brisbane, Australia, April 1998, 107-
117. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, 1995. 
Yang Yuhang, Qin Lu, Tiejun Zhao. (2008). Chinese 
Term Extraction Using Minimal Resources. The 
22nd Int. Conf. on Computational Linguistics (Col-
ing 2008). Manchester, Aug., 2008, 1033-1040. 
Zhou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. 
Recognition of Protein/Gene Names from Text us-
ing an Ensemble of Classifiers. BMC Bioinformat-
ics 2005, 6(Suppl 1):S7. 
216
Statistics Based Hybrid Approach to Chinese Base Phrase Identification 
Tie-jun ZHAO, Mu-yun YANG~ Fang LIU, Jian-min YAO, Hao YU 
Department of Computer Science and Engineering, Harbin Institute of Technology 
{tjzaho, )may, flu.fang, james, yu} @mtlab.hit.edu.en 
ABSTRACT 
This paper extends the base noun 
phrase(BNP) identification i to a research 
on Chinese base phrase identification. ARer 
briefly introducing some basic concepts on 
Chinese base phrase, this paper presents a
statistics based hybrid model for identifying 
7 types of Chinese base phrases in view. 
Experiments how the efficiency of the 
proposed method in simplifying sentence 
structure. Significance of the research es in 
it provides a solid foundation for the 
Chinese parser. 
Keywords: Chinese base phrase 
identification, parsing, statistical model 
1 Introduction 
Decomposing syntactic analysis into 
several phases o as to decrease its difficulty 
is a new stream in NIP research. The 
successful POS tagging has encouraged 
researchers to explore further possibility for 
resolving sub-problems in parsing(Zhou, et 
al, 1999). The typical examples are the 
recognition of BaseNP in English and 
Chinese. 
In English BNP (base noun phrase) is 
defined as simple and non-nesting noun 
phrases, i.e. noun phrases that do not contain 
other noun phrase descendants (Church, 
1988). After that researches on BNP 
identification reports promising results for 
such task in English. Observing that the 
Chinese BNP is different form English, 
(Zhao & Huang, 1999) puts forward the 
definition of Chinese BNP in terms of 
combination of determinative modifier and 
head noun. According to them a BNP in 
Chinese can be recursively defined as: 
BaseNP ::= Determinative modifier + 
Noun I Nominalized verb(NIO 
Determinative modifier ::= Adjective I
Differentiable Adjective(DA) I Verb I Noun I 
Location I String l Numeral + Classifier 
Inspired by these researches, we extend 
the concept of BNP to Base Phrase in 
Chinese. It is based on such knowledge that 
there are many structures, not only NP, in 
which the trivial components closely attach 
to their central words and constitute a basic 
phrase in a Chinese sentence. Obviously, 
resolving all these base phrases will greatly 
benefit Chinese parser by reliving it from 
some pre-processing (though non-trivial) 
and enable it focus on the most subtle 
syntactic structures. 
Since the whole system of Chinese base 
phrase is still under discussing, this paper 
just presents some tentative research 
achievements on statistics based hybrid 
model to Chinese base phrase identification. 
For the 7 types we considered at present, our 
algorithm turns out promising results and 
smoothes the way for a better Chinese 
parser. 
2 Statistics Based Hybrid Approach to 
Chinese Base Phrase Identification 
2.1 Concepts and Defmitions 
In addition to BNP, constituents of 
many local structure in Chinese centers 
around a core word with certain fixed POS 
sequences. Therefore their identification is
slightly different from parsing in that it 
bears relatively simple phenomenon. Like 
BNP identification, identification of these 
phenomena before parsing will provide a 
simpler sequence for parser, and thus 
deserves a separate r search. 
CutTenfly, we are considering 7 Chinese 
base phrases in our research, namely base 
adjective phrase(BADJP), base adverbial 
phrase (BADVP), base noun phrase (BNP), 
73 
base temporal phrase (BTN), base location 
phrase (BNS), base verb phrase (BVP) and 
base quantity phrase (BMP) Though 
theoretically definitions for these base 
phrases are still unavailable, Appendix I lists 
the preliminary illustrations for them in 
BNF format (necessary account for POS 
annotation can also be found).. 
To frame the identification of Chinese 
base phrases, we fm'ther develop the 
following concepts: 
Definition 1: Chinese based phrases are 
recognized as atomic parts of a sentence 
beyond words that posses certain functions 
and meanings. A base phrase may consist of 
words or other base phrases, but its 
constituents, in turn, should not contain any 
base phrases. 
Definition 2: Base phrase tag is the 
token representing the syntactic function of 
the phrase. At present, base tag either falls in 
one of the 7 Chinese base phrases we are 
considering or not: 
Phrase-Tag ::= BADJP I BADVP I BNP I 
Br  r l Bm I BrP I BMP I lVULL 
Definition 3: Boundary tag denotes the 
possible relative position of a word to a base 
phrase. A boundary tag for a gfven word is 
either L( left boundary of a base phrase), 
R( right boundary of a ), I(inside a base 
phrase) or O(outside the base phrase). 
2.2 Duple Based HMM Parser 
Based on above definitions, we could, 
in view of Wojciech's proposal \[Wojeieeh and 
Thorsten, 1998\], interpret the parsing of 
Chinese base phrases as the following: 
Suppose the input as a sequence of POS 
annotations T= (to, ....... t , , ) .  The task is to 
find RC, a most possible sequence of duples 
formed by base phrase tags and boundary 
tags, among the POS sequence T.
RC = (<ro, co > ........ <rn, Cn>), 
in whil~h ri ( l  <i< =n )indicates the boundary 
tags, ci represents he base phrase tags. 
To go along with the POS tagger 
developed previously by us, we first think of 
preserving HMM (hidden Markov Model) 
for parsing Chinese base phrases. Thus the 
following formula is usually?at hand: 
RC = arg max p(RC I T) 
= arg max p(RC)*  p (T IRC)  
p(T) 
For a given sequence of T, this formula 
can be transformed into: 
RC = arg max p(RC IT) 
= arg max p(RC)*p(T  \ [RC)  
Essentially this model could be 
established through bigram or tri-gram 
statistical training by a annotated corpus. In 
practice, we just build our model from 
l O, O00 manual annotated sentences with 
common bi-gram training: 
p(RC p(RC , IRC  ,_,) 
i=1 
p(T  I RC ) = 1FI p (T i  I RC i) 
i= l  
In realization, a Viterbi algorithm is 
adopted to search the best path. An open test 
on additional 1000 sentences i  performed to 
check its accuracy. Results are shown in 
Tablel(note precision is calculated by 
word'k 
Precision 
for R 
Close 85.7% Test 
Open 82.4% 
Test 
Precision Precision for Both for .C RandC 
87.5% 79.0% 
85.1% 74.7% 
Table 1. Results for Duple Based HMM 
2.3 Triple Based MM Exploiting 
Linguistic Information 
Although results shown in Table 1 i s  
encouraging enough for research purposes, 
it is still lies a long way for practical 
Chinese parser we are aiming at. Reasons 
for errors may be account by too 
coarse-grained information provided by RC. 
Observing the fact that the Chinese base 
phrase occurs more frequently with some 
fixed patterns, i.e. some frozen POS chains, 
we decide to improved our previous model 
by emphasizing the contribution given by 
POS information. 
Adding t denoting POS in the duple (r, 
74 
c), we develop a triple in the form of (t,r,e) 
for the calculation of a node. Naturally, the 
new model is changed into a MM (Markov 
model) as: 
TRC = arg max p(TRC ) 
= arg max I~  p(TRC i I TRC i - 1) 
To train this model, we still using a 
bi-gram model. Applying the same corpus 
and tests described above, we got the 
performance of triple based MM identifier 
for Chinese base phrases (see Table 2). 
Precision Precision Precision 
for R ~rC 
89.2% 91 .5% 84.6% 
88.4% 89.9% 83% 
Close 
Open 
for Both 
R and C 
Table 2. Result for Triple Based MM 
2.4 Further  Improvement Through TBED 
Learning 
Like other statistical models, the above 
model, whether duple based or triple based, 
both seem to reach an accuracy ceiling after 
enlarging training set to 12, 000 or so. To 
cover the remaining accuracy, we apply the 
transformation-based error driven (TBED) 
learning strategy described in \[Brill, 1992\] 
to acquired esired rules. 
In our module, some initial rules are 
first designed as compensation of statistical 
model. Applying these rules will cause new 
mistakes as well as make correct 
identifications. Then the module will 
compare the processed texts with training 
sentences, generate new rules according to 
pre-defmed actions and update its rule bank 
after evaluation (see Fig 1.). 
I I Compare and Rules Passing 
Generate New Rules - - - - !~ Evaluation I 
Tt 
TextTraining \] TextPr?eessed \]
Identifier 
'T 
Input Text 
Figure 1. TBED Learning Module 
The dotted line in fig 2. will stop 
functioning if pre-set accuracy is reached by 
the identifier for the Chinese base phrase. 
Evaluation of new rules is based on an 
greedy algorithm: only rule with max 
contribution (max correction and rain error) 
will be added. Design of rule generation 
(pre-defined actions) is similar to those 
described in \[Brill, 1992\]. 
Table 3 shows a significant 
improvement after applying rules obtained 
through TBED learner. It is also the final 
performance of the proposed Chinese base 
phrase identification model. 
Precision Precision Precision for Both for R for C Rand C 
91.2% 92.8% 89% 
90.4% 91.1% 87.1% 
Close 
open 
Table 3. Results after TBED Module 
3 Conclusions and Discussions 
We have accomplished preliminary 
expedments on identification of various 
types of base phrases defined in this paper. 
The data shown in last seetion prove that our 
method generates atisfactory results for 
75 
Chinese base phrase identification. The 
overall process of our method is outlined the 
following figure. 
Input Chinese Sentences 
after Sengmentation and 
POS tagging 
~ Converted into Nodes 
to Be Parsed 
Triple Based Bi-gram 
MM with Viterbi 
Algorithm 
TBED Based 
Correction 
T" ' Output 
\ 
Fig 2. Processing ofChinese Based Phrase Identification 
However, the 7 types Chinese base 
phrases we have proposed are far l~om 
perfection. Even what we have proposed for 
the 7 phrases is still under test. Further 
improvement will focus on two aspects: one 
is to discuss and add new base phrase for a 
broader coverage; the other is to define, 
theoretically or empirically, the Chinese 
base phrases with more strict constraints. Of 
course, new techniques to improved the 
accuracy of statistical model are the constant 
aim of our research. 
To sum up, Chinese base phrase 
identification will reduce complexity of a 
Chinese parser. The successful idemifieation 
of the 7 base phrases clearly simplifies the 
structure of the sentence. We expect hat the 
research described in this paper will lay a 
solid foundation for a high-accuracy 
Chinese parser. 
22(2): pp141-146 
\[Zhou, et al 1999\] Zhou Qiang, Sun 
Mao-Song, Huang Chang-Ning, Chunk 
parsing scheme for Chinese sentences, 
Chinese J. Computer, 22(11): pp1159-1165 
Reference 
\[Church, 1988\] K. Church, A stochastic 
parts program and noun phrase parser for 
unrestricted text, In: Proc. of Second 
Conference on Applied Natural Language 
Processing, 1988 
\[Wojciech and Thorsten, 1998\] Wojciech Skut and 
Thorsten Brants, Chunk Tagger, Statistical 
Recongnition of Noun Phrases, In ESSLLI-98 
Workshop on Automated Acquisition of Syntax 
and Parsing, Saarbrvcken, 1998. 
\[Zhao & Huang, 1999\] Zhao Jun and Huang 
Chang-Ning, The model for Chinese baseNP 
structure analysis, Chinese J. Computer, 
76 
Appendix Illustration of 7 Chinese Base 
Phrases in BNF 
The patterns listed here are far from 
complete (even for the 7 phrases 
themselves). Theoretical definition is 
beyond this paper and what we provide here 
is actually stage results of expert 
observation and linguistic abstraction. 
BADJP ::= d++a \[ d+BADJP \] a + I a+BADJP 
\[ BADVP+a I BADVP+BADJP 
BADVP ::= a+usdi(:~) I d+usdi I vg+usdi I 
BADJP+usdi IBADVP+usdi IBMP+usdi 
BMP ::= m + \[ m*+q* \[ m+q+m \[ d+m+q \] 
f+m+q \[ r+m+q I BMP ? 
BNP ::= a+n I a+usde(~)+n I a+usde+BNP I 
a+BNP \]b+n \] b+usde+n I b+usde+BNP I 
b+BNP I d+usde+n I f+n I f+usde+n I f+BNP 
1 m+n I m+BNP I n+ I n+usde+n I
n+usde+BNP I n+usde+BMP I n+BNP I q+n 
I q+BNP I r+a+n I r+m+n I r+n I r+usde+n I 
r+usde+BNP \[ r+BNP I s+n I s+usde+n \[
s+usde+BNP I t+nl t+usde+n \[ t+usde+BNP 
I vg +usde+n I vg+usde+BNP I BADJP+n 
BADJP+usde+n \] BADJP+usde+BNP 
BADJP+BNP \[ BMP+n \[ BMP+usde+n 
BMP+usde+BNP \[ BMP+BNP \[ BNP+n 
BNP+usde+n \[ BNP+usde+BNP 
BNP+usde+BMP \[ BNP+BNP 
BNS+usde+n \[ BNS+usde+BNP 
BNS+BNP I BTN+usde+n 
BTN+usde+BNP \[ BVP+usde+n 
BVP+usde+BNP 
BNS ::= a+nd I m+nd I n+s I r+nd I 
n+usde+f I n+usde+nd I n+usde+s I 
n+usde+BNS I nd + I r+usde+nd \[ r+usde+s I 
s+usde+nd I s+usde+BNS I BNP BNS I 
BNS + 
BTN ::= a+t I m+t I r+t I t+ I t+usd~t  I 
BMP+t I BTN+t I BNP+usde+t 
BVP ::= a+vg I d+vg I vg+d+a I vg+d+vq I 
vg+d+vb I vg+usdf(~)+a I vg+usdf+d I 
vg+usdf+vq \[ vg+usdf+u I vg+usdf+BADJP I 
vg+ut I vg+vb I vg+ut+vq I vq+vg I vq+BVP 
\] vz+vg I vz+BVP I BADJP+vg I 
BADVP+vg \[ BADVP+BVP I BVP+ut I 
BVP+vq I BVP+BVP 
Symbol 
a 
d 
Part-Of-Speech 
Adjective 
Adverb 
TemporaYspacial 
position word 
Examples 
~(beaut i fu l ) ,  ~( romant ic )  
~(very), ~(s t i l l )  
~(in), _k(on), ~N(between) 
m numeral --(one), ~(two), -~(three) 
n noun ~ ~ (people), ~ ~I~  (tomato), 
"bl-~JL(computer) 
nd Name of place ~(Be i j ing) ,  I I~(Harb in ) ,  
~.\]t~(New York) 
q classifier \]\]~(flock), +(NULL) 
r pronoun '~'~(you), ~(I, me), ~(he, him) 
s location oun I~.(around), ~:gb(outside) 
t time noun ~;~(yesterday), --L~ (July) 
ut tense auxiliary ~,T,~c_(NULL) 
vb Complemental verb ~,~_t(NULL) 
vg common verb ~ll~(know), ~( long  for) 
vq directional verb ~,T  ~i~(NULL) 
vz modal verb ~I ~(can), )~(shou ld )  
Table for POS symbols used in Appendix 
77 
Automatic Information Transfer Between English And Chinese 
Jianmin Yao, Hao Yu, Tiejun Zhao  
School of Computer Science and Technology 
Harbin Institute of Technology 
Harbin, China, 150001 
james@mtlab.hit.edu.cn 
Xiaohong Li 
Department of Foreign Studies 
Harbin Institute of Technology 
Harbin, China, 150001 
goodtreeyale@yahoo.com.cn 
 
Abstract  
The translation choice and transfer modules 
in an English Chinese machine translation 
system are introduced. The translation 
choice is realized on basis of a grammar tree 
and takes the context as a word bag, with the 
lexicon and POS tag information as context 
features. The Bayes minimal error 
probability is taken as the evaluation 
function of the candidate translation. The 
rule-based transfer and generation module 
takes the parsing tree as the input and 
operates on the information of POS tag, 
semantics or even the lexicon. 
Introduction 
Machine translation is urgently needed to get 
away with the language barrier between 
different nations. The task of machine 
translation is to realize mapping from one 
language to another. At present there are three 
main methods for machine translation systems 
[Zhao 2000]: 1) pattern/rule based systems: 
production rules compose the main body of the 
knowledge base. The rules or patterns are often 
manually written or automatically acquired from 
training corpus; 2) example based method. The 
knowledge base is a bilingual corpus of source 
slices S? and their translations T? Given a source 
slice of input S, match S with the source slices 
and choose the most similar as the translation or 
get the translation from it. 3) Statistics based 
method: it is a method based on monolingual 
language model and bilingual language model. 
The probabilities are acquired from large-scale 
(bilingual) corpora.  
Machine translation is more than a 
manipulation of one natural language (e.g. 
Chinese). Not only the grammatical and 
semantic characteristics of the source language 
must be considered, but also those of the target 
language. To sum up, the characteristics of 
bilingual translation is the essence of a machine 
translation system.  
A machine translation system usually 
includes 3 sub-systems [Zhao 1999] ? (1) 
Analysis: to analyse the source language 
sentence and generate a syntactic tree with 
syntactic functional tags; (2) Transfer: map a 
source parsing tree into a target language parsing 
tree; (3) Generation: generate the target 
language sentence according to the target 
language syntactic tree.  
The MTS2000 system developed in Harbin 
Institute of Technology is a bi-directional 
machine translation system based on a 
combination of stochastic and rule-based 
methods. Figure 1 shows the flow of the system.  
 Input English Sentence  
 Morphology Analysis  
 
Syntactic Analysis  
 
Word Translation Choice  
 
Transfer and Generation  
 
 
 
Output Chinese Sentence 
Figure 1 Flowchart of MTS2000 System  
Analysis and transfer are separated in the 
architecture of the MTS2000 system. This 
modularisation is helpful to the integration of 
stochastic method and the rule based method. 
New techniques are easier to be integrated into 
the modularised system. Two modules 
implement the transfer step and the generation 
step after analysis of the source sentence. The 
specific task of transfer and generation is to 
produce a target language sentence given the 
source language syntactic tree. In details, given 
an English syntactic tree (e.g. S[PP[ In/IN 
BNP[our/PRP$ workshop/NN]] BNP[ there/EX] 
VP[ is/VBZ NP[ no/DT NP[ NN[ machine/NN 
tool/NN] SBAR[ but/CC VP[ is/VBZ 
made/VBN PP[ in/IN BNP[ China/NNP ]]]]]]]]), 
using knowledge sources such as grammatical 
features, simple semantic features, construct a 
Chinese syntactic tree, whose terminal nodes 
compromise in sequence the Chinese translation.  
The input sentence are analysed using the 
morphology analyser, part-of-speech tagger, and 
syntactic analyser. After these steps, a syntactic 
parsing tree is obtained which has multiple 
levels with functional tags [Meng 2000]. 
Followed is the parser flow: 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Parser based on Hybrid Methods 
At present, our English parser is able to 
generate syntactic tree ble 
way. The English parsin
information about relatio
in the source sentence
information of the nodes
of transfer and generatio
the nodes is the starting
generation. After syntact
transfer and generation in
choice of ambiguous 
adjustment and inserti
functional words. Trans
implemented using two
word translation choice, 
transfer and translation m
1 Parsing Based Tran
First we will give a f
translation choice in 
[Manning 1999]: Suppose the source sentence to 
be translated to be ES. In the sentence the 
ambiguous word EW has M target translations 
CW1, CW2, ... CWM. And the translations 
occurs in a specific context C with probabilities 
P(CW1 | C)?P(CW2 | C), ... P(CWM|C)?From 
the Bayes minimum error probability formula, 
we get: 
CW = argmax[P(CWk|C)] 
= argmax[logP(CWk) + logP(C|CWk) ] (1) 
Generally when the condition fulfills 
P(CW1|C)>P(CW2|C)>...>P(CWM|C), we may 
choose CW1 as the translation for EW. From the 
Na?ve Bayes formula? 
P(C|CWk) = P({vj | vj in C}|CWk) 
 = ?Vj in C P(vj|sk)              (2) 
So formula (1) can be rewritten as: 
CW = argmax[P(CWk|C)] 
Input Sentence Statistics Knowledge = argmax[logP(CWk)+?Vj in ClogP(vj|CWk)] (3) 
Where P(CWk) denotes the probability that 
CWk occurs in the corpus; P (vj| CWk) denotes 
the probability that the context feature vj 
co-occurs with translation CWk? 
A general algorithm of supervised word 
sense disambiguation is as follows: 
1. comment: Training 
2. for all senses sk of w do 
3.    for all words vj in the vocabulary do 
4.       P(vj|sk) = C(vj, sk)/C(vj) 
5.    end 
6. end 
7. for all senses sk of w do 
8.    P(sk) = C(sk)/C(w) 
9. end 
POS Tagger Manual Rule 
Base 
PPA Resolution 
Layered Parsing eein comparative usa
g 
nsh
, a
, is
n. 
 p
ic 
clu
w
on
fer 
 m
the
od
sla
orm
mParsing Trtree, with the basic 
ip among the nodes 
lso with semantic 
 input to the module 
The information of 
oint of transfer and 
parsing, the task of 
des word translation 
ords, word order 
/deletion of some 
and generation are 
odules: one is for 
 other for structure 
ification. 
tion Choice 
al description for 
achine translation 
10. comment: Disambiguation 
11. for all sense sk of w do 
12.    Score(sk) = logP(sk) 
13.    for all words vj in the context window c do
14.       score(sk) = score(sk) + logP(vj|sk) 
15.    end 
16. end 
17. choose s? = argmaxskscore(sk) 
Figure 5. Bayesian disambiguation 
From the above formal description we can 
see that the key to the stochastic word 
translation is to select proper context and context 
features Vj. Present methods often define a word 
window of some size, i.e. to suppose only words 
within the window contributes to the translation 
choice of the ambiguous word. For example, 
[Huang 1997] uses a word window of length 6 
words for word sense disambiguation; [Xun 
1998] define a moveable window of length 4 
words; [Ng 1997] uses a word window with 
offset ?2. But two problems exist for this 
method: (1) some words that are informative to 
sense disambiguation may not be covered by the 
window; (2) some words that are covered by the 
word window really contribute nothing to the 
sense choice, but only bring noise information. 
After a broad investigation for large-scale 
ambiguous words, we choose the context 
according to the correlation of the context words 
with the ambiguous word, but not only the 
distance from the word. 
From the above analysis, we choose the 
translation choice method based on syntactic 
analysis. Place the module of translation choice 
between the parser and the generator; acquire a 
context set for the ambiguous word. When 
choosing the translation, we may take the 
context set as a word bag, i.e. the grammatical 
context as word bag. No single word is 
considered but only that lexical and 
part-of-speech information are taken as context 
features. Bayes minimum error probability is 
taken as evaluation function for word translation 
choice. 
In this paper, grammatical context is 
considered for word translation choice. The 
structure related features of the ambiguous 
words are taken into account for fully use of the 
parsing result. It has the characteristics below: (1) 
The window size is not defined by human but on 
basis of the grammatical structure of the 
sentence, so we can acquire more efficiently the 
useful context features; (2) The unrelated 
context features in sentence structure are filtered 
out for translation choice; (3) The features are 
based on the structure relationship, but not 100% 
right parsing result. From the above 
characteristics, we can see the method is really 
practical. 
2 Rule Based Transfer & Generation 
For MTS2000, structural transfer is to start from 
the syntactic parsing tree and construct the 
Chinese syntactic tree. While the generation of 
Chinese is to generate a word link from the 
Chinese tree and build the translation sentence 
[Yao 2001]. This module has adopted the 
rule-based knowledge representation method. 
The design of the rule system is highly related to 
the performance of the machine translation 
system. 
The rule description language of the 
machine translation system is in the form of 
production rules, i.e. a rule composed of a 
conditional part and an operational part. The 
conditional part is a scan window of variable 
length, which uses the context constraint 
conditions such as phrases or some linguistic 
features. The operational part generates the 
corresponding translation or some corresponding 
generation features in the operational part. If the 
conditions are met, the operations will be 
performed. The representation of the rule system 
has shown a characteristic of the system, that is 
the integration of transfer and generation. The 
rule description language is similar to natural 
language and consistent with human habits. 
Multiple description methods are implemented. 
The conditional part of the rules is 
composed of node numbers and ?+? symbols 
that is used to link the nodes. The operation part 
consists of corresponding conditional parts and 
translations and also, if necessary, some action 
functions. 
For example, the rule to combine an 
adjective and a noun to generate a noun phrase is 
as follows:  
0:Cate=A + 1:Cate=N 
->0:* + 1:* + _NodeUf(N?0?1) 
in which, ?*? stands for corresponding 
translation of the nodes, _NodeUf() is a function 
that combines the nodes to generate a new node. 
The new translation is generated at the same 
time with the combination of nodes. 
In general, the English Chinese machine 
translation system has the following features in 
the transfer and generation phase: 
1) The grammatical and semantic features are 
described by a string composed of frame 
name and values linked with ?=?; 
2) The conditions may be operated by ?and?, 
?or? and ?not?; 
3) Nodes in the same level of the sentence may 
be scanned and tested arbitrarily; 
4) The action functions and test functions can 
generate corresponding features for feature 
transmission and test. 
The rules are organized into various levels. 
All the rules are put in the knowledge base with 
part-of-speech as the entry feature. The rules 
have different priorities, which decide their 
sequence in rule matching. In general, the more 
specific the rule, the higher is its priority. The 
more general the rule, the lower is its priority. 
The levels of the rules help resolve rule 
collision. 
Conclusion 
The system prototype has been implemented and 
large-scale development and refinement are 
under progress. From our knowledge of the 
system, knowledge acquisition and rule base 
organization is the bottleneck for MTS2000 
system and similar natural language processing 
systems. The knowledge acquisition for word 
translation choice needs large-scale word 
aligned bilingual corpus. We are making 
research on new word translation methods on 
basis of our 60,000-sentence aligned bilingual 
corpus. The transfer and generation knowledge 
base are facing much knowledge collision and 
redundancy problem. The organization 
technique of knowledge base is also an 
important issue in the project. 
References  
Tie-Jun Zhao, En-Dong Xun, Bin Chen, Xiao-Hu 
Liu,Sheng Li, Research on Word Sense 
Disambiguation based on Target Language 
Statistics, Applied Fundamental and Engineering 
Journal, 1999?7?1??101-110 
Meng Yao, Zhao Tiejun, Yu Hao, Li Sheng, A 
Decision Tree Based Corpus Approach to English 
Base Noun Phrase Identification, Proceedings 
International conference on East-Asian Language 
Processing and Internet Information Technology, 
Shenyang, 2000: 5-10 
Christopher D. Manning, Hinrich Sch ? tze, 
Foundation of Statistical Natural Language 
Processing. The MIT Press. pp229-262. 1999. 
Chang-Ning Huang, Juan-Zi Li, A language model 
for word sense disambiguation, 10th anniversary 
for Chinese Linguistic Society, October, 1997, 
Fuzhou  
En-Dong Xun, Sheng Li, Tie-Jun Zhao, Bi-gram 
co-occurrence based stochastic method for word 
sense disambiguation, High Technologies, 1998, 
10(8): 21-25  
Hwee Tou Ng. Exemplar-Based Word Sense 
Disambiguation: Some Recent Improvements. In 
Proceedings of the Second Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-2), August 1997  
Tie-Jun Zhao etc, Principle of Machine Translation, 
Press of Harbin Institute of Technology, 2000. 
Jian-Min Yao, Jing Zhang, Hao Yu, Tie-Jun 
Zhao,Sheng Li, Transfer from an English parsing 
tree to a Chinese syntactic tree, Joint Conference of 
the Society of Computational Linguistics, 2001, 
Taiyuan.-138.  
  
Proceedings of the Second Workshop on Statistical Machine Translation, pages 64?71,
Prague, June 2007. c?2007 Association for Computational Linguistics
 1
Meta-Structure Transformation Model 
for Statistical Machine Translation 
Jiadong Sun, Tiejun, Zhao and Huashen Liang 
MOE-MS Key Lab of National Language Processing and speech 
Harbin Institute of Technology 
No. 92, West Da-zhi Street ,Harbin Heilongjiang ,150001 ,China 
jiadongsun@hit.edu.cn 
{tjzhao, hsliang }@mtlab.hit.edu.cn 
Abstract 
We propose a novel syntax-based model 
for statistical machine translation in which 
meta-structure (MS) and meta-structure se-
quence (SMS) of a parse tree are defined. 
In this framework, a parse tree is decom-
posed into SMS to deal with the structure 
divergence and the alignment can be recon-
structed at different levels of recombination 
of MS (RM). RM pairs extracted can per-
form the mapping between the sub-
structures across languages.  As a result, 
we have got not only the translation for the 
target language, but an SMS of its parse 
tree at the same time. Experiments with 
BLEU metric show that the model signifi-
cantly outperforms Pharaoh, a state-art-the-
art phrase-based system. 
1 Introduction 
The statistical approach has been widely used in 
machine translation, which use the noisy-channel-
based model. A joint probability model, proposed 
by Marcu and Wong (2002), is a kind of phrase-
based one. Och and Ney (2004) gave a framework 
of alignment templates for this kind of models. All 
of the phrase-based models outperformed the 
word-based models, by automatically learning 
word and phrase equivalents from bilingual corpus 
and reordering at the phrase level. But it has been 
found that phrases longer than three words have 
little improvement in the performance (Koehn, 
2003). Above the phrase level, these models have a 
simple distortion model that reorders phrases inde-
pendently, without consideration of their contents 
and syntactic information. 
In recent years, applying different statistical 
learning methods to structured data has attracted 
various researchers. Syntax-based MT approaches 
began with Wu (1997), who introduced the Inver-
sion Transduction Grammars. Utilizing syntactic 
structure as the channel input was introduced into 
MT by Yamada (2001). Syntax-based models have 
been presented in different grammar formalisms. 
The model based on Head-transducer was pre-
sented by Alshawi (2000). Daniel Gildea (2003) 
dealt with the problem of the parse tree isomor-
phism with a cloning operation to either tree-to-
string or tree-to-tree alignment models. Ding and 
Palmer (2005) introduced a version of probabilistic 
extension of Synchronous Dependency Insertion 
Grammars (SDIG) to deal with the pervasive 
structure divergence. All these approaches don?t 
model the translation process, but formalize a 
model that generates two languages at the same 
time, which can be considered as some kind of tree 
transducers. Graehl and Knight (2004) described 
the use of tree transducers for natural language 
processing and addressed the training problems for 
this kind of transducers.  
In this paper, we define a model based on the 
MS decomposition of the parse trees for statistical 
machine translation, which can capture structural 
variations and has a proven generation capacity. 
During the translation process of our model, the 
parse tree of the source language is decomposed 
into different levels of MS and then transformed 
into the ones of the target language in the form of 
RM. The source language can be reordered accord-
ing to the structure transformation. At last, the tar-
get translation string is generated in the scopes of 
RM. In the framework of this model,  
 
64
 2
 
Figure 1: MS and the SMS and RM for a given parser tree 
 
the RM transformation can be regarded as produc-
tion rules and be extracted automatically from the 
bilingual corpus. The overall translation probabil-
ity is thus decomposed.  
In the rest of this paper, we first give the 
definitions for MS, SMS, RM and the 
decomposition of the parse tree in section 2.1, we 
give a detailed description of our model in section 
2.2, section 3 describes the training details and 
section 4 describes the decoding algorithms, and 
then the experiment (section 5) proves that our 
model can outperform the baseline model, 
pharaoh, under the same condition.  
2 The model 
2.1 MS for a parse tree 
      A source language sentence (s1 s2 s3 s4 s5 s6), 
and its parse tree S-P, are given in Figure 1.We 
also give the translation of the sentence, which is 
illustrated as (t1 t2 t3).Its parse tree is T-P.  
Definition 1 
MS of a parse tree  
We call a sub-tree a MS of a parse tree, if it sat-
isfies the following constraints: 
1. An MS should be a sub-tree of a parse tree 
2. Its direct sons of the leaf nodes in the sub-
tree are the words or punctuations of the sen-
tence  
For example, each of the sub-trees in the right- 
hand of Figure 1 is an MS for the parse tree of S-P.  
 The sub-tree of [I [G, D, H]] of S-P is not an MS, 
because the direct sons of the leaf nodes, G, D, H,  
are not words in the sentence of (s1 s2 s3 s4 s5 
s6).  
Definition 2 SMS and RM 
A sequence of MS is called a meta-structure 
sequence (SMS) of a parse tree if and only if,   
1. Its elements are MS of the parse tree 
2. The parse tree can be reconstructed with the 
elements in the same order as in the sequence. 
  It is denoted as SMS [T(S)].1 Two examples 
for the concept of SMS can be found in Figure1. 
RM(recombination of MS) is a sub-sequence 
of SMS. We can express an SMS as differ-
ent )]([1 STRM
k .The parse tree of S-P in Figure1 
is decomposed into SMS and expressed in the 
framework of RM. The two RM, ][21 PSRM ? , 
are used to express its parse tree in Figure1.It is 
noted that there is structure divergence between 
the two parse trees in Figure1. The corresponding 
node of Node I in the tree S-P cannot be found in 
the tree T-P. But under the conception of RM, the 
structure alignments can be achieved at the level 
of RM, which is illustrated in Figure2. 
 
Figure2.The RM alignments for S-P and T-P 
                                                 
1 T[S] denotes the parse tree of a given sentence  
   f and e denote the foreign and target sentences 
65
 3
  In Figure2, both of the parse trees are decom-
posed and reconstructed in the forms of RM. The 
alignments based on RM are illustrated at the 
same time. 
2.2 Description of the model   
In the framework of Statistical machine transla-
tion, the task is to find the sentence e for the given 
foreign language f, which can be described in the 
following formulation.  
)}|(max{arg
~
fePe
e
=                     (1) 
To make the model have the ability to model 
the structure transformation, some hidden vari-
ables are introduced into the probability equation. 
To make the equations simple to read, we take 
some denotations different from the above defini-
tions. SMS[T(S)] is denoted as SM[T(S)].  
The first variable is the SM[T(S)], we induce 
the equation as follows? 
 
?=
))((
)|)]([,()|(
fTSM
ffTSMePfeP
))],([|()|)]([(
)]([
ffTSMePffTSMP
fTSM
?=     
                 ?2? 
 
?
=
)](SM[
))],([|)](SM[(
))],([|(
eT
ffTSMeTeP
ffTSMeP
?
      
? ?=
)](SM[
))],([|)](SM[(
eT
ffTSMeTP  
))],([)],(SM[|( ffTSMeTeP                  (3)  
 
 In order to simplify this model we have two as-
sumptions: 
An assumption is that the generation of SMS [T 
(e)] is only related with SMS[T(f)]: 
))],([|)]([( ffTSMeTSMP  
)])([|)]([( fTSMeTSMP?  
                                                                            (4)  
Here we do all segmentations for any SMS 
of [T (f)] to get different )]([1 fTRM
k . 
 
??
=
=
k
i
iifRMT
fTRMeTRMP
fTSMeTSMP
1
)](
)])([|)]([(
)])([|)]([(
 
                                                                    (5) 
The use of RM is to decompose bi-lingual 
parse trees and get the alignments in different 
hierarchical levels of the structure. 
Now we have another assumption that all 
)|)]([( ffTSMP should have the same prob-
ability? . A simplified form for this model is 
derived:  
=)|( feP  
?? ? ?
))(( ))((fTSM eTSM
))],([)],([|(
)])([|)]([(
)]([ 1
ffTRMeTRMeP
fTRMeTRMP
ii
fTRM
k
i
ii
?
? ?
=   
                                                                        (6) 
, Where ))],([)],([|( ffTRMeTRMeP ii can be re-
garded as a lexical transformation process, which 
will be further decomposed. 
   In order to model the direct translation process 
better by extending the feature functions, the di-
rect translation probability is obtained in the 
framework of maximum entropy model: 
 ( )
( )
( )? ?
?
=
==
)]([)],([, 1
1
)],([)],([,exp[
)],([)],([,exp[
|
fTSMeTSMe
M
m mm
M
m mm
ffTSMeTSMe
ffTSMeTSMe
feP
h
h
?
?   
                                                                            (7) 
We can achieve the translation according to 
the function below: 
 
( ){ }? == Mm mm ffTSMeTSMee h1 )],([)],([,exp[maxarg~ ?       
                                                                             (8) 
A detailed list of the feature functions for the 
model and some explanations are given as below: 
z Just as the derivation in the model, we take 
into consideration of the structure trans-
formation when selecting the features. The 
MS are combined in the forms of RM and 
transformed as a whole structure. 
 ( ) ?
=
=
k
i
ii fTRMeTRMPfeh
1
1
)])([|)]([(log,                                 
                                                                            (9) 
( ) ?
=
=
k
i
ii eTRMfTRMPfeh
1
2
)])([|)]([(log,     
                                                                         (10) 
z Features to model lexical transformation 
processes, and its inverted version, where 
the symbol L (RMi [T(S)]) denotes the 
66
 4
words belonging to this sub-structure in the 
sentence. In Figure1, L (RM1) denotes the 
words, s1 s2 s3, in the source language. 
This part of transformation happens in the 
scope of each RM, which means that all 
the words in any RM can be transformed 
into the target language words just in the 
way of phrase-based model, serving as an-
other reordering factor at a different level: 
 
( ) ?
=
=
k
i
ii fTRMLeTRMLPfeh
1
3
)])([(|)]))([((log,  
                                                                           (11) 
( ) ?
=
=
k
i
ii eTRMLfTRMLPfeh
1
4
)])([(|)]))([((log,  
                                                                           (12) 
z We define a 3-gram model for the RM of 
the target language, which is called a struc-
ture model according to the function of it 
in this model.  
 
( ) )])([)],([|)]([(log, 12
1
5
eTRMeTRMeTRMPfe ii
k
i
ih ??
=
?=
                                                                           (13) 
        This feature can model the recombination of 
the parse structure of the target sentences.  For 
example in Figure3, ),|( BBAACCP  is used to de-
scribe the probability of the RM sequence, (AA, 
BB) should be followed by RM (CC) in  the 
translation process. This function can ensure 
that a more reasonable sub-tree can be generated 
for the target language. That would be explained 
further in section 3. 
                
 
    Figure3. The 3-gram structure model   
    
z The 3-gram language model is also used  
 ( ) )(log,
6
ePfeh =                                                                    
(14) 
The phrase-based model (Koehn, 2003) is a 
special case of this framework, if we take the 
whole structure of the parse tree as the only MS of 
the parse tree of the sentence, and set some special 
feature weights to zero. 
From the description above, we know the 
framework of this model. When transformed to 
target languages, the source language is reordered 
at the RM level first. In this process, only the 
knowledge of the structure is taken into 
consideration. It is obvious that a lot of sentences 
in the source language can have the same RM. So 
this model has better generative ability. At the 
same time, RM is a subsequence of SMS, which 
consists of different hierarchical MS. So RM is a 
structure, which can model the structure mapping 
across the sub-tree structure. By decomposing the 
source parse tree, the isomorphic between the 
parse trees can be obtained, at the level of RM. 
When reordering at the RM level, this model 
just takes an RM as a symbol, and it can perform a 
long distance reordering job according to the 
knowledge of RM alignments.  
3 Training        
For training the model, a parallel tree corpus is 
needed. The methods and details are described as 
follows:  
3.1 Decomposition of the parse tree 
To reduce the amount of MS used in decoding 
and training, we take some constrains for the MS. 
?1?.The height of the sub-tree shouldn?t be 
greater than a fixed value?  ; 
    ? 2?.  ???
)(
)(
heightN
nodesLeafN
 
Given a parse tree, we get the initial SMS in 
such a top -down and left- to ?right way.  
Any node is deleted if the sub-tree can?t satisfy 
the constrains (1), (2). 
 
 Figure3. Decomposition of a parse tree 
 
67
 
 
RMS for Ch-Parse Tree  RMS for EN-Parse Tree Pro for transformation 
AP[AP[AP[a-a]-usde]-m] NPB [DT-JJ-NN-PUNC.] 0.000155497 
AP[AP[AP[r-a]-usde]-m] NPB[PDT-DT-JJ-NN] 0.0151515 
AP[AP[BMP[m-q]-a]-usde] wj ADVP [RB-RB-PUNC.] 0.00344828 
AP[AP[BMP[m-q]-a]-usde] wj DT CD JJ NNS PUNC 0.0833333 
AP[AP[BMP[m-q]-a]-usde] wj DT JJ NN NNS PUNC. 0.015625 
Table 1 some examples of the RM transformation  
 
RM1            RM2 RM3 P(RM3|RM1,RM2)
IN  NP-A[NPB[PRP-NN] IN 0.2479237 
NPB NP-A[NPB[PRP-NN] VBZ 0.2479235 
IN NP-A[NPB[PRP-NN] MD 0.6458637 
<s> NP-A[NPB[PRP-NN] VBD 0.904308 
Table 2 Examples for the 3-gram structure model of RM 
 
Generate all of the SMS by deleting a node in 
any Ms to generate new SMS, applying the same 
operation to any SMS 
3.2 Parallel SMS and Estimation of the pa-
rameters for RM transformations 
We can get bi-lingual SMS by recombining all 
the possible SMS obtained from the parallel 
parse trees. nm ?  Parallel SMS can be obtained 
if m is the number of SMS for a parse tree in the 
source language, n for the target one. 
The alignments of the parallel MS and extrac-
tion can be performed in such a simple way. 
Given the parallel tree corpus, we first get the 
alignments based on the level of words, for which 
we used GIZA++ in both of the directions. Ac-
cording to the knowledge of the word alignments, 
we derived the alignments of leave nodes of the 
given parse trees, which are the direct root nodes 
of the words. Then all the knowledge of the words 
is discarded for the RM extraction. The next step 
for the extraction of the RM is based on the popu-
lar phrase-extraction algorithm of the phrase-
based statistical machine translation model. The 
present alignment and phrase extraction methods 
can be applied to the extraction of the MS and RM 
[T(S)]. 
),(
),(
)|(
EiFi
RM
EIFi
FiEI RMRMCount
RMRMCount
RMRMP
Ei
?=  
    ),( BAountC is the expected number of times A 
is aligned with B in the training corpus.Table1 
shows some parameters for this part in the model. 
Training n-gram model for the monolingual 
structure model is based on the English RM of 
each parse tree, selected from the parallel tree cor-
pus. The 3-gram structure model is defined as fol-
lows: 
=?? )])([)],([|)]([( 12 eTRMeTRMeTRMP iiI                                                     
),,(
),,(
12
12
jII
j
III
RMRMRMCount
RMRMRMCount
??
???     
),,( CBAountC  is the times of the situation, in 
which the RM is consecutive sub-trees of the 
parse trees in the training set. Some 3-gram pa-
rameters in the training task are given in Table2. 
We didn?t meet with the serious data sparseness 
problem in this part of work, because most of the 
MS structures have occurred enough times for 
parameters estimation. But we still set some 
fixed value for the unseen parameters in the 
training set. 
4 Decoding  
A beam search algorithm is applied to this 
model for decoding, which is based on the frame 
of the beam search for phrase-based statistical 
machine translation (Koehn et al 03). 
Here the process of the hypothesis generation is 
presented. Given a sentence and its parse tree, all 
the possible candidate RM are collected, which 
can cover a part of the parse tree at the bottom. 
With the candidates, the hypotheses can be 
formed and extended. 
For example, all the parse tree?s leaf nodes of a 
Chinese sentence in Figure4, are covered by [r], 
[ pron ] and  VP[vg-BNP[pron-n]] in the order of 
choosing candidate RM{ (1),  (2), (3)}.  
68
 6
 
 
 
Figure4. Process of translation based on RM 
),( VBDWRBr                                              (1) 
???how did                                                   
                                                                         
),( PRPpron                                                   (2) 
                                                                            
??you                                                              
 
 
]])[[
]],[[(
NNDTNPBVBVP
npronBNPvgVP
??
??
                 
(3)  
?? ??  ??? find the information 
    
Before the next expansion of a hypothesis, the 
words in the scope of the present RM are trans-
lated into the target language and the correspond-
ing )]([ eTRM i  is generated. For example, when  
),( VBDWRBr , is used to expand the hypothe-
sis , the words in the sub-tree are translated into 
the target language, ???how did.        
We also need to calculate the cost for the hy-
potheses according to the parameters in the model 
to perform the beam search. The task for the beam 
search is to find the hypothesis with the least cost.  
When the expansion of a hypothesis comes to the 
final state, the target language is generated. All of 
the leave nodes of the parse tree for the source 
language are covered. The parser for the target 
language isn?t used for decoding. But a target 
SMS is generated during the process of decoding 
to achieve better reordering performance. 
5 Experiments   
The experiment was conducted for the task of 
Chinese-to-English translation. A corpus, which 
consists of 602,701 sentence pairs, was used as 
the training set. We took CLDC 863 test set as our 
test set (http://www.chineseldc.org/resourse.asp), 
which consists of 467 sentences with an average 
length of 14.287 Chinese words and 4 references. 
To evaluate the result of the translation, the BLEU 
metric (Papineni et al 2002) was used.  
5.1 The baseline 
System used for comparison was Pharaoh 
(Koehn et al, 2003; Koehn, 2004), which uses a 
beam search algorithm for decoding. In its model, 
it takes the following features: language model, 
phrase translation probability in the two directions, 
distortion model, word penalty and phrase penalty, 
all of which can be achieved with the training 
toolkits distributed by Koehn. The training set and 
development set mentioned above were used to 
perform the training task and to tune the feature 
weights by the minimum error training algorithm. 
All the other settings were the same as the default 
ones. SRI Language Modeling Toolkit was used 
to train a 3-gram language model. After training, 
164 MB  language model were obtained. 
5.2 Our model 
All the common features shared with Pharaoh 
were trained with the same toolkits and the same 
corpus. Besides those features, we need to train 
the structure transformation model and the mono-
lingual structure model for our model. First, 
10,000 sentence pairs were selected to achieve the  
69
 7
 
BLEU-n n-gram precisions System 
4 1 2 3 4 5 6 7 8 
Pharaoh 0.2053 0.6449 0.4270 0.2919 0.2053 0.1480 0.1061 0.0752 0.0534
Ms  sys-
tem 
0.2232 0.6917 0.4605 0.3160 0.2232 0.1615 0.1163 0.0826 0.0587
Table3. Comparison of Pharaoh and our system 
  Features 
System Plm(e) P(RT) P( IRT ) Pw( f|e ) Pw( e|f ) Word Phr Ph(RM) 
Pharaoh 0.151 ---- ------ 0.08 0.14 -0.29 0.26 ----- 
MS sys-
tem 
0.157 0.16 0.23 0.06 0.11 -0.20 0.22 0.36 
Table4.Feature weights obtained by minimum error rate training on development set 
 
training set for this part of task. The Collins parser 
and a Chinese parser of our own lab were used. 
After processing this corpus, we get a parallel tree 
corpus. SRI Language Modeling Toolkits were 
used again to train this part of parameters. In this 
experiment, we set 3=? ,and 5.1=? . 149MB 
)]([ sTRMS  pairs and a 25 MB 3-gram mono-
lingual structure model were obtained.  
6. Conclusion and Future work 
A framework for statistical machine translation 
is created in this paper. The results of the experi-
ments show that this model gives better perform-
ance, compared with the baseline system. 
This model can incorporate the syntactic infor-
mation into the process of translation and model 
the sub-structure projections across the parallel 
parse trees. 
The advantage of this frame work lies in that 
the reordering operations can be performed at the 
different levels according to the hierarchical RM 
of the parse tree. 
But we should notice that some independent as-
sumptions were made in the decomposition of the 
parse tree. In the future, a proper method should 
be introduced into this model to achieve the most 
possible decomposition of the parse tree. In fact, 
we can incorporate some other feature functions 
into the model to model the structure transforma-
tion more effectively.  
Acknowledgement 
   Thanks to the reviewers for their reviews and 
comments  on improving our presentation of this 
paper. 
 
 
References 
A.P.Dempster,N.M.Laird, and D.B.Rubin 
1977.Maximum likelihood from imcomplete data 
via the EM algorithm. Journal of the Royal Statisti-
cal Society, 39(Ser B):1-38. 
Christoph Tillman. A projection extension algorithm 
for statistical machine translation. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, Sapporo, Japan, June 30-July 
4, 2003, 1-8.  
Daniel Gildea.2003.Loosely tree based alignment for 
machine translation. In Proceedings of ACL-03 
Daniel Marcu, William Wong. A phrase-based, joint 
probability model for statistical machine translation. 
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, Philadelphia, 
PA, USA, July 11-13, 2002, 133-139.  
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3):3-403. 
F.Casacuberta, E. Vidal: Machine Translation with 
Inferred Stochastic Finite-state Transducers. Com-
putational Linguistics, Vol. 30, No. 2, pp. 205-225, 
June 2004 
Franz J. Och, C. Tillmann, Hermann Ney. Improved 
alignment models for statistical machine translation. 
Proceedings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language Processing 
and Very Large Corpora (EMNLP), College Park, 
MD, USA, June 21-22, 1999, 20-28.  
Franz J. Och, Hermann Ney.2002 Discriminative train-
ing and maximum entropy models. In Proceedings of 
ACL-00, pages 440-447, Hong Kong, Octorber. 
Hiyan Alshawi, Srinvas Bangalore, and Shona Douglas. 
2000. Learning dependency translation models as 
70
 8
?
collections of finite state head transducers Compu-
tational Linguistics, 26(1):45-60. 
Ilya D. Melamed. Automatic evaluation and uniform 
filter cascades for inducing n-best translation lexi-
cons. Proceedings of the Third Workshop on Very 
Large Corpora, Boston, USA, July 30, 1995, 197-
211.  
Jonathan Graehl Kevin Knight  Training Tree Trans-
ducers  In Proceedings of NAACL-HLT 2004, 
pages 105-112. 
Kenji Yamada  and  Kevin Knight 2001. A Syntax-
based statistical translation model. In Proceedings 
of the 39th Annual Meeting of the association for 
computational Linguists(ACL  01), Toulouse, 
France, July 6-11 
Michael John Collins. 1999. Head-driven statistical 
Models for Natural Language Parsing. Ph.D. the-
sis,University of Pennsyvania,Philadelphia. 
P. Koehn, Franz Josef Och, Daniel Marcu. Statistical 
phrase-based translation. Proceedings of the Con-
ference on Human Language Technology, Edmon-
ton, Canada, May 27-June 1, 2003, 127-133. 
P. Koehn: Pharaoh: a Beam Search Decoder for 
Phrase-based Statistical Machine Translation Mod-
els . Meeting of the American Association for ma-
chine translation(AMTA), Washington DC, pp. 115-
124 Sep./Oct. 2004 
Peter F. Brown ,Stephen A. Della Pietra,Vincent 
J.Della Pietra, and Robert Merrcer.1993. The 
mathematics of statistical machine transla-
tion:Parameter estimation.Computational Linguis-
tics,19(2).:263-311. 
Quirk, Chris, Arul Menezes, and Colin Cherry. De-
pendency Tree Translation. Microsoft Research 
Technical Report: MSR-TR-2004-113. 
Regina Barzilay and Lillian Lee. 2003. Learning to 
paraphrase: An supervised approach using multiple-
sequence alignment. In Proceedings of 
HLT/NAACL 
S. Nie    en , H. Ney: Statistical Machine Translation 
with Scarce Resources using Morpho-syntactic 
Information. Computational Linguistics, Vol. 30 No. 
2, pp. 181-204, June 20 
Yuan Ding and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency in-
sert grammars.  In Proceedings of 43rd Annual 
Meeting of the NAACL-HLT2004, pages 273-280.. 
71
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 169?172,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT-WSD: Using Search Engine for Multilingual Chinese-English 
Lexical Sample Task 
PengYuan Liu, TieJun Zhao, MuYun Yang 
MOE-MS Key Laboratory of NLP & Speech, HIT, School of Computer Science and 
Technology, Harbin Institute of Technology, Harbin, Heilongjiang, China 
{pyliu,tjzhao,ymy}@mtlab.hit.edu.cn 
 
 
Abstract 
We have participated in the Multilingual 
Chinese-English Lexical Sample Task of 
SemEval-2007. Our system disambiguates 
senses of Chinese words and finds the 
correct translation in English by using the 
web as WSD knowledge source. Since all 
the statistic data is obtained from search 
engine, the method is considered to be 
unsupervised and does not require any 
sense-tagged corpus. 
1 Introduction 
Due to the lack of sense tagged corpora (and the 
difficulty of manually creating them), the 
unsupervised method tries to avoid, or at least to 
reduce, the knowledge acquisition problem, which 
the supervised methods have to deal with. In order 
to tackle the problem of the knowledge acquisition 
bottleneck, we adopted an unsupervised approach 
based on search engine, which does not require any 
sense tagged corpus. 
The majority of methods using the Web often try 
to automatically generate sense tagged corpora 
(Agirre and Martinez 2000;Agirre and Martinez 
2004;Gonzalo et al 2003; Mihalcea and Moldovan 
1999;Santamaria et al 2003). In this paper, we 
experiment with our initial attempt on another 
research trend that uses the Web not for extracting 
training samples but helping disambiguate directly 
during the translation selection process. 
The approach we present here is inspired by 
(Mihalcea and Moldovan 1999;Brill 2003; Rosso 
et al 2005; Dagan et al 2006; McCarthy 2002). 
Suppose that source ambiguous words are apt to 
appear with its target translation on bilingual web 
pages either parallel or non-parallel. Instead of 
searching the source language or target language 
respectively on web, we try to let the search engine 
think in a bilingual style. First, our system gets the 
co-occurrence information of Chinese context and 
its corresponding English context. Then it computes 
association measurements of Chinese context and 
English context in 4 kinds of way. Finally, it 
selects the correct English translation by 
computing the association measurements. 
In view that this is the first international standard 
evaluation to predict the correct English translation 
for ambiguous Chinese word, we built HIT-WSD 
system as our first attempt on disambiguation by using 
bilingual web search and just want to testify validity 
of our method. 
2 HIT-WSD System 
2.1 Disambiguation Process 
HIT-WSD system disambiguates senses of Chinese 
target ambiguous word and finds the correct 
translation in English by searching bilingual 
information on the web. Figure 1 gives the 
flowchart of our proposed approach. Given an 
ambiguous word with a Chinese sentence, we 
easily create its Chinese context. English context 
can be acquired from a Chinese-English dictionary and 
the translation mapping set(offered by the 
Multilingual Chinese-English Lexical Sample 
Task). System puts Chinese context and English 
context as queries on search engine individually 
and collectively. After this step, frequency and co-
occurrence frequency of Chinese context and English 
 
169
 
 
 
 
 
 
 
 
 
 
 
 
 
 
context will be found. Finally, our system selects the 
most probable English translation by computing 
association measurements. 
Figure 2 gives an example of how the proposed 
approach selects English translations of the 
Chinese ambiguous word ???/dongyao? given 
the sentence and its translation mapping set. This 
instance comes from the training data of Multilin-
gual Chinese-English Lexical Sample Task of Sem-
eval2007. According to the translation mapping set,  
Chinese target word ??? /dongyao? has two 
English Translations: shake and vacillate.  
English Context Candidates set is the 
translations set of the Chinese context. System uses 
translation mapping set to translate Chinese target 
ambiguous word and uses an Chinese-English 
dictionary to translate other words in Chinese 
context. English Context Candidates set could be 
any combination of translations and each 
combination could be selected as the English context. 
 After getting the Chinese context and English 
context, we put them as queries to search engine 
and extract page counts (which can be considered 
as frequency) which search engine returned.  We 
not only search Chinese context and English 
context individually, but also put them together to 
search engine.  
Association measurements: the Dice coefficient, 
point-wise mutual information, Log Likelihood 
score and? P2 P score are computed in the third phase 
while we got all kinds of statistic results from 
search engine. Finally, we determine the 
translation by simply computing the association 
measurements 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: Example of the Chinese ambiguous word 
???/dongyao? selection process 
Figure 1: Flowchart of HIT-WSD System
Chinese sentence,
bilingual mapping
and C-E dictionary 
English 
context 
Search 
Engine
Comparing 
association 
measurements 
English 
Translation
Frequency and Co-
occurrence 
Frequency of 
Chinese context 
and English context Chinese 
context 
 
Chinese Context(CC): ???????? 
English Context Candidates set:  
Shake, shake is, not shake, line shake?/vacillate, 
not vacillate, vacillate is, line vacillate? 
English Context(EC): shake/vacillate 
Putting on Search Engine and getting counts:  
( ) 1880000, ( ) 5450
( ) 113000, ( , ) 77, ( , ) 12CC CC CC
c shake c vacillate
c c shake c vacillate
= =
= = =
Computing association measurements: 
( , )
2 ( , )
(( ( , ) ( )) ( ( , ) ( )
Dice CC shake
c CC shake
c CC shake c shake c CC shake c CC
=
?
+ ? +
2 77
(77 1880000) (77 113000))
7.24 10e
?=
+ ? +
?=  
2 ( , )
(( ( , ) ( )) ( ( , ) ( ))
( , )
CC CC CC
c CC vacillate
c vacillate c vacillate c vacillate c
Dice CC vacillate
?
+ ? +
=
2 12
(12 5450) (12 11300)
3.89 8e
?
+ ? +
= ?=  
Compare and Determine a Translation: 
3.89e-8>7.24e-10, So the answer is vacillate. 
Instance: ???????????????????
??????????<head>??</head>????
?????????? 
Chinese Ambiguous Word: ?? 
Translation Mapping Set: ??-shake/??-vacillate 
Translations of Chinese context in Chinese-English 
dictionary:?/not,?/is,??/line,??/ actualize 
170
2.2  Experiment Settings 
Although the Chinese context can be represented 
with local features, topic features, parts of speech 
and so on, we use sentence segment as Chinese 
context in our experiment system. The sentence 
segment is a window size ? n segment of the 
sentence including the ambiguous words. 
English Context Candidates set could be any 
combination of the translation of words appearing 
in Chinese context. In our experiment system, we 
just choose the translation of the Chinese target 
ambiguous words in the translation mapping set as 
English context. 
We choose googleTP 1 PT and baiduTP 2 PT as our search 
engine, for they are both most widely used for 
English and Chinese language respectively.  
Putting Chinese context and English context as 
queries to the search engine, we will get 
corresponding page counts it returned as figure 2 
shows. 
Four statistical measurements were used in order 
to measure the degree of association of Chinese 
Context (CC) and English Context (EC). CC and 
EC can be seen as two random events occuring in 
the web pages: 
 
1. Point-wise mutual information: 
2( , ) log ( ) ( )
n aMI CC EC
a b a c
?= + ? +  (1) 
2. DICE coefficient: 
2( , )
( ) ( )
aDICE CC EC
a b a c
?= + ? +   (2) 
3. ? P2 P score: 
2
2 ( )X ( , )
( ) ( ) ( ) ( )
n a d b cCC EC
a b a c b d c d
? ? ? ?= + ? + ? + ? +  (3) 
4. Log Likelihood score: 
( , ) 2 ( log
( ) ( )
log log
( ) ( ) ( ) ( )
log )
( ) ( )
n aLL CC EC a
a b a c
n b n cb c
a b b d c d a c
n dd
c d b d
?= ? ? + ? +
? ?+ ? + ?+ ? + + ? +
?+ ? + ? +
Here is the meaning of a, b, c, d and n. 
                                                 
TP
1
PT www.google.com. 
TP
2
PT www.baidu.com. 
Table 1:Training data results of Multilingual Chinese-
English Lexical Sample Task 
 Micro-average Macro-average
Our result 0.336898           0.395993 
Baseline (MFS) 0.4053 0.4618 
Table 2:Official results: Multilingual Chinese-English 
Lexical Sample Task 
a: all counts of the web pages which include 
Both CC and EC. 
b: all counts of the web pages which include CC, 
do not include EC. 
c: all counts of the web pages which include EC, 
do not include CC. 
d: all counts of the web pages which include 
neither CC and EC. 
n= a+ b+ c + d 
We applied our method to the training data of 
Multilingual Chinese-English Lexical Sample Task. 
The results are as showed in Table 1. 
Since only one test result can be uploaded for 
one system, our system selects the settings of one 
of the best results. The final settings of our system 
is: window size is [-1, +2], the search engine is 
baidu and the association measurement is Point-
wise mutual information. 
3 Official Results 
In multilingual Chinese-English lexical sample 
task of SemEval-2007, there are 2686 instances in 
training data for 40 Chinese ambiguous words. All 
these ambiguous words are either nouns or verbs. 
Test data consist of 935 untagged instances of the 
same target words. 
The official result of our system in multilingual 
Chinese-English lexical sample task is reported as 
in Table 2. 
 
 
Precision( Micro-average) 
Context Window Size 
Association 
-1,+1 -1,+2 -2,+2 
MI(Baidu) 0.349 0.349 0.339 
XX(Baidu) 0.338 0.344 0.314 
LL(Baidu) 0.315 0.320 0.293 
DICE(Baidu) 0.285 0.295 0.295 
MI(google) 0.334 0.334 0.339 
XX(google) 0.322 0.316 0.316 
LL(google) 0.295 0.306 0.299 
DICE(google) 0.281 0.278 0.272 
(4)
Measurements
171
4 Conclusions 
In SemEval-2007, we participated in Multilingual 
Chinese-English Lexical Sample Task with a fully 
unsupervised system based on bilingual web search. 
Our initial experiment result shows that our system 
fails to reach MFS (Most Familiar Sense) baseline 
due to our method is too simple where search 
queries are formed (just uses simple context 
window and English target translation). Our 
approach is the first attempt so far as we know on 
using bilingual web search for translation selection 
directly. The system is very simple but seemed to 
achieve a not bad performance when considered 
the performance of fully unsupervised systems in 
SENSEVAL-2, SENSEVAL -3 English tasks. 
For future research, we will investigate the 
dependency of bilingual documents, optimize the 
search queries, filter out potential noises and 
combine the different results in order to devise an 
improved method that can utilize bilingual web 
search better. 
 
References 
Agirre, E.and Martinez, D. 2000. Exploring Automatic 
Word Sense Disambiguation with Decision Lists and 
the Web. Proc. of the COLING-2000.  
Agirre, E.and Martinez, D. 2004. Unsupervised word 
sense disambiguation based on automatically 
retrieved examples: The important of bias. Proc. of 
the EMNLP 2004(Barcelona, Spain, July 2004).  
Brill, E. 2003. Processing Natural Language 
Processing without Natural Language Processing. 
Lecture Notes in Computer Science, Vol. 2588. 
Springer-Verlag (2003) 360?369. 
Dagan, I., Glickman, O., Gliozzo, A., Marmorshtein, E. 
and Strapparava, C. 2006. Direct Word Sense 
Matching for lexical substitution. Proceedings of 
ACL/COLING 2006. 
Gonzalo, J., Verdejo, F. and Chugar, I. 2003. The Web 
as a Resource for WSD. 1PstP MEANING Workshop, 
Spain. 
McCarthy, D. 2002. Lexical Substitution as a Task for 
WSD Evaluation. In Proceedings of the ACL 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions, Philadelphia, USA. 
Mihalcea, R. and Moldovan, D.I. 1999. An Automatic 
Method for Generating Sense Tagged Corpora. Proc. 
of the 16th National Conf. on Artificial Intelligence. 
AAAI Press. 
Rosso, P., Montes, M., Buscaldi, D., Pancardo, A., and 
Villase, A., 2005. Two Web-based Approaches for 
Noun Sense Disambiguation. Int. Conf. on Comput. 
Linguistics and Intelligent Text Processing, 
CICLing-2005, Springer Verlag, LNCS (3406), 
Mexico D.F., Mexico, pp. 261-273 
Santamaria, C., Gonzalo, J. and Verdejo, F. 2003. 
Automatic Association of WWW Directories to Word 
Senses. Computational Linguistics (2003), Vol. 3, 
Issue 3 ? Special Issue on the Web as Corpus, 485?
502. 
172
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
References Extension for the Automatic Evaluation of MT by             Syntactic Hybridization   Bo Wang, Tiejun Zhao, Muyun Yang, Sheng Li School of Computer Science and Technology Harbin Institute of Technology Harbin, China {bowang,tjzhao,ymy,sl}@mtlab.hit.edu.cn       Abstract Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems. We propose a method to extend the reference set of the au-tomatic evaluation only based on multiple manual references and their syntactic struc-tures. In our approach, the syntactic equiva-lents in the reference sentences are identified and hybridized to generate new references. The new method need no external knowledge and can obtain the equivalents of long sub-segments of reference sentences. The experi-mental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments. 1 Introduction While human evaluation of machine translation output remains the most reliable method to assess translation quality, it is a costly and time consum-ing process. The development of automatic ma-chine translation evaluation metrics enables the rapid assessment of system output. By providing immediate feedback on the effectiveness of various techniques, these metrics have guided machine translation research and have facilitated rapid ad-vances in the state of the art. In addition, automatic evaluation metrics are useful in comparing the per-formance of multiple machine translation systems 
on a given translation task. Since automatic evalua-tion metrics are meant to serve as a surrogate for human judgments, their quality is determined by how well they correlate with assessors? preferences and how accurately they predicts human judg-ments. Although current methods for automatically evaluating machine translation output do not re-quire humans to assess individual system output, humans are nevertheless needed to generate a number of reference translations. The quality of machine-generated translations is determined by automatically comparing system output with these references. All current automatic evaluation met-rics are based on the various measures of the gen-eral similarity between the system translation and manual references. This kind of method has an ob-vious drawback: it does not account for combina-tions of lexical and syntactic differences that might occur between a perfectly fluent and accurately-translated machine output and a human reference translation (beyond variations already captured by the different reference translations themselves). Moreover, the set of human reference translations is unlikely to be an exhaustive inventory of ?good translations? for any given foreign language sen-tence. Therefore, it would be highly desirable to extend the coverage of the references for the simi-larity based evaluation methods. To match the system translation with various presentation of the same meaning, many work ha-ven been proposed to extend the references by generating lexical variations. The first strategy fo-cuses on the extension based on paraphrase identi-
37
fication (Lepage and Denoual, 2005; Lassner et al 2005; Zhou et al 2006; Kauchak and Barzilay, 2006; Owczarzak et al 2006; Owczarzak et al 2007). In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of sub-strings they may share. In short, all paraphrases of human-generated references should be considered ?good? translations. The second strategy extends the references with the synonymy (Banerjee and Lavie, 2005; Lassner et al 2005). This is an alter-nation to obtain lexical variations with synonymy dictionaries instead of the paraphrase. In this kind of method, the reference is matched against to the system translation with the pack of the synonymies of the reference words instead of the exact match-ing. Both two strategies can successfully capture the lexical variations and greatly extend the coverage of the references. But they still have two common deficiencies. The first is the demand of the external knowledge. Paraphrase based method need a mass of external corpus to extract paraphrases and syn-onymy based method need manually constructed semantic dictionaries. These demands seriously limit the application on various languages for which the external knowledge is absent. Another deficiency is that the two strategies cannot capture the equivalents of long sub-segments such as a clause. Synonymy based me-thod can only capture the equivalents of single words. Paraphrase based method can capture the equivalents of longer units but the length is still very narrow. In many cases, some long sub-segments can be varied with an entirely different presentation which cannot be decomposed into the variations of words or phrases. To address these problems we propose a novel strategy to generate variations presentation only using existing multiple manual references without any external knowledge. We identify the syntactic components on different level as the replaceable units and determine the syntactic equivalents of the components in the corresponding references. Then the equivalents of the syntactic components are hybridized into new references. The rest of the paper is organized as follows. Section 2 introduces the concept and identification of the syntactic equivalents. Section 3 proposes a process to hybridize the syntactic equivalents effi-
ciently. Experimental results are illustrated in sec-tion 4. We also include some related discussion in Section 5. Finally this work is concluded in Sec-tion 6. 2 Syntactic Equivalents  In our approach, we propose a novel method to obtain the equivalents of the sub-segments from the corresponding references to a single source sentence. A sub-segment can be a word, a phrase or longer unit such as a clause. As we know, the variations of the sentences to the same meaning can be distinguished into two categories. The first is the structural variations. In this case, presenta-tions employ the same words but arrange them in different structure. The second is lexical variations. In this case, presentations have the same structure but employ the different words. In practice, one reference sentence often has both of the two kinds of variations comparing with other corresponding reference sentences. As the previous works, we also focus on the lexical variations. The approach is that the equiva-lents of the words are not obtained by external knowledge. In our strategy, generally speaking, the equivalents of a sub-segment S in a reference sen-tence are identified as the sub-segments which play the same syntactic role in the same structure in the other corresponding references. The equivalents obtained in this way are called syntactic equiva-lents.  Suppose R1 and R2 is a corresponding reference sentence pair. T1 and T2 are the consecutive syntac-tic trees of R1 and R2 respectively. We formally define a syntactic equivalent pair between R1 and R2 with a 4-tuple:  <N1, N2, S1, S2>  where Ni is a non-terminal node in Ti and Si is the sub-segment which is covered by Ni. Then, all the syntactic equivalent pair R1 and R2 can be recur-sively identified using following process:  ?  The first syntactic equivalent pair <N1, N2, S1, S2> is identified where Ni is the root of Ti and Si= Ri. ?  Suppose <N1, N2, S1, S2> is a syntactic equivalent pair. {N11, N12, ?N1m} and { N21, N22, ?N2n} are the child nodes sequences of 
38
N1 and N2 respectively. If n=m and N1i= N2i (i.e. the child nodes sequence of N1 and N2 are exactly the same), for each node pair N1i and N2i a syntactic equivalent pair is identi-fied as < N1i, N2i, S1i, S2i>.  With this process, all equivalent pairs on differ-ent syntactic level can be identified by synchro-nously traveling the two trees from top to bottom. The following is an example of the identification of the equivalent pairs. Figure 1 gives out a refer-ence sentence pair and their syntactic trees. The nodes which are included in certain equivalent pair are surrounded by a rectangle.   (a)   (b)  Figure 1 An example of the identification of the syn-tactic equivalent pairs.    In this example, five equivalent pairs can be identified:  ?  <S, S, ?Machine translation develops con-stantly?, ?MT progresses persistently?> ?  <NP, NP, ?Machine translation?, ?MT?> ?  <VP, VP, ?develops constantly?, ?progresses persistently?> ?  <VV, VV, ?develops?, ?progresses?> ?  <ADV, ADV, ?constantly?, ?persistently?> 3 Hybridization of Syntactic Equivalents  The indentified syntactic equivalents pairs include the sub-segments which sharing the same role in the same syntactic structure. Because of this, we 
can obtain a variation of a reference sentence by switching the two sub-segments of an equivalent pair in this sentence. This operation did not change the structure of the sentence but only replace a sub-segment in the structure with its equivalent.  Consequently, two new references can be gener-ated by switching the two sub-segments of an equivalent pair between two reference sentences. Furthermore when we switch the sub-segments of all equivalent pairs between the two references, multiple new references are generated with various combinations of the switches. This operation is called the syntactic hybridization of the references which can be illustrated by following steps: Suppose R={ri}i=1?n is a reference set containing n reference sentences to a single source sentence. R? is the new reference set containing the original reference sentences and the hybridized reference sentences. R? can be obtained by formula (1):    where rooti is the root node of the syntactic tree of ri. Equ(nt) returns the set of all equivalent of the sub-segments covered by the tree node nt. The de-tailed process of Equ(nt) is:  Equ(nt):  Define set  equ = ? Add Seg(nt) to equ If nt is included in an equivalent pair <nt, nt?, s, s?> Add p? to equ Define childi=1?m is the m children of nt Define hybr = Equ(child1)?Equ(child2)??Equ(childm) Merge hybr into equ Return equ  where Seg(nt) is the sub-segment covered by the tree node nt. Operation S1? S2 generates the Carte-sian product of the sub-segment set S1 and S2, i.e. for each arbitrary sub-segment pair s1 and s2 se-lected from S1 and S respectively, we concatenate s1 and s2. Finally, the reduplicate references in R? are removed.   For the example in Section 2, eight hybridized references can be generated including the original two sentences:  
39
?  Machine Translation develops constantly ?  Machine Translation develops persistently ?  Machine Translation progresses constantly ?  Machine Translation progresses persistently  ?  MT develops constantly ?  MT develops persistently ?  MT progresses constantly ?  MT progresses persistently 4 Experiments  We will show experimental results in this section to verify the effectiveness of the extended set of hybridized reference sentences. In the experiments, multiple translations of the source language sen-tences are evaluated with several popular auto-matic evaluation metrics. The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson?s correlations between the human assessments and evaluation scores using two reference set are calculated and compared.   The multiple translations and human assess-ments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 & 2 describes the detail of the two datasets. The popular automatic evaluation metrics in-clude BLEU (Papieni et al, 2002), GTM (Me-lamed et al, 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are ob-tained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43.  Table 3 & 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of GTM is 2. ROUGE uses skip-bigram with a window of nine words. And METEOR is run in ?exact? mode.   Release Year 2006 Genre Newswire Number of segments 919 Source Language Chinese 
Target Language English Number of system transla-tions 7 Number of reference trans-lations 4 Human assessment scores Score 1-5, ade-quacy & fluency Table 1 Description of LDC2006T04  Release Year 2008 Genre Newswire Number of segments 249 Source Language Arabic Target Language English Number of system transla-tions 8 Number of reference trans-lations 4 Human assessment scores Score 1-7, ade-quacy  Table 2 Description of LDC2008E43  After the hybridization, each source sentence in LDC2006T04 has 31 corresponding reference sen-tences in average and each source sentence in LDC2008E43 has 66 corresponding reference sen-tences in average. The number of the references is greatly increased. And as shown in the results, the usage of the extended reference set improves the correlations with human assessments for all the metrics in most cases except the ROUGE on LDC 2008E43.  Metric Original Extended BLEU 0.3488 0.3564 GTM 0.3671 0.3681 ROUGE 0.4252 0.4325 METEOR 0.4686         0.4723 Table 3 Pearson?s correlations with human assess-ments on sentence level on LDC2006T04  Metric Original Extended BLEU 0.6092 0.6109 GTM 0.5434 0.5438 ROUGE 0.6628 0.6582 METEOR 0.7053         0.7089 Table 4 Pearson?s correlations with human assess-ments on sentence level on LDC2008E43  The following is a real instance in the experi-ments from LDC2008E43:  Four original references: 
40
 ?  Ten churches burned down in 10 days in the American state of Alabama ?  Burning of ten churches in ten days in the American state of Alabama ?  Ten churches set on fire in ten days in American state of Alabama ?  Torching of ten churches within ten days in American state of Alabama  Six additional references:  ?  Torching of ten churches in ten days in the American state of Alabama ?  Torching of ten churches within ten days in the American state of Alabama ?  Torching of ten churches in ten days in American state of Alabama ?  Burning of ten churches within ten days in American state of Alabama ?  Burning of ten churches within ten days in the American state of Alabama ?  Burning of ten churches in ten days in American state of Alabama  The syntactic structure of the original references:  ?  (TOP (S (NPB (CD Ten) (NNS Churches)) (VP (VBN Burned) (PP (IN Down) (PP (IN in) (NP (NPB (CD 10) (NNS Days)) (PP (IN in) (NP (NPB (DT the) (NNP American) (NNP State)) (PP (IN of) (NPB (NNP Ala-bama))))))))))) ?  (TOP (NP (NPB (NN Burning)) (PP (IN of) (NP (NPB (CD Ten) (NNS Churches)) (PP (IN in) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (DT the) (NNP American) (NNP State)) (PP (IN of) (NPB (NNP Alabama))))))))))) ?  (TOP (S (NPB (CD Ten) (NNS Churches)) (VP (VB Set) (PP (IN on) (NPB (NN Fire))) (PP (IN in) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (NNP Ameri-can) (NNP State)) (PP (IN of) (NPB (NNP Alabama)))))))))) ?  (TOP (NP (NPB (NNP Torching)) (PP (IN of) (NP (NPB (CD Ten) (NNS Churches)) (PP (IN within) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (NNP Ameri-
can) (NNP State)) (PP (IN of) (NPB (NNP Alabama)))))))))))  To investigate the distribution of the equivalents we also perform several statistics about the count and the length of the syntactic nodes. In table 5, we list the information about the count of the nodes. The first row is the average words count per refer-ence sentence. The second and third row is the count of all tree nodes and equivalent nodes in all references respectively. The fourth and fifth row is the average count of tree nodes and equivalent nodes per reference sentence respectively.   2006T04 2008E43 Average length of  reference 31.52 34.43 Total tree nodes 211231 62569 Total equivalent nodes 21807 10073 Average tree nodes 57.46 62.82 Average equivalent nodes 5.93 10.11 Table 5 Counts of the tree nodes and equivalent nodes in references.  We also investigate the distribution of the length (count of covered words) of the nodes. First, we count the tree nodes and equivalent nodes whose length is from 1 word to 50 words. Then we calcu-late the pro-portion of equivalent nodes and tree nodes for each length. Figure 2 and 3 illustrate the distribution of absolute count of the equivalent nodes. The X-axis is the length of the nodes and the Y-axis is the count. Figure 4 and 5 illustrate the distribution of the proportions on two datasets re-spectively. The X-axis is the length of the nodes and the Y-axis is the proportion. The investigation reveals four main messages. First, the absolute counts of the short equivalents are much more than those of long equivalents as expected. Second, the proportion of the long equivalents is greater than those of short equiva-lents, this clarify that the reason of large amount of short equivalents is the large amount of short tree nodes. Third, also from the proportion of view we can see that the new method comparably bias to the long equivalents. This happens because the method adopts a top-down survey of the tree. Forth, the multiple references in Arabic-English data seem to match each other better than the references 
41
in Chinese-English data. Arabic-English references have much more equivalents than Chinese-English data and bias to long equivalents more significant.  
 Figure 2 Distribution of absolute length of equivalent node on LDC2006T04  
  Figure 3 Distribution of absolute length of equivalent node on LDC2008E43  
  Figure 4 Distribution of length proportion of equiva-lent nodes on LDC2006T04 
 Figure 5 Distribution of length proportion of equiva-lent nodes on LDC2008E43 5 Discussion  The experimental results verify the positive effect of the hybridized reference for the automatic eval-
evaluation in most cases. Though the improvement of the correlations is not very significant it is stable across the metrics in various styles. Compared with the previous works based on pa-raphrase and synonym the new method has three important advantages. The first is that the hybrid-ized reference can switch the long span sub-segments beyond the words and phrases.  The second is that the switch can be per-formed in multiple levels, i.e. a sub-segment can not only be replaced as a single unit but also can be varied by replacing some child sub-segments of it. It?s noticeable that the multiple level switches also make it possible to present some structural varia-tions by means of the lexical variations. In hybridi-zation, we can realize some structural variation between syntactic nodes by switch their parent node instead of reordering them directly.  The third advantage is that the new method needs no external knowledge which greatly facili-tates the application. But this advantage also re-sults in the main deficiency of this approach: the hybridization references cannot adopt any novel equivalents which are absent in existing references. This deficiency can be overcome by introducing the paraphrase and synonym into the syntactic hy-bridization. It should be indicated that though the hybridiza-tion process generate many new references not all of the new references are reasonable.  In table 6 we compare the effect of hybridized references and manual references with more details on LDC2006T04. In the table, the first column is the contents of the references for each source sen-tence. ?Manual? means the manual references and the number in front of it indicates how many man-ual references are provided. ?Hybr? means the hy-bridized references generated from the manual references in front of the ?+?. The second column is the Pearson?s correlations between human as-sessments and the BLEU scores using the corre-sponding reference set. Besides the set containing 4 references the other correlations are the average of the correlations based on all possible subset con-taining certain number of references. For example correlation of ?2 Manual? is the average of the cor-relations based on 6 possible subset containing 2 references.  Reference Set Correlation 1 Manual 0.2565 
42
2 Manual 0.3057 2 Manual+ Hybr 0.3082 3 Manual 0.3316 3 Manual + Hybr 0.3369 4 Manual 0.3488 4 Manual+ Hybr 0.3564 Table 6 Pearson?s correlations based on incremental reference set  As shown in the Table 6 hybridized references can improve the correlations with human assess-ments on different sizes of manual references set. But it also indicated that though hybridization can generate a mass of novel references the new refer-ences is always not more effective than even one additional manual references. This tells us that the quality of the hybridized references still need to be further refined. Another message revealed by the table is that with the increase of the number of manual refer-ences the improvement of correlation made by ad-ditional manual references is decreasing. However, the improvement made by the hybridized is in-creasing. This happens because the number of hy-bridized references increases much faster than the number of manual references. There are still several noticeable deficiencies of this work. First, it only works when there are more than two existing references. This make it cannot be used to extend the single reference in mass bi-lingual corpus. Second, which is also the most im-portant one is that this method strongly focuses on the precision at the cost of recall. Though we have recognized many equivalents for each sentence but there are still many equivalents that share different context cannot be recognized. This will be our main future work. The last deficiency is the bias to the long equivalents. This problem is caused by the same reason with the second deficiency: this method define the equivalent with the same syntac-tic context. If two sub-nodes do not share the same parent it often have different brothers. 6 Conclusions and Future Work  In this work we present a novel method to extend the coverage of the reference set for the automatic evaluation of machine translation. The new method decomposes the existing references into sub-segments according to the syntactic structure. And then generate new reference sentences by hybridiz-
ing the equivalents of the segments which play the same syntactic role in corresponding references. In this way the new method can not only capture the equivalents of words and phrases like the other methods but also capture the equivalents of long sub-segments which are out of the capability of the other methods. Another important advantage of the new method is the no use of the external knowl-edge which greatly facilitates the application. Experimental results show that with the ex-tended reference set the state-of-the-arts automatic evaluation metrics achieve better correlation with the human assessments. In the future work, we will relax the restriction of the equivalent definition and try to recognize more equivalents. We will also introduce the para-phrase and synonyms into our method to see fur-ther improvement. Another interesting challenge is to hybridize the equivalents in the different order and present the structural variations directly. Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 60773066 and 60736014, the National High Tech-nology Development 863 Program of China under Grant No. 2006AA010108. References  Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Trans-lation and/or Summarization. M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. PhD Dissertation, Uni-versity of Pennsylvania. I. Dan Melamed, Ryan Green, Joseph P. Turian, 2003, Precision and recall of machine translation, In Pro-ceedings of HLT/NAACL 2003. David Kauchak, Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation, In Proceedings of the NAACL 2006. Dan Klein, Christopher Manning. 2003. Accurate Un-lexicalized Parsing. In Proceedings of the 41th Meet-ing of the ACL, pp. 423-430. Yves Lepage, Etienne Denoual. 2005. Automatic gen-eration of paraphrases to be used as translation refer-
43
ences in objective evaluation measures of ma-chine translation, In Proceedings of the IWP 2005. Karolina Owczarzak, Declan Groves, Josef Van Ge-nabith ,Andy Way. 2006. Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation, In Pro-ceedings of the Workshop on Statistical Ma-chine Translation. Karolina Owczarzak, Josef Van Genabith, Andy Way. 2007. Dependency-Based Automatic Evaluation for Machine Translation, In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation. Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-tion of machine translation, In Proceedings of the 40th Meeting of the ACL. Grazia Russo-Lassner, Jimmy Lin, Philip Resnik. 2005. Re-evaluating Machine Translation Results with Pa-raphrase Support, Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, University of Maryland, College Park, MD. Chin-Yew Lin, Franz Josef Och. 2004. Automatic evaluation of machine translation quality using long-est common subsequence and skip-bigram sta-tistics. In Proceedings of the 42th  Meeting of the ACL. Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-evaluating Machine Translation Results with Pa-raphrase Support, In Proceedings of the EMNLP 2006. 
44
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 45?50,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study of Translation Rule Classification for Syntax-based Statistical
Machine Translation
Hongfei Jiang, Sheng Li, Muyun Yang and Tiejun Zhao
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,lisheng,ymy,tjzhao}@mtlab.hit.edu.cn
Abstract
Recently, numerous statistical machine trans-
lation models which can utilize various kinds
of translation rules are proposed. In these
models, not only the conventional syntactic
rules but also the non-syntactic rules can be
applied. Even the pure phrase rules are in-
cludes in some of these models. Although the
better performances are reported over the con-
ventional phrase model and syntax model, the
mixture of diversified rules still leaves much
room for study. In this paper, we present a
refined rule classification system. Based on
this classification system, the rules are classi-
fied according to different standards, such as
lexicalization level and generalization. Espe-
cially, we refresh the concepts of the structure
reordering rules and the discontiguous phrase
rules. This novel classification system may
supports the SMT research community with
some helpful references.
1 Introduction
Phrase-based statistical machine translation mod-
els (Marcu and Wong, 2002; Koehn et al, 2003; Och
and Ney, 2004; Koehn, 2004; Koehn et al, 2007)
have achieved significant improvements in trans-
lation accuracy over the original IBM word-based
model. However, there are still many limitations in
phrase based models. The most frequently pointed
limitation is its inefficacy to modeling the struc-
ture reordering and the discontiguous correspond-
ing. To overcome these limitations, many syntax-
based SMT models have been proposed (Wu, 1997;
Chiang, 2007; Ding et al, 2005; Eisner, 2003; Quirk
et al, 2005; Liu et al, 2007; Zhang et al, 2007;
Zhang et al, 2008a; Zhang et al, 2008b; Gildea,
2003; Galley et al, 2004; Marcu et al, 2006; Bod,
2007). The basic motivation behind syntax-based
model is that the syntax information has the poten-
tial to model the structure reordering and discontigu-
ous corresponding by the intrinsic structural gener-
alization ability. Although remarkable progresses
have been reported, the strict syntactic constraint
(the both sides of the rules should strictly be a sub-
tree of the whole syntax parse) greatly hinders the
utilization of the non-syntactic translation equiva-
lents. To alleviate this constraint, a few works have
attempted to make full use of the non-syntactic rules
by extending their syntax-based models to more
general frameworks. For example, forest-to-string
transformation rules have been integrated into the
tree-to-string translation framework by (Liu et al,
2006; Liu et al, 2007). Zhang et al (2008a) made
it possible to utilize the non-syntactic rules and even
the phrases which are used in phrase based model
by advancing a general tree sequence to tree se-
quence framework based on the tree-to-tree model
presented in (Zhang et al, 2007). In these mod-
els, various kinds of rules can be employed. For
example, as shown in Figure 1 and Figure 2, Fig-
ure 1 shows a Chinese-to-English sentence pair with
syntax parses on both sides and the word alignments
(dotted lines). Figure 2 lists some of the rules which
can be extracted from the sentence pair in Figure 1
by the system used in (Zhang et al, 2008a). These
rules includes not only conventional syntax rules but
also the tree sequence rules (the multi-headed syn-
tax rules ). Even the phrase rules are adopted by
45
the system. Although the better performances are
reported over the conventional phrase-based model
and syntax-based model, the mixture of diversified
rules still leaves much room for study. Given such a
hybrid rule set, we must want to know what kinds of
rules can make more important contributions to the
overall system performance and what kinds of rules
are redundant compared with the others. From en-
gineering point of view, the developers may concern
about which kinds of rules should be preferred and
which kinds of rules could be discard without too
much decline in translation quality. However, one of
the precondition for the investigations of these issues
is what are the ?rule categories?? In other words,
some comprehensive rule classifications are neces-
sary to make the rule analyses feasible. The motiva-
tion of this paper is to present such a rule classifica-
tion.
2 Related Works
A few researches have made some exploratory in-
vestigations towards the effects of different rules by
classifying the translation rules into different sub-
categories (Liu et al, 2007; Zhang et al, 2008a;
DeNeefe et al, 2007). Liu et al (2007) differenti-
ated the rules in their tree-to-string model which in-
tegrated with forest1-to-string into fully lexicalized
rules, non-lexicalized rules and partial lexicalized
rules according to the lexicalization levels. As an
extension, Zhang et al (2008a) proposed two more
categories: Structure Reordering Rules (SRR) and
Discontiguous Phrase Rules (DPR). The SRR stands
for the rules which have at least two non-terminal
leaf nodes with inverted order in the source and tar-
get side. And DPR refers to the rules having at
least one non-terminal leaf node between two termi-
nal leaf nodes. (DeNeefe et al, 2007) made an illu-
minating breakdown of the different kinds of rules.
Firstly, they classify all the GHKM2 rules (Galley et
al., 2004; Galley et al, 2006) into two categories:
lexical rules and non-lexical rules. The former are
the rules whose source side has no source words.
In other words, a non-lexical rule is a purely ab-
1A ?forest? means a sub-tree sequence derived from a given
parse tree
2One reviewer asked about the acronym GHKM. We guess
it is an acronym for the authors of (Galley et al, 2004): Michel
Galley, Mark Hopkins, Kevin Knight and Daniel Marcu.
? ? ???
Figure 1: A syntax tree pair example. Dotted lines stands
for the word alignments.
stract rule. The latter is the complementary set of
the former. And then lexical rules are classified fur-
ther into phrasal rules and non-phrasal rules. The
phrasal rules refer to the rules whose source side
and the yield of the target side contain exactly one
contiguous phrase each. And the one or more non-
terminals can be placed on either side of the phrase.
In other words, each phrasal rule can be simulated
by the conjunction of two more phrase rules. (De-
Neefe et al, 2007) classifies non-phrasal rules fur-
ther into structural rules, re-ordering rules, and non-
contiguous phrase rules. However, these categories
are not explicitly defined in (DeNeefe et al, 2007)
since out of its focus. Our proposed rule classifica-
tion is inspired by these works.
3 Rules Classifications
Currently, there have been several classifications
in SMT research community. Generally, the rules
can be classified into two main groups according to
whether syntax information is involved: bilingual
phrases (Phrase) and syntax rules (Syntax). Fur-
ther, the syntax rules can be divided into three cat-
egories according to the lexicalization levels (Liu et
al., 2007; Zhang et al, 2008a):
1) Fully lexicalized (FLex): all leaf nodes in both
the source and target sides are lexicons (termi-
nals)
2) Unlexicalized (ULex): all leaf nodes in both the
46
??
? ?
??? ?
???
? ?
Figure 2: Some rules can be extracted by the system used in (Zhang et al, 2008a) from the sentence pair in Figure 1.
source and target sides are non-lexicons (non-
terminals)
3) Partially lexicalized (PLex): otherwise.
In Figure 2, R1-R3 are FLex rules, and R5-R8 are
PLex rules.
Following (Zhang et al, 2008b), a syntax rule r
can be formalized into a tuple
< ?s, ?t, AT , ANT >
, where ?s and ?t are tree sequences of source side
and target side respectively, AT is a many-to-many
correspondence set which includes the alignments
between the terminal leaf nodes from source and tar-
get side, and ANT is a one-to-one correspondence
set which includes the synchronizing relations be-
tween the non-terminal leaf nodes from source and
target side.
Then, the syntax rules can also fall into two cat-
egories according to whether equipping with gen-
eralization capability (Chiang, 2007; Zhang et al,
2008a):
1) Initial rules (Initial): all leaf nodes of this rule are
terminals.
2) Abstract rules (Abstract): otherwise, i.e. at least
one leaf node is a non-terminal.
A non-terminal leaf node in a rule is named an ab-
stract node since it has the generalization capabil-
ity. Comparing these two classifications for syntax
rules, we can find that a FLex rule is a initial rule
when ULex rules and PLex rules belong to abstract
rules.
These classifications are clear and easy for un-
derstanding. However, we argue that they need
further refinement for in-depth study. Specially,
more refined differentiations are needed for the ab-
stract rules (ULex rules and PLex rules) since they
play important roles for the characteristic capabil-
ities which are deemed to be the advantages over
the phrase-based model. For instance, the potentials
to model the structure reordering and the discon-
tiguous correspondence. The Structure Reordering
Rules (SRR) and Discontiguous Phrase Rules (DPR)
mentioned by (Zhang et al, 2008a) can be regarded
as more in-depth classification of the syntax rules.
In (Zhang et al, 2008a), they are described as fol-
lows:
Definition 1: The Structure Reordering Rule
(SRR) refers to the structure reordering rule that has
at least two non-terminal leaf nodes with inverted
order in the source and target side.
Definition 2: The Discontiguous Phrase Rule
(DPR) refers to the rule having at least one non-
terminal leaf node between two lexicalized leaf
nodes.
47
Based on these descriptions, R7, R8 in Figure 2
belong to the category of SRR and R6, R7 fall into
the category of DPR. Although these two definitions
are easy implemented in practice, we argue that the
definition of SRR is not complete. The reordering
rules involving the reordering between content word
terminals and non-terminal (such as R5 in Figure
2) also can model the useful structure reorderings.
Moreover, it is not uncommon that a rule demon-
strates the reorderings between two non-terminals
as well as the reorderings between one non-terminal
and one content word terminal. The reason for our
emphasis of content word terminal is that the re-
orderings between the non-terminals and function
word are less meaningful.
One of the theoretical problems with phrase based
SMT models is that they can not effectively model
the discontiguous translations and numerous at-
tempts have been made on this issue (Simard et al,
2005; Quirk and Menezes, 2006; Wellington et al,
2006; Bod, 2007; Zhang et al, 2007). What seems
to be lacking, however, is a explicit definition to the
discontiguous translation. The definition of DPR
in (Zhang et al, 2008a) is explicit but somewhat
rough and not very accurate. For example, in Fig-
ure 3(a), non-terminal node pair ([0,???], [0,?love?]
) is surrounded by lexical terminals. According to
Definition 2, it is a DPR. However, obviously it is
not a discontiguous phrase actually. This rule can be
simulated by conjunctions of three phrases (???, ?I?;
???, ?love?; ???,?you?). In contrast, the translation
rule in Figure 3(b) is an actual discontiguous phrase
rule. The English correspondences of the Chinese
word ??? is dispersed in the English side in which
the correspondence of Chinese word ??? is inserted.
This rule can not be simulated by any conjunctions
of the sub phrases. It must be noted that the dis-
contiguous phrase (???-?switch . . . off?) can not
be abstracted under the existing synchronous gram-
mar frameworks. The fundamental reason is that
the corresponding parts should be abstracted in the
same time and lexicalized in the same time. In other
words, the discontiguous phrase can not be modeled
by the permutation between non-terminals (abstract
nodes). Another point to notice is that our focus in
this paper is the ability demonstrated by the abstract
rules. Thus, we do not pay much attentions to the re-
orderings and discontiguous phrases involved in the
? ?? ? ?
Figure 3: Examples for demonstrating the actual discon-
tiguous phrase. (a) is a negative example for the definition
of DPR in (Zhang et al, 2008a), (b) is a actual discon-
tiguous phrase rule.
2
Figure 4: The rule classifications used in this paper. (a)
shows that the rules can be divided into phrase rules and
syntax rules according to whether a rule includes the syn-
tactic information. (b) illustrates that the syntax rules can
be classified into three kinds according to the lexicaliza-
tion level. (c) shows that the abstract rules can be classi-
fied into more refined sub-categories.
phrase rules (e.g. ?? ??-?switch the light off?)
since they lack the generalization capability. There-
fore, the discontiguous phrase is limited to the rela-
tion between non-terminals and terminals.
On the basis of the above analyses, we present
a novel classification system for the abstract rules
based on the crossings between the leaf node
alignment links. Given an abstract rule r =<
?s, ?t, AT , ANT >, it is
1) a Structure Reordering Rule (SRR), if ? a link
l ? ANT is crossed with a link l? ? {AT ?ANT }
a) a SRR NT2 rule, if the link l? ? ANT
b) a SRR NT-T rule, if the link l? ? AT
2) not a Structure Reordering Rule (N-SRR), other-
wise.
48
??
?
Figure 5: The patterns to show the characteristics of dis-
contiguous phrase rules.
Note that the intersection of SRR NT2 and SRR NT-
T is not necessary an empty set, i.e. a rule can be
both SRR NT2 and SRR NT-T rule.
The basic characteristic of the discontiguous
translation is that the correspondence of one non-
terminal NT is inserted among the correspondences
of one phrase X . Figure 5 (a) illustrates this sit-
uation. However, this characteristic can not sup-
port necessary and sufficient condition. For exam-
ple, if the phrase X can be divided like Figure 5
(b), then the rule in Figure 5 (a) is actually a re-
ordering rule rather than a discontiguous phrase rule.
For sufficient condition, we constrain that the phrase
X = wi . . . wj need to satisfy the requirement: wi
should be connected with wj through word align-
ment links (A word is connected with itself). In Fig-
ure 5(c), f1 is connected with f2 when NT ? is in-
serted between e1 and e2. Thus, the rule in Figure
5(c) is a discontiguous phrase rule.
Definition 3: Given an abstract rule r =<
?s, ?t, AT , ANT >, it is a Discontiguous Phrase iff
? two links lt1, lt2 from AT and a link lnt from ANT ,
satisfy: lt1, lt2 are emitted from the same word and
lt1 is crossed with lnt when lt2 is not crossed with
lnt.
Through Definition 3, we know that the DPR is a
sub-set of the SRR NT-T.
4 Conclusions and Future Works
In this paper, we present a refined rule classifica-
tion system. Based on this classification system, the
rules are classified according to different standards,
such as lexicalization level and generalization. Es-
pecially, we refresh the concepts of the structure re-
ordering rules and the discontiguous phrase rules.
This novel classification system may supports the
SMT research community with some helpful refer-
ences.
In the future works, aiming to analyze the rule
contributions and the redundances issues using the
presented rule classification based on some real
translation systems, we plan to implement some syn-
chronous grammar based syntax translation models
such as the one presented in (Liu et al, 2007) or
in (Zhang et al, 2008a). Taking such a system as
the experimental platform, we can perform compre-
hensive statistics about distributions of different rule
categories. What is more important, the contribu-
tion of each rule category can be evaluated seriatim.
Furthermore, which kinds of rules are preferentially
applied in the 1-best decoding can be studied. All
these investigations could reveal very useful infor-
mation for the optimization of rule extraction and the
improvement of the computational models for syn-
chronous grammar based machine translation.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
Rens Bod. 2007. Unsupervised syntax-based machine
translation: The contribution of discontiguous phrases.
In Proceedings of Machine Translation Summit XI
2007,Copenhagen, Denmark.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. In computational linguistics, 33(2).
Ding, Y. and Palmer, M. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars In Proceedings of ACL.
DeNeefe, S. and Knight, K. and Wang, W. and Marcu, D.
2007. What can syntax-based MT learn from phrase-
based MT? In Proceedings of EMNLP/CONLL.
Michel Galley, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-HLT 2004, pages 273-280.
49
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-rich
syntactic translation models In Proceedings of ACL-
COLING
Daniel Gildea 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proceedings of ACL 2003,
pages 80-87.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
2003.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL 2003, pages 127-133, Edmonton,
Canada, May.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115-124.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. ACL 2007,
demonstration session, Prague, Czech Republic, June
2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation.
In Proceedings of ACL-COLING.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL 2007, pages 704-711.
Daniel Marcu and William Wong. 2002. A phrase based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language Phrases. In
Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440-447.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417-449.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005, pages 271-
279, Ann Arbor, Michigan, June.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proceedings of
HLT/NAACL
Simard, M. and Cancedda, N. and Cavestro, B. and
Dymetman, M. and Gaussier, E. and Goutte, C. and
Yamada, K. and Langlais, P. and Mauser, A. 2005.
Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP, volume 2, pages 901-904.
Benjamin Wellington, Sonjia Waxmonsky and I. Dan
Melamed. 2006. Empirical Lower Bounds on the
Complexity of Translational Equivalence. In Proceed-
ings of ACL-COLING 2006, pages 977-984.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora. In
Proceedings of ACL 1997. Computational Linguistics,
23(3):377-403.
Min Zhang, Hongfei Jiang, Ai Ti AW, Jun Sun, Sheng
Li, and Chew Lim Tan. 2007. A tree-to-tree
alignment-based model for statistical machine trans-
lation. In Proceedings of Machine Translation Summit
XI 2007,Copenhagen, Denmark.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008a. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT
Min Zhang, Hongfei Jiang, Haizhou Li, Ai Ti AW,
and Sheng Li. 2008b. Grammar Comparison Study
for Translational Equivalence Modeling and Statistical
Machine Translation. In Proceedings of Coling
50
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 27?33,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Train the Machine with What It Can Learn 
? Corpus Selection for SMT 
Xiwu Han School of Computer Sci-ence and Technology, Heilongjiang University, Harbin City 150080 China 
hxw@hlju.edu.cn 
Hanzhang Li School of Computer Sci-ence and Technology, Heilongjiang University, Harbin City 150080 China 
lhj@hlju.edu.cn 
Tiejun Zhao School of Computer Science and Technology, Harbin Institute of Technology, Harbin City 150001 China 
tjzhao@mtlab.hit.edu.cn 
   
 
Abstract 
Statistical machine translation relies heavily on available parallel corpora, but SMT may not have the ability or intelligence to make full use of the training set. Instead of col-lecting more and more parallel training cor-pora, this paper aims to improve SMT performance by exploiting the full potential of existing parallel corpora. We first iden-tify literally translated sentence pairs via lexical and grammatical compatibility, and then use these data to train SMT models. One experiment indicates that larger train-ing corpora do not always lead to higher de-coding performance when the added data are not literal translations. And another ex-periment shows that properly enlarging the contribution of literal translation can im-prove SMT performance significantly. 
1 Introduction* 
Parallel corpora are generally considered indis-pensable for the training of a translation model in statistical machine translation (SMT). And most researchers tend to agree on the opinion that the more data is used to estimate the parameters of the translation model, the better it can approxi-mate the true translation probabilities, and in turn this will lead to a better translation performance. However, even if large corpora are easily avail-able, does an SMT system have the ability or intelligence to make full use of a training set?  Another aspect is that larger amounts of train-ing data also require larger computational re-
                                                           * This research is jointly supported by the National Natural Science Foundation of China under Grant No.60773069 and 60873169. 
sources. With increasing quantities of training data, the improvement of translation quality will become smaller and smaller. Therefore, while continuing to collect more and more parallel cor-pora, it is also important to seek effective ways of making better use of available parallel training data. Literal translation and free translation are two basic skills of human translation. A literal trans-lation is a translation that follows closely the form of the source language, also known as word-for-word translation (Larson 1984).  According to Mona Baker (1992) translation needs to maintain equivalence at different levels across languages. In bottom-up sequence, these levels are: the word level, the above word level, the grammatical level, the textual level and the pragmatic level. Lower levels of equivalence are often embedded in literal translation and easily maintained, whereas higher levels are very im-portant for free translation and very difficult to be achieved even for experienced translators be-cause this kind of equivalence more often than not calls for thorough analysis and understanding of the source language, which is obviously what an SMT system cannot be capable of. So from this perspective SMT may be regarded as a be-ginner in learning how to translate. The training of statistical machine translation mainly depends on the alignment probabilities estimated from certain frequencies observed in a parallel corpus. Thus, we may say that SMT translates according to its bilingual scanning ex-periences, and there is actually no deep compre-hension during the coding and decoding process. Since human learners of translation generally begin with the comparatively simpler techniques of literal translation, our efforts described in this paper are intended to discover whether a corpus 
27
of literal translations better suits the training of statistical machine translation.  In the following, section 2 introduces our cor-pus and proposes a combined method to recog-nize sentence pairs of literal translation. Section 3 describes our experiments with the acquired corpus on SMT training from two points of view. Section 4 analyzes the results from a linguistic point of view. And the conclusion is given in Section 5 with some suggestion for further work. 
2 Literal Translation Recognition 
Early machine translations were notorious for bad literal translations especially of idioms. However, good literal translation means to trans-late a sentence originally, and to keep the origi-nal message form, including the construction of the sentence, the meaning of the original words, use of metaphors and so on. Such a translation would be fluent and easy to comprehend by tar-get language readers. If we suppose that the training corpus for SMT is mainly constituted of good translations, our first task is to identify those literally translated sentence pairs. 
2.1 Our Corpus 
The corpus used for our experiment consists of 650,000 bilingual sentence pairs of English and Chinese, which were gathered either from public and free Internet resources or from our own translation works. The sentences are either trans-lated from Chinese to English or vice versa.  To facilitate the process of recognition, before the SMT experiment we preprocessed the corpus for the word and POS information, with English sentences parsed by (Collins 1999)?s head-driven parser and Chinese sentences by the head-driven parser of MI&TLAB at Harbin Institute of Tech-nology (Cao 2006). We define the literally translated sentence pairs as those that either embed enough word pairs which can be looked up in a bilingual dic-tionary, or share enough common grammatical categories. Hence, we invented two cross-lingual measures for the recognition of literal translation, i.e. lexical compatibility and grammatical com-patibility. 
2.2 Method of Lexical Compatibility 
The seed version of our bilingual dictionary is made up of 63,483 entries drawn from the bi-lingual dictionary for the rule-based Chinese-English machine translation system of CEMT2K 
developed by MI&TLAB at Harbin Institute of Technology (Zhao 2001). We extended the seed with synonyms from English WordNet v. 1.2 and Chinese Extended Tongyicicilin v. 1.0. The ex-tending algorithm is as follows. 
Input: The seed version dictionary SD, Chi-nese Extended Tongyicicilin CT, and English WordNet EW Output: An extended Chinese English dic-tionary ED Do: a. For each entry in SD, a) extend the Chinese part with all its synonyms found in CT; b) extend the English part with all its synonyms found in EW; c) accept the extended entry into ED. b. For each entry in ED, a) if its Chinese part is a subset of that of another entry, merge them; b) if its English part is a subset of that of another entry, merge them. 
An entry in our final extended dictionary in turn is organized as bilingual synonym classes, and there are altogether 43,820 entries including 212,367 Chinese and English lexical terms. By looking up Chinese-English word pairs in the extended dictionary, we defined the cross-lingual measure of lexical compatibility for a Chinese-English sentence pair as CL. 
wordsallofnumbertotalthe
uplookedpairswordofnumbertheCL =
 
For the recognition task, we employed a maxi-mum likelihood estimation filtering method with an empirical threshold of 0.85 on the lexical compatibility. Sentence pairs would be accepted as literal translation if their lexical compatibility CL > 0.85. Manual analysis on 15,000 sentence pairs showed that for this method the precision is 94.65% and the recall is only 16.84%. The low recall is obviously due to the limitations of our bilingual dictionary. 
2.3 Method of Grammatical Compatibility 
Although the diversity of grammatical categories tends to be great, some common word classes, such as nouns, pronouns, verbs, adjectives, etc, mainly constitute the vocabularies of most natu-ral languages. And our observations on English 
28
and Chinese parallel corpora show that the more literal a translation is, the more equivalent gram-matical categories the pair of sentences may share. We thus define the cross-lingual measure of grammatical compatibility as CG. 
?
= +
+= n
i ii
iiiG GCGEMax
GCGEMinC
1 1|)||,(|
1|)||,(|?  
GEi is an English grammatical category, |GEi| is the number it occurs in the English sentence, and GCi is the Chinese counterpart (see Table 1). n is the number of common grammatical catego-ries that make differences in the special task of recognizing literal translated sentence pairs. ?i is the weight for the respective category, which is trained by a simple gradient descent algorithm on a sample of 10,000 manually analysed sentence pairs. 
i Chinese English 1 noun noun 2 pronoun pronoun 3 verb verb 4 adjective adjective and adverb 
Table 1: Equivalent grammatical categories  For the recognition task, we also employed a maximum likelihood estimation filtering method with an empirical threshold of 0.82 on the gram-matical compatibility. Sentence pairs would be accepted as literal translation if their grammatical compatibility CG > 0.82. Evaluation on the held-out sample of 5,000 sentence pairs shows a precision ratio of 89.5% and a recall ratio of 42.34%. 
2.4 Combination of the Two Methods 
We simply combined the results of the two methods mentioned above to obtain a larger use-ful corpus. It is very interesting that the intersec-tion between the results of the two methods accounts only for a very small part, which is es-timated to be 17.2% of all the identified sentence pairs. The combined recognition results achieved a precision of 92.33% and a recall of 54.78% on the testing sample of 15,000 sentence pairs. And on the total corpus, our combined method ac-quired 201,062 sentence pairs that were classi-fied to be the results of literal translation. Further analysis on the sampled corpus shows that the wrongly unrecalled literally translated sentence pairs and the wrongly recalled ones are mainly due to bad segmentation of Chinese 
words or bad POS tagging results of both the Chinese and English parsers. In contrast, those sentence pairs correctly unrecalled are usually free transcriptions or bad translations. 
3 SMT Experiments 
3.1 Our Corpus and SMT System 
After excluding some too long sentence pairs, we got our final training corpus, which includes 200,000 Chinese-English sentence pairs of literal translation and 400,000 pairs of free translation1. Our evaluation corpus was drawn from the IWSLT Chinese-to-English MT test set of 2004, which includes 506 Chinese sentences and 16 English reference sentences for each Chinese one. Since our focus is not on a specific SMT ar-chitecture, we use the off-the-shelf phrase-based decoder Pharaoh (Koehn 2004). Pharaoh imple-ments a beam search decoder for phrase-based statistical models, and has the advantages of be-ing freely available and widely used. The phrase bilingual lexicon is derived from the intersection of bi-directional IBM Model 4 alignments, ob-tained with GIZA++ (Och and Ney 2003). For better comparison between experimental results, we kept all the system parameters as default, while only tuning our own parameters. 
3.2 Experiment on Incremental Training Corpora 
This experiment was designed to check whether it is true that larger training corpora always lead to better SMT decoding performance. We ran-domly segmented the 400,000 free translation sentence pairs into 4 subsets, with each of them including 100,000 pairs. A baseline SMT model was trained with the 200,000 literal translation sentence pairs, and then 4 other SMT models were trained on extended corpora, of which each later used corpus includes one more subset than the previous one.  The decoding performances in terms of BLEU and NIST scores of all 5 models are listed in the second and third column of Table 2, and the last column gives the numbers of out-of-vocabulary (OOV) words of each model on the test set. Curves in Figure 1 and 2, respectively, show the trajectories of BLEU and NIST scores in accor-dance with the sizes of extended training corpora. 
 
                                                           1  Note that ?free translations? are identified statistically using our recognition method for literal translations. 
29
Corpus Size BLEU NIST OOV 200,000 0.3835 7.0982 47 300,000 0.3695 6.9096 45 400,000 0.4113 7.1242 32 500,000 0.4194 7.1824 21 600,000 0.4138 7.1566 18 
Table 2: SMT performance with extended corpora  
 Figure 1: Trajectory of BLEU score  
 Figure 2: Trajectory of NIST score  A comparison between the different models? BLEU and NIST scores shows that a larger train-ing data set does not necessarily lead to better SMT decoding performance. Based on the literal translation data, when more and more free trans-lation data are added to the training set, the per-formance measures of the relevant SMT models fall at first, then rise, and at finally fall again. Furthermore, according to our manual analysis of the decoding results, free translation data have actually harmed the SMT model. It is just be-cause the much smaller numbers of OOV words have made up for the impairment that the per-formance measures have risen for two times. They, however, will fall when the decrease in OOV words fails to make it up. 
3.3 Experiment on Weighted Training Cor-pora 
This experiment was designed to exploit both the contribution of literal translation and the advan-tage of a large vocabulary from a larger corpus. To achieve such a goal, minor modifications need to be made towards the training corpus and the module of GIZA++.  We start with an SMT training data set X, which includes n bilingual sentence pairs, i.e. the 
input vector X = {x1, x2, x3, ?, xi, ?, xn-1, xn}. During the original training process, every sen-tence pair xi contributes in the same way to the estimation of parameters in the translation model since the corpus has not been weighted. Now we tried to adjust the contribution of xi according to our previous decision whether it is literal transla-tion or free translation. If we set the weight vec-tor to be W = {w1, w2, w3, ?, wi, ?, wn-1, wn}T, the weighted corpus would become X? = WX = {w1x1, w2 x2, w3x3, ?, wixi, ?, wn-1 xn-1, wn xn}, where     Hereby ? is an empirical weighting parameter in the range of 0<= ? <=1. The module of GIZA++ was modified to en-sure that the weights imposed on sentence pairs could be effectively transmitted to smaller trans-lation units. GIZA++ builds word alignments by means of counting occurrences of word pairs in the training corpus. Given a possibly translatable Chinese-English word pair D = <c, e>, the number N of its occurrences in our original train-ing corpus X can be calculated by summing up its occurrence number Nxi in each sentence pair, i.e. ? == ni xiNN 1  
Thus the weighted occurrence number N? of word pair D in the weighted training corpus can be calculated via the following equation. ? ?= =? ?== ni ni xiixiwi NwNN 1 1 )('  
Finally, GIZA++ estimates word alignment parameters on the basis of N?. Apart from this modification, all other parts of PHARAOH had been untouched to guarantee comparable ex-perimental results. We trained five SMT models of different weights on the previously mentioned corpora of free and literal translations. Table 3 lists both the training parameters and relevant decoding per-formances of the five models. Figures 3 and 4 show the trajectories of BLEU and NIST scores in accordance with the weight variable. We can see that the SMT model achieved the best per-formance when ? was set to be 0.67. 
wi  =    ? when xi is literal translation,            1 ? ? otherwise. 
30
 Corpus Size ? BLEU NIST OOV 400,000 0 0.4001 6.9082 23 600,000 0.5 0.4138 7.0796 18 600,000 0.67 0.4259 7.2997 26 600,000 0.8 0.4243 7.2706 39 200,000 1 0.3835 7.0982 47 
Table 3: SMT performances with weighted corpora  
 Figure 3: Trajectory of BLEU score  
 Figure 4: Trajectory of NIST score  Among the five models, that of ? = 0.5 is the baseline since here all sentence pairs contributed in the same way. Those of ? = 0 and 1 are two special cases designed to explore the isolated contribution of free and literal translation corpora in a contrastive way. Hereby the two models of ? = 0.67 and 0.8 are the central part of our ex-periment. According to the performance traject-ories it seems that a reasonable increase in the contribution of the corpus of literal translations effectively improves the decoding performance of the SMT system since the BLEU scores with ? = 0.67 and 0.8 are higher than that of the baseline which are 0.0121 and 0.0105, and of the NIST scores which are 0.2201 and 0.191. Our further analysis of the translation results and the related evaluation scores with different weight parameters showed that there exists some potential for literal translations to be used to im-prove SMT systems.  Our analysis indicates that two facts caused most of the out-of-vocabulary words (see Table 3). First, some OOV words never occurred in the training corpus; second, most others had been pruned off due to their much lower frequencies. Training corpora for ? = 0.67 and 0.8 have the 
same size of as that for ? = 0.5, but they resulted in much more OOV words than those for ? = 0.5 because the lower weight had decreased some related alignment probabilities very much. It seems that the large OOV increase must have counteracted the potential improvement to a cer-tain degree although it did not have a devastating effects in these two cases. Therefore, a proper selection of a corpus of literal translations as training data would contribute more to the im-provement of SMT models should some heuristic pruning methods be employed to avoid a possi-ble OOV increase. 
4 Related work  
There have been a lot of studies on SMT training data. Most of them are focused on parallel data collections. Some work tried to acquire more parallel sentences from the web (Nie et al 1999; Resnik and Smith 2003; Chen et al 2004). Oth-ers extracted parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). These works aim to collect more parallel training corpora, while our work aims to make better use of existing parallel corpora.  Some studies have also been conducted on parallel data selection and adaptation. Eck et al (2005) proposed a method to select more infor-mative sentences based on n-gram coverage. They used n-grams to estimate the importance of a sentence. The more previously unseen n-grams exist in the sentence, the more important the sen-tence is regarded. A TF-IDF weighting scheme was also tried in their method, but did not show improvements over n-grams. Their goal was to decrease the amount of training data to make SMT systems adaptable to small devices.  Some other works select training data accord-ing to domain information of the test set. Hildebrand et al (2005) used an information re-trieval method for translation model adaptation. They selected sentences similar to the test set from available in-of-domain and out-of-domain training data to form an adapted translation model. L? et al (2007) further used smaller adapted data to optimize the distribution of the whole training data. They took advantage both of larger data and adapted data.  Unlike all the above-mentioned studies, our method selected the training corpus according to basic theories of literal and free translation. This is somewhat similar to L? et al (2007), however, our weighting scheme also tried to make use of 
31
both larger and smaller data, which are free translations and literal translations in our case. Besides, there have also been some studies on language model adaptation in recent years, moti-vated by the fact hat large-scale monolingual corpora are easier to obtain than parallel corpora.. Examples are Zhao et al (2004), Eck et al (2004), Zhang et al (2006) and Mauser et al (2006). Since a language model is built for the target language in SMT, a one pass translation is usually needed to generate the n-best translation candidates in language model adaptation. The principle in our research could also be used for translation re-ranking to further improve SMT performance. 
5 Conclusions 
This paper presents a new method to improve statistical machine translation performance by making better use of the available parallel train-ing corpora. We at first identified literally trans-lated sentence pairs by means of lexical and grammatical compatibility, and then used these data to train SMT models. Experimental results show that literal and free translation corpora con-tribute differently to the training of SMT models. It seems that literal translation training data bet-ter suit SMT system at its present level of intelli-gence. The weighted training data can further improve translation performance by enlarging the contribution of literal translations while maintaining a larger vocabulary from the larger corpus of free translations. Detailed analysis shows that a literal translation corpus would con-tribute more to the improvement of SMT models if some heuristic pruning methods would be em-ployed to avoid possible OOV increase. In future work, we will improve our methods in several aspects. Currently, the recognition method for literal translations and the weighting schemes are very simple. It might work better by trying some supervised recognition techniques or using more complicated methods to determine the weights of sentence pairs with variant literal degree. What?s more, our present test corpus is an out-of-domain one, and this might have im-pacted the observations made in this work. Last, employing our method to the language model might also improve translation performance. 
Acknowledgments 
We are obliged to the authors of English Word-Net version 1.2 and Chinese Extended Tongy-icicilin version 1.0 for the free dictionary re-
sources they provided. We also thank the two reviewers for their constructive advices that we referred to when preparing the last version of this paper. 
References  
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the Transla-tion Model for Statistical Machine Translation based on Information Retrieval. Proceedings of EAMT 2005: 133-142. Arne Mauser, Richard Zens, Evgeny Matusov, Sasa Hasan, Hermann Ney. 2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation. Proceedings of International Work-shop on Spoken Language Translation: 103-110. Bing Zhao, Matthias Eck, Stephan Vogel. 2004. Lan-guage Model Adaptation for Statistical Machine Translation with structured query models. COLING-2004. Dragos Stefan Munteanu and Daniel Marcu. 2005. Improving Machine Translation Performance by Exploiting Comparable Corpora. Computational Linguistics, 31 (4): 477-504. Dragos Stefan Munteanu and Daniel Marcu. 2006. Extracting Parallel Sub-Sentential Fragments from Comparable Corpora. ACL-2006: 81-88. Franz Josef Och and Hermann Ney. 2003. A system-atic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19-52. Hailong Cao. 2006. Research on Chinese Syntactic Parsing Based on Lexicalized Statistical Model, Dissertation for PhD, Harbin Institute of Technol-ogy, Harbin. Jian-Yun Nie, Michel Simard, Pierre Isabelle, Richard Durand. 1999. Cross-Language Information Re-trieval based on Parallel Texts and Automatic Min-ing of Parallel Texts in the Web. SIGIR-1999: 74-81. Jisong Chen, Rowena Chau, Chung-Hsing Yeh. 2004. Discovering Parallel Text from the World Wide Web. ACSW Frontiers 2004: 157-161. Matthias Eck, Stephan Vogel, and Alex Waibel. 2004. Language Model Adaptation for Statistical Ma-chine Translation Based on Information Retrieval. Proceedings of Fourth International Conference on Language Resources and Evaluation: 327-330. Michael Collins. 1999. Head-Driven Statistical Mod-els for Natural Language Parsing. PhD Disserta-tion, University of Pennsylvania. Mona Baker. 2000. In Other Words: A Coursebook on Translaton, Foreign Language Teaching and Re-search Press, Beijing. Mildred L. Larson. 1984. Meaning-based translation: A guide to cross-language equivalence. Lanham, MD: University Press of America. Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In 6th Conference of the Association for 
32
Machine Translation in the Americas (AMTA), Washington, DC. Philip Resnik and Noah A. Smith. 2003. The Web as a Parallel Corpus. Computational Linguistics, 29(3): 349-380. Tiejun Zhao. 2001. Technical Reports for CEMT2K.  MI&TLAB, Harbin Institute of Technology, Harbin. Yajuan L?, Jin Huang and Qun Liu. 2007. Improving Statistical Machine Translation Performance by Training Data Selection and Optimization. Pro-ceedings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing and Computational Natural Language Learning, pp. 343-350. Ying Zhang, Almut Silja Hildebrand, Stephan Vogel.  2006. Distributed Language Modeling for N-best List Re-ranking. EMNLP-2006: 216-223. 
 
33
Coling 2010: Poster Volume, pages 214?222,
Beijing, August 2010
Hybrid Decoding: Decoding with Partial Hypotheses Combination over
Multiple SMT Systems?
Lei Cui?, Dongdong Zhang?, Mu Li?, Ming Zhou?, and Tiejun Zhao?
?School of Computer Science and Technology
Harbin Institute of Technology
{cuilei,tjzhao}@mtlab.hit.edu.cn
?Microsoft Research Asia
{dozhang,muli,mingzhou}@microsoft.com
Abstract
In this paper, we present hybrid decod-
ing ? a novel statistical machine transla-
tion (SMT) decoding paradigm using mul-
tiple SMT systems. In our work, in ad-
dition to component SMT systems, sys-
tem combination method is also employed
in generating partial translation hypothe-
ses throughout the decoding process, in
which smaller hypotheses generated by
each component decoder and hypotheses
combination are used in the following de-
coding steps to generate larger hypothe-
ses. Experimental results on NIST evalu-
ation data sets for Chinese-to-English ma-
chine translation (MT) task show that our
method can not only achieve significant
improvements over individual decoders,
but also bring substantial gains compared
with a state-of-the-art word-level system
combination method.
1 Introduction
In recent years, system combination for SMT has
been known to be quite effective with translation
consensus information built from multiple SMT
systems. The combination approaches can be
classified into two types. One is the combination
with each system?s outputs, which can be seen as
full hypotheses combination. The other is the par-
tial hypotheses (PHS) combination during the de-
coding phase.
A lot of impressive work has been done to im-
prove the performance of the SMT systems by uti-
?This work has been done while the first author was vis-
iting Microsoft Research Asia.
lizing consensus statistics which come from sin-
gle system or multiple systems. For example,
Minimum Bayes Risk (MBR) (Kumar and Byrne,
2004) decoding over n-best list finds a translation
that has lowest expected loss with all the other hy-
potheses, and it shows that improvement over the
Maximum a Posteriori (MAP) decoding. Several
word-based methods (Rosti et al, 2007a; Sim et
al., 2007) have also been proposed. Usually, these
methods take n-best list from different SMT sys-
tems as inputs, and construct a confusion network
for second-pass decoding. There are also a lot of
research work to advance the confusion network
construction by finding better alignment between
the skeleton and the other hypotheses (He et al,
2008; Ayan et al, 2008). Typically, all the ap-
proaches above only use full hypotheses but have
no access to the PHS information.
Moreover, some dedicated efforts have been
tried by manipulating PHS between multiple MT
systems. Collaborative decoding (co-decoding)
(Li et al, 2009) leverages translation consensus
by exchanging partial translation results and re-
ranking both full and partial hypotheses explored
in decoding. However, no new PHS are generated
compared to the individual decoding but only the
ranking is affected. Liu et al (2009) proposes
joint decoding, a method that integrates multiple
translation models in one decoder. Although joint
decoding is able to generate new translations com-
pared to single decoder, it has to use the PHS
existed in one of its component decoder at each
step. Different from their work, we propose a
new perspective which leverages outputs from lo-
cal word-level combination. This will potentially
bring much benefit of performance since word-
214
level combination can produce more promising
PHS.
The word-level system combination method is
employed to generate partial translation hypothe-
ses in our hybrid decoding framework. In this
sense, full hypotheses word-level combination
(FH-Comb) method(Rosti et al, 2007a; Sim et al,
2007; He et al, 2008; Ayan et al, 2008) can be
considered as a special case of hybrid decoding,
where their combinations are only performed on
the largest hypotheses. Similar with FH-Comb,
hybrid decoding also uses word alignment infor-
mation. However, challenge exists in hybrid de-
coding as word alignment needs to be carefully
conducted through the decoding process. Obvi-
ously, document-level word alignment methods
such as GIZA++(Och and Ney, 2000) are quite
time consuming and unpractical to be embedded
into hybrid decoding. We propose a heuristic
method that can conduct word alignment of par-
tial hypotheses based on word alignment informa-
tion of phrase pairs learnt automatically from the
model training process. In this way, more PHS are
generated and the search space is enlarged sub-
stantially, which brings better translation results.
The rest of the paper is organized as follows:
Section 2 gives a formal description of hybrid
decoding, including framework overview, word-
level PHS combination and parameter estimation.
We conduct experiments with different settings
and make comparison between our method and
baseline, as well as a state-of-the-art word-level
system combination method in Section 3. Exper-
imental results discussion is presented in Section
4. Section 5 concludes the paper.
2 Hybrid Decoding
2.1 Overview
Different system combination methods (Li et al,
2009; Liu et al, 2009) offer different frameworks
to coordinate multiple SMT decoders. Hybrid de-
coding provides a new scheme to organize mul-
tiple decoders to work synchronously. As the
decoding algorithms may differ in multiple de-
coders1, hybrid decoding has some difficulty in
1In the SMT area, some decoders use left-right decod-
ing to generate the hypothesis and?Pharaoh?(Koehn et al,
integrating different decoding algorithms. With-
out loss of generality, we assume that bottom-up
CKY-based decoding is adopted in each individ-
ual decoder, which is the same as co-decoding
(Li et al, 2009) and joint decoding (Liu et al,
2009). Hybrid decoding collects n-best PHS of a
source span2 from multiple decoders, then results
from word-level PHS combination of that span are
given back to each decoder, mixed with the origi-
nal PHS. After that, we re-rank the hybrid list and
continue the decoding. In an example with two
decoders, parts of the whole decoding process are
illustrated in Figure 1 and can be summarized as
follows:
3-5 6-6 3-4 5-6
3-6 3-6
1-2 3-6 1-2
1-6 1-6
1-6
Decoder1 Decoder2
Local decoding layer
Local decoding layer
Local decoding layer
Local combination layer
Local combination layer
3-6 3-6
1-6 1-6
Figure 1: Hybrid decoding with two decoders,
where the string ?s-e?means the source span
starts from position s and ends at position e. The
blank rectangles represent the n-best partial trans-
lations of each decoder, and the shaded rectan-
gles illustrate the n-best local combination out-
puts. The ovals denote bottom-up CKY-based de-
coding results.
2003) is one of them, while others adopt bottom-up decoding
which is represented by?Hiero?(Chiang, 2007).
2The word ?span?is used to represent translation unit
in CKY-based decoders, which denotes one or more consec-
utive words in the source sentence.
215
1. Individual decoding. Each individual de-
coder should maintain the n-best PHS of
each span from the bottom. After all the in-
dividual decoders finish translating the same
span, they feed their own partial translations
into a public container which can be used for
word-level PHS combination, then get back
the partial combination outputs for step 3.
2. Local word-level combination. After fed
with PHS from multiple decoders, a confu-
sion network is built and word-level combi-
nation for PHS is conducted. The obtained
new partial translations are given back to
each individual decoder to continue the de-
coding.
3. Mix new PHS with the original ones. The
span in each individual decoder will receive
the corresponding new PHS from the local
combination outputs. The feature space of
the new PHS is not exactly the same with that
of the original ones. It has to be mapped in
some way then the mixed hypotheses are re-
ranked.
In the following sub-sections, we first present
the background of word-level combination for
PHS, then introduce hybrid decoding algorithm in
detail, as well as the feature definition and param-
eter estimation.
2.2 Word-Level Combination for Partial
Hypotheses
Most word-level system combination methods are
based on confusion network decoding. In con-
fusion network construction, one hypothesis has
to be selected as the skeleton which determines
the word order of the combination results. Other
hypotheses are aligned against the skeleton. Ei-
ther votes or some word confidence scores are as-
signed to each word in the network.
Most of the research on confusion network con-
struction focuses on seeking better word align-
ment between the skeleton and the other hypothe-
ses. So far, several word alignment procedures are
used for SMT system combination, which mainly
are GIZA++ alignments (Matusov et al, 2006),
TER alignments (Sim et al, 2007) and IHMM
??||| political ||| 0-0
????||| political and economic ||| 0-0 1-2
??||| economic ||| 0-0
????||| economic interests ||| 0-0 1-1
?? [X1] ||| political and [X1] ||| 0-0 1-2
Figure 2: The example of translation alignment
from phrase-table and rule-table
alignments (He et al, 2008). Similar with general
word-level system combination method, word-
level PHS combination also uses word alignment
information. However, in hybrid decoding, it is
quite time-consuming and impractical to conduct
word alignment like GIZA++ for each span. For-
tunately, unit hypotheses word alignment can be
obtained from the model training process, which
is shown in Figure 2. We devise a heuristic
approach for PHS alignment that leverages the
translation derivations from the sub-phrases. The
derivation information ultimately comes from the
phrase table in phrase-based systems (Koehn et
al., 2003; Xiong et al, 2006) or the rule table in
syntactic-based systems (Chiang, 2007; Liu et al,
2007; Galley et al, 2006).
The derivation is built in a phrase-based sys-
tem as follows. For example, we have two phrase
translations ??? ? ||| our ||| 0-0 1-0?and
????? ||| economic interests ||| 0-0 1-1?,
where string ?m-n?means the mth word in the
source phrase is aligned to the nth word in the tar-
get phrase. When combining the two phrases for
generating ??? ? ?? ???, we obtain
the translation hypothesis as ?our economic in-
terests?and also integrate the alignment fragment
to get ?0-0 1-0 2-1 3-2?. The case is similar in
syntactic-based system for non-terminal substitu-
tion, which we will not discuss further here.
Next, we introduce the skeleton-to-hypothesis
word alignment algorithm in detail. With the
translation derivations, the skeleton-to-hypothesis
(sk2hy) word alignment can be performed based
on the source-to-skeleton (so2sk) and source-to-
hypothesis (so2hy) word alignment as they share
the same source sentence. The basic idea is to
construct the sk2hy word alignment with the min-
imum correspondence subsets (MCS). A MCS is
defined as a triple < SK,HY, SO > where the
216
SK is the subset of skeleton words, HY is the
subset of the hypothesis words, and SO is the
minimum source word set that all target words in
both SK and HY are aligned to. Figure 3 shows
the algorithm for skeleton-to-hypothesis align-
ment. Most of the pseudo-code is self-explained
except for some subroutines, which are listed in
Table 1.
1: procedure SKEHYPALIGN(so2sk, so2hy)
2: repeat
3: Fetch out a source word to SO
4: SO1 = SO2 = SO
5: repeat
6: SO=UNION(SO1, SO2)
7: SK=GETALIGN(SO, so2sk)
8: HY =GETALIGN(SO, so2hy)
9: SO1=GETALIGN(SK, so2sk)
10: SO2=GETALIGN(HY, so2hy)
11: until |SO1| == |SO2| == |SO|
12: simmax = ?infinity
13: for all sk ? SK do
14: for all hy ? HY do
15: sim =SIM(sk, hy)
16: if sim ? simmax then
17: simmax = sim
18: skmax = sk
19: hymax = hy
20: end if
21: end for
22: end for
23: ADDALIGN(skmax, hymax)
24: until all the source words are fetched out
25: end procedure
Figure 3: Algorithm for skeleton-to-hypothesis
alignment
Subroutines Description
UNION(A,B) the union of set A and set B
GETALIGN(S,align) get the words aligned to
S based on align
SIM(w1,w2) similarity between w1 and w2,
we use edit distance here
ADDALIGN(w1,w2) align w1 with w2
Table 1: Description for subroutines
Due to the variety of the word order in n-
best outputs, skeleton selection becomes essen-
tial in confusion network construction. The sim-
plest way is to use the top-1 PHS from any indi-
vidual decoder with the best performance under
some criteria. However, this cannot always lead
to better performance on some evaluation met-
rics (Rosti et al, 2007a). An alternative would
be MBR method with some loss function such as
TER (Snover et al, 2006) or BLEU (Papineni et
al., 2002). We show the experimental results of
two skeleton selection methods for PHS combina-
tion in Section 3.
2.3 Hybrid Decoding Model
For a given source sentence f , any individual de-
coder in hybrid decoding finds the best transla-
tion e? among the possible translation hypotheses
?(f) in terms of a ranking function F :
e? = argmaxe??(f)F(e) (1)
Suppose we have n individual decoders. The
ranking function Fn of the nth decoder can be
written as:
Fn(e) =
m?
i=1
?n,ihn,i(f, e) (2)
where each hn,i(f, e) is a feature function of the
nth decoder, and ?n,i is the corresponding feature
weight. m is the number of features in each de-
coder.
The final result of hybrid decoder is the top-
1 translation from the confusion network, which
is constructed on multiple decoders with the last
layer?s output of CKY-based decoding.
2.4 Hybrid Decoding Algorithm
The hybrid decoder acts as a control unit which
controls the synchronization of multiple individ-
ual decoders. The algorithm is fully demonstrated
in Figure 4. The hybrid decoder pushes the same
span f ji to different decoders and gets back the n-
best PHS (lines 2-6). When the span?s length is
too small, both word alignment and partial com-
bination results are not accurate. We predefine a
fixed threshold ? which is used for determining the
start-up of combination (line 7). When the length
condition holds, the n-best PHS of each individual
217
decoder are stored in container G (lines 8). Con-
fusion network is constructed and new PHS can be
extracted from it and are further mixed and sorted
with the original ones (lines 11-15).
1: procedure HYBRIDDECODING(fn1 , D)
2: for l? 1...n do
3: for all i, j s.t. j ? i = l do
4: G? ?
5: for all d ? D do
6: nbest =DECODING(d, i, j)
7: if j ? i ? ? then
8: ADD(G,nbest)
9: end if
10: end for
11: cn =CONNETBUILD(G)
12: nbest? =GETPARHYP(cn)
13: for all d ? D do
14: MIXSORT(nbestd, nbest?)
15: end for
16: end for
17: end for
18: end procedure
Figure 4: Hybrid decoding algorithm
2.5 Hybrid Decoding Features
Next we present the PHS word-level combination
feature functions for hybrid decoding. Following
(Rosti et al, 2007b), four features are utilized to
model the PHS as:
Word Confidence Feature hwc(e)
The word confidence feature is computed as
hwc(e) =
?n
i=1 ?iciw, where n is the num-
ber of the systems, ?i is the system confi-
dence of system i, and ciw is the word confi-
dence of word w in system i.
Word Penalty Feature hwp(e)
Word penalty feature is the number of words
in the partial hypothesis (PH).
Null Penalty Feature hnp(e)
For null penalty feature, we mean the number
of NULL links along the PH when extracted
from the confusion network.
Language Model Feature hlm(e)
Different from the above three combination
features, which can be obtained during the
confusion network construction or hypothe-
ses extraction, the language model feature
cannot be summed up on the fly. Instead,
it must be re-computed when building each
new PH.
2.6 Feature Space Mapping
The features used in hybrid decoding can be clas-
sified into two categories: features for individual
decoders (FID) and features for PHS word-level
combination (FComb), and they are independent.
When mixing the new PHS with the original ones
of individual decoders, FComb space has to be
mapped to a FID space. However, several features
in FID are not defined in FComb, such as source
to target (S2T) phrase probability, target to source
(T2S) phrase probability, S2T lexical probability,
T2S lexical probability and other model specific
features. A mapping function H needs to be de-
fined as follows:
Ffid = H(Ffcomb) (3)
where Ffcomb denotes the feature vector from
FComb space, while Ffid is the feature vector
from FID space.
An easy mapping function is implemented with
an intuitive motivation: PHS combination results
are better than the ones in individual decoder and
we prefer not to disorder the original search space.
Thus, the undefined feature values of PHS from
FComb space are assigned by corresponding fea-
ture values of the top-1 PH in original decoder.
Experiments show that our method is not only
practical but also quite effective.
2.7 Parameter Estimation
Minimum Error Rate Training (MERT) (Och,
2003) algorithm is adopted to estimate feature
weights for hybrid decoding. As hybrid decoder
makes use of PHS from both individual decoders
and combination results as a whole, we devise
a new feature vector representation. The feature
vectors from FID space and FComb space are sim-
ply concatenated to form a longer vector without
overlapping. The weights are tuned simultane-
ously in order to reach a relatively global optima.
218
3 Experiment
3.1 Data and Metric
We conducted our experiments on the test data
of NIST 2005 and NIST 2006 Chinese-to-English
machine translation tasks. The NIST 2003 test
data is used as the development data to tune the
parameters. Statistics of the data sets are shown in
Table 2. Translation performances are measured
with case-insensitive BLEU4 score (Papineni et
al., 2002). Statistical significance test is per-
formed using the bootstrap re-sampling method
proposed by Koehn (2004).
The bilingual training corpora we used are
listed in Table 3, which contains 498K sentence
pairs, 12.1M Chinese words and 13.8M English
words after pre-processing. Word alignment is
performed by GIZA++ (Och and Ney, 2000) in
both directions with the default setting, and the
intersect-diag-grow method is used to refine the
symmetric word alignment.
Data Set # Sentences
NIST 2003(dev) 919
NIST 2005(test) 1,082
NIST 2006(test) 1,664
Table 2: Statistics of test/dev data sets
LDC ID Description
LDC2003E07 Ch/En Treebank Par Corpus
LDC2003E14 FBIS Multilanguage Texts
LDC2005T06 Ch News Translation Text Part 1
LDC2005T10 Ch/En News Magazine Par Text
LDC2005E83 GALE Y1 Q1 Release - Translations
LDC2006E26 GALE Y1 - En/Ch Par Financial News
LDC2006E34 GALE Y1 Q2 Release - Translations
V2.0
LDC2006E85 GALE Y1 Q3 Release - Translations
LDC2006E92 GALE Y1 Q4 Release - Translations
Table 3: Training corpora for Chinese-English
translation
The language model used for hybrid decoding
and all the baseline systems is a 5-gram model
trained with the Xinhua portion of LDC English
Gigaword Version 3.0 plus the English part of
bilingual training data.
3.2 Implementation
We use two baseline systems. The first one
(SYS1) is re-implementation of Hiero, a hi-
erarchical phrase-based system (Chiang, 2007)
based on Synchronous Context Free Grammar
(SCFG). Phrasal translation rules and hierarchi-
cal translation rules with nonterminals are ex-
tracted from all the bilingual sentence pairs.
The second one (SYS2) is a phrase-based sys-
tem (Xiong et al, 2006) based on Bracketing
Transduction Grammar (Wu, 1997) with a lex-
icalized reordering model (Zhang et al, 2007)
under maximum entropy framework, where the
phrasal translation rules are exactly the same
with that of SYS1. The lexicalized reorder-
ing model is trained using the MaxEnt toolkit
(Zhang, 2006) where the training instances are
extracted from subset of the training corpora,
which contains LDC2003E07, LDC2003E14,
LDC2005T06, LDC2005T10. Both systems use
the bottom-up CKY-based decoding with cube-
pruning (Chiang, 2007) and the beam size is set
to 10 for decoding efficiency.
For hybrid decoder, we set ? to be
sentence.length ? 3, meaning that the PHS of
individual decoders only perform local combi-
nation in the last three layers. The reason why
we adopt this setting is because we find that
starting local combination on short spans hurts
the performance badly on test data. Experimental
results are shown in the next section.
3.3 Translation Results
We present the overall results of hybrid decod-
ing over two baseline systems on both test sets.
We also implement an IHMM-based word-level
system combination method (He et al, 2008) to
make comparison with hybrid decoding system,
and the n-best candidates used for IHMM-based
word-level system combination is set to 10. Pa-
rameters for all the systems are tuned on NIST
2003 test set. The results are shown in Table 4.
In Table 4, we find that the hybrid decoding per-
forms significantly better than SYS1 and SY2 on
both test sets. Besides, compared to IHMM word-
level system combination method, hybrid decod-
ing also brings substantial gains with 0.63% and
0.92% points respectively.
219
NIST 2005 NIST 2006
SYS1 0.3745 0.3346
SYS2 0.3699 0.3296
IHMM Word-Comb 0.3821? 0.3421?
Hybrid 0.3884?+ 0.3513?+
Table 4: Hybrid decoding results on test sets,
*:significantly better than SYS1 and SYS2 with
p<0.01, +:significantly better than IHMM Word-
Comb with p<0.01
We also try different layers for determining
the start-up of local word-level PHS combination.
Figure 5 gives the intuitive BLEU results.
 
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.4
Figure 5: Performance of hybrid decoding with
different start-up settings on NIST 2005 test set,
where the ?lastM? means to conduct local word-
level PHS combination in the last M layers from
the perspective of CKY-based decoding.
As shown in Figure 5, the performance drops
drastically if we start to conduct word-level PHS
combination too early. After considering about ef-
ficiency and performance, we determine to do that
in the last three layers.
We then investigate the effects on hybrid de-
coding with different beam sizes, and compare the
trend with two baseline systems and IHMM-based
word-level system combination method as well.
The results are illustrated in Figure 6.
From what we see in Figure 6, the performance
of each system is monotonically increasing as the
beam size becomes larger. Hybrid decoding per-
forms consistently better than IHMM Word-Comb
when the beam size is small, and the largest im-
provement (+0.63% points) is obtained when the
beam size is set to 10. However, as the beam size
 
0.355
0.36
0.365
0.37
0.375
0.38
0.385
0.39
0.395
0.4
10 20 50 100
SYS1
SY S2
IHMM
Hybrid
Figure 6: Performance of hybrid decoding with
different beam sizes on NIST 2005 test set
increases, the performance gap is getting narrow.
One intuitive observation is that hybrid decoding
performs slightly worse than IHMM Word-Comb
when the beam size is set to 100. One possible
reason for this phenomenon is that, the alignment
noise may be introduced to hybrid decoding since
we have to generate monolingual alignments with
many poor translation derivations.
The confusion network for PHS of each system
can be built independently. We would like to eval-
uate the performance of single system hybrid de-
coding. Table 5 gives the results on both Hiero
and BTG decoders.
NIST 2005 NIST 2006
SYS1 SYS2 SYS1 SYS2
baseline 0.3745 0.3699 0.3346 0.3296
self-comb 0.3770 0.3758? 0.3358 0.3355?
Table 5: Hybrid decoding for single system,
*:significantly better than baseline with p<0.05
Table 5 shows that BTG decoder (SYS2) has
more potential for so-called ?self-boosting?.
The self-combination of BTG decoder improves
the performance substantially over the baseline.
However, we did not observe any significant im-
provement for Hiero decoder (SYS1).
Finally, we examine the impacts of skeleton se-
lection for PHS in hybrid decoding. The results in
Table 6 demonstrate that, compared to the top-1
selection method, translation performance can be
improved significantly with MBR-based skeleton
selection method. It strongly suggests that choos-
ing the skeleton with more consistent word order
220
will lead to better translation results.
NIST 2005 NIST 2006
Top-1 0.3817 0.3415
MBR 0.3884? 0.3513?
Table 6: Skeleton selection in hybrid decoding,
*:significantly better than top-1 skeleton selection
method with p<0.01
4 Discussion
System combination methods have been widely
used in SMT to improve the performance. For
example, in (Rosti et al, 2007a), several combi-
nation methods have been proposed to make use
of different kinds of consensus information. In
(He et al, 2008), better word alignment method is
adopted to advance the word-level system combi-
nation. Our method is different from these meth-
ods in the sense that we do not exclusively rely
on the n-best full hypotheses from each individual
decoder, but emphasize the importance of word-
level combination for PHS. Thus, it enlarges the
search space and is more prone to find better trans-
lations. Experimental results have shown the ef-
fectiveness of our method.
The idea of multiple systems collaborative de-
coding (Li et al, 2009) works well on re-ranking
the outputs of each system using n-gram agree-
ment statistics. However, no new translation re-
sults are generated compared to individual decod-
ing. Our method takes advantage of confusion
network to give PHS which cannot be seen before.
Although (Liu et al, 2009) also work on PHS,
we explore the cooperation of multiple systems
from a new perspective. They use translation
derivations from different decoders jointly as a
bridge to connect different models. Different from
their work, we devise a heuristic method to ob-
tain word alignment information from the deriva-
tion of each decoder, which can be embedded
for word-level PHS combination easily and effi-
ciently.
5 Conclusion and Future Work
In this paper, we propose a new SMT decoding
framework named hybrid decoding, in which mul-
tiple decoders work synchronously to conduct lo-
cal decoding and local word-level PHS combina-
tion in turn. We also devise a heuristic method to
obtain word alignment information directly from
the translation derivations, which is both intuitive
and efficient. Experimental results show that with
hybrid decoding the overall performance can be
improved significantly over both the individual
baseline decoder and the state-of-the-art system
combination method.
In the future, we will involve more individual
SMT decoders into hybrid decoding. In addition,
we would like to keep on this work in two direc-
tions. On the one hand, start-up threshold of PHS
combination will be explored in detail to find its
underlying impact on hybrid decoding. On the
other hand, we will try to employ a more theoreti-
cally sound approach to conduct the feature space
mapping from the feature space of confusion net-
work to that of individual decoders.
References
Ayan, Necip Fazil, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 33-40
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 263-270
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):
pages 201-228
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961-968
He, Xiaodong, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis for combining outputs from ma-
chine translation systems. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 98-107
Koehn, Phillip, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
221
ings of the 2003 Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 48-54
Koehn, Phillip. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388-395
Kumar, Shankar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine trans-
lation. In Proceedings of the 2004 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 169-176
Li, Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009. Collaborative decoding: par-
tial hypothesis re-ranking using translation consen-
sus between decoders. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 585-
592
Liu, Yang, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
704-711
Liu, Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 576-584
Matusov, Evgeny, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced
hypotheses alignment. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 33-40
Och, Franz Josef. and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 440-447
Och, Franz Josef. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160-167
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311-318
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
2007 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 228-235
Rosti, Antti-Veikko, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312-319
Sim, K.C., W. Byrne, M. Gales, H. Sahbi, and P.
Woodland. 2007. Consensus network decoding
for statistical machine translation combinnation. In
32nd IEEE International Conference on Acoustics,
Speech, and Signal Processing
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annota-
tion. In the 7th conference of the Association for
Machine Translation in the Americas, pages 223-
231
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3): pages 377-
404
Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006.
Maximum entropy based phrase reordering model
for statistical machine translation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
521-528
Zhang, Dongdong, Mu Li, Chi-Ho Li, Ming Zhou.
2007. Phrase Reordering Model Integrating Syn-
tactic Knowledge for SMT. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 533-540
Zhang, Le. 2006. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
222
Coling 2010: Poster Volume, pages 665?673,
Beijing, August 2010
Combining Constituent and Dependency Syntactic Views for  
Chinese Semantic Role Labeling 
Shiqi Li1, Qin Lu2, Tiejun Zhao1, Pengyuan Liu3 and Hanjing Li1 
1School of Computer Science and Technology, 
Harbin Institute of Technology 
{sqli,tjzhao,hjlee}@mtlab.hit.edu.cn 
2Department of Computing, 
The Hong Kong Polytechnic University 
csluqin@comp.polyu.edu.hk 
3Institute of Computational Linguistics, 
Peking University 
liupengyuan@pku.edu.cn 
 
Abstract 
This paper presents a novel feature-
based semantic role labeling (SRL) 
method which uses both constituent 
and dependency syntactic views. Com-
paring to the traditional SRL method 
relying on only one syntactic view, the 
method has a much richer set of syn-
tactic features. First we select several 
important constituent-based and de-
pendency-based features from existing 
studies as basic features. Then, we pro-
pose a statistical method to select dis-
criminative combined features which 
are composed by the basic features. 
SRL is achieved by using the SVM 
classifier with both the basic features 
and the combined features. Experimen-
tal results on Chinese Proposition Bank 
(CPB) show that the method outper-
forms the traditional constituent-based 
or dependency-based SRL methods. 
1 Introduction 
Semantic role labeling (SRL) is a major me-
thod in current semantic analysis which is im-
portant to NLP applications. The SRL task is 
to identify semantic roles (or arguments) of 
each predicate and then label them with their 
functional tags, such as 'Arg0' and 'ArgM' in 
PropBank (Palmer et al, 2005), or 'Agent' and 
'Patient' in FrameNet (Baker et al, 1998).  
The significance of syntactic analysis in 
SRL has been proven by (Gildea and Palmer, 
2002; Punyakanok et al, 2005), and syntactic 
parsing has been applied by almost all current 
studies. In terms of syntactic representations, 
the SRL approaches are mainly divided into 
three categories: constituent-based, chunk-
based and dependency-based. Constituent-
based SRL has been studied intensively with 
satisfactory results. Chunk-based SRL has 
been found to be less effective than the con-
stituent-based by (Punyakanok et al, 2005). In 
recent years, the dependency-based SRL has 
been greatly promoted by the CoNLL shared 
tasks on semantic parsing (Hajic et al, 2009). 
However, there is not much research on com-
bined use of different syntactic views (Pradhan 
et al, 2005), on the feature level of SRL.  
This paper introduces a novel method for 
Chinese SRL utilizing both constituent-based 
and dependency-based features. The method 
takes constituent as the basic unit of argument 
and adopts the labeling of PropBank. It follows 
the prevalent feature-based SRL methods to 
first turn predicate-argument pairs into flat 
structures by well-defined linguistic features, 
and then uses machine learning methods to 
predict the semantic labels. The method also 
involves two classification phases: semantic 
role identification (SRI) and semantic role 
classification (SRC). In addition, a heuristic-
based pruning preprocessing (Xue and Palmer, 
2004) is used to filter out a lot of apparently 
inappropriate constituents at the beginning.  
665
And it has been widely reported that, in fea-
ture-based SRL, the performance can be im-
proved by adding several combined features 
each of which is composed by two single fea-
tures (Xue and Palmer, 2004; Toutanova et al, 
2005; Zhao et al, 2009). Thus, in this work, 
we exploit combined use of both constituent-
based and dependency-based features in addi-
tion to using features of singular types of syn-
tactic view. We propose a statistical method to 
select effective combined features using both 
constituent-based and dependency-based fea-
tures to make full use of two syntactic views. 
2 Related Work 
In recent years, many advances have been 
made on SRL using singular syntactic view, 
such as constituent (Gildea and Jurafsky, 2002; 
Xue and Palmer, 2004; Surdeanu et al, 2007), 
dependency (Hacioglu, 2004; Johansson and 
Nugues, 2008; Zhao et al, 2009), and CCG 
(Chen and Rambow, 2003; Boxwell et al 
2009). However, there are few studies on the 
use of multiple syntactic views. We briefly 
review the relevant studies of SRL using 
multiple syntactic views as follows. 
Pradhan et al (2005) built three semantic 
role labelers using constituent, dependency and 
chunk syntactic views, and then heuristically 
combined them at the output level. The method 
was further improved in Pradhan et al (2008) 
which trains two semantic role labelers for 
constituents and dependency separately, and 
then uses the output of the two systems as ad-
ditional features in another labeler using chunk 
parsing. The result shows an improvement to 
each labeler alone. A possible reason for the 
improvement is that the errors caused by dif-
ferent syntactic parsers are compensated. Yet, 
the features of different syntactic views can 
hardly complement each other in labeling. And 
the complexity of using multiple syntactic 
parsers is extremely high. Hacioglu (2004) 
proposed a SRL method to combine constitu-
ent and dependency syntactic views where the 
dependency parses are ob-tained through auto-
matic mapping of constitu-ent parses. It uses 
the constituent parses to get candidates and 
then, the dependency parses to label them.  
Boxwell et al (2009) proposed a SRL me-
thod using features of three syntactic views: 
CCG, CFG and dependency. It primarily uses 
CCG-based features associated with 4 CFG-
based and 2 dependency-based features. The 
combination of these syntactic views leads to a 
substantial performance improvement. Nguyen 
et al (2009) proposed a composite kernel 
based on both constituent and dependency syn-
tactic views and achieved a significant im-
provement in a relation extraction application. 
3 
Compared to related work, the proposed me-
thod integrates the constituent and dependency 
views in a collaborative manner. First, we de-
fine a basic feature set containing features 
from constituent and dependency syntactic 
views. Then, to make better use of two syntac-
tic views, we introduce a statistical method to 
select effective combined features from the 
basic feature set. Finally we use both the basic 
features and the combined features to identify 
and label arguments. One of the drawbacks of 
the related work is the considerable complexity 
caused by multiple syntactic parsing processes. 
In our method, the cost of syntactic parsing 
will increase only slightly as we derive de-
pendency parsing from constituent parsing us-
ing a constituent-to-dependency converter in-
stead of using an additional dependency parser. 
In our method, the feature set used for SRL 
consists of two parts: the basic feature set and 
the combined feature set built upon the basic 
feature set. The basic feature set can be further 
divided into constituent-based features and 
dependency-based features. Constituent fea-
tures focus on hierarchical relations between 
multi-word constituents whereas dependency 
features focus on dependencies between indi-
vidual words, as shown in Figure 1. Take the 
predicate '??' (increased) as an example, in 
Figure 1(a), the NP constituent '?????' 
(China's position) is labeled as 'Arg0'. The ar-
gument and the predicate are connected by the 
path of node types: 'NP-IP-VP-VP'. But in 
Figure 1(b), the individual word '??' (posi-
tion) is labeled as 'Arg0'. And the connection 
between the argument and the predicate is only 
one edge with the relation 'nsubj', which is 
more explicit than the path in the constituent 
structure. So the two syntactic views can com-
plement each other on different linguistic units. 
Design Principle and Basic Features 
666
3.1 Constituent-Based Features 
As a prevalent syntactic feature set for SRL, 
constituent-based features have been 
extensively studied by many researchers. In 
this work, we simply take 26 constituent-based 
features tested by existing studies, and add 8 
new features define by us. Firstly, the 26 
constituent-based features used by others are: 
y The seven "standard" features: predicate (c1), 
path (c2), phrase type (c3), position (c4), 
voice (c5), head word (c6) and predicate 
subcategorization (c7) features proposed by 
(Gildea and Jurafsky, 2002). 
y Syntactic frame (c8) feature from (Xue and 
Palmer, 2004). 
y Head word POS (c9), partial path (c10), 
first/last word in constituent (c11/c12), 
first/last POS in constituent (c13/c14), 
left/right sibling constituent (c15/c16), 
left/right sibling head (c17/c18), left/right 
sibling POS (c19/c20), constituent tree dis-
tance (c21) and temporal cue words (c22) 
features from (Pradhan et al, 2004). 
y Predicate POS (c23), argument's parent 
constituent (c24), argument's parent con-
stituent head (c25) and argument's parent 
constituent POS (c26) inspired by (Pradhan 
et al, 2004). 
Secondly, the 8 new features that we define 
are (we take the 'Arg0' node in Figure 1(a) as 
the example to illustrate them): 
y Locational cue words (c27): a binary feature 
indicating whether the constituent contains 
location cue words, similar to the temporal 
cue words (c22). This feature is defined to 
distinguish the arguments with the 'ArgM-
LOC' type from others. 
y POS pattern of argument's children (c28): 
the left-to-right chain of the POS tags of the 
argument's children, e.g. 'NR-DEG-NN'. 
y Phrase type pattern of argument's children 
(c29): the left-to-right chain of the phrase 
type labels of the argument's children, simi-
lar with the POS pattern of argument's chil-
dren (c28), e.g. 'DNP-NP'. 
y Type of LCA and left child (c30): The phrase 
type of the Lowest Common Ancestor (LCA) 
combined with its left child, e.g. 'IP-NP'. 
y Type of LCA and right child (c31): The 
phrase type of the LCA combined with its 
right child, e.g. 'IP-VP'. 
Three features: bag of words of path (c32), 
bag of words of POS pattern (c33) and bag of 
words of type pattern (c34), for generalizing 
three sparse features: path (c2), POS pattern of 
argument's children (c28) and phrase type pat-
tern of argument's children (c29) by the bag-
of-words representation. 
3.2 Dependency-Based Features 
The dependency parse can effectively repre-
sent the head-dependent relationship between 
words, yet, it lacks constituent information. If 
we want to label constituents using depend-
ency-based features, we should firstly map 
each constituent to one or more appropriate 
words in the dependency tree. In this paper, we 
use the head word of a constituent to represent 
the constituent in the dependency parses.  
The selection method of dependency-based 
features is similar to the method of constitu-
ent-based features. The 35 selected dependen-
cy-based features include: 
y Predicate/Argument relation type (d1/d2), 
relation path (d3), POS pattern of predi-
cate?s children (d4) and relation pattern of 
predicate?s children (d5) features from (Ha-
cioglu, 2004). 
y Child relation set (d6), child POS set (d7), 
predicate/argument parent word (d8/d9), 
predicate/argument parent POS (d10/d11), 
left/right word (d12/d13), left/right POS 
(d14/d15), left/right relation (d16/d17), 
left/right sibling word (d18/d19), left/right 
sibling POS (d20/d21) and left/right sibling 
relation (d22/d23) features as described in 
(Johansson and Nugues, 2008). 
y Dep-exists (d24) and dep-type (d25) features 
from (Boxwell et al, 2009). 
y POS path (d26), POS path length (d27), REL 
path length (d28) from (Che et al, 2008). 
y High/low support verb (d29/d30), high/low 
support noun (d31/d32) features from (Zhao 
et al, 2009). 
y  LCA?s word/POS/relation (d33/d34/d35) 
inspired by (Toutanova et al, 2005). 
To maintain the consistency between two 
syntactic views, the dependency parses are 
generated by a constituent-to-dependency con-
verter (Marneffe et al, 2006), which is suitable 
for semantic analysis as it retrieves the seman-
tic head rather than the general syntactic head, 
using a set of modified Bikel's head rules.  
667
4 Selection of Combined Features 
The combined features, each of which consists 
of two different basic features, have proven to 
be positive for SRL. Several combined features 
have been widely used in SRL, such as 'predi-
cate+head word' and 'position+voice'. But to 
our knowledge, there is no prior report about 
the selection method of combined features for 
SRL. The common entropy-based criteria are 
invalid here because the combined features 
always take lots of distinct values. And the 
greedy method is too complicated to be practi-
cal due to the large number of combinations. 
In this paper, we define two statistical crite-
ria to efficiently estimate the classification per-
formance of each combined feature on the cor-
pus. Inspired by Fisher Linear Discriminant 
Analysis (FLDA) (Fisher, 1938) in which the 
separation of two classes is defined as the ratio 
of the variance between the classes to the vari-
ance within the classes, namely larger ratio can 
lead to better separation between two classes, 
and the discriminant plane can be achieved by 
maximizing the separation. Therefore, in this 
paper, we adopt the ratio of inter-class distance 
to intra-class distance to measure to what ex-
tent a combined feature can partition the data.  
Initially, the feature set contains only the N  
basic features. We construct one combined 
feature abf  at each iteration by combining two 
basic features af  and bf , where , [1, ]a b N?  
and a b? . We push abf  into the feature set and 
take it as the 1N + th feature. Then, all the 
training instances are represented by feature 
vectors using the new feature set, and we then 
quantize the feature vectors of positive and 
negative data orderly to keep their intrinsic 
statistical difference. If the training dataset is 
denoted as :{ , }pos negD D D , then the separation 
criterion, namely the ratio of inter-class to in-
tra-class distance for feature if  can be given as 
( ) ( , )
( , )
i
fi
f pos neg
i
pos neg
InterDist D D
g f
IntraDist D D
=
 
(1)
where the inter-class and the intra-class dis-
tance between posD  and negD  for feature if  are 
specified by (2) and (3), respectively. 
( )2( , ) ( ) ( )
i i if pos neg f pos f neg
InterDist D D Mean D Mean D= ? (2)
2 2( , ) ( ) ( )f fi iif pos neg pos negIntraDist D D S D S D= + (3)
( )
if
Mean D  in (2) and ( )
if
S D  in (3) repre-
sents the sample mean and the corresponding 
sample standard deviation of feature if  in 
dataset D  as given in (4) and (5). 
( )
( )
| |i
x D
f
x i
Mean D
D
?=
?
, [1, 1]i N? +  (4)
( )2( ) ( )
( )
i
i
f
x D
f
Mean D x i
S D
N
?
?
=
?
, [1, 1]i N? +
(5)
Essentially, the inter-class distance reflects 
the distance between the center of positive da-
taset and the center of negative dataset, and the 
intra-class distance indicates the intensity of all 
instances relative to the corresponding center. 
Therefore, larger ratio will lead to a better par-
tition for a feature, as has been pointed out by 
FLDA. In order to compare the ratio between 
different combined features, we further stan-
dardize the value of ( )ig f  by computing its z-
score ( )iZ f  which indicates how many stan-
dard deviations between a sample and its mean, 
as given in (6). 
( ) ( )
( ) i ii
G
g f g f
Z f
S
?=  (6)
where ( )ig f  represents the sample mean as 
given in (7), and GS  represents the sample 
standard deviation of the sequence ( )ig f  
where i  ranges from 1 to N+1 as given in (8).  
1
1
( )
( )
1
N
i
i
i
g f
g f
N
+
== +
?
, [1, 1]i N? +  
(7)
1
2
1
( ( ) ( ))
N
i i
i
G
g f g f
S
N
+
=
?
=
?
, [1, 1]i N? +  
(8)
After figuring out the ( )aZ f  and ( )bZ f  for 
the basic feature af  and bf , and ( )abZ f  for the 
combined feature abf  by (6), we define the 
other criterion, namely the improvement 
( )abI f  of the combined feature, as the smaller 
difference between the z-score of the combined 
668
feature and its two corresponding basic fea-
tures as given in (9). 
( )( ) ( ) Max ( ),  ( )ab ab a bI f Z f Z f Z f= ?  (9)
Finally, the combined feature with a nega-
tive ( )abI f  value is eliminated. Then, we will 
rank the combined features in terms of their z-
score, and use the top N of them for later clas-
sification. The selection method based on the 
two criteria can effectively filter out combined 
features whose means have no significant dif-
ference between positive and negative data, 
and hence retain the potentially useful com-
bined features for the separation. Meanwhile, it 
has a relatively fast speed when dealing with a 
large number of features in comparison to the 
greedy method due to its simplicity. 
5 Performance Evaluation 
5.1 Experimental Setting 
In our experiments, we adopt the three-step 
strategy proposed by (Xue and Palmer, 2004). 
First, argument candidates are generated from 
the input constituent parse tree using the preva-
lent heuristic-based pruning algorithm in (Xue 
and Palmer, 2004). Then, each predicate-
argument pair is converted to a flat feature 
structure by which the similarity between two 
instances can be easily measured. Finally we 
employ the Support Vector Machines (SVM) 
classifier to identify and classify the arguments. 
It is noteworthy that we use the same basic 
features, but different combined features for 
the identification and classification of argu-
ments. We present the result comparison be-
tween using gold-standard parsing and auto-
matic parsing, and also offer an analysis of the 
contribution of the combined features.  
To evaluate the proposed method and com-
pare it with others, we use the most commonly 
used corpus in Chinese SRL, Chinese Proposi-
tion Bank (CPB) version 1.0, as the dataset. 
The CPB corpus contains 760 documents, 
10,364 sentences, 37,183 target predicates and 
88,134 arguments. In this paper, we focus on 
six main types of semantic roles: Arg0, Arg1, 
Arg2, ArgM-ADV, ArgM-LOC and ArgM-
TMP. The number of semantic roles of the six 
types accounted for 95% of all the semantic 
roles in CPB. For SRC, we use the one-versus-
all approach, in which six SVMs will be 
trained to separate each semantic type from the 
remaining types. We divide the corpus into 
three parts: the first 99 documents 
(chtb_001.fid to chtb_099.fid) serve as the test 
data, the last 32 documents (chtb_900.fid to 
chtb_931.fid) serve as the development data 
and the left 629 documents (chtb_100.fid to 
chtb_899.fid) serve as the training data.  
We use the SVM-Light Toolkit version 6.02 
(Joachims, 1999) for the implementation of 
SVM, and use the Stanford Parser version 1.6 
(Levy and Manning, 2003) as the constituent 
parser and the constituent-to-dependency con-
verter. In classifications, we employ the linear 
kernel for SVM and set the regularization pa-
rameter to the default value which is the recip-
rocal of the average Euclidean norm of training 
data. The performance metrics are: accuracy 
(A), precision (P), recall (R) and F-score (F). 
5.2 Combined Feature Selection 
First, we select the combined features for clas-
sifications of SRI and SRC using the method 
described in Section 4 on the training data with 
gold-standard parse trees. Due to the limit of 
this paper, we only list the top-10 combined 
features for SRI and SRC for the 6 different 
types, as shown in Table 1 in which each com-
bined feature is expressed by the IDs of its two 
basic features with a plus sign between them. 
Rank SRI ARG0 ARG1 ARG2 ADV LOC TMP
1 c1+c6 c1+c6 c1+c6 c1+c6 c1+c6 c5+c27 c1+c6 
2 c1+d3 c32+c30 c30+d31 c1+d1 c30+d27 c9+d17 c22+c27
3 d25+d14 c7+c6 c30+d32 c1+c7 c30+d28 c9+d13 c7+c6 
4 c4+d25 c1+c2 c5+c30 c7+c6 c1+c11 c9+c2 d26+d27
5 d25+d22 c1+c12 c30+d24 c1+c5 c24+d33 c23+c27 d26+d28
6 d25+d20 c23+c6 c30+c21 c1+c23 c30+d25 c9+c20 c23+d26
7 d25+d21 c1+c3 c5+c4 c23+c6 c24+d9 c14+c32 c5+d26 
8 d25+d18 c10+d35 c1+c10 c1+c3 c27+c2 c14+c10 d26+d31
9 d25+d19 c10+d1 c30+d10 c5+c6 c22+c2 c9+c26 d26+d32
10 d25+d35 c10+d28 c4+c6 c1+d5 c24+d13 c14+c2 c23+c6 
Table 1. Top-10 combined features for SRI and 
SRC ranked by z-score 
Table 1 shows that the commonly used 
combined features, such as 'predicate+head 
word' (c1+c6) and 'position+voice' (c4+c5) 
proposed by (Xue and Palmer, 2004) are also 
included. In particular, the 'predicate+head 
word' feature takes first place in all semantic 
669
categories except LOC, in which the combina-
tion of the new feature 'locational cue words' 
(c27) and the 'voice (c5)' feature performs the 
best. The results also show that the most fre-
quently occurred basic features in the com-
bined set are 'predicate' (c1), 'head word' (c6), 
'type of LCA and left child' (c30), 'dep-type' 
(d25) and 'POS path' (d26). These basic fea-
tures should be more discriminative when 
combined with others. Additionally, we find 
some other latent effective combined features, 
such as 'predicate subcategorization+head 
word' (c7+c6), 'predicate POS+head word' 
(c23+c6) and 'predicate+phrase type' (c1+c3), 
whose performance will be further validated 
and analyzed later in this section. It is obvious 
that the obtained combined features for SRI 
and SRC are different, and the obtained com-
bined features for each type are also different 
as our selection method is based on positive 
and negative data which are completely differ-
ent for each argument type. In SRI phase, we 
will use the combined features for all the six 
semantic types (after removing duplicates). 
Then, we evaluate the performance of SRL 
based on the top-N combined features. The 
preliminary evaluation on the development set 
suggests that the performance becomes stable 
when N exceeds 20. Therefore, we vary the 
value of N to 5, 10 and 20 in the experiments 
to evaluate the performance of combined fea-
tures. Corresponding to the three different val-
ues of N, we finally obtained 28, 60 and 114 
combined features for the SRL, respectively. 
5.3 SRL Using Gold Parses  
To illustrate each component of the method, 
we constructed 6 SRL systems using 6 differ-
ent feature sets: 'Constituent Only' (CO) - uses 
the constituent-based features, as presented in 
Section 3.1; 'Dependency Only' (DO) - uses 
the dependency-based features, as presented in 
Section 3.2; 'CD' - uses both the constituent-
based features and the dependency-based fea-
tures, but no combined features; 'CD+Top5' - 
obtained by adding the top-5 combined fea-
tures to the 'CD' system; and similarly for the 
'CD+Top10' and the 'CD+Top20' systems. And 
'CO' serves as the baseline in our experiments. 
First, we evaluate the performance of SRI 
using the held-out test set with gold-standard 
constituent parse trees. The corresponding de-
pendency parse trees are automatically gener-
ated by the constituent-to-dependency con-
verter included in the Stanford Parser. The 
testing results of the six systems on the SRI 
phase are shown in Table 2. 
System A (%) P (%) R (%) F (%)
CO 97.87 97.04 97.30 97.17
DO 92.76 92.90 84.19 88.33
CD 97.98 97.44 97.25 97.34
CD+Top5 98.12 97.56 97.58 97.57
CD+Top10 98.15 97.61 97.62 97.61
CD+Top20 98.18 97.68 97.64 97.66
Table 2. Results of SRI using gold parses 
It can be seen from Table 2 that 'CD' and 
'CD+Top20' give only slightly improvement 
over 'CO' by less than 1% point. In other words, 
feature combinations do not seem to be very 
effective for SRI. Then we label all recognized 
constituents in the SRI phase with one of the 
six semantic role types. Table 3 displays the F-
score of each semantic type and the overall 
SRC on the test set with gold-standard parses. 
System Arg0 Arg1 Arg2 ADV LOC TMP ALL
CO 92.40 90.57 59.98 96.25 86.80 98.14 91.23 
DO 90.70 88.22 56.95 94.54 81.23 97.37 89.14 
CD 92.85 91.29 63.35 96.55 87.55 98.32 91.86 
CD+Top5 93.96 92.79 73.48 97.13 88.63 98.31 93.22*1
CD+Top10 94.15 93.23 74.18 97.42 87.17 98.57 93.41*
CD+Top20 94.10 93.19 75.13 97.23 88.05 98.48 93.46*
Table 3. Results of SRC using gold parses  
Table 3 shows that the proposed method 
performs much better in SRC. It improves the 
constituent-based method by more than 2% in 
SRC. The effectiveness of combined features 
can also be clearly seen because the overall F-
scores of the three systems using combined 
features all exceed 93%, significant greater 
than the systems using singular features. The 
improvement is noticeable for all semantic role 
types except the 'TMP' type. It means that the 
dependency parses cannot provide additional 
information to the labeling of this type. The 
results of Table 2 and Table 3 together show 
                                                 
1 The F-score value with an asterisk (*) indicates 
that there is a statistically significant difference 
between this system and the baseline ('CO') using 
the chi-square test (p<0.05). 
670
that our method using combined features can 
effectively improve the performance of SRL 
on the SRC phases, when using gold parses.  
5.4 SRL Using Automatic Parses  
To measure the performance of the algorithm 
in practical conditions, we replicate the above 
experiments using Stanford Parser on the raw 
texts of the test set, without segmentation or 
POS tagging. The dependency parses are also 
generated from the automatic constituent 
parses, as described in Section 5.3. The results 
are shown in Table 4. 
System A (%) P (%) R (%) F (%)
CO 71.54 68.72 70.62 69.66
DO 68.86 65.06 60.68 62.79
CD 73.53 70.63 72.75 71.67*
CD+Top5  73.62 70.69 72.98 71.82*
CD+Top10 73.65 70.71 73.08 71.88*
CD+Top20 73.67 70.70 73.16 71.91*
Table 4. Results of SRI using automatic parses 
Table 4 shows that the proposed method is 
also effective when using automatic parses 
despite the dramatic decrease in F-scores in 
comparison to using gold-standard parses. The 
decline is mainly caused by the heuristic-based 
pruning strategy in which a number of real ar-
guments are pruned when using the constituent 
parses with errors. Further analysis shows that, 
in SRI using gold parses, the ratio of incor-
rectly pruned arguments to the total is less than 
2%, but the ratio jumps to 17% when using 
automatic parses. Next, on the basis of the SRI 
results, we test the performance of SRC using 
the automatic parses, as shown in Table 5. 
System Arg0 Arg1 Arg2 ADV LOC TMP ALL
CO 89.20 88.90 54.47 93.93 81.80 94.38 88.24
DO 88.79 89.32 50.21 91.27 78.26 93.86 87.63
CD 89.75 89.87 57.71 95.28 84.22 94.71 89.16*
CD+Top5 90.75 90.97 65.64 95.53 84.45 94.45 90.16*
CD+Top10 90.96 91.37 67.25 95.31 84.49 94.61 90.45*
CD+Top20 90.94 91.29 67.42 95.22 84.39 94.65 90.42*
Table 5. Results of SRC using auto parses 
Table 5 shows only a slight decline in com-
parison with the result of using gold-standard 
parses, and it maintains the same trend of per-
formance for each semantic role in the Table 3, 
which proves the validity of the proposed me-
thod when using automatic parses. Table 6 
shows the F-score of the overall SRL on both 
the gold-standard and the automatic parse data. 
System Gold Parse (F%) Auto Parse (F%)
CO 89.29 63.13 
DO 82.69 60.34 
CD 90.01 65.56* 
CD+Top5 91.47* 66.37* 
CD+Top10 91.68* 66.61* 
CD+Top20 91.76* 66.61* 
Table 6. Results of overall SRL 
Table 6 shows that the F-score of the 
'CD+Top20' surpasses that of the 'CO' system 
by more than 2% on the gold parses, and more 
than 3% on the automatic parse. In other words, 
the method using constituent and dependency 
syntactic views performs even more effective 
for the automatic parses. The last three rows of 
Table 6 shows that the top-10 combined fea-
tures perform better than the top-5 features by 
adding 32 more features, but the top-20 com-
bined features obtain similar results to the top-
10 features by adding 54 more features. It sug-
gests that only several salient combined fea-
tures can actually improve the performance.  
5.5 Combined Feature Performance 
To evaluate the performance of each combined 
feature to identify the salient combined fea-
tures for SRL, we rank the 60 combined fea-
tures used by the 'CD+Top10' system on the 
test data with gold-standard parses, according 
to the F-score improvement achieved by each 
combined feature. Here we list the top 20 of 
them which are shown in Table 7.  
Rank Feature ? F(%) Rank Feature ? F(%)
1 c1+c6 0.611 11 c10+d1 0.413
2 c1+c10 0.593 12 c5+d26 0.404
3 c4+c6 0.557 13 c24+d9 0.395
4 c9+c20 0.503 14 d25+d35 0.395
5 c23+c6 0.494 15 c30+d24 0.377
6 c1+c3 0.458 16 c9+c26 0.377
7 c9+d13 0.449 17 c10+d28 0.368
8 c14+c10 0.431 18 c30+d29 0.365
9 c1+c5 0.422 19 c30+d30 0.361
10 c24+d33 0.413 20 c7+c6 0.361
Table 7. Top-20 combined features 
As can be seen from Table 7, a half of com-
bined features are composed by constituent 
671
features only, and the other half contain at least 
one dependency-based feature. This indicates 
that dependency features can be helpful to con-
struct combined features for SRL. Through 
analyzing the performance of each combined 
features, we have obtained some new and ef-
fective combined features which were not rec-
ognized before, such as 'predicate+partial 
path' (c1+c10), 'position+head word' (c4+c6), 
'Head word POS+right sibling POS' (c9+c20). 
Observation from these combined features 
suggests that not all combined features are 
composed by two significant basic features. 
Some not significant ones, such as 'partial 
path' (c10) and 'Head word POS' (c9) can also 
produce salient combined features. 
Furthermore, we find that the relative order 
of the combined features in Table 7 is not ex-
actly consistent with their orders in Table 1. 
The inconsistency indicates that the estimation 
criteria used for combined features selection is 
not perfect. In estimation, the effect of com-
bined features is evaluated simply based on the 
distance between the positive and the negative 
dataset by considering the efficiency. But in 
practice, the effects of them are determined 
through one-by-one classification. 
5.6 Comparison to Other Work 
Finally, we compare the proposed method with 
other four representative Chinese SRL systems. 
First, the 'Xue1' system (Xue and Palmer, 2005) 
is a typical feature-based system using 9 basic 
features, 2 combined features and the Maxi-
mum Entropy (ME) classifier. Second, the 'Liu' 
system (Liu et al 2007) which uses 19 basic 
features, 10 combined features and also the 
ME classifier. Third, the 'Che' (Che, 2008) sys-
tem use a hybrid convolution tree kernel to 
directly measure the similarity between two 
constituent structures. Fourth, the 'Xue2' sys-
tem described in (Xue, 2008), which is similar 
to 'Xue1' on basic framework, but using a new 
feature set. The 'Xue2' system evaluates the 
SRL of the verbal predicates and the nominal-
ized predicates separately, and offers no con-
solidated evaluation in (Xue, 2008). So in the 
comparison, we refer to its performance on the 
verbal predicates and the nominalized predi-
cates as 'Xue21' and 'Xue22'. 
All the four systems mentioned above use 
the constituent as the labeling unit and use the 
CPB corpus as the data set, the same as our 
method. And we use the same training and test 
data splits as in the 'Xue1' and 'Che' systems. 
Table 8 shows the comparison results in terms 
of F-score on both gold parses and auto parses.  
System Gold Parse (F%) Auto Parse (F%)
Xue22 69.6 57.3 
Xue1 91.3 61.3 
Liu 91.31 ? 
Che 91.67 65.42 
Ours 91.76 66.61 
Xue21 92.0 66.8 
Table 8. Comparison to other work 
Table 8 shows that our method performs 
better than the 'Xue1', 'Liu' and 'Che' systems 
on both gold parses and automatic parses. It is 
only slightly worse than the 'Xue21', namely the 
verbal predicates part of the 'Xue2' system. But 
for the other part of the 'Xue2' system for the 
nominalized predicates, namely the 'Xue22', our 
method performs much better than it. The re-
sults further verify the validity of the method. 
6 Conclusions 
This paper presents a novel feature-based SRL 
approach for Chinese. Compared to the tradi-
tional feature-based methods, the method can 
effectively integrate the constituent and the 
dependency syntactic views at the feature level. 
The method provides an effective way to con-
nect two syntactic views by a statistical selec-
tion method of combined features to substan-
tially improve the feature-based SRL method. 
The complexity of the method will not increase 
significantly compared to the method using 
one syntactic view as we use a constituent-to-
dependency conversion rather than additional 
dependency parsing. The effectiveness of the 
method has been proven by the experiments on 
CPB using SVM classifier with linear kernel.  
 
Acknowledgments 
This work is supported by the Key Program of 
National Natural Science Foundation of China 
under Grant No. 60736014, the Key Project of 
the National High Technology Research and 
Development Program of China under Grant 
No. 2006AA010108, and the Hong Kong Poly-
technic University under Grant No. G-U297 
and G-U596.  
672
References  
Collins F. Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. 
Proceedings of Coling-ACL-1998. 
Stephen A. Boxwell, Dennis Mehay, and Chris 
Brew. 2009. Brutus: A Semantic Role Labeling 
System Incorporating CCG, CFG, and Depend-
ency Features. Proceedings of ACL-2009. 
Wanxiang Che. 2008. Kernel-based Semantic Role 
Labeling. Ph.D. Thesis. Harbin Institute of 
Technology, Harbin, China. 
John Chen and Owen Rambow. 2003. Use of Deep 
Linguistic Features for the Recognition and La-
beling of Semantic Arguments. Proceedings of 
EMNLP-2003. 
Weiwei Ding and Baobao Chang. 2008. Improving 
Chinese Semantic Role Classification with Hier-
archical Feature Selection Strategy. Proceedings 
of EMNLP-2008. 
Ronald A. Fisher. 1938. The Statistical Utilization 
of Multiple Measurements. Annals of Eugenics, 
8:376-386. 
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic Labeling of Semantic Roles. Computa-
tional Linguistics, 28(3):245-288. 
Daniel Gildea and Martha Palmer. 2002. The Ne-
cessity of Syntactic Parsing for Predicate Argu-
ment Recognition. Proceedings of ACL-2002. 
Kadri Hacioglu. 2004. Semantic Role Labeling 
Using Dependency Trees. Proceedings of COL-
ING-2004. 
Jan Hajic, Massimiliano Ciaramita, Richard Jo-
hansson, et al The CoNLL-2009 Shared Task: 
Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of CoNLL-2009. 
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods. 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed), MIT Press. 
Richard Johansson and Pierre Nugues. 2008. De-
pendency-based Semantic Role Labeling of 
PropBank. Proceedings of EMNLP-2008. 
Roger Levy and Christopher D. Manning. 2003. Is 
it harder to parse Chinese, or the Chinese Tree-
bank. Proceedings of ACL-2003. 
Huaijun Liu, Wanxiang Che, and Ting Liu. 2007. 
Feature Engineering for Chinese Semantic Role 
Labeling. Journal of Chinese Information Proc-
essing, 21(2):79-85. 
Marie-Catherine de Marneffe, Bill MacCartney, 
and Christopher D. Manning. 2006. Generating 
Typed Dependency Parses from Phrase Structure 
Parses. Proceedings of LREC-2006. 
Truc-Vien T. Nguyen, Alessandro Moschitti, and 
Giuseppe Riccardi. 2009. Convolution Kernels 
on Constituent, Dependency and Sequential 
Structures for Relation Extraction. Proceedings 
of EMNLP-2009. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguis-
tics, 31(1):71-106 
Sameer Pradhan, Wayne Waed, Kadri Haciolgu, 
and James H. Martin. 2004. Shallow Semantic 
Parsing using Support Vector Machines. Pro-
ceedings of HLT/NAACL-2004 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin, and Daniel Jurafsky. 2005. 
Semantic Role Labeling Using Different Syntac-
tic Views. Proceedings of ACL-2005. 
Sameer Pradhan, Wayne Ward, and James H. Mar-
tin. 2008. Towards Robust Semantic Role Label-
ing. Computational Linguistics, 34(2): 289-310. 
Vasin Punyakanok, Dan Roth, Wentau Yih. 2005. 
The Necessity of Syntactic Parsing for Semantic 
Role Labeling. Proceedings of IJCAI-2005. 
Mihai Surdeanu, Lluis Marquez, Xavier Carreras, 
and Pere R. Comas. 2007. Combination Strate-
gies for Semantic Role Labeling. Journal of 
Artificial Intelligence Research, 29:105-151. 
Kristina Toutanova, Aria Haghighi, and Christo-
pher D. Manning. 2005. Joint learning improves 
semantic role labeling. Proceedings of ACL-
2005. 
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing Features for Semantic Role Labeling. Pro-
ceedings of EMNLP-2004. 
Nianwen Xue and Martha Palmer. 2005 Automatic 
semantic role labeling for Chinese verbs. Pro-
ceedings of IJCAI-2005. 
Nianwen Xue. 2008. Labeling Chinese Predicates 
with Semantic Roles. Computational Linguistics, 
34(2):225-255. 
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009. 
Semantic Dependency Parsing of NomBank and 
PropBank: An Efficient Integrated Approach via 
a Large-scale Feature Selection. Proceedings of 
EMNLP-2009. 
673
Coling 2010: Poster Volume, pages 701?709,
Beijing, August 2010
Reexamination on Potential for Personalization in Web Search 
Daren Li1  Muyun Yang1  Haoliang Qi2  Sheng Li1  Tiejun Zhao1 
 
1School of Computer Science 
Harbin Institute of Technology 
{drli|ymy|tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
 
2School of Computer Science 
Heilongjiang Institute of Technology 
haoliang.qi@gmail.com 
 
Abstract 
Various strategies have been proposed 
to enhance web search through utiliz-
ing individual user information. How-
ever, considering the well acknowl-
edged recurring queries and repetitive 
clicks among users, it is still an open 
issue whether using individual user in-
formation is a proper direction of ef-
forts in improving the web search. In 
this paper, we first quantitatively dem-
onstrate that individual user informa-
tion is more beneficial than common 
user information. Then we statistically 
compare the benefit of individual and 
common user information through 
Kappa statistic. Finally, we calculate 
potential for personalization to present 
an overview of what queries can bene-
fit more from individual user informa-
tion. All these analyses are conducted 
on both English AOL log and Chinese 
Sogou log, and a bilingual perspective 
statistics consistently confirms our 
findings. 
1 Introduction 
Most of traditional search engines are designed 
to return identical result to the same query 
even for different users. However, it has been 
found that majority of queries are quite ambi-
guous (Cronen-Townsend et al, 2002) as well 
as too short (Silverstein et al, 1999) to de-
scribe the exact informational needs of users. 
Different users may have completely different 
information needs under the same query (Jan-
sen et al, 2000). For example, when users is-
sue a query ?Java? to a search engine, their 
needs can be something ranging from a pro-
gramming language to a kind of coffee. 
In order to solve this problem, personalized 
search is proposed, which is a typical strategy 
of utilizing individual user information. Pitkow 
et al (2002) describe personalized search as 
the contextual computing approach which fo-
cuses on understanding the information con-
sumption patterns of each user, the various 
information foraging strategies and applica-
tions they employ, and the nature of the infor-
mation itself. After that, personalized search 
has gradually developed into one of the hot 
topics in information retrieval. As for various 
personalization models proposed recently, Dou 
et al (2007), however, reveal that they actually 
harms the results for certain queries while im-
proving others. This result based on a large-
scale experiment challenges not only the cur-
rent personalization methods but also the mo-
tivation to improve web search by the persona-
lized strategies. 
In addition, the studies on query logs rec-
orded by search engines consistently report the 
prevailing repeated query submissions by large 
number of users (Silverstein et al, 1999; Spink 
et al, 2001). It is reported that the 25 most fre-
quent queries from the AltaVista cover 1.5% 
of the total query submissions, despite being 
only 0.00000016% of unique queries (Silvers-
tein et al, 1999). As a result, the previous us-
ers? activities may serve as valuable informa-
tion, and technologies focusing on common 
701
user information, such as collaborative filter-
ing (or recommendation) may be a better reso-
lution to web search. Therefore, the justifica-
tion of utilizing individual user information 
deserves further discussion. 
To address this issue, this paper conducts a 
bilingual perspective of survey on two large-
scale query logs publically available: the AOL 
in English and the Sogou1 in Chinese. First we 
quantitatively investigate the evidences for 
exploiting common user information and indi-
vidual user information in these two logs. Af-
ter that we introduce Kappa statistic to meas-
ure the consistency of users? implicit relevance 
judgment inferred from clicks. It is tentatively 
revealed that using individual user information 
is what requires web search to face with after 
common user information is well exploited. 
Finally, we study the distribution of potential 
for personalization over the whole logs to gen-
erally disclose what kind of query deserves for 
individual user information. 
The remainder of this paper is structured as 
follows. Section 2 introduces previous me-
thods employing individual and common user 
information. In Section 3, we quantitatively 
compare the evidences for exploiting common 
user information and individual user informa-
tion. In Section 4, we introduce Kappa statistic 
to measure the consistency of users? clicks on 
the same query and try to statistically present 
the development direction of current web 
search. Section 5 figures out utilizing individu-
al user information as a research issue after 
well exploiting common user information. Sec-
tion 6 presents the potential for personalization 
curve, trying to outline which kind of queries 
benefit the most from individual user informa-
tion. Conclusions and future work are detailed 
in Section 7. 
2 Related Work 
With the rapid expansion of World Wide Web, 
it becomes more and more difficult to find re-
levant information through one-size-fits-all 
information retrieval service provided by clas-
sical search engines. Two kinds of user infor-
mation are mainly used to enhance search en-
                                                 
1 A famous Chinese search engine with a large number of 
Chinese web search users. 
gines: common user information and individu-
al user information. We separately review the 
previous works focusing on using these two 
kinds of information. 
Among various attempts to improve the per-
formance of search engine, collaborative web 
search is the one to take advantage of the repe-
tition of users? behaviors, which we call com-
mon user information. Since there is no unified 
definition on collaborative web search, in this 
paper, we believe that the collaborative web 
search assumes that community search activi-
ties can provide valuable search knowledge, 
and sharing this knowledge facilitates improv-
ing traditional search engine results (Smyth, 
2007). An important technique of collaborative 
web search is Collaborative Filtering (CF, also 
known as collaborative recommendation), in 
which, items are recommended to an active 
user based on historical co-occurrence data 
between users and items (Herlocker et al, 
1999). A number of researchers have explored 
algorithms for collaborative filtering and the 
algorithms can be categorized into two classes: 
memory-based CF and model-based CF. 
Memory-based CF methods apply a nearest-
neighbor-like scheme to predict a user?s rat-
ings based on the ratings given by like-minded 
users (Yu et al, 2004). The model-based ap-
proaches expand memory-based CF to build a 
descriptive model of group-based user prefe-
rences and use the model to predict the ratings. 
Examples of model-based approaches include 
clustering models (Kohrs et al, 1999) and as-
pect models (J. Canny, 2002). 
The other way to improve web search is per-
sonalized web search, focusing on learning the 
individual preferences instead of others? beha-
viors, which is called individual user informa-
tion. Early works learn user profiles from the 
explicit description of users to filter search re-
sults (Chirita et al, 2005). However, most of 
users are not willing to provide explicit feed-
back on search results and describe their inter-
ests (Carroll et al, 1987). Therefore, recent 
researches on the personalized search focus on 
modeling user preference from different types 
of implicit data, such as query history (Speretta 
et al, 2005), browsing history (Sugiyama et al, 
2004), clickthrough data (Sun et al, 2005), 
immediate search context (Shen et al, 2005) 
and other personal information (Teevan et al, 
702
2005). So far, there is still no proper compari-
son between the two solutions. It is still an 
open question which kind of information is 
more effective to build the web search model. 
Considering the difficulty in collecting pri-
vate information, using individual user infor-
mation seems less promising as the cost-
effective solution to web search. To address 
this issue, some researches about the value of 
personalization have been conducted. Teevan 
et al (2007) have done a ground breaking job 
to quantify the benefit for the search engines if 
search results were tailored to satisfy each user. 
The possible improvement by the personalized 
search, named potential for personalization, is 
measured by a gap between the relevance of 
individualized rankings and group ranking 
based on NDCG. However, it is less touched 
for the position of individual user information 
in contrast with common user information in 
large scale query log and how to balance the 
usage of common and individual information 
in information retrieval model. 
This paper tentatively examines individual 
user information against common user infor-
mation on two large-scale search engine logs 
in following aspects: the evidence from clicks 
on the same query, Kappa statistic for the 
whole queries, and overall distribution of que-
ries in terms of number of submissions and 
Kappa value. The bilingual statistics consis-
tently reveals the tendency of using individual 
user information as an equally important issue 
as (if not more than) using common user in-
formation) issue for researches on web search. 
3 Quantitative Evidences for Using 
Common or Individual User Infor-
mation 
To quantitatively investigate the value of 
common user information and individual user 
information in query log, we discriminate the 
evidence for using the two different types of 
user information as follows: 
(1) Evidence for using common user infor-
mation: if there were multiple users who have 
exactly the same click sets on one query, we 
suppose those clicks sets, together with the 
query, as the evidence for exploiting common 
user information. It is clear that such queries 
are able to be better responded with other?s 
search results. Note that common user infor-
mation is hard to be clearly defined, in order to 
simplify the quantitative statistics we give a 
strict definition. Further analysis will be shown 
in following sections. 
(2) Evidence for using individual user in-
formation: if a user?s click set on a query was 
not the same as any other?s, for that query, the 
search intent of the user who issue that query 
can be better inferred from his/her individual 
information than common user information. 
We suppose this kind of clicks, together with 
the related queries, as the evidence for exploit-
ing individual user information. 
Since users may have different search in-
tents when they issue the same query, a query 
can be an evidence for using both common and 
individual user information. In our statistics, if 
a query has both duplicate click sets and 
unique click set, the query is not only counted 
by the first category but also the second cate-
gory.  
The statistics of the two categories are con-
ducted in the query log of both English and 
Chinese search engines. We use a subset of 
AOL Query Log from March 1, 2006 to May 
31, 2006 and Sogou Query Log from March 1, 
2007 to March 31, 2007. The basic statistics of 
AOL and Sogou log are shown in Table 1. No-
tice that the queries in raw AOL and Sogou log 
without clicks are removed in this study. 
 
Item AOL Sogou 
#days 92 31 
#users 6,614,960 7,488,754 
#queries 7,840,348 8,019,229 
#unique queries 4,811,649 4,580,836 
#clicks 12,984,610 17,607,808 
Table 1: Basic statistics of AOL & Sogou log 
 
Table 2 summarizes the statistics of differ-
ent evidence categories over AOL and Sogou 
log. Note that click set refers to the set of 
clicks related to a query submission instead of 
a unique query. As for evidence for using 
common and individual user information, there 
is no clear distinction in terms of number of 
records, number of users in two logs. However, 
in terms of unique query and distinct click set, 
one can?t fail to find that evidence for using 
individual user information clearly exceeds  
703
Log 
The Condition Number 
Repeated queries Click Records User Unique Query 
Distinct 
Click Set 
AOL 
3,745,088 
(47.77% of total 
query submissions) 
Same 2,438,284 277,416 382,267 461,460 
Different 2,563,245 343,846 542,593 1,349,892 
Sogou 
4,252,167 
(53.02% of total 
query submissions) 
Same 2,469,363 1,380,951 228,315 358,346 
Different 5,481,832 1,545,817 752,047 2,171,872 
 
Table 2: Different click behaviors on repeated queries 
that for using common user information, espe-
cially in Sogou log. Therefore, though making 
use of common and individual user informa-
tion can address equally well for half users and 
half visits to the search engine, the fact that  
much more unique queries and click sets ac-
tually claims the significance of needing indi-
vidual user information to personalize web 
results. And methods exploiting individual us-
er information provide a much more challeng-
ing task in terms of problem space, though one 
may argue utilizing common user information 
is much easier to attack. 
4 Kappa Statistics for Individual and 
Common user information 
Section 3 has shown the evidence for using 
individual user information is prevailing than 
common user information in quantity for the 
unique queries in search engines. However, 
these counts deserve a further statistical cha-
racterization. In this section, we introduce 
Kappa statistic to depict the overall consisten-
cy of users? clicks in query logs. 
4.1 Kappa 
Kappa is a statistical measure introduced to 
access the agreement among different raters. 
There are two types of Kappa. One is Cohen?s 
Kappa (Cohen, 1960), which measures only 
the degree of agreement between two raters. 
The other is Fleiss?s Kappa (Fleiss, 1971), 
which generalizes Cohen?s Kappa to measure 
agreement among more than two raters, de-
noted as: 
e
e
P
PP
?
?=
1
?  
where, P is the probability that a randomly 
selected rater agree with another on a random-
ly selected subject. eP is the expected probabil-
ity of agreement if all raters made ratings by 
chance. If we use Kappa to measure the consis-
tency of relevance judgment by different raters, 
P can be interpreted as the probability that 
two random selected raters consistently rate a 
random selected search result as relevant or 
non-relevant one. Similarly, eP can also be 
construed as the expected probability of iden-
tical relevance judgment rated by different ra-
ters all by chance.  
Teevan et al (2008) used Fleiss?s Kappa to 
measure the inter-rater reliability of different 
raters? explicit relevance judgments. We ex-
pand their work and employ Fleiss?s Kappa to 
measure the consistency of implicit relevance 
judgments by users on the same query2. Here 
clicks are treated as a proxy for relevance: 
documents clicked by a user are judged as re-
levant and those not clicked as non-relevant 
(Teevan et al, 2008). As we all know that the 
result set of one query may change over time, 
so we select the longest time span to calculate 
Kappa value of a query, during which the re-
sult set of it preserves unchanged. From Kappa 
value of each query, we can statistically interp-
ret to which extent users share consistent intent 
on the same query according to Table 3 (Lan-
dis and Koch, 1977). Though the interpretation 
in Table 3 is not accepted with no doubt, it can 
give us an intuition about what extent of 
agreement consistency is. In other words, 
Kappa is a measure with statistical sense. 
Meanwhile, Kappa values of queries with  
                                                 
2 There may be more than two users who submitted the 
same query. 
704
  
                                       (a). AOL                                                                  (b). Sogou 
 
Figure 1: Number of unique queries and query submissions as a function of Kappa value. 
 
? Interpretation 
< 0 No agreement 
0.0 ? 0.20 Slight agreement 
0.21 ? 0.40 Fair agreement 
0.41 ? 0.60 Moderate agreement 
0.61 ? 0.80 Substantial agreement 
0.81 ? 1.00 Almost perfect agreement 
Table 3:  Kappa Interpretation 
 
various sizes of click sets are also comparable. 
That is also the reason we choose Kappa to 
measure consistency. 
4.2 Distribution of Kappa 
As introduced in Section 2, common user in-
formation is supposed to be the repetition of 
users? behaviors. We consider that the amount 
of repetition of users? clicks on one query is 
quantified by the consistency of its clicks. To 
statistically present the scale of repetition in 
current query log, we try to give an overview 
of consistency level of two commercial query 
logs. 
Figure 1 plots distribution of Kappa value of 
the two logs in the coordinate with logarithmic 
Y-axis. About 34.5% unique queries (44.0% 
query submissions) in AOL log and only 
13.9% unique queries (15.2% query submis-
sions) in Sogou log have high Kappa values 
above 0.6. According to Table 3, click sets of 
these queries can be regarded as somewhat 
consistent. These queries can be roughly re-
solved by using common user information. On 
the other hand, for the rest of queries which 
constitute majority of the logs, users? click sets 
are rather diversified, which are hard to be sa-
tisfied by returning the same result list to them. 
As a whole, the queries in both AOL and So-
gou can be characterized as less consistently in 
the clicks according to Kappa value, which is a 
statistical support for exploiting individual user 
information. 
5 Individual or Common user infor-
mation: A Tendency View 
The above analyses quantitative analyses have 
shown that the repetition of search is not the 
statistically dominant factor, with the impres-
sion that employing individual user informa-
tion is equally, if not more, important than 
common user information. This section tries to 
further reveal this issue so as to balance the 
position of individual user information and 
common user information from a research 
point. 
Intuitively, a query can be characterized by 
the number of people issuing it, i.e. query fre-
quency if we remove the resubmissions of one 
query by the same people. We try to depict the 
above mentioned query submissions and Kap-
pa values as a function of number of people 
who issue the queries in Figure 2. In Figure 2, 
different numbers of users who issue the same 
query are shown on the x-axis, and the y-axis 
represents the number of different entities (left 
scale) and the average Kappa value (right scale) 
of the queries. We find that the number of que-
ries becomes very small when the number of 
users in a group grows over 10, so we set a 
variant step length for them: with the length 
step of the group size falling between 2 and 10 
set as 1, between 11 and 100 as 10, between 
101 and 1000 as 100 and above 1000 as 1000. 
705
   
 
                                     (a). AOL                                                                      (b). Sogou 
 
Figure 2: Average Kappa value of queries as a function of number of people in a group who issue 
the same query (line) and the number of submissions of the queries issued by the same size of 
group (dark columns). 
 
According to Figure 2(a), Kappa values of 
the queries in AOL log with more than 20 us-
ers are above 0.6, which indicates rather con-
sistent clicks for them, accounting for about  
29.4% of all query submissions. While for 
those queries visited by less than 20 users, the 
Kappa value declines gradually from 0.6 with 
the drop of users. For these queries occupying 
majority of query submissions, exploiting in-
dividual user information is supposed to be a 
better solution since the clicks on them are ra-
ther individualized. 
According to Figure 2(b), though Kappa 
values of queries increase similarly with 
people submitting them in AOL, the overall 
consistency of the queries in Sogou log is 
much lower: with a Kappa value below 0.6 
even for the queries visited by a large number 
of users. This fact indicates that Chinese users 
may be less consistent in their search intents, 
or partially reflects that the Chinese as a non-
inflection language has more ambiguity, which 
can also be implied from Table 2. Therefore, 
individual user information may be more ef-
fective than common user information in So-
gou log. 
Summarized from Figure 2, it is sensible 
that common user information is appropriate 
for the queries in the right-most of X-axis. 
With most number of visiting people, such 
queries bear rather consistent clicks though 
covering only a small proportion of the distinct 
query set. Moving from the right to the left, we 
can find the majority of queries yield a less 
Kappa value, for which the individualized 
clicks require individual user information to 
meet the needs of each user. In this sense, how 
to exploit individual user information is pre-
destined as the next issue of information re-
trieval if common user information was to be 
well utilized. 
6 Queries for Personalization 
Since using individual user information is a 
non-negligible issue in IR research, a subse-
quent issue is what queries can benefit in what 
extent from individual user information. In this 
section, we try to give an overview for this 
issue via a measure named potential for perso-
nalization. 
6.1 Potential for Personalization 
Potential for personalization proposed by Tee-
van et al (2007) is used to measure the norma-
lized Discounted Cumulative Gain (NDCG) 
improvement between the best ranking of the 
results to a group and individuals. NDCG is a 
well-known measure of the quality of a search 
result (J?rvelin and Kek?l?inen, 2000). 
The best ranking of the results to a group is 
the ranking with highest NDCG based on re-
levance judgments of the users in the group. 
For the queries with explicit judgments, the 
best ranking can be generated as follows: re-
sults that all raters thought were relevant are 
ranked first, followed by those that most 
people thought were relevant but a few people 
thought were irrelevant, until the  results most 
people though were irrelevant. In other word,  
706
  
 
(a)  AOL                                                                    (b)   Sogou 
 
Figure 3: Number of unique queries and query submissions as a function of potential for 
personalization 
 
   
 
(a)  AOL                                                                       (b) Sogou 
 
Figure 4: The average NDCG of group best ranking as a function of number of people in group 
(solid line), combining with the distribution of  the number of unique queries issued by the same 
size of group (dark columns) 
 
the best ranking always tries to put the results 
that have the highest collective gain first to get 
the highest NDCG. 
The previous work has shown that the im-
plicit click-based potential for personalization 
is strongly related to variation in explicit 
judgments (J. Teevan et al, 2008). In this pa-
per, we continue using click-based potential 
for personalization to measure the variation. 
Assuming the clicked results as relevant, we 
can calculate the potential for personalization 
of each query over the web search query log to 
present what kind of query can benefit more 
from personalization. 
6.2 Potential for Personalization Distribu-
tion over Query Logs 
Teevan et al (2007) have depicted a potential 
for personalization curve based on explicit 
judgment to characterize the benefit that could 
be obtained by personalizing search results for 
each user. We continue using potential for per-
sonalization based on click-through to roughly 
reveal what kind of query can benefit more 
from personalization. 
First we investigate the number of unique 
queries with different potential for personaliza-
tion, which is shown in Figure 3. We find that 
there are about 53.9% unique queries in AOL 
log and 32.4% unique queries in Sogou log, 
whose potential for personalization is 0. For 
these queries, current web search is able to re-
turn perfect results to all users. However, for 
the rest of queries, even the best group ranking 
of results can?t satisfy everyone who issues the 
query. So these queries should be better served 
by individual user information, covering 
707
46.1% unique queries in AOL and 67.6% in 
Sogou. 
Then, in order to further interpret what kind 
of query individual user information is needed 
most, we further relate potential for personali-
zation to the number of users who submit the 
queries over AOL and Sogou query log as 
shown in Figure 4. For clarity?s sake, we also 
set the same step length as in Figure 2. 
According to Figure 4, the curve of potential 
for personalization is approximately U-shaped 
in both AOL log and Sogou Log. As the num-
ber of users in one group increases, perfor-
mance of the best non-personalized rankings 
first declines, then flattens out and finally 
promotes3. Note that the left part of the curve 
is very similar to what Teevan et al (2007) 
showed in their work. 
Again in Figure 4, the queries which have 
the most potential for personalization are the 
ones which are issued by more than 6 and less 
than 20 users in AOL log. While in Sogou log, 
the queries issued by more than 6 and less than 
4000 users have the most potential for persona-
lization. Such different findings are probably 
caused by the content of query. There are 
many recommended queries in the homepage 
of Sogou search engine, most of which are in-
formational query and clicked by a large num-
ber of users. Even when the size of group who 
issue the same query becomes very big, the 
query still has a wide variation of users? beha-
viors. So the consistency level of queries in 
Sogou log is much lower than the queries in 
AOL log at the same size of group.  
7 Conclusion and Future Work 
In this paper, we try to justify the position of 
individual user information comparing with 
common user information. It is shown that ex-
ploiting individual user information is a non-
trivial issue challenging the IR community 
through the analysis of both English and Chi-
nese large scale search logs. 
We first classify the repetitive queries into 2 
categories according to whether the corres-
ponding clicks are unique among different us-
ers. We find that quantitatively the queries and 
                                                 
3 Note that the different step length dims the actual U-
shape in the figure. 
clicks deserving for individual user informa-
tion is much bigger than those deserving for 
common user information. 
After that we use Kappa statistic to present 
that the overall consistency of query clicks re-
coded in search logs is pretty low, which statis-
tically reveals that the repetition is not the do-
minant factor and individual user information 
is more desired to enhance most queries in cur-
rent query log. 
We also explore the distribution of Kappa 
values over different numbers of users in the 
group who issue the same query, concluding 
that how to utilize individual user information 
to improve the performance of web search en-
gine is the next research issue confronted by 
the IR community when the repeated search of 
users are properly exploited.  
Finally, potential for personalization is cal-
culated over the two query logs to present an 
overview of what kind of queries that the op-
timal group-based retrieval model fails, which 
is supposed to benefit most from individual 
user information. 
One possible enrichment to this work may 
come from the employment of content analysis 
based on text processing techniques. The dif-
ferent clicks, which are the basis of our exami-
nation, may have similar or even exact content 
in their web pages. Though the manual check 
for a small scale sampling from the Sogou log 
yields less than 1% probability for such case, 
the content based examination will be definite-
ly more convincing than simple click counts. 
In addition, the queries for the two types of 
user information are not examined for their 
contents or the related information needs. Con-
tent analysis or linguistic view to these queries 
would be more informative. Both of these is-
sues are to be addressed in our future work. 
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project 
(Grant No.2006AA010108). The authors are 
grateful for the anonymous reviewers for their 
valuable comments. 
 
 
708
References 
Canny John. 2002. Collaborative filtering with pri-
vacy via factor analysis. In Proceedings of SI-
GIR? 02, pages 45-57. 
Carroll M. John and Mary B. Rosson. 1987. Para-
dox of the active user. Interfacing thought: cog-
nitive aspect of human-computer interaction, 
pages 80-111. 
Chirita A. Paul, Wofgang Nejdl, Raluca Paiu, and 
Christian Kohlschutter. 2005. Using odp metada-
ta to personalize search. In Proceedings of SI-
GIR ?05, pages 178-185. 
Cohen Jacob. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20: 37-46 
Dou Zhicheng, Ruihua Song, and Ju-Rong Wen. 
2007. A Large-scale Evaluation and Analysis of 
Personalized Search Strategies. In Proceedings 
of WWW ?07, pages 581-590. 
Fleiss L. Joseph. 1971. Measuring nominal scale 
agreement among many raters. Psychological 
Bulletin, 76(5): 378-382. 
Herlocker L. Jonathan, Joseph A. Konstan, Al 
Borchers, and John Riedl. 1999. An algorithmic 
framework for performing collaborative filtering. 
In Proceedings of SIGIR ?99, pages 230-237. 
Jansen J. Bernard, Amanda Spink, and Tefko Sara-
cevic. 2000. Real life, real users, and real needs: 
a study and analysis of user queries on the web. 
Information Processing and Management, pages 
207-227. 
J?rvelin Kalervo and Jaana Kek?l?inen. 2000. IR 
evaluation methods for retrieving highly relevant 
documents. In Proceedings of SIGIR ?00, pages 
41-48. 
Kohrs Arnd and Bernard Merialdo. 1999. Cluster-
ing for collaborative filtering applications. In 
Proceedings of CIMCA ?99, pages 199-204. 
Landis J. Richard and Gary. G. Koch. 1977. The 
mea-surement of observer agreement for cate-
gorical data. Biometrics 33: 159-174. 
Pitkow James, Hinrich Schutze, Todd Cass, Rob 
Cooley, Don Turnbull, Andy Edmonds, Eytan 
Adar and Thomas Breuel. 2002. Personalized 
search. ACM, 45(9):50-55. 
Shen Xuehua, Bin Tan and ChengXiang Zhai. 2005 
Implicit user modeling for personalized search. 
In Proceedings of CIKM ?05, pages 824-831. 
 
Silverstein Craig, Monika Henzinger, Hannes Ma-
rais and Michael Moricz. 1999. Analysis of a 
very large web search engine query log. SIGIR 
Forum, 33(1):6-12. 
Smyth Barry. 2007. A Community-Based Approach 
to Personalizing Web Search. IEEE Computer, 
40(8): 42-50. 
Speretta Mirco and Susan Gauch. Personalized 
Search based on user search histories. 2005. In 
Proceedings of WI ?05, pages 622-628. 
Spink Amanda, Dietmar Wolfram, Major Jansen, 
Tefko Saracevic. 2001. Searching the web: The 
public and their queries. Journal of the American 
Society for Information Science and Technology, 
52(3), 226-234 
Sugiyama Kazunari, Kenji Hatano, and Masatoshi 
Yoshikawa. 2004. Adaptive web search based on 
user profile constructed without any effort from 
users. In Proceedings of WWW ?04, pages 675-
684. 
Sun Jian-Tao, Hua-Jun Zeng, Huan Liu, Yuchang 
Lu and Zheng Chen. 2005. CubeSVD: a novel 
approach to personalized web search. In Pro-
ceedings of WWW?05, pages 382-390. 
Teevan Jaime, Susan T. Dumais, and Eric Horvitz. 
2005. Personalizing search via automated analy-
sis of interests and activities. In Proceedings of 
SIGIR ?05, pages 449-456. 
Teevan Jaime, Susan T. Dumais and Eric Horvitz. 
2007. Characterizing the value of personalizing 
search. In Proceedings of SIGIR ?07, pages 757-
758. 
Teevan Jaime, Susan T. Dumais and Daniel J. 
Liebling. 2008. To personalize or Not to Perso-
nalize: Modeling Queries with Variation in User 
Intent. In Proceedings of SIGIR ?08, pages 163-
170. 
Townsend Steve Cronen and W. Bruce Croft. 2002. 
Quantifying query ambiguity. In Proceedings of 
HLT ?02, pages 613-622. 
Yu Kai, Anton Schwaighofer, Volker Tresp, Xiao-
wei Xu, Hans-Peter Kriegel. 2004. Probabilistic 
Memory-based Collaborative Filtering. In IEEE 
Transactions on Knowledge and Data Engineer-
ing, pages 56-59. 
 
709
Coling 2010: Poster Volume, pages 748?756,
Beijing, August 2010
Head-modifier Relation based Non-lexical Reordering Model 
for Phrase-Based Translation  
Shui Liu1, Sheng Li1, Tiejun Zhao1, Min Zhang2, Pengyuan Liu3 
1School of Computer Science and Technology, Habin Institute of Technology 
{liushui,lisheng,tjzhao}@mtlab.hit.edu.cn 
2Institute for Infocomm Research 
mzhang@i2r.a-star.edu.sg 
3Institute of Computational Linguistics, Peking University  
liupengyuan@pku.edu.cn 
 
Abstract 
Phrase-based statistical MT (SMT) is a 
milestone in MT. However, the transla-
tion model in the phrase based SMT is 
structure free which greatly limits its 
reordering capacity. To address this is-
sue, we propose a non-lexical head-
modifier based reordering model on 
word level by utilizing constituent based 
parse tree in source side. Our experi-
mental results on the NIST Chinese-
English benchmarking data show that, 
with a very small size model, our me-
thod significantly outperforms the base-
line by 1.48% bleu score. 
1 Introduction 
Syntax has been successfully applied to SMT to 
improve translation performance. Research in 
applying syntax information to SMT has been 
carried out in two aspects. On the one hand, the 
syntax knowledge is employed by directly inte-
grating the syntactic structure into the transla-
tion rules i.e. syntactic translation rules. On this 
perspective, the word order of the target transla-
tion is modeled by the syntax structure explicit-
ly.  Chiang (2005), Wu (1997) and Xiong (2006) 
learn the syntax rules using the formal gram-
mars. While more research is conducted to learn 
syntax rules with the help of linguistic analysis 
(Yamada and Knight, 2001; Graehl and Knight, 
2004). However, there are some challenges to 
these models. Firstly, the linguistic analysis is 
far from perfect. Most of these methods require 
an off-the-shelf parser to generate syntactic 
structure, which makes the translation results 
sensitive to the parsing errors to some extent. 
To tackle this problem, n-best parse trees and 
parsing forest (Mi and Huang, 2008; Zhang, 
2009) are proposed to relieve the error propaga-
tion brought by linguistic analysis. Secondly, 
some phrases which violate the boundary of 
linguistic analysis are also useful in these mod-
els ( DeNeefe et al, 2007; Cowan et al 2006). 
Thus, a tradeoff needs to be found between lin-
guistic sense and formal sense. 
On the other hand, instead of using syntactic 
translation rules, some previous work attempts 
to learn the syntax knowledge separately and 
then integrated those knowledge to the original 
constraint. Marton and Resnik (2008) utilize the 
language linguistic analysis that is derived from 
parse tree to constrain the translation in a soft 
way. By doing so, this approach addresses the 
challenges brought by linguistic analysis 
through the log-linear model in a soft way.  
Starting from the state-of-the-art phrase based 
model Moses ( Koehn e.t. al, 2007), we propose 
a head-modifier relation based reordering model 
and use the proposed model as  a soft syntax 
constraint in the phrase-based translation 
framework. Compared with most of previous 
soft constraint models, we study the way to util-
ize the constituent based parse tree structure by 
mapping the parse tree to sets of head-modifier 
for phrase reordering. In this way, we build a 
word level reordering model instead of phras-
al/constituent level model.  In our model, with 
the help of the alignment and the head-modifier 
dependency based relationship in the source 
side, the reordering type of each target word 
with alignment in source side is identified as 
one of pre-defined reordering types. With these 
reordering types, the reordering of phrase in 
translation is estimated on word level.   
 
748
 
Fig 1. An Constituent based Parse Tree 
 
 
2 Baseline  
Moses, a state-of-the-art phrase based SMT sys-
tem is used as our baseline system. In Moses, 
given the source language f and target language 
e, the decoder is to find: 
ebest = argmaxe p ( e | f ) pLM ( e ) ?
length(e)        (1)    
where p(e|f) can be computed using phrase 
translation model, distortion model and lexical 
reordering model. pLM(e) can be computed us-
ing the language model. ?length(e) is word penalty 
model.  
Among the above models, there are three 
reordering-related components: language model, 
lexical reordering model and distortion model. 
The language model can reorder the local target 
words within a fixed window in an implied way. 
The lexical reordering model and distortion 
reordering model tackle the reordering problem 
between adjacent phrase on lexical level and 
alignment level. Besides these reordering model, 
the decoder induces distortion pruning con-
straints to encourage the decoder translate the 
leftmost uncovered word in the source side 
firstly and to limit the reordering within a cer-
tain range. 
3 Model  
In this paper, we utilize the constituent parse 
tree of source language to enhance the  reorder- 
 
 
ing capacity of the translation model. Instead of 
directly employing the parse tree fragments 
(Bod, 1992; Johnson, 1998) in reordering rules 
(Huang and Knight, 2006; Liu 2006; Zhang and 
Jiang 2008), we make a mapping from trees to 
sets of head-modifier dependency relations 
(Collins 1996 ) which  can be obtained  from the 
constituent based parse tree with the help of 
head rules ( Bikel, 2004 ). 
3.1 Head-modifier Relation  
According to Klein and Manning (2003) and 
Collins (1999), there are two shortcomings in n-
ary Treebank grammar.  Firstly, the grammar is 
too coarse for parsing. The rules in different 
context always have different distributions. Se-
condly, the rules learned from training corpus 
cannot cover the rules in testing set. 
Currently, the state-of-the-art parsing algo-
rithms (Klein and Manning, 2003; Collins 1999) 
decompose the n-ary Treebank grammar into 
sets of head-modifier relationships. The parsing 
rules in these algorithms are constructed in the 
form of finer-grained binary head-modifier de-
pendency relationships. Fig.2 presents an exam-
ple of head-modifier based dependency tree 
mapped from the constituent parse tree in Fig.1.  
 
749
 
Fig. 2. Head-modifier Relationships with Aligned Translation 
 
Moreover, there are several reasons for which 
we adopt the head-modifier structured tree as 
the main frame of our reordering model. Firstly, 
the dependency relationships can reflect some 
underlying binary long distance dependency 
relations in the source side. Thus, binary depen-
dency structure will suffer less from the long 
distance reordering constraint. Secondly, in 
head-modifier relation, we not only can utilize 
the context of dependency relation in reordering 
model, but also can utilize some well-known 
and proved helpful context (Johnson, 1998) of 
constituent base parse tree in reordering model. 
Finally, head-modifier relationship is mature 
and widely adopted method in full parsing.   
3.2 Head-modifier Relation Based Reor-
dering Model  
Before elaborating the model, we define some 
notions further easy understanding. S=<f1, f 
2?fn> is the source sentence; T=<e1,e2,?,em> is 
the target sentence; AS={as(i) | 1? as(i) ? n } 
where as(i) represents that the ith word in source 
sentence  aligned to the as(i)th word in target 
sentence; AT={aT(i) | 1? aT (i) ? n } where aT(i) 
represents that the ith word in target sentence  
aligned to the aT(i)th word in source sentence; 
D= {( d(i), r(i) )| 0? d(i) ?n} is the head-
modifier relation set of  the words in S where 
d(i) represents that the ith word in source sen-
tence is the modifier of d(i)th  word in source 
sentence under relationship r(i); O= < o1, o2,?, 
om > is the sequence of the reordering type of 
every word in target language. The reordering 
model probability is P(O| S, T, D, A).  
Relationship: in this paper, we not only use the 
label of the constituent label as Collins (1996), 
but also use some well-known context in pars-
ing to define the head-modifier relationship r(.), 
including the POS of the modifier m,  the POS 
of the head h, the dependency direction d, the 
parent label of the dependency label l, the 
grandfather label of the dependency relation p, 
the POS of adjacent siblings of the modifier s. 
Thus, the head-modifier relationship can be 
represented as a 6-tuple <m, h, d, l, p, s>. 
 
r(.) relationship 
r(1) <VV, - , -, -, -, - > 
r(2) <NN, NN, right, NP, IP, - > 
r(3) <NN,VV, right, IP, CP, - > 
r(4) <VV, DEC, right, CP, NP, - > 
r(5) <NN,VV, left, VP, CP, - > 
r(6) <DEC, NP, right, NP, VP, - > 
r(7) <NN, VV, left, VP,  TOP, - > 
Table 1. Relations Extracted from Fig 2.  
 
In Table 1, there are 7 relationships extracted 
from the source head-modifier based dependen-
cy tree as shown in Fig.2. Please notice that, in 
this paper, each source word has a correspond-
ing relation.  
Reordering type: there are 4 reordering types 
for target words with linked word in the source 
side in our model: R= {rm1, rm2, rm3 , rm4}. The 
reordering type of target word as(i) is defined  as 
follows: 
? rm1: if the position number of the ith 
word?s head is less than i ( d(i) < i ) in 
source language, while the position num-
ber of the word aligned to i is less than 
750
as(d(i)) (as(i)  < as(d(i)) ) in target lan-
guage;  
? rm2: if the position number of the ith 
word?s head is less than i ( d(i) < i ) in 
source language, while the position num-
ber of the word aligned to i is larger than 
as(d(i)) (as(i) > as(d(i)) ) in target lan-
guage. 
? rm3: if the position number of the ith 
word?s head is larger than i ( d(i) > i ) in 
source language, while the position num-
ber of the word aligned to i is larger than 
as(d(i)) (as(i) > as(d(i))) in target language. 
? rm4: if the position number of the ith 
word?s head is larger than i ( d(i) > i) in 
source language, while the position num-
ber of the word aligned to i is less than 
as(d(i)) (as(i) < as(d(i)) ) in target lan-
guage. 
 
 
Fig. 3.  An example of the reordering types in 
Fig. 2. 
Fig. 3 shows examples of all the reordering 
types. In Fig. 3, the reordering type is labeled at 
the target word aligned to the modifier: for ex-
ample, the reordering type of rm1 belongs to the 
target word ?scale?. Please note that, in general, 
these four types of reordering can be divided 
into 2 categories: the target words order of rm2 
and rm4 is identical with source word order, 
while rm1 and rm3 is the swapped order of 
source. In practice, there are some special cases 
that can?t be classified into any of the defined 
reordering types: the head and modifier in 
source link to the same word in target. In such 
cases, rather than define new reordering types, 
we classify these special cases into these four 
defined reordering types: if the head is right to 
the modifier in source, we classify the reorder-
ing type into rm2; otherwise, we classify the 
reordering type into rm4. 
Probability estimation: we adopt maximum 
likelihood (ML) based estimation in this paper. 
In ML estimation, in order to avoid the data 
sparse problem brought by lexicalization, we 
discard the lexical information in source and 
target language: 
?
?
?
m
1i
Ti (i)))r(a-,-, |P(o
A) D, T, S, |P(O
                                  (2) 
where oi?{rm1,rm2,rm3,rm4} is the reorder-
ing type of ith word in  target language. 
To get a non-zero probability, additive smooth 
ing( Chen and Goodman, 1998) is used: 
?
?
?
?
??
?
?
??
?
?
?
?
?
?
||),,,,,(
),,,,,,(
||)))(((
)))((,(
) )))(((-,-,|P(o
)()()()()()(
)()()()()()(
i
OspldhmF
spldhmoF
OiarF
iaroF
iarF
iaiaiaiaiaia
Ro
iaiaiaiaiaiai
t
Ro
Ti
t
TTTTTT
i
TTTTTT
i
                                                                                 
(3) 
where F(. ) is the frequency of the statistic event 
in training corpus. For a given set of dependen-
cy relationships mapping from constituent tree, 
the reordering type of ith word is confined to 
two types: it is whether one of rm1 and rm2 or 
rm3 and rm4. Therefore, |O|=2 instead of |O|=4 
in (2). The parameter ? is an additive factor to 
prevent zero probability. It is computed as:   
                                        
),,,,,(
1
)()()()()()( iaiaiaiaiaia
Ro
TTTTTT
i
spldhmFC ?
?
???
       
(4) 
where c is a constant parameter(c=5 in this pa-
per). 
   In above, the additive parameter ? is an adap-
tive parameter decreasing with the size of the 
statistic space. By doing this, the data sparse 
problem can be relieved. 
4 Apply the Model to Decoder 
Our decoding algorithm is exactly the same as 
(Kohn, 2004). In the translation procedure, the 
decoder keeps on extending new phrases with-
out overlapping, until all source words are trans-
lated. In the procedure, the order of the target 
751
words in decoding procedure is fixed.  That is, 
once a hypothesis is generated, the order of tar-
get words cannot be changed in the future. Tak-
ing advantage of this feature, instead of compu-
ting a totally new reordering score for a newly 
generated hypothesis, we merely calculate the 
reordering score of newly extended part of the 
hypothesis in decoding. Thus, in decoding, to 
compute the reordering score, the reordering 
types of each target word in the newly extended 
phrase need to be identified.  
The method to identify the reordering types 
in decoding is proposed in Fig.4. According to 
the definition of reordering, the reordering type 
of the target word is identified by the direction 
of head-modifier dependency on the source side, 
the alignment between the source side and tar-
get side, and the relative translated order of 
word pair under the head-modifier relationship. 
The direction of dependency and the alignment 
can be obtained in input sentence and phrase 
table. While the relative translation order needs 
to record during decoding. A word index is em-
ployed to record the order. The index is con-
structed in the form of true/false array: the index 
of the source word is set with true when the 
word has been translated. With the help of this 
index, reordering type of every word in the 
phrase can be identified. 
 
1: Input: alignment array AT; the Start is the 
start position of the phrase in the source side; 
head-modifier relation d(.); source word in-
dex C, where C[i]=true  indicates that the 
ith word in source has been translated.   
2: Output: reordering type array O which re-
serves the reordering types of each word in 
the target phrase 
3: for i = 1, |AT| do 
4:    P  ? aT(i) + Start 
5:    if (d (P)<P) then 
6:      if C [d(p)] = false then 
7:         O[i] ? rm1 
8:      else 
9:         O[i] ? rm2 
10:        end if 
11:  else   
12:     if  C[d(p)] = true then 
13:        O[i] ? rm3 
14:       else 
15:          O[i] ? rm4 
16:       end if 
17:    end if 
18: C[p] ?true //update word index 
19: end for 
Fig. 4.  Identify the Reordering Types of  Newly 
Extended Phrase 
After all the reordering types in the newly ex-
tended phrase are identified, the reordering 
scores of the phrase can be computed by using 
equation (3). 
5 Preprocess the Alignment 
In Fig. 4, the word index is to identify the reor-
dering type of the target translated words. Ac-
tually, in order to use the word index without 
ambiguity, the alignment in the proposed algo-
rithm needs to satisfy some constraints.  
Firstly, every word in the source must have 
alignment word in the target side. Because, in 
the decoding procedure, if the head word is not 
covered by the word index, the algorithm cannot 
distinguish between the head word will not be 
translated in the future and the head word is not 
translated yet. Furthermore, in decoding, as 
shown in Fig.4, the index of source would be set 
with true only when there is word in target 
linked to it. Thus, the index of the source word 
without alignment in target is never set with true.  
 
Fig. 5.  A complicated Example of Alignment in 
Head-modifier based Reordering Model 
Secondly, if the head word has more than one 
alignment words in target, different alignment 
possibly result in different reordering type. For 
example, in Fig. 5, the reordering type of e2 is 
different when f2 select to link word e1 and e3   
in the source side.  
To solve this problem, we modify the align-
ment to satisfy following conditions: a) each 
word in source just has only one alignment 
word in target, and b) each word in target has at 
most one word aligned in source as its anchor 
word which decides the reordering type of the 
target word.  
To make the alignment satisfy above con-
straints, we modify the alignment in corpus. In 
752
order to explain the alignment preprocessing, 
the following notions are defined: if there is a 
link between the source word f j  and target word 
ei, let  l(ei ,fj) = 1 , otherwise l(ei ,fj) = 0; the 
source word fj?F1-to-N , iff  ?i l(ei,fj) >1, such 
as the source word f2 in Fig. 5; the source word 
fj?FNULL, iff ?i l(ei,fj) = 0, such as the source 
word f4 in Fig. 5; the target word ei?E1-to-N  , iff 
?j l(ei,fj) > 1, such as the target word e1 in Fig. 
5.  
In preprocessing, there are 3 types of opera-
tion, including DiscardLink(fj) , BorrowLink( f j )  
and FindAnchor(ei ) : 
DiscardLink( fj ) : if the word fj in source with 
more than one words aligned in target, i.e.  fj?
F1-to-N ; We set the target word en with l(en, fj) = 
1, where en= argmaxi p(ei | fj) and   p(ei | fj) is 
estimated by ( Koehn e.t. al, 2003), while set  
rest of words linked to fj with l (en, fj) = 0.    
BorrowLink( fj ): if the word fj in source with-
out a alignment word in target, i.e.  fj?FNULL ; 
let l(ei,fj)=1 where ei  aligned to the word fj , 
which is the nearest word to  fj  in the source 
side; when there are two words nearest to fj with 
alignment words in the target side at the same 
time, we select the alignment of  the left word 
firstly .  
FindAnchor( ): for the word ei  in target with 
more than one words aligned in source , i.e.  ei
?E1-to-N ; we select the word  fm  aligned to ei as 
its anchor word to decide the reordering type of 
ei  ,  where fm= argmaxj p(ei | fj) and  p(fj | ei) is 
estimated by ( Koehn et al 2003); For the rest 
of words aligned to  ei , we would set their word 
indexes with true in the update procedure of 
decoding  in the 18th line of Fig.4.    
With these operations, the required alignment 
can be obtained by preprocessing the origin 
alignment as shown in Fig. 6. 
1: Input: set of alignment A between target lan-
guage e and source language f  
2: Output: the 1-to-1 alignment required by the 
model 
3:  foreach fi?F1-to-N do 
4:    DiscardLink( fi ) 
5:  end for 
6:  foreach fi  ?FNULL  do 
7:    BorrowLink( fi ) 
8:  end for 
9:  foreach  ei?E1-to-N do  
10:   FindAnchor(ei ) 
11:endfor           
Fig. 6. Alignment Pre-Processing algorithm 
 
 
 
Fig. 7. An Example of Alignment Preprocessing. 
   An example of  the preprocess the alignment 
in Fig. 5 is shown in Fig. 7 : firstly, Discar-
dLink(f2) operation discards the link between f2 
and e1  in (a); then the link between f4 and e3 is 
established by operation BorrowLink(f4 )  in (b); 
at last, FindAnchor(e3) select f2 as the anchor 
word of e3  in source in (c). After the prepro-
cessing, the reordering type of e3   can be identi-
fied. Furthermore, in decoding, when the de-
coder scans over e2, the word index sets the 
word index of f3 and f4 with true. In this way, 
the never-true word indexes in decoding are 
avoided.  
6 Training the Reordering Model 
Before training, we get the required alignment 
by alignment preprocessing as indicated above. 
Then we train the reordering model with this 
alignment: from the first word to the last word 
in the target side, the reordering type of each 
word is identified. In this procedure, we skip the 
words without alignment in source. Finally, all 
the statistic events required in equation (3) are 
added to the model.   
In our model, there are 20,338 kinds of rela-
tions with reordering probabilities which are 
much smaller than most phrase level reordering 
models on the training corpus FBIS.   
Table 1 is the distribution of different reor-
dering types in training model.  
753
Type of Reordering   Percentage   %    
           
rm1 
rm2 
rm3 
3.69 
27.61 
20.94 
rm4 47.75 
Table 1: Percentage of different reordering 
types in model 
From Table 1, we can conclude that the reor-
dering type rm2 and rm4 are preferable in reor-
dering which take over nearly 3/4 of total num-
ber of reordering type and are identical with 
word order of the source. The statistic data indi-
cate that most of the words order doesn?t change 
in our head-modifier reordering view.  This 
maybe can explain why the models (Wu, 1997; 
Xiong, 2006; Koehn, et., 2003) with limited 
capacity of reordering can reach certain perfor-
mance. 
7 Experiment and Discussion  
7.1 Experiment Settings 
We perform Chinese-to-English translation task 
on NIST MT-05 test set, and use NIST MT-02 
as our tuning set. FBIS corpus is selected as our 
training corpus, which contains 7.06M Chinese 
words and 9.15M English words. We use GI-
ZA++(Och and Ney, 2000) to make the corpus 
aligned. A 4-gram language model is trained 
using Xinhua portion of the English Gigaword 
corpus (181M words). All models are tuned on 
BLEU, and evaluated on both BLEU and NIST 
score. 
To map from the constituent trees to sets of 
head-modifier relationships, firstly we use the 
Stanford parser (Klein, 2003) to parse the 
source of corpus FBIS, then we use the head-
finding rules in (Bikel, 2004) to get the head-
modifier dependency sets. 
In our system, there are 7 groups of features. 
They are: 
1. Language model score (1 feature) 
2. word penalty score (1 feature) 
3. phrase model scores (5 features) 
4. distortion score (1 feature) 
5. lexical RM scores (6 features) 
6. Number of each reordering type (4 fea-
tures) 
7. Scores of each reordering type (4 fea-
tures, computed by equation (3)) 
In these feature groups, the top 5 groups of 
features are the baseline model, the left two 
group scores are related with our model.  
In decoding, we drop all the OOV words and 
use default setting in Moses: set the distortion 
limitation with 6, beam-width with 1/100000, 
stack size with 200 and max number of phrases 
for each span with 50.  
7.2 Results and Discussion 
We take the replicated Moses system as our 
baseline. Table 2 shows the results of our model.  
In the table, Baseline model is the model includ-
ing feature group 1, 2, 3 and 4. Baselinerm mod-
el is the Baseline model with feature group 5. H-
M model is the Baseline model with feature 
group 6 and 7. H-Mrm model is the Baselinerm 
model with feature group 6 and 7.  
Model BLEU% NIST 
Baseline 27.06 7.7898 
Baselinerm  27.58     7.8477 
H-M  28.47     8.1491 
H-Mrm 29.06 8.0875 
Table 2: Performance of  the Systems on NIST-
05(bleu4 case-insensitive). 
From table 2, we can conclude that our reor-
dering model is very effective. After adding 
feature group 6 and 7, the performance is im-
proved by 1.41% and 1.48% in bleu score sepa-
rately. Our reordering model is more effective 
than the lexical reordering model in Moses:  
1.41% in bleu score is improved by adding our 
reordering model to Baseline model, while 0.48 
is improved by adding the lexical reordering to 
Baseline model.   
threshold KOR BLEU NIST 
?1 20,338  29.06  8.0875 
?2      13,447 28.83   8.3658 
?3      10,885 28.64 8.0350 
?4        9,518 28.94 8.1002 
?5        8,577       29.18   8.1213 
Table 3: Performance on NIST-05 with Differ-
ent Relation Frequency Threshold (bleu4 case-
insensitive). 
Although our model is lexical free, the data 
sparse problem affects the performance of the 
model. In the reordering model, nearly half 
numbers of the relations in our model occur less 
than three times. To investigate this, we statistic 
754
the frequency of the relationships in our model, 
and expertise our H-M full model with different 
frequency threshold.  
In Table 3, when the frequency of relation is 
not less than the threshold, the relation is added 
into the reordering model; KOR is the number 
of relation type in the reordering model.  
Table 3 shows that, in our model, many rela-
tions occur only once. However, these low-
frequency relations can improve the perfor-
mance of the model according to the experimen-
tal results. Although low frequency statistic 
events always do harm to the parameter estima-
tion in ML, the model can estimate more events 
in the test corpus with the help of low frequency 
event. These two factors affect the experiment 
results on opposite directions: we consider that 
is the reason the result don?t increase or de-
crease with the increasing of frequency thre-
shold in the model. According to the results, the 
model without frequency threshold achieves the 
highest bleu score. Then, the performance drops 
quickly, when the frequency threshold is set 
with 2. It is because there are many events can?t 
be estimated by the smaller model. Although, in 
the model without frequency threshold, there 
are some probabilities overestimated by these 
events which occur only once, the size of the 
model affects the performance to a larger extent. 
When the frequency threshold increases above 3, 
the size of model reduces slowly which makes 
the overestimating problem become the impor-
tant factor affecting performance. From these 
results, we can see the potential ability of our 
model: if our model suffer less from data spars 
problem, the performance should be further im-
proved, which is to be verified in the future.   
8 Related Work and Motivation 
There are several researches on adding linguis-
tic analysis to MT in a ?soft constraint? way. 
Most of them are based on constituents in parse 
tree. Chiang(2005), Marton and Resnik(2008) 
explored the constituent match/violation in hie-
ro; Xiong (2009 a) added constituent parse tree  
based linguistic analysis into BTG model; 
Xiong (2009 b) added source dependency struc-
ture to BTG; Zhang(2009) added tree-kernel to 
BTG model.  All these studies show promising 
results. Making soft constrain is an easy and 
efficient way in adding linguistic analysis into 
formal sense SMT model.   
In modeling the reordering, most of previous 
studies are on phrase level. In Moses, the lexical 
reordering is modeled on adjacent phrases. In 
(Wu, 1996; Xiong, 2006), the reordering is also 
modeled on adjacent translated phrases. In hiero, 
the reordering is modeled on the segments of 
the unmotivated translation rules. The tree-to-
string models (Yamada et al 2001; Liu et 
al.2006) are model on phrases with syntax re-
presentations. All these studies show excellent 
performance, while there are few studies on 
word level model in recent years. It is because, 
we consider, the alignment in word level model 
is complex which limits the reordering capacity 
of word level models.  
However, our work exploits a new direction 
in reordering that, by utilizing the decomposed 
dependency relations mapped from parse tree as 
a soft constraint, we proposed a novel head-
modifier relation based word level reordering 
model. The word level reordering model is 
based on a phrase based SMT framework. Thus, 
the task to find the proper position of translated 
words converts to score the reordering of the 
translated words, which relax the tension be-
tween complex alignment and word level reor-
dering in MT.  
9 Conclusion and Future Work 
Experimental results show our head-modifier 
relationship base model is effective to the base-
line (enhance by 1.48% bleu score), even with 
limited size of model and simple parameter es-
timation. In the future, we will try more compli-
cated smooth methods or use maximum entropy 
based reordering model. We will study the per-
formance with larger distortion constraint, such 
as the performances of   the distortion constraint 
over 15, or even the performance without distor-
tion model.  
10 Acknowledgement 
The work of this paper is funded by National 
Natural Science Foundation of China (grant no. 
60736014), National High Technology Re-
search and Development Program of China (863 
Program) (grant no. 2006AA010108), and Mi-
crosoft Research Asia IFP (grant no. FY09-
RES-THEME-158). 
755
References  
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpo-
ra. Computational Lingustics,23(3):377-403. 
David Chiang. 2005. A hierarchical phrase-based 
model for SMT. ACL-05.263-270. 
David Chiang. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2):201-
228. 
Kenji Yamada and K. Knight. 2001. A syntax-based 
statistical translation model. ACL-01.523-530. 
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic Constraints for Hierarchical Phrased-based 
Translation. ACL-08. 1003-1011. 
Libin  shen, Jinxi Xu and Ralph Weischedel. 2008. A 
New String-to-Dependency Machine Translation 
Algorithm with a Target Dependency Language 
Model. ACL-08. 577-585. 
J. Graehl and K. Knight.2004.Train ing Tree trans-
ducers. In proceedings of the 2004 Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association for Computation-
al Linguistics. 
Dekai Wu. 1996. A Polynomial-Time Algorithm for 
Statistical Machine Translation. In proceedings of 
ACL-1996 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Ma x-
imum Entropy Based Phrase Reordering Model 
for Statistical Machine Translation. In  proceed-
ings of COLING-ACL 2006 
Deyi Xiong, Min Zhang, Aiti AW and Haizhou Li.  
2009a. A Syntax-Driven Bracket Model for 
Phrase-Based Translation. ACL-09.315-323. 
Deyi Xiong, Min Zhang, Aiti AW and Haizhou Li. 
2009b. A Source Dependency Model for Statistic 
Machine translation. MT-Summit 2009. 
Och, F.J. and Ney, H. 2000. Improved statistical 
alignment models. In Proceedings of ACL 38. 
Philipp Koehn, et al Moses: Open Source Toolkit 
for Statistical Machine Translation, ACL 2007. 
Philipp Koehn, Franz Joseph Och, and Daniel Mar-
cu.2003. Statistical Phrase-based Translation. In 
Proceedings of HLT-NAACL. 
Philipp Koehn. 2004. A Beam Search Decoder for 
Phrase-Based Translation model. In : Proceeding 
of AMTA-2004,Washington 
Rens Bod. 1992. Data oriented Parsing( DOP ). In  
Proceedings of COLING-92. 
Mark Johnson. 1998. PCFG models of linguistic tree 
representations. Computational Linguistics, 
24:613-632. 
Liang Huang, Kevin Knight, and Aravind Joshi. Sta-
tistical Syntax-Directed Translation with Ex-
tended Domain of Locality. 2006. In Proceedings 
of the 7th AMTA. 
Yang Liu, Qun Liu, and Shouxun Lin. Tree-to-String 
Alignment Template for Statistical Machine 
Translation. 2006.In Proceedings of the ACL 2006.  
Min Zhang, Hongfei Jiang, Ai Ti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation 
Model. ACL-HLT-08. 559-567. 
Dan Klein, Christopher D. Manning. Accurate Un-
lexicalized Parsing. 2003. In Proceedings of  
ACL-03. 423-430. 
M. Collins. 1996. A new statistical parser based on 
bigram lexical dependencies. In Proceedings of  
ACL-96. 184-191. 
M. Collins. 1999. Head-Driven Statistical Models for 
Natural Language Parsing. Ph.D. thesis, Univ. of 
Pennsylvania. 
Andreas Zollmann. 2005. A Consistent and Efficient 
Estimator for the Data-Oriented Parsing Model. 
Journal of Automata, Languages  and Combinator-
ics. 2005(10):367-388 
Mark Johnson. 2002. The DOP estimation method is 
biased and inconsistent. Computational Linguis-
tics 28, 71-76. 
Daniel M. Bikel. 2004. On the Parameter Space of  
Generative Lexicalized Statistical Parsing Models. 
Ph.D. thesis. Univ. of Pennsylvania. 
S. F. Chen, J. Goodman. An Empirical Study of 
Smoothing Techniques for Language Modeling. 
In Proceedings of the 34th annual meeting on As-
sociation for Computational Linguistics, 
1996.310-318. 
Haitao Mi and Liang Huang.  2008. Forest-based 
translation Rule Extract ion. ENMLP-08. 2006-214. 
Hui Zhang, Min Zhang , Haizhou Li, A iti Aw and 
Chew Lim Tan. Forest-based Tree Sequence to 
String Translation Model. ACL-09: 172-180 
S DeNeefe, K. Knight, W. Wang, and D. Marcu. 
2007. What can syntax-based MT learn from 
phrase-based MT ? In Proc. EMNLP-CoNULL. 
Brooke Cowan, Ivona Kucerova, and Michael 
Collins.2006. A discriminative model for tree-to-
tree translation. In Proc. EMNLP. 
756
Coling 2010: Poster Volume, pages 1203?1210,
Beijing, August 2010
Utilizing Variability of Time and Term Content, within and across 
Users in Session Detection 
Shuqi Sun1, Sheng Li1, Muyun Yang1, Haoliang Qi2, Tiejun Zhao1 
1Harbin Institute of Technology, 2Heilongjiang Institute of Technology 
{sqsun, ymy, tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
haoliang.qi@gmail.com 
Abstract 
In this paper, we describe a SVM classi-
fication framework of session detection 
task on both Chinese and English query 
logs. With eight features on the aspects 
of temporal and content information ex-
tracted from pairs of successive queries, 
the classification models achieve signifi-
cantly superior performance than the stat-
of-the-art method. Additionally, we find 
through ROC analysis that there exists 
great discrimination power variability 
among different features and within the 
same feature across different users. To 
fully utilize this variability, we build lo-
cal models for individual users and com-
bine their predictions with those from the 
global model. Experiments show that the 
local models do make significant im-
provements to the global model, although 
the amount is small. 
1 Introduction 
To provide users better experiences of search 
engines, inspecting users? activities and inferring 
users? interests are indispensible. Query logs rec-
orded by search engines serves well for these 
purposes. Query log conveys the user interest 
information in the form of slices of the query 
stream. Thus the task of session detection con-
sists in distinguishing slice that corresponds to a 
user interest from other ones, and thus this paper, 
we adopt the definition of a session following 
(Jansen et al, 2007): 
(A session is) a series of interactions by the us-
er toward addressing a single information need. 
This definition is equivalent to that of the 
?search goal? proposed by Jones and Klinkner 
(2008), which corresponds to an atomic infor-
mation need, resulting in one or more queries.  
This paper adopts a classification point of 
view to the task of session detection (Jones and 
Klinkner, 2008). Given a pair of successive que-
ries in a query log, we examine it in various 
viewpoints (i.e. features) such as time proximity 
and similarity of the content of the two queries to 
determine whether these two queries cross a bor-
der of a search session. In other words, we classi-
fy the gap between the two queries into two clas-
ses: session shift and session continuation. In 
practice, search goals in a search mission and 
different search missions could be intermingled, 
and increase the difficulty of correctly identify-
ing them. In this paper, we do not take this issue 
into account and simply treat all boundaries be-
tween intermingled search goals as session shifts. 
The chief advantage in this choice is that we will 
have the opportunity to make classification mod-
el working online without caching user?s queries 
that are pending to be assigned to a session. 
Various studies built accurate models in pre-
dicting session boundaries and in distinguishing 
intermingled sessions, and they are summarized 
in Section 2. However, none of these works ana-
lyzed the contribution of individual features from 
a user-oriented viewpoint, or evaluated a fea-
ture?s discrimination power in a general scenario 
independent of its usage, as this paper does by 
conducting ROC analyses. During these analyses, 
we found that the discrimination power of fea-
tures varies dramatically, and for different users, 
the discrimination power of a particular feature 
also does not remain constant.  
Thus, it is appealing to build local models for 
users with have sufficient size of training exam-
ples, and combine the local models? predictions 
with those made by the global model trained by 
the whole training data. However, few of previ-
1203
ous works build user-specific models for the sake 
of characterizing the variability in user?s search 
activities, except that of Murray et al (2006). To 
fully make use of these two aspects of variability, 
inspired by Murray et al, we build users? local 
models based on a much broader range of evi-
dences, and show that different local models vary 
to a great extent, and experiments show that the 
local models do make significant improvements 
to the global model, although the amount is small. 
The remainder of this paper is organized as 
follows: Section 2 summarizes the related work 
of the session detection task. In Section 3, we 
first describe our classification framework as 
well as the features utilized. Then we conduct 
various evaluations on both English and Chinese 
query logs. Section 4 introduces the approaches 
to building local models based on an analysis of 
the variability of the discrimination power of 
features, and combine predictions of local mod-
els with those of the global model. Section 5 dis-
cusses the experimental results and concludes 
this paper. 
2 Related Work 
The simplest method in session detection is 
defining a timeout threshold and marking any 
time gaps of successive queries that exceed the 
threshold as session shifts. The thresholds 
adopted in different studies were significantly 
different, ranging from 5 minutes to 30 minutes 
(Silverstein et al, 1999; He and G?ker, 2000; 
Radlinski and Joachims, 2005; Downey et al, 
2007). Other study suggested adopting a dynamic 
timeout threshold. Murray et al (2006) proposed 
a user-centered hierarchical agglomerative 
clustering algorithm to determine timeout 
threshold for each user dynamically, other than 
setting a fixed threshold. However, Jones and 
Klinkner (2008) pointed out that single timeout 
criterion is always of limited utility, whatever its 
length is, and incorporating timeout features with 
other various features achieved satisfactory 
classification accuracy.  
An effective approach to combining the time 
out features with various evidences for session 
detection is machine learning. He et al (2002) 
collected statistical information from human an-
notated query logs to predict the probability a 
?New? pattern indicates a session shift according 
to the time gap between successive queries. 
?zmutlu and colleagues re-examined He et al?s 
work, and explored other machine learning tech-
niques such as neural networks, multiple linear 
regression, Monte Carlo simulation, conditional 
probabilities (Gayo-Avello, 2009), and HMMs 
(?zmutlu, 2009). 
In recent studies, Jones and Klinkner (2008) 
built logistic regression models to identify search 
goals and missions, and tackled the intermingled 
search goal/mission issue by examining arbitrary 
pairs of queries in the query log. Another contri-
bution of Jones and Klinkner is that they made a 
thorough analysis of contributions of individual 
features. However, they explored the features? 
contributions from a feature selection point of 
view rather than from a user-oriented one, and 
thus failed to characterize the variability of the 
discrimination power of the features when ap-
plied to different users. 
3 Learning to Detect Session Shifts 
3.1 Feature Extraction 
We adopt eight features covering both the tem-
poral and the content aspect of pairs of succes-
sive queries. Most these features are commonly 
used by previous studies (He and G?ker, 2000; 
?zmutlu, 2006; Jones and Klinkner, 2008). 
However, in this paper, we will analyze their 
contributions to the resulted model in a quite dif-
ferent way from that in previous works. 
Let Q = (q1, q2, ? , qn) denote a query log.  
The features are extracted from every successive 
pair of queries (qi, qi+1). Table 1 summarizes the 
features we adopt. The normalization described 
in Table1 is done according to the type of the 
feature. Features describing characters are nor-
malized by the average length of the two queries, 
while those describing character-n-grams are 
normalized by the average size of the n-gram sets 
of the two queries. Character-n-grams (e.g. bi-
grams ?ca? and ?at? in ?cat?) are robust to dif-
ferent representations of the same topic (e.g. ?IR? 
as Information Retrieval) and typos (e.g. 
?speling? as ?spelling?), and serve as a simple 
stemming method. In practice, character-n-grams 
are accumulative, which means they consist of 
all m-grams with m ? n. 
The feature ?avg_ngram_distance?, a variant 
of the ?lexical distance? in (Gayo-Avello, 2009), 
is more complicated than to be described briefly. 
1204
Here we first define n-gram distance (ND) from 
qi to qj, which is formalized as follows: 
j
ji
ji n
n
ND
qin   gram--char. of #
qin occur   qin   gram--char. of #
1)qq( ?=?  
Note that character-n-grams are accumulative 
and there could be multiple occurrences of a 
character-n-gram in a query, so the number of a 
character-n-gram is the sum of that of all m-
grams with m ? n, and multiple occurrences are 
all considered. At last, the average of character-
n-gram distance (ACD) of the pair (qi, qi+1) is:  
2
)qq()qq(
)q,q( 111
iiii
ii
NDND
ACD
?+?
=
++
+
 
There are seven features describing the content 
aspect of a query pair, and they are more or less 
overlapped (e.g. edit_distance vs. common_char). 
However, we show in the next subsection that all 
these features are beneficial to the final perfor-
mance.  
Feature Description 
time_interval time interval between 
successive queries 
avg_ngram_ 
distance 
avg. of character-n-gram 
distances 
edit_disance normalized Levenshtein 
edit distance 
common_prefix normalized length of pre-
fix shared 
common_suffix normalized length of suf-
fix shared 
common_char normalized number of 
characters shared 
common_ngram normalized number of 
character-n-grams shared 
Jaccard_ngram Jaccard distance between 
character-n-gram sets 
Table 1. Features used in classification models 
3.2 Data Preparation 
The query logs we explored include an English 
search log tracked by AOL from Mar 1, 2006 to 
May, 31 2006 (Pass et al, 2006), and a Chinese 
search log tracked by Sogou.com, which is one 
of the major Chinese Search Engines, from Mar 
1, 2007 to Mar 31, 20071. We applied systematic 
sampling over the user space on the two logs, 
which yielded 223 users and 2809 users, corre-
sponding to 6407 and 6917 query instances re-
                                                 
1 http://www.sogou.com/labs/resources.html 
spectively2. Sampling over the user space instead 
of over the query space avoids the bias to the 
most active users who submit much more queries 
than average users. 
For each sampled dataset, we invited annota-
tors who are familiar with IR and search process 
to determine each pair of successive queries of 
interest is across the border of a session. We 
made trivial pre-split process under two rules: 
 Queries from different users are not in the 
same session. 
 Queries from different days are not in the 
same session.  
Table 2 shows some basic statistics of the an-
notated data set. During the annotation process, 
the annotators were guided to identify the user?s 
information need at the finest granularity ever 
possible, because we focus on the atomic infor-
mation needs as described in Section 1. Conse-
quently, the average numbers of queries in a ses-
sion in both query logs are lower than previous 
studies. 
 AOL log Sogou log 
Queries 6407 6917 
Sessions 4571 5726 
Queries per session 1.40 1.21 
Longest session 21 12 
Table 2. Summary of the annotation results in 
both query logs 
3.3 Learning Framework 
In this section we seek to build accurate global 
classification model based on the whole training 
data obtained in the previous sub-subsection for 
both the query logs. We built the models within 
SVM framework. The implementation of SVM 
we used is libSVM (Chang and Lin, 2001). For 
the sake of evaluations and of model integration 
in the next section, we set the prediction of SVM 
to be probability estimation of the test example 
being positive. All features were pre-scaled into 
[0, 1] interval. We adopted the polynomial kernel, 
and for both datasets, we exhaustively tried each 
of the subset of the eight features using 5-fold 
cross validation. We found that using all the 
eight features yielded the best classification ac-
curacy. Thus in the experiments in rest of this 
                                                 
2 The sampling schema and sample size was deter-
mined following (Gayo-Avello, 2009). 
1205
section and the next section, we adopt the entire 
feature set to build global classification models. 
There is one parameter to be determined for 
feature extraction: the length of character-n-
grams. The proper lengths on AOL log and 
Sogou log are different. We tried the length from 
1 to 9, and according to cross validation accuracy, 
we found the best lengths for the two logs as 6 
and 3 respectively. 
3.4 Experimental Results 
3.4.1 Baseline Methods 
We provide two base line methods for compari-
sons. The first method is the commonly used 
timeout methods. We tried different timeout 
thresholds from 5 minutes to 30 minutes with a 
step of 5 minutes, and found that for both query 
logs the 5 minutes? threshold yield the best over-
all performance.  
The second method achieved the best perfor-
mance on the AOL log (Gayo-Avello, 2009), 
which addresses the session detection problem 
using a geometric interpolation method, in com-
parison to previous studies on this query log. We 
re-implemented this method and evaluated it on 
both the datasets. Similarly, the best parameters 
for the two query logs are different, such as the 
length of a character-n-gram. We only report the 
performance with the best parameter settings. 
3.4.2 Analyzing the Performance  
We analyze the performance of the SVM models 
according to precision, recall, F1-mean and F1.5-
mean of predictions on session shift and continu-
ation against human annotation data. 
The F

-mean is defined as: 
RP
PR
+
+
=
2
2)1(
mean-F ?
?
?  
where P denotes precision and R denotes recall. 
He et al (2002) regards recall more important 
than precision, and set the value of   in F

-mean 
to 1.5. We also report performance under this 
measure. 
In addition to traditional precision / recall 
based measures, we also perform ROC (Receiver 
Operating Characteristic) analysis to determine 
the discrimination power of different methods. 
The best merit of ROC analysis is that given a 
reference set, which is usually the human annota-
tion results, it evaluates a set of indicator?s dis-
crimination power for arbitrary binary classifica-
tion problem independent of the critical value 
with which the class predictions are made.  
Specifically, in the context session detection, 
regardless of the critical value that splits the clas-
sifier outputs into positive ones and negative 
ones (e.g. the 5-minutes? timeout threshold and 
50% probability in SVM?s output), the ROC 
analysis provides the overall discrimination pow-
er evaluation of the output set of a certain meth-
od (by trying to set each output value as the criti-
cal value). For the baseline method by Gayo-
Avello, the core of the decision heuristics also 
had a critical value to be determined. For details, 
readers could refer to (Gayo-Avello, 2009).  
3.4.3 Precision, Recall, and F-means 
Before we examine the discrimination power of 
each session detection method?s output independ-
ent of the threshold value selected. In this sub-
subsection, we begin with a more traditional eval-
uation schema: setting a proper threshold to pro-
duce binary predictions. It is straightforward to set 
the threshold for SVM method to 50%, and as 
described in sub-subsection 3.1.1, the threshold 
for timeout method is 5 minutes. The threshold of 
Gayo-Avello?s method is implied in its heuristics. 
Table 3 and Table 4 show the experimental re-
sults on AOL log and Sogou log respectively. 
For each dataset, we performed 1000-times boot-
strap resampling, generating 1000 bootstrapped 
datasets with the same size as the original dataset. 
To test the statistical significance of performance 
differences, we adopted Wilcoxon signed-rank 
test on the performance measures computed from 
the 1000 bootstrapped dataset, and found com-
parisons between each pair of methods were all 
significant at 95% level. 
The results show that SVM method clearly 
outperforms the baseline methods, and timeout 
method performs poorly. It may be argued that 
the poor performance of timeout method is due 
to the improper threshold value chosen. In this 
case, the ROC analysis, which assesses the dis-
crimination power of a method?s output set inde-
pendent of the threshold value chosen, is more 
suitable for performance evaluation. 
Gayo-Avello method significantly outperforms 
the timeout method. But due to its heuristic na-
ture, it is less likely to do better than the super-
vised-learning methods, although it avoids the 
over fitting issue. The Gayo-Avello method?s 
unstable performance in predicting session con-
1206
tinuations implies that its heuristics did not gen-
eralize well to Chinese query logs. 
 Timeout Gayo-Avello SVM 
P 
shift 75.92 89.35 90.96 
cont. 63.05 85.32 92.06 
R 
shift 64.49 87.85 93.82 
cont. 74.77 87.08 88.50 
F1 
shift 69.74 88.60 92.37 
cont. 68.41 86.19 90.25 
F1.5 
shift 67.62 88.31 92.92 
cont. 70.72 86.53 89.57 
Table 3. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on AOL dataset.  
 Timeout Gayo-Avello SVM 
P 
shift 67.75 75.10 87.53 
cont. 52.82 83.51 81.62 
R 
shift 59.52 91.44 86.17 
cont. 61.53 58.84 83.33 
F1 
shift 63.37 82.47 86.85 
cont. 56.84 69.04 82.47 
F1.5 
shift 61.83 85.71 86.59 
cont. 58.56 64.72 82.80 
Table 4. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on Sogou dataset. 
3.4.4 ROC Analysis 
By setting certain threshold value, we analyzed 
the three method?s performance using precision / 
recall based measures. In this sub-subsection, we 
try to set each value in an output set as the 
threshold value, and evaluate the discrimination 
power of methods by the area under the ROC 
curve. 
Figure 1 shows the ROC curves of the SVM 
method and the two baseline methods: timeout 
and Gayo-Avello, for predicting session shifts. 
ROC curves for predicting session continuations 
are symmetric with respect to the reference line, 
so we omit them in the rest of this paper for the 
sake of space limit.  
The results show that SVM method clearly 
outperforms the baseline methods in the prospec-
tive of discrimination power, with ROC area 
0.9562 on AOL dataset and 0.9154 on Sogou 
dataset. The curves of the two baseline methods 
are clearly under that of SVM method. This 
means baseline methods can never achieve accu-
racy as high as SVM method w.r.t. a fixed false 
alarm (classification error) rate, nor false alarm 
rate as low as SVM method w.r.t. a fixed accura-
cy rate. Again, Gayo-Avello method significantly 
outperforms timeout method, while underper-
forms the SVM method. For the question in the 
previous sub-subsection, coinciding with previ-
ous studies (Murray et al, 2006; Jones and 
Klinkner, 2008), applying single timeout thresh-
old always yields limited discrimination power, 
wherever the operating point on ROC curve (i.e. 
threshold value) is set. 
4 Making Use of the Variability of Dis-
crimination Power 
In this section, we first analyze the amount of 
contribution that each feature makes and show 
that the contribution, i.e. the discrimination pow-
er of each feature varies dramatically across dif-
ferent users. Then, we propose an approach to 
making use of this variability. Finally through 
experimental results, we show that the proposed 
approach makes small, yet significant improve-
ments to the SVM method in Section 3. 
4.1 Variability of Discrimination Power 
The ROC analysis of individual feature provides 
adequate characterizations of the discrimination 
power of the feature. Another advantage of 
adopting ROC analysis is that the results are in-
dependent not only of the critical value, but also 
of the scale of the feature values.  
Figure 2 shows the ROC curves of all the eight 
features in both datasets. Note that some features 
are with a higher value indicating session contin-
uation rather than session shift, so their ROC 
curves are below the reference line. The feature 
?time_interval? behaves exactly the same as the 
timeout method in Figure 1. For the rest of the 
features, ?avg_ngram_distance?, ?common_ngram? 
and ?Jaccard_ngram? achieve the best discrimi-
nation powers, showing the character-n-gram 
representation is effective. The feature ?com-
mon_char? performs significantly better in 
Sogou dataset than in AOL dataset, because Chi-
nese characters convey much more information 
than English characters do. ?common_suffix? 
performing worse than ?common_prefix? reflects 
the custom of users. Users tend to add terms at 
the end of the query in a searching iteration, thus 
predicting session continuations by examining 
the common suffixes is problematic. 
1207
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.7707
Gayo-Avello ROC area: 0.9130
SVM ROC area: 0.9562
Reference
AOL
    
0.
00
0.
25
0.
50
1.
00
0.
75
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.6365
Gayo-Avello ROC area: 0.8463
SVM ROC area: 0.9154
Reference
Sogou
 
Figure 1. ROC analysis of SVM method and two baseline methods for predicting session shifts on 
both AOL and Sogou dataset. All comparisons between ROC areas within the same dataset are at 
least 95% statistically significant, because the corresponding confidence intervals do not overlap. 
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.7707
avg_ngram_distance ROC area: 0.9560
edit_disance ROC area: 0.8848
common_prefix ROC area: 0.2177
common_suffix ROC area: 0.2985
common_char ROC area: 0.1360
common_ngram ROC area: 0.0480
Jaccard_ngram ROC area: 0.0464
Reference
AOL
  
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.6365
avg_ngram_distance ROC area: 0.9108
edit_disance ROC area: 0.8333
common_prefix ROC area: 0.2449
common_suffix ROC area: 0.3745
common_char ROC area: 0.0922
common_ngram ROC area: 0.1018
Jaccard_ngram ROC area: 0.0965
Reference
Sogou
 
Figure 2. ROC analysis of individual features for predicting session shifts on both AOL and Sogou 
dataset. Note that some curves with similar ROC area values overlap each other. 
In spite of the discrimination power a feature 
has, its behavior on different users is worth-
while to be examined. For selecting users that 
have sufficient data to draw stable conclusions, 
we consider only users who issued more than 50 
queries in the datasets. Unfortunately, there are 
too few users (6 users) qualified in Sogou da-
taset, so we show only the statistics of ROC 
area values of each of the features in Table 5 
based on 37 users in AOL dataset. 
The statistics in Table 5 show that for differ-
ent users. Recall that in sub-subsection 3.3.2, a 
0.04 difference of ROC area make the perfor-
mance of the SVM method significantly better 
1208
than that of the Gayo-Avello?s method. Thus, 
the discrimination power of a feature is likely to 
vary significantly, because all the standard de-
viations are at 0.03 or even higher level. Espe-
cially, the minimum and maximum values show 
that for these users, some of the findings above 
from the whole dataset do not hold. This implies 
that it is likely more feasible to build specific 
local models for these users to make full use of 
the variability within the same feature. 
Feature avg. sdev. min. max. 
time_interval 0.780 0.088 0.476 0.912
avg_ngram_ 
distance 
0.954 0.034 0.861 1.000
edit_disance 0.883 0.056 0.733 0.990
common_prefix 0.224 0.069 0.099 0.327
common_suffix 0.299 0.113 0.064 0.578
common_char 0.143 0.082 0.037 0.493
common_ngram 0.051 0.037 0.000 0.187
Jaccard_ngram 0.049 0.036 0.000 0.173
Table 5. Average, standard deviation, minimum, 
and maximum ROC areas of individual features 
4.2 Building Local Models 
We built individual local models for each user 
that issued more than 50 queries in AOL dataset. 
We also performed 5-fold cross validations and 
set the prediction to be the probability estima-
tion of a test example being positive. The fea-
ture selection process showed again that all the 
eight features are beneficial, and none of them 
should be excluded. 
In each fold of cross validation, we per-
formed 90%-bagging on the training set 10 
times to get the variance estimations of the local 
model. For each example in the test set, we set 
the final output on it to be the average of the 10 
outputs, and recorded the standard deviation of 
the outputs on this example which is used dur-
ing the model combination. We also conducted 
the same process for the global model for the 
sake of combination process described below. 
4.3 Combing with the Global Model 
Since the predictions of both the local and the 
global models are probability estimations, it is 
reasonable to combine them using linear combi-
nation. For each example, there are two outputs 
Ol and Og coming from local and global models 
accordingly. For each example e of a user?s sub 
dataset U, we have the outputs Ol(e) and Og(e) 
as well as the normalized deviations Dl(e) and 
Dg(e) (by the largest deviation in U of the corre-
sponding models). The final output O(e) is de-
fined as: 
)()(
)()()()(
)(
eDeD
eOeDeOeD
eO
gl
glgl
+
?+?
=
 
 Global Local Combine 
P 
shift 90.48 88.53 90.43 
cont. 91.75 92.12 92.52 
R 
shift 93.94 94.44 94.56 
cont. 87.20 84.16 87.04 
F1 
shift 92.18 91.39 92.45 
cont. 89.41 87.96 89.69 
F1.5 
shift 92.85 92.54 93.25 
cont. 88.55 86.46 88.65 
Table 6. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of global model (bagging), 
local model (bagging) and combined model  
This combination process is similar to (Osl et 
al., 2008). Note that the more the deviation of a 
model is, the less feasible the corresponding 
model is. We compared the performance of 
three models: global model, local model, and 
combined model. The results are summarized in 
Table 6. All comparisons between different 
models are statistically significant at 95% level, 
based on the same bootstrapping settings in sub-
subsection 3.4.3. The combined model shows 
slight (may due to the inferior performance of 
the local model), yet significant improvement to 
the global model. In spite of the amount of the 
improvement, the local model did correct some 
errors of the global model. It may be not ac-
ceptable to build such an expensive combined 
model for a limited improvement. Nevertheless, 
the results do show that the variability across 
different users is exploitable. 
5 Discussion and Conclusion 
In this paper, we built a learning framework of 
detecting sessions which corresponds to user?s 
interest in a query log. We considered two as-
pect of a pair of successive queries: temporal 
aspect and content aspect, and designed eight 
features based on these two aspects, and the 
SVM models built with these features achieved 
satisfactory performance (92.37% F1-mean on 
session shift, 90.25% F1-mean on session con-
tinuation), significantly better than the best-ever 
approach on AOL query log. 
1209
The analysis of the features? discrimination 
power was conducted not only among different 
features, but also within the same feature when 
applied to different users in the query log. By 
analyzing the statistics of ROC area values of 
each of the features based on 37 users in AOL 
dataset, experimental results showed that there 
is considerable variability in both these aspects. 
To make full use of this variability, we built 
local models for individual user and combine 
the yielded predictions with those yielded by the 
global model. Experiments showed that the lo-
cal model did make significant improvements to 
the global model, although the amount was 
small (92.45% vs. 92.18% F1-mean on session 
shift, 89.69% vs. 89.41% F1-mean on session 
continuation). 
In future studies, we will explore other learn-
ing frameworks which better integrate the local 
model and the global model, and will try to ac-
quire more data to build local models. We will 
also analyze more deeply the characteristics of 
ROC analysis in the feature selection process.  
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project (Grant 
No.2006AA010108). The authors are grateful 
for the anonymous reviewers for their valuable 
comments. 
References 
Chang Chih-Chung and Chih-Jen Lin. 2001. 
LIBSVM : a library for support vector machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Downey Doug, Susan Dumais, and Eric Horvitz. 
2007. Models of searching and brows-
ing: languages, studies, and applications. In Pro-
ceedings of the 20th international joint conference 
on Artificial intelligence, pages 2740-2747, Hy-
derabad, India. 
Gayo-Avello Daniel. 2009. A survey on session de-
tection methods in query logs and a proposal for 
future evaluation, Information Science 
179(12):1822-1843. 
He Daqing and Ayse G?ker. 2000. Detecting Session 
Boundaries from Web User Logs. In BCS/IRSG 
22nd Annual Colloqui-um on Information Re-
trieval Research, pages 57-66.  
He Daqing, Ayse G?ke, and David J. Harper. 2002. 
Combining evidence for automatic web session 
identification. Information Processing and Man-
agement: an International Journal, 38(5):727-742. 
Jansen Bernard J., Amanda Spink, Chris Blakely, 
and Sherry Koshman. 2007. Defining a session on 
Web search engines: Research Articles. Journal of 
the American Society for Information Science and 
Technology, 58(6):862-871 
Jones Rosie and Kristina Lisa Klinkner. 2008. Be-
yond the session timeout: automatic hierarchical 
segmentation of search topics in query logs. In 
Proceedings of the 17th ACM conference on In-
formation and knowledge management, pages 
699-708, Napa Valley, California, USA. 
Murray G. Craig, Jimmy Lin, and Abdur Chowdhury. 
2007. Identification of user sessions with hierar-
chical agglomerative clustering. American Society 
for Information Science and Technology, 43(1):1-
9. 
Osl Melanie, Christian Baumgartner, Bernhard Tilg, 
and Stephan Dreiseitl. 2008. On the combination 
of logistic regression and local probability esti-
mates. In Proceedings of Third International Con-
ference on Broadband Communications, Infor-
mation Technology & Biomedical Applications, 
pages 124-128. 
?zmutlu Seda. 2006. Automatic new topic identifi-
cation using multiple linear regression. Infor-
mation Processing and Management: an Interna-
tional Journal, 42(4):934-950. 
?zmutlu Huseyin C. 2009. Markovian analysis for 
automatic new topic identification in search en-
gine transaction logs. Applied Stochastic Models 
in Business and Industry, 25(6):737-768. 
Pass Greg, Abdur Chowdhury, and Cayley Torgeson. 
2006. A picture of search. In Proceedings of the 
1st international conference on Scalable infor-
mation systems, Hong Kong. 
Radlinski Filip and Thorsten Joachims. 2005. Query 
chains: learning to rank from implicit feedback. In 
Proceedings of the eleventh ACM SIGKDD inter-
national conference on Knowledge discovery in 
data mining, pages 239-248, Chicago, Illinois, 
USA. 
Silverstein Craig, Hannes Marais, Monika Henzinger, 
and Michael Moricz. 1999. Analysis of a very 
large web search engine query log. ACM SIGIR 
Forum, 33(1):6-12. 
1210
Coling 2010: Poster Volume, pages 1533?1540,
Beijing, August 2010
All in Strings: a Powerful String-based Automatic MT  
Evaluation Metric with Multiple Granularities 
 
Junguo Zhu1, Muyun Yang1, Bo Wang2, Sheng Li1, Tiejun Zhao1 
 
1 School of Computer Science and Technology, Harbin Institute of Technology 
{jgzhu; ymy; tjzhao; lish}@mtlab.hit.edu.cn 
2 School of Computer Science and Technology, Tianjin University  
bo.wang.1979@gmail.com 
 
 
Abstract 
String-based metrics of automatic ma-
chine translation (MT) evaluation are 
widely applied in MT research. Mean-
while, some linguistic motivated me-
trics have been suggested to improve 
the string-based metrics in sentence-
level evaluation. In this work, we at-
tempt to change their original calcula-
tion units (granularities) of string-based 
metrics to generate new features. We 
then propose a powerful string-based 
automatic MT evaluation metric, com-
bining all the features with various 
granularities based on SVM rank and 
regression models. The experimental 
results show that i) the new features 
with various granularities can contri-
bute to the automatic evaluation of 
translation quality; ii) our proposed 
string-based metrics with multiple gra-
nularities based on SVM regression 
model can achieve higher correlations 
with human assessments than the state-
of-art  automatic metrics. 
1 Introduction 
The automatic machine translation (MT) eval-
uation has aroused much attention from MT 
researchers in the recent years, since the auto-
matic MT evaluation metrics can be applied to 
optimize MT systems in place of the expensive 
and time-consuming human assessments. The 
state-of-art strategy to automatic MT evalua-
tion metrics estimates the system output quali-
ty according to its similarity to human refer-
ences. To capture the language variability ex-
hibited by different reference translations, a 
tendency is to include deeper linguistic infor-
mation into machine learning based automatic 
MT evaluation metrics, such as syntactic and 
semantic information (Amig? et al, 2005; Al-
brecht and Hwa, 2007; Gim?nez and M?rquez, 
2008). Generally, such efforts may achieve 
higher correlation with human assessments by 
including more linguistic features. Neverthe-
less, the complex and variously presented lin-
guistic features often prevents the wide appli-
cation of the linguistic motivated metrics. 
Essentially, linguistic motivated metrics in-
troduce additional restrictions for accepting the 
outputs of translations (Amig? et al, 2009).  
With more linguistic features attributed, the 
model is actually capturing the sentence simi-
larity in a finer granularity. In this sense, the 
practical effect of employing various linguistic 
knowledge is changing the calculation units of 
the matching in the process of the automatic 
evaluation. 
Similarly, the classical string-based metrics 
can be changed in their calculation units direct-
ly. For example, the calculation granularity in 
BLEU (Papineni et al, 2002) metric is word: 
n-grams are extracted on the basis of single 
word as well as adjacent multiple words. And 
the calculation granularity in PosBLEU 
(Popovi? and Ney, 2009) metric is Pos tag, 
which correlate well with the human assess-
ments. Therefore, it is straight forward to apply 
the popular string-based automatic evaluation 
metrics, such as BLEU, to compute the scores 
of the systems outputs in the surface or linguis-
1533
tic tag sequences on various granularities le-
vels. 
In this paper, we attempt to change the orig-
inal calculation units (granularities) of string-
based metrics to generate new features. After 
that, we propose a powerful string-based au-
tomatic MT evaluation metric, combining all 
the features with various granularities based on 
SVM rank (Joachims, 2002) and regression 
(Drucker et al, 1996) models. Our analysis 
indicates that: i) the new features with various 
granularities can contribute to the automatic 
evaluation of translation quality; ii) our pro-
posed string-based metrics with multiple gra-
nularities based on SVM regression model can 
achieve higher correlations with human as-
sessments than the state-of-art automatic me-
trics . 
The remainder of this paper is organized as 
follows: Section 2 reviews the related re-
searches on automatic MT evaluation. Section 
3 describes some new calculation granularities 
of string-based metrics on sentence level. In 
Section 4, we propose string-based metrics 
with multiple granularities based on SVM rank 
and regression models. In Section 5, we 
present our experimental results on different 
sets of data. And conclusions are drawn in the 
Section 6. 
2 Related Work on Automatic Ma-
chine Translation Evaluation 
The research on automatic string-based ma-
chine translation (MT) evaluation is targeted at 
a widely applicable metric of high consistency 
to the human assessments. WER (Nie?en et al, 
2000), PER (Tillmann et al, 1997), and TER 
(Snover et al, 2006) focuses on word error rate 
of translation output. GTM (Melamed et al, 
2003) and the variants of ROUGE (Lin and 
Och, 2004) concentrate on matched longest 
common substring and discontinuous substring 
of translation output according to the human 
references. BLEU (Papineni et al, 2002) and 
NIST (Doddington, 2002) are both based on 
the number of common n-grams between the 
translation hypothesis and human reference 
translations of the same sentence. BLEU and 
NIST are widely adopted in the open MT eval-
uation campaigns; however, the NIST MT 
evaluation in 2005 indicates that they can even 
error in the system level (Le and Przybocki, 
2005). Callison-Burch et al (2006) detailed the 
deficits of the BLEU and other similar metrics, 
arguing that the simple surface similarity cal-
culation between the machines translations and 
the human translations suffers from morpho-
logical issues and fails to capture what are im-
portant for human assessments.  
In order to attack these problems, some me-
trics have been proposed to include more lin-
guistic information into the process of match-
ing, e.g., Meteor (Banerjee and Lavie, 2005) 
metric and MaxSim (Chan nad Ng, 2008) me-
trics, which improve the lexical level by the 
synonym dictionary or stemming technique. 
There are also substantial studies focusing on 
including deeper linguistic information in the 
metrics (Liu and Gildea, 2005; Owczarzak et 
al., 2006; Amig? et al, 2006; Mehay and Brew, 
2007; Gim?nez and M?rquez, 2007; Owczar-
zak et al, 2007; Popovic and Ney, 2007; 
Gim?nez and M?rquez, 2008b). 
A notable trend improving the string-based 
metric is to combine various deeper linguistic 
information via machine learning techniques in 
the metrics (Amig? et al, 2005; Albrecht and 
Hwa, 2007; Gim?nez and M?rquez, 2008). 
Such efforts are practically amount of intro-
ducing additional linguistic restrictions into the 
automatic evaluation metrics (Amig? et al 
2009), achiving a higher performance at the 
cost of lower adaptability to other languages 
owing to the language dependent linguistics 
features. 
Previous work shows that including the new 
features into the evaluation metrics may bene-
fit to describe nature language accurately. In 
this sense, the string-based metrics will be im-
proved, if the finer calculation granularities are 
introduced into the metrics.  
Our study analyzes the role of the calcula-
tion granularities in the performance of metrics. 
We find that the new features with various 
granularities can contribute to the automatic 
evaluation of translation quality. Also we pro-
pose a powerful string based automatic MT 
evaluation metric with multiple granularities 
combined by SVM. Finally, we seek a finer 
feature set of metrics with multiple calculation 
granularities. 
1534
3 The New Calculation Granularities 
of String-based Metrics on Sentence 
Level  
The string-based metrics of automatic machine 
translation evaluation on sentence level adopt a 
common strategy: taking the sentences of the 
documents as plain strings. Therefore, when 
changing the calculation granularities of the 
string-based metrics we can simplify the in-
formation of new granularity with plain strings.  
In this work, five kinds of available calculation 
granularities are defined: ?Lexicon?, ?Letter?, 
?Pos?, ?Constitute? and ?Dependency?.  
Lexicon: The calculation granularity is 
common word in the sentences of the docu-
ments, which is popular practice at present. 
Letter: Split the granularities of ?Lexical? 
into letters. Each letter is taken as a matching 
unit. 
Pos: The Pos tag of each ?Lexicon? is taken 
as a matching unit in this calculation granulari-
ty. 
Constitute: Syntactic Constitutes in a tree 
structure are available through the parser tools. 
We use Stanford Parser (Klein and Manning, 
2003a; Klein and Manning, 2003b) in this 
work.  The Constitute tree is changed into 
plain string, travelling by BFS (Breadth-first 
search traversal) 1.  
Dependency: Dependency relations in a de-
pendency structure are also available through 
the parser tools. The dependency structure can 
also be formed in a tree, and the same 
processing of being changed into plain string is 
adopted as ?Constitute?. 
The following serves as an example:  
Sentence:  
I have a dog 
Pos tag:  
I/PRON have/V a/ART dog/N 
Constitute tree:  
 
                                                 
1 We also attempt some other traversal algorithms, in-
cluding preorder, inorder and postorder traversal, the 
performance are proved to be similar.  
Dependency tree:  
 
Then, we can change the sentence into the 
plain string in multiple calculation granulari-
ties as follows: 
Lexicon string:  
I have a dog 
Letter string:  
I h a v e a d o g 
Pos string: 
PRON V ART N 
Constitute string:  
PRON V ART N NP NP VP S 
Dependency string: 
 a I dog have 
The translation hypothesis and human refer-
ence translations are both changed into those 
strings of various calculation granularities. The 
strings are taken as inputs of the string-based 
automatic MT evaluation metrics. The outputs 
of each metric are calculated on different 
matching units. 
4 String-based Metrics with Multiple 
Granularities Combined by SVM 
Introducing machine learning methods to es-
tablished MT evaluation metric is a popular 
trend. Our study chooses rank and regression 
support vector machine (SVM) as the learning 
model. Features are important for the SVM 
models. 
Plenty of scores can be generated from the 
proposed metrics. In fact, not all these features 
are needed. Therefore, feature selection should 
be a necessary step to find a proper feature set 
and alleviate the language dependency by us-
ing fewer linguistic features. 
Feature selection is an NP-Complete prob-
lem; therefore, we adopt a greedy selection 
algorithm called ?Best One In? to find a local 
optimal feature set. Firstly, we select the fea-
ture among all the features which best corre-
lates with the human assessments.  Secondly, a 
feature among the rest features is added in to 
the feature set, if the correlation with the hu-
man assessments of the metric using new set is 
1535
the highest among all new metrics and higher 
than the previous metric in cross training cor-
pus. The cross training corpus is prepared by 
dividing the training corpus into five parts. 
Each four parts of the five are for training and 
the rest one for testing; then, we integrate 
scores of the five tests as scores of cross train-
ing corpus.  The five-fold cross training can 
help to overcome the overfitting. At the end, 
the feature selection stops, if adding any of the 
rest features cannot lead to higher correlation 
with human assessments than the current me-
tric.  
5 Experiments 
5.1 The Impact of the Calculation Granu-
larities on String-based Metrics 
In this section, we use the data from NIST 
Open MT 2006 evaluation (LDC2008E43), 
which is described in Table 1.  It consists of 
249 source sentences that were translated by 
four human translators as well as 8 MT sys-
tems. Each machine translated sentence was 
evaluated by human judges for their adequacy 
on a 7-point scale. 
 
 NIST 2002  
NIST 
2003  
NIST 
Open 
MT 2006
LDC 
corpus 
LDC2003
T17 
LDC2006
T04 
LDC2008
E43 
Type Newswire Newswire Newswire
Source Chinese Chinese Arabic 
Target English English English 
# of  
sentences 878 919 249 
# of 
systems 3 7 8 
#  of 
references 4 4 4 
Score 
1-5, 
adequacy 
& fluency
1-5, 
adequacy 
& fluency 
1-7 
adequacy
Table 1: Description of LDC2006T04, 
LDC2003T17 and LDC2008E43 
 
To judge the quality of a metric, we com-
pute Spearman rank-correlation coefficient, 
which is a real number ranging from -1 (indi-
cating perfect negative correlations) to +1 (in-
dicating perfect positive correlations), between 
the metric?s scores and the averaged human 
assessments on test sentences. 
We select 21 features in ?lexicon? calcula-
tion granularity and 11?4 in the other calcula-
tion granularities. We analyze the correlation 
with human assessments of the metrics in mul-
tiple calculation granularities.  Table 2 lists the 
optimal calculation granularity of the multiple 
metrics on sentence level in the data 
(LDC2008E43).  
 
Metric Granularity 
BLEU-opt Letter 
NIST-opt Letter 
GTM(e=1) Dependency 
TER Letter 
PER Lexicon 
WER Dependency 
ROUGE-opt Letter 
Table 2 The optimal calculation granularity of the 
multiple metrics 
 
The most remarkable aspect is that not all 
the best metrics are based on the ?lexicon? cal-
culation granularities, such as the ?letter? and 
?dependency?. In other words, the granulari-
ties-shifted string-based metrics are promising 
to contribute to the automatic evaluation of 
translation quality. 
5.2 Correlation with Human Assessments 
of String-based Metrics with Multiple 
Granularities Based on SVM Frame 
We firstly train the SVM rank and regression 
models on LDC2008E43 using all the features 
(21+11? 4 species), without any selection. 
Secondly, the other two SVM rank and regres-
sion models are trained on the same data using 
the feature set via feature selection, which are 
described in Table 3. We have four string-
based evaluation metrics with multiple granu-
larities on rank and regression SVM frame 
?Rank_All, Regression_All, Rank_Select and 
Regression_Select?.  Then we apply the four 
metrics to evaluate the sentences of the test 
data (LDC2006T04 and LDC2003T17). The 
results of Spearman correlation with human 
assessments is summarized in Table 3. For 
comparison, the results from some state-of-art 
metrics (Papineni et al, 2002; Doddington, 
1536
2002; Melamed et al, 2003; Banerjee and La-
vie, 2005; Snover et al, 2006; Liu and Gildea, 
2005) and two machine learning methods (Al-
brecht and Hwa, 2007; Ding Liu and Gildea, 
2007) are also included in Table 3. Of the two 
machine learning methods, both trained on the 
data LDC2006T04. The ?Albrecht, 2007? 
score reported a result of Spearman correlation 
with human assessments on the data 
LDC2003T17 using 53 features, while the 
?Ding Liu, 2007? score reported that under 
five-fold cross validation on the data 
LDC2006T04 using 31 features. 
 
 Feature number 
LDC
2003
T17 
LDC
2006
T04 
Rank_All 65 0.323 0.495
Regression_All 65 0.345 0.507
Rank_Select 16 0.338 0.491
Regression_Select 8 0.341 0.510
Albrecht, 2007 53 0.309 -- 
Ding Liu, 2007 31 -- 0.369
BLEU-opt2 -- 0.301 0.453
NIST-opt -- 0.219 0.417
GTM(e=1) -- 0.270 0.375
METEOR3 -- 0.277 0.463
TER -- -0.250 -0.302
STM-opt -- 0.205 0.226
HWCM-opt -- 0.304 0.377
 
Table 3: Comparison of Spearman correlations with 
human assessments of our proposed metrics and 
some start-of-art metrics and two machine learning 
methods 
?-opt? stands for the optimum values of the pa-
rameters on the metrics 
 
Table 3 shows that the string-based meta-
evaluation metrics with multiple granularities 
based on SVM frame gains the much higher 
Spearman correlation than other start-of-art 
metrics on the two test data and, furthermore, 
our proposed metrics also are higher than the 
machine learning metrics (Albrecht and Hwa, 
2007; Ding Liu and Gildea, 2007).  
The underlining is that our proposed metrics 
are more robust than the aforementioned two 
                                                 
2 The result is computed by mteval11b.pl.  
3 The result is computed by meteor-v0.7. 
machine learning metrics. As shown in Table 1 
the heterogeneity between the training and test 
data in our method is much more significant 
than that of the other two machine learning 
based methods.  
In addition, the ?Regression_Select? metric 
using only 8 features can achieve a high corre-
lation rate which is close to the metric pro-
posed in ?Albrecht, 2007? using 53 features, 
?Ding Liu, 2007? using 31 features, ?Regres-
sion_All? and ?Rank_All? metrics using  65 
features and ?Rank_Select? metric using 16 
features. What is more, ?Regression_Select? 
metric is better than ?Albrecht, 2007?, and 
slightly lower than ?Regression_All? on the 
data LDC2003T17; and better than both ?Re-
gression_All? and ?Rank_All? metrics on the 
data LDC2006T04. That confirms that a small 
cardinal of feature set can also result in a me-
tric having a high correlation with human as-
sessments, since some of the features represent 
the redundant information in different forms. 
Eliminating the redundant information is bene-
fit to reduce complexity of the parameter 
searching and thus improve the metrics per-
formance based on SVM models. Meanwhile, 
fewer features can relieve the language depen-
dency of the machine learning metrics. At last, 
our experimental results show that regression 
models perform better than rank models in the 
string-based metrics with multiple granularities 
based on SVM frame, since ?Regres-
sion_Select? and ?Regression_All? achieve 
higher correlations with human assessments 
than the others. 
5.3 Reliability of Feature Selection  
The motivation of feature selection is keeping 
the validity of the feature set and alleviating 
the language dependency. We also look for-
ward to the higher Spearman correlation on the 
test data with a small and proper feature set.  
We use SVM-Light (Joachims, 1999) to 
train our learning models using NIST Open 
MT 2006 evaluation data (LDC2008E43), and 
test on the two sets of data, NIST?s 2002 and 
2003 Chinese MT evaluations. All the data are 
described in Table 1. To avoid the bias in the 
distributions of the two judges? assessments in 
NIST?s 2002 and 2003 Chinese MT evalua-
tions, we normalize the scores following (Blatz 
et al, 2003). 
1537
We trace the process of the feature selection. 
The selected feature set of the metric based on 
SVM rank includes 16 features and that of the 
metric based on SVM regression includes 8 
features. The selected features are listed in Ta-
ble 4. The values in Table 4 are absolute 
Spearman correlations with human assess-
ments of each single feature score.  The prefix-
es ?C_?, ?D_?, ?L_?, ?P_?, and ?W_? 
represent ?Constitute?, ?Dependency?, ?Let-
ter?, ?Pos? and ?Lexicon? respectively. 
 
Rank spear-man Regression
spear-
man
C_PER .331 C_PER .331
C_ROUGE-W .562 C_ROUGE-W .562
D_NIST9 .479 D_NIST9 .479
D_ROUGE-W .679 D_ROUGE-L .667
L_BLEU6 .702 L_BLEU6 .702
L_NIST9 .691 L_NIST9 .691
L_ROUGE-W .634 L_ROUGE-W .634
P_PER .370 P_ROUGE-W .683
P_ROUGE-W .616  
W_BLEU1_ind .551  
W_BLEU2 .659  
W_GTM .360  
W_METEOR .693  
W_NIST5 .468  
W_ROUGE1 .642  
W_ROUGE-W .683  
 
Table 4: Feature sets of SVM rank and regression 
 
Table 4 shows that 8 features are selected 
from 65 features in the process of feature se-
lection based on SVM regression while 16 fea-
tures based on SVM rank. Fewer features 
based on SVM regression are selected than 
SVM rank. Only one feature in feature set 
based on SVM regression does not occur in 
that based on SVM rank. The reason is that 
there are more complementary advantages be-
tween the common selected features.  
Next, we will verify the reliability of our 
feature selection algorithm. Figure 1 and Fig-
ure 2 show the Spearman correlation values 
between our SVM-based metrics (regression 
and rank) and the human assessments on both 
training data (LDC2008E43) and test data 
(LDC2006T04 and LDC2003T17).  
 
 
 
Figure 1: The Spearman correlation values between 
our SVM rank metrics and the human assessments 
on both training data and test data with the exten-
sion of the feature sets 
 
 
 
Figure 2: The Spearman correlation values between 
our SVM regression metrics and the human as-
sessments on both training data and test data with 
the extension of the feature sets 
 
From Figure 1 and Figure 2, with the exten-
sion of the feature sets, we can find that the 
tendency of correlation obtained by each me-
tric based on SVM rank or regression roughly 
the same on both the training data and test data. 
Therefore, the two feature sets of SVM rank 
and regression models are reliable. 
6 Conclusion 
In this paper we propose an integrated platform 
for automatic MT evaluation by improving the 
string based metrics with multiple granularities. 
Our proposed metrics construct a novel inte-
grated platform for automatic MT evaluation 
based on multiple features. Our  key contribu-
tion consists of two parts: i) we suggest a strat-
egy  of changing the various complex features 
into plain string form. According to the strate-
gy, the automatic MT evaluation frame are 
1538
much more clarified, and the computation of 
the similarity is much more simple, since the 
various linguistic features may express in the 
uniform strings with multiple calculation gra-
nularities. The new features have the same 
form and are dimensionally homogeneous; 
therefore, the consistency of the features is 
enhanced strongly. ii) We integrate the features 
with machine learning and proposed an effec-
tive approach of feature selection. As a result, 
we can use fewer features but obtain the better 
performance. 
In this framework, on the one hand, string-
based metrics with multiple granularities may 
introduce more potential features into automat-
ic evaluation, with no necessarily of new simi-
larity measuring method, compared with the 
other metrics. On the other hand, we succeed 
in finding a finer and small feature set among 
the combinations of plentiful features, keeping 
or improving the performance. Finally, we 
proposed a simple, effective and robust string-
based automatic MT evaluation metric with 
multiple granularities. 
Our proposed metrics improve the flexibility 
and performance of the metrics based on the 
multiple features; however, it still has some 
drawbacks: i) some potential features are not 
yet considered, e.g. the semantic roles; and ii) 
the loss of information exists in the process of 
changing linguistic information into plain 
strings. For example, the dependency label in 
the calculation granularity ?Dependency? is 
lost when changing information into string 
form. Though the final results obtain the better 
performance than the other linguistic metrics, 
the performance is promising to be further im-
proved if the loss of information can be prop-
erly dealt with. 
Acknowledgement 
This work is supported by Natural Science 
foundation China (Grant No.60773066 & 
60736014) and National Hi-tech Program 
(Project No.2006AA010108), and the Natural 
Scientific Reserach Innovation Foundation in 
Harbin Institute of Technology (Grant No. 
HIT.NSFIR.20009070). 
 
References 
Albrecht S. Joshua and Rebecca Hwa. 2007. A 
Reexamination of Machine Learning Approaches 
for Sentence-Level MT Evaluation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 880-
887. 
Amig? Enrique, Julio Gonzalo, Anselmo P?nas, 
and Felisa Verdejo. 2005. QARLA: a Framework 
for the Evaluation of Automatic Summarization. 
In Proceedings of the 43th Annual Meeting of 
the Association for Computational Linguistics. 
Amig? Enrique, Jes?s Gim?nez, Julio Gonzalo,  
Felisa Verdejo. 2009. The Contribution of Lin-
guistic Features to Automatic Machine Transla-
tion Evaluation. In proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL 
and the 4th International Joint Conference on 
Natural Language Processing of the AFNLP. 
Amig? Enrique, Jes?s Gim?nez, Julio Gonzalo, and 
Llu?s M?rquez. 2006. MT Evaluation: Human- 
Like vs. Human Acceptable. In Proceedings of 
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Lin-
guistic, pages 17?24. 
Banerjee Satanjeev and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation 
with improved correlation with human judg-
ments. In Proceedings of the ACL Workshop on 
Intrinsic and Extrinsic Evaluation Measures. 
Blatz John, Erin Fitzgerald, George Foster, Simona 
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto 
Sanchis, and Nicola Ueffing. 2003. Confidence 
estimation for machine translation. In Technical 
Report Natural Language Engineering Workshop 
Final Report, pages 97-100. 
 Callison-Burch Chris, Miles Osborne, and Philipp 
Koehn. 2006. Re-evaluating the Role of BLEU in 
Machine Translation Research. In Proceedings 
of 11th Conference of the European Chapter of 
the Association for Computational Linguistics  
Chan S. Yee and Hwee T. Ng. 2008. MAXSIM: A 
maximum similarity metric for machine transla-
tion evaluation. In Proceedings of ACL-08: HLT, 
pages 55?62. 
Doddington George. 2002. Automatic Evaluation of 
Machine Translation Quality Using N-gram Co- 
Occurrence Statistics. In Proceedings of the 2nd 
International Conference on Human Language 
Technology, pages 138?145. 
1539
Drucker Harris, Chris J. C. Burges, Linda Kaufman, 
Alex Smola, Vladimir Vapnik. 1996. Support 
vector regression machines.  In NIPS. 
Gim?nez Jes?s and Llu?s M?rquez. 2007. Linguistic 
Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL 
Workshop on Statistical Machine Translation. 
Gim?nez Jes?s and Llu?s M?rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through 
Non-Parametric Metric Combinations. In Pro-
ceedings of IJCNLP, pages 319?326. 
Gim?nez Jes?s and Llu?s M?rquez. 2008b. On the 
Robustness of Linguistic Features for Automatic 
MT Evaluation. 
Joachims Thorsten. 2002. Optimizing search en-
gines using clickthrough data. In KDD. 
Klein Dan and Christopher D. Manning. 2003a. 
Fast Exact Inference with a Factored Model for 
Natural Language Parsing. In Advances in 
Neural Information Processing Systems 15, pp. 
3-10.  
Klein Dan and Christopher D. Manning. 2003b. 
Accurate Unlexicalized Parsing. Proceedings of 
the 41st Meeting of the Association for Compu-
tational Linguistics, pp. 423-430. 
Le Audrey and Mark Przybocki. 2005. NIST 2005 
machine translation evaluation official results. 
In Official release of automatic evaluation scores 
for all submission. 
Lin Chin-Yew and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quali-
ty Using Longest Common Subsequence and 
Skip-Bigram Statistics. Proceedings of the 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics, pp. 605-612. 
Liu Ding and Daniel Gildea. 2005. Syntactic Fea-
tures for Evaluation of Machine Translation. In 
Proceedings of ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization, pages 25?32. 
Liu Ding and Daniel Gildea. 2007. Source Lan-
guage Features and Maximum Correlation 
Training for Machine Translation Evaluation. In 
proceedings of NAACL HLT 2007, pages 41?48 
Mehay Dennis and Chris Brew. 2007. BLEUATRE: 
Flattening Syntactic Dependencies for MT Eval-
uation. In Proceedings of the 11th Conference on 
Theoretical and Methodological Issues in Ma-
chine Translation. 
Melamed Dan I., Ryan Green, and Joseph P. Turian. 
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on 
Human Language Technology and the North 
American Chapter of the Association for Com-
putational Linguistics. 
Nie?en Sonja, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. An Evaluation Tool for 
Machine  Translation: Fast Evaluation for MT 
Research. In Proceedings of the 2nd Internation-
al Conference on Language Resources and Eval-
uation . 
Owczarzak Karolina, Declan Groves, Josef Van 
Genabith, and Andy Way. 2006. Contextual Bi-
text- Derived Paraphrases in Automatic MT 
Evaluation. In Proceedings of the 7th Confe-
rence of the Association for Machine Translation 
in the Americas, pages 148?155. 
Owczarzak Karolina, Josef van Genabith, and Andy 
Way. 2007. Labelled Dependencies in Machine 
Translation Evaluation. In Proceedings of the 
ACL Workshop on Statistical Machine Transla-
tion, pages 104?111. 
Papineni Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In 
Proceedings of 40th Annual Meeting of the As-
sociation for Computational Linguistics. 
Popovi? Maja and Hermann Ney. 2007. Word Er-
ror Rates: Decomposition over POS classes and 
Applications for Error Analysis. In Proceedings 
of the Second Workshop on Statistical Machine 
Translation, pages 48?55. 
Popovi? Maja and Hermann Ney. 2009. Syntax-
oriented evaluation measures for machine trans-
lation output. In Proceedings of the 4th EACL 
Workshop on Statistical Machine Translation, 
pages 29?32. 
Snover Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of AMTA, pag-
es 223?231. 
Tillmann Christoph, Stefan Vogel, Hermann Ney, 
A. Zubiaga, and H. Sawaf. 1997. Accelerated 
DP based Search for Statistical Translation. In 
Proceedings of European Conference on Speech 
Communication and Technology. 
1540
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1144?1153, Dublin, Ireland, August 23-29 2014.
A Lexicalized Reordering Model  
for Hierarchical Phrase-based Translation
1
 
 
Hailong Cao1, Dongdong Zhang2, Mu Li2, Ming Zhou2 and Tiejun Zhao1 
1Harbin Institute of Technology, Harbin, P.R. China 
2Microsoft Research Asia, Beijing, P.R. China 
{hailong, tjzhao}@mtlab.hit.edu.cn 
{Dongdong.Zhang, muli, mingzhou}@microsoft.com 
Abstract 
Lexicalized reordering model plays a central role in phrase-based statistical machine translation sys-
tems. The reordering model specifies the orientation for each phrase and calculates its probability con-
ditioned on the phrase. In this paper, we describe the necessity and the challenge of introducing such a 
reordering model for hierarchical phrase-based translation. To deal with the challenge, we propose a 
novel lexicalized reordering model which is built directly on synchronous rules. For each target phrase 
contained in a rule, we calculate its orientation probability conditioned on the rule. We test our model 
on both small and large scale data. On NIST machine translation test sets, our reordering model 
achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline 
hierarchical phrase-based system. 
1 Introduction 
In statistical machine translation, the problem of reordering source language into the word order of the 
target language remains a central research topic. Statistical phrase-based translation models (Och and 
Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the 
phrase, since the order is specified by phrasal translations. However, phrase-based models remain 
weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the 
phrases, two types of models have been developed. 
The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and 
Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Gal-
ley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical 
information. The model in (Koehn et al., 2007) distinguishes three orientations with respect to the pre-
vious and the next phrase?monotone (M), swap (S) and discontinuous (D). For example, we can ex-
tract a phrase pair ?xiayou ||| the lower reach of? whose orientations with respect to the previous and 
the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, 
and has become a standard component of phrase-based systems such as MOSES.  
 
 
Figure 1. Phrase orientations for Chinese-English translation. 
 
The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchro-
nous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) 
and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the rule. For 
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
1144
example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub 
phrase X before ?xiayou? should be put after ?the lower reach of?. 
One problem with the HPB model is that the application of a rule is independent of the actual sub 
phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X 
and ?xiayou?, no matter what is covered by X. This is an over-generalization problem. Much work has 
been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals 
by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich con-
text information for selecting translation rules during decoding. Huang et al. (2010) automatically in-
duce a set of latent syntactic categories to annotate nonterminals. These works alleviate the over-
generalization problem by considering the content of X. In this paper, we try to solve it from an alter-
native view by modeling whether the phrases covered by X prefer the order specified by the rule. This 
has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. 
We propose a novel lexicalized reordering model for hierarchical phrase-based translation and 
achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong HPB 
baseline system. 
2 Related work 
In this section, we briefly review two types of related work which are a nonterminal-based lexicalized 
reordering models and a path-based lexicalized reordering model. Both of them calculate the orienta-
tion for HPB translation. 
2.1 Nonterminal-based lexicalized reordering models 
Xiao et al. (2011) proposed an orientation model for HPB translation. The orientation probability of a 
derivation is calculated as the product of orientation probabilities of all nonterminals except the root.  
In order to define the relative orders of nonterminals and their adjacent phrase, they expand the align-
ment in a rule to include both terminals and nonterminals. There may be multiple ways to segment a 
rule into phrases; they use the maximum adjacent phrase similar to Galley and Manning (2008). They 
significantly outperformed the HPB system on both Chinese-English and German-English translation.  
Xiao et al. (2011) use the boundary word feature of nonterminals without considering their internal 
structure. For example, in Figure 1, suppose nonterminal X1 is not the root node and the orientation 
probability of X1 will condition on ?zhe, xiayou, this, river?.  
In this paper, we will consider how the words covered by the nonterminal X1 are reordered. Rather 
than using ?xiayou? as a feature to determine the orientation of X1 with respect to the next phrase, we 
think the immediately translated source word ?huanghe? could be more informative through it is not 
on the boundary of X1 , since ?huanghe? is the exact starting point from where we search for the next 
phrase to translate. 
Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The 
model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned 
training data for use with hierarchical phrase inventories, and scored orientations in hierarchical de-
coding.  
2.2 Path-based lexicalized reordering model 
The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discon-
tinuous phrase-based translation path in the following two steps: 
1) Represent each rule as a sequence of phrase pairs and non-terminals.  
2) The rules? sequences are used to find the corresponding phrase-based path of a HPB derivation 
and calculate the phrase-based reordering features. 
 
 
Figure 2. The phrase-based path of the derivation in Figure 1. 
1145
A phrase-based path is the sequence of phrase pairs, whose source sides covers the source sentences 
and whose target sides generated the target sentences from left to right. For example, the phrase-based 
path of the derivation in Figure 1 is shown in Figure 2. 
The phrase-based reordering features for the above phrase-based path are: 
 
>)is  thisshi, zhe|<(log DPnext ,                      >)of reachlower   thexiayou,|<(log DPprevious , 
>)of reachlower   thexiayou,|<(log SPnext , >)river yellow  thehuanghe,|<(log SPprevious . 
 
Nguyen and Vogel (2013) achieved significant improvement over both phrase-based and HPB models 
on three language pairs respectively.  
One problem with the above work is that they did not use rules with unaligned source or target 
phrases. Though this can get faster and better Arabic-English translation, it leads to a 0.49 BLEU point 
loss for Chinese-English translation. 
Another problem with path-based model is: there are many forms of HPB rules which we cannot 
map into a reasonable sequence of phrase pairs and non-terminals. We will show this with an example 
derivation shown in Figure 3. The main difference between Figure 3 and Figure 1 is there is such a 
rule <fangzhi X, prevent X from> that a source phrase ?fangzhi? is aligned with a discontinuous target 
phrase ?prevent?from?. This makes it hard to find the corresponding phrase-based path because we 
do not know what is the right order of ?fangzhi ||| prevent?from? and ?daozei ||| the thieves? in the 
discontinuous phrase-based path. We face the following dilemmas: 
 
? If ?fangzhi ||| prevent?from? goes first, then the discontinuous phrase-based path is as shown in 
Figure 4(a). On such a path, we will consider the orientation of ?the thieves? with respect to 
?breaking in?. This is unreasonable because ?the thieves? and ?breaking in? are not adjacent in the 
target side. It does not satisfy the definition of the phrase-based reordering model which predicts 
the orientation with respect to previous or next adjacent target phrase.  
? If ?daozei ||| the thieves? goes first, then the discontinuous phrase-based path is as shown in Figure 
4(b). This is unreasonable because ?The policeman? and ?the thieves? are not adjacent on the tar-
get side. 
 
 
Figure 3. Example of Chinese-English translation and its derivation. 
 
 
          
(a)                                                                                         (b) 
                  
 Figure 4. Two discontinuous phrase-based path candidates of the HPB derivation. 
 
From the above example, we can see that if a target phrase is aligned to a discontinuous target 
phrase in a HPB rule, then it is hard to find a reasonable path whose target sides can generate the tar-
get sentence from left to right.  
1146
3 Our lexicalized reordering model 
Rather than mapping a HPB derivation into a discontinuous phrase-based path and applying reordering 
model built on phrases, we propose a lexicalized reordering model which is built directly on HPB 
rules. For each target phrase contained in a HPB rule, we calculate its orientation probability condi-
tioned on the rule. For the example derivation in Figure 3, we represent it by the structure shown in the 
following figure: 
 
 
Figure 5.  Our representation of the HPB derivation in Figure 3.  
 
Different from Figure 4(a) and Figure 4(b) which contain a discontinuous phrase ?prevent?from?, we 
represent ?prevent?from? as two individual target phrases: ?prevent? and ?from?. Instead of consid-
ering the orientation of ?prevent?from?, we consider the orientation of ?prevent? and ?from? respec-
tively. For example, we will consider the orientation of ?prevent? with respect the previous phrase 
?the policeman? 
prevent)(previousO
, and the orientation of ?prevent? with respect the next phrase ?the 
thieves? prevent)(nextO . The probabilities of both prevent)(previousO
and prevent)(nextO are conditioned 
on the rule <fangzhi X, prevent X from>. 
In Figure 5, every two neighboring target phrases are adjacent in the original target side. In this way, 
we can borrow the phrase-based reordering model which calculates the orientation with respect to pre-
vious and next adjacent phrase.  
More formally, we represent a HPB rule in the general form of: 
 
??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr  
 
where n is the number of nonterminals. ...n,isi 1? , is the source phrase which is a continuous source 
word sequences. ...n,iti 1? , is the target phrase which is a continuous target word sequences. We use 
?  to represent the alignment of words and nonterminals in the rule. Note that is or it can be empty if 
there are adjacent nonterminals or there is nonterminal on the boundary. The lexicalized reordering 
probability of rule r is defined as the product of each target phrase?s orientation probabilities condi-
tioned on the rule r: 
 
)|)(()|)((
0
r,itOPr,itOP inextnext
n
i
ipreviousprevious?
?
 
 
In the above equation, each probability is conditioned on the whole rule. In this way, we avoid the 
problem of mapping a HPB derivation into a discontinuous phrase-based path. There are two ad-
vantages for our reordering model: 
? It is compatible with HPB rules which contain unaligned phrases. 
? It is compatible with HPB rules in which a source phrase is aligned to a discontinuous target 
phrase. 
Actually, our model is compatible with any kind of HPB rules since it is defined on the general 
form of rule. 
Now we describe how to define )( iprevious tO and )( inext tO in the model. Suppose it  contains ik target 
words and we write 
it as )()1-()2()1( ... ii kikiiii wwwwt ?
. Then we define: 
 
),()()( )1(1-)1()1( iiipreviousiprevious wwOwOtO ?? ,          ),()(( 1)()()( ??? iii kikikinextinext wwOwOtO
 
 
1147
where ),( 1?jj wwO is the orientation of two adjacent target words and is determined as follows: 
If ( )(1)( 1??? jj wlmwrm
 )           MwwO jj ?? ),( 1
; 
Else if (  )(1)( 1 jj wlmwrm ???
 )  SwO jj ?? ),( 1
; 
Else                                                         DwwO jj ?? ),( 1
; 
)(wrm is the position of the right most source word aligned to target word w; )(wlm is the position of 
the left most source word aligned to target word w. 
Above is our lexicalized reordering model which is built upon HPB rules. We complete its descrip-
tion using an example. For the rule <fangzhi X, prevent X from>, n=1, 
0 prevent? ??t  and 
1 from? ??t , the lexicalized reordering probability is: 
 
( (prevent)|<fangzhi X, prevent X from>,0 ) ( (prevent)|<fangzhi X, prevent X from> ,0)
( (from)| f i , r t  fr ,1) ( (from)|<fangzhi X,
?
? ?
previous previous next next
previous previous next next
P O P O
O P O  prevent X from> ,1)
 
 
Note that we calculate the orientation of plain phrase pairs in the same way as for HPB rules. We 
can represent a phrase pair in the form of ??? ?,, 00 tsr , which is a rule that does not contain any 
nonterminal. Then we can apply our above model which is general enough to cover both HPB rules 
and plain phrase pairs. 
4 Training and decoding 
The training of our model is similar to the reordering model of Moses. During the standard phrase pair 
extraction and rule extraction, besides the nonterminal alignment in rules, we also keep the lexical 
alignments and orientations. If a phrase pair or a rule is observed with more than one set of alignment, 
we only keep the most frequent one and only count the orientations corresponding to the most frequent 
alignment.  
Following Moses, we use relative frequency and add 0.5 smoothing technique to estimate the orien-
tation probability based on all samples collected from the training corpus. Generally, given a rule r 
with n target phrases, we estimated the reordering probability for each 
it as follows: 
 
0.5 # ( )( ( )| ) 1.5 #( )
? ???? ?
previous i
previous previous i
O t rP O t r, i r
,         0 5 ( ( ), )( ( ) | ) 1 5 ( )
?? ?
next inext next i
. # O t rP O t r, i . # r
 
 
For each parallel sentences pair, we add a start and an end mark on both sides. They are aligned re-
spectively. 
Our phrase pairs and rules are extracted from word aligned parallel sentences. There are many 
phrase pairs and rules which contain unaligned target or source words. How to deal with them is quite 
important for our reordering model. We will describe how to process them in the following two sub-
sections. 
4.1 The processing of unaligned target words 
Our main principle for processing an unaligned word is to: skip it and use the nearest aligned word. 
For example in Figure 3, the orientation of ?prevent? with respect to the next phrase is determined by: 
 
)  the(prevent,prevent)( OOnext ? 
 
If the target word ?the? is unaligned and ?thieves? is aligned with ?daozei?, we will define: 
 
(prevent) (prevent, the) (prevent, thieves)? ? ?nextO O O M 
 
Similarly, in Figure 1, the orientation of ?the lower reach of? with respect with ?the yellow river? is 
determined by O(of, the). Suppose both ?of? and ?the? are unaligned and there are alignments for 
?reach-xiayou? and ?yellow-huanghe?, we will have: 
1148
 SOO = yellow)(reach, = the)(of,  
 
We believe this orientation is consistent with our intuitions.  
More formally, before we determine the orientation of two adjacent target words ),( qp wwO ?we 
apply the following processing procedure: 
 
While (target word
pw is unaligned) p--; 
While (target word 
qw is unaligned) q++; 
 
If all words in a target phrase 
it  are unaligned, we do not need to consider its orientation since it  
does not trigger any movement along the source words at all. Actually, it will be skipped when we de-
termine the orientation of the previous and next aligned target phrases. (See also the decoding algo-
rithm in Section 4.3) 
4.2 The processing of unaligned source words 
The processing of Section 4.1 can guarantee that the orientation is determined based on two aligned 
target words, namely
pw and qw ,which must be continuous or separated by unaligned target words.  
Now we introduce the processing of unaligned source words. Before we determine the orientation 
of two target words ),( qp wwO ?we apply the following procedure to modify the position index of 
the left most source word aligned to 
pw and qw respectively: 
 
While (the 
th1)-)(( pwlm  source word is unaligned) --)( pwlm ; 
While (the thqwlm )1-)((  source word is unaligned) --)( qwlm ; 
 
For the example shown in the Figure 6, initially we have 1)( 1 ?wrm and 4)( 4 ?wlm . Since the 
source words 
3w  and 2w  are unaligned, our procedure will modify the value of )( 4wlm from 4 to 2. 
Finally, since )(1)( 41 wlmwrm ?? , the orientation of the two phrases marked by rectangular boxes in 
Figure 6 is: 
 
MwwOwwO ?? ),(),( 4132  
 
Again, we believe this result is consistent with our intuition. 
 
 
Figure 6. An example of phrases contain unaligned words 
 
Note that during decoding, both the unaligned source and target words are also processed in the 
same way as in the training step. This makes our lexicalized reordering model consistent. 
4.3 Decoding  
Now we introduce how to integrate our reordering model into the HPB system during the standard 
CYK bottom-up decoding.   
During decoding, if we just apply a plain phrase, we do not need to consider the orientation at once. 
It will be triggered when the phrase is used to compose a larger translation hypothesis together with 
other phrases or rules. 
We need to calculate the reordering features whenever we apply a HPB rule or a glue rule during 
the CYK decoding. Generally, given a rule ??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr  defined in 
section 3, we calculate the reordering probability for the span covered by r with algorithm 1. In the 
algorithm, LL(X) represents the lowest rule which covers the left most word of X; LR(X) is the lowest 
1149
rule which covers the right most word of X; Both LL(X) and LR(X) can be found by traversing the 
derivation tree top to down recursively. LI(r) is the index of the last target phrase of rule r.  
As in the example shown in Figure 3, for the rule r2=<X2 jinru, X2 breaking in>, the orientation of 
X2 and ?breaking in? is: 
 
(breaking in) (from, breaking)? ? ?previousO O O D 
 
The right most target word of X2 is ?from?, the lowest rule covering ?from? is r3=<fangzhi X4, prevent 
X4 from> and the index of the last target phrase of r3 is 1. So the reordering probability is: 
 
)1,0, 31 (D|rP)(D|rPprob nextprevious ??  
 
Note that, for readability, we use the product of probabilities to demonstrate the decoding process. 
Actually in practice, we use a linear model which sums the weighted log probabilities. 
 
prob=1; 
for (int i=1; i<=n; i++) 
{ 
   if (
1?it  is not empty and contains aligned words) 
   { 
            ;0,*
;1,*
;1
))(O|LL(XPprob
)i(O|rPprob
)(tOO
iprevious
next
inext
?
??
? ?
 
    } 
   if (
it  is not empty and contains aligned words) 
   { 
            
);|(*
;)(,*
);(
r,iOPprob
))LR(XLI)(O|LR(XPprob
tOO
previous
iinext
iprevious
?
?
?  
    } 
   else if (i<n)  
   {             
          //
iX  and 1?iX  are continuous 
          //or all words between them is unaligned  
      
1
??
??
??
 ( );
 ( );
the firs  phra
??
?
se of ;
( );
* ,LI( ))?
??
?
* ;,0)
;?
?
?
?
?
?
?
?
? ?
p i
q i
q
previous
next p p
previous q
rule r LR X
rule r LL X
t r
O O t
prob P (O | r r
prob P (O | r
i
 
    } 
} 
 
Algorithm 1. Calculating the reordering probability for a span covered by a rule:
??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr . 
 
As shown in Algorithm 1, the reordering probability depends on the lowest rules which cover the 
left/right most word. Therefore, we keep the lowest rules which cover the left/right most word for each 
partial translation. If two partial translations are same in everything but differ in the lowest rule, we 
need to keep both of them, rather than only keep the one with higher score. This will increase the 
complexity of the searching. 
4.4 Discussion 
Orientation can be determined based on word, phrase and hierarchical phrase (Galley and Manning, 
2008). What we adopt in this paper is word based orientation. It is based on the following considera-
tions: 
? Our baseline is a HPB system, which can capture hierarchical orientation. We use word based ori-
entation with the aim to complement the HPB system. 
? Word based orientation is consistent during training and decoding; phrase based orientation is 
prone to inconsistent between training and decoding.  
Galley and Manning (2008) has pointed out an inconsistency in Moses between training and decod-
ing. Here we would like to note that phrase based orientation depends on phrase segmentation. For 
example, in Figure 1, the orientation of phrase ?this is? with respect to next phrase could be either: 
? D, if we think the next phrase is ?the lower reach of ? which is what Figure 1 shows. 
? or S, if the next phrase is ?the lower reach of the yellow river? which can compose a legal phrase 
pair with ?huanghe xiayou? according to the standard phrase pair extraction algorithm.  
1150
The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who 
proposed a word-based reordering model for HPB system. The difference between our work and 
Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for 
the preprocessing approach, while we borrow the idea of lexicalized  reordering models which are 
originally proposed for phrase-based machine translation. 
5 Experiments 
5.1 Experimental settings 
Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 
2007). Besides the standard features of a HPB model, there are six reordering features in our reorder-
ing model which are M, S and D with respect to the previous and next phrase respectively. They are 
integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) 
(Och, 2003) algorithm is adopted to tune feature weights for translation systems. 
We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 
was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and 
MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test 
the effect of our method on a large scale parallel training corpus. 
Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default 
setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gi-
gaword Version 3.0 and the English part of the bilingual training data. Translation performances are 
measured with case-insensitive BLEU4 score (Papineni et al., 2002). 
5.2 Experimental results on FBIS corpus 
We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline 
and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in 
table 1. 
 
 #sentences #words 
Chinese 128832 3016570 
English 128832 3922816 
Table 1. The statistics of FBIS corpus 
 
Table 2 summarizes the translation performance. The first row shows the results of baseline HPB 
system, and the second row shows the results when we integrated our lexicalized reordering model 
(LRM). We get 1.2, 0.8 and 0.7 BLEU point improvements over the baseline HPB system on three test 
sets respectively. 
 
 MT04 MT05 MT08 
HPB 33.53 32.97 25.08 
HPB+LRM 34.71 33.77 25.84 
Table 2. Translation performance on the FBIS corpus. 
5.3 Experimental results on large scale  corpus 
To further test the effect of our reordering model, we use a large scale corpus released by LDC. The 
catalog number of them is LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, 
LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. There are 498K sentence pairs, 12.1M 
Chinese words and 13.8M English words. Table 3 summarizes the translation performance on the 
large scale of corpus.  
 
 MT04 MT05 MT08 
HPB 38.72 37.59 29.03 
HPB+LRM 39.81 38.24 29.63 
Table 3. Translation performance on a large scale parallel corpus. 
1151
Our model is still effective when we train the translation system on large scale data. We get 1.1, 0.7 
and 0.6 BLEU point improvements over the baseline HPB system on three test sets respectively. 
6 Conclusion and future work 
We proposed a novel lexicalized reordering model for hierarchical phrase based machine translation. 
The model is compatible with any kind of HPB rules no matter how complex the alignments are. We 
tested our reordering model on both small and large scale data. On NIST machine translation test sets, 
our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation 
over a strong baseline hierarchical phrase-based system. 
In future work, we will further test our model on other language pairs and compare it with other re-
ordering models for HPB translation. 
Acknowledgments 
We thank anonymous reviewers for insightful comments. The work of Hailong Cao is sponsored by 
Microsoft Research Asia Star Track Visiting Young Faculty Program. The work of HIT is also funded 
by the project of National Natural Science Foundation of China (No. 61173073) and International Sci-
ence & Technology Cooperation Program of China (No. 2014DFA11350). 
Reference 
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion Models for Statistical Machine Translation. In Pro-
ceedings of ACL. 
Colin Cherry, Robert C. Moore and Chris Quirk. 2012. On Hierarchical Re-ordering and Permutation Parsing for 
Phrase-based Decoding. In Proceedings of  NAACL Workshop on SMT. 
David Chiang. 2007. Hierarchical Phrase-based Translation. Computational Linguistics, 33(2):201?228. 
Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Mod-
el. In Proceedings of EMNLP. 
Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh, Kevin Duh and Seiichi Yamamoto. 2010. Hierarchical 
Phrase-based Machine Translation with Word-based Reordering Model. In Proceedings of COLING. 
Zhongjun He, Qun Liu, Shouxun Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule 
Selection. In Proceedings of COLING. 
Liang Huang, Hao Zhang and Daniel Gildea. 2005. Machine Translation as Lexicalized Parsing with Hooks. In 
Proceedings of IWPT. 
Zhongqiang Huang, Martin ?mejrek, and Bowen Zhou. 2010. Soft Syntactic Constraints for Hierarchical Phrase-
based Translation Using Latent Syntactic Distributions. In Proceedings of EMNLP.  
Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013. A Phrase Orientation Model for Hierar-
chical Machine Translation. In Proceedings of ACL Workshop on SMT.  
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola 
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst.. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 
demonstration session. 
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto and Kazuteru Ohashi. 2006. A Clustered Global Phrase 
Reordering Model for Statistical Machine Translation. In Proceedings of ACL. 
Thuylinh Nguyen and Stephan Vogel. 2013. Integrating Phrase-based Reordering Features into Chart-based De-
coder for Machine Translation. In Proceedings of ACL. 
Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL. 
Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. 
Computational Linguistics, 30(4):417?449. 
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL. 
1152
 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In Proceedings of ACL. 
Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of 
HLT-NAACL. 
Roy Tromble, Jason Eisner. 2009. Learning Linear Ordering Problems for Better Translation. In Proceedings of 
EMNLP. 
Xinyan Xiao, Jinsong Su, Yang Liu, Qun Liu, and Shouxun Lin. 2011. An Orientation Model for Hierarchical 
Phrase-based Translation. In Proceedings of IALP.  
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statisti-
cal Machine Translation. In Proceedings of ACL. 
Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. 
In Proceedings of Workshop on SMT. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In 
Proceedings of NAACL Workshop on SMT. 
1153
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2227?2236, Dublin, Ireland, August 23-29 2014.
 Soft Dependency Matching                                                                               
for Hierarchical Phrase-based Machine Translation
1
 
Hailong Cao1, Dongdong Zhang2, Ming Zhou2 and Tiejun Zhao1 
1Harbin Institute of Technology, Harbin, P.R. China 
2Microsoft Research Asia, Beijing, P.R. China 
{hailong, tjzhao}@mtlab.hit.edu.cn 
{Dongdong.Zhang, mingzhou}@microsoft.com 
Abstract 
This paper proposes a soft dependency matching model for hierarchical phrase-based (HPB) machine 
translation. When a HPB rule is extracted, we enrich it with dependency knowledge automatically learnt 
from the training data. The dependency knowledge not only encodes the dependency relations between 
the components inside the rule, but also contains the dependency relations between the rule and its con-
text. When a rule is applied to translate a sentence, the dependency knowledge is used to compute the 
syntactic structural consistency of the rule against the dependency tree of the sentence. We characterize 
the structure consistency by three features and integrate them into the standard SMT log-linear model to 
guide the translation process. Our method is evaluated on multiple Chinese-to-English machine transla-
tion test sets. The experimental results show that our soft matching model achieves 0.7-1.4 BLEU points 
improvements over a strong baseline of an in-house implemented HPB translation system. 
1 Introduction 
HPB model (Chiang, 2007) is widely used and has consistently delivered state-of-the-art performance. 
This model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous 
grammar to well capture the recursiveness of language during translation. In a formal synchronous 
grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which 
may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of 
translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. 
To generate grammatical translations, lots of syntax-based models have been proposed by Galley et 
al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), 
Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic 
structure of either the source sentence or the target sentence. These approaches can generate more 
grammatical translations by capturing the structural difference between language pairs. However, 
these models need special efforts to capture non-syntactic translation knowledge to improve the trans-
lation performance.  
It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 
2010). There has been much work trying to improve HPB model by incorporating syntax information. 
Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work 
go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntac-
tic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 
2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X 
could be refined into NP or PP as shown in rules (5-8) respectively.  
 
(1) <? ? X, borrowed X>                  (2) <? ? X, lent X>  
(3) <X1 ? ? X2, borrowed X2 X1>     (4) <X1 ? ? X2, X1 borrowed X2>  
 
(5) <?? NP, borrowed X>                  (6) <?? NP, lent X> 
(7) <PP ?? NP, borrow X2 X1>          (8) <NP ?? NP, X1 lent X2> 
 
Although augmenting the non-terminals with syntactic tags in these methods achieved better results 
for HPB model, they have limitations that the syntax information on the non-terminals are not discrim-
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
2227
inative enough due to the limited context covered by the HPB rule. For example, rule (5) and (6) are 
still not discriminative when translating below two sentences (9) and (10). 
 
(9) ????????(I borrowed a book from him)    (10) ????????(I lent a book to him) 
 
where the common phrase ??????? appear in both sentences. Obviously, although rule (5) and 
(6) share same source sides, rule (5) can only be applied to the translation of sentence (9) and rule (6) 
to sentence (10). Otherwise, inappropriate application will lead to wrong translations. Rule (5) and (6) 
are not discriminative due to no consideration of their outside context during the translation.  
Motivated by such observation, we proposed an alternative approach, called soft dependency 
matching model, to incorporate into each HPB rule the source syntactic dependencies connecting the 
contents inside the rule with the context outside the rule. The dependency knowledge associated with 
HPB rules is automatically learnt from bilingual training corpus. They make HPB rules discriminative 
according to global context.  
 
 
Figure 1.  Dependency information associated with two rules. LC and RC mean the source context on 
the left and right of the rule respectively. 
 
Figure 1 shows two rules associated with different dependencies. The first one is applicable to the case 
when some word on the left side depends on the word ??? in the rule, and the second one is applica-
ble to the case when the word ??? in the rule depends on some word on the right side. 
During SMT decoding, first we parse the source sentence to get the dependency tree. When a HPB 
rule is applied to translate the sentence, we calculate structural consistency between the dependency 
knowledge associated with the rule and dependency tree structure of the source sentence. The con-
sistency degree is integrated into the SMT log-linear model as features to encourage syntactic hypoth-
eses and penalize the hypotheses violating syntactic constraints. 
Compared with previous work that incorporate syntax knowledge into HPB model, the advantage of 
our soft dependency matching model is: 
? It not only captures the dependency relations between the components inside the rule, but also 
models the dependency relations between the rule and its context from a global view. 
? Without increasing the amount of rules or the searching space, our model can capture the syntactic 
variation for all of the rules (syntactic or non-syntactic, well-formed or ill-formed). 
? Our model can take advantage of the dependency knowledge on both terminals and non-terminals.  
We evaluate the performance of our soft dependency matching model on Chinese-to-English trans-
lation task. Experimental results show that our method can achieve the improvements of 0.7-1.4 
BLEU points over the baseline HPB model on multiple NIST MT evaluation test sets. 
2 Related Work 
Ever since the invention of phrase-based model, a lot of efforts have been made to incorporate linguis-
tic syntax. Cherry(2008) and Marton and Resnik (2008) leverage linguistic constituent to constrain the 
decoding softly. In their methods, a translation hypothesis gets an extra credit if it respects the parse 
tree but may incur a cost if it violates a constituent boundary. The soft constrain based methods 
achieved promising results on various language pairs. One problem of these methods is that exactly 
matching syntactic constraints cannot always guarantee a good translation, and violating syntactic 
structure does not always induce a poor translation. It could be more reasonable if the credit and penal-
ty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge di-
rectly from the syntactic structures over the training corpus.  
Xiong et al. (2009) present a method that automatically learns syntactic constraints from training 
data for the ITG based translation (Wu, 1997; Xiong et al., 2006).  They utilize the syntactic con-
straints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on 
the ITG based model, the method is also applicable to the HPB model. The main difference between 
Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 
2228
against the source syntax tree. For rules which are same in the source side but different in the target 
side, our method will distinguish the inconsistency degree for different rules. While, for such rules, 
Xiong et al. (2009) will give a same score which will be used to compete with rules in other spans. 
More recently, Huang et al. (2013) associate each non-terminal with the distribution of tags that is 
used to measure the consistency of syntactic compatibility of the translation rule on source spans. Our 
work is similar to Huang et al. (2013) since we also represent the syntactic variation of translation 
rules in the form of distribution. The main difference is that they annotate non-terminals with head 
POS tags while we use dependency triples (over both terminals and non-terminals) to explicitly repre-
sent both the dependency relations inside the rule, and that between the rule and its context. 
Both above related work and our work need parse the source sentence to get syntactic context be-
fore decoding. There are also some methods incorporating syntax information without the need of 
online parsing the source sentences (Zollmann and Venugopal, 2006; Shen et al, 2009; Chiang, 2010). 
They parse the training data to label the non-terminals with syntactic tags. During the bottom-up de-
coding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; 
Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006).  
Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. 
They focus on the relative order of each dependent word and its head word after translation, while our 
method models whether the dependency information of a rule matches the context or not.  
Our work utilizes contextual information around translation rules. In this sense, it is similar to He et 
al. (2008) and Liu et al. (2008). The main difference between their work and our work is that they lev-
erage lexical context for rule selection while we focus on the syntactic contextual information. 
3 Hierarchical Phrase based Machine Translation 
Our model proposed in this paper is an extension of the HPB model (Chiang, 2007). Formally, HPB 
model is a weighted synchronous context free grammar. It employs a generalization of the standard 
plain phrase extraction approach in order to acquire the synchronous rules of the grammar directly 
from word-aligned parallel text. Rules have the form of: 
          
where X is a nonterminal,   and   are both strings of terminals and non-terminals from source and tar-
get side respectively, and ? is a one-to-one correspondence between nonterminal occurrences in   and 
 . Associated with each rule is a set of feature functions with the form        . These feature functions 
are combined into a log-linear model. When a rule is applied during SMT decoding, its score is calcu-
lated as: 
?          
 
 
where    is the weight associated with feature function         . The feature weights are typically op-
timized using minimum error rate training algorithm (Och, 2003).  
4 Soft Dependency Matching Model 
In order to incorporate syntactic knowledge to refine both the word ordering and word sense disam-
biguation for HPB model, we propose a soft dependency matching model (SDMM). It extends HPB 
rule into a form which is named as SDMM rule: 
 
              
 
where RDT(rule?s dependency triples) is a set of dependency triples defined on source string   . Each 
element in RDT is a triple representing dependency knowledge in the form: 
 
{m-h-l} 
 
where m and h are the dependent and head respectively, l is the label of the dependency relation type. 
m and h could be any of terminals, non-terminals, LC and RC, where LC denotes the left context and 
RC the right context. 
In the following two sub-sections, we will explain the details of SDMM rule extensions for both 
plain phrases (i.e., there are no non-terminals in both         ) and hierarchical rules (i.e., there are at 
2229
least one non-terminal in both         )  respectively. For simplicity, we ignore the correspondence   
in the representations of both HPB rules and SDMM rules. 
 
 
Figure 2: An illustration of a dependency parse tree for the source side of a word-aligned parallel sen-
tences pair. 
4.1 SDMM Over Plain Phrase Rules 
Figure 2 illustrates a parallel sentence together with word alignments and source dependency parse 
tree, from which we can extract the phrase pairs of HPB rules like: 
 
(11) < ? ? ?, a book >        (12) < ? ? ? ? ?, borrowed a book > 
 
By incorporating syntactic knowledge, we can extend these HPB rules into SDMM rules as shown 
in Figure 3(a) and Figure 3(b) respectively.  
 
Figure 3: An illustration of two phrase pairs annotated with a set of dependency triples. 
 
Formally, the RDT corresponding to phrase pair (11) is {?-LC-dobj}. The RDT corresponding to 
phrase pair (12) is {LC-?-nsubj, LC-?-prep}. 
Now we describe how to build the RDT when a phrase pair is extracted from a sentence pair during 
the training step. First, we initialize RDT to be empty. Then, for each dependency triple ?m-h-l? in the 
parse tree of the source sentence, if either m or h is covered by the source phrase in the rule, we add it 
to RDT. However, if both m and h are covered by the source phrase, we will ignore it because it holds 
less syntactic information beyond HPB rule itself. For example, the dependency triple ?? -? -
nummod? is excluded from RDT for both phrase pair (11) and phrase pair (12). In addition, we do not 
add the dependency triple ?m-h-l? into RDT if both m and h are not contained in source phrase, be-
cause it is not related to phrase pair at all. The dependency triple ??-?-pobj? is such a case for both 
phrase pair (11) and phrase pair (12).  
Finally, we normalize the word in RDT that is not covered by the source phrase with either LC 
(stands for the left context) or RC (stands for the right context) according to its relative position to the 
source phrase. For example, in the RDT for phrase pair (11), we normalize ??-?-dobj? as ??-LC-
dobj? since the word??? is not covered by the source phrase and it is treated as left context.  
Note that for each context word outside the source phrase, we only record whether it is on the left or 
on the right of phrase. We do not further consider its lexical form and its distance to the source phrase. 
For example, in the two dependency triples in Figure 3(b), both the dependent word ??? and ??? are 
normalized into LC. In this way, we can generalize the dependency triples in RDT and alleviate the 
data sparseness problem. In fact, there might be duplicated dependency triples for a phrase pair. In this 
case, we only keep one of them. 
4.2 SDMM over Hierarchical Rules 
Hierarchical rules are usually generated by substituting sub-phrases with non-terminals from plain 
phrase pairs. For example, given the parallel sentence and the two phrase pairs in Section 4.1, we can 
get a hierarchical rule like:  
<? ? X, borrowed X> 
To extend hierarchical rules into SDMM rules, we add dependency information to source terminals 
or non-terminals in RDT. Figure 4 shows an example representing an SDMM rule: 
2230
 
Figure 4: An illustration of a hierarchical rule annotated with a set of dependency triples. 
 
The generation of SDMM rules over hierarchical rules is similar to that of plain phrase rules. The only 
difference lies in processing the non-terminals, whose dependencies are inferred from the words they 
covered. For example, the RDT of the above SDMM rule would be:{LC-?-nsubj, LC-?-prep, X-?-
dobj} 
Similarly, any dependencies over two terminals contained in the source rule are not included in 
RDT, and dependencies inferred from same non-terminals are excluded as well. In addition, depend-
encies between two non-terminals are ignored.   
4.3 SDMM Rule Composing 
A same HPB rule (either plain phrase pair or a hierarchical rule) can be extracted from different bilin-
gual sentences. Therefore, the same HPB rule could be extended into multiple SDMM rules. For ex-
ample, given a parallel sentence pair shown in Figure 5, 
 
 
Figure 5: An example of a dependency tree over the source sentence together with the word-aligned 
target sentence. 
 
we might get a SDMM rule as shown in Figure 6. Compared to the SDMM rule in Figure 4, there is an 
additional dependency triple ?LC-?-tmod? in RDT. 
 
 
Figure 6: An illustration of dependency triples associated to a hierarchical rule. 
 
Intuitively, we can process SDMM rules independently although they share the same information of 
HPB rules. However, this will exacerbate the data sparseness problem and make the computation inef-
ficient due to dramatically increased model size. An alternative way is only to keep the most frequent 
RDT information for the same HPB rules. Though this can get a very concise model, a lot of useful 
syntactic information might be lost. 
We propose a balanced composing method to make a trade-off between knowledge representation 
and computation efficiency of SDMM rules. Suppose there are more than one SDMM rules with dif-
ferent      but the same HPB rule, we compose them by the union and get the new form of RDT as: 
 
    ?    
 
 
 
In addition, we record the frequency of HPB rule as well as that of each dependency triple in RDT 
as: 
             ,                 
 
where              is the number of times that HPB rule           is extracted from the 
training data, and                 is the frequency that    and           co-occur. For ex-
ample, suppose SDMM rules in Figure 4 and Figure 6 occurs 9 and 1 times respectively, we can com-
pose them into the form as shown in Figure 7.  
 
2231
 
Figure 7: Composed form of the dependency annotation of a rule. The integers following the colons 
denote occurring times. 
 
Therefore, the composed SDMM rule will be represented by the original HPB rule <? ? X, bor-
rowed X> together with RDT and its frequency information shown in Table 1. 
 
RDT # 
{ LC-?-tmod, 
   LC-?-nsubj, 
   LC-?-prep, 
    X - ?-dobj } 
1 
10 
10 
10 
Table 1. The RDT and its frequency information of a composed SDMM rule. 
4.4 Consistency of SDMM Rules 
So far we have described how to enrich a rule with RDT in the training step. Now we introduce how to 
use the RDT of each rule to guide the translation process. 
In the decoding, we parse the source sentence to get the dependency parse tree as shown in Figure 8. 
When we apply a rule to get a partial translation for a span, we also extract a set of dependency triples 
based on the parse tree in the exact same way that is used in the training step. We denote this by CDT 
(context dependency triples). Suppose the rule <? ? X, borrowed X> is applied to translate the un-
derlined span in Figure 8, then the CDT for the rule is: {LC-?-nsubj, LC-?- dep, X-?-dobj}. 
 
 
Figure 8: A sentence to be translated and its dependency parse tree. 
 
In order to evaluate whether a SDMM rule is applicable to translate a sentence or not from the syn-
tactic view, we model the structural consistency of SDMM rule against source dependency tree by cal-
culating the matching degree between RDT and CDT. The example in Figure 9 illustrates how we 
compute the matching degree between the SDMM rule in Figure 7 and CDT over the source depend-
ency tree in Figure 8. We estimate the matching degree based on three sets including the relative com-
plement set of CDT in RDT, the intersection set of RDT and CDT, and the relative complement set of 
RDT in CDT. 
 
 
Figure 9: Three different sets of dependency triples to model the structural consistency of syntactic 
matching. 
 
The statistics over above three sets are leveraged to design three features which are incorporated into 
SMT log-linear model to encourage and penalize various syntactic motivated hypotheses. The first 
feature is called as the lost dependency triple feature   . It is calculated based on the set RDT\CDT as: 
 
   ?                   
         
              
2232
 where   is the indicator function whose value is one if and only if the condition is true, otherwise its 
value is zero. The motivation of     is that: if a dependency triple which always co-occur with the HPB 
rule is not observed in CDT, it indicates the current SDMM rule may mismatch with the source sen-
tence and therefore we need to penalize its application. In Figure 9, ?LC-?-prep? is such a dependen-
cy triple. However, for the less frequent dependency triples in RDT such as ?LC-?-tmod? in Figure 8, 
there is no penalty on it although it is not found in CDT.  
The second feature is the unexpected dependency triple feature   , which is computed as :   
   |       | 
 
This feature is the number of dependency triples in CDT that never co-occur with the rule in the train-
ing data. In Figure 9, ?LC-?-dep? is such a case. Intuitively, the higher the value    is, the higher in-
consistency degree is, because it means that many dependency triples in CDT are never observed in 
the training corpus. We should discourage the application of the corresponding SDMM rule.  
The third feature is the matched dependency triple feature  
 
 which is calculated based on 
RDT?CDT. It is directly used to model the structural consistency over all the dependency triples in 
RDT?CDT for the application of HPB rule          . Formally,  
 
 is defined as the sum of log 
probability of each dependency triple in RDT?CDT conditioned on the HPB rule: 
 
   ?        |           
         
 
 
where    |           is the probability of a dependency triple  associated to a HPB rule    
       . We estimate it based on the relative frequency and experimentally use the adding 0.5 
smoothing. 
5 Experiments 
5.1 Experimental Settings 
Our baseline is the re-implementation of the Hiero system (Chiang, 2007). When our soft dependency 
matching model is integrated, the HPB rule is extended into the form of  
              and the score is calculated by: 
 
?           
 
                                              
 
where the additional three features are defined in Section 4.3,   ,    and    are corresponding feature 
weights. 
We test our soft dependency matching model on a Chinese-English translation task. The NIST06 
evaluation data was used as our development set to tune the feature weights, and NIST04, NIST05 and 
NIST08 evaluation data are our test sets. We first conduct experiments by using the FBIS parallel cor-
pus, and then further test the performance of our method on a large scale training corpus. 
Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default set-
ting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 
and the English part of the bilingual training data. Feature weights are tuned with the minimum error 
rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 
score (Papineni et al., 2002). 
All the Chinese sentences in the training set, development set and test set are parsed by an in-house 
developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 
named grammatical relations plus a default relation representing unknown cases. The detailed descrip-
tions about dependency parsing are explained in Chang et al. (2009). 
5.2 Experimental Results on FBIS Corpus 
We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline 
and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus after the pre-
processing. 
 
2233
  #sentences #words 
Chinese 128,832 3,016,570 
English 128,832 3,922,816 
Table 2. The statistics of FBIS corpus 
 
The evaluation results over FBIS corpus are reported in Table 3. The first row shows the results of 
baseline, the next three rows show the effect of three features respectively and the last row gives the 
result when all features are integrated together. Based on Table 3, we can see that each individual fea-
ture improves the performance. Among all integrated features, the third feature  
 
 is the most effec-
tive one. The best performance is achieved when using all three features, where we get 1.4, 0.9 and 1.2 
BLEU points improvements respectively over the baseline on three test sets. 
 
 NIST04 NIST05 NIST08 
Baseline 33.53 32.97 25.08 
Baseline+fl 34.59 33.44 25.69 
Baseline+fu 34.48 33.59 25.51 
Baseline+fm 34.73 33.74 25.76 
Baseline+fl+fu+fm 34.96 33.91 26.28 
Table 3. Translation performance over BLEU% when models are trained on the FBIS corpus. 
5.3 Experimental Results on Large Scale  Corpus 
To further test the effect of our soft dependency matching model, we use a large scale corpus released 
by LDC. The catalog number of them is LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, 
LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. There are 498K sen-
tence pairs, 12.1M Chinese words and 13.8M English words. Table 4 summarizes the translation per-
formance on the large scale of corpus. Our model is still effective when we train the translation system 
on large scale data. We get 1.3, 0.7 and 1.0 BLEU point improvements over the baseline on three test 
sets respectively, which shows that our method can consistently improve HPB system over different 
sized training corpus. 
 
 NIST04 NIST05 NIST08 
Baseline 38.72 37.59 29.03 
Baseline+fl+fu+fm 40.00 38.34 30.06 
Table 4. Translation performance over BLEU% when models are trained on a large scale parallel 
 corpus. 
5.4 Decoding Cost 
Incorporating syntax can improve the translation performance, but it might increase the SMT decoding 
complexity. One advantage of our method is that it does not increase the amount of translation rules, 
so the searching space is not enlarged. Table 5 shows the decoding time comparison with the baseline 
when models are trained on the FBIS corpus. The average decoding time per sentence is only in-
creased by about 12% due to the parsing of source sentences and the computation of the features. We 
believe that this is acceptable given the performance gain. 
 
 NIST04 NIST05 NIST08 
Baseline 0.67sec 0.78sec 0.50sec 
Baseline+fl+fu+fm 0.88sec 0.87sec 0.56sec 
Table 5.  The average decoding time per sentence, measured in second/sentence. 
6 Conclusion and Future Work 
We proposed a soft dependency matching model for HPB machine translation. We enrich the HPB 
rule with dependency knowledge learnt from the training data. The dependency knowledge allows our 
model to capture the both the dependency relations inside the rule and the dependency relations be-
tween the rule and its context from a global view. During decoding, the syntax structural consistency 
of rules against source dependency tree is calculated and converted into SMT log-linear model fea-
2234
tures to guide the translation process. The experimental results show that our soft matching model 
achieves significant improvements over a strong baseline of an in-house implemented HPB system. 
In future work, there is much room to improve the performance via our method. First, we can dis-
criminatively learn the contribution of the dependency knowledge of each rule based on the training 
data. Second, we can go beyond the current ?bag of dependency triples? representation by composing 
them hierarchically to capture deep syntactic information. Third, section 2 has discussed the theoreti-
cal difference with related work on adding source syntax into the HPB model, we are interested in 
empirically comparing our method with them and combining it with them to get further improvement. 
Acknowledgments 
We thank anonymous reviewers for insightful comments. The work of Hailong Cao is sponsored by 
Microsoft Research Asia Star Track Visiting Young Faculty Program. The work of HIT is also funded 
by the project of National Natural Science Foundation of China (No. 61173073) and International Sci-
ence & Technology Cooperation Program of China (No. 2014DFA11350). 
Reference 
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning.  2009. Discriminative Reordering 
with Chinese Grammatical Relations Features. In Proceedings of NAACL Workshop on SSST. 
Colin Cherry. 2008. Cohesive Phrase-based Decoding for Statistical Machine Translation. In Proceedings of 
ACL. 
David Chiang. 2007. Hierarchical Phrase-based Translation. Computational Linguistics, 33(2):201?228. 
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011. Soft Dependency Constraints for Reordering in Hierar-
chical Phrase-Based Translation. In Proceedings of EMNLP.  
Zhongjun He, Qun Liu, Shouxun Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule 
Selection. In Proceedings of COLING. 
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended-
domain of locality. In Proceedings of AMTA. 
Zhongqiang Huang, Martin ?mejrek, and Bowen Zhou. 2010. Soft Syntactic Constraints for Hierarchical Phrase-
based Translation Using Latent Syntactic Distributions. In Proceedings of EMNLP. 
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored Soft Syntactic Contraints for Hierarchical 
Machine Translation. In Proceedings of EMNLP.  
Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL. 
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL. 
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. Statistical phrase based translation. In Proceedings of 
NAACL. 
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van Genabith. 2012. Using Syntactic Head Information in 
Hierarchical Phrase-based Translation. In Proceedings of WMT. 
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree to-string alignment template for statistical machine translation. 
In Proceedings of ACL. 
Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In 
Proceedings of ACL.  
Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP.  
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In Proceedings of ACL. 
 Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm 
with a target dependency language model. In Proceedings of ACL. 
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic 
and contextual information for statistical machine translation. In Proceedings of EMNLP.  
2235
Daniel Stein, Stephan Peitz, David Vilar, and Hermann Ney. 2010. A Cocktail of Deep Syntactic Features for 
Hierarchical Machine Translation. In Conference of the Association for Machine Translation in the Americas. 
Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?404. 
Jun Xie, Haitao Mi and Qun Liu. 2011. A Novel Dependency-to-String Model for Statistical Machine Transla-
tion. In Proceedings of EMNLP. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statisti-
cal Machine Translation. In Proceedings of ACL. 
Deyi Xiong, Min Zhang, Aiti Aw, Haizhou Li. 2009. A Syntax-Driven Bracketing Model for Phrase-Based 
Translation.  In Proceedings of ACL. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In 
Proceedings of NAACL Workshop on SMT.  
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence align-
ment-based tree-to-tree translation model. In Proceedings of ACL. 
Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features In Pro-
ceedings of ACL. 
2236
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 402?411, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Locally Training the Log-Linear Model for SMT
Lemao Liu1, Hailong Cao1, Taro Watanabe2, Tiejun Zhao1, Mo Yu1, CongHui Zhu1
1School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{lmliu,hailong,tjzhao,yumo,chzhu}@mtlab.hit.edu.cn
taro.watanabe@nict.go.jp
Abstract
In statistical machine translation, minimum
error rate training (MERT) is a standard
method for tuning a single weight with regard
to a given development data. However, due to
the diversity and uneven distribution of source
sentences, there are two problems suffered by
this method. First, its performance is highly
dependent on the choice of a development set,
which may lead to an unstable performance
for testing. Second, translations become in-
consistent at the sentence level since tuning is
performed globally on a document level. In
this paper, we propose a novel local training
method to address these two problems. Un-
like a global training method, such as MERT,
in which a single weight is learned and used
for all the input sentences, we perform training
and testing in one step by learning a sentence-
wise weight for each input sentence. We pro-
pose efficient incremental training methods to
put the local training into practice. In NIST
Chinese-to-English translation tasks, our lo-
cal training method significantly outperforms
MERT with the maximal improvements up to
2.0 BLEU points, meanwhile its efficiency is
comparable to that of the global method.
1 Introduction
Och and Ney (2002) introduced the log-linear model
for statistical machine translation (SMT), in which
translation is considered as the following optimiza-
tion problem:
e?(f ;W ) = arg max
e
P(e|f ;W )
= arg max
e
exp
{
W ? h(f, e)
}
?
e? exp
{
W ? h(f, e?)
}
= arg max
e
{
W ? h(f, e)
}
, (1)
where f and e (e?) are source and target sentences,
respectively. h is a feature vector which is scaled
by a weight W . Parameter estimation is one of
the most important components in SMT, and var-
ious training methods have been proposed to tune
W . Some methods are based on likelihood (Och and
Ney, 2002; Blunsom et al2008), error rate (Och,
2003; Zhao and Chen, 2009; Pauls et al2009; Gal-
ley and Quirk, 2011), margin (Watanabe et al2007;
Chiang et al2008) and ranking (Hopkins and May,
2011), and among which minimum error rate train-
ing (MERT) (Och, 2003) is the most popular one.
All these training methods follow the same
pipeline: they train only a single weight on a given
development set, and then use it to translate all the
sentences in a test set. We call them a global train-
ing method. One of its advantages is that it allows us
to train a single weight offline and thereby it is effi-
cient. However, due to the diversity and uneven dis-
tribution of source sentences(Li et al2010), there
are some shortcomings in this pipeline.
Firstly, on the document level, the performance of
these methods is dependent on the choice of a devel-
opment set, which may potentially lead to an unsta-
ble translation performance for testing. As referred
in our experiment, the BLEU points on NIST08 are
402
 Source  Candidate Translation   
i  
i
f  j  
ij
e  h  score  
1 ? ? ?? ? 1 I am students . <2, 1> 0.5 
  2 I was students . <1,1> 0.2 
2 ?? ?? ? ? 1 week several today ? <1,2> 0.3 
  2 today several weeks . <3,2> 0.1 
 
(a) (b)
2 21 2 222,0 ( , ) ( , )h f e h f e? ? ?? ?
2 22 2 212,0 ( , ) ( , )h f e h f e? ?? ?1 11 1 11, 0 ( , ) ( , )h f e h f e? ?? ?
1 12 1 111,0 ( , ) ( , )h f e h f e? ? ?? ?
2 22 2 21( , ) ( , )h f e h f e?
1 11 1 12( , ) ( , )h f e h f e?
<-2,0>
<-1,0>
<1,0>
<2,0>
0h1h
. .* *
2 21 2 22( , ) ( , )h f e h f e?
1 12 1 11( , ) ( , )h f e h f e?
Figure 1: (a). An Example candidate space of dimensionality two. score is a evaluation metric of e. (b). The non-
linearly separable classification problem transformed from (a) via tuning as ranking (Hopkins and May, 2011). Since
score of e11 is greater than that of e12, ?1, 0? corresponds to a possitive example denoted as ???, and ??1, 0? corre-
sponds to a negative example denoted as ?*?. Since the transformed classification problem is not linearly separable,
there does not exist a single weight which can obtain e11 and e21 as translation results meanwhile. However, one can
obtain e11 and e21 with weights: ?1, 1? and ??1, 1?, respectively.
19.04 when the Moses system is tuned on NIST02
by MERT. However, its performance is improved to
21.28 points when tuned on NIST06. The automatic
selection of a development set may partially address
the problem. However it is inefficient since tuning
requires iteratively decoding an entire development
set, which is impractical for an online service.
Secondly, translation becomes inconsistent on the
sentence level (Ma et al2011). Global training
method such as MERT tries to optimize the weight
towards the best performance for the whole set, and
it can not necessarily always obtain good translation
for every sentence in the development set. The rea-
son is that different sentences may need different
optimal weights, and MERT can not find a single
weight to satisfy all of the sentences. Figure 1(a)
shows such an example, in which a development set
contains two sentences f1 and f2 with translations e
and feature vectors h. When we tune examples in
Figure 1(a) by MERT, it can be regarded as a non-
linearly separable classification problem illustrated
in Figure 1(b). Therefore, there exists no single
weightW which simultaneously obtains e11 and e21
as translation for f1 and f2 via Equation (1). How-
ever, we can achieve this with two weights: ?1, 1?
for f1 and ??1, 1? for f2.
In this paper, inspired by KNN-SVM (Zhang et
al., 2006), we propose a local training method,
which trains sentence-wise weights instead of a sin-
gle weight, to address the above two problems.
Compared with global training methods, such as
MERT, in which training and testing are separated,
our method works in an online fashion, in which
training is performed during testing. This online
fashion has an advantage in that it can adapt the
weights for each of the test sentences, by dynam-
ically tuning the weights on translation examples
which are similar to these test sentences. Similar
to the method of development set automatical selec-
tion, the local training method may also suffer the
problem of efficiency. To put it into practice, we
propose incremental training methods which avoid
retraining and iterative decoding on a development
set.
Our local training method has two advantages:
firstly, it significantly outperforms MERT, especially
when test set is different from the development set;
secondly, it improves the translation consistency.
Experiments on NIST Chinese-to-English transla-
tion tasks show that our local training method sig-
nificantly gains over MERT, with the maximum im-
provements up to 2.0 BLEU, and its efficiency is
comparable to that of the global training method.
2 Local Training and Testing
The local training method (Bottou and Vapnik,
1992) is widely employed in computer vision
(Zhang et al2006; Cheng et al2010). Compared
with the global training method which tries to fit
a single weight on the training data, the local one
learns weights based on the local neighborhood in-
formation for each test example. It is superior to
403
the global one when the data sets are not evenly
distributed (Bottou and Vapnik, 1992; Zhang et al
2006).
Algorithm 1 Naive Local Training Method
Input: T = {ti}Ni=1(test set), K (retrieval size),
Dev(development set), D(retrieval data)
Output: Translation results of T
1: for all sentence ti such that 1 ? i ? N do
2: Retrieve the training examples Di with size
K for ti from D according to a similarity;
3: Train a local weight W i based on Dev and
Di;
4: Decode ti with W i;
5: end for
Suppose T be a test set, Dev a development set,
and D a retrieval data. The local training in SMT
is described in the Algorithm 1. For each sentence
ti in test set, training examples Di is retrieved from
D using a similarity measure (line 2), a weight W i
is optimized on Dev and Di (line 3)1, and, finally,
ti is decoded with W i for testing (line 4). At the
end of this algorithm, it returns the translation re-
sults for T . Note that weights are adapted for each
test sentence ti in line 3 by utilizing the translation
examples Di which are similar to ti. Thus, our local
training method can be considered as an adaptation
of translation weights.
Algorithm 1 suffers a problem of training effi-
ciency in line 3. It is impractical to train a weight
W i on Dev and Di from scratch for every sen-
tence, since iteratively decodingDev andDi is time
consuming when we apply MERT. To address this
problem, we propose a novel incremental approach
which is based on a two-phase training.
On the first phase, we use a global training
method, like MERT, to tune a baseline weight on
the development set Dev in an offline manner. On
the second phase, we utilize the retrieved examples
to incrementally tune sentence-wise local weights
based on the baseline weight. This method can
not only consider the common characteristics learnt
from the Dev, but also take into account the knowl-
1Usually, the quality of development set Dev is high, since
it is manually produced with multiple references. This is the
main reason why Dev is used as a part of new development set
to train W i.
edge for each individual sentence learnt from sim-
ilar examples during testing. On the phase of in-
cremental training, we perform decoding only once
for retrieved examples Di, though several rounds of
decoding are possible and potentially better if one
does not seriously care about training speed. Fur-
thermore, instead of on-the-fly decoding, we decode
the retrieval data D offline using the parameter from
our baseline weight and its nbest translation candi-
dates are saved with training examples to increase
the training efficiency.
Algorithm 2 Local Training Method Based on In-
cremental Training
Input: T = {ti}Ni=1 (test set), K (retrieval size),
Dev (development set),
D = {?fs, rs?}s=Ss=1 (retrieval data),
Output: Translation results of T
1: Run global Training (such as MERT) on Dev to
get a baseline weight Wb; // Phase 1
2: Decode each sentence in D to get
D = {?fs, cs, rs?}s=Ss=1 ;
3: for all sentence ti such that 1 ? i ? N do
4: Retrieve K training examples Di =
{?f ij , c
i
j , r
i
j?}
j=K
j=1 for ti from D according to
a similarity;
5: Incrementally train a local weight W i based
on Wb and Di; // Phase 2
6: Decode ti with W i;
7: end for
The two-phase local training algorithm is de-
scribed in Algorithm 2, where cs and rs denote the
translation candidate set and reference set for each
sentence fs in retrieval data, respectively, and K is
the retrieval size. It globally trains a baseline weight
Wb (line 1), and decodes each sentence in retrieval
data D with the weight Wb (line 2). For each sen-
tence ti in test set T , it first retrieves training exam-
ples Di from D (line 4), and then it runs local train-
ing to tune a local weight W i (line 5) and performs
testing with W i for ti (line 6). Please note that the
two-phase training contains global training in line 1
and local training in line 5.
From Algorithm 2, one can see that our method is
effective even if the test set is unknow, for example,
in the scenario of online translation services, since
the global training on development set and decoding
404
on retrieval data can be performed offline.
In the next two sections, we will discuss the de-
tails about the similarity metric in line 4 and the in-
cremental training in line 5 of Algorithm 2.
3 Acquiring Training Examples
In line 4 of Algorithm 2, to retrieve training exam-
ples for the sentence ti , we first need a metric to
retrieve similar translation examples. We assume
that the metric satisfy the property: more similar the
test sentence and translation examples are, the better
translation result one obtains when decoding the test
sentence with the weight trained on the translation
examples.
The metric we consider here is derived from
an example-based machine translation. To retrieve
translation examples for a test sentence, (Watanabe
and Sumita, 2003) defined a metric based on the
combination of edit distance and TF-IDF (Manning
and Schu?tze, 1999) as follows:
dist(f1, f2) = ? ? edit-dist(f1, f2)+
(1? ?)? tf-idf(f1, f2), (2)
where ?(0 ? ? ? 1) is an interpolation weight,
fi(i = 1, 2) is a word sequence and can be also
considered as a document. In this paper, we extract
similar examples from training data. Like example-
based translation in which similar source sentences
have similar translations, we assume that the optimal
translation weights of the similar source sentences
are closer.
4 Incremental Training Based on
Ultraconservative Update
Compared with retraining mode, incremental train-
ing can improve the training efficiency. In the field
of machine learning research, incremental training
has been employed in the work (Cauwenberghs and
Poggio, 2001; Shilton et al2005), but there is lit-
tle work for tuning parameters of statistical machine
translation. The biggest difficulty lies in that the fea-
ture vector of a given training example, i.e. transla-
tion example, is unavailable until actually decoding
the example, since the derivation is a latent variable.
In this section, we will investigate the incremental
training methods in SMT scenario.
Following the notations in Algorithm 2, Wb is
the baseline weight, Di = {?f ij , c
i
j , r
i
j?}
K
j=1 denotes
training examples for ti. For the sake of brevity, we
will drop the index i, Di = {?fj , cj , rj?}Kj=1, in the
rest of this paper. Our goal is to find an optimal
weight, denoted by W i, which is a local weight and
used for decoding the sentence ti. Unlike the global
method which performs tuning on the whole devel-
opment set Dev +Di as in Algorithm 1, W i can be
incrementally learned by optimizing onDi based on
Wb. We employ the idea of ultraconservative update
(Crammer and Singer, 2003; Crammer et al2006)
to propose two incremental methods for local train-
ing in Algorithm 2 as follows.
Ultraconservative update is an efficient way to
consider the trade-off between the progress made on
development set Dev and the progress made on Di.
It desires that the optimal weight W i is not only
close to the baseline weight Wb, but also achieves
the low loss over the retrieved examples Di. The
idea of ultraconservative update can be formalized
as follows:
min
W
{
d(W,Wb) + ? ? Loss(D
i,W )
}
, (3)
where d(W,Wb) is a distance metric over a pair
of weights W and Wb. It penalizes the weights
far away from Wb and it is L2 norm in this paper.
Loss(Di,W ) is a loss function of W defined on Di
and it evaluates the performance of W over Di. ?
is a positive hyperparameter. If Di is more similar
to the test sentence ti, the better performance will be
achieved for the larger ?. In particular, ifDi consists
of only a single sentence ti, the best performance
will be obtained when ? goes to infinity.
4.1 Margin Based Ultraconservative Update
MIRA(Crammer and Singer, 2003; Crammer et al
2006) is a form of ultraconservative update in (3)
whoseLoss is defined as hinge loss based on margin
over the pairwise translation candiates in Di. It tries
to minimize the following quadratic program:
1
2
||W ?Wb||
2+
?
K
K?
j=1
max
1?n?|cj |
(
`jn?W ??h(fj , ejn)
)
with
?h(fj , ejn) = h(fj , ej?)? h(fj , ejn), (4)
405
where h(fj , e) is the feature vector of candidate e,
ejn is a translation member of fj in cj , ej? is the
oracle one in cj , `jn is a loss between ej? and ejn
and it is the same as referred in (Chiang et al2008),
and |cj | denotes the number of members in cj .
Different from (Watanabe et al2007; Chiang
et al2008) employing the MIRA to globally train
SMT, in this paper, we apply MIRA as one of local
training method for SMT and we call it as margin
based ultraconservative update (MBUU for shortly)
to highlight its advantage of incremental training in
line 5 of Algorithm 2.
Further, there is another difference between
MBUU and MIRA in (Watanabe et al2007; Chi-
ang et al2008). MBUU is a batch update mode
which updates the weight with all training examples,
but MIRA is an online one which updates with each
example (Watanabe et al2007) or part of examples
(Chiang et al2008). Therefore, MBUU is more ul-
traconservative.
4.2 Error Rate Based Ultraconservative
Update
Instead of taking into account the margin-based
hinge loss between a pair of translations as the Loss
in (3), we directly optimize the error rate of trans-
lation candidates with respect to their references in
Di. Formally, the objective function of error rate
based ultraconservative update (EBUU) is as fol-
lows:
1
2
?W ?Wb?
2 +
?
K
K?
j=1
Error(rj ; e?(fj ;W )), (5)
where e?(fj ;W ) is defined in Equation (1), and
Error(rj , e) is the sentence-wise minus BLEU (Pa-
pineni et al2002) of a candidate e with respect to
rj .
Due to the existence of L2 norm in objective
function (5), the optimization algorithm MERT can
not be applied for this question since the exact line
search routine does not hold here. Motivated by
(Och, 2003; Smith and Eisner, 2006), we approxi-
mate the Error in (5) by the expected loss, and then
derive the following function:
1
2
?W?Wb?
2+
?
K
K?
j=1
?
e
Error(rj ; e)P?(e|fj ;W ),
(6)
Systems NIST02 NIST05 NIST06 NIST08
Moses 30.39 26.31 25.34 19.07
Moses hier 33.68 26.94 26.28 18.65
In-Hiero 31.24 27.07 26.32 19.03
Table 1: The performance comparison of the baseline In-
Hiero VS Moses and Moses hier.
with
P?(e|fj ;W ) =
exp[?W ? h(fj , e)]
?
e??cj exp[?W ? h(fj , e
?)]
, (7)
where ? > 0 is a real number valued smoother. One
can see that, in the extreme case, for ? ? ?, (6)
converges to (5).
We apply the gradient decent method to minimize
the function (6), as it is smooth with respect to ?.
Since the function (6) is non-convex, the solution
obtained by gradient descent method may depend on
the initial point. In this paper, we set the initial point
as Wb in order to achieve a desirable solution.
5 Experiments and Results
5.1 Setting
We conduct our experiments on the Chinese-to-
English translation task. The training data is FBIS
corpus consisting of about 240k sentence pairs. The
development set is NIST02 evaluation data, and the
test datasets are NIST05, NIST06,and NIST08.
We run GIZA++ (Och and Ney, 2000) on the
training corpus in both directions (Koehn et al
2003) to obtain the word alignment for each sen-
tence pair. We train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus using the SRILM Toolkits (Stolcke, 2002) with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998). In our experiments the translation per-
formances are measured by case-insensitive BLEU4
metric (Papineni et al2002) and we use mteval-
v13a.pl as the evaluation tool. The significance test-
ing is performed by paired bootstrap re-sampling
(Koehn, 2004).
We use an in-house developed hierarchical
phrase-based translation (Chiang, 2005) as our base-
line system, and we denote it as In-Hiero. To ob-
tain satisfactory baseline performance, we tune In-
Hiero system for 5 times using MERT, and then se-
406
Methods Steps Seconds
Global method Decoding 2.0
Local method Retrieval +0.6
Local training +0.3
Table 2: The efficiency of the local training and testing
measured by sentence averaged runtime.
Methods NIST05 NIST06 NIST08
Global MERT 27.07 26.32 19.03
Local MBUU 27.75+ 27.88+ 20.84+
EBUU 27.85+ 27.99+ 21.08+
Table 3: The performance comparison of local train-
ing methods (MBUU and EBUU) and a global method
(MERT). NIST05 is the set used to tune ? for MBUU and
EBUU, and NIST06 and NIST08 are test sets. + means
the local method is significantly better than MERT with
p < 0.05.
lect the best-performing one as our baseline for the
following experiments. As Table 1 indicates, our
baseline In-Hiero is comparable to the phrase-based
MT (Moses) and the hierarchical phrase-based MT
(Moses hier) implemented in Moses, an open source
MT toolkit2 (Koehn et al2007). Both of these sys-
tems are with default setting. All three systems are
trained by MERT with 100 best candidates.
To compare the local training method in Algo-
rithm 2, we use a standard global training method,
MERT, as the baseline training method. We do not
compare with Algorithm 1, in which retraining is
performed for each input sentence, since retraining
for the whole test set is impractical given that each
sentence-wise retraining may take some hours or
even days. Therefore, we just compare Algorithm
2 with MERT.
5.2 Runtime Results
To run the Algorithm 2, we tune the baseline weight
Wb on NIST02 by MERT3. The retrieval data is set
as the training data, i.e. FBIS corpus, and the re-
trieval size is 100. We translate retrieval data with
Wb to obtain their 100 best translation candidates.
We use the simple linear interpolated TF-IDF met-
ric with ? = 0.1 in Section 3 as the retrieval metric.
2See web: http://www.statmt.org
3Wb is exactly the weight of In-Hiero in Table 1.
NIST05 NIST06 NIST08
NIST02 0.665 0.571 0.506
Table 4: The similarity of development and three test
datasets.
For an efficient tuning, the retrieval process is par-
allelized as follows: the examples are assigned to 4
CPUs so that each CPU accepts a query and returns
its top-100 results, then all these top-100 results are
merged into the final top-100 retrieved examples to-
gether with their translation candidates. In our ex-
periments, we employ the two incremental training
methods, i.e. MBUU and EBUU. Both of the hyper-
parameters ? are tuned on NIST05 and set as 0.018
and 0.06 for MBUU and EBUU, respectively. In
the incremental training step, only one CPU is em-
ployed.
Table 2 depicts that testing each sentence with lo-
cal training method takes 2.9 seconds, which is com-
parable to the testing time 2.0 seconds with global
training method4. This shows that the local method
is efficient. Further, compared to the retrieval, the
local training is not the bottleneck. Actually, if we
use LSH technique (Andoni and Indyk, 2008) in re-
trieval process, the local method can be easily scaled
to a larger training data.
5.3 Results and Analysis
Table 3 shows the main results of our local train-
ing methods. The EBUU training method signifi-
cantly outperforms the MERT baseline, and the im-
provement even achieves up to 2.0 BLEU points on
NIST08. We can also see that EBUU and MBUU are
comparable on these three test sets. Both of these
two local training methods achieve significant im-
provements over the MERT baseline, which proves
the effectiveness of our local training method over
global training method.
Although both local methods MBUU and EBUU
achieved improvements on all the datasets, their
gains on NIST06 and NIST08 are significantly
higher than those achieved on NIST05 test dataset.
We conjecture that, the more different a test set and
a development set are, the more potential improvem-
4The runtime excludes the time of tuning and decoding on D
in Algorithm 2, since both of them can be performanced offline.
407
0 . 0 0 0 . 0 2 0 . 0 4 0 . 0 6 0 . 0 8 0 . 1 01 82 02 2
2 42 62 8  
 
 N I S T 0 5 N I S T 0 6 N I S T 0 8BLEU l
Figure 2: The peformance of EBUU for different ? over
all the test datasets. The horizontal axis denotes the val-
ues of ? in function (6), and the vertical one denotes the
BLEU points.
Metthods Dev NIST08
NIST02 19.03
MERT NIST05 20.06
NIST06 21.28
EBUU NIST02 21.08
Table 5: The comparison of MERT with different de-
velopment datasets and local training method based on
EBUU.
nts local training has for the sentences in this test set.
To test our hypothesis, we measured the similarity
between the development set and a test set by the
average value5 of accumulated TF-IDF scores of de-
velopment dataset and each sentence in test datasets.
Table 4 shows that NIST06 and NIST08 are more
different from NIS02 than NIST05, thus, this is po-
tentially the reason why local training is more effec-
tive on NIST06 and NIST08.
As mentioned in Section 1, the global training
methods such as MERT are highly dependent on de-
velopment sets, which can be seen in Table 5. There-
fore, the translation performance will be degraded if
one chooses a development data which is not close
5Instead of using the similarity between two documents de-
velopment and test datasets, we define the similarity as the av-
erage similarity of the development set and the sentences in test
set. The reason is that it reduces its dependency on the number
of sentences in test dataset, which may cause a bias.
Methods Number Percents
MERT 1735 42.3%
EBUU 1606 39.1%
Table 6: The statistics of sentences with 0.0 sentence-
level BLEU points over three test datasets.
to the test data. We can see that, with the help of the
local training, we still gain much even if we selected
an unsatisfactory development data.
As also mentioned in Section 1, the global meth-
ods do not care about the sentence level perfor-
mance. Table 6 depicts that there are 1735 sentences
with zero BLEU points in all the three test datasets
for MERT. Besides obtaining improvements on doc-
ument level as referred in Table 3, the local training
methods can also achieve consistent improvements
on sentence level and thus can improve the users?
experiences.
The hyperparameters ? in both MBUU (4) and
EBUU (6) has an important influence on transla-
tion performance. Figure 2 shows such influence
for EBUU on the test datasets. We can see that, the
performances on all these datasets improve as ? be-
comes closer to 0.06 from 0, and the performance
continues improving when ? passes over 0.06 on
NIST08 test set, where the performance constantly
improves up to 2.6 BLEU points over baseline. As
mentioned in Section 4, if the retrieved examples are
very similar to the test sentence, the better perfor-
mance will be achieved with the larger ?. There-
fore, it is reasonable that the performances improved
when ? increased from 0 to 0.06. Further, the turn-
ing point appearing at 0.06 proves that the ultra-
conservative update is necessary. We can also see
that the performance on NIST08 consistently im-
proves and achieves the maximum gain when ? ar-
rives at 0.1, but those on both NIST05 and NIST06
achieves the best when it arrives at 0.06. This
phenomenon can also be interpreted in Table 4 as
the lowest similarity between the development and
NIST08 datasets.
Generally, the better performance may be
achieved when more examples are retrieved. Actu-
ally, in Table 7 there seems to be little dependency
between the numbers of examples retrieved and the
translation qualities, although they are positively re-
408
Retrieval Size NIST05 NIST06 NIST08
40 27.66 27.81 20.87
70 27.77 27.93 21.08
100 27.85 27.99 21.08
Table 7: The performance comparison by varying re-
trieval size in Algorithm 2 based on EBUU.
Methods NIST05 NIST06 NIST08
MERT 27.07 26.32 19.03
EBUU 27.85 27.99 21.08
Oracle 29.46 29.35 22.09
Table 8: The performance of Oracle of 2-best results
which consist of 1-best resluts of MERT and 1-best
resluts of EBUU.
lated approximately.
Table 8 presents the performance of the oracle
translations selected from the 1-best translation re-
sults of MERT and EBUU. Clearly, there exists more
potential improvement for local training method.
6 Related Work
Several works have proposed discriminative tech-
niques to train log-linear model for SMT. (Och and
Ney, 2002; Blunsom et al2008) used maximum
likelihood estimation to learn weights for MT. (Och,
2003; Moore and Quirk, 2008; Zhao and Chen,
2009; Galley and Quirk, 2011) employed an eval-
uation metric as a loss function and directly opti-
mized it. (Watanabe et al2007; Chiang et al2008;
Hopkins and May, 2011) proposed other optimiza-
tion objectives by introducing a margin-based and
ranking-based indirect loss functions.
All the methods mentioned above train a single
weight for the whole development set, whereas our
local training method learns a weight for each sen-
tence. Further, our translation framework integrates
the training and testing into one unit, instead of treat-
ing them separately. One of the advantages is that it
can adapt the weights for each of the test sentences.
Our method resorts to some translation exam-
ples, which is similar as example-based translation
or translation memory (Watanabe and Sumita, 2003;
He et al2010; Ma et al2011). Instead of using
translation examples to construct translation rules
for enlarging the decoding space, we employed them
to discriminatively learn local weights.
Similar to (Hildebrand et al2005; Lu? et al
2007), our method also employes IR methods to re-
trieve examples for a given test set. Their methods
utilize the retrieved examples to acquire translation
model and can be seen as the adaptation of trans-
lation model. However, ours uses the retrieved ex-
amples to tune the weights and thus can be consid-
ered as the adaptation of tuning. Furthermore, since
ours does not change the translation model which
needs to run GIZA++ and it incrementally trains lo-
cal weights, our method can be applied for online
translation service.
7 Conclusion and Future Work
This paper proposes a novel local training frame-
work for SMT. It has two characteristics, which
are different from global training methods such as
MERT. First, instead of training only one weight for
document level, it trains a single weight for sentence
level. Second, instead of considering the training
and testing as two separate units, we unify the train-
ing and testing into one unit, which can employ the
information of test sentences and perform sentence-
wise local adaptation of weights.
Local training can not only alleviate the prob-
lem of the development data selection, but also re-
duce the risk of sentence-wise bad translation re-
sults, thus consistently improve the translation per-
formance. Experiments show gains up to 2.0 BLEU
points compared with a MERT baseline. With the
help of incremental training methods, the time in-
curred by local training was negligible and the local
training and testing totally took 2.9 seconds for each
sentence.
In the future work, we will further investigate the
local training method, since there are more room for
improvements as observed in our experiments. We
will test our method on other translation models and
larger training data6.
Acknowledgments
We would like to thank Hongfei Jiang and Shujie
Liu for many valuable discussions and thank three
6Intuitionally, when the corpus of translation examples is
larger, the retrieval results in Algorithm 2 are much similar as
the test sentence. Therefore our method may favor this.
409
anonymous reviewers for many valuable comments
and helpful suggestions. This work was supported
by National Natural Science Foundation of China
(61173073,61100093), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207), and the
Fundamental Research Funds for Central Univer-
sites (HIT.NSRIF.2013065).
References
Alexandr Andoni and Piotr Indyk. 2008. Near-optimal
hashing algorithms for approximate nearest neighbor
in high dimensions. Commun. ACM, 51(1):117?122,
January.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL,
pages 200?208, Columbus, Ohio, June. Association
for Computational Linguistics.
Le?on Bottou and Vladimir Vapnik. 1992. Local learning
algorithms. Neural Comput., 4:888?900, November.
G. Cauwenberghs and T. Poggio. 2001. Incremental
and decremental support vector machine learning. In
Advances in Neural Information Processing Systems
(NIPS*2000), volume 13.
Stanley F Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. In Technical Report TR-10-98. Harvard Univer-
sity.
Haibin Cheng, Pang-Ning Tan, and Rong Jin. 2010. Ef-
ficient algorithm for localized support vector machine.
IEEE Trans. on Knowl. and Data Eng., 22:537?549,
April.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, ACL ?05, pages 263?270, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585, December.
Michel Galley and Chris Quirk. 2011. Optimal search
for minimum error rate training. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, pages 38?49, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging smt and tm with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
S. Hildebrand, M. Eck, S. Vogel, and Alex Waibel. 2005.
Adaptation of the translation model for statistical ma-
chine translation based on information retrieval. In
Proceedings of EAMT. Association for Computational
Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, pages
1352?1362, Edinburgh, Scotland, UK., July. Associa-
tion for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, ACL ?07,
pages 177?180, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP.
ACL.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2010. Adaptive development data selection for
log-linear model in statistical machine translation. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 662?
670, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
410
343?350, Prague, Czech Republic, June. Association
for Computational Linguistics.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrim-
inative learning - a translation memory-inspired ap-
proach. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1239?1248, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press, Cambridge, MA, USA.
Robert C. Moore and Chris Quirk. 2008. Random
restarts in minimum error rate training for statistical
machine translation. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics -
Volume 1, COLING ?08, pages 585?592, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?00, pages 440?447, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 295?302, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Adam Pauls, John Denero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1418?1427, Singapore, August. Association for
Computational Linguistics.
Alistair Shilton, Marimuthu Palaniswami, Daniel Ralph,
and Ah Chung Tsoi. 2005. Incremental training of
support vector machines. IEEE Transactions on Neu-
ral Networks, 16(1):114?131.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe and Eiichiro Sumita. 2003. Example-
based decoding for statistical machine translation. In
Proc. of MT Summit IX, pages 410?417.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hao Zhang, Alexander C. Berg, Michael Maire, and Ji-
tendra Malik. 2006. Svm-knn: Discriminative near-
est neighbor classification for visual category recog-
nition. In Proceedings of the 2006 IEEE Computer
Society Conference on Computer Vision and Pattern
Recognition - Volume 2, CVPR ?06, pages 2126?2136,
Washington, DC, USA. IEEE Computer Society.
Bing Zhao and Shengyuan Chen. 2009. A simplex
armijo downhill algorithm for optimizing statistical
machine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, Compan-
ion Volume: Short Papers, NAACL-Short ?09, pages
21?24, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
411
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 524?534,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Pivot-Based Statistical Machine Translation 
Using Random Walk 
 
Xiaoning Zhu1*
Conghui Zhu1, and Tiejun Zhao1 
, Zhongjun He2, Hua Wu2, Haifeng Wang2,  
Harbin Institute of Technology, Harbin, China1 
Baidu Inc., Beijing, China2 
{xnzhu, chzhu, tjzhao}@mtlab.hit.edu.cn 
{hezhongjun,wu_hua,wanghaifeng}@baidu.com 
 
 
 
 
 
                                                          
* This work was done when the first author was visiting Baidu. 
Abstract 
This paper proposes a novel approach that uti-
lizes a machine learning method to improve 
pivot-based statistical machine translation 
(SMT). For language pairs with few bilingual 
data, a possible solution in pivot-based SMT 
using another language as a "bridge" to gen-
erate source-target translation. However, one 
of the weaknesses is that some useful source-
target translations cannot be generated if the 
corresponding source phrase and target phrase 
connect to different pivot phrases. To allevi-
ate the problem, we utilize Markov random 
walks to connect possible translation phrases 
between source and target language. Experi-
mental results on European Parliament data, 
spoken language data and web data show that 
our method leads to significant improvements 
on all the tasks over the baseline system. 
1 Introduction 
Statistical machine translation (SMT) uses bilin-
gual corpora to build translation models. The 
amount and the quality of the bilingual data 
strongly affect the performance of SMT systems. 
For resource-rich language pairs, such as Chinese-
English, it is easy to collect large amounts of bi-
lingual corpus. However, for resource-poor lan-
guage pairs, such as Chinese-Spanish, it is difficult 
to build a high-performance SMT system with the 
small scale bilingual data available.  
The pivot language approach, which performs 
translation through a third language, provides a 
possible solution to the problem. The triangulation 
method (Wu and Wang, 2007; Cohn and Lapata, 
2007) is a representative work for pivot-based ma-
chine translation. With a triangulation pivot ap-
proach, a source-target phrase table can be 
obtained by combining the source-pivot phrase 
table and the pivot-target phrase table. However, 
one of the weaknesses is that some corresponding 
source and target phrase pairs cannot be generated, 
because they are connected to different pivot 
phrases (Cui et al, 2013). As illustrated in Figure 
1, since there is no direct translation between ??
?? henkekou? and ?really delicious?, the trian-
gulation method is unable to establish a relation 
between ???? henkekou? and the two Spanish 
phrases. 
To solve this problem, we apply a Markov ran-
dom walk method to pivot-based SMT system. 
Random walk has been widely used. For example, 
Brin and Page (1998) used random walk to dis-
cover potential relations between queries and doc-
uments for link analysis in information retrieval. 
Analogous to link analysis, the aim of pivot-based 
translation is to discover potential translations be-
tween source and target language via the pivot 
language.  
524
The goal of this paper is to extend the previous 
triangulation approach by exploring implicit trans-
lation relations using random walk method. We 
evaluated our approach in several translation tasks, 
including translations between European lan-
guages; Chinese-Spanish spoken language transla-
tion and Chinese-Japanese translation with English 
as the pivot language. Experimental results show 
that our approach achieves significant improve-
ments over the conventional pivot-based method, 
triangulation method. 
The remainder of this paper is organized as fol-
lows. In section 2, we describe the related work. 
We review the triangulation method for pivot-
based machine translation in section 3. Section 4 
describes the random walk models. In section 5 
and section 6, we describe the experiments and 
analyze the performance, respectively. Section 7 
gives a conclusion of the paper. 
2 Related Work 
Several methods have been proposed for pivot-
based translation. Typically, they can be classified 
into 3 kinds of methods: 
Transfer Method: Within the transfer frame-
work (Utiyama and Isahara, 2007; Wang et al, 
2008; Costa-juss? et al, 2011), a source sentence 
is first translated to n pivot sentences via a source-
pivot translation system, and then each pivot sen-
tence is translated to m target sentences via a piv-
ot-target translation system. At each step (source 
to pivot and pivot to target), multiple translation 
outputs will be generated, thus a minimum Bayes-
risk system combination method is often used to 
select the optimal sentence (Gonz?lez-Rubio et al, 
2011; Duh et al, 2011). A problem with the trans-
fer method is that it needs to decode twice. On one 
hand, the time cost is doubled; on the other hand, 
the translation error of the source-pivot translation 
system will be transferred to the pivot-target trans-
lation. 
Synthetic Method: A synthetic method creates 
a synthetic source-target corpus using source-pivot 
translation model or pivot-target translation model 
(Utiyama et al, 2008; Wu and Wang, 2009). For 
example, we can translate each pivot sentence in 
the pivot-target corpus to source language with a 
pivot-source model, and then combine the translat-
ed source sentence with the target sentence to ob-
tain a synthetic source-target corpus, and vice 
versa. However, it is difficult to build a high quali-
ty translation system with a corpus created by a 
machine translation system. 
Triangulation Method: The triangulation 
method obtains source-target model by combining 
source-pivot and pivot-target translation models 
(Wu and Wang, 2007; Cohn and Lapata 2007), 
which has been shown to work better than the oth-
er pivot approaches (Utiyama and Isahara, 2007). 
As we mentioned earlier, the weakness of triangu-
lation is that the corresponding source and target 
phrase pairs cannot be connected in the case that 
they connect to different pivot phrases. 
3 The Triangulation Method 
In this section, we review the triangulation method 
for pivot-based translation. 
With the two additional bilingual corpora, the 
source-pivot and pivot-target translation models 
can be trained. Thus, a pivot model can be ob-
tained by merging these two models. In the trans-
lation model, the phrase translation probability and 
the lexical weight are language dependent, which 
will be introduced in the next two sub-sections. 
Figure 1: An example of random walk on phrase table. The dashed line indicates an implicit relation 
in the phrase table. 
???? 
feichanghaochi 
really delicious 
very tasty 
 
???
henkekou 
realmente delicioso 
 
Chinese English Spanish 
muy delicioso 
 
525
3.1 Phrase Translation Probability 
The triangulation method assumes that there exist 
translations between phrases s  and phrase p  in 
source and pivot languages, and between phrase 
p  and phrase t  in pivot and target languages. 
The phrase translation probability ?  between 
source and target languages is determined by the 
following model: 
( | ) ( | , ) ( | )
          ( | ) ( | )
p
p
s t s p t p t
s p p t
? ? ?
? ?
=
=
?
?
       (1) 
3.2 Lexical Weight 
Given a phrase pair ( , )s t and a word alignment 
a  between the source word positions 1, ,i n= ?  
and the target word positions 0,1, ,j m= ? , the 
lexical weight of phrase pair ( , )s t  can be calcu-
lated with the following formula (Koehn et al 
2003) : 
( , )1
1
( | , ) ( | )
{ | ( , ) }
n
i j
i j ai
p s t a s t
j i j a?
?
? ?=
=
? ?? (2) 
In formula 2, the lexical translation probability 
distribution ( | )s t?  between source word s  and 
target word t  can be estimated with formula 3. 
'
'
( , )
( | )
( , )
s
count s t
s t
count s t
? =
?
            (3) 
Thus the alignment a  between the source 
phrase s  and target phrase t  via pivot phrase p  
is needed for computing the lexical weight. The 
alignment a  can be obtained as follows: 
1 2{( , ) | : ( , ) & ( , ) }a s t p s p a p t a= ? ? ?    (4) 
where 1a  and 2a  indicate the word alignment be-
tween the phrase pair ( , )s p  and ( , )p t , respec-
tively. 
The triangulation method requires that both the 
source and target phrases connect to the same piv-
ot phrase. Otherwise, the source-target phrase pair 
cannot be discovered. As a result, some useful 
translation relations will be lost. In order to allevi-
ate this problem, we propose a random walk model, 
to discover the implicit relations among the source, 
pivot and target phrases. 
4 Random Walks on Translation Graph 
For phrase-based SMT, all source-target phrase 
pairs are stored in a phrase table. In our random 
walk approach, we first build a translation graph 
according to the phrase table. A translation graph 
contains two types of nodes: source phrase and 
target phrase. A source phrase s  and a target 
phrase t  are connected if exists a phrase pair 
( , )s t  in the phrase table. The edge can be 
weighted according to translation probabilities or 
alignments in the phrase table. For the pivot-based 
translation, the translation graph can be derived 
from the source-pivot phrase table and pivot-target 
phrase table.  
Our random walk model is inspired by two 
works (Szummer and Jaakkola, 2002; Craswell 
and Szummer,2007). The general process of ran-
dom walk can be described as follows: 
Let ( , )G V E= be a directed graph with n  ver-
tices and m  edges. For a vertex v V? , ( )v?  de-
notes the set of neighbors of v  in G . A random 
walk on G  follows the following process: start at 
a vertex 0v , chose and walk along a random 
neighbor 1v , with 1 0( )v v?? . At the second step, 
start from 1v  and chose a random neighbor 2v , and 
so on. 
Let S be the set of source phrases, and P be the 
set of pivot phrases. Then the nodes V are the un-
ion of S and P. The edges E correspond to the rela-
tions between phrase pairs.  
Let R represent the binary relations between 
source phrases and pivot phrases. Then the 1-step 
translation ikR from node i to node k can be direct-
ly obtained in the phrase table. 
Define operator ?  to denote the calculation of 
relation R. Then 2-step translation ijR  from node i 
to node j can be obtained with the following for-
mula.  
ij ik kjR R R= ?                           (4) 
We use |0 ( | )tR k i  to denote a t-step translation 
relation from node i to node k. In order to calculate 
the translation relations efficiently, we use a ma-
trix A to represent the graph. A t step translation 
probability can be denoted with the following for-
mula. 
526
|0 ( | ) [ ]
t
t ikP k i A=                         (5) 
where A is a matrix whose i,k-th element is ikR . 
4.1 Framework of Random Walk Approach 
The overall framework of random walk for pivot-
based machine translation is shown in Figure 2. 
Before using random walk model, we have two 
phrase tables: source-pivot phrase table (SP phrase 
table) and pivot-target phrase table (PT phrase ta-
ble). After applying the random walk approach, we 
can achieve two extended phrase table: extended 
source-pivot phrase table (S?P? phrase table) and 
extended pivot-target phrase table (P?T? phrase 
table). The goal of pivot-based SMT is to get a 
source-target phrase table (ST phrase table) via SP 
phrase table and PT phrase table.  
Our random walk was applied on SP phrase ta-
ble or PT phrase table separately. In next 2 sub-
sections, we will explain how the phrase transla-
tion probabilities and lexical weight are obtained 
with random walk model on the phrase table. 
Figure 3 shows some possible decoding pro-
cesses of random walk based pivot approach. In 
figure 3-a, the possible source-target phrase pair 
can be obtained directly via a pivot phrase, so it 
does not need a random walk model. In figure 3-b 
and figure 3-c, one candidate source-target phrase 
pair can be obtained by random walks on source-
pivot side or pivot-target side. Figure 3-d shows 
that the possible source-target can only by ob-
tained by random walks on source-pivot side and 
pivot-target side. 
4.2 Phrase Translation Probabilities 
For the translation probabilities, the binary relation 
R is the translation probabilities in the phrase table. 
The operator ?  is multiplication. According to 
formula 5, the random walk sums up the probabili-
ties of all paths of length t between the node i and 
k. 
Figure 2: Framework of random walk based pivot translation. The ST phrase table was generated by combin-
ing SP and PT phrase table through triangulation method. The phrase table with superscript ??? means that it 
was enlarged by random walk. 
 
S?P?
Phrase Table
P?T? 
Phrase Table
 SP 
Phrase Table
PT 
Phrase Table
ST 
Phrase Table
S?T?
Phrase Table
Pivot without 
random walk
Pivot with 
random walkrandom walk
random walk
Figure 3: Some possible decoding processes of random walk based pivot approach. The ? stands for the 
source phrase (S); the ? represents the pivot phrase (P) and the ? stands for the target phrase (T). 
 
(a) Pivot without  
       random walk 
S P T 
(d) Random walk on   
     both sides 
S P T 
(b) Random walk on  
      source-pivot side 
S P T 
(c) Random walk on 
      pivot-target side 
S P T 
527
Take source-to-pivot phrase graph as an exam-
ple; denote matrix A contains s+p nodes (s source 
phrases and p pivot phrases) to represent the trans-
lation graph.  
( ) ( )ij s p s p
A g
+ ? +
? ?= ? ?                         (6) 
where ijg  is the i,j-th elements of matrix A. 
We can split the matrix A into 4 sub-matrixes: 
0
0
s s sp
ps p p
A
A
A
?
?
? ?
= ? ?
? ?
                      (7) 
where the sub-matrix [ ]sp ik s pA p ?=  represents the 
translation probabilities from source to pivot lan-
guage, and psA  represents the similar meaning. 
Take 3 steps walks as an example: 
Step1: 
0
0
s s sp
ps p p
A
A
A
?
?
? ?
= ? ?
? ?
 
Step2: 
2
0
0
sp ps s p
p s ps sp
A A
A
A A
?
?
?? ?
= ? ??? ?
 
Step3: 
3
0
0
s s sp ps sp
ps sp ps p p
A A A
A
A A A
?
?
? ?? ?
= ? ?? ?? ?
 
For the 3 steps example, each step performs a 
translation process in the form of matrix?s self-
multiplication.  
1. The first step means the translation from 
source language to pivot language. The matrix 
A is derived from the phrase table directly and 
each element in the graph indicates a transla-
tion rule in the phrase table.  
2. The second step demonstrates a procedure: S-
P-S?. With 2 steps random walks, we can find 
the synonymous phrases, and this procedure is 
analogous to paraphrasing (Bannard and 
Callison-Burch, 2005). For the example shown 
in  figure 1 as an example, the hidden relation 
between ???? henkekou? and ?????
feichanghaochi? can be found through Step 2. 
3. The third step describes the following proce-
dure: S-P-S?-P?. An extended source-pivot 
phrase table is generated by 3-step random 
walks. Compared with the initial phrase table 
in Step1, although the number of phrases is 
not increased, the relations between phrase 
pairs are increased and more translation rules 
can be obtained. Still for the example in Fig-
ure 1 , the hidden relation between ????
henkekou? and ?really delicious? can be gen-
erated in Step 3. 
4.3 Lexical Weights 
To build a translation graph, the two sets of phrase 
translation probabilities are represented in the 
phrase tables. However, the two lexical weights 
are not presented in the graph directly. To deal 
with this, we should conduct a word alignment 
random walk model to obtain a new alignment a 
after t steps. For the computation of lexical 
weights, the relation R can be expressed as the 
word alignment in the phrase table. The operator 
?  can be induced with the following formula. 
1 2{( , ) | : ( , ) & ( , ) }a x y p x z a z y a= ? ? ?         (8) 
where a1 and a2 represent the word alignment 
information inside the phrase pairs ( , )x y  and 
( , )y z respectively. An example of word 
alignment inducing is shown in Figure 4. With a 
new word alignment, the two lexical weights can 
be calculated by formula 2 and formula 3. 
Figure 4: An example of word alignment induction with 3 steps random walks 
?   ??   ?   ?   ? 
could   you   fill   out   this   form ?   ?   ??   ??   ?? 
please   fill   out   this   form 
?   ??   ?   ?   ? 
could   you   fill   out   this   form 
step 1 
step 2 
step 3 
528
5 Experiments 
5.1 Translation System and Evaluation Met-
ric 
In our experiments, the word alignment was ob-
tained by GIZA++ (Och and Ney, 2000) and the 
heuristics ?grow-diag-final? refinement rule. 
(Koehn et al, 2003). Our translation system is an 
in-house phrase-based system using a log-linear 
framework including a phrase translation model, a 
language model, a lexicalized reordering model, a 
word penalty model and a phrase penalty model, 
which is analogous to Moses (Koehn et al, 2007). 
The baseline system is the triangulation method 
based pivot approach (Wu and Wang, 2007).  
To evaluate the translation quality, we used 
BLEU (Papineni et al, 2002) as our evaluation 
metric. The statistical significance using 95% con-
fidence intervals were measured with paired boot-
strap resampling (Koehn, 2004). 
5.2 Experiments on Europarl 
5.2.1. Data sets 
We mainly test our approach on Europarl1
We perform our experiments on different trans-
lation directions and via different pivot languages. 
As a most widely used language in the world 
(Mydans, 2011), English was used as the pivot 
language for granted when carrying out experi-
ments on different translation directions. For trans-
lating Portuguese to Swedish, we also tried to 
perform our experiments via different pivot lan-
 corpus, 
which is a multi-lingual corpus including 21 Euro-
pean languages. Due to the size of the data, we 
only select 11 languages which were added to 
Europarl from 04/1996 or 01/1997, including Dan-
ish (da), German (de), Greek (el), English (en), 
Spanish (es), Finnish (fi), French (fr), Italian (it) 
Dutch (nl) Portuguese (pt) and Swedish (sv). In 
order to avoid a trilingual scenario, we split the 
training corpus into 2 parts by the year of the data: 
the data released in odd years were used for train-
ing source-pivot model and the data released in 
even years were used for training pivot-target 
model.  
                                                          
1 http://www.statmt.org/europarl/ 
guages. Table 1 and Table 2 summarized the train-
ing data. 
 
Language 
Pairs  
(src-pvt) 
Sentence 
Pairs # 
Language 
Pairs 
(pvt-tgt) 
Sentence 
Pairs # 
da-en 974,189 en-da 953,002 
de-en 983,411 en-de 905,167 
el-en 609,315 en-el 596,331 
es-en 968,527 en-es 961,782 
fi-en 998,429 en-fi 903,689 
fr-en 989,652 en-fr 974,637 
it-en 934,448 en-it 938,573 
nl-en 982,696 en-nl 971,379 
pt-en 967,816 en-pt 960,214 
sv-en 960,631 en-sv 869,254 
 
Table1. Training data for experiments using English as 
the pivot language. For source-pivot (src-pvt; xx-en) 
model training, the data of odd years were used. Instead 
the data of even years were used for pivot-target (pvt-
src; en-xx) model training. 
 
 
Language 
Pairs  
(src-pvt) 
Sentence 
Pairs # 
Language 
Pairs 
(pvt-tgt) 
Sentence 
Pairs # 
pt-da 941,876 da-sv 865,020 
pt-de 939,932 de-sv 814,678 
pt-el 591,429 el-sv 558,765 
pt-es 934,783 es-sv 827,964 
pt-fi 950,588 fi-sv 872,182 
pt-fr 954,637 fr-sv 860,272 
pt-it 900,185 it-sv 813,000 
pt-nl 945,997 nl-sv 864,675 
 
Table2. Training data for experiments via different piv-
ot languages. For source-pivot (src-pvt; pt-xx) model 
training, the data of odd years were used. Instead the 
data of even years were used for pivot-target (pvt-src; 
xx-sv) model training. 
 
Test Set Sentence # Reference # 
WMT06 2,000 1 
WMT07 2,000 1 
WMT08 2,000 1 
 
Table3. Statistics of test sets. 
529
 
Several test sets have been released for the 
Europarl corpus. In our experiments, we used 
WMT20062, WMT20073 and WMT20084 as our 
test data. The original test data includes 4 lan-
guages and extended versions with 11 languages 
of these test sets are available by the EuroMatrix5
5.2.2. Experiments on Different Translation 
Directions 
  
project. Table 3 shows the test sets. 
We build 180 pivot translation systems6
The baseline system was built following the tra-
ditional triangulation pivot approach. Table 4 lists 
the results on Europarl training data. Limited by 
 (including 
90 baseline systems and 90 random walk based 
systems) using 10 source/target languages and 1 
pivot language (English).  
                                                          
2 http://www.statmt.org/wmt06/shared-task/ 
3 http://www.statmt.org/wmt07/shared-task.html 
4 http://www.statmt.org/wmt08/shared-task.html 
5 http://matrix.statmt.org/test_sets/list 
6 Given N languages, a total of N*(N-1) SMT systems should 
be build to cover the translation between each language.  
the length of the paper, we only show the results 
on WMT08, the tendency of the results on 
WMT06 and WMT07 is similar to WMT08. 
Several observations can be made from the table.  
1. In all 90 language pairs, our method achieves 
general improvements over the baseline system.  
2. Among 90 language pairs, random walk 
based approach is significantly better than the 
baseline system in 75 language pairs. 
3. The improvements of our approach are not 
equal in different translation directions. The im-
provement ranges from 0.06 (it-es) to 1.21 (pt-da). 
One possible reason is that the performance is re-
lated with the source and target language. For ex-
ample, when using Finnish as the target language, 
the improvement is significant over the baseline. 
This may be caused by the great divergence be-
tween Uralic language (Finnish) and Indo-
European language (the other European language 
in Table4). From the table we can find that the 
translation between languages in different lan-
guage family is worse than that in some language 
family. But our random walk approach can im-
 TGT 
SRC 
da de el es fi fr it nl pt sv 
Baseline 
RW 
da - 
19.83 
20.15* 
20.46 
21.02* 
27.59 
28.29* 
14.76 
15.63* 
24.11 
24.71* 
20.49 
20.82* 
22.26 
22.57* 
24.38 
24.88* 
28.33 
28.87* 
Baseline 
RW 
de 
23.35 
23.69* 
- 
19.83 
20.05 
26.21 
26.70* 
12.72 
13.57* 
22.43 
22.78* 
18.82 
19.32* 
23.74 
24.11* 
23.05 
23.35* 
21.17 
21.27 
Baseline 
RW 
el 
23.24 
23.82* 
18.12 
18.49* 
- 
32.28 
32.48 
13.31 
14.08* 
27.35 
27.67* 
23.19 
23.63* 
20.80 
21.26* 
27.62 
27.86 
22.70 
23.15* 
Baseline 
RW 
es 
25.34 
26.07* 
19.67 
20.17* 
27.24 
27.52 
- 
13.93 
14.61* 
32.91 
33.16 
27.67 
27.92 
22.37 
22.85* 
34.73 
34.93 
24.83 
25.50* 
Baseline 
RW 
fi 
18.29 
18.63* 
13.20 
13.40 
14.72 
15.00* 
20.17 
20.48* 
- 
17.52 
17.84* 
14.76 
15.01 
15.50 
16.04* 
17.30 
17.68* 
16.63 
16.79 
Baseline 
RW 
fr 
25.67 
26.51* 
20.02 
20.45* 
26.58 
26.75 
37.50 
37.80* 
13.90 
14.75* 
- 
28.51 
28.71 
22.65 
23.33* 
33.81 
33.93 
24.64 
25.59* 
Baseline 
RW 
it 
22.63 
23.27* 
17.81 
18.40* 
24.24 
24.66* 
34.36 
35.42* 
13.20 
14.11* 
30.16 
30.48* 
- 
21.37 
21.81* 
30.84 
30.92* 
22.12 
22.64* 
Baseline 
RW 
nl 
22.49 
22.76 
19.86 
20.45* 
18.56 
19.10* 
24.69 
25.19* 
11.96 
12.63* 
21.48 
22.05* 
18.36 
18.67* 
- 
21.71 
22.13* 
19.83 
22.17* 
Baseline 
RW 
pt 
24.08 
25.29* 
19.11 
19.83* 
25.30 
26.20* 
36.59 
37.13* 
13.33 
14.21* 
32.47 
32.78* 
28.08 
28.44* 
21.52 
22.46* 
- 
22.90 
23.90* 
Baseline 
RW 
sv 
31.24 
31.75* 
20.26 
20.74* 
22.06 
22.59* 
29.21 
29.87* 
15.39 
16.13* 
25.63 
26.18* 
21.25 
21.81* 
22.30 
22.62* 
25.60 
26.09* 
- 
Table4. Experimental results on Europarl with different translation directions (BLEU% on WMT08). 
RW=Random Walk. * indicates the results are significantly better than the baseline (p<0.05). 
530
prove the performance of translations between dif-
ferent language families. 
5.2.3. Experiments via Different Pivot Lan-
guages 
In addition to using English as the pivot language, 
we also try some other languages as the pivot 
language. In this sub-section, experiments were 
carried out from translating Portuguese to Swedish 
via different pivot languages.  
Table 5 summarizes the BLEU% scores of dif-
ferent pivot language when translating from Por-
tuguese to Swedish. Similar to Table 4, our 
approach still achieves general improvements over 
the baseline system even if the pivot language has 
been changed. From the table we can see that for 
most of the pivot language, the random walk based 
approach gains more than 1 BLEU score over the 
baseline. But when using Finnish as the pivot lan-
guage, the improvement is only 0.02 BLEU scores 
on WMT08. This phenomenon shows that the piv-
ot language can also influence the performance of 
random walk approach. One possible reason for 
the poor performance of using Finnish as the pivot 
language is that Finnish belongs to Uralic lan-
guage family, and the other languages belong to 
Indo-European family. The divergence between 
different language families led to a poor perfor-
mance. Thus how to select a best pivot language is 
our future work. 
The problem with random walk is that it will 
lead to a larger phrase table with noises. In this 
sub-section, a pre-pruning (before random walk) 
and a post-pruning (after random walk) method 
were introduced to deal with this problem.  
We used a naive pruning method which selects 
the top N phrase pairs in the phrase table. In our 
experiments, we set N to 20. For pre-pruning, we 
prune the SP phrase table and PT phrase table be-
fore applying random walks. Post-pruning means 
that we prune the ST phrase table after random 
walks. For the baseline system, we also apply a 
pruning method before combine the SP and PT 
phrase table. We test our pruning method on pt-en-
sv translation task. Table 6 shows the results. 
With a pre- and post-pruning method, the ran-
dom walk approach is able to achieve further im-
provements. Our approach achieved BLEU scores 
of 25.11, 24.69 and 24.34 on WMT06, WMT07 
and WMT08 respectively, which is much better 
than the baseline and the random walk approach 
with pruning.  Moreover, the size of the phrase 
table is about half of the no-pruning method. 
When adopting a post-pruning method, the per-
formance of translation did not improved signifi-
cantly over the pre-pruning, but the scale of the 
phrase table dropped to 69M, which is only about 
2 times larger than the triangulation method. 
Phrase table pruning is a key work to improve 
the performance of random walk. We plan to ex-
plore more approaches for phrase table pruning. 
E.g. using significance test (Johnson et al, 2007) 
or monolingual key phrases (He et al, 2009) to 
filter the phrase table. 
 
 
Table5. Experimental results on translating from Portu-
guese to Swedish via different pivot language. 
RW=Random Walk. * indicates the results are signifi-
cantly better than the baseline (p<0.05). 
 
 
Table6. Results of Phrase Table Filtering 
 
trans 
language 
WMT 
06 
WMT 
07 
WMT 
08 
Baseline 
RW 
pt-da-sv 
23.40 
24.47* 
22.80 
24.21* 
22.49 
23.75* 
Baseline 
RW 
pt-de-sv 
22.72 
23.12* 
22.21 
23.26* 
21.76 
22.35* 
Baseline 
RW 
pt-el-sv 
22.53 
23.75* 
22.19 
23.22* 
21.37 
22.40* 
Baseline 
RW 
pt-en-sv 
23.54 
24.66* 
23.24 
24.22* 
22.90 
23.90* 
Baseline 
RW 
pt-es-sv 
23.58 
24.65* 
23.37 
24.10* 
22.80 
23.77* 
Baseline 
RW 
pt-fi-sv 
21.06 
21.17 
20.06 
20.42* 
20.26 
20.28 
Baseline 
RW 
pt-fr-sv 
23.55 
24.75* 
23.09 
24.15* 
22.89 
23.96* 
Baseline 
RW 
pt-it-sv 
23.65 
24.74* 
22.96 
24.18* 
22.79 
24.02* 
Baseline 
RW 
pt-nl-sv 
21.87 
23.06* 
21.83 
22.76* 
21.36 
22.29* 
 WMT 
06 
WMT 
07 
WMT 
08 
Phrase 
Pairs # 
Baseline 
+pruning 
23.54 
24.05
* 
23.24 
23.70
* 
22.90 
23.59
* 
46M 
32M 
RW 
+pre-pruning 
+post-pruning 
24.66 
25.11 
25.19
* 
24.22 
24.69 
24.79
* 
23.90 
24.34 
24.41
* 
215M 
109M 
69M 
531
5.3 Experiments on Spoken Language 
The European languages show various degrees of 
similarity to one another. In this sub-section, we 
consider translation from Chinese to Spanish with 
English as the pivot language. Chinese belongs to 
Sino-Tibetan Languages and English/Spanish be-
longs to Indo-European Languages, the gap be-
tween two languages is wide. 
A pivot task was included in IWSLT 2008 in 
which the participants need to translate Chinese to 
Spanish via English. A Chinese-English and an 
English-Spanish data were supplied to carry out 
the experiments. The entire training corpus was 
tokenized and lowercased. Table 7 and Table 8 
summarize the training data and test data. 
Table 9 shows the similar tendency with Table 4. 
The random walk models achieved BLEU% scores 
32.09, which achieved an absolute improvement of 
2.08 percentages points on BLEU over the base-
line.   
 
Corpus 
Sentence 
pair # 
Source 
word # 
Target 
word # 
CE 20,000 135,518 182,793 
ES 19,972 153,178 147,560 
 
Table 7: Training Data of IWSLT2008 
 
Test Set Sentence # Reference # 
IWSLT08 507 16 
 
Table8. Test Data of IWSLT2008 
 
System BLEU% phrase pairs # 
Baseline 30.01 143,790 
+pruning 30.25 108,407 
RW 31.57 2,760,439 
+pre-pruning 31.99 1,845,648 
+post-pruning 32.09* 1,514,694 
 
Table9. Results on IWSLT2008 
5.4 Experiments on Web Data 
The setting with Europarl data is quite artificial as 
the training data for directly translating between 
source and target actually exists in the original 
data sets. The IWSLT data set is too small to rep-
resent the real scenario. Thus we try our experi-
ment on a more realistic scenario: translating from 
Chinese to Japanese via English with web crawled 
data. 
All the training data were crawled on the web. 
The scale of Chinese-English and English-
Japanese is 10 million respectively. The test set 
was built in house with 1,000 sentences and 4 ref-
erences. 
 
System BLEU% phrase pairs # 
Baseline 28.76 4.5G 
+pruning 28.90 273M 
RW 29.13 46G 
+pre-pruning 29.44 11G 
+post-pruning 29.51* 3.4G 
 
Table10. Results on Web Data 
 
Table 10 lists the results on web data. From the 
table we can find that the random walk model can 
achieve an absolute improvement of 0.75 percent-
ages points on BLEU over the baseline.  
In this subsection, the training data contains 
parallel sentences with different domains. And the 
two training corpora (Chinese-English and Eng-
lish-Japanese) are typically very different. It 
means that our random walk approach is robust in 
the realistic scenario. 
6 Discussions 
The random walk approach mainly improves the 
performance of pivot translation in two aspects: 
reduces the OOVs and provides more hypothesis 
phrases for decoding.  
6.1 OOV 
Out-of-vocabulary (OOV 7
We count the OOVs when decoding with trian-
gulation model and random walk model on 
IWSLT2008 data. The statistics shows that when 
using triangulation model, there are 11% OOVs 
when using triangulation model, compared with 
9.6% when using random walk model. Less OOV 
often lead to a better result. 
) terms cause serious 
problems for machine translation systems (Zhang 
et al, 2005). The random walk model can reduce 
the OOVs. As illustrated in Figure 1, the Chinese 
phrase ????henkekou? cannot be connected to 
any Spanish phrase, thus it is a OOV term.  
                                                          
7 OOV refer to phrases here. 
532
6.2 Hypothesis Phrases 
To illustrate how the random walk method helps 
improve the performance of machine translation, 
we illustrate an example as follows: 
 
- Source: ? ? ? ?? 
              wo xiang yao zhentou 
- Baseline trans: Quiero almohada 
- Random Walk trans: Quiero una almohada 
 
For translating a Chinese sentence ??????
wo xiang yao zhentou? to Spanish, we can get two 
candidate translations. In this case, the random 
walk translation is better than the baseline system. 
The key phrase in this sentence is ??? zhentou?, 
figure 5 shows the extension process. In this case, 
the article ?a? is hidden in the source-pivot phrase 
table. The same situation often occurs in articles 
and prepositions. Random walk is able to discover 
the hidden relations (hypothesis phrases) among 
source, pivot and target phrases. 
 
 
 
 
 
 
 
 
 
 
7 Conclusion and Future Work 
In this paper, we proposed a random walk method 
to improve pivot-based statistical machine transla-
tion. The random walk method can find implicit 
relations between phrases in the source and target 
languages. Therefore, more source-target phrase 
pairs can be obtained than conventional pivot-
based method. Experimental results show that our 
method achieves significant improvements over 
the baseline on Europarl corpus, spoken language 
data and the web data.  
A critical problem in the approach is the noise 
that may bring in. In this paper, we used a simple 
filtering to reduce the noise. Although the filtering 
method is effective, other method may work better. 
In the future, we plan to explore more approaches 
for phrase table pruning. 
Acknowledgments 
We would like to thank Jianyun Nie, Muyun Yang 
and Lemao Liu for insightful discussions, and 
three anonymous reviewers for many invaluable 
comments and suggestions to improve our paper. 
This work is supported by National Natural Sci-
ence Foundation of China (61100093), and the 
Key Project of the National High Technology Re-
search and Development Program of China 
(2011AA01A207). 
References  
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics, pages 
597-604 
Sergey Brin and Lawrence Page. 1998. The Anatomy of 
a Large-Scale Hypertextual Web Search Engine. In 
Proceedings of the Seventh International World 
Wide Web Conference  
Trevor Cohn and Mirella Lapata. 2007. Machine Trans-
lation by Triangulation: Make Effective Use of Mul-
ti-Parallel Corpora. In Proceedings of 45th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 828-735. 
Marta R. Costa-juss?, Carlos Henr?quez, and Rafael E. 
Banchs. 2011. Enhancing Scarce-Resource Language 
Translation through Pivot Combinations. In Proceed-
ings of the 5th International Joint Conference on 
Natural Language Processing, pages 1361-1365 
Nick Craswell and Martin Szummer. 2007. Random 
Walks on the Click Graph. In Proceedings of the 
30th annual international ACM SIGIR conference on 
Research and development in information retrieval, 
pages 239-246 
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun Zhao 
and Dequan Zheng. 2013. Phrase Table Combination 
Deficiency Analyses in Pivot-based SMT. In Pro-
ceedings of 18th International Conference on Appli-
cation of Natural Language to Information Systems, 
pages 355-358. 
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime 
Tsukada and Masaaki Nagata. 2011. Generalized 
Minimum Bayes Risk System Combination. In Pro-
ceedings of the 5th International Joint Conference 
on Natural Language Processing, pages 1356?1360 
Jes?s Gonz?lez-Rubio, Alfons Juan and Francisco 
Casacuberta. 2011. Minimum Bayes-risk System 
Figure 5: Phrase extension process. The dotted line 
indicates an implicit relation in the phrase table. 
??? 
ge zhentou 
?? 
zhentou 
pillow 
a pillow 
almohada 
una 
almohada 
533
Combination. In Proceedings of the 49th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 1268?1277 
Zhongjun He, Yao Meng, Yajuan L?, Hao Yu and Qun 
Liu. 2009. Reducing SMT Rule Table with Mono-
lingual Key Phrase. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 121-
124 
Howard Johnson, Joel Martin, George Foster, and Ro-
land Kuhn. 2007. Improving  translation quality by 
discarding most of the phrase table. In Proceedings 
of the 2007 Joint Conference on Empirical Methods 
in Natural Language Processing and Computational 
Natural Language Learning, pages 967?975. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In HLT-NAACL: 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, pages 127-133 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 388?395. 
Philipp Koehn. 2005. Europarl: A Parallel Corpus for 
Statistical Machine Translation. In Proceedings of 
MT Summit X, pages 79-86. 
Philipp Koehn, Hieu Hoang, Alexanda Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Rich-
ard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
In Proceedings of the 45th Annual Meeting of the 
Association for Computational Linguistics, demon-
stration session, pages 177?180. 
Franz Josef Och and Hermann Ney. 2000. A compari-
son of alignment models for statistical machine 
translation. In Proceedings of the 18th International 
Conference on Computational Linguistics, pages 
1086?1090 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings 
of the 40th Annual Meeting of the Association for 
Computation Linguistics, pages 311-319 
Karl Pearson. 1905. The Problem of the Random Walk. 
Nature, 27(1865):294 
Mydans, Seth. 2011. Across cultures, English is the 
word. New York Times. 
Martin Szummer and Tommi Jaakkola. 2002. Partially 
Labeled Classification with Markov Random Walks. 
In Advances in Neural Information Processing Sys-
tems, pages 945-952 
Kristina Toutanova, Christopher D. Manning and An-
drew Y. Ng. 2004. Learning Random Walk Models 
for Inducting Word Dependency Distributions. In 
Proceedings of the 21st International Conference on 
Machine Learning.  
Masao Utiyama and Hitoshi Isahara. 2007. A Compari-
son of Pivot Methods for Phrase-Based Statistical 
Machine Translation. In Proceedings of Human 
Language Technology: the Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics, pages 484-491 
Masao Utiyama, Andrew Finch, Hideo Okuma, Michael 
Paul, Hailong Cao, Hirofumi Yamamoto, Keiji Ya-
suda, and Eiichiro Sumita. 2008. The NICT/ATR 
speech Translation System for IWSLT 2008. In Pro-
ceedings of the International Workshop on Spoken 
Language Translation, pages 77-84 
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi Liu, 
Jianfeng Li, Dengjun Ren, and Zhengyu Niu. 2008. 
The TCH Machine Translation System for IWSLT 
2008. In Proceedings of the International Workshop 
on Spoken Language Translation, pages 124-131 
Hua Wu and Haifeng Wang. 2007. Pivot Language Ap-
proach for Phrase-Based Statistical Machine Transla-
tion. In Proceedings of 45th Annual Meeting of the 
Association for Computational Linguistics, pages 
856-863.  
Hua Wu and Haifeng Wang. 2009. Revisiting Pivot 
Language Approach for Machine Translation. In 
Proceedings of the 47th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 4th 
IJCNLP of the AFNLP, pages 154-162 
Ying Zhang, Fei Huang, Stephan Vogel. 2005. Mining 
translations of OOV terms from the web through 
cross-lingual query expansion. In Proceedings of the 
27th ACM SIGIR. pages 524-525 
 
 
534
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Improving Pivot-Based Statistical Machine Translation by Pivoting 
the Co-occurrence Count of Phrase Pairs 
 
Xiaoning Zhu1*, Zhongjun He2, Hua Wu2, Conghui Zhu1,  
Haifeng Wang2, and Tiejun Zhao1 
Harbin Institute of Technology, Harbin, China1 
{xnzhu,chzhu,tjzhao}@mtlab.hit.edu.cn 
Baidu Inc., Beijing, China2 
{hezhongjun,wu_hua,wanghaifeng}@baidu.com 
 
 
                                                 
* This work was done when the first author was visiting Baidu. 
Abstract 
To overcome the scarceness of bilingual 
corpora for some language pairs in ma-
chine translation, pivot-based SMT uses 
pivot language as a "bridge" to generate 
source-target translation from source-
pivot and pivot-target translation. One of 
the key issues is to estimate the probabili-
ties for the generated phrase pairs. In this 
paper, we present a novel approach to 
calculate the translation probability by 
pivoting the co-occurrence count of 
source-pivot and pivot-target phrase pairs. 
Experimental results on Europarl data 
and web data show that our method leads 
to significant improvements over the 
baseline systems. 
1 Introduction 
Statistical Machine Translation (SMT) relies on 
large bilingual parallel data to produce high qual-
ity translation results. Unfortunately, for some 
language pairs, large bilingual corpora are not 
readily available. To alleviate the parallel data 
scarceness, a conventional solution is to intro-
duce a ?bridge? language (named pivot language) 
to connect the source and target language (de 
Gispert and Marino, 2006; Utiyama and Isahara, 
2007; Wu and Wang, 2007; Bertoldi et al., 2008; 
Paul et al., 2011; El Kholy et al., 2013; Zahabi et 
al., 2013), where there are large amounts of 
source-pivot and pivot-target parallel corpora. 
Among various pivot-based approaches, the 
triangulation method (Cohn and Lapata, 2007; 
Wu and Wang, 2007) is a representative work in 
pivot-based machine translation. The approach 
proposes to build a source-target phrase table by 
merging the source-pivot and pivot-target phrase 
table. One of the key issues in this method is to 
estimate the translation probabilities for the gen-
erated source-target phrase pairs. Conventionally, 
the probabilities are estimated by multiplying the 
posterior probabilities of source-pivot and pivot-
target phrase pairs. However, it has been shown 
that the generated probabilities are not accurate 
enough (Cui et al., 2013). One possible reason 
may lie in the non-uniformity of the probability 
space. Through Figure 1. (a), we can see that the 
probability distributions of source-pivot and piv-
ot-target language are calculated separately, and 
the source-target probability distributions are 
induced from the source-pivot and pivot-target 
probability distributions. Because of the absence 
of the pivot language (e.g., p2 is in source-pivot 
probability space but not in pivot-target one), the 
induced source-target probability distribution is 
not complete, which will result in inaccurate 
probabilities.  
To solve this problem, we propose a novel ap-
proach that utilizes the co-occurrence count of 
source-target phrase pairs to estimate phrase 
translation probabilities more precisely. Different 
from the triangulation method, which merges the 
source-pivot and pivot-target phrase pairs after 
training the translation model, we propose to 
merge the source-pivot and pivot-target phrase 
pairs immediately after the phrase extraction step, 
and estimate the co-occurrence count of the 
source-pivot-target phrase pairs. Finally, we 
compute the translation probabilities according 
to the estimated co-occurrence counts, using the 
standard training method in phrase-based SMT 
(Koehn et al., 2003). As Figure 1. (b) shows, the 
1665
source-target probability distributions are calcu-
lated in a complete probability space. Thus, it 
will be more accurate than the traditional trian-
gulation method. Figure 2. (a) and (b) show the 
difference between the triangulation method and 
our co-occurrence count method. 
Furthermore, it is common that a small stand-
ard bilingual corpus can be available between the 
source and target language. The direct translation 
model trained with the standard bilingual corpus 
exceeds in translation performance, but its weak-
ness lies in low phrase coverage. However, the 
pivot model has characteristics characters. Thus, 
it is important to combine the direct and pivot 
translation model to compensate mutually and 
further improve the translation performance. To 
deal with this problem, we propose a mixed 
model by merging the phrase pairs extracted by 
pivot-based method and the phrase pairs extract-
ed from the standard bilingual corpus. Note that, 
this is different from the conventional interpola-
tion method, which interpolates the direct and 
pivot translation model. See Figure 2. (b) and (c) 
for further illustration. 
(a) the triangulation method                         (b) the co-occurrence count method 
 
Figure 1: An example of probability space evolution in pivot translation. 
 
 
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
PT phrase 
pairs
SP model PT model
ST pivot 
model
Phrase Extraction Phrase Extraction
Train Train
Merge
Standard 
ST corpus
ST phrase 
pairs
ST direct 
model
Phrase Extraction
Train
Interpolate
ST interpolated 
model
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
ST pivot 
model
ST phrase 
pairs
Phrase Extraction Phrase Extraction
Train
PT phrase 
pairs
Merge
Standard 
ST corpus
ST phrase 
pairs
ST direct 
model
Phrase Extraction
Train
Interpolate
ST interpolated 
model
Large SP 
corpus
Large PT 
corpus
SP phrase 
pairs
PT phrase 
pairs
ST mixed 
pairs
ST phrase 
pairs
Phrase Extraction Phrase Extraction
Merge
Standard 
ST corpus
ST phrase 
pairs
Phrase Extraction
Train
ST mixed 
model
Mix
        (a) the triangulation method        (b) the co-occurrence count method            (c) the mixed model 
 
Figure 2: Framework of the triangulation method, the co-occurrence count method and the mixed 
model. The shaded box in (b) denotes difference between the co-occurrence count method and the 
triangulation method. The shaded box in (c) denotes the difference between the interpolation model 
and the mixed model. 
1666
The remainder of this paper is organized as 
follows. In Section 2, we describe the related 
work. We introduce the co-occurrence count 
method in Section 3, and the mixed model in 
Section 4. In Section 5 and Section 6, we de-
scribe and analyze the experiments. Section 7 
gives a conclusion of the paper. 
2 Related Work 
Several methods have been proposed for pivot-
based translation. Typically, they can be classi-
fied into 3 kinds as follows: 
Transfer Method: The transfer method 
(Utiyama and Isahara, 2007; Wang et al., 2008; 
Costa-juss? et al., 2011) connects two translation 
systems: a source-pivot MT system and a pivot-
target MT system. Given a source sentence, (1) 
the source-pivot MT system translates it into the 
pivot language, (2) and the pivot-target MT sys-
tem translates the pivot sentence into the target 
sentence. During each step (source to pivot and 
pivot to target), multiple translation outputs will 
be generated, thus a minimum Bayes-risk system 
combination method is often used to select the 
optimal sentence (Gonz?lez-Rubio et al., 2011; 
Duh et al., 2011). The problem with the transfer 
method is that it needs to decode twice. On one 
hand, the time cost is doubled; on the other hand, 
the translation error of the source-pivot transla-
tion system will be transferred to the pivot-target 
translation. 
Synthetic Method: It aims to create a synthet-
ic source-target corpus by: (1) translate the pivot 
part in source-pivot corpus into target language 
with a pivot-target model; (2) translate the pivot 
part in pivot-target corpus into source language 
with a pivot-source model; (3) combine the 
source sentences with translated target sentences 
or/and combine the target sentences with trans-
lated source sentences (Utiyama et al., 2008; Wu 
and Wang, 2009). However, it is difficult to 
build a high quality translation system with a 
corpus created by a machine translation system. 
Triangulation Method: The triangulation 
method obtains source-target phrase table by 
merging source-pivot and pivot-target phrase 
table entries with identical pivot language 
phrases and multiplying corresponding posterior 
probabilities (Wu and Wang, 2007; Cohn and 
Lapata, 2007), which has been shown to work 
better than the other pivot approaches (Utiyama 
and Isahara, 2007). A problem of this approach is 
that the probability space of the source-target 
phrase pairs is non-uniformity due to the mis-
matching of the pivot phrase.  
3 Our Approach 
In this section, we will introduce our method for 
learning a source-target phrase translation model 
with a pivot language as a bridge. We extract the 
co-occurrence count of phrase pairs for each lan-
guage pair with a source-pivot and a pivot-target 
corpus. Then we generate the source-target 
phrase pairs with induced co-occurrence infor-
mation. Finally, we compute translation proba-
bilities using the standard phrase-based SMT 
training method. 
3.1 Phrase Translation Probabilities 
Following the standard phrase extraction method 
(Koehn et al., 2003), we can extract phrase pairs 
???, ???  and ???, ???  from the corresponding word-
aligned source-pivot and pivot-target training 
corpus, where ?? , ??  and ??  denotes the phrase in 
source, pivot and target language respectively. 
Formally, given the co-occurrence count 
????, ??? and ????, ???, we can estimate  ????, ???  by 
Equation 1: 
????, ??? ? ???????, ???, ????, ????
??
 (1) 
where ????  is a function to merge the co-
occurrences count ????, ???  and ????, ??? . We pro-
pose four calculation methods for function ????. 
Given the co-occurrence count ????, ???  and 
????, ???, we first need to induce the co-occurrence 
count ????, ?,? ??? . The ????, ?,? ???  is counted when 
the source phrase, pivot phrase and target phrase 
occurred together, thus we can infer that 
????, ?,? ???  is smaller than ????, ???  and ????, ??? . In 
this circumstance, we consider that ????, ?,? ???  is 
approximately equal to the minimum value of 
????, ??? and ????, ???, as shown in Equation 2. 
????, ??, ??? ? ?min?????, ???, ????, ????
??
 (2) 
Because the co-occurrence count of source-
target phrase pairs needs the existence of pivot 
phrase ?? , we intuitively believe that the co-
occurrence count ????, ???  is equal to the co-
occurrence count ????, ?,? ???. Under this assump-
tion, we can obtain the co-occurrence count 
????, ??? as shown in Equation 3. Furthermore, to 
testify our assumption, we also try the maximum 
value (Equation 4) to infer the co-occurrence 
count of ???, ???  phrase pair. 
1667
????, ??? ? ?min?????, ???, ????, ????
??
 (3) 
????, ??? ? ?max?????, ???, ????, ????
??
 (4) 
In addition, if source-pivot and pivot-target 
parallel corpus greatly differ in quantities, then 
the minimum function would likely just take the 
counts from the smaller corpus. To deal with the 
problem of the imbalance of the parallel corpora, 
we also try the arithmetic mean (Equation 5) and 
geometric mean (Equation 6) function to infer 
the co-occurrence count of source-target phrase 
pairs. 
????, ??? ? ??????, ??? ? ????, ????/2
??
 (5) 
????, ??? ? ??????, ??? ? ????, ???
??
 (6) 
When the co-occurrence count of source-target 
language is calculated, we can estimate the 
phrase translation probabilities with the follow-
ing Equation 7 and Equation 8. 
?????|?? ? ????, ???? ????, ?????  (7) 
????|??? ? ????, ???? ????, ?????  (8) 
3.2 Lexical Weight 
Given a phrase pair ???, ??? and a word alignment 
a between the source word positions ? ? 1,? , ? 
and the target word positions ? ? 0,? ,? , the 
lexical weight of phrase pair ???, ??? can be calcu-
lated by the following Equation 9 (Koehn et al., 
2003). 
??????|?, ?? ??
1
|??|??, ?? ? ??| ? ????|??????,????
?
???
(9) 
The lexical translation probability distribution 
???|?? between source word s and target word t 
can be estimated with Equation 10. 
???|?? ? ???, ??? ????, ????  (10)
To compute the lexical weight for a phrase 
pair ???, ??? generated by ???, ??? and ???, ???, we need 
the alignment information ?, which can be ob-
tained as Equation 11 shows. 
? ? ???, ??|??: ??, ?? ? ??&??, ?? ? ??? (11)
where ??  and ??  indicate the word alignment 
information in the phrase pair ???, ???  and ???, ??? 
respectively. 
4 Integrate with Direct Translation 
If a standard source-target bilingual corpus is 
available, we can train a direct translation model. 
Thus we can integrate the direct model and the 
pivot model to obtain further improvements. We 
propose a mixed model by merging the co-
occurrence count in direct translation and pivot 
translation. Besides, we also employ an interpo-
lated model (Wu and Wang, 2007) by merging 
the direct translation model and pivot translation 
model using a linear interpolation. 
4.1 Mixed Model 
Given ?  pivot languages, the co-occurrence 
count can be estimated using the method de-
scribed in Section 3.1. Then the co-occurrence 
count and the lexical weight of the mixed model 
can be estimated with the following Equation 12 
and 13. 
???, ?? ??????, ??
?
???
 (12)
??????|?, ?? ????
?
???
??,?????|?, ?? (13)
where ????, ??  and ??,?????|?, ??  are the co-
occurrence count and lexical weight in the direct 
translation model respectively. ????, ??  and 
??,?????|?, ?? denote the co-occurrence count and 
lexical weight in the pivot translation model. ?? 
is the interpolation coefficient, requiring 
? ?????? ? 1. 
4.2 Interpolated Model 
Following Wu and Wang (2007), the interpolated 
model can be modelled with Equation 14. 
?????|?? ? ?????????|??
?
???
 (14)
where ??????|?? is the phrase translation probabil-
ity in direct translation model; ??????|??  is the 
phrase translation probability in pivot translation 
model. The lexical weight is obtained with Equa-
tion 13. ?? is the interpolation coefficient, requir-
ing ? ?? ? 1???? . 
1668
5 Experiments on Europarl Corpus 
Our first experiment is carried out on Europarl1 
corpus, which is a multi-lingual corpus including 
21 European languages (Koehn, 2005). In our 
work, we perform translations among French (fr), 
German (de) and Spanish (es). Due to the rich-
ness of available language resources, we choose 
English (en) as the pivot language. Table 1 
summarized the statistics of training data. For the 
language model, the same monolingual data ex-
tracted from the Europarl are used. 
The word alignment is obtained by GIZA++ 
(Och and Ney, 2000) and the heuristics ?grow-
diag-final? refinement rule (Koehn et al., 2003). 
Our translation system is an in-house phrase-
based system analogous to Moses (Koehn et al., 
2007). The baseline system is the triangulation 
method (Wu and Wang, 2007), including an in-
terpolated model which linearly interpolate the 
direct and pivot translation model. 
                                                 
1 http://www.statmt.org/europarl 
We use WMT082  as our test data, which con-
tains 2000 in-domain sentences and 2051 out-of-
domain sentences with single reference. The 
translation results are evaluated by case-
insensitive BLEU-4 metric (Papineni et al., 
2002). The statistical significance tests using 
95% confidence interval are measured with 
paired bootstrap resampling (Koehn, 2004). 
5.1 Results 
We compare 4 merging methods with the base-
line system. The results are shown in Table 2 and 
Table 3. We find that the minimum method out-
performs the others, achieving significant im-
provements over the baseline on all translation 
directions. The absolute improvements range 
from 0.61 (fr-de) to 1.54 (es-fr) in BLEU% score 
on in-domain test data, and range from 0.36 (fr-
de) to 2.05 (fr-es) in BLEU% score on out-of-
domain test data. This indicates that our method 
is effective and robust in general. 
                                                 
2 http://www.statmt.org/wmt08/shared-task.html 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words
Target 
Words
de-en 1.9M 48.5M 50.9M
es-en 1.9M 54M 51.7M
fr-en 2M 58.1M 52.4M
 
Table 1: Training data of Europarl corpus 
 
System 
BLEU% 
de-es de-fr es-de es-fr fr-de fr-es 
Baseline 27.04 23.01 20.65 33.84 20.87 38.31 
Minimum 27.93* 23.94* 21.52* 35.38* 21.48* 39.62* 
Maximum 25.70 21.59 20.26 32.58 20.50 37.30 
Arithmetic mean 26.01 22.24 20.13 33.38 20.37 37.37 
Geometric mean 27.31 23.49* 21.10* 34.76* 21.15* 39.19* 
 
Table 2: Comparison of different merging methods on in-domain test set. * indicates the results are 
significantly better than the baseline (p<0.05). 
 
System 
BLEU% 
de-es de-fr es-de es-fr fr-de fr-es 
Baseline 15.34 13.52 11.47 21.99 12.19 25.00 
Minimum 15.77* 14.08* 11.99* 23.90* 12.55* 27.05* 
Maximum 13.41 11.83 10.17 20.48 10.83 22.75 
Arithmetic mean 13.96 12.10 10.57 21.07 11.30 23.70 
Geometric mean 15.09 13.30 11.52 23.32* 12.46* 26.22* 
 
Table 3: Comparison of different merging methods on out-of-domain test set. 
 
1669
The geometric mean method also achieves im-
provement, but not as significant as the minimum 
method. However, the maximum and the arith-
metic mean methods show a decrement in BLEU 
scores. This reminds us that how to choose a 
proper merging function for the co-occurrence 
count is a key problem.  In the future, we will 
explore more sophisticated method to merge co-
occurrence count. 
5.2 Analysis 
The pivot-based translation is suitable for the 
scenario that there exists large amount of source-
pivot and pivot-target bilingual corpora and only 
a little source-target bilingual data. Thus, we 
randomly select 10K, 50K, 100K, 200K, 500K, 
1M, 1.5M sentence pairs from the source-target 
bilingual corpora to simulate the lack of source-
target data. With these corpora, we train several 
direct translation models with different scales of 
bilingual data. We interpolate each direct transla-
tion model with the pivot model (both triangula-
tion method and co-occurrence count method) to 
obtain the interpolated model respectively. We 
also mix the direct model and pivot model using 
the method described in Section 4.1.  Following 
 
(a) German-English-Spanish                                        (b) German-English-French 
 
 
(c) Spanish-English-German                                        (d) Spanish-English-French 
 
 
(e) French-English-German                                         (f) French-English-Spanish 
 
Figure 3: Comparisons of pivot-based methods on different scales of source-target standard corpora. 
(direct: direct model; tri: triangulation model; co: co-occurrence count model; tri+inter: triangulation 
model interpolated with direct model ; co+inter: co-occurrence count model interpolated with direct 
model; co+mix: mixed model). X-axis represents the scale of the standard training data. 
22.5
23
23.5
24
24.5
25
25.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
26.5
27
27.5
28
28.5
29
29.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
33.5
34
34.5
35
35.5
36
36.5
37
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
19.5
20
20.5
21
21.5
22
22.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
37.5
38
38.5
39
39.5
40
40.5
41
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
19.5
20
20.5
21
21.5
22
22.5
BL
EU
%
direct
tri
co
tri+inter
co+inter
co+mix
1670
Wu and Wang (2007), we set ?? ? 0.9, ?? ? 0.1, 
?? ? 0.9  and ?? ? 0.1  empirically. The experi-
ments are carried out on 6 translation directions: 
German-Spanish, German-French, Spanish-
German, Spanish-French, French-German and 
French-Spanish. The results are shown in Figure 
3. We only list the results on in-domain test sets. 
The trend of the results on out-of domain test 
sets is similar with in-domain test sets. 
The results are explained as follows: 
(1) Comparison of Pivot Translation and Di-
rect Translation 
The pivot translation models are better than 
the direct translation models trained on a small 
source-target bilingual corpus. With the incre-
ment of source-target corpus, the direct model 
first outperforms the triangulation model and 
then outperforms the co-occurrence count model 
consecutively. 
Taking Spanish-English-French translation as 
an example, the co-occurrence count model 
achieves BLEU% scores of 35.38, which is close 
to the direct translation model trained with 200K 
source-target bilingual data. Compared with the 
co-occurrence count model, the triangulation 
model only achieves BLEU% scores of 33.84, 
which is close to the direct translation model 
trained with 50K source-target bilingual data. 
(2) Comparison of Different Interpolated 
Models 
For the pivot model trained by triangulation 
method and co-occurrence count method, we 
interpolate them with the direct translation model 
trained with different scales of bilingual data. 
Figure 3 shows the translation results of the dif-
ferent interpolated models. For all the translation 
directions, our co-occurrence count method in-
terpolated with the direct model is better than the 
triangulation model interpolated with the direct 
model.  
The two interpolated model are all better than 
the direct translation model. With the increment 
of the source-target training corpus, the gap be-
comes smaller. This indicates that the pivot mod-
el and its affiliated interpolated model are suita-
ble for language pairs with small bilingual data. 
Even if the scale of source-pivot and pivot-target 
corpora is close to the scale of source-target bi-
lingual corpora, the pivot translation model can 
help the direct translation model to improve the 
translation performance. Take Spanish-English-
French translation as an issue, when the scale of 
Spanish-French parallel data is 1.5M sentences 
pairs, which is close to the Spanish-English and 
English-French parallel data, the performance of 
co+mix model is still outperforms the direct 
translation model. 
(3) Comparison of Interpolated Model and 
Mixed Model 
When only a small source-target bilingual 
corpus is available, the mix model outperforms 
the interpolated model. With the increasing of 
source-target corpus, the mix model is close to 
the interpolated model or worse than the interpo-
lated model. This indicates that the mix model 
has a better performance when the source-target 
corpus is small which is close to the realistic sce-
nario. 
5.3 Integrate the Co-occurrence Count 
Model and Triangulation Model 
Experimental results in the previous section 
show that, our co-occurrence count models gen-
erally outperform the baseline system. In this 
section, we carry out experiments that integrates 
co-occurrence count model into the triangulation 
model. 
For French-English-German translation, we 
apply a linear interpolation method to integrate 
the co-occurrence count model into triangulation 
model following the method described in Section 
4.2.  We set ? as the interpolation coefficient of 
triangulation model and 1 ? ? as the interpola-
tion coefficient of co-occurrence count model 
respectively. The experiments take 9 values for 
interpolation coefficient, from 0.1 to 0.9. The 
results are shown in Figure 4. 
 
 
Figure 4: Results of integrating the co-
occurrence count model and the triangulation 
model. 
 
When using interpolation coefficient ranging 
from 0.2 to 0.7, the integrated models outperform 
the triangulation and the co-occurrence count 
model. However, for the other intervals, the inte-
20.4
20.6
20.8
21
21.2
21.4
21.6
21.8
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
BL
EU
%
Interpolation Coefficient 
integrated triangulation
co-occurrence
1671
grated models perform slightly lower than the 
co-occurrence count model, but still show better 
results than the triangulation model. The trend of 
the curve infers that the integrated model synthe-
sizes the contributions of co-occurrence count 
model and triangulation model. Additionally, it 
also indicates that, the choice of the interpolation 
coefficient affects the translation performances. 
6 Experiments on Web Data 
The experimental on Europarl is artificial, as the 
training data for directly translating between 
source and target language actually exists in the 
original data sets. Thus, we conducted several 
experiments on a more realistic scenario: trans-
lating Chinese (zh) to Japanese (jp) via English 
(en) with web crawled data. 
As mentioned in Section 3.1, the source-pivot 
and pivot-target parallel corpora can be imbal-
anced in quantities. If one parallel corpus was 
much larger than another, then minimum heuris-
tic function would likely just take the counts 
from the smaller corpus.  
In order to analyze this issue, we manually set 
up imbalanced corpora. For source-pivot parallel 
corpora, we randomly select 1M, 2M, 3M, 4M 
and 5M Chinese-English sentence pairs. On the 
other hand, we randomly select 1M English-
Japanese sentence pairs as pivot-target parallel 
corpora. The training data of Chinese-English 
and English-Japanese language pairs are summa-
rized in Table 4. For the Chinese-Japanese direct 
corpus, we randomly select 5K, 10K, 20K, 30K, 
40K, 50K, 60K, 70K, 80K, 90K and 100K sen-
tence pairs to simulate the lack of bilingual data. 
We built a 1K in-house test set with four refer-
ences. For Japanese language model training, we 
used the monolingual part of English-Japanese 
corpus. 
Table 5 shows the results of different co-
occurrence count merging methods. First, the 
minimum method and the geometric mean meth-
od outperform the other two merging methods 
and the baseline system with different training 
corpus. When the scale of source-pivot and piv-
ot-target corpus is roughly balanced (zh-en-jp-1), 
the minimum method achieves an absolute im-
provement of 2.06 percentages points on BLEU 
over the baseline, which is also better than the 
other merging methods. While, with the growth 
of source-pivot corpus, the gap between source-
pivot corpus and pivot-target corpus becomes 
bigger. In this circumstance, the geometric mean 
method becomes better than the minimum meth-
od. Compared to the minimum method, the geo-
metric mean method considers both the source-
pivot and the pivot-target corpus, which may 
lead to a better result in the case of imbalanced 
training corpus. 
Language 
Pairs 
Sentence 
Pairs 
Source 
Words
Target 
Words
zh-en-1 1M 18.1M 17.7M
zh-en-2 2M 36.2M 35.5M
zh-en-3 3M 54.2M 53.2M
zh-en-4 4M 72.3M 70.9M
zh-en-5 5M 90.4M 88.6M
en-jp 1M 9.2M 11.1M
 
Table 4: Training data of web corpus 
 
System 
BLEU% 
zh-en-jp-1* zh-en-jp-2 zh-en-jp-3 zh-en-jp-4 zh-en-jp-5
Baseline 29.07 29.39 29.44 29.67 29.80 
Minimum 31.13* 31.28* 31.43* 31.62* 32.02* 
Maximum 28.88 29.01 29.12 29.37 29.59 
Arithmetic mean 29.08 29.36 29.51 29.79 30.01 
Geometric mean 30.77* 31.30* 31.75* 32.07* 32.34* 
 
Table 5: Comparison of different merging methods on the imbalanced web data. ( zh-en-jp-1 means 
the translation system is trained with zh-en-1 as source-pivot corpus and en-jp as pivot-target corpus, 
and so on. ) 
1672
Furthermore, with the imbalanced corpus zh-
en-jp-5, we compared the translation perfor-
mance of our co-occurrence count model (with 
geometric mean merging method), triangulation 
model, interpolated model, mixed model and the 
direct translation models. Figure 5 summarized 
the results. 
The co-occurrence count model can achieve an 
absolute improvement of 2.54 percentages points 
on BLEU over the baseline. The triangulation 
method outperforms the direct translation when 
only 5K sentence pairs are available. Meanwhile, 
the number is 10K when using the co-occurrence 
count method. The co-occurrence count models 
interpolated with the direct model significantly 
outperform the other models. 
 
 
Figure 5: Results on Chinese-Japanese Web Data. 
X-axis represents the scale of the standard train-
ing data. 
 
In this experiment, the training data contains 
parallel sentences on various domains. And the 
training corpora (Chinese-English and English-
Japanese) are typically very different, since they 
are obtained on the web. It indicates that our co-
occurrence count method is robust in the realistic 
scenario. 
7 Conclusion 
This paper proposed a novel approach for pivot-
based SMT by pivoting the co-occurrence count 
of phrase pairs. Different from the triangulation 
method merging the source-pivot and pivot-
target language after training the translation 
model, our method merges the source-pivot and 
pivot-target language after extracting the phrase 
pairs, thus the computing for phrase translation 
probabilities is under the uniform probability 
space. The experimental results on Europarl data 
and web data show significant improvements 
over the baseline systems. We also proposed a 
mixed model to combine the direct translation 
and pivot translation, and the experimental re-
sults show that the mixed model has a better per-
formance when the source-target corpus is small 
which is close to the realistic scenario. 
A key problem in the approach is how to learn 
the co-occurrence count. In this paper, we use the 
minimum function on balanced corpora and the 
geometric mean function on imbalanced corpora 
to estimate the co-occurrence count intuitively. 
In the future, we plan to explore more effective 
approaches. 
Acknowledgments 
We would like to thank Yiming Cui for insight-
ful discussions, and three anonymous reviewers 
for many invaluable comments and suggestions 
to improve our paper. This work is supported by 
National Natural Science Foundation of China 
(61100093), and the State Key Development 
Program for Basic Research of China (973 Pro-
gram, 2014CB340505). 
Reference 
Nicola Bertoldi, Madalina Barbaiani, Marcello 
Federico, and Roldano Cattoni. 2008. Phrase-
Based statistical machine translation with Piv-
ot Languages. In Proceedings of the 5th Inter-
national Workshop on Spoken Language 
Translation (IWSLT), pages 143-149. 
Trevor Cohn and Mirella Lapata. 2007. Machine 
Translation by Triangulation: Make Effective 
Use of Multi-Parallel Corpora. In Proceedings 
of 45th Annual Meeting of the Association for 
Computational Linguistics, pages 828-735. 
Marta R. Costa-juss?, Carlos Henr?quez, and Ra-
fael E. Banchs. 2011. Enhancing Scarce-
Resource Language Translation through Pivot 
Combinations. In Proceedings of the 5th In-
ternational Joint Conference on Natural Lan-
guage Processing, pages 1361-1365. 
Yiming Cui, Conghui Zhu, Xiaoning Zhu, Tiejun 
Zhao and Dequan Zheng. 2013. Phrase Table 
Combination Deficiency Analyses in Pivot-
based SMT. In Proceedings of 18th Interna-
tional Conference on Application of Natural 
Language to Information Systems, pages 355-
358. 
Adria de Gispert and Jose B. Marino. 2006. 
Catalan-English statistical machine translation 
without parallel corpus: bridging through 
Spanish. In Proceedings of 5th International 
Conference on Language Resources and Eval-
uation (LREC), pages 65-68. 
29
31
33
35
37
39
5K 20K 40K 60K 80K 100K
BL
EU
%
direct
tri
co-occur
tri+inter
co+inter
co+mix
1673
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, 
Hajime Tsukada and Masaaki Nagata. 2011. 
Generalized Minimum Bayes Risk System 
Combination. In Proceedings of the 5th Inter-
national Joint Conference on Natural Lan-
guage Processing, pages 1356-1360. 
Ahmed El Kholy, Nizar Habash, Gregor Leusch, 
Evgeny Matusov and Hassan Sawaf. 2013. 
Language Independent Connectivity Strength 
Features for Phrase Pivot Statistical Machine 
Translation. In Proceedings of the 51st Annual 
Meeting of the Association for Computational 
Linguistics, pages 412-418. 
Ahmed El Kholy, Nizar Habash, Gregor Leusch, 
Evgeny Matusov and Hassan Sawaf. 2013. Se-
lective Combination of Pivot and Direct Sta-
tistical Machine Translation Models. In Pro-
ceedings of the 6th International Joint Confer-
ence on Natural Language Processing, pages 
1174-1180. 
Jes?s Gonz?lez-Rubio, Alfons Juan and Francis-
co Casacuberta. 2011. Minimum Bayes-risk 
System Combination. In Proceedings of the 
49th Annual Meeting of the Association for 
Computational Linguistics, pages 1268-1277. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
HLT-NAACL: Human Language Technology 
Conference of the North American Chapter of 
the Association for Computational Linguistics, 
pages 127-133. 
Philipp Koehn. 2004. Statistical significance 
tests for machine translation evaluation. In 
Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Pro-
cessing (EMNLP), pages 388-395. 
Philipp Koehn. 2005. Europarl: A Parallel Cor-
pus for Statistical Machine Translation. In 
Proceedings of MT Summit X, pages 79-86. 
Philipp Koehn, Hieu Hoang, Alexanda Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ondrej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: Open Source Toolkit for 
Statistical Machine Translation. In Proceed-
ings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, demon-
stration session, pages 177-180. 
Philipp Koehn, Alexandra Birch, and Ralf Stein-
berger. 2009. 462 Machine Translation Sys-
tems for Europe. In Proceedings of the MT 
Summit XII. 
Gregor Leusch, Aur?lien Max, Josep Maria 
Crego and Hermann Ney. 2010. Multi-Pivot 
Translation by System Combination. In Pro-
ceedings of the 7th International Workshop on 
Spoken Language Translation, pages 299-306. 
Franz Josef Och and Hermann Ney. 2000. A 
comparison of alignment models for statistical 
machine translation. In Proceedings of the 
18th International Conference on Computa-
tional Linguistics, pages 1086-1090. 
Michael Paul, Andrew Finch, Paul R. Dixon and 
Eiichiro Sumita. 2011. Dialect Translation: In-
tegrating Bayesian Co-segmentation Models 
with Pivot-based SMT. In Proceedings of the 
2011 Conference on Empirical Methods in 
Natural Language Processing, pages 1-9. 
Michael Paul and Eiichiro Sumita. 2011. Trans-
lation Quality Indicators for Pivot-based Sta-
tistical MT. In Proceedings of the 5th Interna-
tional Joint Conference on Natural Language 
Processing, pages 811-818. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computation Linguistics, pag-
es 311-319. 
Rie Tanaka, Yohei Murakami and Toru Ishida. 
2009. Context-Based Approach for Pivot 
Translation Services. In the Twenty-first In-
ternational Conference on Artificial Intelli-
gence, pages 1555-1561. 
J?rg Tiedemann. 2012. Character-Based Pivot 
Translation for Under-Resourced Languages 
and Domains. In Proceedings of the 13th Con-
ference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 
141-151. 
Masatoshi Tsuchiya, Ayu Purwarianti, Toshiyu-
kiWakita and Seiichi Nakagawa. 2007. Ex-
panding Indonesian-Japanese Small Transla-
tion Dictionary Using a Pivot Language. In 
Proceedings of the ACL 2007 Demo and Post-
er Sessions, pages 197-200. 
Takashi Tsunakawa, Naoaki Okazaki and 
Jun'ichi Tsujii. 2010. Building a Bilingual 
Lexicon Using Phrase-based Statistical Ma-
chine Translation via a Pivot Language. In 
1674
Proceedings of the 22th International Confer-
ence on Computational Linguistics (Coling), 
pages 127-130. 
Masao Utiyama and Hitoshi Isahara. 2007. A 
Comparison of Pivot Methods for Phrase-
Based Statistical Machine Translation. In Pro-
ceedings of Human Language Technology: the 
Conference of the North American Chapter of 
the Association for Computational Linguistics, 
pages 484-491. 
Masao Utiyama, Andrew Finch, Hideo Okuma, 
Michael Paul, Hailong Cao, Hirofumi Yama-
moto, Keiji Yasuda,and Eiichiro Sumita. 2008. 
The NICT/ATR speech Translation System for 
IWSLT 2008. In Proceedings of the Interna-
tional Workshop on Spoken Language Trans-
lation, pages 77-84. 
Haifeng Wang, Hua Wu, Xiaoguang Hu, Zhanyi 
Liu, Jianfeng Li, Dengjun Ren, and Zhengyu 
Niu. 2008. The TCH Machine Translation 
System for IWSLT 2008. In Proceedings of 
the International Workshop on Spoken Lan-
guage Translation, pages 124-131. 
Hua Wu and Haifeng Wang. 2007. Pivot Lan-
guage Approach for Phrase-Based Statistical 
Machine Translation. In Proceedings of 45th 
Annual Meeting of the Association for Compu-
tational Linguistics, pages 856-863. 
Hua Wu and Haifeng Wang. 2009. Revisiting 
Pivot Language Approach for Machine Trans-
lation. In Proceedings of the 47th Annual 
Meeting of the Association for Computational 
Linguistics and the 4th IJCNLP of the AFNLP, 
pages 154-162. 
Samira Tofighi Zahabi, Somayeh Bakhshaei and 
Shahram Khadivi. Using Context Vectors in 
Improving a Machine Translation System with 
Bridge Language. In Proceedings of the 51st 
Annual Meeting of the Association for Compu-
tational Linguistics, pages 318-322. 
 
 
1675
Proceedings of NAACL-HLT 2013, pages 563?568,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Compound Embedding Features for Semi-supervised Learning   Mo Yu1, Tiejun Zhao1, Daxiang Dong2, Hao Tian2 and Dianhai Yu2 Harbin Institute of Technology, Harbin, China Baidu Inc., Beijing, China {yumo,tjzhao}@mtlab.hit.edu.cn {dongdaxiang,tianhao,yudianhai}@baidu.com      Abstract 
To solve data sparsity problem, recently there has been a trend in discriminative methods of NLP to use representations of lexical items learned from unlabeled data as features. In this paper, we investigated the usage of word representations learned by neural language models, i.e. word embeddings. The direct us-age has disadvantages such as large amount of computation, inadequacy with dealing word ambiguity and rare-words, and the problem of linear non-separability. To overcome these problems, we instead built compound features from continuous word embeddings based on clustering. Experiments showed that the com-pound features not only improved the perfor-mances on several NLP tasks, but also ran faster, suggesting the potential of embeddings.  
1 Introduction Supervised learning methods have achieved great successes in the field of Natural Language Pro-cessing (NLP). However, in practice most methods are usually limited by the problem of data sparsity, since it is impossible to obtain sufficient labeled data for all NLP tasks. In these situations semi-supervised learning can help to make use of both labeled data and easy-to-obtain unlabeled data. The semi-supervised framework that is widely applied to NLP is to first learn word representa-tions, which are feature vectors of lexical items, from unlabeled data and then plug them into a su-pervised system. These methods are very effective in utilizing large-scale unlabeled data and have successfully improved performances of state-of-
the-art supervised systems on a variety of tasks (Koo et al, 2008; Huang and Yates, 2009; T?ck-str?m et al, 2012).  With the development of neural language mod-els (NLM) (Bengio et al, 2003; Mnih and Hinton, 2009), recently researchers become interested in word representations (also called word embed-dings) learned by these models. Word embeddings are dense low dimensional real-valued vectors. They are composed of some latent features, which are expected to capture useful syntactic and seman-tic properties. Word embeddings are usually served as the first layer in deep learning systems for NLP (Collobert and Weston, 2008; Socher et al, 2011a, 2011b) and help these systems perform compara-bly with the state-of-the-art models based on hand-crafted features. They also have been directly added as features to the state-of-the-art models of chunking and NER, and have achieved significant improvements (Turian et al 2010). Although the direct usage of continuous embed-dings has been proved to be an effective method for enhancing the state-of-the-art supervised mod-els, it has some disadvantages, which made them be out-performed by simpler Brown cluster fea-tures (Turian et al 2010) and made them computa-tionally complicated. Firstly, embeddings of rare words are insufficiently trained since they are only updated few times and are close to their random initial values. As shown in (Turian et al 2010), this is the main reason that models with embedding features made more errors than those with Brown cluster features. Secondly, in NLMs, each word has its unique representation, so it is difficult to represent different senses for ambiguous words. Thirdly, word embeddings are unsuitable for linear models in some tasks as will be proved in Section 
563
4.2. This is possibly because in these tasks, either the target labels are correlated with combinations of different dimensions of word embeddings, or discriminative information may be coded in differ-ent intervals in the same dimension. So treating embeddings directly as inputs to a linear model could not fully utilize them. Moreover, since em-beddings are dense vectors, it will introduce large amount of computations when they are directly used as inputs, making the method impractical. In this paper, we first introduced the idea of clustering embeddings to overcome the last two disadvantages discussed above. The high-dimensional cluster features make samples from different classes better separated by linear models. And models with these features can still run fast because the clusters are sparse and discrete.  Second, we proposed the compound features based on clustering. Compound features, which are conjunctive features of neighboring words, have been widely used in NLP models for improving the performances because they are more discriminative. Compound features of embeddings can also help a model to better predict labels associated with rare-words and ambiguous words, because compound features composed of embeddings of nearby words can help to better describe the property of these words. Compound features are difficult to build on dense embeddings. However they are easy to in-duce from the sparse embedding clusters proposed in this paper.   Experiments on chunking and NER showed that based on the same embeddings, the compound fea-tures managed to achieve better performances. Moreover, we proposed analyses to reveal the rea-sons for the improvements of embedding-clusters and compound features. They suggest that these features can better deal with rare-words and word ambiguity, and are more suitable for linear models. In addition, although Brown clustering was con-sidered better in (Turian et al2010), our experi-ment results and comparisons showed that our compound features from embedding clustering is at least comparable with those from Brown clustering. Since embeddings can greatly benefit from the im-provement and developing of deep learning in the future, we believe that our proposed method has a large space of performance growth and will benefit more applications in NLP. In the rest of the paper, Section 2 introduces how compound embedding features were obtained. 
Section 3 gives experimental results. In Section 4, we give analysis about the advantages of com-pound features. Section 5 gives the conclusions. 2 Clustering of Word Embeddings  
2.1 Learning Word Embeddings Word embeddings in this paper were trained by NLMs (Bengio et al, 2003). The model predicts the scores of probabilities of words given their context information in the sentences. It first con-verts the current word and its context words (e.g. n-1 words before it as in n-gram models) into em-beddings. Then these embeddings are put together and propagate forward on the network to compute the score of current word. After minimizing the loss on training data, embeddings are learned and can be further used as smoothing representations for words. 2.2 Clustering of embeddings In order to get compound features of embeddings, we first induce discrete clusters from the embed-dings. Concretely, the k-means clustering algo-rithm is used. Each word is treated as a single sample. A cluster is represented as the mean of the embeddings of words assigned to it. Similarities between words and clusters are measured by Eu-clidean distance. As discussed and experimented later, different numbers of ks contain information of different granularity. So we combine clustering results achieved by different ks as features to better utilize the embeddings. 2.3 Compound features Based on embedding clusters, more powerful com-pound features can be built. Compound features are conjunctions between basic features of words and their contexts, which are widely used in NLP. Koo et al (2008) also observed that compound features of Brown clusters achieved more im-provements on parsing.     It is also necessary to build compound embed-ding features since they can better deal with rare-words and ambiguous words. For example, alt-hough embedding of a rare-word is not fully trained and hence inaccurate, embeddings of its context words can still be accurate as long as they 
564
are not rare and are fully trained. So we could uti-lize the combination of embeddings before and after the word to predict its tag correctly. We con-ducted analysis to verify our theory in Section4. We combined the compound features together with other state-of-the-art human-craft features in supervised models. Examples of the resulted fea-ture templates in chunking and NER are shown in Table 1 & 2. The feature 
1101 ccyy ??  in the last row is an example of compound feature made up of the embedding clusters of words before and af-ter current word. Compound feature extraction can similarly be applied to form compound features of Brown clusters. For example, Brown clusters can replace embedding clusters in 3th row of Table 1. Words }1,0{,1}2:2{, , ???? iiiii www  POS }2,1{,1}2:2{, , ????? iiiii ppp  Cluster 11}1,0{,1}2:2{, ,, ccccc iiiii ?????  Transition },,,{ 1100001 cccpwyy ??  Table 1: Chunking features. Cluster features are suitable for both Brown clusters and embedding clusters. Sym-bol iy is the tag predicted on word iw . Words }1,0{,1}2:2{, , ???? iiiii www  Pre/suffix 1: }4:1{,0:1 }4:2{,0 , ?? ?? iiii ww  Orthography ( ) ( )00 , wCapwHyp  POS }2,1{,1}2:2{, , ????? iiiii ppp  Chunking }2,1{,1}2:2{, , ????? iiiii bbb  Cluster 11}1,0{,1}2:2{, ,, ccccc iiiii ?????  Transition },,,{ 1100001 cccpwyy ??  Table 2: NER features. Hyp indicates if word contains hyphen and Cap indicates if first letter is capitalized.  3 Experiments 
3.1 Experimental settings We tested our compound features on the same chunking (CoNLL2000) and NER (CoNLL2003) tasks in (Turian et al, 2010). The Brown cluster features were used for comparison, which shared the same feature template used by clusters of em-beddings. To compare with the work of (Turian et al 2010), which aimed to solve the same problem but using embedding directly, we used the same word embeddings (CW 50) and Brown clusters (1000 clusters) they provided. The embeddings in (Turian et al 2010) are trained on RCV corpus, while the CoNLL2000 data is a part of the WSJ corpus. Since we believe that word representations 
trained on similar domain may better help to im-prove the results, we also used embeddings and Brown clusters trained on unlabeled WSJ data from (Nivre et al 2007) for comparison. Moreover, we wish to find out whether our method extends well to languages other than Eng-lish. So we conducted experiments on Chinese NER, where large amount of training data exists, which makes improving accuracies more difficult. We used data from People?s Daily (Jan.-Jun. 1998) and converted them following the style of Penn CTB (Xue et al 2005). Data from April was cho-sen as test set (1,309,616 words in 55,177 sentenc-es), others for training (6,119,063 words in 255,951 sentences). The Chinese word representa-tions were trained on Chinese Wikipedia until March 2011. The features used in Chinese NER are similar to those in English, except for the or-thography, pre/suffixes, and chunking features. We did little pre-processing work for the train-ing of word representations on WSJ data. The da-tasets were tokenized and capital words were kept. For training of Chinese Wikipedia, we retained the bodies of all articles and replaced words with fre-quencies lower than 10 as an ?UK_WORD? token. On each dataset, we induced embeddings with 64 dimensions based on 7-gram models and 1000 Brown clusters. The method in (Schwenk, 2007) was used to accelerate the training processes of NLMs. All the NLMs were trained for 5 epochs.  For clustering of embeddings we choose k=500 and 2500 since such combination performed best on development set as shown in the next section. We chose the Sofia-ml toolkit (Sculley 2010) for clustering of embeddings in order to save time. In the experiments CRF models were used and were optimized by ASGD (implemented by L?on Bottou). For comparison we re-implemented the direct usage of embeddings in (Turian et al 2010) with CRFsuite (Okazaki, 2007) since their features contain continuous values. 3.2 Performances Table 3 shows the chunking results. The results reported in (Turian et al 2010) were denoted as ?direct?. Based on the same word representations, our compound features got better performances in all cases. The embedding features trained on unla-beled WSJ data yield further improvements, show-
565
ing that word representations from similar domains can better help the supervised tasks. System Direct Compound Baseline 93.75 +Embedding (RCV) 94.10 94.19 +Brown (RCV) 94.11 94.24 +Brown&Emb (RCV) 94.35 94.42 +Embedding (WSJ) 94.20 94.37 +Brown (WSJ) 94.25 94.36 +Brown&Emb (WSJ) 94.43 94.58 Table 3:  F1-scores of chunking In the experiments of NER, first we evaluated how the numbers of clusters k will affect the per-formances on development set (Figure 1). The re-sults showed that both the cluster features (excluding all compound embedding features) and compound features could achieve better results than direct usage of the same embeddings. It also showed that the performances did not vary much when k was between 500 and 3000. When k=2500, the result was a little higher than others. We finally chose combination of k=500 and 2500, which achieved best results on development set.  
 Figure 1: Relation between numbers of clusters k and performances on development set. The performances of NER on test set are shown in Table 4. Our baseline is slightly lower than that in (Turian et al 2010), because the first-order CRF cannot utilize context information of NE tags. Despite of this, same conclusions with chunking held.  System Direct Compound Baseline 83.78 +Embedding 87.38 88.46 +Brown 88.14 88.23 +Brown&Embedding 88.85 89.06 Table 4:  F1-scores of English NER on test data Performances on Chinese NER are shown in Table 5. Similar results were observed as in Eng-lish NER, showing that our method extends to oth-er languages as well. 
System Direct Compound Baseline 88.24 +Embedding 89.98 90.37 +Brown 90.24 90.55 +Brown&Embedding 90.66 90.96 Table 5:  F1-scores of Chinese NER on test data Above results gave evidences that although clus-tering embeddings may lose some information, the derived compound features did have better perfor-mances. The compound features can also improve the performances of Brown clusters, but not as much as they did on embeddings. And the combi-nation of embedding-clusters and Brown-clusters could further improve the performances, since they made use of different type of context information.  The compound features also reduced the time cost of using embedding features. For example, the time for tagging one sentence in English NER was reduced from 5.6 ms to 1.6 ms, shown in Table 6. Embedding Time (ms) Baseline 1.2 Embeddings (direct) 5.6 Embeddings (compound) 1.6 Table 6:  Running time of different features  4 Analysis  Our compound embedding features greatly out-performed the direct usage of same embeddings on English NER. In this section we conducted anal-yses to show the reasons for the improvements. 4.1 Rare-words and ambiguous words To show the compound features have stronger abil-ities to handle rare words, we counted the numbers of errors made on words with different frequencies on unlabeled data. Here the word frequencies are from the results of Brown clustering provided by (Turian et al 2010). We compared our compound embedding features with direct usage of embed-dings as well as Brown clusters, which is believed to work better on rare words. Figure 2(a) shows that the compound features indeed resulted in few-er errors than the two baseline methods in most cases. Errors of embeddings occurred on words with frequencies lower than 2K and those in the range of 16 to 256 were reduced by 10.55% and 24.44%, respectively. Our compound features also reduced the errors caused by ambiguous words, as shown in Figure 
566
2(b), where the numbers of senses for a word are measured by the numbers of different POS tags it has in Penn Treebank. 12.1% of the errors on am-biguous words were reduced, comparing to 8.4% of the errors on unambiguous ones. 
 (a) 
 (b) Figure 2: Errors incurred on words with different fre-quencies (a) and ambiguous words (b) in NER. 4.2 Linear separability of embeddings Another reason for the good performances of com-pound features on NER is that they made linear models better separate named entities (NEs) and non-NEs, which are more difficult to be linearly separated when embeddings are directly used as features. Here we designed an experiment to prove this. Based on training data of CoNLL2003, a clas-sification task was built to tell whether a word be-longs to NE or not. Linear SVM and a non-linear model Multilayer Perceptron (MLP) were used to build the classifiers. As shown in Table 7, when embeddings were directly used as features, MLP performed much better than linear SVM. And the linear model was under-fitting on this task since it had similar accuracies on both training set and de-velopment set. Above observations showed that linear models could not separate NEs and non-NEs well in the space of embeddings. When clusters of embeddings were used as fea-tures, the accuracies of linear models increased even when there were only one or two non-zero 
features for each sample. At the same time the per-formances of MLP decreased because of the loss of information during clustering. The gaps between accuracies of linear models and non-linear ones decreased in the spaces of clusters, showing that cluster features are more suitable for linear models. At last, the compound features made the linear model out-perform all non-linear ones, since extra context information could be utilized. Embeddings Models Accuracy   direct linear 94.38  direct MLP 96.87  cluster 1000 linear 95.31  cluster 1000 MLP 95.32  cluster 500+2500 linear 96.10  cluster 500+2500 MLP 96.02  compound linear 97.30 Table 7:  Performances of linear and non-linear models on development set with different embedding features. 5 Conclusion and perspectives In this paper, we first introduced the idea of clus-tering embeddings and then proposed the com-pound features based on clustering, in order to overcome the disadvantages of the direct usage of continuous embeddings. Experiments showed that the compound features built on the same original word representation features (either embeddings or Brown clusters) achieve better performances on the same tasks. Further analyses showed that the com-pound features reduced errors on rare-words and ambiguous words and could be better utilized by linear models. The usage of word embeddings also has some limitations, e.g. they are weak in capturing struc-tural information of languages, which is necessary in NLP. In the future, we will research on task-specific representations for sub-structures, such as phrases and sub-trees based on word embeddings and documents representations (Xu et al, 2012). Acknowledgments We would like to thank Dr. Hua Wu, Haifeng Wang, Jie Zhou and Rui Zhang for many discus-sions and thank the anonymous reviewers for their valuable suggestions.  This work was supported by National Natural Science Foundation of China (61173073), and the Key Project of the National High Technology Research and Development Pro-gram of China (2011AA01A207). 
567
References  Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic language models. The Journal of Machine Learning Research, 3:1137?1155. Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160?167. ACM. Finkel, J., Grenager, T., and Manning, C. (2005). Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363?370. Association for Computational Linguistics. Huang, F. and Yates, A. (2009). Distributional representations for handling sparsity in supervised sequence labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 495?503. Association for Computational Linguistics. Koo, T., Carreras, X., and Collins, M. (2008). Simple semi-supervised dependency parsing.  In Proceed-ings of Association for Computational Linguistics, pages 595?603. Association for Computational Linguistics. Mnih, A. and Hinton, G. E. (2009). A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081?1088. Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson, J., Riedel, S., and Yuret, D. (2007). The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915?932. Okazaki, N. (2007). Crfsuite: a fast implementation of conditional random fields (crfs). URL http://www.chokkan.org/software/crfsuite. Schwenk, H. (2007). Continuous space language models. Computer Speech & Language, 21(3):492?518. Sculley, D. (2010). Web-scale k-means clustering. In Proceedings of the 19th international conference on World Wide Web, pages 1177?1178. ACM. Socher, R., Huang, E., Pennington, J., Ng, A., and Manning, C. (2011a). Dynamic pooling and unfolding recursive auto-encoders for paraphrase 
detection. Advances in Neural Information Processing Systems, 24:801?809. Socher, R., Pennington, J., Huang, E., Ng, A., and Manning, C. (2011b). Semi-supervised recursive auto-encoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151?161. Association for Computational Linguistics. T?ckstr?m, O., McDonald, R., and Uszkoreit, J. (2012). Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477?487, Montr?al, Canada, June 3-8, 2012. Turian, J., Ratinov, L., and Bengio, Y. (2010). Word representations: a simple and general method for semi-supervised learning. In Annual Meeting-Association For Computational Linguistics. Urbana, 51:61801. Xu, Z., Chen, M., Weinberger, K., and Sha, F. An alternative text representation to TF-IDF and Bag-of-Words. In Proceedings of 21st ACM Conf. of Information and Knowledge Management (CIKM), Hawaii, 2012. Xue, N., Xia, F., Chiou, F., and Palmer, M. (2005). The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207. 
568
Proceedings of the ACL 2010 Conference Short Papers, pages 6?11,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Joint Rule Selection Model for Hierarchical Phrase-based Translation?
Lei Cui?, Dongdong Zhang?, Mu Li?, Ming Zhou?, and Tiejun Zhao?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
{cuilei,tjzhao}@mtlab.hit.edu.cn
?Microsoft Research Asia, Beijing, China
{dozhang,muli,mingzhou}@microsoft.com
Abstract
In hierarchical phrase-based SMT sys-
tems, statistical models are integrated to
guide the hierarchical rule selection for
better translation performance. Previous
work mainly focused on the selection of
either the source side of a hierarchical rule
or the target side of a hierarchical rule
rather than considering both of them si-
multaneously. This paper presents a joint
model to predict the selection of hierar-
chical rules. The proposed model is esti-
mated based on four sub-models where the
rich context knowledge from both source
and target sides is leveraged. Our method
can be easily incorporated into the prac-
tical SMT systems with the log-linear
model framework. The experimental re-
sults show that our method can yield sig-
nificant improvements in performance.
1 Introduction
Hierarchical phrase-based model has strong ex-
pression capabilities of translation knowledge. It
can not only maintain the strength of phrase trans-
lation in traditional phrase-based models (Koehn
et al, 2003; Xiong et al, 2006), but also char-
acterize the complicated long distance reordering
similar to syntactic based statistical machine trans-
lation (SMT) models (Yamada and Knight, 2001;
Quirk et al, 2005; Galley et al, 2006; Liu et al,
2006; Marcu et al, 2006; Mi et al, 2008; Shen et
al., 2008).
In hierarchical phrase-based SMT systems, due
to the flexibility of rule matching, a huge number
of hierarchical rules could be automatically learnt
from bilingual training corpus (Chiang, 2005).
SMT decoders are forced to face the challenge of
?This work was finished while the first author visited Mi-
crosoft Research Asia as an intern.
proper rule selection for hypothesis generation, in-
cluding both source-side rule selection and target-
side rule selection where the source-side rule de-
termines what part of source words to be translated
and the target-side rule provides one of the candi-
date translations of the source-side rule. Improper
rule selections may result in poor translations.
There is some related work about the hierarchi-
cal rule selection. In the original work (Chiang,
2005), the target-side rule selection is analogous to
the model in traditional phrase-based SMT system
such as Pharaoh (Koehn et al, 2003). Extending
this work, (He et al, 2008; Liu et al, 2008) in-
tegrate rich context information of non-terminals
to predict the target-side rule selection. Different
from the above work where the probability dis-
tribution of source-side rule selection is uniform,
(Setiawan et al, 2009) proposes to select source-
side rules based on the captured function words
which often play an important role in word re-
ordering. There is also some work considering to
involve more rich contexts to guide the source-side
rule selection. (Marton and Resnik, 2008; Xiong
et al, 2009) explore the source syntactic informa-
tion to reward exact matching structure rules or
punish crossing structure rules.
All the previous work mainly focused on either
source-side rule selection task or target-side rule
selection task rather than both of them together.
The separation of these two tasks, however, weak-
ens the high interrelation between them. In this pa-
per, we propose to integrate both source-side and
target-side rule selection in a unified model. The
intuition is that the joint selection of source-side
and target-side rules is more reliable as it conducts
the search in a larger space than the single selec-
tion task does. It is expected that these two kinds
of selection can help and affect each other, which
may potentially lead to better hierarchical rule se-
lections with a relative global optimum instead of
a local optimum that might be reached in the pre-
6
vious methods. Our proposed joint probability
model is factored into four sub-models that can
be further classified into source-side and target-
side rule selection models or context-based and
context-free selection models. The context-based
models explore rich context features from both
source and target sides, including function words,
part-of-speech (POS) tags, syntactic structure in-
formation and so on. Our model can be easily in-
corporated as an independent feature into the prac-
tical hierarchical phrase-based systems with the
log-linear model framework. The experimental re-
sults indicate our method can improve the system
performance significantly.
2 Hierarchical Rule Selection Model
Following (Chiang, 2005), ??, ?? is used to repre-
sent a synchronous context free grammar (SCFG)
rule extracted from the training corpus, where ?
and ? are the source-side and target-side rule re-
spectively. Let C be the context of ??, ??. For-
mally, our joint probability model of hierarchical
rule selection is described as follows:
P (?, ?|C) = P (?|C)P (?|?,C) (1)
We decompose the joint probability model into
two sub-models based on the Bayes formulation,
where the first sub-model is source-side rule se-
lection model and the second one is the target-side
rule selection model.
For the source-side rule selection model, we fur-
ther compute it by the interpolation of two sub-
models:
?Ps(?) + (1? ?)Ps(?|C) (2)
where Ps(?) is the context-free source model
(CFSM) and Ps(?|C) is the context-based source
model (CBSM), ? is the interpolation weight that
can be optimized over the development data.
CFSM is the probability of source-side rule se-
lection that can be estimated based on maximum
likelihood estimation (MLE) method:
Ps(?) =
?
? Count(??, ??)
Count(?)
(3)
where the numerator is the total count of bilin-
gual rule pairs with the same source-side rule that
are extracted based on the extraction algorithm in
(Chiang, 2005), and the denominator is the total
amount of source-side rule patterns contained in
the monolingual source side of the training corpus.
CFSM is used to capture how likely the source-
side rule is linguistically motivated or has the cor-
responding target-side counterpart.
For CBSM, it can be naturally viewed as a clas-
sification problem where each distinct source-side
rule is a single class. However, considering the
huge number of classes may cause serious data
sparseness problem and thereby degrade the clas-
sification accuracy, we approximate CBSM by a
binary classification problem which can be solved
by the maximum entropy (ME) approach (Berger
et al, 1996) as follows:
Ps(?|C) ? Ps(?|?,C)
=
exp[
?
i ?ihi(?, ?,C)]?
?? exp[
?
i ?ihi(?
? , ?, C)]
(4)
where ? ? {0, 1} is the indicator whether the
source-side rule is applied during decoding, ? = 1
when the source-side rule is applied, otherwise
? = 0; hi is a feature function, ?i is the weight
of hi. CBSM estimates the probability of the
source-side rule being selected according to the
rich context information coming from the surface
strings and sub-phrases that will be reduced to
non-terminals during decoding.
Analogously, we decompose the target-side rule
selection model by the interpolation approach as
well:
?Pt(?) + (1? ?)Pt(?|?,C) (5)
where Pt(?) is the context-free target model
(CFTM) and Pt(?|?,C) is the context-based tar-
get model (CBTM), ? is the interpolation weight
that can be optimized over the development data.
In the similar way, we compute CFTM by the
MLE approach and estimate CBTM by the ME
approach. CFTM computes how likely the target-
side rule is linguistically motivated, while CBTM
predicts how likely the target-side rule is applied
according to the clues from the rich context infor-
mation.
3 Model Training of CBSM and CBTM
3.1 The acquisition of training instances
CBSM and CBTM are trained by ME approach for
the binary classification, where a training instance
consists of a label and the context related to SCFG
rules. The context is divided into source context
7
Figure 1: Example of training instances in CBSM and CBTM.
and target context. CBSM is trained only based
on the source context while CBTM is trained over
both the source and the target context. All the
training instances are automatically constructed
from the bilingual training corpus, which have la-
bels of either positive (i.e., ? = 1) or negative (i.e.,
? = 0). This section explains how the training in-
stances are constructed for the training of CBSM
and CBTM.
Let s and t be the source sentence and target
sentence,W be the word alignment between them,
rs be a source-side rule that pattern-matches a
sub-phrase of s, rt be the target-side rule pattern-
matching a sub-phrase of t and being aligned to rs
based on W , and C(r) be the context features re-
lated to the rule r which will be explained in the
following section.
For the training of CBSM, if the SCFG rule
?rs, rt? can be extracted based on the rule extrac-
tion algorithm in (Chiang, 2005), ?? = 1, C(rs)?
is constructed as a positive instance, otherwise
?? = 0, C(rs)? is constructed as a negative in-
stance. For example in Figure 1(a), the context of
source-side rule ?X1 hezuo? that pattern-matches
the phrase ?youhao hezuo? produces a positive
instance, while the context of ?X1 youhao? that
pattern-matches the source phrase ?de youhao? or
?shuangfang de youhao? will produce a negative
instance as there are no corresponding plausible
target-side rules that can be extracted legally1.
For the training of CBTM, given rs, suppose
there is a SCFG rule set {?rs, rkt ?|1 ? k ? n}
extracted from multiple distinct sentence pairs in
the bilingual training corpus, among which we as-
sume ?rs, rit? is extracted from the sentence pair
?s, t?. Then, we construct ?? = 1, C(rs), C(rit)?
1Because the aligned target words are not contiguous and
?cooperation? is aligned to the word outside the source-side
rule.
as a positive instance, while the elements in {?? =
0, C(rs), C(r
j
t )?|j 6= i ? 1 ? j ? n} are viewed
as negative instances since they fail to be applied
to the translation from s to t. For example in Fig-
ure 1(c), Rule (1) and Rule (2) are two different
SCFG rules extracted from Figure 1(a) and Figure
1(b) respectively, where their source-side rules are
the same. As Rule (1) cannot be applied to Fig-
ure 1(b) for the translation and Rule (2) cannot
be applied to Figure 1(a) for the translation either,
?? = 1, C(ras ), C(r
a
t )? and ?? = 1, C(r
b
s), C(r
b
t )?
are constructed as positive instances while ?? =
0, C(ras ), C(r
b
t )? and ?? = 0, C(r
b
s), C(r
a
t )? are
viewed as negative instances. It is noticed that
this instance construction method may lead to a
large quantity of negative instances and choke the
training procedure. In practice, to limit the size
of the training set, the negative instances con-
structed based on low-frequency target-side rules
are pruned.
3.2 Context-based features for ME training
ME approach has the merit of easily combining
different features to predict the probability of each
class. We incorporate into the ME based model
the following informative context-based features
to train CBSM and CBTM. These features are
carefully designed to reduce the data sparseness
problem and some of them are inspired by pre-
vious work (He et al, 2008; Gimpel and Smith,
2008; Marton and Resnik, 2008; Chiang et al,
2009; Setiawan et al, 2009; Shen et al, 2009;
Xiong et al, 2009):
1. Function word features, which indicate
whether the hierarchical source-side/target-
side rule strings and sub-phrases covered by
non-terminals contain function words that are
often important clues of predicting syntactic
structures.
8
2. POS features, which are POS tags of the
boundary source words covered by non-
terminals.
3. Syntactic features, which are the constituent
constraints of hierarchical source-side rules
exactly matching or crossing syntactic sub-
trees.
4. Rule format features, which are non-
terminal positions and orders in source-
side/target-side rules. This feature interacts
between source and target components since
it shows whether the translation ordering is
affected.
5. Length features, which are the length
of sub-phrases covered by source non-
terminals.
4 Experiments
4.1 Experiment setting
We implement a hierarchical phrase-based system
similar to the Hiero (Chiang, 2005) and evaluate
our method on the Chinese-to-English translation
task. Our bilingual training data comes from FBIS
corpus, which consists of around 160K sentence
pairs where the source data is parsed by the Berke-
ley parser (Petrov and Klein, 2007). The ME train-
ing toolkit, developed by (Zhang, 2006), is used to
train our CBSM and CBTM. The training size of
constructed positive instances for both CBSM and
CBTM is 4.68M, while the training size of con-
structed negative instances is 3.74M and 3.03M re-
spectively. Following (Setiawan et al, 2009), we
identify function words as the 128 most frequent
words in the corpus. The interpolation weights are
set to ? = 0.75 and ? = 0.70. The 5-gram lan-
guage model is trained over the English portion
of FBIS corpus plus Xinhua portion of the Giga-
word corpus. The development data is from NIST
2005 evaluation data and the test data is from
NIST 2006 and NIST 2008 evaluation data. The
evaluation metric is the case-insensitive BLEU4
(Papineni et al, 2002). Statistical significance in
BLEU score differences is tested by paired boot-
strap re-sampling (Koehn, 2004).
4.2 Comparison with related work
Our baseline is the implemented Hiero-like SMT
system where only the standard features are em-
ployed and the performance is state-of-the-art.
We compare our method with the baseline and
some typical approaches listed in Table 1 where
XP+ denotes the approach in (Marton and Resnik,
2008) and TOFW (topological ordering of func-
tion words) stands for the method in (Setiawan et
al., 2009). As (Xiong et al, 2009)?s work is based
on phrasal SMT system with bracketing transduc-
tion grammar rules (Wu, 1997) and (Shen et al,
2009)?s work is based on the string-to-dependency
SMT model, we do not implement these two re-
lated work due to their different models from ours.
We also do not compare with (He et al, 2008)?s
work due to its less practicability of integrating
numerous sub-models.
Methods NIST 2006 NIST 2008
Baseline 0.3025 0.2200
XP+ 0.3061 0.2254
TOFW 0.3089 0.2253
Our method 0.3141 0.2318
Table 1: Comparison results, our method is signif-
icantly better than the baseline, as well as the other
two approaches (p < 0.01)
As shown in Table 1, all the methods outper-
form the baseline because they have extra mod-
els to guide the hierarchical rule selection in some
ways which might lead to better translation. Ap-
parently, our method also performs better than the
other two approaches, indicating that our method
is more effective in the hierarchical rule selection
as both source-side and target-side rules are se-
lected together.
4.3 Effect of sub-models
Due to the space limitation, we analyze the ef-
fect of sub-models upon the system performance,
rather than that of ME features, part of which have
been investigated in previous related work.
Settings NIST 2006 NIST 2008
Baseline 0.3025 0.2200
Baseline+CFSM 0.3092? 0.2266?
Baseline+CBSM 0.3077? 0.2247?
Baseline+CFTM 0.3076? 0.2286?
Baseline+CBTM 0.3060 0.2255?
Baseline+CFSM+CFTM 0.3109? 0.2289?
Baseline+CFSM+CBSM 0.3104? 0.2282?
Baseline+CFTM+CBTM 0.3099? 0.2299?
Baseline+all sub-models 0.3141? 0.2318?
Table 2: Sub-model effect upon the performance,
*: significantly better than baseline (p < 0.01)
As shown in Table 2, when sub-models are inte-
9
grated as independent features, the performance is
improved compared to the baseline, which shows
that each of the sub-models can improve the hier-
archical rule selection. It is noticeable that the per-
formance of the source-side rule selection model
is comparable with that of the target-side rule se-
lection model. Although CFSM and CFTM per-
form only slightly better than the others among
the individual sub-models, the best performance is
achieved when all the sub-models are integrated.
5 Conclusion
Hierarchical rule selection is an important and
complicated task for hierarchical phrase-based
SMT system. We propose a joint probability
model for the hierarchical rule selection and the
experimental results prove the effectiveness of our
approach.
In the future work, we will explore more useful
features and test our method over the large scale
training corpus. A challenge might exist when
running the ME training toolkit over a big size
of training instances from the large scale training
data.
Acknowledgments
We are especially grateful to the anonymous re-
viewers for their insightful comments. We also
thank Hendra Setiawan, Yuval Marton, Chi-Ho Li,
Shujie Liu and Nan Duan for helpful discussions.
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1): pages 39-72.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
ACL, pages 263-270.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 New Features for Statistical Machine Trans-
lation. In Proc. HLT-NAACL, pages 218-226.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Translation Models. In Proc.
ACL-Coling, pages 961-968.
Kevin Gimpel and Noah A. Smith. 2008. Rich Source-
Side Context for Statistical Machine Translation. In
Proc. the Third Workshop on Statistical Machine
Translation, pages 9-17.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving Statistical Machine Translation using Lexi-
calized Rule Selection. In Proc. Coling, pages 321-
328.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. EMNLP.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. HLT-
NAACL, pages 127-133.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum Entropy based Rule Selection
Model for Syntax-based Statistical Machine Trans-
lation. In Proc. EMNLP, pages 89-97.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-String Statistical Translation Rules.
In Proc. ACL, pages 704-711.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proc. ACL-Coling, pages 609-616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phrases. In Proc. EMNLP, pages 44-52.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrased-Based Trans-
lation. In Proc. ACL, pages 1003-1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
Based Translation. In Proc. ACL, pages 192-199.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. ACL,
pages 311-318.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. HLT-NAACL,
pages 404-411.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proc. ACL, pages 271-279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
New String-to-Dependency Machine Translation Al-
gorithm with a Target Dependency Language Model.
In Proc. ACL, pages 577-585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective Use of Lin-
guistic and Contextual Information for Statistical
Machine Translation. In Proc. EMNLP, pages 72-
80.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological Ordering of Function
Words in Hierarchical Phrase-based Translation. In
Proc. ACL, pages 324-332.
10
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): pages 377-
403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proc. ACL-
Coling, pages 521-528.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A Syntax-Driven Bracketing Model for
Phrase-Based Translation. In Proc. ACL, pages
315-323.
Kenji Yamada and Kevin Knight. 2001. A Syntax-
based Statistical Translation Model. In Proc. ACL,
pages 523-530.
Le Zhang. 2006. Maximum entropy mod-
eling toolkit for python and c++. avail-
able at http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
11
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 151?160,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Target-dependent Twitter Sentiment Classification 
 
 
Long Jiang1   Mo Yu2   Ming Zhou1   Xiaohua Liu1   Tiejun Zhao2 
1 Microsoft Research Asia 2 School of Computer Science & Technology 
Beijing, China Harbin Institute of Technology 
 Harbin, China 
{longj,mingzhou,xiaoliu}@microsoft.com {yumo,tjzhao}@mtlab.hit.edu.cn 
 
  
  
 
 
Abstract 
Sentiment analysis on Twitter data has attract-
ed much attention recently. In this paper, we 
focus on target-dependent Twitter sentiment 
classification; namely, given a query, we clas-
sify the sentiments of the tweets as positive, 
negative or neutral according to whether they 
contain positive, negative or neutral senti-
ments about that query. Here the query serves 
as the target of the sentiments. The state-of-
the-art approaches for solving this problem 
always adopt the target-independent strategy, 
which may assign irrelevant sentiments to the 
given target. Moreover, the state-of-the-art 
approaches only take the tweet to be classified 
into consideration when classifying the senti-
ment; they ignore its context (i.e., related 
tweets). However, because tweets are usually 
short and more ambiguous, sometimes it is not 
enough to consider only the current tweet for 
sentiment classification. In this paper, we pro-
pose to improve target-dependent Twitter sen-
timent classification by 1) incorporating 
target-dependent features; and 2) taking relat-
ed tweets into consideration. According to the 
experimental results, our approach greatly im-
proves the performance of target-dependent 
sentiment classification. 
1 Introduction 
Twitter, as a micro-blogging system, allows users 
to publish tweets of up to 140 characters in length 
to tell others what they are doing, what they are 
thinking, or what is happening around them. Over 
the past few years, Twitter has become very popu-
lar. According to the latest Twitter entry in Wik-
ipedia, the number of Twitter users has climbed to 
190 million and the number of tweets published on 
Twitter every day is over 65 million1.  
As a result of the rapidly increasing number of 
tweets, mining people?s sentiments expressed in 
tweets has attracted more and more attention. In 
fact, there are already many web sites built on the 
Internet providing a Twitter sentiment search ser-
vice, such as Tweetfeel2 , Twendz3 , and Twitter 
Sentiment4. In those web sites, the user can input a 
sentiment target as a query, and search for tweets 
containing positive or negative sentiments towards 
the target. The problem needing to be addressed 
can be formally named as Target-dependent Sen-
timent Classification of Tweets; namely, given a 
query, classifying the sentiments of the tweets as 
positive, negative or neutral according to whether 
they contain positive, negative or neutral senti-
ments about that query. Here the query serves as 
the target of the sentiments. 
The state-of-the-art approaches for solving this 
problem, such as (Go et al, 20095; Barbosa and 
Feng, 2010), basically follow (Pang et al, 2002), 
who utilize machine learning based classifiers for 
the sentiment classification of texts. However, their 
classifiers actually work in a target-independent 
way: all the features used in the classifiers are in-
dependent of the target, so the sentiment is decided 
no matter what the target is. Since (Pang et al, 
2002) (or later research on sentiment classification 
                                                          
1 http://en.wikipedia.org/wiki/Twitter 
2 http://www.tweetfeel.com/ 
3 http://twendz.waggeneredstrom.com/ 
4 http://twittersentiment.appspot.com/ 
5 The algorithm used in Twitter Sentiment 
151
of product reviews) aim to classify the polarities of 
movie (or product) reviews and each movie (or 
product) review is assumed to express sentiments 
only about the target movie (or product), it is rea-
sonable for them to adopt the target-independent 
approach. However, for target-dependent sentiment 
classification of tweets, it is not suitable to exactly 
adopt that approach. Because people may mention 
multiple targets in one tweet or comment on a tar-
get in a tweet while saying many other unrelated 
things in the same tweet, target-independent ap-
proaches are likely to yield unsatisfactory results:  
1. Tweets that do not express any sentiments 
to the given target but express sentiments 
to other things will be considered as being 
opinionated about the target. For example, 
the following tweet expresses no sentiment 
to Bill Gates but is very likely to be classi-
fied as positive about Bill Gates by target-
independent approaches. 
"People everywhere love Windows & vista. 
Bill Gates" 
2. The polarities of some tweets towards the 
given target are misclassified because of 
the interference from sentiments towards 
other targets in the tweets. For example, 
the following tweet expresses a positive 
sentiment to Windows 7 and a negative 
sentiment to Vista. However, with target-
independent sentiment classification, both 
of the targets would get positive polarity. 
?Windows 7 is much better than Vista!? 
In fact, it is easy to find many such cases by 
looking at the output of Twitter Sentiment or other 
Twitter sentiment analysis web sites. Based on our 
manual evaluation of Twitter Sentiment output, 
about 40% of errors are because of this (see Sec-
tion 6.1 for more details).  
In addition, tweets are usually shorter and more 
ambiguous than other sentiment data commonly 
used for sentiment analysis, such as reviews and 
blogs. Consequently, it is more difficult to classify 
the sentiment of a tweet only based on its content. 
For instance, for the following tweet, which con-
tains only three words, it is difficult for any exist-
ing approaches to classify its sentiment correctly. 
?First game: Lakers!? 
However, relations between individual tweets 
are more common than those in other sentiment 
data. We can easily find many related tweets of a 
given tweet, such as the tweets published by the 
same person, the tweets replying to or replied by 
the given tweet, and retweets of the given tweet. 
These related tweets provide rich information 
about what the given tweet expresses and should 
definitely be taken into consideration for classify-
ing the sentiment of the given tweet. 
In this paper, we propose to improve target-
dependent sentiment classification of tweets by 
using both target-dependent and context-aware 
approaches. Specifically, the target-dependent ap-
proach refers to incorporating syntactic features 
generated using words syntactically connected 
with the given target in the tweet to decide whether 
or not the sentiment is about the given target. For 
instance, in the second example, using syntactic 
parsing, we know that ?Windows 7? is connected 
to ?better? by a copula, while ?Vista? is connected 
to ?better? by a preposition. By learning from 
training data, we can probably predict that ?Win-
dows 7? should get a positive sentiment and 
?Vista? should get a negative sentiment.  
In addition, we also propose to incorporate the 
contexts of tweets into classification, which we call 
a context-aware approach. By considering the sen-
timent labels of the related tweets, we can further 
boost the performance of the sentiment classifica-
tion, especially for very short and ambiguous 
tweets. For example, in the third example we men-
tioned above, if we find that the previous and fol-
lowing tweets published by the same person are 
both positive about the Lakers, we can confidently 
classify this tweet as positive. 
The remainder of this paper is structured as fol-
lows. In Section 2, we briefly summarize related 
work. Section 3 gives an overview of our approach. 
We explain the target-dependent and context-
aware approaches in detail in Sections 4 and 5 re-
spectively. Experimental results are reported in 
Section 6 and Section 7 concludes our work. 
2 Related Work  
In recent years, sentiment analysis (SA) has be-
come a hot topic in the NLP research community. 
A lot of papers have been published on this topic. 
152
2.1 Target-independent SA 
Specifically, Turney (2002) proposes an unsuper-
vised method for classifying product or movie re-
views as positive or negative. In this method, 
sentimental phrases are first selected from the re-
views according to predefined part-of-speech pat-
terns. Then the semantic orientation score of each 
phrase is calculated according to the mutual infor-
mation values between the phrase and two prede-
fined seed words. Finally, a review is classified 
based on the average semantic orientation of the 
sentimental phrases in the review. 
In contrast, (Pang et al, 2002) treat the senti-
ment classification of movie reviews simply as a 
special case of a topic-based text categorization 
problem and investigate three classification algo-
rithms: Naive Bayes, Maximum Entropy, and Sup-
port Vector Machines. According to the 
experimental results, machine learning based clas-
sifiers outperform the unsupervised approach, 
where the best performance is achieved by the 
SVM classifier with unigram presences as features. 
2.2 Target-dependent SA 
Besides the above mentioned work for target-
independent sentiment classification, there are also 
several approaches proposed for target-dependent 
classification, such as (Nasukawa and Yi, 2003; 
Hu and Liu, 2004; Ding and Liu, 2007). (Nasuka-
wa and Yi, 2003) adopt a rule based approach, 
where rules are created by humans for adjectives, 
verbs, nouns, and so on. Given a sentiment target 
and its context, part-of-speech tagging and de-
pendency parsing are first performed on the con-
text. Then predefined rules are matched in the 
context to determine the sentiment about the target. 
In (Hu and Liu, 2004), opinions are extracted from 
product reviews, where the features of the product 
are considered opinion targets. The sentiment 
about each target in each sentence of the review is 
determined based on the dominant orientation of 
the opinion words appearing in the sentence. 
As mentioned in Section 1, target-dependent 
sentiment classification of review sentences is 
quite different from that of tweets. In reviews, if 
any sentiment is expressed in a sentence containing 
a feature, it is very likely that the sentiment is 
about the feature. However, the assumption does 
not hold in tweets. 
2.3 SA of Tweets 
As Twitter becomes more popular, sentiment anal-
ysis on Twitter data becomes more attractive. (Go 
et al, 2009; Parikh and Movassate, 2009; Barbosa 
and Feng, 2010; Davidiv et al, 2010) all follow the 
machine learning based approach for sentiment 
classification of tweets. Specifically, (Davidiv et 
al., 2010) propose to classify tweets into multiple 
sentiment types using hashtags and smileys as la-
bels. In their approach, a supervised KNN-like 
classifier is used. In contrast, (Barbosa and Feng, 
2010) propose a two-step approach to classify the 
sentiments of tweets using SVM classifiers with 
abstract features. The training data is collected 
from the outputs of three existing Twitter senti-
ment classification web sites. As mentioned above, 
these approaches work in a target-independent way, 
and so need to be adapted for target-dependent sen-
timent classification. 
3 Approach Overview  
The problem we address in this paper is target-
dependent sentiment classification of tweets. So 
the input of our task is a collection of tweets con-
taining the target and the output is labels assigned 
to each of the tweets. Inspired by (Barbosa and 
Feng, 2010; Pang and Lee, 2004), we design a 
three-step approach in this paper:  
1. Subjectivity classification as the first step 
to decide if the tweet is subjective or neu-
tral about the target;  
2. Polarity classification as the second step to 
decide if the tweet is positive or negative 
about the target if it is classified as subjec-
tive in Step 1;  
3. Graph-based optimization as the third step 
to further boost the performance by taking 
the related tweets into consideration.  
In each of the first two steps, a binary SVM 
classifier is built to perform the classification. To 
train the classifiers, we use SVM-Light 6  with a 
linear kernel; the default setting is adopted in all 
experiments. 
                                                          
6 http://svmlight.joachims.org/ 
153
3.1 Preprocessing 
In our approach, rich feature representations are 
used to distinguish between sentiments expressed 
towards different targets. In order to generate such 
features, much NLP work has to be done before-
hand, such as tweet normalization, POS tagging, 
word stemming, and syntactic parsing.  
In our experiments, POS tagging is performed 
by the OpenNLP POS tagger7. Word stemming is 
performed by using a word stem mapping table 
consisting of about 20,000 entries. We also built a 
simple rule-based model for tweet normalization 
which can correct simple spelling errors and varia-
tions into normal form, such as ?gooood? to 
?good? and ?luve? to ?love?. For syntactic parsing 
we use a Maximum Spanning Tree dependency 
parser (McDonald et al, 2005). 
3.2 Target-independent Features 
Previous work (Barbosa and Feng, 2010; Davidiv 
et al, 2010) has discovered many effective features 
for sentiment analysis of tweets, such as emoticons, 
punctuation, prior subjectivity and polarity of a 
word. In our classifiers, most of these features are 
also used. Since these features are all generated 
without considering the target, we call them target-
independent features. In both the subjectivity clas-
sifier and polarity classifier, the same target-
independent feature set is used. Specifically, we 
use two kinds of target-independent features: 
1. Content features, including words, punctu-
ation, emoticons, and hashtags (hashtags 
are provided by the author to indicate the 
topic of the tweet). 
2. Sentiment lexicon features, indicating how 
many positive or negative words are in-
cluded in the tweet according to a prede-
fined lexicon. In our experiments, we use 
the lexicon downloaded from General In-
quirer8. 
4 Target-dependent Sentiment Classifica-
tion  
Besides target-independent features, we also incor-
porate target-dependent features in both the subjec-
                                                          
7 http://opennlp.sourceforge.net/projects.html 
8 http://www.wjh.harvard.edu/~inquirer/ 
tivity classifier and polarity classifier. We will ex-
plain them in detail below. 
4.1 Extended Targets 
It is quite common that people express their senti-
ments about a target by commenting not on the 
target itself but on some related things of the target. 
For example, one may express a sentiment about a 
company by commenting on its products or tech-
nologies. To express a sentiment about a product, 
one may choose to comment on the features or 
functionalities of the product. It is assumed that 
readers or audiences can clearly infer the sentiment 
about the target based on those sentiments about 
the related things. As shown in the tweet below, 
the author expresses a positive sentiment about 
?Microsoft? by expressing a positive sentiment 
directly about ?Microsoft technologies?. 
?I am passionate about Microsoft technologies 
especially Silverlight.? 
In this paper, we define those aforementioned 
related things as Extended Targets. Tweets ex-
pressing positive or negative sentiments towards 
the extended targets are also regarded as positive 
or negative about the target. Therefore, for target-
dependent sentiment classification of tweets, the 
first thing is identifying all extended targets in the 
input tweet collection.  
In this paper, we first regard all noun phrases, 
including the target, as extended targets for sim-
plicity. However, it would be interesting to know 
under what circumstances the sentiment towards 
the target is truly consistent with that towards its 
extended targets. For example, a sentiment about 
someone?s behavior usually means a sentiment 
about the person, while a sentiment about some-
one?s colleague usually has nothing to do with the 
person. This could be a future work direction for 
target-dependent sentiment classification. 
In addition to the noun phrases including the 
target, we further expand the extended target set 
with the following three methods:  
1. Adding mentions co-referring to the target 
as new extended targets. It is common that 
people use definite or demonstrative noun 
phrases or pronouns referring to the target 
in a tweet and express sentiments directly 
on them. For instance, in ?Oh, Jon Stewart. 
How I love you so.?, the author expresses 
154
a positive sentiment to ?you? which actual-
ly refers to ?Jon Stewart?. By using a sim-
ple co-reference resolution tool adapted 
from (Soon et al, 2001), we add all the 
mentions referring to the target into the ex-
tended target set. 
2. Identifying the top K nouns and noun 
phrases which have the strongest associa-
tion with the target. Here, we use 
Pointwise Mutual Information (PMI) to 
measure the association. 
)()(
),(log),( tpwp
twptwPMI ?
 
Where p(w,t), p(w), and p(t) are probabili-
ties of w and t co-occurring, w appearing, 
and t appearing in a tweet respectively. In 
the experiments, we estimate them on a 
tweet corpus containing 20 million tweets. 
We set K = 20 in the experiments based on 
empirical observations. 
3. Extracting head nouns of all extended tar-
gets, whose PMI values with the target are 
above some predefined threshold, as new 
extended targets. For instance, suppose we 
have found ?Microsoft Technologies? as 
the extended target, we will further add 
?technologies? into the extended target set 
if the PMI value for ?technologies? and 
?Microsoft? is above the threshold. Simi-
larly, we can find ?price? as the extended 
targets for ?iPhone? from ?the price of 
iPhone? and ?LoveGame? for ?Lady Ga-
ga? from ?LoveGame by Lady Gaga?. 
4.2 Target-dependent Features 
Target-dependent sentiment classification needs to 
distinguish the expressions describing the target 
from other expressions. In this paper, we rely on 
the syntactic parse tree to satisfy this need. Specif-
ically, for any word stem wi in a tweet which has 
one of the following relations with the given target 
T or any from the extended target set, we generate 
corresponding target-dependent features with the 
following rules:  
? wi is a transitive verb and T (or any of the 
extended target) is its object; we generate a 
feature wi _arg2. ?arg? is short for ?argu-
ment?. For example, for the target iPhone 
in ?I love iPhone?, we generate 
?love_arg2? as a feature. 
? wi is a transitive verb and T (or any of the 
extended target) is its subject; we generate 
a feature wi_arg1 similar to Rule 1. 
? wi is a intransitive verb and T (or any of the 
extended target) is its subject; we generate 
a feature wi_it_arg1. 
?  wi is an adjective or noun and T (or any of 
the extended target) is its head; we gener-
ate a feature wi_arg1. 
?  wi is an adjective or noun and it (or its 
head) is connected by a copula with T (or 
any of the extended target); we generate a 
feature wi_cp_arg1. 
? wi is an adjective or intransitive verb ap-
pearing alone as a sentence and T (or any 
of the extended target) appears in the pre-
vious sentence; we generate a feature 
wi_arg. For example, in ?John did that. 
Great!?, ?Great? appears alone as a sen-
tence, so we generate ?great_arg? for the 
target ?John?. 
? wi is an adverb, and the verb it modifies 
has T (or any of the extended target) as its 
subject; we generate a feature arg1_v_wi. 
For example, for the target iPhone in the 
tweet ?iPhone works better with the Cell-
Band?, we will generate the feature 
?arg1_v_well?. 
Moreover, if any word included in the generated 
target-dependent features is modified by a nega-
tion9, then we will add a prefix ?neg-? to it in the 
generated features. For example, for the target iPh-
one in the tweet ?iPhone does not work better with 
the CellBand?, we will generate the features 
?arg1_v_neg-well? and ?neg-work_it_arg1?. 
To overcome the sparsity of target-dependent 
features mentioned above, we design a special bi-
nary feature indicating whether or not the tweet 
contains at least one of the above target-dependent 
features. Target-dependent features are binary fea-
tures, each of which corresponds to the presence of 
the feature in the tweet. If the feature is present, the 
entry will be 1; otherwise it will be 0. 
                                                          
9 Seven negations are used in the experiments: not, no, never, 
n?t, neither, seldom, hardly. 
155
5 Graph-based Sentiment Optimization  
As we mentioned in Section 1, since tweets are 
usually shorter and more ambiguous, it would be 
useful to take their contexts into consideration 
when classifying the sentiments. In this paper, we 
regard the following three kinds of related tweets 
as context for a tweet. 
1. Retweets. Retweeting in Twitter is essen-
tially the forwarding of a previous message. 
People usually do not change the content 
of the original tweet when retweeting. So 
retweets usually have the same sentiment 
as the original tweets.  
2. Tweets containing the target and published 
by the same person. Intuitively, the tweets 
published by the same person within a 
short timeframe should have a consistent 
sentiment about the same target.  
3. Tweets replying to or replied by the tweet 
to be classified.  
Based on these three kinds of relations, we can 
construct a graph using the input tweet collection 
of a given target. As illustrated in Figure 1, each 
circle in the graph indicates a tweet. The three 
kinds of edges indicate being published by the 
same person (solid line), retweeting (dash line), 
and replying relations (round dotted line) respec-
tively. 
 
 
 
Figure 1. An example graph of tweets about a target 
 
If we consider that the sentiment of a tweet only 
depends on its content and immediate neighbors, 
we can leverage a graph-based method for senti-
ment classification of tweets. Specifically, the 
probability of a tweet belonging to a specific sen-
timent class can be computed with the following 
formula: 
??
)(
))(())(|()|(),|(
dN
dNpdNcpcpGcp ??
 
Where c is the sentiment label of a tweet which 
belongs to {positive, negative, neutral}, G is the 
tweet graph, N(d) is a specific assignment of sen-
timent labels to all immediate neighbors of the 
tweet, and ? is the content of the tweet. 
We can convert the output scores of a tweet by 
the subjectivity and polarity classifiers into proba-
bilistic form and use them to approximate p(c| ?). 
Then a relaxation labeling algorithm described in 
(Angelova and Weikum, 2006) can be used on the 
graph to iteratively estimate p(c|?,G) for all tweets. 
After the iteration ends, for any tweet in the graph, 
the sentiment label that has the maximum p(c| ?,G) 
is considered the final label. 
6 Experiments  
Because there is no annotated tweet corpus public-
ly available for evaluation of target-dependent 
Twitter sentiment classification, we have to create 
our own. Since people are most interested in sen-
timents towards celebrities, companies and prod-
ucts, we selected 5 popular queries of these kinds: 
{Obama, Google, iPad, Lakers, Lady Gaga}. For 
each of those queries, we downloaded 400 English 
tweets10 containing the query using the Twitter API.  
We manually classify each tweet as positive, 
negative or neutral towards the query with which it 
is downloaded. After removing duplicate tweets, 
we finally obtain 459 positive, 268 negative and 
1,212 neutral tweets. 
Among the tweets, 100 are labeled by two hu-
man annotators for inter-annotator study. The re-
sults show that for 86% of them, both annotators 
gave identical labels. Among the 14 tweets which 
the two annotators disagree on, only 1 case is a 
positive-negative disagreement (one annotator con-
siders it positive while the other negative), and the 
other 13 are all neutral-subjective disagreement. 
This probably indicates that it is harder for humans 
to decide if a tweet is neutral or subjective than to 
decide if it is positive or negative. 
                                                          
10 In this paper, we use sentiment classification of English 
tweets as a case study; however, our approach is applicable to 
other languages as well. 
156
6.1 Error Analysis of Twitter Sentiment Out-
put 
We first analyze the output of Twitter Sentiment 
(TS) using the five test queries. For each query, we 
randomly select 20 tweets labeled as positive or 
negative by TS. We also manually classify each 
tweet as positive, negative or neutral about the cor-
responding query. Then, we analyze those tweets 
that get different labels from TS and humans. Fi-
nally we find two major types of error: 1) Tweets 
which are totally neutral (for any target) are classi-
fied as subjective by TS; 2) sentiments in some 
tweets are classified correctly but the sentiments 
are not truly about the query. The two types take 
up about 35% and 40% of the total errors, respec-
tively.  
The second type is actually what we want to re-
solve in this paper. After further checking those 
tweets of the second type, we found that most of 
them are actually neutral for the target, which 
means that the dominant error in Twitter Sentiment 
is classifying neutral tweets as subjective. Below 
are several examples of the second type where the 
bolded words are the targets. 
 ?No debate needed, heat can't beat lakers or 
celtics? (negative by TS but positive by human) 
?why am i getting spams from weird people ask-
ing me if i want to chat with lady gaga? (positive 
by TS but neutral by human) 
?Bringing iPhone and iPad apps into cars? 
http://www.speakwithme.com/ will be out soon and 
alpha is awesome in my car.? (positive by TS but 
neutral by human) 
?Here's a great article about Monte Veronese 
cheese. It's in Italian so just put the url into Google 
translate and enjoy http://ow.ly/3oQ77? (positive 
by TS but neutral by human) 
6.2 Evaluation of Subjectivity Classification 
We conduct several experiments to evaluate sub-
jectivity classifiers using different features. In the 
experiments, we consider the positive and negative 
tweets annotated by humans as subjective tweets 
(i.e., positive instances in the SVM classifiers), 
which amount to 727 tweets. Following (Pang et 
al., 2002), we balance the evaluation data set by 
randomly selecting 727 tweets from all neutral 
tweets annotated by humans and consider them as 
objective tweets (i.e., negative instances in the 
classifiers). We perform 10-fold cross-validations 
on the selected data. Following (Go et al, 2009; 
Pang et al, 2002), we use accuracy as a metric in 
our experiments. The results are listed below. 
 
Features Accuracy (%) 
Content features 61.1 
+ Sentiment lexicon features 63.8 
+ Target-dependent features 68.2 
Re-implementation of (Bar-
bosa and Feng, 2010) 
60.3 
 
Table 1. Evaluation of subjectivity classifiers. 
 
As shown in Table 1, the classifier using only 
the content features achieves an accuracy of 61.1%. 
Adding sentiment lexicon features improves the 
accuracy to 63.8%. Finally, the best performance 
(68.2%) is achieved by combining target-
dependent features and other features (t-test: p < 
0.005). This clearly shows that target-dependent 
features do help remove many sentiments not truly 
about the target. We also re-implemented the 
method proposed in (Barbosa and Feng, 2010) for 
comparison. From Table 1, we can see that all our 
systems perform better than (Barbosa and Feng, 
2010) on our data set. One possible reason is that 
(Barbosa and Feng, 2010) use only abstract fea-
tures while our systems use more lexical features. 
To further evaluate the contribution of target ex-
tension, we compare the system using the exact 
target and all extended targets with that using only 
the exact target. We also eliminate the extended 
targets generated by each of the three target exten-
sion methods and reevaluate the performances. 
 
Target Accuracy (%) 
Exact target 65.6 
+ all extended targets 68.2 
- co-references 68.0 
- targets found by PMI 67.8 
- head nouns 67.3 
 
Table 2. Evaluation of target extension methods. 
 
As shown in Table 2, without extended targets, 
the accuracy is 65.6%, which is still higher than 
those using only target-independent features. After 
adding all extended targets, the accuracy is im-
proved significantly to 68.2% (p < 0.005), which 
suggests that target extension does help find indi-
157
rectly expressed sentiments about the target. In 
addition, all of the three methods contribute to the 
overall improvement, with the head noun method 
contributing most. However, the other two meth-
ods do not contribute significantly.  
6.3 Evaluation of Polarity Classification  
Similarly, we conduct several experiments on posi-
tive and negative tweets to compare the polarity 
classifiers with different features, where we use 
268 negative and 268 randomly selected positive 
tweets. The results are listed below. 
 
Features Accuracy (%) 
Content features 78.8 
+ Sentiment lexicon features 84.2 
+ Target-dependent features 85.6 
Re-implementation of (Bar-
bosa and Feng, 2010) 
83.9 
 
Table 3. Evaluation of polarity classifiers. 
 
From Table 3, we can see that the classifier us-
ing only the content features achieves the worst 
accuracy (78.8%). Sentiment lexicon features are 
shown to be very helpful for improving the per-
formance. Similarly, we re-implemented the meth-
od proposed by (Barbosa and Feng, 2010) in this 
experiment. The results show that our system using 
both content features and sentiment lexicon fea-
tures performs slightly better than (Barbosa and 
Feng, 2010). The reason may be same as that we 
explained above. 
Again, the classifier using all features achieves 
the best performance. Both the classifiers with all 
features and with the combination of content and 
sentiment lexicon features are significantly better 
than that with only the content features (p < 0.01). 
However, the classifier with all features does not 
significantly outperform that using the combina-
tion of content and sentiment lexicon features. We 
also note that the improvement by target-dependent 
features here is not as large as that in subjectivity 
classification. Both of these indicate that target-
dependent features are more useful for improving 
subjectivity classification than for polarity classifi-
cation. This is consistent with our observation in 
Subsection 6.2 that most errors caused by incorrect 
target association are made in subjectivity classifi-
cation. We also note that all numbers in Table 3 
are much bigger than those in Table 1, which sug-
gests that subjectivity classification of tweets is 
more difficult than polarity classification. 
Similarly, we evaluated the contribution of tar-
get extension for polarity classification. According 
to the results, adding all extended targets improves 
the accuracy by about 1 point. However, the con-
tributions from the three individual methods are 
not statistically significant. 
6.4 Evaluation of Graph-based Optimization  
As seen in Figure 1, there are several tweets which 
are not connected with any other tweets. For these 
tweets, our graph-based optimization approach will 
have no effect. The following table shows the per-
centages of the tweets in our evaluation data set 
which have at least one related tweet according to 
various relation types.  
 
Relation type Percentage 
Published by the same person11 41.6 
Retweet 23.0 
Reply 21.0 
All 66.2 
 
Table 4. Percentages of tweets having at least one relat-
ed tweet according to various relation types. 
 
According to Table 4, for 66.2% of the tweets 
concerning the test queries, we can find at least one 
related tweet. That means our context-aware ap-
proach is potentially useful for most of the tweets. 
To evaluate the effectiveness of our context-
aware approach, we compared the systems with 
and without considering the context.  
 
System Accuracy 
F1-score (%) 
pos neu neg 
Target-dependent 
sentiment classifier 
66.0 57.5 70.1 66.1 
+Graph-based op-
timization 
68.3 63.5 71.0 68.5 
 
Table 5. Effectiveness of the context-aware approach. 
 
As shown in Table 5, the overall accuracy of the 
target-dependent classifiers over three classes is 
66.0%. The graph-based optimization improves the 
performance by over 2 points (p < 0.005), which 
clearly shows that the context information is very 
                                                          
11 We limit the time frame from one week before to one week 
after the post time of the current tweet. 
158
useful for classifying the sentiments of tweets. 
From the detailed improvement for each sentiment 
class, we find that the context-aware approach is 
especially helpful for positive and negative classes. 
 
Relation type Accuracy (%) 
Published by the same person 67.8 
Retweet 66.0 
Reply 67.0 
 
Table 6. Contribution comparison between relations. 
 
We further compared the three types of relations 
for context-aware sentiment classification; the re-
sults are reported in Table 6. Clearly, being pub-
lished by the same person is the most useful 
relation for sentiment classification, which is con-
sistent with the percentage distribution of the 
tweets over relation types; using retweet only does 
not help. One possible reason for this is that the 
retweets and their original tweets are nearly the 
same, so it is very likely that they have already got 
the same labels in previous classifications. 
7 Conclusions and Future Work 
Twitter sentiment analysis has attracted much at-
tention recently. In this paper, we address target-
dependent sentiment classification of tweets. Dif-
ferent from previous work using target-
independent classification, we propose to incorpo-
rate syntactic features to distinguish texts used for 
expressing sentiments towards different targets in a 
tweet. According to the experimental results, the 
classifiers incorporating target-dependent features 
significantly outperform the previous target-
independent classifiers.  
In addition, different from previous work using 
only information on the current tweet for sentiment 
classification, we propose to take the related tweets 
of the current tweet into consideration by utilizing 
graph-based optimization. According to the exper-
imental results, the graph-based optimization sig-
nificantly improves the performance. 
As mentioned in Section 4.1, in future we would 
like to explore the relations between a target and 
any of its extended targets. We are also interested 
in exploring relations between Twitter accounts for 
classifying the sentiments of the tweets published 
by them. 
Acknowledgments 
We would like to thank Matt Callcut for refining 
the language of this paper, and thank Yuki Arase 
and the anonymous reviewers for many valuable 
comments and helpful suggestions. We would also 
thank Furu Wei and Xiaolong Wang for their help 
with some of the experiments and the preparation 
of the camera-ready version of the paper. 
References  
Ralitsa Angelova, Gerhard Weikum. 2006. Graph-based 
text classification: learn from your neighbors. SIGIR 
2006: 485-492 
Luciano Barbosa and Junlan Feng. 2010. Robust Senti-
ment Detection on Twitter from Biased and Noisy 
Data. Coling 2010. 
Christopher Burges. 1998. A Tutorial on Support Vector 
Machines for Pattern Recognition. Data Mining and 
Knowledge Discovery, 2(2):121-167. 
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth 
Patwardhan S. 2005. Identifying sources of opinions 
with conditional random fields and extraction pat-
terns. In Proc. of the 2005 Human Language Tech-
nology Conf. and Conf. on Empirical Methods in 
Natural Language Processing (HLT/EMNLP 2005). 
pp. 355-362 
Dmitry Davidiv, Oren Tsur and Ari Rappoport. 2010. 
Enhanced Sentiment Learning Using Twitter Hash-
tags and Smileys. Coling 2010. 
Xiaowen Ding and Bing Liu. 2007. The Utility of Lin-
guistic Rules in Opinion Mining. SIGIR-2007 (poster 
paper), 23-27 July 2007, Amsterdam.  
Alec Go, Richa Bhayani, Lei Huang. 2009. Twitter Sen-
timent Classification using Distant Supervision. 
Vasileios Hatzivassiloglou and Kathleen.R. McKeown. 
2002. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th ACL and the 8th 
Conference of the European Chapter of the ACL. 
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining (KDD-2004, full paper), 
Seattle, Washington, USA, Aug 22-25, 2004. 
Thorsten Joachims. Making Large-scale Support Vector 
Machine Learning Practical. In B. Sch?olkopf, C. J. 
C. Burges, and A. J. Smola, editors, Advances in 
kernel methods: support vector learning, pages 169-
184. MIT Press, Cambridge, MA, USA, 1999. 
159
Soo-Min Kim and Eduard Hovy 2006. Extracting opi-
nions, opinion holders, and topics expressed in online 
news media text, In Proc. of ACL Workshop on Sen-
timent and Subjectivity in Text, pp.1-8, Sydney, Aus-
tralia.  
Ryan McDonald, F. Pereira, K. Ribarov, and J. Haji?c. 
2005. Non-projective dependency parsing using 
spanning tree algorithms. In Proc. HLT/EMNLP. 
Tetsuya Nasukawa, Jeonghee Yi. 2003. Sentiment anal-
ysis: capturing favorability using natural language 
processing. In Proceedings of K-CAP. 
Bo Pang, Lillian Lee. 2004. A Sentimental Education: 
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of 
ACL 2004. 
Bo Pang, Lillian Lee, Shivakumar Vaithyanathan. 2002. 
Thumbs up? Sentiment Classification using Machine 
Learning Techniques.  
Ravi Parikh and Matin Movassate. 2009. Sentiment 
Analysis of User-Generated Twitter Updates using 
Various Classification Techniques. 
Wee. M. Soon, Hwee. T. Ng, and Danial. C. Y. Lim. 
2001. A Machine Learning Approach to Coreference 
Resolution of Noun Phrases. Computational Linguis-
tics, 27(4):521?544. 
Peter D. Turney. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised Clas-
sification of Reviews. In proceedings of ACL 2002. 
Janyce Wiebe. 2000. Learning subjective adjectives 
from corpora. In Proceedings of AAAI-2000. 
Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005. 
Recognizing Contextual Polarity in Phrase-Level 
Sentiment Analysis. In Proceedings of NAACL 2005. 
160
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 791?801,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Additive Neural Networks for Statistical Machine Translation
Lemao Liu1, Taro Watanabe2, Eiichiro Sumita2, Tiejun Zhao1
1School of Computer Science and Technology
Harbin Institute of Technology (HIT), Harbin, China
2National Institute of Information and Communication Technology (NICT)
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{lmliu | tjzhao}@mtlab.hit.edu.cn
{taro.watanabe | eiichiro.sumita}@nict.go.jp
Abstract
Most statistical machine translation
(SMT) systems are modeled using a log-
linear framework. Although the log-linear
model achieves success in SMT, it still
suffers from some limitations: (1) the
features are required to be linear with
respect to the model itself; (2) features
cannot be further interpreted to reach
their potential. A neural network is
a reasonable method to address these
pitfalls. However, modeling SMT with a
neural network is not trivial, especially
when taking the decoding efficiency
into consideration. In this paper, we
propose a variant of a neural network, i.e.
additive neural networks, for SMT to go
beyond the log-linear translation model.
In addition, word embedding is employed
as the input to the neural network, which
encodes each word as a feature vector.
Our model outperforms the log-linear
translation models with/without embed-
ding features on Chinese-to-English and
Japanese-to-English translation tasks.
1 Introduction
Recently, great progress has been achieved in
SMT, especially since Och and Ney (2002) pro-
posed the log-linear model: almost all the state-
of-the-art SMT systems are based on the log-linear
model. Its most important advantage is that arbi-
trary features can be added to the model. Thus,
it casts complex translation between a pair of lan-
guages as feature engineering, which facilitates re-
search and development for SMT.
Regardless of how successful the log-linear
model is in SMT, it still has some shortcomings.
This joint work was done while the first author visited
NICT.
On the one hand, features are required to be lin-
ear with respect to the objective of the translation
model (Nguyen et al, 2007), but it is not guaran-
teed that the potential features be linear with the
model. This induces modeling inadequacy (Duh
and Kirchhoff, 2008), in which the translation per-
formance may not improve, or may even decrease,
after one integrates additional features into the
model. On the other hand, it cannot deeply in-
terpret its surface features, and thus can not ef-
ficiently develop the potential of these features.
What may happen is that a feature p does initially
not improve the translation performance, but after
a nonlinear operation, e.g. log(p), it does. The
reason is not because this feature is useless but the
model does not efficiently interpret and represent
it. Situations such as this confuse explanations for
feature designing, since it is unclear whether such
a feature contributes to a translation or not.
A neural network (Bishop, 1995) is a reason-
able method to overcome the above shortcomings.
However, it should take constraints, e.g. the de-
coding efficiency, into account in SMT. Decod-
ing in SMT is considered as the expansion of
translation states and it is handled by a heuris-
tic search (Koehn, 2004a). In the search pro-
cedure, frequent computation of the model score
is needed for the search heuristic function, which
will be challenged by the decoding efficiency for
the neural network based translation model. Fur-
ther, decoding with non-local (or state-dependent)
features, such as a language model, is also a prob-
lem. Actually, even for the (log-) linear model,
efficient decoding with the language model is not
trivial (Chiang, 2007).
In this paper, we propose a variant of neural net-
works, i.e. additive neural networks (see Section
3 for details), for SMT. It consists of two com-
ponents: a linear component which captures non-
local (or state dependent) features and a non-linear
component (i.e., neural nework) which encodes lo-
791
XProceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 802?810,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Hierarchical Phrase Table Combination for Machine Translation
Conghui Zhu1 Taro Watanabe2 Eiichiro Sumita2 Tiejun Zhao1
1School of Computer Science and Technology
Harbin Institute of Technology (HIT), Harbin, China
2National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
{chzhu,tjzhao}@mtlab.hit.edu.cn
{taro.watanabe,Sumita}@nict.go.jp
Abstract
Typical statistical machine translation sys-
tems are batch trained with a given train-
ing data and their performances are large-
ly influenced by the amount of data. With
the growth of the available data across
different domains, it is computationally
demanding to perform batch training ev-
ery time when new data comes. In face
of the problem, we propose an efficient
phrase table combination method. In par-
ticular, we train a Bayesian phrasal inver-
sion transduction grammars for each do-
main separately. The learned phrase ta-
bles are hierarchically combined as if they
are drawn from a hierarchical Pitman-Yor
process. The performance measured by
BLEU is at least as comparable to the tra-
ditional batch training method. Further-
more, each phrase table is trained sepa-
rately in each domain, and while compu-
tational overhead is significantly reduced
by training them in parallel.
1 Introduction
Statistical machine translation (SMT) system-
s usually achieve ?crowd-sourced? improvements
with batch training. Phrase pair extraction, the
key step to discover translation knowledge, heav-
ily relies on the scale of training data. Typi-
cally, the more parallel corpora used, the more
phrase pairs and more accurate parameters will
be learned, which can obviously be beneficial to
improving translation performances. Today, more
parallel sentences are drawn from divergent do-
mains, and the size keeps growing. Consequent-
ly, how to effectively use those data and improve
translation performance becomes a challenging is-
sue.
This joint work was done while the first author visited
NICT.
Batch retraining is not acceptable for this case,
since it demands serious computational overhead
when training on a large data set, and it requires
us to re-train every time new training data is avail-
able. Even if we can handle the large computation
cost, improvement is not guaranteed every time we
perform batch tuning on the newly updated train-
ing data obtained from divergent domains. Tradi-
tional domain adaption methods for SMT are also
not adequate in this scenario. Most of them have
been proposed in order to make translation sys-
tems perform better for resource-scarce domain-
s when most training data comes from resource-
rich domains, and ignore performance on a more
generic domain without domain bias (Wang et al,
2012). As an alternative, incremental learning
may resolve the gap by incrementally adding da-
ta sentence-by-sentence into the training data. S-
ince SMT systems trend to employ very large scale
training data for translation knowledge extraction,
updating several sentence pairs each time will be
annihilated in the existing corpus.
This paper proposes a new phrase table combi-
nation method. First, phrase pairs are extracted
from each domain without interfering with oth-
er domains. In particular, we employ the non-
parametric Bayesian phrasal inversion transduc-
tion grammar (ITG) of Neubig et al (2011) to per-
form phrase table extraction. Second, extracted
phrase tables are combined as if they are drawn
from a hierarchical Pitman-Yor process, in which
the phrase tables represented as tables in the Chi-
nese restaurant process (CRP) are hierarchically
chained by treating each of the previously learned
phrase tables as prior to the current one. Thus, we
can easily update the chain of phrase tables by ap-
pending the newly extracted phrase table and by
treating the chain of the previous ones as its prior.
Experiment results indicate that our method can
achieve better translation performance when there
exists a large divergence in domains, and can
802
achieve at least comparable results to batch train-
ing methods, with a significantly less computa-
tional overhead.
The rest of the paper is organized as follows.
In Section 2, we introduce related work. In sec-
tion 3, we briefly describe the translation mod-
el with phrasal ITGs and Pitman-Yor process. In
section 4, we explain our hierarchical combination
approach and give experiment results in section 5.
We conclude the paper in the last section.
2 Related Work
Bilingual phrases are cornerstones for phrase-
based SMT systems (Och and Ney, 2004; Koehn
et al, 2003; Chiang, 2005) and existing translation
systems often get ?crowd-sourced? improvements
(Levenberg et al, 2010). A number of approaches
have been proposed to make use of the full poten-
tial of the available parallel sentences from vari-
ous domains, such as domain adaptation and in-
cremental learning for SMT.
The translation model and language model
are primary components in SMT. Previous work
proved successful in the use of large-scale data for
language models from diverse domains (Brants et
al., 2007; Schwenk and Koehn, 2008). Alterna-
tively, the language model is incrementally up-
dated by using a succinct data structure with a
interpolation technique (Levenberg and Osborne,
2009; Levenberg et al, 2011).
In the case of the previous work on translation
modeling, mixed methods have been investigat-
ed for domain adaptation in SMT by adding do-
main information as additional labels to the orig-
inal phrase table (Foster and Kuhn, 2007). Un-
der this framework, the training data is first di-
vided into several parts, and phase pairs are ex-
tracted with some sub-domain features. Then al-
l the phrase pairs and features are tuned together
with different weights during decoding. As a way
to choose the right domain for the domain adap-
tion, a classifier-based method and a feature-based
method have been proposed. Classification-based
methods must at least add an explicit label to indi-
cate which domain the current phrase pair comes
from. This is traditionally done with an automat-
ic domain classifier, and each input sentence is
classified into its corresponding domain (Xu et al,
2007). As an alternative to the classification-based
approach, Wang et al (2012) employed a feature-
based approach, in which phrase pairs are enriched
by a feature set to potentially reflect the domain in-
formation. The similarity calculated by a informa-
tion retrieval system between the training subset
and the test set is used as a feature for each paral-
lel sentence (Lu et al, 2007). Monolingual topic
information is taken as a new feature for a domain
adaptive translation model and tuned on the devel-
opment set (Su et al, 2012). Regardless of under-
lying methods, either classifier-based or feature-
based method, the performance of current domain
adaptive phrase extraction methods is more sensi-
tive to the development set selection. Usually the
domain similar to a given development data is usu-
ally assigned higher weights.
Incremental learning in which new parallel sen-
tences are incrementally updated to the training
data is employed for SMT. Compared to tradi-
tional frequent batch oriented methods, an online
EM algorithm and active learning are applied to
phrase pair extraction and achieves almost compa-
rable translation performance with less computa-
tional overhead (Levenberg et al, 2010; Gonza?lez-
Rubio et al, 2011). However, their methods usu-
ally require numbers of hyperparameters, such as
mini-batch size, step size, or human judgment to
determine the quality of phrases, and still rely on a
heuristic phrase extraction method in each phrase
table update.
3 Phrase Pair Extraction with
Unsupervised Phrasal ITGs
Recently, phrase alignment with ITGs (Cherry
and Lin, 2007; Zhang et al, 2008; Blunsom et
al., 2008) and parameter estimation with Gibb-
s sampling (DeNero and Klein, 2008; Blunsom
and Cohn, 2010) are popular. Here, we em-
ploy a method proposed by Neubig et al (2011),
which uses parametric Bayesian inference with the
phrasal ITGs (Wu, 1997). It can achieve com-
parable translation accuracy with a much small-
er phrase table than the traditional GIZA++ and
heuristic phrase extraction methods. It has al-
so been proved successful in adjusting the phrase
length granularity by applying character-based
SMT with more sophisticated inference (Neubig
et al, 2012).
ITG is a synchronous grammar formalism
which analyzes bilingual text by introducing in-
verted rules, and each ITG derivation corresponds
to the alignment of a sentence pair (Wu, 1997).
Translation probabilities of ITG phrasal align-
803
ments can be estimated in polynomial time by s-
lightly limiting word reordering (DeNero and K-
lein, 2008).
More formally, P (?e, f?; ?x, ?t
) are the proba-
bility of phrase pairs ?e, f?, which is parameter-
ized by a phrase pair distribution ?t and a symbol
distribution ?x. ?x is a Dirichlet prior, and ?t is es-
timated with the Pitman-Yor process (Pitman and
Yor, 1997; Teh, 2006), which is expressed as
?t ? PY
(
d, s, Pdac
) (1)
where d is the discount parameter, s is the strength
parameter, and , and Pdac is a prior probability
which acts as a fallback probability when a phrase
pair is not in the model.
Under this model, the probability for a phrase
pair found in a bilingual corpus ?E,F ? can be rep-
resented by the following equation using the Chi-
nese restaurant process (Teh, 2006):
P
(
?ei, fi?; ?E,F ?
)
= 1C + s(ci ? d? ti)+
1
C + s(s+ d? T )? Pdac(?ei, fi?) (2)
where
1. ci and ti are the customer and table count of
the ith phrase pair ?ei, fi? found in a bilingual
corpus ?E,F ?;
2. C and T are the total customer and table count
in corpus ?E,F ?;
3. d and s are the discount and strengthen hyper-
parameters.
The prior probability Pdac is recursively defined
by breaking a longer phrase pair into two through
the recursive ITG?s generative story as follows
(Neubig et al, 2011):
1. Generate symbol x from Px(x; ?x) with three
possible values: Base, REG, or INV .
2. Depending on the value of x take the following
actions.
a. If x = Base, generate a new phrase pair
directly from Pbase.
b. If x = REG, generate ?e1, f1? and
?e2, f2? from P
(
?e, f?; ?x, ?t
), and con-
catenate them into a single phrase pair
?e1e2, f1f2?.
Figure 1: A word alignment (a), and its hierarchi-
cal derivation (b).
c. If x = INV , follow a similar process as b,
but concatenate f1 and f2 in reverse order
?e1e2, f2f1?.
Note that the Pdac is recursively defined through
the binary branched P , which in turns employs
Pdac as a prior probability. Pbase is a base measure
defined as a combination of the IBM Models in t-
wo directions and the unigram language models in
both sides. Inference is carried out by a heuristic
beam search based block sampling with an effi-
cient look ahead for a faster convergence (Neubig
et al, 2012).
Compared to GIZA++ with heuristic phrase ex-
traction, the Bayesian phrasal ITG can achieve
competitive accuracy under a smaller phrase ta-
ble size. Further, the fallback model can incor-
porate phrases of all granularity by following the
ITG?s recursive definition. Figure 1 (b) illustrates
an example of the phrasal ITG derivation for word
alignment in Figure 1 (a) in which a bilingual sen-
tence pair is recursively divided into two through
the recursively defined generative story.
4 Hierarchical Phrase Table
Combination
We propose a new phrase table combination
method, in which individually learned phrase ta-
ble are hierarchically chained through a hierarchi-
cal Pitman-Yor process.
Firstly, we assume that the whole train-
ing data ?E,F ? can be split into J domains,
{?E1, F 1?, . . . , ?EJ , F J?}. Then phrase pairs are
804
Figure 2: A hierarchical phrase table combination (a), and a basic unit of a Chinese restaurant process
with K tables and N customers.
extracted from each domain j (1 ? j ? J) sepa-
rately with the method introduced in Section 3. In
traditional domain adaptation approaches, phrase
pairs are extracted together with their probabili-
ties and/or frequencies so that the extracted phrase
pairs are merged uniformly or after scaling.
In this work, we extract the table counts for each
phrase pair under the Chinese restaurant process
given in Section 3. In Figure 2 (b), a CRP is illus-
trated which has K tables and N customers with
each chair representing a customer. Meanwhile
there are two parameters, discount and strength for
each domain similar to the ones in Equation (1).
Our proposed hierarchical phrase table combi-
nation can be formally expressed as following:
?1 ? PY (d1, s1, P 2)
? ? ? ? ? ?
?j ? PY (dj , sj , P j+1)
? ? ? ? ? ?
?J ? PY
(
dJ , sJ , P Jbase
) (3)
Here the (j + 1)th layer hierarchical Pitman-Yor
process is employed as a base measure for the
jth layer hierarchical Pitman-Yor process. The
hierarchical chain is terminated by the base mea-
sure from the J th domain P Jbase. The hierarchi-
cal structure is illustrated in Figure 2 (a) in which
the solid lines implies a fall back using the ta-
ble counts from the subsequent domains, and the
dotted lines means the final fallback to the base
measure P Jbase. When we query a probability of
a phrase pair ?e, f?, we first query the probabil-
ity of the first layer P 1(?e, f?). If ?e, f? is not
in the model, we will fallback to the next level of
P 2(?e, f?). This process continues until we reach
the Jth base measure of P J(?e, f?). Each fallback
can be viewed as a translation knowledge integra-
tion process between subsequent domains.
For example in Figure 2 (a), the ith phrase pair
?ei, fi? appears only in the domain 1 and domain
2, so its translation probability can be calculated
by substituting Equation (3) with Equation (2):
P
(
?ei, fi?; ?E,F ?
)
= 1C1 + s1 (c
1
i ? d1 ? t1i )
+ s
1 + d1 ? T 1
(C1 + s1)? (C2 + s2)(c
2
i ? d2 ? t2i )
+
J?
j=1
(sj + dj ? T j
Cj + sj
)
? P Jbase(?ei, fi?) (4)
where the superscript indicates the domain for the
corresponding counts, i.e. cji for the customer
count in the jth domain. The first term in Equa-
tion (4) is the phrase probability from the first do-
main, and the second one comes from the second
domain, but weighted by the fallback weight of the
1st domain. Since ?ei, fi? does not appear in the
rest of the layers, the last term is taken from al-
l the fallback weight from the second layer to the
J th layer with the final P Jbase. All the parameter-
s ?j and hyperparameters dj and sj , are obtained
by learning on the jth domain. Returning the hy-
perparameters again when cascading another do-
main may improve the performance of the combi-
nation weight, but we will leave it for future work.
The hierarchical process can be viewed as an in-
stance of adapted integration of translation knowl-
edge from each sub-domain.
805
Algorithm 1 Translation Probabilities Estima-
tion
Input: cji , tji , P jbase, Cj , T j , dj and sjOutput: The translation probabilities for each
pair
1: for all phrase pair ?ei, fi? do
2: Initialize the P (?ei, fi?) = 0 and wi = 1
3: for all domain ?Ej , Fj? such that 1 6 j 6
J ? 1 do
4: if ?ei, fi? ? ?Ej , Fj? then
5: P (?ei, fi?) += wi ? (Cji ? dj ?
tji )/(Cj + sj)
6: end if
7: wi = wi ? (sj + dj ? T j)/(Cj + sj)
8: end for
9: P (?ei, fi?) += wi? (CJi ?dJ ? tJi + (sJ +
dJ ? T J)? P Jbase(?ei, fi?))/(CJ + sJ)
10: end for
Our approach has several advantages. First,
each phrase pair extraction can concentrate on a s-
mall portion of domain-specific data without inter-
fering with other domains. Since no tuning stage
is involved in the hierarchical combination, we can
easily include a new phrase table from a new do-
main by simply chaining them together. Second,
phrase pair phrase extraction in each domain is
completely independent, so it is easy to parallelize
in a situation where the training data is too large
to fit into a small amount of memory. Finally, new
domains can be integrated incrementally. When
we encounter a new domain, and if a phrase pair is
completely new in terms of the model, the phrase
pair is simply appended to the current model, and
computed without the fallback probabilities, since
otherwise, the phrase pair would be boosted by the
fallback probabilities. Pitman-Yor process is also
employed in n-gram language models which are
hierarchically represented through the hierarchi-
cal Pitman-Yor process with switch priors to in-
tegrate different domains in all the levels (Wood
and Teh, 2009). Our work incrementally combines
the models from different domains by directly em-
ploying the hierarchical process through the base
measures.
5 Experiment
We evaluate the proposed approach on the
Chinese-to-English translation task with three data
sets with different scales.
Data set Corpus #sent. pairs
IWSLT HIT 52, 603
BTEC 19, 975
Domain 1 47, 993
Domain 2 30, 272
FBIS Domain 3 49, 509
Domain 4 38, 228
Domain 5 55, 913
News 221, 915
News 95, 593
LDC Magazine 98, 335
Magazine 254, 488
Finance 86, 112
Table 1: The sentence pairs used in each data set.
5.1 Experiment Setup
The first data set comes from the IWSLT2012
OLYMPICS task consisting of two training sets:
the HIT corpus, which is closely related to the Bei-
jing 2008 Olympic Games, and the BTEC corpus,
which is a multilingual speech corpus containing
tourism-related sentences. The second data set,
the FBIS corpus, is a collection of news articles
and does not have domain information itself, so a
Latent Dirichlet Allocation (LDA) tool, PLDA1,
is used to divide the whole corpus into 5 different
sub-domains according to the concatenation of the
source side and target side as a single sentence (Li-
u et al, 2011). The third data set is composed of 5
corpora2 from LDC with various domains, includ-
ing news, magazine, and finance. The details are
shown in Table 1.
In order to evaluate our approach, four phrase
pair extraction methods are performed:
1. GIZA-linear: Phase pairs are extracted in each
domain by GIZA++ (Och and Ney, 2003) and
the ?grow-diag-final-and? method with a max-
imum length 7. The phrase tables from vari-
ous domains are linearly combined by averag-
ing the feature values.
2. Pialign-linear: Similar to GIZA-linear, but we
employed the phrasal ITG method described in
Section 3 using the pialign toolkit 3 (Neubig et
1http://code.google.com/p/plda/
2In particular, they come from LDC catalog number:
LDC2002E18, LDC2002E58, LDC2003E14, LDC2005E47,
LDC2006E26, in this order.
3http://www.phontron.com/pialign/
806
Methods IWSLT FBIS LDCBLEU Size BLEU Size BLEU Size
GIZA-linear 19.222 1,200,877 29.342 15,369,028 30.67 77,927,347
Pialign-linear 19.534 876,059 29.858 7,235,342 31.12 28,877,149
GIZA-batch 19.616 1,185,255 31.38 13,737,258 32.06 63,606,056
Pialign-batch 19.506 841,931 31.104 6,459,200
Pialign-adaptive 19.624 841,931 30.926 6,459,200
Hier-combin 20.32 876,059 31.29 7,235,342 32.03 28,877,149
Table 2: BLEU scores and phrase table size by alignment method and probabilities estimation method.
Pialign was run with five samples. Because of computational overhead, the baseline Pialign-batch and
Pialign-adaptive were not run on the largest data set.
al., 2011). Extracted phrase pairs are linearly
combined by averaging the feature values.
3. GIZA-batch: Instead of splitting into each do-
main, the data set is merged as a single corpus
and then a heuristic GZA-based phrase extrac-
tion is performed, similar as GIZA-linear.
4. Pialign-batch: Similar to the GIZA-batch, a s-
ingle model is estimated from a single, merged
corpus. Since pialign cannot handle large data,
we did not experiment on the largest LDC data
set.
5. Pialign-adaptive: Alignment and phrase pairs
extraction are same to Pialign-batch, while
translation probabilities are estimated by the
adaptive method with monolingual topic in-
formation (Su et al, 2012). The method es-
tablished the relationship between the out-of-
domain bilingual corpus and in-domain mono-
lingual corpora via topic distribution to esti-
mate the translation probability.
?(e?|f?) =
?
tf
?(e?, tf |f?)
=
?
tf
?(e?|tf , f?) ? P (tf |f?)
(5)
where ?(e?|tf , f?) is the probability of translating f?
into e? given the source-side topic f? , P (tf |f?) is
the phrase-topic distribution of f.
The method we proposed is named Hier-
combin. It extracts phrase pairs in the same way as
the Pialign-linear. In the phrase table combination
process, the translation probability of each phrase
pair is estimated by the Hier-combin and the other
features are also linearly combined by averaging
the feature values. Pialign is used with default pa-
rameters. The parameter ?samps? is set to 5, which
indicates 5 samples are generated for a sentence
pair.
The IWSLT data consists of roughly 2, 000 sen-
tences and 3, 000 sentences each from the HIT and
BTEC for development purposes, and the test da-
ta consists of 1, 000 sentences. For the FBIS and
LDC task, we used NIST MT 2002 and 2004 for
development and testing purposes, consisting of
878 and 1, 788 sentences respectively. We em-
ploy Moses, an open-source toolkit for our exper-
iment (Koehn et al, 2007). SRILM Toolkit (Stol-
cke, 2002) is employed to train 4-gram language
models on the Xinhua portion of Gigaword cor-
pus, while for the IWLST2012 data set, only its
training set is used. We use batch-MIRA (Cher-
ry and Foster, 2012) to tune the weight for each
feature and translation quality is evaluated by the
case-insensitive BLEU-4 metric (Papineni et al,
2002). The BLEU scores reported in this paper
are the average of 5 independent runs of indepen-
dent batch-MIRA weight training, as suggested by
(Clark et al, 2011).
5.2 Result and Analysis
5.2.1 Performances of various extraction
methods
We carry out a series of experiments to evaluate
translation performance. The results are listed in
Table 2. Our method significantly outperforms the
baseline Pialign-linear. Except for the translation
probabilities, the phrase pairs of two methods are
exactly same, so the number of phrase pairs are
equal in the two methods. Further more, the per-
formance of the baseline Pialign-adaptive is also
higher than the baseline Pialign-linear?s and lower
than ours. This proves that the adaptive method
807
Methods Task Time(minute)
Batch Retraining 536.9
Hierarchical Parallel Extraction 122.55
Combination Integrating 1.5
Total 124.05
Table 3: Minutes used for alignment and phase
pair extraction in the FBIS data set.
with monolingual topic information is useful in
the tasks, but our approach with the hierarchical
Pitman-Yor process can estimate more accurate
translation probabilities based on all the data from
various domains.
Compared with the GIZA-batch, our approach
achieves competitive performance with a much s-
maller phrase table. The number of phase pairs
generated by our method is only 73.9%, 52.7%,
and 45.4% of the GIZA-batch?s respectively. In
the IWLST2012 data set, there is a huge difference
gap between the HIT corpus and the BTEC corpus,
and our method gains 0.814 BLEU improvement.
While the FBIS data set is artificially divided and
no clear human assigned differences among sub-
domains, our method loses 0.09 BLEU.
In the framework we proposed, phrase pairs are
extracted from each domain completely indepen-
dent of each other, so those tasks can be executed
on different machines, at different times, and of
course in parallel when we assume that the do-
mains are not incrementally added in the train-
ing data. The runtime of our approach and the
batch-based ITGs sampling method in the FBIS
data set is listed in Table 3 measured on a 2.7 GHz
E5-2680 CPU and 128 Gigabyte memory. When
comparing the hier-combin with the pialign-batch,
the BLEU scores are a little higher while the time
spent for training is much lower, almost one quar-
ter of the pialign-batch.
Even the performance of the pialign-linear is
better than the Baseline GIZA-linear?s, which
means that phrase pair extraction with hierarchi-
cal phrasal ITGs and sampling is more suitable
for domain adaptation tasks than the combination
GIZA++ and a heuristic method.
Generally, the hierarchical combination method
exploits the nature of a hierarchical Pitman-Yor
process and gains the advantage of its smoothing
effect, and our approach can incrementally gener-
ate a succinct phrase table based on all the data
from various domains with more accurate prob-
abilities. Traditional SMT phrase pair extraction
is batch-based, while our method has no obvious
shortcomings in translation accuracy, not to men-
tion efficiency.
5.2.2 Effect of Integration Order
Here, we evaluate whether our hierarchical com-
bination is sensitive to the order of the domains
when forming a hierarchical structure. Through
Equation (3), in our experiments, we chained the
domains in the order listed in Table 1, which is
in almost chronological order. Table 4 shows the
BLEU scores for the three data sets, in which the
order of combining phrase tables from each do-
main is alternated in the ascending and descending
of the similarity to the test data. The similarity be-
tween the data from each domain and the test data
is calculated using the perplexity measure with 5-
gram language model. The model learned from
the domain more similar to the test data is placed
in the front so that it can largely influence the
parameter computation with less backoff effects.
There is a big difference between the two opposite
order in IWSLT 2012 data set, in which more than
one point of decline in BLEU score when taking
the BTEC corpus as the first layer. Note that the
perplexity of BTEC was 344.589 while that of HIT
was 107.788. The result may indicate that our hi-
erarchical phrase combination method is sensitive
to the integration order when the training data is
small and there exists large gap in the similarity.
However, if most domains are similar (FBIS data
set) or if there are enough parallel sentence pairs
(NIST data set) in each domain, then the transla-
tion performances are almost similar even with the
opposite integrating orders.
IWSLT FBIS LDC
Descending 20.154 30.491 31.268
Ascending 19.066 30.388 31.254
Difference 1.088 0.103 0.014
Table 4: BLEU scores for the hierarchical model
with different integrating orders. Here Pialign was
run without multi-samples.
6 Conclusion and Future Work
In this paper, we present a novel hierarchical
phrase table combination method for SMT, which
can exploit more of the potential from all of da-
ta coming from various fields and generate a suc-
808
cinct phrase table with more accurate translation
probabilities. The method assumes that a com-
bined model is derived from a hierarchical Pitman-
Yor process with each prior learned separately in
each domain, and achieves BLEU scores competi-
tive with traditional batch-based ones. Meanwhile,
the framework has natural characteristics for par-
allel and incremental phrase pair extraction. The
experiment results on three different data sets in-
dicate the effectiveness of our approach.
In future work, we will also introduce incre-
mental learning for phase pair extraction inside a
domain, which means using the current translation
probabilities already obtained as the base measure
of sampling parameters for the upcoming domain.
Furthermore, we will investigate any tradeoffs be-
tween the accuracy of the probability estimation
and the coverage of phrase pairs.
Acknowledgments
We would like to thank our colleagues in both
HIT and NICT for insightful discussions, and
three anonymous reviewers for many invaluable
comments and suggestions to improve our paper.
This work is supported by National Natural Sci-
ence Foundation of China (61100093, 61173073,
61073130, 61272384), and the Key Project of the
National High Technology Research and Develop-
ment Program of China (2011AA01A207).
References
Phil Blunsom and Trevor Cohn. 2010. Inducing syn-
chronous grammars with slice sampling. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 238?241,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statisti-
cal machine translation. In Proceedings of ACL,
pages 200?208, Columbus, Ohio, June. Association
for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computation-
al Natural Language Learning (EMNLP-CoNLL),
pages 858?867, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montre?al, Canada, June. Association for
Computational Linguistics.
Colin Cherry and Dekang Lin. 2007. Inversion
transduction grammar for joint phrasal translation
modeling. In Proceedings of SSST, NAACL-HLT
2007/AMTA Workshop on Syntax and Structure in
Statistical Translation, pages 17?24.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ?05, pages 263?
270, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: controlling for opti-
mizer instability. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 176?181, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John DeNero and Dan Klein. 2008. The complexi-
ty of phrase alignment problems. In Proceedings of
ACL-08: HLT, Short Papers, pages 25?28, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 128?135.
Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Martinez, and
Francisco Casacuberta. 2011. Fast incremental ac-
tive learning for statistical machine translation. A-
VANCES EN INTELIGENCIA ARTIFICIAL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 45?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Abby Levenberg and Miles Osborne. 2009. Stream-
based randomised language models for smt. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2-
Volume 2, pages 756?764. Association for Compu-
tational Linguistics.
809
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 394?
402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
177?186, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirichlet
allocation with data placement and pipeline process-
ing. ACM Transactions on Intelligent Systems and
Technology (TIST), 2(3):1?18.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computation-
al Natural Language Learning (EMNLP-CoNLL),
pages 343?350, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistic-
s: Human Language Technologies, pages 632?641,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Graham Neubig, Taro Watanabe, Shinsuke Mori, and
Tatsuya Kawahara. 2012. Machine translation with-
out words through substring alignment. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 165?174, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449, Decem-
ber.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Jim Pitman and Marc Yor. 1997. The two-parameter
poisson-dirichlet distribution derived from a stable
subordinator. The Annals of Probability, 25(2):855?
900.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In International Joint Conference on
Natural Language Processing, pages 661?668.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen, X-
iaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 459?468.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 985?992. Association for Computa-
tional Linguistics.
Wei Wang, Klaus Macherey, Wolfgang Macherey,
Franz Och, and Peng Xu. 2012. Improved do-
main adaptation for statistical machine translation.
In Proceedings of the Conference of the Association
for Machine translation, Americas.
F. Wood and Y. W. Teh. 2009. A hierarchical non-
parametric Bayesian approach to statistical language
model domain adaptation. In Proceedings of the In-
ternational Conference on Artificial Intelligence and
Statistics, volume 12.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Jia Xu, Yonggang Deng, Yuqing Gao, and Hermann
Ney. 2007. Domain dependent statistical machine
translation. In Proceedings of the MT Summit XI.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97?105,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
810
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 312?317,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Cross-lingual Projections between Languages from Different Families
Mo Yu1 Tiejun Zhao1 Yalong Bai1 Hao Tian2 Dianhai Yu2
1School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
{yumo,tjzhao,ylbai}@mtlab.hit.edu.cn
2Baidu Inc., Beijing, China
{tianhao,yudianhai}@baidu.com
Abstract
Cross-lingual projection methods can ben-
efit from resource-rich languages to im-
prove performances of NLP tasks in
resources-scarce languages. However,
these methods confronted the difficulty of
syntactic differences between languages
especially when the pair of languages
varies greatly. To make the projection
method well-generalize to diverse lan-
guages pairs, we enhance the projec-
tion method based on word alignments
by introducing target-language word rep-
resentations as features and proposing a
novel noise removing method based on
these word representations. Experiments
showed that our methods improve the per-
formances greatly on projections between
English and Chinese.
1 Introduction
Most NLP studies focused on limited languages
with large sets of annotated data. English and
Chinese are examples of these resource-rich lan-
guages. Unfortunately, it is impossible to build
sufficient labeled data for all tasks in all lan-
guages. To address NLP tasks in resource-scarce
languages, cross-lingual projection methods were
proposed, which make use of existing resources
in resource-rich language (also called source lan-
guage) to help NLP tasks in resource-scarce lan-
guage (also named as target language).
There are several types of projection methods.
One intuitive and effective method is to build a
common feature space for all languages, so that
the model trained on one language could be di-
rectly used on other languages (McDonald et al,
2011; Ta?ckstro?m et al, 2012). We call it di-
rect projection, which becomes very popular re-
cently. The main limitation of these methods is
that target language has to be similar to source
language. Otherwise the performance will de-
grade especially when the orders of phrases be-
tween source and target languages differ a lot.
Another common type of projection methods
map labels from resource-rich language sentences
to resource-scarce ones in a parallel corpus us-
ing word alignment information (Yarowsky et al,
2001; Hwa et al, 2005; Das and Petrov, 2011).
We refer them as projection based on word align-
ments in this paper. Compared to other types of
projection methods, this type of methods is more
robust to syntactic differences between languages
since it trained models on the target side thus fol-
lowing the topology of the target language.
This paper aims to build an accurate projec-
tion method with strong generality to various pairs
of languages, even when the languages are from
different families and are typologically divergent.
As far as we know, only a few works focused
on this topic (Xia and Lewis 2007; Ta?ckstro?m
et al, 2013). We adopted the projection method
based on word alignments since it is less affected
by language differences. However, such methods
also have some disadvantages. Firstly, the models
trained on projected data could only cover words
and cases appeared in the target side of parallel
corpus, making it difficult to generalize to test data
in broader domains. Secondly, the performances
of these methods are limited by the accuracy of
word alignments, especially when words between
two languages are not one-one aligned. So the ob-
tained labeled data contains a lot of noises, making
the models built on them less accurate.
This paper aims to build an accurate projection
method with strong generality to various pairs of
languages. We built the method on top of projec-
tion method based on word alignments because of
its advantage of being less affected by syntactic
differences, and proposed two solutions to solve
the above two difficulties of this type of methods.
312
Firstly, we introduce Brown clusters of target
language to make the projection models cover
broader cases. Brown clustering is a kind of word
representations, which assigns word with similar
functions to the same cluster. They can be ef-
ficiently learned on large-scale unlabeled data in
target language, which is much easier to acquire
even when the scales of parallel corpora of minor
languages are limited. Brown clusters have been
first introduced to the field of cross-lingual projec-
tions in (Ta?ckstro?m et al, 2012) and have achieved
great improvements on projection between Euro-
pean languages. However, their work was based
on the direct projection methods so that it do not
work very well between languages from different
families as will be shown in Section 3.
Secondly, to reduce the noises in projection, we
propose a noise removing method to detect and
correct noisy projected labels. The method was
also built on Brown clusters, based on the assump-
tion that instances with similar representations of
Brown clusters tend to have similar labels. As far
as we know, no one has done any research on re-
moving noises based on the space of word repre-
sentations in the field of NLP.
Using above techniques, we achieved a projec-
tion method that adapts well on different language
pairs even when the two languages differ enor-
mously. Experiments of NER and POS tagging
projection from English to Chinese proved the ef-
fectiveness of our methods.
In the rest of our paper, Section 2 describes the
proposed cross-lingual projection method. Evalu-
ations are in Section 3. Section 4 gives concluding
remarks.
2 Proposed Cross-lingual Projection
Methods
In this section, we first briefly introduce the cross-
lingual projection method based on word align-
ments. Then we describe how the word represen-
tations (Brown clusters) were used in the projec-
tion method. Section 2.3 describes the noise re-
moving methods.
2.1 Projection based on word alignments
In this paper we consider cross-lingual projec-
tion based on word alignment, because we want
to build projection methods that can be used be-
tween language pairs with large differences. Fig-
ure 1 shows the procedure of cross-lingual projec-
tion methods, taking projection of NER from En-
glish to Chinese as an example. Here English is
the resource-rich language and Chinese is the tar-
get language. First, sentences from the source side
of the parallel corpus are labeled by an accurate
model in English (e.g., ?Rongji Zhu? and ?Gan
Luo? were labeled as ?PER?), since the source
language has rich resources to build accurate NER
models. Then word alignments are generated from
the parallel corpus and serve as a bridge, so that
unlabeled words in the target language will get the
same labels with words aligning to them in the
source language, e.g. the first word ??(??)??
in Chinese gets the projected label ?PER?, since it
is aligned to ?Rongji? and ?Zhu?. In this way, la-
bels in source language sentences are projected to
the target sentences.
... ...
... ...O inspected
??
(O)O have
?
(O)O others
?? (O)O and
PER Yi ? (O)
PER Wu ?? (PER)
O ,
PER Gan
? (O)
PER Luo
?(??)? (PER)
O ,
PER Rongji
PER Zhu
Figure 1: An example of projection of NER. La-
bels of Chinese sentence (right) in brackets are
projected from the source sentence.
From the projection procedure we can see that a
labeled dataset of target language is built based on
the projected labels from source sentences. The
projected dataset has a large size, but with a lot
of noises. With this labeled dataset, models of the
target language can be trained in a supervised way.
Then these models can be used to label sentences
in target language. Since the models are trained
on the target language, this projection approach is
less affected by language differences, comparing
with direct projection methods.
2.2 Word Representation features for
Cross-lingual Projection
One disadvantage of above method is that the cov-
erage of projected labeled data used for training
313
Words wi,i?{?2:2}, wi?1/wi,i?{0,1}
Cluster ci,i?{?2:2}, ci?1/ci,i?{?1,2}, c?1/c1
Transition y?1/y0/{w0, c0, c?1/c1}
Table 1: NER features. ci is the cluster id of wi.
target language models are limited by the cover-
age of parallel corpora. For example in Figure 1,
some Chinese politicians in 1990?s will be learned
as person names, but some names of recent politi-
cians such as ?Obama?, which did not appeared in
the parallel corpus, would not be recognized.
To broader the coverage of the projected data,
we introduced word representations as features.
Same or similar word representations will be as-
signed to words appearing in similar contexts,
such as person names. Since word representations
are trained on large-scale unlabeled sentences in
target language, they cover much more words than
the parallel corpus does. So the information of a
word in projected labeled data will apply to other
words with the same or similar representations,
even if they did not appear in the parallel data.
In this work we use Brown clusters as word rep-
resentations on target languages. Brown clustering
assigns words to hierarchical clusters according to
the distributions of words before and after them.
Taking NER as an example, the feature template
may contain features shown in Table 1. The cluster
id of the word to predict (c0) and those of context
words (ci, i ? {?2,?1, 1, 2}), as well as the con-
junctions of these clusters were used as features in
CRF models in the same way the traditional word
features were used. Since Brown clusters are hi-
erarchical, the cluster for each word can be rep-
resented as a binary string. So we also use prefix
of cluster IDs as features, in order to compensate
for clusters containing small number of words. For
languages lacking of morphological changes, such
as Chinese, there are no pre/suffix or orthography
features. However the cluster features are always
available for any languages.
2.3 Noise Removing in Word Representation
Space
Another disadvantage of the projection method is
that the accuracy of projected labels is badly af-
fected by non-literate translation and word align-
ment errors, making the data contain many noises.
For example in Figure 1, the word ???(Wu Yi)?
was not labeled as a named entity since it was
not aligned to any words in English due to the
alignment errors. A more accurate model will be
trained if such noises can be reduced.
A direct way to remove the noises is to mod-
ify the label of a word to make it consistent with
the majority of labels assigned to the same word in
the parallel corpus. The method is limited when a
word with low frequency has many of its appear-
ances incorrectly labeled because of alignment er-
rors. In this situation the noises are impossible to
remove according to the word itself. The error in
Figure 1 is an example of this case since the other
few occurrences of the word ???(Wu Yi)? also
happened to fail to get the correct label.
Such difficulties can be easily solved when we
turned to the space of Brown clusters, based on
the observation that words in a same cluster tend
to have same labels. For example in Figure 1, the
word ???(Wu Yi)?, ??(??)?(Zhu Rongji)?
and ???(Luo Gan)? are in the same cluster, be-
cause they are all names of Chinese politicians
and usually appear in similar contexts. Having ob-
served that a large portion of words in this cluster
are person names, it is reasonable to modified the
label of ???(Wu Yi)? to ?PER?.
The space of clusters is also less sparse so it is
also possible to use combination of the clusters to
help noise removing, in order to utilize the context
information of data instances. For example, we
could represent a instance as bigram of the cluster
of target word and that of the previous word. And
it is reasonable that its label should be same with
other instances with the same cluster bigrams.
The whole noise removing method can be rep-
resented as following: Suppose a target word wi
was assigned label yi during projection with prob-
ability of alignment pi. From the whole projected
labeled data, we can get the distribution pw(y) for
the word wi, the distribution pc(y) for its cluster
ci and the distribution pb(y) for the bigram ci?1ci.
We choose y?i = y?, which satisfies
y? = argmaxy(?y,yipi + ?x?{w,c,b}px(y)) (1)
?y,yi is an indicator function, which is 1 when
y equals to yi. In practices, we set pw/c/b(y) to 0
for the ys that make the probability less than 0.5.
With the noise removing method, we can build a
more accurate labeled dataset based on the pro-
jected data and then use it for training models.
314
3 Experimental Results
3.1 Data Preparation
We took English as resource-rich language and
used Chinese to imitate resource-scarce lan-
guages, since the two languages differ a lot. We
conducted experiments on projections of NER and
POS tagging. The resource-scarce languages were
assumed to have no training data. For the NER
experiments, we used data from People?s Daily
(April. 1998) as test data (55,177 sentences). The
data was converted following the style of Penn
Chinese Treebank (CTB) (Xue et al, 2005). For
evaluation of projection of POS tagging, we used
the test set of CTB. Since English and Chinese
have different annotation standards, labels in the
two languages were converted to the universal
POS tag set (Petrov et al, 2011; Das and Petrov,
2011) so that the labels between the source and tar-
get languages were consistent. The universal tag
set made the task of POS tagging easier since the
fine-grained types are no more cared.
The Brown clusters were trained on Chinese
Wikipedia. The bodies of all articles are retained
to induce 1000 clusters using the algorithm in
(Liang, 2005) . Stanford word segmentor (Tseng
et al, 2005) was used for Chinese word segmenta-
tion. When English Brown clusters were in need,
we trained the word clusters on the tokenized En-
glish Wikipedia.
We chose LDC2003E14 as the parallel corpus,
which contains about 200,000 sentences. GIZA++
(Och and Ney, 2000) was used to generate word
alignments. It is easier to obtain similar amount
of parallel sentences between English and minor
languages, making the conclusions more general
for problems of projection in real applications.
3.2 Performances of NER Projection
Table 2 shows the performances of NER projec-
tion. We re-implemented the direct projection
method with projected clusters in (Ta?ckstro?m et
al., 2012). Although their method was proven to
work well on European language pairs, the results
showed that projection based on word alignments
(WA) worked much better since the source and tar-
get languages are from different families.
After we add the clusters trained on Chinese
Wikipedia as features as in Section 2.2, a great
improvement of about 9 points on the average F1-
score of the three entity types was achieved, show-
ing that the word representation features help to
System avgPrec
avg
Rec
avg
F1
Direct projection 47.48 28.12 33.91
Proj based on WA 71.6 37.84 47.66
+clusters(from en) 63.96 46.59 53.75
+clusters(ch wiki) 73.44 47.63 56.60
Table 2: Performances of NER projection.
recall more named entities in the test set. The per-
formances of all three categories of named entities
were improved greatly after adding word repre-
sentation features. Larger improvements were ob-
served on person names (14.4%). One of the rea-
sons for the improvements is that in Chinese, per-
son names are usually single words. Thus Brown-
clustering method can learn good word representa-
tions for those entities. Since in test set, most enti-
ties that are not covered are person names, Brown
clusters helped to increase the recall greatly.
In (Ta?ckstro?m et al, 2012), Brown clusters
trained on the source side were projected to the
target side based on word alignments. Rather than
building a same feature space for both the source
language and the target language as in (Ta?ckstro?m
et al, 2012), we tried to use the projected clus-
ters as features in projection based on word align-
ments. In this way the two methods used exactly
the same resources. In the experiments, we tried
to project clusters trained on English Wikipedia
to Chinese words. They improved the perfor-
mance by about 6.1% and the result was about
20% higher than that achieved by the direct pro-
jection method, showing that even using exactly
the same resources, the proposed method out-
performed that in (Ta?ckstro?m et al, 2012) much
on diverse language pairs.
Next we studied the effects of noise removing
methods. Firstly, we removed noises according to
Eq(1), which yielded another huge improvement
of about 6% against the best results based on clus-
ter features. Moreover, we conducted experiments
to see the effects of each of the three factors. The
results show that both the noise removing methods
based on words and on clusters achieved improve-
ments between 1.5-2 points. The method based on
bigram features got the largest improvement of 3.5
points. It achieved great improvement on person
names. This is because a great proportion of the
vocabulary was made up of person names, some of
which are mixed in clusters with common nouns.
315
While noise removing method based on clusters
failed to recognize them as name entities, cluster
bigrams will make use of context information to
help the discrimination of these mixed clusters.
System PER LOC ORG AVG
By Eq(1) 59.77 55.56 72.26 62.53
By clusters 49.75 53.10 72.46 58.44
By words 49.00 54.69 70.59 58.09
By bigrams 58.39 55.01 66.88 60.09
Table 3: Performances of noise removing methods
3.3 Performances of POS Projection
In this section we test our method on projection
of POS tagging from English to Chinese, to show
that our methods can well extend to other NLP
tasks. Unlike named entities, POS tags are asso-
ciated with single words. When one target word
is aligned to more than one words with different
POS tags on the source side, it is hard to decide
which POS tag to choose. So we only retained the
data labeled by 1-to-1 alignments, which also con-
tain less noises as pointed out by (Hu et al, 2011).
The same feature template as in the experiments
of NER was used for training POS taggers.
The results are listed in Table 4. Because of the
great differences between English and Chinese,
projection based on word alignments worked bet-
ter than direct projection did. After adding word
cluster features and removing noises, an error re-
duction of 12.7% was achieved.
POS tagging projection can benefit more from
our noise removing methods than NER projection
could, i.e. noise removing gave rise to a higher
improvement (2.7%) than that achieved by adding
cluster features on baseline system (1.5%). One
possible reason is that our noise removing meth-
ods assume that labels are associated with single
words, which is more suitable for POS tagging.
Methods Accuracy
Direct projection (Ta?ckstro?m) 62.71
Projection based on WA 66.68
+clusters (ch wiki) 68.23
+cluster(ch)&noise removing 70.92
Table 4: Performances of POS tagging projection.
4 Conclusion and perspectives
In this paper we introduced Brown clusters of
target languages to cross-lingual projection and
proposed methods for removing noises on pro-
jected labels. Experiments showed that both the
two techniques could greatly improve the perfor-
mances and could help the projection method well
generalize to languages differ a lot.
Note that although projection methods based on
word alignments are less affected by syntactic dif-
ferences, the topological differences between lan-
guages still remain an importance reason for the
limitation of performances of cross-lingual projec-
tion. In the future we will try to make use of repre-
sentations of sub-structures to deal with syntactic
differences in more complex tasks such as projec-
tion of dependency parsing. Future improvements
also include combining the direct projection meth-
ods based on joint feature representations with the
proposed method as well as making use of pro-
jected data from multiple languages.
Acknowledgments
We would like to thank the anonymous review-
ers for their valuable comments and helpful sug-
gestions. This work was supported by National
Natural Science Foundation of China (61173073),
and the Key Project of the National High Technol-
ogy Research and Development Program of China
(2011AA01A207).
References
P.F. Brown, P.V. Desouza, R.L. Mercer, V.J.D. Pietra,
and J.C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational linguistics,
18(4):467?479.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 600?609.
P.L. Hu, M. Yu, J. Li, C.H. Zhu, and T.J. Zhao.
2011. Semi-supervised learning framework for
cross-lingual projection. In Web Intelligence
and Intelligent Agent Technology (WI-IAT), 2011
IEEE/WIC/ACM International Conference on, vol-
ume 3, pages 213?216. IEEE.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural language
engineering, 11(3):311?326.
316
W. Jiang and Q. Liu. 2010. Dependency parsing and
projection based on word-pair classification. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL, volume 10,
pages 12?20.
P. Liang. 2005. Semi-supervised learning for natural
language. Ph.D. thesis, Massachusetts Institute of
Technology.
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
62?72. Association for Computational Linguistics.
F.J. Och and H. Ney. 2000. Giza++: Training of statis-
tical translation models.
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure.
O Ta?ckstro?m, R McDonald, and J Nivre. 2013. Tar-
get language adaptation of discriminative transfer
parsers. Proceedings of NAACL-HLT.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and
C. Manning. 2005. A conditional random field
word segmenter for sighan bakeoff 2005. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chi-
nese Language Processing, volume 171. Jeju Island,
Korea.
F Xia and W Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proc. of
the Conference on Human Language Technologies
(HLT/NAACL 2007), pages 452?459.
N. Xue, F. Xia, F.D. Chiou, and M. Palmer. 2005. The
penn chinese treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proceedings
of the first international conference on Human lan-
guage technology research, pages 1?8. Association
for Computational Linguistics.
317
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 393?398,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Tightly-coupled Unsupervised Clustering and
Bilingual Alignment Model for Transliteration
Tingting Li1, Tiejun Zhao1, Andrew Finch2, Chunyue Zhang1
1Harbin Institute of Technology, Harbin, China
2NICT, Japan
1{ttli, tjzhao, cyzhang}@mtlab.hit.edu.cn
2andrew.finch@nict.go.jp
Abstract
Machine Transliteration is an essential
task for many NLP applications. Howev-
er, names and loan words typically orig-
inate from various languages, obey dif-
ferent transliteration rules, and therefore
may benefit from being modeled inde-
pendently. Recently, transliteration mod-
els based on Bayesian learning have over-
come issues with over-fitting allowing for
many-to-many alignment in the training of
transliteration models. We propose a nov-
el coupled Dirichlet process mixture mod-
el (cDPMM) that simultaneously clusters
and bilingually aligns transliteration data
within a single unified model. The un-
ified model decomposes into two class-
es of non-parametric Bayesian component
models: a Dirichlet process mixture mod-
el for clustering, and a set of multino-
mial Dirichlet process models that perf-
orm bilingual alignment independently for
each cluster. The experimental results
show that our method considerably outper-
forms conventional alignment models.
1 Introduction
Machine transliteration methods can be catego-
rized into phonetic-based models (Knight et al,
1998), spelling-based models (Brill et al, 2000),
and hybrid models which utilize both phonetic
and spelling information (Oh et al, 2005; Oh et
al., 2006). Among them, statistical spelling-based
models which directly align characters in the train-
ing corpus have become popular because they
are language-independent, do not require phonet-
ic knowledge, and are capable of achieving state-
of-the-art performance (Zhang et al, 2012b). A
major problem with real-word transliteration cor-
pora is that they are usually not clean, may con-
tain name pairs with various linguistic origins and
this can hinder the performance of spelling-based
models because names from different origins obey
different pronunciation rules, for example:
?Kim Jong-il/???? (Korea),
?Kana Gaski/??? (Japan),
?Haw King/??? (England),
?Jin yong/??? (China).
The same Chinese character ??? should be
aligned to different romanized character se-
quences: ?Kim?, ?Kana?, ?King?, ?Jin?. To ad-
dress this issue, many name classification metho-
ds have been proposed, such as the supervised lan-
guage model-based approach of (Li et al, 2007),
and the unsupervised approach of (Huang et al,
2005) that used a bottom-up clustering algorithm.
(Li et al, 2007) proposed a supervised translitera-
tion model which classifies names based on their
origins and genders using a language model; it
switches between transliteration models based on
the input. (Hagiwara et al, 2011) tackled the is-
sue by using an unsupervised method based on the
EM algorithm to perform a soft classification.
Recently, non-parametric Bayesian
models (Finch et al, 2010; Huang et al,
2011; Hagiwara et al, 2012) have attracted
much attention in the transliteration field. In
comparison to many of the previous alignment
models (Li et al, 2004; Jiampojamarn et al,
2007; Berg-Kirkpatrick et al, 2011), the non-
parametric Bayesian models allow unconstrained
monotonic many-to-many alignment and are able
to overcome the inherent over-fitting problem.
Until now most of the previous work (Li et al,
2007; Hagiwara et al, 2011) is either affected by
the multi-origins factor, or has issues with over-
fitting. (Hagiwara et al, 2012) took these two fac-
tors into consideration, but their approach still op-
erates within an EM framework and model order
selection by hand is necessary prior to training.
393
We propose a simple, elegant, fully-
unsupervised solution based on a single generative
model able to both cluster and align simultaneous-
ly. The coupled Dirichlet Process Mixture Model
(cDPMM) integrates a Dirichlet process mixture
model (DPMM) (Antoniak, 1974) and a Bayesian
Bilingual Alignment Model (BBAM) (Finch et
al., 2010). The two component models work
synergistically to support one another: the clus-
tering model sorts the data into classes so that
self-consistent alignment models can be built
using data of the same type, and at the same time
the alignment probabilities from the alignment
models drive the clustering process.
In summary, the key advantages of our model
are as follows:
? it is based on a single, unified generative
model;
? it is fully unsupervised;
? it is an infinite mixture model, and does not
require model order selection ? it is effec-
tively capable of discovering an appropriate
number of clusters from the data;
? it is able to handle data from multiple origins;
? it can perform many-to-many alignment
without over-fitting.
2 Model Description
In this section we describe the methodology and
realization of the proposed cDPMM in detail.
2.1 Terminology
In this paper, we concentrate on the alignment
process for transliteration. The proposed cDP-
MM segments a bilingual corpus of transliteration
pairs into bilingual character sequence-pairs. We
will call these sequence-pairs Transliteration U-
nits (TUs). We denote the source and target of
a TU as sm1 = ?s1, ..., sm? and tn1 = ?t1, ..., tn?
respectively, where si (ti) is a single character in
source (target) language. We use the same no-
tation (s, t) = (?s1, ..., sm?, ?t1, ..., tn?) to de-
note a transliteration pair, which we can write as
x = (sm1 , tn1 ) for simplicity. Finally, we express
the training set itself as a set of sequence pairs:
D = {xi}Ii=1. Our aim is to obtain a bilingual
alignment ?(s1, t1), ..., (sl, tl)? for each transliter-
ation pair xi, where each (sj , tj) is a segment of
the whole pair (a TU) and l is the number of seg-
ments used to segment xi.
2.2 Methodology
Our cDPMM integrates two Dirichlet process
models: the DPMM clustering model, and the
BBAM alignment model which is a multinomial
Dirichlet process.
A Dirichlet process mixture model, models the
data as a mixture of distributions ? one for each
cluster. It is an infinite mixture model, and the
number of components is not fixed prior to train-
ing. Equation 1 expresses the DPMM hierarchi-
cally.
Gc|?c, G0c ? DP (?c, G0c)
?k|Gc ? Gc
xi|?k ? f(xi|?k) (1)
where G0c is the base measure and ?c > 0 is the
concentration parameter for the distribution Gc.
xi is a name pair in training data, and ?k repre-
sents the parameters of a candidate cluster k for
xi. Specifically ?k contains the probabilities of all
the TUs in cluster k. f(xi|?k) (defined in Equa-
tion 7) is the probability that mixture component
k parameterized by ?k will generate xi.
The alignment component of our cDPMM is
a multinomial Dirichlet process and is defined as
follows:
Ga|?a, G0a ? DP (?a, G0a)
(sj , tj)|Ga ? Ga (2)
The subscripts ?c? and ?a? in Equations 1 and 2
indicate whether the terms belong to the clustering
or alignment model respectively.
The generative story for the cDPMM is sim-
ple: first generate an infinite number of clusters,
choose one, then generate a transliteration pair us-
ing the parameters that describe the cluster. The
basic sampling unit of the cDPMM for the cluster-
ing process is a transliteration pair, but the basic
sampling unit for BBAM is a TU. In order to inte-
grate the two processes in a single model we treat
a transliteration pair as a sequence of TUs gener-
ated by a BBAM model. The BBAM generates a
sequence (a transliteration pair) based on the joint
source-channel model (Li et al, 2004). We use a
blocked version of a Gibbs sampler to train each
BBAM (see (Mochihashi et al, 2009) for details
of this process).
2.3 The Alignment Model
This model is a multinomial DP model. Under the
Chinese restaurant process (CRP) (Aldous, 1985)
394
interpretation, each unique TU corresponds to a
dish served at a table, and the number of customers
in each table represents the count of a particular
TU in the model.
The probability of generating the jth TU (sj , tj)
is,
P
(
(sj , tj)|(s?j , t?j)
)
=
N
(
(sj , tj)
)
+ ?aG0a
(
(sj , tj)
)
N + ?a (3)
where N is the total number of TUs generated
so far, and N
(
(sj , tj)
)
is the count of (sj , tj).
(s?j , t?j) are all the TUs generated so far except
(sj , tj). The base measure G0a is a joint spelling
model:
G0a
(
(s, t)
)
= P (|s|)P (s||s|)P (|t|)P (t||t|)
= ?
|s|
s
|s|! e
??sv?|s|s ?
?|t|t
|t|! e
??tv?|t|t
(4)
where |s| (|t|) is the length of the source (target)
sequence, vs (vt) is the vocabulary (alphabet) size
of the source (target) language, and ?s (?t) is the
expected length of source (target) side.
2.4 The Clustering Model
This model is a DPMM. Under the CRP interpre-
tation, a transliteration pair corresponds to a cus-
tomer, the dish served on each table corresponds
to an origin of names.
We use z = (z1, ..., zI), zi ? {1, ...,K} to in-
dicate the cluster of each transliteration pair xi in
the training set and ? = (?1, ..., ?K) to represent
the parameters of the component associated with
each cluster.
In our model, each mixture component is a
multinomial DP model, and since ?k contains the
probabilities of all the TUs in cluster k, the num-
ber of parameters in each ?k is uncertain and
changes with the transliteration pairs that belong
to the cluster. For a new cluster (the K + 1th clus-
ter), we use Equation 4 to calculate the probability
of each TU. The cluster membership probability
of a transliteration pair xi is calculated as follows,
P (zi = k|D, ?, z?i) ? nkn? 1 + ?c
P (xi|z, ?k) (5)
P (zi = K + 1|D, ?, z?i) ? ?cn? 1 + ?c
P (xi|z, ?K+1)
(6)
where nk is the number of transliteration pairs in
the existing cluster k ? {1, ...,K} (cluster K + 1
is a newly created cluster), zi is the cluster indi-
cator for xi, and z?i is the sequence of observed
clusters up to xi. As mentioned earlier, basic sam-
pling units are inconsistent for the clustering and
alignment model, therefore to couple the models
the BBAM generates transliteration pairs as a se-
quence of TUs, these pairs are then used directly
in the DPMM.
Let ? = ?(s1, t1), ..., (sl, tl)? be a derivation of
a transliteration pair xi. To make the model inte-
gration process explicit, we use function f to cal-
culate the probability P (xi|z, ?k), where f is de-
fined as follows,
f(xi|?k) =
{ ?
??R
?
(s,t)?? P (s, t|?k) k ? {1, ...,K}?
??R
?
(s,t)?? G0c(s, t) k = K + 1
(7)
where R denotes the set of all derivations of xi,
G0c is the same as Equation 4.
The cluster membership zi is sampled together
with the derivation ? in a single step according to
P (zi = k|D, ?, z?i) and f(xi|?k). Following the
method of (Mochihashi et al, 2009), first f(xi|?k)
is calculated by forward filtering, and then a sam-
ple ? is taken by backward sampling.
3 Experiments
3.1 Corpora
To empirically validate our approach, we investi-
gate the effectiveness of our model by conduct-
ing English-Chinese name transliteration genera-
tion on three corpora containing name pairs of
varying degrees of mixed origin. The first two cor-
pora were drawn from the ?Names of The World?s
Peoples? dictionary published by Xin Hua Pub-
lishing House. The first corpus was construct-
ed with names only originating from English lan-
guage (EO), and the second with names originat-
ing from English, Chinese, Japanese evenly (ECJ-
O). The third corpus was created by extracting
name pairs from LDC (Linguistic Data Consor-
tium) Named Entity List, which contains names
from all over the world (Multi-O). We divided the
datasets into training, development and test sets
for each corpus with a ratio of 10:1:1. The details
of the division are displayed in Table 2.
395
cDPMM Alignment BBAM Alignment
mun|? din|? ger|?(0, English) mun|? din|? ger|?
ding|? guo|?(2, Chinese) din|? g| guo|?
tei|? be|?(3, Japanese) t| |? e| ibe|?
fan|? chun|? yi|?(2, Chinese) fan|? chun|? y| i|?
hong|? il|? sik|?(5, Korea) hong|? i|? l| si|? k|
sei|? ichi|? ro|?(4, Japanese) seii|? ch| i|? ro|?
dom|? b|? ro|? w|? s|? ki|?(0, Russian) do|? mb|? ro|? w|? s|? ki|?
he|? dong|? chang|?(2, Chinese) he|? don|? gchang|?
b|? ran|? don|?(0, English) b|? ran|? don|?
Table 1: Typical alignments from the BBAM and cDPMM.
3.2 Baselines
We compare our alignment model with
GIZA++ (Och et al, 2003) and the Bayesian
bilingual alignment model (BBAM). We employ
two decoding models: a phrase-based machine
translation decoder (specifically Moses (Koehn
et al, 2007)), and the DirecTL decoder (Jiampo-
jamarn et al, 2009). They are based on different
decoding strategies and optimization targets, and
therefore make the comparison more compre-
hensive. For the Moses decoder, we applied the
grow-diag-final-and heuristic algorithm to extract
the phrase table, and tuned the parameters using
the BLEU metric.
Corpora Corpus ScaleTraining Development Testing
EO 32,681 3,267 3,267
ECJ-O 32,500 3,250 3,250
Multi-O 33,291 3,328 3,328
Table 2: Statistics of the experimental corpora.
To evaluate the experimental results, we uti-
lized 3 metrics from the Named Entities Workshop
(NEWS) (Zhang et al, 2012a): word accuracy in
top-1 (ACC), fuzziness in top-1 (Mean F-score)
and mean reciprocal rank (MRR).
3.3 Parameter Setting
In our model, there are several important parame-
ters: 1) max s, the maximum length of the source
sequences of the alignment tokens; 2) max t, the
maximum length of the target sequences of the
alignment tokens; and 3) nc, the initial number of
classes for the training data. We set max s = 6,
max t = 1 and nc = 5 empirically based on a
small pilot experiment. The Moses decoder was
used with default settings except for the distortion-
limit which was set to 0 to ensure monotonic de-
coding. For the DirecTL decoder the following
settings were used: cs = 4, ng = 9 and nBest =
5. cs denotes the size of context window for fea-
tures, ng indicates the size of n-gram features and
nBest is the size of transliteration candidate list
for updating the model in each iteration. The con-
centration parameter ?c, ?a of the clustering mod-
el and the BBAM was learned by sampling its val-
ue. Following (Blunsom et al, 2009) we used
a vague gamma prior ?(10?4, 104), and sampled
new values from a log-normal distribution whose
mean was the value of the parameter, and variance
was 0.3. We used the Metropolis-Hastings algo-
rithm to determine whether this new sample would
be accepted. The parameters ?s and ?t in Equa-
tion 4 were set to ?s = 4 and ?t = 1.
Model EO ECJ-O Multi-O
#(Clusters) cDPMM 5.8 9.5 14.3
#(Targets)
GIZA++ 14.43 5.35 6.62
BBAM 6.06 2.45 2.91
cDPMM 9.32 3.45 4.28
Table 3: Alignment statistics.
3.4 Experimental Results
Table 3 shows some details of the alignment re-
sults. The #(Clusters) represents the average num-
ber of clusters from the cDPMM. It is averaged
over the final 50 iterations, and the classes which
contain less than 10 name pairs are excluded. The
#(Targets) represents the average number of En-
glish character sequences that are aligned to each
Chinese sequence. From the results we can see
that in terms of the number of alignment targe-
ts: GIZA++ > cDPMM > BBAM. GIZA++ has
considerably more targets than the other approach-
es, and this is likely to be a symptom of it over-
fitting the data. cDPMM can alleviate the over-
fitting through its BBAM component, and at the
same time effectively model the diversity in Chi-
nese character sequences caused by multi-origin.
Table 1 shows some typical TUs from the align-
ments produced by BBAM and cDPMM on cor-
pus Multi-O. The information in brackets in Ta-
ble 1, represents the ID of the class and origin of
396
Corpora Model EvaluationACC M-Fscore MRR
EO
GIZA 0.7241 0.8881 0.8061
BBAM 0.7286 0.8920 0.8043
cDPMM 0.7398 0.8983 0.8126
ECJ-O
GIZA 0.5471 0.7278 0.6268
BBAM 0.5522 0.7370 0.6344
cDPMM 0.5643 0.7420 0.6446
Multi-O
GIZA 0.4993 0.7587 0.5986
BBAM 0.5163 0.7769 0.6123
cDPMM 0.5237 0.7796 0.6188
Table 4: Comparison of different methods using
the Moses phrase-based decoder.
the name pair; the symbol ? ? indicates a ?NUL-
L? alignment. We can see the Chinese characters
??(ding) ?(yi) ?(dong)? have different align-
ments in different origins, and that the cDPMM
has provided the correct alignments for them.
We used the sampled alignment from running
the BBAM and cDPMMmodels for 100 iterations,
and combined the alignment tables of each class
together. The experiments are therefore investigat-
ing whether the alignment has been meaningfully
improved by the clustering process. We would ex-
pect further gains from exploiting the class infor-
mation in the decoding process (as in (Li et al,
2007)), but this remains future research. The top-
10 transliteration candidates were used for testing.
The detailed experimental results are shown in Ta-
bles 4 and 5.
Our proposed model obtained the highest per-
formance on all three datasets for all evaluation
metrics by a considerable margin. Surprisingly,
for dataset EO although there is no multi-origin
factor, we still observed a respectable improve-
ment in every metric. This shows that although
names may have monolingual origin, there are hid-
den factors which can allow our model to succeed,
possibly related to gender or convention. Other
models based on supervised classification or clus-
tering with fixed classes may fail to capture these
characteristics.
To guarantee the reliability of the compara-
tive results, we performed significance testing
based on paired bootstrap resampling (Efron et al,
1993). We found all differences to be significant
(p < 0.05).
4 Conclusion
In this paper we propose an elegant unsupervised
technique for monotonic sequence alignment
based on a single generative model. The key ben-
Corpora Model EvaluationACC M-Fscore MRR
EO
GIZA 0.6950 0.8812 0.7632
BBAM 0.7152 0.8899 0.7839
cDPMM 0.7231 0.8933 0.7941
ECJ-O
GIZA 0.3325 0.6208 0.4064
BBAM 0.3427 0.6259 0.4192
cDPMM 0.3521 0.6302 0.4316
Multi-O
GIZA 0.3815 0.7053 0.4592
BBAM 0.3934 0.7146 0.4799
cDPMM 0.3970 0.7179 0.4833
Table 5: Comparison of different methods using
the DirecTL decoder.
efits of our model are that it can handle data from
multiple origins, and model using many-to-many
alignment without over-fitting. The model oper-
ates by clustering the data into classes while si-
multaneously aligning it, and is able to discover
an appropriate number of classes from the data.
Our results show that our alignment model can im-
prove the performance of a transliteration gener-
ation system relative to two other state-of-the-art
aligners. Furthermore, the system produced gains
even on data of monolingual origin, where no ob-
vious clusters in the data were expected.
Acknowledgments
We thank the anonymous reviewers for their valu-
able comments and helpful suggestions.We also
thank Chonghui Zhu, Mo Yu, and Wenwen Zhang
for insightful discussions. This work was support-
ed by National Natural Science Foundation of Chi-
na (61173073), and the Key Project of the Nation-
al High Technology Research and Development
Program of China (2011AA01A207).
References
D.J. Aldous. 1985. Exchangeability and Related Top-
ics. E?cole d?E?te? St Flour 1983. Springer, 1985,
1117:1?198.
C.E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. Annals of Statistics. 2:1152, 174.
Taylor Berg-Kirkpatrick and Dan Klein. 2011. Simple
effective decipherment via combinatorial optimiza-
tion. In Proc. of EMNLP, pages 313?321.
P. Blunsom, T. Cohn, C. Dyer, and Osborne, M. 2009.
A Gibbs sampler for phrasal synchronous grammar
induction. In Proc. of ACL, pages 782?790.
Eric Brill and Robert C. Moore. 2000. An Improved
Error Model for Noisy Channel Spelling Correction.
In Proc. of ACL, pages 286?293.
397
B. Efron and R. J. Tibshirani 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York, NY.
Andrew Finch and Eiichiro Sumita. 2010. A Bayesian
Model of Bilingual Segmentation for Translitera-
tion. In Proc. of the 7th International Workshop on
Spoken Language Translation, pages 259?266.
Masato Hagiwara and Satoshi Sekine. 2011. Latent
Class Transliteration based on Source Language O-
rigin. In Proc. of ACL (Short Papers), pages 53-57.
Masato Hagiwara and Satoshi Sekine. 2012. Latent
semantic transliteration using dirichlet mixture. In
Proc. of the 4th Named Entity Workshop, pages 30?
37.
Fei Huang, Stephan Vogel, and Alex Waibel. 2005.
Clustering and Classifying Person Names by Origin.
In Proc. of AAAI, pages 1056?1061.
Yun Huang, Min Zhang and Chew Lim Tan. 2011.
Nonparametric Bayesian Machine Transliteration
with Synchronous Adaptor Grammars. In Proc. of
ACL, pages 534?539.
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek
Sherif. 2007. Applying Many-to-Many Alignments
and Hidden Markov Models to Letter-to-Phoneme
Conversion. In Proc. of NAACL, pages 372?379.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer and Grzegorz Kondrak. 2009.
DirecTL: a Language Independent Approach to
Transliteration. In Proc. of the 2009 Named Entities
Workshop: Shared Task on Transliteration (NEWS
2009), pages 1056?1061.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Journal of Computational Linguis-
tics, pages 28?31.
Philipp Koehn and Hieu Hoang and Alexandra Birch
and Chris Callison-Burch and Marcello Federico
and Nicola Bertoldi and Brooke Cowan and Wade
Shen and Christine Moran and Richard Zens and
Chris Dyer and Ondrej Bojar and Alexandra Con-
stantin and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL.
Haizou Li, Min Zhang, and Jian Su 2004. A join-
t source-channel model for machine transliteration.
In ACL ?04: Proceedings of the 42nd Annual Meet-
ing on Association for Computational Linguistics.
Association for Computational Linguistics, Morris-
town, NJ, USA, 159.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic Transliteration of Personal
Names. In Proc. of ACL, pages 120?127.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ue-
da. 2009. Bayesian Unsupervised Word Segmen-
tation with Nested Pitman-Yor Language Modeling.
In Proc. of ACL/IJCNLP, pages 100?108.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Journal of Comput. Linguist., 29(1):19-51.
Jong-Hoon Oh, and Key-Sun Choi. 2005. Machine
Learning Based English-to-Korean Transliteration
Using Grapheme and Phoneme Information. Jour-
nal of IEICE Transactions, 88-D(7):1737-1748.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A machine transliteration model based on
correspondence between graphemes and phonemes.
Journal of ACM Trans. Asian Lang. Inf. Process.,
5(3):185-208.
Min Zhang, Haizhou Li, Ming Liu and A Kumaran.
2012a. Whitepaper of NEWS 2012 shared task on
machine transliteration. In Proc. of the 4th Named
Entity Workshop (NEWS 2012), pages 1?9.
Min Zhang, Haizhou Li, A Kumaran and Ming Liu.
2012b. Report of NEWS 2012 Machine Translitera-
tion Shared Task. In Proc. of the 4th Named Entity
Workshop (NEWS 2012), pages 10?20.
398
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967?976,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Knowledge-Based Question Answering as Machine Translation
Junwei Bao
? ?
, Nan Duan
?
, Ming Zhou
?
, Tiejun Zhao
?
?
Harbin Institute of Technology
?
Microsoft Research
baojunwei001@gmail.com
{nanduan, mingzhou}@microsoft.com
tjzhao@hit.edu.cn
Abstract
A typical knowledge-based question an-
swering (KB-QA) system faces two chal-
lenges: one is to transform natural lan-
guage questions into their meaning repre-
sentations (MRs); the other is to retrieve
answers from knowledge bases (KBs) us-
ing generated MRs. Unlike previous meth-
ods which treat them in a cascaded man-
ner, we present a translation-based ap-
proach to solve these two tasks in one u-
nified framework. We translate questions
to answers based on CYK parsing. An-
swers as translations of the span covered
by each CYK cell are obtained by a ques-
tion translation method, which first gener-
ates formal triple queries as MRs for the
span based on question patterns and re-
lation expressions, and then retrieves an-
swers from a given KB based on triple
queries generated. A linear model is de-
fined over derivations, and minimum er-
ror rate training is used to tune feature
weights based on a set of question-answer
pairs. Compared to a KB-QA system us-
ing a state-of-the-art semantic parser, our
method achieves better results.
1 Introduction
Knowledge-based question answering (KB-QA)
computes answers to natural language (NL) ques-
tions based on existing knowledge bases (KBs).
Most previous systems tackle this task in a cas-
caded manner: First, the input question is trans-
formed into its meaning representation (MR) by
an independent semantic parser (Zettlemoyer and
Collins, 2005; Mooney, 2007; Artzi and Zettle-
moyer, 2011; Liang et al, 2011; Cai and Yates,
?
This work was finished while the author was visiting Mi-
crosoft Research Asia.
2013; Poon, 2013; Artzi et al, 2013; Kwiatkowski
et al, 2013; Berant et al, 2013); Then, the answer-
s are retrieved from existing KBs using generated
MRs as queries.
Unlike existing KB-QA systems which treat se-
mantic parsing and answer retrieval as two cas-
caded tasks, this paper presents a unified frame-
work that can integrate semantic parsing into the
question answering procedure directly. Borrow-
ing ideas from machine translation (MT), we treat
the QA task as a translation procedure. Like MT,
CYK parsing is used to parse each input question,
and answers of the span covered by each CYK cel-
l are considered the translations of that cell; un-
like MT, which uses offline-generated translation
tables to translate source phrases into target trans-
lations, a semantic parsing-based question trans-
lation method is used to translate each span into
its answers on-the-fly, based on question patterns
and relation expressions. The final answers can be
obtained from the root cell. Derivations generated
during such a translation procedure are modeled
by a linear model, and minimum error rate train-
ing (MERT) (Och, 2003) is used to tune feature
weights based on a set of question-answer pairs.
Figure 1 shows an example: the question direc-
tor of movie starred by Tom Hanks is translated to
one of its answers Robert Zemeckis by three main
steps: (i) translate director of to director of ; (ii)
translate movie starred by Tom Hanks to one of it-
s answers Forrest Gump; (iii) translate director of
Forrest Gump to a final answer Robert Zemeckis.
Note that the updated question covered by Cell[0,
6] is obtained by combining the answers to ques-
tion spans covered by Cell[0, 1] and Cell[2, 6].
The contributions of this work are two-fold: (1)
We propose a translation-based KB-QA method
that integrates semantic parsing and QA in one
unified framework. The benefit of our method
is that we don?t need to explicitly generate com-
plete semantic structures for input questions. Be-
967
Cell[0, 6] 
Cell[2, 6] 
Cell[0, 1] 
director of movie starred by Tom Hanks 
(ii) movie starred by Tom Hanks ? Forrest Gump 
(iii) director of Forrest Gump ? Robert Zemeckis 
(i) director of ? director of 
Figure 1: Translation-based KB-QA example
sides which, answers generated during the transla-
tion procedure help significantly with search space
pruning. (2) We propose a robust method to trans-
form single-relation questions into formal triple
queries as their MRs, which trades off between
transformation accuracy and recall using question
patterns and relation expressions respectively.
2 Translation-Based KB-QA
2.1 Overview
Formally, given a knowledge base KB and an N-
L question Q, our KB-QA method generates a set
of formal triples-answer pairs {?D,A?} as deriva-
tions, which are scored and ranked by the distribu-
tion P (?D,A?|KB,Q) defined as follows:
exp{
?
M
i=1
?
i
? h
i
(?D,A?,KB,Q)}
?
?D
?
,A
?
??H(Q)
exp{
?
M
i=1
?
i
? h
i
(?D
?
,A
?
?,KB,Q)}
? KB denotes a knowledge base
1
that stores a
set of assertions. Each assertion t ? KB is in
the form of {e
ID
sbj
, p, e
ID
obj
}, where p denotes
a predicate, e
ID
sbj
and e
ID
obj
denote the subject
and object entities of t, with unique IDs
2
.
? H(Q) denotes the search space {?D,A?}. D
is composed of a set of ordered formal triples
{t
1
, ..., t
n
}. Each triple t = {e
sbj
, p, e
obj
}
j
i
?
D denotes an assertion in KB, where i and
j denotes the beginning and end indexes of
the question span from which t is trans-
formed. The order of triples in D denotes
the order of translation steps from Q to A.
E.g., ?director of, Null, director of ?
1
0
, ?Tom
1
We use a large scale knowledge base in this paper, which
contains 2.3B entities, 5.5K predicates, and 18B assertions. A
16-machine cluster is used to host and serve the whole data.
2
Each KB entity has a unique ID. For the sake of conve-
nience, we omit the ID information in the rest of the paper.
Hanks, Film.Actor.Film, Forrest Gump?
6
2
and
?Forrest Gump, Film.Film.Director, Robert
Zemeckis?
6
0
are three ordered formal triples
corresponding to the three translation steps in
Figure 1. We define the task of transforming
question spans into formal triples as question
translation. A denotes one final answer ofQ.
? h
i
(?) denotes the i
th
feature function.
? ?
i
denotes the feature weight of h
i
(?).
According to the above description, our KB-
QA method can be decomposed into four tasks as:
(1) search space generation for H(Q); (2) ques-
tion translation for transforming question spans in-
to their corresponding formal triples; (3) feature
design for h
i
(?); and (4) feature weight tuning for
{?
i
}. We present details of these four tasks in the
following subsections one-by-one.
2.2 Search Space Generation
We first present our translation-based KB-QA
method in Algorithm 1, which is used to generate
H(Q) for each input NL question Q.
Algorithm 1: Translation-based KB-QA
1 for l = 1 to |Q| do
2 for all i, j s.t. j ? i = l do
3 H(Q
j
i
) = ?;
4 T = QTrans(Q
j
i
,KB);
5 foreach formal triple t ? T do
6 create a new derivation d;
7 d.A = t.e
obj
;
8 d.D = {t};
9 update the model score of d;
10 insert d toH(Q
j
i
);
11 end
12 end
13 end
14 for l = 1 to |Q| do
15 for all i, j s.t. j ? i = l do
16 for all m s.t. i ? m < j do
17 for d
l
? H(Q
m
i
) and d
r
? H(Q
j
m+1
) do
18 Q
update
= d
l
.A+ d
r
.A;
19 T = QTrans(Q
update
,KB);
20 foreach formal triple t ? T do
21 create a new derivation d;
22 d.A = t.e
obj
;
23 d.D = d
l
.D
?
d
r
.D
?
{t};
24 update the model score of d;
25 insert d toH(Q
j
i
);
26 end
27 end
28 end
29 end
30 end
31 returnH(Q).
968
The first half (from Line 1 to Line 13) gen-
erates a formal triple set T for each unary span
Q
j
i
? Q, using the question translation method
QTrans(Q
j
i
,KB) (Line 4), which takesQ
j
i
as the
input. Each triple t ? T returned is in the form of
{e
sbj
, p, e
obj
}, where e
sbj
?s mention occurs inQ
j
i
,
p is a predicate that denotes the meaning expressed
by the context of e
sbj
in Q
j
i
, e
obj
is an answer of
Q
j
i
based on e
sbj
, p and KB. We describe the im-
plementation detail of QTrans(?) in Section 2.3.
The second half (from Line 14 to Line 31) first
updates the content of each bigger spanQ
j
i
by con-
catenating the answers to its any two consecutive
smaller spans covered by Q
j
i
(Line 18). Then,
QTrans(Q
j
i
,KB) is called to generate triples for
the updated span (Line 19). The above operations
are equivalent to answering a simplified question,
which is obtained by replacing the answerable
spans in the original question with their corre-
sponding answers. The search spaceH(Q) for the
entire question Q is returned at last (Line 31).
2.3 Question Translation
The purpose of question translation is to translate
a span Q to a set of formal triples T . Each triple
t ? T is in the form of {e
sbj
, p, e
obj
}, where e
sbj
?s
mention
3
occurs inQ, p is a predicate that denotes
the meaning expressed by the context of e
sbj
in
Q, e
obj
is an answer to Q retrieved from KB us-
ing a triple query q = {e
sbj
, p, ?}. Note that if
no predicate p or answer e
obj
can be generated,
{Q, Null,Q} will be returned as a special triple,
which sets e
obj
to be Q itself, and p to be Null.
This makes sure the un-answerable spans can be
passed on to the higher-level operations.
Question translation assumes each span Q is a
single-relation question (Fader et al, 2013). Such
assumption simplifies the efforts of semantic pars-
ing to the minimum question units, while leaving
the capability of handling multiple-relation ques-
tions (Figure 1 gives one such example) to the out-
er CYK-parsing based translation procedure. Two
question translation methods are presented in the
rest of this subsection, which are based on ques-
tion patterns and relation expressions respectively.
2.3.1 Question Pattern-based Translation
A question pattern QP includes a pattern string
QP
pattern
, which is composed of words and a slot
3
For simplicity, a cleaned entity dictionary dumped from
the entire KB is used to detect entity mentions inQ.
Algorithm 2:QP-based Question Translation
1 T = ?;
2 foreach entity mention e
Q
? Q do
3 Q
pattern
= replace e
Q
inQ with [Slot];
4 foreach question patternQP do
5 ifQ
pattern
==QP
pattern
then
6 E = Disambiguate(e
Q
,QP
predicate
);
7 foreach e ? E do
8 create a new triple query q;
9 q = {e,QP
predicate
, ?};
10 {A
i
} = AnswerRetrieve(q,KB);
11 foreach A ? {A
i
} do
12 create a new formal triple t;
13 t = {q.e
sbj
, q.p,A};
14 t.score = 1.0;
15 insert t to T ;
16 end
17 end
18 end
19 end
20 end
21 return T .
symbol [Slot], and a KB predicate QP
predicate
,
which denotes the meaning expressed by the con-
text words in QP
pattern
.
Algorithm 2 shows how to generate formal
triples for a span Q based on question pattern-
s (QP-based question translation). For each en-
tity mention e
Q
? Q, we replace it with [Slot]
and obtain a pattern string Q
pattern
(Line 3). If
Q
pattern
can match one QP
pattern
, then we con-
struct a triple query q (Line 9) using QP
predicate
as its predicate and one of the KB entities re-
turned by Disambiguate(e
Q
,QP
predicate
) as it-
s subject entity (Line 6). Here, the objective of
Disambiguate(e
Q
,QP
predicate
) is to output a set
of disambiguated KB entities E in KB. The name
of each entity returned equals the input entity
mention e
Q
and occurs in some assertions where
QP
predicate
are the predicates. The underlying
idea is to use the context (predicate) information to
help entity disambiguation. The answers of q are
returned by AnswerRetrieve(q,KB) based on q
and KB (Line 10), each of which is used to con-
struct a formal triple and added to T for Q (from
Line 11 to Line 16). Figure 2 gives an example.
Question patterns are collected as follows: First,
5W queries, which begin with What, Where, Who,
When, or Which, are selected from a large scale
query log of a commercial search engine; Then, a
cleaned entity dictionary is used to annotate each
query by replacing all entity mentions it contains
with the symbol [Slot]. Only high-frequent query
patterns which contain one [Slot] are maintained;
969
?                    : who is the director of Forrest Gump 
?????????    : who is the director of [Slot] 
???????????: Film.Film.Director 
?                    : <Forrest Gump, Film.Film.Director, ?> 
?                     : <Forrest Gump, Film.Film.Director, Robert Zemeckis> 
KB 
Figure 2: QP-based question translation example
Lastly, annotators try to manually label the most-
frequent 50,000 query patterns with their corre-
sponding predicates, and 4,764 question patterns
with single labeled predicates are obtained.
From experiments (Table 3 in Section 4.3) we
can see that, question pattern based question trans-
lation can achieve high end-to-end accuracy. But
as human efforts are needed in the mining proce-
dure, this method cannot be extended to large scale
very easily. Besides, different users often type the
questions with the same meaning in different NL
expressions. For example, although the question
Forrest Gump was directed by which moviemaker
means the same as the question Q in Figure 2, no
question pattern can cover it. We need to find an
alternative way to alleviate such coverage issue.
2.3.2 Relation Expression-based Translation
Aiming to alleviate the coverage issue occurring in
QP-based method, an alternative relation expres-
sion (RE) -based method is proposed, and will be
used when the QP-based method fails.
We define RE
p
as a relation expression set for
a given KB predicate p ? KB. Each relation ex-
pressionRE ? RE
p
includes an expression string
RE
expression
, which must contain at least one con-
tent word, and a weight RE
weight
, which denotes
the confidence thatRE
expression
can represent p?s
meaning in NL. For example, is the director of
is one relation expression string for the predicate
Film.Film.Director, which means it is usually used
to express this relation (predicate) in NL.
Algorithm 3 shows how to generate triples for
a question Q based on relation expressions. For
each possible entity mention e
Q
? Q and a K-
B predicate p ? KB that is related to a KB enti-
ty e whose name equals e
Q
, Sim(e
Q
,Q,RE
p
) is
computed (Line 5) based on the similarity between
question context and RE
p
, which measures how
likely Q can be transformed into a triple query
Algorithm 3:RE-based Question Translation
1 T = ?;
2 foreach entity mention e
Q
? Q do
3 foreach e ? KB s.t. e.name==e
Q
do
4 foreach predicate p ? KB related to e do
5 score = Sim(e
Q
,Q,RE
p
);
6 if score > 0 then
7 create a new triple query q;
8 q = {e, p, ?};
9 {A
i
} = AnswerRetrieve(q,KB);
10 foreach A ? {A
i
} do
11 create a new formal triple t;
12 t = {q.e
sbj
, q.p,A};
13 t.score = score;
14 insert t to T ;
15 end
16 end
17 end
18 end
19 end
20 sort T based on the score of each t ? T ;
21 return T .
q = {e, p, ?}. If this score is larger than 0, which
means there are overlaps betweenQ?s context and
RE
p
, then q will be used as the triple query of Q,
and a set of formal triples will be generated based
on q andKB (from Line 7 to Line 15). The compu-
tation of Sim(e
Q
,Q,RE
p
) is defined as follows:
?
n
1
|Q| ? n+ 1
? {
?
?
n
?Q,?
n
?
e
Q
=?
P (?
n
|RE
p
)}
where n is the n-gram order which ranges from 1
to 5, ?
n
is an n-gram occurring inQ without over-
lapping with e
Q
and containing at least one con-
tent word, P (?
n
|RE
p
) is the posterior probability
which is computed by:
P (?
n
|RE
p
) =
Count(?
n
,RE
p
)
?
?
?
n
?RE
p
Count(?
?
n
,RE
p
)
Count(?,RE
p
) denotes the weighted sum of
times that ? occurs inRE
p
:
Count(?,RE
p
) =
?
RE?RE
p
{#
?
(RE) ? RE
weight
}
where #
?
(RE) denotes the number of times that
? occurs inRE
expression
, andRE
weight
is decided
by the relation expression extraction component.
Figure 3 gives an example, where n-grams with
rectangles are the ones that occur in bothQ?s con-
text and the relation expression set of a given pred-
icate p = Film.F ilm.Director. Unlike the QP-
based method which needs a perfect match, the
970
?                                 : Forrest Gump was directed by which moviemaker 
????????????????????: is directed by 
was directed and written by 
is the moviemaker of 
was famous as the director of 
? 
?                                  : <Forrest Gump, Film.Film.Director, ?> 
?                                   : <Forrest Gump, Film.Film.Director, Robert Zemeckis> 
KB 
Figure 3: RE-based question translation example
RE-based method allows fuzzy matching between
Q andRE
p
, and records this (Line 13) in generat-
ed triples, which is used as features later.
Relation expressions are mined as follows: Giv-
en a set of KB assertions with an identical predi-
cate p, we first extract all sentences from English
Wiki pages
4
, each of which contains at least one
pair of entities occurring in one assertion. Then,
we extract the shortest path between paired entities
in the dependency tree of each sentence as an RE
candidate for the given predicate. The intuition is
that any sentence containing such entity pairs oc-
cur in an assertion is likely to express the predi-
cate of that assertion in some way. Last, all rela-
tion expressions extracted are filtered by heuristic
rules, i.e., the frequency must be larger than 4, the
length must be shorter than 10, and then weighted
by the pattern scoring methods proposed in (Ger-
ber and Ngomo, 2011; Gerber and Ngomo, 2012).
For each predicate, we only keep the relation ex-
pressions whose pattern scores are larger than a
pre-defined threshold. Figure 4 gives one relation
expression extraction example. The statistics and
overall quality of the relation expressions are list-
ed in Section 4.1.
{ Forrest Gump , Robert Zemeckis }  
{ Titanic, James Cameron }  
{ The Dark Knight Rises , C hristopher  Nolan }  
Paired entity of a 
KB predicate  
??Film.Film.Director 
Passage retrieval  
from Wiki pages  
Relation expression 
weighting  
Robert Zemeckis  is the director of Forrest Gump  
James Cameron  is the moviemaker of Titanic 
The Dark Knight Rises is directed by C hristopher  Nolan  
is the director of           ||| 0.25  
is the moviemaker of   ||| 0.23  
is directed by                 ||| 0.20  
Figure 4: RE extraction example
4
http://en.wikipedia.org/wiki/Wikipedia:Database download
2.3.3 Question Decomposition
Sometimes, a question may provide multiple con-
straints to its answers. movie starred by Tom Han-
ks in 1994 is one such question. All the films as
the answers of this question should satisfy the fol-
lowing two constraints: (1) starred by Tom Hanks;
and (2) released in 1994. It is easy to see that such
questions cannot be translated to single triples.
We propose a dependency tree-based method to
handle such multiple-constraint questions by (i)
decomposing the original question into a set of
sub-questions using syntax-based patterns; and (ii)
intersecting the answers of all sub-questions as the
final answers of the original question. Note, ques-
tion decomposition only operates on the original
question and question spans covered by complete
dependency subtrees. Four syntax-based patterns
(Figure 5) are used for question decomposition. If
a question matches any one of these patterns, then
sub-questions are generated by collecting the path-
s between n
0
and each n
i
(i > 0) in the pattern,
where each n denotes a complete subtree with a
noun, number, or question word as its root node,
the symbol ? above prep
?
denotes this preposition
can be skipped in matching. For the question men-
tioned at the beginning, its two sub-questions gen-
erated are movie starred by Tom Hanks and movie
starred in 1994, as its dependency form matches
pattern (a). Similar ideas are used in IBM Wat-
son (Kalyanpur et al, 2012) as well.
???? 
?? 
????? 
?? 
????? 
?? 
? 
? 
???? 
?? ?? ???? 
(a) 
?? 
???? 
?? 
?? 
(c) 
and  ?? 
????? 
???? 
?? 
?? 
(d) 
????? and  ???? 
?? 
????? 
(b) 
Figure 5: Four syntax-based patterns for question
decomposition
As dependency parsing is not perfect, we gen-
erate single triples for such questions without con-
sidering constraints as well, and add them to the
search space for competition. h
syntax constraint
(?)
971
is used to boost triples that are converted from sub-
questions generated by question decomposition.
The more constraints an answer satisfies, the bet-
ter. Obviously, current patterns used can?t cover
all cases but most-common ones. We leave a more
general pattern mining method for future work.
2.4 Feature Design
The objective of our KB-QA system is to seek the
derivation ?
?
D,
?
A? that maximizes the probability
P (?D,A?|KB,Q) described in Section 2.1 as:
?
?
D,
?
A? = argmax
?D,A??H(Q)
P (?D,A?|KB,Q)
= argmax
?D,A??H(Q)
M
?
i=1
?
i
? h
i
(?D,A?,KB,Q)
We now introduce the feature sets {h
i
(?)} that are
used in the above linear model:
? h
question word
(?), which counts the number of
original question words occurring inA. It pe-
nalizes those partially answered questions.
? h
span
(?), which counts the number of spans
in Q that are converted to formal triples. It
controls the granularity of the spans used in
question translation.
? h
syntax subtree
(?), which counts the number
of spans inQ that are (1) converted to formal
triples, whose predicates are not Null, and
(2) covered by complete dependency subtrees
at the same time. The underlying intuition
is that, dependency subtrees of Q should be
treated as units for question translation.
? h
syntax constraint
(?), which counts the num-
ber of triples in D that are converted from
sub-questions generated by the question de-
composition component.
? h
triple
(?), which counts the number of triples
in D, whose predicates are not Null.
? h
triple
weight
(?), which sums the scores of all
triples {t
i
} in D as
?
t
i
?D
t
i
.score.
? h
QP
count
(?), which counts the number of
triples in D that are generated by QP-based
question translation method.
? h
RE
count
(?), which counts the number of
triples in D that are generated by RE-based
question translation method.
? h
staticrank
sbj
(?), which sums the static rank
scores of all subject entities in D?s triple set
as
?
t
i
?D
t
i
.e
sbj
.static rank.
? h
staticrank
obj
(?), which sums the static rank
scores of all object entities inD?s triple set as
?
t
i
?D
t
i
.e
obj
.static rank.
? h
confidence
obj
(?), which sums the confidence
scores of all object entities inD?s triple set as
?
t?D
t.e
obj
.confidence.
For each assertion {e
sbj
, p, e
obj
} stored in KB,
e
sbj
.static rank and e
obj
.static rank denote the
static rank scores
5
for e
sbj
and e
obj
respectively;
e
obj
.confidence rank represents the probability
p(e
obj
|e
sbj
, p). These three scores are used as fea-
tures to rank answers generated in QA procedure.
2.5 Feature Weight Tuning
Given a set of question-answer pairs {Q
i
,A
ref
i
}
as the development (dev) set, we use the minimum
error rate training (MERT) (Och, 2003) algorithm
to tune the feature weights ?
M
i
in our proposed
model. The training criterion is to seek the feature
weights that can minimize the accumulated errors
of the top-1 answer of questions in the dev set:
?
?
M
1
= argmin
?
M
1
N
?
i=1
Err(A
ref
i
,
?
A
i
;?
M
1
)
N is the number of questions in the dev set, A
ref
i
is the correct answers as references of the i
th
ques-
tion in the dev set,
?
A
i
is the top-1 answer candi-
date of the i
th
question in the dev set based on
feature weights ?
M
1
, Err(?) is the error function
which is defined as:
Err(A
ref
i
,
?
A
i
;?
M
1
) = 1? ?(A
ref
i
,
?
A
i
)
where ?(A
ref
i
,
?
A
i
) is an indicator function which
equals 1 when
?
A
i
is included in the reference set
A
ref
i
, and 0 otherwise.
3 Comparison with Previous Work
Our work intersects with two research directions:
semantic parsing and question answering.
Some previous works on semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2007; Wong and Mooney,
5
The static rank score of an entity represents a general
indicator of the overall quality of that entity.
972
2007; Kwiatkowski et al, 2010; Kwiatkowski
et al, 2011) require manually annotated logical
forms as supervision, and are hard to extend result-
ing parsers from limited domains, such as GEO,
JOBS and ATIS, to open domains. Recent work-
s (Clarke and Lapata, 2010; Liang et al, 2013)
have alleviated such issues using question-answer
pairs as weak supervision, but still with the short-
coming of using limited lexical triggers to link NL
phrases to predicates. Poon (2013) has proposed
an unsupervised method by adopting grounded-
learning to leverage the database for indirect su-
pervision. But transformation from NL questions
to MRs heavily depends on dependency parsing
results. Besides, the KB used (ATIS) is limited as
well. Kwiatkowski et al (2013) use Wiktionary
and a limited manual lexicon to map POS tags to
a set of predefined CCG lexical categories, which
aims to reduce the need for learning lexicon from
training data. But it still needs human efforts to de-
fine lexical categories, which usually can not cover
all the semantic phenomena.
Berant et al (2013) have not only enlarged the
KB used for Freebase (Google, 2013), but also
used a bigger lexicon trigger set extracted by the
open IE method (Lin et al, 2012) for NL phrases
to predicates linking. In comparison, our method
has further advantages: (1) Question answering
and semantic parsing are performed in an join-
t way under a unified framework; (2) A robust
method is proposed to map NL questions to their
formal triple queries, which trades off the mapping
quality by using question patterns and relation ex-
pressions in a cascaded way; and (3) We use do-
main independent feature set which allowing us to
use a relatively small number of question-answer
pairs to tune model parameters.
Fader et al (2013) map questions to formal
(triple) queries over a large scale, open-domain
database of facts extracted from a raw corpus by
ReVerb (Fader et al, 2011). Compared to their
work, our method gains an improvement in two
aspects: (1) Instead of using facts extracted us-
ing the open IE method, we leverage a large scale,
high-quality knowledge base; (2) We can han-
dle multiple-relation questions, instead of single-
relation queries only, based on our translation
based KB-QA framework.
Espana-Bonet and Comas (2012) have proposed
an MT-based method for factoid QA. But MT in
there work means to translate questions into n-
best translations, which are used for finding simi-
lar sentences in the document collection that prob-
ably contain answers. Echihabi and Marcu (2003)
have developed a noisy-channel model for QA,
which explains how a sentence containing an an-
swer to a given question can be rewritten into that
question through a sequence of stochastic opera-
tions. Compared to the above two MT-motivated
QA work, our method uses MT methodology to
translate questions to answers directly.
4 Experiment
4.1 Data Sets
Following Berant et al (2013), we use the same
subset of WEBQUESTIONS (3,778 questions) as
the development set (Dev) for weight tuning in
MERT, and use the other part of WEBQUES-
TIONS (2,032 questions) as the test set (Test). Ta-
ble 1 shows the statistics of this data set.
Data Set # Questions # Words
WEBQUESTIONS 5,810 6.7
Table 1: Statistics of evaluation set. # Questions is
the number of questions in a data set, # Words is
the averaged word count of a question.
Table 2 shows the statistics of question patterns
and relation expressions used in our KB-QA sys-
tem. As all question patterns are collected with hu-
man involvement as we discussed in Section 2.3.1,
the quality is very high (98%). We also sample
1,000 instances from the whole relation expression
set and manually label their quality. The accuracy
is around 89%. These two resources can cover 566
head predicates in our KB.
# Entries Accuracy
Question Patterns 4,764 98%
Relation Expressions 133,445 89%
Table 2: Statistics of question patterns and relation
expressions.
4.2 KB-QA Systems
Since Berant et al (2013) is one of the latest
work which has reported QA results based on a
large scale, general domain knowledge base (Free-
base), we consider their evaluation result on WE-
BQUESTIONS as our baseline.
Our KB-QA system generates the k-best deriva-
tions for each question span, where k is set to 20.
973
The answers with the highest model scores are
considered the best answers for evaluation. For
evaluation, we follow Berant et al (2013) to al-
low partial credit and score an answer using the F1
measure, comparing the predicted set of entities to
the annotated set of entities.
One difference between these two systems is the
KB used. Since Freebase is completely contained
by our KB, we disallow all entities which are not
included by Freebase. By doing so, our KB pro-
vides the same knowledge as Freebase does, which
means we do not gain any extra advantage by us-
ing a larger KB. But we still allow ourselves to
use the static rank scores and confidence scores of
entities as features, as we described in Section 2.4.
4.3 Evaluation Results
We first show the overall evaluation results of our
KB-QA system and compare them with baseline?s
results on Dev and Test. Note that we do not re-
implement the baseline system, but just list their
evaluation numbers reported in the paper. Com-
parison results are listed in Table 3.
Dev (Accuracy) Test (Accuracy)
Baseline 32.9% 31.4%
Our Method 42.5% (+9.6%) 37.5% (+6.1%)
Table 3: Accuracy on evaluation sets. Accuracy is
defined as the number of correctly answered ques-
tions divided by the total number of questions.
Table 3 shows our KB-QA method outperforms
baseline on both Dev and Test. We think the po-
tential reasons of this improvement include:
? Different methods are used to map NL phras-
es to KB predicates. Berant et al (2013)
have used a lexicon extracted from a subset
of ReVerb triples (Lin et al, 2012), which
is similar to the relation expression set used
in question translation. But as our relation
expressions are extracted by an in-house ex-
tractor, we can record their extraction-related
statistics as extra information, and use them
as features to measure the mapping quality.
Besides, as a portion of entities in our KB
are extracted from Wiki, we know the one-
to-one correspondence between such entities
and Wiki pages, and use this information in
relation expression extraction for entity dis-
ambiguation. A lower disambiguation error
rate results in better relation expressions.
? Question patterns are used to map NL context
to KB predicates. Context can be either con-
tinuous or discontinues phrases. Although
the size of this set is limited, they can actually
cover head questions/queries
6
very well. The
underlying intuition of using patterns is that
those high-frequent questions/queries should
and can be treated and solved in the QA task,
by involving human effort at a relative small
price but with very impressive accuracy.
In order to figure out the impacts of question
patterns and relation expressions, another exper-
iment (Table 4) is designed to evaluate their in-
dependent influences, where QP
only
and RE
only
denote the results of KB-QA systems which only
allow question patterns and relation expressions in
question translation respectively.
Settings Test (Accuracy) Test (Precision)
QP
only
11.8% 97.5%
RE
only
32.5% 73.2%
Table 4: Impacts of question patterns and relation
expressions. Precision is defined as the num-
ber of correctly answered questions divided by the
number of questions with non-empty answers gen-
erated by our KB-QA system.
From Table 4 we can see that the accuracy of
RE
only
on Test (32.5%) is slightly better than
baseline?s result (31.4%). We think this improve-
ment comes from two aspects: (1) The quality of
the relation expressions is better than the quality
of the lexicon entries used in the baseline; and
(2) We use the extraction-related statistics of re-
lation expressions as features, which brings more
information to measure the confidence of map-
ping between NL phrases and KB predicates, and
makes the model to be more flexible. Meanwhile,
QP
only
perform worse (11.8%) than RE
only
, due
to coverage issue. But by comparing the precision-
s of these two settings, we find QP
only
(97.5%)
outperforms RE
only
(73.2%) significantly, due to
its high quality. This means how to extract high-
quality question patterns is worth to be studied for
the question answering task.
As the performance of our KB-QA system re-
lies heavily on the k-best beam approximation, we
evaluate the impact of the beam size and list the
comparison results in Figure 6. We can see that as
6
Head questions/queries mean the questions/queries with
high frequency and clear patterns.
974
we increase k incrementally, the accuracy increase
at the same time. However, a larger k (e.g. 200)
cannot bring significant improvements comparing
to a smaller one (e.g., 20), but using a large k has
a tremendous impact on system efficiency. So we
choose k = 20 as the optimal value in above ex-
periments, which trades off between accuracy and
efficiency.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
5 20 50 100 200
Accuracy on Test 
Accuracy
Figure 6: Impacts of beam size on accuracy.
Actually, the size of our system?s search space
is much smaller than the one of the semantic parser
used in the baseline.This is due to the fact that, if
triple queries generated by the question translation
component cannot derive any answer from KB, we
will discard such triple queries directly during the
QA procedure. We can see that using a small k
can achieve better results than baseline, where the
beam size is set to be 200.
4.4 Error Analysis
4.4.1 Entity Detection
Since named entity recognizers trained on Penn
TreeBank usually perform poorly on web queries,
We instead use a simple string-match method to
detect entity mentions in the question using a
cleaned entity dictionary dumped from our KB.
One problem of doing so is the entity detection
issue. For example, in the question who was Es-
ther?s husband ?, we cannot detect Esther as an
entity, as it is just part of an entity name. We need
an ad-hoc entity detection component to handle
such issues, especially for a web scenario, where
users often type entity names in their partial or ab-
breviation forms.
4.4.2 Predicate Mapping
Some questions lack sufficient evidences to detec-
t predicates. where is Byron Nelson 2012 ? is an
example. Since each relation expression must con-
tain at least one content word, this question cannot
match any relation expression. Except for Byron
Nelson and 2012, all the others are non-content
words.
Besides, ambiguous entries contained in rela-
tion expression sets of different predicates can
bring mapping errors as well. For the follow-
ing question who did Steve Spurrier play pro
football for? as an example, since the unigram
play exists in both Film.Film.Actor and Ameri-
can Football.Player.Current Team ?s relation ex-
pression sets, we made a wrong prediction, which
led to wrong answers.
4.4.3 Specific Questions
Sometimes, we cannot give exact answers to
superlative questions like what is the first book
Sherlock Holmes appeared in?. For this example,
we can give all book names where Sherlock
Holmes appeared in, but we cannot rank them
based on their publication date , as we cannot
learn the alignment between the constraint word
first occurred in the question and the predicate
Book.Written Work.Date Of First Publication
from training data automatically. Although we
have followed some work (Poon, 2013; Liang
et al, 2013) to handle such special linguistic
phenomena by defining some specific operators,
it is still hard to cover all unseen cases. We leave
this to future work as an independent topic.
5 Conclusion and Future Work
This paper presents a translation-based KB-QA
method that integrates semantic parsing and QA
in one unified framework. Comparing to the base-
line system using an independent semantic parser
with state-of-the-art performance, we achieve bet-
ter results on a general domain evaluation set.
Several directions can be further explored in the
future: (i) We plan to design a method that can
extract question patterns automatically, using ex-
isting labeled question patterns and KB as weak
supervision. As we discussed in the experiment
part, how to mine high-quality question patterns is
worth further study for the QA task; (ii) We plan
to integrate an ad-hoc NER into our KB-QA sys-
tem to alleviate the entity detection issue; (iii) In
fact, our proposed QA framework can be general-
ized to other intelligence besides knowledge bases
as well. Any method that can generate answers to
questions, such as the Web-based QA approach,
can be integrated into this framework, by using
them in the question translation component.
975
References
Yoav Artzi and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations. In
EMNLP, pages 421?432.
Yoav Artzi, Nicholas FitzGerald, and Luke S. Zettle-
moyer. 2013. Semantic parsing with combinatory
categorial grammars. In ACL (Tutorial Abstracts),
page 2.
Jonathan Berant, Andrew Chou, Roy Frostig, and Per-
cy Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, pages 1533?
1544.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL, pages 423?433.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411?441.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
ACL.
Cristina Espana-Bonet and Pere R. Comas. 2012. Full
machine translation for factoid question answering.
In EACL, pages 20?29.
Anthony Fader, Stephen Soderland, and Oren Etzion-
i. 2011. Identifying relations for open information
extraction. In EMNLP, pages 1535?1545.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL, pages 1608?1618.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In ISWC.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2012.
Extracting multilingual natural-language patterns
for rdf predicates. In ESWC.
Google. 2013. Freebase. In http://www.freebase.com.
Aditya Kalyanpur, Siddharth Patwardhan, Branimir
Boguraev, Adam Lally, and Jennifer Chu-Carroll.
2012. Fact-based question decomposition in deep-
qa. IBM Journal of Research and Development,
56(3):13.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In EMNLP, pages 1223?1233.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In EMNLP, pages 1512?1523.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling seman-
tic parsers with on-the-fly ontology matching. In
EMNLP, pages 1545?1556.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL, pages 590?599.
Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389?446.
Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity
linking at web scale. In AKBC-WEKEX, pages 84?
88.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In CICLing, pages 311?324.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160?
167.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In ACL, pages 933?943.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In HLT-NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In ACL.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, Vol. 2, pages 1050?
1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658?666.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678?687.
976
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 304?307,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PKU_HIT: An Event Detection System Based on Instances Expansion 
and Rich Syntactic Features 
 
 
Shiqi Li1, Pengyuan Liu2, Tiejun Zhao1, Qin Lu3 and Hanjing Li1 
1School of Computer Science and Technology, 
Harbin Institute of Technology, Harbin 150001, China 
{sqli,tjzhao,hjlee}@mtlab.hit.edu.cn 
2Institute of Computational Linguistics, 
Peking University, Beijing 100871, China 
liupengyuan@pku.edu.cn 
3Department of Computing, 
The Hong Kong Polytechnic University, Hong Kong, China 
csluqin@comp.polyu.edu.hk 
 
  
 
Abstract 
This paper describes the PKU_HIT system 
on event detection in the SemEval-2010 
Task. We construct three modules for the 
three sub-tasks of this evaluation. For 
target verb WSD, we build a Na?ve 
Bayesian classifier which uses additional 
training instances expanded from an 
untagged Chinese corpus automatically. 
For sentence SRL and event detection, we 
use a feature-based machine learning 
method which makes combined use of 
both constituent-based and dependency-
based features. Experimental results show 
that the Macro Accuracy of the WSD 
module reaches 83.81% and F-Score of 
the SRL module is 55.71%. 
1 Introduction 
In this paper, we describe the system submitted 
to the SemEval-2010 Task 11 on event detection 
in Chinese news sentences (Zhou, 2010). The 
objective of the task is to detect and analyze 
basic event contents in Chinese news sentences, 
similar to the frame semantic structure extraction 
task in SemEval-2007. However, this task is a 
more complex as it involves three interrelated 
subtasks: (1) target verb word sense 
disambiguation (WSD), (2) sentence semantic 
role labeling (SRL) and (3) event detection (ED).  
Therefore, the architecture of the system that 
we develop for the task consists of three modules: 
WSD, SRL and ED. First, the WSD module is to 
recognize key verbs or verb phrases which 
describe the basic event in a sentence, and then 
select an appropriate situation description 
formula for the recognized key verbs (or verb 
phrases); Then, the SRL module anchors the 
arguments to suitable constituents in the sentence, 
and then label each argument with three 
functional tags, namely constituent type tag, 
semantic role tags and event role tag. Finally, in 
the ED module, complete situation description of 
the sentence can be achieved by combining the 
results of the WSD module and the SRL module. 
For the WSD module, we consider the subtask 
as a general WSD problem. First of all, we 
automatically extract many instances from an 
untagged Chinese corpus using a heuristic rule 
inspired by Yarowsky (1993). Then we train a 
Na?ve Bayesian (NB) classifier based on both the 
extracted instances and the official training data. 
We then use the NB classifier to predict situation 
the description formula and natural explanation 
of each target verb in testing data. 
For the SRL module, we use a rich syntactic 
feature-based learning method. As the state-of-
the-art method in the field of SRL, feature-based 
method represents a predicate-argument structure 
(PAS) by a flat vector using a set of linguistic 
features. Then PAS can be directly classified by 
machine learning algorithms based on the 
corresponding vectors. In feature-based SRL, the 
304
significance of syntactic information in SRL was 
proven by (Punyakanok et al, 2005). In our 
method, we exploit a rich set of syntactic 
features from two syntactic views: constituent 
and dependency. As the two syntactic views 
focus on different syntactic elements, 
constituent-based features and dependency-based 
features can complement each other in SRL to 
some extent. Finally, the ED module can be 
readily implemented by combining the SRL and 
the WSD result using some simply rules.  
2 System Description 
2.1 Target Verb WSD 
The WSD module is based on a simple heuristic 
rule by which we can extract sense-labeled 
instances automatically. The heuristic rule 
assumes that one sense per 3-gram which is 
proposed by us initially through investigating a 
Chinese sense-tagged corpus STC (Wu et al, 
2006). The assumption is similar to the 
celebrated one sense per collocation supposition 
(Yarowsky, 1993), whereas ours has more 
expansibility. STC is an ongoing project which is 
to build a sense-tagged corpus containing sense-
tagged 1, 2 and 3 months of People?s Daily 2000 
now. According to our investigation, given a 
specific 3-gram (w-1wverbw1) to any target verb, 
on average, we expect to see the same label 
95.4% of the time. Based on this observation, we 
consider one sense per 3-gram (w-1wverbw1) or at 
least we can extract instances with this pattern. 
For all the 27 multiple-sense target verbs in 
the official training data, we found their 3-gram 
(w-1wverbw1) and extracted the instances with the 
same 3-gram from a Chinese monolingual corpus 
? the 2001 People?s Daily (about 116M bytes). 
We consider the same 3-gram instances should 
have the same label. Then an additional sense-
labeled training corpus is built automatically in 
expectation of having 95.4% precision at most. 
And this corpus has 2145 instances in total 
(official training data have 4608 instances). 
We build four systems to investigate the effect 
of our instances expansion using the Na?ve 
Bayesian classifier. System configuration is 
shown in Table 1. In column 1, BL means 
baseline, X means instance expansion, 3 and 15 
means the window size. In column 2, wi is the i-
th word relative to the target word, wi-1wi is the 2-
gram of words, wj/j is the word with position 
information (j?[-3,+3]). In the last column, ?O? 
means using only the original training data and 
?O+A? means using both the original and 
additional training data. Syntactic feature and 
parameter optimizing are not used in this module. 
 
System Features Window Size 
Training 
Data 
BL_3 
wi, wi-1wi, wj/j
?3 O 
X_3 ?3 O+A 
BL_15 ?15 O 
X_15 ?15 O+A 
Table 1: The system configuration 
2.2 Sentence SRL and Event Detection 
We use a feature-based machine learning method 
to implement the SRL module in which three 
tags are labeled, namely the semantic role tag, 
the event role tag and the phrase type tag. We 
consider the SRL task as a four-step pipeline: (1) 
parsing which generates a constituent parse tree 
for the input sentence; (2) pruning which filters 
out many apparently impossible constituents 
(Xue and Palmer, 2004); (3) semantic role 
identification (SRI) which identifies the 
constituent that will be the semantic role of a 
predicate in a sentence, and (4) semantic role 
classification (SRC) which determines the type 
of identified semantic role. The machine learning 
method takes PAS as the classification unit 
which consists of a target predicate and an 
argument candidate. The SRI step utilizes a 
binary classifier to determine whether the 
argument candidate in the PAS is a real argument. 
Finally, in the SRC step, the semantic role tag 
and the event role tag of each identified 
argument can be obtained by two multi-value 
classifications on the SRI results. The remaining 
phrase type tag can be directly extracted from the 
constituent parsing tree.  
The selection of the feature set is the most 
important factor for the feature-based SRL 
method. In addition to constituent-based features 
and dependency-based features, we also consider 
WSD-based features. To our knowledge, the 
combined use of constituents-based syntactic 
features and dependency-based syntactic features 
is the first attempts to use them both on the 
feature level of SRL. As a prevalent kind of 
syntactic features for SRL, constituent-based 
features have been extensively studied by many 
researchers. In this module, we use 34 
constituent-based features, 35 dependency-based 
features, and 2 WSD-based features. Among the 
constituent-based features, 26 features are 
manually selected from effective features proven 
by existing SRL studies and 8 new features are 
305
defined by us. Firstly, the 26 constituent-based 
features used by others are: 
y predicate (c1), path (c2), phrase type (c3), 
position (c4), voice (c5), head word (c6), 
predicate subcategorization (c7), syntactic 
frame (c8), head word POS (c9), partial path 
(c10), first/last word (c11/c12), first/last POS 
(c13/c14), left/right sibling type (c15/c16), 
left/right sibling head (c17/c18), left/right 
sibling POS (c19/c20), constituent tree 
distance (c21), temporal cue words (c22), 
Predicate POS (c23), argument's parent 
type(c24), argument's parent head (c25) and 
argument's parent POS (c26). 
And the 8 new features we define are: 
y Locational cue words (c27): a binary feature 
indicating whether the constituent contains 
location cue word.  
y POS pattern of argument (c28): the left-to-
right chain of POS tags of argument's children. 
y Phrase type pattern of argument (c29): the 
left-to-right chain of phrase type labels of 
argument's children. 
y Type of LCA and left child (c30): The phrase 
type of the Lowest Common Ancestor (LCA) 
combined with its left child. 
y Type of LCA and right child (c31): The phrase 
type of the LCA combined with its right child. 
y Three features: word bag of path (c32), word 
bag of POS pattern (c33) and word bag of type 
pattern (c34), for generalizing three sparse 
features: path (c7), POS pattern argument (c28) 
and phrase type pattern of argument (c29) by 
the bag-of-words representation. 
Secondly, the selection of dependency-based 
features is similar to that of constituent-based 
features. But dependency parsing lacks 
constituent information. If we want to use 
dependency-based features to label constituents, 
we should map a constituent to one or more 
appropriate words in dependency trees. Here we 
use head word of a constituent to represent it in 
dependency parses. The 35 dependency-based 
features we adopt are:  
y Predicate/Argument relation (d1/d2), relation 
path (d3), POS pattern of predicate?s children 
(d4), relation pattern of predicate?s children 
(d5) , child relation set (d6), child POS set (d7), 
predicate/argument parent word (d8/d9), 
predicate/argument parent POS (d10/d11), 
left/right word (d12/d13), left/right POS 
(d14/d15), left/right relation (d16/d17), 
left/right sibling word (d18/d19), left/right 
sibling POS (d20/d21), left/right sibling 
relation (d22/d23), dep-exists (d24) and dep-
type (d25), POS path (d26), POS path length 
(d27), relation path length (d28), high/low 
support verb (d29/d30), high/low support noun 
(d31/d32) and LCA?s word/POS/relation 
(d33/d34/d35). 
In this work, the dependency parse trees are 
generated from the constituent parse trees using a 
constituent-to-dependency converter (Marneffe 
et al, 2006). The converter is suitable for 
semantic analysis as it can retrieve the semantic 
head rather than the general syntactic head.  
Lastly, the 2 WSD-based features are: 
y Situation description formula (s1): predicate?s 
situation description formula generated by the 
WSD module. 
y Natural explanation (s2): predicate?s natural 
explanation generated by the WSD module. 
3 Experimental Results and Discussion 
3.1 Target Verb WSD 
System Micro-A (%) Macro-A (%) Rank
BL_3 81.30 83.81 3/7 
X_3 79.82 82.58 4/7 
BL_15 79.23 82.18 5/7 
X_15 77.74 81.42 6/7 
Table 2: Official results of the WSD systems 
Table 2 shows the official result of the WSD 
system. BL_3 with window size three using the 
original training corpus achieves the best result 
in our submission. It indicates the local features 
are more effective in our systems. There are two 
possible reasons why the performances of the X 
system with instance expansion are lower than 
the BL system. First, the additional instances 
extracted based on 3-gram provide a few local 
features but many topical features. But, local 
features are more effective for our systems as 
mentioned above. The local feature related 
information that the classifier gets from the 
additional instances is not sufficient. Second, the 
granularity of the WSD module is too small to be 
distinguished by 3-grams. As a result, the 
additional corpus built upon 3-gram has more 
exceptional instances (noises), and therefore it 
impairs the performance of X_3 and X_15. 
Taking the verb ??? ? (belong to ) as an 
example, it has two senses in the task, but both 
senses have the same natural explanation: ???
?????????? (part of or belong to), 
which is always considered as the sense in 
general SRL. The difference between the two 
senses is in their situation description formulas: 
?partof (x,y)+NULL? vs. ?belongto (x,y)+NULL?.  
306
3.2 Sentence SRL and Event Detection 
In the SRL module, we use the training data 
provided by SemEval-2010 to train the SVM 
classifiers without any external resources. The 
training data contain 4,608 sentences, 100 target 
predicates and 13,926 arguments. We use the 
SVM-Light Toolkit (Joachims, 1999) for the 
implementation of SVM, and use the Stanford 
Parser (Levy and Manning, 2003) as the parser 
and the constituent-to-dependency converter. We 
employ the linear kernel for SVM and set the 
regularization parameter to the default value 
which is the reciprocal of the average Euclidean 
norm of the training data. The evaluation results 
of our SRL module on the official test data are 
shown in Table 3, where ?AB?, ?SR?, ?PT? and 
?ER? represent argument boundary, semantic role 
tag, phrase type tag, and event role tag. 
 
Tag Precision(%) Recall(%) F-Score(%)
AB 73.10 66.83 69.82 
AB+SR 67.44 61.65 64.42 
AB+PT 61.78 56.48 59.01 
AB+ER 69.05 63.12 65.95 
Overall 58.33 53.32 55.71 
Table 3: Official results of the SRL system 
It is clear that ?AB? plays an important role as 
the labeling of the other three tags is directly 
based on it. Through analyzing the results, we 
find that errors in the recognition of ?AB? are 
mainly caused by two factors: the automatic 
constituent parsing and the pruning algorithm. It 
is inevitable that some constituents and 
hierarchical relations are misidentified in 
automatic parsing of Chinese. These errors are 
further enlarged by the heuristic-based pruning 
algorithm because the algorithm is built upon the 
gold-standard paring trees, and therefore a lot of 
real arguments are pruned out when using the 
noisy automatic parses. So the pruning algorithm 
is the current bottleneck of SRL in the evaluation.  
 
System Micro-A (%) Macro-A (%) Rank
BL_3 20.33 20.19 4/7 
X_3 20.05 20.23 5/7 
BL_15 20.05 20.22 6/7 
X_15 20.05 20.14 7/7 
Table 4: Official results of the ED systems 
From the fact that the results of ?AB+SR? and 
?AB+ER? are close to that of ?AB?, it can be 
inferred that the SR and ER results should be 
satisfactory if the errors in ?AB? are not 
propagated. Furthermore, the result of ?AB+PT? 
is low as the phrase types here is inconsistent 
with those in Stanford Parser. The problem 
should be improved by a set of mapping rules. 
Finally, in the ED module, we combine the 
results of WSD and SRL by filling variables of 
the situation description formula obtained by the 
WSD module with the arguments obtained by the 
SRL module according to their event role tags. 
Table 4 shows the final results which are 
generated by combining the results of WSD and 
SRL. Obviously the reduced overall ranking 
comparing to WSD is due to the SRL module. 
4 Conclusions 
In this paper, we propose a modular approach for 
the SemEval-2010 Task on Chinese event 
detection. Our system consists of three modules: 
WSD, SRL and ED. The WSD module is based 
on instances expansion, and the SRL module is 
based on rich syntactic features. Evaluation 
results show that our system is good at WSD, 
semantic role tagging and event role tagging, but 
poor at pruning and boundary detection. In future 
studies, we will modify the pruning algorithm to 
reduce the bottleneck of the current system. 
 
Acknowledgments 
This work is partially supported by the Hong 
Kong Polytechnic University under Grant No. G-
U297 and G-U596, and by the National Natural 
Science Foundation of China under Grant No. 
60736014 and 60803094. 
References  
Thorsten Joachims. 1999. Making large-Scale SVM 
Learning Practical. Advances in Kernel Methods. 
Support Vector Learning, B. Sch?lkopf and C. 
Burges and A. Smola (ed), MIT Press. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank. 
Proceedings of ACL-2003. 
Vasin Punyakanok, Dan Roth, and Wentau Yih. 2005. 
The necessity of syntactic parsing for semantic role 
labeling. Proceedings of IJCAI-2005. 
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen 
Yu. 2006. A Chinese corpus with word sense 
annotation. Proceedings of ICCPOL-2006. 
David Yarowsky. 1993. One sense per collocation. 
Proceedings of the ARPA Workshop on Human 
Language Technology. 
Qiang Zhou. 2010. SemEval-2010 task 11: Event 
detection in Chinese News Sentences. Proceedings 
of SemEval-2010. 
307
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 371?374,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PengYuan@PKU: Extracting Infrequent Sense Instance with the 
Same N-gram Pattern for the SemEval-2010 Task 15 
Peng-Yuan Liu1  Shui Liu2  Shi-Wen Yu1  Tie-Jun Zhao2  
1Institute of Computational Linguistics, Peking University, Beijing, China 
2Department of Computer Science, Harbin Institute of Technology, Harbin, China 
{liupengyuan,yusw}@pku.edu.cn,{tjzhao,liushui}@mtlab.hit.edu.cn 
  
 
Abstract 
 
This paper describes our infrequent sense 
identification system participating in the 
SemEval-2010 task 15 on Infrequent Sense 
Identification for Mandarin Text to Speech 
Systems. The core system is a supervised 
system based on the ensembles of Na?ve 
Bayesian classifiers. In order to solve the 
problem of unbalanced sense distribution, we 
intentionally extract only instances of 
infrequent sense with the same N-gram pattern 
as the complemental training data from an 
untagged Chinese corpus ? People?s Daily of 
the year 2001. At the same time, we adjusted 
the prior probability to adapt to the 
distribution of the test data and tuned the 
smoothness coefficient to take the data 
sparseness into account. Official result shows 
that, our system ranked the first with the best 
Macro Accuracy 0.952. We briefly describe 
this system, its configuration options and the 
features used for this task and present some 
discussion of the results. 
1 Introduction 
We participated in the SemEval-2010 task 15 on 
Infrequent Sense Identification for Mandarin 
Text to Speech Systems. This task required 
systems to disambiguating the homograph word, 
a word that has the same POS (part of speech) 
but different pronunciation. In this case, we still 
considered it as a WSD (word sense 
disambiguation) problem, but it  is a little 
different from WSD. In this task, two or more 
senses of the same word may correspond to one 
pronunciation. That is, the sense granularity is 
coarser than traditional WSD.  
The challenge of this task is the much skewed 
distribution in real text: the most frequent 
pronunciation accounts for usually over 80%. In 
fact, in the training data provided by the 
organizer , we found that the sense distribution 
of some words are distinctly unbalanced. For 
each of these words, there are fewer than ten 
instances of one sense whereas the dominant 
sense instances are hundreds or more. At the 
same time, according to the task description on 
the task 15 of SemEval-
2010(http://semeval2.fbk.eu/semeval2.php?locati
on=tasks), the test dataset of this task is 
intentionally divided into the infrequent 
pronunciation instances and the frequent ones by 
half and half. Apparently, if we use traditional 
methods and only the provided training dataset  
to train whatever classifier, it is very likely that 
we will  get an disambiguation result that all (at 
least the overwhelming number) the test 
instances of these words would be labeled with 
the most frequent pronunciation (sense) tag. 
Then our system is meaningless for the target of 
the task  is focused on the performance of 
identifying the infrequent sense. 
In order to solve the problem of the 
unbalanced sense distribution in the training data 
and the  fairly balanced sense distribution in the 
test data, we designed our PengYuan@PKU 
system, which attempts to extract infrequent 
sense instances only and adjust the prior 
probability so as to counteract the problem as far 
as possible. The core system is a supervised 
system based on the ensembles of Na?ve 
Bayesian classifiers. The complemental training 
data is extracted from an untagged Chinese 
corpus ? People?s Daily of the year 2001 
automatically. Besides the motivation of 
investigating the function of our method of 
compensating infrequent sense instances, we are 
also interested in the role where the smoothness 
plays when it encounters with such a data 
sparseness here. 
In section 2, we will describe our system that 
includes the core classifier, its configuration 
options and features. In section 3, we will show 
the official results of this task and present some 
analyses and discussions. Section 4 is related 
371
works. The conclusion and future work are in 
section 5. 
2 System Description 
2.1 Na?ve Bayesian Classifier and Features 
For a na?ve Bayesian classifier, the joint 
probability of observing a certain combination of 
context features with a particular sense is 
expressed as: 
1 2
1
( , ,..., , ) ( ) ( | )
n
n i
i
p F F F S p S p F S
=
= ?          (1) 
In equation (1), (F1, F2,?, Fn) is feature 
variables, S is classification variable and p(S) is 
the prior probability of classification variable. 
Any  parameter  that  has  a  value  of zero  
indicates that  the  associated word never occurs 
with  the  specified  sense  value.  These zero 
values are smoothed by additive smoothing 
method as expressed below: 
( , )( | ) ( )
i k
i k
k
C F SP F S C S N
l+= +    , ??(0,1)    (2) 
In equation (2), ?is the smoothness variable. 
C(Sk) is the times of instances with Sk label. 
C(Fi,Sk) is the concurrences times of Fi and Sk. N 
is the times of total words in the corpus. 
The features and their weights of context used 
in one single Na?ve Bayesian classifier are 
described in Table 1. 
 
Features Description weights 
w-i?wi 
Content words appearing 
within the window of ?i 
words on each side of the 
target word 
1 
wj/j 
j?[-3,3] 
Word forms and their 
position information of the 
words at fixed positions 
from the target word. 
3 
wk-1wk 
k?(-i,i] 
word bigrams appearing 
within the window of ?i 
1 when 
i>3, else 
3 
Pk-1Pk 
k?(-i,i] 
POS bigrams appearing 
within the window of ?i 1 
 
Table 1: Features and their weights used in one 
Na?ve Bayesian classifier 
2.2 Ensembles the Na?ve Bayesian 
Classifiers 
The ensemble strategy of our system is like 
Pederson (2000). The windows of context have 
seven different sizes (i): 3, 5, 7, 9, 11, 13 and 15 
words. The first step in the ensemble approach is 
to train a separate Na?ve Bayesian classifier for 
each of the seven window sizes. 
Each of the seven member classifiers votes for 
the most probable sense given the particular 
context represented by that classifier; the 
ensemble disambiguates by assigning the sense 
that receives the majority of the votes. 
2.3 Infrequent Sense Instances Acquisition 
N-gram Increasing Instances Number 
(-1,1) 246 
(-2,0) 229 3-gram 
(0,2) 551 
1026(9135) 
(-1,0) 1123 2-gram (0,1) 1844 2967(9135) 
 
Table 2: The overview of the training data before and 
after the extracting stage 
 
Sense Distribution 
After 
Target 
Words Before 
(O)  (O+E3)  (O+E2) 
 ? 128 51 128 66 128 2621 
 ? 503 83 503 83 503 194 
 ?? 168 13 168 16 168 23 
 ? 175 10 175 27 175 88 
 ? 487 42 487 63 487 267 
 ?? 134 44 134 44 134 49 
 ?? 125 11 125 11 125 12 
 ? 2020 8 2020 12 2020 25 
 ? 300 3 300 6 300 32 
 ? 268 3 268 4 268 45 
 ? 1625 41 1625 346 1625 1625 
 ? 144 13 144 15 144 33 
 ?? 136 8 136 9 136 16 
 ? 1666 253 1666 847 1666 1567 
 ? 142 17 142 17 142 17 
 ? 438 76 438 136 438 414 
 
Table 3: The sense distributions of the training data 
before and after the extracting stage 
Our system uses a special heuristic rule to extract 
the sense labeled infrequent sense instances 
automatically. The heuristic rule assumes that 
one sense per N-gram which we testified initially 
through investigating a Chinese sense-tagged 
corpus STC (Wu et al, 2006). Our assumption is 
inspired by the celebrated one sense per 
collocation supposition (Yarowsky, 1993). STC 
is an ongoing project of building a sense-tagged 
                                                          
1 We intentionally control the sense distribution of word 
(???) and change it from approximately 2.5:1 to 1:2 so as 
to investigate the influence. 
372
corpus which contained the sense-tagged 1, 2 and 
3 months of People?s Daily of the year 2000. 
According to our investigation, to any target 
multi-sense word, given a specific N-gram (N>1) 
including the target word, we will expect to see 
the same label that range from 88.6% to 99.2% 
of the time on average. So, based on the training 
data, we can extract instance with the same N -
gram pattern from the untagged Chinese corpus 
and we assume if the N-gram is the same then 
the sense-label is the same. 
For all the 16 multiple-sense target words in 
the training data of task 15, we found the N-gram 
of infrequence sense instances and  extracted2 the 
instances with the same N-gram from People?s 
Daily of the year 2001(about 116M bytes). We 
extracted as many as possible until the total 
number of them is equal to the dominant sense 
instance number. We appointed the same N-gram 
instances the same sense tag and (merge?) it into 
the original training corpus. Table 2 and 3 show 
the overview and the sense distribution of the 
training data before and after the extracting stage. 
Number 9135 in brackets of Table 2 is the 
instance number of original training corpus. O, 
O+E3, O+E2 in Table 3 mean original training 
data, original training data plus extracted 3-gram 
instances and original training data plus extracted 
2-gram instances respectively. Limited to the 
scale of the corpus, the unbalance sense 
distribution of some words does not improve 
much. 
2.4 Other Configuration Options 
Systems Training Data p(S) ? 
_3.001 O+E3 0.5 0.001 
_3.1 O+E3 0.5 0.1 
_2.001 O+E2 0.5 0.001 
_2.1 O+E2 0.5 0.1 
 
Table 4: The system configuration 
To formula (1), we tune the prior probability of 
classification variable p(S) as a constant to match 
the sense distribution of test data. Considering 
the data sparseness as there may have been in the 
test stage, to formula (2), we set 2 kinds of?to 
investigate  the effect of smoothness. 
In total, we develop four systems based on 
various configuration options. They are showed 
in Table 4. 
                                                          
2 In order to guarantee the extracted instances are not 
duplicated in the training data or in the test data in case, our 
system filters the repeated instances automatically if they 
are already in the original training or test dataset. 
3 Results and Discussions 
3.1 Official Results 
System 
ID 
Micro  
Accuracy 
Macro 
Accuracy 
Rank 
_3.001 0.974 0.952 1/9 
_3.1 0.965 0.942 2/9 
_2.001 0.965 0.941 3/9 
_2.1 0.965 0.942 2/9 
Baseline 0.924 0.895  
 
Table 5: Official results 1 of PengYuan@PKU 
 
Precision Words 
_3.001 _3.1 _2.001 _2.1 baseline 
?   0.844 0.789 0.789 0.789 0.711 
?   0.976 0.962 0.969 0.962 0.863 
?? 0.901 0.901 0.901 0.901 0.901 
?   0.978 0.989 0.978 0.989 0.957 
?   0.925 0.853  0.864 0.853  0.925 
?? 0.956 0.944 0.956 0.944 0.700  
?? 0.971 0.956 0.956 0.956 0.956 
?   0.998  0.997 0.997 0.997 0.996 
?   0.987 0.974 0.974 0.974 0.987 
?   0.956 0.963 0.971 0.963 0.956 
?   0.983 0.975  0.969 0.975  0.978 
?   0.924 0.949 0.937 0.949 0.886 
?? 0.986 0.986 0.986 0.986 0.959 
?   0.986 0.989 0.989 0.989 0.869  
?   0.875    0.900 0.875    0.900 0.838   
?   0.981 0.946 0.953 0.946 0.844 
 
Table 6: Official results 2 of PengYuan@PKU 
Macro Accuracy is the average disambiguation 
precision of each target word. Micro Accuracy is 
the disambiguation precision of total instances of 
all words. For task 15 whose instance 
distribution of the target words is very 
unbalanced in the test dataset, Macro Accuracy 
maybe a better evaluation indicator. Our systems 
achieved from 1st to 4th position (ranked by 
Macro Accuracy) out of all nine systems that 
participated in this task. Our best system is 
PengYuan@PKU_3.001 which uses original 
training data plus extracted 3-gram instances as 
our training data, P(S) is tuned to 0.5 and?is 
equal to 0.001.  
3.2 Discussions 
From the official result in Table 5 and Table 6 
we can see, for this task, our classifier and 
strategy of extracting infrequency instances is 
effective. Basically, for each target word, the 
373
performances of our systems are superior to the 
baseline.  
From Table 6, we also see the performances of 
our systems are influenced by different ? and 
different instance extracting patterns. 
Comparatively smaller probability ? of 
nonoccurrence features is better. Using the 
Extracting 3-gram instances is better than that of 
using 2-gram. (By using the 3-gram method of 
extracting instances, we obtain a better result 
than that of 2-gram.) 
Our original idea for the system is two-folds. 
On one hand, we consider the relieving of data 
sparseness through more instances extracted by 
2-gram pattern can achieve a better performance 
than that of 3-gram pattern, though the instances 
extracted through 2-gram pattern induce more 
noise. On the other hand, we assume that the 
performance would be better if we had given a 
larger probability of nonoccurrence features, for 
this strategy favors more infrequent sense 
instances. However the unbalance of sense 
distribution in the real test data as is shown in 
Table 5 went beyond our expectation. It is very 
hard for us to evaluate our system from the 
viewpoint of smoothness and instance sense 
distribution. 
4 Related Work 
To our knowledge, the methods of auto-
acquiring sense-labeled instances include using 
parallel corpora like Gale et al (1992) and Ng et 
al. (2003), extracting by monosemous relative of 
WordNet like Leacock et al (1998), Mihalcea 
and Moldovan (1999), Agirre and Mart?nez 
(2004), Mart?nez et al (2006) and PengYuan et 
al. (2008). The method proposed by Mihalcea 
and Moldovan (2000) is also an effective way. 
5 Conclusion and Future Work 
We participated in the SemEval-2010 task 15 on 
Infrequent Sense Identification for Mandarin 
Text to Speech Systems. Official results show 
our system which extract infrequent sense 
instances is effective.  
For the future studies, we will focus on how to 
identify the infrequent sense instances effectively 
based on the plan to change the proposition 
between dominant sense and infrequent sense 
step by step. 
 
Acknowledgments 
This work was supported by the project of 
National Natural Science Foundation of China 
(No.60903063) and China Postdoctoral Science 
Foundation funded project (No.20090450007). 
References  
Claudia Leacock, Martin Chodorow and George A. 
Miller, Using  Corpus Statistics and WordNet 
Relations for Sense Identification. Computational 
Linguistics, 1998, 24(1):147~166 
David Mart?inez, Eneko Agirre and Xinglong Wang. 
Word relatives in context for word sense 
disambiguation.  Proceedings of the 2006 
Australasian Language Technology Workshop 
(ALTW2006), 2006:42~50 
David Yarowsky. 1993. One sense per collocation. 
Proceedings of the ARPA Workshop on Human 
Language Technology. 
Eneko Agirre and David Mart?inez. Unsupervised 
WSD based  on  automatically retrieved  examples:  
The  importance of bias. Proceedings of the 
International Conference on Empirical Methods  in  
Natural Language Processing, EMNLP, 
2004:25~32 
Hwee Tou Ng, Bin Wang, Yee Seng Chan. Exploiting 
Parallel Texts for Word Sense Disambiguation: An 
Empirical Study. Proceeding of the 41st ACL, 455-
462, Sappora, Japan. 
Liu Peng-yuan Zhao Tie-jun Yang Mu-yun Li Zhuang. 
2008. Unsupervised Translation Disambiguation 
Based on Equivalent PseudoTranslation Model. 
Journal of Electronics & Information Technology. 
30(7):1690-1695. 
Rada Mihalcea and Dan I. Moldovan. 1999. An 
automatic method for generating sense tagged 
corpora. Proceedings of AAAI-99, Orlando, FL, 
July, pages 461?466. 
Rada Mihalcea and Dan .I. Moldovan. 2000. An 
iterative approach to word sense disambiguation. 
Proceedings of FLAIRS-2000, pages 219?223, 
Orlando, FL, May. 
Ted. Pedersen. 2000. A Simple Approach to Building 
Ensembles of Na?ve Bayesian Classifiers for Word 
Sense Disambiguation.  Proceedings  of  the  First 
Annual  Meeting  of  the  North  American  Chapter  
of  the  Association  for Computational Linguistics, 
pages 63-69, Seattle, WA, May.  
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen 
Yu. 2006. A Chinese corpus with word sense 
annotation. Proceedings of ICCPOL-2006. 
William A. Gale, Kenneth W. Church and David 
Yarowsky. A method for disambiguating word 
senses in a large corpus. Computers and the 
Humanities, 26(2):415-539 
374
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 102?109,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Using Deep Belief Nets for Chinese Named Entity Categorization 
Yu Chen1, You Ouyang2, Wenjie Li2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{csyouyang, cswjli}@comp.polyu.edu.hk
Abstract 
Identifying named entities is essential in 
understanding plain texts. Moreover, the 
categories of the named entities are indicative 
of their roles in the texts. In this paper, we 
propose a novel approach, Deep Belief Nets 
(DBN), for the Chinese entity mention 
categorization problem. DBN has very strong 
representation power and it is able to 
elaborately self-train for discovering 
complicated feature combinations. The 
experiments conducted on the Automatic 
Context Extraction (ACE) 2004 data set 
demonstrate the effectiveness of DBN. It 
outperforms the state-of-the-art learning 
models such as SVM or BP neural network. 
1 Introduction 
Named entities (NE) are defined as the names of 
existing objects, such as persons, organizations 
and etc. Identifying NEs in plain texts provides 
structured information for semantic analysis. 
Hence the named entity recognition (NER) task 
is a fundamental task for a wide variety of 
natural language processing applications, such as 
question answering, information retrieval and etc. 
In a text, an entity may either be referred to by a 
common noun, a noun phrase, or a pronoun. 
Each reference of the entity is called a mention. 
NER indeed requires the systems to identify 
these entity mentions from plain texts. The task 
can be decomposed into two sub-tasks, i.e., the 
identification of the entities in the text and the 
classification of the entities into a set of pre-
defined categories. In the study of this paper, we 
focus on the second sub-task and assume that the 
boundaries of all the entity mentions to be 
categorized are already correctly identified. 
In early times, NER systems are mainly based 
on handcrafted rule-based approaches. Although 
rule-based approaches achieved reasonably good 
results, they have some obvious flaws. First, they 
require exhausted handcraft work to construct a 
proper and complete rule set, which partially 
expressing the meaning of entity. Moreover, 
once the interest of task is transferred to a 
different domain or language, rules have to be 
revised or even rewritten. The discovered rules 
are indeed heavily dependent on the task 
interests and the particular corpus. Finally, the 
manually-formatted rules are usually incomplete 
and their qualities are not guaranteed. 
Recently, more attentions are switched to the 
applications of machine learning models with 
statistic information. In this camp, entity 
categorization is typically cast as a multi-class 
classification process, where the named entities 
are represented by feature vectors. Usually, the 
vectors are abstracted by some lexical and 
syntactic features instead of semantic feature. 
Many learning models, such as Support Vector 
Machine (SVM) and Neural Network (NN), are 
then used to classify the entities by their feature 
vectors. 
Entity categorization in Chinese attracted less 
attention when compared to English or other 
western languages. This is mainly because the 
unique characteristics of Chinese. One of the 
most common problems is the lack of boundary 
information in Chinese texts. For this problem, 
character-based methods are reported to be a 
possible substitution of word-based methods. As 
to character-based methods, it is important to 
study the implicit combination of characters.  
In our study, we explore the use of Deep 
Belief Net (DBN) in character-based entity 
categorization. DBN is a neural network model 
which is developed under the deep learning 
architecture. It is claimed to be able to 
automatically learn a deep hierarchy of the input 
features with increasing levels of abstraction for 
the complex problem. In our problem, DBN is 
used to automatically discover the complicated 
composite effects of the characters to the NE 
categories from the input data. With DBN, we 
need not to manually construct the character 
combination features for expressing the semantic 
relationship among characters in entities. 
Moreover, the deep structure of DBN enables the 
possibility of discovering very sophisticated 
102
combinations of the characters, which may even 
be hard to discover by human. 
The rest of this paper is organized as follow. 
Section 2 reviews the related work on name 
entity categorization. Section 3 introduces the 
methodology of the proposed approach. Section 
4 provides the experimental results. Finally, 
section 5 concludes the whole paper. 
2 Related work 
Over the past decades, NER has evolved from 
simple rule-based approaches to adapted self-
training machine learning approaches. 
As early rule-based approaches, MacDonald 
(1993) utilized local context, which implicate 
internal and external evidence, to aid on 
categorization. Wacholder (1997) employed an 
aggregation of classification method to capture 
internal rules. Both used hand-written rules and 
knowledge bases. Later, Collins (1999) adopted 
the AdaBoost algorithm to find a weighted 
combination of simple classifiers. They reported 
that the combination of simple classifiers can 
yield some powerful systems with much better 
performances. As a matter of fact, these methods 
all need manual studies on the construction of the 
rule set or the simple classifiers. 
Machine learning models attract more 
attentions recently. Usually, they train 
classification models based on context features. 
Various lexical and syntactic features are 
considered, such as N-grams, Part-Of-Speech 
(POS), and etc. Zhou and Su (2002) integrated 
four different kinds of features, which convey 
different semantic information, for a 
classification model based on the Hidden 
Markov Model (HMM). Koen (2006) built a 
classifier with the Conditional Random Field 
(CRF) model to classify noun phrases in a text 
with the WordNet SynSet. Isozaki and Kazawa 
(2002) studied the use of SVM instead. 
There were fewer studies in Chinese entity 
categorization. Guo and Jiang (2005) applied 
Robust Risk Minimization to classify the named 
entities. The features include seven traditional 
lexical features and two external-NE-hints based 
features. An important result they reported is that 
character-based features can be as good as word-
based features since they avoid the Chinese word 
segmentation errors. In (Jing et al, 2003), it was 
further reported that pure character-based models 
can even outperform word-based models with 
character combination features.  
Deep Belief Net is introduced in (Hinton et al, 
2006). According to their definition, DBN is a 
deep neural network that consists of one or more 
Restricted Boltzmann Machine (RBM) layers 
and a Back Propagation (BP) layer. This multi-
layer structure leads to a strong representation 
power of DBN. Moreover, DBN is quite efficient 
by using RBM to implement the middle layers, 
since RBM can be learned very quickly by the 
Contrastive Divergence (CD) approach. 
Therefore, we believe that DBN is very suitable 
for the character-level Chinese entity mention 
categorization approach. It can be used to solve 
the multi-class categorization problem with just 
simple binary features as the input. 
3 Deep Belief Network for Chinese 
Entity Categorization 
3.1 Problem Formalization 
An Entity mention categorization is a process of 
classifying the entity mentions into different 
categories. In this paper, we assume that the 
entity mentions are already correctly detected 
from the texts. Moreover, an entity mention 
should belong to one and only one predefined 
category. Formally, the categorization function 
of the name entities is 
( ( ))if V e C?            (1) 
where 
ie  is an entity mention from all the 
mention set E, ( )iV e  is the binary feature 
vector of 
ie , C={C1, C2, ?, CM} is the pre-
defined categories. Now the question is to find a 
classification function : Df R C?  which maps 
the feature vector V(ei) of an entity mention to its 
category. Generally, this classification function 
is learned from training data consisting of entity 
mentions with labeled categories. The learned 
function is then used to predict the category of 
new entity mentions by their feature vectors. 
3.2 Character-based Features 
As mentioned in the introduction, we intend to 
use character-level features for the purpose of 
avoiding the impact of the Chinese word 
segmentation errors. Denote the character 
dictionary as D={d1, d2, ?, dN}. To an e, it?s 
feature vector is V(e)={ v1, v2, ?, vN }. Each unit 
vi can be valued as Equation 2. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (2) 
103
For example, there is an entity mention ??
? ?Clinton?. So its feature vector is a vector 
with the same length as the character dictionary, 
in which all the dimensions are 0 except the three 
dimensions standing for ?, ?, and ?. The 
representation is clearly illustrated in Figure 1 
below. Since our objective is to test the 
effectiveness of DBN for this task. Therefore, we 
do not involve any other feature. 
 
Fig. 1. Generating the character-level features 
Characters compose the named entity and 
express its meaning. As a matter of fact, the 
composite effect of the characters to the 
mention category is quite complicated. For 
example, ?? ?Mr. Li? and ?? ?Laos? both 
have character ?, but ?? ?Mr. Li? indicates 
a person but ?? ?Laos? indicates a country. 
These are totally different NEs. Another 
example is ????? ?Capital of Paraguay? 
and ??? ?Asuncion?. They are two entity 
mentions point to the same entity despite that 
the two entities do not have any common 
characters. In such case, independent character 
features are not sufficient to determine the 
categories of the entity mentions. So we should 
also introduce some features which are able to 
represent the combinational effects of the 
characters. However, such kind of features is 
very hard to discover. Meanwhile, a complete 
set of combinations is nearly impossible to be 
found manually due to the exponential number 
of all the possible combinations. As in our 
study, we adopt DBN to automatically find the 
character combinations.  
3.3 Deep Belief Nets 
Deep Belief Network (DBN) is a complicated 
model which combines a set of simple models 
that are sequentially connected (Ackley, 1985). 
This deep architecture can be viewed as multiple 
layers. In DBN, upper layers are supposed to 
represent more ?abstract? concepts that explain 
the input data whereas lower layers extract ?low-
level features? from the data. DBN often consists 
of many layers, including multiple Restricted 
Boltzmann Machine (RBM) layers and a Back 
Propagation (BP) layer.  
 
Fig. 2.  The structure of a DBN. 
As illustrated in Figure 2, when DBN receives 
a feature vector, the feature vector is processed 
from the bottom to the top through several RBM 
layers in order to get the weights in each RBM 
layer, maintaining as many features as possible 
when they are transferred to the next layer. RBM 
deals with feature vectors only and omits the la-
bel information. It is unsupervised. In addition, 
each RBM layer learns its parameters indepen-
dently. This makes the parameters optimal for 
the relevant RBM layer but not optimal for the 
whole model. To solve this problem, there is a 
supervised BP layer on top of the model which 
fine-tunes the whole model in the learning 
process and generates the output in the inference 
process. After the processing of all these layers, 
the final feature vector consists of some sophisti-
cated features, which reflect the structured in-
formation among the original features. With this 
new feature vector, the classification perfor-
mance is better than directly using the original 
feature vector. 
None of the RBM is capable of guaranteeing 
that all the information conveyed to the output is 
accurate or important enough. However the 
learned information produced by preceding RBM 
layer will be continuously refined through the 
next RBM layer to weaken the wrong or insigni-
ficant information in the input. Each layer can 
detect feature in the relevant spaces. Multiple 
layers help to detect more features in different 
spaces. Lower layers could support object detec-
tion by spotting low-level features indicative of 
object parts. Conversely, information about ob-
jects in the higher layers could resolve lower-
level ambiguities. The units in the final layer 
share more information from the data. This in-
creases the representation power of the whole 
model. It is certain that more layers mean more 
computation time. 
104
DBN has some attractive features which make 
it very suitable for our problem. 
1) The unsupervised process can detect the 
structures in the input and automatically ob-
tain better feature vectors for classification. 
2) The supervised BP layer can modify the 
whole network by back-propagation to im-
prove both the feature vectors and the classi-
fication results. 
3) The generative model makes it easy to in-
terpret the distributed representations in the 
deep hidden layers. 
4) This is a fast learning algorithm that can 
find a fairly good set of parameters quickly 
and can ensure the efficiency of DBN. 
3.3.1 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which is 
the core component of DBN. RBM is Boltzmann 
Machine with no connection within the same 
layer. An RBM is constructed with one visible 
layer and one hidden layer. Each visible unit in 
the visible layer V  is an observed variable iv  
while each hidden unit in the hidden layer H  is 
a hidden variable 
jh
. Its joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (3) 
In RBM, the parameters that need to be esti-
mated are ( , , )W b c? ?  and 2( , ) {0,1}v h ? . 
To learn RBM, the optimum parameters are 
obtained by maximizing the above probability on 
the training data (Hinton, 1999). However, the 
probability is indeed very difficult in practical 
calculation. A traditional way is to find the gra-
dient between the initial parameters and the re-
spect parameters. By modifying the previous pa-
rameters with the gradient, the expected parame-
ters can gradually approximate the target para-
meters as 
0
( 1) ( ) ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (4) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging to 
the target. 
Traditionally, the Markov chain Monte Carlo 
method (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(5) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the av-
erage over the data states and its relevant sample 
in hidden unit. 
h v? ?
 denotes the multiplication 
of the average over the model states in visible 
unit and its relevant sample in hidden unit. 
However, MCMC requires estimating an ex-
ponential number of terms. Therefore, it typically 
takes a long time to converge to 
h v? ?
. Hinton 
(2002) introduced an alternative algorithm, i.e., 
the contrastive divergence (CD) algorithm, as a 
substitution. It is reported that CD can train the 
model much more efficiently than MCMC. To 
estimate the distribution ( )p x , CD considers a 
series of distributions { ( )np x } which indicate the 
distributions in n steps. It approximates the gap 
of two different Kullback-Leiler divergences 
(Kullback, 1987) as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (6) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution p?  
defined by the model. In each step, the gap is 
approximately minimized so that we can obtain 
the final distribution which has the smallest 
Kullback-Leiler divergence with the fantasy dis-
tribution.  
After n steps, the gradient can be estimated 
and used in Equation 4 to adjust the weights of 
RBM. In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust the 
weight of RBM. In this case, the estimate of the 
gradient is just the gap between the products of 
the visual layer and the hidden layer, i.e., 
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (7) 
Figure 3 below illustrates the process of learning 
RBM with CD-based gradient estimation. 
 
105
Fig. 3.  Learning RBM with CD-based gradient 
estimation 
3.3.2 Back-propagation (BP) 
The RBM layers provide an unsupervised analy-
sis on the structures of data set. They automati-
cally detect sophisticated feature vectors. The 
last layer in DBN is the BP layer. It takes the 
output from the last RBM layer and applies it in 
the final supervised learning process. In DBN, 
not only is the supervised BP layer used to gen-
erate the final categories, but it is also used to 
fine-tune the whole network. Specifically speak-
ing, when the parameters in BP layer are 
changed during its iterating process, the changes 
are passed to the other RBM layers in a top-to-
bottom sequence. 
The BP algorithm has a feed-forward step and 
a back-propagation step. In the feed-forward step, 
the input values are propagated to obtain the out-
put values. In the back-propagation step, the out-
put values are compared to the real category la-
bels and used them to modify the parameters of 
the model. We consider the weight
ijw  
which 
indicates the edge pointing from the i-th node in 
one RBM layer to the j-th node in its upper layer. 
The computation in feed-forward is 
i ijo w , 
where 
io  is the stored output for the unit i. In 
the back-propagation step, we compute the error 
E in the upper layers and also the gradient with 
respect to this error, i.e., 
i ijE o w? ?
. Then the 
weight
ijw  
will be adjusted by the gradient des-
cent. 
ij i i j
i ij
Ew o oo w? ? ?
?? ? ? ? ??
 (8) 
where ??  is used to control the length of the 
moving step. 
3.3.3 DBN-based Entity Mention Categori-
zation 
For each entity mention, it is represented by the 
character feature vector as introduced in section 
3.2 and then fed to DBN. The training procedure 
can be divided into two phases. The first phase is 
the parameter estimation process of the RBMs on 
all the inputted feature vectors. When a feature 
vector is fed to DBN, the first RBM layer is 
adjusted automatically according to this vector. 
After the first RBM layer is ready, its output 
becomes the input of the second RBM layer. The 
weights of the second RBM layer are also 
adjusted. The similar procedure is carried out on 
all the RBM layers. Then DBN will operates in 
the second phase, the back-propagation 
algorithm. The labeled categories of the entity 
mention are used to tune the parameters of the 
BP layer. Moreover, the changes of the BP layer 
are also fed back to the RBM layers. The 
procedure will iterate until the terminating 
condition is met. It can be a fixed number of 
iterations or a pre-given precision threshold. 
Once the weights of all the layers in DBN are 
obtained, the estimated model could be used to 
prediction. 
 
Fig. 4.  The mention categorization process 
of DBN 
Figure 4 illustrates the classification process of 
DBN. In prediction, for an entity mention e, we 
first calculate its feature vector V(e) and used as 
the input of DBN. V(e) is passed through all the 
layers to get the outputs for all RBM layers and 
last back-propagation layer. In the ith RBM layer, 
the dimensions in the input vector Vinput_i(e) are 
combined to yield the dimensions of the next 
feature vector Voutput_i(e) as input of the next layer. 
After the feature vector V(e) goes through all the 
RBM layers, it is indeed transformed to another 
feature vector V?(e) which consists of 
complicated combinations of the original 
character features and contains rich structured 
information between the characters. This feature 
vector is then fed into the BP layer to get the 
final category c(e). 
4 Experiments 
4.1 Experiment Setup 
In our experiment, we use the ACE 2004 corpus 
to evaluate our approach. The objective of this 
study is that the correctly detected Chinese entity 
mentions categorization using DBN from the text 
and figure out the suitability of DBN on this task. 
Moreover, an entity mention should belong to 
one and only one category. 
106
According to the guideline of the ACE04 task, 
there are five categories for consideration in total, 
i.e., Person, Organization, Geo-political entity, 
Location, and Facility. Moreover, each entity 
mention is expressed in two forms, i.e., the head 
and the extent. For example, ??????? 
?President Clinton of USA? is the extent of an 
entity mention and ???  ?Clinton? is the 
corresponding head. The two phrases both point 
to a named entity whose name is Clinton and he 
is the president of USA.  Here we make the 
?breakdown? strategy mentioned in Li et al 
(2007) that only the entity head is considered to 
generate the feature vector, considering that the 
information from the entity head refines the 
name entity. Although the entity extent includes 
more information, it also brings many noises 
which may make the learning process much 
more difficult. 
   In our experiments, we test the machine 
learning models under a 4-flod cross-validation. 
All entity mentions are divided into four parts 
randomly where three parts are used for training 
and one for test. In total, 7746 mentions are used 
for training and 2482 mentions are used for 
testing at each round. Precision is chosen as the 
evaluation criterion, calculated by the proportion 
of the number of correctly categorized instances 
and the number of total instances. Since all the 
instances should be classified, the recall value is 
equal to the precision value. 
4.2 Evaluation on Named Entity categoriza-
tion 
First of all, we provide some statistics of the data 
set. The distribution of entity mentions in each 
category is given in table 1. The size of the 
character dictionary in the corpus is 1185, so 
does the dimension of each feature vector. 
Type Quantity 
Person 4197 
Organization 1783 
Geo-political entity 287 
Location 3263 
Facility 399 
Table 1.  Number of entity mentions in each 
category 
In the first experiment, we compare the 
performance of DBN with some popular 
classification algorithms, including Support 
Vector Machine (labeled by SVM) and a 
traditional BP neutral network (labeled by NN 
(BP)). To implement the models, we use the 
LibSVM toolkit1 for SVM and the neural neutral 
network toolbox in Matlab2 for BP. The DBN in 
this experiment includes two RBM layers and 
one BP layer. Results of the first experiment are 
given in Table 2.  
Learning Model Precision 
DBN 91.45% 
SVM 90.29% 
NN(BP) 87.23% 
Table 2.  Performances of the systems with 
different classification models 
In this experiment, the DBN has three RBM 
layers and one BP layer. And the numbers of 
units in each RBM layer are 900, 600 and 300 
respectively. NN (BP) has the same structure as 
DBN. As for SVM, we choose the linear kernel 
with the penalty parameter C=1 and set the other 
parameters as default after comparing different 
kernels and parameters. 
In the results, DBN achieved better 
performance than both SVM and BP neural 
network. This clearly proved the advantages of 
DBN. The deep architecture of DBN yields 
stronger representation power which makes it 
able to detect more complicated and efficient 
features, thus better performance is achieved.  
In the second experiment, we intend to 
examine the performance of DBN with different 
number of RBM layers, from one RBM layer 
plus one BP layer to three RBM layers plus one 
BP layer. The amount of the units in the first 
RBM layer is set 900 and the amount in the 
second RBM layer is 600, if the second layer 
exists. As for the third RBM layers, the amount 
of units is set to 300. 
Construction of Neural Network Precision 
Three RBMs and One BP 91.45% 
Two RBMs and One BP 91.42% 
One RBM and one BP 91.05% 
Table 3.  Performance of DBNs with different 
number s of RBM layers 
Results in Table 3 show that the performance 
tends to be better when more RBM layers are 
incorporated. More RBM layers do enhance the 
representation power of DBN. However, it is 
also noted that the improvement is not significant 
from two layers to three layers. The reason may 
                                                 
1 available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
2 available at 
http://www.mathworks.com/access/helpdesk/help/toolbox
/nnet/backprop.html 
107
be that two-RBM DBN already has enough 
representation power for modeling this data set 
and thus one more RBM layer brings 
insignificant improvement. It is also mentioned 
in Hinton (2006) that more than three RBM 
layers are indeed not necessary. Another 
important result in Table 3 is that the DBN with 
One RBM and one BP performs much better than 
the neutral network with only BP in Table 1. 
This clearly showed the effectiveness of feature 
combination by the RBM layer again. 
As to the amount of units in each RBM layer, 
it is manually fixed in upper experiments. This 
number certainly affects the representation 
power of an RBM layer, consequently the 
representation power of the whole DBN. In this 
set of experiment, we intend to study the 
effectiveness of the unit size to the performance 
of DBN. A series of DBNs with only one RBM 
layer and different unit numbers for this RBM 
layer is evaluated. The results are provided in 
Table 4 below. 
Construction of Neural Network Precision 
one RBM(300 units) + one BP 90.61% 
one RBM(600 units) + one BP 90.69% 
one RBM(900 units) + one BP 91.05% 
one RBM(1200 units) + one BP 90.98% 
one RBM(1500 units) + one BP 90.61% 
one RBM(1800 units) + one BP 90.57% 
Table 4.  Performance of One-RBM DBNs 
with different number of units 
Based on the results, we can see that the 
performance is quite stable with different unit 
numbers. But the numbers that are closer to the 
original feature size seem to be some better. This 
could suggest that we should not decrease or 
increase the dimension of the vector feature too 
much when casting the vector transformation by 
RBM layers. 
Finally, we show the results of the individual 
categories. For each category, the Precision-
Recall-F values are provided in table 5, in which 
the F-measure is calculated by 
2*Precision*Recall-measure= Precision+RecallF
    (9) 
Type P R F 
Person 91.26% 96.26% 93.70% 
Organization 89.86% 89.04% 89.45% 
Location 77.58% 59.21% 76.17% 
Geo-political 
entity 
93.60% 91.89% 92.74% 
Facility 77.43% 63.72% 69.91% 
Table 5.  Performances of the system on each 
category 
5 Conclusions 
In this paper we presented our recent work on 
applying a novel machine learning model, the 
Deep Belief Nets, on Chinese entity mention 
categorization. It is demonstrated that DBN is 
very suitable for character-level mention 
categorization approaches due to its strong 
representation power and the ability on 
discovering complicated feature combinations. 
We conducted a series of experiments to prove 
the benefits of DBN. Experimental results 
clearly showed the advantages of DBN that it 
obtained better performance than existing 
approaches such as SVM and traditional BP 
neutral network. 
References  
David Ackley, Geoffrey Hinton, and Terrence 
Sejnowski. 1985. A learning algorithm for 
Boltzmann machines. Cognitive Science. 9. 
David MacDonald. 1993. Internal and external 
evidence in the identification and semantic 
categorization of proper names. Corpus 
Processing for Lexical Acquisition, MIT Press, 61-
76. 
Geoffrey Hinton. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Geoffrey Hinton. 2002. Training products of experts 
by minimizing contrastive divergence. Neural 
Computation, 14, 1771?1800. 
Geoffrey Hinton, Simon Osindero, and Yee-Whey 
Teh. 2006. A fast learning algorithm for deep 
belief nets. Neural Computation. 18, 1527?1554 . 
GuoDong Zhou and Jian Su. 2002. Named entity 
recognition using an hmm-based chunk tagger. In 
proceedings of ACL. 473-480. 
Hideki Isozaki and Hideto Kazawa. 2002. Efficient 
support vector classifiers for named entity 
recognition. In proceedings of IJCNLP. 1-7. 
Honglei Guo, Jianmin Jiang, Guang Hu and Tong 
Zhang. 2005. Chinese named entity recognition 
based on multilevel linguistics features. In 
pr ceedings of IJCNLP. 90-99. 
Jing, Hongyan, Radu Florian, Xiaoqiang Luo, Tong 
Zhang and Abraham Ittycheriah. 2003. How to get 
a Chinese name (entity): Segmentation and 
combination issues. In proceedings of EMNLP. 
200-207. 
Koen Deschacht and Marie-Francine Moens. 2006, 
Efficient Hierarchical Entity Classifier Using 
Conditional Random Field. In Proceedings of the 
108
2nd Workshop on Ontology Learning and 
Population. 33-40. 
Michael Collins and Yoram Singer. 1999. 
Unsupervised models for named entity 
classification. In Proceedings of EMNLP'99. 
Nina Wacholder, Yael Ravin and Misook Choi. 1997. 
Disambiguation of Proper Names in Text. In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing. 
Solomon Kullback. 1987. Letter to the Editor: The 
Kullback-Leibler distance. The American 
Statistician 41 (4): 340?341. 
Wenjie Li and Donglei Qian. 2007. Detecting, 
Categorizing and Clustering Entity Mentions in 
Chinese Text, in Proceedings of the 30th Annual 
International ACM SIGIR Conference (SIGIR?07). 
647-654. 
Yoshua Bengio and Yann LeCun. 2007. Scaling 
learning algorithms towards ai. Large-Scale Ker-
nel Machines. MIT Press. 
 
109
Exploring Deep Belief Network for Chinese Relation Extraction 
Yu Chen1, Wenjie Li2, Yan Liu2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{cswjli, csyliu}@comp.polyu.edu.hk 
Abstract 
Relation extraction is a fundamental 
task in information extraction that 
identifies the semantic relationships 
between two entities in the text. In this 
paper, a novel model based on Deep 
Belief Network (DBN) is first 
presented to detect and classify the 
relations among Chinese entities. The 
experiments conducted on the 
Automatic Content Extraction (ACE) 
2004 dataset demonstrate that the 
proposed approach is effective in 
handling high dimensional feature 
space including character N-grams, 
entity types and the position 
information. It outperforms the state-
of-the-art learning models such as 
SVM or BP neutral network. 
1 Introduction 
Information Extraction (IE) is to automatically 
pull out the structured information required by 
the users from a large volume of plain text. It 
normally includes three sequential tasks, i.e., 
entity extraction, relation extraction and event 
extraction. In this paper, we limit our focus on 
relation extraction.  
In early time, pattern-based approaches were 
the main focus of most research studies in 
relation extraction. Although pattern-based 
approaches achieved reasonably good results, 
they have some obvious flaws. It requires 
expensive handcraft work to assemble patterns 
and not all relations can be identified by a set 
of reliable patterns (Willy Yap, 2009). Also, 
once the interest of task is transferred to a 
different domain or a different language, 
patterns have to be revised or even rewritten. 
That is to say, the discovered patterns are 
heavily dependent on the task in a specific 
domain or on a particular corpus. 
Naturally, a vast amount of work was spent 
on feature-based machine learning approaches 
in later years. In this camp, relation extraction 
is typically cast as a classification problem, 
where the most important issue is to train a 
model to scale and measure the similarity of 
features reflecting relation instances. The 
entity semantic information expressing relation 
was often formulated as the lexical and 
syntactic features, which are identical to a 
certain linear vector in high dimensions. Many 
learning models are capable of self-training 
and classifying these vectors according to 
similarity, such as Support Vector Machine 
(SVM) and Neural Network (NN).  
Recently, kernel-based approaches have 
been developing rapidly. These approaches 
involved kernels of structure representations, 
like parse tree or dependency tree, in similarity 
calculation. In fact, feature-based approaches 
can be viewed as the special and simplified 
kinds of kernel-based approaches. They used 
dot-product as the kernel function and did not 
range over the intricate structure information 
(Ji, et al 2009). 
Relation extraction in Chinese received 
quite limited attention as compared to English 
and other western languages. The main reason 
is the unique characteristic of Chinese, such as 
more flexible grammar, lack of boundary 
information and morphological variations etc 
(Sun and Dong, 2009). Especially, the existing 
Chinese syntactic analysis tools at current 
stage are not yet reliable to capture the 
valuable structured information. It is urgent to 
develop approaches that are in particular 
suitable for Chinese relation extraction. 
In this paper, we explore the use of Deep 
Belief Network (DBN), a new feature-based 
machine learning model for Chinese relation 
extraction. It is a neural network model 
developed under the deep learning architecture 
that is claimed by Hinton (2006) to be able to 
automatically learn a deep hierarchy of 
features with increasing levels of abstraction 
for the complex problems like natural language 
processing (NLP). It avoids assembling 
patterns that express the semantic relation 
information and meanwhile it succeeds to 
produce accurate model that is not confined to 
the parsing results.  
The rest of this paper is structured in the 
following manner. Section 2 reviews the 
previous work on relation extraction. Section 3 
presents task definition, briefly introduces the 
DBN model and the feature construction. 
Section 4 provides the experimental results. 
Finally, Section 5 concludes the paper.  
2 Related Work 
Over the past decades, relation extraction had 
come to a significant progress from simple 
pattern-based approaches to adapted self-
training machine learning approaches. 
Brin (1998) used Dual Iterative Pattern 
Relation Expansion, a bootstrapping-based 
system, to find the largest common substrings 
as patterns. It had the ability of searching 
patterns automatically and was good for large 
quantity of uniform contexts. Chen (2006) 
proposed graph algorithm called label 
propagation, which transferred the pattern 
similarity to probability of propagating the 
label information from any vertex to its nearby 
vertices. The label matrix indicated the relation 
type. 
Feature-based approaches utilized the linear 
vector of carefully chosen lexical and syntactic 
features derived from different levels of text 
analysis and ranging from part-of-speech (POS) 
tagging to full parsing and dependency parsing 
(Zhang 2009). Jing and Zhai (2007) defined a 
unified graphic representation of features that 
served as a general framework in order to 
systematically explore the information at 
diverse levels in three subspaces and finally 
estimated the effectiveness of these features. 
They reported that the basic unit feature was 
generally sufficient to achieve state-of-art 
performance. Meanwhile, over-inclusion 
complex features were harmful. 
Kernel-based approaches utilize kernel 
functions on structures between two entities, 
such as sequences and trees, to measure the 
similarity between two relation instances. 
Zelenok (2003) applied parsing tree kernel 
function to distinguish whether there was an 
existing relationship between two entities. 
However, they limited their task on Person-
affiliation and organization-location.  
The previous work mainly concentrated on 
relation extraction in English. Relatively, less 
attention was drawn on Chinese relation 
extraction. However, its importance is being 
gradually recognized. For instance, Zhang et al 
(2008) combined position information, entity 
type and context features in a feature-based 
approach and Che (2005) introduced the edit 
distance kernel over the original Chinese string 
representation.  
DBN is a new feature-based approach for 
NLP tasks. According to the work by Hinton 
(2006), DBN consisted of several layers 
including multiple Restricted Boltzmann 
Machine (RBM) layers and a Back 
Propagation (BP) layer. It was reported to 
perform very well in many classification 
problems (Ackley, 1985), which is from the 
origin of its ability to scale gracefully and be 
computationally tractable when applied to high 
dimensional feature vectors. Furthermore, to 
against the combinations of feature were 
intricate, it detected invariant representations 
from local translations of the input by deep 
architecture.  
3 Deep Belief Network for Chinese 
Relation Extraction 
3.1 Task Definition 
Relation extraction, promoted by the 
Automatic Content Extraction (ACE) program, 
is a task of finding predefined semantic 
relations between pairs of entities from the 
texts. According to the ACE program, an entity 
is an object or a set of objects in the world 
while a relation is an explicitly or implicitly 
stated relationship between entities. The task 
can be formalized as:  
1 2( , , )e e s r?       (1) 
where 
1e  and 2e  are the two entities in a 
sentence s  under concern and r  is the relation 
between them. We call the triple 
1 2( , , )e e s  the 
relation candidate. According to the ACE 2004 
guideline 1 , five relation types are defined. 
They are: 
Role: it represents an affiliation between a 
Person entity and an Organization, Facility, 
or GPE (a Geo-political entity) entities. 
Part: it represents the part-whole relationship 
between Organization, Facility and GPE 
entities. 
At: it represents that a Person, Organization, 
GPE, or Facility entity is location at a 
Location entities. 
Near: it represents the fact that a Person, 
Organization, GPE or Facility entity is near 
(but not necessarily ?At?) a Location or 
GPE entities. 
Social: it represents personal and professional 
affiliations between Person entities. 
3.2 Deep Belief Networks (DBN) 
DBN often consists of several layers, 
including multiple RBM layers and a BP layer. 
As illustrated in Figure 1, each RBM layer 
learns its parameters independently and 
unsupervisedly. RBM makes the parameters 
optimal for the relevant RBM layer and detect 
complicated features, but not optimal for the 
whole model. There is a supervised BP layer 
on top of the model which fine-tunes the whole 
model in the learning process and generates the 
output in the inference process. RBM keeps 
information as more as possible when it 
transfers vectors to next layer. It makes 
networks to avoid local optimum. RBM is also 
adopted to ensure the efficiency of the DBN 
model. 
 
Fig. 1.  The structure of a DBN. 
                                                 
1 available at http://www.nist.gov/speech/tests/ace/. 
Deep architecture of DBN represents many 
functions compactly. It is expressible by 
integrating different levels of simple functions 
(Y. Bengio and Y. LeCun). Upper layers are 
supposed to represent more ?abstract? concepts 
that explain the input data whereas lower 
layers extract ?low-level features? from the 
data. In addition, none of the RBM guarantees 
that all the information conveyed to the output 
is accurate or important enough. The learned 
information produced by preceding RBM layer 
will be continuously refined through the next 
RBM layer to weaken the wrong or 
insignificant information in the input. Multiple 
layers filter valuable features. The units in the 
final layer share more information from the 
data. This increases the representation power 
of the whole model. The final feature vectors 
used for classification consist of sophisticated 
features which reflect the structured 
information, promote better classification 
performance than direct original feature vector.  
3.3 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which 
is the core component of DBN. RBM is 
Boltzmann Machine with no connection within 
the same layer. An RBM is constructed with 
one visible layer and one hidden layer. Each 
visible unit in the visible layer V  is an 
observed variable 
iv  while each hidden unit in 
the hidden layer H  is a hidden variable 
jh
. Its 
joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (2) 
In RBM, 2( , ) {0,1}v h ? and ( , , )W b c? ? are 
the parameters that need to be estimated?W  
is the weight tying visible layer and hidden 
layer. b is the bias of units v and c is the bias of 
units h. 
To learn RBM, the optimum parameters are 
obtained by maximizing the joint distribution 
( , )p v h  on the training data (Hinton, 1999). A 
traditional way is to find the gradient between 
the initial parameters and the expected 
parameters. By modifying the previous 
parameters with the gradient, the expected 
parameters can gradually approximate the 
target parameters as 
0
( 1) ( ) log ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (3) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging 
to the target. 
Traditionally, the Monte Carlo Markov 
chain (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(4) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the 
average over the data states and its relevant 
sample in hidden unit. 
h v? ?
 denotes the 
multiplication of the average over the model 
states in visible units and its relevant sample in 
hidden units. 
  
Fig. 2.  Learning RBM with CD-based 
gradient estimation 
However, MCMC requires estimating an 
exponential number of terms. Therefore, it 
typically takes a long time to converge to 
h v? ?
. Hinton (2002) introduced an alternative 
algorithm, i.e., the contrastive divergence (CD) 
algorithm, as a substitution. It is reported that 
CD can train the model much more efficiently 
than MCMC. To estimate the distribution ( )p x , 
CD considers a series of distributions { ( )np x } 
which indicate the distributions in n steps. It 
approximates the gap of two different 
Kullback-Leiler divergences as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (5) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution 
p?  defined by the model.  
In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust 
the weight of RBM as Equation 6.  
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (6) 
Figure 2 below illustrates the process of 
learning RBM with CD-based gradient 
estimation. 
3.4 Back-Propagation (BP) 
The RBM layers provide an unsupervised 
analysis on the structures of data set. They 
automatically detect sophisticated feature 
vectors. The last layer in DBN is the BP layer. 
It takes the output from the last RBM layer and 
applies it in the final supervised learning 
process. In DBN, not only is the supervised BP 
layer used to generate the final categories, but 
it is also used to fine-tune the whole network. 
Specifically speaking, when the BP layer is 
changed during its iterating process, the 
changes are passed to the other RBM layers in 
a top-to-bottom sequence. 
3.5 The Feature Set 
DBN is able to detect high level hidden 
features from lexical, syntactic and/or position 
characteristic. As mentioned in related work, 
over-inclusion complex features are harmful. 
We therefore involve only three kinds of low 
level features in this study. They are described 
below. 
3.5.1 Character-based Features 
Since Chinese text is written without word 
boundaries, the word-level features are limited 
by the efficiency of word segmentation results. 
In the paper presented by H. Jing (2003) and 
some others, they observed that pure character-
based models can even outperform word-based 
models. Li et al?s (2008) work relying on 
character-based features also achieved 
significant performance in relation extraction. 
We denote the character dictionary as D={d1, 
d2, ?, dN}. In our experiment, N is 1500. To 
an e, it?s character-based feature vector is 
V(e)={ v1, v2, ?, vN }. Each unit vi can be 
valued as Equation 8. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (7) 
3.5.2 Entity Type Features 
According to the ACE 2004 guideline, there 
are five entity types in total, including Person, 
Organization, GPE, Location, and Facility. We 
recognize and classify the relation between the 
recognized entities. The entities in ACE 2004 
corpus were labeled with these five types. 
Type features are distinctive for classification. 
For example, the entities of Location cannot 
appear in the Role relation.  
3.5.3 Relative Position Features 
We define three types of position features 
which depict the relative structures between 
the two entities, including Nested, Adjacent 
and Separated. For each relation candidate 
triple 
1 2( , , )e e s , let .starte  and .ende  denote 
the starting and end positions of e  in a 
document. Table 1 summarizes the conditions 
for each type, where }2,1{, ?ji  and ji ? .  
Type Condition 
Nested ( .start, .end) ( .start, .end)i i j je e e e?
 
Adjacent .end= .start-1i je e
 
Separated ( .start< .start)&( .end+1< .start)i j i je e e e
 
Table 1. The internal postion structure features 
between two named entities 
We combine the character-based features of 
two entities, their type information and 
position information as the feature vector of 
relation candidate.  
3.6 Order of Entity Pair 
A relation is basically an order pair. For 
example, ?Bank of China in Hong Kong? 
conveys the ACE-style relation ?At? between 
two entities ?Bank of China (Organization)? 
and ?Hong Kong (Location)?. We can say that 
Bank of China can be found in Hong Kong, 
but not vice verse. The identified relation is 
said to be correct only when both its type and 
the order of the entity pair are correct. We 
don?t explicitly incorporate such order 
restriction as an individual feature but use the 
specified rules to sort the two entities in a 
relation once the relation type is recognized. 
As for those symmetric relation types, the 
order needs not to be concerned. Either order is 
considered correct in the ACE standard. As for 
those asymmetric relation types, we simply 
select the first (in adjacent and separated 
structure) or outer (in nested structures) as the 
first entity. In most cases, this treatment leads 
to the correct order. We also make use of 
entity types to verify (and rectify if necessary) 
this default order. For example, considering 
?At? is a relation between a Person, 
Organization, GPE, or Facility entity and a 
Location entity, the Location entity must be 
placed after the Person, Organization, GPE, or 
Facility entity in a relation. 
4 Experiments and Evaluations 
4.1 Experiment Setup 
The experiments are conducted on the ACE 
2004 Chinese relation extraction dataset, 
which consists of 221 documents selected from 
broadcast news and newswire reports. There 
are 2620 relation instances and 11800 pairs of 
entities have no relationship in the dataset. The 
size of the feature space is 3017.  
We examine the proposed DBN model 
using 4-fold cross-validation. The performance 
is measured by precision, recall, and F-
measure. 
2*Precision*Recall-measure= Precision+RecallF
    (8) 
In the following experiments, we plan to test 
the effectiveness of the DBN model in three 
ways: 
Detection Only: For each relation candidate, 
we only recognize whether there is a certain 
relationship between the two entities, no 
matter what type of relation they hold.  
Detection and Classification in Sequence: 
For each relation candidate, when it is 
detected to be an instance of relation, it 
proceeds to detect the type of the relation 
the two entities hold. 
Detection and Classification in Combination: 
We define N+1 relation label, N for relation 
types defined by ACE and one for NULL 
indicating there is no relationship between 
the two entities. In this way, the processes 
of detection and classification are combined. 
We will compare DBN with a well-known 
Support Vector Machine model (labeled as 
SVM in the tables) and a traditional BP neutral 
network model (labeled as NN (BP only)). 
Among them, SVM has been successfully 
applied in many classification applications. We 
use the LibSVM toolkit 2  to implement the 
SVM model. 
4.2 Evaluation on Detection Only 
We first evaluate relation detection, where 
only two output classes are concerned, i.e. 
NULL (which means no relation recognized) 
and RELATION. The parameters used in DBN, 
SVM and NN (BP only) are tuned 
experimentally and the results with the best 
parameter settings are presented in Table 2. In 
each of our experiments, we test many 
parameters of SVM and chose the best set of 
that to show below. 
Regarding the structure of DBN, we 
experiment with different combinations of unit 
numbers in the RBM layers. Finally we choose 
DBN with three RBM layers and one BP layer. 
And the numbers of units in each RBM layer 
are 2400, 1800 and 1200 respectively, which is 
the best size of each layer in our experiment. 
Our empirical results showed that the numbers 
of units in adjoining layers should not decrease 
the dimension of feature vector too much when 
casting the vector transformation. NN has the 
same structure as DBN. As for SVM, we 
choose the linear kernel with the penalty 
parameter C=0.3, which is the best penalty 
coefficient, and set the other parameters as 
default after comparing different kernels and 
parameter values.  
Model Precision Recall F-measure 
DBN 67.8% 70.58% 69.16% 
SVM 73.06% 52.42% 61.04% 
NN (BP 
only) 
51.51% 61.77% 56.18% 
Table 2. Performances of DBN, SVM and NN 
models for detection only 
As showed in Table 2, with their best 
parameter settings, DBN performs much better 
                                                 
2 http://www.csie.ntu.edu.tw/~cjlin/libsvm/  
than both SVM and NN (BP only) in terms of 
F-measure. It tells that DBN is quite good in 
this binary classification task. Since RBM is a 
fast approach to approximate global optimum 
of networks, its advantage over NN (BP only) 
is clearly demonstrated in their results.  
4.3 Evaluation on Detection and 
Classification in Sequence 
In the next experiment, we go one step further. 
If a relation is detected, we classified it into 
one of the 5 pre-defined relation types. For 
relation type classification, DBN and NN (BP 
only) have the same structures as they are in 
the first experiment. We adopt SVM linear 
kernel again and set C to 0.09 and other 
parameters as default. The overall performance 
of detection and classification of three models 
are illustrated in Table 3 below. DBN again is 
more effective than SVM and NN. 
Model Precision Recall F-measure 
DBN 63.67% 59% 61.25% 
SVM 67.78% 47.43% 55.81% 
NN  61% 45.62% 52.2% 
Table 3. Performances of DBN and other 
classification models for detection and 
classification in sequence 
4.4 Evaluation on Detection and 
Classification in Combination 
In the third experiment, we unify relation 
detection and relation type classification into 
one classification task. All the candidates are 
directly classified into one of the 6 classes, 
including 5 relation types and a NULL class. 
Parameter settings of the three models in this 
experiment are identical to those in the second 
experiment, except that C in SVM is set to 0.1. 
Model Precision Recall F-measure 
DBN 65.8% 59.15% 62.3% 
SVM 75.25% 44.07% 55.59% 
NN (BP 
only) 
63.2% 45.7% 53.05% 
Table 4. Performances of DBN, SVM and NN 
models for detection and classification in 
combination 
As demonstrated, DBN outperforms both 
SVM and NN (BP only) in all these three 
experiments consistently. In this regard, the 
advantages of DBN over the other two models 
are apparent. RBM approximates expected 
parameters rapidly and the deep DBN 
architecture yields stronger representativeness 
of complicated, efficient features.  
Comparing the results of the second and the 
third experiments, SVM perform better 
(although not quite significantly) when 
detection and classification are in sequence 
than in combination. This finding is consistent 
with our previous work (to be added later). It 
can possibly be that preceding detection helps 
to deal with the severe unbalance problem, i.e. 
there are much more relation candidates that 
don?t hold pre-defined relations. However, 
DBN obtaining the opposite result cause by 
that the amount of examples we have is not 
sufficient for DBN to self-train itself well for 
type classification. We will further exam this 
issue in our feature work. 
4.5 Evaluation on DBN Structure 
Next, we compare the performance of DBN 
with different structures by changing the 
number of RBM layers. All the candidates are 
directly classified into 6 types in this 
experiment.  
DBN  Precision Recall F-measure 
3 RBMs + 
BP 
65.8% 59.15% 62.3% 
2 RBMs + 
BP 
65.22% 57.1% 60.09% 
1 RBM + 
BP 
64.35% 55.5% 59.6% 
Table 5. Performance RBM with different 
layers 
The results provided in Table 5 show that 
the performance can be improved when more 
RBM layers are incorporated. Multiple RBM 
layers enhance representation power. Since it 
was reported by Hinton (2006) that three RBM 
layer is enough to detect the complex features 
and more RBM layer are of less help, we do 
not try to go beyond the three layers in this 
experiment. Note that the improvement is more 
obvious from two layers to three layers than 
from one layer to two layers. 
4.6 Error Analysis 
Finally, we provide the test results for 
individual relation types in Table 6. We can 
see that the proposed model performs better on 
?Role? and ?Part? relations. When taking a 
closer look at their relation instance 
distributions, the instances of these two types 
comprise over 63% percents of all the relation 
instances in the dataset. Clearly their better 
results benefit from the amount of training data. 
It further implies that if we have more training 
data, we should be able to train a more 
powerful DBN. The same characteristic is also 
observed in Table 7 which shows the 
distributions of the identified relations against 
the gold standard.  However, the sizes of ?At? 
relation instances and ?Role? relation instances 
are similar, its result is much worse. We 
believe it is from the origin of that the position 
feature is not distinctive for ?At? relation, as 
shown in Table 8. ?Near? and ?Social? are two 
symmetric relation types. Ideally, they should 
have better results. But due to quite small 
number of training examples, you can see that 
they are actually the types with the worst F-
measure. 
Type Precision Recall F-measure 
Role 65.19% 69.2% 67.14% 
Part 67.86% 71.43% 69.59% 
At 51.15% 60% 55.22% 
Near 15.38% 33.33% 20.05% 
Social 25% 35.71% 29.41% 
Table 6. Performance of DBN for each 
relation type 
 R P A N S Null 
Role (R) 191 1 5 0 0 96 
Part (P) 1 95 12 0 0 32 
At (A) 4 8 111 2 1 91 
Near (N) 0 1 0 2 0 10 
Social (S) 1 0 0 0 5 14 
Table 7. Distribution of the identified relations 
Type Adjacent  Separated Nested  
Role 7 63 223 
Part 1 17 122 
At 21 98 98 
Near 0 8 5 
Social 10 10 10 
           Identified 
Standard 
Table 8.  Statistic of position feature 
The main mistakes observed in Table 7 are 
wrongly classifying a ?Part? relation as a ?At? 
relations. We further inspect these 12 mistakes 
and find that it is indeed difficult to distinct the 
two types for the given entity pairs. Here is a 
typical example: entity 1: ?????  (the 
Democratic Party of the United States, defined 
as an organization entity), entity 2: ?? (the 
United States, defined as a GPE entity). 
Therefore, the major problem we have to face 
is how to effectively recall more relations. 
Given the limited training resources, it is 
needed to well explore the appropriate external 
knowledge or the Web resources. 
5 Conclusions 
In this paper we present our recent work on 
applying a novel machine learning model, 
namely Deep Belief Network, to Chinese 
relation extraction. DBN is demonstrated to 
be effective for Chinese relation extraction 
because of its strong representativeness. We 
conduct a series of experiments to prove the 
benefits of DBN. Experimental results clearly 
show the strength of DBN which obtains 
better performance than other existing models 
such as SVM and the traditional BP neutral 
network. In the future, we will explore if it is 
possible to incorporate the appropriate 
external knowledge in order to recall more 
relation instances, given the limited training 
resource. 
References 
Ackley D., Hinton G. and Sejnowski T. 1985. A 
learning algorithm for Boltzmann machines, 
Cognitive Science, 9. 
Brin Sergey. 1998. Extracting patterns and relations 
from world wide web, In Proceedings of 
WebDB Workshop at 6th International 
Conference on Extending Database 
Technology (WebDB?98), 172-183. 
Che W.X. Improved-Edit-Distance Kernel for 
Chinese Relation Extraction, In Dale, R.,Wong, 
K.-F., Su, J., Kwong, O.Y. (eds.) IJCNLP 
2005.LNCS(LNAI). vol. 2651. 
H. Jing, R. Florian, X. Luo, T. Zhang, A. 
Ittycheriah. 2003. How to get a Chinese name 
(entity): Segmentation and combination issues. 
In proceedings of EMNLP. 200-207. 
Hinton, G.. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Hinton, G. E. 2002. Training products of experts by 
minimizing contrastive divergence, Neural 
Computation, 14(8), 1711?1800. 
Hinton G. E., Osindero S. and Teh Y. 2006. A fast 
learning algorithm for deep belief nets, Neural 
Computation, 18. 1527?1554. 
Ji Zhang, You Ouyang, Wenjie Li and Yuexian 
Hou. 2009. A Novel Composite Kernel 
Approach to Chinese Entity Relation 
Extraction. in Proceedings of the 22nd 
International Conference on the Computer 
Processing of Oriental Languages, Hong Kong, 
pp240-251. 
Ji Zhang, You Ouyang, Wenjie Li, and Yuexian 
Hou. 2009. Proceedings of the 22nd 
International Conference on Computer 
Processing of Oriental Languages. 236-247.  
Jiang J. and Zhai C. 2007. A Systematic 
Exploration of the Feature Space for Relation 
Extraction, In Proceedings of NAACL/HLT, 
113?120. 
Jinxiu Chen, Donghong Ji, Chew L., Tan and 
Zhengyu Niu. 2006. Relation extraction using 
label propagation based semi-supervised 
learning, In Proceedings of ACL?06, 129?136. 
Li W.J., Zhang P., Wei F.R., Hou Y.X. and Lu, Q. 
2008. A Novel Feature-based Approach to 
Chinese Entity Relation Extraction, In 
Proceeding of ACL 2008 (Companion Volume), 
89?92 
Sun Xia and Dong Lehong, 2009. Feature-based 
Approach to Chinese Term Relation Extraction. 
International Conference on Signal Processing 
Systems. 
Willy Yap and Timothy Baldwin. 2009. 
Experiments on Pattern-based Relation 
Learning. Proceeding of the 18th ACM 
conference on Information and knowledge 
management. 1657-1660.  
Y. Bengio and Y. LeCun. 2007. Scaling learning 
algorithms towards ai. Large-Scale Kernel 
Machines. MIT Press. 
Zelenko D. Aone C and Richardella A. 2003. 
Kernel Methods for Relation Extraction, 
Journal of Machine Learning Research 
2003(2), 1083?1106. 
Zhang P., Li W.J., Wei F.R., Lu Q. and Hou Y.X. 
2008. Exploiting the Role of Position Feature 
in Chinese Relation Extraction, In Proceedings 
of the 6th International Conference on 
Language Resources and Evaluation (LREC). 
 
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 52?56,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Syllable-based Machine Transliteration with Extra Phrase Features  
 
 
Chunyue Zhang, Tingting Li, Tiejun Zhao 
MOE-MS Key Laboratory of Natural Language Processing and Speech 
Harbin Institute of Technology 
Harbin,China 
{cyzhang,ttli,tjzhao}@mtlab.hit.edu.cn 
  
 
 
Abstract 
This paper describes our syllable-based phrase 
transliteration system for the NEWS 2012 
shared task on English-Chinese track and its 
back. Grapheme-based Transliteration maps the 
character(s) in the source side to the target 
character(s) directly. However, character-based 
segmentation on English side will cause 
ambiguity in alignment step. In this paper we 
utilize Phrase-based model to solve machine 
transliteration with the mapping between 
Chinese characters and English syllables rather 
than English characters. Two heuristic rule-
based syllable segmentation algorithms are 
applied. This transliteration model also 
incorporates three phonetic features to enhance 
discriminative ability for phrase. The primary 
system achieved 0.330 on Chinese-English and 
0.177 on English-Chinese in terms of top-1 
accuracy. 
1 Introduction  
Machine transliteration, based on the pronunciation, 
transforms the script of a word from a source 
language to a target language automatically.  
  With a continuous growth of out-of-vocabulary 
names to be transliterated, the traditional 
dictionary-based methods are no longer suitable. 
So data-driven method is gradually prevailing now, 
and many new approaches are explored. 
  Knight(1998) proposes a phoneme-based 
approach to solve the transliteration between 
English names and Japanese katakana. It makes 
use of a common phonetic representation as a pivot.  
  The phoneme-based approach needs a 
pronunciation dictionary for one or two languages. 
These dictionaries usually do not exist or can't 
cover all the names. So grapheme-based(Li et al, 
2004) approach has gained lots of attention 
recently. Huang(2011) proposes a novel 
nonparametric Bayesian using synchronous 
adaptor grammars to model the grapheme-based 
transliteration. Zhang(2010) builds the pivot 
transliteration model with grapheme-based method. 
  The hybrid approach tries to utilize both phoneme 
and grapheme information, and usually integrates 
the output of multiple engines to improve 
transliteration. Oh and Choi(2006) integrate both 
phoneme and grapheme features into a single 
leaning framework.  
  As an instance of grapheme-based approach, 
Jia(2009) views machine transliteration as a special 
example of machine translation and uses the 
phrase-based machine translation model to solve it. 
The approach is simple and effective. Our paper 
follows this way. However, using the English 
letters and Chinese characters as basic mapping 
units will make ambiguity in the alignment and 
translation step. One Chinese character usually 
maps one syllable, so syllabifying English words 
can be more discriminative. 
  We present a solution to this ambiguity by 
replacing the English character with an English 
syllable which is consecutive characters and can 
keep some phonetic properties. For this purpose, 
two heuristic and simple syllable segmentation 
algorithms are used to syllabify English side into 
syllables sequence. Besides two above, three extra 
phrase features for transliteration are used to 
enhance the model. 
  The rest of this paper is organized as follows. 
Section 2 introduces the phrase-based model 
briefly. Section 3 describes two rule-based syllable 
52
segmentation methods and three new special 
features for transliteration in detail. Experiments 
and analyses are discussed in section 4. 
Conclusions and future work are addressed in 
section 5. 
2 Phrase-based Machine Transliteration 
Model 
Machine transliteration can be regarded as a 
special instance of machine translation. Jia(2009) 
solves transliteration with phrase-based model 
firstly. There an English character is treated as a 
word in machine translation. On the contrast, 
character is replaced by syllable in this paper. Then 
transliteration can be viewed as a pure translation 
task. The phrase-based machine transliteration can 
be formulated by equation 1. 
?
?
??
n
i
iie
xhxpe
1
~ )(exp)(maxarg ?
           (1) 
? n is the number of features 
? 
i?  is the weight of feature i  
  In our phrase-based transliteration system, the 
following features are used by default: 
? the bidirectional probability between 
source phrase and the target phrase 
? The bidirectional lexical probability 
between source phrase and target phrase  
? the fluency of the output, namely language 
model 
? the length penalty 
3 Syllable Segmentation and Extra 
Phrase Features 
This section describes two rule-based syllable 
segmentation algorithms and three extra phrase 
features added to machine transliteration model. 
3.1 Syllable Segmentation Algorithm  
In (Jia et al, 2009), the basic alignment units are 
English character and Chinese character(called 
c2c). This setup is the simplest format to 
implement the model. However, transliteration 
from English to Chinese usually maps an English 
syllable to a single Chinese character. As one 
Chinese character usually corresponds to many 
English characters, the c2c method has only a 
modest discriminative ability. Obviously 
syllabifying English is more suitable for this 
situation. Yang(2010) utilizes a CRF-based 
segmentor to syllabify English and Kwong(2011) 
syllabifies English with the Onset First Principle. 
Alternatively, inspired by (Jiang, 2007), two 
heuristic rule-based methods are addressed to 
syllabify the English names in this paper.  
  Given an English name E, it can be syllabified 
into a syllable sequence SE = {e1,e2,...,en} with  
one of the following two linguistic methods. 
 
Simple Segmentation Algorithm(SSA): 
1.  {'a', 'o' , 'e', 'i', 'u'} are defined as vowels. 'y' is 
defined as a vowel when it is not followed by a 
vowel; 'r' is defined as a vowel when it follows a 
vowel and is followed by a consonant1. All other 
characters are defined as consonants; this forms the 
basic vowel set; 
2.  A consecutive vowels sequence, formed by the 
basic vowel set, is treated as a new vowel symbol; 
Step 1 and 2 form the new vowel set; 
3.  A consonant and its following vowel are treated 
as a syllable; 
4.  Consecutive consonants are separated; a vowel 
symbol(in the new vowel set) followed by a 
consonant is separated; 
5. The rest isolated characters sequences are 
regarded as individual syllables in each word. 
 
  SSA treats all the consecutive vowels as a single 
new vowel simply. In fact, many consecutive 
vowels like "io" often align two or more Chinese 
characters, such as " zio ?  ?". It is better to 
separate it as two syllables rather than one syllable 
in alignment step. So we present another segment 
algorithm which takes more details into 
consideration.  
 
Fine-grained Segment Algorithm(FSA): 
1.  Replace 'x' in English names with 'k s' firstly; 
2. {'a','o','e','i','u'} are defined as vowels. 'y' is 
defined as a vowel when it is not followed by a 
vowel; 
3.  When 'w' follows 'a','e','o' and isn't followed by 
'h', treat 'w' and the preceding vowel as a new 
vowel symbol; Step 2 and 3 form the basic vowel 
set; 
4. A consecutive vowels sequence which is formed 
by the basic vowel set is treated as a new vowel 
                                                          
1 A review points the SSA lacking of ability to deal with 'h'. 
We leave it for the future work. 
53
symbol, excepting 'iu', 'eo', 'io', 'oi', 'ia', 'ui', 'ua', 
'uo' ; Step 2, 3 and 4 form the new vowel set; 
5. Consecutive consonants are separated; a vowel 
symbol(in the new vowel set) followed by a 
consonant sequence is separated; 
6. A consonant and its following vowel are treated 
as a syllable; the rest of the isolated consonants 
and vowels are regarded as individual syllables in 
each word. 
   
  After segmenting the English characters sequence, 
the new transliteration units, syllables, will be 
more discriminative. 
3.2 Extra phrase features 
The default features of phrase can't express the 
special characteristic of transliteration. We propose 
three features trying to explore the transliteration 
property. 
  Begin and End Feature(BE) 
  When a Chinese character is chosen as the 
corresponding transliteration, its position in the 
transliteration result is important. Such as a 
syllable "zu" that can be transliterate into "?" or "
?" in Chinese while "?" will be preferred if it 
appears at the beginning position.  
  To explore this kind of information, the pseudo 
characters "B" and "E" are added to the train and 
test data. So in the extracted phrase table, "B" 
always precedes the Chinese character that prefers 
at the first position, and "E" always follows the 
Chinese character that appears at the last position. 
  Phrase Length Feature  
  Chinese character can be pronounced according 
to its pinyin format which is written like English 
word. And the longer English syllable is, the 
longer pinyin format it often has. So the length 
information of Chinese character and its pinyin can 
be used to disambiguate the phrase itself. Here we 
definite two new features to address it. Suppose 
<e,c> as a phrase pair, e= {e1,e2,...,em},c = 
{c1,c2,...,cn},ei stands for an English syllable and 
ci stands for a Chinese character. p(ci) is the pinyin 
format of ci. #(ei) is equal to the number of 
characters in a syllable. #p(cj) is equal to the 
number of characters in a pinyin sequence. And 
then, 
L1 = Sum(#(ei)) / Sum(#(p(cj)) 
L2 = m / n 
4 Experiments 
This section describes the data sets, experimental 
setup, experimental results and analyses. 
4.1 Data Sets 
The training set of English-Chinese transliteration 
track contains 37753 pairs of names. We pick up 
3000 pairs from the training data randomly as the 
closed test set and the rest 34753 pairs as our 
training data set. In the official dev set some 
semantic translation pairs are found, such as 
"REPUBLIC OF CUBA ?????", and some 
many-to-one cases like "SHELL BEACH ???
?" also appear. We modify or delete these cases 
from the original dev set. At last, 3223 pairs are 
treated as the final dev set to tune the weights of 
system features. 
 
Language Segmentation Algorithm Number 
 
English 
Character-based 6.82 
SSA  4.24 
FSA 4.48 
Chinese Character-based 3.17 
Table 1: Average syllables of names based on 
different segmentation methods 
 
Language Segmentation Algorithm Number 
 
English 
Character-based 26 
SSA  922 
FSA 463 
Chinese Character-based 368 
Table 2 :Total number of unique units 
 
  For the Chinese-English back transliteration track, 
the final training and test sets are formed in the 
same way; the original dev set is used directly. 
  Here we use Character-based which treats single 
character as a "syllable", Simple and Fine-grained 
segmentation algorithms to deal with English 
names. Table 1 and table 2 show some syllabic 
statistics information. Table 1 shows the average 
syllables of the three segmentation approaches in 
training data. Table 2 shows the total number of 
unique units. 
4.2 Experimental Setup 
The Moses (Koehn et al, 2007) is used to 
implement the model in this paper. The 
Srilm(Stolcke et al, 2002) toolkit is used to count 
54
n-gram on the target of the training set. Here we 
use a 3-gram language model. In the transliteration 
model training step, the Giza++(Och et al, 2003) 
generates the alignment with the grow-diag-and-
final heuristic, while other setup is default. In order 
to guarantee monotone decoding, the distortion 
distance is limited to 0. The MERT is used to tune 
model's weights. The method of (Jia et al, 2009) is 
the baseline setup.   
4.3 Evaluation Metrics 
The following 4 metrics are used to measure the 
quality of the transliteration results (Li et al, 
2009a): Word Accuracy in Top-1 (ACC), 
Fuzziness in Top-1 (Mean F-score), Mean 
Reciprocal Rank (MRR), MAPref. 
4.4 Results 
Table 3 shows the performance of our system 
corresponding to baseline, SSA and FSA on the 
closed test set of EnCh track. BE, L1,L2 and 
BE+L1+L2 are implemented on the basis of FSA.  
   
 ACC Mean 
F-score 
MRR MAPre
f 
Baseline 0.628 0.847 0.731 0.628 
SSA  0.639 0.850 0.738 0.639 
FSA 0.661 0.861 0.756 0.661 
BE  0.648 0.856 0.751 0.648 
L1 0.661 0.864 0.756 0.661 
L2 0.619 0.844 0.727 0.619 
BE+L1+L2 0.665 0.863 0.762 0.665 
Table 3:The held-in results of EnCh 
 
  Table 3 shows that the forward transliteration 
performance gets consistent improvement from 
baseline to FSA. None of new three features can 
improve by self, while combining three features 
can gain a little.  
 
 ACC Mean 
F-score 
MRR MAPre
f 
EnCh_Pri 0.330 0.676 0.408 0.319 
EnCh_2 0.317 0.667 0.399 0.308 
ChEn_pri 0.177 0.702 0.257 0.173 
Table 4:  The final official results of EnCh and 
ChEn 
 
  According to the performance of closed test, the 
transliteration results of EnCh and ChEn based on 
BE+L1+L2 are chosen as the primary 
submissions(EnCh_Pri and ChEn_Pri). And the 
result of FSA is the contrastive 
submission(EnCh_2). The table 4 shows the final 
official results of EnCh and ChEn. 
5 Conclusions and future work  
This paper uses the phrase-based machine 
translation to model the transliteration task and the 
state-of-the-art translation system Moses is used to 
implement it. We participate in the NEWS 2012 
Machine Transliteration Shared Task English-
Chinese and Chinese-English tracks. 
  To improve the capability of the basic phrase-
based machine transliteration, two heuristic and 
rule-based English syllable segmentation methods 
are addressed. System can also be more robust 
with combination of three new special features for 
transliteration. The experimental results show that 
the Fine-grained Segmentation can improve the 
performance remarkably in English-Chinese 
transliteration track. 
  In the future, extensive error analyses will be 
made and methods will be proposed according to 
the specific error type. More syllable segmentation 
methods such as statistical-based will be tried. 
Acknowledgments  
The authors would like to thank all the reviews for 
their help about correcting grammatical errors of 
this paper and invaluable suggestions. This work is 
supported by the project of National High 
Technology Research and Development Program 
of China (863 Program) (No. 2011AA01A207) and 
the project of National Natural Science Foundation 
of China (No. 61100093). 
References  
Andreas Stolcke. 2002. SRILM - an Extensible 
Language Modeling Toolkit. In Proc. of ICSLP, 
Denver, USA. 
Dong Yang, Paul Dixon and Sadaoki Furui. 2010. 
Jointly optimizing a two-step conditional random 
field model for machine transliteration and its fast 
decoding algorithm. In Proceedings of the ACL 2010 
Conference Short Papers. pp. 275--280 Uppsala, 
Sweden. 
55
Franz Josef Och, Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Comput.Linguistics 29, 1, 19?51. 
Haizhou Li , Min Zhang, Jian Su. 2004. A Joint Source 
Channel Model for Machine Transliteration. In 
Proceedings of the 42nd ACL, pp. 159-166. 
Kevin Knight, Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics, Vol. 24, 
No. 4, pp. 599-612. 
Long Jiang , Ming Zhou , Leefeng Chien and Cheng 
Niu. Named entity translation with web mining and 
transliteration, Proceedings of the 20th international 
joint conference on Artifical intelligence, p.1629-
1634, January 06-12, 2007, Hyderabad, India 
Min Zhang, Xiangyu Duan, Vladimir Pervouchine, and 
Haizhou Li. 2010. Machine transliteration: 
Leveraging on third languages. In Coling 2010: 
Posters, pages 1444?1452, Beijing, China, August. 
Coling 2010 Organizing Committee. 
Oi Yee Kwong. 2011. English-Chinese Personal 
Name Transliteration by Syllable-Based Maximum 
Matching. In the Proceedings of the 2011 Named 
Entities Workshop,2011,pp.96-100. 
Philipp Koehn, Hieu Hoang, Marcello Federico Nicola 
Bertoldi , Brooke Cowan and Wade Shen . 2007. 
Moses: Open Source Toolkit for Statistical Machine 
Translation. In Proceedings of the 45th ACL 
Companion Volume of the Demo and Poster Sessions, 
pp. 177-180. 
Yun Huang, Min Zhang and Chewlim Tan. 2011. 
Nonparametric Bayesian Machine Transliteration 
with Synchronous Adaptor Grammars. In 
Proceedings of ACL-HLT 2011: Short 
Papers,Portland, Oregon, pp.534-539. 
Yuxiang Jia, Danqing Zhu, and Shiwen Y. 2009. A 
Noisy Channel Model for Grapheme-based Machine 
Transliteration, In the Proceedings of the 2009 
Named Entities Workshop, 2009, pp. 88-91. 
 
56
