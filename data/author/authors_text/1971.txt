Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 11?20,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
It?s a Contradiction?No, it?s Not:
A Case Study using Functional Relations
Alan Ritter, Doug Downey, Stephen Soderland and Oren Etzioni
Turing Center
Department of Computer Science and Engineering
University of Washington
Box 352350
Seattle, WA 98195, USA
{aritter,ddowney,soderlan,etzioni}@cs.washington.edu
Abstract
Contradiction Detection (CD) in text is a
difficult NLP task. We investigate CD
over functions (e.g., BornIn(Person)=Place),
and present a domain-independent algorithm
that automatically discovers phrases denoting
functions with high precision. Previous work
on CD has investigated hand-chosen sentence
pairs. In contrast, we automatically harvested
from the Web pairs of sentences that appear
contradictory, but were surprised to find that
most pairs are in fact consistent. For example,
?Mozart was born in Salzburg? does not con-
tradict ?Mozart was born in Austria? despite
the functional nature of the phrase ?was born
in?. We show that background knowledge
about meronyms (e.g., Salzburg is in Austria),
synonyms, functions, and more is essential for
success in the CD task.
1 Introduction and Motivation
Detecting contradictory statements is an important
and challenging NLP task with a wide range of
potential applications including analysis of politi-
cal discourse, of scientific literature, and more (de
Marneffe et al, 2008; Condoravdi et al, 2003;
Harabagiu et al, 2006). De Marneffe et al present a
model of CD that defines the task, analyzes different
types of contradictions, and reports on a CD system.
They report 23% precision and 19% recall at detect-
ing contradictions in the RTE-3 data set (Voorhees,
2008). Although RTE-3 contains a wide variety of
contradictions, it does not reflect the prevalence of
seeming contradictions and the paucity of genuine
contradictions, which we have found in our corpus.
1.1 Contradictions and World Knowledge
Our paper is motivated in part by de Marneffe et al?s
work, but with some important differences. First,
we introduce a simple logical foundation for the CD
task, which suggests that extensive world knowl-
edge is essential for building a domain-independent
CD system. Second, we automatically generate a
large corpus of apparent contradictions found in ar-
bitrary Web text. We show that most of these appar-
ent contradictions are actually consistent statements
due to meronyms (Alan Turing was born in London
and in England), synonyms (George Bush is mar-
ried to both Mrs. Bush and Laura Bush), hypernyms
(Mozart died of both renal failure and kidney dis-
ease), and reference ambiguity (one John Smith was
born in 1997 and a different John Smith in 1883).
Next, we show how background knowledge enables
a CD system to discard seeming contradictions and
focus on genuine ones.
De Marneffe et al introduced a typology of con-
tradiction in text, but focused primarily on contra-
dictions that can be detected from linguistic evi-
dence (e.g. negation, antonymy, and structural or
lexical disagreements). We extend their analysis to
a class of contradictions that can only be detected
utilizing background knowledge. Consider for ex-
ample the following sentences:
1) ?Mozart was born in Salzburg.?
2) ?Mozart was born in Vienna.?
3) ?Mozart visited Salzburg.?
4) ?Mozart visited Vienna.?
Sentences 1 & 2 are contradictory, but 3 & 4 are
not. Why is that? The distinction is not syntactic.
Rather, sentences 1 and 2 are contradictory because
11
the relation expressed by the phrase ?was born in?
can be characterized here as a function from peo-
ple?s names to their unique birthplaces. In contrast,
?visited? does not denote a functional relation.1
We cannot assume that a CD system knows, in
advance, all the functional relations that might ap-
pear in a corpus. Thus, a central challenge for a
function-based CD system is to determine which re-
lations are functional based on a corpus. Intuitively,
we might expect that ?functional phrases? such as
?was born in? would typically map person names
to unique place names, making function detection
easy. But, in fact, function detection is surprisingly
difficult because name ambiguity (e.g., John Smith),
common nouns (e.g., ?dad? or ?mom?), definite de-
scriptions (e.g., ?the president?), and other linguistic
phenomena can mask functions in text. For example,
the two sentences ?John Smith was born in 1997.?
and ?John Smith was born in 1883.? can be viewed
as either evidence that ?was born in? does not de-
note a function or, alternatively, that ?John Smith?
is ambiguous.
1.2 A CD System Based on Functions
We report on the AUCONTRAIRE CD system, which
addresses each of the above challenges. First, AU-
CONTRAIRE identifies ?functional phrases? statis-
tically (Section 3). Second, AUCONTRAIRE uses
these phrases to automatically create a large cor-
pus of apparent contradictions (Section 4.2). Fi-
nally, AUCONTRAIRE sifts through this corpus to
find genuine contradictions using knowledge about
synonymy, meronymy, argument types, and ambi-
guity (Section 4.3).
Instead of analyzing sentences directly, AUCON-
TRAIRE relies on the TEXTRUNNER Open Informa-
tion Extraction system (Banko et al, 2007; Banko
and Etzioni, 2008) to map each sentence to one or
more tuples that represent the entities in the sen-
tences and the relationships between them (e.g.,
was born in(Mozart,Salzburg)). Using extracted tu-
ples greatly simplifies the CD task, because nu-
merous syntactic problems (e.g., anaphora, rela-
tive clauses) and semantic challenges (e.g., quantifi-
cation, counterfactuals, temporal qualification) are
1Although we focus on function-based CD in our case study,
we believe that our observations apply to other types of CD as
well.
delegated to TEXTRUNNER or simply ignored. Nev-
ertheless, extracted tuples are a convenient approxi-
mation of sentence content, which enables us to fo-
cus on function detection and function-based CD.
Our contributions are the following:
? We present a novel model of the Contradiction
Detection (CD) task, which offers a simple log-
ical foundation for the task and emphasizes the
central role of background knowledge.
? We introduce and evaluate a new EM-style al-
gorithm for detecting whether phrases denote
functional relations and whether nouns (e.g.,
?dad?) are ambiguous, which enables a CD sys-
tem to identify functions in arbitrary domains.
? We automatically generate a corpus of seem-
ing contradictions from Web text, and report
on a set of experiments over this corpus, which
provide a baseline for future work on statistical
function identification and CD. 2
2 A Logical Foundation for CD
On what basis can a CD system conclude that two
statements T and H are contradictory? Logically,
contradiction holds when T |= ?H . As de Marneffe
et al point out, this occurs when T and H contain
antonyms, negation, or other lexical elements that
suggest that T and H are directly contradictory. But
other types of contradictions can only be detected
with the help of a body of background knowledge
K: In these cases, T and H alone are mutually con-
sistent. That is,
T |=\ ?H ?H |=\ ?T
A contradiction between T and H arises only in
the context of K. That is:
((K ? T ) |= ?H) ? ((K ?H) |= ?T )
Consider the example of Mozart?s birthplace in
the introduction. To detect a contradiction, a CD
system must know that A) ?Mozart? refers to the
same entity in both sentences, that B) ?was born in?
denotes a functional relation, and that C) Vienna and
Salzburg are inconsistent locations.
2The corpus is available at http://www.cs.
washington.edu/research/aucontraire/
12
Of course, world knowledge, and reasoning about
text, are often uncertain, which leads us to associate
probabilities with a CD system?s conclusions. Nev-
ertheless, the knowledge base K is essential for CD.
We now turn to a probabilistic model that helps
us simultaneously estimate the functionality of re-
lations (B in the above example) and ambiguity of
argument values (A above). Section 4 describes the
remaining components of AUCONTRAIRE.
3 Detecting Functionality and Ambiguity
This section introduces a formal model for comput-
ing the probability that a phrase denotes a function
based on a set of extracted tuples. An extracted tuple
takes the form R(x, y) where (roughly) x is the sub-
ject of a sentence, y is the object, and R is a phrase
denoting the relationship between them. If the re-
lation denoted by R is functional, then typically the
object y is a function of the subject x. Thus, our dis-
cussion focuses on this possibility, though the anal-
ysis is easily extended to the symmetric case.
Logically, a relation R is functional in a vari-
able x if it maps it to a unique variable y:
?x, y1, y2 R(x, y1) ? R(x, y2) ? y1 = y2. Thus,
given a large random sample of ground instances of
R, we could detect with high confidence whether R
is functional. In text, the situation is far more com-
plex due to ambiguity, polysemy, synonymy, and
other linguistic phenomena. Deciding whether R is
functional becomes a probabilistic assessment based
on aggregated textual evidence.
The main evidence that a relation R(x, y) is func-
tional comes from the distribution of y values for
a given x value. If R denotes a function and x is
unambiguous, then we expect the extractions to be
predominantly a single y value, with a few outliers
due to noise. We aggregate the evidence that R is
locally functional for a particular x value to assess
whether R is globally functional for all x.
We refer to a set of extractions with the same
relation R and argument x as a contradiction set
R(x, ?). Figure 1 shows three example contradic-
tion sets. Each example illustrates a situation com-
monly found in our data. Example A in Figure 1
shows strong evidence for a functional relation. 66
out of 70 TEXTRUNNER extractions for was born in
(Mozart, PLACE) have the same y value. An am-
biguous x argument, however, can make a func-
tional relation appear non-functional. Example B
depicts a distribution of y values that appears less
functional due to the fact that ?John Adams? refers
to multiple, distinct real-world individuals with that
name. Finally, example C exhibits evidence for a
non-functional relation.
A. was born in(Mozart, PLACE):
Salzburg(66), Germany(3), Vienna(1)
B. was born in(John Adams, PLACE):
Braintree(12), Quincy(10), Worcester(8)
C. lived in(Mozart, PLACE):
Vienna(20), Prague(13), Salzburg(5)
Figure 1: Functional relations such as example A have a
different distribution of y values than non-functional rela-
tions such as C. However, an ambiguous x argument as in
B, can make a functional relation appear non-functional.
3.1 Formal Model of Functions in Text
To decide whether R is functional in x for all x,
we first consider how to detect whether R is lo-
cally functional for a particular value of x. The local
functionality of R with respect to x is the probabil-
ity that R is functional estimated solely on evidence
from the distribution of y values in a contradiction
set R(x, ?).
To decide the probability that R is a function, we
define global functionality as the average local func-
tionality score for each x, weighted by the probabil-
ity that x is unambiguous. Below, we outline an EM-
style algorithm that alternately estimates the proba-
bility that R is functional and the probability that x
is ambiguous.
Let R?x indicate the event that the relation R is
locally functional for the argument x, and that x is
locally unambiguous for R. Also, let D indicate
the set of observed tuples, and define DR(x,?) as the
multi-set containing the frequencies for extractions
of the form R(x, ?). For example the distribution of
extractions from Figure 1 for example A is
Dwas born in(Mozart,?) = {66, 3, 1}.
Let ?fR be the probability that R(x, ?) is locally
functional for a random x, and let ?f be the vector
of these parameters across all relations R. Likewise,
?ux represents the probability that x is locally unam-
biguous for random R, and ?u the vector for all x.
13
We wish to determine the maximum a pos-
teriori (MAP) functionality and ambiguity pa-
rameters given the observed data D, that is
arg max?f ,?u P (?
f ,?u|D). By Bayes Rule:
P (?f ,?u|D) =
P (D|?f ,?u)P (?f ,?u)
P (D)
(1)
We outline a generative model for the data,
P (D|?f ,?u). Let us assume that the event R?x de-
pends only on ?fR and ?
u
x , and further assume that
given these two parameters, local ambiguity and lo-
cal functionality are conditionally independent. We
obtain the following expression for the probability
of R?x given the parameters:
P (R?x|?
f ,?u) = ?fR?
u
x
We assume each set of data DR(x,?) is gener-
ated independently of all other data and parameters,
given R?x. From this and the above we have:
P (D|?f ,?u) =
?
R,x
(
P (DR(x,?)|R
?
x)?
f
R?
u
x
+P (DR(x,?)|?R
?
x)(1? ?
f
R?
u
x)
)
(2)
These independence assumptions allow us to ex-
press P (D|?f ,?u) in terms of distributions over
DR(x,?) given whether or not R
?
x holds. We use the
URNS model as described in (Downey et al, 2005)
to estimate these probabilities based on binomial
distributions. In the single-urn URNS model that we
utilize, the extraction process is modeled as draws of
labeled balls from an urn, where the labels are either
correct extractions or errors, and different labels can
be repeated on varying numbers of balls in the urn.
Let k = maxDR(x,?), and let n =
?
DR(x,?);
we will approximate the distribution over DR(x,?)
in terms of k and n. If R(x, ?) is locally func-
tional and unambiguous, there is exactly one cor-
rect extraction label in the urn (potentially repeated
multiple times). Because the probability of correct-
ness tends to increase with extraction frequency, we
make the simplifying assumption that the most fre-
quently extracted element is correct.3 In this case, k
is the number of correct extractions, which by the
3As this assumption is invalid when there is not a unique
maximal element, we default to the prior P (R?x) in that case.
URNS model has a binomial distribution with pa-
rameters n and p, where p is the precision of the ex-
traction process. If R(x, ?) is not locally functional
and unambiguous, then we expect k to typically take
on smaller values. Empirically, the underlying fre-
quency of the most frequent element in the?R?x case
tends to follow a Beta distribution.
Under the model, the probability of the evidence
given R?x is:
P (DR(x,?)|R
?
x) ? P (k, n|R
?
x) =
(
n
k
)
pk(1? p)n?k
And the probability of the evidence given ?R?x is:
P (DR(x,?)|?R
?
x) ? P (k, n|?R
?
x)
=
(n
k
) ? 1
0
p?k+?f?1(1?p?)n+?f?1?k
B(?f ,?f )
dp?
=
(n
k
)
?(n? k + ?f )?(?f + k)
B(?f , ?f )?(?f + ?f + n)
(3)
where n is the sum over DR(x,?), ? is the Gamma
function and B is the Beta function. ?f and ?f are
the parameters of the Beta distribution for the ?R?x
case. These parameters and the prior distributions
are estimated empirically, based on a sample of the
data set of relations described in Section 5.1.
3.2 Estimating Functionality and Ambiguity
Substituting Equation 3 into Equation 2 and apply-
ing an appropriate prior gives the probability of pa-
rameters ?f and ?u given the observed data D.
However, Equation 2 contains a large product of
sums?with two independent vectors of coefficients,
?f and ?u?making it difficult to optimize analyti-
cally.
If we knew which arguments were ambiguous,
we would ignore them in computing the function-
ality of a relation. Likewise, if we knew which rela-
tions were non-functional, we would ignore them in
computing the ambiguity of an argument. Instead,
we initialize the ?f and ?u arrays randomly, and
then execute an algorithm similar to Expectation-
Maximization (EM) (Dempster et al, 1977) to arrive
at a high-probability setting of the parameters.
Note that if ?u is fixed, we can compute the ex-
pected fraction of locally unambiguous arguments x
for which R is locally functional, using DR(x?,?) and
14
Equation 3. Likewise, for fixed ?f , for any given
x we can compute the expected fraction of locally
functional relations R that are locally unambiguous
for x.
Specifically, we repeat until convergence:
1. Set ?fR =
1
sR
?
x P (R
?
x|DR(x,?))?
u
x for all R.
2. Set ?ux =
1
sx
?
R P (R
?
x|DR(x,?))?
f
R for all x.
In both steps above, the sums are taken over only
those x or R for which DR(x,?) is non-empty. Also,
the normalizer sR =
?
x ?
u
x and likewise sx =?
R ?
f
R.
As in standard EM, we iteratively update our pa-
rameter values based on an expectation computed
over the unknown variables. However, we alter-
nately optimize two disjoint sets of parameters (the
functionality and ambiguity parameters), rather than
just a single set of parameters as in standard EM.
Investigating the optimality guarantees and conver-
gence properties of our algorithm is an item of future
work.
By iteratively setting the parameters to the expec-
tations in steps 1 and 2, we arrive at a good setting
of the parameters. Section 5.2 reports on the perfor-
mance of this algorithm in practice.
4 System Overview
AUCONTRAIRE identifies phrases denoting func-
tional relations and utilizes these to find contradic-
tory assertions in a massive, open-domain corpus of
text.
AUCONTRAIRE begins by finding extractions of
the form R(x, y), and identifies a set of relations
R that have a high probability of being functional.
Next, AUCONTRAIRE identifies contradiction sets
of the form R(x, ?). In practice, most contradiction
sets turned out to consist overwhelmingly of seem-
ing contradictions?assertions that do not actually
contradict each other for a variety of reasons that
we enumerate in section 4.3. Thus, a major chal-
lenge for AUCONTRAIRE is to tease apart which
pairs of assertions in R(x, ?) represent genuine con-
tradictions.
Here are the main components of AUCONTRAIRE
as illustrated in Figure 2:
Extractor: Create a set of extracted assertions E
from a large corpus of Web pages or other docu-
ments. Each extraction R(x, y) has a probability p
Figure 2: AUCONTRAIRE architecture
of being correct.
Function Learner: Discover a set of functional re-
lations F from among the relations in E . Assign to
each relation in F a probability pf that it is func-
tional.
Contradiction Detector: Query E for assertions
with a relation R in F , and identify sets C of po-
tentially contradictory assertions. Filter out seeming
contradictions in C by reasoning about synonymy,
meronymy, argument types, and argument ambigu-
ity. Assign to each potential contradiction a proba-
bility pc that it is a genuine contradiction.
4.1 Extracting Factual Assertions
AUCONTRAIRE needs to explore a large set of
factual assertions, since genuine contradictions are
quite rare (see Section 5). We used a set of extrac-
tions E from the Open Information Extraction sys-
tem, TEXTRUNNER (Banko et al, 2007), which was
run on a set of 117 million Web pages.
TEXTRUNNER does not require a pre-defined set
of relations, but instead uses shallow linguistic anal-
ysis and a domain-independent model to identify
phrases from the text that serve as relations and
phrases that serve as arguments to that relation.
TEXTRUNNER creates a set of extractions in a sin-
gle pass over the Web page collection and provides
an index to query the vast set of extractions.
Although its extractions are noisy, TEXTRUNNER
provides a probability that the extractions are cor-
15
rect, based in part on corroboration of facts from
different Web pages (Downey et al, 2005).
4.2 Finding Potential Contradictions
The next step of AUCONTRAIRE is to find contra-
diction sets in E .
We used the methods described in Section 3 to
estimate the functionality of the most frequent rela-
tions in E . For each relation R that AUCONTRAIRE
has judged to be functional, we identify contradic-
tion sets R(x, ?), where a relation R and domain ar-
gument x have multiple range arguments y.
4.3 Handling Seeming Contradictions
For a variety of reasons, a pair of extractions
R(x, y1) and R(x, y2) may not be actually contra-
dictory. The following is a list of the major sources
of false positives?pairs of extractions that are not
genuine contradictions, and how they are handled
by AUCONTRAIRE. The features indicative of each
condition are combined using Logistic Regression,
in order to estimate the probability that a given pair,
{R(x, y1), R(x, y2)} is a genuine contradiction.
Synonyms: The set of potential contradictions
died from(Mozart,?) may contain assertions that
Mozart died from renal failure and that he died from
kidney failure. These are distinct values of y, but
do not contradict each other, as the two terms are
synonyms. AUCONTRAIRE uses a variety of knowl-
edge sources to handle synonyms. WordNet is a re-
liable source of synonyms, particularly for common
nouns, but has limited recall. AUCONTRAIRE also
utilizes synonyms generated by RESOLVER (Yates
and Etzioni, 2007)? a system that identifies syn-
onyms from TEXTRUNNER extractions. Addition-
ally, AUCONTRAIRE uses edit-distance and token-
based string similarity (Cohen et al, 2003) between
apparently contradictory values of y to identify syn-
onyms.
Meronyms: For some relations, there is no con-
tradiction when y1 and y2 share a meronym,
i.e. ?part of? relation. For example, in the set
born in(Mozart,?) there is no contradiction be-
tween the y values ?Salzburg? and ?Austria?, but
?Salzburg? conflicts with ?Vienna?. Although this
is only true in cases where y occurs in an up-
ward monotone context (MacCartney and Manning,
2007), in practice genuine contradictions between
y-values sharing a meronym relationship are ex-
tremely rare. We therefore simply assigned contra-
dictions between meronyms a probability close to
zero. We used the Tipster Gazetteer4 and WordNet
to identify meronyms, both of which have high pre-
cision but low coverage.
Argument Typing: Two y values are not contra-
dictory if they are of different argument types. For
example, the relation born in can take a date or a
location for the y value. While a person can be
born in only one year and in only one city, a per-
son can be born in both a year and a city. To avoid
such false positives, AUCONTRAIRE uses a sim-
ple named-entity tagger5 in combination with large
dictionaries of person and location names to as-
sign high-level types (person, location, date, other)
to each argument. AUCONTRAIRE filters out ex-
tractions from a contradiction set that do not have
matching argument types.
Ambiguity: As pointed out in Section 3, false con-
tradictions arise when a single x value refers to mul-
tiple real-world entities. For example, if the con-
tradiction set born in(John Sutherland, ?) includes
birth years of both 1827 and 1878, is one of these a
mistake, or do we have a grandfather and grandson
with the same name? AUCONTRAIRE computes the
probability that an x value is unambiguous as part
of its Function Learner (see Section 3). An x value
can be identified as ambiguous if its distribution of
y values is non-functional for multiple functional re-
lations.
If a pair of extractions, {R(x, y1), R(x, y2)}, does
not fall into any of the above categories and R is
functional, then it is likely that the sentences under-
lying the extractions are indeed contradictory. We
combined the various knowledge sources described
above using Logistic Regression, and used 10-fold
cross-validation to automatically tune the weights
associated with each knowledge source. In addi-
tion, the learning algorithm also utilizes the follow-
ing features:
? Global functionality of the relation, ?fR
? Global unambiguity of x, ?ux
4http://crl.nmsu.edu/cgi-bin/Tools/CLR/
clrcat
5http://search.cpan.org/?simon/
Lingua-EN-NamedEntity-1.1/NamedEntity.pm
16
? Local functionality of R(x, ?)
? String similarity (a combination of token-based
similarity and edit-distance) between y1 and y2
? The argument types (person, location, date, or
other)
The learned model is then used to estimate how
likely a potential contradiction {R(x, y1), R(x, y2)}
is to be genuine.
5 Experimental Results
We evaluated several aspects of AUCONTRAIRE:
its ability to detect functional relations and to de-
tect ambiguous arguments (Section 5.2); its preci-
sion and recall in contradiction detection (Section
5.3); and the contribution of AUCONTRAIRE?s key
knowledge sources (Section 5.4).
5.1 Data Set
To evaluate AUCONTRAIRE we used TEXTRUN-
NER?s extractions from a corpus of 117 million Web
pages. We restricted our data set to the 1,000 most
frequent relations, in part to keep the experiments
tractable and also to ensure sufficient statistical sup-
port for identifying functional relations.
We labeled each relation as functional or not,
and computed an estimate of the probability it is
functional as described in section 3.2. Section 5.2
presents the results of the Function Learner on this
set of relations. We took the top 2% (20 relations)
as F , the set of functional relations in our exper-
iments. Out of these, 75% are indeed functional.
Some examples include: was born in, died in, and
was founded by.
There were 1.2 million extractions for all thou-
sand relations, and about 20,000 extractions in 6,000
contradiction sets for all relations in F .
We hand-tagged 10% of the contradiction sets
R(x, ?) where R ? F , discarding any sets with over
20 distinct y values since the x argument for that
set is almost certainly ambiguous. This resulted in a
data set of 567 contradiction sets containing a total
of 2,564 extractions and 8,844 potentially contradic-
tory pairs of extractions.
We labeled each of these 8,844 pairs as contradic-
tory or not. In each case, we inspected the original
sentences, and if the distinction was unclear, con-
sulted the original source Web pages, Wikipedia ar-
ticles, and Web search engine results.
In our data set, genuine contradictions over func-
tional relations are surprisingly rare. We found only
110 genuine contradictions in the hand-tagged sam-
ple, only 1.2% of the potential contradiction pairs.
5.2 Detecting Functionality and Ambiguity
We ran AUCONTRAIRE?s EM algorithm on the
thousand most frequent relations. Performance con-
verged after 5 iterations resulting in estimates of the
probability that each relation is functional and each
x argument is unambiguous. We used these proba-
bilities to generate the precision-recall curves shown
in Figure 3.
The graph on the left shows results for function-
ality, while the graph on the right shows precision at
finding unambiguous arguments. The solid lines are
results after 5 iterations of EM, and the dashed lines
are from computing functionality or ambiguity with-
out EM (i.e. assuming uniform values of ?c when
computing ?f and vice versa). The EM algorithm
improved results for both functionality and ambigu-
ity, increasing area under curve (AUC) by 19% for
functionality and by 31% for ambiguity.
Of course, the ultimate test of how well AUCON-
TRAIRE can identify functional relations is how well
the Contradiction Detector performs on automati-
cally identified functional relations.
5.3 Detecting Contradictions
We conducted experiments to evaluate how well
AUCONTRAIRE distinguishes genuine contradic-
tions from false positives.
The bold line in Figure 4 depicts AUCONTRAIRE
performance on the distribution of contradictions
and seeming contradictions found in actual Web
data. The dashed line shows the performance of AU-
CONTRAIRE on an artificially ?balanced? data set
that we constructed to contain 50% genuine contra-
dictions and 50% seeming ones.
Previous research in CD presented results on
manually selected data sets with a relatively bal-
anced mix of positive and negative instances. As
Figure 4 suggests, this is a much easier problem than
CD ?in the wild?. The data gathered from the Web
is badly skewed, containing only 1.2% genuine con-
tradictions.
17
Functionality
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
AuContraireNo Iteration
Ambiguity
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
AuContraireNo Iteration
Figure 3: After 5 iterations of EM, AUCONTRAIRE achieves a 19% boost to area under the precision-recall curve
(AUC) for functionality detection, and a 31% boost to AUC for ambiguity detection.
Recall
Pre
cisi
on
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
Web DistributionBalanced Data
Figure 4: Performance of AUCONTRAIRE at distinguish-
ing genuine contradictions from false positives. The bold
line is results on the actual distribution of data from the
Web. The dashed line is from a data set constructed to
have 50% positive and 50% negative instances.
5.4 Contribution of Knowledge Sources
We carried out an ablation study to quantify how
much each knowledge source contributes to AU-
CONTRAIRE?s performance. Since most of the
knowledge sources do not apply to numeric argu-
ment values, we excluded the extractions where y
is a number in this study. As shown in Figure 5,
performance of AUCONTRAIRE degrades with no
knowledge of synonyms (NS), with no knowledge
of meronyms (NM), and especially without argu-
ment typing (NT). Conversely, improvements to any
of these three components would likely improve the
performance of AUCONTRAIRE.
The relatively small drop in performance from
no meronyms does not indicate that meronyms are
not essential to our task, only that our knowledge
sources for meronyms were not as useful as we
hoped. The Tipster Gazetteer has surprisingly low
coverage for our data set. It contains only 41% of
the y values that are locations. Many of these are
matches on a different location with the same name,
which results in incorrect meronym information. We
estimate that a gazetteer with complete coverage
would increase area under the curve by approxi-
mately 40% compared to a system with meronyms
from the Tipster Gazetteer and WordNet.
AuContraire NS NM NT
Percentage AUC
0
20
40
60
80
100
Figure 5: Area under the precision-recall curve for the
full AUCONTRAIRE and for AUCONTRAIRE with knowl-
edge removed. NS has no synonym knowledge; NM has
no meronym knowledge; NT has no argument typing.
To analyze the errors made by AUCONTRAIRE,
we hand-labeled all false-positives at the point of
maximum F-score: 29% Recall and 48% Precision.
18
Figure 6 reveals the central importance of world
knowledge for the CD task. About half of the errors
(49%) are due to ambiguous x-arguments, which we
found to be one of the most persistent obstacles to
discovering genuine contradictions. A sizable por-
tion is due to missing meronyms (34%) and missing
synonyms (14%), suggesting that lexical resources
with broader coverage than WordNet and the Tipster
Gazetteer would substantially improve performance.
Surprisingly, only 3% are due to errors in the extrac-
tion process.
Extraction Errors (3%)
Missing Synonyms (14%)
Missing Meronyms (34%)
Ambiguity (49%)
Figure 6: Sources of errors in contradiction detection.
All of our experimental results are based on the
automatically discovered set of functions F . We
would expect AUCONTRAIRE?s performance to im-
prove substantially if it were given a large set of
functional relations as input.
6 Related Work
Condoravdi et al (2003) first proposed contradiction
detection as an important NLP task, and Harabagiu
et al (2006) were the first to report results on con-
tradiction detection using negation, although their
evaluation corpus was a balanced data set built
by manually negating entailments in a data set
from the Recognizing Textual Entailment confer-
ences (RTE) (Dagan et al, 2005). De Marneffe et
al. (2008) reported experimental results on a contra-
diction corpus created by annotating the RTE data
sets.
RTE-3 included an optional task, requiring sys-
tems to make a 3-way distinction: {entails, contra-
dicts, neither} (Voorhees, 2008). The average per-
formance for contradictions on the RTE-3 was preci-
sion 0.11 at recall 0.12, and the best system had pre-
cision 0.23 at recall 0.19. We did not run AUCON-
TRAIRE on the RTE data sets because they contained
relatively few of the ?functional contradictions? that
AUCONTRAIRE tackles. On our Web-based data
sets, we achieved a precision of 0.62 at recall 0.19,
and precision 0.92 at recall 0.51 on the balanced data
set. Of course, comparisons across very different
data sets are not meaningful, but merely serve to un-
derscore the difficulty of the CD task.
In contrast to previous work, AUCONTRAIRE is
the first to do CD on data automatically extracted
from the Web. This is a much harder problem than
using an artificially balanced data set, as shown in
Figure 4.
Automatic discovery of functional relations has
been addressed in the database literature as Func-
tional Dependency Mining (Huhtala et al, 1999;
Yao and Hamilton, 2008). This focuses on dis-
covering functional relationships between sets of at-
tributes, and does not address the ambiguity inherent
in natural language.
7 Conclusions and Future Work
We have described a case study of contradiction de-
tection (CD) based on functional relations. In this
context, we introduced and evaluated the AUCON-
TRAIRE system and its novel EM-style algorithm
for determining whether an arbitrary phrase is func-
tional. We also created a unique ?natural? data set
of seeming contradictions based on sentences drawn
from a Web corpus, which we make available to the
research community.
We have drawn two key lessons from our case
study. First, many seeming contradictions (approx-
imately 99% in our experiments) are not genuine
contradictions. Thus, the CD task may be much
harder on natural data than on RTE data as sug-
gested by Figure 4. Second, extensive background
knowledge is necessary to tease apart seeming con-
tradictions from genuine ones. We believe that these
lessons are broadly applicable, but verification of
this claim is a topic for future work.
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, ONR grant N00014-
08-1-0431 as well as gifts from the Utilika Founda-
tion and Google, and was carried out at the Univer-
sity of Washington?s Turing Center.
19
References
M. Banko and O. Etzioni. 2008. The tradeoffs between
traditional and open relation extraction. In Proceed-
ings of ACL.
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
W.W. Cohen, P. Ravikumar, and S.E. Fienberg. 2003.
A comparison of string distance metrics for name-
matching tasks. In IIWeb.
Cleo Condoravdi, Dick Crouch, Valeria de Paiva, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entailment,
intensionality and text understanding. In Proceedings
of the HLT-NAACL 2003 workshop on Text meaning,
pages 38?45, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
Marie-Catherine de Marneffe, Anna Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL 2008.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical Society Se-
ries B, 39(1):1?38.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In Procs. of IJCAI.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text pro-
cessing. In AAAI.
Yka? Huhtala, Juha Ka?rkka?inen, Pasi Porkka, and Hannu
Toivonen. 1999. TANE: An efficient algorithm for
discovering functional and approximate dependencies.
The Computer Journal, 42(2):100?111.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
Ellen M. Voorhees. 2008. Contradictions and justifica-
tions: Extensions to the textual entailment task. In
Proceedings of ACL-08: HLT, pages 63?71, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Hong Yao and Howard J. Hamilton. 2008. Mining func-
tional dependencies from data. Data Min. Knowl. Dis-
cov., 16(2):197?219.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
20
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 563?570, Vancouver, October 2005. c?2005 Association for Computational Linguistics
KnowItNow: Fast, Scalable Information Extraction from the Web
Michael J. Cafarella, Doug Downey, Stephen Soderland, Oren Etzioni
Department of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350
{mjc,ddowney,soderlan,etzioni}@cs.washington.edu
Abstract
Numerous NLP applications rely on
search-engine queries, both to ex-
tract information from and to com-
pute statistics over the Web corpus.
But search engines often limit the
number of available queries. As a
result, query-intensive NLP applica-
tions such as Information Extraction
(IE) distribute their query load over
several days, making IE a slow, off-
line process.
This paper introduces a novel archi-
tecture for IE that obviates queries to
commercial search engines. The ar-
chitecture is embodied in a system
called KNOWITNOW that performs
high-precision IE in minutes instead
of days. We compare KNOWITNOW
experimentally with the previously-
published KNOWITALL system, and
quantify the tradeoff between re-
call and speed. KNOWITNOW?s ex-
traction rate is two to three orders
of magnitude higher than KNOW-
ITALL?s.
1 Background and Motivation
Numerous modern NLP applications use the Web as their
corpus and rely on queries to commercial search engines
to support their computation (Turney, 2001; Etzioni et al,
2005; Brill et al, 2001). Search engines are extremely
helpful for several linguistic tasks, such as computing us-
age statistics or finding a subset of web documents to an-
alyze in depth; however, these engines were not designed
as building blocks for NLP applications. As a result,
the applications are forced to issue literally millions of
queries to search engines, which limits the speed, scope,
and scalability of the applications. Further, the applica-
tions must often then fetch some web documents, which
at scale can be very time-consuming.
In response to heavy programmatic search engine use,
Google has created the ?Google API? to shunt program-
matic queries away from Google.com and has placed hard
quotas on the number of daily queries a program can is-
sue to the API. Other search engines have also introduced
mechanisms to limit programmatic queries, forcing ap-
plications to introduce ?courtesy waits? between queries
and to limit the number of queries they issue.
To understand these efficiency problems in more detail,
consider the KNOWITALL information extraction sys-
tem (Etzioni et al, 2005). KNOWITALL has a generate-
and-test architecture that extracts information in two
stages. First, KNOWITALL utilizes a small set of domain-
independent extraction patterns to generate candidate
facts (cf. (Hearst, 1992)). For example, the generic pat-
tern ?NP1 such as NPList2? indicates that the head of
each simple noun phrase (NP) in NPList2 is a member of
the class named in NP1. By instantiating the pattern for
class City, KNOWITALL extracts three candidate cities
from the sentence: ?We provide tours to cities such as
Paris, London, and Berlin.? Note that it must also fetch
each document that contains a potential candidate.
Next, extending the PMI-IR algorithm (Turney, 2001),
KNOWITALL automatically tests the plausibility of the
candidate facts it extracts using pointwise mutual in-
formation (PMI) statistics computed from search-engine
hit counts. For example, to assess the likelihood that
?Yakima? is a city, KNOWITALL will compute the PMI
between Yakima and a set of k discriminator phrases that
tend to have high mutual information with city names
(e.g., the simple phrase ?city?). Thus, KNOWITALL re-
quires at least k search-engine queries for every candidate
extraction it assesses.
Due to KNOWITALL?s dependence on search-engine
queries, large-scale experiments utilizing KNOWITALL
take days and even weeks to complete, which makes re-
search using KNOWITALL slow and cumbersome. Pri-
vate access to Google-scale infrastructure would provide
563
sufficient access to search queries, but at prohibitive cost,
and the problem of fetching documents (even if from a
cached copy) would remain (as we discuss in Section
2.1). Is there a feasible alternative Web-based IE system?
If so, what size Web index and how many machines are
required to achieve reasonable levels of precision/recall?
What would the architecture of this IE system look like,
and how fast would it run?
To address these questions, this paper introduces a
novel architecture for web information extraction. It
consists of two components that supplant the generate-
and-test mechanisms in KNOWITALL. To generate ex-
tractions rapidly we utilize our own specialized search
engine, called the Bindings Engine (or BE), which ef-
ficiently returns bindings in response to variabilized
queries. For example, in response to the query ?Cities
such as ProperNoun(Head(?NounPhrase?))?, BE will
return a list of proper nouns likely to be city names. To
assess these extractions, we use URNS, a combinatorial
model, which estimates the probability that each extrac-
tion is correct without using any additional search engine
queries.1 For further efficiency, we introduce an approx-
imation to URNS, based on frequency of extractions? oc-
currence in the output of BE, and show that it achieves
comparable precision/recall to URNS.
Our contributions are as follows:
1. We present a novel architecture for Information Ex-
traction (IE), embodied in the KNOWITNOW sys-
tem, which does not depend on Web search-engine
queries.
2. We demonstrate experimentally that KNOWITNOW
is the first system able to extract tens of thousands
of facts from the Web in minutes instead of days.
3. We show that KNOWITNOW?s extraction rate is two
to three orders of magnitude greater than KNOW-
ITALL?s, but this increased efficiency comes at the
cost of reduced recall. We quantify this tradeoff for
KNOWITNOW?s 60,000,000 page index and extrap-
olate how the tradeoff would change with larger in-
dices.
Our recent work has described the BE search engine
in detail (Cafarella and Etzioni, 2005), and also analyzed
the URNS model?s ability to compute accurate probability
estimates for extractions (Downey et al, 2005). However,
this is the first paper to investigate the composition of
these components to create a fast IE system, and to com-
pare it experimentally to KNOWITALL in terms of time,
1In contrast, PMI-IR, which is built into KNOWITALL, re-
quires multiple search engine queries to assess each potential
extraction.
recall, precision, and extraction rate. The frequency-
based approximation to URNS and the demonstration of
its success are also new.
The remainder of the paper is organized as follows.
Section 2 provides an overview of BE?s design. Sec-
tion 3 describes the URNS model and introduces an ef-
ficient approximation to URNS that achieves similar pre-
cision/recall. Section 4 presents experimental results. We
conclude with related and future work in Sections 5 and
6.
2 The Bindings Engine
This section explains how relying on standard search en-
gines leads to a bottleneck for NLP applications, and pro-
vides a brief overview of the Bindings Engine (BE)?our
solution to this problem. A comprehensive description of
BE appears in (Cafarella and Etzioni, 2005).
Standard search engines are computationally expen-
sive for IE and other NLP tasks. IE systems issue multiple
queries, downloading all pages that potentially match an
extraction rule, and performing expensive processing on
each page. For example, such systems operate roughly as
follows on the query (?cities such as ?NounPhrase??):
1. Perform a traditional search engine query to find
all URLs containing the non-variable terms (e.g.,
?cities such as?)
2. For each such URL:
(a) obtain the document contents,
(b) find the searched-for terms (?cities such as?) in
the document text,
(c) run the noun phrase recognizer to determine
whether text following ?cities such as? satisfies
the linguistic type requirement,
(d) and if so, return the string
We can divide the algorithm into two stages: obtaining
the list of URLs from a search engine, and then process-
ing them to find the ?NounPhrase? bindings. Each stage
poses its own scalability and speed challenges. The first
stage makes a query to a commercial search engine; while
the number of available queries may be limited, a single
one executes relatively quickly. The second stage fetches
a large number of documents, each fetch likely resulting
in a random disk seek; this stage executes slowly. Nat-
urally, this disk access is slow regardless of whether it
happens on a locally-cached copy or on a remote doc-
ument server. The observation that the second stage is
slow, even if it is executed locally, is important because
it shows that merely operating a ?private? search engine
does not solve the problem (see Section 2.1).
The Bindings Engine supports queries contain-
ing typed variables (such as NounPhrase) and
564
string-processing functions (such as ?head(X)? or
?ProperNoun(X)?) as well as standard query terms. BE
processes a variable by returning every possible string
in the corpus that has a matching type, and that can be
substituted for the variable and still satisfy the user?s
query. If there are multiple variables in a query, then all
of them must simultaneously have valid substitutions.
(So, for example, the query ?<NounPhrase> is located
in <NounPhrase>? only returns strings when noun
phrases are found on both sides of ?is located in?.) We
call a string that meets these requirements a binding for
the variable in question. These queries, and the bindings
they elicit, can usefully serve as part of an information
extraction system or other common NLP tasks (such as
gathering usage statistics). Figure 1 illustrates some of
the queries that BE can handle.
president Bush <Verb>
cities such as ProperNoun(Head(<NounPhrase>))
<NounPhrase> is the CEO of <NounPhrase>
Figure 1: Examples of queries that can be handled by
BE. Queries that include typed variables and string-
processing functions allow NLP tasks to be done ef-
ficiently without downloading the original document
during query processing.
BE?s novel neighborhood index enables it to process
these queries with O(k) random disk seeks and O(k) se-
rial disk reads, where k is the number of non-variable
terms in its query. As a result, BE can yield orders of
magnitude speedup as shown in the asymptotic analysis
later in this section. The neighborhood index is an aug-
mented inverted index structure. For each term in the cor-
pus, the index keeps a list of documents in which the term
appears and a list of positions where the term occurs, just
as in a standard inverted index (Baeza-Yates and Ribeiro-
Neto, 1999). In addition, the neighborhood index keeps
a list of left-hand and right-hand neighbors at each posi-
tion. These are adjacent text strings that satisfy a recog-
nizer for one of the target types, such as NounPhrase.
As with a standard inverted index, a term?s list is pro-
cessed from start to finish, and can be kept on disk as a
contiguous piece. The relevant string for a variable bind-
ing is included directly in the index, so there is no need
to fetch the source document (thus causing a disk seek).
Expensive processing such as part-of-speech tagging or
shallow syntactic parsing is performed only once, while
building the index, and is not needed at query time. It
is important to note that simply preprocessing the corpus
and placing the results in a database would not avoid disk
seeks, as we would still have to explicitly fetch these re-
sults. The run-time efficiency of the neighborhood index
Query Time Index Space
BE O(k) O(N)
Standard engine O(k + B) O(N)
Table 1: BE yields considerable savings in query time
over a standard search engine. k is the number of con-
crete terms in the query, B is the number of variable
bindings found in the corpus, and N is the number of
documents in the corpus. N and B are typically ex-
tremely large, while k is small.
comes from integrating the results of corpus processing
with the inverted index (which determines which of those
results are relevant).
The neighborhood index avoids the need to return to
the original corpus, but it can consume a large amount
of disk space, as parts of the corpus text are folded into
the index several times. To conserve space, we perform
simple dictionary-lookup compression of strings in the
index. The storage penalty will, of course, depend on the
exact number of different types added to the index. In our
experiments, we created a useful IE system with a small
number of types (including NounPhrase) and found that
the neighborhood index increased disk space only four
times that of a standard inverted index.
Asymptotic Analysis:
In our asymptotic analysis of BE?s behavior, we count
query time as a function of the number of random disk
seeks, since these seeks dominate all other processing
tasks. Index space is simply the number of bytes needed
to store the index (not including the corpus itself).
Table 1 shows that BE requires only O(k) random disk
seeks to process queries with an arbitrary number of vari-
ables whereas a standard engine takes O(k + B), where
k is the number of concrete query terms, and B is the
number of bindings found in a corpus of N documents.
Thus, BE?s performance is the same as that of a standard
search engine for queries containing only concrete terms.
For variabilized queries, N may be in the billions and B
will tend to grow with N . In our experiments, eliminating
the B term from our query processing time has resulted
in speedups of two to three orders of magnitude over a
standard search engine. The speedup is at the price of a
small constant multiplier to index size.
2.1 Discussion
While BE has some attractive properties for NLP compu-
tations, is it necessary? Could fast, large-scale informa-
tion extraction be achieved merely by operating a ?pri-
vate? search engine?
The release of open-source search engines such as
Nutch2, coupled with the dropping price of CPUs and
2http://lucene.apache.org/nutch/
565
8.16
0.06
0
1
2
3
4
5
6
7
8
9
10
BE Nutch
El
ap
se
d 
m
in
u
te
s
Figure 2: Average time to return the relevant bindings
in response to a set of queries was 0.06 CPU minutes
for BE, compared to 8.16 CPU minutes for the com-
parable processing on Nutch. This is a 134-fold speed
up. The CPU resources, network, and index size were
the same for both systems.
disks, makes it feasible for NLP researchers to operate
their own large-scale search engines. For example, Tur-
ney operates a search engine with a terabyte-sized index
of Web pages, running on a local eight-machine Beowulf
cluster (Turney, 2004). Private search engines have two
advantages. First, there is no query quota or need for
?courtesy waits? between queries. Second, since the en-
gine is local, network latency is minimal.
However, to support IE, we must also execute the sec-
ond stage of the algorithm (see the beginning of this sec-
tion). In this stage, each document that matches a query
has to be retrieved from an arbitrary location on a disk.3
Thus, the number of random disk seeks scales linearly
with the number of documents retrieved. Moreover, many
NLP applications require the extraction of strings match-
ing particular syntactic or semantic types from each page.
The lack of linguistic data in the search engine?s index
means that many pages are fetched only to be discarded
as irrelevant.
To quantify the speedup due to BE, we compared it to a
standard search index built on the open-source Nutch en-
gine. All of our Nutch and BE experiments were carried
out on the same corpus of 60 million Web pages and were
run on a cluster of 23 dual-Xeon machines, each with two
local 140 Gb disks and 4 Gb of RAM. We set al config-
uration values to be exactly the same for both Nutch and
BE. BE gave a 134-fold speed up on average query pro-
cessing time when compared to the same queries with the
Nutch index, as shown in Figure 2.
3Moving the disk head to an arbitrary location on the disk
is a mechanical operation that takes about 5 milliseconds on
average.
3 The URNS Model
To realize the speedup from BE, KNOWITNOW must also
avoid issuing search engine queries to validate the cor-
rectness of each extraction, as required by PMI compu-
tation. We have developed a probabilistic model obviat-
ing search-engine queries for assessment. The intuition
behind this model is that correct instances of a class or
relation are likely to be extracted repeatedly, while ran-
dom errors by an IE system tend to have low frequency
for each distinct incorrect extraction.
Our probabilistic model, which we call URNS, takes the
form of a classic ?balls-and-urns? model from combina-
torics. We think of IE abstractly as a generative process
that maps text to extractions. Each extraction is modeled
as a labeled ball in an urn. A label represents either an
instance of the target class or relation, or represents an
error. The information extraction process is modeled as
repeated draws from the urn, with replacement.
Formally, the parameters that characterize an urn are:
? C ? the set of unique target labels; |C| is the number
of unique target labels in the urn.
? E ? the set of unique error labels; |E| is the number
of unique error labels in the urn.
? num(b) ? the function giving the number of balls
labeled by b where b ? C ? E. num(B) is the
multi-set giving the number of balls for each label
b ? B.
The goal of an IE system is to discern which of the
labels it extracts are in fact elements of C, based on re-
peated draws from the urn. Thus, the central question we
are investigating is: given that a particular label x was
extracted k times in a set of n draws from the urn, what
is the probability that x ? C? We can express the prob-
ability that an element extracted k of n times is of the
target relation as follows.
P (x ? C|x appears k times in n draws) =
?
r?num(C)( rs )k(1 ? rs )n?k
?
r??num(C?E)( r
?
s )k(1 ? r
?
s )n?k
(1)
where s is the total number of balls in the urn, and the
sum is taken over possible repetition rates r.
A few numerical examples illustrate the behavior of
this equation. Let |C| = |E| = 2, 000 and assume
for simplicity that all labels are repeated on the same
number of balls (num(ci) = RC for all ci ? C, and
num(ei) = RE for all ei ? E). Assume that the ex-
traction rules have precision p = 0.9, which means that
RC = 9 ? RE ? target balls are nine times as common
in the urn as error balls. Now, for k = 3 and n = 10, 000
we have P (x ? C) = 93.0%. Thus, we see that a small
number of repetitions can yield high confidence in an ex-
traction. However, when the sample size increases so that
566
n = 20, 000, and the other parameters are unchanged,
then P (x ? C) drops to 19.6%. On the other hand, if
C balls repeat much more frequently than E balls, say
RC = 90?RE (with |E| set to 20,000, so that p remains
unchanged), then P (x ? C) rises to 99.9%.
The above examples enable us to illustrate the advan-
tages of URNS over the noisy-or model used in previous
work. The noisy-or model assumes that each extraction is
an independent assertion that the extracted label is ?true,?
an assertion that is correct a fraction p of the time. The
noisy-or model assigns the following probability to ex-
tractions:
Pnoisy?or(x ? C|x appears k times) = 1 ? (1 ? p)k
Therefore, the noisy-or model will assign the same
probability ? 99.9% ? in all three of the above exam-
ples, although this is only correct in the case for which
n = 10, 000 and RC = 90?RE . As the other two exam-
ples show, for different sample sizes or repetition rates,
the noisy-or model can be highly inaccurate. This is not
surprising given that the noisy-or model ignores the sam-
ple size and the repetition rates.
URNS uses an EM algorithm to estimate its parameters,
and currently the algorithm takes roughly three minutes
to terminate.4 Fortunately, we determined experimen-
tally that we can approximate URNS?s precision and recall
using a far simpler frequency-based assessment method.
This is true because good precision and recall merely re-
quire an appropriate ordering of the extractions for each
relation, and not accurate probabilities for each extrac-
tion. For unary relations, we use the simple approxima-
tion that items extracted more often are more likely to
be true, and order the extractions from most to least ex-
tracted. For binary relations like CapitalOf(X,y),
in which we extract several different candidate capitals y
for each known country X, we use a smoothed frequency
estimate to order the extractions. Let freq(R(X, y)) de-
note the number of times that the binary relation R(X, y)
is extracted; we define:
smoothed freq(R(X, y)) = freq(R(X, y))maxy? freq(R(X, y?)) + 1
We found that sorting by smoothed frequency (in de-
scending order) performed better than simply sorting by
freq for relations R(X, y) in which different known X val-
ues may have widely varying Web presence.
Unlike URNS, our frequency-based assessment does
not yield accurate probabilities to associate with each ex-
traction, but for the purpose of returning a ranked list of
high-quality extractions it is comparable to URNS (see
4This code has not been optimized at all. We believe that
we can easily reduce its running time to less than a minute on
average, and perhaps substantially more.
0.75
0.8
0.85
0.9
0.95
1
0 50 100 150 200 250
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 3: Country: KNOWITALL maintains some-
what higher precision than KNOWITNOW throughout
the recall-precision curve.
Figures 3 through 6), and it has the advantage of being
much faster. Thus, in the experiments reported on below,
we use frequency-based assessment as part of KNOWIT-
NOW.
4 Experimental Results
This section contrasts the performance of KNOWITNOW
and KNOWITALL experimentally. Before considering the
experiments in detail, we note that a key advantage of
KNOWITNOW is that it does not make any queries to Web
search engines. As a result, KNOWITNOW?s scale is not
limited by a query quota, though it is limited by the size
of its index.
We report on the following metrics:
? Recall: how many distinct extractions does each
system return at high precision?5
? Time: how long did each system take to produce
and rank its extractions?
? Extraction Rate: how many distinct high-quality
extractions does the system return per minute? The
extraction rate is simply recall divided by time.
We contrast KNOWITALL and KNOWITNOW?s preci-
sion/recall curves in Figures 3 through 6. We com-
pared KNOWITNOW with KNOWITALL on four rela-
tions: Corp, Country, CeoOf(Corp,Ceo), and
CapitalOf(Country,City). The unary relations
were chosen to examine the difference between a relation
with a small number of correct instances (Country) and
one with a large number of extractions (Corp). The bi-
nary relations were chosen to cover both functional rela-
tions (CapitalOf) and set-valued relations (CeoOf?
we treat former CEOs as correct instances of the relation).
5Since we cannot compute ?true recall? for most relations
on the Web, the paper uses the term ?recall? to refer to the size
of the set of facts extracted.
567
0.75
0.8
0.85
0.9
0.95
1
0 50 100 150 200
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 4: CapitalOf: KNOWITNOW does nearly as
well as KNOWITALL, but has more difficulty than
KNOWITALL with sparse data for capitals of more ob-
scure countries.
For the two unary relations, both systems created ex-
traction rules from eight generic patterns. These are hy-
ponym patterns like ?NP1 {,} such as NPList2? or ?NP2
{,} and other NP1?, which extract members of NPList2
or NP2 as instances of NP1. For the binary relations,
the systems instantiated rules from four generic patterns.
These are patterns for a generic ?of? relation. They are
?NP1 , rel of NP2?, ?NP1 the rel of NP2?, ?rel of NP2
, NP1?, and ?NP2 rel NP1?. When rel is instantiated for
CeoOf, these patterns become ?NP1 , CEO of NP2? and
so forth.
Both KNOWITNOW and KNOWITALL merge extrac-
tions with slight variants in the name, such as those dif-
fering only in punctuation or whitespace, or in the pres-
ence or absence of a corporate designator. For binary
extractions, CEOs with the same last name and same
company were also merged. Both systems rely on the
OpenNlp maximum-entropy part-of-speech tagger and
chunker (Ratnaparkhi, 1996), but KNOWITALL applies
them to pages downloaded from the Web based on the re-
sults of Google queries, whereas KNOWITNOW applies
them once to crawled and indexed pages.6 Overall, each
of the above elements of KNOWITALL and KNOWIT-
NOW are the same to allow for controlled experiments.
Whereas KNOWITNOW runs a small number of vari-
abilized queries (one for each extraction pattern, for
each relation), KNOWITALL requires a stopping crite-
rion. Otherwise, KNOWITALL will continue to query
Google and download URLs found in its result pages over
many days and even weeks. We allowed a total of 6 days
of search time for KNOWITALL, allocating more search
for the relations that continued to be most productive. For
CeoOf KNOWITNOW returned all pairs of Corp,Ceo
6Our time measurements for KNOWITALL are not affected
by the tagging and chunking time because it is dominated
by time required to query Google, waiting a second between
queries.
0.75
0.8
0.85
0.9
0.95
1
0 5,000 10,000 15,000 20,000 25,000
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowItNow-URNS
KnowItAll-PMI
Figure 5: Corp: KNOWITALL?s PMI assessment main-
tains high precision. KNOWITNOW has low recall up
to precision 0.85, then catches up with KNOWITALL.
in its corpus; KNOWITALL searched for CEOs of a ran-
dom selection of 10% of the corporations it found, and
we projected the total extractions and search effort for all
corporations. For CapitalOf, both KNOWITNOW and
KNOWITALL looked for capitals of a set of 195 coun-
tries.
Table 2 shows the number of queries, search time, dis-
tinct correct extractions at precision 0.8, and extraction
rate for each relation. Search time for KNOWITNOW is
measured in seconds and search time for KNOWITALL
is measured in hours. The number of extractions per
minute counts the distinct correct extractions. Since we
limit KNOWITALL to one Google query per second, the
time for KNOWITALL is proportional to the number of
queries. KNOWITNOW?s extraction rate is from 275 to
4,707 times that of KNOWITALL at this level of preci-
sion.
While the number of distinct correct extractions from
KNOWITNOW at precision 0.8 is roughly comparable to
that of 6 days search effort from KNOWITALL, the sit-
uation is different at precision 0.9. KNOWITALL?s PMI
assessor is able to maintain higher precision than KNOW-
ITNOW?s frequency-based assessor. The number of cor-
rect corporations for KNOWITNOW drops from 23,128 at
precision 0.8 to 1,116 at precision 0.9. KNOWITALL is
able to identify 17,620 correct corporations at precision
0.9. Even with the drop in recall, KNOWITNOW?s ex-
traction rate is still 305 times higher than KNOWITALL?s.
The reason for KNOWITNOW?s difficulty at precision 0.9
is due to extraction errors that occur with high frequency,
particularly generic references to companies (?the Seller
is a corporation ...?, ?corporations such as Banks?, etc.)
and truncation of certain company names by the extrac-
tion rules. The more expensive PMI-based assessment
was not fooled by these systematic extraction errors.
Figures 3 through 6 show the recall-precision curves
for KNOWITNOW with URNS assessment, KNOWIT-
NOW with the simpler frequency-based assessment, and
568
Google Queries Time Extractions Extractions per minute
NOW ALL NOW (sec) ALL (hrs) NOW ALL NOW ALL ratio
Corp 0 (16) 201,878 42 56.1 23,128 23,617 33,040 7.02 4,707
Country 0 (16) 35,480 42 9.9 161 203 230 0.34 672
CeoOf 0 (6) 263,646 51 73.2 2,402 5,823 2,836 1.33 2,132
CapitalOf 0 (6) 17,216 55 4.8 169 192 184 0.67 275
Table 2: Comparison of KNOWITNOW with KNOWITALL for four relations, showing number of Google queries
(local BE queries in parentheses), search time, correct extractions at precision 0.8, and extraction rate (the
number of correct extractions at precision 0.8 per minute of search). Overall, KNOWITNOW took a total of
slightly over 3 minutes as compared to a total of 6 days of search for KNOWITALL.
0.75
0.8
0.85
0.9
0.95
1
0 2,000 4,000 6,000
Correct Extractions
Pr
ec
is
io
n
KnowItNow-freq KnowitNow-URNS
KnowItAll-PMI
Figure 6: CeoOf: KNOWITNOW has difficulty dis-
tinguishing low frequency correct extractions from
noise. KNOWITALL is able to cope with the sparse
data more effectively.
KNOWITALL with PMI-based assessment. For each of
the four relations, PMI is able to maintain a higher pre-
cision than either frequency-based or URNS assessment.
URNS and frequency-based assessment give roughly the
same levels of precision.
For the relations with a small number of correct in-
stances, Country and CapitalOf, KNOWITNOW is
able to identify 70-80% as many instances as KNOW-
ITALL at precision 0.9. In contrast, Corp and CeoOf
have a huge number of correct instances and a long tail
of low frequency extractions that KNOWITNOW has dif-
ficulty distinguishing from noise. Over one fourth of
the corporations found by KNOWITALL had Google hit
counts less than 10,500, a sparseness problem that was
exacerbated by KNOWITNOW?s limited index size.
Figure 7 shows projected recall from larger KNOW-
ITNOW indices, fitting a sigmoid curve to the recall
from index size of 10M, 20M, up to 60M pages. The
curve was fitted using logistic regression, and is restricted
to asymptote at the level reported for Google-based
KNOWITALL for each relation. We report re-
call at precision 0.9 for capitals of 195 coun-
tries and CEOs of a random selection of the
top 5,000 corporations as ranked by PMI.
Recall is defined as the percent of countries with a
0
0.2
0.4
0.6
0.8
1
0 100 200 300 400
KnowItNow index size (millions)
R
ec
al
l a
t 0
.
9 
pr
ec
is
io
n
KnowItNow CeoOf
Google CeoOf
KnowItNow CapitalOf
Google CapitalOf
Figure 7: Projections of recall (at precision 0.9) as a
function of KNOWITNOW index size. At 400 million
pages, KNOWITNOW?s recall rapidly approaches the
recall achieved by KNOWITALL using roughly 300,000
Google queries.
correct capital or the number of correct CEOs divided by
the number of corporations.
The curve for CeoOf is rising steeply enough that a
400 million page KNOWITNOW index may approach the
same level of recall yielded by KNOWITALL when it uses
300,000 Google queries. As shown in Table 2, KNOW-
ITALL takes slightly more than three days to generate
these results. KNOWITNOW would operate over a cor-
pus 6.7 times its current one, but the number of required
random disk seeks (and the asymptotic run time analy-
sis) would remain the same. We thus expect that with a
larger corpus we can construct a KNOWITNOW system
that reproduces KNOWITALL levels of precision and re-
call while still executing in the order of a few minutes.
5 Related Work
There has been very little work published on how to make
NLP computations such as PMI-IR and IE fast for large
corpora. Indeed, extraction rate is not a metric typically
used to evaluate IE systems, but we believe it is an im-
portant metric if IE is to scale.
Hobbs et al point out the advantage of fast text
processing for rapid system development (Hobbs et al,
1992). They could test each change to system parameters
569
and domain-specific patterns on a large sample of docu-
ments, having moved from a system that took 36 hours to
process 100 documents to FASTUS, which took only 11
minutes. This allowed them to develop one of the highest
performing MUC-4 systems in only one month.
While there has been extensive work in the IR and
Web communities on improvements to the standard in-
verted index scheme, there has been little work on effi-
cient large-scale search to support natural language ap-
plications. One exception is Resnik?s Linguist?s Search
Engine (Elkiss and Resnik, 2004), a tool for searching
large corpora of parse trees. There is little published in-
formation about its indexing system, but the user man-
ual suggests its corpus is a combination of indexed sen-
tences and user-specific document collections driven by
the user?s AltaVista queries. In contrast, the BE system
has a single index, constructed just once, that serves all
queries. There is no published performance data avail-
able for Resnik?s system.
6 Conclusions and Future Directions
In previous work, statistical NLP computation over large
corpora has been a slow, offline process, as in KNOW-
ITALL (Etzioni et al, 2005) and also in PMI-IR appli-
cations such as sentiment classification (Turney, 2002).
Technology trends, and open source search engines such
as Nutch, have made it feasible to create ?private? search
engines that index large collections of documents; but as
shown in Figure 2, firing large numbers of queries at pri-
vate search engines is still slow.
This paper described a novel and practical approach
towards substantially speeding up IE. We described
KNOWITNOW, which extracts thousands of facts in min-
utes instead of days. Furthermore, we sketched URNS,
a probabilistic model that both obviates the need for
search-engine queries and outputs more accurate prob-
abilities than PMI-IR. Finally, we introduced a simple,
efficient approximation to URNS, whose probability esti-
mates are not as good, but which has comparable preci-
sion/recall to URNS, making it an appropriate assessor for
KNOWITNOW.
The speed and massively improved extraction rate of
KNOWITNOW come at the cost of reduced recall. We
quantified this tradeoff in Table 2, and also argued that as
KNOWITNOW?s index size increases from 60 million to
400 million pages, KNOWITNOW would achieve in min-
utes the same precision/recall that takes KNOWITALL
days to obtain. Of course, a hybrid approach is possi-
ble where KNOWITNOW has, say, a 100 million page
index and, when necessary, augments its results with a
limited number of queries to Google. Investigating the
extraction-rate/recall tradeoff in such a hybrid system is
a natural next step.
While our experiments have used the Web corpus, our
approach transfers readily to other large corpora; exper-
imentation with other corpora is another topic for future
work. In conclusion, we believe that our techniques trans-
form IE from a slow, offline process to an online one.
They could open the door to a new class of interactive IE
applications, of which KNOWITNOW is merely the first.
7 Acknowledgments
This research was supported in part by NSF grant IIS-
0312988, DARPA contract NBCHD030010, ONR grant
N00014-02-1-0324, and gifts from Google and the Tur-
ing Center.
References
R. Baeza-Yates and B. Ribeiro-Neto. 1999. Modern Informa-
tion Retrieval. Addison Wesley.
E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng. 2001.
Data-intensive question answering. In Procs. of Text RE-
trieval Conference (TREC-10), pages 393?400.
M. Cafarella and O. Etzioni. 2005. A Search Engine for Nat-
ural Language Applications. In Procs. of the 14th Interna-
tional World Wide Web Conference (WWW 2005).
D. Downey, O. Etzioni, and S. Soderland. 2005. A Probabilistic
Model of Redundancy in Information Extraction. In Procs.
of the 19th International Joint Conference on Artificial Intel-
ligence (IJCAI 2005).
E. Elkiss and P. Resnik, 2004. The Linguist?s Search Engine
User?s Guide. University of Maryland.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Un-
supervised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
M. Hearst. 1992. Automatic Acquisition of Hyponyms from
Large Text Corpora. In Procs. of the 14th International
Conference on Computational Linguistics, pages 539?545,
Nantes, France.
J.R. Hobbs, D. Appelt, M. Tyson, J. Bear, and D. Israel. 1992.
Description of the FASTUS system used for MUC-4. In
Procs. of the Fourth Message Understanding Conference,
pages 268?275.
A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tag-
ger. In Procs. of the Empirical Methods in Natural Language
Processing Conference, Univ. of Pennsylvania.
P. D. Turney. 2001. Mining the Web for Synonyms: PMI-IR
versus LSA on TOEFL. In Procs. of the Twelfth European
Conference on Machine Learning (ECML-2001), pages 491?
502, Freiburg, Germany.
P. D. Turney. 2002. Thumbs up or thumbs down? semantic
orientation applied to unsupervised classification of reviews.
In Procs. of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL?02), pages 417?424.
P. D. Turney, 2004. Waterloo MultiText System. Institute for
Information Technology, Nat?l Research Council of Canada.
570
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 696?703,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Sparse Information Extraction:
Unsupervised Language Models to the Rescue
Doug Downey, Stefan Schoenmackers, and Oren Etzioni
Turing Center, Department of Computer Science and Engineering
University of Washington, Box 352350
Seattle, WA 98195, USA
{ddowney,stef,etzioni}@cs.washington.edu
Abstract
Even in a massive corpus such as the Web, a
substantial fraction of extractions appear in-
frequently. This paper shows how to assess
the correctness of sparse extractions by uti-
lizing unsupervised language models. The
REALM system, which combines HMM-
based and n-gram-based language models,
ranks candidate extractions by the likeli-
hood that they are correct. Our experiments
show that REALM reduces extraction error
by 39%, on average, when compared with
previous work.
Because REALM pre-computes language
models based on its corpus and does not re-
quire any hand-tagged seeds, it is far more
scalable than approaches that learn mod-
els for each individual relation from hand-
tagged data. Thus, REALM is ideally suited
for open information extraction where the
relations of interest are not specified in ad-
vance and their number is potentially vast.
1 Introduction
Information Extraction (IE) from text is far from in-
fallible. In response, researchers have begun to ex-
ploit the redundancy in massive corpora such as the
Web in order to assess the veracity of extractions
(e.g., (Downey et al, 2005; Etzioni et al, 2005;
Feldman et al, 2006)). In essence, such methods uti-
lize extraction patterns to generate candidate extrac-
tions (e.g., ?Istanbul?) and then assess each candi-
date by computing co-occurrence statistics between
the extraction and words or phrases indicative of
class membership (e.g., ?cities such as?).
However, Zipf?s Law governs the distribution of
extractions. Thus, even the Web has limited redun-
dancy for less prominent instances of relations. In-
deed, 50% of the extractions in the data sets em-
ployed by (Downey et al, 2005) appeared only
once. As a result, Downey et al?s model, and re-
lated methods, had no way of assessing which ex-
traction is more likely to be correct for fully half of
the extractions. This problem is particularly acute
when moving beyond unary relations. We refer to
this challenge as the task of assessing sparse extrac-
tions.
This paper introduces the idea that language mod-
eling techniques such as n-gram statistics (Manning
and Schu?tze, 1999) and HMMs (Rabiner, 1989) can
be used to effectively assess sparse extractions. The
paper introduces the REALM system, and highlights
its unique properties. Notably, REALM does not
require any hand-tagged seeds, which enables it to
scale to Open IE?extraction where the relations of
interest are not specified in advance, and their num-
ber is potentially vast (Banko et al, 2007).
REALM is based on two key hypotheses. The
KnowItAll hypothesis is that extractions that oc-
cur more frequently in distinct sentences in the
corpus are more likely to be correct. For exam-
ple, the hypothesis suggests that the argument pair
(Giuliani, New York) is relatively likely to be
appropriate for the Mayor relation, simply because
this pair is extracted for the Mayor relation rela-
tively frequently. Second, we employ an instance of
the distributional hypothesis (Harris, 1985), which
696
can be phrased as follows: different instances of
the same semantic relation tend to appear in sim-
ilar textual contexts. We assess sparse extractions
by comparing the contexts in which they appear to
those of more common extractions. Sparse extrac-
tions whose contexts are more similar to those of
common extractions are judged more likely to be
correct based on the conjunction of the KnowItAll
and the distributional hypotheses.
The contributions of the paper are as follows:
? The paper introduces the insight that the sub-
field of language modeling provides unsuper-
vised methods that can be leveraged to assess
sparse extractions. These methods are more
scalable than previous assessment techniques,
and require no hand tagging whatsoever.
? The paper introduces an HMM-based tech-
nique for checking whether two arguments are
of the proper type for a relation.
? The paper introduces a relational n-gram
model for the purpose of determining whether
a sentence that mentions multiple arguments
actually expresses a particular relationship be-
tween them.
? The paper introduces a novel language-
modeling system called REALM that combines
both HMM-based models and relational n-
gram models, and shows that REALM reduces
error by an average of 39% over previous meth-
ods, when applied to sparse extraction data.
The remainder of the paper is organized as fol-
lows. Section 2 introduces the IE assessment task,
and describes the REALM system in detail. Section
3 reports on our experimental results followed by a
discussion of related work in Section 4. Finally, we
conclude with a discussion of scalability and with
directions for future work.
2 IE Assessment
This section formalizes the IE assessment task and
describes the REALM system for solving it. An IE
assessor takes as input a list of candidate extractions
meant to denote instances of a relation, and outputs
a ranking of the extractions with the goal that cor-
rect extractions rank higher than incorrect ones. A
correct extraction is defined to be a true instance of
the relation mentioned in the input text.
More formally, the list of candidate extrac-
tions for a relation R is denoted as ER =
{(a1, b1), . . . , (am, bm)}. An extraction (ai, bi) is
an ordered pair of strings. The extraction is correct
if and only if the relation R holds between the argu-
ments named by ai and bi. For example, for R =
Headquartered, a pair (ai, bi) is correct iff there
exists an organization ai that is in fact headquartered
in the location bi.1
ER is generated by applying an extraction mech-
anism, typically a set of extraction ?patterns?, to
each sentence in a corpus, and recording the results.
Thus, many elements of ER are identical extractions
derived from different sentences in the corpus.
This task definition is notable for the minimal
inputs required?IE assessment does not require
knowing the relation name nor does it require hand-
tagged seed examples of the relation. Thus, an IE
Assessor is applicable to Open IE.
2.1 System Overview
In this section, we describe the REALM system,
which utilizes language modeling techniques to per-
form IE Assessment.
REALM takes as input a set of extractions ER,
and outputs a ranking of those extractions. The
algorithm REALM follows is outlined in Figure 1.
REALM begins by automatically selecting from ER
a set of bootstrapped seeds SR intended to serve as
correct examples of the relation R. REALM utilizes
the KnowItAll hypothesis, setting SR equal to the
h elements in ER extracted most frequently from
the underlying corpus. This results in a noisy set of
seeds, but the methods that use these seeds are noise
tolerant.
REALM then proceeds to rank the remaining
(non-seed) extractions by utilizing two language-
modeling components. An n-gram language model
is a probability distribution P (w1, ..., wn) over con-
secutive word sequences of length n in a corpus.
Formally, if we assume a seed (s1, s2) is a correct
extraction of a relation R, the distributional hypoth-
esis states that the context distribution around the
seed extraction, P (w1, ..., wn|wi = s1, wj = s2)
for 1 ? i, j ? n tends to be ?more similar? to
1For clarity, our discussion focuses on relations between
pairs of arguments. However, the methods we propose can be
extended to relations of any arity.
697
P (w1, ..., wn|wi = e1, wj = e2) when the extrac-
tion (e1, e2) is correct. Naively comparing context
distributions is problematic, however, because the
arguments to a relation often appear separated by
several intervening words. In our experiments, we
found that when relation arguments appear together
in a sentence, 75% of the time the arguments are
separated by at least three words. This implies that
n must be large, and for sparse argument pairs it is
not possible to estimate such a large language model
accurately, because the number of modeling param-
eters is proportional to the vocabulary size raised to
the nth power. To mitigate sparsity, REALM utilizes
smaller language models in its two components as a
means of ?backing-off? from estimating context dis-
tributions explicitly, as described below.
First, REALM utilizes an HMM to estimate
whether each extraction has arguments of the proper
type for the relation. Each relation R has a set
of types for its arguments. For example, the rela-
tion AuthorOf(a, b) requires that its first ar-
gument be an author, and that its second be some
kind of written work. Knowing whether extracted
arguments are of the proper type for a relation can
be quite informative for assessing extractions. The
challenge is, however, that this type information is
not given to the system since the relations (and the
types of the arguments) are not known in advance.
REALM solves this problem by comparing the dis-
tributions of the seed arguments and extraction ar-
guments. Type checking mitigates data sparsity by
leveraging every occurrence of the individual extrac-
tion arguments in the corpus, rather than only those
cases in which argument pairs occur near each other.
Although argument type checking is invalu-
able for extraction assessment, it is not suf-
ficient for extracting relationships between ar-
guments. For example, an IE system us-
ing only type information might determine that
Intel is a corporation and that Seattle is
a city, and therefore erroneously conclude that
Headquartered(Intel, Seattle) is cor-
rect. Thus, REALM?s second step is to employ an
n-gram-based language model to assess whether the
extracted arguments share the appropriate relation.
Again, this information is not given to the system,
so REALM compares the context distributions of the
extractions to those of the seeds. As described in
REALM(Extractions ER = {e1, ..., em})
SR = the h most frequent extractions in ER
UR = ER - SR
TypeRankings(UR)? HMM-T(SR, UR)
RelationRankings(UR)? REL-GRAMS(SR, UR)
return a ranking of ER with the elements of SR at the
top (ranked by frequency) followed by the elements of
UR = {u1, ..., um?h} ranked in ascending order of
TypeRanking(ui) ?RelationRanking(ui).
Figure 1: Pseudocode for REALM at run-time.
The language models used by the HMM-T and
REL-GRAMS components are constructed in a pre-
processing step.
Section 2.3, REALM employs a relational n-gram
language model in order to accurately compare con-
text distributions when extractions are sparse.
REALM executes the type checking and relation
assessment components separately; each component
takes the seed and non-seed extractions as arguments
and returns a ranking of the non-seeds. REALM then
combines the two components? assessments into a
single ranking. Although several such combinations
are possible, REALM simply ranks the extractions in
ascending order of the product of the ranks assigned
by the two components. The following subsections
describe REALM?s two components in detail.
We identify the proper nouns in our corpus us-
ing the LEX method (Downey et al, 2007). In ad-
dition to locating the proper nouns in the corpus,
LEX also concatenates each multi-token proper noun
(e.g.,Los Angeles) together into a single token.
Both of REALM?s components construct language
models from this tokenized corpus.
2.2 Type Checking with HMM-T
In this section, we describe our type-checking com-
ponent, which takes the form of a Hidden Markov
Model and is referred to as HMM-T. HMM-T ranks
the set UR of non-seed extractions, with a goal of
ranking those extractions with arguments of proper
type for R above extractions containing type errors.
Formally, let URi denote the set of the ith arguments
of the extractions in UR. Let SRi be defined simi-
larly for the seed set SR.
Our type checking technique exploits the distri-
butional hypothesis?in this case, the intuition that
698
Intel , headquartered in Santa+Clara
Figure 2: Graphical model employed by HMM-
T. Shown is the case in which k = 2. Corpus
pre-processing results in the proper noun Santa
Clara being concatenated into a single token.
extraction arguments in URi of the proper type will
likely appear in contexts similar to those in which
the seed arguments SRi appear. In order to iden-
tify terms that are distributionally similar, we train
a probabilistic generative Hidden Markov Model
(HMM), which treats each token in the corpus as
generated by a single hidden state variable. Here, the
hidden states take integral values from {1, . . . , T},
and each hidden state variable is itself generated by
some number k of previous hidden states.2 For-
mally, the joint distribution of the corpus, repre-
sented as a vector of tokens w, given a correspond-
ing vector of states t is:
P (w|t) =
?
i
P (wi|ti)P (ti|ti?1, . . . , ti?k) (1)
The distributions on the right side of Equation 1
can be learned from a corpus in an unsupervised
manner, such that words which are distributed sim-
ilarly in the corpus tend to be generated by simi-
lar hidden states (Rabiner, 1989). The generative
model is depicted as a Bayesian network in Figure 2.
The figure also illustrates the one way in which our
implementation is distinct from a standard HMM,
namely that proper nouns are detected a priori and
modeled as single tokens (e.g., Santa Clara is
generated by a single hidden state). This allows
the type checker to compare the state distributions
of different proper nouns directly, even when the
proper nouns contain differing numbers of words.
To generate a ranking of UR using the learned
HMM parameters, we rank the arguments ei accord-
ing to how similar their state distributions P (t|ei)
2Our implementation makes the simplifying assumption that
each sentence in the corpus is generated independently.
are to those of the seed arguments.3 Specifically, we
define a function:
f(e) =
?
ei?e
KL(
?
w??SRi
P (t|w?)
|SRi|
, P (t|ei)) (2)
where KL represents KL divergence, and the outer
sum is taken over the arguments ei of the extraction
e. We rank the elements of UR in ascending order of
f(e).
HMM-T has two advantages over a more tradi-
tional type checking approach of simply counting
the number of times in the corpus that each extrac-
tion appears in a context in which a seed also ap-
pears (cf. (Ravichandran et al, 2005)). The first
advantage of HMM-T is efficiency, as the traditional
approach involves a computationally expensive step
of retrieving the potentially large set of contexts in
which the extractions and seeds appear. In our ex-
periments, using HMM-T instead of a context-based
approach results in a 10-50x reduction in the amount
of data that is retrieved to perform type checking.
Secondly, on sparse data HMM-T has the poten-
tial to improve type checking accuracy. For exam-
ple, consider comparing Pickerington, a sparse
candidate argument of the type City, to the seed
argument Chicago, for which the following two
phrases appear in the corpus:
(i) ?Pickerington, Ohio?
(ii) ?Chicago, Illinois?
In these phrases, the textual contexts surrounding
Chicago and Pickerington are not identical,
so to the traditional approach these contexts offer
no evidence that Pickerington and Chicago
are of the same type. For a sparse token like
Pickerington, this is problematic because the
token may never occur in a context that precisely
matches that of a seed. In contrast, in the HMM, the
non-sparse tokens Ohio and Illinois are likely
to have similar state distributions, as they are both
the names of U.S. States. Thus, in the state space
employed by the HMM, the contexts in phrases (i)
and (ii) are in fact quite similar, allowing HMM-
T to detect that Pickerington and Chicago
are likely of the same type. Our experiments quan-
tify the performance improvements that HMM-T of-
3The distribution P (t|ei) for any ei can be obtained from
the HMM parameters using Bayes Rule.
699
fers over the traditional approach for type checking
sparse data.
The time required to learn HMM-T?s parameters
scales proportional to T k+1 times the corpus size.
Thus, for tractability, HMM-T uses a relatively small
state space of T = 20 states and a limited k value
of 3. While these settings are sufficient for type
checking (e.g., determining that Santa Clara is
a city) they are too coarse-grained to assess relations
between arguments (e.g., determining that Santa
Clara is the particular city in which Intel is
headquartered). We now turn to the REL-GRAMS
component, which performs the latter task.
2.3 Relation Assessment with REL-GRAMS
REALM?s relation assessment component, called
REL-GRAMS, tests whether the extracted arguments
have a desired relationship, but given REALM?s min-
imal input it has no a priori information about the
relationship. REL-GRAMS relies instead on the dis-
tributional hypothesis to test each extraction.
As argued in Section 2.1, it is intractable to build
an accurate language model for context distributions
surrounding sparse argument pairs. To overcome
this problem, we introduce relational n-gram mod-
els. Rather than simply modeling the context distri-
bution around a given argument, a relational n-gram
model specifies separate context distributions for an
arguments conditioned on each of the other argu-
ments with which it appears. The relational n-gram
model allows us to estimate context distributions for
pairs of arguments, even when the arguments do not
appear together within a fixed window of n words.
Further, by considering only consecutive argument
pairs, the number of distinct argument pairs in the
model grows at most linearly with the number of
sentences in the corpus. Thus, the relational n-gram
model can scale.
Formally, for a pair of arguments (e1, e2), a re-
lational n-gram model estimates the distributions
P (w1, ..., wn|wi = e1, e1 ? e2) for each 1 ? i ?
n, where the notation e1 ? e2 indicates the event
that e2 is the next argument to either the right or the
left of e1 in the corpus.
REL-GRAMS begins by building a relational n-
gram model of the arguments in the corpus. For
notational convenience, we represent the model?s
distributions in terms of ?context vectors? for each
pair of arguments. Formally, for a given sentence
containing arguments e1 and e2 consecutively, we
define a context of the ordered pair (e1, e2) to be
any window of n tokens around e1. Let C =
{c1, c2, ..., c|C|} be the set of all contexts of all ar-
gument pairs found in the corpus.4 For a pair of ar-
guments (ej , ek), we model their relationship using
a |C| dimensional context vector v(ej ,ek), whose i-th
dimension corresponds to the number of times con-
text ci occurred with the pair (ej , ek) in the corpus.
These context vectors are similar to document vec-
tors from Information Retrieval (IR), and we lever-
age IR research to compare them, as described be-
low.
To assess each extraction, we determine how sim-
ilar its context vector is to a canonical seed vec-
tor (created by summing the context vectors of the
seeds). While there are many potential methods
for determining similarity, in this work we rank ex-
tractions by decreasing values of the BM25 dis-
tance metric. BM25 is a TF-IDF variant intro-
duced in TREC-3(Robertson et al, 1992), which
outperformed both the standard cosine distance and
a smoothed KL divergence on our data.
3 Experimental Results
This section describes our experiments on IE assess-
ment for sparse data. We start by describing our
experimental methodology, and then present our re-
sults. The first experiment tests the hypothesis that
HMM-T outperforms an n-gram-based method on
the task of type checking. The second experiment
tests the hypothesis that REALM outperforms multi-
ple approaches from previous work, and also outper-
forms each of its HMM-T and REL-GRAMS compo-
nents taken in isolation.
3.1 Experimental Methodology
The corpus used for our experiments consisted of a
sample of sentences taken from Web pages. From
an initial crawl of nine million Web pages, we se-
lected sentences containing relations between proper
nouns. The resulting text corpus consisted of about
4Pre-computing the set C requires identifying in advance
the potential relation arguments in the corpus. We consider the
proper nouns identified by the LEX method (see Section 2.1) to
be the potential arguments.
700
three million sentences, and was tokenized as de-
scribed in Section 2. For tractability, before and after
performing tokenization, we replaced each token oc-
curring fewer than five times in the corpus with one
of two ?unknown word? markers (one for capital-
ized words, and one for uncapitalized words). This
preprocessing resulted in a corpus containing about
sixty-five million total tokens, and 214,787 unique
tokens.
We evaluated performance on four relations:
Conquered, Founded, Headquartered, and
Merged. These four relations were chosen because
they typically take proper nouns as arguments, and
included a large number of sparse extractions. For
each relationR, the candidate extraction listER was
obtained using TEXTRUNNER (Banko et al, 2007).
TEXTRUNNER is an IE system that computes an in-
dex of all extracted relationships it recognizes, in the
form of (object, predicate, object) triples. For each
of our target relations, we executed a single query
to the TEXTRUNNER index for extractions whose
predicate contained a phrase indicative of the rela-
tion (e.g., ?founded by?, ?headquartered in?), and
the results formed our extraction list. For each rela-
tion, the 10 most frequent extractions served as boot-
strapped seeds. All of the non-seed extractions were
sparse (no argument pairs were extracted more than
twice for a given relation). These test sets contained
a total of 361 extractions.
3.2 Type Checking Experiments
As discussed in Section 2.2, on sparse data HMM-T
has the potential to outperform type checking meth-
ods that rely on textual similarities of context vec-
tors. To evaluate this claim, we tested the HMM-T
system against an N-GRAMS type checking method
on the task of type-checking the arguments to a re-
lation. The N-GRAMS method compares the context
vectors of extractions in the same way as the REL-
GRAMS method described in Section 2.3, but is not
relational (N-GRAMS considers the distribution of
each extraction argument independently, similar to
HMM-T). We tagged an extraction as type correct iff
both arguments were valid for the relation, ignoring
whether the relation held between the arguments.
The results of our type checking experiments are
shown in Table 1. For all types, HMM-T outper-
forms N-GRAMS, and HMM-T reduces error (mea-
Type HMM-T N-GRAMS
Conquered 0.917 0.767
Founded 0.827 0.636
Headquartered 0.734 0.589
Merged 0.920 0.854
Average 0.849 0.712
Table 1: Type Checking Performance. Listed is area
under the precision/recall curve. HMM-T outper-
forms N-GRAMS for all relations, and reduces the
error in terms of missing area under the curve by
46% on average.
sured in missing area under the precision/recall
curve) by 46%. The performance difference on each
relation is statistically significant (p < 0.01, two-
sampled t-test), using the methodology for measur-
ing the standard deviation of area under the preci-
sion/recall curve given in (Richardson and Domin-
gos, 2006). N-GRAMS, like REL-GRAMS, employs
the BM-25 metric to measure distributional similar-
ity between extractions and seeds. Replacing BM-
25 with cosine distance cuts HMM-T?s advantage
over N-GRAMS, but HMM-T?s error rate is still 23%
lower on average.
3.3 Experiments with REALM
The REALM system combines the type checking
and relation assessment components to assess ex-
tractions. Here, we test the ability of REALM to
improve the ranking of a state of the art IE system,
TEXTRUNNER. For these experiments, we evalu-
ate REALM against the TEXTRUNNER frequency-
based ordering, a pattern-learning approach, and the
HMM-T and REL-GRAMS components taken in iso-
lation. The TEXTRUNNER frequency-based order-
ing ranks extractions in decreasing order of their ex-
traction frequency, and importantly, for our task this
ordering is essentially equivalent to that produced by
the ?Urns? (Downey et al, 2005) and Pointwise Mu-
tual Information (Etzioni et al, 2005) approaches
employed in previous work.
The pattern-learning approach, denoted as PL, is
modeled after Snowball (Agichtein, 2006). The al-
gorithm and parameter settings for PL were those
manually tuned for the Headquartered relation
in previous work (Agichtein, 2005). A sensitivity
analysis of these parameters indicated that the re-
701
Conquered Founded Headquartered Merged Average
Avg. Prec. 0.698 0.578 0.400 0.742 0.605
TEXTRUNNER 0.738 0.699 0.710 0.784 0.733
PL 0.885 0.633 0.651 0.852 0.785
PL+ HMM-T 0.883 0.722 0.727 0.900 0.808
HMM-T 0.830 0.776 0.678 0.864 0.787
REL-GRAMS 0.929 (39%) 0.713 0.758 0.886 0.822
REALM 0.907 (19%) 0.781 (27%) 0.810 (35%) 0.908 (38%) 0.851 (39%)
Table 2: Performance of REALM for assessment of sparse extractions. Listed is area under the preci-
sion/recall curve for each method. In parentheses is the percentage reduction in error over the strongest
baseline method (TEXTRUNNER or PL) for each relation. ?Avg. Prec.? denotes the fraction of correct
examples in the test set for each relation. REALM outperforms its REL-GRAMS and HMM-T components
taken in isolation, as well as the TEXTRUNNER and PL systems from previous work.
sults are sensitive to the parameter settings. How-
ever, we found no parameter settings that performed
significantly better, and many settings performed
significantly worse. As such, we believe our re-
sults reasonably reflect the performance of a pattern
learning system on this task. Because PL performs
relation assessment, we also attempted combining
PL with HMM-T in a hybrid method (PL+ HMM-T)
analogous to REALM.
The results of these experiments are shown in Ta-
ble 2. REALM outperforms the TEXTRUNNER and
PL baselines for all relations, and reduces the miss-
ing area under the curve by an average of 39% rel-
ative to the strongest baseline. The performance
differences between REALM and TEXTRUNNER are
statistically significant for all relations, as are differ-
ences between REALM and PL for all relations ex-
cept Conquered (p < 0.01, two-sampled t-test).
The hybrid REALM system also outperforms each
of its components in isolation.
4 Related Work
To our knowledge, REALM is the first system to use
language modeling techniques for IE Assessment.
Redundancy-based approaches to pattern-based
IE assessment (Downey et al, 2005; Etzioni et al,
2005) require that extractions appear relatively fre-
quently with a limited set of patterns. In contrast,
REALM utilizes all contexts to build a model of ex-
tractions, rather than a limited set of patterns. Our
experiments demonstrate that REALM outperforms
these approaches on sparse data.
Type checking using named-entity taggers has
been previously shown to improve the precision of
pattern-based IE systems (Agichtein, 2005; Feld-
man et al, 2006), but the HMM-T type-checking
component we develop differs from this work in im-
portant ways. Named-entity taggers are limited in
that they typically recognize only small set of types
(e.g., ORGANIZATION, LOCATION, PERSON),
and they require hand-tagged training data for each
type. HMM-T, by contrast, performs type check-
ing for any type. Finally, HMM-T does not require
hand-tagged training data.
Pattern learning is a common technique for ex-
tracting and assessing sparse data (e.g. (Agichtein,
2005; Riloff and Jones, 1999; Pas?ca et al, 2006)).
Our experiments demonstrate that REALM outper-
forms a pattern learning system closely modeled af-
ter (Agichtein, 2005). REALM is inspired by pat-
tern learning techniques (in particular, both use the
distributional hypothesis to assess sparse data) but
is distinct in important ways. Pattern learning tech-
niques require substantial processing of the corpus
after the relations they assess have been specified.
Because of this, pattern learning systems are un-
suited to Open IE. Unlike these techniques, REALM
pre-computes language models which allow it to as-
sess extractions for arbitrary relations at run-time.
In essence, pattern-learning methods run in time lin-
ear in the number of relations whereas REALM?s run
time is constant in the number of relations. Thus,
REALM scales readily to large numbers of relations
whereas pattern-learning methods do not.
702
A second distinction of REALM is that its type
checker, unlike the named entity taggers employed
in pattern learning systems (e.g., Snowball), can be
used to identify arbitrary types. A final distinction is
that the language models REALM employs require
fewer parameters and heuristics than pattern learn-
ing techniques.
Similar distinctions exist between REALM and a
recent system designed to assess sparse extractions
by bootstrapping a classifier for each target relation
(Feldman et al, 2006). As in pattern learning, con-
structing the classifiers requires substantial process-
ing after the target relations have been specified, and
a set of hand-tagged examples per relation, making
it unsuitable for Open IE.
5 Conclusions
This paper demonstrated that unsupervised language
models, as embodied in the REALM system, are an
effective means of assessing sparse extractions.
Another attractive feature of REALM is its scal-
ability. Scalability is a particularly important con-
cern forOpen Information Extraction, the task of ex-
tracting large numbers of relations that are not spec-
ified in advance. Because HMM-T and REL-GRAMS
both pre-compute language models, REALM can be
queried efficiently to perform IE Assessment. Fur-
ther, the language models are constructed indepen-
dently of the target relations, allowing REALM to
perform IE Assessment even when relations are not
specified in advance.
In future work, we plan to develop a probabilistic
model of the information computed by REALM. We
also plan to evaluate the use of non-local context for
IE Assessment by integrating document-level mod-
eling techniques (e.g., Latent Dirichlet Allocation).
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, DARPA contract
NBCHD030010, ONR grant N00014-05-1-0185 as
well as a gift from Google. The first author is sup-
ported by an MSR graduate fellowship sponsored by
Microsoft Live Labs. We thank Michele Banko, Jeff
Bilmes, Katrin Kirchhoff, and Alex Yates for helpful
comments.
References
E. Agichtein. 2005. Extracting Relations From Large
Text Collections. Ph.D. thesis, Department of Com-
puter Science, Columbia University.
E. Agichtein. 2006. Confidence estimation methods for
partially supervised relation extraction. In SDM 2006.
M. Banko, M. Cararella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Procs. of IJCAI 2007.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In Procs. of IJCAI 2005.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locat-
ing complex named entities in web text. In Procs. of
IJCAI 2007.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
R. Feldman, B. Rosenfeld, S. Soderland, and O. Etzioni.
2006. Self-supervised relation extraction from the
web. In ISMIS, pages 755?764.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26?47.
New York: Oxford University Press.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Procs. of ACL/COLING 2006.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE, 77(2):257?286.
D. Ravichandran, P. Pantel, and E. H. Hovy. 2005. Ran-
domized Algorithms and NLP: Using Locality Sensi-
tive Hash Functions for High Speed Noun Clustering.
In Procs. of ACL 2005.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-level Boot-strapping.
In Procs. of AAAI-99, pages 1044?1049.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC-3. In
Text REtrieval Conference, pages 21?30.
703
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 651?656,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Adding High-Precision Links to Wikipedia
Thanapon Noraset Chandra Bhagavatula Doug Downey
Department of Electrical Engineering & Computer Science
Northwestern University
Evanston, IL 60208
{nor|csbhagav}@u.northwestern.edu, ddowney@eecs.northwestern.edu
Abstract
Wikipedia?s link structure is a valuable
resource for natural language processing
tasks, but only a fraction of the concepts
mentioned in each article are annotated
with hyperlinks. In this paper, we study
how to augment Wikipedia with additional
high-precision links. We present 3W, a
system that identifies concept mentions in
Wikipedia text, and links each mention
to its referent page. 3W leverages
rich semantic information present in
Wikipedia to achieve high precision. Our
experiments demonstrate that 3W can add
an average of seven new links to each
Wikipedia article, at a precision of 0.98.
1 Introduction
Wikipedia forms a valuable resource for
many Natural Language Processing and
Information Extraction tasks, such as Entity
Linking (Cucerzan, 2007; Han and Zhao,
2009), Ontology Construction (Wu and Weld,
2008; Syed et al., 2008) and Knowledge Base
Population (Hoffart et al., 2013; Lehmann et al.,
2013). Wikipedia?s links provide disambiguated
semantic information. For example, when a
system processes the text ?Chicago was received
with critical acclaim? from an article, the system
does not need to infer the referent entity of
?Chicago? if the word is already hyperlinked to
the Wikipedia page of the Oscar-winning film.
Unfortunately, in Wikipedia only a fraction of the
phrases that can be linked are in fact annotated
with a hyperlink. This is due to Wikipedia?s
conventions of only linking to each concept once,
and only when the links have a certain level of
utility for human readers.
1
We see this as an
1
http://en.wikipedia.org/wiki/
Wikipedia:Manual_of_Style_(linking)
opportunity to improve Wikipedia as a resource
for NLP systems. Our experiments estimate that
as of September 2013, there were an average of
30 references to Wikipedia concepts left unlinked
within each of English Wikipedia?s four million
pages.
In this paper, our goal is to augment Wikipedia
with additional high-precision links, in order
to provide a new resource for systems that
use Wikipedia?s link structure as a foundation.
Identifying references to concepts (called
mentions) in text and linking them to Wikipedia
is a task known as Wikification. Wikification for
general text has been addressed in a wide variety
of recent work (Mihalcea and Csomai, 2007;
Milne and Witten, 2008b; McNamee and Dang,
2009; Ratinov et al., 2011). The major challenge
of this task is to resolve the ambiguity of phrases,
and recent work makes use of various kinds of
information found in the document to tackle
the challenge. In contrast to this body of work,
here we focus on the special case of Wikifying
Wikipedia articles, instead of general documents.
This gives us an advantage over general-text
systems due to Wikipedia?s rich content and
existing link structure.
We introduce 3W, a system that identifies
mentions within Wikipedia and links each
to its referent concept. We show how a
Wikipedia-specific Semantic Relatedness measure
that leverages the link structure of Wikipedia
(Milne and Witten, 2008b) allows 3W to be
radically more precise at high levels of yield when
compared to baseline Wikifiers that target general
text. Our experiment shows that 3W can add on
average seven new links per article at precision of
0.98, adding approximately 28 million new links
to 4 million articles across English Wikipedia.
2
2
http://websail.cs.northwestern.edu/
projects/3W
651
2 Problem Definition
In this section, we define our link extraction task.
A link l is a pair of a surface form s
l
and a
concept t
l
. A surface form is a span of tokens
in an article, and the concept is a Wikipedia
article referred to by the surface form. For
existing hyperlinks, the surface form corresponds
to the anchor text and the concept is the link
target. For example, a hyperlink [[Chicago City
| Chicago]] has surface form ?Chicago City? and
referent concept Chicago.
3
Given documentsD =
{d
1
, ..., d
|D|
} and a set of links L = {l
1
, .., l
|L|
} ?
D, our goal is to generate a set of high-precision
links L
?
for D, distinct from L. In this paper, the
document set D consists of articles from English
Wikipedia, and L is the set of existing links on
Wikipedia.
The task can be divided into 3 steps. The first
step is to extract a set of potential mentionsM =
{m
1
, ..,m
|M|
} where m is, similar to l, a pair of
surface form s
m
and a set of candidate concepts
C(m) = {t
1
, ..., t
|C(m)|
}. For m having |C(m)| >
1, we need to disambiguate it by selecting only
one target concept t
m
? C(m). Since the correct
concept may not exist in C(m) and the previous
step could output an incorrect concept, the final
step is to decide whether to link and include m in
L
?
. We describe the details of these steps in the
following section.
3 System Overview
In this section, we describe in detail how 3W adds
high-precision links to Wikipedia.
3.1 Mention Extraction
In this step, we are given a document d, and
the goal is to output a set of mentions M. Our
system finds a set of potential surface forms, s
m
,
by finding substrings in d that match the surface
form of some links in L. For example, from the
phrase ?map of the United States on the wall?,
we can match 4 potential surface forms: ?map?,
?United States?, ?map of the United States?, and
?wall?. Notice that some of them are overlapping.
The system selects a non-overlapping subset of the
surface forms that maximizes the following score
function:
Score(M) =
?
m?M
T (s
m
)PL(s
m
)
|C(m)|
(1)
3
http://en.wikipedia.org/wiki/Chicago
where PL(s
m
) is the probability that the text s
m
is linked (that is, the fraction of the occurrences of
the string s
m
in the corpus that are hyperlinked),
T (s
m
) is the number of tokens in s
m
, and |C(m)|
is the number of candidate concepts. Intuitively,
we prefer a longer surface form that is frequently
linked and has a specific meaning. Furthermore,
we eliminate common surface forms (i.e. ?wall?)
by requiring that PL(s
m
) exceed a threshold. In
the previous example, we are left with only ?map
of the United States?.
Because Wikipedia?s concepts are largely noun
phrases, 3W only looks for surface forms from
top-level noun phrases generated by the Stanford
Parser (Socher et al., 2013). In addition, each
name entity (NE) (Finkel et al., 2005) is treated
as an atomic token, meaning that multi-word NEs
such as ?California Institute of the Arts? will not
be broken into multiple surface forms.
Finally, the system pairs the result surface forms
with a set of candidate concepts, C(m), and
outputs a set of mentions. C(m) consists of those
concepts previously linked to the surface form in
L. For instance, the surface form ?map of the
United States? has been linked to three distinct
concepts in English Wikipedia.
3.2 Disambiguation
Given a set of mentions M from the previous
step, The next step is to select a concept t ?
C(m) for each m ? M. We take the common
approach of ranking the candidate concepts.
3W uses a machine learning model to perform
pair-wise ranking of t ? C(m) and select the
top-ranked candidate concept. We refer to 3W?s
disambiguation component as the ranker. The
ranker requires a feature vector for each candidate
concept of a mention. The rest of this section
describes the features utilized by the ranker. The
first two feature groups are commonly used in
Wikification systems. The third feature group is
specifically designed for mentions in Wikipedia
articles.
3.2.1 Prior Probability Features
The conditional probability of a concept t given
mention surface s
m
, P (t|s
m
), is a common
feature used for disambiguation. It forms
a very strong Wikification baseline (? 86%
in micro-accuracy). This probability can be
estimated using Wikipedia links (L). In
addition, we use the external partition of the
652
Google ?Cross-Lingual Dictionary? described in
(Spitkovsky and Chang, 2012) to get the estimates
for the probability from links outside Wikipedia.
3.2.2 Lexical Features
To make use of text around a mention m,
we create bag-of-word vectors of the mention?s
source document d(m), and of a set of words
surrounding the mention, referred to as the context
c(m). To compare with a concept, we also
create bag-of-word vectors of candidate concept?s
document d(t) and candidate concept?s context
c(t). We then compute cosine similarities between
the mention?s vectors for d(m) and c(m), with
the concept candidate vectors for d(t) and c(t) as
in the Illinois Wikifier (Ratinov et al., 2011). In
addition to similarities computed over the top-200
words (utilized in the Illinois Wikifier), we also
compute similarity features over vectors of all
words.
3.2.3 Wikipedia-specific Features
Because the links in an article are often related
to one another, the existing links in a document
form valuable clues for disambiguating mentions
in the document. For each concept candidate
t ? C(m), we compute a Semantic Relatedness
(SR) measure between t and each concept from
existing links in the source document. Our SR
measure is based on the proportion of shared
inlinks, as introduced by Milne and Witten
(2008b). However, because Milne and Witten
were focused on general text, they computed SR
only between t and the unambiguous mentions
(i.e. those m with |C(m)| = 1) identified
in the document. In our work, d(m) is a
Wikipedia article which is rich in existing links
to Wikipedia concepts, and we can compute
SR with all of them, resulting in a valuable
feature for disambiguation as illustrated in our
experiments. We use the SR implementation of
Hecht et al. (2012). It is a modified version of
Milne and Witten?s measure that emphasizes links
in Wikipedia article?s overview. In addition, we
add boolean features indicating whether s
m
or t
has already been linked in a document.
3.2.4 Reranking
The millions of existing Wikipedia links in L form
a valuable source of training examples for our
ranker. However, simply training on the links
in L may result in poor performance, because
those links exhibit systematic differences from the
mentions inM that the ranker will be applied to.
The reason is that our mention extractor attempts
to populate M with all mentions, whereas
L which contains only the specific subset of
mentions that meet the hyperlinking conventions
of Wikipedia, As a result, the features for M
are distributed differently from those in L, and a
model trained on L may not might not perform
well on M. Our strategy is to leverage L to
train an initial ranker, and then hand-label a small
set of mentions from M to train a second-stage
re-ranker that takes the ranking output of the
initial ranker as a feature.
3.3 Linker
Our linker is a binary classifier that decides
whether to include (link) each mention in M
to the final output L
?
. Previous work has
typically used a linker to determine so-called NIL
mentions, where the referred-to concept is not
in the target knowledge base (e.g., in the TAC
KBP competition, half of the given mentions are
NIL (Ji and Grishman, 2011)). The purpose
of our linker is slightly different, because we
also use a linker to control the precision of our
output. We use a probabilistic linker that predicts
a confidence estimate that the mention with its
top-ranked candidate is correct. Our linker uses
the same features as the ranker and an additional
set of confidence signals: the number of times the
top candidate concept appears in L, and the score
difference between the top-ranked candidate and
the second-ranked candidate.
4 Experiments and Result
In this section, we provide an evaluation of our
system and its subcomponents.
4.1 Experiment Setup
We trained our initial ranker models from 100,000
randomly selected existing links (L). These links
were excluded when building feature values (i.e.
the prior probability, or Semantic Relatedness).
We formed an evaluation set of new links by
applying our mention extractor to 2,000 randomly
selected articles, and then manually labeling 1,900
of the mentions with either the correct concept
or ?no correct concept.? We trained and tested
our system on the evaluation set, using 10-fold
cross validation. For each fold, we partitioned data
653
Model Acc Prec Recall F1
Prior 0.876 0.891 0.850 0.870
OnlyWikiLink
?Wiki
0.896 0.905 0.871 0.888
OnlyWikiLink 0.944 0.950 0.920 0.935
Table 1: 10-fold cross validation performance of the initial
rankers by Accuracy (excluded ?-candidate mentions), BOT
Precision, BOT Recall, BOT F1 on the 100,000 existing links.
into 3 parts. We used 760 mentions for training
the final ranker. The linker was trained with 950
mentions and we tested our system using the other
190 mentions. Previous work has used various ML
approaches for ranking, such as SVMs (Dredze et
al., 2010). We found logistic regression produces
similar accuracy to SVMs, but is faster for our
feature set. For the linker, we use an SVM with
probabilistic output (Wu et al., 2004; Chang and
Lin, 2011) to estimate a confidence score for each
output link.
4.2 Result
We first evaluate 3W?s mention extraction. From
the selected 2, 000 articles, the system extracted
59, 454 mentions (?30/article), in addition to
the original 54, 309 links (?27/article). From
the 1, 900 hand-labeled mentions, 1, 530 (80.5%)
were solvable in that 3W candidate set contained
the correct target.
As described in section 3.2.4, 3W employs
a 2-stage ranker. We first evaluate just the
initial ranker, using 10-fold cross validation
on 100,000 existing links. We show micro
accuracy and bag-of-title (BOT) performance
used by Milne and Witten (2008b) in Table
1. The ranker with all features (OnlyWikiLink)
outperforms the ranker without Wikipedia-specific
features (OnlyWikiLink?Wiki) by approximately
five points in F1. This demonstrates that
Wikipedia?s rich semantic content is helpful for
disambiguation.
Next, we evaluate our full system performance
(disambiguation and linking) over the
hand-labeled evaluation set. We experimented
with different configurations of the rankers and
linkers. Our Baseline system disambiguates
a mention m by selecting the most common
concept for the surface s(m). OnlyWikiLink
uses the ranker model trained on only Wikipedia
links, ignoring the labeled mentions. 3W is our
system using all features described in section 3.2,
Model Acc Yield %Yield
Baseline 0.828 5 0.33%
OnlyWikiLink 0.705 150 9.80%
3W?Wiki 0.868 253 16.54%
3W 0.877 365 23.86%
Table 2: 10-fold cross validation performance of the system
over 1,900 labeled mentions. Acc is disambiguation accuracy
of solvable mentions. Yield is the number of output new
mentions at precision ? 0.98, and %Yield is the percentage
of Yield over the solvable mentions (recall).
0 0.2 0.4 0.6 0.8 10.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
Precision Recall Comparisons
 
 
BaselineOnlyWikiLinks3W?Wiki3W
Figure 1: Plot between Precision and Recall of systems on
1,900 mentions from 10-fold cross validation.
and 3W?Wiki is 3W without Wikipedia-specific
features. The last two configurations are trained
using the labeled mentions.
Table 2 shows the disambiguation accuracy of
each system over the solvable mentions. Our final
system, 3W, has the best disambiguation accuracy.
To evaluate the linking performance, we select
the confidence threshold such that the system
outputs mentions with precision of ? 0.98. The
third column in Table 2 shows the yield, i.e. the
number of mentions output at precision 0.98. 3W
outputs the largest number of new links (365).
Nearly half (157) are new concepts that have not
been linked in the source article. We find that the
Rerank feature helps increase recall: without it,
the yield of 3W drops by 27%. Using %Yield,
we estimate that 3W will output 14, 000 new links
for the selected 2, 000 articles (?7/article), and
approximately 28 million new links across the 4
million articles of English Wikipedia.
Adjusting the confidence threshold allows
the system to trade off precision and recall.
Figure 1 shows a precision and recall curve.
3W and OnlyWikiLink are comparable for
654
many high-precision points, but below 0.95
OnlyWikiLink?s precision drops quickly. Plots
that finish at higher rightmost points in the graph
indicate systems that achieve higher accuracy on
the complete evaluation set.
5 Conclusions and Future Work
We presented 3W, a system that adds
high-precision links to Wikipedia. Whereas
many Wikification systems focus on general text,
3W is specialized toward Wikipedia articles.
We showed that leveraging the link structure of
Wikipedia provides advantages in disambiguation.
In experiments, 3W was shown to Wikipedia with
?7 new links per article (an estimated 28m across
4 million Wikipedia articles) at high precision.
Acknowledgments
This work was supported in part by DARPA
contract D11AP00268 and the Allen Institute for
Artificial Intelligence.
References
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A library for support vector machines.
ACM Transactions on Intelligent Systems and
Technology, 2:27:1?27:27. Software available at
http://www.csie.ntu.edu.tw/
?
cjlin/
libsvm.
Silviu Cucerzan. 2007. Large-scale named
entity disambiguation based on Wikipedia data.
In Proceedings of EMNLP-CoNLL 2007, pages
708?716.
Mark Dredze, Paul McNamee, Delip Rao, Adam
Gerber, and Tim Finin. 2010. Entity
disambiguation for knowledge base population. In
Proceedings of the 23rd International Conference
on Computational Linguistics, pages 277?285.
Association for Computational Linguistics.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local
information into information extraction systems
by gibbs sampling. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370. Association for
Computational Linguistics.
Xianpei Han and Jun Zhao. 2009. Named
entity disambiguation by leveraging wikipedia
semantic knowledge. In Proceedings of the 18th
ACM conference on Information and knowledge
management, pages 215?224. ACM.
Brent Hecht, Samuel H Carton, Mahmood Quaderi,
Johannes Sch?oning, Martin Raubal, Darren Gergle,
and Doug Downey. 2012. Explanatory
semantic relatedness and explicit spatialization for
exploratory search. In Proceedings of the 35th
international ACM SIGIR conference on Research
and development in information retrieval, pages
415?424. ACM.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artificial Intelligence, 194:28?61.
Heng Ji and Ralph Grishman. 2011. Knowledge
base population: Successful approaches and
challenges. In Proceedings of the 49th Annual
Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1148?1158. Association for Computational
Linguistics.
Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, S?oren Auer, et al. 2013. Dbpedia-a
large-scale, multilingual knowledge base extracted
from wikipedia. Semantic Web Journal.
Paul McNamee and Hoa Trang Dang. 2009. Overview
of the tac 2009 knowledge base population track. In
Text Analysis Conference (TAC), volume 17, pages
111?113.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge.
In Proceedings of the sixteenth ACM conference
on Conference on information and knowledge
management, pages 233?242. ACM.
David Milne and Ian H Witten. 2008b. Learning to
link with wikipedia. In Proceedings of the 17th
ACM conference on Information and knowledge
management, pages 509?518. ACM.
Lev Ratinov, Dan Roth, Doug Downey, and Mike
Anderson. 2011. Local and global algorithms for
disambiguation to wikipedia. In ACL.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with
compositional vector grammars. In In Proceedings
of the ACL conference. Citeseer.
Valentin I. Spitkovsky and Angel X. Chang. 2012.
A cross-lingual dictionary for english wikipedia
concepts. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur
Doan, Bente Maegaard, Joseph Mariani, Jan Odijk,
and Stelios Piperidis, editors, Proceedings of
the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources
Association (ELRA).
655
Zareen Saba Syed, Tim Finin, and Anupam Joshi.
2008. Wikipedia as an ontology for describing
documents. In ICWSM.
Fei Wu and Daniel S Weld. 2008. Automatically
refining the wikipedia infobox ontology. In
Proceedings of the 17th international conference on
World Wide Web, pages 635?644. ACM.
Ting-Fan Wu, Chih-Jen Lin, and Ruby C Weng. 2004.
Probability estimates for multi-class classification
by pairwise coupling. Journal of Machine Learning
Research, 5(975-1005):4.
656
Learning Representations for Weakly
Supervised Natural Language
Processing Tasks
Fei Huang?
Temple University
Arun Ahuja??
Northwestern University
Doug Downey?
Northwestern University
Yi Yang?
Northwestern University
Yuhong Guo?
Temple University
Alexander Yates?
Temple University
Finding the right representations for words is critical for building accurate NLP systems when
domain-specific labeled data for the task is scarce. This article investigates novel techniques for
extracting features from n-gram models, Hidden Markov Models, and other statistical language
models, including a novel Partial Lattice Markov Random Field model. Experiments on part-
of-speech tagging and information extraction, among other tasks, indicate that features taken
from statistical language models, in combination with more traditional features, outperform
traditional representations alone, and that graphical model representations outperform n-gram
models, especially on sparse and polysemous words.
? 1805 N. Broad St., Wachman Hall 324, Philadelphia, PA 19122, USA.
E-mail: {fei.huang,yuhong,yates}@temple.edu.
?? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ahuja@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: ddowney@eecs.northwestern.edu.
? 2133 Sheridan Road, Evanston, IL, 60208. E-mail: yya518@eecs.northwestern.edu.
Submission received: 13 June 2012; revised submission received: 25 November 2012, accepted for publication:
15 January 2013.
doi:10.1162/COLI a 00167
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
1. Introduction
NLP systems often rely on hand-crafted, carefully engineered sets of features to achieve
strong performance. Thus, a part-of-speech (POS) tagger would traditionally use a
feature like, ?the previous token is the? to help classify a given token as a noun
or adjective. For supervised NLP tasks with sufficient domain-specific training data,
these traditional features yield state-of-the-art results. However, NLP systems are in-
creasingly being applied to the Web, scientific domains, personal communications like
e-mails and tweets, among many other kinds of linguistic communication. These texts
have very different characteristics from traditional training corpora in NLP. Evidence
from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing
(Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan,
Ward, and Martin 2007), among other NLP tasks (Daume? III and Marcu 2006; Chelba
and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer,
Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades
significantly when tested on domains different from those used for training. Collecting
labeled training data for each new target domain is typically prohibitively expensive.
In this article, we investigate representations that can be applied to weakly supervised
learning, that is, learning when domain-specific labeled training data are scarce.
A growing body of theoretical and empirical evidence suggests that traditional,
manually crafted features for a variety of NLP tasks limit systems? performance in this
weakly supervised learning for two reasons. First, feature sparsity prevents systems
from generalizing accurately, because many words and features are not observed in
training. Also because word frequencies are Zipf-distributed, this often means that there
is little relevant training data for a substantial fraction of parameters (Bikel 2004b), espe-
cially in new domains (Huang and Yates 2009). For example, word-type features form
the backbone of most POS-tagging systems, but types like ?gene? and ?pathway? show
up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier
trained on newswire data and tested on biomedical data will have seen few training
examples related to sentences with features ?gene? and ?pathway? (Blitzer, McDonald,
and Pereira 2006; Ben-David et al. 2010).
Further, because words are polysemous, word-type features prevent systems from
generalizing to situations in which words have different meanings. For instance, the
word type ?signaling? appears primarily as a present participle (VBG) in Wall Street
Journal (WSJ) text, as in, ?Interest rates rose, signaling that . . . ? (Marcus, Marcinkiewicz,
and Santorini 1993). In biomedical text, however, ?signaling? appears primarily in the
phrase ?signaling pathway,? where it is considered a noun (NN) (PennBioIE 2005); this
phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates 2010).
Our response to the sparsity and polysemy challenges with traditional NLP repre-
sentations is to seek new representations that allow systems to generalize to previously
unseen examples. That is, we seek representations that permit classifiers to have close
to the same accuracy on examples from other domains as they do on the domain of the
training data. Our approach depends on the well-known distributional hypothesis,
which states that a word?s meaning is identified with the contexts in which it appears
(Harris 1954; Hindle 1990). Our goal is to develop probabilistic statistical language
models that describe the contexts of individual words accurately. We then construct
representations, or mappings from word tokens and types to real-valued vectors,
from statistical language models. Because statistical language models are designed to
model words? contexts, the features they produce can be used to combat problems
with polysemy. And by careful design of the statistical language models, we can limit
86
Huang et al. Computational Linguistics
the number of features that they produce, controlling how sparse those features are in
training data.
Our specific contributions are as follows:
1. We show how to generate representations from a variety of language
models, including n-gram models, Brown clusters, and Hidden Markov
Models (HMMs). We also introduce a Partial-Lattice Markov Random
Field (PL-MRF), which is a tractable variation of a Factorial Hidden
Markov Model (Ghahramani and Jordan 1997) for language modeling,
and we show how to produce representations from it.
2. We quantify the performance of these representations in experiments
on POS tagging in a domain adaptation setting, and weakly supervised
information extraction (IE). We show that the graphical models outperform
n-gram representations, even when the n-gram models leverage larger
corpora for training. The PL-MRF representation achieves a state-of-the-art
93.8% accuracy on a biomedical POS tagging task, which represents a
5.5 percentage point absolute improvement over more traditional POS
tagging representations, a 4.8 percentage point improvement over a tagger
using an n-gram representation, and a 0.7 percentage point improvement
over a tagger with an n-gram representation using several orders of
magnitude more training data. The HMM representation improves
over the n-gram model by 7 percentage points on our IE task.
3. We analyze how sparsity, polysemy, and differences between domains
affects the performance of a classifier using different representations.
Results indicate that statistical language model representations, and
especially graphical model representations, provide the best features
for sparse and polysemous words.
The next section describes background material and related work on representation
learning for NLP. Section 3 presents novel representations based on statistical language
models. Sections 4 and 5 discuss evaluations of the representations, first on sequence-
labeling tasks in a domain adaptation setting, and second on a weakly supervised set-
expansion task. Section 6 concludes and outlines directions for future work.
2. Background and Previous Work on Representation Learning
2.1 Terminology and Notation
In a traditional machine learning task, the goal is to make predictions on test data using
a hypothesis that is optimized on labeled training data. In order to do so, practitioners
predefine a set of features and try to estimate classifier parameters from the observed
features in the training data. We call these feature sets representations of the data.
Formally, let X be an instance space for a learning problem. Let Z be the space of
possible labels for an instance, and let f : X ? Z be the target function to be learned.
A representation is a function R: X ? Y , for some suitable feature space Y (such as Rd).
We refer to dimensions of Y as features, and for an instance x ? X we refer to values
for particular dimensions of R(x) as features of x. Given a set of training examples, a
learning machine?s task is to select a hypothesis h from the hypothesis space H, a subset
of ZR(X ). Errors by the hypothesis are measured using a loss function L(x, R, f, h) that
87
Computational Linguistics Volume 40, Number 1
measures the cost of the mismatch between the target function f (x) and the hypothesis
h(R(x)).
As an example, the instance set for POS tagging in English is the set of all English
sentences, and Z is the space of POS sequences containing labels like NN (for noun) and
VBG (for present participle). The target function f is the mapping between sentences
and their correct POS labels. A traditional representation in NLP converts sentences
into sequences of vectors, one for each word position. Each vector contains values for
features like, ?+1 if the word at this position ends with -tion, and 0 otherwise.? A
typical loss function would count the number of words that are tagged differently by
f (x) and h(R(x)).
2.2 Representation-Learning Problem Formulation
Machine learning theory assumes that there is a distribution D over X from which
data is sampled. Given a training set S = {(x1, z1), . . . , (xN, zN )} ? (D(X ),Z )N, a fixed
representation R, a hypothesis space H, and a loss function L, a machine learning
algorithm seeks to identify the hypothesis in H that will minimize the expected loss
over samples from distribution D:
h? = argmin
h?H
Ex?D(X )L(x, R, f, h) (1)
The representation-learning paradigm breaks the traditional notion of a fixed rep-
resentation R. Instead, we allow a space of possible representations R. The full learning
problem can then be formulated as the task of identifying the best R ? R and h ? H
simultaneously:
R?, h? = argmin
R?R,h?H
Ex?D(X )L(x, R, f, h) (2)
The representation-learning problem formulation in Equation (2) can in fact be
reduced to the general learning formulation in Equation (1) by setting the fixed rep-
resentation R to be the identity function, and setting the hypothesis space to be R?H
from the representation-learning task. We introduce the new formulation primarily as
a way of changing the perspective on the learning task: most NLP systems consider
a fixed, manually crafted transformation of the original data to some new space, and
investigate hypothesis classes over that space. In the new formulation, systems learn
the transformation to the feature space, and then apply traditional classification or
regression algorithms.
2.3 Theory on Domain Adaptation
We refer to the distribution D over the instance space X as a domain. For example,
the newswire domain is a distribution over sentences that gives high probability to
sentences about governments and current events; the biomedical literature domain
gives high probability to sentences about proteins and regulatory pathways. In domain
adaptation, a system observes a set of training examples (R(x), f (x)), where instances
x ? X are drawn from a source domain DS, to learn a hypothesis for classifying ex-
amples drawn from a separate target domain DT. We assume that large quantities of
unlabeled data are available for the source domain and target domain, and call these
88
Huang et al. Computational Linguistics
samples US and UT, respectively. For any domain D, let R(D) represent the induced
distribution over the feature space Y given by PrR(D)[y] = PrD[{x such that R(x) = y}].
Previous work by Ben-David et al. (2007, 2010) proves theoretical bounds on an
open-domain learning machine?s performance. Their analysis shows that the choice of
representation is crucial to domain adaptation. A good choice of representation must
allow a learning machine to achieve low error rates on the source domain. Just as
important, however, is that the representation must simultaneously make the source
and target domains look as similar to one another as possible. That is, if the labeling
function f is the same on the source and target domains, then for every h ? H, we can
provably bound the error of h on the target domain by its error on the source domain
plus a measure of the distance between DS and DT:
Ex?DTL(x, R, f, h) ? Ex?DSL(x, R, f, h) + d1(R(DS), R(DT )) (3)
where the variation divergence d1 is given by
d1(D,D?) = 2 sup
B?B
|PrD[B] ? PrD? [B]| (4)
where B is the set of measurable sets under D and D? (Ben-David et al. 2007, 2010).
Crucially, the distance between domains depends on the features in the representa-
tion. The more that features appear with different frequencies in different domains, the
worse this bound becomes. In fact, one lower bound for the d1 distance is the accuracy
of the best classifier for predicting whether an unlabeled instance y = R(x) belongs to
domain S or T (Ben-David et al. 2010). Thus, if R provides one set of common features for
examples from S, and another set of common features for examples from T, the domain
of an instance becomes easy to predict, meaning the distance between the domains
grows, and the bound on our classifier?s performance grows worse.
In light of Ben-David et al.?s theoretical findings, traditional representations in
NLP are inadequate for domain adaptation because they contribute to the d1 distance
between domains. Although many previous studies have shown that lexical features
allow learning systems to achieve impressively low error rates during training, they also
make texts from different domains look very dissimilar. For instance, a feature based on
the word ?bank? or ?CEO? may be common in a domain of newswire text, but scarce
or nonexistent in, say, biomedical literature. Ben David et al.?s theory predicts greater
variance in the error rate of the target domain classifier as the distance grows.
At the same time, traditional representations contribute to data sparsity, a lack of
sufficient training data for the relevant parameters of the system. In traditional super-
vised NLP systems, there are parameters for each word type in the data, or perhaps
even combinations of word types. Because vocabularies can be extremely large, this
leads to an explosion in the number of parameters. As a consequence, for many of their
parameters, supervised NLP systems have zero or only a handful of relevant labeled
examples (Bikel 2004a, 2004b). No matter how sophisticated the learning technique, it
is difficult to estimate parameters without relevant data. Because vocabularies differ
across domains, domain adaptation greatly exacerbates this issue of data sparsity.
2.4 Problem Formulation for the Domain Adaptation Setting
Formally, we define the task of representation learning for domain adaptation as the
following optimization problem: Given a set of unlabeled instances US drawn from the
89
Computational Linguistics Volume 40, Number 1
source domain and unlabeled instances UT from the target domain, as well as a set of
labeled instances LS drawn from the source domain, identify a function R? from the
space of possible representations R that minimizes
R?, h? = argmin
R?R,h?H
(
Ex?DSL(x, R, f, h)
)
+ ?d1(R(DS), R(DT )) (5)
where ? is a free parameter.
Note that there is an underlying tension between the two terms of the objec-
tive function: The best representation for the source domain would naturally include
domain-specific features, and allow a hypothesis to learn domain-specific patterns.
We are aiming, however, for the best general classifier, which happens to be trained
on training data from one domain (or a few domains). The domain-specific features
contribute to distance between domains, and to classifier errors on data taken from
domains not seen in training. By optimizing for this combined objective function, we
allow the optimization method to trade off between features that are best for classifying
source-domain data and features that allow generalization to new domains.
Unlike the representation-learning problem-formulation in Equation (2), Equa-
tion (5) does not reduce to the standard machine-learning problem (Equation (1)). In
a sense, the d1 term acts as a regularizer on R, which also affects H. Representation
learning for domain adaptation is a fundamentally novel learning task.
2.5 Tractable Representation Learning: Statistical Language Models
as Representations
For most hypothesis classes and any interesting space of representations, Equations (2)
and (5) are completely intractable to optimize exactly. Even given a fixed representation,
it is intractable to compute the best hypothesis for many hypothesis classes. And the d1
metric is intractable to compute from samples of a distribution, although Ben-David
et al. (2007, 2010) propose some tractable bounds. We view these problem formulations
as high-level goals rather than as computable objectives.
As a tractable objective, in this work we describe an investigation into the use of
statistical language models as a way to represent the meanings of words. This approach
depends on the well-known distributional hypothesis, which states that a word?s
meaning is identified with the contexts in which it appears (Harris 1954; Hindle 1990).
From this hypothesis, we can formulate the following testable prediction, which we call
the statistical language model representation hypothesis, or LMRH:
To the extent that a model accurately describes a word?s possible contexts, parameters
of that model are highly informative descriptors of the word?s meaning, and are
therefore useful as features in NLP tasks like POS tagging, chunking, NER, and
information extraction.
The LMRH says, essentially, that for NLP tasks, we can decouple the task of optimiz-
ing a representation from the task of optimizing a hypothesis. To learn a representation,
we can train a statistical language model on unlabeled text, and then use parameters
or latent states from the statistical language model to create a representation function.
Optimizing a hypothesis then follows the standard learning framework, using the
representation from the statistical language model.
90
Huang et al. Computational Linguistics
The LMRH is similar to the manifold and cluster assumptions behind other semi-
supervised approaches to machine learning, such as Alternating Structure Optimization
(ASO) (Ando and Zhang 2005) and Structural Correspondence Learning (SCL) (Blitzer,
McDonald, and Pereira 2006). All three of these techniques use predictors built on
unlabeled data as a way to harness the manifold and cluster assumptions. However,
the LMRH is distinct from at least ASO and SCL in important ways. Both ASO and SCL
create multiple ?synthetic? or ?pivot? prediction tasks using unlabeled data, and find
transformations of the input feature space that perform well on these tasks. The LMRH,
on the other hand, is more specific ? it asserts that for language problems, if we opti-
mize word representations on a single task (the language modeling task), this will lead
to strong performance on weakly supervised tasks. In reported experiments on NLP
tasks, both ASO and SCL use certain synthetic predictors that are essentially language
modeling tasks, such as the task of predicting whether the next token is of word type w.
To the extent that these techniques? performance relies on language-modeling tasks as
their ?synthetic predictors,? they can be viewed as evidence in support of the LMRH.
One significant consequence of the LMRH is that it allows us to leverage well-
developed techniques and models from statistical language modeling. Section 3
presents a series of statistical language models that we investigate for learning repre-
sentations for NLP.
2.6 Previous Work
There is a long tradition of NLP research on representations, mostly falling into one of
four categories: 1) vector space models of meaning based on document-level lexical co-
occurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010);
2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990;
Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and
Jordan 2003; Va?yrynen and Honkela 2004, 2005; Va?yrynen, Honkela, and Lindqvist
2007); 3) using clusters that are induced from distributional similarity (Brown et al.
1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse
features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu
2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao
et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008;
Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih
and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert
and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering
for representations, but where previous work has used bigram and trigram statistics to
form clusters, we build sophisticated models that attempt to capture the context of a
word, and hence its similarity to other words, more precisely. Our experiments show
that the new graphical models provide representations that outperform those from
previous work on several tasks.
Neural network statistical language models have recently achieved state-of-the-art
perplexity results (Mnih and Hinton 2009), and representations based on them have im-
proved in-domain chunking, NER, and SRL (Weston, Ratle, and Collobert 2008; Turian,
Bergstra, and Bengio 2009; Turian, Ratinov, and Bengio 2010). As far as we are aware,
Turian, Ratinov, and Bengio (2010) is the only other work to test a learned representation
on a domain adaptation task, and they show improvement on out-of-domain NER
with their neural net representations. Though promising, the neural network models
are computationally expensive to train, and these statistical language models work
only on fixed-length histories (n-grams) rather than full observation sequences. Turian,
91
Computational Linguistics Volume 40, Number 1
Ratinov, and Bengio?s (2010) tests also show that Brown clusters perform as well or
better than neural net models on all of their chunking and NER tests. We concentrate on
probabilistic graphical models with discrete latent states instead. We show that HMM-
based and other representations significantly outperform the more commonly used
Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings
of sequence-labeling tasks.
Most previous work on domain adaptation has focused on the case where some
labeled data are available in both the source and target domains (Chan and Ng 2006;
Daume? III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daume? III 2007; Jiang
and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze,
Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are
known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this
problem setting have focused on appropriately weighting examples from the source and
target domains so that the learning algorithm can balance the greater relevance of the
target-domain data with the larger source-domain data set. In some cases, researchers
combine this approach with semi-supervised learning to include unlabeled examples
from the target domain as well (Daume? III, Kumar, and Saha 2010). These techniques
do not handle open-domain corpora like the Web, where they require expert input to
acquire labels for each new single-domain corpus, and it is difficult to come up with
a representative set of labeled training data for each domain. Our technique requires
only unlabeled data from each new domain, which is significantly easier and cheaper to
acquire. Where target-domain labeled data is available, however, these techniques can
in principle be combined with ours to improve performance, although this has not yet
been demonstrated empirically.
A few researchers have considered the more general case of domain adaptation
without labeled data in the target domain. Perhaps the best known is Blitzer, McDonald,
and Pereira?s (2006) Structural Correspondence Learning (SCL). SCL uses ?pivot? words
common to both source and target domains, and trains linear classifiers to predict these
pivot words from their context. After an SVD reduction of the weight vectors for these
linear classifiers, SCL projects the original features through these weight vectors to
obtain new features that are added to the original feature space. Like SCL, our language
modeling techniques attempt to predict words from their context, and then use the
output of these predictions as new features. Unlike SCL, we attempt to predict all words
from their context, and we rely on traditional probabilistic methods for language mod-
eling. Our best learned representations, which involve significantly different techniques
from SCL, especially latent-variable probabilistic models, significantly outperform SCL
in POS tagging experiments.
Other approaches to domain adaptation without labeled data from the target do-
main include Satpal and Sarawagi (2007), who show that by changing the optimization
function during conditional random field (CRF) training, they can learn classifiers that
port well to new domains. Their technique selects feature subsets that minimize the
distance between training text and unlabeled test text, but unlike our techniques, theirs
cannot learn representations with features that do not appear in the original feature set.
In contrast, we learn hidden features through statistical language models. McClosky,
Charniak, and Johnson (2010) use classifiers from multiple source domains and features
that describe how much a target document diverges from each source domain to deter-
mine an optimal weighting of the source-domain classifiers for parsing the target text.
However, it is unclear if this ?source-combination? technique works well on domains
that are not mixtures of the various source domains. Dai et al. (2007) use KL-divergence
between domains to directly modify the parameters of their naive Bayes model for a
92
Huang et al. Computational Linguistics
text classification task trained purely on the source domain. These last two techniques
are not representation learning, and are complementary to our techniques.
Our representation-learning approach to domain adaptation is an instance of
semi-supervised learning. Of the vast number of semi-supervised approaches to
sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki?s
(2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text
to achieve the current best performance on in-domain chunking, and semi-supervised
approaches to improving in-domain SRL with large quantities of unlabeled text
(Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Fu?rstenau and
Lapata 2009). Ando and Zhang?s (2005) semi-supervised sequence labeling technique
has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and
Pereira 2006); our representation-learning approaches outperform it. Unlike most semi-
supervised techniques, we concentrate on a particularly simple task decomposition: un-
supervised learning for new representations, followed by standard supervised learning.
In addition to our task decomposition being simple, our learned representations are also
task-independent, so we can learn the representation once, and then apply it to any task.
One of the best-performing representations that we consider for domain adaptation
is based on the HMM (Rabiner 1989). HMMs have of course also been used for super-
vised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and
Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised
POS tagging have focused on incorporating prior knowledge into the POS induction
model (Grac?a et al. 2009; Toutanova and Johnson 2007), or on new training techniques
like contrastive estimation (Smith and Eisner 2005) for alternative sequence models.
Despite the fact that completely connected, standard HMMs perform poorly at the POS
induction task (Johnson 2007), we show that they still provide very useful features
for a supervised POS tagger. Experiments in information extraction have previously
also shown that HMMs provide informative features for this quite different, semantic
processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010).
This article extends our previous work on learning representations for do-
main adaptation (Huang and Yates 2009, 2010) by investigating new language
representations?the naive Bayes representation and PL-MRF representation (Huang
et al. 2011)?by analyzing results in terms of polysemy, sparsity, and domain diver-
gence; by testing on new data sets including a Chinese POS tagging task; and by pro-
viding an empirical comparison with Brown clusters as representations.
3. Learning Representations of Distributional Similarity
In this section, we will introduce several representation learning models.
3.1 Traditional POS-Tagging Representations
As an example of our terminology, we begin by describing a representation used in
traditional POS taggers (this representation will later form a baseline for our POS
tagging experiments). The instance set X is the set of English sentences, and Z is the set
of POS tag sequences. A traditional representation TRAD-R maps a sentence x ? X to a
sequence of boolean-valued vectors, one vector per word xi in the sentence. Dimensions
for each latent vector include indicators for the word type of xi and various orthographic
features. Table 1 presents the full list of features in TRAD-R. Because our IE task classifies
word types rather than tokens, this baseline is not appropriate for that task. Herein, we
93
Computational Linguistics Volume 40, Number 1
Table 1
Summary of features provided by our representations. ?a1[g(a)] represents a set of boolean
features, one for each value of a, where the feature is true iff g(a) is true. xi represents a token at
position i in sentence x, w represents a word type, Suffixes = {-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity},
k (and k) represents a value for a latent state (set of latent states) in a latent-variable model, y?
represents the maximum a posteriori sequence of states y for x, yi is the latent variable for xi, and
yi,j is the latent variable for xi at layer j. prefix(y,p) is the p-length prefix of the Brown cluster y.
Representation Features
TRAD-R ?w1[xi = w]
?s?Suffixes1[xi ends with s]
1[xi contains a digit]
n-GRAM-R ?w? ,w??P(w?ww??)/P(w)
LSA-R ?w,j{v?left(w)}j
?w,j{v?right(w)}j
NB-R ?k1[y?i = k]
HMM-TOKEN-R ?k1[y?i = k]
HMM-TYPE-R ?kP(y = k|x = w)
I-HMM-TOKEN-R ?j,k1[y?i,j = k]
I-HMM-TYPE-R ?j,kP(y.,j = k|x = w)
BROWN-TOKEN-R ?j?{?2,?1,0,1,2}
?p?{4,6,10,20} prefix(yi+j, p)
BROWN-TYPE-R ?p prefix(y, p)
LATTICE-TOKEN-R ?j,k1[y?i,j = k]
LATTICE-TYPE-R ?kP(y = k|x = w)
describe how we can learn representations R by using a variety of statistical language
models, for use in both our IE and POS tagging tasks. All representations for POS
tagging inherit the features from TRAD-R; all representations for IE do not.
3.2 n-gram Representations
n-gram representations, which we call n-GRAM-R, model a word type w in terms of the
n-gram contexts in which w appears in a corpus. Specifically, for word w we generate
the vector P(w?ww??)/P(w), the conditional probability of observing the word sequence
w? to the left and w?? to the right of w. Each dimension in this vector represents a com-
bination of the left and right words. The experimental section describes the particular
corpora and statistical language modeling methods used for estimating probabilities.
Note that these features depend only on the word type w, and so for every token xi = w,
n-GRAM-R provides the same set of features regardless of local context.
One drawback of n-GRAM-R is that it does not handle sparsity well?the features
are as sparsely observed as the lexical features in TRAD-R, except that n-GRAM-R fea-
tures can be obtained from larger corpora. As an alternative, we apply latent semantic
analysis (LSA) (Deerwester et al. 1990) to compute a reduced-rank representation. For
word w, let vright(w) represent the right context vector of w, which in each dimension
contains the value of P(ww??)/P(w) for some word w??, as observed in the n-gram
model. Similarly, let vleft(w) be the left context vector of w. We apply LSA to the set
94
Huang et al. Computational Linguistics
   
 


	 	 	 	
 	


 
Figure 1
A graphical representation of the naive Bayes statistical language model. The B and E are special
dummy words for the beginning and end of the sentence.
of right context vectors and the set of left context vectors separately,1 to find reduced-
rank versions v?right(w) and v
?
left(w), where each dimension represents a combination
of several context word types. We then use each component of v?right(w) and v
?
left(w)
as features. After experimenting with different choices for the number of dimensions to
reduce our vectors to, we choose a value of 10 dimensions as the one that maximizes
the performance of our supervised sequence labelers on held-out data. We call this
model LSA-R.
3.3 A Context-Dependent Representation Using Naive Bayes
The n-GRAM-R and LSA-R representations always produce the same features F for a
given word type w, regardless of the local context of a particular token xi = w. Our
remaining representations are all context-dependent, in the sense that the features
provided for token xi depend on the local context around xi. We begin with a statis-
tical language model based on the Naive Bayes model with categorical latent states
S = {1, . . . , K}. First, we form trigrams from our sentences. For each trigram, we form a
separate Bayes net in which each token from the trigram is conditionally independent
given the latent state. For tokens xi?1, xi, and xi+1, the probability of this trigram given
latent state Yi = y is given by:
P(xi?1, xi, xi+1|yi) = Pleft(xi?1|yi)Pmid(xi|yi)Pright(xi+1|yi) (6)
where Pleft, Pmid, and Pright are multinomial distributions conditioned on the latent state.
The probability of a whole sentence is then given by the product of the probabilities
of its trigrams. Figure 1 shows a graphical representation of this model. We train our
models using standard expectation-maximization (Dempster, Laird, and Rubin 1977)
with random initialization of the parameters.
Because our factorization of the sentence does not take into account the fact that the
trigrams overlap, the resulting statistical language model is mass-deficient. Worse still,
it is throwing away information from the dependencies among trigrams which might
help make better clustering decisions. Nevertheless, this model closely mirrors many
of the clustering algorithms used in previous approaches to representation learning for
sequence labeling (Ushioda 1996; Miller, Guinness, and Zamanian 2004; Koo, Carreras,
1 Compare with Dhillon, Foster, and Ungar (2011), who use canonical correlation analysis to find a
simultaneous reduction of the left and right context vectors, a significantly more complex undertaking.
95
Computational Linguistics Volume 40, Number 1
and Collins 2008; Lin and Wu 2009; Ratinov and Roth 2009), and therefore serves as an
important benchmark.
Given a naive Bayes statistical language model, we construct an NB-R representa-
tion that produces |S| boolean features Fs(xi) for each token xi and each possible latent
state s ? S:
Fs(xi) =
{
true if s = arg maxs??SP(xi?1, xi, xi+1|yi = s?),
false otherwise.
For a reasonable choice of S (i.e., |S|  |V|), each feature should be observed often
in a sufficiently large training data set. Therefore, compared with n-GRAM-R, NB-R
produces far fewer features. On the other hand, its features for xi depend not just on
the contexts in which xi has appeared in the statistical language model?s training data,
but also on xi?1 and xi+1 in the current sentence. Furthermore, because the range of
the features is much more restrictive than real-valued features, it is less prone to data
sparsity or variations across domains than real-valued features.
3.4 Context-Dependent, Structured Representations: The Hidden Markov Model
In previous work, we have implemented several representations based on hidden
Markov models (Rabiner 1989), which we used for both sequential labeling (like POS
tagging [Huang et al. 2011] and NP chunking [Huang and Yates 2009]) and IE (Downey,
Schoenmackers, and Etzioni 2007). Figure 2 shows a graphical model of an HMM. An
HMM is a generative probabilistic model that generates each word xi in the corpus
conditioned on a latent variable yi. Each yi in the model takes on integral values from 1
to K, and each one is generated by the latent variable for the preceding word, yi?1. The
joint distribution for a corpus x = (x1, . . . , xN ) and a set of state vectors y = (y1, . . . , yN )
is given by: P(x, y) =
?
i P(xi|yi)P(yi|yi?1). Using expectation-maximization (EM)
(Dempster, Laird, and Rubin 1977), it is possible to estimate the distributions for
P(xi|yi) and P(yi|yi?1) from unlabeled data.
We construct two different representations from HMMs, one for sequence-labeling
tasks and one for IE. For sequence labeling, we use the Viterbi algorithm to produce the
optimal setting y? of the latent states for a given sentence x, or y? = argmax
y
P(x, y). We
use the value of y?i as a new feature for xi that represents a cluster of distributionally
similar words. For IE, we require features for word types w, rather than tokens xi.
Applying Bayes? rule to the HMM parameters, we compute a distribution P(Y|x = w),
where Y is a single latent node, x is a single token, and w is its word type. We then use
each of the K values for P(Y = k|x = w), where k ranges from 1 to K, as features. This set
   
 

	 	 	 	
 	

Figure 2
The Hidden Markov Model.
96
Huang et al. Computational Linguistics
of features represents a ?soft clustering? of w into K different clusters. We refer to these
representations as HMM-TOKEN-R and HMM-TYPE-R, respectively.
We also compare against a multi-layer variation of the HMM from our previous
work (Huang and Yates 2010). This model trains an ensemble of M independent HMM
models on the same corpus, initializing each one randomly. We can then use the Viterbi-
optimal decoded latent state of each independent HMM model as a separate feature for
a token, or the posterior distribution for P(Y|x = w) from each HMM as a separate set
of features for each word type. We refer to this statistical language model as an I-HMM,
and the representations as I-HMM-TOKEN-R and I-HMM-TYPE-R, respectively.
Finally, we compare against Brown clusters (Brown et al. 1992) as learned features.
Although not traditionally described as such, Brown clustering involves constructing
an HMM model in which each word type is restricted to having exactly one latent state
that may generate it. Brown et al. describe a greedy agglomerative clustering algorithm
for training this model on unlabeled text. Following Turian, Ratinov, and Bengio (2010),
we use Percy Liang?s implementation of this algorithm for our comparison, and we test
runs with 100, 320, 1,000 and 3,200 clusters. We use features from these clusters identical
to Turian et al.?s.2 Turian et al. have shown that Brown clusters match or exceed the
performance of neural network-based statistical language models in domain adaptation
experiments for named-entity recognition, as well as in-domain experiments for NER
and chunking.
Because HMM-based representations offer a small number of discrete states as
features, they have a much greater potential to combat sparsity than do n-gram mod-
els. Furthermore, for token-based representations, these models can potentially handle
polysemy better than n-gram statistical language models by providing different features
in different contexts.
3.5 A Novel Lattice Statistical Language Model Representation
Our final statistical language model is a novel latent-variable statistical language model,
called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3. The
model contains a lattice of M ? N latent states, where N is the number of words in a
sentence and M is the number of layers in the model. The dotted and solid lines in the
figure together form a complete lattice of edges between these nodes; the PL-MRF uses
only the solid edges. Formally, let c = 	N2 
, where N is the length of the sentence; let i
denote a position in the sentence, and let j denote a layer in the lattice. If i < c and j is
odd, or if j is even and i > c, we delete edges between yi,j and yi,j+1 from the complete
lattice. The same set of nodes remains, but the partial lattice contains fewer edges and
paths between the nodes. A central ?trunk? at i = c connects all layers of the lattice, and
branches from this trunk connect either to the branches in the layer above or the layer
below (but not both).
The result is a model that retains most of the edges of the complete lattice, but
unlike the complete lattice, it supports tractable inference. As M, N ? ?, five out of
every six edges from the complete lattice appear in the PL-MRF. However, the PL-MRF
makes the branches conditionally independent from one another, except through the
trunk. For instance, the left branch between layers 1 and 2 ((y1,1, y1,2) and (y2,1, y2,2)) in
Figure 3 are disconnected; similarly, the right branch between layers 2 and 3 ((y4,2, y4,3)
and (y5,2, y5,3)) are disconnected, except through the trunk and the observed nodes. As
2 Percy Liang?s implementation is available at http://metaoptimize.com/projects/wordreprs/.
97
Computational Linguistics Volume 40, Number 1
y4,1
y3,1
y4,2
y3,2
y4,3
y3,3
y4,4
y3,4
y4,5
y3,5
x1
y2,1
y1,1
x2
y2,2
y1,2
x3
y2,3
y1,3
x4
y2,4
y1,4
x5
y2,5
y1,5
Figure 3
The PL-MRF model for a five-word sentence and a four-layer lattice. Dashed gray edges are part
of a complete lattice, but not part of the PL-MRF.
a result, excluding the observed nodes, this model has a low tree-width of 2 (excluding
observed nodes), and a variety of efficient dynamic programming and message-passing
algorithms for training and inference can be readily applied (Bodlaender 1988). Our
inference algorithm passes information from the branches inwards to the trunk, and
then upward along the trunk, in time O(K4MN). In contrast, a fully connected lattice
model has tree-width = min(M, N), making inference and learning intractable (Sutton,
McCallum, and Rohanimanesh 2007), partly because of the difficulty in enumerating
and summing over the exponentially-many configurations y for a given x.
We can justify the choice of this model from a linguistic perspective as a way to
capture the multi-dimensional nature of words. Linguists have long argued that words
have many different features in a high dimensional space: They can be separately
described by part of speech, gender, number, case, person, tense, voice, aspect, mass
vs. count, and a host of semantic categories (agency, animate vs. inanimate, physical vs.
abstract, etc.), to name a few (Sag, Wasow, and Bender 2003). In the PL-MRF, each layer
of nodes is intended to represent some latent dimension of words.
We represent the probability distribution for PL-MRFs as log-linear models that
decompose over cliques in the MRF graph. Let Cliq(x, y) represent the set of all maximal
cliques in the graph of the MRF model for x and y. Expressing the lattice model in log-
linear form, we can write the marginal probability P(x) of a given sentence x as:
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x?,y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
where score(c, x, y) = exp(?c ? fc(xc, yc)). Our model includes parameters for transitions
between two adjacent latent variables on layer j: ?transi,s,i+1,s?,j for yi,j = s and yi+1,j = s
?. It
also includes observation parameters for latent variables and tokens, as well as for pairs
of adjacent latent variables in different layers and their tokens: ?obsi,j,s,w and ?
obs
i,j,s,j+1,s?,w for
yi,j = s, yi,j+1 = s?, and xi = w.
98
Huang et al. Computational Linguistics
As with our HMM models, we create two representations from PL-MRFs, one for
tokens and one for types. For tokens, we decode the model to compute y?, the matrix of
optimal latent state values for sentence x. For each layer j and and each possible latent
state value k, we add a boolean feature for token xi that is true iff y?i,j = k. For word
types, we compute distributions over the latent state space. Let y be a column vector of
latent variables for word type w. For a PL-MRF model with M layers of binary variables,
there are 2M possible values for y. Our type representation computes a probability
distribution over these 2M possible values, and uses each probability as a feature for
w.3 We refer to these two representations as LATTICE-TOKEN-R and LATTICE-TYPE-R,
respectively.
We train the PL-MRF using contrastive estimation (Smith and Eisner 2005), which
iteratively optimizes the following objective function on a corpus X:
?
x?X
log
?
y
?
c?Cliq(x,y) score(c, x, y)
?
x??N (x),y?
?
c?Cliq(x?,y? ) score(c, x
?, y?)
(7)
where N (x), the neighborhood of x, indicates a set of perturbed variations of the original
sentence x. Contrastive estimation seeks to move probability mass away from the per-
turbed neighborhood sentences and onto the original sentence. We use a neighborhood
function that includes all sentences which can be obtained from the original sentence by
swapping the order of a consecutive pair of words. Training uses gradient descent over
this non-convex objective function with a standard software package (Liu and Nocedal
1989) and converges to a local maximum or saddle point.
For tractability, we modify the training procedure to train the PL-MRF one layer
at a time. Let ?i represent the set of parameters relating to features of layer i, and let
??i represent all other parameters. We fix ??0 = 0, and optimize ?0 using contrastive
estimation. After convergence, we fix ??1, and optimize ?1, and so on. For training each
layer, we use a convergence threshold of 10?6 on the objective function in Equation (7),
and each layer typically converges in under 100 iterations.
4. Domain Adaptation with Learned Representations
We evaluate the representations described earlier on POS tagging and NP chunking
tasks in a domain adaptation setting.
4.1 A Rich Problem Setting for Representation Learning
Existing supervised NLP systems are domain-dependent: There is a substantial drop in
their performance when tested on data from a new domain. Domain adaptation is the
task of overcoming this domain dependence. The aim is to build an accurate system for
3 This representation is only feasible for small numbers of layers, and in our experiments that require type
representations, we used M = 10. For larger values of M, other representations are also possible. We also
experimented with a representation which included only M possible values: For each layer l, we included
P(yl = 0|w) as a feature. We used the less-compact representation in our experiments because results
were better.
99
Computational Linguistics Volume 40, Number 1
a target domain by training on labeled examples from a separate source domain. This
problem is sometimes also called transfer learning (Raina et al. 2007).
Two of the challenges for NLP representations, sparsity and polysemy, are exacer-
bated by domain adaptation. New domains come with new words and phrases that
appear rarely (or even not at all) in the training domain, thus increasing problems
with data sparsity. And even for words that do appear commonly in both domains, the
contexts around the words will change from the training domain to the target domain.
As a result, domain adaptation adds to the challenge of handling polysemous words,
whose meaning depends on context.
In short, domain adaptation is a challenging setting for testing NLP representations.
We now present several experiments testing our representations against state-of-the-
art POS taggers in a variety of domain adaptation settings, showing that the learned
representations surpass the previous state-of-the-art, without requiring any labeled data
from the target domain.
4.2 Experimental Set-up
For domain adaptation, we test our representations on two sequence labeling tasks:
POS tagging and chunking. To incorporate learned representation into our models, we
follow this general procedure, although the details vary by experiment and are given in
the following sections. First, we collect a set of unannotated text from both the training
domain and test domain. Second, we learn representations on the unannotated text.
We then automatically annotate both the training and test data with features from the
learned representation. Finally, we train a supervised linear-chain CRF model on the
annotated training set and apply it to the test set.
A linear-chain CRF is a Markov random field (Darroch, Lauritzen, and Speed 1980)
in which the latent variables form a path with edges only between consecutive nodes in
the path, and all latent variables are globally conditioned on the observations. Let X be a
random variable over data sequences, and Z be a random variable over corresponding
label sequences. The conditional distribution over the label sequence Z given X has the
form
p?(Z = z|X = x) ? exp
?
?
?
i
?
j
?j fj(zi?1, zi, x, i)
?
? (8)
where fj(zi?1, zi, x, i) is a real-valued feature function of the entire observation sequence
and the labels at positions i and i ? 1 in the label sequence, and ?j is a parameter to be
estimated from training data.
We use an open source CRF software package designed by Sunita Sarawagi to train
and apply our CRF models.4 As is standard, we use two kinds of feature functions:
transition and observation. Transition feature functions indicate, for each pair of labels
l and l?, whether zi = l and zi?1 = l?. Boolean observation feature functions indicate, for
each label l and each feature f provided by a representation, whether zi = l and xi has
feature f . For each label l and each real-valued feature f in representation R, real-valued
observation feature functions have value f (x) if zi = l, and are zero otherwise.
4 Available from http://sourceforge.net/projects/crf/.
100
Huang et al. Computational Linguistics
4.3 Domain Adaptation for POS Tagging
Our first experiment tests the performance of all the representations we introduced
earlier on an English POS tagging task, trained on newswire text, to tag biomedical re-
search literature. We follow Blitzer et al.?s experimental set-up. The labeled data consists
of the WSJ portion of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993)
as source domain data, and 561 labeled sentences (9,576 tokens) from the biomedical
research literature database MEDLINE as target domain data (PennBioIE 2005). Fully
23% of the tokens in the labeled test text are never seen in the WSJ training data. The
unlabeled data consists of the WSJ text plus 71,306 additional sentences of MEDLINE
text (Blitzer, McDonald, and Pereira 2006). As a preprocessing step, we replace hapax
legomena (defined as words that appear once in our unlabeled training data) with
the special symbol *UNKNOWN*, and do the same for words in the labeled test sets that
never appeared in any of our unlabeled training text.
For representations, we tested TRAD-R, n-GRAM-R, LSA-R, NB-R, HMM-TOKEN-
R, I-HMM-TOKEN-R (between 2 and 8 layers), and LATTICE-TOKEN-R (8, 12, 16,
and 20 layers). Each latent node in the I-HMMs had 80 possible values, creating
808 ? 1015 possible configurations of the eight-layer I-HMM for a single word. Each
node in our PL-MRF is binary, creating a much smaller number (220 ? 106) of possible
configurations for each word in a 20-layer representation. To give the n-gram model
the largest training data set available, we trained it on the Web 1Tgram corpus (Brants
and Franz 2006). We included the top 500 most common n-grams for each word type,
and then used mutual information on the training data to select the top 10,000 most
relevant n-gram features for all word types, in order to keep the number of features
manageable. We incorporated n-gram features as binary values indicating whether xi
appeared with the n-gram or not. For comparison, we also report on the performance of
Brown clusters (100, 320, 1,000, and 3,200 possible clusters), following Turian, Ratinov,
and Bengio (2010). Finally, we compare against Blitzer, McDonald, and Pereira (2006)
SCL technique, described in Section 2.6, and the standard semi-supervised learning
algorithm ASO (Ando and Zhang 2005), whose results on this task were previously
reported by Blitzer, McDonald, and Pereira (2006).
Table 2 shows the results for the best variation of each kind of model?20 layers for
the PL-MRF, 7 layers for the I-HMM, and 3,200 clusters for the Brown clustering. All
statistical language model representations outperform the TRAD-R baseline.
In nearly all cases, learned representations significantly outperformed TRAD-R. The
best representation, the 20-layer LATTICE-TOKEN-R, reduces error by 47% (35% on
OOV) relative to the baseline TRAD-R, and by 44% (24% on out-of-vocabulary words
(OOV)) relative to the benchmark SCL system. For comparison, this model achieved a
96.8% in-domain accuracy on Sections 22?24 of the Penn Treebank, about 0.5 percentage
point shy of a state-of-the-art in-domain system with more sophisticated supervised
learning (Shen, Satta, and Joshi 2007). The BROWN-TOKEN-R representation, which
Turian, Ratinov, and Bengio (2010) demonstrated performed as well or better than
a variety of neural network statistical language models as representations, achieved
accuracies between the SCL system and the HMM-TOKEN-R. The WEB1T-n-GRAM-R,
I-HMM-TOKEN-R, and LATTICE-TOKEN-R all performed quite close to one another,
but the I-HMM-TOKEN-R and LATTICE-TOKEN-R were trained on many orders of
magnitude less text. The LSA-R and NB-R outperformed the TRAD-R baseline but
not the SCL system. The n-GRAM-R, which was trained on the same text as the
other representations except the WEB1T-n-GRAM-R, performed far worse than the
WEB1T-n-GRAM-R.
101
Computational Linguistics Volume 40, Number 1
Table 2
Learned representations, and especially latent-variable statistical language model
representations, significantly outperform a traditional CRF system on domain adaptation for
POS tagging. Percent error is shown for all words and out-of-vocabulary (OOV) words. The
SCL+500bio system was given 500 labeled training sentences from the biomedical domain.
1.8% of tokens in the biomedical test set had POS tags like ?HYPHENATED?, which are not
part of the tagset for the training data, and were labeled incorrectly by all systems without
access to labeled data from the biomedical domain. As a result, an error rate of 1.8 + 3.9 = 5.7
serves as a reasonable lower bound for a system that has never seen labeled examples from
the biomedical domain.
Model All words OOV words
TRAD-R 11.7 32.7
n-GRAM-R 11.7 32.2
LSA-R 11.6 31.1
NB-R 11.6 30.7
ASO 11.6 29.1
SCL 11.1 28
BROWN-TOKEN-R 10.0 25.2
HMM-TOKEN-R 9.5 24.8
WEB1T-n-GRAM-R 6.9 24.4
I-HMM-TOKEN-R 6.7 24
LATTICE-TOKEN-R 6.2 21.3
SCL+500bio 3.9 ?
The amount of unlabeled training data has a significant impact on the performance
of these representations. This is apparent in the difference between WEB1T-n-GRAM-
R and n-GRAM-R, but it is also true for our other representations. Figure 4 shows the
accuracy of a representative subset of our taggers on words not seen in labeled training
data, as we vary the amount of unlabeled training data available to the language
Figure 4
Learning curve for representations: target domain accuracy of our taggers on OOV words
(not seen in labeled training data), as a function of the number of unlabeled examples given
to the language models.
102
Huang et al. Computational Linguistics
models. Performance grows steadily for all representations we measured, and none
of the learning curves appears to have peaked. Furthermore, the margin between the
more complex graphical models and the simpler n-gram models grows with increasing
amounts of training data.
4.3.1 Sparsity and Polysemy. We expected that statistical language model represen-
tations would perform well in part because they provide meaningful features for
sparse and polysemous words. For sparse tokens, these trends are already evident
in the results in Table 2: Models that provide a constrained number of features, like
HMM-based models, tend to outperform models that provide huge numbers of fea-
tures (each of which, on average, is only sparsely observed in training data), like
TRAD-R.
As for polysemy, HMM models significantly outperform naive Bayes models and
the n-GRAM-R. The n-GRAM-R?s features do not depend on a token type?s context at all,
and the NB-R?s features depend only on the tokens immediately to the right and left of
the current word. In contrast, the HMM takes into account all tokens in the surrounding
sentence (although the strength of the dependence on more distant words decreases
rapidly). Thus the performance of the HMM compared with n-GRAM-R and NB-R,
as well as the performance of the LATTICE-TOKEN-R compared with the WEB1T-n-
GRAM-R, suggests that representations that are sensitive to the context of a word
produce better features.
To test these effects more rigorously, we selected 109 polysemous word types from
our test data, along with 296 non-polysemous word types. The set of polysemous word
types was selected by filtering for words in our labeled data that had at least two
POS tags that began with distinct letters (e.g., VBZ and NNS). An initial set of non-
polysemous word types was selected by filtering for types that appeared with just
one POS tag. We then manually inspected these initial selections to remove obvious
cases of word types that were in fact polysemous within a single part-of-speech, such
as ?bank.? We further define sparse word types as those that appear five times or
fewer in all of our unlabeled data, and we define non-sparse word types as those that
appear at least 50 times in our unlabeled data. Table 3 shows our POS tagging results
on the tokens of our labeled biomedical data with word types matching these four
categories.
As expected, all of our statistical language models outperform the baseline by
a larger margin on polysemous words than on non-polysemous words. The margin
between graphical model representations and the WEB1T-n-GRAM-R model also in-
creases on polysemous words, except for the NB-R. The WEB1T-n-GRAM-R uses none
of the local context to decide which features to provide, and the NB-R uses only the
immediate left and right context, so both models ignore most of the context. In contrast,
the remaining graphical models use Viterbi decoding to take into account all tokens
in the surrounding sentence, which helps to explain their relative improvement over
WEB1T-n-GRAM-R on polysemous words.
The same behavior is evident for sparse words, as compared with non-sparse
words: All of the statistical language model representations outperform the baseline
by a larger margin on sparse words than not-sparse words, and all of the graphical
models perform better relative to the WEB1T-n-GRAM-R on sparse words than not-
sparse words. By reducing the feature space from millions of possible n-gram fea-
tures to L categorical features, these models ensure that each of their features will
be observed often in a reasonably sized training data set. Thus representations based
103
Computational Linguistics Volume 40, Number 1
Table 3
Graphical models consistently outperform n-gram models by a larger margin on sparse words
than not-sparse words, and by a larger margin on polysemous words than not-polysemous
words. One exception is the NB-R, which performs worse relative to WEB1T-n-GRAM-R on
polysemous words than non-polysemous words. For each graphical model representation,
we show the difference in performance between that representation and WEB1T-n-GRAM-R
in parentheses. For each representation, differences in accuracy on polysemous and
non-polysemous subsets were statistically significant at p < 0.01 using a two-tailed
Fisher?s exact test. Likewise for performance on sparse vs. non-sparse categories.
polysemous not polysemous sparse not sparse
tokens 159 4,321 463 12,194
TRAD-R 59.5 78.5 52.5 89.6
WEB1T-n-GRAM-R 68.2 85.3 61.8 94.0
NB-R 64.5 88.7 57.8 89.4
(-WEB1T-n-GRAM-R) (?3.7) (+3.4) (?4.0) (?4.6)
HMM-TOKEN-R 67.9 83.4 60.2 91.6
(-WEB1T-n-GRAM-R) (?0.3) (?1.9) (?1.6) (?2.4)
I-HMM-TOKEN-R 75.6 85.2 62.9 94.5
(-WEB1T-n-GRAM-R) (+7.4) (?0.1) (+1.1) (+0.5)
LATTICE-TOKEN-R 70.5 86.9 65.2 94.6
(-WEB1T-n-GRAM-R) (+2.3) (+1.6) (+3.4) (+0.6)
on graphical models help address two key issues in building representations for POS
tagging.
4.3.2 Domain Divergence. Besides sparsity and polysemy, Ben-David et al.?s (2007, 2010)
theoretical analysis of domain adaptation shows that the distance between two domains
under a representation R of the data is crucial for a good representation. We test their
predictions using learned representations.
Ben-David et al.?s (2007, 2010) analysis depends on a particular notion of distance,
the d1 divergence, that is computationally intractable to calculate. For our analysis, we
resort instead to two different computationally efficient approximations of this measure.
The first uses a more standard notion of distance: the Jensen-Shannon Divergence (dJS),
a distance metric for probability distributions:
dJS(p||q) = 12
?
i
[
pilog
( pi
mi
)
+ qilog
( qi
mi
)]
where mi =
pi+qi
2 .
Intuitively, we aim to measure the distance between two domains by measuring
whether features appear more commonly in one domain than in the other. For instance,
the biomedical domain is far from the newswire domain under the TRAD-R repre-
sentation because word-based features like protein, gene, and pathway appear far more
commonly in the biomedical domain than the newswire domain. Likewise, bank and
president appear far more commonly in newswire text. Since the d1 distance is related
to the optimal classifier for distinguishing two domains, it makes sense to measure the
distance by comparing the frequencies of these features: a classifier can easily use the
occurrence of words like bank and protein to accurately predict whether a given sentence
belongs to the newswire or biomedical domain.
104
Huang et al. Computational Linguistics
More formally, let S and T be two domains, and let f be a feature5 in representation
R?that is, a dimension of the image space of R. Let V be the set of possible values
that f can take on. Let US be an unlabeled sample drawn from S, and likewise for
UT. We first compute the relative frequencies of the different values of f in R(US) and
R(UT ), and then compute dJS between these empirical distributions. Let pf represent the
empirical distribution over V estimated from observations of feature f in R(US), and let
qf represent the same distribution estimated from R(UT ).
Definition 1
JS domain divergence for a feature or df (US, UT ) is the domain divergence between
domains S and T under feature f from representation R, and is given by
df (US, UT ) = dJS(pf ||qf )
For a multidimensional representation, we compute the full domain divergence as a
weighted sum over the domain divergences for its features. Because individual features
may vary in their relevance to a sequence-labeling task, we use weights to indicate
their importance to the overall distance between the domains. We set the weight wf
for feature f proportional to the L1 norm of CRF parameters related to f in the trained
POS tagger. That is, let ? be the CRF parameters for our trained POS tagger, and let
?f = {?l,v|l be the state for zi and v be the value for f}. We set wf =
||?f ||1
||?||1 .
Definition 2
JS Domain Divergence or dR(US, UT ), is the distance between domains S and T under
representation R, and is given by
dR(US, UT ) =
?
f
wf df (US, UT )
Blitzer (2008) uses a different notion of domain divergence to approximate the d1
divergence, which we also experimented with. He trains a CRF classifier on examples
labeled with a tag indicating which domain the example was drawn from. We refer to
this type of classifier as a domain classifier. Note that these should not be confused
with our CRFs used for POS tagging, which take as input examples which are labeled
with POS sequences. For the domain classifier, we tag every token from the WSJ domain
as 0, and every token from the biomedical domain as 1. Blitzer then uses the accuracy
of his domain classifier on a held-out test set as his measure of domain divergence. A
high accuracy for the domain classifier indicates that the representation makes the two
domains easy to separate, and thus high accuracy signifies a high domain divergence. To
measure domain divergence using a domain classifier, we trained our representations
on all of the unlabeled data for this task, as before. We then used 500 randomly sampled
sentences from the WSJ domain, and 500 randomly sampled biomedical sentences, and
labeled these with 0 for the WSJ data and 1 for the biomedical data. We measured
the error rate of our domain-classifier CRF as the average error rate across folds when
performing three-fold cross-validation on these 1,000 sentences.
5 For simplicity, the definition we provide here works only for discrete features, although it is possible to
extend this definition to continuous-valued features.
105
Computational Linguistics Volume 40, Number 1
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.32 0.37 0.42 0.47
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Divergence under the Representation 
LATTICE-R
I-HMM-R
Trad-R
Ngram-R
1 HMM 
7 HMMs 
8 layer LATTICE 
20 layer LATTICE 
Figure 5
Target-domain POS tagging accuracy for a model developed using a representation R correlates
strongly with lower JS domain divergence between WSJ and biomedical text under each
representation R. The correlation coefficients r2 for the linear regressions drawn in the
figure are both greater than 0.97.
Figure 5 plots the accuracies and JS domain divergences for our POS taggers.
Figure 6 shows the difference between target-domain error and source-domain error
as a function of JS domain divergence. Figures 7 and 8 show the same information,
except that the x axis plots the accuracy of a domain classifier as the way of mea-
suring domain divergence. These results give empirical support to Ben-David et al.?s
(2007, 2010) theoretical analysis: Smaller domain divergence?whether measured by
JS domain divergence or by the accuracy of a domain classifier?correlates strongly
with better target-domain accuracy. Furthermore, smaller domain divergence correlates
strongly with a smaller difference in the accuracy of the taggers on the source and
target domains.
Figure 6
Smaller JS domain divergence correlates with a smaller difference between target-domain error
and source-domain error.
106
Huang et al. Computational Linguistics
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98
Ta
rg
et
 D
om
ai
n 
 
Ta
gg
in
g 
Ac
cu
ra
cy
 
Domain Classification Accuracy 
Trad Rep
I-HMM
Ngram
PL-MRF
1 HMM 
7 HMMs 
20 layer PL-MRF 
8 layer PL-MRF 
Figure 7
Target-domain tagging accuracy decreases with the accuracy of a CRF domain classifier.
Intuitively, this means that training data from a source domain is less helpful for tagging in
a target domain when source-domain data is easy to distinguish from target-domain data.
Figure 8
Better domain classification correlates with a larger difference between target-domain error and
source-domain error.
Although both the JS domain divergence and the domain classifier provide only
approximations of the d1 metric for domain divergence, they agree very strongly:
In both cases, the LATTICE-TOKEN-R representations had the lowest domain diver-
gence, followed by the I-HMM-TOKEN-R representations, followed by TRAD-R, with
n-GRAM-R somewhere between LATTICE-TOKEN-R and I-HMM-TOKEN-R. The main
difference between the two metrics appears to be that the JS domain divergence gives
a greater domain divergence to the eight-layer LATTICE-TOKEN-R model and the
n-GRAM-R, placing them past the four- through eight-layer I-HMM-TOKEN-R represen-
tations. The domain classifier places these models closer to the other LATTICE-TOKEN-R
representations, just past the seven-layer I-HMM-TOKEN-R representation.
107
Computational Linguistics Volume 40, Number 1
The domain divergences of all models, using both techniques for measuring diver-
gence, remain significantly far from zero, even under the best representation. As a result,
there is ample room to experiment with even less-divergent representations of the two
domains, to see if they might yield ever-increasing target-domain accuracies. Note that
this is not simply a matter of adding more layers to the layered models. The I-HMM-
TOKEN-R model performed best with seven layers, and the eight-layer representation
had about the same accuracy and domain divergence as the five-layer model. This
may be explained by the fact that the I-HMM layers are trained independently, and so
additional layers may be duplicating other ones, and causing the supervised classifier
to overfit. But it also shows that our current methodology has no built-in technique
for constraining the domain divergence in our representations?the decrease in domain
divergence from our more sophisticated representations is a coincidental byproduct of
our training methodology, but there is no guarantee that our current mechanisms will
continue to decrease domain divergence simply by increasing the number of layers. An
important consideration for future research is to devise explicit learning mechanisms
that guide representations towards smaller domain divergences.
4.4 Domain Adaptation for Noun-Phrase Chunking and Chinese POS Tagging
We test the generality of our representations by using them for other tasks, domains, and
languages. Here, we report on further sequence-labeling tasks in a domain adaptation
setting: noun phrase chunking for adaptation from news text to biochemistry journals,
and POS tagging in Mandarin for a variety of domains. In the next section, we describe
the use of our representations in a weakly supervised information extraction task.
For chunking, the training set consists of the CoNLL 2000 shared task data for
source-domain labeled data (Sections 15?18 of the WSJ portion of the Penn Treebank,
labeled with chunk tags) (Tjong, Sang, and Buchholz 2000). For test data, we used
biochemistry journal data from the Open American National Corpus6 (OANC). One
of the authors manually labeled 198 randomly selected sentences (5,361 tokens) from
the OANC biochemistry text with noun-phrase chunk information.7 We focus on noun
phrase chunks because they are relatively easy to annotate manually, but contain a large
variety of open-class words that vary from domain to domain. The labeled training set
consists of 8,936 sentences and 211,726 tokens. Twenty-three percent of chunks in the
test set begin with an OOV word (especially adjective-noun constructions like ?aqueous
formation? and ?angular recess?), and 29% begin with a word seen at most twice in
training data; we refer to these as OOV chunks and rare chunks. For our unlabeled
data, we use 15,000 sentences (358,000 tokens; Sections 13?19) of the Penn Treebank
and 45,000 sentences (1,083,000 tokens) from the OANC?s biochemistry section. We
tested TRAD-R (augmented with features for automatically generated POS tags), LSA-R,
n-GRAM-R, NB-R, HMM-TOKEN-R, I-HMM-TOKEN-R (7 layers, which performed best
for POS tagging) and LATTICE-TOKEN-R (20 layers) representations.
Figure 9 shows our NP chunking results for this domain adaptation task. The
performance improvements for the HMM-based chunkers are impressive: LATTICE-
TOKEN-R reduces error by 57% with respect to TRAD-R, and comes close to state-of-the-
art results for chunking on newswire text. The results suggest that this representation
allows the CRF to generalize almost as well to out-of-domain text as in-domain text.
6 Available from http://www.anc.org/OANC/.
7 The labeled data for this experiment are available from the first author?s Web site.
108
Huang et al. Computational Linguistics
F1
on
B
io
ch
em
is
tr
y
Te
xt
0.72 0.74 
0.75 0.76 
0.84 
0.87 0.89 0.86 0.87 0.87 0.88 
0.91 
0.94 0.94 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Trad-R Ngram-R LSA-R NB-R HMM-R I-HMM-R LATTICE-R
OOV ALL
Freq: 0 1 2 all
Chunks: 284 39 39 1,258
R P R P R P R P
TRAD-R .74 .70 .85 .87 .79 .86 .86 .87
n-GRAM-R .74 .74 .85 .85 .79 .86 .87 .87
LSA-R .76 .74 .82 .83 .78 .85 .87 .88
NB-R .73 .78 .86 .73 .86 .75 .88 .88
HMM-TOKEN-R .80 .89 .92 .88 .92 .90 .91 .90
I-HMM-TOKEN-R .90 .86 .92 .95 .87 .97 .95 .92
LATTICE-TOKEN-R .92 .85 .94 .95 .87 .97 .95 .93
Figure 9
On biomedical journal data from the OANC, our best NP chunker outperforms the baseline
CRF chunker by 0.17 F1 on chunks that begin with OOV words, and by 0.08 on all chunks. The
table shows performance breakdowns (recall and precision) for chunks whose first word has
frequency 0, 1, and 2 in training data, and the number of chunks in test data that fall into each
of these categories.
Improvements are greatest on OOV and rare chunks, where LATTICE-TOKEN-R made
absolute improvements over TRAD-R by 0.17 and 0.09 F1, respectively. Improvements
for the single-layer HMM-TOKEN-R were smaller but still significant: 36% relative re-
duction in error overall, and 32% for OOV chunks.
The improved performance from our HMM-based chunker caused us to wonder
how well the chunker could work without some of its other features. We removed all
tag features and orthographic features and all features for word types that appear fewer
than 20 times in training. This chunker still achieves 0.91 F1 on OANC data, and 0.93
F1 on WSJ data (Section 20), outperforming the TRAD-R system in both cases. It has
only 20% as many features as the baseline chunker, greatly improving its training time.
Thus these features are more valuable to the chunker than features from automatically
produced tags and features for all but the most common words.
For Chinese POS tagging, we use text from the UCLA Corpus of Written Chinese
(Tao and Xiao 2007), which is part of the Lancaster Corpus of Mandarin Chinese
(LCMC). The UCLA Corpus consists of 11,192 sentences of word-segmented and POS-
tagged text in 13 genres (see Table 4). We use gold-standard word segmentation labels
for training and testing. The LCMC tagset consists of 50 Chinese POS tags. On average,
each genre contains 5,284 word tokens, for a total of 68,695 tokens among all genres. We
use the ?news? genre as our source domain, which we use for training and development
109
Computational Linguistics Volume 40, Number 1
Table 4
POS tagging accuracy: the LATTICE-TOKEN-R and other graphical model representations
outperform TRAD-R and state-of-the-art Chinese POS taggers on all target domains. For target
domains, * indicates the performance is statistically significantly better than the Stanford and
TRAD-R baselines at p < 0.05, using a two-tailed ?2 test; ** indicates significance at p < 0.01.
On the news domain, the Stanford tagger is significantly different from all other systems
using a two-tailed ?2 test with p < 0.01.
Domain Stanford TRAD NGR LSA NB HMM I-H LAT
lore 88.4 84.0 84.2 85.3 85.3 89.7 89.9 90.1*
religion 83.5 79.1 79.4 79.8 80.0 85.2 85.6 85.9*
humour 89.0 84.2 84.5 86.2 86.8 89.6 89.6 89.9*
general-fic 87.5 84.5 85.0 85.3 85.7 89.4 89.7 89.9*
essay 88.4 83.2 83.7 84.0 84.3 89.0 89.1 90.1*
mystery 87.4 82.4 83.4 84.3 85.3 90.1 91.1 91.3**
romance 87.5 84.2 84.5 85.3 86.1 89.0 89.5 89.8**
science-fic 88.6 82.1 82.5 83.0 83.0 87.0 88.3 88.6
skills 82.7 77.3 77.7 78.2 78.4 84.9 85.0 85.1**
science 86.0 82.0 82.3 82.4 82.4 87.8 87.8 87.9*
adventure-fic 82.1 74.3 75.2 76.1 77.8 81.7 82.0 82.2
report 91.7 84.2 85.1 85.3 86.1 91.9 91.9 91.9
news 98.8** 96.9 92.3 93.4 94.3 94.2 97.0 97.1
all but news 87.0 81.2 82.0 82.8 83.6 88.1 88.4 88.8**
all domains 88.7 83.2 83.6 84.4 85.5 89.5 89.7 90.0**
data. For test data, we randomly select 20% of every other genre. For our unlabeled
data, we use all of the ?news? text, plus the remaining 80% of the texts from the other
genres. As before, we replace hapax legomena in the unlabeled data with the special
symbol *UNKNOWN*, and do the same for word types in the labeled test sets that never
appear in our unlabeled training texts. We compare against a state-of-the-art Chinese
POS tagger for in-domain text, the CRF-based Stanford tagger (Tseng, Jurafsky, and
Manning 2005). We obtained the code for this tagger,8 and retrained it on our training
data set.
The Chinese POS tagging results are shown in Table 4. The LATTICE-TOKEN-R
outperforms the state-of-the-art Stanford tagger on all target domains. Overall, on all
out-of-domain tests, LATTICE-TOKEN-R provides a relative reduction in error of 13.8%
compared with the Stanford tagger. The best performance is on the ?mystery? domain,
where the LATTICE-TOKEN-R model reaches 91.3% accuracy, a 3.9 percentage points
improvement over the Stanford tagger. Its performance on the in-domain ?news? test set
is significantly worse (1.7 percentage points) than the Stanford tagger, suggesting that
the Stanford tagger relies on domain-dependent features that are helpful for tagging
news, but not for tagging in general. The LATTICE-TOKEN-R?s accuracy is still signifi-
cantly worse on out-of-domain text than in-domain text, but the gap between the two
(8.3 percentage points) is better than the gap for the Stanford tagger (11.8 percentage
points). We believe that the lower out-of-domain performance of our Chinese POS
tagger, compared with our English POS tagger and our chunker, was at least in part
due to having far less unlabeled text available for this task.
8 Available at http://nlp.stanford.edu/software/tagger.shtml.
110
Huang et al. Computational Linguistics
5. Information Extraction Experiments
In this section, we evaluate our learned representations on their ability to capture
semantic, rather than syntactic, information. Specifically, we investigate a set-expansion
task in which we?re given a corpus and a few ?seed? noun phrases from a semantic
category (e.g., Superheroes), and our goal is to identify other examples of the category
in the corpus. This is a different type of weakly supervised task from the earlier domain
adaptation tasks because we are given only a handful of positive examples from a cate-
gory, rather than a large sample of positively and negatively labeled training examples
from a separate domain.
Existing set-expansion techniques utilize the distributional hypothesis: Candidate
noun phrases for a given semantic class are ranked based on how similar their contex-
tual distributions are to those of the seeds. Here, we measure how performance on the
set-expansion task varies when we employ different representations for the contextual
distributions.
5.1 Methods
The set-expansion task we address is formalized as follows. Given a corpus, a set of
seeds from some semantic category C, and a separate set of candidate phrases P, output
a ranking of the phrases in P in decreasing order of likelihood of membership in the
semantic category C.
For any given representation R, the set-expansion algorithm we investigate is
straightforward: We rank candidate phrases in increasing order of the distance between
their feature vectors and those of the seeds. The particular distance metrics utilized are
detailed subsequently.
Because set expansion is performed at the level of word types rather than to-
kens, it requires type-based representations. We compare HMM-TYPE-R, n-GRAM-R,
LATTICE-TYPE-R, and BROWN-TYPE-R in this experiment. We used a 25-state HMM,
and the LATTICE-TYPE-R as described in the previous section. Following previous set-
expansion experiments with n-grams (Ahuja and Downey 2010), we use a trigram
model with Kneser-Ney smoothing for n-GRAM-R.
The distances between the candidate phrases and the seeds for HMM-TYPE-R,
n-GRAM-R, and LATTICE-TYPE-R representations are calculated by first creating a
prototypical ?seed feature vector? equal to the mean of the feature vectors for each
of the seeds in the given representation. Then, we rank candidate phrases in order
of increasing distance between their feature vector and the seed feature vector. As a
distance measure between vectors (in this case, probability distributions), we compute
the average of five standard distance measures, including KL and JS divergence, and
cosine, Euclidean, and L1 distance. In experiments, we found that improving upon
this simple averaging was not easy?in fact, tuning a weighted average of the distance
measures for each representation did not improve results significantly on held-out data.
For Brown clusters, we use prefixes of all possible lengths as features. We define
the similarity between two Brown representation feature vectors to be the number of
features they share in common (this is equivalent to the length of the longest common
prefix between the two original Brown cluster labels). The candidate phrases are then
ranked in decreasing order of the sum of their similarity scores to each of the seeds. We
experimented with normalizing the similarity scores by the longer of the two vector
lengths, and found this to decrease results slightly. We use unnormalized (integer)
similarity scores for Brown clusters in our experiments.
111
Computational Linguistics Volume 40, Number 1
5.2 Data Sets
We utilized a set of approximately 100,000 sentences of Web text, joining multi-word
named entities in the corpus into single tokens using the Lex algorithm (Downey,
Broadhead, and Etzioni 2007). This process enables each named entity (the focus of the
set-expansion experiments) to be treated as a single token, with a single representation
vector for comparison. We developed all word type representations using this corpus.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia
?listOf? pages from Pantel et al. (2009) and augmented these with our own manually
defined categories, such that each list contained at least ten distinct examples occurring
in our corpus. In all, we had 432 examples across 16 distinct categories such as Coun-
tries, Greek Islands, and Police TV Dramas.
5.3 Results
For each semantic category, we tested five different random selections of five seed
examples, treating the unselected members of the category as positive examples, and
all other candidate phrases as negative examples. We evaluate using the area under the
precision-recall curve (AUC) metric.
The results are shown in Table 5. All representations improve performance over
a random baseline, equal to the average AUC over five random orderings for each
category, and the graphical models outperform the n-gram representation. I-HMM-
TYPE-R and Brown clustering in the particular case of 1,000 clusters perform best, with
HMM-TYPE-R performing nearly as well. Brown clusters give somewhat lower results
as the number of clusters varies.
As with POS tagging, we expect that language model representations improve
performance on the IE task by providing informative features for sparse word types.
However, because the IE task classifies word types rather than tokens, we expect the rep-
resentations to provide less benefit for polysemous word types. To test these hypotheses,
we measured how IE performance changed in sparse or polysemous settings. We identi-
fied polysemous categories as those for which fewer than 90% of the category members
had the category as a clear dominant sense (estimated manually); other categories were
considered non-polysemous. Categories whose members had a median number of
occurrences in the corpus of less than 30 were deemed sparse, and others non-sparse.
Table 5
I-HMM-TYPE-R outperforms the other methods, improving performance over a random
baseline by twice as much as either n-GRAM-R or LATTICE-TYPE-R.
model AUC
I-HMM-TYPE-R 0.18
HMM-TYPE-R 0.17
BROWN-TYPE-R-3200 0.16
BROWN-TYPE-R-1000 0.18
BROWN-TYPE-R-320 0.15
BROWN-TYPE-R-100 0.13
LATTICE-TYPE-R 0.11
n-GRAM-R baseline 0.10
Random baseline 0.10
112
Huang et al. Computational Linguistics
Table 6
Graphical models as representations for IE consistently perform better relative to n-gram models
on sparse words, but not necessarily polysemous words.
polysemous not-polysemous sparse not-sparse
types 222 210 266 166
categs. 12 4 13 3
n-GRAM-R 0.07 0.17 0.06 0.25
LATTICE-TYPE-R 0.09 0.15 0.1 0.19
-n-GRAM-R +0.02 ?0.02 +0.04 ?0.06
HMM-TYPE-R 0.14 0.26 0.15 0.32
-n-GRAM-R +0.07 +0.09 +0.09 +0.07
IE performance on these subsets of the data are shown in Table 6. Both graphical
model representations outperform the n-gram representation more on sparse words, as
expected. For polysemy, the picture is mixed: The LATTICE-TYPE-R outperforms
n-GRAM-R on polysemous categories, whereas HMM-TYPE-R?s performance advan-
tage over n-GRAM-R decreases.
One surprise on the IE task is that the LATTICE-TYPE-R performs significantly less
well than the HMM-TYPE-R, whereas the reverse is true on POS tagging. We suspect
that the difference is due to the issue of classifying types vs. tokens. Because of their
more complex structure, PL-MRFs tend to depend more on transition parameters than
do HMMs. Furthermore, our decision to train the PL-MRFs using contrastive estimation
with a neighborhood that swaps consecutive pairs of words also tends to emphasize
transition parameters. As a result, we believe the posterior distribution over latent states
given a word type is more informative in our HMM model than the PL-MRF model.
We measured the entropy of these distributions for the two models, and found that
H(PPL-MRF(y|x = w)) = 9.95 bits, compared with H(PHMM(y|x = w)) = 2.74 bits, which
supports the hypothesis that the drop in the PL-MRF?s performance on IE is due to its
dependence on transition parameters. Further experiments are warranted to investigate
this issue.
5.4 Testing the Language Model Representation Hypothesis in IE
The language model representation hypothesis (Section 2) suggests that all else being
equal, more accurate language models will provide features that lead to better perfor-
mance on NLP tasks. Here, we test this hypothesis on the set expansion IE task.
Figures 10 and 11 show how the performance of the HMM-TYPE-R varies with the
language modeling accuracy of the underlying HMM. Language modeling accuracy
is measured in terms of perplexity on held-out text. Here, we use set expansion data
sets from previous work (Ahuja and Downey 2010). The first two are composed of
extractions from the TextRunner information extraction system (Banko et al. 2007) and
are denoted as Unary (361 examples) and Binary (265 examples). The second, Wikipedia
(2,264 examples), is a sample of Wikipedia concept names. We evaluate the performance
of several different trained HMMs with numbers of latent states K ranging from 5 to
1,600 (to help illustrate how IE and LM performance varies even when model capacity
is fixed, we include three distinct models with K = 100 states trained separately over
the full corpus). We used a distributed implementation of HMM training and corpus
113
Computational Linguistics Volume 40, Number 1
K = 5
K = 10
K = 25 K = 50
K = 100
K = 100
K = 100
K = 200
K = 400
Figure 10
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy of
the HMM varies, on TextRunner data sets. IE accuracy (in terms of area under the precision-recall
curve) tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
5 10
25 50
100
100
100
200 400
5
5
25
2550
50
100
200
100
200
800
1600 400
Figure 11
Information extraction (IE) performance of HMM-TYPE-R as the language modeling accuracy
of the HMM varies on the Wikipedia data set. Number labels indicate the number of latent
states K, and performance is shown for three training corpus sizes (the full corpus consists of
approximately 60 million tokens). IE accuracy (in terms of area under the precision-recall curve)
tends to increase as language modeling accuracy improves (i.e., perplexity decreases).
partitioning techniques (Yang, Yates, and Downey 2013) to enable training of our larger
capacity HMM models on large data sets.
The results provide support for the language model representation hypothesis,
showing that IE performance does tend to improve as language model perplexity
decreases. On the smaller Unary and Binary sets (Figure 10), although IE accuracy
114
Huang et al. Computational Linguistics
does decrease for the lowest-perplexity models, overall language model perplexity
exhibits a negative correlation with IE area under the precision-recall curve (the Pearson
correlation coefficient is ?0.18 for Unary, and ?0.28 for Binary). For Wikipedia (Fig-
ure 11), the trend is more consistent, with IE performance increasing monotonically
as perplexity decreases for models trained on the full training corpus (the Pearson
correlation coefficient is ?0.90).
Figure 11 also illustrates how LM and IE performance changes as the amount
of training text varies. In general, increasing the training corpus size increases IE
performance and decreases perplexity. Over all data points in the figure, IE perfor-
mance correlates most strongly with model perplexity (?0.68 Pearson correlation, ?0.88
Spearman correlation), followed by corpus size (0.66, 0.71) and model capacity (?0.05,
0.38). The small negative Pearson correlation between model capacity and IE perfor-
mance is primarily due to the model with 1,600 states trained on 4% of the corpus.
This model has a large parameter space and sparse training data, and thus suffers from
overfitting in terms of both model perplexity and IE performance. If we ignore this
overfit model, the Pearson correlation between model capacity and IE performance for
the other models in the Figure is 0.24.
Our results show that IE based on distributional similarity tends to improve as the
quality of the latent variable model used to measure distributional similarity improves.
A similar trend was exhibited in our previous work (Ahuja and Downey 2010); here, we
extend the previous results to models with more latent states and a larger, more reliable
test set (Wikipedia). The results suggest that scaling up the training of latent variable
models to utilize larger training corpora and more latent states may be a promising
direction for improving IE capabilities.
6. Conclusion and Future Work
Our study of representation learning demonstrates that by using statistical language
models to aggregate information across many unannotated examples, it is possible to
find accurate distributional representations that can provide highly informative features
to weakly supervised sequence labelers and named-entity classifiers. For both domain
adaptation and weakly supervised set expansion, our results indicate that graphical
models outperform n-gram models as representations, in part for their greater ability to
handle sparsity and polysemy. Our IE task provides important evidence to support the
Language Model Representation Hypothesis, showing that the AUC of the IE system
correlates more with language model perplexity than the size of the training data or
the capacity of the language model. Finally, our sequence labeling experiments provide
empirical evidence in support of theoretical work on domain adaptation, showing that
target-domain tagging accuracy is highly correlated with two different measures of
domain divergence.
Representation learning remains a promising area for finding further improve-
ments in various NLP tasks. The representations we have described are trained in
an unsupervised fashion, so a natural extension is to investigate supervised or semi-
supervised representation-learning techniques. As mentioned previously, our current
techniques have no built-in methods for enforcing that they provide similar features in
different domains; devising a mechanism that enforces this could allow for less domain-
divergent and potentially more accurate representations. We have considered sequence
labeling, but another promising direction is to apply these techniques to more complex
structured prediction tasks, like parsing or relation extraction. Our current approach
to sequence labeling requires retraining of a CRF for every new domain; incremental
115
Computational Linguistics Volume 40, Number 1
retraining techniques for new domains would speed up the process. Finally, models
that combine our representation learning approach with instance weighting and other
forms of supervised domain adaptation may take better advantage of labeled data in
target domains, when it is available.
Acknowledgments
This material is based on work supported
by the National Science Foundation under
grant no. IIS-1065397.
References
Ahuja, Arun and Doug Downey. 2010.
Improved extraction assessment through
better language models. In Proceedings of
the Annual Meeting of the North American
Chapter of the Association of Computational
Linguistics (NAACL-HLT), pages 225?228,
Los Angeles, CA.
Ando, Rie Kubota and Tong Zhang. 2005.
A high-performance semi-supervised
learning method for text chunking.
In Proceedings of the ACL, pages 1?9,
Ann Arbor, MI.
Banko, Michele, Michael J. Cafarella,
Stephen Soderland, Matt Broadhead, and
Oren Etzioni. 2007. Open information
extraction from the web. In Proceedings of
the IJCAI, pages 2670?2676, Hyderabad.
Banko, Michele and Robert C. Moore.
2004. Part of speech tagging in context.
In Proceedings of the COLING, pages
556?561, Geneva.
Ben-David, Shai, John Blitzer, Koby
Crammer, Alex Kulesza, Fernando Pereira,
and Jenn Wortman. 2010. A theory of
learning from different domains. Machine
Learning, 79:151?175.
Ben-David, Shai, John Blitzer, Koby
Crammer, and Fernando Pereira. 2007.
Analysis of representations for domain
adaptation. In Advances in Neural
Information Processing Systems 20,
pages 127?144, Vancouver.
Bengio, Yoshua. 2008. Neural net language
models. Scholarpedia, 3(1):3,881.
Bengio, Yoshua, Re?jean Ducharme, Pascal
Vincent, and Christian Janvin. 2003.
A neural probabilistic language model.
Journal of Machine Learning Research,
3:1,137?1,155.
Bengio, Yoshua, Jerome Louradour,
Ronan Collobert, and Jason Weston.
2009. Curriculum learning. In Proceedings
of the International Conference on Machine
Learning (ICML), pages 41?48,
Montreal.
Bikel, Daniel M. 2004a. A distributional
analysis of a lexicalized statistical
parsing model. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing, pages 182?189,
Barcelona.
Bikel, Daniel M. 2004b. Intricacies of Collins?
parsing model. Computational Linguistics,
30(4):479?511.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
3:993?1,022.
Blitzer, John. 2008. Domain Adaptation of
Natural Language Processing Systems.
Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
Blitzer, John, Koby Crammer, Alex Kulesza,
Fernando Pereira, and Jenn Wortman.
2007. Learning bounds for domain
adaptation. In Advances in Neural
Information Processing Systems,
pages 129?136, Vancouver.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Association for Computational Linguistics
(ACL), pages 40?47, Prague.
Blitzer, John, Ryan McDonald, and
Fernando Pereira. 2006. Domain
adaptation with structural correspondence
learning. In Proceedings of the EMNLP,
pages 120?128, Sydney.
Bodlaender, Hans L. 1988. Dynamic
programming on graphs with bounded
treewidth. In Proceedings of the 15th
International Colloquium on Automata,
Languages and Programming,
pages 105?118, Tampere.
Brants, Thorsten and Alex Franz. 2006.
Web 1t 5-gram version 1. www.ldc.
upenn.edu/catalog/.
Brown, Peter F., Vincent J. Della Pietra,
Peter V. deSouza, Jenifer C. Lai, and
Robert L. Mercer. 1992. Class-based
n-gram models of natural language.
Computational Linguistics, 18:467?479.
Candito, Marie and Benoit Crabbe. 2009.
Improving generative statistical parsing
with semi-supervised word clustering.
In Proceedings of the IWPT, pages 138?141,
Paris.
116
Huang et al. Computational Linguistics
Chan, Yee Seng and Hwee Tou Ng. 2006.
Estimating class priors in domain
adaptation for word sense disambiguation.
In Proceedings of the Association for
Computational Linguistics (ACL),
pages 89?96, Sydney.
Chelba, Ciprian and Alex Acero. 2004.
Adaptation of maximum entropy
classifier: Little data can help a lot.
In Proceedings of the EMNLP,
pages 285?292, Barcelona.
Collobert, Robert and Jason Weston. 2008. A
unified architecture for natural language
processing: Deep neural networks with
multitask learning. In Proceedings of the
International Conference on Machine Learning
(ICML), pages 160?167, Helsinki.
Dai, Wenyuan, Gui-Rong Xue, Qiang Yang,
and Yong Yu. 2007. Transferring naive
Bayes classifiers for text classification.
In Proceedings of the National Conference
on Artificial Intelligence (AAAI),
pages 540?545, Vancouver.
Darroch, J. N., S. L. Lauritzen, and
T. P. Speed. 1980. Markov fields and
log-linear interaction models for
contingency tables. The Annals of
Statistics, 8(3):522?539.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
ACL, pages 256?263, Prague.
Daume? III, Hal, Abhishek Kumar, and
Avishek Saha. 2010. Frustratingly easy
semi-supervised domain adaptation.
In Proceedings of the ACL Workshop
on Domain Adaptation (DANLP),
pages 53?59, Uppsala.
Daume? III, Hal and Daniel Marcu. 2006.
Domain adaptation for statistical
classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
Deerwester, Scott, Susan T. Dumais,
George W. Furnas, Thomas K. Landauer,
and Richard Harshman. 1990. Indexing by
latent semantic analysis. Journal of the
American Society of Information Science,
41(6):391?407.
Dempster, Arthur, Nan Laird, and Donald
Rubin. 1977. Likelihood from incomplete
data via the EM algorithm. Journal of
the Royal Statistical Society, Series B,
39(1):1?38.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the latent words language
model. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 21?29,
Singapore.
Dhillon, Paramveer S., Dean Foster, and
Lyle Ungar. 2011. Multi-View Learning of
Word Embeddings via CCA. In Proceedings
of the Advances in Neural Information
Processing Systems (NIPS), volume 24,
pages 886?874, Granada.
Downey, Doug, Matthew Broadhead, and
Oren Etzioni. 2007. Locating complex
named entities in web text. In Proceedings
of the 20th International Joint Conference
on Artificial Intelligence (IJCAI 2007),
pages 2,733?2,739, Hyderabad.
Downey, Doug, Stefan Schoenmackers, and
Oren Etzioni. 2007. Sparse information
extraction: Unsupervised language models
to the rescue. In Proceedings of the ACL,
pages 696?703, Prague.
Dredze, Mark and Koby Crammer. 2008.
Online methods for multi-domain learning
and adaptation. In Proceedings of EMNLP,
pages 689?697, Honolulu, HI.
Dredze, Mark, Alex Kulesza, and Koby
Crammer. 2010. Multi-domain learning
by confidence weighted parameter
combination. Machine Learning,
79:123?149.
Finkel, Jenny Rose and Christopher D.
Manning. 2009. Hierarchical Bayesian
domain adaptation. In Proceedings of
HLT-NAACL, pages 602?610, Boulder, CO.
Fu?rstenau, Hagen and Mirella Lapata. 2009.
Semi-supervised semantic role labeling.
In Proceedings of the 12th Conference of the
European Chapter of the ACL, pages 220?228,
Athens.
Ghahramani, Zoubin and Michael I. Jordan.
1997. Factorial hidden Markov models.
Machine Learning, 29(2-3):245?273.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Conference on
Empirical Methods in Natural Language
Processing, pages 167?202, Pittsburgh, PA.
Goldwater, Sharon and Thomas L. Griffiths.
2007. A fully Bayesian approach to
unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages 744?751,
Prague.
Grac?a, Joa?o V., Kuzman Ganchev, Ben Taskar,
and Fernando Pereira. 2009. Posterior vs.
parameter sparsity in latent variable
models. In Proceedings of the Neural
Information Processing Systems Conference
(NIPS), pages 664?672, Vancouver.
Harris, Z. 1954. Distributional structure.
Word, 10(23):146?162.
Hindle, Donald. 1990. Noun classification
from predicage-argument structures.
In Proceedings of the ACL, pages 268?275,
Pittsburgh, PA.
117
Computational Linguistics Volume 40, Number 1
Honkela, Timo. 1997. Self-organizing
maps of words for natural language
processing applications. In Proceedings
of the International ICSC Symposium on
Soft Computing, pages 401?407, Millet,
Alberta.
Huang, Fei and Alexander Yates. 2009.
Distributional representations for
handling sparsity in supervised sequence
labeling. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 495?503,
Singapore.
Huang, Fei and Alexander Yates. 2010.
Exploring representation-learning
approaches to domain adaptation. In
Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language
Processing (DANLP), pages 23?30, Uppsala.
Huang, Fei, Alexander Yates, Arun Ahuja,
and Doug Downey. 2011. Language
models as representations for weakly
supervised NLP tasks. In Proceedings
of the Conference on Natural Language
Learning (CoNLL), pages 125?134,
Portland, OR.
Jiang, Jing and ChengXiang Zhai. 2007a.
Instance weighting for domain
adaptation in NLP. In Proceedings
of ACL, pages 264?271, Prague.
Jiang, Jing and ChengXiang Zhai. 2007b. A
two-stage approach to domain adaptation
for statistical classifiers. In Proceedings of
the Conference on Information and Knowledge
Management (CIKM), pages 401?410, Lisbon.
Johnson, Mark. 2007. Why doesn?t EM find
good HMM POS-taggers. In Proceedings of
the EMNLP, pages 296?305, Prague.
Kaski, S. 1998. Dimensionality reduction
by random mapping: Fast similarity
computation for clustering. In
Proceedings of the IJCNN, pages 413?418,
Washington, DC.
Koo, Terry, Xavier Carreras, and Michael
Collins. 2008. Simple semi-supervised
dependency parsing. In Proceedings of
the Annual Meeting of the Association of
Computational Linguistics (ACL),
pages 595?603, Columbus, OH.
Lin, Dekang and Xiaoyun Wu. 2009. Phrase
clustering for discriminative learning.
In Proceedings of the ACL-IJCNLP,
pages 1,030?1,038, Singapore.
Liu, Dong C. and Jorge Nocedal. 1989. On
the limited memory method for large scale
optimization. Mathematical Programming B,
45(3):503?528.
Mansour, Y., M. Mohri, and
A. Rostamizadeh. 2009. Domain
adaptation with multiple sources.
In Proceedings of the Advances in Neural
Information Processing Systems,
pages 1,041?1,048, Vancouver.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Martin, Sven, Jorg Liermann, and Hermann
Ney. 1998. Algorithms for bigram and
trigram word clustering. Speech
Communication, 24:19?37.
McClosky, David. 2010. Any Domain Parsing:
Automatic Domain Adaptation for Parsing.
Ph.D. thesis, Brown University,
Providence, RI.
McClosky, David, Eugene Charniak, and
Mark Johnson. 2010. Automatic domain
adaptation for parsing. In North American
Chapter of the Association for Computational
Linguistics - Human Language Technologies
2010 Conference (NAACL-HLT 2010),
pages 28?36, Los Angeles, CA.
Miller, Scott, Jethran Guinness, and
Alex Zamanian. 2004. Name tagging with
word clusters and discriminative training.
In Proceedings of the Annual Meeting of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL),
pages 337?342, Boston, MA.
Mnih, Andriy and Geoffrey Hinton. 2007.
Three new graphical models for statistical
language modelling. In Proceedings of
the 24th International Conference on
Machine Learning, pages 641?648,
Corvallis, OR.
Mnih, Andriy and Geoffrey Hinton. 2009.
A scalable hierarchical distributed
language model. In Proceedings of the
Neural Information Processing Systems
(NIPS), pages 1,081?1,088, Vancouver.
Mnih, Andriy, Zhang Yuecheng, and
Geoffrey Hinton. 2009. Improving a
statistical language model through
non-linear prediction. Neurocomputing,
72(7-9):1414?1418.
Morin, Frederic and Yoshua Bengio. 2005.
Hierarchical probabilistic neural network
language model. In Proceedings of the
International Workshop on Artificial
Intelligence and Statistics, pages 246?252,
Barbados.
Pantel, Patrick, Eric Crestan, Arkady
Borkovsky, Ana-Maria Popescu,
and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set
expansion. In Proceedings of the EMNLP,
pages 938?947, Singapore.
118
Huang et al. Computational Linguistics
PennBioIE. 2005. Mining the bibliome
project. http://bioie.ldc.upenn.edu/.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 183?190, Columbus, OH.
Pradhan, Sameer, Wayne Ward, and James H.
Martin. 2007. Towards robust semantic role
labeling. In Proceedings of NAACL-HLT,
pages 556?563, Rochester, NY.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition.
Proceedings of the IEEE, 77(2):257?285.
Raina, Rajat, Alexis Battle, Honglak Lee,
Benjamin Packer, and Andrew Y. Ng.
2007. Self-taught learning: Transfer
learning from unlabeled data.
In Proceedings of the 24th International
Conference on Machine Learning,
pages 759?766, Corvallis, OR.
Ratinov, Lev and Dan Roth. 2009. Design
challenges and misconceptions in named
entity recognition. In Proceedings of the
Conference on Natural Language Learning
(CoNLL), pages 147?155, Boulder, CO.
Ritter, H. and T. Kohonen. 1989.
Self-organizing semantic maps.
Biological Cybernetics, 61(4):241?254.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Synactic Theory: A Formal
Introduction. CSLI Publications, Stanford,
CA, second edition.
Sahlgren, Magnus. 2001. Vector-based
semantic analysis: Representing word
meanings based on random labels.
In Proceedings of the Semantic Knowledge
Acquisition and Categorization Workshop,
pages 1?12, Helsinki.
Sahlgren, Magnus. 2005. An introduction
to random indexing. In Methods and
Applications of Semantic Indexing Workshop
at the 7th International Conference on
Terminology and Knowledge Engineering
(TKE), 87:1?9.
Sahlgren, Magnus. 2006. The word-space
model: Using distributional analysis to
represent syntagmatic and paradigmatic
relations between words in high-dimensional
vector spaces. Ph.D. thesis, Stockholm
University.
Salton, Gerard and Michael J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill.
Satpal, Sandeep and Sunita Sarawagi.
2007. Domain adaptation of conditional
probability models via feature subsetting.
In Proceedings of ECML/PKDD,
pages 224?235, Warsaw.
Sekine, Satoshi. 1997. The domain
dependence of parsing. In Proceedings of
Applied Natural Language Processing
(ANLP), pages 96?102, Washington, DC.
Shen, Libin, Giorgio Satta, and Aravind K.
Joshi. 2007. Guided learning for
bidirectional sequence classification.
In Proceedings of the ACL, pages 760?767,
Prague.
Smith, Noah A. and Jason Eisner. 2005.
Contrastive estimation: Training
log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 354?362,
Ann Arbor, MI.
Sutton, Charles, Andrew McCallum, and
Khashayar Rohanimanesh. 2007. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. Journal of
Machine Learning Research, 8:693?723.
Suzuki, Jun and Hideki Isozaki. 2008.
Semi-supervised sequential labeling and
segmentation using giga-word scale
unlabeled data. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL-HLT),
pages 665?673, Columbus, OH.
Suzuki, Jun, Hideki Isozaki, Xavier Carreras,
and Michael Collins. 2009. An empirical
study of semi-supervised structured
conditional models for dependency
parsing. In Proceedings of the EMNLP,
pages 551?560, Singapore.
Tao, Hongyin and Richard Xiao. 2007.
The UCLA Chinese corpus. UCREL.
www.lancaster.ac.uk/fass/projects/
corpus/UCLA/.
Tjong, Erik F., Kim Sang, and Sabine
Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking.
In Proceedings of the 4th Conference
on Computational Natural Language
Learning, pages 127?132, Lisbon.
Toutanova, Kristina and Mark Johnson.
2007. A Bayesian LDA-based model for
semi-supervised part-of-speech
tagging. In Proceedings of the NIPS,
pages 1,521?1,528, Vancouver.
Tseng, Huihsin, Daniel Jurafsky, and
Christopher Manning. 2005.
Morphological features help POS
tagging of unknown words across
language varieties. In Proceedings
of the Fourth SIGHAN Workshop,
pages 32?39, Jeju Island.
119
Computational Linguistics Volume 40, Number 1
Turian, Joseph, James Bergstra, and
Yoshua Bengio. 2009. Quadratic
features and deep architectures for
chunking. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics - Human
Language Technologies (NAACL HLT),
pages 245?248, Boulder, CO.
Turian, Joseph, Lev Ratinov, and Yoshua
Bengio. 2010. Word representations:
A simple and general method for
semi-supervised learning. In Proceedings
of the Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 384?394, Uppsala.
Turney, Peter D. and Patrick Pantel. 2010.
From frequency to meaning: Vector
space models of semantics. Journal of
Artificial Intelligence Research, 37:141?188.
Ushioda, Akira. 1996. Hierarchical clustering
of words. In Proceedings of the International
Conference on Computational Linguistics
(COLING), pages 1,159?1,162, Copenhagen.
Va?yrynen, Jaakko and Timo Honkela. 2004.
Word category maps based on emergent
features created by ICA. In Proceedings of
the STePs 2004 Cognition + Cybernetics
Symposium, pages 173?185, Tikkurila.
Va?yrynen, Jaakko and Timo Honkela. 2005.
Comparison of independent component
analysis and singular value decomposition
in word context analysis. In Proceedings
of the International and Interdisciplinary
Conference on Adaptive Knowledge
Representation and Reasoning (AKRR),
pages 135?140, Espoo.
Va?yrynen, Jaakko, Timo Honkela, and
Lasse Lindqvist. 2007. Towards explicit
semantic features using independent
component analysis. In Proceedings of the
Workshop Semantic Content Acquisition
and Representation (SCAR), pages 20?27,
Stockholm.
Weston, Jason, Frederic Ratle, and
Ronan Collobert. 2008. Deep learning
via semi-supervised embedding.
In Proceedings of the 25th International
Conference on Machine Learning,
pages 1,168?1,175, Helsinki.
Yang, Yi, Alexander Yates, and Doug
Downey. 2013. Overcoming the memory
bottleneck in distributed training
of latent variable models of text.
In Proceedings of the NAACL-HLT,
pages 579?584, Atlanta, GA.
Zhao, Hai, Wenliang Chen, Chunyu Kit,
and Guodong Zhou. 2009. Multilingual
dependency learning: A huge feature
engineering method to semantic
dependency parsing. In Proceedings of the
CoNLL 2009 Shared Task, pages 55?60,
Boulder, CO.
120
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 225?228,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improved Extraction Assessment through Better Language Models
Arun Ahuja, Doug Downey
EECS Dept., Northwestern University
Evanston, IL 60208
{arun.ahuja, ddowney}@eecs.northwestern.edu
Abstract
A variety of information extraction techniques
rely on the fact that instances of the same
relation are ?distributionally similar,? in that
they tend to appear in similar textual con-
texts. We demonstrate that extraction accu-
racy depends heavily on the accuracy of the
language model utilized to estimate distribu-
tional similarity. An unsupervised model se-
lection technique based on this observation is
shown to reduce extraction and type-checking
error by 26% over previous results, in experi-
ments with Hidden Markov Models. The re-
sults suggest that optimizing statistical lan-
guage models over unlabeled data is a promis-
ing direction for improving weakly supervised
and unsupervised information extraction.
1 Introduction
Many weakly supervised and unsupervised informa-
tion extraction techniques assess the correctness of
extractions using the distributional hypothesis?the
notion that words with similar meanings tend to oc-
cur in similar contexts (Harris, 1985). A candidate
extraction of a relation is deemed more likely to be
correct when it appears in contexts similar to those
of ?seed? instances of the relation, where the seeds
may be specified by hand (Pas?ca et al, 2006), taken
from an existing, incomplete knowledge base (Snow
et al, 2006; Pantel et al, 2009), or obtained in an un-
supervised manner using a generic extractor (Banko
et al, 2007). We refer to this technique as Assess-
ment by Distributional Similarity (ADS).
Typically, distributional similarity is computed by
comparing co-occurrence counts of extractions and
seeds with various contexts found in the corpus. Sta-
tistical Language Models (SLMs) include methods
for more accurately estimating co-occurrence proba-
bilities via back-off, smoothing, and clustering tech-
niques (e.g. (Chen and Goodman, 1996; Rabiner,
1989; Bell et al, 1990)). Because SLMs can be
trained from only unlabeled text, they can be applied
for ADS even when the relations of interest are not
specified in advance (Downey et al, 2007). Unla-
beled text is abundant in large corpora like the Web,
making nearly-ceaseless automated optimization of
SLMs possible. But how fruitful is such an effort
likely to be?to what extent does optimizing a lan-
guage model over a fixed corpus lead to improve-
ments in assessment accuracy?
In this paper, we show that an ADS technique
based on SLMs is improved substantially when
the language model it employs becomes more ac-
curate. In a large-scale set of experiments, we
quantify how language model perplexity correlates
with ADS performance over multiple data sets and
SLM techniques. The experiments show that accu-
racy over unlabeled data can be used for selecting
among SLMs?for an ADS approach utilizing Hid-
den Markov Models, this results in an average error
reduction of 26% over previous results in extraction
and type-checking tasks.
2 Extraction Assessment with Language
Models
We begin by formally defining the extraction and
typechecking tasks we consider, then discuss statis-
tical language models and their utilization for ex-
traction assessment.
225
The extraction task we consider is formalized as
follows: given a corpus, a target relation R, a list
of seed instances SR, and a list of candidate extrac-
tions UR, the task is to order elements of UR such
that correct instances for R are ranked above extrac-
tion errors. Let URi denote the set of the ith argu-
ments of the extractions in UR, and let SRi be de-
fined similarly for the seed set SR. For relations of
arity greater than one, we consider the typechecking
task, an important sub-task of extraction (Downey et
al., 2007). The typechecking task is to rank extrac-
tions with arguments that are of the proper type for a
relation above type errors. As an example, the ex-
traction Founded(Bill Gates, Oracle) is
type correct, but is not correct for the extraction task.
2.1 Statistical Language Models
A Statistical Language Model (SLM) is a probabil-
ity distribution P (w) over word sequences w =
(w1, ..., wr). The most common SLM techniques
are n-gram models, which are Markov models in
which the probability of a given word is dependent
on only the previous n?1 words. The accuracy of an
n-gram model of a corpus depends on two key fac-
tors: the choice of n, and the smoothing technique
employed to assign probabilities to word sequences
seen infrequently in training. We experiment with
choices of n from 2 to 4, and two popular smoothing
approaches, Modified Kneser-Ney (Chen and Good-
man, 1996) and Witten-Bell (Bell et al, 1990).
Unsupervised Hidden Markov Models (HMMs)
are an alternative SLM approach previously shown
to offer accuracy and scalability advantages over n-
gram models in ADS (Downey et al, 2007). An
HMM models a sentence w as a sequence of obser-
vations wi each generated by a hidden state variable
ti. Here, hidden states take values from {1, . . . , T},
and each hidden state variable is itself generated by
some number k of previous hidden states. Formally,
the joint distribution of a word sequence w given a
corresponding state sequence t is:
P (w|t) =
?
i
P (wi|ti)P (ti|ti?1, . . . , ti?k) (1)
The distributions on the right side of Equation 1 are
learned from the corpus in an unsupervised manner
using Expectation-Maximization, such that words
distributed similarly in the corpus tend to be gen-
erated by similar hidden states (Rabiner, 1989).
2.2 Performing ADS with SLMs
The Assessment by Distributional Similarity (ADS)
technique is to rank extractions in UR in decreas-
ing order of distributional similarity to the seeds,
as estimated from the corpus. In our experiments,
we utilize an ADS approach previously proposed for
HMMs (Downey et al, 2007) and adapt it to also ap-
ply to n-gram models, as detailed below.
Define a context of an extraction argument ei to
be a string containing the m words preceding and m
words following an occurrence of ei in the corpus.
Let Ci = {c1, c2, ..., c|Ci|} be the union of all con-
texts of extraction arguments ei and seed arguments
si for a given relation R. We create a probabilis-
tic context vector for each extraction ei where the
j-th dimension of the vector is the probability of the
context surrounding given the extraction, P (cj |ei),
computed from the language model. 1
We rank the extractions in UR according to how
similar their arguments? contextual distributions,
P (c|ei), are to those of the seed arguments. Specifi-
cally, extractions are ranked according to:
f(e) =
?
ei?e
KL(
?
w??SRi
P (c|w?)
|SRi|
, P (c|ei)) (2)
where KL represents KL Divergence, and the outer
sum is taken over arguments ei of the extraction e.
For HMMs, we alternatively rank extractions us-
ing the HMM state distributions P (t|ei) in place of
the probabilistic context vectors P (c|ei). Our exper-
iments show that state distributions are much more
accurate for ADS than are HMM context vectors.
3 Experiments
In this section, we present experiments showing that
SLM accuracy correlates strongly with ADS perfor-
mance. We also show that SLM performance can be
used for model selection, leading to an ADS tech-
nique that outperforms previous results.
3.1 Experimental Methodology
We experiment with a wide range of n-gram and
HMM models. The n-gram models are trained us-
ing the SRILM toolkit (Stolcke, 2002). Evaluating a
1For example, for context cj = ?I visited in July? and ex-
traction ei = ?Boston,? P (cj |ei) is P(?I visited Boston in July?)
/ P(?Boston?), where each string probability is computed using
the language model.
226
LM Unary Binary Wikipedia
HMM 1-5 -.911 -.361 -.994
HMM 2-5 -.856 .120 -.930
HMM 3-5 -.823 -.683 .922
HMM 1-10 -.916 -.967 -.905
HMM 2-10 -.877 -.797 -.963
HMM 3-10 -.957 -.669 -.924
HMM 1-25 -.933 -.850 -.959
HMM 1-50 -.942 -.942 -.947
HMM 1-100 -.896 -.877 -.942
N-Gram -.512 -.999 .024
Table 1: Pearson Correlation value for extraction perfor-
mance (in AUC) and SLM performance (in perplexity).
Extraction accuracy increases as perplexity decreases,
with an average correlation coefficient of -0.742. ?HMM
k-T ? denotes an HMM model of order k, with T states.
variety of HMM configurations over a large corpus
requires a scalable training architecture. We con-
structed a parallel HMM codebase using the Mes-
sage Passing Interface (MPI), and trained the models
on a supercomputing cluster. All language models
were trained on a corpus of 2.8M sentences of Web
text (about 60 million tokens). SLM performance is
measured using the standard perplexity metric, and
assessment accuracy is measured using area under
the precision-recall curve (AUC), a standard metric
for ranked lists of extractions. We evaluated perfor-
mance on three distinct data sets. The first two data
sets evaluate ADS for unsupervised information ex-
traction, and were taken from (Downey et al, 2007).
The first, Unary, was an extraction task for unary
relations (Company, Country, Language, Film) and
the second, Binary, was a type-checking task for
binary relations (Conquered, Founded, Headquar-
tered, Merged). The 10 most frequent extractions
served as bootstrapped seeds. The two test sets con-
tained 361 and 265 extractions, respectively. The
third data set, Wikipedia, evaluates ADS on weakly-
supervised extraction, using seeds and extractions
taken from Wikipedia ?List of? pages (Pantel et al,
2009). Seed sets of various sizes (5, 10, 15 and
20) were randomly selected from each list, and we
present results averaged over 10 random samplings.
Other members of the seed list were added to a test
set as correct extractions, and elements from other
lists were added as errors. The data set included
Figure 1: HMM 1-100 Performance. Information Extrac-
tion performance (in AUC) increases as SLM accuracy
improves (perplexity decreases).
2264 extractions across 36 unary relations, includ-
ing Composers and US Internet Companies.
3.2 Optimizing Language Models for IE
The first question we investigate is whether opti-
mizing individual language models leads to bet-
ter performance in ADS. We measured the correla-
tion between SLM perplexity and ADS performance
as training proceeds in HMMs, and as n and the
smoothing technique vary in the n-gram models. Ta-
ble 1 shows that as the SLM becomes more accurate
(i.e. as perplexity decreases), ADS performance in-
creases. The correlation is strong (averaging -0.742)
and is consistent across model configurations and
data sets. The low positive correlation for the n-
gram models on Wikipedia is likely due to a ?floor
effect?; the models have low performance overall
on the difficult Wikipedia data set. The lowest-
perplexity n-gram model (Mod Kneser-Ney smooth-
ing with n=3, KN3) does exhibit the best IE per-
formance, at 0.039 (the average performance of the
HMM models is more than twice this, at 0.084). Fig-
ure 1 shows the relationship between SLM and ADS
performance in detail for the best-performing HMM
configuration.
3.3 Model Selection
Different language models can be configured in dif-
ferent ways: for example, HMMs require choices for
the hyperparameters k and T . Here, we show that
227
Figure 2: Model Selection for HMMs. SLM perfor-
mance is a good predictor of extraction performance
across model configurations.
SLM perplexity can be used to select a high-quality
model configuration for ADS using only unlabeled
data. We evaluate on the Unary and Binary data sets,
since they have been employed in previous work
on our corpora. Figure 2 shows that for HMMs,
ADS performance increases as perplexity decreases
across various model configurations (a similar rela-
tionship holds for n-gram models). A model selec-
tion technique that picks the HMM model with low-
est perplexity (HMM 1-100) results in better ADS
performance than previous results. As shown in Ta-
ble 2, HMM 1-100 reduces error over the HMM-T
model in (Downey et al, 2007) by 26%, on average.
The experiments also reveal an important difference
between the HMM and n-gram approaches. While
KN3 is more accurate in SLM than our HMM mod-
els, it performs worse in ADS on average. For exam-
ple, HMM 1-25 underperforms KN3 in perpexity, at
537.2 versus 227.1, but wins in ADS, 0.880 to 0.853.
We hypothesize that this is because the latent state
distributions in the HMMs provide a more informa-
tive distributional similarity measure. Indeed, when
we compute distributional similarity for HMMs us-
ing probabilistic context vectors as opposed to state
distributions, ADS performance for HMM 1-25 de-
creases to 5.8% below that of KN3.
4 Conclusions
We presented experiments showing that estimating
distributional similarity with more accurate statisti-
cal language models results in more accurate extrac-
Relation HMM-T Best HMM
Company .966 .985
Country .886 .942
Languages .936 .914
Film .803 .801
Unary Avg .898 .911
Conquered .917 .923
Founded .827 .799
Merged .920 .925
Headquartered .734 .964
Binary Average .849 .903
Table 2: Extraction Performance Results in AUC for In-
dividual Relations. The lowest-perplexity HMM, 1-100,
outperforms the HMM-T model from previous work.
tion assessment. We note that significantly larger,
more powerful language models are possible beyond
those evaluated here, which (based on the trajectory
observed in Figure 2) may offer significant improve-
ments in assessment accuracy.
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
T. C. Bell, J. G. Cleary, and I. H. Witten. 1990. Text
Compression. Prentice Hall, January.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proc. of ACL.
D. Downey, S. Schoenmackers, and O. Etzioni. 2007.
Sparse information extraction: Unsupervised language
models to the rescue. In Proc. of ACL.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Procs. of ACL/COLING 2006.
P. Pantel, E. Crestan, A. Borkovsky, A. M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proc. of EMNLP.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL 2006.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 2.
228
Proceedings of NAACL-HLT 2013, pages 579?584,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Overcoming the Memory Bottleneck in Distributed Training of
Latent Variable Models of Text
Yi Yang
Northwestern University
Evanston, IL
yiyang@eecs.northwestern.edu
Alexander Yates
Temple University
Philadelphia, PA
yates@temple.edu
Doug Downey
Northwestern University
Evanston, IL
ddowney@eecs.northwestern.edu
Abstract
Large unsupervised latent variable models
(LVMs) of text, such as Latent Dirichlet Al-
location models or Hidden Markov Models
(HMMs), are constructed using parallel train-
ing algorithms on computational clusters. The
memory required to hold LVM parameters
forms a bottleneck in training more powerful
models. In this paper, we show how the mem-
ory required for parallel LVM training can
be reduced by partitioning the training corpus
to minimize the number of unique words on
any computational node. We present a greedy
document partitioning technique for the task.
For large corpora, our approach reduces mem-
ory consumption by over 50%, and trains the
same models up to three times faster, when
compared with existing approaches for paral-
lel LVM training.
1 Introduction
Unsupervised latent variable models (LVMs) of text
are utilized extensively in natural language process-
ing (Griffiths and Steyvers, 2004; Ritter et al, 2010;
Downey et al, 2007; Huang and Yates, 2009; Li and
McCallum, 2005). LVM techniques include Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), Hid-
den Markov Models (HMMs) (Rabiner, 1989), and
Probabilistic Latent Semantic Analysis (Hofmann,
1999), among others.
LVMs become more predictive as they are trained
on more text. However, training LVMs on mas-
sive corpora introduces computational challenges, in
terms of both time and space complexity. The time
complexity of LVM training has been addressed
through parallel training algorithms (Wolfe et al,
2008; Chu et al, 2006; Das et al, 2007; Newman
et al, 2009; Ahmed et al, 2012; Asuncion et al,
2011), which reduce LVM training time through the
use of large computational clusters.
However, the memory cost for training LVMs re-
mains a bottleneck. While LVM training makes se-
quential scans of the corpus (which can be stored on
disk), it requires consistent random access to model
parameters. Thus, the model parameters must be
stored in memory on each node. Because LVMs in-
clude a multinomial distribution over words for each
latent variable value, the model parameter space in-
creases with the number of latent variable values
times the vocabulary size. For large models (i.e.,
with many latent variable values) and large cor-
pora (with large vocabularies), the memory required
for training can exceed the limits of the commod-
ity servers comprising modern computational clus-
ters. Because model accuracy tends to increase with
both corpus size and model size (Ahuja and Downey,
2010; Huang and Yates, 2010), training accurate lan-
guage models requires that we overcome the mem-
ory bottleneck.
We present a simple technique for mitigating the
memory bottleneck in parallel LVM training. Ex-
isting parallelization schemes begin by partitioning
the training corpus arbitrarily across computational
nodes. In this paper, we show how to reduce mem-
ory footprint by instead partitioning the corpus to
minimize the number of unique words on each node
(and thereby minimize the number of parameters the
node must store). Because corpus partitioning is
a pre-processing step in parallel LVM training, our
579
technique can be applied to reduce the memory foot-
print of essentially any existing LVM or training ap-
proach. The accuracy of LVM training for a fixed
model size and corpus remains unchanged, but in-
telligent corpus partitioning allows us to train larger
and typically more accurate models using the same
memory capacity.
While the general minimization problem we en-
counter is NP-hard, we develop greedy approxima-
tions that work well. In experiments with both
HMM and LDA models, we show that our technique
offers large advantages over existing approaches in
terms of both memory footprint and execution time.
On a large corpus using 50 nodes in parallel, our best
partitioning method can reduce the memory required
per node to less than 1/10th that when training with-
out corpus partitioning, and to half that of a random
partitioning. Further, our approach reduces the train-
ing time of an existing parallel HMM codebase by
3x. Our work includes the release of our partitioning
codebase, and an associated codebase for the paral-
lel training of HMMs.1
2 Problem Formulation
In a distributed LVM system, a training corpus D =
{d1, d2, . . . , dN} of documents is distributed across
T computational nodes. We first formalize the mem-
ory footprint on each node nt, where t = {1, ..., T}.
Let Dt ? D denote the document collection on node
nt, and Vt be the number of word types (i.e., the
number of unique words) in Dt. Let K be the num-
ber of latent variable values in the LVM.
With these quantities, we can express how many
parameters must be held in memory on each com-
putational node for training LVMs in a distributed
environment. In practice, the LVM parameter space
is dominated by an observation model: a condi-
tional distribution over words given the latent vari-
able value. Thus, the observation model includes
K(Vt? 1) parameters. Different LVMs include var-
ious other parameters to specify the complete model.
For example, a first-order HMM includes additional
distributions for the initial latent variable and latent
variable transitions, for a total of K(Vt ? 1) + K2
parameters. LDA, on the other hand, includes just a
1https://code.google.com/p/
corpus-partition/
single multinomial over the latent variables, making
a total of K(Vt ? 1) + K ? 1 parameters.
The LVM parameters comprise almost all of the
memory footprint for LVM training. Further, as the
examples above illustrate, the number of parame-
ters on each node tends to vary almost linearly with
Vt (in practice, Vt is typically larger than K by an
order of magnitude or more). Thus, in this paper
we attempt to minimize memory footprint by lim-
iting Vt on each computational node. We assume
the typical case in a distributed environment where
nodes are homogeneous, and thus our goal is to par-
tition the corpus such that the maximum vocabulary
size Vmax = maxTt=1Vt on any single node is mini-
mized. We define this task formally as follows.
Definition CORPUSPART : Given a corpus of
N documents D = {d1, d2, . . . , dN}, and T nodes,
partition D into T subsets D1, D2, . . . , DT , such
that Vmax is minimized.
For illustration, consider the following small ex-
ample. Let corpus C contain three short docu-
ments {c1=?I live in Chicago?, c2=?I am studying
physics?, c3=?Chicago is a city in Illinois?}, and
consider partitioning C into 2 non-empty subsets,
i.e., T = 2. There are a total of three possibilities:
? {{c1, c2}, {c3}}. Vmax = 7
? {{c1, c3}, {c2}}. Vmax = 8
? {{c2, c3}, {c1}}. Vmax = 10
The decision problem version of
CORPUSPART is NP-Complete, by a re-
duction from independent task scheduling (Zhu and
Ibarra, 1999). In this paper, we develop greedy
algorithms for the task that are effective in practice.
We note that CORPUSPART has a submodu-
lar problem structure, where greedy algorithms are
often effective. Specifically, let |S| denote the vo-
cabulary size of a set of documents S, and let S? ?
S. Then for any document c the following inequality
holds.
|S? ? c| ? |S?| ? |S ? c| ? |S|
That is, adding a document c to the subset S? in-
creases vocabulary size at least as much as adding
c to S; the vocabulary size function is submodular.
The CORPUSPART task thus seeks a partition
of the data that minimizes the maximum of a set of
submodular functions. While formal approximation
580
guarantees exist for similar problems, to our knowl-
edge none apply directly in our case. For example,
(Krause et al, 2007) considers maximizing the mini-
mum over a set of monotonic submodular functions,
which is the opposite of our problem. The distinct
task of minimizing a single submodular function has
been investigated in e.g. (Iwata et al, 2001).
It is important to emphasize that data partition-
ing is a pre-processing step, after which we can em-
ploy precisely the same Expectation-Maximization
(EM), sampling, or variational parameter learning
techniques as utilized in previous work. In fact,
for popular learning techniques including EM for
HMMs (Rabiner, 1989) and variational EM for LDA
(Wolfe et al, 2008), it can be shown that the param-
eter updates are independent of how the corpus is
partitioned. Thus, for those approaches our parti-
tioning is guaranteed to produce the same models as
any other partitioning method; i.e., model accuracy
is unchanged.
Lastly, we note that we target synchronized LVM
training, in which all nodes must finish each train-
ing iteration before any node can proceed to the
next iteration. Thus, we desire balanced partitions to
help ensure iterations have similar durations across
nodes. We achieve this in practice by constraining
each node to hold at most 3% more than Z/T to-
kens, where Z is the corpus size in tokens.
3 Corpus Partitioning Methods
Our high-level greedy partitioning framework is
given in Algorithm 1. The algorithm requires an-
swering two key questions: How do we select which
document to allocate next? And, given a document,
on which node should it be placed? We present al-
ternative approaches to each question below.
Algorithm 1 Greedy Partitioning Framework
INPUT: {D, T}
OUTPUT: {D1, . . . , DT }
Objective: Minimize Vmax
Initialize each subset Dt = ? for T nodes
repeat
document selection:Select document d from D
node selection: Select node nt, and add d to Dt
Remove d from D
until all documents are allocated
A baseline partitioning method commonly used
in practice simply distributes documents across
nodes randomly. As our experiments show, this
baseline approach can be improved significantly.
In the following, set operations are interpreted as
applying to the set of unique words in a document.
For example, |d?Dt| indicates the number of unique
word types in node nt after document d is added to
its document collection Dt.
3.1 Document Selection
For document selection, previous work (Zhu and
Ibarra, 1999) proposed a heuristic DISSIMILARITY
method that selects the document d that is least sim-
ilar to any of the node document collections Dt,
where the similarity of d and Dt is calculated as:
Sim(d,DT ) = |d ? Dt|. The intuition behind the
heuristic is that dissimilar documents are more likely
to impact future node selection decisions. Assigning
the dissimilar documents earlier helps ensure that
more greedy node selections are informed by these
impactful assignments.
However, DISSIMILARITY has a prohibitive time
complexity of O(TN2), because we must compare
T nodes to an order of N documents for a total of
N iterations. To scale to large corpora, we propose
a novel BATCH DISSIMILARITY method. In BATCH
DISSIMILARITY, we select the top L most dissim-
ilar documents in each iteration, instead of just the
most dissimilar. Importantly, L is altered dynami-
cally: we begin with L = 1, and then increase L by
one for iteration i+1 iff using a batch size of L+1 in
iteration i would not have altered the algorithm?s ul-
timate selections (that is, if the most dissimilar doc-
ument in iteration i + 1 is in fact the L + 1st most
dissimilar in iteration i). In the ideal case where L
is incremented each iteration, BATCH DISSIMILAR
will have a reduced time complexity of O(TN3/2).
Our experiments revealed two key findings re-
garding document selection. First, BATCH DISSIM-
ILARITY provides a memory reduction within 0.1%
of that of DISSIMILARITY (on small corpora where
running DISSIMILARITY is tractable), but partitions
an estimated 2,600 times faster on our largest eval-
uation corpus. Second, we found that document se-
lection has relatively minor impact on memory foot-
print, providing a roughly 5% incremental benefit
over random document selection. Thus, although
581
we utilize BATCH DISSIMILARITY in the final sys-
tem we evaluate, simple random document selection
may be preferable in some practical settings.
3.2 Node Selection
Given a selected document d, the MINIMUM
method proposed in previous work selects node nt
having the minimum number of word types after al-
location of d to nt (Zhu and Ibarra, 1999). That is,
MINIMUM minimizes |d ?Dt|. Here, we introduce
an alternative node selection method JACCARD that
selects node nt maximizing the Jaccard index, de-
fined here as |d ?Dt|/|d ?Dt|.
Our experiments showed that our JACCARD node
selection method outperforms the MINIMUM selec-
tion method. In fact, for the largest corpora used
in our experiments, JACCARD offered an 12.9%
larger reduction in Vmax than MINIMUM. Our
proposed system, referred to as BJAC, utilizes
our best-performing strategies for document selec-
tion (BATCH DISSIMILARITY) and node selection
(JACCARD).
4 Evaluation of Partitioning Methods
We evaluate our partitioning method against the
baseline and Z&I, the best performing scalable
method from previous work, which uses random
document selection and MINIMUM node selection
(Zhu and Ibarra, 1999). We evaluate on three cor-
pora (Table 1): the Brown corpus of newswire text
(Kucera and Francis, 1967), the Reuters Corpus Vol-
ume1 (RCV1) (Lewis et al, 2004), and a larger Web-
Sent corpus of sentences gathered from the Web
(Downey et al, 2007).
Corpus N V Z
Brown 57339 56058 1161183
RCV1 804414 288062 99702278
Web-Sent 2747282 214588 58666983
Table 1: Characteristics of the three corpora. N = #
of documents, V = # of word types, Z = # of tokens.
We treat each sentence as a document in the Brown
and Web-Sent corpora.
Table 2 shows how the maximum word type size
Vmax varies for each method and corpus, for T = 50
nodes. BJAC significantly decreases Vmax over the
Corpus baseline Z&I BJAC
Brown 6368 5714 4369
RCV1 49344 32136 24923
Web-Sent 72626 45989 34754
Table 2: Maximum word type size Vmax for each
partitioning method, for each corpus. For the larger
corpora, BJAC reduces Vmax by over 50% compared
to the baseline, and by 23% compared to Z&I.
random partitioning baseline typically employed in
practice. Furthermore, the advantage of BJAC over
the baseline is maintained as more computational
nodes are utilized, as illustrated in Figure 1. BJac
reduces Vmax by a larger factor over the baseline as
more computational nodes are employed.
  0
  20,000
  40,000
  60,000
  80,000
  100,000
  120,000
  140,000
10 20 30 40 50 60 70 80 90 100
nu
m
ber
 of 
wo
rd t
ype
s
 number of nodes
Vmax by baselineVmax by BJac
Figure 1: Effects of partitioning as the number of
computational nodes increases (Web-Sent corpus).
With 100 nodes, BJac?s Vmax is half that of the base-
line, and 1/10th of the full corpus vocabulary size.
5 Evaluation in Parallel LVM Systems
We now turn to an evaluation of our corpus parti-
tioning within parallel LVM training systems.
Table 3 shows the memory footprint required for
HMM and LDA training for three different partition-
ing methods. We compare BJAC with the random
partitioning baseline, Zhu?s method, and with all-
words, the straightforward approach of simply stor-
ing parameters for the entire corpus vocabulary on
every node (Ahuja and Downey, 2010; Asuncion et
al., 2011). All-words has the same memory footprint
as when training on a single node.
For large corpora, BJAC reduces memory size
per node by approximately a factor of two over the
random baseline, and by a factor of 8-11 over all-
582
LVM Corpus all-words baseline BJAC
HMM
Brown 435.3 56.2 40.9
RCV1 2205.4 384.1 197.8
Web-Sent 1644.8 561.7 269.7
LDA
Brown 427.7 48.6 33.3
RCV1 2197.7 376.5 190.1
Web-Sent 1637.2 554.1 262.1
Table 3: Memory footprint of computational nodes
in megabytes(MB), using 50 computational nodes.
Both models utilize 1000 latent variable values.
words. The results demonstrate that in addition to
the well-known savings in computation time offered
by parallel LVM training, distributed computation
also significantly reduces the memory footprint on
each node. In fact, for the RCV1 corpus, BJAC re-
duces memory footprint to less than 1/10th that of
training with all words on each computational node.
We next evaluate the execution time for an itera-
tion of model training. Here, we use a parallel im-
plementation of HMMs, and measure iteration time
for training on the Web-sent corpus with 50 hidden
states as the number of computational nodes varies.
We compare against the random baseline and against
the all-words approach utilized in an existing paral-
lel HMM codebase (Ahuja and Downey, 2010). The
results are shown in Table 4. Moving beyond the all-
words method to exploit corpus partitioning reduces
training iteration time, by a factor of two to three.
However, differences in partitioning methods have
only small effects in iteration time: BJAC has essen-
tially the same iteration time as the random baseline
in this experiment.
It is also important to consider the additional time
required to execute the partitioning methods them-
selves. However, in practice this additional time
is negligible. For example, BJAC can partition the
Web-sent corpus in 368 seconds, using a single com-
putational node. By contrast, training a 200-state
HMM on the same corpus requires over a hundred
CPU-days. Thus, BJAC?s time to partition has a neg-
ligible impact on total training time.
6 Related Work
The CORPUSPART task has some similarities
to the graph partitioning task investigated in other
T all-words baseline BJAC
25 4510 1295 1289
50 2248 740 735
100 1104 365 364
200 394 196 192
Table 4: Average iteration time(sec) for training an
HMM with 50 hidden states on Web-Sent. Partition-
ing with BJAC outperforms all-words, which stores
parameters for all word types on each node.
parallelization research (Hendrickson and Kolda,
2000). However, our LVM training task differs sig-
nificantly from those in which graph partitioning is
typically employed. Specifically, graph partitioning
tends to be used for scientific computing applica-
tions where communication is the bottleneck. The
graph algorithms focus on creating balanced parti-
tions that minimize the cut edge weight, because
edge weights represent communication costs to be
minimized. By contrast, in our LVM training task,
memory consumption is the bottleneck and commu-
nication costs are less significant.
Zhu & Ibarra (1999) present theoretical results
and propose techniques for the general partitioning
task we address. In contrast to that work, we fo-
cus on the case where the data to be partitioned is a
large corpus of text. In this setting, we show that our
heuristics partition faster and provide smaller mem-
ory footprint than those of (Zhu and Ibarra, 1999).
7 Conclusion
We presented a general corpus partitioning tech-
nique which can be exploited in LVM training to re-
duce memory footprint and training time. We eval-
uated the partitioning method?s performance, and
showed that for large corpora, our approach reduces
memory consumption by over 50% and learns mod-
els up to three times faster when compared with ex-
isting implementations for parallel LVM training.
Acknowledgments
This work was supported in part by NSF Grants
IIS-101675 and IIS-1065397, and DARPA contract
D11AP00268.
583
References
Amr Ahmed, Moahmed Aly, Joseph Gonzalez, Shra-
van Narayanamurthy, and Alexander J. Smola. 2012.
Scalable inference in latent variable models. In Pro-
ceedings of the fifth ACM international conference on
Web search and data mining, WSDM ?12, pages 123?
132, New York, NY, USA. ACM.
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Human Language Technologies: Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL HLT).
Arthur U. Asuncion, Padhraic Smyth, and Max Welling.
2011. Asynchronous distributed estimation of topic
models for document analysis. Statistical Methodol-
ogy, 8(1):3 ? 17. Advances in Data Mining and Statis-
tical Learning.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Olukotun.
2006. Map-Reduce for machine learning on multicore.
In Bernhard Scho?lkopf, John C. Platt, and Thomas
Hoffman, editors, NIPS, pages 281?288. MIT Press.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personaliza-
tion: scalable online collaborative filtering. In Pro-
ceedings of the 16th international conference on World
Wide Web, WWW ?07, pages 271?280, New York, NY,
USA. ACM.
D. Downey, S. Schoenmackers, and O. Etzioni. 2007.
Sparse information extraction: Unsupervised language
models to the rescue. In Proc. of ACL.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235, April.
Bruce Hendrickson and Tamara G Kolda. 2000. Graph
partitioning models for parallel computing. Parallel
computing, 26(12):1519?1534.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?99, pages
50?57, New York, NY, USA. ACM.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Satoru Iwata, Lisa Fleischer, and Satoru Fujishige. 2001.
A combinatorial strongly polynomial algorithm for
minimizing submodular functions. J. ACM, 48:761?
777.
Andreas Krause, H. Brendan Mcmahan, Google Inc, Car-
los Guestrin, and Anupam Gupta. 2007. Selecting
observations against adversarial objectives. Technical
report, In NIPS, 2007a.
H. Kucera and W. N. Francis. 1967. Computational
analysis of present-day American English. Brown
University Press, Providence, RI.
David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li,
G. Dietterich, and Fan Li. 2004. Rcv1: A new bench-
mark collection for text categorization research. Jour-
nal of Machine Learning Research, 5:361?397.
Wei Li and Andrew McCallum. 2005. Semi-supervised
sequence modeling with syntactic topic models. In
Proceedings of the 20th national conference on Artifi-
cial intelligence - Volume 2, AAAI?05, pages 813?818.
AAAI Press.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Jason Wolfe, Aria Haghighi, and Dan Klein. 2008. Fully
distributed EM for very large datasets. In Proceed-
ings of the 25th international conference on Machine
learning, ICML ?08, pages 1184?1191, New York,
NY, USA. ACM.
Huican Zhu and Oscar H. Ibarra. 1999. On some ap-
proximation algorithms for the set partition problem.
In Proceedings of the 15th Triennial Conf. of Int. Fed-
eration of Operations Research Society.
584
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1375?1384,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Local and Global Algorithms for Disambiguation to Wikipedia
Lev Ratinov 1 Dan Roth1 Doug Downey2 Mike Anderson3
1University of Illinois at Urbana-Champaign
{ratinov2|danr}@uiuc.edu
2Northwestern University
ddowney@eecs.northwestern.edu
3Rexonomy
mrander@gmail.com
Abstract
Disambiguating concepts and entities in a con-
text sensitive way is a fundamental problem
in natural language processing. The compre-
hensiveness of Wikipedia has made the on-
line encyclopedia an increasingly popular tar-
get for disambiguation. Disambiguation to
Wikipedia is similar to a traditional Word
Sense Disambiguation task, but distinct in that
the Wikipedia link structure provides addi-
tional information about which disambigua-
tions are compatible. In this work we analyze
approaches that utilize this information to ar-
rive at coherent sets of disambiguations for a
given document (which we call ?global? ap-
proaches), and compare them to more tradi-
tional (local) approaches. We show that previ-
ous approaches for global disambiguation can
be improved, but even then the local disam-
biguation provides a baseline which is very
hard to beat.
1 Introduction
Wikification is the task of identifying and link-
ing expressions in text to their referent Wikipedia
pages. Recently, Wikification has been shown to
form a valuable component for numerous natural
language processing tasks including text classifica-
tion (Gabrilovich and Markovitch, 2007b; Chang et
al., 2008), measuring semantic similarity between
texts (Gabrilovich and Markovitch, 2007a), cross-
document co-reference resolution (Finin et al, 2009;
Mayfield et al, 2009), and other tasks (Kulkarni et
al., 2009).
Previous studies on Wikification differ with re-
spect to the corpora they address and the subset
of expressions they attempt to link. For exam-
ple, some studies focus on linking only named en-
tities, whereas others attempt to link all ?interest-
ing? expressions, mimicking the link structure found
in Wikipedia. Regardless, all Wikification systems
are faced with a key Disambiguation to Wikipedia
(D2W) task. In the D2W task, we?re given a text
along with explicitly identified substrings (called
mentions) to disambiguate, and the goal is to out-
put the corresponding Wikipedia page, if any, for
each mention. For example, given the input sen-
tence ?I am visiting friends in <Chicago>,? we
output http://en.wikipedia.org/wiki/Chicago ? the
Wikipedia page for the city of Chicago, Illinois, and
not (for example) the page for the 2002 film of the
same name.
Local D2W approaches disambiguate each men-
tion in a document separately, utilizing clues such
as the textual similarity between the document and
each candidate disambiguation?s Wikipedia page.
Recent work on D2W has tended to focus on more
sophisticated global approaches to the problem, in
which all mentions in a document are disambiguated
simultaneously to arrive at a coherent set of dis-
ambiguations (Cucerzan, 2007; Milne and Witten,
2008b; Han and Zhao, 2009). For example, if a
mention of ?Michael Jordan? refers to the computer
scientist rather than the basketball player, then we
would expect a mention of ?Monte Carlo? in the
same document to refer to the statistical technique
rather than the location. Global approaches utilize
the Wikipedia link graph to estimate coherence.
1375
m1 = Taiwan m2 = China m3 = Jiangsu Province..............
t1 = Taiwan t5 =People's Republic of China t7 = Jiangsu
..............
Document text  with mentions
t2 = Chinese Taipei t3 =Republic of China t4 = China t6 = History of China
?(m1, t1)
?(m1, t2)
?(m1, t3)
?(t1, t7) ?(t3, t7) ?(t5, t7)
Figure 1: Sample Disambiguation to Wikipedia problem with three mentions. The mention ?Jiangsu? is unambiguous.
The correct mapping from mentions to titles is marked by heavy edges
In this paper, we analyze global and local ap-
proaches to the D2W task. Our contributions are
as follows: (1) We present a formulation of the
D2W task as an optimization problem with local and
global variants, and identify the strengths and the
weaknesses of each, (2) Using this formulation, we
present a new global D2W system, called GLOW. In
experiments on existing and novel D2W data sets,1
GLOW is shown to outperform the previous state-
of-the-art system of (Milne and Witten, 2008b), (3)
We present an error analysis and identify the key re-
maining challenge: determining when mentions re-
fer to concepts not captured in Wikipedia.
2 Problem Definition and Approach
We formalize our Disambiguation to Wikipedia
(D2W) task as follows. We are given a document
d with a set of mentions M = {m1, . . . ,mN},
and our goal is to produce a mapping from the set
of mentions to the set of Wikipedia titles W =
{t1, . . . , t|W |}. Often, mentions correspond to a
concept without a Wikipedia page; we treat this case
by adding a special null title to the set W .
The D2W task can be visualized as finding a
many-to-one matching on a bipartite graph, with
mentions forming one partition and Wikipedia ti-
tles the other (see Figure 1). We denote the output
matching as an N -tuple ? = (t1, . . . , tN ) where ti
is the output disambiguation for mention mi.
1The data sets are available for download at
http://cogcomp.cs.illinois.edu/Data
2.1 Local and Global Disambiguation
A local D2W approach disambiguates each men-
tion mi separately. Specifically, let ?(mi, tj) be a
score function reflecting the likelihood that the can-
didate title tj ?W is the correct disambiguation for
mi ? M . A local approach solves the following
optimization problem:
??local = argmax?
N
?
i=1
?(mi, ti) (1)
Local D2W approaches, exemplified by (Bunescu
and Pasca, 2006) and (Mihalcea and Csomai, 2007),
utilize ? functions that assign higher scores to titles
with content similar to that of the input document.
We expect, all else being equal, that the correct
disambiguations will form a ?coherent? set of re-
lated concepts. Global approaches define a coher-
ence function ?, and attempt to solve the following
disambiguation problem:
?? = argmax
?
[
N
?
i=1
?(mi, ti) + ?(?)] (2)
The global optimization problem in Eq. 2 is NP-
hard, and approximations are required (Cucerzan,
2007). The common approach is to utilize the
Wikipedia link graph to obtain an estimate pairwise
relatedness between titles ?(ti, tj) and to efficiently
generate a disambiguation context ??, a rough ap-
proximation to the optimal ??. We then solve the
easier problem:
?? ? argmax
?
N
?
i=1
[?(mi, ti) +
?
tj???
?(ti, tj)] (3)
1376
Eq. 3 can be solved by finding each ti and then map-
ping mi independently as in a local approach, but
still enforces some degree of coherence among the
disambiguations.
3 Related Work
Wikipedia was first explored as an information
source for named entity disambiguation and in-
formation retrieval by Bunescu and Pasca (2006).
There, disambiguation is performed using an SVM
kernel that compares the lexical context around the
ambiguous named entity to the content of the can-
didate disambiguation?s Wikipedia page. However,
since each ambiguous mention required a separate
SVM model, the experiment was on a very limited
scale. Mihalcea and Csomai applied Word Sense
Disambiguation methods to the Disambiguation to
Wikipedia task (2007). They experimented with
two methods: (a) the lexical overlap between the
Wikipedia page of the candidate disambiguations
and the context of the ambiguous mention, and (b)
training a Naive Bayes classiffier for each ambigu-
ous mention, using the hyperlink information found
in Wikipedia as ground truth. Both (Bunescu and
Pasca, 2006) and (Mihalcea and Csomai, 2007) fall
into the local framework.
Subsequent work on Wikification has stressed that
assigned disambiguations for the same document
should be related, introducing the global approach
(Cucerzan, 2007; Milne and Witten, 2008b; Han and
Zhao, 2009; Ferragina and Scaiella, 2010). The two
critical components of a global approach are the se-
mantic relatedness function ? between two titles,
and the disambiguation context ??. In (Milne and
Witten, 2008b), the semantic context is defined to
be a set of ?unambiguous surface forms? in the text,
and the title relatedness ? is computed as Normal-
ized Google Distance (NGD) (Cilibrasi and Vitanyi,
2007).2 On the other hand, in (Cucerzan, 2007) the
disambiguation context is taken to be all plausible
disambiguations of the named entities in the text,
and title relatedness is based on the overlap in cat-
egories and incoming links. Both approaches have
limitations. The first approach relies on the pres-
2(Milne and Witten, 2008b) also weight each mention in ??
by its estimated disambiguation utility, which can be modeled
by augmenting ? on per-problem basis.
ence of unambiguous mentions in the input docu-
ment, and the second approach inevitably adds ir-
relevant titles to the disambiguation context. As we
demonstrate in our experiments, by utilizing a more
accurate disambiguation context, GLOW is able to
achieve better performance.
4 System Architecture
In this section, we present our global D2W system,
which solves the optimization problem in Eq. 3. We
refer to the system as GLOW, for Global Wikifica-
tion. We use GLOW as a test bed for evaluating local
and global approaches for D2W. GLOW combines
a powerful local model ? with an novel method
for choosing an accurate disambiguation context ??,
which as we show in our experiments allows it to
outperform the previous state of the art.
We represent the functions ? and ? as weighted
sums of features. Specifically, we set:
?(m, t) =
?
i
wi?i(m, t) (4)
where each feature ?i(m, t) captures some aspect
of the relatedness between the mention m and the
Wikipedia title t. Feature functions ?i(t, t?) are de-
fined analogously. We detail the specific feature
functions utilized in GLOW in following sections.
The coefficients wi are learned using a Support Vec-
tor Machine over bootstrapped training data from
Wikipedia, as described in Section 4.5.
At a high level, the GLOW system optimizes the
objective function in Eq. 3 in a two-stage process.
We first execute a ranker to obtain the best non-null
disambiguation for each mention in the document,
and then execute a linker that decides whether the
mention should be linked to Wikipedia, or whether
instead switching the top-ranked disambiguation to
null improves the objective function. As our exper-
iments illustrate, the linking task is the more chal-
lenging of the two by a significant margin.
Figure 2 provides detailed pseudocode for GLOW.
Given a document d and a set of mentions M , we
start by augmenting the set of mentions with all
phrases in the document that could be linked to
Wikipedia, but were not included in M . Introducing
these additional mentions provides context that may
be informative for the global coherence computation
(it has no effect on local approaches). In the second
1377
Algorithm: Disambiguate to Wikipedia
Input: document d, Mentions M = {m1, . . . ,mN}
Output: a disambiguation ? = (t1, . . . , tN ).
1) Let M ? = M? { Other potential mentions in d}
2) For each mention m?i ? M ?, construct a set of disam-
biguation candidates Ti = {ti1, . . . , tiki}, t
i
j 6= null
3) Ranker: Find a solution ? = (t?1, . . . , t?|M?|), where
t?i ? Ti is the best non-null disambiguation of m?i.
4) Linker: For each m?i, map t?i to null in ? iff doing so
improves the objective function
5) Return ? entries for the original mentions M .
Figure 2: High-level pseudocode for GLOW.
step, we construct for each mention mi a limited set
of candidate Wikipedia titles Ti thatmi may refer to.
Considering only a small subset of Wikipedia titles
as potential disambiguations is crucial for tractabil-
ity (we detail which titles are selected below). In the
third step, the ranker outputs the most appropriate
non-null disambiguation ti for each mention mi.
In the final step, the linker decides whether the
top-ranked disambiguation is correct. The disam-
biguation (mi, ti) may be incorrect for several rea-
sons: (1) mention mi does not have a corresponding
Wikipedia page, (2) mi does have a corresponding
Wikipedia page, but it was not included in Ti, or
(3) the ranker erroneously chose an incorrect disam-
biguation over the correct one.
In the below sections, we describe each step of the
GLOW algorithm, and the local and global features
utilized, in detail. Because we desire a system that
can process documents at scale, each step requires
trade-offs between accuracy and efficiency.
4.1 Disambiguation Candidates Generation
The first step in GLOW is to extract all mentions that
can refer to Wikipedia titles, and to construct a set
of disambiguation candidates for each mention. Fol-
lowing previous work, we use Wikipedia hyperlinks
to perform these steps. GLOW utilizes an anchor-
title index, computed by crawling Wikipedia, that
maps each distinct hyperlink anchor text to its tar-
get Wikipedia titles. For example, the anchor text
?Chicago? is used in Wikipedia to refer both to the
city in Illinois and to the movie. Anchor texts in the
index that appear in document d are used to supple-
ment the mention setM in Step 1 of the GLOW algo-
rithm in Figure 2. Because checking all substrings
Baseline Feature: P (t|m), P (t)
Local Features: ?i(t,m)
cosine-sim(Text(t),Text(m)) : Naive/Reweighted
cosine-sim(Text(t),Context(m)): Naive/Reweighted
cosine-sim(Context(t),Text(m)): Naive/Reweighted
cosine-sim(Context(t),Context(m)): Naive/Reweighted
Global Features: ?i(ti, tj)
I[ti?tj ]?PMI(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?NGD(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?PMI(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ]?NGD(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ] : avg/max
I[ti?tj ]?PMI(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?NGD(InLinks(ti),InLinks(tj)) : avg/max
I[ti?tj ]?PMI(OutLinks(ti),OutLinks(tj)) : avg/max
I[ti?tj ]?NGD(OutLinks(ti),OutLinks(tj)) : avg/max
Table 1: Ranker features. I[ti?tj ] is an indicator variable
which is 1 iff ti links to tj or vise-versa. I[ti?tj ] is 1 iff
the titles point to each other.
in the input text against the index is computation-
ally inefficient, we instead prune the search space
by applying a publicly available shallow parser and
named entity recognition system.3 We consider only
the expressions marked as named entities by the
NER tagger, the noun-phrase chunks extracted by
the shallow parser, and all sub-expressions of up to
5 tokens of the noun-phrase chunks.
To retrieve the disambiguation candidates Ti for
a given mention mi in Step 2 of the algorithm, we
query the anchor-title index. Ti is taken to be the
set of titles most frequently linked to with anchor
text mi in Wikipedia. For computational efficiency,
we utilize only the top 20 most frequent target pages
for the anchor text; the accuracy impact of this opti-
mization is analyzed in Section 6.
From the anchor-title index, we compute two lo-
cal features ?i(m, t). The first, P (t|m), is the frac-
tion of times the title t is the target page for an an-
chor text m. This single feature is a very reliable
indicator of the correct disambiguation (Fader et al,
2009), and we use it as a baseline in our experiments.
The second, P (t), gives the fraction of all Wikipedia
articles that link to t.
4.2 Local Features ?
In addition to the two baseline features mentioned in
the previous section, we compute a set of text-based
3Available at http://cogcomp.cs.illinois.edu/page/software.
1378
local features ?(t,m). These features capture the in-
tuition that a given Wikipedia title t is more likely to
be referred to by mention m appearing in document
d if the Wikipedia page for t has high textual simi-
larity to d, or if the context surrounding hyperlinks
to t are similar to m?s context in d.
For each Wikipedia title t, we construct a top-
200 token TF-IDF summary of the Wikipedia page
t, which we denote as Text(t) and a top-200 to-
ken TF-IDF summary of the context within which
t was hyperlinked to in Wikipedia, which we denote
as Context(t). We keep the IDF vector for all to-
kens in Wikipedia, and given an input mention m in
a document d, we extract the TF-IDF representation
of d, which we denote Text(d), and a TF-IDF rep-
resentation of a 100-token window around m, which
we denote Context(m). This allows us to define
four local features described in Table 1.
We additionally compute weighted versions of
the features described above. Error analysis has
shown that in many cases the summaries of the dif-
ferent disambiguation candidates for the same sur-
face form s were very similar. For example, con-
sider the disambiguation candidates of ?China? and
their TF-IDF summaries in Figure 1. The major-
ity of the terms selected in all summaries refer to
the general issues related to China, such as ?legal-
ism, reform, military, control, etc.?, while a minority
of the terms actually allow disambiguation between
the candidates. The problem stems from the fact
that the TF-IDF summaries are constructed against
the entire Wikipedia, and not against the confusion
set of disambiguation candidates of m. Therefore,
we re-weigh the TF-IDF vectors using the TF-IDF
scheme on the disambiguation candidates as a ad-
hoc document collection, similarly to an approach
in (Joachims, 1997) for classifying documents. In
our scenario, the TF of the a token is the original
TF-IDF summary score (a real number), and the IDF
term is the sum of all the TF-IDF scores for the to-
ken within the set of disambiguation candidates for
m. This adds 4 more ?reweighted local? features in
Table 1.
4.3 Global Features ?
Global approaches require a disambiguation context
?? and a relatedness measure ? in Eq. 3. In this sec-
tion, we describe our method for generating a dis-
ambiguation context, and the set of global features
?i(t, t?) forming our relatedness measure.
In previous work, Cucerzan defined the disam-
biguation context as the union of disambiguation
candidates for all the named entity mentions in the
input document (2007). The disadvantage of this ap-
proach is that irrelevant titles are inevitably added to
the disambiguation context, creating noise. Milne
and Witten, on the other hand, use a set of un-
ambiguous mentions (2008b). This approach uti-
lizes only a fraction of the available mentions for
context, and relies on the presence of unambigu-
ous mentions with high disambiguation utility. In
GLOW, we utilize a simple and efficient alternative
approach: we first train a local disambiguation sys-
tem, and then use the predictions of that system as
the disambiguation context. The advantage of this
approach is that unlike (Milne and Witten, 2008b)
we use all the available mentions in the document,
and unlike (Cucerzan, 2007) we reduce the amount
of irrelevant titles in the disambiguation context by
taking only the top-ranked disambiguation per men-
tion.
Our global features are refinements of previously
proposed semantic relatedness measures between
Wikipedia titles. We are aware of two previous
methods for estimating the relatedness between two
Wikipedia concepts: (Strube and Ponzetto, 2006),
which uses category overlap, and (Milne and Wit-
ten, 2008a), which uses the incoming link structure.
Previous work experimented with two relatedness
measures: NGD, and Specificity-weighted Cosine
Similarity. Consistent with previous work, we found
NGD to be the better-performing of the two. Thus
we use only NGD along with a well-known Pon-
twise Mutual Information (PMI) relatedness mea-
sure. Given a Wikipedia title collection W , titles
t1 and t2 with a set of incoming links L1, and L2
respectively, PMI and NGD are defined as follows:
NGD(L1, L2) = Log(Max(|L1|, |L2|))? Log(|L1 ? L2|)Log(|W |)? Log(Min(|L1|, |L2|))
PMI(L1, L2) = |L1 ? L2|/|W ||L1|/|W ||L2|/|W |
The NGD and the PMI measures can also be com-
puted over the set of outgoing links, and we include
these as features as well. We also included a fea-
ture indicating whether the articles each link to one
1379
another. Lastly, rather than taking the sum of the re-
latedness scores as suggested by Eq. 3, we use two
features: the average and the maximum relatedness
to ??. We expect the average to be informative for
many documents. The intuition for also including
the maximum relatedness is that for longer docu-
ments that may cover many different subtopics, the
maximum may be more informative than the aver-
age.
We have experimented with other semantic fea-
tures, such as category overlap or cosine similar-
ity between the TF-IDF summaries of the titles, but
these did not improve performance in our experi-
ments. The complete set of global features used in
GLOW is given in Table 1.
4.4 Linker Features
Given the mention m and the top-ranked disam-
biguation t, the linker attempts to decide whether t is
indeed the correct disambiguation of m. The linker
includes the same features as the ranker, plus addi-
tional features we expect to be particularly relevant
to the task. We include the confidence of the ranker
in t with respect to second-best disambiguation t?,
intended to estimate whether the ranker may have
made a mistake. We also include several properties
of the mention m: the entropy of the distribution
P (t|m), the percent of Wikipedia titles in which m
appears hyperlinked versus the percent of times m
appears as plain text, whether m was detected by
NER as a named entity, and a Good-Turing estimate
of how likely m is to be out-of-Wikipedia concept
based on the counts in P (t|m).
4.5 Linker and Ranker Training
We train the coefficients for the ranker features us-
ing a linear Ranking Support Vector Machine, using
training data gathered from Wikipedia. Wikipedia
links are considered gold-standard links for the
training process. The methods for compiling the
Wikipedia training corpus are given in Section 5.
We train the linker as a separate linear Support
Vector Machine. Training data for the linker is ob-
tained by applying the ranker on the training set. The
mentions for which the top-ranked disambiguation
did not match the gold disambiguation are treated
as negative examples, while the mentions the ranker
got correct serve as positive examples.
Mentions/Distinct titles
data set Gold Identified Solvable
ACE 257/255 213/212 185/184
MSNBC 747/372 530/287 470/273
AQUAINT 727/727 601/601 588/588
Wikipedia 928/813 855/751 843/742
Table 2: Number of mentions and corresponding dis-
tinct titles by data set. Listed are (number of men-
tions)/(number of distinct titles) for each data set, for each
of three mention types. Gold mentions include all dis-
ambiguated mentions in the data set. Identified mentions
are gold mentions whose correct disambiguations exist in
GLOW?s author-title index. Solvable mentions are identi-
fied mentions whose correct disambiguations are among
the candidates selected by GLOW (see Table 3).
5 Data sets and Evaluation Methodology
We evaluate GLOW on four data sets, of which
two are from previous work. The first data set,
from (Milne and Witten, 2008b), is a subset of the
AQUAINT corpus of newswire text that is annotated
to mimic the hyperlink structure in Wikipedia. That
is, only the first mentions of ?important? titles were
hyperlinked. Titles deemed uninteresting and re-
dundant mentions of the same title are not linked.
The second data set, from (Cucerzan, 2007), is taken
from MSNBC news and focuses on disambiguating
named entities after running NER and co-reference
resolution systems on newsire text. In this case,
all mentions of all the detected named entities are
linked.
We also constructed two additional data sets. The
first is a subset of the ACE co-reference data set,
which has the advantage that mentions and their
types are given, and the co-reference is resolved. We
asked annotators on Amazon?s Mechanical Turk to
link the first nominal mention of each co-reference
chain to Wikipedia, if possible. Finding the accu-
racy of a majority vote of these annotations to be
approximately 85%, we manually corrected the an-
notations to obtain ground truth for our experiments.
The second data set we constructed, Wiki, is a sam-
ple of paragraphs from Wikipedia pages. Mentions
in this data set correspond to existing hyperlinks in
the Wikipedia text. Because Wikipedia editors ex-
plicitly link mentions to Wikipedia pages, their an-
chor text tends to match the title of the linked-to-
page?as a result, in the overwhelming majority of
1380
cases, the disambiguation decision is as trivial as
string matching. In an attempt to generate more
challenging data, we extracted 10,000 random para-
graphs for which choosing the top disambiguation
according to P (t|m) results in at least a 10% ranker
error rate. 40 paragraphs of this data was utilized for
testing, while the remainder was used for training.
The data sets are summarized in Table 2. The ta-
ble shows the number of annotated mentions which
were hyperlinked to non-null Wikipedia pages, and
the number of titles in the documents (without
counting repetitions). For example, the AQUAINT
data set contains 727 mentions,4 all of which refer
to distinct titles. The MSNBC data set contains 747
mentions mapped to non-null Wikipedia pages, but
some mentions within the same document refer to
the same titles. There are 372 titles in the data set,
when multiple instances of the same title within one
document are not counted.
To isolate the performance of the individual com-
ponents of GLOW, we use multiple distinct metrics
for evaluation. Ranker accuracy, which measures
the performance of the ranker alone, is computed
only over those mentions with a non-null gold dis-
ambiguation that appears in the candidate set. It is
equal to the fraction of these mentions for which the
ranker returns the correct disambiguation. Thus, a
perfect ranker should achieve a ranker accuracy of
1.0, irrespective of limitations of the candidate gen-
erator. Linker accuracy is defined as the fraction of
all mentions for which the linker outputs the correct
disambiguation (note that, when the title produced
by the ranker is incorrect, this penalizes linker accu-
racy). Lastly, we evaluate our whole system against
other baselines using a previously-employed ?bag of
titles? (BOT) evaluation (Milne and Witten, 2008b).
In BOT, we compare the set of titles output for a doc-
ument with the gold set of titles for that document
(ignoring duplicates), and utilize standard precision,
recall, and F1 measures.
In BOT, the set of titles is collected from the men-
tions hyperlinked in the gold annotation. That is,
if the gold annotation is { (China, People?s Repub-
lic of China), (Taiwan, Taiwan), (Jiangsu, Jiangsu)}
4The data set contains votes on how important the mentions
are. We believe that the results in (Milne and Witten, 2008b)
were reported on mentions which the majority of annotators
considered important. In contrast, we used all the mentions.
Generated data sets
Candidates k ACE MSNBC AQUAINT Wiki
1 81.69 72.26 91.01 84.79
3 85.44 86.22 96.83 94.73
5 86.38 87.35 97.17 96.37
20 86.85 88.67 97.83 98.59
Table 3: Percent of ?solvable? mentions as a function
of the number of generated disambiguation candidates.
Listed is the fraction of identified mentions m whose
target disambiguation t is among the top k candidates
ranked in descending order of P (t|m).
and the predicted anotation is: { (China, People?s
Republic of China), (China, History of China), (Tai-
wan, null), (Jiangsu, Jiangsu), (republic, Govern-
ment)} , then the BOT for the gold annotation is:
{People?s Republic of China, Taiwan, Jiangsu} , and
the BOT for the predicted annotation is: {People?s
Republic of China, History of China, Jiangsu} . The
title Government is not included in the BOT for pre-
dicted annotation, because its associate mention re-
public did not appear as a mention in the gold anno-
tation. Both the precision and the recall of the above
prediction is 0.66. We note that in the BOT evalua-
tion, following (Milne and Witten, 2008b) we con-
sider all the titles within a document, even if some
the titles were due to mentions we failed to identify.5
6 Experiments and Results
In this section, we evaluate and analyze GLOW?s
performance on the D2W task. We begin by eval-
uating the mention detection component (Step 1 of
the algorithm). The second column of Table 2 shows
how many of the ?non-null? mentions and corre-
sponding titles we could successfully identify (e.g.
out of 747 mentions in the MSNBC data set, only
530 appeared in our anchor-title index). Missing en-
tities were primarily due to especially rare surface
forms, or sometimes due to idiosyncratic capitaliza-
tion in the corpus. Improving the number of iden-
tified mentions substantially is non-trivial; (Zhou et
al., 2010) managed to successfully identify only 59
more entities than we do in the MSNBC data set, us-
ing a much more powerful detection method based
on search engine query logs.
We generate disambiguation candidates for a
5We evaluate the mention identification stage in Section 6.
1381
Data sets
Features ACE MSNBC AQUAINT Wiki
P (t|m) 94.05 81.91 93.19 85.88
P (t|m)+Local
Naive 95.67 84.04 94.38 92.76
Reweighted 96.21 85.10 95.57 93.59
All above 95.67 84.68 95.40 93.59
P (t|m)+Global
NER 96.21 84.04 94.04 89.56
Unambiguous 94.59 84.46 95.40 89.67
Predictions 96.75 88.51 95.91 89.79
P (t|m)+Local+Global
All features 97.83 87.02 94.38 94.18
Table 4: Ranker Accuracy. Bold values indicate the
best performance in each feature group. The global ap-
proaches marginally outperform the local approaches on
ranker accuracy , while combing the approaches leads to
further marginal performance improvement.
mention m using an anchor-title index, choosing
the 20 titles with maximal P (t|m). Table 3 eval-
uates the accuracy of this generation policy. We
report the percent of mentions for which the cor-
rect disambiguation is generated in the top k can-
didates (called ?solvable? mentions). We see that
the baseline prediction of choosing the disambigua-
tion t which maximizes P (t|m) is very strong (80%
of the correct mentions have maximal P (t|m) in all
data sets except MSNBC). The fraction of solvable
mentions increases until about five candidates per
mention are generated, after which the increase is
rather slow. Thus, we believe choosing a limit of 20
candidates per mention offers an attractive trade-off
of accuracy and efficiency. The last column of Ta-
ble 2 reports the number of solvable mentions and
the corresponding number of titles with a cutoff of
20 disambiguation candidates, which we use in our
experiments.
Next, we evaluate the accuracy of the ranker. Ta-
ble 4 compares the ranker performance with base-
line, local and global features. The reweighted lo-
cal features outperform the unweighted (?Naive?)
version, and the global approach outperforms the
local approach on all data sets except Wikipedia.
As the table shows, our approach of defining the
disambiguation context to be the predicted dis-
ambiguations of a simpler local model (?Predic-
tions?) performs better than using NER entities as
in (Cucerzan, 2007), or only the unambiguous enti-
Data set Local Global Local+Global
ACE 80.1 ? 82.8 80.6 ? 80.6 81.5 ? 85.1
MSNBC 74.9 ? 76.0 77.9 ? 77.9 76.5 ? 76.9
AQUAINT 93.5 ? 91.5 93.8 ? 92.1 92.3 ? 91.3
Wiki 92.2 ? 92.0 88.5 ? 87.2 92.8 ? 92.6
Table 5: Linker performance. The notation X ? Y
means that when linking all mentions, the linking accu-
racy is X , while when applying the trained linker, the
performance is Y . The local approaches are better suited
for linking than the global approaches. The linking accu-
racy is very sensitive to domain changes.
System ACE MSNBC AQUAINT Wiki
Baseline: P (t|m) 69.52 72.83 82.67 81.77
GLOW Local 75.60 74.39 84.52 90.20
GLOW Global 74.73 74.58 84.37 86.62
GLOW 77.25 74.88 83.94 90.54
M&W 72.76 68.49 83.61 80.32
Table 6: End systems performance - BOT F1. The per-
formance of the full system (GLOW) is similar to that of
the local version. GLOW outperforms (Milne and Witten,
2008b) on all data sets.
ties as in (Milne and Witten, 2008b).6 Combining
the local and the global approaches typically results
in minor improvements.
While the global approaches are most effective for
ranking, the linking problem has different charac-
teristics as shown in Table 5. We can see that the
global features are not helpful in general for predict-
ing whether the top-ranked disambiguation is indeed
the correct one.
Further, although the trained linker improves ac-
curacy in some cases, the gains are marginal?and
the linker decreases performance on some data sets.
One explanation for the decrease is that the linker
is trained on Wikipedia, but is being tested on non-
Wikipedia text which has different characteristics.
However, in separate experiments we found that
training a linker on out-of-Wikipedia text only in-
creased test set performance by approximately 3
percentage points. Clearly, while ranking accuracy
is high overall, different strategies are needed to
achieve consistently high linking performance.
A few examples from the ACE data set help il-
6In NER we used only the top prediction, because using all
candidates as in (Cucerzan, 2007) proved prohibitively ineffi-
cient.
1382
lustrate the tradeoffs between local and global fea-
tures in GLOW. The global system mistakenly links
?<Dorothy Byrne>, a state coordinator for the
Florida Green Party, said . . . ? to the British jour-
nalist, because the journalist sense has high coher-
ence with other mentions in the newswire text. How-
ever, the local approach correctly maps the men-
tion to null because of a lack of local contextual
clues. On the other hand, in the sentence ?In-
stead of Los Angeles International, for example,
consider flying into <Burbank> or John Wayne Air-
port in Orange County, Calif.?, the local ranker
links the mention Burbank to Burbank, California,
while the global system correctly maps the entity to
Bob Hope Airport, because the three airports men-
tioned in the sentence are highly related to one an-
other.
Lastly, in Table 6 we compare the end system
BOT F1 performance. The local approach proves
a very competitive baseline which is hard to beat.
Combining the global and the local approach leads
to marginal improvements. The full GLOW sys-
tem outperforms the existing state-of-the-art system
from (Milne and Witten, 2008b), denoted as M&W,
on all data sets. We also compared our system with
the recent TAGME Wikification system (Ferragina
and Scaiella, 2010). However, TAGME is designed
for a different setting than ours: extremely short
texts, like Twitter posts. The TAGME RESTful API
was unable to process some of our documents at
once. We attempted to input test documents one sen-
tence at a time, disambiguating each sentence inde-
pendently, which resulted in poor performance (0.07
points in F1 lower than the P (t|m) baseline). This
happened mainly because the same mentions were
linked to different titles in different sentences, lead-
ing to low precision.
An important question is why M&W underper-
forms the baseline on the MSNBC and Wikipedia
data sets. In an error analysis, M&W performed
poorly on the MSNBC data not due to poor disam-
biguations, but instead because the data set contains
only named entities, which were often delimited in-
correctly by M&W. Wikipedia was challenging for
a different reason: M&W performs less well on the
short (one paragraph) texts in that set, because they
contain relatively few of the unambiguous entities
the system relies on for disambiguation.
7 Conclusions
We have formalized the Disambiguation to
Wikipedia (D2W) task as an optimization problem
with local and global variants, and analyzed the
strengths and weaknesses of each. Our experiments
revealed that previous approaches for global disam-
biguation can be improved, but even then the local
disambiguation provides a baseline which is very
hard to beat.
As our error analysis illustrates, the primary re-
maining challenge is determining when a mention
does not have a corresponding Wikipedia page.
Wikipedia?s hyperlinks offer a wealth of disam-
biguated mentions that can be leveraged to train
a D2W system. However, when compared with
mentions from general text, Wikipedia mentions
are disproportionately likely to have corresponding
Wikipedia pages. Our initial experiments suggest
that accounting for this bias requires more than sim-
ply training a D2W system on a moderate num-
ber of examples from non-Wikipedia text. Apply-
ing distinct semi-supervised and active learning ap-
proaches to the task is a primary area of future work.
Acknowledgments
This research supported by the Army Research
Laboratory (ARL) under agreement W911NF-09-
2-0053 and by the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. The third
author was supported by a Microsoft New Faculty
Fellowship. Any opinions, findings, conclusions or
recommendations are those of the authors and do not
necessarily reflect the view of the ARL, DARPA,
AFRL, or the US government.
References
R. Bunescu and M. Pasca. 2006. Using encyclope-
dic knowledge for named entity disambiguation. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), Trento, Italy, pages 9?16, April.
Ming-Wei Chang, Lev Ratinov, Dan Roth, and Vivek
Srikumar. 2008. Importance of semantic represen-
tation: dataless classification. In Proceedings of the
1383
23rd national conference on Artificial intelligence -
Volume 2, pages 830?835. AAAI Press.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
google similarity distance. IEEE Trans. on Knowl. and
Data Eng., 19(3):370?383.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716, Prague, Czech Republic, June. Association
for Computational Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2009. Scaling wikipedia-based named entity disam-
biguation to arbitrary web text. In Proceedings of
the WikiAI 09 - IJCAI Workshop: User Contributed
Knowledge and Artificial Intelligence: An Evolving
Synergy, Pasadena, CA, USA, July.
Paolo Ferragina and Ugo Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Jimmy Huang, Nick Koudas, Gareth J. F.
Jones, Xindong Wu, Kevyn Collins-Thompson, and
Aijun An, editors, Proceedings of the 19th ACM con-
ference on Information and knowledge management,
pages 1625?1628. ACM.
Tim Finin, Zareen Syed, James Mayfield, Paul Mc-
Namee, and Christine Piatko. 2009. Using Wikitol-
ogy for Cross-Document Entity Coreference Resolu-
tion. In Proceedings of the AAAI Spring Symposium
on Learning by Reading and Learning to Read. AAAI
Press, March.
Evgeniy Gabrilovich and Shaul Markovitch. 2007a.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, pages 1606?1611, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2007b.
Harnessing the expertise of 70,000 human editors:
Knowledge-based feature generation for text catego-
rization. J. Mach. Learn. Res., 8:2297?2345, Decem-
ber.
Xianpei Han and Jun Zhao. 2009. Named entity dis-
ambiguation by leveraging wikipedia semantic knowl-
edge. In Proceeding of the 18th ACM conference on
Information and knowledge management, CIKM ?09,
pages 215?224, New York, NY, USA. ACM.
Thorsten Joachims. 1997. A probabilistic analysis of
the rocchio algorithm with tfidf for text categoriza-
tion. In Proceedings of the Fourteenth International
Conference on Machine Learning, ICML ?97, pages
143?151, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and
Soumen Chakrabarti. 2009. Collective annotation
of wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?09,
pages 457?466, New York, NY, USA. ACM.
James Mayfield, David Alexander, Bonnie Dorr, Jason
Eisner, Tamer Elsayed, Tim Finin, Clay Fink, Mar-
jorie Freedman, Nikesh Garera, James Mayfield, Paul
McNamee, Saif Mohammad, Douglas Oard, Chris-
tine Piatko, Asad Sayeed, Zareen Syed, and Ralph
Weischede. 2009. Cross-Document Coreference Res-
olution: A Key Technology for Learning by Reading.
In Proceedings of the AAAI 2009 Spring Symposium
on Learning by Reading and Learning to Read. AAAI
Press, March.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, pages 233?242, New York, NY, USA.
ACM.
David Milne and Ian H. Witten. 2008a. An effec-
tive, low-cost measure of semantic relatedness ob-
tained from wikipedia links. In In the Wikipedia and
AI Workshop of AAAI.
David Milne and Ian H. Witten. 2008b. Learning to link
with wikipedia. In Proceedings of the 17th ACM con-
ference on Information and knowledge management,
CIKM ?08, pages 509?518, New York, NY, USA.
ACM.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! computing semantic relatedness using
wikipedia. In proceedings of the 21st national confer-
ence on Artificial intelligence - Volume 2, pages 1419?
1424. AAAI Press.
Yiping Zhou, Lan Nie, Omid Rouhani-Kalleh, Flavian
Vasile, and Scott Gaffney. 2010. Resolving surface
forms to wikipedia topics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (Coling 2010), pages 1335?1343, Beijing, China,
August. Coling 2010 Organizing Committee.
1384
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 343?351,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Scaling Semi-supervised Naive Bayes with Feature Marginals
Michael R. Lucas and Doug Downey
Northwestern University
2133 Sheridan Road
Evanston, IL 60208
mlucas@u.northwestern.edu
ddowney@eecs.northwestern.edu
Abstract
Semi-supervised learning (SSL) methods
augment standard machine learning (ML)
techniques to leverage unlabeled data.
SSL techniques are often effective in text
classification, where labeled data is scarce
but large unlabeled corpora are readily
available. However, existing SSL tech-
niques typically require multiple passes
over the entirety of the unlabeled data,
meaning the techniques are not applicable
to large corpora being produced today.
In this paper, we show that improving
marginal word frequency estimates using
unlabeled data can enable semi-supervised
text classification that scales to massive
unlabeled data sets. We present a novel
learning algorithm, which optimizes a
Naive Bayes model to accord with statis-
tics calculated from the unlabeled corpus.
In experiments with text topic classifica-
tion and sentiment analysis, we show that
our method is both more scalable and more
accurate than SSL techniques from previ-
ous work.
1 Introduction
Semi-supervised Learning (SSL) is a Machine
Learning (ML) approach that utilizes large
amounts of unlabeled data, combined with a
smaller amount of labeled data, to learn a tar-
get function (Zhu, 2006; Chapelle et al, 2006).
SSL is motivated by a simple reality: the amount
of available machine-readable data is exploding,
while human capacity for hand-labeling data for
any given ML task remains relatively constant.
Experiments in text classification and other do-
mains have demonstrated that by leveraging un-
labeled data, SSL techniques improve machine
learning performance when human input is limited
(e.g., (Nigam et al, 2000; Mann and McCallum,
2010)).
However, current SSL techniques have scal-
ability limitations. Typically, for each target
concept to be learned, a semi-supervised classi-
fier is trained using iterative techniques that exe-
cute multiple passes over the unlabeled data (e.g.,
Expectation-Maximization (Nigam et al, 2000) or
Label Propagation (Zhu and Ghahramani, 2002)).
This is problematic for text classification over
large unlabeled corpora like the Web: new tar-
get concepts (new tasks and new topics of interest)
arise frequently, and performing even a single pass
over a large corpus for each new target concept is
intractable.
In this paper, we present a new SSL text classi-
fication approach that scales to large corpora. In-
stead of utilizing unlabeled examples directly for
each given target concept, our approach is to pre-
compute a small set of statistics over the unlabeled
data in advance. Then, for a given target class and
labeled data set, we utilize the statistics to improve
a classifier.
Specifically, we introduce a method that ex-
tends Multinomial Naive Bayes (MNB) to lever-
age marginal probability statistics P (w) of each
word w, computed over the unlabeled data. The
marginal statistics are used as a constraint to im-
prove the class-conditional probability estimates
P (w|+) and P (w|?) for the positive and negative
classes, which are often noisy when estimated over
sparse labeled data sets. We refer to the technique
as MNB with Frequency Marginals (MNB-FM).
In experiments with large unlabeled data sets
and sparse labeled data, we find that MNB-
FM is both faster and more accurate on aver-
age than standard SSL methods from previous
work, including Label Propagation, MNB with
Expectation-Maximization,, and the recent Semi-
supervised Frequency Estimate (SFE) algorithm
(Su et al, 2011). We also analyze how MNB-343
FM improves accuracy, and find that surprisingly
MNB-FM is especially useful for improving class-
conditional probability estimates for words that
never occur in the training set.
The paper proceeds as follows. We formally de-
fine the task in Section 2. Our algorithm is defined
in Section 3. We present experimental results in
Section 4, and analysis in Section 5. We discuss
related work in Section 6 and conclude in Section
7 with a discussion of future work.
2 Problem Definition
We consider a semi-supervised classification task,
in which the goal is to produce a mapping
from an instance space X consisting of T -tuples
of non-negative integer-valued features w =
(w1, . . . , wT ), to a binary output space Y =
{?,+}. In particular, our experiments will fo-
cus on the case in which the wi?s represent word
counts in a given document, in a corpus of vocab-
ulary size T .
We assume the following inputs:
? A set of zero or more labeled documents
DL = {(wd, yd)|d = 1, . . . , n}, drawn i.i.d.
from a distribution P (w, y) for w ? X and
y ? Y .
? A large set of unlabeled documents DU =
{(wd)|d = n+1, . . . , n+u} drawn from the
marginal distribution P (w) =
?
y
P (w, y).
The goal of the task is to output a classifer
f : X ? Y that performs well in predicting the
classes of given unlabeled documents. The met-
rics of evaluation we focus on in our experiments
are detailed in Section 4.
Our semi-supervised technique utilizes statis-
tics computed over the labeled corpus, denoted as
follows. We use N+w to denote the sum of the
occurrences of word w over all documents in the
positive class in the labeled data DL. Also, let
N+ =
?n
w?DL N
+
w be the sum value of all word
counts in the labeled positive documents. The
count of the remaining words in the positive doc-
uments is represented as N+?w = N+ ?N+w . The
quantitiesN?,N?w , andN??w are defined similarly
for the negative class.
3 MNB with Feature Marginals
We now introduce our algorithm, which scalably
utilizes large unlabeled data stores for classifica-
tion tasks. The technique builds upon the multino-
mial Naive Bayes model, and is denoted as MNB
with Feature Marginals (MNB-FM).
3.1 MNB-FM Method
In the text classification setting , each feature value
wd represents count of observations of word w in
document d. MNB makes the simplifying assump-
tion that word occurrences are conditionally inde-
pendent of each other given the class (+ or ?) of
the example. Formally, let the probability P (w|+)
of the w in the positive class be denoted as ?+w . Let
P (+) denote the prior probability that a document
is of the positive class, and P (?) = 1?P (+) the
prior for the negative class. Then MNB represents
the class probability of an example as:
P (+|d) =
?
w?d
(?+w )w
dP (+)
?
w?d
(??w )w
dP (?) +
?
w?d
(?+w )w
dP (+)
(1)
MNB estimates the parameters ?+w from the
corresponding counts in the training set. The
maximum-likelihood estimate of ?+w is N+w /N+,
and to prevent zero-probability estimates we em-
ploy ?add-1? smoothing (typical in MNB) to ob-
tain the estimate:
?+w =
N+w + 1
N+ + |T | .
After MNB calculates ?+w and ??w from the train-
ing set for each feature in the feature space, it can
then classify test examples using Equation 1.
MNB-FM attempts to improve MNB?s esti-
mates of ?+w and ??w , using statistics computed over
the unlabeled data. Formally, MNB-FM leverages
the equality:
P (w) = ?+wPt(+) + ??wPt(?) (2)
The left-hand-side of Equation 2, P (w), repre-
sents the probability that a given randomly drawn
token from the unlabeled data happens to be the
word w. We write Pt(+) to denote the probabil-
ity that a randomly drawn token (i.e. a word oc-
currence) from the corpus comes from the posi-
tive class. Note that Pt(+) can differ from P (+),
the prior probability that a document is positive,
due to variations in document length. Pt(?) is de-
fined similarly for the negative class. MNB-FM is
motivated by the insight that the left-hand-side of344
Equation 2 can be estimated in advance, without
knowledge of the target class, simply by counting
the number of tokens of each word in the unla-
beled data.
MNB-FM then uses this improved estimate of
P (w) as a constraint to improve the MNB param-
eters on the right-hand-side of Equation 2. We
note that Pt(+) and Pt(?), even for a small train-
ing set, can typically be estimated reliably? ev-
ery token in the training data serves as an obser-
vation of these quantities. However, for large and
sparse feature spaces common in settings like text
classification, many features occur in only a small
fraction of examples?meaning ?+w and ??w must
be estimated from only a handful of observations.
MNB-FM attempts to improve the noisy estimates
?+w and ??w utilizing the robust estimate for P (w)
computed over unlabeled data.
Specifically, MNB-FM proceeds by assuming
the MLEs for P (w) (computed over unlabeled
data), Pt(+), and Pt(?) are correct, and re-
estimates ?+w and ??w under the constraint in Equa-
tion 2.
First, the maximum likelihood estimates of ?+w
and ??w given the training data DL are:
argmax
?+w ,??w
P (DL|?+w , ??w )
= argmax
?+w ,??w
?+(N+w )w (1? ?+w )(N
+
?w)
??(N?w )w (1? ??w )(N
?
?w)
= argmax
?+w ,??w
N+w ln(?+w ) +N+?w ln(1? ?+w )+
N?w ln(??w ) +N??w ln(1? ??w )
(3)
We can rewrite the constraint in Equation 2 as:
??w = K ? ?+wL
where for compactness we represent:
K = P (w)Pt(?)
;L = Pt(+)Pt(?)
.
Substituting the constraint into Equation 3
shows that we wish to choose ?+w as:
argmax
?+w
N+w ln(?+w ) +N+?w ln(1? ?+w )+
N?w ln(K ? L?+w ) +N??w ln(1?K + L?+w )
The optimal values for ?+w are thus located at the
solutions of:
0 = N
+
w
?+w
+ N
+
?w
?+w ? 1
+ LN
?
w
L?+w ?K
+ LN
?
?w
L?+w ?K + 1
Both ?+w and ??w are constrained to valid prob-
abilities in [0,1] when ?+w ? [0, KL ]. If N+?w and
N?w have non-zero counts, vertical asymptotes ex-
ist at 0 and KL and guarantee a solution in thisrange. Otherwise, a valid solution may not ex-
ist. In that case, we default to the add-1 Smooth-
ing estimates used by MNB. Finally, after optimiz-
ing the values ?+w and ??w for each word w as de-
scribed above, we normalize the estimates to ob-
tain valid conditional probability distributions, i.e.
with ?w ?+w =
?
w ??w = 1
3.2 MNB-FM Example
The following concrete example illustrates how
MNB-FM can improve MNB parameters using the
statistic P (w) computed over unlabeled data. The
example comes from the Reuters Aptemod text
classification task addressed in Section 4, using
bag-of-words features for the Earnings class. In
one experiment with 10 labeled training examples,
we observed 5 positive and 5 negative examples,
with the word ?resources? occurring three times
in the set (once in the positive class, twice in the
negative class).
MNB uses add-1 smoothing to estimate the con-
ditional probability of the word ?resources? in
each class as ?+w = 1+1216+33504 = 5.93e-5, and
??w = 2+1547+33504 = 8.81e-5. Thus, ?
+
w
??w
= 0.673
implying that ?resources? is a negative indicator
of the Earnings class. However, this estimate is
inaccurate. In fact, over the full dataset, the pa-
rameter values we observe are ?+w = 93168549 =5.70e-4 and ??w = 263564717 = 4.65e-4, with a ratio
of ?+w??w = 1.223. Thus, in actuality, the word ?re-sources? is a mild positive indicator of the Earn-
ings class. Yet because MNB estimates its param-
eters from only the sparse training data, it can be
inaccurate.
The optimization in MNB-FM seeks to accord
its parameter estimates with the feature frequency,
computed from unlabeled data, of P (w) = 4.89e-
4. We see that compared with P (w), the ?+w and
??w that MNB estimates from the training data are
both too low by almost an order of magnitude.
Further, the maximum likelihood estimate for ??w
(based on an occurrence count of 2 out of 547 ob-
servations) is somewhat more reliable than that for
?+w (1 of 216 observations). As a result, ?+w is ad-
justed upward relatively more than ??w via MNB-
FM?s constrained ML estimation. MNB-FM re-
turns ?+w = 6.52e-5 and ??w = 6.04e-5. The ratio345
?+w
??w
is 1.079, meaning MNB-FM correctly identi-
fies the word ?resources? as an indicator of the
positive class.
The above example illustrates how MNB-FM
can leverage frequency marginal statistics com-
puted over unlabeled data to improve MNB?s
conditional probability estimates. We analyze
how frequently MNB-FM succeeds in improving
MNB?s estimates in practice, and the resulting im-
pact on classification accuracy, below.
4 Experiments
In this section, we describe our experiments quan-
tifying the accuracy and scalability of our pro-
posed technique. Across multiple domains, we
find that MNB-FM outperforms a variety of ap-
proaches from previous work.
4.1 Data Sets
We evaluate on two text classification tasks: topic
classification, and sentiment detection. In topic
classification, the task is to determine whether a
test document belongs to a specified topic. We
train a classifier separately (i.e., in a binary clas-
sification setting) for each topic and measure clas-
sification performance for each class individually.
The sentiment detection task is to determine
whether a document is written with a positive or
negative sentiment. In our case, the goal is to de-
termine if the given text belongs to a positive re-
view of a product.
4.1.1 RCV1
The Reuters RCV1 corpus is a standard large cor-
pus used for topic classification evaluations (Lewis
et al, 2004). It includes 804,414 documents with
several nested target classes. We consider the 5
largest base classes after punctuation and stop-
words were removed. The vocabulary consisted
of 288,062 unique words, and the total number of
tokens in the data set was 99,702,278. Details of
the classes can be found in Table 1.
4.1.2 Reuters Aptemod
While MNB-FM is designed to improve the scala-
bility of SSL to large corpora, some of the com-
parison methods from previous work were not
tractable on the large topic classification data set
RCV1. To evaluate these methods, we also exper-
imented with the Reuters ApteMod dataset (Yang
and Liu, 1999), consisting of 10,788 documents
belonging to 90 classes. We consider the 10 most
Class # Positive
CCAT 381327 (47.40%)
GCAT 239267 (29.74%)
MCAT 204820 (25.46%)
ECAT 119920 (14.91%)
GPOL 56878 (7.07%)
Table 1: RCV1 dataset details
Class # Positive
Earnings 3964 (36.7%)
Acquisitions 2369 (22.0%)
Foreign 717 (6.6%)
Grain 582 (5.4%)
Crude 578 (5.4%)
Trade 485 (4.5%)
Interest 478 (4.4%)
Shipping 286 (2.7%)
Wheat 283 (2.6%)
Corn 237 (2.2%)
Table 2: Aptemod dataset details
frequent classes, with varying degrees of posi-
tive/negative skew. Punctuation and stopwords
were removed during preprocessing. The Apte-
mod data set contained 33,504 unique words and
a total of 733,266 word tokens. Details of the
classes can be found in Table 2.
4.1.3 Sentiment Classification Data
In the domain of Sentiment Classification, we
tested on the Amazon dataset from (Blitzer et al,
2007). Stopwords listed in an included file were
ignored for our experiments and we only the con-
sidered unigram features. Unlike the two Reuters
data sets, each category had a unique set of doc-
uments of varying size. For our experiments, we
only used the 10 largest categories. Details of the
categories can be found in Table 3.
In the Amazon Sentiment Classification data
set, the task is to determine whether a review is
positive or negative based solely on the reviewer?s
submitted text. As such, the positive and negative
Class # Instances # Positive Vocabulary
Music 124362 113997 (91.67%) 419936
Books 54337 47767 (87.91%) 220275
Dvd 46088 39563 (85.84%) 217744
Electronics 20393 15918 (78.06%) 65535
Kitchen 18466 14595 (79.04%) 47180
Video 17389 15017 (86.36%) 106467
Toys 12636 10151 (80.33%) 37939
Apparel 8940 7642 (85.48%) 22326
Health 6507 5124 (78.75%) 24380
Sports 5358 4352 (81.22%) 24237
Table 3: Amazon dataset details346
labels are equally relevant. For our metrics, we
calculate the scores for both the positive and neg-
ative class and report the average of the two (in
contrast to the Reuters data sets, in which we only
report the scores for the positive class).
4.2 Comparison Methods
In addition to Multinomial Naive Bayes (discussed
in Section 3), we evaluate against a variety of
supervised and semi-supervised techniques from
previous work, which provide a representation of
the state of the art. Below, we detail the compar-
ison methods that we re-implemented for our ex-
periments.
4.2.1 NB + EM
We implemented a semi-supervised version of
Naive Bayes with Expectation Maximization,
based on (Nigam et al, 2000). We found that 15
iterations of EM was sufficient to ensure approxi-
mate convergence of the parameters.
We also experimented with different weighting
factors to assign to the unlabeled data. While per-
forming per-data-split cross-validation was com-
putationally prohibitive for NB+EM, we per-
formed experiments on one class from each data
set that revealed weighting unlabeled examples at
1/5 the weight of a labeled example performed
best. We found that our re-implementation of
NB+EM slightly outperformed published results
on a separate data set (Mann and McCallum,
2010), validating our design choices.
4.2.2 Logistic Regression
We implemented Logistic Regression using L2-
Normalization, finding this to outperform L1-
Normalized and non-normalized versions. The
strength of the normalization was selected for each
training data set of each size utilized in our exper-
iments.
The strength of the normalization in the logis-
tic regression required cross-validation, which we
limited to 20 values logarithmically spaced be-
tween 10?4 and 104. The optimal value was se-
lected based upon the best average F1 score over
the 10 folds. We selected a normalization param-
eter separately for each subset of the training data
during experimentation.
4.2.3 Label Propagation
For our large unlabeled data set sizes, we found
that a standard Label Propogation (LP) approach,
which considers propagating information between
all pairs of unlabeled examples, was not tractable.
We instead implemented a constrained version of
LP for comparison.
In our implementation, we limit the number of
edges in the propagation graph. Each node prop-
agates to only to its 10 nearest neighbors, where
distance is calculated as the cosine distance be-
tween the tf-idf representation of two documents.
We found the tf-idf weighting to improve perfor-
mance over that of simple cosine distance. Propa-
gation was run for 100 iterations or until the en-
tropy dropped below a predetermined threshold,
whichever occurred first. Even with these aggres-
sive constraints, Label Propagation was intractable
to execute on some of the larger data sets, so we
do not report LP results for the RCV1 dataset or
for the 5 largest Amazon categories.
4.2.4 SFE
We also re-implemented a version of the recent
Semi-supervised Frequency Estimate approach
(Su et al, 2011). SFE was found to outperform
MNB and NB+EM in previous work. Consis-
tent with our MNB implementation, we use Add-
1 Smoothing in our SFE calculations although its
use is not specifically mentioned in (Su et al,
2011).
SFE also augments multinomial Naive Bayes
with the frequency information P (w), although in
a manner distinct from MNB-FM. In particular,
SFE uses the equality P (+|w) = P (+, w)/P (w)
and estimates the rhs using P (w) computed over
all the unlabeled data, rather than using only la-
beled data as in standard MNB. The primary dis-
tinction between MNB-FM and SFE is that SFE
adjusts sparse estimates P (+, w) in the same way
as non-sparse estimates, whereas MNB-FM is de-
signed to adjust sparse estimates more than non-
sparse ones. Further, it can be shown that as P (w)
of a word w in the unlabeled data becomes larger
than that in the labeled data, SFE?s estimate of the
ratio P (w|+)/P (w|?) approaches one. Depend-
ing on the labeled data, such an estimate can be ar-
bitrarily inaccurate. MNB-FM does not have this
limitation.
4.3 Results
For each data set, we evaluate on 50 randomly
drawn training splits, each comprised of 1,000 ran-
domly selected documents. Each set included at
least one positive and one negative document. We347
Data Set MNB-FM SFE MNB NBEM LProp Logist.
Apte (10) 0.306 0.271 0.336 0.306 0.245 0.208
Apte (100) 0.554 0.389 0.222 0.203 0.263 0.330
Apte (1k) 0.729 0.614 0.452 0.321 0.267 0.702
Amzn (10) 0.542 0.524 0.508 0.475 0.470* 0.499
Amzn (100) 0.587 0.559 0.456 0.456 0.498* 0.542
Amzn (1k) 0.687 0.611 0.465 0.455 0.539* 0.713
RCV1 (10) 0.494 0.477 0.387 0.485 - 0.272
RCV1 (100) 0.677 0.613 0.337 0.470 - 0.518
RCV1 (1k) 0.772 0.735 0.408 0.491 - 0.774
* Limited to 5 of 10 Amazon categories
Table 4: F1, training size in parentheses
respected the order of the training splits such that
each sample was a strict subset of any larger train-
ing sample of the same split.
We evaluate on the standard metric of F1 with
respect to the target class. For Amazon, in which
both the ?positive? and ?negative? classes are po-
tential target classes, we evaluate using macro-
averaged scores.
The primary results of our experiments are
shown in Table 4. The results show that MNB-FM
improves upon the MNB classifier substantially,
and also tends to outperform the other SSL and
supervised learning methods we evaluated. MNB-
FM is the best performing method over all data
sets when the labeled data is limited to 10 and 100
documents, except for training sets of size 10 in
Aptemod, where MNB has a slight edge.
Tables 5 and 6 present detailed results of the
experiments on the RCV1 data set. These exper-
iments are limited to the 5 largest base classes
and show the F1 performance of MNB-FM and
the various comparison methods, excluding Label
Propagation which was intractable on this data set.
Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.641 0.643 0.580 0.639 0.532
GCAT 0.639 0.686 0.531 0.732 0.466
MCAT 0.572 0.505 0.393 0.504 0.225
ECAT 0.306 0.267 0.198 0.224 0.096
GPOL 0.313 0.283 0.233 0.326 0.043
Average 0.494 0.477 0.387 0.485 0.272
Table 5: RCV1: F1, |DL|= 10
Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.797 0.793 0.624 0.713 0.754
GCAT 0.849 0.848 0.731 0.837 0.831
MCAT 0.776 0.737 0.313 0.516 0.689
ECAT 0.463 0.317 0.017 0.193 0.203
GPOL 0.499 0.370 0.002 0.089 0.114
Average 0.677 0.613 0.337 0.470 0.518
Table 6: RCV1: F1, |DL|= 100
Method 1000 5000 10k 50k 100k
MNB-FM 1.44 1.61 1.69 2.47 5.50
NB+EM 2.95 3.43 4.93 10.07 16.90
MNB 1.15 1.260 1.40 2.20 3.61
Labelprop 0.26 4.17 10.62 67.58 -
Table 7: Runtimes of SSL methods (sec.)
The runtimes of our methods can be seen in Ta-
ble 7. The results show the runtimes of the SSL
methods discussed in this paper as the size of the
unlabeled dataset grows. As expected, we find that
MNB-FM has runtime similar to MNB, and scales
much better than methods that take multiple passes
over the unlabeled data.
5 Analysis
From our experiments, it is clear that the perfor-
mance of MNB-FM improves on MNB, and in
many cases outperforms all existing SSL algo-
rithms we evaluated. MNB-FM improves the con-
ditional probability estimates in MNB and, sur-
prisingly, we found that it can often improve these
estimates for words that do not even occur in the
training set.
Tables 8 and 9 show the details of the improve-
ments MNB-FM makes on the feature marginal
estimates. We ran MNB-FM and MNB on the
RCV1 class MCAT and stored the computed fea-
ture marginals for direct comparison. For each
word in the vocabulary, we compared each clas-
sifier?s conditional probability ratios, i.e. ?+/??,
to the true value over the entire data set. We com-
puted which classifier was closer to the correct ra-
tio for each word. These results were averaged
over 5 iterations. From the data, we can see that
MNB-FM improves the estimates for many words
not seen in the training set as well as the most com-
mon words, even with small training sets.
5.1 Ranking Performance
We also analyzed how well the different meth-
ods rank, rather than classify, the test documents.
We evaluated ranking using the R-precision met-
ric, equal to the precision (i.e. fraction of positive
documents classified correctly) of the R highest-
ranked test documents, where R is the total num-
ber of positive test documents.
Logistic Regression performed particularly well
on the R-Precision Metric, as can be seen in Tables
10, 11, and 12. Logistic Regression performed
less well in the F1 metric. We find that NB+EM348
Fraction Improved vs MNB Avg Improvement vs MNB Probability Mass
Word Freq. Known Half Known Unknown Known Half Known Unknown Known Half Known Unknown
0-10?6 - 0.165 0.847 - -0.805 0.349 - 0.02% 7.69%
10?6-10?5 0.200 0.303 0.674 0.229 -0.539 0.131 0.00% 0.54% 14.77%
10?5-10?4 0.322 0.348 0.592 -0.597 -0.424 0.025 0.74% 10.57% 32.42%
10?4-10?3 0.533 0.564 0.433 0.014 0.083 -0.155 7.94% 17.93% 7.39%
> 10?3 - - - - - - - - -
Table 8: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|DL| = 10). ?Known?
indicates words occurring in both positive and negative training examples, ?Half Known? indicates words
occurring in only positive or negative training examples, while ?Unknown? indicates words that never
occur in labelled examples. Data is for the RCV1 MCAT category. MNB-FM improves estimates by a
substantial amount for unknown words and also the most common known and half-known words.
Fraction Improved vs MNB Avg Improvement vs MNB Probability Mass
Word Freq. Known Half Known Unknown Known Half Known Unknown Known Half Known Unknown
0-10?6 0.567 0.243 0.853 0.085 -0.347 0.143 0.00% 0.22% 7.49%
10?6-10?5 0.375 0.310 0.719 -0.213 -0.260 0.087 0.38% 4.43% 10.50%
10?5-10?4 0.493 0.426 0.672 -0.071 -0.139 0.067 18.68% 20.37% 4.67%
10?4-10?3 0.728 0.669 - 0.233 0.018 - 31.70% 1.56% -
> 10?3 - - - - - - - - -
Table 9: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|DL| = 100). Data is
for the RCV1 MCAT category (see Table 8). MNB-FM improves estimates by a substantial amount for
unknown words and also the most common known and half-known words.
performs particularly well on the R-precision met-
ric on ApteMod, suggesting that its modelling as-
sumptions are more accurate for that particular
data set (NB+EM performs significantly worse on
the other data sets, however). MNB-FM performs
essentially equivalently well, on average, to the
best competing method (Logistic Regression) on
the large RCV1 data set. However, these experi-
ments show that MNB-FM offers more advantages
in document classification than in document rank-
ing.
The ranking results show that LR may be pre-
ferred when ranking is important. However, LR
underperforms in classification tasks (in terms of
F1, Tables 4-6). The reason for this is that LR?s
learned classification threshold becomes less accu-
rate when datasets are small and classes are highly
Class MNB-FM SFE MNB NBEM LProp Logist.
Apte (10) 0.353 0.304 0.359 0.631 0.490 0.416
Apte (100) 0.555 0.421 0.343 0.881 0.630 0.609
Apte (1k) 0.723 0.652 0.532 0.829 0.754 0.795
Amzn (10) 0.536 0.527 0.516 0.481 0.535* 0.544
Amzn (100) 0.614 0.562 0.517 0.480 0.573* 0.639
Amzn (1k) 0.717 0.650 0.562 0.483 0.639* 0.757
RCV1 (10) 0.505 0.480 0.421 0.450 - 0.512
RCV1 (100) 0.683 0.614 0.474 0.422 - 0.689
RCV1 (1k) 0.781 0.748 0.535 0.454 - 0.802
* Limited to 5 of 10 Amazon categories
Table 10: R-Precision, training size in parentheses
skewed. In these cases, LR classifies too fre-
quently in favor of the larger class which is detri-
mental to its performance. This effect is visible
in Tables 5 and 6, where LR?s performance sig-
nificantly drops for the ECAT and GPOL classes.
ECAT and GPOL represent only 14.91% and
7.07% of the RCV1 dataset, respectively.
6 Related Work
To our knowledge, MNB-FM is the first approach
that utilizes a small set of statistics computed over
Data SetMNB-FM SFE MNB NBEM Logist.
CCAT 0.637 0.631 0.620 0.498 0.653
GCAT 0.663 0.711 0.600 0.792 0.671
MCAT 0.580 0.492 0.477 0.510 0.596
ECAT 0.291 0.217 0.214 0.111 0.297
GPOL 0.354 0.352 0.193 0.341 0.341
Average 0.505 0.480 0.421 0.450 0.512
Table 11: RCV1: R-Precision, DL= 10
Class MNB-FM SFE MNB NBEM Logist.
CCAT 0.805 0.797 0.765 0.533 0.809
GCAT 0.849 0.858 0.780 0.869 0.843
MCAT 0.782 0.753 0.579 0.533 0.774
ECAT 0.471 0.293 0.203 0.119 0.498
GPOL 0.509 0.370 0.042 0.056 0.520
Average 0.683 0.614 0.474 0.422 0.689
Table 12: RCV1: R-Precision, DL= 100349
a large unlabeled data set as constraints to im-
prove a semi-supervised classifier. Our exper-
iments demonstrate that MNB-FM outperforms
previous approaches across multiple text classi-
fication techniques including topic classification
and sentiment analysis. Further, the MNB-FM ap-
proach offers scalability advantages over most ex-
isting semi-supervised approaches.
Current popular Semi-Supervised Learning ap-
proaches include using Expectation-Maximization
on probabilistic models (e.g. (Nigam et al,
2000)); Transductive Support Vector Machines
(Joachims, 1999); and graph-based methods such
as Label Propagation (LP) (Zhu and Ghahramani,
2002) and their more recent, more scalable vari-
ants (e.g. identifying a small number of represen-
tative unlabeled examples (Liu et al, 2010)). In
general, these techniques require passes over the
entirety of the unlabeled data for each new learn-
ing task, intractable for massive unlabeled data
sets. Naive implementations of LP cannot scale
to large unlabeled data sets, as they have time
complexity that increases quadratically with the
number of unlabeled examples. Recent LP tech-
niques have achieved greater scalability through
the use of parallel processing and heuristics such
as Approximate-Nearest Neighbor (Subramanya
and Bilmes, 2009), or by decomposing the sim-
ilarity matrix (Lin and Cohen, 2011). Our ap-
proach, by contrast, is to pre-compute a small
set of marginal statistics over the unlabeled data,
which eliminates the need to scan unlabeled data
for each new task. Instead, the complexity of
MNB-FM is proportional only to the number of
unique words in the labeled data set.
In recent work, Su et al propose the Semi-
supervised Frequency Estimate (SFE), which like
MNB-FM utilizes the marginal probabilities of
features computed from unlabeled data to im-
prove the Multinomial Naive Bayes (MNB) clas-
sifier (Su et al, 2011). SFE has the same scal-
ability advantages as MNB-FM. However, unlike
our approach, SFE does not compute maximum-
likelihood estimates using the marginal statistics
as a constraint. Our experiments show that MNB-
FM substantially outperforms SFE.
A distinct method for pre-processing unlabeled
data in order to help scale semi-supervised learn-
ing techniques involves dimensionality reduction
or manifold learning (Belkin and Niyogi, 2004),
and for NLP tasks, identifying word representa-
tions from unlabeled data (Turian et al, 2010). In
contrast to these approaches, MNB-FM preserves
the original feature set and is more scalable (the
marginal statistics can be computed in a single
pass over the unlabeled data set).
7 Conclusion
We presented a novel algorithm for efficiently
leveraging large unlabeled data sets for semi-
supervised learning. Our MNB-FM technique op-
timizes a Multinomial Naive Bayes model to ac-
cord with statistics of the unlabeled corpus. In ex-
periments across topic classification and sentiment
analysis, MNB-FM was found to be more accu-
rate and more scalable than several supervised and
semi-supervised baselines from previous work.
In future work, we plan to explore utilizing
richer statistics from the unlabeled data, beyond
word marginals. Further, we plan to experiment
with techniques for unlabeled data sets that also
include continuous-valued features. Lastly, we
also wish to explore ensemble approaches that
combine the best supervised classifiers with the
improved class-conditional estimates provided by
MNB-FM.
8 Acknowledgements
This work was supported in part by DARPA con-
tract D11AP00268.
References
Mikhail Belkin and Partha Niyogi. 2004. Semi-
supervised learning on riemannian manifolds. Ma-
chine Learning, 56(1):209?239.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Association for Computational Linguis-
tics, Prague, Czech Republic.
O. Chapelle, B. Scho?lkopf, and A. Zien, editors. 2006.
Semi-Supervised Learning. MIT Press, Cambridge,
MA.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the Sixteenth International Confer-
ence on Machine Learning, ICML ?99, pages 200?
209, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. The Journal of Ma-
chine Learning Research, 5:361?397.350
Frank Lin and William W Cohen. 2011. Adaptation
of graph-based semi-supervised methods to large-
scale text data. In The 9th Workshop on Mining and
Learning with Graphs.
Wei Liu, Junfeng He, and Shih-Fu Chang. 2010. Large
graph construction for scalable semi-supervised
learning. In ICML, pages 679?686.
Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. J. Mach. Learn.
Res., 11:955?984, March.
Kamal Nigam, Andrew Kachites McCallum, Sebastian
Thrun, and Tom Mitchell. 2000. Text classifica-
tion from labeled and unlabeled documents using
em. Mach. Learn., 39(2-3):103?134, May.
Jiang Su, Jelber Sayyad Shirab, and Stan Matwin.
2011. Large scale text classification using semisu-
pervised multinomial naive bayes. In Lise Getoor
and Tobias Scheffer, editors, ICML, pages 97?104.
Omnipress.
Amar Subramanya and Jeff A. Bilmes. 2009. En-
tropic graph regularization in non-parametric semi-
supervised classification. In Neural Information
Processing Society (NIPS), Vancouver, Canada, De-
cember.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Urbana, 51:61801.
Yiming Yang and Xin Liu. 1999. A re-examination
of text categorization methods. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 42?49. ACM.
X. Zhu and Z. Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation.
Technical report, Technical Report CMU-CALD-
02-107, Carnegie Mellon University.
Xiaojin Zhu. 2006. Semi-supervised learning litera-
ture survey.
351
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 125?134,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Language Models as Representations for Weakly-Supervised NLP Tasks
Fei Huang and Alexander Yates
Temple University
Broad St. and Montgomery Ave.
Philadelphia, PA 19122
fei.huang@temple.edu
yates@temple.edu
Arun Ahuja and Doug Downey
Northwestern University
2133 Sheridan Road
Evanston, IL 60208
a-ahuja@northwestern.edu
ddowney@eecs.northwestern.edu
Abstract
Finding the right representation for words is
critical for building accurate NLP systems
when domain-specific labeled data for the
task is scarce. This paper investigates lan-
guage model representations, in which lan-
guage models trained on unlabeled corpora
are used to generate real-valued feature vec-
tors for words. We investigate ngram mod-
els and probabilistic graphical models, includ-
ing a novel lattice-structured Markov Random
Field. Experiments indicate that language
model representations outperform traditional
representations, and that graphical model rep-
resentations outperform ngram models, espe-
cially on sparse and polysemous words.
1 Introduction
NLP systems often rely on hand-crafted, carefully
engineered sets of features to achieve strong perfor-
mance. Thus, a part-of-speech (POS) tagger would
traditionally use a feature like, ?the previous token
is the? to help classify a given token as a noun or
adjective. For supervised NLP tasks with sufficient
domain-specific training data, these traditional fea-
tures yield state-of-the-art results. However, NLP
systems are increasingly being applied to texts like
the Web, scientific domains, and personal commu-
nications like emails, all of which have very differ-
ent characteristics from traditional training corpora.
Collecting labeled training data for each new target
domain is typically prohibitively expensive. We in-
vestigate representations that can be applied when
domain-specific labeled training data is scarce.
An increasing body of theoretical and empirical
evidence suggests that traditional, manually-crafted
features limit systems? performance in this setting
for two reasons. First, feature sparsity prevents sys-
tems from generalizing accurately to words and fea-
tures not seen during training. Because word fre-
quencies are Zipf distributed, this often means that
there is little relevant training data for a substantial
fraction of parameters (Bikel, 2004), especially in
new domains (Huang and Yates, 2009). For exam-
ple, word-type features form the backbone of most
POS-tagging systems, but types like ?gene? and
?pathway? show up frequently in biomedical liter-
ature, and rarely in newswire text. Thus, a classifier
trained on newswire data and tested on biomedical
data will have seen few training examples related to
sentences with features ?gene? and ?pathway? (Ben-
David et al, 2009; Blitzer et al, 2006).
Further, because words are polysemous, word-
type features prevent systems from generalizing to
situations in which words have different meanings.
For instance, the word type ?signaling? appears pri-
marily as a present participle (VBG) in Wall Street
Journal (WSJ) text, as in, ?Interest rates rose, sig-
naling that . . . ? (Marcus et al, 1993). In biomedical
text, however, ?signaling? appears primarily in the
phrase ?signaling pathway,? where it is considered
a noun (NN) (PennBioIE, 2005); this phrase never
appears in the WSJ portion of the Penn Treebank
(Huang and Yates, 2010a).
Our response to these problems with traditional
NLP representations is to seek new representations
that allow systems to generalize more accurately to
previously unseen examples. Our approach depends
on the well-known distributional hypothesis, which
states that a word?s meaning is identified with the
contexts in which it appears (Harris, 1954; Hin-
dle, 1990). Our goal is to develop probabilistic lan-
125
guage models that describe the contexts of individ-
ual words accurately. We then construct represen-
tations, or mappings from word tokens and types
to real-valued vectors, from these language models.
Since the language models are designed to model
words? contexts, the features they produce can be
used to combat problems with polysemy. And by
careful design of the language models, we can limit
the number of features that they produce, controlling
how sparse those features are in training data.
In this paper, we analyze the performance
of language-model-based representations on tasks
where domain-specific training data is scarce. Our
contributions are as follows:
1. We introduce a novel factorial graphical model
representation, a Partial-Lattice Markov Random
Field (PL-MRF), which is a tractable variation of
a Factorial Hidden Markov Model (HMM) for lan-
guage modeling.
2. In experiments on POS tagging in a domain adap-
tation setting and on weakly-supervised informa-
tion extraction (IE), we quantify the performance of
representations derived from language models. We
show that graphical models outperform ngram rep-
resentations. The PL-MRF representation achieves a
state-of-the-art 93.8% accuracy on the POS tagging
task, while the HMM representation improves over
the ngram model by 10% on the IE task.
3. We analyze how the performance of the different
representations varies due to the fundamental chal-
lenges of sparsity and polysemy.
The next section discusses previous work. Sec-
tions 3 and 4 present the existing representations we
investigate and the new PL-MRF, respectively. Sec-
tions 5 and 6 describe our two tasks and the results
of using our representations on each of them. Sec-
tion 7 concludes.
2 Previous Work
There is a long tradition of NLP research on rep-
resentations, mostly falling into one of four cate-
gories: 1) vector space models of meaning based
on document-level lexical cooccurrence statistics
(Salton and McGill, 1983; Turney and Pantel, 2010;
Sahlgren, 2006); 2) dimensionality reduction tech-
niques for vector space models (Deerwester et al,
1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005;
Blei et al, 2003; Va?yrynen et al, 2007); 3) using
clusters that are induced from distributional similar-
ity (Brown et al, 1992; Pereira et al, 1993; Mar-
tin et al, 1998) as non-sparse features (Lin and Wu,
2009; Candito and Crabbe, 2009; Koo et al, 2008;
Zhao et al, 2009); 4) and recently, language models
(Bengio, 2008; Mnih and Hinton, 2009) as represen-
tations (Weston et al, 2008; Collobert and Weston,
2008; Bengio et al, 2009), some of which have al-
ready yielded state of the art performance on domain
adaptation tasks (Huang and Yates, 2009; Huang and
Yates, 2010a; Huang and Yates, 2010b; Turian et al,
2010) and IE (Ahuja and Downey, 2010; Downey et
al., 2007b). In contrast to this previous work, we de-
velop a novel Partial Lattice MRF language model
that incorporates a factorial representation of latent
states, and demonstrate that it outperforms the pre-
vious state-of-the-art in POS tagging in a domain
adaptation setting. We also analyze the novel PL-
MRF representation on an IE task, and several repre-
sentations along the key dimensions of sparsity and
polysemy.
Most previous work on domain adaptation has fo-
cused on the case where some labeled data is avail-
able in both the source and target domains (Daume?
III, 2007; Jiang and Zhai, 2007; Daume? III and
Marcu, 2006; Finkel and Manning, 2009; Dredze
et al, 2010; Dredze and Crammer, 2008). Learn-
ing bounds are known (Blitzer et al, 2007; Man-
sour et al, 2009). Daume? III et al (2010) use semi-
supervised learning to incorporate labeled and unla-
beled data from the target domain. In contrast, we
investigate a domain adaptation setting where no la-
beled data is available for the target domain.
3 Representations
A representation is a set of features that describe
instances for a classifier. Formally, let X be an
instance set, and let Z be the set of labels for a
classification task. A representation is a function
R : X ? Y for some suitable feature space Y (such
as Rd). We refer to dimensions of Y as features, and
for an instance x ? X we refer to values for partic-
ular dimensions of R(x) as features of x.
3.1 Traditional POS-Tagging Representations
As a baseline for POS tagging experiments and an
example of our terminology, we describe a repre-
sentation used in traditional supervised POS taggers.
The instance set X is the set of English sentences,
and Z is the set of POS tag sequences. A traditional
representation TRAD-R maps a sentence x ? X to a
sequence of boolean-valued vectors, one vector per
126
Representation Feature
TRAD-R ?w1[xi = w]
?s?Suffixes1[xi ends with s]
1[xi contains a digit]
NGRAM-R ?w?,w??P (w?ww??)/P (w)
HMM-TOKEN-R ?k1[yi? = k]
HMM-TYPE-R ?kP (y = k|x = w)
I-HMM-TOKEN-R ?j,k1[yi,j? = k]
BROWN-TOKEN-R ?j?{?2,?1,0,1,2}
?p?{4,6,10,20} prefix(yi+j , p)
BROWN-TYPE-R ?p prefix(y, p)
LATTICE-TOKEN-R ?j,k1[yi,j? = k]
LATTICE-TYPE-R ?kP (y = k|x = w)
Table 1: Summary of features provided by our repre-
sentations. ?a1[g(a)] represents a set of boolean fea-
tures, one for each value of a, where the feature is
true iff g(a) is true. xi represents a token at position
i in sentence x, w represents a word type, Suffixes =
{-ing,-ogy,-ed,-s,-ly,-ion,-tion,-ity}, k (and k) represents
a value for a latent state (set of latent states) in a latent-
variable model, y? represents the optimal setting of latent
states y for x, yi is the latent variable for xi, and yi,j is
the latent variable for xi at layer j. prefix(y,p) is the p-
length prefix of the Brown cluster y.
word xi in the sentence. Dimensions for each latent
vector include indicators for the word type of xi and
various orthographic features. Table 1 presents the
full list of features in TRAD-R. Since our IE task
classifies word types rather than tokens, this base-
line is not appropriate for that task. Below, we de-
scribe how we can learn representations R by using
a variety of language models, for use in both our IE
and POS tagging tasks. All representations for POS
tagging inherit the features from TRAD-R; all repre-
sentations for IE do not.
3.2 Ngram Representations
N-gram representations model a word type w in
terms of the n-gram contexts in which w appears
in a corpus. Specifically, for word w we generate
the vector P (w?ww??)/P (w), the conditional prob-
ability of observing the word sequence w? to the left
and w?? to the right of w. The experimental section
describes the particular corpora and language mod-
eling methods used for estimating probabilities.
3.3 HMM-based Representations
In previous work, we have implemented several
representations based on HMMs (Rabiner, 1989),
which we used for both POS tagging (Huang and
Yates, 2009) and IE (Downey et al, 2007b). An
HMM is a generative probabilistic model that gen-
erates each word xi in the corpus conditioned on a
latent variable yi. Each yi in the model takes on in-
tegral values from 1 to K, and each one is generated
by the latent variable for the preceding word, yi?1.
The joint distribution for a corpus x = (x1, . . . , xN )
and a set of state vectors y = (y1, . . . , yN ) is
given by: P (x,y) = ?i P (xi|yi)P (yi|yi?1). Us-
ing Expectation-Maximization (EM) (Dempster et
al., 1977), it is possible to estimate the distributions
for P (xi|yi) and P (yi|yi?1) from unlabeled data.
We construct two different representations from
HMMs, one for POS tagging and one for IE. For
POS tagging, we use the Viterbi algorithm to pro-
duce the optimal setting y? of the latent states for a
given sentence x, or y? = arg maxy P (x,y). We
use the value of yi? as a new feature for xi that repre-
sents a cluster of distributionally-similar words. For
IE, we require features for word types w, rather than
tokens xi. We use the K-dimensional vector that
represents the distribution P (y|x = w) as the fea-
ture vector for word type w. This set of features
represents a ?soft clustering? of w into K different
clusters. We refer to these representations as HMM-
TOKEN-R and HMM-TYPE-R, respectively.
Because HMM-based representations offer a
small number of discrete states as features, they have
a much greater potential to combat feature sparsity
than do ngram models. Furthermore, for token-
based representations, these models can potentially
handle polysemy better than ngram language models
by providing different features in different contexts.
We also compare against a variation of the HMM
from our previous work (Huang and Yates, 2010a),
henceforth HY10. This model independently trains
M separate HMM models on the same corpus, ini-
tializing each one randomly. We can then use the
Viterbi-optimal decoded latent state of each inde-
pendent HMM model as a separate feature for a to-
ken. We refer to this language model as an I-HMM,
and the representation as I-HMM-TOKEN-R.
Finally, we compare against Brown clusters
(Brown et al, 1992) as learned features. Although
not traditionally described as such, Brown cluster-
ing involves constructing an HMM model in which
127
each type is restricted to having exactly one latent
state that may generate it. Brown et al describe a
greedy agglomerative clustering algorithm for train-
ing this model on unlabeled text. Following Turian
et al (2010), we use Percy Liang?s implementation
of this algorithm for our comparison, and we test
runs with 100, 320, and 1000 clusters. We use fea-
tures from these clusters identical to Turian et al?s.1
Turian et al have shown that Brown clusters match
or exceed the performance of neural network-based
language models in domain adaptation experiments
for named-entity recognition, as well as in-domain
experiments for NER and chunking.
4 A Novel Lattice Language Model
Representation
Our final language model is a novel latent-variable
language model with rich latent structure, shown in
Figure 1. The model contains a lattice of M ?N la-
tent states, where N is the number of words in a sen-
tence and M is the number of layers in the model.
We can justify the choice of this model from a lin-
guistic perspective as a way to capture the multi-
dimensional nature of words. Linguists have long
argued that words have many different features in a
high dimensional space: they can be separately de-
scribed by part of speech, gender, number, case, per-
son, tense, voice, aspect, mass vs. count, and a host
of semantic categories (agency, animate vs. inani-
mate, physical vs. abstract, etc.), to name a few (Sag
et al, 2003). Our model seeks to capture a multi-
dimensional representation of words by creating a
separate layer of latent variables for each dimension.
The values of the M layers of latent variables for a
single word can be used as M distinct features in
our representation. The I-HMM attempts to model
the same intuition, but unlike a lattice model the I-
HMM layers are entirely independent, and as a re-
sult there is no mechanism to enforce that the layers
model different dimensions. Duh (2005) previously
used a 2-layer lattice for tagging and chunking, but
in a supervised setting rather than for representation
learning.
Let Cliq(x,y) represent the set of all maximal
cliques in the graph of the MRF model for x and y.
1Percy Liang?s implementation is available at
http://metaoptimize.com/projects/wordreprs/. Turian et al
also tested a run with 3200 clusters in their experiments, which
we have been training for months, but which has not finished in
time for publication.
y4,1
y3,1
y
y4,2
y3,2
y
y4,3
y3,3
y
y4,4
y3,4
y
y4,5
y3,5
y
x1
2,1
y1,1
x2
2,2
y1,2
x3
2,3
y1,3
x4
2,4
y1,4
x5
2,5
y1,5
Figure 1: The Partial Lattice MRF (PL-MRF) Model for a
5-word sentence and a 4-layer lattice. Dashed gray edges
are part of a full lattice, but not the PL-MRF.
Expressing the lattice model in log-linear form, we
can write the marginal probability P (x) of a given
sentence x as:
?
y
?
c?Cliq(x,y) score(c,x,y)
?
x?,y?
?
c?Cliq(x?,y?) score(c,x?,y?)
where score(c,x,y) = exp(?c ? fc(xc,yc)). Our
model includes parameters for transitions between
two adjacent latent variables on layer j: ?transi,s,i+1,s?,j
for yi,j = s and yi+1,j = s?. It also includes obser-
vation parameters for latent variables and tokens, as
well as for pairs of adjacent latent variables in differ-
ent layers and their tokens: ?obsi,j,s,w and ?obsi,j,s,j+1,s?,w
for yi,j = s, yi,j+1 = s?, and xi = w.
Computationally, the lattice MRF is preferable to
a na??ve Factorial HMM (Ghahramani and Jordan,
1997) representation, which would require O(2M )
parameters for an M -layer model. However, ex-
act training and inference in supervised settings are
still intractable for this model (Sutton et al, 2007),
and thus it has not yet been explored as a language
model, which requires even more difficult, unsuper-
vised training. Training is intractable in part because
of the difficulty in enumerating and summing over
the exponentially-many configurations y for a given
x. We address this difficulty in two ways: by modi-
fying the model, and by modifying the training pro-
cedure.
4.1 Partial Lattice MRF
Instead of the full lattice model, we construct a
Partial Lattice MRF (PL-MRF) model by deleting
128
certain edges between latent layers of the model
(dashed gray edges in Figure 1). Let c = bN2 c,
where N is the length of the sentence. If i < c
and j is odd, or if j is even and i > c, we delete
edges between yi,j and yi,j+1. The same lattice of
nodes remains, but fewer edges and paths. A cen-
tral ?trunk? at i = c connects all layers of the lat-
tice, and branches from this trunk connect either to
the branches in the layer above or the layer below
(but not both). The result is a model that retains
most2 of the edges of the full model. Additionally,
the pruned model makes the branches conditionally
independent from one another, except through the
trunk. For instance, the right branch at layers 1
and 2 in Figure 1 (y1,4, y1,5, y2,4, and y2,5) are dis-
connected from the right branch at layers 3 and 4
(y3,4, y3,5, y4,4, and y4,5), except through the trunk
and the observed nodes. As a result, excluding the
observed nodes, this model has a low tree-width of
2 (excluding observed nodes), and a variety of ef-
ficient dynamic programming and message-passing
algorithms for training and inference can be readily
applied (Bodlaender, 1988).3 Our inference algo-
rithm passes information from the branches inwards
to the trunk, and then upward along the trunk, in
time O(K4MN).
As with our HMM models, we create two repre-
sentations from PL-MRFs, one for tokens and one
for types. For tokens, we decode the model to com-
pute y?, the matrix of optimal latent state values for
sentence x. For each layer j and and each possi-
ble latent state value k, we add a boolean feature
for token xi that is true iff y?i,j = k. For types,
we compute distributions over the latent state space.
Let y be the column vector of latent variables for
word x. For each possible configuration of values k
of the latent variables y, we add a real-valued fea-
tures for x given by P (y = k|x = w). We refer
to these two representations as LATTICE-TOKEN-R
and LATTICE-TYPE-R, respectively.
4.2 Parameter Estimation
We train the PL-MRF using contrastive estimation,
which iteratively optimizes the following objective
function on a corpus X:
?
x?X
log
?
y
?
c?Cliq(x,y) score(c,x,y)
?
x??N (x),y?
?
c?Cliq(x?,y?) score(c,x?,y?)
2As M, N ??, 5 out of every 6 edges are kept.
3c.f. a tree-width of min(M ,N ) for the unpruned model
where N (x), the neighborhood of x, indicates a
set of perturbed variations of the original sentence
x. Contrastive estimation seeks to move probability
mass away from the perturbed neighborhood sen-
tences and onto the original sentence. We use a
neighborhood function that includes all sentences
which can be obtained from the original sentence by
swapping the order of a consecutive pair of words.
Training uses gradient descent over this non-convex
objective function with a standard software package
(Liu and Nocedal, 1989) and converges to a local
maximum (Smith and Eisner, 2005).
For tractability, we modify the training procedure
to train the PL-MRF one layer at a time. Let ?i rep-
resent the set of parameters relating to features of
layer i, and let ??i represent all other parameters.
We fix ??0 = 0, and optimize ?0 using contrastive
estimation. After convergence, we fix ??1, and opti-
mize ?1, and so on. We use a convergence threshold
of 10?6, and each layer typically converges in under
100 iterations.
5 Domain Adaptation for a POS Tagger
We evaluate the representations described above on
a POS tagging task in a domain adaptation setting.
5.1 Experimental Setup
We use the same experimental setup as in HY10:
the Penn Treebank (Marcus et al, 1993) Wall Street
Journal portion for our labeled training data; 561
MEDLINE sentences (9576 types, 14554 tokens,
23% OOV tokens) from the Penn BioIE project
(PennBioIE, 2005) for our labeled test set; and all of
the unlabeled text from the Penn Treebank WSJ por-
tion plus a MEDLINE corpus of 71,306 unlabeled
sentences to train our language models. The two
texts come from two very different domains, mak-
ing this data a tough test for domain adaptation.
We use an open source Conditional Random Field
(CRF) (Lafferty et al, 2001) software package4 de-
signed by Sunita Sarawagi and William W. Cohen
to implement our supervised models. Let X be a
training corpus, Z the corresponding labels, and R
a representation function. For each token xi in X,
we include a parameter in our CRF model for all
features R(xi) and all possible labels in Z. Further-
more, we include transition parameters for pairs of
consecutive labels zi, zi+1.
4Available from http://sourceforge.net/projects/crf/
129
For representations, we tested TRAD-R,
NGRAM-R, HMM-TOKEN-R, I-HMM-TOKEN-R
(between 2 and 8 layers), and LATTICE-TOKEN-R
(8, 12, 16, and 20 layers). Following HY10, each
latent node in the I-HMMs have 80 possible values,
creating 808 ? 1015 possible configurations of the
8-layer I-HMM for a single word. Each node in
the PL-MRF is binary, creating a much smaller
number (220 ? 106) of possible configurations for
each word in a 20-layer representation. NGRAM-R
was trained using an unsmoothed trigram model on
the Web 1Tgram corpus. To keep the feature set
manageable, we included the top 500 most common
ngrams for each word type, and then used mutual
information on the training data to select the top
10,000 most relevant ngram features for all word
types. We incorporated ngram features as binary
values indicating whether xi appeared with the
ngram or not. We also report on the performance
of Brown clusters and Blitzer et al?s Structural
Correspondence Learning (SCL) (2006) technique,
which uses manually-selected ?pivot? words (like
?of?, ?the?) to learn domain-independent features.
Finally, we compare against the self-training CRF
technique from HY10.
5.2 Results and Discussion
For each representation, we measured the accuracy
of the POS tagger on the biomedical test text. Ta-
ble 2 shows the results for the best variation of each
kind of model ? 20 layers for the PL-MRF, 7 lay-
ers for the I-HMM, and 1000 clusters for the Brown
clustering. All language model representations sig-
nificantly outperform the SCL model and the TRAD-
R baseline. The novel PL-MRF model outperforms
the previous state of the art, the I-HMM model, and
much of the performance increase comes from a
11.3% relative reduction in error on words that ap-
pear in biomedical texts but not in newswire texts.
Both graphical model representations significantly
outperform the ngram model, which is trained on far
more text. For comparison, our best model, the PL-
MRF, achieved a 96.8% in-domain accuracy on sec-
tions 22-24 of the Penn Treebank, about 0.5% shy
of a state-of-the-art in-domain system (Shen et al,
2007) with more sophisticated supervised learning.
We expected that language model representations
perform well in part because they provide meaning-
ful features for sparse and polysemous words. To
test this, we selected 109 polysemous word types
model % error OOV % error
TRAD-R 11.7 32.7
TRAD-R+self-training 11.5 29.6
SCL 11.1 -
BROWN-TOKEN-R 10.8 25.4
HMM-TOKEN-R 9.5 24.8
NGRAM-R 6.9 24.4
I-HMM-TOKEN-R 6.7 24
LATTICE-TOKEN-R 6.2 21.3
SCL+500bio 3.9 -
Table 2: PL-MRF representations reduce error by 7.5%
relative to the previous state-of-the-art I-HMM, and ap-
proach within 2.3% absolute error a SCL+500bio model
with access to 500 labeled sentences from the target do-
main. 1.8% of the tags in the test set are new tags that
do not occur in the WSJ training data, so an error rate of
3.9+1.8 = 5.7% error is a reasonable bound for the best
possible performance of a model that has seen no exam-
ples from the target domain.
from our test data, along with 296 non-polysemous
word types, chosen based on POS tags and manual
inspection. We further define sparse word types as
those that appear 5 times or fewer in all of our unla-
beled data, and non-sparse word types as those that
appear at least 50 times in our unlabeled data. Table
3 shows results on these subsets of the data.
As expected, all of our language models outper-
form the baseline by a larger margin on polysemous
words than on non-polysemous words. The mar-
gin between graphical model representations and the
ngram model also increases on polysemous words,
presumably because the Viterbi decoding of these
models takes into account the tokens in the sur-
rounding sentence. The same behavior is evident for
sparsity: all of the language model representations
outperform the baseline by a larger margin on sparse
words than not-sparse words, and all of the graphical
models perform better relative to the ngram model
on sparse words as well. Thus representations based
on graphical models address two key issues in build-
ing representations for POS tagging.
6 Information Extraction Experiments
In this section, we evaluate our learned representa-
tions on a different task that investigates the abil-
ity of each representation to capture semantic, rather
than syntactic, information. Specifically, we inves-
130
POS Tagging Information Extraction
polys. not polys. sparse not sparse polys. not polys. sparse not sparse
tokens/types 159 4321 463 12194 222 210 266 166
categories - - - - 12 4 13 3
TRAD-R 59.5 78.5 52.5 89.6 - - - -
Ngram 68.2 85.3 61.8 94.0 0.07 0.17 0.06 0.25
HMM 67.9 83.4 60.2 91.6 0.14 0.26 0.15 0.32
(-Ngram) (-0.3) (-1.9) (-1.6) (-2.4) (+0.07) (+0.09) (+0.09) (+0.07)
I-HMM 75.6 85.2 62.9 94.5 - - - -
(-Ngram) (+7.4) (-0.1) (+1.1) (+0.5) - - - -
PL-MRF 70.5 86.9 65.2 94.6 0.09 0.15 0.1 0.19
(-Ngram) (+2.3) (+1.6) (+3.4) (+0.6) (+0.02) (-0.02) (+0.04) (-0.06)
Table 3: Graphical models consistently outperform ngram models by a larger margin on sparse words than not-sparse
words. On polysemous words, the difference between graphical model performance and ngram performance grows
for POS tagging, where the context surrounding polysemous words is available to the language model, but not for
information extraction. For tagging, we show number of tokens and accuracies. For IE, we show number of types,
categories, and AUCs.
tigate a set-expansion task in which we?re given a
corpus and a few ?seed? noun phrases from a se-
mantic category (e.g. Superheroes), and our goal is
to identify other examples of the category in the cor-
pus. This is a weakly-supervised task because we are
given only a handful of examples of the category,
rather than a large sample of positively and nega-
tively labeled training examples.
Existing set-expansion techniques utilize the dis-
tributional hypothesis: candidate noun phrases for a
given semantic class are ranked based on how sim-
ilar their contextual distributions are to those of the
seeds. Here, we measure how performance on the
set-expansion task varies when we employ different
representations for the contextual distributions.
6.1 Methods
The set-expansion task we address is formalized as
follows: given a corpus, a set of seeds from some
semantic category C, and a separate set of candidate
phrases P , output a ranking of the phrases in P in
decreasing order of likelihood of membership in C.
For any given representation R, the set-expansion
algorithm we investigate is straightforward: we cre-
ate a prototypical ?seed representation vector? equal
to the mean of the representation vectors for each
of the seeds. Then, we rank candidate phrases in
increasing order of the distance between the candi-
date phrase representation and the seed representa-
tion vector. As a measure of distance between rep-
resentations, we compute the average of five stan-
dard distance measures, including KL and Jensen-
Shannon divergence, and cosine, Euclidean, and L1
distance. In experiments, we found that improving
upon this simple averaging was not easy?in fact,
tuning a weighted average of the distance measures
for each representation did not improve results sig-
nificantly on held-out data.
Because set expansion is performed at the level
of word types rather than tokens, it requires type-
based representations. We compare HMM-TYPE-
R, NGRAM-R, LATTICE-TYPE-R, and BROWN-
TYPE-R in this experiment. We used a 25-state
HMM, and the same PL-MRF as in the previous
section. Following previous set-expansion experi-
ments with n-grams (Ahuja and Downey, 2010), we
employ a trigram model with Kneser-Ney smooth-
ing for NGRAM-R. For Brown clusters, instead of
distance metrics like KL divergence (which assume
distributions), we rank extractions by the number
of matches between a word?s BROWN-TYPE-R fea-
tures and seed features.
6.2 Data Sets
We utilized a set of approximately 100,000 sen-
tences of Web text, joining multi-word named enti-
ties in the corpus into single tokens using the Lex
algorithm (Downey et al, 2007a). This process
enables each named entity (the focus of the set-
expansion experiments) to be treated as a single to-
ken, with a single representation vector for compar-
ison. We developed all word type representations
131
model AUC
HMM-TYPE-R 0.18
BROWN-TYPE-R 0.16
LATTICE-TYPE-R 0.11
NGRAM-R 0.10
Random baseline 0.10
Table 4: HMM-TYPE-R outperforms the other methods,
improving performance by 12.5% over Brown clusters,
and by 80% over the traditional NGRAM-R.
using this corpus.
To obtain examples of multiple semantic cat-
egories, we utilized selected Wikipedia ?listOf?
pages from (Pantel et al, 2009) and augmented these
with our own manually defined categories, such that
each list contained at least ten distinct examples oc-
curring in our corpus. In all, we had 432 exam-
ples across 16 distinct categories such as Countries,
Greek Islands, and Police TV Dramas.
6.3 Results
For each semantic category, we tested five differ-
ent random selections of five seed examples, treating
the unselected members of the category as positive
examples, and all other candidate phrases as nega-
tive examples. We evaluate using the area under the
precision-recall curve (AUC) metric.
The results are shown in Table 4. All represen-
tations improve performance over a random base-
line, equal to the average AUC over five random or-
derings for each category, and the graphical models
outperform the ngram representation. HMM-TYPE-
R performs the best overall, and Brown clustering
with 1000 clusters is comparable (320 and 100 clus-
ter perform slightly worse).
As with POS tagging, we expect that language
model representations improve performance on the
IE task by providing informative features for sparse
word types. However, because the IE task classifies
word types rather than tokens, we expect the rep-
resentations to provide less benefit for polysemous
word types. To test these hypotheses, we measured
how IE performance changed in sparse or polyse-
mous settings. We identified polysemous categories
as those for which fewer than 90% of the category
members had the category as a clear dominant sense
(estimated manually); other categories were consid-
ered non-polysemous. Categories whose members
had a median number of occurrences in the cor-
pus less than 30 were deemed sparse, and others
non-sparse. IE performance on these subsets of the
data are shown in Table 3. Both graphical model
representations outperform the ngram representation
more on sparse words, as expected. For polysemy,
the picture is mixed: the PL-MRF outperform n-
grams on polysemous categories, whereas HMM?s
performance advantage over n-grams decreases.
One surprise on the IE task is that the LATTICE-
TYPE-R performs significantly less well than the
HMM-TYPE-R, whereas the reverse is true on POS
tagging. We suspect that the difference is due to the
issue of classifying types vs. tokens. Because of
their more complex structure, PL-MRFs tend to de-
pend more on transition parameters than do HMMs.
Furthermore, our decision to train the PL-MRFs
using contrastive estimation with a neighborhood
that swaps consecutive pairs of words also tends to
emphasize transition parameters. As a result, we
believe the posterior distribution over latent states
given a word type is more informative in our HMM
model than the PL-MRF model. We measured the
entropy of these distributions for the two models,
and found that H(PPL-MRF(y|x = w)) = 9.95 bits,
compared with H(PHMM(y|x = w)) = 2.74 bits,
which supports the hypothesis that the drop in the
PL-MRF?s performance on IE is due to its depen-
dence on transition parameters. Further experiments
are warranted to investigate this issue.
7 Conclusion and Future Work
Our investigation into language models as represen-
tations shows that graphical models can be used to
combat polysemy and, especially, sparsity in rep-
resentations for weakly-supervised classifiers. Our
novel factorial graphical model yields a state-of-the-
art POS tagger for domain adaptation, and HMMs
improve significantly over all other representations
in an information extraction task. Important direc-
tions for future research include models for han-
dling polysemy in IE, and richer language models
that incorporate more linguistic intuitions about how
words interact with their contexts.
Acknowledgments
This research was supported in part by NSF grant
IIS-1065397 and a Microsoft New Faculty Fellow-
ship.
132
References
Arun Ahuja and Doug Downey. 2010. Improved extrac-
tion assessment through better language models. In
Proceedings of the Annual Meeting of the North Amer-
ican Chapter of the Association of Computational Lin-
guistics (NAACL-HLT).
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jenn Wortman. 2009.
A theory of learning from different domains. Machine
Learning, (to appear).
Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
2009. Curriculum learning. In International Confer-
ence on Machine Learning (ICML).
Yoshua Bengio. 2008. Neural net language models.
Scholarpedia, 3(1):3881.
Daniel M. Bikel. 2004. Intricacies of Collins Parsing
Model. Computational Linguistics, 30(4):479?511.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP.
John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jenn Wortman. 2007. Learning bounds
for domain adaptation. In Advances in Neural Infor-
mation Processing Systems.
Hans L. Bodlaender. 1988. Dynamic programming on
graphs with bounded treewidth. In Proc. 15th Interna-
tional Colloquium on Automata, Languages and Pro-
gramming, pages 105?118.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,
and J. C. Lai. 1992. Class-based n-gram models of
natural language. Computational Linguistics, pages
467?479.
M. Candito and B. Crabbe. 2009. Improving generative
statistical parsing with semi-supervised word cluster-
ing. In IWPT, pages 138?141.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In International Conference
on Machine Learning (ICML).
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26.
Hal Daume? III, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In Proceedings of the ACL Workshop on
Domain Adaptation (DANLP).
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In ACL.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American Society of
Information Science, 41(6):391?407.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society, Series
B, 39(1):1?38.
D. Downey, M. Broadhead, and O. Etzioni. 2007a. Lo-
cating complex named entities in web text. In Procs.
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI 2007).
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007b. Sparse information extraction: Unsupervised
language models to the rescue. In ACL.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of EMNLP, pages 689?697.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2010.
Multi-domain learning by confidence weighted param-
eter combination. Machine Learning, 79.
Kevin Duh. 2005. Jointly labeling multiple sequences: A
Factorial HMM approach. In 43rd Annual Meeting of
the Assoc. for Computational Linguistics (ACL 2005),
Student Research Workshop.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of HLT-NAACL, pages 602?610.
Zoubin Ghahramani and Michael I. Jordan. 1997. Facto-
rial hidden markov models. Machine Learning, 29(2-
3):245?273.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
D. Hindle. 1990. Noun classification from predicage-
argument structures. In ACL.
T. Honkela. 1997. Self-organizing maps of words for
natural language processing applications. In Proceed-
ings of the International ICSC Symposium on Soft
Computing.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised se-
quence labeling. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Fei Huang and Alexander Yates. 2010a. Exploring
representation-learning approaches to domain adapta-
tion. In Proceedings of the ACL 2010 Workshop on
Domain Adaptation for Natural Language Processing
(DANLP).
Fei Huang and Alexander Yates. 2010b. Open-domain
semantic role labeling by modeling word spans. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL).
133
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In ACL.
S. Kaski. 1998. Dimensionality reduction by random
mapping: Fast similarity computation for clustering.
In IJCNN, pages 413?418.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proceedings of the
Annual Meeting of the Association of Computational
Linguistics (ACL), pages 595?603.
J. Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning.
D. Lin and X Wu. 2009. Phrase clustering for discrimi-
native learning. In ACL-IJCNLP, pages 1030?1038.
D.C. Liu and J. Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathemati-
cal Programming B, 45(3):503?528.
Y. Mansour, M. Mohri, and A. Rostamizadeh. 2009. Do-
main adaptation with multiple sources. In Advances in
Neural Information Processing Systems.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
S. Martin, J. Liermann, and H. Ney. 1998. Algorithms
for bigram and trigram word clustering. Speech Com-
munication, 24:19?37.
A. Mnih and G. E. Hinton. 2009. A scalable hierarchi-
cal distributed language model. In Neural Information
Processing Systems (NIPS), pages 1081?1088.
P. Pantel, E. Crestan, A. Borkovsky, A. M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity and
entity set expansion. In Proc. of EMNLP.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu/.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 183?190.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?285.
Ivan A. Sag, Thomas Wasow, and Emily M. Bender.
2003. Synactic Theory: A Formal Introduction. CSLI,
Stanford, CA, second edition.
M. Sahlgren. 2005. An introduction to random index-
ing. In Methods and Applications of Semantic Index-
ing Workshop at the 7th International Conference on
Terminology and Knowledge Engineering (TKE).
M. Sahlgren. 2006. The word-space model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, Stockholm
University.
G. Salton and M.J. McGill. 1983. Introduction to Mod-
ern Information Retrieval. McGraw-Hill.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics (ACL 2007), pages
760?767.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
354?362, Ann Arbor, Michigan, June.
Charles Sutton, Andrew McCallum, and Khashayar Ro-
hanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn. Res.,
8:693?723.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 384?394.
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
J. J. Va?yrynen, T. Honkela, and L. Lindqvist. 2007.
Towards explicit semantic features using independent
component analysis. In Proceedings of the Work-
shop Semantic Content Acquisition and Representa-
tion (SCAR).
Jason Weston, Frederic Ratle, and Ronan Collobert.
2008. Deep learning via semi-supervised embedding.
In Proceedings of the 25th International Conference
on Machine Learning.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In CoNLL 2009 Shared Task.
134
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 30?33,
Baltimore, Maryland, USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Active Learning with Constrained Topic Model
Yi Yang
Northwestern University
yiyang@u.northwestern.edu
Shimei Pan
IBM T. J. Watson Research Center
shimei@us.ibm.com
Doug Downey
Northwestern University
ddowney@eecs.northwestern.edu
Kunpeng Zhang
University of Illinois at Chicago
kzhang6@uic.edu
Abstract
Latent Dirichlet Allocation (LDA) is a topic
modeling tool that automatically discovers
topics from a large collection of documents.
It is one of the most popular text analysis
tools currently in use. In practice however,
the topics discovered by LDA do not al-
ways make sense to end users. In this ex-
tended abstract, we propose an active learn-
ing framework that interactively and itera-
tively acquires user feedback to improve the
quality of learned topics. We conduct exper-
iments to demonstrate its effectiveness with
simulated user input on a benchmark dataset.
1 Introduction
Statistical topic models such as Latent Dirichlet Al-
location (LDA) (Blei et al., 2003) provide powerful
tools for uncovering hidden thematic patterns in text
and are useful for representing and summarizing the
contents of large document collections. However,
when using topic models in practice, users often face
one critical problem: topics discovered by the model
do not always make sense. A topic may contain the-
matically unrelated words. Moreover, two thematic
related words may appear in different topics. This
is mainly because the objective function optimized
by LDA may not reflect human judgments of topic
quality (Boyd-Graber et al., 2009).
Potentially, we can solve these problems by incor-
porating additional user guidance or domain knowl-
edge in topic modeling. With standard LDA how-
ever, it is impossible for users to interact with the
model and provide feedback. (Hu et al., 2011) pro-
posed an interactive topic modeling framework that
allows users to add word must-links. However, it
has several limitations. Since the vocabulary size of
a large document collection can be very large, users
may need to annotate a large number of word con-
straints for this method to be effective. Thus, this
process can be very tedious. More importantly, it
cannot handle polysemes. For example, the word
?pound? can refer to either a currency or a unit of
mass. If a user adds a must-link between ?pound?
and another financial term, then he/she cannot add
a must-link between ?pound? and any measurement
terms. Since word must-links are added without
context, there is no way to disambiguate them. As a
result, word constraints frequently are not as effec-
tive as document constraints.
Active learning (Settles, 2010) provides a use-
ful framework which allows users to iteratively give
feedback to the model to improve its quality. In gen-
eral, with the same amount of human labeling, ac-
tive learning often results in a better model than that
learned by an off-line method.
In this extended abstract, we propose an active
learning framework for LDA. It is based on a new
constrained topic modeling framework which is ca-
pable of handling pairwise document constraints.
We present several design choices and the pros and
cons of each choice. We also conduct simulated ex-
periments to demonstrate the effectiveness of the ap-
proach.
2 Active Learning With Constrained Topic
Modeling
In this section, we first summarize our work on con-
strained topic modeling. Then, we introduce an
active topic learning framework that employs con-
strained topic modeling.
In LDA, a document?s topic distribution
~
? is
drawn from a Dirichlet distribution with prior ~?.
A simple and commonly used Dirichlet distribution
uses a symmetric ~? prior. However, (Wallach et al.,
2009) has shown that an asymmetric Dirichlet prior
over the document-topic distributions
~
? and a sym-
metric Dirichlet prior over the topic-word distribu-
tions
~
? yield significant improvements in model per-
formance. Our constrained topic model uses asym-
metric priors to encode constraints.
To incorporate user feedback, we focus on two
1
30
Figure 1: Diagram illustrating the topic model active learning framework.
types of document constraints. A must-link be-
tween two documents indicates that they belong to
the same topics, while a cannot-link indicates that
they belong to different topics.
Previously, we proposed a constrained LDA
framework called cLDA,
1
which is capable of incor-
porating pairwise document constraints. Given pair-
wise document constraints, the topic distribution of
a document cannot be assumed to be independently
sampled. More specifically, we denote the collection
of documents as D = {d
1
, d
2
, ..., d
N
}. We also de-
noteM
i
? D as the set of documents sharing must-
links with document d
i
, and C
i
? D as the set of
documents sharing cannot-links with document d
i
.
~
?
i
is the topic distribution of d
i
, and ~? is the global
document-topic hyper-parameter shared by all doc-
uments.
Given the documents inM
i
, we introduce an aux-
iliary variable ~?
M
i
:
~?
i
M
= T ?
1
|M
i
|
?
j?M
i
~
?
j
, (1)
where T controls the concentration parameters. The
larger the value of T is, the closer
~
?
i
is to the average
of
~
?
j
?s.
Given the documents in C
i
, we introduce another
auxiliary variable:
~?
i
C
= T ? arg
~
?
i
maxmin
j?C
i
KL(
~
?
i
,
~
?
j
), (2)
whereKL(
~
?
i
,
~
?
j
) is the KL-divergence between two
distributions
~
?
i
and
~
?
j
. This means we choose a vec-
tor that is maximally far away from C
i
, in terms of
KL divergence to its nearest neighbor in C
i
.
In such a way, we force documents sharing must-
links to have similar topic distributions while docu-
ments sharing cannot-links to have dissimilar topic
distributions. Note that it also encodes constraint as
soft preference rather than hard constraint. We use
Collapsed Gibbs Sampling for LDA inference. Dur-
ing Gibbs Sampling, instead of always drawing
~
?
i
1
currently in submission.
from Dirichlet(~?), we draw
~
?
i
based on the fol-
lowing distribution:
~
?
i
? Dir(?~?+?
M
~?
i
M
+?
C
~?
i
C
) = Dir(~?
i
). (3)
Here, ?
g
, ?
M
and ?
C
are the weights to control the
trade-off among the three terms. In our experiment,
we choose T = 100, ?
g
= ?
M
= ?
C
= 1.
Our evaluation has shown that cLDA is effective
in improving topic model quality. For example, it
achieved a significant topic classification error re-
duction on the 20 Newsgroup dataset. Also, top-
ics learned by cLDA are more coherent than those
learned by standard LDA.
2.1 Active Learning with User Interaction
In this subsection, we present an active learning
framework to iteratively acquire constraints from
users. As shown in Figure 1, given a document col-
lection, the framework first runs standard LDA with
a burnin component. Since it uses a Gibbs sampler
(Griffiths and Steyvers, 2004) to infer topic samples
for each word token, it usually takes hundreds of it-
erations for the sampler to converge to a stable state.
Based on the results of the burnt-in model, the sys-
tem generates a target document and a set of anchor
documents for a user to annotate. Target document is
a document on which the active learner solicits user
feedback, and anchor documents are representatives
of a topic model?s latent topics. If a large portion of
the word tokens in a document belongs to topic i, we
say the document is an anchor document for topic i.
A user judges the content of the target and the
anchor documents and then informs the system
whether the target document is similar to any of the
anchor documents. The user interface is designed
so that the user can drag the target document near
an anchor document if she considers both to be the
same topic. Currently, one target document can be
must-linked to only one anchor document. Since
it is possbile to have multiple topics in one docu-
ment, in the future, we will allow user to add must
links between one target and mulitple anchor doc-
uments. After adding one or more must-links, the
31
system automatically adds cannot-links between the
target document and the rest anchor documents.
Given this input, the system adds them to a con-
straint pool. It then uses cLDA to incorporate these
constraints and generates an updated topic model.
Based on the new topic model, the system chooses a
new target document and several new anchor docu-
ments for the user to annotate. This process contin-
ues until the user is satisfied with the resulting topic
model.
How to choose the target and anchor documents
are the key questions that we consider in the next
subsections.
2.2 Target Document Selection
A target document is defined as a document on
which the active learner solicits user feedback. We
have investigated several strategies for selecting a
target document.
Random: The active learner randomly selects a doc-
ument from the corpus. Although this strategy is
the simplest, it may not be efficient since the model
may have enough information about the document
already.
MaxEntropy: The entropy of a document d is com-
puted as H
d
= ?
?
K
i=1
?
dk
log ?
dk
, where K is the
number of topics, and ? is model?s document-topic
distribution. Therefore, the system will select a doc-
ument about which it is most confused. A uniform
? implies that the model has no topic information
about the document and thus assigns equal probabil-
ity to all topics.
MinLikelihood: The likelihood of a document d is
computed as L
d
= (
?
N
i=1
?
K
k=1
?
ki
?
dk
)/N , where
N is the number of tokens in d, and ? is model?s
topic-word distribution. Since the overall likeli-
hood of the input documents is the objective func-
tion LDA aims to maximize, using this criteria, the
system will choose a document that is most difficult
for which the current model achieves the lowest ob-
jective score.
2.3 Anchor Documents Selection
Given a target document d, the active learner then
generates one or more anchor documents based on
the target document?s topic distribution ?
d
. It filters
out topics with trivial value in ?
d
and extracts an an-
chor topic set T
anc
which only contains topics with
non-trivial value in ?
d
. A trivial ?
di
means that the
mass of ith component in ?
d
is neglectable, which
indicates that the model rarely assign topic i to doc-
ument d. For each topic t in T
anc
, the active learner
selects an anchor document who has minimum Eu-
clidean distance with an ideal anchor ?
?
t
. In the ideal
anchor ?
?
t
, all the components are zero except the
value of the t
th
component is 1. For example, if a
target document d?s ?
d
is {0.5, 0.3, 0.03, 0.02, 0.15}
in a K = 5 topic model, the active learner would
generate T
anc
= {0, 1, 4} and for each t in T
anc
, an
anchor document.
However, it is possible that some topics learned
by LDA are only ?background? topics which have
significant non-trivial probabilities over many doc-
uments (Song et al., 2009). Since background top-
ics are often uninteresting ones, we use a weighted
anchor topic selection method to filter them. A
weighted k
th
component of ?
?
dk
for document d is
defined as follows: ?
?
dk
= ?
dk
/
?
D
i=0
?
ik
. There-
fore, instead of keeping the topics with non-trivial
values, we keep those whose weighted values are
non-trivial.
3 Evaluation
In this section, we evaluate our active learning
framework. Topic models are often evaluated us-
ing perplexity on held-out test data. However, re-
cent work (Boyd-Graber et al., 2009; Chuang et al.,
2013) has shown that human judgment sometimes
is contrary to the perplexity measure. Following
(Mimno et al., 2011), we employ Topic Coherence,
a metric which was shown to be highly consistent
with human judgment, to measure a topic model?s
quality. It relies upon word co-occurrence statistics
within documents, and does not depend on external
resources or human labeling.
We followed (Basu et al., 2004) to create a Mix3
sub-dataset from the 20 Newsgroups data
2
, which
consists of two newsgroups with similar topics
(rec.sport.hockey, rec.sport.baseball) and one with
a distinctive topic (sci.space). We use this dataset
to evaluate the effectiveness of the proposed frame-
work.
3.1 Simulated Experiments
We first burn-in LDA for 500 iterations. Then for
each additional iteration, the active learner generates
one query which consists of one target document and
one or more anchor documents. We simulate user
feedback using the documents? ground truth labels.
If a target document has the same label as one of
the anchor documents, we add a must-link between
them. We also add cannot-links between the target
document and the rest of the anchor documents. All
these constraints are added into a constraint pool.
We also augment the constraint pool with derived
constraints. For example, due to transitivity, if there
is a must-link between (a, b) and (b, c), then we add
2
Available at http://people.csail.mit.edu/
jrennie/20Newsgroups
32
Topic Words
1 writes, like, think, good, know, better, even, people, run, hit
2 space, nasa, system, gov, launch, orbit, moon, earth, access, data
3 game, play, hockey, season, league, fun, wing, cup, shot, score
1 baseball, hit, won, shot, hitter, base, pitching, cub, ball, yankee
2 space, nasa, system, gov, launch, obit, moon, earth, mission, shuttle
3 hockey, nhl, playoff, star, wing, cup, king, detroit, ranger
Table 1: Ten most probable words of each topic before (above) and after active learning (below).
a must link between (a, c). We simulate the process
for 100 iterations to acquire constraints. After that,
we keep cLDA running for 400 more iterations with
the acquired constraints until it converges.
Figure 2: Topic coherence with different number of
iterations.
Figure 2 shows the topic coherence scores for dif-
ferent target document selection strategies. This re-
sult indicates 1). MaxEntropy has the best topic co-
herence score. 2). All active learning strategies out-
perform standard LDA, and the results are statisti-
cally significant at p = 0.05. With standard LDA,
500 more iterations without any constraints does not
improve the topic coherence. However, by active
learning with cLDA for 500 iterations, the topic co-
herences are significantly improved.
Using MaxEntropy target document selection
method, we demonstrate the improvement of the
most probable topic keywords before and after ac-
tive learning. Table 1 shows that before active learn-
ing, topic 1?s most probable words are incoherent
and thus it is difficult to determine the meaning of
the topic . After active learning, in contrast, topic 1?s
most probable words become more consistent with
a ?baseball? topic. This example suggests that the
active learning framework that interactively and it-
eratively acquires pairwise document constraints is
effective in improving the topic model?s quality.
4 Conclusion
We presented a novel active learning framework for
LDA that employs constrained topic modeling to
actively incorporate user feedback encoded as pair-
wise document constraints. With simulated user in-
put, our preliminary results demonstrate the effec-
tiveness of the framework on a benchmark dataset.
In the future, we will perform a formal user study
in which real users will interact with the system to
iteratively refine topic models.
Acknowledgments
This work was supported in part by DARPA contract
D11AP00268.
References
Sugato Basu, A. Banjeree, ER. Mooney, Arindam Baner-
jee, and Raymond J. Mooney. 2004. Active semi-
supervision for pairwise constrained clustering. In
SDM, pages 333?344.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022.
Jordan Boyd-Graber, Jonathan Chang, Sean Gerrish,
Chong Wang, and David Blei. 2009. Reading tea
leaves: How humans interpret topic models. In NIPS.
Jason Chuang, Sonal Gupta, Christopher D. Manning,
and Jeffrey Heer. 2013. Topic model diagnostics:
Assessing domain relevance via topical alignment. In
ICML.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228?5235.
Yuening Hu, Jordan Boyd-Graber, and Brianna Satinoff.
2011. Interactive topic modeling. In ACL, pages 248?
257.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
EMNLP, pages 262?272.
Burr Settles. 2010. Active learning literature survey.
Technical report, University of Wisconsin Madison.
Yangqiu Song, Shimei Pan, Shixia Liu, Michelle X.
Zhou, and Weihong Qian. 2009. Topic and keyword
re-ranking for lda-based topic modeling. In CIKM,
pages 1757?1760.
Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking lda: Why priors matter. In
NIPS, pages 1973?1981.
33
