Significance tests for the evaluation of ranking methods
Stefan Evert
Institut fu?r maschinelle Sprachverarbeitung
Universita?t Stuttgart
Azenbergstr. 12, 70174 Stuttgart, Germany
evert@ims.uni-stuttgart.de
Abstract
This paper presents a statistical model that in-
terprets the evaluation of ranking methods as
a random experiment. This model predicts the
variability of evaluation results, so that appro-
priate significance tests for the results can be
derived. The paper concludes with an empirical
validation of the model on a collocation extrac-
tion task.
1 Introduction
Many tools in the area of natural-language process-
ing involve the application of ranking methods to
sets of candidates, in order to select the most use-
ful items from an all too often overwhelming list.
Examples of such tools range from syntactic parsers
(where alternative analyses are ranked by their plau-
sibility) to the extraction of collocations from text
corpora (where a ranking according to the scores as-
signed by a lexical association measure is the essen-
tial component of an extraction ?pipeline?).
To this end, a scoring function g is applied to the
candidate set, which assigns a real number g(x) ?
R to every candidate x.1 Conventionally, higher
scores are assigned to candidates that the scoring
function considers more ?useful?. Candidates can
then be selected in one of two ways: (i) by compar-
ison with a pre-defined threshold ? ? R (i.e. x is
accepted iff g(x) ? ?), resulting in a ?-acceptance
set; (ii) by ranking the entire candidate set accord-
ing to the scores g(x) and selecting the n highest-
scoring candidates, resulting in an n-best list (where
n is either determined by practical constraints or in-
teractively by manual inspection). Note that an n-
best list can also be interpreted as a ?-acceptance set
with a suitably chosen cutoff threshold ?g(n) (deter-
mined from the scores of all candidates).
Ranking methods usually involve various heuris-
tics and statistical guesses, so that an empirical eval-
1Some systems may directly produce a sorted candidate list
without assigning explicit scores. However, unless this opera-
tion is (implicitly) based on an underlying scoring function, the
result will in most cases be a partial ordering (where some pairs
of candidates are incomparable) or lead to inconsistencies.
uation of their performance is necessary. Even when
there is a solid theoretical foundation, its predictions
may not be borne out in practice. Often, the main
goal of an evaluation experiment is the comparison
of different ranking methods (i.e. scoring functions)
in order to determine the most useful one.
A widely-used evaluation strategy classifies the
candidates accepted by a ranking method into
?good? ones (true positives, TP) and ?bad? ones
(false positives, FP). This is sometimes achieved by
comparison of the relevant ?-acceptance sets or n-
best lists with a gold standard, but for certain ap-
plications (such as collocation extraction), manual
inspection of the candidates leads to more clear-cut
and meaningful results. When TPs and FPs have
been identified, the precision ? of a ?-acceptance
set or an n-best list can be computed as the pro-
portion of TPs among the accepted candidates. The
most useful ranking method is the one that achieves
the highest precision, usually comparing n-best lists
of a given size n. If the full candidate set has been
annotated, it is also possible to determine the recall
R as the number of accepted TPs divided by the to-
tal number of TPs in the candidate set. While the
evaluation of extraction tools (e.g. in information
retrieval) usually requires that both precision and
recall are high, ranking methods often put greater
weight on high precision, possibly at the price of
missing a considerable number of TPs. Moreover,
when n-best lists of the same size are compared,
precision and recall are fully equivalent.2 For these
reasons, I will concentrate on the precision ? here.
As an example, consider the identification of col-
locations from text corpora. Following the method-
ology described by Evert and Krenn (2001), Ger-
man PP-verb combinations were extracted from a
chunk-parsed version of the Frankfurter Rundschau
Corpus.3 A cooccurrence frequency threshold of
2Namely, ? = nTP ? R/n, where nTP stands for the total
number of TPs in the candidate set.
3The Frankfurter Rundschau Corpus is a German newspa-
per corpus, comprising ca. 40 million words of text. It is part of
the ECI Multilingual Corpus 1 distributed by ELSNET. For this
f ? 30 was applied, resulting in a candidate set
of 5 102 PP-verb pairs. The candidates were then
ranked according to the scores assigned by four
association measures: the log-likelihood ratio G2
(Dunning, 1993), Pearson?s chi-squared statistic X2
(Manning and Schu?tze, 1999, 169?172), the t-score
statistic t (Church et al, 1991), and mere cooccur-
rence frequency f .4 TPs were identified according
to the definition of Krenn (2000). The graphs in
Figure 1 show the precision achieved by these mea-
sures, for n ranging from 100 to 2 000 (lists with
n < 100 were omitted because the graphs become
highly unstable for small n). The baseline precision
of 11.09% corresponds to a random selection of n
candidates.
0 500 1000 1500 2000
0
10
20
30
40
50
n?best list
prec
ision
 (%)
baseline = 11.09%
G2tX2f
Figure 1: Evaluation example: candidates for Ger-
man PP-verb collocations are ranked by four differ-
ent association measures.
From Figure 1, we can see that G2 and t are the
most useful ranking methods, t being marginally
better for n ? 800 and G2 for n ? 1 500. Both mea-
sures are by far superior to frequency-based rank-
ing. The evaluation results also confirm the argu-
ment of Dunning (1993), who suggested G2 as a
more robust alternative to X2. Such results cannot
be taken at face value, though, as they may simply
be due to chance. When two equally useful rank-
ing methods are compared, method A might just
happen to perform better in a particular experiment,
with B taking the lead in a repetition of the experi-
experiment, the corpus was annotated with the partial parser
YAC (Kermes, 2003).
4See Evert (2004) for detailed information about these as-
sociation measures, as well as many further alternatives.
ment under similar conditions. The causes of such
random variation include the source material from
which the candidates are extracted (what if a slightly
different source had been used?), noise introduced
by automatic pre-processing and extraction tools,
and the uncertainty of human annotators manifested
in varying degrees of inter-annotator agreement.
Most researchers understand the necessity of test-
ing whether their results are statistically significant,
but it is fairly unclear which tests are appropriate.
For instance, Krenn (2000) applies the standard ?2-
test to her comparative evaluation of collocation ex-
traction methods. She is aware, though, that this
test assumes independent samples and is hardly suit-
able for different ranking methods applied to the
same candidate set: Krenn and Evert (2001) sug-
gest several alternative tests for related samples. A
wide range of exact and asymptotic tests as well as
computationally intensive randomisation tests (Yeh,
2000) are available and add to the confusion about
an appropriate choice.
The aim of this paper is to formulate a statisti-
cal model that interprets the evaluation of ranking
methods as a random experiment. This model de-
fines the degree to which evaluation results are af-
fected by random variation, allowing us to derive
appropriate significance tests. After formalising the
evaluation procedure in Section 2, I recast the pro-
cedure as a random experiment and make the under-
lying assumptions explicit (Section 3.1). On the ba-
sis of this model, I develop significance tests for the
precision of a single ranking method (Section 3.2)
and for the comparison of two ranking methods
(Section 3.3). The paper concludes with an empiri-
cal validation of the statistical model in Section 4.
2 A formal account of ranking methods
and their evaluation
In this section I present a formalisation of rankings
and their evaluation, giving ?-acceptance sets a ge-
ometrical interpretation that is essential for the for-
mulation of a statistical model in Section 3.
The scores computed by a ranking method are
based on certain features of the candidates. Each
candidate can therefore be represented by its feature
vector x ? ?, where ? is an abstract feature space.
For all practical purposes, ? can be equated with a
subset of the (possibly high-dimensional) real Eu-
clidean space Rm. The complete set of candidates
corresponds to a discrete subset C ? ? of the fea-
ture space.5 A ranking method is represented by
5More precisely, C is a multi-set because there may be mul-
tiple candidates with identical feature vectors. In order to sim-
plify notation I assume that C is a proper subset of ?, which
a real-valued function g : ? ? R on the feature
space, called a scoring function (SF). In the follow-
ing, I assume that there are no candidates with equal
scores, and hence no ties in the rankings.6
The ?-acceptance set for a SF g contains all can-
didates x ? C with g(x) ? ?. In a geomet-
rical interpretation, this condition is equivalent to
x ? Ag(?) ? ?, where
Ag(?) := {x ? ? | g(x) ? ? }
is called the ?-acceptance region of g. The ?-
acceptance set of g is then given by the intersection
Ag(?)?C =: Cg(?). The selection of an n-best list
is based on the ?-acceptance region Ag(?g(n)) for
a suitably chosen n-best threshold ?g(n).7
As an example, consider the collocation extrac-
tion task introduced in Section 1. The feature vec-
tor x associated with a collocation candidate rep-
resents the cooccurrence frequency information for
this candidate: x = (O11, O12, O21, O22), where
Oij are the cell counts of a 2 ? 2 contingency
table (Evert, 2004). Therefore, we have a four-
dimensional feature space ? ? R4, and each as-
sociation measure defines a SF g : ? ? R. The
selection of collocation candidates is usually made
in the form of an n-best list, but may also be based
on a pre-defined threshold ?.8
For an evaluation in terms of precision and re-
call, the candidates in the set C are classified into
true positives C+ and false positives C?. The pre-
cision corresponding to an acceptance region A is
then given by
?A := |C+ ?A| / |C ?A| , (1)
i.e. the proportion of TPs among the accepted candi-
dates. The precision achieved by a SF g with thresh-
old ? is ?Cg(?). Note that the numerator in Eq. (1)
reduces to n for an n-best list (i.e. ? = ?g(n)),
yielding the n-best precision ?g,n. Figure 1 shows
graphs of ?g,n for 100 ? n ? 2 000, for the SFs
g1 = G2, g2 = t, g3 = X2, and g4 = f .
can be enforced by adding a small amount of random jitter to
the feature vectors of candidates.
6Under very general conditions, random jittering (cf. Foot-
note 5) ensures that no two candidates have equal scores. This
procedure is (almost) equivalent to breaking ties in the rankings
randomly.
7Since I assume that there are no ties in the rankings, ?g(n)
can always be determined in such a way that the acceptance set
contains exactly n candidates.
8For instance, Church et al (1991) use a threshold of ? =
1.65 for the t-score measure corresponding to a nominal sig-
nificance level of ? = .05. This threshold is obtained from the
limiting distribution of the t statistic.
3 Significance tests for evaluation results
3.1 Evaluation as a random experiment
When an evaluation experiment is repeated, the re-
sults will not be exactly the same. There are many
causes for such variation, including different source
material used by the second experiment, changes in
the tool settings, changes in the evaluation criteria,
or the different intuitions of human annotators. Sta-
tistical significance tests are designed to account for
a small fraction of this variation that is due to ran-
dom effects, assuming that all parameters that may
have a systematic influence on the evaluation results
are kept constant. Thus, they provide a lower limit
for the variation that has to be expected in an actual
repetition of the experiment. Only when results are
significant can we expect them to be reproducible,
but even then a second experiment may draw a dif-
ferent picture.
In particular, the influence of qualitatively differ-
ent source material or different evaluation criteria
can never be predicted by statistical means alone.
In the example of the collocation extraction task,
randomness is mainly introduced by the selection
of a source corpus, e.g. the choice of one partic-
ular newspaper rather than another. Disagreement
between human annotators and uncertainty about
the interpretation of annotation guidelines may also
lead to an element of randomness in the evaluation.
However, even significant results cannot be gener-
alised to a different type of collocation (such as
adjective-noun instead of PP-verb), different eval-
uation criteria, a different domain or text type, or
even a source corpus of different size, as the results
of Krenn and Evert (2001) show.
A first step in the search for an appropriate sig-
nificance test is to formulate a (plausible) model
for random variation in the evaluation results. Be-
cause of the inherent randomness, every repetition
of an evaluation experiment under similar condi-
tions will lead to different candidate sets C+ and
C?. Some elements will be entirely new candidates,
sometimes the same candidate appears with a differ-
ent feature vector (and thus represented by a differ-
ent point x ? ?), and sometimes a candidate that
was annotated as a TP in one experiment may be
annotated as a FP in the next. In order to encapsu-
late all three kinds of variation, let us assume that
C+ and C? are randomly selected from a large set
of hypothetical possibilities (where each candidate
corresponds to many different possibilities with dif-
ferent feature vectors, some of which may be TPs
and some FPs).
For any acceptance region A, both the number of
TPs in A, TA := |C+ ?A|, and the number of FPs
in A, FA := |C? ?A|, are thus random variables.
We do not know their precise distributions, but it is
reasonable to assume that (i) TA and FA are always
independent and (ii) TA and TB (as well as FA and
FB) are independent for any two disjoint regions A
and B. Note that TA and TB cannot be indepen-
dent for A ? B 6= ? because they include the same
number of TPs from the region A ? B. The total
number of candidates in the region A is also a ran-
dom variable NA := TA+FA, and the same follows
for the precision ?A, which can now be written as
?A = TA/NA.9
Following the standard approach, we may now
assume that ?A approximately follows a normal
distribution with mean piA and variance ?2A, i.e.
?A ? N(piA, ?2A). The mean piA can be interpreted
as the average precision of the acceptance region
A (obtained by averaging over many repetitions of
the evaluation experiment). However, there are two
problems with this assumption. First, while ?A is
an unbiased estimator for pia, the variance ?2A can-
not be estimated from a single experiment.10 Sec-
ond, ?A is a discrete variable because both TA and
NA are non-negative integers. When the number
of candidates NA is small (as in Section 3.3), ap-
proximating the distribution of ?A by a continuous
normal distribution will not be valid.
It is reasonable to assume that the distribution of
NA does not depend on the average precision piA. In
this case, NA is called an ancillary statistic and can
be eliminated without loss of information by condi-
tioning on its observed value (see Lehmann (1991,
542ff) for a formal definition of ancillary statistics
and the merits of conditional inference). Instead of
probabilities P (?A) we will now consider the con-
ditional probabilities P (?A |NA). Because NA is
fixed to the observed value, ?A is proportional to
TA and the conditional probabilities are equivalent
to P (TA |NA). When we choose one of the NA
candidates at random, the probability that it is a TP
(averaged over many repetitions of the experiment)
9In the definition of the n-best precision ?g,n, i.e. for
A = Cg(?g(n)), the number of candidates in A is constant:
NA = n. At first sight, this may seem to be inconsistent with
the interpretation of NA as a random variable. However, one
has to keep in mind that ?g(n), which is determined from the
candidate set C, is itself a random variable. Consequently, A is
not a fixed acceptance region and its variation counter-balances
that of NA.
10Sometimes, cross-validation is used to estimate the vari-
ability of evaluation results. While this method is appropri-
ate e.g. for machine learning and classification tasks, it is not
useful for the evaluation of ranking methods. Since the cross-
validation would have to be based on random samples from a
single candidate set, it would not be able to tell us anything
about random variation between different candidate sets.
should be equal to the average precision piA. Conse-
quently, P (TA |NA) should follow a binomial dis-
tribution with success probability piA, i.e.
P (TA = k |NA) =
(
NA
k
)
? (piA)
k ? (1? piA)
NA?k (2)
for k = 0, . . . , NA. We can now make inferences
about the average precision piA based on this bino-
mial distribution.11
As a second step in our search for an appropriate
significance test, it is essential to understand exactly
what question this test should address: What does it
mean for an evaluation result (or result difference)
to be significant? In fact, two different questions
can be asked:
A: If we repeat an evaluation experiment under
the same conditions, to what extent will the ob-
served precision values vary? This question is
addressed in Section 3.2.
B: If we repeat an evaluation experiment under
the same conditions, will method A again per-
form better than method B? This question is
addressed in Section 3.3.
3.2 The stability of evaluation results
Question A can be rephrased in the following way:
How much does the observed precision value for
an acceptance region A differ from the true aver-
age precision piA? In other words, our goal here
is to make inferences about piA, for a given SF g
and threshold ?. From Eq. (2), we obtain a bino-
mial confidence interval for the true value piA, given
the observed values of TA and NA (Lehmann, 1991,
89ff). Using the customary 95% confidence level,
piA should be contained in the estimated interval in
all but one out of twenty repetitions of the experi-
ment. Binomial confidence intervals can easily be
computed with standard software packages such as
R (R Development Core Team, 2003). As an ex-
ample, assume that an observed precision of ?A =
40% is based on TA = 200 TPs out of NA = 500
accepted candidates. Precision graphs as those in
Figure 1 display ?A as a maximum-likelihood es-
timate for piA, but its true value may range from
35.7% to 44.4% (with 95% confidence).12
11Note that some of the assumptions leading to Eq. (2) are
far from self-evident. As an example, (2) tacitly assumes that
the success probability is equal to piA regardless of the particu-
lar value of NA on which the distribution is conditioned, which
need not be the case. Therefore, an empirical validation is nec-
essary (see Section 4).
12This confidence interval was computed with the R com-
mand binom.test(200,500).
Figure 2 shows binomial confidence intervals for
the association measures G2 and X2 as shaded re-
gions around the precision graphs. It is obvious
that a repetition of the evaluation experiment may
lead to quite different precision values, especially
for n < 1 000. In other words, there is a consider-
able amount of uncertainty in the evaluation results
for each individual measure. However, we can be
confident that both ranking methods offer a substan-
tial improvement over the baseline.
0 500 1000 1500 2000
0
10
20
30
40
50
n?best list
prec
ision
 (%)
baseline = 11.09%
G2X2
Figure 2: Precision graphs for the G2 and X2 mea-
sures with 95% confidence intervals.
For an evaluation based on n-best lists (as in the
collocation extraction example), it has to be noted
that the confidence intervals are estimates for the
average precision piA of a fixed ?-acceptance re-
gion (with ? = ?g(n) computed from the observed
candidate set). While this region contains exactly
NA = n candidates in the current evaluation, NA
may be different from n when the experiment is re-
peated. Consequently, piA is not necessarily identi-
cal to the average precision of n-best lists.
3.3 The comparison of ranking methods
Question B can be rephrased in the following way:
Does the SF g1 on average achieve higher precision
than the SF g2? (This question is normally asked
when g1 performed better than g2 in the evaluation.)
In other words, our goal is to test whether piA > piB
for given acceptance regions A of g1 and B of g2.
The confidence intervals obtained for two SF g1
and g2 will often overlap (cf. Figure 2, where the
confidence intervals of G2 and X2 overlap for all
list sizes n), suggesting that there is no significant
difference between the two ranking methods. Both
observed precision values are consistent with an av-
erage precision piA = piB in the region of overlap,
so that the observed differences may be due to ran-
dom variation in opposite directions. However, this
conclusion is premature because the two rankings
are not independent. Therefore, the observed pre-
cision values of g1 and g2 will tend to vary in the
same direction, the degree of correlation being de-
termined by the amount of overlap between the two
rankings. Given acceptance regions A := Ag1(?1)
and B := Ag2(?2), both SF make the same decision
for any candidates in the intersection A ? B (both
SF accept) and in the ?complement? ? \ (A ? B)
(both SF reject). Therefore, the performance of g1
and g2 can only differ in the regions D1 := A \ B
(g1 accepts, but g2 rejects) and B \ A (vice versa).
Correspondingly, the counts TA and TB are corre-
lated because they include the same number of TPs
from the region A?B (namely, the set C+?A?B),
Indisputably, g1 is a better ranking method than
g2 iff piD1 > piD2 and vice versa.13 Our goal is thus
to test the null hypothesis H0 : piD1 = piD2 on the
basis of the binomial distributions P (TD1 |ND1)
and P (TD2 |ND2). I assume that these distribu-
tions are independent because D1 ? D2 = ? (cf.
Section 3.1). The number of candidates in the
difference regions, ND1 and ND2 , may be small,
especially for acceptance regions with large over-
lap (this was one of the reasons for using condi-
tional inference rather than a normal approximation
in Section 3.1). Therefore, it is advisable to use
Fisher?s exact test (Agresti, 1990, 60?66) instead
of an asymptotic test that relies on large-sample ap-
proximations. The data for Fisher?s test consist of
a 2? 2 contingency table with columns (TD1 , FD1)
and (TD2 , FD2). Note that a two-sided test is called
for because there is no a priori reason to assume
that g1 is better than g2 (or vice versa). Although
the implementation of a two-sided Fisher?s test is
not trivial, it is available in software packages such
as R.
Figure 3 shows the same precision graphs as
Figure 2. Significant differences between the G2
and X2 measures according to Fisher?s test (at a
95% confidence level) are marked by grey triangles.
13Note that piD1 > piD2 does not necessarily entail piA >
piB if NA and NB are vastly different and piA?B ? piDi . In
this case, the winner will always be the SF that accepts the
smaller number of candidates (because the additional candi-
dates only serve to lower the precision achieved in A ? B).
This example shows that it is ?unfair? to compare acceptance
sets of (substantially) different sizes just in terms of their over-
all precision. Evaluation should therefore either be based on
n-best lists or needs to take recall into account.
Contrary to what the confidence intervals in Fig-
ure 2 suggested, the observed differences turn out
to be significant for all n-best lists up to n = 1250
(marked by a thin vertical line).
0 500 1000 1500 2000
0
10
20
30
40
50
n?best list
prec
ision
 (%)
baseline = 11.09%
G2X2
Figure 3: Significant differences between the G2
and X2 measures at 95% confidence level.
4 Empirical validation
In order to validate the statistical model and the sig-
nificance tests proposed in Section 3, it is neces-
sary to simulate the repetition of an evaluation ex-
periment. Following the arguments of Section 3.1,
the conditions should be the same for all repetitions
so that the amount of purely random variation can
be measured. To achieve this, I divided the Frank-
furter Rundschau Corpus into 80 contiguous, non-
overlapping parts, each one containing approx. 500k
words. Candidates for PP-verb collocations were
extracted as described in Section 1, with a frequency
threshold of f ? 4. The 80 samples of candidate
sets were ranked using the association measures G2,
X2 and t as scoring functions, and true positives
were manually identified according to the criteria
of (Krenn, 2000).14 The true average precision piA
of an acceptance set A was estimated by averaging
over all 80 samples.
Both the confidence intervals of Section 3.2 and
the significance tests of Section 3.3 are based on
the assumption that P (TA |NA) follows a binomial
distribution as given by Eq. (2). Unfortunately, it
14I would like to thank Brigitte Krenn for making her annota-
tion database of PP-verb collocations (Krenn, 2000) available,
and for the manual annotation of 1 913 candidates that were not
covered by the existing database.
is impossible to test the conditional distribution di-
rectly, which would require that NA is the same for
all samples. Therefore, I use the following approach
based on the unconditional distribution P (?A). If
NA is sufficiently large, P (?A |NA) can be approx-
imated by a normal distribution with mean ? = piA
and variance ?2 = piA(1? piA)/NA (from Eq. (2)).
Since ? does not depend on NA and the standard
deviation ? is proportional to (NA)?1/2, it is valid
to make the approximation
P (?A |NA) ? P (?A) (3)
as long as NA is relatively stable. Eq. (3) allows us
to pool the data from all samples, predicting that
P (?A) ? N(?, ?
2) (4)
with ? = piA and ?2 = piA(1 ? piA)/N . Here, N
stands for the average number of TPs in A.
These predictions were tested for the measures
g1 = G2 and g2 = t, with cutoff thresholds ?1 =
32.5 and ?2 = 2.09 (chosen so that N = 100 candi-
dates are accepted on average). Figure 4 compares
the empirical distribution of ?A with the expected
distribution according to Eq. (4). These histograms
show that the theoretical model agrees quite well
with the empirical results, although there is a lit-
tle more variation than expected.15 The empirical
standard deviation is between 20% and 40% larger
than expected, with s = 0.057 vs. ? = 0.044 for G2
and s = 0.066 vs. ? = 0.047 for t. These findings
suggest that the model proposed in Section 3.1 may
indeed represent a lower bound on the true amount
of random variation.
Further evidence for this conclusion comes from
a validation of the confidence intervals defined in
Section 3.2. For a 95% confidence interval, the true
proportion piA should fall within the confidence in-
terval in all but 4 of the 80 samples. For G2 (with
? = 32.5) and X2 (with ? = 239.0), piA was out-
side the confidence interval in 9 cases each (three
of them very close to the boundary), while the con-
fidence interval for t (with ? = 2.09) failed in 12
cases, which is significantly more than can be ex-
plained by chance (p < .001, binomial test).
5 Conclusion
In the past, various statistical tests have been used
to assess the significance of results obtained in the
evaluation of ranking methods. There is much con-
fusion about their validity, though, mainly due to
15The agreement is confirmed by the Kolmogorov test of
goodness-of-fit, which does not reject the theoretical model (4)
in either case.
Histogram for G2
precision
num
ber o
f sam
ples
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0
5
10
15
20 observedexpected
Histogram for t
precision
num
ber o
f sam
ples
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0
5
10
15
20 observedexpected
Figure 4: Distribution of the observed precision ?A for ?-acceptance regions of the association measures
G2 (left panel) and t (right panel). The solid lines indicate the expected distribution according to Eq. (2).
the fact that assumptions behind the application
of a test are seldom made explicit. This paper
is an attempt to remedy the situation by interpret-
ing the evaluation procedure as a random experi-
ment. The model assumptions, motivated by intu-
itive arguments, are stated explicitly and are open
for discussion. Empirical validation on a colloca-
tion extraction task has confirmed the usefulness
of the model, indicating that it represents a lower
bound on the variability of evaluation results. On
the basis of this model, I have developed appro-
priate significance tests for the evaluation of rank-
ing methods. These tests are implemented in the
UCS toolkit, which was used to produce the graphs
in this paper and can be downloaded from http:
//www.collocations.de/.
References
Alan Agresti. 1990. Categorical Data Analysis.
John Wiley & Sons, New York.
Kenneth Church, William Gale, Patrick Hanks, and
Donald Hindle. 1991. Using statistics in lexical
analysis. In Lexical Acquisition: Using On-line
Resources to Build a Lexicon, pages 115?164.
Lawrence Erlbaum.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Computa-
tional Linguistics, 19(1):61?74.
Stefan Evert and Brigitte Krenn. 2001. Methods
for the qualitative evaluation of lexical associa-
tion measures. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics, pages 188?195, Toulouse, France.
Stefan Evert. 2004. An on-line reposi-
tory of association measures. http:
//www.collocations.de/AM/.
Hannah Kermes. 2003. Off-line (and On-line) Text
Analysis for Computational Lexicography. Ph.D.
thesis, IMS, University of Stuttgart. Arbeitspa-
piere des Instituts fu?r Maschinelle Sprachverar-
beitung (AIMS), volume 9, number 3.
Brigitte Krenn and Stefan Evert. 2001. Can we
do better than frequency? a case study on ex-
tracting pp-verb collocations. In Proceedings of
the ACL Workshop on Collocations, pages 39?46,
Toulouse, France, July.
Brigitte Krenn. 2000. The Usual Suspects: Data-
Oriented Models for the Identification and Rep-
resentation of Lexical Collocations., volume 7 of
Saarbru?cken Dissertations in Computational Lin-
guistics and Language Technology. DFKI & Uni-
versita?t des Saarlandes, Saarbru?cken, Germany.
E. L. Lehmann. 1991. Testing Statistical Hypothe-
ses. Wadsworth, 2nd edition.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. MIT Press, Cambridge, MA.
R Development Core Team, 2003. R: A language
and environment for statistical computing. R
Foundation for Statistical Computing, Vienna,
Austria. ISBN 3-900051-00-3. See also http:
//www.r-project.org/.
Alexander Yeh. 2000. More accurate tests for the
statistical significance of result differences. In
Proceedings of the 18th International Conference
on Computational Linguistics (COLING 2000),
Saarbru?cken, Germany.
83
84
85
86
Methods for the Qualitative Evaluation of Lexical Association Measures
Stefan Evert
IMS, University of Stuttgart
Azenbergstr. 12
D-70174 Stuttgart, Germany
evert@ims.uni-stuttgart.de
Brigitte Krenn
Austrian Research Institute
for Artificial Intelligence (?FAI)
Schottengasse 3
A-1010 Vienna, Austria
brigitte@ai.univie.ac.at
Abstract
This paper presents methods for a qual-
itative, unbiased comparison of lexical
association measures and the results we
have obtained for adjective-noun pairs
and preposition-noun-verb triples ex-
tracted from German corpora. In our
approach, we compare the entire list
of candidates, sorted according to the
particular measures, to a reference set
of manually identified ?true positives?.
We also show how estimates for the
very large number of hapaxlegomena
and double occurrences can be inferred
from random samples.
1 Introduction
In computational linguistics, a variety of (statis-
tical) measures have been proposed for identify-
ing lexical associations between words in lexi-
cal tuples extracted from text corpora. Methods
used range from pure frequency counts to infor-
mation theoretic measures and statistical signifi-
cance tests. While the mathematical properties of
those measures have been extensively discussed,1
the strategies employed for evaluating the iden-
tification results are far from adequate. Another
crucial but still unsolved issue in statistical col-
location identification is the treatment of low-
frequency data.
In this paper, we first specify requirements for a
qualitative evaluation of lexical association mea-
1See for instance (Manning and Sch?tze, 1999, chap-
ter 5), (Kilgarriff, 1996), and (Pedersen, 1996).
sures (AMs). Based on these requirements, we
introduce an experimentation procedure, and dis-
cuss the evaluation results for a number of widely
used AMs. Finally, methods and strategies for
handling low-frequency data are suggested.
The measures2 ? Mutual Information (   )
(Church and Hanks, 1989), the log-likelihood
ratio test (Dunning, 1993), two statistical tests:
t-test and  -test, and co-occurrence frequency ?
are applied to two sets of data: adjective-noun
(AdjN) pairs and preposition-noun-verb (PNV)
triples, where the AMs are applied to (PN,V)
pairs. See section 3 for a description of the base
data. For evaluation of the association measures,

-best strategies (section 4.1) are supplemented
with precision and recall graphs (section 4.2) over
the complete data sets. Samples comprising par-
ticular frequency strata (high versus low frequen-
cies) are examined (section 4.3). In section 5,
methods for the treatment of low-frequency data,
single (hapaxlegomena) and double occurrences
are discussed. The significance of differences be-
tween the AMs is addressed in section 6.
2 The Qualitative Evaluation of
Association Measures
2.1 State-of-the-art
A standard procedure for the evaluation of AMs is
manual judgment of the  -best candidates identi-
fied in a particular corpus by the measure in ques-
tion. Typically, the number of true positives (TPs)
2For a more detailed description of these measures
and relevant literature, see (Manning and Sch?tze, 1999,
chapter 5) or http://www.collocations.de/EK/,
where several other AMs are discussed as well.
among the 50 or 100 (or slightly more) highest
ranked word combinations is manually identified
by a human evaluator, in most cases the author
of the paper in which the evaluation is presented.
This method leads to a very superficial judgment
of AMs for the following reasons:
(1) The identification results are based on small
subsets of the candidates extracted from the cor-
pus. Consequently, results achieved by individ-
ual measures may very well be due to chance (cf.
sections 4.1 and 4.2), and evaluation with respect
to frequency strata is not possible (cf. section
4.3). (2) For the same reason, it is impossible
to determine recall values, which are important
for many practical applications. (3) The introduc-
tion of new measures or changes to the calculation
methods require additional manual evaluation, as
new  -best lists are generated.
2.2 Requirements
To improve the reliability of the evaluation re-
sults, a number of properties need to be con-
trolled. We distinguish between two classes:
(1) Characteristics of the set of candidate data
employed for collocation identification: (i) the
syntactic homogeneity of the base data, i.e.,
whether the set of candidate data consists only of
adjective-noun, noun-verb, etc. pairs or whether
different types of word combinations are mixed;
(ii) the grammatical status of the individual word
combinations in the base set, i.e., whether they
are part of or constitute a phrase or simply co-
occur within a given text window; (iii) the per-
centage of TPs in the base set, which is typically
higher among high-frequency data than among
low-frequency data.
(2) The evaluation strategies applied: Instead
of examining only a small sample of  -best can-
didates for each measure as it is common practice,
we make use of recall and precision values for  -
best samples of arbitrary size, which allows us to
plot recall and precision curves for the whole set
of candidate data. In addition, we compare preci-
sion curves for different frequency strata.
3 The Base Data
The base data for our experiments are extracted
from two corpora which differ with respect to size
and text type. The base sets also differ with re-
spect to syntactic homogeneity and grammatical
correctness. Both candidate sets have been man-
ually inspected for TPs.
The first set comprises bigrams of adjacent,
lemmatized AdjN pairs extracted from a small
(  
	 word) corpus of freely available Ger-
man law texts.3 Due to the extraction strategy, the
data are homogeneous and grammatically correct,
i.e., there is (almost) always a grammatical de-
pendency between adjacent adjectives and nouns
in running text. Two human annotators indepen-
dently marked candidate pairs perceived as ?typ-
ical? combinations, including idioms ((die) hohe
See, ?the high seas?), legal terms (?ble Nachrede,
?slander?), and proper names (Rotes Kreuz, ?Red
Cross?). Candidates accepted by either one of the
annotators were considered TPs.
The second set consists of PNV triples ex-
tracted from an 8 million word portion of the
Frankfurter Rundschau Corpus4, in which part-
of-speech tags and minimal PPs were identified.5
The PNV triples were selected automatically such
that the preposition and the noun are constituents
of the same PP, and the PP and the verb co-occur
within a sentence. Only main verbs were con-
sidered and full forms were reduced to bases.6
The PNV data are partially inhomogeneous and
not fully grammatically correct, because they in-
clude combinations with no grammatical relation
between PN and V. PNV collocations were man-
ually annotated. The criteria used for the dis-
tinction between collocations and arbitrary word
combinations are: There is a grammatical rela-
tion between the verb and the PP, and the triple
can be interpreted as support verb construction
and/or a metaphoric or idiomatic reading is avail-
able, e.g.: zur Verf?gung stellen (at_the availabil-
ity put, ?make available?), am Herzen liegen (at
the heart lie, ?have at heart?).7
3See (Schmid, 1995) for a description of the part-of-
speech tagger used to identify adjectives and nouns in the
corpus.
4The Frankfurter Rundschau Corpus is part of the Euro-
pean Corpus Initiative Multilingual Corpus I.
5See (Skut and Brants, 1998) for a description of the tag-
ger and chunker.
6Mmorph ? the MULTEXT morphology tool provided by
ISSCO/SUISSETRA, Geneva, Switzerland ? has been em-
ployed for determining verb infinitives.
7For definitions of and literature on idioms, metaphors
and support verb constructions (Funktionsverbgef?ge) see
for instance (Bu?mann, 1990).
AdjN data PNV data
total 11 087 total 294 534
 
 4 652
 
	 14 654
colloc. 15.84% colloc. 6.41%

 
 
= 737

 
	
= 939
Table 1: Base sets used for evaluation
General statistics for the AdjN and PNV base
sets are given in Table 1. Manual annotation was
performed for AdjN pairs with frequency   
and PNV triples with
 
	
only (see section
5 for a discussion of the excluded low-frequency
candidates).
4 Experimental Setup
After extraction of the base data and manual iden-
tification of TPs, the AMs are applied, resulting in
an ordered candidate list for each measure (hence-
forth significance list, SL). The order indicates the
degree of collocativity. Multiple candidates with
identical scores are listed in random order. This is
necessary, in particular, when co-occurrence fre-
quency is used as an association measure.
4.1  -Best Lists
In this approach, the set of the  highest ranked
word combinations is evaluated for each measure,
and the proportion of TPs among this  -best list
(the precision) is computed. Another measure of
goodness is the proportion of TPs in the base data
that are also contained in the  -best list (the re-
call). While precision measures the quality of the

-best lists produced, recall measures their cov-
erage, i.e., how many of all true collocations in
the corpus were identified. The most problematic
aspect here is that conclusions drawn from  -best
lists for a single (and often small) value of  are
only snapshots and likely to be misleading.
For instance, considering the set of AdjN base
data with
 	

we might arrive at the following
results (Table 2 gives the precision values of the
 highest ranked word combinations with 

    ): As expected from the results of other
studies (e.g. Lezius (1999)), the precision of  
is significantly lower than that of log-likelihood,8
8This is to a large part due to the fact that  systemati-
cally overestimates the collocativity of low-frequency pairs,
cf. section 4.3.
whereas the t-test competes with log-likelihood,
especially for larger values of  . Frequency leads
to clearly better results than    and   , and, for


 
, comes close to the accuracy of t-test and
log-likelihood.
Adjective-Noun Combinations


  


 
Log-Likelihood 65.00% 42.80%
t-Test 57.00% 42.00%
 36.00% 34.00%
Mutual Information 23.00% 23.00%
Frequency 51.00% 41.20%
Table 2: Precision values for  -best AdjN pairs.
4.2 Precision and Recall Graphs
For a clearer picture, however, larger portions of
the SLs need to be examined. A well suited means
for comparing the goodness of different AMs are
the precision and recall graphs obtained by step-
wise processing of the complete SLs (Figures 1 to
10 below).9
The  -axis represents the percentage of data
processed in the respective SL, while the  -
axis represents the precision (or recall) values
achieved. For instance, the precision values for


  
and 
   for the AdjN data can be
read from the  -axis in Figure 1 at positions where



 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 904?911,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Words and Echoes: Assessing and Mitigating
the Non-Randomness Problem in Word Frequency Distribution Modeling
Marco Baroni
CIMeC (University of Trento)
C.so Bettini 31
38068 Rovereto, Italy
marco.baroni@unitn.it
Stefan Evert
IKW (University of Osnabru?ck)
Albrechtstr. 28
49069 Osnabru?ck, Germany
stefan.evert@uos.de
Abstract
Frequency distribution models tuned to
words and other linguistic events can pre-
dict the number of distinct types and their
frequency distribution in samples of arbi-
trary sizes. We conduct, for the first time,
a rigorous evaluation of these models based
on cross-validation and separation of train-
ing and test data. Our experiments reveal
that the prediction accuracy of the models
is marred by serious overfitting problems,
due to violations of the random sampling as-
sumption in corpus data. We then propose
a simple pre-processing method to allevi-
ate such non-randomness problems. Further
evaluation confirms the effectiveness of the
method, which compares favourably to more
complex correction techniques.
1 Introduction
Large-Number-of-Rare-Events (LNRE) models
(Baayen, 2001) are a class of specialized statistical
models that allow us to estimate the characteristics
of the distribution of type probabilities in type-rich
linguistic populations (such as words) from limited
samples (our corpora). They also allow us to
extrapolate quantities such as vocabulary size (the
number of distinct types) and the number of hapaxes
(types occurring just once) beyond a given corpus or
make predictions for completely unseen data from
the same underlying population.
LNREmodels have applications in theoretical lin-
guistics, e.g. for comparing the type richness of mor-
phological or syntactic processes that are attested to
different degrees in the data (Baayen, 1992). Con-
sider for example a very common prefix such as re-
and a rather rare prefix such as meta-. With LNRE
models we can answer questions such as: If we
could obtain as many tokens of meta- as we have
of re-, would we also see as many distinct types?
In other words, is the prefix meta- as productive as
the prefix re-? Practical NLP applications, on the
other hand, include estimating how many out-of-
vocabulary words we will encounter given a lexicon
of a certain size, or making informed guesses about
type counts in very large data sets (e.g., how many
typos are there on the Internet?)
In this paper, after introducing LNRE models
(Section 2), we present an evaluation of their per-
formance based on separate training and test data
as well as cross-validation (Section 3). As far as
we know, this is the first time that such a rigorous
evaluation has been conducted. The results show
how evaluating on the training set, a common strat-
egy in LNRE research, favours models that overfit
the training data and perform poorly on unseen data.
They also confirm the observation by Evert and Ba-
roni (2006) that current LNRE models achieve only
unsatisfactory prediction accuracy, and this is the is-
sue we turn to in the second part of the paper (Sec-
tion 4). Having identified the violation of the ran-
dom sampling assumption by real-world data as one
of the main factors affecting the quality of the mod-
els, we present a new approach to alleviating non-
randomness problems. Further evaluation shows our
solution to outperform Baayen?s (2001) partition-
adjustment method, the former state-of-the-art in
non-randomness correction. Section 5 concludes by
904
pointing out directions for future work.
2 LNRE models
Baayen (2001) introduces a family of models for
Zipf-like frequency distributions of linguistic pop-
ulations, referred to as LNRE models. Such a lin-
guistic population is formally described by a finite
or countably infinite set of types ?i and their occur-
rence probabilities pii. Word frequency models are
not concerned with the probabilities (i.e., relative
frequencies) of specific individual types, but rather
the overall distribution of these probabilities.
Numbering the types in order of decreasing prob-
ability (pi1 ? pi2 ? pi3 ? . . ., called a popula-
tion Zipf ranking), we can specify a LNRE model
for their distribution as a function that computes pii
from the Zipf rank i of ?i. For instance, the Zipf-
Mandelbrot law1 is defined by the equation
pii =
C
(i + b)a
(1)
with parameters a > 1 and b > 0. It is mathemati-
cally more convenient to formulate LNRE models in
terms of a type density function g(pi) on the interval
pi ? [0, 1], such that
? B
A
g(pi) dpi (2)
is the (approximate) number of types ?i with A ?
pii ? B. Evert (2004) shows that Zipf-Mandelbrot
corresponds to a type density of the form
g(pi) :=
{
C ? pi???1 A ? pi ? B
0 otherwise
(3)
with parameters 0 < ? < 1 and 0 ? A < B.2
Models that are formulated in terms of such a type
density g have many direct applications (e.g. using g
as a Bayesian prior), and we refer to them as proper
LNRE models.
Assuming that a corpus of N tokens is a random
sample from such a population, we can make pre-
dictions about lexical statistics such as the number
1The Zipf-Mandelbrot law is an extension of Zipf?s law
(which has a = 1 and b = 0). While the latter originally refers
to type frequencies in a given sample, the Zipf-Mandelbrot law
is formulated for type probabilities in a population.
2In this equation, C is a normalizing constant required in
order to ensure
R 1
0 pig(pi) dpi = 1, the equivalent of
P
i pii = 1.
V (N) of different types in the corpus (the vocab-
ulary size), the number V1(N) of hapax legomena
(types occurring just once), as well as the further dis-
tribution of type frequencies Vm(N). Since the pre-
cise values would be different from sample to sam-
ple, the model predictions are given by expectations
E[V (N)] and E[Vm(N)], which can be computed
with relative ease from the type density function g.
By comparing expected and observed values of V
and Vm (for the lowest frequency ranks, usually up
to m = 15), the parameters of a LNRE model can
be estimated (we refer to this as training the model),
allowing inferences about the population (such as
the total number of types in the population) as well
as further applications of the estimated type density
(e.g. for Good-Turing smoothing). Since we can cal-
culate expected values for samples of arbitrary size
N , we can use the trained model to predict how
many new types would be seen in a larger corpus,
how many hapaxes there would be, etc. This kind of
vocabulary growth extrapolation has become one of
the most important applications of LNRE models in
linguistics and NLP.
A detailed account of the mathematics of LNRE
models can be found in Baayen (2001, Ch. 2).
Baayen describes two LNRE models, lognormal
and GIGP, as well as several other approaches (in-
cluding a version of Zipf?s law and the Yule-Simon
model) that are not based on a type density and
hence do not qualify as proper LNRE models. Two
LNRE models based on Zipf?s law, ZM and fZM, are
introduced by Evert (2004).
In the following, we will only consider proper
LNRE models because of their considerably greater
utility, and because their performance in extrapo-
lation tasks appears to be better than, or at least
comparable to, the other models (Evert and Baroni,
2006). In addition, we exclude the lognormal model
because of its computational complexity and numer-
ical instability.3 In initial evaluation experiments,
the performance of lognormal was also inferior to
the remaining three models (ZM, fZM and GIGP).
Note that ZM is the most simplistic model, with only
2 parameters and assuming an infinite population
vocabulary, while fZM and GIGP have 3 parameters
3There are no closed form equations for the expectations of
the lognormal model, which have to be calculated by numerical
integration.
905
and can model populations of different sizes.
3 Evaluation of LNRE models
LNRE models are traditionally evaluated by look-
ing at how well expected values generated by them
fit empirical counts extracted from the same data-
set used for parameter estimation, often by visual
inspection of differences between observed and pre-
dicted data in plots. More rigorously, Baayen (2001)
and Evert (2004) compare the frequency distribu-
tion observed in the training set to the one predicted
by the model with a multivariate chi-squared test.
As we will show below, evaluating standard LNRE
models on the same data that were used to estimate
their parameters favours overfitting, which results in
poor performance on unseen data.
Evert and Baroni (2006) attempt, for the first time,
to evaluate LNRE models on unseen data. However,
rather than splitting the data into separate training
and test sets, they evaluate the models in an extra-
polation setting, where the parameters of the model
are estimated on a subset of the data used for testing.
Evert and Baroni do not attempt to cross-validate the
results, and they do not provide a quantitative evalu-
ation, relying instead on visual inspection of empir-
ical and observed vocabulary growth curves.
3.1 Data and procedure
We ran our experiments with three corpora in differ-
ent languages and representing different textual ty-
pologies: the British National Corpus (BNC), a ?bal-
anced? corpus of British English of about 100 mil-
lion tokens illustrating different communicative set-
tings, genres and topics; the deWaC corpus, a Web-
crawled corpus of about 1.5 billion German words;
and the la Repubblica corpus, an Italian newspaper
corpus of about 380 million words.4
From each corpus, we extracted 20 non-
overlapping samples of randomly selected docu-
ments, amounting to a total of 4 million tokens each
(punctuation marks and entirely non-alphabetical to-
kens were removed before sampling, and all words
were converted to lowercase). Each of these sam-
ples was then split into a training set of 1 million to-
kens (the training size N0) and a test set of 3 million
4See www.natcorp.ox.ac.uk, http://wacky.
sslmit.unibo.it and http://sslmit.unibo.it/
repubblica
tokens. The documents in the la Repubblica sam-
ples were ordered chronologically before splitting,
to simulate a typical scenario arising when working
with newspaper data, where the data available for
training precede, chronologically, the data one wants
to generalize to.
We estimate parameters of the ZM, fZM and
GIGP models on each training set, using the zipfR
toolkit.5 The models are then used to predict the
expected number of distinct types, i.e., vocabulary
size V , at sample sizes of 1, 2 and 3 million tokens,
equivalent to 1, 2 and 3 times the size of the training
set (we refer to these as the prediction sizesN0, 2N0
and 3N0, respectively). Finally, the expected vo-
cabulary size E[V (N)] is compared to the observed
value V (N) in the test set for N = N0, N = 2N0
and N = 3N0. We also look at V1(N), the number
of hapax legomena, in the same way.
Our main focus is V prediction, since this is by
far the most useful measure in practical applica-
tions, where we are typically interested in knowing
how many types (or how many types belonging to
a certain category) we will see as our sample size
increases (How many typos are there on the Web?
How many types with prefix meta- would we see
if we had as many types of meta- as we have of
re-?) Hapax legomena counts, on the other hand,
play a central role in quantifying morphological pro-
ductivity (Baayen, 1992) and they give us a first in-
sight into how good the models are at predicting fre-
quency distributions, besides vocabulary size (as we
will see, a model?s success in predicting V does not
necessary imply that the model is also capturing the
right frequency distribution).
For all models, corpora and prediction sizes,
goodness-of-fit of the model on the training set
is measured with a multivariate chi-squared test
(Baayen, 2001, 118-122). Performance of the mod-
els in prediction of V is assessed via relative error,
computed for each of the 20 samples from a corpus
and the 3 prediction sizes as follows:
e =
E[V (N)]? V (N)
V (N)
where N = k ? N0 is the prediction size (for k =
1, 2, 3), V (N) is the observed V in the relevant test
5http://purl.org/stefan.evert/zipfR
906
set at size N , and E[V (N)] is the corresponding ex-
pected V predicted by a model.6
For each corpus and prediction size we obtain 20
values ei (viz., e1, . . . , e20). As a summary measure,
we report the square root of the mean square relative
error (rMSE) calculated according to
?
rMSE =
?
?
?
? 1
20
?
20?
i=1
(ei)2
This gives us an overall assessment of prediction ac-
curacy (we take the square root to obtain values on
the same scale as relative errors, and thus easier to
interpret). We complement rMSEs with reports on
the average relative error (indicating whether there
is a systematic under- or overestimation bias) and its
asymptotic 95% confidence intervals, based on the
empirical standard deviation of the ei across the 20
trials (the confidence intervals are usually somewhat
larger than the actual range of values found in the
experiments, so they should be seen as ?pessimistic
estimates? of the actual variance).
3.2 Results
The panels of Figure 1 report rMSE values for the
3 corpora and for each prediction size. For now,
we focus on the first 3 histograms of each panel,
that present rMSEs for the 3 LNRE models intro-
duced above: ZM, fZM and GIGP (the remaining
histograms will be discussed later).7
For all corpora and all extrapolation sizes beyond
N0, the simple ZM model outperforms the more so-
phisticated fZM and GIGP models (which seem to
be very similar to each other). Even at the largest
prediction size of 3N0, ZM?s rMSE is well below
10%, whereas the other models have, in the worst
case (BNC 3N0), a rMSE above 15%. Figure 2
presents plots of average relative error and its em-
pirical confidence intervals (again, focus for now on
the ZM, fZM and GIGP results; the rest of the figure
is discussed later). We see that the poor performance
6We normalize by V (N) rather than (a function of)
E[V (N)] because in the latter case we would favour models
that overestimate V , compared to ones that are equally ?close?
to the correct value but underestimate V .
7A table with the full numerical results is available upon
request; we find, however, that graphical summaries such as
those presented in this paper make the results easier to interpret.
of fZM and GIGP is due to their tendency to under-
estimate the true vocabulary size V , while variance
is comparable across models.
The rMSEs of V1 prediction are reported in Fig-
ure 3. V1 prediction performance is poorer across
the board, and ZM is no longer outperforming the
other models. For space reasons, we do not present
relative error and variance plots for V1, but the gen-
eral trends are the same observed for V , except that
the bias of ZM towards V1 overestimation is much
clearer than for V .
Interestingly, goodness-of-fit on the training data
is not a good predictor of V and V1 prediction per-
formance on unseen data. This is shown in Figure
4, which plots rMSE for prediction of V against
goodness-of-fit (quantified by multivariate X2 on
the training set, as discussed above) for all corpora
and LNREmodels at the 3N0 prediction size (but the
same patterns emerge at other prediction sizes and
with V1). The larger X2, the poorer the training set
fit; the larger rMSE, the worse the prediction. Thus,
ideally, we should see a positive correlation between
X2 and rMSE. Focusing for now on the circles (pin-
pointing the ZM, fZM and GIGP models), we see
that there is instead a negative correlation between
goodness of fit on the training set and quality of pre-
diction on unseen data.8
First, these results indicate that, if we take good-
ness of fit on the training set as a criterion for choos-
ing the best model (as done by Baayen and Evert),
we end up selecting the worst model for actual pre-
diction tasks. This is, we believe, a very strong
case for applying the split train-test cross-validation
method used in other areas of statistical NLP to fre-
quency distribution modeling. Second, the data sug-
gest that the more sophisticated models are overfit-
ting the training set, leading to poorer performance
than the simpler ZM on unseen data. We turn now to
what we think is the main cause for this overfitting.
4 Non-randomness and echoes
The results in the previous section indicate that the
V s predicted by LNRE models are at best ?ballpark
estimates? (and V1 predictions, with a relative error
that is often above 20%, do not even qualify as plau-
8With correlation coefficients of r < ?.8, significant at the
0.01 level despite the small sample size.
907
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V] vs. V on test set (BNC)
rMSE  
  
(%)
0
5
10
15
20
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V] vs. V on test set (DEWAC)
rMSE  
  
(%)
0
5
10
15
20
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V] vs. V on test set (REPUBBLICA)
rMSE  
  
(%)
0
5
10
15
20
Figure 1: rMSEs of predicted V on the BNC, deWaC and la Repubblica data-sets
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
Relative error: E[V] vs. V on test set (BNC)
relativ
e error
 (%)
?
40
?
20
0
20
40
l
l l
l l l
l N02N03N0
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
Relative error: E[V] vs. V on test set (DEWAC)
relativ
e error
 (%)
?
20
?
10
0
10
20
l
l l
l l
l
l N02N03N0
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
Relative error: E[V] vs. V on test set (REPUBBLICA)
relativ
e error
 (%)
?
20
?
10
0
10
20
l
l l
l l
l
l N02N03N0
Figure 2: Average relative errors and asymptotic 95% confidence intervals of V prediction on BNC, deWaC
and la Repubblica data-sets
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V1] vs. V1 on test set (BNC)
rMSE  
  
(%)
0
10
20
30
40
50
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V1] vs. V1 on test set (DEWAC)
rMSE  
  
(%)
0
10
20
30
40
50
ZM fZM GIGP fZMecho GIGPecho GIGPpartition
N02N03N0
rMSE for E[V1] vs. V1 on test set (REPUBBLICA)
rMSE  
  
(%)
0
10
20
30
40
50
Figure 3: rMSEs of predicted V1 on the BNC, deWaC and la Repubblica data-sets
908
0 5000 10000 15000
0
5
10
15
Accuracy for V on test set (3N0)
X2
rMSE
  
  
(%)
l
ll
l
ll
l
ll
l standard
echomodelpartition?adjusted
Figure 4: Correlation between X2 and V prediction
rMSE across corpora and models
sible ballpark estimates). Although such rough esti-
mates might be more than adequate for many practi-
cal applications, is it possible to further improve the
quality of LNRE predictions?
A major factor hampering prediction quality is
that real texts massively violate the randomness as-
sumption made in LNRE modeling: words, rather
obviously, are not picked at random on the basis
of their population probability (Evert and Baroni,
2006; Baayen, 2001). The topic-driven ?clumpi-
ness? of low frequency content words reduces the
number of hapax legomena and other rare events
used to estimate the parameters of LNRE models,
leading the models to underestimate the type rich-
ness of the population. Interestingly (but unsurpris-
ingly), ZM with its assumption of an infinite pop-
ulation, is less prone to this effect, and thus it has
a better prediction performance than the more so-
phisticated fZM and GIGP models, despite its poor
goodness-of-fit.
The effect of non-randomness is illustrated very
clearly for the BNC (but the same could be shown
for the other corpora) by Figure 5, a comparison
of rMSE for prediction of V from our experiments
above to results obtained on versions of the BNC
samples with words scrambled in random order,
thus forcibly removing non-randomness effects. We
see from this figure that the performance of both
fZM and GIGP improves dramatically when they
are trained and tested on randomized sequences of
words. Interestingly, randomization has instead a
negative effect on ZM performance.
ZM fZM GIGP ZMrandom fZMrandom GIGPrandom
N02N03N0
rMSE for E[V] vs. V on test set (BNC)
rMSE
  
  
(%)
0
5
10
15
20
Figure 5: rMSEs of predicted V on unmodified
vs. randomized versions of the BNC sets
4.1 Previous approaches to non-randomness
While non-randomness is widely acknowledged as
a serious problem for the statistical analysis of cor-
pus data, very few authors have suggested correc-
tion strategies. The key problem of non-random data
seems to be that the occurrence frequencies of a type
in different documents do not follow the binomial
distribution assumed by random sampling models.
One approach is therefore to model this distribu-
tion explicitly, replacing the binomial with its sin-
gle parameter pi by a more complex distribution that
has additional parameters (Church and Gale, 1995;
Katz, 1996). However, these distributions are cur-
rently not applicable to LNRE modeling, which is
based on the overall frequencies of types in a cor-
pus rather than their frequencies in individual doc-
uments. The overall frequencies can only be calcu-
lated by summation over all documents in the cor-
pus, resulting in a mathematically and numerically
intractable model. In addition, the type density g(pi)
would have to be extended to a multi-dimensional
function, requiring a large number of parameters to
be estimated from the data.
Baayen (2001) suggests a different approach,
which partitions the population into ?normal? types
that satisfy the random sampling assumption, and
?totally underdispersed? types, which are assumed
to concentrate all occurrences in the corpus into a
909
single ?burst?. Using a standard LNRE model for
the normal part of the population and a simple lin-
ear growth model for the underdispersed part, ad-
justed values for E[V ] and E[Vm] can easily be cal-
culated. These so-called partition-adjusted models
(which introduce one additional parameter) are thus
the only viable models for non-randomness correc-
tion in LNRE modeling and have to be considered
the state of the art.
4.2 Echo adjustment
Rather than making more complex assumptions
about the population distribution or the sampling
model, we propose that non-randomness should be
tackled as a pre-processing problem. The issue, we
argue, is really with the way we count occurrences
of types. The fact that a rare topic-specific word oc-
curs, say, four times in a single document does not
make it any less a hapax legomenon for our purposes
than if the word occurred once (this is the case, for
example, of the word chondritic in the BNC, which
occurs 4 times, all in the same scientific document).
We operationalize our intuition by proposing that,
for our purposes, each content word (at least each
rare, topic-specific content word) occurs maximally
once in a document, and all other instances of that
word in the document are really instances of a spe-
cial ?anaphoric? type, whose function is that of
?echoing? the content words in the document. Thus,
in the BNC document mentioned above, the word
chondritic is counted only once, whereas the other
three occurrences are considered as tokens of the
echo type. Thus, we are counting what in the in-
formation retrieval literature is known as document
frequencies. Intuitively, these are less susceptible to
topical clumpiness effects than plain token frequen-
cies. However, by replacing repeated words with
echo tokens, we can stick to a sampling model based
on random word token sampling (rather than docu-
ment sampling), so that the LNRE models can be
applied ?as is? to echo-adjusted corpora.
Echo-adjustment does not affect the sample size
N nor the vocabulary size V , making the interpre-
tation of results obtained with echo-adjusted mod-
els entirely straightforward. N does not change be-
cause repeated types are replaced with echo tokens,
not deleted. V does not change because only re-
peated types are replaced. Thus, no type present in
the original corpus disappears (more precisely, V in-
creases by 1 because of the addition of the echo type,
but given the large size of V this can be ignored for
all practical purposes). Thus, the expected V com-
puted for a specified sample size N with a model
trained on an echo-adjusted corpus can be directly
compared to observed values at N , and to predic-
tions made for the same N by models trained on an
unprocessed corpus. The same is not true for the pre-
diction of the frequency distribution, where, for the
same N , echo-based models predict the distribution
of document frequencies.
We are proposing echoes as a model for the us-
age of (rare) content words. It would be diffi-
cult to decide where the boundary is between top-
ical words that are inserted once in a discourse
and then anaphorically modulated and ?general-
purpose? words that constitute the frame of the dis-
course and can occur multiple times. Luckily, we
do not have to make this decision when estimating
a LNRE model, since model fitting is based on the
distribution of the lowest frequencies. For example,
with the default zipfR model fitting setting, only the
lowest 15 spectrum elements are used to fit the mod-
els. For any reasonably sized corpus, it is unlikely
that function words and common content words will
occur in less than 16 documents, and thus their dis-
tribution will be irrelevant for model fitting. Thus,
we can ignore the issue of what is the boundary be-
tween topical words to be echo-adjusted and general
words, as long as we can be confident that the set
of lowest frequency words used for model fitting be-
long to the topical set.9 This makes practical echo-
adjustment extremely simple, since all we have to
do is to replace all repetitions of a word in the same
document with echo tokens, and estimate the param-
eters of a plain LNRE model with the resulting ver-
sion of the training corpus.
4.3 Experiments with echo adjustment
Using the same training and test sets as in Sec-
tion 3.1, we train the partition-adjusted GIGP model
9The issue becomes more delicate if we want to predict
the frequency spectrum rather than V , since a model trained
on echo-adjusted data will predict echo-adjusted frequencies
across the board. However, in many theoretical and practical
settings only the lowest frequency spectrum elements are of in-
terest, where, again, it is safe to assume that words are highly
topic-dependent, and echo-adjustment is appropriate.
910
implemented in the LEXSTATS toolkit (Baayen,
2001). We estimate the parameters of echo-adjusted
ZM, fZM and GIGP models on versions of the train-
ing corpora that have been pre-processed as de-
scribed above. The performance of the models is
evaluated with the same measures as in Section 3.1
(for prediction of V1, echo-adjusted versions of the
test data are used).
Figure 1 reports the performance of the echo-
adjusted fZM and GIGP models and of partition-
adjusted GIGP (echo-adjusted ZM performed sys-
tematically much worse than the other echo-adjusted
models and typically worse than uncorrected ZM,
and it is not reported in the figure). Both correction
methods lead to a dramatic improvement, bringing
the prediction performance of fZM and GIGP to lev-
els comparable to ZM (with the latter outperforming
the corrected models on the BNC, but being outper-
formed on la Repubblica). Moreover, echo-adjusted
GIGP is as good as partitioned GIGP on la Repub-
blica, and better on both BNC and deWaC, suggest-
ing that the much simpler echo-adjustment method
is at least as good and probably better than Baayen?s
partitioning. The mean error and confidence interval
plots in Figure 2 show that the echo-adjusted models
have a much weaker underestimation bias than the
corresponding unadjusted models, and are compara-
ble to, if not better than, ZM (although they might
have a tendency to display more variance, as clearly
illustrated by the performance of echo-adjusted fZM
on la Repubblica at 3N0 prediction size). Finally,
the echo-adjusted models clearly stand out with re-
spect to ZM when it comes to V1 prediction (Fig-
ure 3), indicating that echo-adjusted versions of the
more sophisticated fZM and GIGP models should
be the focus of future work on improving predic-
tion of the full frequency distribution, rather than
plain ZM. Moreover, echo-adjusted GIGP is outper-
forming partitioned GIGP, and emerging as the best
model overall.10 Reassuringly, for the echoed mod-
els there is a very strong positive correlation between
goodness-of-fit on the training set and quality of pre-
diction, as illustrated for V prediction at 3N0 by
the triangles in Figure 4 (again, the patterns in this
10In looking at the V1 data, it must be kept in mind, how-
ever, that V1 has a different interpretation when predicted by
echo-adjusted models, i.e., it is the number of document-based
hapaxes, the number of types that occur in one document only.
figure represent the general trend for echo-adjusted
models found in all settings).11 This indicates that
the over-fitting problem has been resolved, and for
echo-adjusted models goodness-of-fit on the train-
ing set is a reliable indicator of prediction accuracy.
5 Conclusion
Despite the encouraging results we reported, much
work, of course, remains to be done. Even with
the echo-adjusted models, prediction of V1 suffers
from large errors and prediction of V quickly deteri-
orates with increasing prediction sizeN . If the mod-
els? estimates for 3 times the size of the training set
have acceptable errors of around 5%, for many ap-
plications we might want to extrapolate to 100N0 or
more (recall the example of estimating type counts
for the entire Web). Moreover, echo-adjusted mod-
els make predictions pertaining to the distribution of
document frequencies, rather than plain token fre-
quencies. The full implications of this remain to
be investigated. Finally, future work should system-
atically explore to what extent different textual ty-
pologies are affected by the non-randomness prob-
lem (notice, e.g., that non-randomness seems to be a
greater problem for the BNC than for the more uni-
form la Repubblica corpus).
References
Baayen, Harald. 1992. Quantitative aspects of morpho-
logical productivity. Yearbook of Morphology 1991,
109-150.
Baayen, Harald. 2001. Word frequency distributions.
Dordrecht: Kluwer.
Church, KennethW. andWilliam A. Gale. 1995. Poisson
mixtures. Journal of Natural Language Engineering
1, 163-190.
Evert, Stefan. 2004. A simple LNRE model for random
character sequences. Proceedings of JADT 2004, 411-
422.
Evert, Stefan and Marco Baroni. 2006. Testing the ex-
trapolation quality of word frequency models. Pro-
ceedings of Corpus Linguistics 2005.
Katz, Slava M. 1996. Distribution of content words and
phrases in text and language modeling. Natural Lan-
guage Engineering, 2(2) 15-59.
11With significant correlation coefficients of r = .76 for 2N0
(p < 0.05) and r = .94 for 3N0 (p 0.01).
911
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 29?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
zipfR: Word Frequency Distributions in R
Stefan Evert
IKW (University of Osnabru?ck)
Albrechtstr. 28
49069 Osnabru?ck, Germany
stefan.evert@uos.de
Marco Baroni
CIMeC (University of Trento)
C.so Bettini 31
38068 Rovereto, Italy
marco.baroni@unitn.it
Abstract
We introduce the zipfR package, a power-
ful and user-friendly open-source tool for
LNRE modeling of word frequency distribu-
tions in the R statistical environment. We
give some background on LNRE models,
discuss related software and the motivation
for the toolkit, describe the implementation,
and conclude with a complete sample ses-
sion showing a typical LNRE analysis.
1 Introduction
As has been known at least since the seminal work
of Zipf (1949), words and other type-rich linguis-
tic populations are characterized by the fact that
even the largest samples (corpora) do not contain in-
stances of all types in the population. Consequently,
the number and distribution of types in the avail-
able sample are not reliable estimators of the number
and distribution of types in the population. Large-
Number-of-Rare-Events (LNRE) models (Baayen,
2001) are a class of specialized statistical models
that estimate the distribution of occurrence proba-
bilities in such type-rich linguistic populations from
our limited samples.
LNRE models have applications in many
branches of linguistics and NLP. A typical use
case is to predict the number of different types (the
vocabulary size) in a larger sample or the whole
population, based on the smaller sample available to
the researcher. For example, one could use LNRE
models to infer how many words a 5-year-old child
knows in total, given a sample of her writing. LNRE
models can also be used to quantify the relative
productivity of two morphological processes (as
illustrated below) or of two rival syntactic construc-
tions by looking at their vocabulary growth rate as
sample size increases. Practical NLP applications
include making informed guesses about type counts
in very large data sets (e.g., How many typos are
there on the Internet?) and determining the ?lexical
richness? of texts belonging to different genres. Last
but not least, LNRE models play an important role
as a population model for Bayesian inference and
Good-Turing frequency smoothing (Good, 1953).
However, with a few notable exceptions (such as
the work by Baayen on morphological productivity),
LNRE models are rarely if ever employed in linguis-
tic research and NLP applications. We believe that
this has to be attributed, at least in part, to the lack of
easy-to-use but sophisticated LNRE modeling tools
that are reliable and robust, scale up to large data
sets, and can easily be integrated into the workflow
of an experiment or application. We have developed
the zipfR toolkit in order to remedy this situation.
2 LNRE models
In the field of LNRE modeling, we are not interested
in the frequencies or probabilities of individual word
types (or types of other linguistic units), but rather
in the distribution of such frequencies (in a sam-
ple) and probabilities (in the population). Conse-
quently, the most important observations (in mathe-
matical terminology, the statistics of interest) are the
total number V (N) of different types in a sample of
N tokens (also called the vocabulary size) and the
number Vm(N) of types that occur exactly m times
29
in the sample. The set of values Vm(N) for all fre-
quency ranks m = 1, 2, 3, . . . is called a frequency
spectrum and constitutes a sufficient statistic for the
purpose of LNRE modeling.
A LNRE model M is a population model that
specifies a certain distribution for the type proba-
bilities in the population. This distribution can be
linked to the observable values V (N) and Vm(N)
by the standard assumption that the observed data
are a random sample of size N from this popula-
tion. It is most convenient mathematically to formu-
late a LNRE model in terms of a type density func-
tion g(pi), defined over the range of possible type
probabilities 0 < pi < 1, such that
? b
a g(pi) dpi is
the number of types with occurrence probabilities
in the range a ? pi ? b.1 From the type density
function, expected values E
[
V (N)
]
and E
[
Vm(N)
]
can be calculated with relative ease (Baayen, 2001),
especially for the most widely-used LNRE models,
which are based on Zipf?s law and stipulate a power
law function for g(pi). These models are known as
GIGP (Sichel, 1975), ZM and fZM (Evert, 2004).
For example, the type density of the ZM and fZM
models is given by
g(pi) :=
{
C ? pi???1 A ? pi ? B
0 otherwise
with parameters 0 < ? < 1 and 0 ? A < B.
Baayen (2001) also presents approximate equations
for the variances Var
[
V (N)
]
and Var
[
Vm(N)
]
. In
addition to such predictions for random samples, the
type density g(pi) can also be used as a Bayesian
prior, where it is especially useful for probability es-
timation from low-frequency data.
Baayen (2001) suggests a number of models that
calculate the expected frequency spectrum directly
without an underlying population model. While
these models can sometimes be fitted very well to
an observed frequency spectrum, they do not inter-
pret the corpus data as a random sample from a pop-
ulation and hence do not allow for generalizations.
They also cannot be used as a prior distribution for
Bayesian inference. For these reasons, we do not see
1Since type probabilities are necessarily discrete, such a
type density function can only give an approximation to the true
distribution. However, the approximation is usually excellent
for the low-probability types that are the center of interest for
most applications of LNRE models.
them as proper LNRE models and do not consider
them useful for practical application.
3 Requirements and related software
As pointed out in the previous section, most appli-
cations of LNRE models rely on equations for the
expected values and variances of V (N) and Vm(N)
in a sample of arbitrary size N . The required ba-
sic operations are: (i) parameter estimation, where
the parameters of a LNRE model M are determined
from a training sample of size N0 by comparing
the expected frequency spectrum E
[
Vm(N0)
]
with
the observed spectrum Vm(N0); (ii) goodness-of-fit
evaluation based on the covariance matrix of V and
Vm; (iii) interpolation and extrapolation of vocabu-
lary growth, using the expectations E
[
V (N)
]
; and
(iv) prediction of the expected frequency spectrum
for arbitrary sample size N . In addition, Bayesian
inference requires access to the type density g(pi)
and distribution function G(a) =
? 1
a g(pi) dpi, while
random sampling from the population described by
a LNRE model M is a prerequisite for Monte Carlo
methods and simulation experiments.
Up to now, the only publicly available implemen-
tation of LNRE models has been the lexstats toolkit
of Baayen (2001), which offers a wide range of
models including advanced partition-adjusted ver-
sions and mixture models. While the toolkit sup-
ports the basic operations (i)?(iv) above, it does
not give access to distribution functions or random
samples (from the model distribution). It has not
found widespread use among (computational) lin-
guists, which we attribute to a number of limitations
of the software: lexstats is a collection of command-
line programs that can only be mastered with expert
knowledge; an ad-hoc Tk-based graphical user in-
terfaces simplifies basic operations, but is fully sup-
ported on the Linux platform only; the GUI also has
only minimal functionality for visualization and data
analysis; it has restrictive input options (making its
use with languages other than English very cumber-
some) and works reliably only for rather small data
sets, well below the sizes now routinely encountered
in linguistic research (cf. the problems reported in
Evert and Baroni 2006); the standard parameter es-
timation methods are not very robust without exten-
sive manual intervention, so lexstats cannot be used
30
as an off-the-shelf solution; and nearly all programs
in the suite require interactive input, making it diffi-
cult to automate LNRE analyses.
4 Implementation
First and foremost, zipfR was conceived and de-
veloped to overcome the limitations of the lexstats
toolkit. We implemented zipfR as an add-on library
for the popular statistical computing environment R
(R Development Core Team, 2003). It can easily
be installed (from the CRAN archive) and used off-
the-shelf for standard LNRE modeling applications.
It fully supports the basic operations (i)?(iv), cal-
culation of distribution functions and random sam-
pling, as discussed in the previous section. We have
taken great care to offer robust parameter estimation,
while allowing advanced users full control over the
estimation procedure by selecting from a wide range
of optimization techniques and cost functions. In
addition, a broad range of data manipulation tech-
niques for word frequency data are provided. The
integration of zipfR within the R environment makes
the full power of R available for visualization and
further statistical analyses.
For the reasons outlined above, our software
package only implements proper LNRE models.
Currently, the GIGP, ZM and fZM models are sup-
ported. We decided not to implement another LNRE
model available in lexstats, the lognormal model, be-
cause of its numerical instability and poor perfor-
mance in previous evaluation studies (Evert and Ba-
roni, 2006).
More information about zipfR can be found on its
homepage at http://purl.org/stefan.evert/zipfR/.
5 A sample session
In this section, we use a typical application example
to give a brief overview of the basic functionality of
the zipfR toolkit. zipfR accepts a variety of input for-
mats, the most common ones being type frequency
lists (which, in the simplest case, can be newline-
delimited lists of frequency values) and tokenized
(sub-)corpora (one word per line). Thus, as long as
users can extract frequency data or at least tokenize
the corpus of interest with other tools, they can per-
form all further analysis with zipfR.
Suppose that we want to compare the relative pro-
ductivity of the Italian prefix ri- with that of the
rarer prefix ultra- (roughly equivalent to English re-
and ultra-, respectively), and that we have frequency
lists of the word types containing the two prefixes.2
In our R session, we import the data, create fre-
quency spectra for the two classes, and we plot the
spectra to look at their frequency distribution (the
output graph is shown in the left panel of Figure 1):
ItaRi.tfl <- read.tfl("ri.txt")
ItaUltra.tfl <- read.tfl("ultra.txt")
ItaRi.spc <- tfl2spc(ItaRi.tfl)
ItaUltra.spc <- tfl2spc(ItaUltra.tfl)
> plot(ItaRi.spc,ItaUltra.spc,
+ legend=c("ri-","ultra-"))
We can then look at summary information about
the distributions:
> summary(ItaRi.spc)
zipfR object for frequency spectrum
Sample size: N = 1399898
Vocabulary size: V = 1098
Class sizes: Vm = 346 105 74 43 ...
> summary(ItaUltra.spc)
zipfR object for frequency spectrum
Sample size: N = 3467
Vocabulary size: V = 523
Class sizes: Vm = 333 68 37 15 ...
We see that the ultra- sample is much smaller than
the ri- sample, making a direct comparison of their
vocabulary sizes problematic. Thus, we will use the
fZM model (Evert, 2004) to estimate the parameters
of the ultra- population (notice that the summary of
an estimated model includes the parameters of the
relevant distribution as well as goodness-of-fit infor-
mation):
> ItaUltra.fzm <- lnre("fzm",ItaUltra.spc)
> summary(ItaUltra.fzm)
finite Zipf-Mandelbrot LNRE model.
Parameters:
Shape: alpha = 0.6625218
Lower cutoff: A = 1.152626e-06
Upper cutoff: B = 0.1368204
[ Normalization: C = 0.673407 ]
Population size: S = 8732.724
...
Goodness-of-fit (multivariate chi-squared):
X2 df p
19.66858 5 0.001441900
Now, we can use the model to predict the fre-
quency distribution of ultra- types at arbitrary sam-
ple sizes, including the size of our ri- sample. This
allows us to compare the productivity of the two pre-
fixes by using Baayen?sP , obtained by dividing the
2The data used for illustration are taken from an Italian
newspaper corpus and are distributed with the toolkit.
31
ri?ultra?
Frequency Spectrum
m
V m
0
50
100
150
200
250
300
350
0 200000 600000 1000000
0
200
0
400
0
600
0
800
0
Vocabulary Growth
N
E[[V((
N))]]
ri?ultra?
Figure 1: Left: Comparison of the observed ri- and ultra- frequency spectra. Right: Interpolated ri- vs. ex-
trapolated ultra- vocabulary growth curves.
number of hapax legomena by the overall sample
size (Baayen, 1992):
> ItaUltra.ext.spc<-lnre.spc(ItaUltra.fzm,
+ N(ItaRi.spc))
> Vm(ItaUltra.ext.spc,1)/N(ItaRi.spc)
[1] 0.0006349639
> Vm(ItaRi.spc,1)/N(ItaRi.spc)
[1] 0.0002471609
The rarer ultra- prefix appears to be more produc-
tive than the more frequent ri-. This is confirmed by
a visual comparison of vocabulary growth curves,
that report changes in vocabulary size as sample size
increases. For ri-, we generate the growth curve
by binomial interpolation from the observed spec-
trum, whereas for ultra- we extrapolate using the
estimated LNRE model (Baayen 2001 discuss both
techniques).
> sample.sizes <- floor(N(ItaRi.spc)/100)
+ *(1:100)
> ItaRi.vgc <- vgc.interp(ItaRi.spc,
+ sample.sizes)
> ItaUltra.vgc <- lnre.vgc(ItaUltra.fzm,
+ sample.sizes)
> plot(ItaRi.vgc,ItaUltra.vgc,
+ legend=c("ri-","ultra-"))
The plot (right panel of Figure 1) confirms the
higher (potential) type richness of ultra-, a ?fancier?
prefix that is rarely used, but, when it does get used,
is employed very productively (see discussion of
similar prefixes in Gaeta and Ricca 2003).
References
Baayen, Harald. 1992. Quantitative aspects of morpho-
logical productivity. Yearbook of Morphology 1991,
109?150.
Baayen, Harald. 2001. Word frequency distributions.
Dordrecht: Kluwer.
Evert, Stefan. 2004. A simple LNRE model for random
character sequences. Proceedings of JADT 2004, 411?
422.
Evert, Stefan and Marco Baroni. 2006. Testing the ex-
trapolation quality of word frequency models. Pro-
ceedings of Corpus Linguistics 2005.
Gaeta, Livio and Davide Ricca. 2003. Italian prefixes
and productivity: a quantitative approach. Acta Lin-
guistica Hungarica, 50 89?108.
Good, I. J. (1953). The population frequencies of
species and the estimation of population parameters.
Biometrika, 40(3/4), 237?264.
R Development Core Team (2003). R: A lan-
guage and environment for statistical computing. R
Foundation for Statistical Computing, Vienna, Aus-
tria. ISBN 3-900051-00-3. See also http://www.
r-project.org/.
Sichel, H. S. (1975). On a distribution law for word fre-
quencies. Journal of the American Statistical Associ-
ation, 70, 542?547.
Zipf, George K. 1949. Human behavior and the princi-
ple of least effort. Cambridge (MA): Addison-Wesley.
32
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 110?114, Dublin, Ireland, August 23-29 2014.
Distributional Semantics in R with the wordspace Package
Stefan Evert
Professur f?ur Korpuslinguistik
Friedrich-Alexander-Universit?at Erlangen-N?urnberg
Bismarckstr. 6, 91054 Erlangen, Germany
stefan.evert@fau.de
Abstract
This paper introduces the wordspace package, which turns Gnu R into an interactive laboratory
for research in distributional semantics. The package includes highly efficient implementations
of a carefully chosen set of key functions, allowing it to scale up to real-life data sets.
1 Introduction
Distributional semantic models (DSMs) represent the meaning of a target term (which can be a word
form, lemma, morpheme, word pair, etc.) in the form of a feature vector that records either co-occurrence
frequencies of the target term with a set of feature terms (term-term model) or its distribution across
text units (term-context model). Such DSMs have become an indispensable ingredient in many NLP
applications that require flexible broad-coverage lexical semantics (Turney and Pantel, 2010).
Distributional modelling is an empirical science. DSM representations are determined by a wide
range of parameters such as size and type of the co-occurrence context, feature selection, weighting of
co-occurrence frequencies (often with statistical association measures), distance metric, dimensionality
reduction method and the number of latent dimensions used. Despite recent efforts to carry out systematic
evaluation studies (Bullinaria and Levy, 2007; Bullinaria and Levy, 2012), the precise effects of these
parameters and their relevance for different application settings are still poorly understood.
The wordspace package for Gnu R (R Development Core Team, 2010) aims to provide a flexible,
powerful and easy to use ?interactive laboratory? that enables its users to build DSMs and experiment
with them, but that also scales up to the large models required by real-life applications.
2 Related work
One reason for the popularity of distributional approaches is that even large-scale models can be im-
plemented with relative ease. In the geometric interpretation, most operations involved in building and
using a DSM can be expressed concisely in terms of matrix and vector algebra: matrix multiplication,
inner and outer products, matrix decomposition, and vector norms and metrics.
In order to make DSMs accessible to a large group of users, several dedicated software packages have
been developed. Most of these packages either implement a particular model, limiting their flexibility, or
impose a complex framework of classes, making it hard for users to carry out operations not envisioned
by the package developers. Examples of the first category are HiDeX (Shaoul and Westbury, 2010), a
modern reimplementation of the HAL model, and Semantic Vectors (Widdows and Cohen, 2010), which
enforces a random indexing representation in order to improve scalability.
A typical example of the second category is the S-Space package (Jurgens and Stevens, 2010), which
defines a complete pipeline for building and evaluating a DSM; researchers wishing e.g. to evaluate the
model on a different task need to implement the evaluation procedure in the form of a suitable Java
class. Two Python-based software packages in this category are Gensim (
?
Reh?u?rek and Sojka, 2010) and
DISSECT (Dinu et al., 2013), which has a particular focus on learning compositional models.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
110
In order to avoid locking its users into a particular framework or class of distributional model, the
wordspace package takes a different approach. It builds on the statistical software environment Gnu
R, which provides efficient dense and sparse matrix algebra, sophisticated statistical analysis and visual-
ization, as well as numerous machine learning methods as part of its core library and through a wealth of
add-on packages. R itself is already an excellent environment for the interactive exploration of DSMs.
Like many other R packages, wordspace does not define its own complex framework. It extends
functionality that is already available (and easy to use) with a small set of carefully designed functions
that (i) encapsulate non-trivial R operations in a convenient and user-friendly way and (ii) provide highly
efficient and memory-friendly C implementations of key operations in order to improve scalability. Of
the other DSM software packages available, DISSECT seems closest in spirit to wordspace. There-
fore, it is used as a point of reference for the performance comparison in Sec. 5.
3 The wordspace package
The wordspace package is an open-source project distributed under the GNU General Public License.
Since the pacakge is linked to the R interpreter and its functions are always invoked from interpreted
code, this does not preclude commercial applications involving closed-source components. The package
source code is hosted on R-Forge.
1
It can easily be installed from the Comprehensive R Archive Network
(CRAN), which provides pre-compiled binaries for Windows and Mac OS X.
3.1 Input formats
The most general representation of a distributional model takes the form of a sparse matrix, with entries
specified as a triplet of row label (target term), column label (feature term) and co-occurrence frequency
(cf. left panel of Fig. 1). The wordspace package creates DSM objects from such triplet representa-
tions, which can easily be imported into R from a wide range of file and database formats. Ready-made
import functions are provided for TAB-delimited text files (as used by DISSECT), which may be com-
pressed to save disk space, and for term-document models from the text-mining framework tm for R.
The native input format is a pre-compiled sparse matrix representation generated by the UCS toolkit.
2
In this way, UCS serves as a hub for the preparation of co-occurrence data, which can be collected from
dependency pairs, extracted from a corpus indexed with the IMS Corpus Workbench,
3
or imported from
various other formats such as the Ngram Statistics Package (NSP).
4
3.2 Features
The wordspace package offers flexible convenience functions to filter DSMs by properties of their
rows (targets) and columns (features), combine multiple co-occurrence matrices by rows or by columns,
and merge data obtained from different corpora. Co-occurrence frequencies can be weighted by a tf.idf
scheme or one of various statistical association measures, rescaled e.g. by a logarithmic transformation,
standardized and row-normalized.
Efficient implementations are provided for dimensionality reduction by randomized SVD (Halko et
al., 2009) or random indexing, for computing a distance matrix between a set of row vectors, and for
the identification of the nearest neighbours of a given target term. Additional functions compute centroid
representations for sentence contexts and support the evaluation of DSMs in standard classification, clus-
tering and regression tasks. Several freely available gold standard data sets are included in the package.
Due to its philosophy, wordspace only provides essential functionality that cannot easily be
achieved with basic R functions or does not scale well in the standard implementation. Many further
analyses and operations (e.g. partial least-squares regression for learning compositional DSMs) can be
performed with standard R functions or one of more than 5000 add-on packages available from CRAN.
1
http://wordspace.r-forge.r-project.org/
2
http://www.collocations.de/software.html
3
http://cwb.sourceforge.net/
4
http://ngram.sourceforge.net/
111
dog-n walk-v 6343
dog-n walk-n 2461
dog-n bite-v 1732
cat-n tail-n 1285
cat-n jump-v 541
. . . . . . . . .
noun rel verb f mode
dog subj bite 3 spoken
dog subj bite 12 written
dog obj bite 4 written
dog obj stroke 3 written
. . . . . . . . . . . . . . .
Figure 1: Typical input data: triplet representation of a sparse co-occurrence matrix (left panel) and
verb-noun cooccurrences from the BNC used as example data in Sec. 4 (right panel).
4 Example session
Fig. 3 shows the full set of R commands required to compile and use a DSM with the wordspace
package, based on a built-in table containing co-occurrence counts of verbs and their subject/object
nouns in the British National Corpus (BNC), an excerpt of which is shown in the right panel of Fig. 1.
After loading the package (line 1), we use a standard R function to extract data for the written part of
the BNC (line 3). The dsm() function constructs a basic DSM object from co-occurrence data in various
sparse matrix representations (line 4). Note that multiple entries for the same noun-verb combination are
automatically aggregated, resulting in a 19831 ? 4854 noun-verb co-occurrence matrix. Highly sparse
vectors are unreliable as indicators of word meaning, so we filter out rows and columns with less than
3 nonzero entries using the subset() method for DSM objects. The required nonzero counts have
automatically been added by the dsm() constructor. Since deleting rows and columns changes the
nonzero counts, we apply the process recursively until both constraints are satisfied (line 9).
Co-occurrence counts are then weighted by the log-likelihood association measure, log-transformed
to deskew their distribution, and row vectors are normalized to Euclidean unit length. This is achieved
with a single function call and minimal memory overhead (line 13). For dimensionality reduction, the
efficient randomized SVD algorithm is used (line 15), resulting in a plain R matrix VObj300.
Typical applications of a DSM are to compute distances or cosine similarities between pairs of target
terms (line 17) and to find the nearest neighbours of a given term (line 20). The final DSM can be
evaluated e.g. by comparison with the RG65 data set of semantic similarity ratings (Rubenstein and
Goodenough, 1965). Here, we obtain a Pearson correlation of r = 0.521 (with 95% confidence interval
0.317 . . . 0.679) and a Spearman rank correlation of ? = 0.511 (line 26). The correlation can also be
visualized with a built-in plot method (line 29), shown in the left panel of Fig. 2.
The commands in lines 31?37 illustrate the use of standard R functions for further analysis and vi-
sualization. Here, an implementation of non-metric multidimensional scaling in the R package MASS
produces a semantic map of the nearest neighbours of book. A slightly more polished version of this
plot is shown in the right panel of Fig. 2. The only step that takes a non-negligible amount of time is
dimensionality reduction with randomized SVD (approx. 14 seconds on the reference system, see Sec. 5).
5 Performance comparison
In order to determine the usefulness of the wordspace package for realistically sized data sets, a
benchmark was carried out using the W ?W projection of the Distributional Memory tensor (Baroni
and Lenci, 2010), resulting in a sparse 30686? 30686 matrix with 60 million nonzero entries (6.4% fill
rate). Execution times of key operations and file sizes of the native serialization format are shown in
Table 1 and compared against DISSECT v0.1.0 as the closest ?competitor?. Tests were carried out on a
2012 MacBook Pro with a 2.6 GHz 4-core Intel Core i7 CPU, 16 GiB RAM and a 768 GB SSD.
Runtimes for nearest neighbours (NN) are averaged over 198 nouns, and those for cosine similarity are
averaged over 351 noun pairs. Both sample sets were taken from the WordSim-353 data set. DISSECT
requires about 2.5 GiB RAM to carry out the complete process, while R requires slightly above 4 GiB.
Most of the RAM is needed to load the non-native input format (which happens to be the native format
of DISSECT). For the remaining steps, memory usage remains well below 3 GiB.
112
wordspace DISSECT size (.rda / .pkl)
build model from triples file 186.0 s 503.3 s
save model 57.6 s 1.5 s 228.9 MB / 725.7 MB
normalize row vectors 0.5 s 1.3 s
SVD projection to 300 latent dimensions 353.6 s 296.6 s
save latent vectors 10.4 s 0.4 s 71.5 MB / 185.0 MB
20 nearest neighbours (full matrix) 119 ms 1269 ms
20 nearest neighbours (300 dims) 10 ms 92 ms
cosine similarity (full matrix) 4 ms < 1 ms
cosine similarity (300 dims) < 1 ms < 1 ms
Table 1: Performance comparison: wordspace vs. DISSECT on Distributional Memory.
6 Further development
The wordspace package is under very active development. Main objectives for the near future are (i)
sparse SVD using SVDLIBC (which is more efficient than randomized SVD in certain cases), (ii) sparse
non-negative matrix factorization (NMF), and (iii) built-in support for a wider range of file and database
formats. In addition, new weighting functions and distance metrics are continuously being added.
References
Marco Baroni and Alessandro Lenci. 2010. Distributional Memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?712.
John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word co-occurrence
statistics: A computational study. Behavior Research Methods, 39(3):510?526.
John A. Bullinaria and Joseph P. Levy. 2012. Extracting semantic representations from word co-occurrence
statistics: Stop-lists, stemming and SVD. Behavior Research Methods, 44(3):890?907.
Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. DISSECT ? distributional semantics composition
toolkit. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System
Demonstrations, pages 31?36, Sofia, Bulgaria, August.
N. Halko, P. G. Martinsson, and J[oel] A. Tropp. 2009. Finding structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions. Technical Report 2009-05, ACM, California Institute of
Technology, September.
David Jurgens and Keith Stevens. 2010. The S-Space package: An open source package for word space models.
In Proceedings of the ACL 2010 System Demonstrations, pages 30?35, Uppsala, Sweden, July.
R Development Core Team, 2010. R: A Language and Environment for Statistical Computing. R Foundation for
Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0. See also http://www.r-project.org/.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In Proceedings
of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA.
Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Cyrus Shaoul and Chris Westbury. 2010. Exploring lexical co-occurrence space using HiDEx. Behavior Research
Methods, 42(2):393?413.
Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Dominic Widdows and Trevor Cohen. 2010. The Semantic Vectors package: New algorithms and public tools for
distributional semantics. In IEEE Fourth International Conference on Semantic Computing (ICSC 2010), pages
9?15.
113
l
l
l
ll
l
ll
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l l
l
l ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
0 1 2 3 4
40
50
60
70
80
90
100
Correlation with RG65 ratings
human rating
distr
ibuti
ona
l mo
del
|rho| = 0.511, p = 0.0000, |r| = 0.317 .. 0.679 (8 pairs not found)
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
?30 ?20 ?10 0 10 20 30
?
40
?
30
?
20
?
10
0
10
20
Neighbourhood graph for BOOK
paper
novel
magazine
works
article
textbook
guide
poem
essay leaflet
editiontext
pamphlet
booklet
catalogue
book
Figure 2: Two visualizations created by the sample code in Fig. 2.
1 library(wordspace)
2
3 Triples <- subset(DSM_VerbNounTriples_BNC, mode == "written")
4 VObj <- dsm(target=Triples$noun, feature=Triples$verb, score=Triples$f,
5 raw.freq=TRUE, sort=TRUE)
6 dim(VObj)
7 [1] 19831 4854
8
9 VObj <- subset(VObj, nnzero >= 3, nnzero >= 3, recursive=TRUE)
10 dim(VObj)
11 [1] 12428 3735
12
13 VObj <- dsm.score(VObj, score="simple-ll", transform="log", normalize=TRUE)
14
15 VObj300 <- dsm.projection(VObj, method="rsvd", n=300, oversampling=4)
16
17 pair.distances("book", "paper", VObj300, method="cosine", convert=FALSE)
18 book/paper
19 0.7322982
20 nearest.neighbours(VObj300, "book", n=15) # defaults to angular distance
21 paper novel magazine works article textbook guide poem
22 42.92059 48.03492 49.10742 49.33028 49.54836 49.82660 50.29588 50.37111
23 essay leaflet edition text pamphlet booklet catalogue
24 50.45991 50.53009 50.78630 50.95731 51.12786 51.21351 52.43824
25
26 eval.similarity.correlation(RG65, VObj300, format="HW")
27 rho p.value missing r r.lower r.upper
28 RG65 0.5113531 1.342741e-05 8 0.520874 0.3172827 0.6785674
29 plot(eval.similarity.correlation(RG65, VObj300, format="HW", details=TRUE))
30
31 nn <- nearest.neighbours(VObj300, "book", n=15)
32 nn.terms <- c("book", names(nn)) # nn = distances labelled with the neighbour terms
33 nn.dist <- dist.matrix(VObj300, terms=nn.terms, method="cosine")
34 library(MASS) # a standard R package that includes two MDS implementations
35 mds <- isoMDS(nn.dist, p=2)
36 plot(mds$points, pch=20, col="red")
37 text(mds$points, labels=nn.terms, pos=3)
Figure 3: Complete example code for building, using and evaluating a DSM with wordspace.
114
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 15?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Distributional Semantic Models
Stefan Evert, University of Osnabr?ck
1. DESCRIPTION
Distributional semantic models (DSM) -- also known as "word space" or "distributional 
similarity" models -- are based on the assumption that the meaning of a word can (at 
least to a certain extent) be inferred from its usage, i.e. its distribution in text.  Therefore, 
these models dynamically build semantic representations -- in the form of high-
dimensional vector spaces -- through a statistical analysis of the contexts in which 
words occur.  DSMs are a promising technique for solving the lexical acquisition 
bottleneck by unsupervised learning, and their distributed representation provides a 
cognitively plausible, robust and flexible architecture for the organisation and processing 
of semantic information.
Since the seminal papers of Landauer & Dumais (1997) and Sch?tze (1998), DSMs 
have been an active area of research in computational linguistics.  Amongst many other 
tasks, they have been applied to solving the TOEFL synonym test (Landauer & Dumais 
1997, Rapp 2004), automatic thesaurus construction (Lin 1998), identification of 
translation equivalents (Rapp 1999), word sense induction and discrimination (Sch?tze 
1998), POS induction (Sch?tze 1995), identification of analogical relations (Turney 
2006), PP attachment disambiguation (Pantel & Lin 2000), semantic classification 
(Versley 2008), as well as the prediction of fMRI (Mitchell et al 2008) and EEG (Murphy 
et al 2009) data.  Recent years have seen renewed and rapidly growing interest in 
distributional approaches, as shown by the series of workshops on DSM held at Context 
2007 [1], ESSLLI 2008 [2], EACL 2009 [3], CogSci 2009 [4], NAACL-HLT 2010 [5], ACL 
2010 [6] and ESSLLI 2010 [7].
The proposed tutorial aims to 
- introduce the most common DSM architectures and their parameters, as well as 
prototypical applications;
- equip participants with the mathematical techniques needed for the implementation of 
DSMs, in particular those of matrix algebra;
- illustrate visualisation techniques and mathematical arguments that help in 
understanding the high-dimensional DSM vector spaces and making sense of key 
operations such as SVD dimensionality reduction; and
- provide an overview of current research on DSMs, available software, evaluation tasks 
and future trends.
The tutorial is targeted both at participants who are new to the field and need a 
comprehensive overview of DSM techniques and applications, and at experienced 
scientists who want to get up to speed on current directions in DSM research.
15
An implementation of all methods presented in the tutorial will be provided as 
supplementary material, using the open-source statistical programming language R [8].  
This implementation, which is based on the code and data sets available at [9], is 
intended as a "toy laboratory" for participants, but can also form a sound basis for 
practical applications and further DSM research.
2. TUTORIAL OUTLINE
1) Introduction
    - motivation and brief history of distributional semantics
    - common DSM architectures
    - prototypical applications
    - concrete examples used in the tutorial
2) Taxonomy of DSM parameters including
    - size and type of context window
    - feature scaling (tf.idf, statistical association measures, ...)
    - normalisation and standardisation of rows and/or columns
    - distance/similarity measures: Euclidean, Minkowski p-norms, cosine, entropy-
based, ...
    - dimensionality reduction: feature selection, SVD, random indexing (RI)
3) Elements of matrix algebra for DSM
    - basic matrix and vector operations
    - norms and distances, angles, orthogonality
    - projection and dimensionality reduction
4) Making sense of DSMs: mathematical analysis and visualisation techniques
    - nearest neighbours and clustering
    - semantic maps: PCA, MDS, SOM
    - visualisation of high-dimensional spaces
    - supervised classification based on DSM vectors
    - understanding dimensionality reduction with SVD and RI
    - term-term vs. term-context matrix, connection to first-order association
    - SVD as a latent class model
5) Current research topics and future directions
    - overview of current research on DSMs
    - evaluation tasks and data sets
    - available "off-the-shelf" DSM software
    - limitations and key problems of DSMs
    - trends for future work
Each of the five parts will be compressed into a slot of roughly 30 minutes, leaving a 30-
minute coffee break.  In order to cover the large amount of material in a relatively short 
16
time, the discussion of mathematical and implementational aspects will aim primarily at 
an intuitive understanding of key issues and skip technical details.  Full descriptions are 
provided as part of the handouts and supplementary material, esp. the thoroughly 
commented R implementation.
3. INSTRUCTOR
Stefan Evert
Juniorprofessor of Computational Linguistics
University of Osnabr?ck, Germany
Stefan Evert has studied mathematics, physics and English linguistics, and holds a PhD 
degree in computational linguistics.  His research interests include the statistical 
analysis of corpus frequency data (significance tests in corpus linguistics, statistical 
association measures, Zipf's law and word frequency distributions), quantitative 
approaches to lexical semantics (collocations, multiword expressions and DSM), as well 
as processing large text corpora (IMS Open Corpus Workbench, data model and query 
language of the Nite XML Toolkit, tools for the Web as corpus).  Stefan Evert has 
published extensively on collocations and association measures, has co-organised 
several workshops on multiword expressions as well as the ESSLLI 2008 workshop on 
distributional lexical semantics, and has co-taught an advanced course on DSM at 
ESSLLI 2009 with Alessandro Lenci, as well as a course on Computational Lexical 
Semantics with Gemma Boleda.  The main focus of his current research is on 
understanding and improving DSMs for applications in natural language processing and 
lexical semantics.
URLS
[1] http://clic.cimec.unitn.it/marco/beyond_words/
[2] http://wordspace.collocations.de/doku.php/esslli:start
[3] http://art.uniroma2.it/gems/
[4] http://www.let.rug.nl/disco2009/
[5] http://sites.google.com/site/compneurowsnaacl10/
[6] http://art.uniroma2.it/gems010/
[7] http://clic.cimec.unitn.it/roberto/ESSLLI10-dsm-workshop/
[8] http://www.R-project.org/
[9] http://wordspace.collocations.de/doku.php/course:schedule
REFERENCES
Landauer, Thomas K. and Dumais, Susan T. (1997). A solution to Plato's problem: The 
latent semantic analysis theory of acquisition, induction and representation of 
knowledge. Psychological Review, 104(2), 211-240.
17
Lin, Dekang (1998). Automatic retrieval and clustering of similar words. In Proceedings 
of the 17th International Conference on Computational Linguistics (COLING-ACL 1998), 
pages 768-774, Montreal, Canada.
Mitchell, Tom M.; Shinkareva, Svetlana V.; Carlson, Andrew; Chang, Kai-Min; Malave, 
Vicente L.; Mason, Robert A.; Just, Marcel Adam (2008). Predicting human brain activity 
associated with the meanings of nouns. Science, 320, 1191-1195.
Murphy, Brian; Baroni, Marco; Poesio, Massimo (2009). EEG responds to conceptual 
stimuli and corpus semantics. In Proceedings of the 2009 Conference on Empirical 
Methods in Natural Language Processing, pages 619-627, Singapore.
Pantel, Patrick; Lin, Dekang (2000). An unsupervised approach to prepositional phrase 
attachment using contextually similar words. In Proceedings of the 38th Annual Meeting 
of the Association for Computational Linguistics, Hongkong, China.
Rapp, Reinhard (1999). Automatic identification of word translations from unrelated 
English and German corpora. In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, Maryland.
Rapp, Reinhard (2004). A freely available automatically generated thesaurus of related 
words. In Proceedings of the 4th International Conference on Language Resources and 
Evaluation (LREC 2004), pages 395-398.
Sch?tze, Hinrich (1995). Distributional part-of-speech tagging. In Proceedings of the 7th 
Conference of the European Chapter of the Association for Computational Linguistics 
(EACL 1995), pages 141-148.
Sch?tze, Hinrich (1998). Automatic word sense discrimination. Computational 
Linguistics, 24(1), 97-123.
Turney, Peter D. (2006). Similarity of semantic relations. Computational Linguistics, 32
(3), 379-416.
Versley, Yannick (2008). Decorrelation and shallow semantic patterns for distributional 
clustering of nouns and verbs. In Proceedings of the ESSLLI Workshop on Distributional 
Lexical Semantics, pages 55-62, Hamburg, Germany.
18
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 181?186, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
KLUE-CORE: A regression model of semantic textual similarity
Paul Greiner and Thomas Proisl and Stefan Evert and Besim Kabashi
Friedrich-Alexander-Universit?t Erlangen-N?rnberg
Department Germanistik und Komparatistik
Professur f?r Korpuslinguistik
Bismarckstr. 6
91054 Erlangen, Germany
{paul.greiner,thomas.proisl,stefan.evert,besim.kabashi}@fau.de
Abstract
This paper describes our system entered for the
*SEM 2013 shared task on Semantic Textual
Similarity (STS). We focus on the core task
of predicting the semantic textual similarity of
sentence pairs.
The current system utilizes machine learn-
ing techniques trained on semantic similarity
ratings from the *SEM 2012 shared task; it
achieved rank 20 out of 90 submissions from
35 different teams. Given the simple nature of
our approach, which uses only WordNet and
unannotated corpus data as external resources,
we consider this a remarkably good result, mak-
ing the system an interesting tool for a wide
range of practical applications.
1 Introduction
The *SEM 2013 shared task on Semantic Textual
Similarity (Agirre et al, 2013) required participants
to implement a software system that is able to pre-
dict the semantic textual similarity (STS) of sentence
pairs. Being able to reliably measure semantic simi-
larity can be beneficial for many applications, e.g. in
the domains of MT evaluation, information extrac-
tion, question answering, and summarization.
For the shared task, STS was measured on a scale
ranging from 0 (indicating no similarity at all) to 5
(semantic equivalence). The system predictions were
evaluated against manually annotated data.
2 Description of our approach
Our system KLUE-CORE uses two approaches to
estimate STS between pairs of sentences: a distri-
butional bag-of-words model inspired by Sch?tze
(1998), and a simple alignment model that links each
word in one sentence to the semantically most similar
word in the other sentence. For the alignment model,
word similarities were obtained from WordNet (using
a range of state-of-the-art path-based similarity mea-
sures) and from two distributional semantic models
(DSM).
All similarity scores obtained in this way were
passed to a ridge regression learner in order to obtain
a final STS score. The predictions for new sentence
pairs were then transformed to the range [0,5], as
required by the task definition.
2.1 The training data
We trained our system on manually annotated sen-
tence pairs from the STS task at SemEval 2012
(Agirre et al, 2012). Pooling the STS 2012 training
and test data, we obtained 5 data sets from differ-
ent domains, comprising a total of 5343 sentence
pairs annotated with a semantic similarity score in
the range [0,5]. The data sets are paraphrase sen-
tence pairs (MSRpar), sentence pairs from video de-
scriptions (MSRvid), MT evaluation sentence pairs
(MTnews and MTeuroparl), and glosses from two
different lexical semantic resources (OnWN).
All sentence pairs were pre-processed with Tree-
Tagger (Schmid, 1995)1 for part-of-speech annota-
tion and lemmatization.
1http://www.ims.uni-stuttgart.de/forschung/
ressourcen/werkzeuge/treetagger.html
181
2.2 Similarity on word level
Our alignment model (Sec. 2.3.1) is based on similar-
ity scores for pairs of words. We obtained a total of
11 different word similarity measures from WordNet
(Miller et al, 1990) and in a completely unsupervised
manner from distributional semantic models.
2.2.1 WordNet
We computed three state-of-the-art WordNet simi-
larity measures, namely path similarity, Wu-Palmer
similarity and Leacock-Chodorow similarity (Budan-
itsky and Hirst, 2006). As usual, for each pair of
words the synsets with the highest similarity score
were selected. For all three measures, we made use of
the implementations provided as part of the Natural
Language ToolKit for Python (Bird et al, 2009).
2.2.2 Distributional semantics
Word similarity scores were also obtained from two
DSM: Distributional Memory (Baroni and Lenci,
2010) and a model compiled from a version of the
English Wikipedia.2 For Distributional Memory, we
chose the collapsed W ?W matricization, resulting
in a 30686?30686 matrix that was further reduced
to 300 latent dimensions using randomized SVD
(Halko et al, 2009). For the Wikipedia DSM, we
used a L2/R2 context window and mid-frequency
feature terms, resulting in a 77598?30484 matrix.
Co-occurrence frequency counts were weighted us-
ing sparse log-likelihood association scores with a
square root transformation, and reduced to 300 latent
dimensions with randomized SVD. In both cases, tar-
get terms are POS-disambiguated lemmas of content
words, and the angle between vectors was used as a
distance measure (equivalent to cosine similarity).
For each DSM, we computed the following se-
mantic distances: (i) angle: the angle between the
two word vectors; (ii) fwdrank: the (logarithm of
the) forward neighbour rank, i.e. which rank the sec-
ond word occupies among the nearest neighbours
of the first word; (iii) bwdrank: the (logarithm of
the) backward neighbour rank, i.e. which rank the
first word occupies among the nearest neighbours of
the second word; (iv) rank: the (logarithm of the)
arithmetic mean of forward and backward neighbour
2For this purpose, we used the pre-processed and linguis-
tically annotated Wackypedia corpus available from http://
wacky.sslmit.unibo.it/.
rank; (v) lowrank: the (logarithm of the) harmonic
mean of forward and backward neighbour rank.
A composite similarity score in the range [0,1]
was obtained by linear regression on all five distance
measures, using the WordSim-353 noun similarity
ratings (Finkelstein et al, 2002) for parameter esti-
mation. This score is referred to as similarity below.
Manual inspection showed that word pairs with simi-
larity < 0.7 were completely unrelated in many cases,
so we also included a ?strict? version of similarity
with all lower scores set to 0. We further included
rank and angle, which were linearly transformed to
similarity values in the range [0,1].
2.3 Similarity on sentence level
Similarity scores for sentence pairs were obtained in
two different ways: with a simple alignment model
based on the word similarity scores from Sec. 2.2
(described in Sec. 2.3.1) and with a distributional
bag-of-words model (described in Sec. 2.3.2).
2.3.1 Similarity by word alignment
The sentence pairs were preprocessed in the follow-
ing way: input words were transformed to lower-
case; common stopwords were eliminated; and dupli-
cate words within each sentence were deleted. For
the word similarity scores from Sec. 2.2.2, POS-
disambiguated lemmas according to the TreeTagger
annotation were used.
Every word of the first sentence in a given pair
was then compared with every word of the second
sentence, resulting in a matrix of similarity scores
for each of the word similarity measures described
in Sec. 2.2. Since we were not interested in an asym-
metric notion of similarity, matrices were set up so
that the shorter sentence in a pair always corresponds
to the rows of the matrix, transposing the similarity
matrix if necessary. From each matrix, two similar-
ity scores for the sentence pair were computed: the
arithmetic mean of the row maxima (marked as short
in Tab. 4), and the artihmetic mean of the column
maxima (marked as long in Tab. 4).
This approach corresponds to a simple word align-
ment model where each word in the shorter sentence
is aligned to the semantically most similar word in
the longer sentence (short), and vice versa (long).
Note that multiple source words may be aligned to
the same target word, and target words can remain
182
unaligned without penalty. Semantic similarities are
then averaged across all alignment pairs.
In total, we obtained 22 sentence similarity scores
from this approach.
2.3.2 Distributional similarity
We computed distributional similarity between the
sentences in each pair directly using bag-of-words
centroid vectors as suggested by Sch?tze (1998),
based on the two word-level DSM introduced in
Sec. 2.2.2.
For each sentence pair and DSM, we computed (i)
the angle between the centroid vectors of the two sen-
tences and (ii) a z-score relative to all other sentences
in the same data set of the training or test collection.
Both values are measures of semantic distance, but
are automatically transformed into similarity mea-
sures by the regression learner (Sec. 2.4).
For the z-scores, we computed the semantic dis-
tance (i.e. angle) between the first sentence of a given
pair and the second sentences of all word pairs in the
same data set. The resulting list of angles was stan-
dardized to z-scores, and the z-score corresponding
to the second sentence from the given pair was used
as a measure of forward similarity between the first
and second sentence. In the same way, a backward
z-score between the second and first sentence was
determined. We used the average of the forward and
backward z-score as our second STS measure.
The z-transformation was motivated by our obser-
vation that there are substantial differences between
the individual data sets in the STS 2012 training and
test data. For some data sets (MSRpar and MSRvid),
sentences are often almost identical and even a single-
word difference can result in low similarity ratings;
for other data sets (e.g. OnWN), similarity ratings
seem to be based on the general state of affairs de-
scribed by the two sentences rather than their par-
ticular wording of propositional content. By using
other sentences in the same data set as a frame of
reference, corpus-based similarity scores can roughly
be calibrated to the respective notion of STS.
In total, we obtained 4 sentence (dis)similarity
scores from this approach. Because of technical is-
sues, only the z-score measures were used in the
submitted system. The experiments in Sec. 3 also
focus on these z-scores.
2.4 The regression model
The 24 individual similarity scores described in
Sec. 2.3.1 and 2.3.2 were combined into a single
STS prediction by supervised regression.
We conducted experiments with various machine
learning algorithms implemented in the Python li-
brary scikit-learn (Pedregosa et al, 2011). In partic-
ular, we tested linear regression, regularized linear
regression (ridge regression), Bayesian ridge regres-
sion, support vector regression and regression trees.
Our final system submitted to the shared task uses
ridge regression, a shrinkage method applied to linear
regression that uses a least-squares regularization on
the regression coefficients (Hastie et al, 2001, 59).
Intuitively speaking, the regularization term discour-
ages large value of the regression coefficients, which
makes the learning technique less prone to overfit-
ting quirks of the training data, especially with large
numbers of features.
We tried to optimise our results by training the indi-
vidual regressors for each test data set on appropriate
portions of the training data. For our task submis-
sion, we used the following training data based on
educated guesses inspired by the very small amount
of development data provied: for the headlines test
set we trained on both glosses and statistical MT
data, for the OnWN and FNWN test sets we trained
on glosses only (OnWN), and for the SMT test set
we trained on statistical MT data only (MTnews and
MTeuroparl). We decided to omit the Microsoft Re-
search Paraphrase Corpus (MSRpar and MSRvid)
because we felt that the types of sentence pairs in this
corpus were too different from the development data.
For our submission, we used all 24 features de-
scribed in Sec. 2.3 as input for the ridge regression
algorithm. Out of 90 submissions by 35 teams, our
system ranked on place 20.3
3 Experiments
In this section, we describe some post-hoc experi-
ments on the STS 2013 test data, which we performed
in order to find out whether we made good decisions
regarding the machine learning method, training data,
3This paper describes the run listed as KLUE-approach_2 in
the official results. The run KLUE-approach_1 was produced by
the same system without the bag-of-words features (Sec. 2.3.2);
it was only submitted as a safety backup.
183
similarity features, and other parameters. Results of
our submitted system are typeset in italics, the best
results in each column are typeset in bold font.
3.1 Machine learning algorithms
Tab. 1 gives an overview of the performance of vari-
ous machine learning algorithms. All regressors were
trained on the same combinations of data sets (see
Sec. 2.4 above) using all available features, and eval-
uated on the STS 2013 test data. Overall, our choice
of ridge regression is justified. Especially for the
OnWN test set, however, support vector regression
is considerably better (it would have achieved rank
11 instead of 17 on this test set). If we had happened
to use the best learning algorithm for each test set,
we would have achieved a mean score of 0.54768
(putting our submission at rank 14 instead of 20).
3.2 Regularization strength
We also experimented with different regularization
strengths, as determined by the parameter ? of the
ridge regression algorithm (see Tab. 2). Changing ?
from its default value ? = 1 does not seem to have
a large impact on the performance of the regressor.
Setting ? = 2 for all test sets would have minimally
improved the mean score (rank 19 instead of 20).
Even choosing the optimal ? for each test set would
only have resulted in a slightly improved mean score
of 0.53811 (also putting our submission at rank 19).
3.3 Composition of training data
As described above, we suspected that using different
combinations of the training data for different test
sets might lead to better results. The overview in
Tab. 3 confirms our expectations. We did, however,
fail to correctly guess the optimal combinations for
each test set. We would have obtained the best re-
sults by training on glosses (OnWN) for the headlines
test set (rank 35 instead of 40 in this category), by
training on MSR data (MSRpar and MSRvid) for the
OnWN (rank 11 instead of 17) and FNWN test sets
(rank 9 instead of 10), and by combining glosses and
machine translation data (OnWN, MTnews MTeu-
roparl) for the SMT test set (rank 30 instead of 33).
Had we found the optimal training data for each test
set, our system would have achieved a mean score of
0.55021 (rank 11 instead of 20).
3.4 Features
For our submission, we used all the features de-
scribed in Sec. 2. Tab. 4 shows what results each
group of features would have achieved by itself (all
runs use ridge regression, default ? = 1 and the same
combinations of training data as in our submission).
In Tab. 4, the line labelled wp500 shows the re-
sults obtained using only word-alignment similarity
scores (Sec. 2.3.1) based on the Wikipedia DSM
(Sec. 2.2.2) as features. The following two lines give
separate results for the alignments from shorter to
longer sentence, i.e. row maxima (wp500-short) and
from longer to shorter sentence, i.e. column maxima
(wp500-long), respectively. Below are corresponding
results for word alignments based on Distributional
Memory (dm, dm-short, dm-long) and WordNet simi-
larity as described in Sec. 2.2.1 (WN, WN-short, WN-
long). The line labelled bow represents the two z-
score similarities obtained from distributional bag-of-
words models (Sec. 2.3.2); bow-wp500 (Wikipedia
DSM) and bow-dm (Distributional Memory) each
correspond to a single distributional feature.
Combining all the available features indeed results
in the highest mean score. However, for OnWN and
SMT a subset of the features would have led to better
results. Using only the bag-of-words scores would
have improved the results for the OnWN test set by
a considerable margin (rank 8 instead of 17), using
only the alignment scores based on WordNet would
have improved the results for the SMT test set (rank
17 instead of 33). If we had used the optimal subset
of features for each test set, the mean score would
have increased to 0.55556 (rank 9 instead of 20).
4 Conclusion
Our experiments show that it is essential for high-
quality semantic textual similarity to adapt a corpus-
based system carefully to each particular data set
(choice of training data, feature engineering, tuning
of machine learning algorithm). Many of our edu-
cated guesses for parameter settings turned out to be
fairly close to the optimal values, though there would
have been some room for improvement.
Overall, our simple approach, which makes very
limited use of external resources, performs quite well
? achieving rank 20 out of 90 submissions ? and will
be a useful tool for many real-world applications.
184
headlines OnWN FNWN SMT mean
Ridge Regression 0.65102 0.68693 0.41887 0.33599 0.53546
Linear Regression 0.65184 0.68118 0.39707 0.32756 0.52966
Bayesian Ridge 0.65164 0.68962 0.42344 0.33003 0.53474
SVM SVR 0.52208 0.73330 0.40479 0.30810 0.49357
Decision Tree 0.29320 0.50633 0.05022 0.17072 0.28510
Table 1: Evaluation results for different machine learning algorithms
? headlines OnWN FNWN SMT mean
1 0.65102 0.68693 0.41887 0.33599 0.53546
0.01 0.65184 0.68129 0.39773 0.32773 0.52980
0.1 0.65186 0.68224 0.40246 0.32900 0.53087
0.5 0.65161 0.68492 0.41346 0.33311 0.53374
0.9 0.65114 0.68660 0.41816 0.33560 0.53523
2 0.64941 0.68917 0.42290 0.33830 0.53659
5 0.64394 0.69197 0.42265 0.33669 0.53491
Table 2: Evaluation results for different regularization strengths of the ridge regression learner
headlines OnWN FNWN SMT mean
def 0.65440 0.68693 0.41887 0.32694 0.53357
smt 0.65322 0.62643 0.24895 0.33599 0.50684
def+smt 0.65102 0.59665 0.24953 0.33867 0.49962
msr 0.63633 0.73396 0.43073 0.33168 0.54185
def+smt+msr 0.65008 0.65093 0.39636 0.28645 0.50777
approach2 0.65102 0.68693 0.41887 0.33599 0.53546
Table 3: Evaluation results for different training sets (?approach2? refers to our shared task submission, cf. Sec. 2.4)
headlines OnWN FNWN SMT mean
wp500 0.57099 0.59199 0.31740 0.31320 0.46899
wp500-long 0.57837 0.59012 0.30909 0.30075 0.46614
wp500-short 0.58271 0.58845 0.34205 0.29474 0.46794
dm 0.42129 0.55945 0.21139 0.27426 0.38910
dm-long 0.40709 0.56511 0.28993 0.23826 0.38037
dm-short 0.44780 0.53555 0.28709 0.24484 0.38853
WN 0.63654 0.65149 0.41025 0.35624 0.52783
WN-long 0.62749 0.63828 0.39684 0.33399 0.51297
WN-short 0.64986 0.66175 0.41441 0.33350 0.52759
bow 0.52384 0.74046 0.31917 0.24611 0.46808
bow-wp500 0.52726 0.73624 0.32797 0.24460 0.46841
bow-dm 0.21908 0.66873 0.17096 0.20176 0.32138
all 0.65102 0.68693 0.41887 0.33599 0.53546
Table 4: Evaluation results for different sets of similarity scores as features (cf. Sec. 3.4)
185
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2012. Semeval-2012 task 6:
A pilot on semantic textual similarity. In First Joint
Conference on Lexical and Computational Semantics,
pages 385?393. Association for Computational Linguis-
tics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Marco Baroni and Alessandro Lenci. 2010. Distribu-
tional Memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?712.
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python. O?Reilly
Media, Sebastopol, CA. Online version available at
http://www.nltk.org/book.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?47.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud
Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2002. Placing search in context: The concept revisited.
ACM Transactions on Information Systems, 20(1):116?
131.
N. Halko, P. G. Martinsson, and J. A. Tropp. 2009. Find-
ing structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions.
Technical Report 2009-05, ACM, California Institute
of Technology, September.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2001. The Elements of Statistical Learning. Data Min-
ing, Inference, and Prediction. Springer, New York,
NY.
George A. Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine J. Miller. 1990. Introduc-
tion to WordNet: An on-line lexical database. Interna-
tional Journal of Lexicography, 3(4):235?244.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceedings
of the EACL SIGDAT-Workshop, pages 47?50, Dublin.
Hinrich Sch?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
186
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 395?401, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
KLUE: Simple and robust methods for polarity classification
Thomas Proisl and Paul Greiner and Stefan Evert and Besim Kabashi
Friedrich-Alexander-Universit?t Erlangen-N?rnberg
Department Germanistik und Komparatistik
Professur f?r Korpuslinguistik
Bismarckstr. 6
91054 Erlangen, Germany
{thomas.proisl,paul.greiner,stefan.evert,besim.kabashi}@fau.de
Abstract
This paper describes our approach to the
SemEval-2013 task on ?Sentiment Analysis
in Twitter?. We use simple bag-of-words mod-
els, a freely available sentiment dictionary auto-
matically extended with distributionally similar
terms, as well as lists of emoticons and inter-
net slang abbreviations in conjunction with fast
and robust machine learning algorithms. The
resulting system is resource-lean, making it rel-
atively independent of a specific language. De-
spite its simplicity, the system achieves compet-
itive accuracies of 0.70?0.72 in detecting the
sentiment of text messages. We also apply our
approach to the task of detecting the context-
dependent sentiment of individual words and
phrases within a message.
1 Introduction
The SemEval-2013 task on ?Sentiment Analysis in
Twitter? (Wilson et al, 2013) focuses on polarity clas-
sification, i. e. the problem of determining whether
a textual unit, e. g. a document, paragraph, sentence
or phrase, expresses a positive, negative or neutral
sentiment (for a review of research topics and re-
cent developments in the field of sentiment analysis
see Liu (2012)). There are two subtasks: in task B,
?Message Polarity Classification?, whole messages
have to be classified as being of positive, negative
or neutral sentiment; in task A, ?Contextual Polarity
Disambiguation?, a marked instance of a word or
phrase has to be classified in the context of a whole
message.
The training data for task B consist of approxi-
mately 10 200 manually annotated Twitter messages,
the training data for task A of approximately 9 500
marked instances in approximately 6 300 Twitter mes-
sages.1 The test data consist of in-domain Twit-
ter messages (3 813 messages for task B and 4 435
marked instances in 2 826 messages for task A) and
out-of-domain SMS text messages (2 094 messages
for task B, 2 334 marked instances in 1 437 messages
for task A). The distribution of messages and marked
instances over sentiment categories in the training
and test sets is shown in Tab. 1.
pos neg neu total
train-B 3 783 1 600 4 832 10 215
test-B Twitter 1 572 601 1 640 3 813
test-B SMS 492 394 1 208 2 094
train-A 5 862 3 166 463 9 491
test-A Twitter 2 734 1 541 160 4 435
test-A SMS 1 071 1 104 159 2 334
Table 1: The data sets for both tasks
The main focus of the current paper lies on experi-
menting with resource-lean and robust methods for
task B, the classification of whole messages. We do,
however, apply our approach also to task A.
2 Features used for polarity classification
Our general approach is quite simple: we extract
feature vectors from the training data (based on the
1These figures indicate the amount of training data we were
actually able to use. Due to Twitter?s licensing conditions, the
training data could only be made available as a collection of IDs.
Even when using the official Twitter API for collecting the actual
messages rather than the screen-scraping approach suggested by
the task organizers, ca. 10% of the data were not (or no longer)
available.
395
original messages and a small number of additional
resources) and feed them into fast and robust super-
vised machine learning algorithms implemented in
the Python machine learning library scikit-learn (Pe-
dregosa et al, 2011). For task B, the features are
computed on the basis of the whole message; for task
A, we use essentially the same features, but compute
them once for the marked word or phrase and once
for the rest of the message. All the features we use
are described in some more detail in the following
subsections.
2.1 Bag of words
We experimented with three different sets of bag-of-
words features: unigrams, unigrams and bigrams, and
an extended unigram model that includes a simple
treatment of negation. For all three models we simply
use the word frequencies as feature weights.
Our preprocessing pipeline starts with a simple
preliminary tokenization step (lowercasing the whole
message and splitting it on whitespace). In the re-
sulting list of tokens, all user IDs and web URLs are
replaced with placeholders.2 Any remaining punctu-
ation is stripped from the tokens and empty tokens
are deleted. In the extended unigram model, up to
three tokens following a negation marker are then
prefixed with not_ (fewer tokens if another negation
marker or the end of the message is reached). Finally
all words are stemmed using the Snowball stemmer.3
For a token unigram or bigram to be included in
the bag of words models, it has to occur in at least
five messages.
As an additional feature we include the total num-
ber of tokens per message.
2.2 Features based on a sentiment dictionary
Widely-used algorithms such as SentiStrength (Thel-
wall et al, 2010) rely heavily on dictionaries contain-
ing sentiment ratings of words and/or phrases. We
use features based on an extended version of AFINN-
111 (Nielsen, 2011).4
The AFINN sentiment dictionary contains senti-
ment ratings ranging from ?5 (very negative) to 5
2The regular expression for matching web URLs has
been taken from http://daringfireball.net/2010/07/
improved_regex_for_matching_urls.
3http://snowball.tartarus.org/
4http://www2.imm.dtu.dk/pubdb/p.php?6010
(very positive) for 2 476 word forms. In order to ob-
tain a better coverage, we extended the dictionary
with distributionally similar words. For this pur-
pose, large-vocabulary distributional semantic mod-
els (DSM) were constructed from a version of the
English Wikipedia5 and the Google Web 1T 5-Grams
database (Brants and Franz, 2006). The Wikipedia
DSM consists of 122 281 case-folded word forms
as target terms and 30 484 mid-frequency content
words (lemmatised) as feature terms; the Web1T5
DSM of 241 583 case-folded word forms as target
terms and 100 063 case-folded word forms as fea-
ture terms. Both DSMs use a context window of two
words to the left and right, and were reduced to 300
latent dimensions using randomized singular value
decomposition (Halko et al, 2009).
For each AFINN entry, the 30 nearest neighbours
according to each DSM were considered as exten-
sion candidates. Sentiment ratings for the new candi-
dates were computed by averaging over the 30 near-
est neighbours of the respective candidate term (with
scores set to 0 for all neighbours not listed in AFINN),
and rescaling to the range [?5,5].6 After some ini-
tial experiments, only candidates with a computed
rating ??2.5 or ? 2.5 were retained, resulting in an
extended dictionary of 2 820 word forms.
As with the bag of words model, we make use of
a simple heuristic treatment of negation: following a
negation marker, the polarity of the next sentiment-
carrying token up to a distance of at most four tokens
is multiplied by ?1.
The sentiment dictionary is used to extract four
features: I) the number of tokens that express a posi-
tive sentiment, II) the number of tokens that express
a negative sentiment, III) the total number of tokens
that express a sentiment according to our sentiment
dictionary and IV) the arithmetic mean of all the sen-
timent scores from the sentiment dictionary in the
message.
5We used the pre-processed and linguistically annotated
Wackypedia corpus available from http://wacky.sslmit.
unibo.it/.
6Scaling coefficients were determined by regression on ex-
tension candidates that were already listed in AFINN.
396
2.3 Features based on emoticons and internet
slang abbreviations
In addition to the sentiment dictionary we use a list
of 212 emoticons and 95 internet slang abbreviations
from Wikipedia. We manually classified these 307
emotion markers as negative (?1), neutral (0) or pos-
itive (1).
The extracted features based on this list are similar
to the ones based on the sentiment dictionary. We use
I) the number of positive emotion markers, II) the
number of negative emotion markers, III) the total
number of emotion markers and IV) the arithmetic
mean of all the emotion markers in the message.
3 Experiments
In this section we evaluate different classifiers (multi-
nomial Naive Bayes,7 Linear SVM8 and Maximum
Entropy9) and various combinations of features on
the gold test sets. We vary the bag-of-words model
(bow), the use of AFINN (sent), our extensions to
the sentiment dictionary (ext) and the list of emotion
markers (emo). To present as clear a picture of the
classifiers? performances as possible, we report F-
scores for each of the three classes, the weighted av-
erage of all three F-scores (Fw), the (unweighted) av-
erage of the positive and negative F-scores (Fpos+neg;
this is the value shown in the official task results and
used for ranking systems), as well as accuracy.
Results for submitted systems are typeset in italics,
the best results in each column are typeset in bold
font.
3.1 Task B: Message Polarity Classification
Experiments with just a simple unigram bag-of-
words model show that for both the Twitter (Tab. 3)
and the SMS data (Tab. 4) the Maximum Entropy
classifier outperforms multinomial Naive Bayes and
Linear SVM by a considerable margin. For compar-
ison, we also include some weak baselines (Tab. 2).
The random baselines classify messages randomly,10
7We always use the default setting alpha = 1.0.
8In all experiments, we use the following parameters:
penalty = ?l1?, dual = False, C = 1.0.
9We use the following parameter settings in our experiments:
penalty = ?l1?, C = 1.0.
10randomuniform assumes a uniform probability distribution
(all categories have equal probabilities), randomweighted has
learned the probability distribution from the training data,
the majority baselines simply assign all messages
to the most frequent category in the training data.11
As one would expect, all three learning algorithms
are vastly superior to those baselines. Using both
unigrams and bigrams in the bag-of-words model
improves classifier peformance; so does the extended
unigram model with negations.
For the Twitter data, adding the sentiment dictio-
nary, the dictionary extensions and the list of emo-
tion markers further improves classifier performance,
with the best results being achieved by a combina-
tion of all these features with a uni- and bigram bag-
of-words model. The best combination of features
would have been the fourth best system out of 35
constrained systems (sixth best out of all 51 systems),
one rank higher than our task submission.12
For the SMS data, adding the sentiment dictio-
nary and the dictionary extensions seems to improve
the official score Fpos+neg, but slightly decreases
weighted average F-score and accuracy. This might
be due to the greater orthographical variation in SMS
texts. Emotion markers seem to be a much better
sentiment indicator in the SMS data. But while just
combining the list of emotion markers with the ex-
tended unigram bag-of-words model leads to the best
weighted average F-score and accuracy, Fpos+neg is
best when a combination of all features is used. This
is also the system we submitted, being the third best
system (out of 44) for that task.
3.2 Task A: Contextual Polarity
Disambiguation
The results for task A are similar to those for task
B in that Maximum Entropy is the best classifier for
the unigram bag-of-words model for both the Twitter
(Tab. 5) and the SMS data (Tab. 6). Adding negation
treatment to the bag-of-words model increases classi-
fier performance, as do the inclusion of AFINN and
the use of emotion markers. Interestingly, extend-
ing the sentiment dictionary based on distributional
similarity leads to slightly worse results. Therefore,
randomweighted,binary uses the same probability distribution but
classifies messages only as either positive or negative.
11majority classifies all messages as neutral, as this is the most
frequent category in the training data, majoritybinary does binary
classification and thus classifies all messages as positive.
12Evaluation results for all SemEval-2013 tasks are avail-
able online: http://www.cs.york.ac.uk/semeval-2013/
index.php?id=evaluation-results.
397
classifier Fpos Fneg Fneu Fw Fpos+neg Acc
randomuniform 0.3666 0.2128 0.3745 0.3458 0.2897 0.3318
randomweighted 0.3912 0.1681 0.4521 0.3820 0.2796 0.3835
randomweighted,binary 0.5186 0.2042 0.000 0.2460 0.3614 0.3349
majority 0.0000 0.0000 0.6015 0.2587 0.0000 0.4301
majoritybinary 0.5838 0.0000 0.0000 0.2407 0.2919 0.4123
Table 2: Some weak baselines for task B, Twitter test set
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.6355 0.5093 0.6898 0.6390 0.5724 0.6423
LinearSVM uni - - - 0.6412 0.4884 0.6876 0.6371 0.5648 0.6418
MaxEnt uni - - - 0.6705 0.5109 0.7212 0.6671 0.5907 0.6761
MaxEnt uni+bi - - - 0.6845 0.5192 0.7257 0.6762 0.6019 0.6845
MaxEnt unineg - - - 0.6797 0.5284 0.7242 0.6750 0.6041 0.6824
MaxEnt unineg + - - 0.6860 0.5661 0.7284 0.6854 0.6261 0.6911
MaxEnt unineg - - + 0.6807 0.5393 0.7229 0.6766 0.6100 0.6835
MaxEnt unineg + + - 0.6841 0.5529 0.7258 0.6814 0.6185 0.6874
MaxEnt unineg + + + 0.6963 0.5650 0.7325 0.6912 0.6306 0.6968
MaxEnt unineg + - + 0.6952 0.5753 0.7338 0.6929 0.6353 0.6984
MaxEnt uni+bi + - + 0.7034 0.5706 0.7358 0.6964 0.6370 0.7018
MaxEnt uni+bi + + + 0.7052 0.5720 0.7371 0.6979 0.6386 0.7031
MaxEnt - + + + 0.6920 0.3532 0.6533 0.6220 0.5226 0.6370
Table 3: Evaluation results for task B on the Twitter test set
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.4918 0.4773 0.5541 0.5250 0.4845 0.5153
LinearSVM uni - - - 0.5833 0.5046 0.7229 0.6490 0.5440 0.6442
MaxEnt uni - - - 0.6260 0.5015 0.7903 0.6974 0.5638 0.7015
MaxEnt uni+bi - - - 0.6003 0.5380 0.7658 0.6840 0.5692 0.6829
MaxEnt unineg - - - 0.6528 0.5412 0.7884 0.7100 0.5970 0.7125
MaxEnt unineg + - - 0.6399 0.5955 0.7744 0.7092 0.6177 0.7073
MaxEnt unineg - - + 0.6596 0.5507 0.8033 0.7220 0.6052 0.7259
MaxEnt unineg + + - 0.6374 0.5905 0.7731 0.7068 0.6140 0.7049
MaxEnt unineg + + + 0.6506 0.5900 0.7903 0.7198 0.6203 0.7197
MaxEnt unineg + - + 0.6556 0.5833 0.7908 0.7200 0.6195 0.7202
MaxEnt uni+bi + - + 0.6318 0.5896 0.7750 0.7064 0.6107 0.7044
MaxEnt uni+bi + + + 0.6341 0.5783 0.7746 0.7047 0.6062 0.7030
MaxEnt - + + + 0.5961 0.3421 0.7179 0.6186 0.4691 0.6342
Table 4: Evaluation results for task B on the SMS test set
398
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.7799 0.6164 0.0498 0.6967 0.6981 0.7067
LinearSVM uni - - - 0.7759 0.6046 0.0576 0.6905 0.6902 0.6949
MaxEnt uni - - - 0.7974 0.6155 0.0110 0.7059 0.7065 0.7218
MaxEnt uni+bi - - - 0.8071 0.6320 0.0222 0.7179 0.7195 0.7335
MaxEnt unineg - - - 0.8058 0.6380 0.0110 0.7188 0.7219 0.7342
MaxEnt unineg + - - 0.8160 0.6610 0.0317 0.7339 0.7385 0.7479
MaxEnt unineg + + - 0.8153 0.6583 0.0316 0.7325 0.7368 0.7466
MaxEnt unineg + + + 0.8141 0.6608 0.0330 0.7326 0.7374 0.7468
MaxEnt unineg + - + 0.8153 0.6664 0.0331 0.7353 0.7409 0.7493
Table 5: Evaluation results for task A on the Twitter test set
classifier bow sent ext emo Fpos Fneg Fneu Fw Fpos+neg Acc
Multin. NB uni - - - 0.6766 0.6657 0.0213 0.6268 0.6712 0.6452
LinearSVM uni - - - 0.6628 0.6533 0.0365 0.6157 0.6581 0.6290
MaxEnt uni - - - 0.6829 0.6630 0.0117 0.6277 0.6729 0.6491
MaxEnt uni+bi - - - 0.6825 0.6504 0.0230 0.6224 0.6665 0.6435
MaxEnt unineg - - - 0.7008 0.6770 0.0120 0.6427 0.6889 0.6654
MaxEnt unineg + - - 0.7127 0.6962 0.0238 0.6579 0.7044 0.6804
MaxEnt unineg + + - 0.7108 0.6954 0.0238 0.6568 0.7031 0.6791
MaxEnt unineg + + + 0.7090 0.7017 0.0237 0.6589 0.7054 0.6808
MaxEnt unineg + - + 0.7114 0.7034 0.0238 0.6608 0.7074 0.6829
Table 6: Evaluation results for task A on the SMS test set
we could have improved upon our task submission
by excluding the sentiment dictionary extensions ?
however, the gains are very small and the system?s
ranks would still be the same (17/28 for the Twitter
data, 16/26 for the SMS data).
4 Discussion
4.1 Error analysis
4.1.1 Task B: Message Polarity Classification
The most prominent problem, according to the con-
fusion matrix in Tab. 7, is that a lot of negative mes-
sages are classified as neutral; the same problem
exists to a lesser extent for positive messages.
A qualitative analysis of mis-classified messages
for which the MaxEnt classifier indicated high con-
fidence suggests that the human annotators did not
clearly distinguish between sentiment expressed by
the authors of messages and their own response to
message content. For example, the messages shown
predicted
pos neg neu
go
ld
pos 979 352 70 40 523 100
neg 70 47 287 213 244 134
neu 191 191 58 75 1391 942
Table 7: Task B, confusion matrix for tweets/SMS
in (1) and (2) report a negative and positive event,
respectively, in a neutral way and should therefore
be annotated with neutral sentiment. However, in the
test data they are labelled as negative and positive by
the human annotators.
(1) MT @LccSy #Syria, Deir Ezzor | Marba?eh:
Aerial shelling dropped explosive barrels on
residential buildings in the town. Tue, 23
October.
399
(2) European Exchanges open with a slight
rise: (AGI) Rome, October 24 - Euro-
pean Exchanges opened with a slight ris...
http://t.co/mAljf6eT
This problem is probably a major factor in the mis-
classification of many negative and positive messages
as neutral. In order to better reproduce the human
annotations, the system would additionally have to
decide whether a reported event is of a negative, pos-
itive or neutral nature per se ? a quite different task
that would require external training data and world
knowledge.
An analysis of mis-classified positive messages
further suggests that certain punctuation marks, espe-
cially multiple exclamation marks, might be useful
as additional features.
4.1.2 Task A: Contextual Polarity
Disambiguation
The confusion matrix in Tab. 8 shows that mes-
sages marked as negative in the test data often mis-
classified as positive and vice versa, while neutral
instances are overwhelmingly classified as positive
or negative. This suggests that for the classifiers we
use, there might be too few neutral instances in the
training data (cf. Tab. 1).
predicted
pos neg neu
go
ld
pos 2329 826 397 239 8 6
neg 550 341 980 761 11 2
neu 109 92 48 65 3 2
Table 8: Task A, confusion matrix for tweets/SMS
4.2 Conclusion and future work
We use a resource-lean approach, relying only on
three external resources: a stemmer, a relatively
small sentiment dictionary and an even smaller list
of emotion markers. Stemmers are already avail-
able for many languages and both kinds of lexical
resources can be gathered relatively easily for other
languages. The list of emotion markers should apply
to most languages. This makes our whole system rel-
atively language-independent, provided that a similar
amount of manually labelled training data is avail-
able.13 In fact, the learning curve for our system
(Fig. 1) suggests that even as few as 3 000?3 500
labelled messages might be sufficient. The similar
0.0
0.2
0.4
0.6
0.8
1.0
Amount of training data
Scor
e
500 1500 2500 3500 4500 5500 6500 7500 8500 9500
Fpos+negAccuracy
Figure 1: Learning curve of our system for the ?Message
Polarity Classification? task, evaluated on the Twitter data
evaluation results for the Twitter and the SMS data
show that not relying on Twitter-specific features like
hashtags pays off: by making our system as generic
as possible, it is robust, not overfitted to the training
data, and generalizes well to other types of data. The
methods discussed in the current paper are particu-
larly well suited to the ?Message Polarity Classifica-
tion? task, our system ranking amongst the best. It
turns out, however, that simply applying the same ap-
proach to the ?Contextual Polarity Disambiguation?
task yields only mediocre results.
In the future, we would like to experiment with a
couple of additional features. Determining the near-
est neighbors of a message based on Latent Semantic
Analysis might be a useful addition, as might be the
use of part-of-speech tags created by an in-domain
POS tagger (Gimpel et al, 2011)14. We would also
like to find out whether a heuristic treatment of inten-
sifiers and detensifiers, the normalization of character
repetitions, or the inclusion of some punctuation-
based features could further improve classifier per-
formance.
13For task B, even the extended unigram bag-of-words model
by itself, without any additional resources, would have per-
formed quite well as the 9th best constrained system on the
Twitter test set (13th best system overall) and the 5th best system
on the SMS test set.
14http://www.ark.cs.cmu.edu/TweetNLP/
400
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium, Philadelphia,
PA.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Di-
panjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 42?47, Portland,
Oregon. Association for Computational Linguistics.
N. Halko, P. G. Martinsson, and J. A. Tropp. 2009. Find-
ing structure with randomness: Stochastic algorithms
for constructing approximate matrix decompositions.
Technical Report 2009-05, ACM, California Institute
of Technology, September.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technologies.
Morgan & Claypool.
Finn ?rup Nielsen. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ?Making
Sense of Microposts?: Big things come in small pack-
ages, number 718 in CEUR Workshop Proceedings,
pages 93?98, Heraklion.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in short
strength detection informal text. Journal of the Amer-
ican Society for Information Science and Technology,
61(12):2544?2558.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyanov. 2013.
SemEval-2013 task 2: Sentiment analysis in Twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013). Association for
Computational Linguistics.
401
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 160?170,
Dublin, Ireland, August 23-24 2014.
Contrasting Syntagmatic and Paradigmatic Relations:
Insights from Distributional Semantic Models
Gabriella Lapesa
3,1
1
Universit?at Osnabr?uck
Institut f?ur
Kognitionswissenschaft
glapesa@uos.de
Stefan Evert
2
2
FAU Erlangen-N?urnberg
Professur f?ur
Korpuslinguistik
stefan.evert@fau.de
Sabine Schulte im Walde
3
3
Universit?at Stuttgart
Institut f?ur Maschinelle
Sprachverarbeitung
schulte@ims.uni-stuttgart.de
Abstract
This paper presents a large-scale evalua-
tion of bag-of-words distributional models
on two datasets from priming experiments
involving syntagmatic and paradigmatic
relations. We interpret the variation in
performance achieved by different settings
of the model parameters as an indication
of which aspects of distributional patterns
characterize these types of relations. Con-
trary to what has been argued in the litera-
ture (Rapp, 2002; Sahlgren, 2006) ? that
bag-of-words models based on second-
order statistics mainly capture paradig-
matic relations and that syntagmatic rela-
tions need to be gathered from first-order
models ? we show that second-order mod-
els perform well on both paradigmatic and
syntagmatic relations if their parameters
are properly tuned. In particular, our re-
sults show that size of the context window
and dimensionality reduction play a key
role in differentiating DSM performance
on paradigmatic vs. syntagmatic relations.
1 Introduction
Distributional takes on the representation and ac-
quisition of word meaning rely on the assump-
tion that words with similar meaning tend to oc-
cur in similar contexts: this assumption, known as
distributional hypothesis, has been first proposed
by Harris (1954). Distributional Semantic Mod-
els (henceforth, DSMs) are computational mod-
els that operationalize the distributional hypoth-
esis; they produce semantic representations for
words in the form of distributional vectors record-
ing patterns of co-occurrence in large samples of
language data (Sahlgren, 2006; Baroni and Lenci,
2010; Turney and Pantel, 2010). Comparison be-
tween distributional vectors allows the identifica-
tion of shared contexts as an empirical correlate of
the semantic similarity between the target words.
As noted in Sahlgren (2008), the notion of seman-
tic similarity applied in distributional approaches
to meaning is an easy target of criticism, as it is
employed to capture a wide range of semantic re-
lations, such as synonymy, antonymy, hypernymy,
up to topical relatedness.
The study presented in this paper contributes
to the debate concerning the nature of the seman-
tic representations built by DSMs, and it does so
by comparing the performance of several DSMs
in a classification task conducted on priming data
and involving paradigmatic and syntagmatic rela-
tions. Paradigmatic relations hold between words
that occur in similar contexts; they are also called
relations in absentia (Sahlgren, 2006) because
paradigmatically related words do not co-occur.
Examples of paradigmatic relations are synonyms
(e.g., frigid?cold) and antonyms (e.g., cold?hot).
Syntagmatic relations hold between words that co-
occur (relations in praesentia) and therefore ex-
hibit a similar distribution across contexts. Typi-
cal examples of syntagmatic relations are phrasal
associates (e.g., help?wanted) and syntactic collo-
cations (e.g., dog?bark).
Distributional modeling has already tackled the
issue of paradigmatic and syntagmatic relations
(Sahlgren, 2006; Rapp, 2002). Key contributions
of the present work are the scope of its evaluation
(in terms of semantic relations and model parame-
ters) and the new perspective on paradigmatic vs.
syntagmatic models provided by our results.
Concerning the scope of the evaluation, this is
the first study in which the comparison involves
such a wide range of semantic relations (paradig-
matic: synonyms, antonyms and co-hyponyms;
syntagmatic: syntactic collocations, backward and
forward phrasal associates). Moreover, our eval-
uation covers a large number of DSM parame-
ters: source corpus, size and direction of the con-
text window, criteria for feature selection, feature
160
weighting, dimensionality reduction and index of
distributional relatedness. We consider the varia-
tion in performance achieved by different parame-
ter settings as a cue towards characteristic aspects
of specific relations (or groups of relations).
Our work also differs from previous studies
(Sahlgren, 2006; Rapp, 2002) in its focus on
second-order models. We aim to show that they
are able to capture both paradigmatic and syn-
tagmatic relations with appropriate parameter set-
tings. In addition, this focus provides a uniform
experimental design for the evaluation. For ex-
ample, parameters like window size and direction-
ality apply to bag-of-words DSMs and colloca-
tion lists but not to term-context models; dimen-
sionality reduction, whose effect has not yet been
explored systematically in the context of syntag-
matic and paradigmatic relations, is not applicable
to collocation lists.
This paper is structured as follows. Section 2
summarizes previous work. Section 3 describes
the experimental setup, in terms of task, datasets
and evaluated parameters. Section 4 introduces
our model selection methodology. Section 5
presents the results of our evaluation study. Sec-
tion 6 summarizes main findings and sketches on-
going and future work.
2 Previous Work
In this section we discuss previous work relevant
to the distributional modeling of paradigmatic and
syntagmatic relations. For space constraints, we
focus only on two studies (Rapp, 2002; Sahlgren,
2006) in which the two classes of relations are
compared at a global level, and not on studies
that are concerned with specific semantic rela-
tions, e.g., synonymy (Edmonds and Hirst, 2002;
Curran, 2003), hypernymy (Weeds et al., 2004;
Lenci and Benotto, 2012) or syntagmatic predicate
preferences (McCarthy and Carroll, 2003; Erk et
al., 2010), etc.
In previous studies, the comparison of syntag-
matic and paradigmatic relations has been imple-
mented in terms of an opposition between differ-
ent classes of corpus-based models: term-context
models (words as targets, documents or context re-
gions as features) vs. bag-of-words models (words
as targets and features) in Sahlgren (2006); col-
location lists vs. bag-of-words models in Rapp
(2002). Given the high terminological variation
in the literature, in this paper we will adopt the
labels syntagmatic and paradigmatic to character-
ize different types of semantic relations, and we
will use the labels first-order and second-order
to characterize corpus-based models with respect
to the kind of co-occurrence information they en-
code. We will refer to collocation lists and term-
document DSMs as first-order models, and to bag-
of-words DSMs as second-order models
1
.
Rapp (2002) integrates first-order (co-
occurrence lists) and second-order (bag-of-words
DSMs) information to distinguish syntagmatic
and paradigmatic relations. Under the assumption
that paradigmatically related words will be found
among the closest neighbors of a target word in
the DSM space and that paradigmatically and syn-
tagmatically related words will be intermingled
in the list of collocates of the target word, Rapp
proposes to exploit a comparison of the most
salient collocates and the nearest DSM neighbors
to distinguish between the two types of relations.
Sahlgren (2006) compares term-context and
bag-of-words DSMs in a number of tasks involv-
ing syntagmatic and paradigmatic relations. First,
a comparison between the thesaurus entries for tar-
get words (containing both paradigmatically and
syntagmatically related words) and neighbors in
the distributional spaces is conducted. It shows
that, while term-context DSMs produce both syn-
tagmatically and paradigmatically related words,
the nearest neighbors in a bag-of-words DSM
mainly provide paradigmatic information. Bag-
of-words models also performed better than term-
context models in predicting association norms,
in the TOEFL multiple-choice synonymy task and
in the prediction of antonyms (although the dif-
ference in performance was less significant here).
Last, word neighborhoods are analysed in terms of
their part-of-speech distribution. Sahlgren (2006)
observes that bag-of-words spaces contain more
neighbors with the same part of speech as the tar-
get than term-context spaces. He concludes that
bag-of-words spaces privilege paradigmatic rela-
tions, based on the assumption that paradigmati-
cally related word pairs belong to the same part of
speech, while this is not necessarily the case for
syntagmatically related word pairs.
1
Term-document models encode first-order information
because dot products between row vectors are related to co-
occurrence counts of the corresponding words (within docu-
ments). More precisely, for a binary term-document matrix,
cosine similarity is identical to the square root of the MI
2
as-
sociation measure. Please note that our terminology differs
from that of Sch?utze (1998) and Peirsman et al. (2008).
161
Summing up, in both Rapp (2002) and Sahlgren
(2006) it is claimed that second-order models per-
form poorly in predicting syntagmatic relations.
However, neither of those studies involves datasets
containing exclusively syntagmatic relations, as
the evaluation focuses either on paradigmatic rela-
tions (TOEFL multiple choice test, antonymy test)
or on resources containing both types of relations
(thesauri, association norms).
3 Experimental Setting
3.1 Evaluation Task and Data
In this study, bag-of-words DSMs are evaluated on
two datasets containing experimental items from
two priming studies. Each item is a word triple
(target, consistent prime, inconsistent prime) with
a particular semantic relation between target and
consistent prime. Following previous work on
modeling priming effects as a comparison between
prime-target pairs (McDonald and Brew, 2004;
Pad?o and Lapata, 2007; Herda?gdelen et al., 2009),
we evaluate our models in a classification task.
The goal is to identify the consistent prime on the
basis of its distributional relatedness to the tar-
get: if a particular DSM (i.e., a certain parame-
ter combination) is sensitive to a specific relation
(or group of relations), we expect the consistent
primes to be closer to the target in semantic space
than the inconsistent ones.
The first dataset is derived from the Semantic
Priming Project (SPP) (Hutchison et al., 2013).
To the best of our knowledge, our study repre-
sents the first evaluation of bag-of-words DSMs
on items from this dataset. The original data con-
sist of 1661 word triples (target, consistent prime,
inconsistent prime) collected within a large-scale
project aiming at characterizing English words in
terms of a set of lexical and associative/semantic
characteristics, along with behavioral data from
visual lexical decision and naming studies
2
. We
manually discarded all triples containing proper
nouns, adverbs or inflected words. We then
selected five subsets involving different seman-
tic relations, namely: synonyms (SYN), 436
triples (example of a consistent prime and tar-
get: frigid?cold); antonyms (ANT): 135 triples
(e.g., hot?cold); cohyponyms (COH): 159 triples
(e.g., table?chair); forward phrasal associates
(FPA): 144 triples (e.g., help?wanted); back-
2
The dataset is available at http://spp.montana.edu/
ward phrasal associates (BPA): 89 triples (e.g.,
wanted?help).
The second priming dataset is the Generalized
Event Knowledge dataset (henceforth GEK), al-
ready evaluated in Lapesa and Evert (2013): a
collection of 402 triples (target, consistent prime,
inconsistent prime) from three priming studies
conducted to demonstrate that event knowledge
is responsible for facilitation of the processing
of words that denote events and their partici-
pants. The first study was conducted by Fer-
retti et al. (2001), who found that verbs facili-
tate the processing of nouns denoting prototypi-
cal participants in the depicted event and of ad-
jectives denoting features of prototypical partic-
ipants. The study covered five thematic rela-
tions: agent (e.g., pay?customer), patient, fea-
ture of the patient, instrument, location. The sec-
ond study (McRae et al., 2005) focussed on prim-
ing from nouns to verbs. It involved four re-
lations: agent (e.g., reporter?interview), patient,
instrument, location. The third study (Hare et
al., 2009) investigated priming from nouns to
nouns, referring to participants of the same event
or the event itself. The dataset involves seven
relations: event-people (e.g., trial?judge), event-
thing, location-living, location-thing, people-
instrument, instrument-people, instrument-thing.
In the presentation of our results we group syn-
onyms with antonyms and cohyponyms from SPP
as paradigmatic relations, and the entire GEK
dataset with backward and forward phrasal asso-
ciates from SPP as syntagmatic relations.
3.2 Evaluated Parameters
DSMs evaluated in this paper belong to the class of
bag-of-words models. We defined a large vocab-
ulary of target words (27522 lemma types) con-
taining all the items from the evaluated datasets
as well as items from other state-of-the-art evalu-
ation studies (Baroni and Lenci, 2010; Baroni and
Lenci, 2011). Context words were filtered by part-
of-speech (nouns, verbs, adjectives, and adverbs).
Distributional models were built using the UCS
toolkit
3
and the wordspace package for R
4
. The
following parameters have been evaluated:
? Source corpus (abbreviated as corpus in plots
1-4): We compiled DSMs from three corpora
often used in DSM evaluation studies and that
3
http://www.collocations.de/software.html
4
http://r-forge.r-project.org/projects/wordspace/
162
differ in both size and quality: British National
Corpus
5
, ukWaC, and WaCkypedia EN
6
.
? Size of the context window (win.size): As
this parameter quantifies the amount of shared
context involved in the computation of similar-
ity, we expect it to be crucial in determining
whether syntagmatic or paradigmatic relations
are captured. We therefore use a finer granu-
larity for window size than Lapesa and Evert
(2013): 1, 2, 4, 8 and 16 words.
? Directionality of the context window
(win.direction): When collecting co-occurrence
information from the source corpora, we use ei-
ther a directed window (i.e., separate frequency
counts for co-occurrences of a context term
to the left and to the right of the target term)
or an undirected window (i.e., no distinction
between left and right context when collecting
co-occurrence counts).
? Context selection: From the full co-occurrence
matrix collected as described above, we select
dimensions (columns) according to the follow-
ing parameters:
? Criterion for context selection (criterion):
We select the top-ranked dimensions either
according to marginal frequency (i.e., we use
the most frequent words as context terms)
or number of nonzero co-occurrence counts
(i.e., we use the context terms that co-occur
with the highest number of targets).
? Number of context dimensions (con-
text.dim): We select the top-ranked 5000,
10000, 20000, 50000 or 100000 dimensions,
according to the criterion above.
? Feature scoring (score): Co-occurrence counts
are weighted using one of the following associa-
tion measures: frequency, Dice coefficient, sim-
ple log-likelihood, Mutual Information, t-score,
z-score or tf.idf.
7
? Feature transformation (transformation): A
transformation function may be applied to re-
duce the skewness of feature scores. Possible
transformations are: none, square root, logarith-
mic and sigmoid.
5
http://www.natcorp.ox.ac.uk/
6
Both ukWaC and WaCkypedia EN are available at:
wacky.sslmit.unibo.it/doku.php?id=corpora
7
See Evert (2008) for a description of these measures and
details on the calculation of association scores. Note that
we compute ?sparse? versions of the association measures
(where negative values are clamped to zero) in order to pre-
serve the sparseness of the co-occurrence matrix.
? Distance metric (metric): We apply cosine dis-
tance (i.e., angle between vectors) or Manhattan
distance.
? Dimensionality reduction: We apply singular
value decomposition in order to project distri-
butional vectors to a relatively small number of
latent dimensions and compare the results to the
unreduced runs
8
. For the SVD-based models,
there are two additional parameters:
? Number of latent dimensions (red.dim):
Whether to use the first 100, 300, 500, 700
or 900 latent dimensions from the SVD anal-
ysis.
? Number of skipped dimensions (dim.skip):
When selecting latent dimensions, we option-
ally skip the first 50 or 100 SVD compo-
nents. This parameter was inspired by Bul-
linaria and Levy (2012), who found that dis-
carding the initial components of the reduced
matrix, i.e. the SVD components with highest
variance, improves evaluation results.
? Index of distributional relatedness (rel.index):
We propose two alternative ways of quantify-
ing the degree of relatedness between two words
a and b represented in a DSM. The first op-
tion (and standard in distributional modeling)
is to compute the distance (cosine or Manhat-
tan) between the vectors of a and b. The sec-
ond option, proposed in this work, is based on
neighbor rank, i.e. we determine the rank of
the target among the nearest neighbors of each
prime. We expect that the target will occur in a
higher position among the neighbors of the con-
sistent prime than among those of the inconsis-
tent prime. Since this corresponds to a lower
numeric rank value for the consistent prime, we
can treat neighbor rank as a measure of dissim-
ilarity. Neighbor rank is particularly interesting
as an index of relatedness because, unlike a dis-
tance metric, it can capture asymmetry effects
9
.
4 Methodology
In our evaluation study, we tested all the possible
combinations of the parameters listed in section
8
For efficiency reasons, we use randomized SVD (Halko
et al., 2009) with a sufficiently high oversampling factor to
ensure a good approximation.
9
Note that our use of neighbor rank is fully consistent with
the experimental design (primes are shown before targets).
See Lapesa and Evert (2013) for an analysis of the perfor-
mance of neighbor rank as a predictor of priming and discus-
sion of the implications of using rank in cognitive modeling.
163
3.2, resulting in a total of 537600 different model
runs (33600 in the setting without dimensionality
reduction, 504000 in the dimensionality-reduced
setting). The models were generated and evaluated
on a large HPC cluster within approx. 4 weeks.
Our methodology for model selection follows
the proposal of Lapesa and Evert (2013), who con-
sider DSM parameters as predictors of model per-
formance. We analyze the influence of individual
parameters and their interactions using general lin-
ear models with performance (percent accuracy)
as a dependent variable and the model parame-
ters as independent variables, including all two-
way interactions. Analysis of variance ? which
is straightforward for our full factorial design ? is
used to quantify the importance of each parameter
or interaction. Robust optimal parameter settings
are identified with the help of effect displays (Fox,
2003), which marginalize over all the parameters
not shown in a plot and thus allow an intuitive in-
terpretation of the effect sizes of categorical vari-
ables irrespective of the dummy coding scheme.
For each dataset, a separate linear model was
fitted. The results are reported and compared in
section 5. Table 1 lists the global goodness-of-fit
(R
2
) on each dataset, for the reduced and unre-
duced runs. Despite some variability across re-
lations and between unreduced and reduced runs,
the R
2
values are always high (? 75%), showing
that the linear model explains a large part of the
observed performance differences. It is therefore
justified to base our analysis on the linear models.
Relation Dataset Unreduced Reduced
Syntagmatic GEK 93% 87%
Syntagmatic FPA 90% 79%
Syntagmatic BPA 88% 77%
Paradigmatic SYN 92% 85%
Paradigmatic COH 89% 75%
Paradigmatic ANT 89% 76%
Table 1: Evaluation, Global R
2
5 Results
In this section, we present the results of our study.
We begin by looking at the distribution of accu-
racy for different datasets, and by comparing re-
duced and unreduced experimental runs in terms
of minimum, maximum and mean performance.
The results displayed in table 2 show that di-
mensionality reduction with SVD improves the
performance of the models for all datasets but
GEK. We conclude that the information lost by ap-
plying SVD reduction (namely, meaningful distri-
butional features, which are replaced by the gener-
Relation Dataset
Unreduced Reduced
Min Max Mean Min Max Mean
Syntagmatic GEK 54.8 98.4 86.6 48.0 97.0 80.8
Syntagmatic FPA 41.0 98.0 82.3 43.0 98.6 82.1
Syntagmatic BPA 49.4 97.7 83.8 41.6 98.9 83.9
Paradigmatic SYN 54.8 98.4 86.6 57.3 99.0 88.2
Paradigmatic COH 49.0 100.0 92.6 54.3 100.0 94.0
Paradigmatic ANT 69.6 100.0 94.2 57.8 100.0 94.3
Table 2: Distribution of Accuracy
alization encoded in the reduced dimensions) is ir-
relevant to other tasks, but crucial for modeling the
relations in the GEK dataset. This interpretation is
consistent with the detrimental effect of SVD in
tasks involving vector composition reported in the
literature (Baroni and Zamparelli, 2010).
5.1 Importance of Parameters
To obtain further insights into DSM performance
we explore the effect of specific model parameters,
comparing syntagmatic vs. paradigmatic relations
and reduced vs. unreduced runs.
In order to establish a ranking of the parameters
according to their importance wrt. model perfor-
mance, we use a feature ablation approach. The
ablation value for a given parameter is the propor-
tion of variance (R
2
) explained by this parameter
together with all its interactions, corresponding to
the reduction in adjusted R
2
of the linear model fit
if the parameter were left out. In other words, it
allows us to find out whether a certain parameter
has a substantial effect on model performance (on
top of all other parameters). Figures 1 to 4 display
the feature ablation values of all the evaluated pa-
rameters in the unreduced and reduced setting, for
paradigmatic and syntagmatic relations. Parame-
ters are ranked according to their average feature
ablation values in each setting.
Two parameters, namely feature score and fea-
ture transformation, are consistently crucial in
determining DSM performance, both in reduced
and unreduced runs, and for both paradigmatic
and syntagmatic relations. In the next section we
will show that it is possible to identify optimal (or
nearly optimal) values for those parameters that
are constant across relations.
A comparison of figures 1 and 2 with figures 3
and 4 allows us to identify parameters that lose
or gain explanatory power when SVD comes into
play. Feature ablation shows that the effect of the
index of distributional relatedness is substan-
tially smaller in the SVD-reduced runs, but this pa-
rameter still plays an important role. On the other
hand, two parameters gain explanatory power in a
164
ll
l
l
l
l
l
l
lcriterion
win.direction
context.dim
win.size
corpus
metric
transformation
rel.index
score
0 20 40 60Feature Ablation
l SYNANTCOH
Figure 1: Paradigmatic, unreduced
l
l
l
l
l
l
l
l
lcriterion
win.direction
context.dim
win.size
corpus
metric
transformation
rel.index
score
0 20 40 60Feature Ablation
l GEKFPABPA
Figure 2: Syntagmatic, unreduced
l
l
l
l
l
l
l
l
l
l
lcriterionwin.direction
context.dimdim.skip
rel.indexwin.size
red.dimmetric
corpustransformation
score
0 10 20 30Feature Ablation
l SYNANTCOH
Figure 3: Paradigmatic, reduced
l
l
l
l
l
l
l
l
l
l
lcriterionwin.direction
context.dimrel.index
dim.skipred.dim
metriccorpus
win.sizetransformation
score
0 10 20 30Feature Ablation
l GEKFPABPA
Figure 4: Syntagmatic, reduced
SVD-reduced setting: the size of the context win-
dow and the source corpus. Optimal values are
discussed in section 5.2.
Three parameters consistently have little or no
explanatory power: directionality of the con-
text window, criterion for context selection and
number of context dimensions.
We conclude this section by comparing rela-
tions within groups. Within paradigmatic rela-
tions, we note a significant drop in explanatory
power for the relatedness index when it comes to
antonyms. Within syntagmatic relations, the size
of the context window appears to be more crucial
on the GEK dataset than it is for FPA and BPA:
in the next section, the analysis of the best choices
for this parameter will provide a clue for the inter-
pretation of this opposition.
5.2 Best Parameter Values
In this section, we identify the best parameter val-
ues for syntagmatic and paradigmatic relations by
inspecting partial effects plots
10
. Our discussion
starts from the parameters that contribute to the
leading topic of this paper, namely the comparison
between syntagmatic and paradigmatic relations:
10
The partial effect plots in figures 5 to 12 display param-
eter values on the x-axis and their effect size in terms of pre-
dicted accuracy on the y-axis (see section 4 for more details
concerning the calculation of effect size).
window size, parameters related to dimensionality
reduction, and relatedness index.
As already anticipated in the feature ablation
analysis, the size of the context window plays
a crucial role in contrasting syntagmatic and
paradigmatic relations, as well as different rela-
tions within those general groups. The plots in fig-
ures 5 and 6 display its partial effect for paradig-
matic relations in the unreduced and reduced set-
tings, respectively. The plots in figures 7 and 8
display its partial effect for syntagmatic relations.
When no dimensionality reduction is involved, a
very small context window (i.e., one word) is suffi-
cient for all paradigmatic relations, and DSM per-
formance decreases as soon as we enlarge the con-
text window. The picture changes when apply-
ing dimensionality reduction: a 4-word window
is a robust choice for all paradigmatic relations
(although ANT show a further increase in perfor-
mance with an 8-word window), even in the SYN
task that is traditionally associated with very small
windows of 1 or 2 words (cf. Sahlgren (2006)).
A significant interaction between window size
and number of skipped dimensions (not shown for
reasons of space) sheds further light on this matter.
Without skipping SVD dimensions, the reduced
models achieve optimal performance for a 2-word
window and degrade more (COH) or less (ANT)
165
l l
l
l
l
l
l l l
ll l
l
l
l
84
87
90
93
96
1 2 4 8 16
dataset
l
l
l
SYNANTCOH
Figure 5: Window, paradigmatic, unreduced
l
l l l l
l
l
l l l
l
l
l l l
84
87
90
93
96
1 2 4 8 16
dataset
l
l
l
SYNANTCOH
Figure 6: Window, paradigmatic, reduced
l
l
l l l
l
l l l
l
l
l
l
l
l
74
76
78
80
82
84
86
1 2 4 8 16
dataset
l
l
l
GEKFPABPA
Figure 7: Window, syntagmatic, unreduced
l
l
l
l
l
l
l
l
l l
l
l
l l l
74
76
78
80
82
84
86
1 2 4 8 16
dataset
l
l
l
GEKFPABPA
Figure 8: Window, syntagmatic, reduced
quickly for larger windows. With 50 or 100 di-
mensions skipped, performance improves up to a
4- or 8-word window. Our interpretation is that the
first SVD dimensions capture general domain and
topic information dominating the co-occurrence
data; removing these dimensions reveals paradig-
matic semantic relations even for larger windows.
For syntagmatic relations without dimensionality
reduction, a larger context window of 4 words is
needed for FPA and BPA; a further increase of the
window is detrimental. For the GEK dataset, per-
formance peaks at 8 words, and decreases only
minimally for even larger windows. Again, di-
mensionality reduction improves performance for
large co-occurrence windows. For FPA and BPA,
the optimum seems to be achieved with a win-
dow of 4?8 words; performance on GEK contin-
ues to increase up to 16 words, the largest win-
dow size considered in our experiments. Such pat-
terns reflect differences in the nature of the se-
mantic relations involved: smaller windows pro-
vide better contextual representations for paradig-
matic relations while larger windows are needed to
capture syntagmatic relations with bag-of-words
DSMs (because co-occurring words then share a
large portion of their context windows). Interme-
diate window sizes are sufficient for phrasal col-
locates (which are usually adjacent), while event-
based relatedness (GEK) requires larger windows.
Returning briefly to the slight preference shown
by ANT for a larger window, we notice that ANT
seems to be more similar to the syntagmatic rela-
tions than SYN and COH. This is in line with the
observations of Justeson and Katz (1992) concern-
ing the tendency of antonyms to co-occur (e.g., in
coordinations such as short and long). Like syn-
onyms, antonyms are interchangeable in absentia;
but they also enter into syntagmatic patterns that
are uncommon for synonyms.
We now focus on the parameters related to di-
mensionality reduction, namely the number of la-
tent dimensions (figures 9 and 10) and the num-
ber of skipped dimensions (figures 11 and 12).
These parameters represent an extension of the
experiments conducted on the GEK dataset by
Lapesa and Evert (2013). They have already been
applied by Bullinaria and Levy (2012) to a differ-
ent set of tasks, including the TOEFL multiple-
choice synonymy task. In particular, Bullinaria
and Levy found that discarding the initial SVD di-
mensions (with highest variance) leads to substan-
tial improvements, especially in the TOEFL task.
In our experiments, we found no difference be-
tween syntagmatic and paradigmatic relations wrt.
the number of latent dimensions: the more, the
better in both cases (900 dimensions). The number
of skipped dimensions, however, shows some vari-
ability across the different relations. The results
for SYN are in agreement with the findings of Bul-
linaria and Levy (2012) on TOEFL: skipping 50
or 100 initial dimensions improves performance.
Skipping dimensions makes minimal difference
166
ll
l
l l
l
l
l l
l
l
l
l l l
87
90
93
96
100 300 500 700 900
dataset
l
l
l
SYNANTCOH
Figure 9: Latent dimensions, paradigmatic
l
l
l
l l
l
l
l
l l
l
l
l
l
l
76
78
80
82
84
86
100 300 500 700 900
dataset
l
l
l
GEKFPABPA
Figure 10: Latent dimensions, syntagmatic
l
l
l
l
l
ll
l
l
87
89
91
93
95
0 50 100
dataset
l
l
l
SYNANTCOH
Figure 11: Skipped dimensions, paradigmatic
l
l
l
l l
l
l
l
l
80
81
82
83
84
0 50 100
dataset
l
l
l
GEKFPABPA
Figure 12: Skipped dimensions, syntagmatic
for COH (best choice is 50 dimensions), while the
full range of reduced dimensions is necessary for
ANT. Within syntagmatic relations, the full range
of latent dimensions ensures good performance on
phrasal associates (even if skipping 50 dimensions
is not detrimental for BPA). GEK shows a pattern
similar to SYN, with 50 skipped dimensions lead-
ing to a considerable improvement.
We now inspect the best values for the related-
ness index. As shown in figure 13 for the unre-
duced runs and in figure 14 for the reduced runs,
neighbor rank is consistently better than distance
on all datasets. This is not surprising because, as
discussed in section 3.2, our use of neighbor rank
captures asymmetry and mirrors the experimental
setting, in which targets are shown after primes.
A further observation may be made relating to the
degree of asymmetry of different relations. The
unreduced setting in particular shows that syntag-
matic relations are subject to stronger asymme-
try effects than the paradigmatic ones, presumably
due to the directional nature of the relations in-
volved (phrasal associates and syntactic colloca-
tions). Among paradigmatic relations, antonyms
appear to be the least asymmetric ones (because
using neighbor rank instead of distance makes a
comparatively small difference).
We conclude by briefly summarizing the opti-
mal choices for the remaining parameters. The
corresponding partial effects plots are not shown
because of space constraints.
A very strong interaction between score and
transformation characterizes all four settings
(paradigmatic or syntagmatic datasets, reduced or
unreduced experimental runs). Association mea-
sures outperform raw co-occurrence frequency.
Measures based on significance tests (simple-ll,
t-score, z-score) are better than Dice, and to a
lesser extent, MI. Simple-ll is the best choice in
combination with a logarithmic transformation for
paradigmatic relations, z-score appears to be the
best measure for syntagmatic relations in combi-
nation with a square root transformation. The dif-
ference is small, however, and simple-ll with log
transformation works well across all datasets. On-
going experiments with standard tasks show a sim-
ilar pattern, suggesting that this combination of
score and transformation parameters is appropri-
ate for DSMs, regardless of the task involved.
The optimal distance metric is the cosine
distance, consistently outperforming Manhattan.
Concerning source corpus, BNC consistently
yields the worst results, while WaCkypedia and
ukWaC appear to be almost equivalent in the unre-
duced runs. The trade-off between quality and
quantity appears to be strongly biased towards
sheer corpus size in the case of distributional mod-
els. For syntagmatic relations and SVD-reduced
models, ukWaC is clearly the best choice. This
suggests that syntagmatic relations are better cap-
tured by features from a larger lexical inventory,
combined with the abstraction performed by SVD.
167
ll
l
l
l
ll
l
l
l
l
l
75
80
85
90
95
SYN ANT COH GEK FPA BPA
rel.index
l
l
distrank
Figure 13: Relatedness index, unreduced
l
l l
l
l
l
l
l
l
l
l
l
75
80
85
90
95
SYN ANT COH GEK FPA BPA
rel.index
l
l
distrank
Figure 14: Relatedness index, reduced
Concerning minimally explanatory parameters,
inspection of partial effect plots supported the
choice of ?unmarked? default values for direc-
tionality of the context window (i.e., undirected)
and criterion for context selection (i.e., fre-
quency), as well as an intermediate number of
context dimensions (i.e., 50000 dimensions).
5.3 Best Settings
We conclude by comparing the performance
achieved by our robust choice of optimal param-
eter values (?best setting?) from section 5.2 with
the performance of the best model for each dataset.
For space constraints, the analysis of best settings
focuses on the reduced experimental runs. Our
best settings, shown in table 3, perform fairly well
on the respective datasets
11
.
dataset corpus win score transf r.dim d.sk acc best
GEK ukwac 16 s-ll log 900 50 96.0 97.0
FPA ukwac 8 z-sc root 900 0 93.0 98.6
BPA ukwac 8 z-sc root 900 0 95.5 98.9
SYN ukwac 4 s-ll log 900 50 96.3 99.0
COH ukwac 4 s-ll log 900 50 98.7 100
ANT wacky 8 s-ll log 900 0 100 100
Table 3: Best settings: datasets, parameter values,
accuracy (acc), accuracy of the best model (best)
best setting corpus win score transf r.dim d.sk
Syntagmatic ukwac 8 z-sc root 900 0
Paradigmatic ukwac 4 s-ll log 900 50
General ukwac 4 s-ll log 900 0
Table 4: General best settings: parameter values
Dataset Best Synt. Best Para. General
GEK 92.5 94.8 91.3
FPA 93.0 90.2 91.7
BPA 95.5 97.7 95.5
SYN 94.4 96.3 96.3
COH 99.3 98.7 98.7
ANT 99.2 99.2 99.2
Table 5: General best settings: accuracy
11
Abbreviations in tables 3 and 4: win = window size;
transf = transformation; z-sc = z-score; s-ll = simple-ll; r.dim
= number of latent dimensions; d.sk = number of skipped di-
mensions. Parameters with fixed values for all datasets: num-
ber of context dimensions = 50k; direction = undirected; cri-
terion = frequency; metric = cosine; relatedness index = rank.
As a next step, we identified parameter combi-
nations that work well for all types of syntagmatic
and paradigmatic relations, as well as an even
more general setting that is suitable for paradig-
matic and syntagmatic relations alike. Best set-
tings are shown in table 4, their performance on
each dataset is reported in table 5. General models
achieve fairly good performance on all relations.
6 Conclusion
We presented a large-scale evaluation study of
bag-of-words DSMs on a classification task de-
rived from priming experiments. The leading
theme of our study is a comparison between syn-
tagmatic and paradigmatic relations in terms of
the aspects of distributional similarity that char-
acterize them. Our results show that second-order
DSMs are capable of capturing both syntagmatic
and paradigmatic relations, if parameters are prop-
erly tuned. Size of the co-occurrence window as
well as parameters connected to dimensionality re-
duction play a key role in adapting DSMs to par-
ticular relations. Even if we do not address the
more specific task of distinguishing between rela-
tions (e.g., synonyms vs. antonyms; see Scheible
et al. (2013) and references therein), we believe
that such applications may benefit from our de-
tailed analyses on the effects of DSM parameters.
Ongoing and future work is concerned with the
expansion of the evaluation setting to other classes
of models (first-order models, dependency-based
second-order models) and parameters (e.g., di-
mensionality reduction with Random Indexing).
Acknowledgments
We are grateful to Ken MacRae for providing us
the GEK priming data and to the three review-
ers. This research was funded by the DFG Col-
laborative Research Centre SFB 732 (Gabriella
Lapesa) and the DFG Heisenberg Fellowship
SCHU-2580/1-1 (Sabine Schulte im Walde).
168
References
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):1?49.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 1?10.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193.
John A. Bullinaria and Joseph P. Levy. 2012. Ex-
tracting semantic representations from word co-
occurrence statistics: stop-lists, stemming and svd.
Behavior Research Methods, 44:890?907.
James Curran. 2003. From distributional to semantic
similarity. Ph.D. thesis, Institute for Communicat-
ing and Collaborative Systems, School of Informat-
ics, University of Edinburgh.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Lin-
guistics, 28(2):105?144.
Katrin Erk, Sebastian Pad?o, and Ulrike Pad?o. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36(4):723?763.
Stefan Evert. 2008. Corpora and collocations. In Anke
L?udeling and Merja Kyt?o, editors, Corpus Linguis-
tics. An International Handbook, chapter 58. Mou-
ton de Gruyter, Berlin, New York.
Todd Ferretti, Ken McRae, and Ann Hatherell. 2001.
Integrating verbs, situation schemas, and thematic
role concepts. Journal of Memory and Language,
44(4):516?547.
John Fox. 2003. Effect displays in R for gener-
alised linear models. Journal of Statistical Software,
8(15):1?27.
Nathan Halko, Per-Gunnar Martinsson, and Joel A.
Tropp. 2009. Finding structure with randomness:
Stochastic algorithms for constructing approximate
matrix decompositions. Technical Report 2009-05,
ACM, California Institute of Technology.
Mary Hare, Michael Jones, Caroline Thomson, Sarah
Kelly, and Ken McRae. 2009. Activating event
knowledge. Cognition, 111(2):151?167.
Zelig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Amac Herda?gdelen, Marco Baroni, and Katrin Erk.
2009. Measuring semantic relatedness with vector
space models and random walks. In Proceedings
of the 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 50?53.
Keith A. Hutchison, David A. Balota, James H. Neely,
Michael J. Cortese, Emily R. Cohen-Shikora, Chi-
Shing Tse, Melvin J. Yap, Jesse J. Bengson, Dale
Niemeyer, and Erin Buchanan. 2013. The seman-
tic priming project. Behavior Research Methods,
45(4):1099?1114.
John. S. Justeson and Slava M. Katz. 1992. Redefining
antonymy: The textual structure of a semantic rela-
tion. Literary and Linguistic Computing, 7(3):176?
184.
Gabriella Lapesa and Stefan Evert. 2013. Evaluat-
ing neighbor rank and distance measures as predic-
tors of semantic priming. In Proceedings of the
ACL Workshop on Cognitive Modeling and Compu-
tational Linguistics (CMCL 2013), pages 66?74.
Alessandro Lenci and Giulia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of *SEM 2012: The First Joint Con-
ference on Lexical and Computational Semantics ?
Volume 1, pages 75?79.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Scott McDonald and Chris Brew. 2004. A distri-
butional model of semantic context effects in lexi-
cal processing. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 17?24.
Ken McRae, Mary Hare, Jeffrey L. Elman, and Todd
Ferretti. 2005. A basis for generating expectan-
cies for verbs from nouns. Memory & Cognition,
33(7):1174?1184.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Yves Peirsman, Kris Heylen, and Dirk Speelman.
2008. Putting things in order. First and second order
context models for the calculation of semantic sim-
ilarity. In JADT 2008: 9es Journ?ees internationales
d?Analyse statistique des Donn?ees Textuelles.
Reinhard Rapp. 2002. The computation of word asso-
ciations: Comparing syntagmatic and paradigmatic
approaches. In Proceedings of the 19th Interna-
tional Conference on Computational Linguistics -
Volume 1, pages 1?7.
Magnus Sahlgren. 2006. The word-space model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, University
of Stockolm.
169
Magnus Sahlgren. 2008. The distributional hypothe-
sis. Rivista di Linguistica (Italian Journal of Lin-
guistics), 20(1):33?53.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering Distributional Dif-
ferences between Synonyms and Antonyms in a
Word Space Model. In Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing, pages 489?497.
Hinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 27(1):97?
123.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th International
Conference of Computational Linguistics, pages
1015?1021, Geneva, Switzerland.
170
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 532?540,
Dublin, Ireland, August 23-24, 2014.
SemantiKLUE: Robust Semantic Similarity at Multiple Levels Using
Maximum Weight Matching
Thomas Proisl and Stefan Evert and Paul Greiner and Besim Kabashi
Friedrich-Alexander-Universit?t Erlangen-N?rnberg (FAU)
Department Germanistik und Komparatistik
Professur f?r Korpuslinguistik
Bismarckstr. 6, 91054 Erlangen, Germany
{thomas.proisl,stefan.evert,paul.greiner,besim.kabashi}@fau.de
Abstract
Being able to quantify the semantic similar-
ity between two texts is important for many
practical applications. SemantiKLUE com-
bines unsupervised and supervised tech-
niques into a robust system for measuring
semantic similarity. At the core of the sys-
tem is a word-to-word alignment of two
texts using a maximum weight matching
algorithm. The system participated in three
SemEval-2014 shared tasks and the com-
petitive results are evidence for its usability
in that broad field of application.
1 Introduction
Semantic similarity measures the semantic equiv-
alence between two texts ranging from total dif-
ference to complete semantic equivalence and is
usually encoded as a number in a closed interval,
e. g. [0,5]. Here is an example for interpreting the
numeric similarity scores taken from Agirre et al.
(2013, 33):
0. The two sentences are on different topics.
1. The two sentences are not equivalent, but are
on the same topic.
2. The two sentences are not equivalent, but
share some details.
3. The two sentences are roughly equivalent, but
some important information differs/missing.
4. The two sentences are mostly equivalent, but
some unimportant details differ.
5. The two sentences are completely equivalent,
as they mean the same thing.
Systems capable of reliably predicting the semantic
similarity between two texts can be beneficial for a
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
broad range of NLP applications, e. g. paraphrasing,
MT evaluation, information extraction, question
answering and summarization.
A general system for semantic similarity aiming
at being applicable in such a broad scope has to
be able to adapt to the use case at hand, because
different use cases might, for example, require dif-
ferent similarity scales: For one application, two
texts dealing roughly with the same topic should
get a high similarity score, whereas for another ap-
plication being able to distinguish between subtle
differences in meaning might be important. The
three SemEval-2014 shared tasks focussing on se-
mantic similarity (cf. Sections 3, 4 and 5 for more
detailed task descriptions) provide a rich testbed
for such a general system, as the individual tasks
and subtasks have slightly different objectives.
In the remainder of this paper, we describe
SemantiKLUE, a general system for measuring se-
mantic similarity between texts that we built based
on our experience from participating in the *SEM
2013 shared task on ?Semantic Textual Similarity?
(Greiner et al., 2013).
2 System Description
SemantiKLUE operates in two stages. In the first,
unsupervised stage, a number of similarity mea-
sures are computed. Those measures are the same
for all tasks and range from simple heuristics to dis-
tributional approaches to resource-heavy methods
based on WordNet and dependency structures. The
idea is to have a variety of similarity measures that
can capture small differences in meaning as well
as broad thematical similarities. In the second, su-
pervised stage, all similarity measures obtained in
this way are passed to a support vector regression
learner that is trained on the available gold standard
data in order to obtain a final semantic similarity
score. This way, the proper similarity scale for a
given task can be learned. The few remaining out-
liers in the predictions for new text pairs are cut
532
off to fit the interval required by the task definition
([0,4] or [0,5]).
Our submissions for the individual tasks were
created using incomplete versions from different
developmental stages of the system. In the follow-
ing sections we describe the current version of the
complete system for which we also report compa-
rable results for all tasks (cf. Sections 3? 5).
The whole system is implemented in Python.
2.1 Preprocessing
We use Stanford CoreNLP
1
for part-of-speech tag-
ging, lemmatizing and parsing the input texts. We
utilize the CCprocessed variant of the Stanford De-
pendencies (collapsed dependencies with propaga-
tion of conjunct dependencies; de Marneffe and
Manning (2008, 13?15)) to create a graph represen-
tation of the texts using the NetworkX
2
(Hagberg
et al., 2008) module. All the similarity measures
described below are computed on the basis of that
graph representation. It is important to keep in
mind that by basing all computations on the Stan-
ford Dependencies model we effectively ignore
most of the prepositions when using measures that
work on tokens.
3
For some tasks, we perform some
additional task-specific preprocessing steps prior
to parsing, cf. task descriptions below.
2.2 Simple Measures
We use four simple heuristic similarity measures
that need very little preprocessing. The first two
are word form overlap and lemma overlap between
the two texts. We take the sets of word form to-
kens/lemmatized tokens in text A and text B and
calculate the Jaccard coefficient:
overlap =
|A?B|
|A?B|
.
The third is a heuristic for the difference in text
length that was used by Gale and Church (1993) as
a similarity measure for aligning sentences:
z
i
=
d
i
?
d
, where d
i
= b
i
?
?
N
j=1
b
j
?
N
j=1
a
j
a
i
.
For each of the N text pairs we calculate the differ-
ence d
i
between the observed length of text B and
1http://nlp.stanford.edu/software/corenlp.
shtml
2http://networkx.github.com/
3
That is because in the CCprocessed variant of the Stanford
Dependencies most prepositions are ?collapsed? into depen-
dency relations and are therefore represented as edges and not
as vertices in the graph.
the expected length of text B based on the length
of text A. By dividing that difference d
i
by the stan-
dard deviation of all those differences, we obtain
our heuristic z
i
.
The fourth is a binary feature expressing whether
the two texts differ in their use of negation. We
check if one of the texts contains any of the lem-
mata no, not or none and the other doesn?t. That
feature is motivated by the comparatively large
number of sentences in the SICK dataset (Marelli
et al., 2014b) that mainly differ in their use of nega-
tion, e. g. sentence pair 42 in the training data that
has a gold similarity score of 3.4:
? Two people are kickboxing and spectators are
watching
? Two people are kickboxing and spectators are
not watching
2.3 Measures Based on Distributional
Document Similarity
We obtain document similarity scores from two
large-vocabulary distributional semantic models
(DSMs).
The first model is based on a 10-billion word
Web corpus consisting of Wackypedia and ukWaC
(Baroni et al., 2009), UMBC WebBase (Han et
al., 2013), and UKCOW 2012 (Sch?fer and Bild-
hauer, 2012). Target terms and feature terms are
POS-disambiguated lemmata.
4
We use parame-
ters suggested by recent evaluation experiments:
co-occurrence counts in a symmetric 4-word win-
dow, the most frequent 30,000 lexical words as
features, log-likelihood scores with an additional
log-transformation, and SVD dimensionality re-
duction of L2-normalized vectors to 1000 latent
dimensions. This model provides distributional
representations for 150,000 POS-disambiguated
lemmata as target terms.
The second model was derived from the second
release of the Google Books N-Grams database
(Lin et al., 2012), using the dependency pairs pro-
vided in this version. Target and feature terms are
case-folded word forms; co-occurrence ounts are
based on direct syntactic relations. Here, the most
frequent 50,000 word forms were used as features.
All other parameters are identical to the first DSM.
This model provides distributional representations
for 250,000 word forms.
We compute bag-of-words centroid vectors for
each text as suggested by (Sch?tze, 1998). For each
4
e.g. can_N for the noun can
533
text pair and DSM, we calculate the cosine similar-
ity between the two centroid vectors as a measure
of their semantic similarity. We also determine the
number of unknown words in both texts according
to both DSMs as additional features.
2.4 Alignment-based Measures
We also use features based on word-level similar-
ity. We separately compute similarities between
words using state-of-the-art WordNet similarity
measures and the two distributional semantic mod-
els described above. The words from both texts
are then aligned using those similarity scores to
maximize the similarity total. We use two types
of alignment: One-to-one alignment where some
words in the longer text remain unaligned and one-
to-many alignment where all words are aligned.
The one-to-many alignment is based on the one-to-
one alignment and aligns each previously unaligned
word in the longer text to the most similar word in
the shorter text. The discussion of the alignment
algorithm is based on the former case.
2.4.1 Alignment via Maximum Weight
Matching
We opt for a graphical solution to the alignment
problem. The similarities between the words from
both texts can be modelled as a bipartite graph in
which every word from text A is a vertice on the
left-hand side of the graph and every word from
text B a vertex on the right-hand side. Weighted
edges connect every word from text A to every
word from text B. The weight of an edge corre-
sponds to the similarity between the two words it
connects. In order to obtain an optimal one-to-one
alignment we have to select edges in such a way
that no two edges share a common vertice and that
the sum of the edge weights is maximized. That
corresponds to the problem of finding the maxi-
mum weight matching in the graph. SemantiKLUE
utilizes the NetworkX implementation of Galil?s
(1986) algorithms for finding that maximum weight
matching.
Figure 1 visualizes the one-to-one alignment be-
tween two sentences. For the one-to-many align-
ment, the previously unaligned words are aligned
as indicated by the dashed lines.
2.4.2 Measures Based on Distributional
Word Similarities
For each of the two DSMs described in Section 2.3
we compute the best one-to-one and the best one-
A
woman
is
using
a
machine
made
for
sewing
A
woman
is
sewing
with
a
machine
Figure 1: Alignment between a sentence pair from
the SICK data set.
to-many alignment using the cosine similarity be-
tween two words as edge weight. For each of
those two alignments we compute the following
two similarity measures: I) the arithmetic mean of
the cosines between all the aligned words from text
A and text B and II) the arithmetic mean ignoring
identical word pairs.
In addition to those eight measures, we use the
lemma-based DSM for computing the distribution
of cosines between lemma pairs. For both align-
ments, we categorize the cosines between aligned
lemma pairs into five heuristically determined inter-
vals ([0.2,0.35), [0.35,0.5), [0.5,0.7), [0.7,0.999),
[0.999,1.0])
5
and use the proportions as features.
Intuitively, the top bins correspond to links between
identical words, paradigmatically related words
and topically related words. All in all, we use a
total of 18 features computed from the DSM-based
alignments.
2.4.3 Measures Based on WordNet
We utilize two state-of-the-art (Budanitsky and
Hirst, 2006) WordNet similarity measures for cre-
ating alignments: Leacock and Chodorow?s (1998)
normalized path length and Lin?s (1998) universal
similarity measure. For both of those similarity
measures we compute the best one-to-one and the
best one-to-many alignment. For each alignment
we compute the following two similarity measures:
I) the arithmetic mean of the similarities between
the aligned words from text A and text B and II) the
arithmetic mean ignoring identical word pairs.
5
Values in the interval [0.0,0.2) are discarded as they
would be collinear with the other features.
534
We also include the number of unknown words
in both texts according to WordNet as additional
features.
2.5 Measures Using the Dependency
Structure
We expect that the information encoded in the de-
pendency structure of the texts can be beneficial in
determining the semantic similarity between them.
Therefore, we use three heuristics for measuring
similarity on the level of syntactic dependencies.
The first simply measures the overlap of depen-
dency relation labels between the two texts (cf. Sec-
tion 2.2). The second utilizes the fact that the Stan-
ford Dependencies are organized in a hierarchy (de
Marneffe and Manning, 2008, 11?12) to compute
Leacock and Chodorow?s normalized path lengths
between individual dependency relations. That
measure for the similarity between dependency re-
lations is then used to determine the best one-to-one
alignment between dependency relations from text
A and text B and to compute the arithmetic mean
of the similarities between the aligned dependency
relations. The third heuristic gives an indication
of the quality of the one-to-one alignment and can
be used to distinguish texts that contain the same
words in different syntactic structures. It uses the
one-to-one alignment created with similarity scores
from the lemma-based DSM (cf. Section 2.4.2) to
compute the average overlap of neighbors for all
aligned word pairs. The overlap of neighbors is
determined by computing the Jaccard coefficient
of sets N
A
and N
B
. Set N
A
contains all words from
text B that are aligned to words from text A that
are connected to the target word via a single depen-
dency relation. N
B
contains all words from text B
that are connected to the word aligned to the target
word in text A via a single dependency relation.
2.6 Experimental Features
As an experiment, we included features from a com-
mercial text clustering software that is currently
being developed by our team (Greiner and Evert, in
preparation). We used this tool ? which combines
ideas from Latent Semantic Indexing and distribu-
tional semantics with multiple clustering steps ? as
a black box.
We loaded all training, development and test
items for a given task into the system and applied
the clustering algorithm. However, we did not
make use of the resulting topic clusters. Instead, we
computed cosine similarities for each pair (s
1
,s
2
)
of sentences (or other textual units) based on the in-
ternal representation. In addition, we computed the
average neighbour rank of the two sentences, based
on the rank of s
2
among the nearest neighbours of
s
1
and vice versa.
Since these features are generated from the task
data themselves, they should adapt automatically
to the range of meaning differences present in a
given data set.
2.7 Machine Learning
Using all the features described above, we have a
total of 39 individual features that measure seman-
tic similarity between two texts (cf. Sections 2.2 to
2.5) and two experimental features (cf. Section 2.6).
In order to obtain a single similarity score, we use
the scikit-learn
6
(Pedregosa et al., 2011) implemen-
tation of support vector regression. In our cross-
validation experiments we got the best results with
an RBF kernel of degree 2 and a penaltyC= 0.7, so
those are the parameters we use in our experiments.
The SemEval-2014 Task 1 also includes a classi-
fication subtask for which we use the same 39+2
features for training a support vector classifier.
Cross-validation suggests that the best parameter
setting is a polynomial kernel of degree 2 and a
penalty C = 2.5.
3 SemEval-2014 Task 1
3.1 Task Description
The focus of the shared task on ?Evaluation of com-
positional distributional semantic models on full
sentences through semantic relatedness and textual
entailment? (Marelli et al., 2014a) lies on the com-
positional nature of sentence semantics. By using
a specially created data set (Marelli et al., 2014b)
that tries to avoid multiword expressions and other
idiomatic features of language outside the scope of
compositional semantics, it provides a testbed for
systems implementing compositional variants of
distributional semantics. There is also an additional
subtask for detecting the entailment relation (entail-
ment, neutral, contradiction) between to sentences.
Although SemantiKLUE lacks a truly sophisti-
cated component for dealing with compositional
semantics (besides trying to incorporate the depen-
dency structure of the texts), the system takes the
seventh place in the official ranking by Pearson
correlation with a correlation coefficient of 0.780
6http://scikit-learn.org/
535
(best of 17 systems: 0.828). In the entailment sub-
task, the system even takes the fourth place with an
accuracy of 0.823 (best of 18 systems: 0.846).
3.2 Experiments
The official runs we submitted for this task were
created by a work-in-progress version of Semanti-
KLUE that did not contain all the features de-
scribed above. In this section, we report on some
post-hoc experiments with the complete system us-
ing all the features as well as various subsets of
features. See Table 1 for an overview of the results.
Run r ? MSE Acc.
primary run 0.780 0.736 0.403 0.823
best run 0.782 0.738 0.398 0.823
complete system 0.798 0.754 0.373 0.820
no deps 0.793 0.748 0.383 0.817
no deps, no WN 0.763 0.713 0.432 0.793
complete + experimental 0.801 0.757 0.367 0.823
only DSM alignment 0.729 0.670 0.484 0.746
only WordNet 0.708 0.636 0.515 0.715
only simple 0.676 0.667 0.561 0.754
only DSM document 0.660 0.568 0.585 0.567
only deps 0.576 0.565 0.688 0.614
Table 1: Results for task 1 (Pearson?s r, Spearman?s
? , mean squared error and accuracy).
The whole system as described above, without
the experimental features, performs even a bit bet-
ter in the semantic similarity subtask (taking place
6) and only slightly worse in the entailment subtask
(still taking place 4) than the official submissions.
Adding the experimental features slightly improves
the results but does not lead to a better position in
the ranking.
We are particularly interested in the impact of
the resource-heavy features derived from the de-
pendency structure of the texts and from Word-
Net. If we use the complete system without the
dependency-based features (emulating the case of
a language for which we have access to a WordNet-
like resource but not to a parser), we get results
that are only marginally worse than those for the
complete system and lead to the same places in the
rankings. Additionally leaving out WordNet has a
bigger impact and results in places 9 and 8 in the
rankings.
Regarding the individual feature groups, the
DSM-alignment-based measures are the best fea-
ture group for predicting semantic similarity and
the simple heuristic measures are the best feature
group for predicting entailment.
4 SemEval-2014 Task 3
4.1 Task Description
Unlike the other tasks, which focus on similar-sized
texts, the shared task on ?Cross-Level Semantic
Similarity? (Jurgens et al., 2014) is about measur-
ing semantic similarity between textual units of
different lengths. It comprises four subtasks com-
paring I) paragraphs to sentences, II) sentences to
phrases, III) phrases to words and IV) words to
word senses (taken from WordNet). Due to the
nature of this task, performance in it might be es-
pecially useful as an indicator for the usefulness of
a system in the area of summarization.
SemantiKLUE takes the fourth place out of 38 in
both the official ranking by Pearson correlation and
the alternative ranking by Spearman correlation.
4.2 Additional Preprocessing
For the official run we perform some additional pre-
processing on the data for the two subtasks on com-
paring phrases to words and words to word senses.
On the word level we combine the word with the
glosses of all its WordNet senses and on the word
sense level we replace the WordNet sense indica-
tion with its corresponding lemmata and gloss. As
our post-hoc experiments show that has a nega-
tive effect on performance in the phrase-to-word
subtask. Therefore, we skip the additional prepro-
cessing on that level for our experiments described
below.
4.3 Experiments
For each of the four subtasks, we perform the
same experiments as described in Section 3.2: We
compare the official run submitted from a work-
in-progress version of SemantiKLUE with the re-
sults from the whole system; we see how the sys-
tem performs without dependency-based features
and WordNet-based features; we try out the experi-
mental features; we determine the most important
feature group for the subtask. Table 2 gives an
overview of the results.
4.3.1 Paragraph to Sentence
Our submitted run takes the fifth place (ties with
another system) in the official ranking by Pearson
correlation with a correlation coefficient of 0.817
(best of 34 systems: 0.837) and seventh place in
the alternative ranking by Spearman correlation.
536
Run Paragraph to sent. Sent. to phrase Phrase to word Word to sense
r ? r ? r ? r ?
official 0.817 0.802 0.754 0.739 0.215 0.218 0.314 0.327
complete system 0.817 0.802 0.754 0.739 0.284 0.289 0.316 0.330
no deps 0.815 0.802 0.752 0.739 0.309 0.313 0.312 0.329
no deps, no WN 0.813 0.802 0.736 0.721 0.335 0.335 0.234 0.248
complete + experimental 0.816 0.800 0.752 0.738 0.292 0.298 0.318 0.330
only DSM alignment 0.799 0.789 0.724 0.711 0.302 0.301 0.216 0.216
only WordNet 0.787 0.769 0.664 0.641 0.186 0.171 0.313 0.311
only simple 0.807 0.793 0.686 0.672 0.128 0.121 0.089 0.093
only DSM document 0.629 0.624 0.546 0.558 0.247 0.240 0.144 0.148
only deps 0.655 0.621 0.449 0.440 0.036 0.057 ?0.080 ?0.076
Table 2: Results for task 3 (Pearson?s r and Spearman?s ?).
The complete SemantiKLUE system gives identical
results. Leaving out the resource-heavy features
based on the dependency structure and WordNet
diminishes the results only very slightly, though
it still resolves the tie and puts the system on the
sixth place in the Pearson ranking. Adding the
experimental features to the complete system has a
minor negative effect.
Probably due to the length of the texts, our sim-
ple heuristic measures surpass the DSM-alignment-
based measures as the best feature group for pre-
dicting semantic similarity.
4.3.2 Sentence to Phrase
In this subtask, SemantiKLUE takes the fourth
place in both the official ranking with a Pearson
correlation coefficient of 0.754 (best of 34 systems:
0.777) and in the alternative ranking by Spearman
correlation. The complete system performs iden-
tically to our submitted run and leaving out the
dependency-based features has little impact on the
results. Additionally also leaving out the WordNet-
based features has more impact on the results and
puts the system on the eighth place in the official
ranking. Just as in the paragraph-to-sentence sub-
task, adding the experimental features to the com-
plete system has a slightly negative effect.
For this subtask, the DSM-alignment-based mea-
sures are clearly the feature group that yields the
best results.
4.3.3 Phrase to Word
For our submitted run we performed the additional
preprocessing described in Section 4.2 resulting
in the eleventh place in the official ranking with
a Pearson correlation coefficient of 0.215 (best of
22 systems: 0.415) and the 14th place in the alter-
native ranking by Spearman correlation. For our
experiments with the complete system we skip that
additional preprocessing step, i. e. we do not add
the WordNet glosses to the word, and drastically
improve the results, putting our system on the third
place in the official ranking. Even more interesting
is the observation that leaving out the resource-
heavy features further improves the results, putting
the system on the second place. In consistency
with those observations, the DSM-alignment-based
measures are not only the strongest individual fea-
ture group but also yield better results when taken
alone than the complete system.
In contrast to the first two subtasks, adding the
experimental features to the complete systems has
a slightly positive effect here.
4.3.4 Word to Sense
In the word-to-sense subtask, SemantiKLUE takes
the third place in both the official ranking with a
Pearson correlation coefficient of 0.316 (best of
20 systems: 0.381) and in the alternative rank-
ing by Spearman correlation. The complete sys-
tem performs slightly better than our submitted
run and adding the experimental features gives
another marginal improvement. Leaving out the
dependency-based features has little impact but
also leaving out the WordNet-based features sev-
erly hurts performance. The reason for that be-
haviour becomes clear when we look at the results
for the individual feature groups: the WordNet-
based measures are clearly the strongest feature
group for predicting the semantic similarity be-
tween words and word senses.
5 SemEval-2014 Task 10
5.1 Task Description
The shared task on ?Multilingual Semantic Textual
Similarity? (Agirre et al., 2014) is a continuation
of the SemEval-2012 and *SEM 2013 shared tasks
537
Run d
e
f
t
-
f
o
r
u
m
d
e
f
t
-
n
e
w
s
h
e
a
d
l
i
n
e
s
i
m
a
g
e
s
O
n
W
N
t
w
e
e
t
-
n
e
w
s
w
.
m
e
a
n
best run 0.349 0.643 0.733 0.773 0.855 0.640 0.694
complete (all training data) 0.432 0.638 0.660 0.736 0.810 0.659 0.676
best overall training data 0.464 0.672 0.657 0.771 0.836 0.690 0.700
best overall, no deps 0.457 0.675 0.636 0.764 0.834 0.690 0.694
best overall, no deps, no WN 0.426 0.653 0.617 0.719 0.780 0.636 0.654
best overall + experimental 0.466 0.674 0.673 0.772 0.849 0.687 0.706
best individual training data 0.475 0.706 0.711 0.788 0.852 0.715 0.727
best individ., no deps 0.465 0.700 0.699 0.781 0.848 0.722 0.722
best individ., no deps, no WN 0.448 0.722 0.677 0.752 0.791 0.706 0.697
best individ. + experimental 0.475 0.711 0.715 0.795 0.864 0.721 0.733
Table 3: Results for task 10.
on semantic textual similarity (Agirre et al., 2012;
Agirre et al., 2013). It comprises two subtasks:
English semantic textual similarity and Spanish
semantic textual similarity. For each subtask, there
are sentence pairs from various genres.
We only participate in the English subtask and
take the 13th place out of 38 with a weighted mean
of Pearson correlation coefficients of 0.694 (best
system: 0.761).
5.2 Experiments
From participating in the *SEM 2013 shared task
on semantic textual similarity (Greiner et al., 2013)
we already know that the composition of the train-
ing data is one of the strongest influences on system
performance in this task. As the individual data sets
are not very similar to each other, we tried to come
up with a good subset of the available training data
for each data set. In doing so, we were moderately
successful as the results in Table 3 show. Run-
ning the complete system with all of the available
training data on all test data sets results in a lower
weighted mean than our submitted run. If we stick
to using the same training data for all test data sets
and optimize the subset of the training data we use,
we achieve a slightly better result than our submit-
ted run (the optimal subset consists of the FNWN,
headlines, MSRpar, MSRvid and OnWN data sets).
Using that optimal subset of the training data and
adding the experimental features to the complete
system has a minor positive effect on the weighted
mean, with the biggest impact on the headlines and
OnWN data sets. Using the complete system with-
out the dependency-based features gives roughly
the same results but omitting all resource-heavy
features has clearly a negative impact on the re-
sults.
In another experiment we try to optimize our
strategy of finding the best subset of the training
data for each test data set. Doing that gives us a
considerably higher weighted mean than using the
same training data for every test data set, putting
our system on the eighth place. Using the complete
system, we find that the best training data subsets
for the individual test data sets are those shown in
Table 4.
test set training sets
deft-forum FNWN, headlines, MSRvid
deft-news FNWN, MSRpar, MSRvid
headlines FNWN, headlines, MSRpar
images FNWN, MSRpar, MSRvid
OnWN FNWN, MSRvid, OnWN
tweet-news FNWN, headlines, MSRpar, MSRvid
Table 4: Optimal subsets of training data for use
with the complete SemantiKLUE system.
If we add the experimental features to the com-
plete system and still optimize the training data sub-
sets, we get a small boost to the results. Leaving out
the dependency-based features does not really hurt
performance but also omitting the WordNet-based
features has a negative impact on the results.
6 Conclusion
SemantiKLUE is a robust system for predicting the
semantic similarity between two texts that can also
be used to predict entailment. The system achieves
good or very good results in three SemEval-2014
tasks representing a broad variety of semantic simi-
larity problems (cf. Table 5 for an overview of the
results of all subtasks). Our two-staged strategy of
computing several similarity measures and using
them as input for a machine learning mechanism
538
Subtask submitted run complete system winner score
score rank score rank
Task 1, similarity 0.780 7/17 0.798 6/17 0.828
Task 1, entailment 0.823 4/18 0.820 4/18 0.846
Task 3, par-2-sent 0.817 5/34 0.817 5/34 0.837
Task 3, sent-2-phr 0.754 4/34 0.754 4/34 0.777
Task 3, phr-2-word 0.215 11/22 0.284 3/22 0.415
Task 3, word-2-sense 0.314 3/20 0.316 3/20 0.381
Task 3 overall N/A 4/38 N/A 3/38 N/A
Task 10, deft-forum 0.349 20/38 0.464 12/38 0.531
Task 10, deft-news 0.643 22/37 0.672 19/37 0.785
Task 10, headlines 0.733 15/37 0.657 20/37 0.784
Task 10, images 0.773 16/37 0.771 17/37 0.834
Task 10, OnWN 0.855 3/36 0.836 7/36 0.875
Task 10, tweet-news 0.640 20/37 0.690 12/37 0.792
Task 10 overall 0.694 13/38 0.700 13/38 0.761
Table 5: Overview of results.
proves itself to be adaptable to the needs of the
individual tasks.
Using the maximum-weight-matching algorithm
for aligning words from both texts that have similar
distributional semantics leads to very sound fea-
tures. Even without the resource-heavy features,
the system yields competitive results. In some use
cases, those expensive features are almost negligi-
ble. Without being dependent on the availability of
resources like a dependency parser or a WordNet-
like lexical database, SemantiKLUE can easily be
adapted to other languages.
Our experimental features from the commercial
topic clustering software are useful in some cases;
in others at least they do not hurt performance.
We feel that the heuristics based on the depen-
dency structure of the texts do not exhaust all the
possibilities that dependency parsing has to offer.
In the future we would like to try out more mea-
sures based on those structures. Probably some
kind of graph edit distance incorporating the sim-
ilarities between both dependency relations and
words might turn out to be a powerful feature.
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2012. SemEval-2012 task
6: A pilot on semantic textual similarity. In First
Joint Conference on Lexical and Computational Se-
mantics, pages 385?393. ACL.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), volume 1: Proceedings of the Main
Conference and the Shared Task, pages 32?43. ACL.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide
Web: A collection of very large linguistically pro-
cessed Web-crawled corpora. Language Resources
and Evaluation, 43(3):209?226.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Marie-Catherine de Marneffe and Christopher D. Man-
ning, 2008. Stanford typed dependencies manual.
Stanford University.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Zvi Galil. 1986. Efficient algorithms for finding
maximum matching in graphs. Computing Surveys,
18(1):23?38.
Paul Greiner and Stefan Evert. in preparation. The
Klugator Engine: A distributional approach to open
questions in market research.
Paul Greiner, Thomas Proisl, Stefan Evert, and Besim
Kabashi. 2013. KLUE-CORE: A regression model
of semantic textual similarity. In Second Joint Con-
ference on Lexical and Computational Semantics
(*SEM), volume 1: Proceedings of the Main Con-
ference and the Shared Task, pages 181?186. ACL.
Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart.
2008. Exploring network structure, dynamics, and
function using NetworkX. In G?el Varoquaux,
539
Travis Vaught, and Jarrod Millman, editors, Pro-
ceedings of the 7th Python in Science Conference
(SciPy2008), pages 11?15, Pasadena, CA.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013.
UMBC_EBIQUITY-CORE: Semantic textual
similarity systems. In Proceedings of the Second
Joint Conference on Lexical and Computational
Semantics. ACL.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalua-
tion (SemEval-2014).
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 265?283. MIT Press, Cambridge, MA.
Yuri Lin, Jean-Baptiste Michel, Erez Lieberman Aiden,
Jon Orwant, Will Brockman, and Slav Petrov. 2012.
Syntactic annotations for the Google Books Ngram
Corpus. In Proceedings of the ACL 2012 System
Demonstrations, pages 169?174, Jeju Island, Korea.
ACL.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, pages
296?304, San Francisco, CA. Morgan Kaufmann.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014).
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-
elli. 2014b. A SICK cure for the evaluation of com-
positional distributional semantic models. In Pro-
ceedings of LREC 2014, Reykjavik. ELRA.
Fabian Pedregosa, Ga?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ?douard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Roland Sch?fer and Felix Bildhauer. 2012. Building
large corpora from the web using a new efficient
tool chain. In Proceedings of the Eighth Interna-
tional Conference on Language Resources and Eval-
uation (LREC ?12), pages 486?493, Istanbul, Turkey.
ELRA.
Hinrich Sch?tze. 1998. Automatic word sense discrim-
ination. Computational Linguistics, 24(1):97?123.
540
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 551?555,
Dublin, Ireland, August 23-24, 2014.
SentiKLUE: Updating a Polarity Classifier in 48 Hours
Stefan Evert and Thomas Proisl and Paul Greiner and Besim Kabashi
Friedrich-Alexander-Universit?t Erlangen-N?rnberg
Department Germanistik und Komparatistik
Professur f?r Korpuslinguistik
Bismarckstr. 6, 91054 Erlangen, Germany
{stefan.evert,thomas.proisl,paul.greiner,besim.kabashi}@fau.de
Abstract
SentiKLUE is an update of the KLUE po-
larity classifier ? which achieved good and
robust results in SemEval-2013 with a sim-
ple feature set ? implemented in 48 hours.
1 Introduction
The SemEval-2014 shared task on ?Sentiment
Analysis in Twitter? (Rosenthal et al., 2014) is a re-
run of the corresponding shared task from SemEval-
2013 (Nakov et al., 2013) with new test data.
It focuses on polarity classification in computer-
mediated communication such as Twitter, other
micro-blogging services, and SMS. There are two
subtasks: the goal of Message Polarity Classifica-
tion (B) is to classify an entire SMS, tweet or other
message as positive (pos), negative (neg) or neutral
(ntr); in the subtask on Contextual Polarity Disam-
biguation (A), a single word or short phrase has to
be classified in the context of the whole message.
The training data are the same as in SemEval-
2013. The test data from 2013 are used as a devel-
opment set in order to select features and tune ma-
chine learning algorithms, but may not be included
in the training data. The 2014 test set comprises
the development data, new Twitter messages, Live-
Journal entries as out-of-domain data, and a small
number of tweets containing sarcasm (see Rosen-
thal et al. (2014) for further details). For subtask B,
there are 10,239 training items, 5,907 items in the
development set, and 3,861 additional unseen items
in the new test set. For subtask A, there are 9,505
training items, 6,769 items in the development set,
and 3,912 additional items in the test set.
Our team participated in the SemEval-2013
shared task with a relatively simple, but robust
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
system (KLUE) based on a maximum entropy clas-
sifier and a small set of features (Proisl et al., 2013).
Despite its simplicity, KLUE performed very well
in subtask B, ranking 5th out of 36 constrained
systems on the Twitter data and 3rd out of 28 on
the SMS data. Results for contextual polarity dis-
ambiguation (subtask A) were less encouraging,
with rank 14 out of 21 constrained systems on the
Twitter data and rank 12 out of 19 on the SMS data.
This paper describes our efforts to bring the
KLUE system up to date within a period of 48
hours. The results obtained by the new SentiKLUE
system are summarised in Table 1, showing that the
update was successful. The ranking of the system
has improved substantially in subtask A, making it
one of the best-performing systems in the shared
task. Rankings in subtask B are similar to those
of the previous year, showing that SentiKLUE has
kept up with recent developments. Moreover, dif-
ferences to the best-performing systems are much
smaller than in SemEval-2013.
2 Updating the KLUE polarity classifier
The KLUE polarity classifier is described in de-
tail by Proisl et al. (2013). It used the following
features as input for a maximum entropy classifier:
? The AFINN sentiment lexicon (Nielsen, 2011),
which provides numeric polarity scores ranging
from ?5 to +5 for 2,476 English word forms,
extended with distributionally similar words.
For each input message, the number of positive
and negative words as well as their average
polarity score were computed.
? Emoticons and Internet slang expressions that
were manually classified as positive, negative
or neutral. Features were generated in the same
way as for the sentiment lexicon.
? A bag-of-words representation that generates a
separate feature for each word form that occurs
in at least 5 different messages ( f ? 5). Only
551
task subset rank score best
B LJ14 3 / 42 73.99 74.84
B SMS13 4 / 42 67.40 70.28
B Twit13 6 / 42 69.06 72.12
B Twit14 10 / 42 67.02 70.96
B Sarcasm 24 / 42 43.36 58.16
A LJ14 1 / 20 85.61 85.61
A SMS13 6 / 20 85.16 89.31
A Twit13 2 / 20 90.11 90.14
A Twit14 2 / 20 84.83 86.63
A Sarcasm 2 / 20 79.32 82.75
Table 1: SentiKLUE results in SemEval 2014
Task 9 (among constrained systems). See Rosen-
thal et al. (2014) for further details and rankings
including the unconstrained systems.
single words (unigrams) were used, since ex-
periments with additional bigram features did
not lead to a clear improvement.
? A negation heuristic, which inverts the polar-
ity score of the first sentiment word within 4
tokens after a negation marker. In the bag-of-
words representation, the next 3 tokens after a
negation marker are prefixed with not_.
? For subtask A, these features were computed
both for the marked word or phrase and for the
rest of the message.
In order to improve the KLUE classifier, we drew
inspiration from two other systems participating
in the SemEval-2013 task: NRC-Canada (Moham-
mad et al., 2013), which won the task by a large
margin over competing systems, and GU-MLT-LT
(G?nther and Furrer, 2013), which used similar fea-
tures to our classifier, but obtained better results
due to careful selection and tuning of the machine
learning algorithm.
Mohammad et al. (2013) used a huge set of fea-
tures, including several sentiment lexica (both man-
ually and automatically created), word n-grams (up
to 4-grams with low frequency threshold), charac-
ter n-grams (3-grams to 5-grams), Twitter-derived
word clusters and a negation heuristic similar to
our approach. Features with the largest impact
in subtask B were sentiment lexica (esp. large au-
tomatically generated word lists), word n-grams,
character n-grams and the negation heuristic, in this
order. NRC-Canada achieved F-scores of 68.46
(SMS) and 69.02 (Twitter) in task B, as well as
88.00 (SMS) and 88.93 (Twitter) in task A.
G?nther and Furrer (2013) claim that state-of-
the-art results can be obtained with a small fea-
ture set if a suitable machine learning algorithm
is chosen. They used stochastic gradient descent
(SGD) and tuned its parameters by grid search. GU-
MLT-LT achieved scores of 62.15 (SMS) and 65.27
(Twitter) in task B, as well as 88.37 (SMS) and
85.19 (Twitter) in task A.
We therefore decided to make use of a wider
range of sentiment lexica, extend the bag-of-words
representation to bigrams, implement character n-
gram features, and experiment with different ma-
chine learning algorithms, resulting in the Senti-
KLUE system described in the following section.
3 The SentiKLUE system
SentiKLUE is an improved version of the KLUE
system and uses the same tokenisation, preprocess-
ing and negation heuristics; see Proisl et al. (2013)
for details. The features described below are used
as input for a machine learning classifier that pre-
dicts the polarity categories positive (pos), nega-
tive (neg) or neutral (ntr). As in KLUE and GU-
MLT-LT, the implementations of the Python library
scikit-learn (Pedregosa et al., 2011)
1
are used. We
tested four different learning algorithms: logistic
regression (MaxEnt), stochastic gradient descent
(SGD), linear SVM (LinSVM) and SVM with a
RBF kernel (SVM). Parameters were tuned by grid
search and the best-performing algorithm was cho-
sen for each subtask. SentiKLUE makes use of the
following features:
? Several sentiment lexica, which are treated as
lists of positive and negative polarity words.
Numerical scores are converted by setting ap-
propriate cutoff thresholds. For each lexicon,
we compute the number of positive and neg-
ative words occurring in a message as fea-
tures, with separate counts for negated and non-
negated contexts.
? AFINN (Nielsen, 2011)
2
? Bing Liu lexicon (Hu and Liu, 2004)
3
? MPQA (Wilson et al., 2005)
4
? SentiWords (Guerini et al., 2013)
5
; we cre-
1
http://scikit-learn.org/
2
http://www2.imm.dtu.dk/pubdb/views/publication_de-
tails.php?id=6010
3
http://www.cs.uic.edu/~liub/FBS/sentiment-
analysis.html
4
http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/
5
http://hlt.fbk.eu/technologies/sentiwords
552
ated two word lists with score thresholds of
0.3 and 0.1
? Sentiment140 (Mohammad et al., 2013)
6
,
which was compiled from a corpus of 1.6
million tweets for NRC-Canada; we created
separate lists for normal words and hashtags
with a score threshold of 1.0
? NRC Hashtag Sentiment Lexicon (Moham-
mad et al., 2013)
7
, which contains words
that exhibit a strong statistical association
(PMI score) to positive or negative hashtags,
also compiled for NRC-Canada; again, we
created separate lists for normal words and
hashtags with a score threshold of 0.8
? a manual extension including synonyms,
antonyms and several word lists from on-
line sources, compiled by the SNAP team
(Schulze Wettendorf et al., 2014)
? an automatic extension with distributionally
similar words (DSM extension), using a strat-
egy similar to Proisl et al. (2013)
? Word form unigrams and bigrams. After
some experimentation, the document frequency
threshold was set to f ? 5 for subtask B and
f ? 2 for subtask A.
? In order to include information from character
n-grams, we used a Perl implementation of n-
gram language models (Evert, 2008) that has
already been applied successfully to text cat-
egorization tasks (boilerplate detection in the
CLEANEVAL 2007 competition). We trained
three separate models on positive, negative and
neutral messages. We selected a 5-gram model
(n = 5) with strong smoothing (q = 0.7), which
minimized cross-entropy on the training data
(measured by cross-validation). For each mes-
sage in the training and test data, three features
were generated, specifying per-character cross-
entropy for each of the three n-gram models.
8
? Counts of positive and negative emoticons us-
ing the same lists as in the KLUE system.
? The same negation heuristic as in KLUE.
9
6
http://www.umiacs.umd.edu/~saif/WebPages/Abstracts/
NRC-SentimentAnalysis.htm
7
ibid.
8
Note that these features had to be generated by cross-
validation on the training data to avoid catastrophic overfitting.
9
The full list of negation markers is not, don?t, doesn?t,
won?t, can?t, mustn?t, isn?t, aren?t, wasn?t, weren?t, couldn?t,
shouldn?t, wouldn?t. To our surprise, including further nega-
tion markers such as none, ain?t or hasn?t led to a decrease in
classification quality.
For subtask A, we chose a simplistic strategy and
computed the same set of features for the marked
word or phrase instead of the entire message. In
order to take context into account, the three class
probabilities assigned to the complete message by
a MaxEnt classifier were included as additional
features. No other features describing the context
of the marked expression were used.
Optionally, features were standardized and prior
class weights (2? for positive, 4? for negative)
were used in order to balance the predicted labels.
The best-performing machine learning algorithms
on the development set were MaxEnt for subtask B
(L1 penalty, C = 0.3) and linear SVM for subtask A
(L1 penalty, L2 loss, C = 0.5), as shown in Table 2.
4 Experiments and conclusion
In order to determine the importance of individ-
ual features, ablation experiments were carried out
for both subtasks by deactivating one group of fea-
tures at a time. Tables 3 and 4 show the resulting
changes in the official criterion F
p/n
separately for
each subset of the development and test sets, as
well as micro-averaged across the full development
set (DEV) and test set (GOLD). Rows are ordered
by feature impact on the full gold standard. Posi-
tive values indicate that a feature group has a neg-
ative impact on classification quality: results are
improved by omitting the features (which is often
the case for the Sarcasm subset).
The most important features are bag-of-words
unigrams and bigrams, closely followed by senti-
ment lexica. Training class weights had a strong
positive impact in subtask B, but decreased per-
formance in subtask A. In our official submission,
they were only used for subtask B. Full-message
polarity is the third most important feature in sub-
task A. Other features contributed relatively small
individual effects, but were necessary to achieve
state-of-the-art performance in combination. They
are often specific to one of the subtasks or to a
particular subset of the gold standard.
The bottom half of each table shows ablation
results for individual sentiment lexica, with all
other features active. Key resources are the stan-
dard lexica (AFINN, Liu, MPQA) as well as
Twitter-specific lexica (Sentiment140, NRC Hash-
tag). Noisy word lists (DSM extension, SNAP,
SentiWords) have a small or even a negative effect.
Surprisingly, the standard lexica seem to give mis-
leading cues on the Twitter 2014 subset (Table 3).
553
CV development set test set (gold standard)
task classifier F
all
F
pos
F
neg
F
ntr
F
all
F
p/n
acc. F
pos
F
neg
F
ntr
F
all
F
p/n
acc.
B MaxEnt .727 .724 .651 .772 .735 .688 .734 .731 .650 .750 .726 .691 .725
B SGD .725 .728 .645 .773 .736 .686 .734 .733 .656 .749 .727 .695 .726
B LinSVM .702 .687 .604 .743 .700 .646 .701 .699 .599 .716 .689 .649 .690
B SVM .702 .721 .631 .742 .716 .676 .712 .729 .636 .720 .709 .683 .706
A MaxEnt .864 .890 .872 .179 .849 .881 .863 .893 .853 .171 .841 .873 .856
A SGD .864 .889 .867 .223 .849 .878 .860 .891 .847 .188 .839 .869 .852
A LinSVM .860 .892 .876 .064 .847 .884 .865 .895 .856 .064 .838 .875 .857
A SVM .855 .890 .873 .024 .842 .881 .862 .892 .853 .014 .832 .872 .854
Table 2: Performance of different machine learning algorithms on the training data (CV), development set
and test set (F
all
= weighted average F-score; F
p/n
= official score; best results highlighted in bold font).
Task B SMS Twitter DEV LJ14 SMS13 Twit13 Twit14 Sarcasm GOLD
? bag of words ?.0837 ?.0322 ?.0502 ?.0344 ?.0807 ?.0316 ?.0335 +.0511 ?.0430
? sentiment lexica ?.0445 ?.0354 ?.0389 ?.0690 ?.0422 ?.0372 ?.0092 +.0750 ?.0363
? training weights ?.0033 ?.0413 ?.0266 ?.0275 ?.0077 ?.0482 ?.0204 ?.0342 ?.0294
? emoticons ?.0071 ?.0107 ?.0087 ?.0006 ?.0067 ?.0105 +.0004 +.0492 ?.0048
? bow bigrams ?.0074 ?.0005 ?.0035 +.0010 ?.0105 ?.0012 ?.0096 +.0956 ?.0028
? feature scaling ?.0027 ?.0010 ?.0014 ?.0021 ?.0030 ?.0026 ?.0004 ?.0034 ?.0020
? character n-grams +.0029 ?.0068 ?.0033 +.0012 +.0040 ?.0044 ?.0056 +.0056 ?.0015
? negation ?.0098 +.0019 ?.0014 ?.0016 ?.0049 +.0002 ?.0012 +.0351 ?.0002
? bow f ? 2 +.0017 +.0026 +.0022 +.0004 +.0021 ?.0003 +.0021 +.0171 +.0013
sentiment lexica:
? standard lexica ?.0206 ?.0135 ?.0152 ?.0245 ?.0234 ?.0124 +.0035 +.0586 ?.0124
? Twitter lexica ?.0026 +.0000 ?.0019 ?.0118 ?.0073 ?.0007 ?.0094 +.0034 ?.0066
? SentiWords ?.0008 ?.0010 ?.0009 ?.0034 ?.0015 ?.0005 ?.0075 +.0165 ?.0017
? hashtag lexica ?.0011 +.0021 +.0005 ?.0045 ?.0039 +.0035 +.0011 ?.0302 ?.0005
? DSM extension +.0047 ?.0032 ?.0002 ?.0070 +.0039 +.0022 ?.0025 +.0392 +.0002
? manual extension ?.0008 ?.0018 ?.0011 ?.0015 ?.0019 +.0000 +.0041 +.0361 +.0009
only standard lexica ?.0124 ?.0119 ?.0120 ?.0088 ?.0101 ?.0108 ?.0095 +.0439 ?.0094
only DSM extension ?.0303 ?.0260 ?.0262 ?.0427 ?.0287 ?.0251 +.0021 +.0183 ?.0230
Table 3: Results of feature ablation experiments for subtask B. Values show change in F
p/n
-score if feature
is excluded. Rows are sorted by impact of features on the full SemEval-2014 test data (GOLD).
Task A SMS Twitter DEV LJ14 SMS13 Twit13 Twit14 Sarcasm GOLD
? bag of words ?.0283 ?.0252 ?.0256 ?.0207 ?.0292 ?.0249 ?.0411 ?.0041 ?.0273
? sentiment lexica ?.0027 ?.0231 ?.0151 ?.0078 ?.0023 ?.0245 ?.0144 ?.0109 ?.0141
? context (class probs) +.0027 ?.0050 ?.0022 ?.0105 +.0017 ?.0057 ?.0171 +.0390 ?.0062
? negation ?.0081 ?.0041 ?.0052 ?.0064 ?.0063 ?.0024 ?.0058 +.0000 ?.0043
? bow bigrams ?.0045 ?.0009 ?.0022 ?.0014 ?.0046 +.0007 ?.0033 +.0208 ?.0014
? character n-grams ?.0015 +.0003 ?.0004 +.0003 ?.0038 +.0001 ?.0012 +.0085 ?.0012
? feature scaling +.0001 +.0001 +.0001 +.0009 +.0005 ?.0002 ?.0029 ?.0041 ?.0004
? emoticons +.0023 +.0026 +.0025 +.0016 +.0038 +.0012 ?.0062 +.0000 +.0004
bow f ? 5 +.0027 +.0000 +.0009 +.0082 +.0027 +.0006 ?.0025 +.0243 +.0015
? training weights +.0046 +.0072 +.0059 +.0104 +.0037 +.0050 +.0000 ?.0145 +.0040
sentiment lexica:
? standard lexica ?.0100 ?.0024 ?.0050 +.0014 ?.0086 ?.0035 ?.0055 +.0000 ?.0044
? Twitter lexica ?.0039 ?.0016 ?.0024 ?.0009 ?.0038 ?.0024 ?.0052 ?.0085 ?.0031
? hashtag lexica ?.0023 ?.0007 ?.0012 +.0000 ?.0014 ?.0019 ?.0030 ?.0126 ?.0017
? manual extensions ?.0016 +.0003 ?.0004 +.0021 ?.0025 ?.0009 +.0002 +.0000 ?.0007
? SentiWords +.0017 +.0005 +.0010 +.0001 +.0013 ?.0013 +.0001 +.0000 ?.0002
? DSM extensions +.0099 +.0011 +.0044 ?.0008 +.0098 ?.0006 ?.0004 ?.0085 +.0019
only standard lexica +.0030 ?.0038 ?.0011 ?.0019 +.0035 ?.0048 ?.0027 ?.0168 ?.0019
only DSM lexica ?.0114 ?.0085 ?.0094 ?.0035 ?.0117 ?.0104 ?.0057 ?.0338 ?.0089
Table 4: Results of feature ablation experiments for subtask A. Values show change in F
p/n
-score if feature
is excluded. Rows are sorted by impact of features on the full SemEval-2014 test data (GOLD).
554
References
Stefan Evert. 2008. A lightweight and efficient tool for
cleaning Web pages. In Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC 2008), Marrakech, Morocco.
Marco Guerini, Lorenzo Gatti, and Marco Turchi.
2013. Sentiment analysis: How to derive prior po-
larities from SentiWordNet. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2013), pages 1259?
1269, Seattle, WA, October.
Tobias G?nther and Lenz Furrer. 2013. GU-MLT-LT:
Sentiment analysis of short messages using linguis-
tic features and stochastic gradient descent. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the
Seventh International Workshop on Semantic Evalu-
ation (SemEval 2013), pages 328?332, Atlanta, GA.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD ?04), pages
168?177, Seattle, WA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation (Se-
mEval 2013), pages 321?327, Atlanta, GA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis
in Twitter. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval-2013).
Finn ?rup Nielsen. 2011. A new ANEW: Evaluation
of a word list for sentiment analysis in microblogs.
In Proceedings of the ESWC2011 Workshop on Mak-
ing Sense of Microposts: Big things come in small
packages, number 718 in CEUR Workshop Proceed-
ings, pages 93?98, Heraklion, Greece, May.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. 2011. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research,
12:2825?2830.
Thomas Proisl, Paul Greiner, Stefan Evert, and Besim
Kabashi. 2013. KLUE: Simple and robust meth-
ods for polarity classification. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 395?401, Atlanta, Geor-
gia, USA, June. Association for Computational Lin-
guistics.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment analysis in Twitter. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Clemens Schulze Wettendorf, Robin Jegan, Allan
K?rner, Julia Zerche, Nataliia Plotnikova, Julian
Moreth, Tamara Schertl, Verena Obermeyer, Su-
sanne Streil, Tamara Willacker, and Stefan Evert.
2014. SNAP: A multi-stage XML pipeline for as-
pect based sentiment analysis. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 347?354,
Vancouver, BC, Canada.
555
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 578?584,
Dublin, Ireland, August 23-24, 2014.
SNAP: A Multi-Stage XML-Pipeline for Aspect Based Sentiment Analysis
Clemens Schulze Wettendorf and Robin Jegan and Allan K
?
orner and Julia Zerche
and Nataliia Plotnikova and Julian Moreth and Tamara Schertl and Verena Obermeyer
and Susanne Streil and Tamara Willacker and Stefan Evert
Friedrich-Alexander-Universit?at Erlangen-N?urnberg
Department Germanistik und Komparatistik
Professur f?ur Korpuslinguistik
Bismarckstr. 6, 91054 Erlangen, Germany
{clemens.schulze.wettendorf, robin.jegan, allan.koerner, julia.zerche,
nataliia.plotnikova, julian.moreth, tamara.schertl, verena.obermeyer,
susanne.streil, tamara.willacker, stefan.evert}@fau.de
Abstract
This paper describes the SNAP system,
which participated in Task 4 of SemEval-
2014: Aspect Based Sentiment Analysis.
We use an XML-based pipeline that com-
bines several independent components to
perform each subtask. Key resources
used by the system are Bing Liu?s senti-
ment lexicon, Stanford CoreNLP, RFTag-
ger, several machine learning algorithms
and WordNet. SNAP achieved satisfactory
results in the evaluation, placing in the top
half of the field for most subtasks.
1 Introduction
This paper describes the approach of the SemaN-
tic Analyis Project (SNAP) to Task 4 of SemEval-
2014: Aspect Based Sentiment Analysis (Pontiki
et al., 2014). SNAP is a team of undergraduate
students at the Corpus Linguistics Group, FAU
Erlangen-N?urnberg, who carried out this work as
part of a seminar in computational linguistics.
Task 4 was divided into the four subtasks As-
pect term extraction (1), Aspect term polarity (2),
Aspect category detection (3) and Aspect category
polarity (4), which were evaluated in two phases
(A: subtasks 1/3; B: subtasks 2/4). Subtasks 1 and
3 were carried out on two different datasets, one
of laptop reviews and one of restaurant reviews.
Subtasks 2 and 4 only made use of the latter.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
Task Dataset Rank Score Best
1 Lap 10 of 21 0.624 0.746
1 Res 20 of 21 0.465 0.840
3 Res 6 of 15 0.782 0.886
2 Lap 7 of 23 0.641 0.705
2 Res 12 of 24 0.708 0.810
4 Res 11 of 18 0.696 0.829
Table 1: Ranking among constrained systems.
The developed system consists of one module
per subtask, in addition to a general infrastruc-
ture and preprocessing module, All modules ac-
cept training and test data in the XML format spec-
fied by the task organizers. The modules can be
combined into a pipeline, where each step adds
new annotation corresponding to one of the four
subtasks.
Table 1 shows our ranking among all con-
strained systems (counting only the best run from
each team), the score achieved by SNAP (accu-
racy or F-score, depending on subtask), and the
score achieved by the best system in the respec-
tive subtask. Because of a preprocessing mistake
that was only discovered after phase A of the eval-
uation had ended, results for subtasks 1 and 3 are
significantly lower than the results achieved dur-
ing development of the system.
2 Sentiment lexicon
Early on in the project it was decided that a com-
prehensive, high-quality sentiment lexicon would
play a crucial role in building a successful sys-
tem. After a review of several existing lexica, Bing
578
Liu?s sentiment word list (Hu and Liu, 2004) was
taken as a foundation and expanded with extensive
manual additions.
The first step was an exhaustive manual web-
search to find additional candidates for the lexicon.
The candidates were converted to a common for-
mat, and redundant entries were discareded. The
next step consisted of further expansion with the
help of online thesauri, from which large num-
ber of synonyms and antonyms for existing entries
were obtained. Since the coverage of the lexicon
was still found to be insufficient, it was further
complemented with entries from two other exist-
ing sentiment lexica, AFINN (Nielsen, 2011) and
MPQA (Wilson et al., 2005).
Finally the augmented lexicon was compared
with the original word lists from AFINN, MPQA
and Bing Liu in order to measure the reliabilty of
the entries. The reliability score of each entry is
the number of sources in which it is found.
3 Infrastructure and preprocessing
Within the scope of Task 4 ? but not one of the
official subtasks ? the goal of the infrastructure
module was (i) to support the other modules with
a set of project-specific tools and (ii) to provide
a common API to the training and test data aug-
mented with several layers of linguistic annota-
tion. In order to roll out the required data as
quick as possible, the Stanford CoreNLP suite
1
was used as an off-the-shelf tool. The XML files
provided by the task organizers were parsed with
the xml.etree.ElementTree API, which is part of
the standard library of Python 2.7.
Since the module for subtask 1 was pursuing an
IOB-tagging approach for aspect term identifica-
tion, the part-of-speech tags provided by CoreNLP
had to be extended. During the process of merging
the original XML files with the CoreNLP annota-
tions, IOB tags were generated indicating whether
each token is part of an aspect term or not. See
Section 4 for further information.
For determining the polarity of an aspect term,
the subtask 2 module made use of syntactic depen-
dencies between words (see Section 5 for details).
For this purpose, the dependency trees produced
by CoreNLP were converted into a more accessi-
ble format with the help of the Python software
package NetworkX.
2
.
1
http://nlp.stanford.edu/software/corenlp.shtml
2
http://networkx.github.io/
4 Aspect term extraction
The approach chosen by the aspect term extrac-
tion module (subtask 1) was to treat aspect term
extraction as a tagging task. We used a standard
IOB tagset indicating whether each token is at the
beginning of an aspect term (ATB), inside an as-
pect term (ATI), or not part of an aspect term at
all (ATX).
First experiments were carried out with uni-
gram, bigram and trigram taggers implemented in
NLTK (Bird et al., 2009), which were trained on
IOB tags derived from the annotations in the Task
4 gold standard (comprising both trial and training
data). We also tested higher-order n-gram taggers
and the NLTK implementation of the Brill tagger
(Brill, 1992).
For a more sophisticated approach we used RF-
Tagger (Schmid and Laws, 2008), which extends
the standard HMM tagging model with complex
hidden states that consist of features correspond-
ing to different pieces of information. RFTagger
was developed for morphological tagging, where
complex tags such as N.Reg.Nom.Sg.Neut
are decomposed into the main syntactic category
(N) and additional morpho-syntactic features rep-
resenting case (Nom), number (Sg), etc.
In our case, the tagger was used for joint an-
notation of part-of-speech tags and IOB tags for
the aspect term boundaries, based on the ratio-
nale that the additional information encoded in
the hidden states (compared to a simple IOB tag-
ger) would allow RFTagger to learn more mean-
ingful aspect term patterns. We decided to en-
code the IOB tags as the main category and the
part-of-speech tags as additional features, since
changing these categories, meaning POS tags as
the main category and IOB tags as additional fea-
tures, had resulted in lesser performance. The
training data were thus converted into word-
annotation pairs such as screen_AT/ATB.NN
or beautiful/ATX.JJ. Note that known as-
pect terms from the gold standard (as well as ad-
ditional candidates that were generated through
comparisons of known aspect terms with lists from
WordNet) were extended with the suffix _AT in a
preprocessing step. Our intention was to enable
the tagger to learn directly that tokens with this
suffix are likely to be aspect terms.
Table 2 shows tagging accuracy for different al-
gorithms, computed by ten-fold cross-validation
over a gold standard comprising the training and
579
Tagger Rest. Laptops
Unigram 83.41% 83.91%
Bigram (backoff: U) 85.37% 85.74%
Trigram (backoff: U+Bi) 85.41% 86.33%
Brill (backoff: U+Bi+T) 85.48% 86.47%
RFTagger 95.20% 96.47%
Table 2: Accuracy of different aspect term taggers.
trial data sets. The table shows that the bigram, tri-
gram and Brill taggers achieve only marginal im-
provements over a simplistic unigram tagger, even
when they are combined through back-off linking.
The RFTagger achieved by far the best accuracy
on both data sets.
4.1 Results and debugging
Our final results for the full aspect term extraction
procedure are shown in Table 3.
Score Rest. Laptops
Precision 57.14% 64.54%
Recall 39.15% 60.40%
F
1
-Score 46.47% 62.40%
Table 3: Aspect term extraction results.
The huge difference between tagging accuracy
achieved in the development phase and the aspect
term extraction quality obtained on the SemEval-
2014 test set is caused by different factors. First,
Table 2 shows the tagging accuracy across all to-
kens, not limited to aspect terms. A tagger that
works particularly well for many irrelevant tokens
(punctuation, verbs, etc.), correctly marking them
ATX, may achieve high accuracy even if it has low
recall on tokens belonging to aspect terms. Sec-
ond, the official scores only consider an aspect
term candidate to be a true positive if it covers
exactly the same tokens as the gold standard an-
notation. If the tagger disagrees with the human
annotators on whether an adjective or determiner
should be considered part of an aspect term, this
will be counted as a mistake despite the overlap.
Thus, even a relatively small number of tagging
mistakes near aspect term boundaries will be pun-
ished severly in the evaluation. Unseen words as
well as long or unusual noun phrases turned out to
be particularly difficult.
Table 3 indicates a serious problem with the
restaurant data, which has surprisingly low recall,
resulting in an F
1
-score almost 16 percent points
lower than for the laptop data. A careful exami-
nation of the trial, training and test data revealed
an early mistake in the preprocessing code as the
main culprit. Once this mistake was corrected, the
recall score for restaurants was similar to the score
for laptops.
5 Aspect term polarity
Subtask 2 is concerned with opinion sentences,
i.e. sentences that contain one or more aspect
terms and express subjective opinions about (some
of) these aspects. Such opinions are expressed
through opinion words; common opinion words
with their corresponding confidence values (nu-
meric values from 1 to 6 expressing the level of
certainty that a word is positive or negative, cf.
Sec. 2) are collected in sentiment lexica.
The preprocessing stage in this subtask starts
with a sentence segmentation step that uses the
output of the Stanford CoreNLP parser.
3
All de-
pendencies map onto a directed graph represen-
tation where words of each sentence are nodes
in the graph and grammatical relations are edge
labels. All aspect terms (Sec. 2) are marked in
each dependency graph. When processing such a
graph we extract all positive and negative opinion
words occurring in each sentence by comparing
them with word lists contained in our sentiment
lexica. A corresponding confidence value from
lexica is assigned for each opinion word, the num-
ber of positive and negative aspect terms occurring
in each sentence are counted and their confidence
values are summed up. These values serve as fea-
tures for supervised machine learning using algo-
rithms implemented in scikit-learn (Pedregosa et
al., 2011).
All opinion words that build a dependency with
an aspect term are stored for each sentence. A
dominant word of each dependency is stored as a
governor, whereas a subordinate one is stored as a
dependent. Both direct and indirect dependencies
are processed. If there are several indirect depen-
dencies to an aspect term, they are processed re-
cursively. Using lists of extracted dependencies
between opinion words and aspect terms hand-
writen rules assign corresponding confidence val-
ues to aspect terms.
3
nlp.stanford.edu/software/dependencies manual.pdf
580
5.1 Features based on a sentiment lexica
The extended sentiment dictionaries were used to
extract five features: I) tokens expressing a pos-
itive sentiment belonging to one aspect term, II)
tokens expressing a negative sentiment, III) con-
fidence values of positive tokens, IV) confidence
values of negative tokens, V) a sum of all confi-
dence values for all positive and all negative opin-
ion words occurring in a sentence.
5.2 Features based on hand-written rules
We made use of direct and indirect negation mark-
ers, so that all opinion words belonging to a
negated aspect term swap their polarity signs. We
added rules for negative particles not and no that
directly precede an opinion word, for adverbs
barely, scarcely and too, for such constructions
as could have been and wish in the subjunctive
mood. After swapping polarity signs of opinion
words, a general set of hand-written rules was ap-
plied to the graph dependencies. The rules fol-
low the order of importance of dependencies scal-
ing from least important up to most important.
We placed the dependencies in the following or-
der: acomp, advmod, nsubjpass, conj and, amod,
prep of, prep worth, prep on, prep in, nsubj, inf-
mod, dobj, xcomp, rcmod, conj or, appos. All de-
pendencies can be grouped into three categories
based on the direction of the polarity assignment.
The first group (acomp, advmod, amod, rcmod,
prep in) includes dependencies where a governor
of a dependency takes over polarity of a dependent
if the latter is defined. The second group (infmod,
conj or, prep on, prep worth, prep of, conj and)
covers dependencies in which a dependent ele-
ment takes over polarity of a governor if the lat-
ter is defined. The third group (dobj, xcomp) is
for cases when both governor and dependent are
defined. Here a governor takes over polarity of a
dependent.
5.3 Experiments
In this section we compare two approches to as-
pect term polarity detection. The first approach
simply counts all positive and negative words in
each sentence and then assigns a label based on
which of the two counts is larger. It does not make
use of machine learning techniques and its accu-
racy is only about 54%. Results improve signifi-
cantly with supervised machine learning based on
the feature sets described above. We experimented
with different classifiers (Maximum Entropy, Lin-
ear SVM and SVMs with RBF kernel) and var-
ious subsets of features. By default, we worked
on the level of single opinion words that express
a positive or negative polarity (sg). We added
the following features in different combinations:
an extended list of opinion words (ex) obtained
from a distribution semantic model, based on near-
est neighbours of known opinion words (Proisl et
al., 2013); potential misspellings of know opin-
ion words, within a maximal Levenshtein distance
of 1 (lv); word combinations and fixed phrases
(ml) containing up to 3 words (e.g., good man-
nered, put forth, tried and true, up and down);
and the sums of positive and negative opinion
words in the whole sentence (st). The best results
for the laptops data were achieved with a Max-
imum Entropy classifier, excluding misspellings
(lv) and word combinations (ml); the correspond-
ing line in Table 4 is highlighted in bold font.
Even though MaxEnt achieved the best results dur-
ing development, we decided to use SVM with a
RBF kernel for the test set, assuming that it would
be able to exploit interdependencies between fea-
tures. The accuracy achieved by the submitted
system is highlighted in italics in the table. The
training test data provided for restaurants and lap-
tops categories were split equally into two sets
where the first set (first half) was used for training
a model and the second set was used for the test
and evaluation stages. Experiments on the restau-
rants data produced similar results.
classifier sg ex lv ml st Acc
MaxEnt + + ? ? ? 0.5589
MaxEnt + + + ? ? 0.4905
MaxEnt + + ? + ? 0.5479
MaxEnt + + ? ? + 0.6506
MaxEnt + + ? + + 0.5742
SVM
rbf
+ + ? ? ? 0.5581
SVM
rbf
+ + + ? ? 0.4905
SVM
rbf
+ + ? + ? 0.5479
SVM
rbf
+ + ? ? + 0.6402
SVM
rbf
+ + ? + + 0.5717
Table 4: Results for laptops category on train set.
6 Aspect category detection
Subtask 3 deals with determining which aspect
categories out of a predefined set occur in a given
sentence. The developed module consists of two
581
independent parts ? one based on machine learn-
ing, the other on similarities between WordNet
synsets (?synonym sets?, roughly corresponding
to concepts). While both approaches achieved
similar performance during development, combin-
ing them resulted in overall better scores. How-
ever, the success of this method crucially depends
on accurate indentification of aspect terms.
6.1 A WordNet-based approach
4
The WordNet-based component operates on previ-
ously identified aspect terms (from the gold stan-
dard in the evaluation, but from the module de-
scribed in Sec. 4 in a real application setting).
For each term, it finds all synsets and compares
them to a list of ?key synsets? that characterize
the different aspect categories (e.g. the category
food is characterized by the key synset meal.n.01,
among others). The best match is chosen and
added to an internal lexicon, which maps each
unique phrase appearing as an aspect term to ex-
actly one aspect category. As a similarity measure
for synsets we used path similarity, which deter-
mines the length of the shortest path between two
synsets in the WordNet hypernym/hyponym tax-
onomy. Key synsets were extracted from a list of
high frequency terms and tested manually to cre-
ate an accurate represenation for each category.
In the combined approach this component was
taken as a foundation and was augmented by high-
confidence suggestions from the machine learning
component (see below).
Additional extensions include a high-
confidence lexicon based on nearest neighbours
from a distributional semantic model, a rudi-
mentary lexicon of international dishes, and
the application of a spellchecker; together, they
accounted only for a small increase in F-score on
the development data (from 0.758 to 0.768).
6.2 A machine learning approach
5
The machine learning component is essentially a
basic bag-of-words model. It employs a multino-
mial Naive Bayes classifier in a one-vs-all setup
to achieve multi-label classification. In addition
to tuning of the smoothing parameters, a probabil-
ity threshold was introduced that every predicted
category has to pass to be assigned to a sentence.
4
We used the version of WordNet included in NLTK 2.0.4
(Bird et al., 2009), accessed through the NLTK API.
5
We used machine learning algorithms implemented in
scikit-learn 0.14.1 (Pedregosa et al., 2011).
Test Train AT Mode F
1
SE14 Dev Sub1 All 0.782
SE14 Dev* Sub1 WN 0.666
SE14 Dev Sub1* ML 0.788
SE14 Dev Gold All 0.848
SE14 Dev* Gold WN 0.829
SE14 Dev Gold* ML 0.788
Dev (cv) Dev Gold All 0.800
Dev (cv) Dev* Gold WN 0.768
Dev (cv) Dev Gold* ML 0.769
*indicates data sets not used by a given component
Table 5: Aspect category detection results.
Different thresholds were used for the stand-alone
component (th = 0.625) and the combined ap-
proach (th = 0.9). In the latter case all predici-
tions of the WordNet-based component were ac-
cepted, but only high-confidence predictions from
the Naive Bayes classifier were added.
6.3 Results
Table 5 summarizes the results of different exper-
iments with aspect category detection. In all cases
the training data consisted of the combined official
train and trial sets (Dev). The last three rows show
results obtained by ten-fold cross-validation in the
development phase, the other rows show the cor-
responding results on the official test set (SE14).
The first three rows are based on automatically de-
tected aspect terms from the module described in
Sec. 4 (Sub 1), the other rows used gold standard
aspect terms. Separate results are provided for the
combined approach (Mode: All) as well as for the
two individual components (WN = WordNet, ML
= machine learning). Note that the WN compo-
nent does not require any training data, while the
ML component does not make use of aspect terms
marked in the input.
With gold standard aspect terms, the WordNet-
based approach is equal to or better than the Naive
Bayes classifier, and best results are achived by a
combination of the two components. However, the
poor accuracy of the automatic aspect term extrac-
tion (cf. Table 3) has a disastrous effect: even the
combined approach used in the official submis-
sion performs less well than the ML component
alone. Nevertheless the experiment with gold stan-
dard aspect terms suggests that the matching from
aspect term to category works quite well, with
a small additional improvement from the Naive
582
Bayes bag-of-words model.
7 Aspect category polarity
The general approach was to allocate each aspect
term to the corresponding aspect categories. A
simple rule set was then used to determine the po-
larity of each aspect category based on the polari-
ties of the aligned aspect terms. In cases where no
aspect terms are marked (but sentences are still la-
belled with aspect categories), the idea was to fall
back on the sentiment values for the entire sen-
tences provided by the CoreNLP suite.
6
7.1 Term / category alignment
To establish a basis for creating the mapping rules,
the first step was to work out the distribution of
aspect terms and aspect categories in the train-
ing data. The most common case is that an as-
pect category aligns with a single aspect term
(1476?); there are also many aspect categories
with multiple aspect terms (1179?) and some as-
pect categories without any aspect terms. Since
the WordNet-Approach from Sec. 6 showed rela-
tively good results (especially if gold standard as-
pect terms are available, which is the case here),
a modified version was used to assign each aspect
term to one of the annotated categories.
7.2 Polarity allocation
After the assignment of aspect terms to their ac-
cording aspect category ? if needed ? the aspect
category polarity can be determined. For this, the
polarity values of all aspect terms assigned to this
category were collected, and duplicates were re-
moved in order to produce a unique set (e.g. 1,
1, -1, 0, 0 would be reduced to 1, -1, 0). A set
with both negative and positive polarity values in-
dicates a conflict for the corresponding aspect cat-
egory, while a neutral polarity value would be ig-
nored, if positive or negative polarity values occur.
Our method achieved an accuracy of 89.16% for
sentences annotated with just a single aspect cat-
egory. In cases where only one aspect term had
been assigned to a aspect category the accuracy
was unsuprisingly high (96.61%), whereas the ac-
curacy decreased in cases of multiple assigned as-
pect terms (78.44%). For aspect categories with-
out aligned aspect terms, as well as the category
anecdotes/miscellaneous, the sentiment values of
the CoreNLP sentiment analysis tool had to be
6
http://nlp.stanford.edu/software/corenlp.shtml
used, which led to a poor accuracy in those cases,
namely 52.74%.
7.3 Results
On the official test set, the module for subtask 4
achieved an accuracy of 69.56%. An important
factor is the very low accuracy in cases where the
CoreNLP sentiment value for the entire sentence
had to be used. We expect a considerable improve-
ment from using a modified version of the subtask
2 module (Sec. 5) to compute overall sentence po-
larity.
8 Conclusion
We have shown a modular system working as a
pipeline that modifies the input sentences step by
step by adding new information as XML tags.
Aspect term extraction was handled as a tagging
task that utilized an IOB tagset to find aspect
terms with the final version relying on Schmid?s
RFTagger. Determination of aspect term polar-
ity was achieved through a machine learning ap-
proach that uses SVMs with RBF kernel. This
was supported by an augmented sentiment lexicon
based on several different sources, which was ex-
panded manually by a team of students. Aspect
category detection in turn employs a combination
approach of an algorithm depending on WordNet
synsets and a bag-of-words Naive Bayes classifier.
Finally aspect category polarity was calculated by
combining the results from the last two modules.
Overall results were satisfactory, being mostly
in the top half of submitted systems. During phase
A of testing (subtasks 1 and 3), a preprocessing er-
ror caused a massive drop in performance in aspect
term extraction. This carried over to the other sub-
task, because the module uses aspect terms among
other features to identify aspect categories. Scores
for phase B (subtasks 2 and 4) were very close to
test results during development with the exception
of cases where the CoreNLP sentiment value for
an entire sentence had to be used.
References
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Eric Brill. 1992. A simple rule-based part of speech
tagger. In Proceedings of the Third Conference on
Applied Natural Language Processing, pages 152?
155, Trento, Italy.
583
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD ?04), pages
168?177, Seattle, WA.
Finn
?
Arup Nielsen. 2011. A new ANEW: Evaluation
of a word list for sentiment analysis in microblogs.
In Proceedings of the ESWC2011Workshop onMak-
ing Sense of Microposts: Big things come in small
packages, number 718 in CEUR Workshop Proceed-
ings, pages 93?98, Heraklion, Greece, May.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014), Dublin, Ireland.
Thomas Proisl, Paul Greiner, Stefan Evert, and Besim
Kabashi. 2013. KLUE: Simple and robust meth-
ods for polarity classification. In Second Joint
Conference on Lexical and Computational Seman-
tics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 395?401, Atlanta, Geor-
gia, USA, June. Association for Computational Lin-
guistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained POS tagging. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, volume 1, pages 777?
784.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 347?354,
Vancouver, BC, Canada.
584
Proceedings of the NAACL HLT 2010 Sixth Web as Corpus Workshop, pages 32?40,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Google Web 1T 5-Grams Made Easy (but not for the computer)
Stefan Evert
Institute of Cognitive Science
University of Osnabru?ck
49069 Osnabru?ck, Germany
stefan.evert@uos.de
Abstract
This paper introduces Web1T5-Easy, a sim-
ple indexing solution that allows interactive
searches of the Web 1T 5-gram database and
a derived database of quasi-collocations. The
latter is validated against co-occurrence data
from the BNC and ukWaC on the automatic
identification of non-compositional VPC.
1 Introduction
The Google Web 1T 5-gram (Web1T5) database
(Brants and Franz, 2006) consists of frequency
counts for bigram, trigrams, 4-grams and 5-grams
extracted from 1 trillion words of English Web text,
i.e. from a corpus 10,000 times the size of the British
National Corpus (Aston and Burnard, 1998). While
primarily designed as a resource to build better lan-
guage models for machine translation and other NLP
applications, its public release in 2006 was greeted
with great enthusiasm by many researchers in com-
putational linguistics. As one example, Mitchell et
al. (2008) used the Web1T5 data successfully to pre-
dict fMRI neural activation associated with concrete
noun concepts.
For linguistic applications, though, the Web1T5
database presents three major obstacles:
(i) The lack of linguistic annotation: Google?s to-
kenisation splits hyphenated compounds (e.g., part-
time is split into a three-token sequence part|-|time)
and differs in many other ways from the rules used
in liguistic corpora. The n-grams are neither anno-
tated with part-of-speech tags nor lemmatised, and
there are separate entries for sentence-initial upper-
case and the corresponding lowercase forms.
(ii) The application of frequency thresholds: De-
spite the enormous size of the database, its com-
pilers found it necessary to omit low-frequency n-
grams with fewer than 40 occurrences. This means
that non-adjacent word combinations are listed only
if the occur in a relatively frequent pattern. As a
consequence, it is impossible to obtain reliable fre-
quency estimates for latent phenomena by pooling
data (e.g. the co-occurrence frequency of a particu-
lar verb with nouns denoting animals).
(iii) The difficulty of interactive search: The com-
plete Web1T5 database consists of 24.4 GiB of
binary-sorted, compressed text files. While this for-
mat is suitable for building n-gram language models
and other offline processing, searching the database
is not efficient enough for interactive use. Except for
simple, case-sensitive prefix searches ? which can
be restricted to a single file containing 50?90 MiB
of compressed text ? every query requires a linear
scan of the full database.
This paper presents a simple open-source soft-
ware solution to the third problem, called Web1T5-
Easy. The n-gram data are encoded and indexed
in a relational database. Building on convenient
open-source tools such as SQLite and Perl, the
software aims to strike a good balance between
search efficiency and ease of use and implemen-
tation. With its focus on interactive, but accu-
rate search it complements the approximate index-
ing and batch processing approaches of Hawker et
al. (2007). Web1T5-Easy can be downloaded from
http://webascorpus.sf.net/Web1T5-Easy/.1
1An online demo of the complete Web1T5 database is avail-
able at http://cogsci.uos.de/~korpora/ws/Web1T5/.
32
word 1 word 2 word 3 f
supplement depend on 193
supplement depending on 174
supplement depends entirely 94
supplement depends on 338
supplement derived from 2668
supplement des coups 77
supplement described in 200
Table 1: Example of Web1T5 3-gram frequency data (ex-
cerpt from file 3gm-0088.gz).
The rest of this paper is organised as follows. Sec-
tion 2 describes the general system architecture in
more detail. Section 3 explains how collocations
(with a maximal span size of four tokens) and dis-
tributional semantic models (DSM) can be approxi-
mated on the basis of Web1T5 frequency data. Some
technical aspects are summarised in Section 4. Sec-
tion 5 addresses the consequences of problems (i)
and (ii). The linguistic usefulness of Web1T5 col-
location data is validated on a multiword extraction
task from the MWE 2008 workshop.2 Section 6 con-
cludes with a brief outlook on the future develop-
ment of Web1T5-Easy.
2 System architecture
While designing the fastest possible indexing archi-
tecture for the Web1T5 database is an interesting
computer science problem, linguistic applications
typically do not require the millisecond response
times of a commercial search engine. It is sufficient
for interactive queries to be completed within a few
seconds, and many users will also be willing to wait
several minutes for the result of a complex search
operation. Given the tabular format of the Web1T5
n-gram frequency data (cf. Table 1), it was a natural
choice to make use of a standard relational database
(RDBMS). Database tables can be indexed on sin-
gle or multiple columns for fast access, and the SQL
query languge allows flexible analysis and aggrega-
tion of frequency data (see Section 2.2 for some ex-
amples). While the indexing procedure can be very
time-consuming, it is carried out offline and has to
run only once.
2http://multiword.sf.net/mwe2008/
Web1T5-Easy was designed to balance com-
putational efficiency against implementation effort
and ease of use. Its main ingredients are the
public-domain embedded relational database engine
SQLite and the open-source scripting language Perl
which are connected through the portable DBI/DBD
interface.3 The Web1T5-Easy package consists of
two sets of Perl scripts. The first set automates
pre-processing and indexing, detailed in Section 2.1.
The second set, which facilitates command-line ac-
cess to the database and provides a Web-based GUI,
is described in Section 2.2. Technical details of the
representation format and performance figures are
presented in Section 4.
The embedded database engine SQLite was pre-
ferred over a full-fledged RDBMS such as MySQL
or PostgreSQL for several reasons: (i) running the
database as a user-level process gives better con-
trol over huge database files and expensive indexing
operations, which might otherwise clog up a ded-
icated MySQL server computer; (ii) each SQLite
database is stored in a single, platform-independent
file, so it can easily be copied to other locations or
servers; (iii) an embedded database avoids the over-
head of exchanging large amounts of data between
client and server; (iv) tight integration with the ap-
plication program allows more flexible use of the
database than pure SQL queries (e.g., a Perl script
can define its own SQL functions, cf. Section 3).
It is quite possible that the sophisticated query op-
timisers of MySQL and commercial RDMBS im-
plementations would improve performance on com-
plex SQL queries. Since Web1T5-Easy uses the
generic DBI interface, it can easily be adapted to any
RDMBS back-end for which DBI/DBD drivers are
available.
2.1 The indexing procedure
Indexing of the Web1T5 n-gram data is carried out
in four stages:
1. In an optional pre-processing step, words are
filtered and normalised to lowercase.4 Each
3See the Web pages at http://www.sqlite.org/, http:
//www.perl.org/ and http://dbi.perl.org/.
4The default filter replaces numbers by the code NUM, var-
ious punctuation symbols by the code PUN, and all ?messy?
strings by the code UNK. It can easily be replaced by a user-
defined normalisation mapping.
33
word in an n-gram entry is then coded as a nu-
meric ID, which reduces database size and im-
proves both indexing and query performance
(see Section 4 for details on the representation
format). The resulting tuples of n + 1 integers
(n word IDs plus frequency count) are inserted
into a database table.
2. If normalisation was applied, the table will con-
tain multiple entries for many n-grams.5 In
Stage 2, their frequency counts are aggregated
with a suitable SQL query. This is one of the
most expensive and disk-intensive operations
of the entire indexing procedure.
3. A separate SQL index is created for each n-
gram position (e.g., word 1, word 2 and word 3
in Table 1). Multi-column indexes are currently
omitted, as they would drastically increase the
size of the database files.6 Moreover, the use of
an index only improves query execution speed
if it is highly selective, as explained in Sec-
tion 4. If desired, the Perl scripts can trivially
be extended to create additional indexes.
4. A statistical analysis of the database is per-
formed to improve query optimisation (i.e., ap-
propriate selection of indexes).
The indexing procedure is carried out separately for
bigrams, trigrams, 4-grams and 5-grams, using a
shared lexicon table to look up numeric IDs. Users
who do not need the larger n-grams can easily skip
them, resulting in a considerably smaller database
and much faster indexing.
2.2 Database queries and the Web GUI
After the SQLite database has been populated and
indexed, it can be searched with standard SQL
queries (typically a join between one of the n-gram
tables and the lexicon table), e.g. using the sqlite3
5For example, with the default normalisation, bought 2 bot-
tles, bought 5 bottles, Bought 3 bottles, BOUGHT 2 BOT-
TLES and many other trigrams are mapped to the representation
bought NUM bottles. The database table thus contains multiple
entries of the trigram bought NUM bottles, whose frequency
counts have to be added up.
6For the 5-gram table, 10 different two-column indexes
would be required to cover a wide range of queries, more than
doubling the size of the database file.
command-line utility. Since this requires detailed
knowledge of SQL syntax as well as the database
layout and normalisation rules, the Web1T5-Easy
package offers a simpler, user-friendly query lan-
guage, which is internally translated into appropriate
SQL code.
A Web1T5-Easy query consists of 2?5 search
terms separated by blanks. Each search term is ei-
ther a literal word (e.g. sit), a set of words in square
brackets (e.g. [sit,sits,sat,sitting]), a prefix
(under%) or suffix (%ation) expression, * for an ar-
bitrary word, or ? to skip a word. The difference be-
tween the latter two is that positions marked by * are
included in the query result, while those marked by ?
are not. If a query term cannot match because of nor-
malisation, an informative error message is shown.
Matches can be ranked by frequency or by associa-
tion scores, according to one of the measures recom-
mended by Evert (2008): t-score (t), log-likelihood
(G2), chi-squared with Yates? correction (X2), point-
wise MI, or a version of the Dice coefficient.
For example, the query web as corpus shows
that the trigram Web as Corpus occurs 1,104
times in the Google corpus (case-insensitive).
%ly good fun lists ways of having fun such
as really good fun (12,223?), jolly good fun
(3,730?) and extremely good fun (2,788?). The
query [sit,sits,sat,sitting] * ? chair re-
turns the patterns SIT in . . . chair (201,084?), SIT
on . . . chair (61,901?), SIT at . . . chair (1,173?),
etc. Corpus frequencies are automatically summed
over all fillers in the third slot.
The query implementation is available as a
command-line version and as a CGI script that
provides a Web-based GUI to the Web1T5-Easy
database. The CGI version also offers CSV and
XML output formats for use as a Web service.
3 Quasi-collocations and DSM
Many corpus linguists and lexicographers will par-
ticularly be interested in using the Web1T5 database
as a source of collocations (in the sense of Sinclair
(1991)). While the British National Corpus at best
provides sufficient data for a collocational analysis
of some 50,000 words (taking f ? 50 to be the min-
imum corpus frequency necessary), Web1T5 offers
comprehensive collocation data for almost 500,000
34
Figure 1: Quasi-collocations for the node word corpus in the Web GUI of Web1T5-Easy.
words (which have at least 50 different collocates in
the database, and f ? 10,000 in the original Google
corpus).
Unfortunately, the Web1T5 distribution does not
include co-occurrence frequencies of word pairs,
except for data on immediately adjacent bigrams.
It is possible, though, to derive approximate co-
occurrence frequencies within a collocational span
of up to 4 tokens. In this approach, each n-gram ta-
ble yields information about a specific collocate po-
sition relative to the node. For instance, one can
use the 4-gram table to identify collocates of the
node word corpus at position +3 (i.e., 3 tokens to
the right of the node) with the Web1T5-Easy query
corpus ? ? *, and collocates at position ?3 (i.e.,
3 tokens to the left of the node) with the query
* ? ? corpus. Co-occurrence frequencies within
a collocational span, e.g. (?3,+3), are obtained by
summation over all collocate positions in this win-
dow, collecting data from multiple n-gram tables.
It has to be kept in mind that such quasi-
collocations do not represent the true co-occurrence
frequencies, since an instance of co-occurrence of
two words is counted only if it forms part of an n-
gram with f ? 40 that has been included in Web1T5.
Especially for larger distances of 3 or 4 tokens, this
limitation is likely to discard most of the evidence
for co-occurrence and put a focus on collocations
that form part of a rigid multiword unit or insti-
tutionalised phrase. Thus, cars becomes the most
salient collocate of collectibles merely because the
two words appear in the slogan from collectibles to
cars (9,443,572?). Section 5 validates the linguistic
usefulness of Web1T5 quasi-collocations in a multi-
word extraction task.
Web1T5-Easy compiles frequency data for quasi-
collocations in an additional step after the complete
n-gram data have been indexed. For each pair of co-
occurring words, the number of co-occurrences in
each collocational position (?4,?3, . . . ,+3,+4) is
recorded. If the user has chosen to skip the largest
n-gram tables, only a shorter collocational span will
be available.
The Web GUI generates SQL code to determine
co-occurrence frequencies within a user-defined col-
locational span on the fly, by summation over the
appropriate columns of the quasi-collocations table.
Collocates can be ranked by a range of association
measures (t, G2, X2, MI, Dice, or frequency f ),
which are implemented as user-defined SQL func-
tions in the Perl code. In this way, sophisticated
statistical analyses can be performed even if they
are not directly supported by the RDBMS back-end.
Figure 1 shows an example of quasi-collocations in
the Web GUI, ranked according to the t-score mea-
sure. On the right-hand side of the table, the distri-
bution across collocate positions is visualised.
In computational linguistics, collocations play
an important role as the term-term co-occurrence
matrix underlying distributional semantic models
35
size (GiB) database file no. of rows
0.23 vocabulary 5,787,556
7.24 2-grams 153,634,491
32.81 3-grams 594,453,302
64.32 4-grams 933,385,623
75.09 5-grams 909,734,581
31.73 collocations 494,138,116
211.42 total 3,091,133,669
Table 2: Size of the fully indexed Web1T5 database, in-
cluding quasi-collocations.
(DSM), with association scores used as feature
weights (see e.g. Curran (2004, Sec. 4.3)). The
Web1T5-Easy quasi-collocations table provides a
sparse representation of such a term-term matrix,
where only 494?106 or 0.0015% of the 5.8?106 ?
5.8?106 = 33.5?1012 cells of a full co-occurrence
matrix are populated with nonzero entries.
4 Technical aspects
An essential feature of Web1T5-Easy is the numeric
coding of words in the n-gram tables, which allows
for compact storage and more efficient indexing of
the data than a full character string representation. A
separate lexicon table lists every (normalised) word
form together with its corpus frequency and an in-
teger ID. The lexicon is sorted by decreasing fre-
quency: since SQLite encodes integers in a variable-
length format, it is advantageous to assign low ID
numbers to the most frequent terms.
Every table is stored in its own SQLite database
file, e.g. vocabulary for the lexicon table and
collocations for quasi-collocations (cf. Sec-
tion 3). The database files for different n-gram sizes
(2-grams, 3-grams, 4-grams, 5-grams) share
the same layout and differ only in the number of
columns. Table 2 lists the disk size and number of
rows of each database file, with default normalisa-
tion applied. While the total size of 211 GiB by far
exceeds the original Web1T5 distribution, it can eas-
ily be handled by modern commodity hardware and
is efficient enough for interactive queries.
Performance measurements were made on a
midrange 64-bit Linux server with 2.6 GHz AMD
Opteron CPUs (4 cores) and 16 GiB RAM. SQLite
database files and temporary data were stored on a
fast, locally mounted hard disk. Similar or better
hardware will be available at most academic institu-
tions, and even in recent personal desktop PCs.
Indexing the n-gram tables in SQLite took about
two weeks. Since the server was also used for mul-
tiple other memory- and disk-intensive tasks during
this period, the timings reported here should only
be understood as rough indications. The indexing
process might be considerably faster on a dedicated
server. Roughly equal amounts of time were spent
on each of the four stages listed in Section 2.1.
Database analysis in Stage 4 turned out to be of
limited value because the SQLite query optimiser
was not able to make good use of this information.
Therefore, a heuristic optimiser based on individual
term frequencies was added to the Perl query scripts.
This optimiser chooses the n-gram slot that is most
likely to speed up the query, and explicitly disables
the use of indexes for all other slots. Unless another
constraint is much more selective, preference is al-
ways given to the first slot, which represents a clus-
tered index (i.e. database rows are stored in index
order) and can be scanned very efficiently.
With these explicit optimisations, Stage 4 of the
indexing process can be omitted. If normalisation is
not required, Stage 2 can also be skipped, reducing
the total indexing time by half.
At first sight, it seems to be easy to compile the
database of quasi-collocations one node at a time,
based on the fully indexed n-gram tables. However,
the overhead of random disk access during index
lookups made this approach intractable.7 A brute-
force Perl script that performs multiple linear scans
of the complete n-gram tables, holding as much data
in RAM as possible, completed the compilation of
co-occurrence frequencies in about three days.
Table 3 shows execution times for a selection of
Web1T5-Easy queries entered in the Web GUI. In
general, prefix queries that start with a reasonably
specific term (such as time of *) are very fast,
even on a cold cache. The query %ly good fun
is a pathological case: none of the terms is selec-
tive enough to make good use of the corresponding
7In particular, queries like * ? ? corpus that scan for col-
locates to the left of the node word are extremely inefficient,
since the index on the last n-gram slot is not clustered and ac-
cesses matching database rows in random order.
36
Web1T5-Easy query cold cache warm cache
corpus linguistics 0.11s 0.01s
web as corpus 1.29s 0.44s
time of * 2.71s 1.09s
%ly good fun 181.03s 24.37s
[sit,sits,sat,sitting] * ? chair 1.16s 0.31s
* linguistics (association ranking) 11.42s 0.05s
university of * (association ranking) 1.48s 0.48s
collocations of linguistics 0.21s 0.13s
collocations of web 6.19s 3.89s
Table 3: Performance of interactive queries in the Web GUI of Web1T5-Easy. Separate timings are given for a cold
disk cache (first query) and warm disk cache (repeated query). Re-running a query with modified display or ranking
settings will only take the time listed in the last column.
index, and entries matching the wildcard expression
%ly in the first slot are scattered across the entire
trigram table.
5 Validation on a MWE extraction task
In order to validate the linguistic usefulness
of Web1T5 quasi-collocations, they were evalu-
ated on the English VPC shared task from the
MWE 2008 workshop.8 This data set consists of
3,078 verb-particle constructions (VPC), which have
been manually annotated as compositional or non-
compositional (Baldwin, 2008). The task is to iden-
tify non-compositional VPC as true positives (TP)
and re-rank the data set accordingly. Evaluation is
carried out in terms of precision-recall graphs, using
average precision (AP, corresponding to the area un-
der a precision-recall curve) as a global measure of
accuracy.
Frequency data from the Web1T5 quasi-
collocations table was used to calculate association
scores and rankings. Since previous studies suggest
that no single association measure works equally
well for all tasks and data sets, several popular mea-
sures were included in the evaluation: t-score (t),
chi-squared with Yates? continuity correction (X2),
Dice coefficient (Dice), co-occurrence frequency
( f ), log-likelihood (G2) and Mutual Information
(MI); see e.g. Evert (2008) for full equations and
references. The results are compared against rank-
ings obtained from more traditional, linguistically
8http://multiword.sf.net/mwe2008/
annotated corpora of British English: the balanced,
100-million-word British National Corpus (Aston
and Burnard, 1998) and the 2-billion-word Web
corpus ukWaC (Baroni et al, 2009).
For BNC and ukWaC, three different extraction
methods were used: (i) adjacent bigrams of verb +
particle/preposition; (ii) shallow syntactic patterns
based on POS tags (allowing pronouns and simple
noun phrases between verb and particle); and (iii)
surface co-occurrence within a collocational span of
3 tokens to the right of the node (+1,+3), filtered
by POS tag. Association scores were calculated us-
ing the same measures as for the Web1T5 quasi-
collocations. Preliminary experiments with different
collocational spans showed consistently lower accu-
racy than for (+1,+3). In each case, the same asso-
ciation measures were applied as for Web1T5.
Evaluation results are shown in Figure 3 (graphs)
and Table 4 (AP). The latter also describes the cover-
age of the corpus data by listing the number of can-
didates for which no frequency information is avail-
able (second column). These candidates are always
ranked at the end of the list. While the BNC has
a coverage of 92%?94% (depending on extraction
method), scaling up to Web1T5 completely elimi-
nates the missing data problem.
However, identification of non-compositional
VPC with the Web1T5 quasi-collocations is consid-
erably less accurate than with linguistically anno-
tated data from the much smaller BNC. For recall
values above 50%, the precision of statistical associ-
ation measures such as t and X2 is particularly poor
37
coverage average precision (%)
(missing) t X2 Dice f G2 MI
BNC (bigrams) 242 30.04 29.75 27.12 26.55 29.86 22.79
BNC (syntactic patterns) 201 30.42 30.49 27.48 25.87 30.64 22.48
BNC (span +1 . . .+3) 185 29.15 32.12 30.13 24.33 31.06 22.58
ukWaC (bigrams) 171 29.28 30.32 27.79 25.37 29.63 25.13
ukWaC (syntactic patterns) 162 29.20 31.19 27.90 24.19 30.06 25.08
ukWaC (span +1 . . .+3) 157 27.82 32.66 30.54 23.03 30.01 25.76
Web1T5 (span +1 . . .+3) 3 25.83 25.27 25.33 20.88 25.77 20.81
BNC untagged (+1 . . .+3) 39 27.22 27.85 28.98 22.51 28.13 19.60
Table 4: Evaluation results for English non-compositional VPC (Baldwin, 2008): average precision (AP) as a global
indicator. The baseline AP for random candidate ranking is 14.29%. The best result in each row is highlighted in bold.
(Figure 3.h). On the annotated corpora, where nodes
and collocates are filtered by POS tags, best results
are obtained with the least constrained extraction
method and the chi-squared (X2) measure. Scal-
ing up to the 2-billion-word ukWaC corpus gives
slightly better coverage and precision than on the
BNC. Moreover, X2 is now almost uniformly better
than (or equal to) any other measure (Figure 3.f).
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l l
l
ll
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
?4 ?2 0 2 4 6
?5
0
5
ukWaC (rescaled X?)
W
eb
1T
5 
(r
es
ca
le
d 
X
?)
l TP
FP
Figure 2: Comparison of X2 association scores on ukWaC
and Web1T5. Axes are rescaled logarithmically, preserv-
ing sign to indicate positive vs. negative association.
In order to determine whether the poor perfor-
mance of Web1T5 is simply due to the lack of lin-
guistic annotation or whether it points to an intrin-
sic problem of the n-gram database, co-occurrence
data were extracted from an untagged version of the
BNC using the same method as for the Web1T5 data.
While there is a significant decrease in precision (cf.
Figure 3.g and the last row of Table 4), the results
are still considerably better than on Web1T5. In the
MWE 2008 competition, Ramisch et al (2008) were
also unable to improve on the BNC results using a
phrase entropy measure based on search engine data.
The direct comparison of X2 association scores on
ukWaC and Web1T5 in Figure 2 reveals that the lat-
ter are divided into strongly positive and strongly
negative association, while scores on ukWaC are
spread evenly across the entire range. It is re-
markable that many true positives (TP) exhibit neg-
ative association in Web1T5, while all but a few
show the expected positive association in ukWaC.
This unusual pattern, which may well explain the
poor VPC evaluation results, can also be observed
for adjacent bigrams extracted from the 2-grams ta-
ble (not shown). It suggests a general problem of
the Web1T5 data that is compounded by the quasi-
collocations approach.
6 Future work
A new release of Web1T5-Easy is currently in
preparation. It will refactor the Perl code into
reusable and customisable modules that can easily
be embedded in user scripts and adapted to other
databases such as Brants and Franz (2009). We are
looking forward to Web1T5 v2, which promises eas-
ier indexing and much richer interactive queries.
38
(a)
0 20 40 60 80 100
10
20
30
40
50
BNC (bigrams)
Recall (%)
P
re
ci
si
on
 (%
)
t
X2
Dice
f
0 20 40 60 80 100
10
20
30
40
50
ukWaC (bigrams)
Recall (%)
P
re
ci
si
on
 (%
)
t
X2
Dice
f
(b)
(c)
0 20 40 60 80 100
10
20
30
40
50
BNC (syntactic pattern)
Recall (%)
P
re
ci
si
on
 (%
)
t
X2
Dice
f
0 20 40 60 80 100
10
20
30
40
50
ukWaC (syntactic pattern)
Recall (%)
P
re
ci
si
on
 (%
)
t
X2
Dice
f
(d)
(e)
0 20 40 60 80 100
10
20
30
40
50
BNC (span +1,+3)
Recall (%)
P
re
ci
si
on
 (%
)
t
X2
Dice
f
0 20 40 60 80 100
10
20
30
40
50
ukWaC (span +1,+3)
Recall (%)
P
re
ci
si
on
 (%
)
t
X2
Dice
f
(f)
(g)
0 20 40 60 80 100
10
20
30
40
50
BNC untagged (span +1,+3)
Recall (%)
P
re
ci
si
on
 (%
)
t
X2
Dice
f
0 20 40 60 80 100
10
20
30
40
50
Web1T5 quasi?collocations (span +1,+3)
Recall (%)
P
re
ci
si
on
 (%
)
t
X2
Dice
f
(h)
Figure 3: Evaluation results for English non-compositional VPC (Baldwin, 2008): precision-recall graphs. Rankings
according to the Web1T5 quasi-collocations are shown in the bottom right panel (h). The baseline precision is 14.29%.
39
References
Guy Aston and Lou Burnard. 1998. The BNC Hand-
book. Edinburgh University Press, Edinburgh. See
also the BNC homepage at http://www.natcorp.
ox.ac.uk/.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proceedings of the LREC Workshop To-
wards a Shared Task for Multiword Expressions (MWE
2008), pages 1?2, Marrakech, Morocco.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: A
collection of very large linguistically processed Web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1. Linguistic Data Consortium, Philadel-
phia, PA. http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T13.
Thorsten Brants and Alex Franz. 2009. Web
1T 5-gram, 10 European Languages Version
1. Linguistic Data Consortium, Philadelphia,
PA. http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2009T25.
James Richard Curran. 2004. From Distributional to Se-
mantic Similarity. Ph.D. thesis, University of Edin-
burgh.
Stefan Evert. 2008. Corpora and collocations. In Anke
Lu?deling and Merja Kyto?, editors, Corpus Linguistics.
An International Handbook, chapter 58. Mouton de
Gruyter, Berlin.
Tobias Hawker, Mary Gardiner, and Andrew Bennetts.
2007. Practical queries of a massive n-gram database.
In Proceedings of the Australasian Language Technol-
ogy Workshop 2007, pages 40?48, Melbourne, Aus-
tralia.
Tom M. Mitchell, Svetlana V. Shinkareva, Andrew Carl-
son, Kai-Min Chang, Vicente L. Malave, Robert A.
Mason, and Marcel Adam Just. 2008. Predicting
human brain activity associated with the meanings of
nouns. Science, 320:1191?1195.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for
the extraction of multiword expressions. In Proceed-
ings of the LREC Workshop Towards a Shared Task
for Multiword Expressions (MWE 2008), pages 50?53,
Marrakech, Morocco.
John Sinclair. 1991. Corpus, Concordance, Collocation.
Oxford University Press, Oxford.
40
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 66?74,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Evaluating Neighbor Rank and Distance Measures
as Predictors of Semantic Priming
Gabriella Lapesa
Universita?t Osnabru?ck
Institut fu?r Kognitionswissenschaft
Albrechtstr. 28, 49069 Osnabru?ck
glapesa@uos.de
Stefan Evert
FAU Erlangen-Nu?rnberg
Professur fu?r Korpuslinguistik
Bismarckstr. 6, 91054 Erlangen
severt@fau.de
Abstract
This paper summarizes the results of a
large-scale evaluation study of bag-of-
words distributional models on behavioral
data from three semantic priming experi-
ments. The tasks at issue are (i) identifi-
cation of consistent primes based on their
semantic relatedness to the target and (ii)
correlation of semantic relatedness with
latency times. We also provide an evalu-
ation of the impact of specific model pa-
rameters on the prediction of priming. To
the best of our knowledge, this is the first
systematic evaluation of a wide range of
DSM parameters in all possible combina-
tions. An important result of the study
is that neighbor rank performs better than
distance measures in predicting semantic
priming.
1 Introduction
Language production and understanding make ex-
tensive and immediate use of world knowledge
information that concerns prototypical events.
Plenty of experimental evidence has been gathered
to support this claim (see McRae and Matzuki,
2009, for an overview). Specifically, a number of
priming studies have been conducted to demon-
strate that event knowledge is responsible for fa-
cilitation of processing of words that denote events
and their participants (Ferretti et al, 2001; McRae
et al, 2005; Hare et al, 2009). The aim of our re-
search is to investigate to which extent such event
knowledge surfaces in linguistic distribution and
can thus be captured by Distributional Semantic
Models (henceforth, DSMs). In particular, we test
the capabilities of bag-of-words DSMs in simu-
lating priming data from the three aforementioned
studies.
DSMs have already proven successful in sim-
ulating priming effects (Pado? and Lapata, 2007;
Herdag?delen et al, 2009; McDonald and Brew,
2004). Therefore, in this work, we aim at a more
specific contribution to the study of distributional
modeling of priming: to identify the indexes of
distributional relatedness that produce the best
performance in simulating priming data and to as-
sess the impact of specific model parameters on
such performance. In addition to distance in the
semantic space, traditionally used as an index of
distributional relatedness in DSMs, we also intro-
duce neighbor rank as a predictor of priming ef-
fects. Distance and a number of rank-based mea-
sures are compared with respect to their perfor-
mance in two tasks: the identification of congruent
primes on the basis of distributional relatedness
to the targets (we measure accuracy in picking up
the congruent prime) and the prediction of latency
times (we measure correlation between distribu-
tional relatedness and reaction times). The results
of our experiments show that neighbor rank is a
better predictor than distance for priming data.
Our approach to DSM evaluation constitutes
a methodological contribution of this study: we
use linear models with performance (accuracy or
correlation) as a dependent variable and various
model parameters as independent variables, in-
stead of looking for optimal parameter combina-
tions. This approach is robust to overfitting and
allows to analyze the influence of individual pa-
rameters as well as their interactions.
The paper is structured as follows. Section
2 provides an overview of the modeled datasets.
Section 3 introduces model parameters and in-
dexes of distributional relatedness evaluated in this
paper, describes the experimental tasks and out-
lines our statistical approach to DSM evaluation.
Section 4 presents results for the accuracy and cor-
relation tasks and evaluates the impact of model
parameters on performance. We conclude in sec-
tion 5 by sketching ongoing work and future de-
velopments of our research.
66
Dataset Relation N Primec Primei Target Fac
V-N
AGENT 28 Pay Govern Customer 27*
PATIENT 18 Invite Arrest Guest 32*
PATIENT FEATURE 20 Comfort Hire Upset 33*
INSTRUMENT 26 Cut Dust Rag 32*
LOCATION 24 Confess Dance Court - 5
N-V
AGENT 30 Reporter Carpenter Interview 18*
PATIENT 30 Bottle Ball Recycle 22*
INSTRUMENT 32 Chainsaw Detergent Cut 16*
LOCATION 24 Beach Pub Tan 18*
N-N
EVENT-PEOPLE 18 Trial War Judge 32*
EVENT-THING 26 War Gun Banquet 33*
LOCATION-LIVING 24 Church Gym Athlete 37*
LOCATION-THING 30 Pool Garage Car 29*
PEOPLE-INSTRUMENT 24 Hiker Barber Compass 45*
INSTRUMENT-PEOPLE 24 Razor Compass Barber -10
INSTRUMENT-THING 24 Hair Scissors Oven 58*
Table 1: Overview of datasets: thematic relations, number of triples, example stimuli, facilitation effects
2 Data
This section introduces the priming datasets which
are the object of the present study. All the experi-
ments we aim to model were conducted to provide
evidence for the immediate effect of event knowl-
edge in language processing.
The first dataset comes from Ferretti et al
(2001), who found that verbs facilitate the process-
ing of nouns denoting prototypical participants in
the depicted event and of adjectives denoting fea-
tures of prototypical participants. In what follows,
the dataset from this study will be referred to as
V-N dataset.
The second dataset comes from McRae et al
(2005). In this experiment, nouns were found to
facilitate the processing of verbs denoting events
in which they are prototypical participants. In this
paper, this dataset is referred to as N-V dataset.
The third dataset comes from Hare et al (2009),
who found a facilitation effect from nouns to
nouns denoting events or their participants. We
will refer to this dataset as N-N dataset.
Experimental items and behavioral data from
these three experiments have been pooled together
in a global dataset that contains 404 word triples
(Target, Congruent Prime, Incongruent Prime).
For every triple, the dataset contains mean reac-
tion times for the congruent and incongruent con-
ditions, and a label for the thematic relation in-
volved. Table 1 provides a summary of the exper-
imental data. It specifies the number of triples for
every relation in the datasets (N) and gives an ex-
ample triple (Primecongruent , Primeincongruent , Tar-
get). Facilitation effects and stars marking signif-
icance by participants and items reported in the
original studies are also specified for every rela-
tion (Fac). Relations for which the experiments
showed no priming effect are highlighted in bold.
3 Method
3.1 Models
Building on the Distributional Hypothesis (Har-
ris, 1954), DSMs are employed to produce seman-
tic representations of words from patterns of co-
occurrence in texts or documents (Sahlgren, 2006;
Turney and Pantel, 2010). Semantic representa-
tions in the form of distributional vectors are com-
pared to quantify the amount of shared contexts as
an empirical correlate of semantic similarity. For
the purposes of this study, similarity is understood
in terms of topical relatedness (words connected
to a particular situation) rather than attributional
similarity (synonyms and near-synonyms).
DSMs evaluated in this study belong to the class
of bag-of-words models: the distributional vector
of a target word consists of co-occurrence counts
with other words, resulting in a word-word co-
occurrence matrix. The models cover a large vo-
cabulary of target words (27668 words in the un-
tagged version; 31713 words in the part-of-speech
tagged version). It contains the stimuli from the
datasets described in section 2 and further target
words from state-of-the-art evaluation studies (Ba-
roni and Lenci, 2010; Baroni and Lenci, 2011;
Mitchell and Lapata, 2008). Contexts are fil-
tered by part-of-speech (nouns, verbs, adjectives,
and adverbs) and by frequency thresholds. Nei-
ther syntax nor word order were taken into ac-
count when gathering co-occurrence information.
Distributional models were built using the UCS
67
toolkit1 and the wordspace package for R2. The
evaluated parameters are:
? Corpus: British National Corpus3; ukWaC4;
WaCkypedia EN5; WP5006; and a concate-
nation of BNC, ukWaC, and WaCkype-
dia EN (called the joint corpus);
? Window size: 2, 5, or 15 words to the left
and to the right of the target;
? Part of speech: no part of speech tags; part
of speech tags for targets; part of speech tags
for targets and contexts;
? Scoring measure: frequency; Dice coeffi-
cient; simple log-likelihood; Mutual Infor-
mation; t-score; z-score;7
? Vector transformation: no transformation;
square root, sigmoid or logarithmic transfor-
mation;
? Dimensionality reduction: no dimension-
ality reduction; Singular Value Decompo-
sition to 300 dimensions using randomized
SVD (Halko et al, 2009); Random Indexing
(Sahlgren, 2005) to 1000 dimensions;
? Distance measure: cosine, euclidean or
manhattan distance.
3.2 Indexes of Distributional Relatedess
3.2.1 Distance and Rank
The indexes of distributional relatedness described
in this section represent alternative perspectives
on the semantic representation inferred by DSMs
from co-occurrence data.
Given a target, a prime, and a matrix of dis-
tances produced by a distributional model, we test
the following indexes of relatedness between tar-
get and prime:
? Distance: distance between the vectors of
target and prime in the semantic space;
? Backward association: rank of prime
among the neighbors of target, as in Hare et
al. (2009);8
? Forward association: rank of target in the
neighbors of prime;
1http://www.collocations.de/software.html
2http://r-forge.r-project.org/projects/wordspace/
3http://www.natcorp.ox.ac.uk/
4http://wacky.sslmit.unibo.it/doku.php?id=corpora
5http://wacky.sslmit.unibo.it/doku.php?id=corpora
6A subset of WaCkypedia EN containing the initial 500
words of each article, which amounts to 230 million tokens.
7See Evert (2004) for a description of these measures and
details on the calculation of association scores.
8This type of association is labeled as ?backward? be-
cause it goes from targets to primes, while in the experimental
setting targets are shown after primes.
? Average rank: average of backward and for-
ward association.
Indexes of distributional relatedness were consid-
ered as an additional parameter in the evaluation,
labeled relatedness index below. Every combi-
nation of the parameters described in section 3.1
with each value of the relatedness index param-
eter defines a DSM. The total number of models
evaluated in our study amounts to 38880.
3.2.2 Motivation for Rank
This section provides some motivation for the use
of neighbor rank as a predictor of priming effects
in DSMs, on the basis of general cognitive princi-
ples and of previous modeling experiments.
In distributional semantic modeling, similar-
ity between words is calculated according to Eu-
clidean geometry: the more similar two words are,
the closer they are in the semantic space. One of
the axioms of spatial models is symmetry (Tver-
sky, 1977): the distance between point a and point
b is equal to the distance between point b and point
a. Cognitive processes, however, often violate the
symmetry axiom. For example, asymmetric asso-
ciations are often found in word association norms
(Griffiths et al, 2007).
Our study also contains a case of asymmetry.
In particular, the results from Hare et al (2009),
which constitute our N-N dataset, show priming
from PEOPLE to INSTRUMENTs, but not from IN-
STRUMENTs to PEOPLE. This asymmetry can-
not be captured by distance measures for reasons
stated above. However, the use of rank-based in-
dexes allows to overcome the limitation of sym-
metrical distance measures by introducing direc-
tionality (in our case, target? prime vs. prime?
target), and this without discarding the established
and proven measures.
Rank has already proven successful in model-
ing priming effects with DSMs. Hare et al (2009)
conducted a simulation on the N-N dataset using
LSA (Landauer and Dumais, 1997) and BEAGLE
(Jones and Mewhort, 2007) trained on the TASA
corpus. Asymmetric priming was correctly pre-
dicted by the context-only version of BEAGLE us-
ing rank (namely, rank of prime among neighbors
of target, cf. backward rank in section 3.2.1).
Our study extends the approach of Hare et al
(2009) in a number of directions. First, we in-
troduce and evaluate several different rank-based
measures (section 3.2.1). Second, we evaluate
rank in connection with specific parameters and on
68
larger corpora. Third, we extend the use of rank-
based measures to the distributional simulation of
two other experiments on event knowledge (Fer-
retti et al, 2001; McRae et al, 2005). Note that
our simulation differs from the one by Hare et al
(2009) with respect to tasks (they test for a sig-
nificant difference of mean distances between tar-
get and related vs. unrelated prime) and the class
of DSMs (we use term-term models, rather than
LSA; our models are not sensitive to word order,
unlike BEAGLE).
3.3 Tasks and Analysis of Results
The aim of this section is to introduce the exper-
imental tasks whose results will be discussed in
section 4 and to describe the main features of the
analysis we applied to interpret these results.
Two experiments have been carried out:
? A classification experiment: given a target
and two primes, distributional information is
used to identify the congruent prime. Perfor-
mance in this task is measured by classifica-
tion accuracy (section 4.1).
? A prediction experiment: the informa-
tion concerning distributional relatedness be-
tween targets and congruent primes is tested
as a predictor for latency times. Performance
in this task is quantified by Pearson correla-
tion (section 4.2).
Concerning the interpretation of the evaluation re-
sults, it would hardly be meaningful to look at the
best parameter combination or the average across
all models. The best model is likely to be over-
fitted tremendously (after testing 38880 param-
eter settings over a dataset of 404 data points).
Mean performance is largely determined by the
proportions of ?good? and ?bad? parameter set-
tings among the evaluation runs, which include
many non-optimal parameter values that were only
included for completeness.
Instead, we analyze the influence of individ-
ual DSM parameters and their interactions using
linear models with performance (accuracy or cor-
relation) as a dependent variable and the various
model parameters as independent variables. This
approach allows us to identify parameters that
have a significant effect on model performance
and to test for interactions between the parameters.
Based on the partial effects of each parameter (and
significant interactions) we can select a best model
in a robust way.
This statistical analysis contains some elements
of novelty with respect to the state-of-the-art DSM
evaluation. Broadly speaking, approaches to DSM
evaluation described in the literature fall into two
classes. The first one can be labeled as best model
first, as it implies the identification of the opti-
mal configuration of parameters on an initial task,
considered more basic; the best performing model
on the general task is therefore evaluated on other
tasks of interest. This is the approach adopted, for
example, by Pado? and Lapata (2007). In the sec-
ond approach, described in Bullinaria and Levy
(2007; 2012), evaluation is conducted via incre-
mental tuning of parameters: parameters are eval-
uated sequentially to identify the best performing
value on a number of tasks. Such approaches to
DSM evaluation have specific limitations. The
former approach does not assess which parame-
ters are crucial in determining model performance,
since its goal is the evaluation of performance of
the same model on different tasks. The latter ap-
proach does not allow for parameter interactions,
considering parameters individually. Both limita-
tions are avoided in the analysis used here.
4 Results
4.1 Identification of Congruent Prime
This section presents the results from the first task
evaluated in our study. We used the DSMs to iden-
tify which of the two primes is the congruent one
based on their distributional relatedness to the tar-
get. For every triple in the dataset, the different in-
dexes of distributional relatedness (parameter re-
latedness index) were used to compare the associ-
ation between the target and the congruent prime
with the association between the target and the in-
congruent prime. Accuracy of DSMs in picking up
the congruent prime was calculated on the global
dataset and separately for each subset.9
Figure 1 displays the distribution of the accu-
racy scores of all tested models in the task, on the
global dataset. All accuracy values are specified
as percentages. Minimum, maximum, mean and
standard deviation of the accuracy values for the
global dataset and for the three subsets are dis-
played in table 2.
The mean performance on N-N is lower than on
9The small number of triples for which no prediction
could be made because of missing words in the DSMs were
considered mistakes. The coverage of the models ranges from
97.8% to 100% of the triples, with a mean of 99%.
69
0500
1000
1500
2000
50 60 70 80 90 100
Figure 1: Identification of congruent prime: distri-
bution of accuracy (%) for global dataset
Dataset Min Max Mean ?
Global 50.2 96.5 80.2 9.2
V-N 45.8 95.8 80.0 8.4
N-V 49.1 99.1 82.7 9.7
N-N 47.6 97.6 78.7 10.0
Table 2: Identification of congruent prime: mean
and range for global dataset and subsets
N-V and slightly lower than on V-N. This effect
may be interpreted as being due to mediated prim-
ing, as no verb is explicitly involved in the N-N
relationship. Yet, the relatively high accuracy on
N-N and its relatively small difference from N-V
and V-N does not speak in favor of a different un-
derlying mechanism responsible for this effect. In-
deed, McKoon and Ratcliff (1992) suggested that
effects traditionally considered as instances of me-
diated priming are not due to activation spreading
through a mediating node, but result from a direct
but weaker relatedness between prime and target
words. This hypothesis found computational sup-
port in McDonald and Lowe (2000).10
4.1.1 Model Parameters and Accuracy
The aim of this section is to assess which param-
eters have the most significant impact on the per-
formance of DSMs in the task of identification of
the congruent prime.
We trained a linear model with the eight DSM
parameters as independent variables (R2 = 0.70)
and a second model that also includes all two-way
interactions (R2 = 0.89). Given the improvement
in R2 as a consequence of the inclusion of two-way
interactions in the linear model, we will focus on
the results from the model with interactions. Table
3 shows results from the analysis of variance for
10The interpretation of the N-N results in terms of spread-
ing activation is also rejected by Hare et al (2009, 163).
the model with interactions. For every parameter
(and interaction of parameters) we report degrees
of freedom (df ), percentage of explained variance
(R2), and a significance code (signif ). We only
list significant interactions that explain at least 1%
of the variance. Even though all parameters and
many interactions are highly significant due to the
large number of DSMs that were tested, an analy-
sis of their predictive power in terms of explained
variance allows us to make distinctions between
parameters.
Parameter df R2 signif
corpus 4 7.44 ***
window 2 4.39 ***
pos 2 0.92 ***
score 5 7.39 ***
transformation 3 3.79 ***
distance 2 22.20 ***
dimensionality reduction 2 10.52 ***
relatedness index 3 13.67 ***
score:transformation 15 4.53 ***
distance:relatedness index 12 2.24 ***
distance:dim.reduction 4 2.16 ***
window:dim.reduction 4 1.73 ***
Table 3: Accuracy: Parameters and interactions
Results in table 3 indicate that distance, dimen-
sionality reduction and relatedness index are the
parameters with the strongest explanatory power,
followed by corpus and score. Window and trans-
formation have a weaker explanatory power, while
pos falls below the 1% threshold. There is a
strong interaction between score and transforma-
tion, which has more influence than one of the in-
dividual parameters, namely transformation.
Figures 2 to 7 display the partial effects of dif-
ferent model parameters (pos was excluded be-
cause of its low explanatory power). One of the
main research questions behind this work was
whether neighbor rank performs better than dis-
tance in predicting priming data. The partial ef-
fect of relatedness index in Figure 6 confirms our
hypothesis: forward rank achieves the best perfor-
mance, distance the worst.11
Accuracy improves for models trained on big-
ger corpora (parameter corpus, figure 2; corpora
are ordered by size) and larger context windows
(parameter window, figure 3). Cosine is the best
performing distance measure (figure 4). Interest-
ingly, dimensionality reduction is found to neg-
atively affect model performance: as shown in
figure 7, both random indexing (ri) and singular
11Backward rank is equivalent to distance in this task.
70
74
76
78
80
82
84
86
bnc wp500 wacky ukwac joint
l
l
l
l
l
Figure 2: Corpus
74
76
78
80
82
84
86
2 5 15
l
l
l
Figure 3: Window
74
76
78
80
82
84
86
cos eucl man
l
l
l
Figure 4: Distance
l l
l
l
l
l
no
ne
freq Dice MI s?ll t?sc z?sc
74
76
78
80
82
84
86
l none
log
root
sigmoid
Figure 5: Score + Transformation
74
76
78
80
82
84
86
dist back_rank forw_rank avg_rank
l l
l
l
Figure 6: Rel. Index
74
76
78
80
82
84
86
none ri rsvd
l
l
l
Figure 7: Dim. Reduction
value decomposition (rsvd) cause a decrease in
predicted accuracy.
Because of the strong interaction between score
and transformation, only their combined effect
is shown (figure 5). Among the scoring mea-
sures, stochastic association measures perform
better than frequency: in particular log-likelihood
(simple-ll), z-score and t-score are the best mea-
sures. We can identify a general tendency of trans-
formation to lower accuracy. This is true for all
scores except log-likelihood: square root and (to a
lesser extent) logarithmic transformation result in
an improvement for this measure.
Figure 8 displays the interaction between the
parameters distance and dimensionality reduction.
Despite a general tendency for dimensionality re-
duction to lower accuracy, we found an interac-
tion between cosine distance and singular value
decomposition: in this combination, accuracy re-
mains stable and is even minimally higher com-
pared to no dimensionality reduction.
l l
l
cos eucl man
68
70
72
74
76
78
80
82
84
86
88
l none
ri
rsvd
Figure 8: Distance + Dimensionality Reduction
4.2 Correlation to Reaction Times
The results reported in section 4.1 demonstrate
that forward rank is the best index for identifying
which of the two primes is the congruent one. The
aim of this section is to find out whether rank is
also a good predictor of latency times. We check
correlation between distributional relatedness and
reaction times and evaluate the impact of model
parameters on this task.
Figure 9 displays the distribution of Pearson
correlation coefficient achieved by the different
DSMs on the global dataset.
0
500
1000
1500
0.0 0.1 0.2 0.3 0.4 0.5
Figure 9: Distribution of Pearson correlation be-
tween relatedness and RT in the global dataset
Figure 9 shows that the majority of the models
perform rather poorly, and that only few models
achieve moderate correlation with RT. DSM per-
71
formance in the correlation task appears to be less
robust to non-optimal parameter settings than in
the accuracy task (cf. figure 1).
Minimum, maximum, mean and standard devi-
ation correlation for the global dataset and for the
three evaluation subsets are shown in table 4. In all
the cases, absolute correlation values are used so
as not to distinguish between positive and negative
correlation.
Dataset Min Max Mean ?
Global -0.26 0.47 0.19 0.10
V-N -0.34 0.57 0.2 0.12
N-V -0.35 0.41 0.11 0.06
N-N -0.29 0.42 0.16 0.09
Table 4: Mean and range of Pearson correlation
coefficients on global dataset and subsets
4.2.1 Model Parameters and Correlation
In this section we discuss the impact of differ-
ent model parameters on correlation with reaction
times.
We trained a linear model with absolute Pearson
correlation on the global dataset as dependent vari-
able and the eight DSM parameters as independent
variables (R2 = 0.53), and a second model that in-
cludes two-way interactions (R2 = 0.77). Table
5 is based on the model with interactions; it re-
ports the degrees of freedom (df ), proportion of
explained variance (R2) and a significance code
(signif ) for every parameter and every interaction
of parameters (above 1% of explained variance).
Parameter df R2 signif
corpus 4 7.45 ***
window 2 0.47 ***
pos 2 0.20 ***
score 5 3.03 ***
transformation 3 3.52 ***
distance 2 4.27 ***
dimensionality reduction 2 10.57 ***
relatedness index 3 23.40 ***
dim.reduction:relatedness index 6 5.21 ***
distance:dim.reduction 4 4.11 ***
distance:relatedness index 6 3.77 ***
score:transformation 15 3.22 ***
score:relatedness index 15 1.37 ***
Table 5: Correlation: Parameters and interactions
Relatedness index is the most important param-
eter, followed by dimensionality reduction and
corpus. The explanatory power of the other pa-
rameters (score, transformation, distance) is lower
than for the accuracy task, and two parameters
(window and pos) explain less than 1% of the vari-
ance each. By contrast, the explanatory power of
interactions is higher in this task. Table 5 shows
the five relevant interactions with an overall higher
R2 compared to the accuracy task (cf. table 3).
The partial effect plot for relatedness index (fig-
ure 14) confirms the findings of the accuracy task:
forward rank is the best value for this parameter.
The best values for the other parameters, however,
show opposite tendencies with respect to the accu-
racy task. Models trained on smaller corpora (fig-
ure 10) perform better than those trained on big-
ger ones. Cosine is still the best distance measure,
but manhattan distance performs equally well in
this task (parameter distance, figure 12). Singu-
lar value decomposition (parameter dimensional-
ity reduction, figure 15) weakens the correlation
values achieved by the models, but no significant
difference is found between random indexing and
the unreduced data.
Co-occurrence frequency performs better than
statistical association measures and transforma-
tion improves correlation: figure 13 displays the
interaction between these two parameters. Trans-
formation has a positive effect for every score, but
the optimal transformation differs. Its impact is
particularly strong for the Dice coefficient, which
reaches the same performance as frequency when
combined with a square root transformation.
Let us conclude by discussing the interaction
between distance and dimensionality reduction
(figure 16). Based on the partial effects of the indi-
vidual parameters, any combination of manhattan
or cosine distance with random indexing or no di-
mensionality reduction should be close to optimal.
However, the interaction plot reveals that manhat-
tan distance with random indexing is the best com-
bination, outperforming the second best (cosine
without dimensionality reduction) by a consider-
able margin. The positive effect of random index-
ing is quite surprising and will require further in-
vestigation.
l l l
cos eucl man
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
l none
ri
rsvd
Figure 16: Distance + Dimensionality Reduction
72
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
bnc wp500 wacky ukwac joint
l
l l l
l
Figure 10: Corpus
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
2 5 15
l l
l
Figure 11: Window
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
cos eucl man
l
l
l
Figure 12: Distance
l l
l
l
l
l
freq Dice MI s?ll t?sc z?sc
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
l none
log
root
sigmoid
Figure 13: Score + Transformation
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
dist back_rank forw_rank avg_rank
l
l
l
l
Figure 14: Rel. Index
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
none ri rsvd
l l
l
Figure 15: Dim. Reduction
5 Conclusion
In this paper, we presented the results of a large-
scale evaluation of distributional models and their
parameters on behavioral data from priming ex-
periments. Our study is, to the best of our knowl-
edge, the first systematic evaluation of such a wide
range of DSM parameters in all possible combina-
tions. Our study also provides a methodological
contribution to the problem of DSM evaluation.
We propose to apply linear modeling to determine
the impact of different model parameters and their
interactions on the performance of the models. We
believe that this type of analysis is robust against
overfitting. Moreover, effects can be tested for
significance and various forms of interactions be-
tween model parameters can be captured.
The main findings of our evaluation can be sum-
marized as follows. Forward association (rank of
target among the nearest neighbors of the prime)
performs better than distance in both tasks at is-
sue: identification of congruent prime and correla-
tion with latency times. This finding confirms and
extends the results of previous studies (Hare et al,
2009). The relevance of rank-based measures for
cognitive modeling is discussed in section 3.2.2.
Identification of congruent primes on the ba-
sis of distributional relatedness between prime and
target is improved by employing bigger corpora
and by using statistical association measures as
scoring functions, while correlation to reaction
times is strengthened by smaller corpora and co-
occurrence frequency or Dice coefficient. A sig-
nificant interaction between transformation and
scoring function is found in both tasks: consider-
ing the interaction between these two parameters
turned out to be vital for the identification of opti-
mal parameter values.
Some preliminary analyses of individual the-
matic relations showed substantial improvements
of correlations. Therefore, future work will focus
on finer-grained linear models for single relations
and on further modeling of reaction times, extend-
ing the study by Hutchinson et al (2008).
Further research steps also include an evalua-
tion of syntax-based models (Baroni and Lenci,
2010; Pado? and Lapata, 2007) and term-document
models on the tasks tackled in this paper, as well
as an evaluation of all models on standard tasks.
Acknowledgments
We are grateful to Ken MacRae for providing us
the priming data modeled here and to Alessandro
Lenci for his contribution to the development of
this study. We would also like to thank the Com-
putational Linguistics group at the University of
Osnabru?ck and the Corpus Linguistics group at the
University Erlangen for feedback. Thanks also go
to three anonymous reviewers, whose comments
helped improve our analysis, and to Sascha Alex-
eyenko for helpful advice. The first author?s PhD
project is funded by a Lichtenberg grant from the
Ministry of Science and Culture of Lower Saxony.
73
References
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):1?49.
Marco Baroni and Alessandro Lenci. 2011. How
we blessed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
GEMS ?11, pages 1?10. Association for Computa-
tional Linguistics.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39:510?526.
John A. Bullinaria and Joseph P. Levy. 2012. Ex-
tracting semantic representations from word co-
occurrence statistics: stop-lists, stemming and svd.
Behavior Research Methods, 44:890?907.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis,
IMS, University of Stuttgart.
Todd Ferretti, Ken McRae, and Ann Hatherell. 2001.
Integrating verbs, situation schemas, and thematic
role concepts. Journal of Memory and Language,
44(4):516?547.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological Review, 114:211?244.
Nathan Halko, Per-Gunnar Martinsson, and Joel A.
Tropp. 2009. Finding structure with randomness:
Stochastic algorithms for constructing approximate
matrix decompositions. Technical Report 2009-05,
ACM, California Institute of Technology.
Mary Hare, Michael Jones, Caroline Thomson, Sarah
Kelly, and Ken McRae. 2009. Activating event
knowledge. Cognition, 111(2):151?167.
Zelig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Amac Herdag?delen, Marco Baroni, and Katrin Erk.
2009. Measuring semantic relatedness with vector
space models and random walks. In Proceedings
of the 2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 50?53.
Keith A. Hutchinson, David A. Balota, Michael J.
Cortese, and Jason M. Watson. 2008. Predicting
semantic priming at the item level. The Quarterly
Journal of Experimental Psychology, 61(7):1036?
1066.
Michael Jones and Douglas Mewhort. 2007. Repre-
senting word meaning and order information in a
composite holographic lexicon. Psychological Re-
view, 114:1?37.
Thomas K. Landauer and Susan T. Dumais. 1997.
A solution to Plato?s problem: The latent seman-
tic analysis theory of the acquisition, induction, and
representation of knowledge. Psychological Review,
104:211?240.
Will Lowe and Scott McDonald. 2000. The direct
route: mediated priming in semantic space. Tech-
nical report, Division of Informatics, University of
Edinburgh.
Scott McDonald and Chris Brew. 2004. A distribu-
tional model of semantic context effects in lexical
processing. In Proceedings of ACL-04, pages 17?
24.
Gain McKoon and Roger Ratcliff. 1992. Spreading ac-
tivation versus compound cue accounts of priming:
Mediated priming revisited. Journal of Experimen-
tal Psychology: Learning, Memory and Cognition,
18:1155?1172.
Ken McRae and Kazunaga Matzuki. 2009. People use
their knowledge of common events to understand
language, and do so as quickly as possible. Lan-
guage and Linguistics Compass, 3(6):1417?1429.
Ken McRae, Mary Hare, Jeffrey L. Elman, and Todd
Ferretti. 2005. A basis for generating expectan-
cies for verbs from nouns. Memory & Cognition,
33(7):1174?1184.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Proceedings of the Methods and Appli-
cations of Semantic Indexing Workshop at the 7th In-
ternational Conference on Terminology and Knowl-
edge Engineering, TKE 2005.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis, University
of Stockolm.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84:327?352.
74
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 50?59,
Dublin, Ireland, August 23, 2014.
NaDiR: Naive Distributional Response Generation
Institut f?ur
Maschinelle Sprachverarbeitung
Universit?at Stuttgart
Gabriella Lapesa
Institut f?ur
Kognitionswissenschaft
Universit?at Osnabr?uck
glapesa@uos.de
Stefan Evert
Professur f?ur
Korpuslinguistik
FAU Erlangen-N?urnberg
stefan.evert@fau.de
Abstract
This paper describes NaDiR (Naive DIstributional Response generation), a corpus-based system
that, from a set of word stimuli as an input, generates a response word relying on association
strength and distributional similarity. NaDiR participated in the CogALex 2014 shared task on
multiword associations (restricted systems track), operationalizing the task as a ranking problem:
candidate words from a large vocabulary are ranked by their average association or similarity to
a given set of stimuli. We also report on a number of experiments conducted on the shared
task data, comparing first-order models (based on co-occurrence and statistical association) to
second-order models (based on distributional similarity).
1 Introduction
This paper describes NaDiR, a corpus-based system designed for the reverse association task. NaDiR
is an acronym for Naive Distributional Response generation. NaDiR is naive because it is based on a
very simple algorithm that operationalizes the multiword association task as a ranking problem: candi-
date words from a large vocabulary are ranked by their average statistical association or distributional
similarity to a given set of stimuli, then the highest-ranked candidate is selected as NaDiR?s response.
We compare models based on collocations (first-order models, see Evert (2008) for an overview) to
models based on distributional similarity (second-order models; see Sahlgren (2006), Turney and Pan-
tel (2010), and reference therein for a review). Previous work on this task showed that co-occurrence
models outperform distributional semantic models (henceforth, DSMs), and that using rank measures
improves performance because it accounts for directionality of the association/similarity (e.g., the asso-
ciation from stimulus to response may be larger than the association from response to stimulus). Our
results corroborate both claims.
The paper is structured as follows: section 2 provides an overview of the task and of the problems
we encountered in its implementation; section 3 summarizes related work; section 4 describes NaDiR in
detail; section 5 reports the results of our experiments on the shared task training and test data; section 6
describes ongoing and future work on NaDiR.
2 The Task and its Problems
The shared task datasets are derived from the Edinburgh Associative Thesaurus (Kiss et al., 1973)
1
. The
Edinburgh Associative Thesaurus (henceforth, EAT) contains free associations to approximately 8000
English cue words. For each cue (e.g., visual) EAT lists all associations collected in the survey (e.g., aid,
eyes, aids, see, eye, seen, sight, etc.) sorted according to the number of subjects who responded with the
respective word. The CogALex shared task on multiword association is based on the EAT dataset, and
is in fact a reverse association task (Rapp, 2014). The top five responses for a target word are provided
as stimuli (e.g., aid, eyes, aids, see, eye), and the participating systems are required to generate the
original cue as a response (e.g., visual). The training and the test sets are random extracts of 2000 EAT
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.eat.rl.ac.uk/
50
items each, with minimal pre-processing (only items containing multiword units and non-alphabetical
characters were discarded).
A key problem we had to tackle while developing our system was the unrestricted set of possible re-
sponses in combination with a discrete association task, which requires the algorithm to pick exactly the
right answer out of tens of thousands of possible responses. This feature makes this task much more dif-
ficult than the multiple-choice tasks often used to evaluate distributional semantic models. The problem
is further complicated by the fact that the response may be an inflected form and only a prediction of the
exact form was accepted as a correct answer. The need for a solution to these issues motivates various
aspects of the NaDiR algorithm, described in section 4.
3 Related Work
Previous studies based on free association norms differ considerably in terms of the type of task (regular
free association task ? one stimulus, one response vs. multiword association task ? many stimuli, one
response), gold standards, and key features of the evaluated models (e.g., source corpora used and choice
of a candidate vocabulary from which responses are selected).
In regular free association tasks (one stimulus, one response), responses are known to contain both
paradigmatically and syntagmatically related words. Rapp (2002) proposes to integrate first-order (co-
occurrence lists) and second-order (bag-of-words DSMs) information to distinguish syntagmatic from
paradigmatic relations by exploiting the comparison of most salient collocates and nearest neighbors.
A task derived from the EAT norms was used in the ESSLLI 2008 shared task
2
. Results from first-
order co-occurrence data turned out to be much better than those from second-order DSMs, in line with
the findings made by Rapp (2002) and Wettler et al. (2005).
A similar picture emerges from studies on the multiword association task. Models based on first-order
co-occurrence (collocations) outperform models based on vector similarity. This superiority, however, is
not validated via a direct comparison: results were obtained by studies with different features and goals
(see Rapp (2014) for a review; see Griffiths et al. (2007) and Smith et al. (2013) for evaluations of
models based on Latent Semantic Analysis). A specific feature of successful studies on the multiword
association task is that they introduce an element of directionality (Rapp, 2013; Rapp, 2014), which
allows a correct implementation of the directionality of the modeled effects (from stimulus to response).
Our survey of related studies motivated the choice to base NaDiR on first-order or second-order co-
occurrence statistics, and to use collocate or neighbor rank to account for directionality. Our main contri-
bution to research on the reverse association task is a systematic experimental comparison of first-order
and second-order models (using the same gold standard, same source corpus, and same candidate vocab-
ulary), which enables us to give a sound answer to the question whether first-order models are indeed
superior for multiword association tasks.
4 NaDiR
NaDiR operationalizes the multiword association task as a ranking problem. For each set of stimuli,
the possible response words (?candidates?) are ranked according to their average association strength or
distributional similarity to the stimulus words. The top-ranked candidate is selected as NaDiR?s response.
One advantage of the ranking approach is that it provides additional insights into the experimental results:
if the model prediction is not correct, the rank of the correct answer can be used as a measure how ?close?
the model came to the human associations.
Since neither a fixed set of response candidates nor an indication of the source of the training and
test data were available (and we did not google for the training sets), we compiled a large vocabulary of
possible responses. We believe that restricting the vocabulary to the 8,033 cue words in the EAT would
have improved our results considerably. More details concerning the choice of the candidate vocabulary
are reported in section 4.1.
2
http://wordspace.collocations.de/doku.php/data:esslli2008:correlation with free
association norms
51
NaDiR uses either first-order or second-order co-occurrence statistics to predict the association
strength between stimuli and responses. In the first case (?collocations?), we apply one of several stan-
dard statistical association measures to co-occurrence counts obtained from a large corpus. In the second
case, association is quantified by cosine similarity in a distributional semantic model built from the same
corpus. Both first-order and second-order statistics were collected from UKWaC in order to compete in
the constrained track of the shared task.
Recent experiments (Hare et al., 2009; Lapesa and Evert, 2013; Lapesa et al., to appear) suggest
that semantic relations are often better captured by neighbour ranks rather than direct use of statistical
association measures or cosine similarity values. Therefore, NaDiR can alternatively quantify association
strength by collocate rank and similarity by neighbour rank. In our experiments (section 5), we compare
the different approaches.
NaDiR is designed for the multiword association task, and it contains additional features related to the
particular design of the CogALex shared task:
? We reduce the number of candidates by selecting the most likely response POS with a machine-
learning algorithm (section 4.1);
? NaDiR operates on lemmatized data in order to reduce sparseness. We lemmatize stimuli using a
heuristic method (section 4.1), predict a response lemma, and then use machine-learning techniques
to generate a plausible word form (section 4.3).
4.1 Pre-processing and Vocabulary
Our experiments were conducted on the UKWaC
3
corpus. UKWaC contains 2 billion words, web-
crawled from the .uk domain between 2005 and 2007. The release of UKWaC also contains linguistic
annotation (pos-tagging and lemmatization) performed with Tree Tagger
4
.
To assign a part-of-speech tag and a lemma to every word in the dataset without relying on external
tools, we adopted the following mapping strategy based on the linguistic annotation already available in
UKWaC:
1. We extracted all attested wordform/part of speech/lemma combinations from UKWaC, together
with their frequency;
2. Every word form in the training set was assigned to the most frequent part of speech/lemma combi-
nation attested in UKWaC.
We believe that the advantages of constructing distributional models based on lemmatized words over-
come the drawbacks of this type of out-of-context lemmatization and part-of-speech assignment.
The part-of-speech information added to every word in the dataset by the mapping procedure was
used to train a classifier that, given the parts of speech of the stimuli, predicts the part of speech of the
response. We trained a support-vector machine, using the svm function from the R package e1071
5
,
with standard settings.
The part-of-speech classifier is based on a coarse part-of-speech tagset with only five tags: N (noun),
J (adjective), V (verb), R (adverb), other (closed-class words). We considered each row of the dataset
as an observation, with the part of speech of the response as predicted value, and the part of speech of
the stimulus words as predictors. Every observation is represented as a bag of tags, i.e., a vector listing
for each of the five tags how often it occurs among the stimuli. For example, if a set of stimuli contains
3 nouns, one verb and one adjective, the corresponding bag-of-tags vector looks as follows: {N = 3; V =
1; J = 1; R = 0; other = 0}. On the training set, the part-of-speech classifier achieves an accuracy of
72%.
The vocabulary of our models only contains lemmatized open-class words (this information is avail-
able in the annotation of the corpus). By inspecting the frequencies of stimuli and response words in the
training dataset, we established a reasonable minimum frequency threshold for candidate words of 100
occurrences in UKWaC. With this threshold, only 10 response words and 16 stimulus words from the
3
wacky.sslmit.unibo.it/doku.php?id=corpora
4
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
5
http://cran.r-project.org/web/packages/e1071/index.html
52
training dataset are excluded from the vocabulary. Given the large size of the dataset, we decided that
a minimal loss in coverage would be justified by the reduced computational complexity. The resulting
candidate vocabulary contains 155,811 words.
4.2 First- and Second-order Statistics
The aim of this section is to describe the parameters involved in the collection of first-order and second-
order statistics from UKWaC. All models have been built and evaluated using the UCS toolkit
6
and the
wordspace package for R (Evert, to appear)
7
.
First-order Models
Collocation data are compiled from UKWaC based on the vocabulary described in section 4.1. Both
nodes (rows of the co-occurrence matrix) and collocates (columns of the co-occurrence matrix) are cho-
sen from this vocabulary. Collection of first-order models involved the manipulation of a number of
parameters, briefly summarized below.
We adopted three different window sizes:
? symmetric window, 2 words to the left and to the right of the node;
? asymmetric window, 3 words to the left of the node;
? asymmetric window, 3 words to the right of the node.
We tested the following association scores (Evert, 2008):
? co-occurrence frequency;
? simple log-likelihood (similar to local MI used by Baroni and Lenci (2010));
? conditional probability.
Our experiments involved a third parameter, the index of association strength, which determines al-
ternative ways of quantifying the degree of association between targets and contexts in the first-order
model. Given two words a and b represented in a first-order model, we propose two alternative ways of
quantifying the degree of association between a and b. The first option (and standard in corpus-based
modeling) is to compute the association score between a and b. The alternative choice is based on rank
among collocates. Given two words a and b, in our task stimulus and potential response, we consider:
? forward rank: the rank of the potential response among the collocates of the stimulus;
? backward rank: the rank of the stimulus among the collocates of the potential response;
? average rank: the average of forward and backward rank.
Second-order Models
Based on the results of a large-scale evaluation of DSM parameters (Lapesa and Evert, under review)
and the modeling of semantic priming effects (Lapesa and Evert, 2013; Lapesa et al., to appear), we
identified a robust configuration of parameters for second-order models that we decided to adopt in this
study. Second-order models involved in our experiments share the following parameter settings:
? The target words (rows) are defined by the vocabulary described in section 4.1.
? The context words (columns) are the 50,000 most frequent context words in the respective co-
occurrence matrices. The 50 most frequent words in UKWaC are discarded.
? Co-occurrence vectors are scored with a sparse version of simple-log likelihood, in which negative
values clamped to zero in order to preserve the sparseness of the co-occurrence matrix. Scored
vectors are rescaled by applying a logarithmic transformation.
? We reduce the scored co-occurrence matrix to 1000 latent dimensions using randomized SVD
(Halko et al., 2009).
? We adopt cosine distance (i.e. the angle between vectors) as a distance metric for the computation
of vector similarity.
6
http://www.collocations.de/software.html
7
http://r-forge.r-project.org/projects/wordspace/
53
Our experiments on second-order models involved the manipulation of two parameters: window size
and index of association strength.
The size of the context window quantifies the amount of shared context involved in the computation of
similarity. We expect the manipulation of window size to be crucial in determining model performance,
as different context windows will enable the model to capture different types of relations between re-
sponse and stimulus words (Sahlgren, 2006; Lapesa et al., to appear). In our experiments with NaDiR,
we adopted three different window sizes:
? symmetric window, 2 words to the left and to the right of the target;
? symmetric window, 4 words to the left and to the right of the target;
? symmetric window, 16 words to the left and to the right of the target.
The values for index of association strength are the same as for the first-order models, computing ranks
among the nearest neighbors of the stimulus or response word. The use of rank-based measures is of
particular interest, because: (i) it allows us to model directionality (while, for example, cosine distance is
symmetric); (ii) it already proved successful in modeling behavioral data (Hare et al., 2009; Lapesa and
Evert, 2013); (iii) since the vocabulary of first-order and second-order models are identical, rank-based
measures allow a direct comparison between the two classes of models, as well as experiments based on
their combination.
4.3 Response Generation
To generate a response for a set of stimuli in the training/test dataset, we apply the following procedure:
1. For each set of stimuli, we compute association strengths or similarities between each stimulus and
each response candidate, adopting one of the measures described in section 4.2.
2. From the set of potential responses, we select the words whose POS agrees with the predictions of
the classifier described in section 4.1. Stimulus words are discarded from the potential answers.
3. We compute the average association strength or similarity across all five stimuli; if a stimulus does
not appear in the model, it is simply omitted from the average.
4. The top-ranked candidate is the POS-disambiguated lemma suggested as a response by NaDiR.
5. We generate a suitable word form by inverting the heuristic lemmatization; if the full Penn tag (e.g.,
NNS: noun, common, plural; NN: noun, common, singular or mass, etc.) of the response is known,
this step can be implemented as a deterministic lookup (since a word form is usually determined
uniquely by lemma and Penn tag). We therefore trained a second SVM classifier that predicts the
full Penn tag of the response based on the full tags of the stimuli. On the training set, this part-of-
speech classifier reaches an accuracy of 68%.
5 Experiments
In our experiments, we compared first-order (collocations) and second-order (DSM) models; for each
class of models, we evaluated the different parameter values described in section 4.2. Table 1 summarizes
the evaluated parameters for first-order and second-order models.
Model Window Score Relatedness Index
first-order symmetric, 2 frequency association score
left 3, right 0 simple log-likelihood forward rank
left 0, right 3 conditional probability backward rank
average rank
second-order symmetric, 2 simple log-likelihood distance
symmetric, 4 forward rank
symmetric, 16 backward rank
average rank
Table 1: Evaluated Parameters for First- and Second-order Models
54
Tables 2 to 5 display the results of our experiments on the training data, separately for first-order (tables
2-4) and second-order models (table 5). Parameter configurations are reported in the Parameter column
8
.
The number of correct responses in the lemmatized version is reported in the column Lemma (showing
how often our system predicted the correct lemma). The column Wordform reports the number of correct
responses for which, before inverting the lemmatization, the inflected form was already identical to the
lemma. As the task of predicting exactly one word is particularly difficult, we further characterize the
performance of our evaluated models by reporting the number of cases in which the correct answer from
the training set was among the first 10 (< 10), 50 (< 50), or 100 (< 100) ranked candidates. In the last
column, we report the average rank of the correct responses (Avg correct).
The results reported in tables 2 to 5 allowed us to identify best parameter configurations for the first-
order (symmetric 2 words window, frequency, backward rank) and second-order models (2 words win-
dow, distance). We evaluated these configurations on the test data (table 6). Table 7 compares the
performance of the best first-order and the best second-order model on the training and test datasets,
both for lemmatized response (Training-Lemma, Test-Lemma) and generation of the correct word form
(Training-Inflected, Test-Inflected).
A considerable portion of the experiments reported in this paper were conducted after the submission
deadline of the CogALex shared task. As a consequence, our submitted results do not correspond to the
best overall configuration found in the evaluation study. The submission was based on a second order
model, a 4-word window, and cosine distance as index of distributional similarity. In this configuration,
NaDiR generated 262 correct responses, corresponding to an accuracy of 13%.
Parameters Lemma Wordform < 10 < 50 < 100 Avg correct
Freq
ass
2 2 85 372 561 1400
Freq
fwd
0 0 77 359 550 6258
Freq
bwd
555 464 973 1269 1369 1546
Freq
avg
424 322 677 848 934 5969
Simple-ll
ass
33 28 237 721 985 933
Simple-ll
fwd
405 319 760 916 947 12031
Simple-ll
bwd
531 444 914 1141 1253 1971
Simple-ll
avg
490 388 785 918 950 11645
Cond.prob
ass
18 16 329 746 970 978
Cond.prob
fwd
0 0 77 359 550 6258
Cond.prob
bwd
422 359 856 1129 1255 1719
Cond.prob
avg
343 256 611 860 971 5948
Table 2: First Order Models - Symmetric Window: 2 words to the left/right of the node - Training Data
5.1 Discussion
The results of our experiments are in line with the tendencies identified in the literature (see section
3). First-order models based on direct co-occurrence (high scores are assigned to words that co-occur),
outperform second-order models based on distributional similarity (smaller distances between words that
occur in similar contexts).
For the first-order models, the best index of association strength is backward rank (the rank of the
stimulus among the collocates of the potential response), fully congruent with the experimental setting
(in the EAT norm, subjects produced the stimuli as free associations of the expected response). Surpris-
ingly, frequency outperforms simple-log likelihood (which is usually considered to be among the best
association measures for the identification of collocations). In line with the results achieved by Rapp
(2014), a symmetric window of 2 words to the left and to the right of the target achieves best results.
For the second-order models, the smallest context window (2 words) achieves the best performance.
8
Abbreviations used in the tables: ass = association score; dist = distance; fwd = forward rank; bwd = backward rank; avg
= average rank.
55
Parameters Lemma Wordform < 10 < 50 < 100 Avg correct
Freq
ass
1 1 63 279 450 1733
Freq
fwd
0 0 32 219 395 7575
Freq
bwd
358 292 789 1124 1247 1974
Freq
avg
277 191 515 690 793 7251
Simple-ll
ass
23 18 196 618 878 1259
Simple-ll
fwd
271 196 605 789 842 14177
Simple-ll
bwd
369 296 737 1002 1135 2848
Simple-ll
avg
346 251 636 798 845 13760
Cond.prob
ass
7 6 209 588 806 1234
Cond.prob
fwd
0 0 32 219 395 7575
Cond.prob
bwd
284 230 659 974 1109 2318
Cond.prob
avg
201 137 462 711 851 7230
Table 3: First Order Models ? Asymmetric Window: 3 words to the left of the node ? Training Data
Parameters Lemma Wordform < 10 < 50 < 100 Avg correct
Freq
ass
1 1 63 279 450 1733
Freq
fwd
0 0 32 219 395 7575
Freq
bwd
358 292 789 1124 1247 1974
Freq
avg
277 191 515 690 793 7251
Simple-ll
ass
25 22 220 643 891 1168
Simple-ll
fwd
321 250 708 895 936 12244
Simple-ll
bwd
507 424 884 1142 1246 2223
Simple-ll
avg
402 314 740 901 939 11868
Cond.prob
ass
26 20 279 665 864 1282
Cond.prob
fwd
0 0 59 298 498 7543
Cond.prob
bwd
381 319 791 1094 1201 1981
Cond.prob
avg
278 209 535 800 922 7214
Table 4: First Order Models ? Asymmetric Window: 3 words to the right of the node ? Training Data
Parameters Lemma Wordform < 10 < 50 < 100 Avg correct
2
dist
264 208 686 1077 1224 936
2
fwd
127 83 380 703 849 1560
2
bwd
73 56 275 584 720 3524
2
avg
157 106 436 750 911 1507
4
dist
255 200 665 1037 1195 997
4
fwd
108 73 338 651 824 1750
4
bwd
77 57 254 545 694 3843
4
avg
129 87 397 710 862 1694
16
dist
206 158 546 910 1062 1433
16
fwd
63 40 252 512 667 2481
16
bwd
49 37 188 449 581 4949
16
avg
79 56 282 560 713 2416
Table 5: Second order models ? Training data
Considering the good results from collocation-based models, we would have expected a better perfor-
mance from larger windows, traditionally considered to be more sensitive to syntagmatic relations. A
significant difference between first-order and second-order models is the fact that neighbor rank works
less well than the distance between vectors, while collocate rank outperformed the association scores.
56
Model Lemma Wordform < 10 < 50 < 100 Avg correct
first-order 572 490 1010 1303 1408 1366
second-order 304 246 734 1119 1256 569
Table 6: Best models (first order and second order) ? Performance on test data
Model Training-Lemma Training-Inflected Test-Lemma Test-Inflected
first-order 27.7% (555) 26.9% (538) 28.6% (572) 27.7% (554)
second-order 13.2% (264) 12.0% (241) 15.0% (304) 14.0% (279)
Table 7: Performance (% accuracy and number of correct responses) of the best first-order and second-
order model on training vs. test dataset (lemmatized response vs. response with restored inflection)
The observation for second-order models contrasts with previous work showing that rank consistently
outperforms distance in modeling priming effects (Lapesa and Evert, 2013; Lapesa et al., to appear) and
also in standard tasks such as prediction of similarity ratings and noun clustering (Lapesa and Evert, un-
der review). Among the standard tasks, the only case in which the use of neighbor rank did not produce
significant improvements with respect to vector distance was the TOEFL multiple-choice synonymy task.
Despite clear differences, the TOEFL task and the reverse association task share the property that they
involve multiple stimuli. The results presented in this paper, together with those achieved on the TOEFL
task, seem to suggest that a better strategy for the use of neighbor rank needs to be developed when
multiple stimuli are involved.
6 Conclusions and Future Work
The results of the evaluation reported in this paper confirmed the tendencies identified in previous studies:
first-order models, based on direct co-occurrence, outperform second-order models, based on distribu-
tional similarity. We consider the experimental results described in this paper as a first exploration into
the dynamics of the reverse association task, and we believe that our systematic evaluation of first- and
second-order models represents a good starting point for future work, which targets improvements of
NaDiR at many levels.
The first point of improvement concerns the size of the vocabulary. We aim at finding a more op-
timal cutoff on the training data, for example by implementing a frequency bias similar to Wettler et
al. (2005). We are confident that NaDiR will significantly benefit from a smaller range of potential
responses (compared to the 155,811 lemmatized candidate words in the current version).
We are also conducting experiments using log ranks instead of plain ranks: since we compute an arith-
metic mean of the rank values, a single very high rank (from a poorly matched stimulus) will dominate
the average. We therefore assume that log ranks will improve results and make NaDiR?s responses more
robust.
An interesting research direction targets the integration of first- and second-order statistics in the pro-
cess of response generation. The evaluation results reported in this paper revealed that a very small
context window achieves the best performance for second-order models: as widely acknowledged in the
literature (Sahlgren, 2006; Lapesa et al., to appear), smaller context windows highlight paradigmatic
relations. First-order models, on the other hand, highlight syntagmatic relations (Rapp, 2002). The best
second-order and first-order models from the evaluation reported in this paper are likely to focus on dif-
ferent types of relations between response and stimulus words: this leads us to believe that an integration
of the two sources may produce improvements in NaDiR?s performance.
At a general level, we plan to make more elaborate use of the training data. In the experiments
presented in this paper, training data were used to set a frequency threshold for potential responses, train
the part-of-speech classifiers, and find the best configuration for first- and second-order models.
A possible new application of NaDiR is the modeling of datasets containing semantic norms or concept
properties, such as the McRae norms (McRae et al., 2005) or BLESS (Baroni and Lenci, 2011). Those
datasets are standard in DSM evaluation, and their modeling can be implemented in terms of a reverse
57
association task, with the additional advantage that the relations between concepts and properties in those
datasets are labelled with property types for the McRae norms (e.g., encyclopedic, taxonomic, situated)
or semantic relations (e.g., hypernymy, meronymy, event-related) for BLESS. This allows a specific
evaluation for each property type or semantic relation, which will in turn give new insights into the
semantic knowledge encoded in the different corpus-based representations (first order vs. second order
vs. hybrid) and how model parameters affect these representations (e.g., window size in the comparison
of syntagmatic vs. paradigmatic relations).
Acknowledgments
Gabriella Lapesa?s research is funded by the DFG Collaborative Research Centre SFB 732 (University
of Stuttgart).
References
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):1?49.
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS ?11, pages 1?10.
Association for Computational Linguistics.
Stefan Evert. 2008. Corpora and collocations. In Anke L?udeling and Merja Kyt?o, editors, Corpus Linguistics. An
International Handbook, chapter 58. Mouton de Gruyter, Berlin, New York.
Stefan Evert. to appear. Distributional semantics in R with the wordspace package. In Proceedings of COLING
2014: System Demonstrations.
Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. 2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. 2009. Finding structure with randomness: Stochastic
algorithms for constructing approximate matrix decompositions. Technical Report 2009-05, ACM, California
Institute of Technology.
Mary Hare, Michael Jones, Caroline Thomson, Sarah Kelly, and Ken McRae. 2009. Activating event knowledge.
Cognition, 111(2):151?167.
G. R. Kiss, C. Armstrong, R. Milroy, and J. Piper. 1973. An associative thesaurus of English and its computer
analysis. In The Computer and Literary Studies. Edinburgh University Press.
Gabriella Lapesa and Stefan Evert. 2013. Evaluating neighbor rank and distance measures as predictors of se-
mantic priming. In Proceedings of the ACL Workshop on Cognitive Modeling and Computational Linguistics
(CMCL 2013), pages 66?74.
Gabriella Lapesa, Stefan Evert, and Sabine Schulte im Walde. to appear. Contrasting syntagmatic and paradig-
matic relations: Insights from distributional semantic models. In Proceedings of the 3rd Joint Conference on
Lexical and Computational Semantics (*SEM). Dublin, Ireland, August 2014.
Ken McRae, George Cree, Mark Seidenberg, and Chris McNorgan. 2005. Semantic feature production norms for
a large set of living and nonliving things. Behavior Research Methods, 4(37):547?559.
Reinhard Rapp. 2002. The computation of word associations: Comparing syntagmatic and paradigmatic ap-
proaches. In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, pages
1?7.
Reinhard Rapp. 2013. From stimulus to associations and back. In Proceedings of the 10th Workshop on Natural
Language Processing and Cognitive Science.
Reinhard Rapp. 2014. Corpus-based computation of reverse associations. In Proceedings of the Ninth Interna-
tional Conference on Language Resources and Evaluation (LREC?14).
Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and
paradigmatic relations between words in high-dimensional vector spaces. Ph.D. thesis, University of Stockolm.
58
Kevin A. Smith, David E. Huber, and Edward Vul. 2013. Multiply-constrained semantic search in the remote
associates test. Cognition, 128(1):64?75.
Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141?188.
Manfred Wettler, Reinhard Rapp, and Peter Sedlmeier. 2005. Free word associations correspond to contiguities
between words in texts. Journal of Quantitative Linguistics, 1(12):111?122.
59
