Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 382?385,
Prague, June 2007. c?2007 Association for Computational Linguistics
UC3M: Classification of Semantic Relations between Nominals using 
Sequential Minimal Optimization  
Isabel Segura Bedmar 
Computer Science Department 
University Carlos III of Madrid 
isegura@inf.uc3m.es 
Doaa Samy 
Computer Science Department 
University Carlos III of Madrid 
dsamy@inf.uc3m.es 
Jose L. Martinez 
Computer Science Department 
University Carlos III of Madrid 
jlmartinez@inf.uc3m.es 
 
Abstract 
This paper presents a method for auto-
matic classification of semantic relations 
between nominals using Sequential 
Minimal Optimization. We participated 
in the four categories of SEMEVAL task 
4 (A: No Query, No Wordnet; B: Word-
Net, No Query; C: Query, No WordNet; 
D: WordNet and Query) and for all train-
ing datasets. Best scores were achieved 
in category B using a set of feature vec-
tors including lexical file numbers of 
nominals obtained from WordNet and a 
new feature WordNet Vector designed 
for the task1. 
1 Introduction 
The survey of the state-of-art reveals an increas-
ing interest in automatically discovering the un-
derlying semantics in natural language. In this 
interdisciplinary field, the growing interest is 
justified by the number of applications which 
can directly benefit from introducing semantic 
information. Question Answering, Information 
Retrieval and Text Summarization are examples 
of these applications (Turney and Littman, 2005; 
Girju et al, 2005).  
In the present work and for the purpose of the 
SEMEVAL task 4, our scope is limited to the 
semantic relationships between nominals. By 
this definition, we understand it is the process of 
discovering the underlying relations between 
two concepts expressed by two nominals.  
                                                 
1 This work has been partially supported by the Re-
gional Government of Madrid Ander the Research 
Network MAVIR (S-0505/TIC-0267) 
Within the framework of SEMEVAL, nomi-
nals can occur either on the phrase, clause or the 
sentence level. This fact constitutes the major 
challenge in this task since most of the previous 
research limited their approaches to certain types 
of nominals mainly the ?compound nomi-
nals?(Girju et al 2005). 
The paper is divided as follows; section 2 is a 
brief introduction to SMO used as the classifier 
for the task. Section 3 is dedicated to the de-
scription of the set of features applied in our ex-
periments. In section 4, we discuss the experi-
ment?s results compared to the baselines of the 
SEMEVAL task and the top scores. Finally, we 
summarize our approach, pointing out conclu-
sions and future directions of our work. 
2 Sequential Minimal Optimization 
We decided to use Support Vector Machine 
(SVM), as one of the most successful Machine 
Learning techniques, achieving the best per-
formances for many classification tasks. Algo-
rithm performance and time efficiency are key 
issues in our task, considering that our final goal 
is to apply this classification in a Question An-
swering System.  
Sequential Minimal Optimization (SMO) is a 
fast method to train SVM. SMO breaks the large 
quadratic programming (QP) optimization prob-
lem needed to be resolved in SVM into a series 
of smallest possible QP problems. These small 
QP problems are analytically solved, avoiding, 
in this way, a time-consuming numerical QP 
optimization as an inner loop. We used Weka 
(Witten and Frank, 2005) an implementation of 
the SMO (Platt, 1998). 
382
3 Features 
Prior to the classification of semantic rela-
tions, characteristics of each sentence are auto-
matically extracted using GATE (Cunningham 
et al, 2002). GATE is an infrastructure for de-
veloping and deploying software components for 
Language Engineering. We used the following 
GATE components: English Tokenizer, Part-Of-
Speech (POS) tagger and Morphological ana-
lyser. 
The set of features used for the classification 
of semantic relations includes information from 
different levels: word tokens, POS tags, verb 
lemmas, semantic information from WordNet, 
etc. Semantic features are only applied in cate-
gories B and D.  
On the lexical level, the set of word features 
include the two nominals, their heads in case one 
of the nominals in question or both are com-
pound nominals ( e.g. the relation between 
<e1>tumor shrinkage</e1> and <e2>radiation 
therapy </e2> is actually between the head of 
the first ?shrinkage? and ?radiation_therapy?). 
More features include: the two words before the 
first nominal, the two words after the second 
nominal, and the word list in-between (Wang et 
al., 2006).  
On the POS level, we opted for using a set of 
POS features since word features are often too 
sparse. This set includes POS tags of the two 
words occurring before the first nominal and the 
two words occurring after the second nominal 
together with the tag list of the words in-
between (Wang et al, 2006). POS tags of nomi-
nals are considered redundant information.  
Information regarding verbs and prepositions, 
occurring in-between the two nominals, is highly 
considered. In case of the verb, the system takes 
into account the verb token and the information 
concerning the voice and lemma. In the same 
way, the system keeps track of the prepositions 
occurring between both nominals. In addition, a 
feature, called numinter, indicating the number 
of words between nominals is considered. 
Other important feature is the path from the 
first nominal to the second nominal. This feature 
is built by the concatenation of the POS tags be-
tween both nominals.  
The feature related to the query provided for 
each sentence is only considered in the catego-
ries C and D according to the SEMEVAL re-
strictions. 
On the semantic level, we used features ob-
tained from WordNet.  In addition to the Word-
Net sense keys, provided for each nominal, we 
extracted its synset number and its lexical file 
number.  
Based on the work of Rosario, Hearst and 
Fillmore (2002), we suppose that these lexical 
file numbers can help to determine if the nomi-
nals satisfy the restrictions for each relation. For 
example, in the relation Theme-Tool, the theme 
should be an object, an event, a state of being, an 
agent, or a substance. Else, it is possible to af-
firm that the relation is false.  
For the Part-Whole relation and due to its 
relevance in this classification task, a feature 
indicating metonymy relation in WordNet was 
taken into account. 
Furthermore, we designed a new feature, 
called WordNet vector. For constructing this 
vector, we selected the synsets of the third level 
of depth in WordNet and we detected if each is 
ancestor or not of the nominal. It is a binary vec-
tor, i.e. if the synset is ancestor of the nominal it 
is assigned the value 1, else it is assigned the 
value 0. In this way, we worked with two vec-
tors, one for each nominal. Each vector has a 
dimension of 13 coordinates. Each coordinate 
represents one of the 13 nodes in the third level 
of depth in WordNet. Our initial hypothesis con-
siders that this representation for the nominals 
could perform well on unseen data.  
4 Experiment Results 
Cross validation is a way to test the ability of 
the model to classify unseen examples. We 
trained the system using 10-fold cross-
validation; the fold number recommended for 
small training datasets. For each relation and for 
each category (A, B, C, D) we selected the set of 
features that obtained the best results using the 
indicated cross validation.  
We submitted 16 sets of results as we partici-
pated in the four categories (A, B, C, D). We 
also used all the possible sizes of the training 
dataset (1: 1 to 35, 2:1 to 70, 3:1 to 106, 4:1 to 
140). 
383
 A: No Query, No 
WordNet 
B: No Query, 
WordNet 
C: No Query, No 
WordNet 
D: Query, Word-
Net 
 Prec Rec F Prec Rec F Prec Rec F Prec Rec F 
Cause-Effect 50.0 51.2 50.6 66.7   73.2 69.8 42.9   36.6   39.5   59.0   56.1   57.5   
Instrument-Agency 47.5 50.0 48.7 73.7   73.7   73.7   51.4   50.0   50.7   67.5   71.1   69.2   
Product-Producer 65.3 51.6 57.7 83.7   66.1   73.9   67.4   50.0   57.4   74.5   61.3   67.3   
Origin-Entity 50.0 27.8 35.7 63.0   47.2   54.0   54.5   33.3   41.4   63.3   52.8   57.6   
Theme-Tool 50.0 27.6 35.6 50.0   48.3   49.1   47.4   31.0   37.5   40.9   31.0   35.3   
Part-Whole 26.5 34,6 30.0 72.4   80.8   76.4   34.0   61.5   43.8   57.1   76.9   65.6   
Content-Container 48.4 39.5 43.5 57.6   50.0   53.5   48.6   44.7   46.6   63.6   55.3   59.2   
Avg for UC3M 48.2 40.3   43.1   66.7 62.8  64.3  49.4   43.9   45.3   60.9   57.8   58.8   
Avg for all systems 59.2 58.7 58.0 65.3 64.4 63.6 59.9 59.0 58.4 64.9 60.4 60.6 
Max Avg F   64.8   72.4   65.1   62.6 
Table  1 Scores for A4, B4, C4 and D4
For some learning algorithms such as decision 
trees and rule learning, appropriate selection of 
features is crucial. For the SVM model, this is 
not so important due to its learning mechanism, 
where irrelevant features are usually balanced 
between positive and negative examples for a 
given binary classification problem. However, in 
the experiments we observed that certain fea-
tures have strong influence on the results, and its 
inclusion or elimination from the vector, influ-
enced remarkably the outcomes. 
In this section, we will briefly discuss the ex-
periments in the four categories highlighting the 
most relevant observations. 
In category A, we expected to obtain better 
results, but the overall performance of the sys-
tem has decreased in the seven relations. This 
shows that our system has over-fitted the train-
ing set. The contrast between the F score values 
in the cross-validation and the final test results 
demonstrates this fact. For all the relations in the 
category A4, we obtained an average of 
F=43.1% [average score of all participating 
teams: F=58.0% and top average score: 
F=64.8%].  
In Product-Producer relation, only two fea-
tures were used: the two heads of the nominals. 
In training, we obtained an average F= 60% us-
ing cross-validation, while in the final test data, 
we achieved an average score F=57.7%. For the 
relation Theme-Tool, other set of features was 
employed: nominals, their heads, verb, preposi-
tion and the list of word between both nominals. 
Based on the results of the 10-fold cross valida-
tion, we expected to obtain an average of the 
F=70%. Nevertheless, the score obtained is F 
=30%.   
In category B, our system has achieved better 
scores. Our average score F is 64.3% and it is 
above the average of participating teams 
(F=63.6%) and the baseline.  
Best results in this category were achieved in 
the relations: Instrument-Agency (F=73.7%), 
Product-Producer (F=73.9%), Part-Whole 
(F=76.4%). However, for the relation Theme-
Tool the system obtained lower scores 
(F=49.1%). 
It is obvious that introducing WordNet infor-
mation has improved notably the results com-
pared with the results obtained in the category 
A.  
In categories C and D, only three groups have 
participated. In category C (as in category A), 
the system results have decreased obviously 
(F=45.3%) with respect to the expected scores in 
the 10-fold cross validation. Moreover, the score 
obtained is lower than the average score of all 
participants (F=58.4%) and the best score 
(F=65.1%). For example, in training the Instru-
ment-Agent relation, the system achieved an 
average F=78% using 10-fold cross-validation, 
while for the final score it only obtained 
F=50.7%.  
Results reveal that the main reason behind the 
low scores in A and C, is the absence of infor-
mation from WordNet. Hence, the vector design 
needs further consideration in case no semantic 
information is provided. 
In category D, both WordNet senses and 
query were used, we achieved an average score 
F=58.8%. The average score for all participants 
is F=60.6% and the best system achieved 
F=62.6%. However, the slight difference shows 
that our system worked relatively well in this 
category.  
384
Both run time and accuracy depend critically 
on the values given to two parameters: the upper 
bound on the coefficient?s values in the equation 
for the hyperplane (-C), and the degree of the 
polynomials in the non-linear mapping (-E) 
(Witten and Frank, 2005). Both are set to 1 by 
default. The best settings for a particular dataset 
can be found only by experimentation.  
We made numerous experiments to find the 
best value for the parameter C (C=1, C=10, 
C=100, C=1000, C=10000), but the results were 
not remarkably affected. Probably, this is due to 
the small size of the training set. 
5 Conclusions and Future Work  
In our first approach to automatic classifica-
tion of semantic relations between nominals and 
as expected from the training phase, our system 
achieved its best performance using WordNet 
information. In general, we obtained better 
scores in category 4 (size of training: 1 to 140), 
i.e., when all the training examples are used.  
On the other hand, overfitting the training 
data (most probably due to the small size of 
training dataset) is the main reason behind the 
low scores obtained by our system. 
These facts lead us to the conclusion that se-
mantic features from WordNet, in general, play 
a key role in the classification task. However, 
the relevance of WordNet-related features var-
ies. For example, lexical file numbers proved to 
be highly effective, while the use of the Word-
Net Vector did not improve significantly the re-
sults. Thus, we consider that a level 3 WordNet 
Vector is rather abstract to represent each nomi-
nal. Developing a WordNet Vector with a deeper 
level (> 3) could be more effective as the repre-
sentation of nouns is more descriptive. 
Query features, on the other hand, did not im-
prove the performance of the system. This is due 
to the fact that the same query could represent 
both positive and negative examples of the rela-
tion. However, to improve results in categories 
A and C, more features need to introduced, es-
pecially context and syntactic information such 
as chunks or dependency relations. 
To improve results across the whole dataset, 
wider use of semantic information is necessary. 
For example, the immediate hypernym for each 
synset obtained from WordNet could help in 
improving the system performance (Nastase et 
al., 2006). Besides, information regarding the 
entity features could help in the classification of 
some relations like Origin-Entity or Product-
Producer. Other semantic resources such as 
VerbNet, FrameNet, PropBank, etc. could also 
be used. 
Furthermore, we consider introducing a Word 
Sense Disambiguation module to obtain the cor-
responding synsets of the nominals. Also, in-
formation concerning the synsets of the list of 
the context words could be of great value for the 
classification task (Wang et al, 2006). 
References 
Hamish Cunningham, Diana Maynard and Kalina 
Bontcheva, Valentin Tablan, Cristian Ursu. 2002. The 
GATE User Guide. http://gate.ac.uk/ 
Roxana Girju, Dan Moldovan, Marta Tatu and Daniel An-
tohe. 2005. On the semantics of noun compunds. Com-
puter Speech and Language 19 pp. 479-496. 
Vivi Nastase, Jelber Sayyad-Shirbad, Marina Sokolova and 
Stan Szpakowicz. 2006. Learning noun-modifier seman-
tic relations with corpus-based and WordNet-based fea-
tures. In Proc. of the 21st National Conference on Artifi-
cial Intelligence (AAAI 2006). Boston, MA. 
John C. Platt. 1998. Sequential Minimal Optimization: A 
Fast Algorithm for Training Support Vector Machines, 
Microsoft Research Technical Report MSR-TR-98-14. 
Barbara Rosario, Marti A. Hearst, and Charles Fillmore. 
2002. ?The descent of hierarchy, and selection in rela-
tions semantics?. In Proceedings of the 40 th Annual 
Meeting of the Association for Computacional Linguis-
tics (ACL?02), Philadelphia, PA, pages 417-424. 
Ian H. Witten, Eibe Frank. 2005. Data Mining: Practical 
machine learning tools and techniques. Morgan Kauf-
mann.  
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic rela-tions. Ma-
chine Learning, in press. 
Ting Wang, Yaoyong Li, Kalina Bontcheva, Hamish Cun-
ningham and Ji Wang. 2006. Automatic Extraction of 
Hierarchical Relations from Text. In Proceedings of the 
Third European Semantic Web Conference (ESWC 
2006), Lecture Notes in Computer Science 4011, 
Springer, 2006. 
385
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 541?545,
Dublin, Ireland, August 23-24, 2014.
Sensible: L2 Translation Assistance by Emulating the Manual
Post-Editing Process
Liling Tan, Anne-Kathrin Schumann, Jose M.M. Martinez and Francis Bond
1
Universit?at des Saarland / Campus, Saarbr?ucken, Germany
Nanyang Technological University
1
/ 14 Nanyang Drive, Singapore
alvations@gmail.com, anne.schumann@mx.uni-saarland.de,
j.martinez@mx.uni-saarland.de, bond@ieee.org
Abstract
This paper describes the Post-Editor Z sys-
tem submitted to the L2 writing assis-
tant task in SemEval-2014. The aim of
task is to build a translation assistance
system to translate untranslated sentence
fragments. This is not unlike the task
of post-editing where human translators
improve machine-generated translations.
Post-Editor Z emulates the manual pro-
cess of post-editing by (i) crawling and ex-
tracting parallel sentences that contain the
untranslated fragments from a Web-based
translation memory, (ii) extracting the pos-
sible translations of the fragments indexed
by the translation memory and (iii) apply-
ing simple cosine-based sentence similar-
ity to rank possible translations for the un-
translated fragment.
1 Introduction
In this paper, we present a collaborative submis-
sion between Saarland University and Nanyang
Technological University to the L2 Translation As-
sistant task in SemEval-2014. Our team name
is Sensible and the participating system is Post-
Editor Z (PEZ).
The L2 Translation Assistant task concerns the
translation of an untranslated fragment from a par-
tially translated sentence. For instance, given a
sentence, ?Ich konnte B?arbel noch on the border
in einen letzten S-Bahn-Zug nach Westberlin set-
zen.?, the aim is to provide an appropriate transla-
tion for the underline phrase, i.e. an der Grenze.
The aim of the task is not unlike the task of
post-editing where human translators correct er-
rors provided by machine-generated translations.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
The main difference is that in the context of post-
editing the source text is provided. A transla-
tion workflow that incorporates post-editing be-
gins with a source sentence, e.g. ?I could still
sit on the border in the very last tram to West
Berlin.? and the human translator is provided with
a machine-generated translation with untranslated
fragments such as the previous example and some-
times ?fixing? the translation would simply re-
quire substituting the appropriate translation for
the untranslated fragment.
2 Related Tasks and Previous
Approaches
The L2 writing assistant task lies between the
lines of machine translation and crosslingual word
sense disambiguation (CLWSD) or crosslingual
lexical substitution (CLS) (Lefever and Hoste,
2013; Mihalcea et al. 2010).
While CLWSD systems resolve the correct
semantics of the translation by providing the
correct lemma in the target language, CLS at-
tempts to provide also the correct form of the
translation with the right morphology. Machine
translation tasks focus on producing translations
of whole sentences/documents while crosslingual
word sense disambiguation targets a single lexical
item.
Previously, CLWSD systems have tried distri-
butional semantics and string matching methods
(Tan and Bond, 2013), unsupervised clustering of
word alignment vectors (Apidianaki, 2013) and
supervised classification-based approaches trained
on local context features for a window of three
words containing the focus word (van Gompel,
2010; van Gompel and van den Bosch, 2013; Rud-
nick et al., 2013). Interestingly, Carpuat (2013)
approached the CLWSD task with a Statistical MT
system .
Short of concatenating outputs of CLWSD /
CLS outputs and dealing with a reordering issue
541
and responding to the task organizers? call to avoid
implementing a full machine translation system
to tackle the task, we designed PEZ as an Auto-
matic Post-Editor (APE) that attempts to resolve
untranslated fragments.
3 Automatic Post-Editors
APEs target various types of MT errors from de-
terminer selection (Knight and Chander, 1994) to
grammatical agreement (Mare?cek et al., 2011).
Untranslated fragments from machine translations
are the result of out-of-vocabulary (OOV) words.
Previous approaches to the handling of un-
translated fragments include using a pivot lan-
guage to translate the OOV word(s) into a third
language and then back into to the source lan-
guage, thereby extracting paraphrases to OOV
(Callison-burch and Osborne, 2006), combining
sub-lexical/constituent translations of the OOV
word(s) to generate the translation (Huang et al.,
2011) or finding paraphrases of the OOV words
that have available translations (Marton et al.,
2009; Razmara et al., 2013).
1
However the simplest approach to handle un-
translated fragments is to increase the size of par-
allel data. The web is vast and infinite, a human
translator would consult the web when encounter-
ing a word that he/she cannot translate easily. The
most human-like approach to post-editing a for-
eign untranslated fragment is to do a search on
the web or a translation memory and choose the
most appropriate translation of the fragment from
the search result given the context of the machine
translated sentence.
4 Motivation
When post-editing an untranslated fragment, a hu-
man translator would (i) first query a translation
memory or parallel corpus for the untranslated
fragment in the source language, (ii) then attempt
to understand the various context that the fragment
can occur in and (iii) finally he/she would sur-
mise appropriate translations for the untranslated
fragment based on semantic and grammatical con-
straints of the chosen translations.
1
in MT, evaluation is normally performed using automatic
metrics based on automatic evaluation metrics that compares
scores based on string/word similarity between the machine-
generated translation and a reference output, simply remov-
ing OOV would have improved the metric ?scores? of the
system (Habash, 2008; Tan and Pal, 2014).
The PEZ system was designed to emulate the
manual post-editing process by (i) first crawling
a web-based translation memory, (ii) then extract-
ing parallel sentences that contain the untranslated
fragments and the corresponding translations of
the fragments indexed by the translation memory
and (iii) finally ranking them based on cosine sim-
ilarity of the context words.
5 System Description
The PEZ system consists of three components,
viz (i) a Web Translation Memory (WebTM)
crawler, (ii) the XLING reranker and (iii) a longest
ngram/string match module.
5.1 WebTM Crawler
Given the query fragment and the context sen-
tence, ?Die Frau kehrte alone nach Lima zur?uck?,
the crawler queries www.bab.la and returns
sentences containing the untranslated fragment
with various possible tranlsations, e.g:
? isoliert : Darum sollten wir den Kaffee nicht
isoliert betrachten.
? alleine : Die Kommission kann nun aber f?ur
ihr Verhalten nicht alleine die Folgen tragen.
? Allein : Allein in der Europischen Union
sind.
The retrieval mechanism is based on the
fact that the target translations of the queried
word/phrase are bolded on a web-based TM and
thus they can be easily extracted by manipulating
the text between <bold>...</bold> tags. Al-
though the indexed translations were easy to ex-
tract, there were few instances where the transla-
tions were embedded betweeen the bold tags on
the web-based TM.
5.2 XLING Reranker
XLING is a light-weight cosine-based sentence
similarity script used in the previous CLWSD
shared task in SemEval-2013 (Tan and Bond,
2013). Given the sentences from the WebTM
crawler, the reranker first removes all stopwords
from the sentences and then ranks the sentences
based on the number of overlapping stems.
In situations where there are no overlapping
content words from the sentences, XLING falls
back on the most common translation of the un-
translated fragment.
542
en-de en-es fr-en nl-en
acc wac rec acc wac rec acc wac rec acc wac rec
WebTM 0.160 0.184 0.647 0.145 0.175 0.470 0.055 0.067 0.210 0.092 0.099 0.214
XLING 0.152 0.178 0.647 0.141 0.171 0.470 0.055 0.067 0.210 0.088 0.095 0.214
PEZ 0.162 0.233 0.878 0.239 0.351 0.819 0.081 0.116 0.321 0.115 0.152 0.335
Table 1: Results for Best Evaluation of the System Runs.
5.3 Longest Ngram/String Matches
Due to the low coverage of the indexed trans-
lations on the web TM, it is necessary to ex-
tract more candidate translations. Assuming lit-
tle knowledge about the target language, human
translator would find parallel sentences containing
the untranslated fragment and resort to finding re-
peating phrases that occurs among the target lan-
guage sentences.
For instance, when we query the phrase history
book from the context ?Von ihr habe ich mehr gel-
ernt als aus manchem history book.?, the longest
ngram/string matches module retrieves several tar-
get language sentences without any indexed trans-
lation:
? Ich weise darauf hin oder nehme an, dass
dies in den Geschichtsb?uchern auch so
erw?ahnt wird.
? Wenn die Geschichtsb?ucher geschrieben wer-
den wird unser Zeitalter, denke ich, wegen
drei Dingen erinnert werden.
? Ich bin sicher, Pr?asident Mugabe hat sich
nun einen Platz in den Geschichtsb?uchern
gesichert, wenn auch aus den falschen
Gr?unden.
? In den Geschichtsb?uchern wird f?ur jeden
einzelnen Tag der letzten mehr als 227 Jahre
an Gewalttaten oder Tragdien auf dem eu-
rop?aischen Kontinent erinnert.
By simply spotting the repeating word/string
from the target language sentences it is pos-
sible to guess that the possible candidates
for ?history book? are Geschichtsb?ucher or
Geschichtsb?uchern. Computationally, this can
be achieved by looking for the longest matching
ngrams or the longest matching string across the
target language sentences fetched by the WebTM
crawler.
5.4 System Runs
We submitted three system runs to the L2 writing
assistant task in Semeval-2014.
1. WebTM: a baseline configuration which out-
puts the most frequent indexed translation of
the untranslated fragment from the Web TM.
2. XLING: reranks the WebTM outputs based
on cosine similarity.
3. PEZ: similar to the XLING but when the
WebTM fetches no output, the system looks
for longest common substring and reranks the
outputs based on cosine similarity.
6 Evaluation
The evaluation of the task is based on three met-
rics, viz. absolute accuracy (acc), word-based ac-
curacy (wac) and recall (rec).
Absolute accuracy measures the number of
fragments that match the gold translation of the
untranslated fragments. Word-based accuracy as-
signs a score according to the longest consecutive
matching substring between output fragment and
reference fragment; it is computed as such:
wac =
|longestmatch(output,reference)|
max(|output|,|reference|)
Recall accounts for the number of fragments for
which output was given (regardless of whether it
was correct).
7 Results
Table 1 presents the results for the best evalua-
tion scores of the PEZ system runs for the En-
glish to German (en-de), English to Spanish (en-
es), French to English (fr-en) and Dutch to English
(nl-en) evaluations. Figure 1 presents the word ac-
curacy of the system runs for both best and out-of-
five (oof) evaluation
2
.
The results show that using the longest
ngram/string improves the recall and subsequently
the accuracy and word accuracy of the system.
However, this is not true when guessing untrans-
lated fragments from L1 English to L2. This is
due to the low recall of the system when search-
ing for the untranslated fragment in French and
2
Please refer to http://goo.gl/y9f5Na for results
of other competing systems
543
Figure 1: Word Accuracy of System Runs (best on
the left, oof on the right).
Dutch, where the English words/phases indexed in
the TM is much larger than other languages.
8 Error Analysis
We manually inspected the English-German out-
puts from the PEZ system and identified several
particularities of the outputs that account for the
low performance of the system for this language
pair.
8.1 Weird Expressions in the TM
When attempting to translate Nevertheless in the
context of ?Nevertheless hat sich die neue Bun-
desrepublik Deutschland unter amerikanischem
Druck an der militrischen Einmischung auf dem
Balkan beteiligt.? where the gold translation is
Trotzdem or Nichtsdestotrotz. The PEZ system re-
trieves the following sentence pairs that contains a
rarely used expression nichtsdestoweniger from a
literally translated sentence pair in the TM:
? EN: But nevertheless it is a fact that nobody
can really recognize their views in the report.
? DE: Aber nichtsdestoweniger kann sich nie-
mand so recht in dem Bericht wiederfinden.
Another example of weird expression is when
translating ?husband? in the context of ?In der
Silvesternacht sind mein husband und ich auf die
Bahnhofstra?e gegangen.?. PEZ provided a lesser
use yet valid translation Gemahl instead of the
gold translation Mann. In this case, it is also a
matter of register where in a more formal register
one will use Gemahl instead of Mann.
8.2 Missing / Additional Words from
Matches
When extracting candidate translations from the
TM index or longest ngram/string, there are sev-
eral matches where the PEZ system outputs a par-
tial phrase or phrases with additional tokens that
cause the disparity between the absolute accuracy
and word accuracy. An instance of missing words
is as follows:
? Input: Eine genetische Veranlagung
plays a decisive role.
? PEZ: Eine genetische Veranlagung
eine entscheidende rolle.
? Gold: Eine genetische Veranlagung
spielt (dabei) eine entscheidende rolle.
For the addition of superfluous words is as fol-
lows:
? Input: Ger?ate wie Handys sind
not permitted wenn sie nicht unterrichtlichen
Belangen dienen.
? PEZ: Ger?ate wie Handys sind es verboten,
wenn sie nicht unterrichtlichen Belangen
dienen.
? Gold: Ger?ate wie Handys sind verboten
wenn sie nicht unterrichtlichen Belangen di-
enen.
8.3 Case Sensitivity
For the English-German evaluation , there are sev-
eral instances where the PEZ system produces the
correct translation of the phrase but in lower cases
and this resulted in poorer accuracy. This is unique
to German target language and possibly contribut-
ing to the lower scores as compared to the English-
Spanish evaluation.
9 Conclusion
In this paper, we presented the PEZ automatic
post-editor system in the L2 writing assistant task
in SemEval-2014. The PEZ post-editing system is
a resource lean approach to provide translation for
untranslated fragments based on no prior training
data and simple string manipulations from a web-
based translation memory.
The PEZ system attempts to emulate the pro-
cess of a human translator post-editing out-
of-vocabulary words from a machine-generated
544
translation. The best configuration of the PEZ sys-
tem involves a simple string search for the longest
common ngram/string from the target language
sentences without having word/phrasal alignment
and also avoiding the need to handle word reorder-
ing for multi-token untranslated fragments.
Acknowledgements
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union?s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement n
?
317471.
References
Marianna Apidianaki. 2013. Limsi : Cross-lingual
word sense disambiguation using translation sense
clustering. In Second Joint Conference on Lexi-
cal and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
178?182, Atlanta, Georgia, USA, June.
Chris Callison-burch and Miles Osborne. 2006. Im-
proved statistical machine translation using para-
phrases. In In Proceedings of HLT/NAACL-2006,
pages 17?24.
Marine Carpuat. 2013. Nrc: A machine translation ap-
proach to cross-lingual word sense disambiguation
(semeval-2013 task 10). In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 188?192, Atlanta, Georgia, USA, June.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In ACL, pages 57?
60.
Chung-Chi Huang, Ho-Ching Yen, Ping-Che Yang,
Shih-Ting Huang, and Jason S. Chang. 2011. Using
sublexical translations to handle the oov problem in
machine translation. ACM Trans. Asian Lang. Inf.
Process., 10(3):16.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In AAAI, pages 779?784.
David Mare?cek, Rudolf Rosa, Petra Galu?s?c?akov?a, and
Ond?rej Bojar. 2011. Two-step translation with
grammatical post-processing. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
WMT ?11, pages 426?432, Stroudsburg, PA, USA.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
EMNLP, pages 381?390.
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1105?
1115, Sofia, Bulgaria, August.
Alex Rudnick, Can Liu, and Michael Gasser. 2013.
Hltdi: Cl-wsd using markov random fields for
semeval-2013 task 10. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 171?177, Atlanta, Georgia, USA, June.
Liling Tan and Francis Bond. 2013. Xling: Match-
ing query sentences to a parallel corpus using topic
models for wsd. In Second Joint Conference on Lex-
ical and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
167?170, Atlanta, Georgia, USA, June.
Liling Tan and Santanu Pal. 2014. Manawi: Using
multi-word expressions and named entities to im-
prove machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
Baltimore, USA, August.
Maarten van Gompel and Antal van den Bosch. 2013.
Wsd2: Parameter optimisation for memory-based
cross-lingual word-sense disambiguation. In Second
Joint Conference on Lexical and Computational Se-
mantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 183?187, Atlanta, Geor-
gia, USA, June.
Maarten van Gompel. 2010. Uvt-wsd1: A cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, SemEval ?10, pages 238?241,
Stroudsburg, PA, USA.
545
