Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 505?513,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Improved Sentence A lignment on Parallel Web Pages Using a  
Stochastic T ree A lignment Model 
Lei Shi 
Microsoft Research Asia 
5F Sigma Center, 49 Zhichun Road, Beijing 
100190, P. R. China 
leishi@microsoft.com?
Ming Zhou 
Microsoft Research Asia 
5F Sigma Center, 49 Zhichun Road, Beijing 
100190, P. R. China 
mingzhou@microsoft.com?
 
 
 
 
Abstract 
Parallel web pages are important source 
of training data for statistical machine 
translation. In this paper, we present a 
new approach to sentence alignment on 
parallel web pages. Parallel web pages 
tend to have parallel structures?and the 
structural correspondence can be indica-
tive information for identifying parallel 
sentences. In our approach, the web page 
is represented as a tree, and a stochastic 
tree alignment model is used to exploit 
the structural correspondence for sentence 
alignment. Experiments show that this 
method significantly enhances alignment 
accuracy and robustness for parallel web 
pages which are much more diverse and 
noisy than standard parallel corpora such 
as ?Hansard?. With improved sentence 
alignment performance, web mining sys-
tems are able to acquire parallel sentences 
of higher quality from the web. 
1 Introduction 
Sentence-aligned parallel bilingual corpora have 
been essential resources for statistical machine 
translation (Brown et al 1993), and many other 
multi-lingual natural language processing applica-
tions. The task of aligning parallel sentences has 
received considerable attention since the renais-
sance of data driven machine translation in late 
1980s.  
During the past decades, a number of methods 
have been proposed to address the sentence align-
ment problem. Although excellent performance 
was reported on clean corpora, they are less robust 
with presence of noise. A recent study by (Singh 
and Husain 2005) completed a systematic evalua-
tion on different sentence aligners under various 
conditions. Their experiments showed that the per-
formance of sentence aligners are sensitive to 
properties of the text, such as format complexity 
(presence of elements other than text), structural 
distance (a scale from literal to free translation), 
the amount of noise (text deletions or preprocess-
ing errors) and typological distance between lan-
guages. Their performance varies on different type 
of texts and they all demonstrate marked perfor-
mance degradation over noisy data. The results 
suggest that there is currently no universal solution 
to sentence alignment under all conditions, and 
different methods should be applied to different 
types of texts. 
In this paper, we specifically address sentence 
alignment on parallel web pages. It has come to 
attention with the increasing trend of acquiring 
large-scale parallel data from the web. Currently, 
large-scale parallel data are not readily available 
for most language pairs and domains. But due to a 
sharply increasing number of bilingual web sites, 
web mining shows great promise as a solution to 
this knowledge bottleneck problem. Many systems 
(Ma 1999; Chen 2000; Yang 2002; Resnik 2003; 
Chen 2004) have been developed to discover paral-
lel web pages, and sentence aligners are used to 
extract parallel sentences from the mined web cor-
pora. Sentence alignment performance on parallel 
web pages, therefore, becomes an increasingly im-
portant issue for large-scale high-quality parallel 
data acquisition.  
Compared with clean parallel corpora such as 
"Hansard" (Brown et al 1993), which consists of 
505
French-English translations of political debates in 
the Canadian parliament, texts from the web are far 
more diverse and noisy. They are from many dif-
ferent domains and of various genres. Their trans-
lation may be non-literal or written in disparate 
language pairs. Noise is abundant with frequent 
insertions, deletions or non-translations. And there 
are many very short sentences of 1-3 words. Due 
to the characteristics of web corpora, direct appli-
cation of conventional alignment methods without 
exploiting additional web document information 
yields unsatisfactory alignment results. 
Our approach to this problem is to make use of 
the structural parallelism between parallel web 
pages. Structural parallelism is the phenomenon, 
that when representing the same content in two 
different languages, authors have a very strong 
tendency to use the same document structure. As is 
shown in Figure 1, sentences located in similar 
position on both pages are more likely to be trans-
lations. Hence, correspondence in the web page 
structure is an informative indication of parallel 
sentences. In our approach, the web page is 
represented as a tree, and a stochastic tree align-
ment model is used to find the most probable 
alignment of the tree pair based on their structure 
and the texts in tree nodes. The tree alignment then 
acts as useful information to constrain the scope of 
search for parallel sentences.  
The paper is organized as follows: In section 2, 
we briefly survey previous approaches to sentence 
alignment. In section 3, we present the stochastic 
tree alignment model, including parameter estima-
tion and decoding. Then in section 4, we describe 
how to use the tree alignment model in sentence 
alignment. Benchmarks are shown in section 5, 
and the paper is concluded in section 6. 
2 Sentence A lignment Models 
Sentence alignment methods can be categorized 
into three major categories: the length-based, lex-
icon-based and hybrid method which combines the 
length-based model and lexicon-based model as 
complement to each other.  
The length model was based on the intuition that 
the length of a translated sentence is likely to be 
similar to that of the source sentence. (Brown et. at. 
1991) used word count as the sentence length, 
whereas (Gale and Church 1993) used character 
count. Dynamic programming is used to search the 
optimal sentence alignment. Both algorithms have 
achieved remarkably good results for language 
pairs like English-French and English-German 
 
Figure 1. Example of parallel web pages 
506
with an error rate of 4% on average. But they are 
not robust with respect to non-literal translations, 
deletions and disparate language pairs.   
Unlike the length-based model, which totally 
ignores word identity, lexicon-based methods use 
lexical information to align parallel sentences. 
Kay?s? (Kay and Roscheisen 1993) approach is 
based on the idea that words that are translations of 
each other will have similar distribution in source 
and target texts. By adopting the IBM model 1, 
(Chen 1993) used word translation probabilities, 
which he showed gives better accuracy than the 
sentence length based method. Melamed (Me-
lamed 1996) rather used word correspondence 
from a different perspective as geometric corres-
pondence for sentence alignment.  
The hybrid method combines the length model 
with the lexical method. (Simard and Plamondon 
1996) used a two-pass approach, where the first 
pass performs length-based alignment at the cha-
racter level as in (Gale and Church 1993) and the 
second pass uses IBM Model 1, following (Chen 
1993). Moore?s? (Moore 2002) approach is similar 
to Simard?s. The difference is that Moore used the 
data obtained in the first pass to train the IBM 
model in the second pass, so that his approach does 
not require a priori knowledge about the language 
pair. Instead of using a two-pass approach, (Zhao 
and Vogel 2002) combines the length model and 
the IBM model 1 in a unified framework under a 
maximum likelihood criterion. To make it more 
robust on noisy text, they developed a background 
model to handle text deletions.  
To further improve sentence alignment accuracy 
and robustness, methods that make use of addi-
tional language or corpus specific information 
were developed. In Brown and? Church?s length-
based aligner, they assume prior alignment on 
some corpus specific anchor points to constrain 
and keep the Viterbi search on track. (Wu 1994) 
implemented a length-based model for Chinese-
English with language specific lexical clues to im-
prove accuracy. (Simard et al 1992) used cognates, 
which only exists in closely related language pairs. 
(Chuang and Yeh 2005) exploited the statistically 
ordered matching of punctuation marks in two lan-
guages to achieve high accuracy sentence align-
ment. In their web parallel data mining system, 
(Chen and Nie 2000) used HTML tags in the same 
way as cognates in (Simard et al 1992) for align-
ing Chinese-English parallel sentences. Tree based 
alignment models have been successfully applied 
in machine translation (Wu 1997, Yamada & 
Knight 2001, Gildea 2003).  
3 The Stochastic T ree A lignment Model 
The structure of the HTML document is recursive, 
with HTML markup tags embedded within other 
markup tags. While converting an HTML docu-
ment into the tree representation, such hierarchical 
order is maintained. Each node of the tree is la-
beled with their corresponding HTML tag (e.g. 
body, title, img etc.) and in labeling tree nodes, 
only markup tags are used and attribute value pairs 
are dropped. Among all markup tags in the HTML 
file, those of our most interest are tags containing 
content text, which is what we want to align. These 
tags are those surrounding a text chunk or have the 
attribute of ?ALT?. Comments, scripts and style 
specifications are not regarded as content text and 
hence are eliminated. Figure 2 illustrates the tree 
representation of an example HTML document. 
html
head body
a
#text2
divtitle
#text1
Img
#text3. . .
<html>
<head><title>text1</title></head>
<body>
<a href=?.html?>text2</a>
<div> ??</div>
<img src=?.jpg?,alt=text3>
</body>
</html>
 
Figure. 2 An example HTML document and its 
tree representation 
 
3.1 T ree A lignment Model 
Given two trees, the tree alignment is the non-
directional alignments of their nodes. A node in 
one tree can be aligned with at most one node in 
the other tree. It is valid for a node to be aligned 
with nothing (NULL) and such case is regarded as 
node deletion in tree alignment. To comply with 
the tree hierarchical structure, we constrain that the 
alignments keep the tree hierarchy invariant i.e. if 
node A is aligned with node B, then the children of 
A are either deleted or aligned with the children of 
B. Besides, to simplify the model training and de-
coding, the tree alignment model also keeps the 
507
sequential order invariant, i.e. if node A is aligned 
with node B, then the left sibling nodes of A cannot 
be aligned with the right sibling nodes of B.  
The stochastic tree alignment model assigns 
probabilities to tree alignments, based on the par-
ticular configuration of the alignment and model 
parameters. Then, the decoder is able to find the 
most probable (optimal) alignment of two trees. To 
facilitate the presentation of the tree alignment 
model, the following symbols are introduced: giv-
en a HTML document D, ??denotes the corres-
ponding tree; ??? denotes the ith node of ??? , 
and???? ?denotes the sub-tree rooted at???? . Espe-
cially, ?1? ?is the root of the tree??? . ?[? ,? ]? ?denotes 
the forest consisting of the sub-trees rooted at sibl-
ing nodes from ??? to????. ??? . ? denotes the text in 
the node ??? , and ??? . ?  denotes the label (i.e. 
HTML tag) of the node????; ??? .?? denotes the jth 
child of the node ????; ??? .?[? ,?] denotes the con-
secutive sequence of ???? ?s? children? nodes? from?
??? .??  to???? .?? ; the sub-tree rooted at ??? .??  is 
represented as ??? .???   and the forest of the sub-
trees rooted at ???? ?s? children? is? represented? as 
???? .??. To accommodate node deletion, NULL is 
introduced to denote the empty node. Finally, the 
tree alignment is referred as A. 
Given two HTML documents F (in French) and 
E (in English) represented as trees ??and???, the 
tree alignment task is defined as finding the align-
ment A that maximizes the conditional 
probability ?Pr(?|?? ,??) . Based on the Bayes? 
Rule, Pr(?|?? ,??) ? ?Pr(?? ,??|?)Pr(?) , where 
Pr(?? ,??|?) is the probability of synchronously 
generating ?? and ?? given the alignment A, and 
Pr(?) is the prior knowledge of the tree alignment. 
To simplify computation, we assume a uniform 
prior probability Pr(?). Hence, the tree alignment 
task is to find the A that maximizes the synchron-
ous probability?Pr(?? ,??|?). 
Based on the hierarchical structure of the tree, in 
order to facilitate the presentation and computation 
of the tree alignment probabilistic model, the fol-
lowing alignment probabilities are defined in a hie-
rarchically recursive manner: 
Pr(??? ,???|?) : The probability of synchronously 
generating sub-tree pair {??? ,???} given the align-
ment A; 
Pr(??? ,???|?): The probability of synchronously 
generating node pair?{??? ,???};  
Pr(?[? ,?]? ,?[? ,? ]? |?) : The probability of synchron-
ously generating forest pairs {?[? ,?]? ,?[? ,? ]? }  given 
the alignment A.  
From the definition, the tree pair generative 
probability ?Pr(?? ,??|?)  equals to the root sub-
tree pair generative probability?Pr(?1? ,?1?|?). The 
alignment of the sub-tree pair ???and ???  may have 
the following configurations, based on which the 
tree pair generative probability Pr(??? ,???|?)  can 
be calculated: 
(1) If???? ?is aligned with ??? , and the children of 
??? ? are aligned with children of ??? (as is 
shown in Fig. 3a), then we have 
 
?Pr???? ,?????? = Pr(??? ,???)Pr(??? .??,??? .??|?) 
 
(2) If ???? is deleted, and the children of ???  is 
aligned with ???  (as shown in Fig. 3b), then we 
have 
 
Pr???? ,?????? = Pr(??? |????)Pr(??? .??,???|?) 
 
(3) If ???is deleted, and????  is aligned with child-
ren of ???  (as shown in Fig. 3c), then we have 
 
Pr(??? ,???|?) = Pr(??? ,??? .??|?)Pr(???|????) 
 
(a)
(b)
NULL
(c)
NULL
FmT FmTFmT EiT EiT EiT
 
Figure. 3 
 
The above equations involve forest pair generative 
probabilities. The alignment of the forest 
?[? ,?]? ?and ?[? ,? ]?  may have the following configura-
tions, based on which their forest pair generative 
probability?Pr(?[? ,?]? ,?[? ,? ]? |?)?can be calculated: 
508
(4) If ???  is aligned with ??? , and ?[?+1,?]?  is 
aligned with ?[?+1,? ]?  (as is shown in Fig. 4a), 
then 
  
Pr??[? ,?]? ,?[?,? ]? ???
= Pr(??? ,???|?)Pr(?[?+1,?]? ,?[?+1,? ]? |?) 
 
(5) If????  is deleted, and the forest rooted at???? ?s?
children ??? .???is combined with ?[?+1,?]?  for 
alignment with ?[? ,? ]? , then 
 
Pr??[? ,?]? ,?[?,? ]? ???
= Pr(??? |????)Pr(??? .????[?+1,?]? ,?[?,? ]? |?) 
 
(6) If????  is deleted, and the forest rooted at?????s?
children ???? .??  is combined with ?[? ,? ]?  for 
alignment with ?[? ,?]? , then 
 
Pr??[? ,?]? ,?[?,? ]? ???
= Pr(???|????)Pr(?[? ,?]? ,??? .????[?+1,? ]? |?) 
      
(a)
E j]1,[iT ?Ei?F n]1,[mT ?Fm?
(b)
Ej][i,TF n]1,[mT ?Fm? .CFTFm
(c)
Ei?F n][m,T E j]1,[iT ?.CFTEiNULLNULL  
Figure. 4 
 
Finally, the node pair probability is modeled 
as ?Pr(??? ,???) = Pr(??? . ?,??? . ?)Pr(??? . ?,??? . ?) , 
where Pr(??? . ?,??? . ?)?is the generative probability 
of the translationally equivalent text chunks 
in???? ?and???? , and Pr(??? . ?,??? . ?)? is their HTML 
tag pair probability. The text chunk generative 
probability Pr(??? . ?,??? . ?)  can be modeled in a 
variety of ways. The conventional length-based, 
lexicon-based or hybrid methods used for sentence 
alignment can be applied here. In the next sub-
section, we focus on how to estimate the tag pair 
probability?Pr(??? . ?,??? . ?)? from a set of parallel 
web pages. We expect pairs of the same or similar 
HTML tags to have high probabilities and the 
probabilities for pairs of disparate tags to be low. 
3.2 Parameter Estimation Using Expectation-
Maximization 
One way to estimate the tag pair generative proba-
bility Pr(?, ??)? is to manually align nodes between 
parallel trees, and use the manually aligned trees as 
the training data for maximum likelihood estima-
tion. However, this is a time-consuming and error-
prone procedure. Instead, the Expectation Maximi-
zation (EM) (Dempster, Laird and Rubin 1977) 
algorithm is used to estimate the 
parameters ?Pr(?, ??)? on 5615 manually verified 
parallel web page pairs from 45 different bilingual 
web sites. The parameter estimation proceeds as 
follows:  
 
1. Start with initial parameter values. 
2. Expectation: estimate count???????(?, ?? )?which is 
the expectation of aligning tag l with 'l . 
3. Maximization: update the parameters based 
to maximum likelihood estimation 
? ? ? ?? ???
'
'
'
'
,
,,Pr
l
llcount
llcountll   and 
? ? ? ?
? ? ? ?? ?
??
'
],,[
,,
)Pr(
''
l
NULLlcountlNULLcount
NULLlcountlNULLcount
NULLl
 
4. Repeat step 2 and 3 until the parameters 
stabilize  
 
In step 2, count???????(?, ?? ) is the expected count of l  
being aligned with 'l  in the training corpus. By 
definition, count???????(?, ?? ) is calculated as  
count???????(?, ?? ) = ?Pr(?|??
?
,??)count(?, ?? ) 
where  count(?, ?? ) is the number of occurrence of l 
being aligned with l? in the tree alignment A. 
To efficiently compute count???????(?, ?? )  without 
enumerating?the?exponential?number?of?A?s?in?the?
above equation, we extended the inside-outside 
algorithm presented in (Lari and Young, 1990). 
The inside probability ?(??? ,???) is defined as the 
509
probability of generating sub-tree pair {??? ,???} 
when ???  is aligned with???? . It is estimated as: 
 ? ? ? ? ? ?CFNCFNNNNN EiFmEiFmEiFm .,.,Pr, ?? ?  
 
where ?(??? .??,??? .??) is the inside probability 
for the forest pair?(??? .??,??? .??)  
 ? ? ? ???
A
E
i
F
m
E
i
F
m ACFNCFNCFNCFN .,.Pr.,.? .  
The inside probability can be estimated recursively 
according to the various alignment configurations 
presented in Figure 3 and Figure 4.  The outside 
probability??(??? ,???) is defined as the probability 
of generating the part of ??and ?? ?excluding the 
sub-trees ???and???? , when ???  is aligned with???? . 
It is estimated as: 
 ? ? ? ?
? ? ? ?? ?
? ? ? ?? ?
? ?
?
? ?
?
?
?
?
qk pk
E
ki
F
km
E
i
E
pi
F
m
F
qm
E
i
E
pi
F
m
F
qm
qp
E
pi
F
qm
E
i
F
m
NULLaNULLa
NRC FaNRC Fa
NLC FaNLC Fa
aaNN
])|Pr()|Pr(
.,.
.,.
,[,
,,
,,
,,
,
,,
?
?
??
 
where ?? ,??  is the qth  ancestor of ??? , and ?? ,??  is 
the pth ancestor of ??? . ?? ,?? (? < ?) is an ancestor 
of ??? ?and a decedent of ??? ,?? . Similarly ?? ,?? (? <
?? is an ancestor of ???, and a decedent of ??,??. 
a.LC F(N) is the forest rooted at a and to the left of 
N, and a.RC F(N). a.RC F(N) is the forest rooted as 
a and to the right of N. Once inside and outside 
probabilities are computed, the expected counts 
can be calculated as 
? ? ? ? ? ?
? ?? ?
?
?
?
EF
F
m
E
iTT
llN
llN
EF
E
i
F
m
E
i
F
m
TT
NNNNllcount
,
.
.
'
' ),Pr(
,,
,
??  
where Pr(?? ,??)?is the generative probability of 
the tree pair {?? ,??} over all possible alignment 
configurations. Pr(?? ,??)?can be estimated using 
dynamic programming techniques that will be pre-
sented in the next sub-section. Furthermore, the 
expected count of tag deletion is estimated as: 
count???????(?,????) = ? count(?, ?? )
?
? ? count???????(?, ?? )
??????
 
count???????(????, ?) = ? count(?? , ?)
?
? ? count???????(?? , ?)
??????
 
3.3 Dynamic Programming for Decoding 
An intuitive way to find the optimal tree alignment 
is to enumerate all alignments and pick the one 
with the highest probability. But it is intractable 
since the total number of alignments is exponential. 
Based on the observation that if two trees are op-
timally aligned, the alignment of their sub-trees 
must also be optimal, dynamic programming can 
be applied to find the optimal tree alignment using 
that of the sub-trees in a bottom-up manner. That is 
we first compute the optimal alignment probabili-
ties of small trees and use them to compute that of 
the bigger tree by trying different alignment confi-
gurations. This procedure is recursive until the op-
timal alignment probability of the whole tree is 
obtained. The following is the pseudo-code of the 
bottom-up decoding algorithm:  
 
where |??| and |??| are the number of nodes in 
??and ?? . The decoding algorithm finds the op-
timal alignment and its probability for every sub-
trees and forests. By replacing the selection opera-
tion with summing probabilities of all configura-
tions, the sub-tree pair generative probability 
Pr(?? ,??) can be calculated along the way. The 
worst-case time complexity of the algorithm 
is ??(|??||??|?????(??)+ ????(??)?2) , where 
the degree of a tree is defined as the largest degree 
of its nodes.  
4 Sentence A lignment with T ree A lign-
ment Model 
Since the tree alignment model aligns parallel web 
pages at the tree node level instead of the sentence 
level, we integrate the tree alignment model with 
the sentence alignment model in a cascaded mode, 
in which the whole sentence alignment process is 
divided into two steps. In the first step, the tree 
alignment decoder finds the optimal alignment of 
the two trees. Nodes having texts should be aligned 
with nodes containing their translations. Then in 
the second step, the conventional sentence aligner 
is used to align sentences within text chunks in the 
for i=|??| to 1 (bottom-up) { 
for j=|??|?to 1 (bottom-up) { 
Select and store optimal alignments of their children fo-
rests ???? . CF?and ???? . CF 
by testing configurations 4-6; 
Select and store the optimal alignment of the sub-tree 
pair ????  and???? ?by testing configurations 1-3; 
Store the optimal configuration}} 
510
aligned nodes. In this step, various sentence align-
ment models can be applied, including the length-
based model, the lexicon-based model and the hy-
brid model. Language or corpus specific informa-
tion may also be used to further improve sentence 
alignment accuracy. The tree alignment acts as 
constraints that confine the scope of the search of 
sentence aligners.  
5 Evaluation 
To evaluate the effectiveness of exploiting web 
page document structure with the tree alignment 
model for improving sentence alignment accuracy, 
we compared the performance of three types of 
sentence alignment methods on parallel web pages.  
The first type is to simply discard web page 
layout information. Web pages are converted to 
plain texts, and HTML tags are removed prior to 
performing sentence alignment. The second type is 
the baseline method of using web page document 
information. Instead of exploiting full HTML doc-
ument structure, it follows Chen?s?approach?(Chen 
and Nie 2000) which uses HTML tags in the same 
way as cognates used in (Simard et al 1992). The 
third type is the combination of tree alignment 
model and conventional sentence models.  
Each type of the web page sentence aligner 
makes use of three conventional sentence align-
ment models, one is the length based model fol-
lowing (Brown 1991), one is the lexicon based 
model following (Chen 1993), and the other one is 
the hybrid model presented in (Zhao 2002). To be 
fair in performance comparisons, the text genera-
tive probability Pr(?? . ?,?? . ?)  in tree node 
alignment is modeled in accordance with that in 
the sentence alignment model. All these sentence 
aligners are implemented to handle sentence bead 
types?of??1-0?,??0-1?,?1-1?,??1-2?,?1-3?,?2-1??and?
?3-1?. 
The test corpus is 150 parallel web page pairs 
randomly drawn from 20 Chinese-English bilin-
gual web sites on topics related to politics, sports, 
computer and literature. By manual annotation, 
9,824 parallel sentence pairs are found. All sen-
tence aligners run through the test parallel web 
pages, and each extracts a set of sentence pairs that 
it regards as parallel. The output pairs are matched 
with the annotated parallel sentences from the test 
corpus. Only exact matches of the sentence pairs 
are counted as correct. 
Our evaluation metrics are precision (P), recall 
(R) and F-measure (F) defined as: 
 
pairsoutput   totalof #
pairs sentence alignedcorrectly  of #
P ?  
pairs parallel  trueof #
pairs sentence alignedcorrectly  of #
R ?  
RP
R*P*2
F ??  
 
Based on the results in table 1, we can see that 
both Type 2 and Type 3 aligners outperform con-
ventional sentence alignment models. Leveraging 
HTML document information can enhance sen-
tence alignment quality. Especially, by using the 
tree alignment model, Type 3 aligners achieve a 
significant increase of around 7% on both preci-
sion and recall. Compared with the tree alignment 
model, the improvement by the Type 2 aligners is 
marginal. A reason for this is that the tree align-
ment model not only exploits HTML tag similari-
ties as in the Type 2 method, but also takes into 
account location of texts. In the tree alignment 
model, texts at similar locations in the tree hierar-
chical structure are more probable to be transla-
tions than those in disparate locations, even though 
they all have the same tag.  
We also evaluate the performance of the tree 
aligner. Since sentence alignment is performed 
within the text chunks of aligned nodes, tree 
alignment accuracy is very important for correct 
sentence alignment. We measure the alignment 
 Length Lexicon Hybrid 
P R F P R F P R F 
Type I 85.6% 72.8% 78.7% 83.1% 75.2% 78.9% 87.3% 76.4% 81.5% 
Type II 86.3% 74.8% 80.1% 85.7% 77.0% 81.1% 88.1% 78.6% 83.1% 
Type III 93.2% 79.3% 85.7% 92.9% 80.4% 86.2% 94.3% 83.1% 88.3% 
Table 1. Performance comparison between different types of sentence alignment methods 
 
511
accuracy on all nodes as well as that specifically 
on text nodes on the test corpus. The evaluation 
result is shown in table 2. 
 
Benchmarks in Table 2 show that the tree 
alignment model yields very reliable results with 
high accuracy in aligning both text nodes and non-
text nodes. After an analysis on text node align-
ment errors, we find that 79.7% of them have texts 
of very short length (no more than 4 words), which 
may not contain sufficient information to be identi-
fied as parallel. 
6 Conclusions 
In this paper, we present a new approach to sen-
tence alignment on parallel web pages. Due to the 
diversity and noisy nature of web corpora, a sto-
chastic tree alignment model is employed to ex-
ploit document structure in parallel web pages as 
useful information for identifying parallel sen-
tences. The tree alignment model can be combined 
with various conventional sentence alignment 
models to extract parallel sentences from parallel 
web pages. Experimental results show that exploit-
ing structural parallelism inherent in parallel web 
pages provides superior alignment performance 
over conventional sentence alignment methods and 
significant improvement (around 7% in both preci-
sion and recall) is achieved by using the stochastic 
tree alignment model. With improved sentence 
alignment performance, web parallel data mining 
systems are able to acquire parallel sentences of 
higher quality and quantity from the web.  
References: 
Brown, P. F., J. C. Lai and R. L. Mercer. 1991. Aligning 
Sentences in Parallel Corpora . Proceedings of ACL 
1991.  
Brown, P. E., S. A. D. Pietra, V. J. D. Pietra, and R. L. 
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation, Computa-
tional Linguistics V19(2), 1993  
Chen Jisong., Chau R. and C.-H. Yeh. 2004. Discover-
ing Parallel Text from the World Wide Web, Proceed-
ings of the second workshop on Australasian Infor-
mation Security, Data Mining and Web Intelligence, 
and Software Internationalization. 
Chen Jiang and Nie Jianyun. 2000. Automatic construc-
tion of parallel English-Chinese corpus for cross-
language information retrieval. Proceedings of the 
sixth conference on applied natural language 
processing 
Chen Stanley. 1993. Aligning Sentences in Bilingual 
Corpora Using Lexical Information. Proceedings of 
ACL 1993 
Chuang T.C. and Yeh.K.C. 2005. Aligning Parallel Bi-
lingual Corpora Statistically with Punctuation Crite-
ria. Computational Linguistics and Chinese Lan-
guage Processing. Vol. 10, 2005, pp. 95-122 
Dempster, A., Laird, N., and Rubin, D. 1977. Maximum 
likelihood from incomplete data via the EM algo-
rithm.  Journal of the Royal Statistical Society, Series 
B, 39(1):1?38. 
Gale W. A. and K. Church. 1993. A Program for Align-
ing Sentences in Parallel Corpora, Computational 
Linguistics, 19(1):75?102 
Gildea. D. 2003. Loosely Tree-Based Alignment for 
Machine Translation. In Proceedings of ACL 2003 
Kay Martin and Roscheisen Martin 1993. Text Transla-
tion Alignment. Computational Linguistics 
19(1):121--142. 
Lari K. and S. J. Young. 1990. The estimation of sto-
chastic context free grammars using the Inside-
Outside algorithm, Computer Speech and Language, 
4:35?56 
Ma, Xiaoyi and M. Liberman. 1999. Bits: A Method for 
Bilingual Text Search over the Web. Proceedings of 
Machine Translation Summit VII. 
Melamed. I. Dan. 1996. A Geometric Approach to 
Mapping Bitext Correspondence . Proceedings of 
EMNLP 96 
Moore Robert. C. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora . Proceedings of 5th 
Conference of the Association for Machine Transla-
tion in the Americas, pp. 135-244 
Resnik, P. and N.A. Smith. 2003. The Web as a Parallel 
Corpus.Computational Linguistics, 29(3)  
Simard, M. and Plamondon, P. 1996 Bilingual Sentence 
Alignment: Balancing Robustness and Accuracy. 
Proceedings of AMTA-96, Canada. 
Simard, M., Foster, G. and Isabelle, P. 1992, Using 
Cognates to Align Sentences in Bilingual Corpora . 
Proceedings of the Fourth International Conference 
 total correct accuracy 
all node alignment 18492 17966 97.2% 
text node alignment 3646 3577 98.1% 
Table 2. Tree Alignment Metrics 
512
on Theoretical and Methodological Issues in Ma-
chine translation (TMI92)  
Singh, A. K. and Husain, S. (2005). Comparison, selec-
tion and use of sentence alignment algorithms for 
new language pairs. Proceedings of the ACL Work-
shop on Building and Using Parallel Texts.  
Wu. Dekai. 1994. Aligning a parallel English-Chinese 
corpus statistically with lexical criterias. Proceedings 
of ACL 1994.  
Wu.?Dekai.? ?Stochastic Inversion Transduction Gram-
mar and Bilingual Parsing of Parallel Corpora??
Computational Linguistics, 23(3):374(1997) 
Yamada H. and Knight K. 2001 A Syntax based statis-
tical translation model. In Proceedings of ACL-01 
Yang C. C., and Li K. W., Mining English/Chinese Pa-
rallel Documents from the World Wide Web, Pro-
ceedings of the International World Wide Web Con-
ference, Honolulu, Hawaii, 2002. 
Zhao Bin. and Stephan. Vogel. 2002. Adaptive Parallel 
Sentences Mining F rom Web Bilingual News Collec-
tion. 2002 IEEE International Conference on Data 
Mining. 745-748 
513
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 489?496,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A DOM Tree Alignment Model for Mining Parallel Data from the Web 
Lei Shi1, Cheng Niu1, Ming Zhou1, and Jianfeng Gao2 
1Microsoft Research Asia, 5F Sigma Center, 49 Zhichun Road, Beijing 10080, P. R. China 
2Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA 
{leishi,chengniu,mingzhou,jfgao}@microsoft.com 
 
Abstract 
This paper presents a new web mining 
scheme for parallel data acquisition. 
Based on the Document Object Model 
(DOM), a web page is represented as a 
DOM tree. Then a DOM tree alignment 
model is proposed to identify the transla-
tionally equivalent texts and hyperlinks 
between two parallel DOM trees. By 
tracing the identified parallel hyperlinks, 
parallel web documents are recursively 
mined. Compared with previous mining 
schemes, the benchmarks show that this 
new mining scheme improves the mining 
coverage, reduces mining bandwidth, and 
enhances the quality of mined parallel 
sentences. 
1 Introduction 
Parallel bilingual corpora are critical resources 
for statistical machine translation (Brown 1993), 
and cross-lingual information retrieval (Nie 
1999). Additionally, parallel corpora have been 
exploited for various monolingual natural lan-
guage processing (NLP) tasks, such as word-
sense disambiguation (Ng 2003) and paraphrase 
acquisition (Callison 2005). 
However, large scale parallel corpora are not 
readily available for most language pairs. Even 
where resources are available, such as for Eng-
lish-French, the data are usually restricted to 
government documents (e.g., the Hansard corpus, 
which consists of French-English translations of 
debates in the Canadian parliament) or newswire 
texts. The "governmentese" that characterizes 
these document collections cannot be used on its 
own to train data-driven machine translation sys-
tems for a range of domains and language pairs.  
With a sharply increasing number of bilingual 
web sites, web mining for parallel data becomes 
a promising solution to this knowledge acquisi-
tion problem. In an effort to estimate the amount 
of bilingual data on the web, (Ma and Liberman 
1999) surveyed web pages in the de (German 
web site) domain, showing that of 150,000 web-
sites in the .de domain, 10% are German-English 
bilingual. Based on such observations, some web 
mining systems have been developed to auto-
matically obtain parallel corpora from the web 
(Nie et al1999; Ma and Liberman 1999; Chen, 
Chau and Yeh 2004; Resnik and Smith 2003  
Zhang et al2006 ). These systems mine parallel 
web documents within bilingual web sites, ex-
ploiting the fact that URLs of many parallel web 
pages are named with apparent patterns to facili-
tate website maintenance. Hence given a bilin-
gual website, the mining systems use pre-defined 
URL patterns to discover candidate parallel 
documents within the site. Then content-based 
features will be used to verify the translational 
equivalence of the candidate pairs. 
However, due to the diversity of web page 
styles and website maintenance mechanisms, 
bilingual websites use varied naming schemes 
for parallel documents. For example, the United 
Nation?s website, which contains thousands of 
parallel pages, simply names the majority of its 
web pages with some computer generated ad-hoc 
URLs. Such a website then cannot be mined by 
the URL pattern-based mining scheme. To fur-
ther improve the coverage of web mining, other 
patterns associated with translational parallelism 
are called for. 
Besides, URL pattern-based mining may raise 
concerns on high bandwidth cost and slow 
download speed. Based on descriptions of (Nie et 
al 1999; Ma and Liberman 1999; Chen, Chau 
and Yeh 2004), the mining process requires a full 
host crawling to collect URLs before using URL 
patterns to discover the parallel documents. 
Since in many bilingual web sites, parallel 
documents are much sparser than comparable 
documents, a significant portion of internet 
bandwidth is wasted on downloading web pages 
without translational counterparts.  
Furthermore, there is a lack of discussion on 
the quality of mined data. To support machine 
translation, parallel sentences should be extracted 
from the mined parallel documents. However, 
current sentence alignment models, (Brown et al
1991; Gale & Church 1991; Wu 1994; Chen 
489
1993; Zhao and Vogel, 2002; etc.) are targeted 
on traditional textual documents. Due to the 
noisy nature of the web documents, parallel web 
pages may consist of non-translational content 
and many out-of-vocabulary words, both of 
which reduce sentence alignment accuracy. To 
improve sentence alignment performance on the 
web data, the similarity of the HTML tag struc-
tures between the parallel web documents should 
be leveraged properly in the sentence alignment 
model. 
In order to improve the quality of mined data 
and increase the mining coverage and speed, this 
paper proposes a new web parallel data mining 
scheme. Given a pair of parallel web pages as 
seeds, the Document Object Model1  (DOM) is 
used to represent the web pages as a pair of 
DOM trees. Then a stochastic DOM tree align-
ment model is used to align translationally 
equivalent content, including both textual chunks 
and hyperlinks, between the DOM tree pairs. The 
parallel hyperlinks discovered are regarded as 
anchors to new parallel data. This makes the 
mining scheme an iterative process. 
The new mining scheme has three advantages: 
(i) Mining coverage is increased. Parallel hyper-
links referring to parallel web page is a general 
and reliable pattern for parallel data mining. 
Many bilingual websites not supporting URL 
pattern-based mining scheme support this new 
mining scheme. Our mining experiment shows 
that, using the new web mining scheme, the web 
mining throughput is increased by 32%; (ii) The 
quality of the mined data is improved. By lever-
aging the web pages? HTML structures, the sen-
tence aligner supported by the DOM tree align-
ment model outperforms conventional ones by 
7% in both precision and recall;  (iii) The band-
width cost is reduced by restricting web page 
downloads to the links that are very likely to be 
parallel. 
The rest of the paper is organized as follows: 
In the next section, we introduce the related work. 
In Section 3, a new web parallel data mining 
scheme is presented. Three component technolo-
gies, the DOM tree alignment model, the sen-
tence aligner, and the candidate parallel page 
verification model are presented in Section 4, 5, 
and 6. Section 7 presents experiments and 
benchmarks. The paper is finally concluded in 
Section 8. 
                                                 
1
 See http://www.w3.org/DOM/ 
2 Related Work 
The parallel data available on the web have been 
an important knowledge source for machine 
translation. For example, Hong Kong Laws, an 
English-Chinese Parallel corpus released by Lin-
guistic Data Consortium (LDC) is downloaded 
from the Department of Justice of the Hong 
Kong Special Administrative Region website. 
Recently, web mining systems have been built 
to automatically acquire parallel data from the 
web. Exemplary systems include PTMiner (Nie 
et al1999), STRAND (Resnik and Smith, 2003), 
BITS (Ma and Liberman, 1999), and PTI (Chen, 
Chau and Yeh, 2004). Given a bilingual website, 
these systems identify candidate parallel docu-
ments using pre-defined URL patterns. Then 
content-based features are employed for candi-
date verification. Particularly, HTML tag simi-
larities have been exploited to verify parallelism 
between pages. But it is done by simplifying 
HTML tags as a string sequence instead of a hi-
erarchical DOM tree. Tens of thousands parallel 
documents have been acquired with accuracy 
over 90%.  
To support machine translation, parallel sen-
tence pairs should be extracted from the parallel 
web documents. A number of techniques for 
aligning sentences in parallel corpora have been 
proposed. (Gale & Church 1991; Brown et al 
1991; Wu 1994) used sentence length as the ba-
sic feature for alignment. (Kay & Roscheisen 
1993; and Chen 1993) used lexical information 
for sentence alignment. Models combining 
length and lexicon information were proposed in 
(Zhao and Vogel, 2002; Moore 2002). Signal 
processing techniques is also employed in sen-
tence alignment by (Church 1993; Fung & 
McKeown 1994). Recently, much research atten-
tion has been paid to aligning sentences in com-
parable documents (Utiyama et al2003, 
Munteanu et al2004).  
 The DOM tree alignment model is the key 
technique of our mining approach. Although, to 
our knowledge, this is the first work discussing 
DOM tree alignments, there is substantial re-
search focusing on syntactic tree alignment 
model for machine translation. For example, (Wu 
1997; Alshawi, Bangalore, and Douglas, 2000; 
Yamada and Knight, 2001) have studied syn-
chronous context free grammar. This formalism 
requires isomorphic syntax trees for the source 
sentence and its translation. (Shieber and Scha-
bes 1990) presents a synchronous tree adjoining 
grammar (STAG) which is able to align two syn-
490
tactic trees at the linguistic minimal units. The 
synchronous tree substitution grammar (STSG) 
presented in (Hajic etc. 2004) is a simplified ver-
sion of STAG which allows tree substitution op-
eration, but prohibits the operation of tree ad-
junction.  
3 A New Parallel Data Mining Scheme 
Supported by DOM Tree Alignment 
Our new web parallel data mining scheme con-
sists of the following steps:  
 
(1) Given a web site, the root page and web 
pages directly linked from the root page are 
downloaded. Then for each of the 
downloaded web page, all of its anchor texts 
(i.e. the hyperlinked words on a web page) 
are compared with a list of predefined strings 
known to reflect translational equivalence 
among web pages (Nie et al1999). Exam-
ples of such predefined trigger strings in-
clude: (i) trigger words for English transla-
tion {English, English Version,  , 

, etc.}; and (ii) trigger words for Chinese 
translation {Chinese, Chinese Version, Sim-
plified Chinese, Traditional Chinese,   , 


, etc.}. If both categories of trigger 
words are found, the web site is considered 
bilingual, and every web page pair are sent to 
Step 2 for parallelism verification. 
(2) Given a pair of the plausible parallel web 
pages, a verification module is called to de-
termine if the page pair is truly translation-
ally equivalent.  
(3) For each verified pair of parallel web pages, 
a DOM tree alignment model is called to ex-
tract parallel text chunks and hyperlinks. 
(4) Sentence alignment is performed on each 
pair of the parallel text chunks, and the re-
sulting parallel sentences are saved in an 
output file. 
(5) For each pair of parallel hyperlinks, the cor-
responding pair of web pages is downloaded, 
and then goes to Step 2 for parallelism veri-
fication. If no more parallel hyperlinks are 
found, stop the mining process. 
Our new mining scheme is iterative in nature. 
It fully exploits the information contained in the 
parallel data and effectively uses it to pinpoint 
the location holding more parallel data. This ap-
proach is based on our observation that parallel 
pages share similar structures holding parallel 
content, and parallel hyperlinks refer to new par-
allel pages. 
By exploiting both the HTML tag similarity 
and the content-based translational equivalences, 
the DOM tree alignment model extracts parallel 
text chunks. Working on the parallel text chunks 
instead of the text of the whole web page, the 
sentence alignment accuracy can be improved by 
a large margin. 
In the next three sections, three component 
techniques, the DOM tree alignment model, sen-
tence alignment model, and candidate web page 
pair verification model are introduced. 
4 DOM Tree Alignment Model 
The Document Object Model (DOM) is an appli-
cation programming interface for valid HTML 
documents. Using DOM, the logical structure of 
a HTML document is represented as a tree where 
each node belongs to some pre-defined node 
types (e.g. Document, DocumentType, Element, 
Text, Comment, ProcessingInstruction etc.). 
Among all these types of nodes, the nodes most 
relevant to our purpose are Element nodes (cor-
responding to the HTML tags) and Text nodes 
(corresponding to the texts). To simplify the de-
scription of the alignment model, minor modifi-
cations of the standard DOM tree are made: (i) 
Only the Element nodes and Text nodes are kept 
in our document tree model. (ii) The ALT attrib-
ute is represented as Text node in our document 
tree model. The ALT text are textual alternative 
when images cannot be displayed, hence is help-
ful to align images and hyperlinks. (iii) the Text 
node (which must be a leaf) and its parent Ele-
ment node are combined into one node in order 
to concise the representation of  the alignment 
model. The above three modifications are exem-
plified in Fig. 1. 
 
 
Fig. 1 Difference between Standard DOM and 
Our Document Tree 
 
Despite these minor differences, our document 
tree is still referred as DOM tree throughout this 
paper. 
491
4.1 DOM Tree Alignment 
Similar to STSG, our DOM tree alignment model 
supports node deletion, insertion and substitution. 
Besides, both STSG and our DOM tree align-
ment model define the alignment as a tree hierar-
chical invariance process, i.e. if node A is aligned 
with node B, then the children of A are either 
deleted or aligned with the children of B.  
But two major differences exist between 
STSG and our DOM tree alignment model: (i) 
Our DOM tree alignment model requires the 
alignment a sequential order invariant process, 
i.e. if node A is aligned with node B, then the 
sibling nodes following A have to be either de-
leted or aligned with the sibling nodes following 
B.  (ii) (Hajic etc. 2004) presents STSG in the 
context of language generation, while we search 
for the best alignment on the condition that both 
trees are given.  
To facilitate the presentation of the tree align-
ment model, the following symbols are intro-
duced: given a HTML document D, DT refers to 
the corresponding DOM tree; DiN refers to the i
th
 
node of DT (here the index of the node is in the 
breadth-first order), and DiT refers to the sub-tree 
rooted at DiN , so 
D
1N refers to the root of 
DT , 
and DT=D1T ;  [ ]
D
ji,T refers to the forest consisting 
of the sub-trees rooted at nodes from DiT to
D
jT . 
t.N Di refers to the text of node
D
iN ; l.N
D
i refers to 
the HTML tag of the node DiN ; jC.N
D
i  refers to 
the jth child of the node DiN ; [ ]nmC ,Di .N refers to 
the consecutive sequence of DiN ?s children nodes 
from 
m
C.N Di to nC.N
D
i ; the sub-tree rooted at 
jC.N
D
i is represented as jTC.N
D
i  and the forest 
rooted at [ ]nmC ,
D
i .N  is represented as [ ]nmTC ,
D
i .N . 
Finally NULL  refers to the empty node intro-
duced for node deletion.  
To accommodate the hierarchical structure of 
the DOM tree, two different translation prob-
abilities are defined: ( )EiFm TTPr : probability of translating sub-tree 
E
iT into sub-tree
F
m
T ; 
( )EiFm NNPr : probability of translating node 
E
iN into 
F
m
N . 
Besides, [ ] [ ]( )ATT E jiF nm ,Pr ,,  represents the prob-
ability of translating the forest [ ]
E
jiT , into 
[ ]
F
nm
T
,
based on the alignment A. The tree align-
ment A is defined as a mapping from target 
nodes onto source nodes or the null node.  
Given two HTML documents F (in French) 
and E (in English), the tree alignment task is 
defined as searching for A which maximizes the 
following probability: ( ) ( ) ( )EEFEF TAATTTTA Pr,Pr,Pr ?               (1) 
where ( )ETAPr  represents the prior knowledge 
of the alignment configurations.  
By introducing dp  which refers to the prob-
ability of a source or target node deletion occur-
ring in an alignment configuration, the alignment 
prior ( )ETAPr  is assumed as the following bi-
nominal distribution: 
 ( ) ( ) MdLdE ppTA ?? 1Pr  
where L is the count of non-empty alignments in 
A, and M is the count of source and target node 
deletions in A. 
As to ( )ATT EF ,Pr , we can estimate as 
( ) ( )ATTATT EFEF ,Pr,Pr 11= , and ( )ATTr EiFl ,P  
can be calculated recursively depending on the 
alignment configuration of A : 
(1) If FlN is aligned with EiN , and the children of 
F
lN are aligned with the children of 
E
iN , then 
we have ( )
( ) [ ] 



=

	

 ATCNTCNNN
ATT
K
E
iK
F
l
E
i
F
l
E
i
F
l
,..PrPr
,Pr
'
,1,1
    
where K and K? are degree of FlN  and 
E
iN . 
(2) If FlN is deleted, and the children of FlN  is 
aligned with EiT , then we have ( ) ( ) [ ]( )ATTCNNULLNATT EiKFlFlEiFl ,.PrPr,Pr ,1=
where K is the degree of FlN  
(3) If EiN is deleted, and FlN is aligned with the 
children of EiN , then  
( ) ( )ATCTTATT KEiFlEiFl ,.Pr,Pr ],1[=               
where K is the degree of EiN . 
To complete the alignment model, 
[ ]( )ATTr E jiF nm ,P ,],[  is to be estimated. As mentioned 
before, only the alignment configurations with 
unchanged node sequential order are considered 
as valid. So, [ ]( )ATTr E jiF nm ,P ,],[ is estimated recur-
sively according to the following five alignment 
configurations of A: 
(4) If F
m
T is aligned with EiT , and [ ]
F
nm
T
,1+  is 
492
aligned with [ ]
E
jiT ,1+ , then  
[ ]( ) ( ) [ ]( )ATTrNNATTr E jiF nmEiFmE jiF nm ,PPr,P ,1],1[,],[ ++=    
(5) If F
m
T is deleted, and [ ]
F
nm
T
,1+ is aligned with 
[ ]
E
jiT , , then 
[ ]( ) ( ) [ ]( )ATTrNULLNATTr E jiF nmFmE jiF nm ,PPr,P ,],1[,],[ +=
 
(6) If EiT is deleted, and [ ]F nmT , is aligned with 
[ ]
E
jiT ,1+ , then 
[ ]( ) [ ]( )ATTATTr E jiF nmE jiF nm ,Pr,P ,1],[,],[ +=     
(7) If F
m
N  is deleted, and F
m
N ?s children [ ]K
F
m
CN
,1.  
is combined with [ ]
F
nm
T
,1+ to aligned with [ ]
E
jiT , , 
then 
[ ]( )
( ) [ ]( )ATTTCNrNULLN
ATTr
E
ji
F
nmK
F
m
F
m
E
ji
F
nm
,.PPr
,P
,],1[],1[
,],[
+
=
   
where K is the degree of .F
m
N  
(8) EiN  is deleted, and EiN ?s children [ ]KEi CN ,1.  
is combined with [ ]
E
jiT ,1+ to be aligned with 
[ ]
F
nm
T
,
, then 
[ ]( ) [ ]( )ATTCNTATTr EKEiFEF jinmjinm ,.Pr,P ,1],[,],[ ],1[ +=       
where K is the degree of .EiN  
 
Finally, the node translation probability is 
modeled as ( ) ( ) ( )tNtNlNlNNN EiFlEiFlEjFl ..Pr..PrPr ?  . And 
the text translation probability ( )EF ttPr  is model 
using IBM model I (Brown et al1993). 
4.2 Parameter Estimation Using Expecta-
tion-Maximization 
Our tree alignment model involves three catego-
ries of parameters: the text translation probability ( )EF ttPr , tag mapping probability ( )'Pr ll , and 
node deletion probability dp .  
Conventional parallel data released by LDC 
are used to train IBM model I for estimating the 
text translation probability ( )EF ttPr .   
One way to estimate ( )'Pr ll and dp  is to 
manually align nodes between parallel DOM 
trees, and use them as training corpora for 
maximum likelihood estimation. However, this is 
a very time-consuming and error-prone proce-
dure. In this paper, the inside outside algorithm 
presented in (Lari and Young, 1990) is extended 
to train parameters ( )'Pr ll  and dp  by optimally 
fitting the existing parallel DOM trees. 
4.3 Dynamic Programming for Decoding 
It is observed that if two trees are optimally 
aligned, the alignment of their sub-trees must be 
optimal as well. In the decoding process, dy-
namic programming techniques can be applied to 
find the optimal tree alignment using that of the 
sub-trees in a bottom up manner. The following 
is the pseudo-code of the decoding algorithm: 
 
For i= || FT  to 1  (bottom-up) { 
For j= || ET to 1 (bottom-up) { 
derive the best alignments among 
[ ]iK
F
i TCT ,1.  and [ ]jK
E
j TCT ,1. , and then com-
pute the best alignment between 
F
iN and 
E
jN .  
where || FT and || ET are number of nodes in 
FT and ET ; iK and jK are the degrees of 
F
iN and 
E
jN . The time complexity of the decoding algo-
rithm is )))(degree)((degree|||TO(| 2F EFE TTT +?? , 
where the degree of a tree is defined as the larg-
est degree of its nodes. 
5 Aligning Sentences Using Tree Align-
ment Model 
To exploit the HTML structure similarities be-
tween parallel web documents, a cascaded ap-
proach is used in our sentence aligner implemen-
tation.  
First, text chunks associated with DOM tree 
nodes are aligned using the DOM tree alignment 
model. Then for each pair of parallel text chunks, 
the sentence aligner described in (Zhao et al
2002), which combines IBM model I and the 
length model of (Gale & Church 1991) under a 
maximum likelihood criterion, is used to align 
parallel sentences.  
6 Web Document Pair Verification 
Model 
To verify whether a candidate web document 
pair is truly parallel, a binary maximum entropy 
based classifier is used.  
Following (Nie et al1999) and  (Resnik and 
Smith, 2003), three features are used: (i) file 
length ratio;  (ii) HTML tag similarity; (iii) sen-
tence alignment score.  
493
The HTML tag similarity feature is computed 
as follows: all of the HTML tags of a given web 
page are extracted, and concatenated as a string. 
Then, a minimum edit distance between the two 
tag strings associated with the candidate pair is 
computed, and the HMTL tag similarity score is 
defined as the ratio of match operation number to 
the total operation number.  
The sentence alignment score is defined as the 
ratio of the number of aligned sentences and the 
total number of sentences in both files. 
Using these three features, the maximum en-
tropy model is trained on 1,000 pairs of web 
pages manually labeled as parallel or non-
parallel. The Iterative Scaling algorithm (Pietra, 
Pietra and Lafferty 1995) is used for the training. 
7 Experimental Results 
The DOM tree alignment based mining system is 
used to acquire English-Chinese parallel data 
from the web. The mining procedure is initiated 
by acquiring Chinese website list. 
We have downloaded about 300,000 URLs of 
Chinese websites from the web directories at 
cn.yahoo.com, hk.yahoo.com and tw.yahoo.com. 
And each website is sent to the mining system 
for English-Chinese parallel data acquisition. To 
ensure that the whole mining experiment to be 
finished in schedule, we stipulate that it takes at 
most 10 hours on mining each website. Totally 
11,000 English-Chinese websites are discovered, 
from which 63,214 pairs of English-Chinese par-
allel web documents are mined. After sentence 
alignment, totally 1,069,423 pairs of English-
Chinese parallel sentences are extracted. 
In order to compare the system performance, 
100 English-Chinese bilingual websites are also 
mined using the URL pattern based mining 
scheme. Following (Nie et al1999; Ma and 
Liberman 1999; Chen, Chau and Yeh 2004), the 
URL pattern-based mining consists of three steps: 
(i) host crawling for URL collection; (ii) candi-
date pair identification by pre-defined URL pat-
tern matching; (iii) candidate pair verification. 
Based on these mining results, the quality of 
the mined data, the mining coverage and mining 
efficiency are measured.  
First, we benchmarked the precision of the 
mined parallel documents. 3,000 pairs of Eng-
lish-Chinese candidate documents are randomly 
selected from the output of each mining system, 
and are reviewed by human annotators. The 
document level precision is shown in Table 1.  
 
 URL pattern DOM Tree Align-
ment 
Precision 93.5% 97.2% 
Table 1: Precision of Mined Parallel Documents 
 
The document-level mining precision solely 
depends on the candidate document pair verifica-
tion module. The verification modules of both 
mining systems use the same features, and the 
only difference is that in the new mining system 
the sentence alignment score is computed with 
DOM tree alignment support. So the 3.7% im-
provement in document-level precision indirectly 
confirms the enhancement of sentence alignment. 
Secondly, the accuracy of sentence alignment 
model is benchmarked as follows: 150 English-
Chinese parallel document pairs are randomly 
taken from our mining results. All parallel sen-
tence pairs in these document pairs are manually 
annotated by two annotators with cross-
validation. We have compared sentence align-
ment accuracy with and without DOM tree 
alignment support. In case of no tree alignment 
support, all the texts in the web pages are ex-
tracted and sent to sentence aligner for alignment. 
The benchmarks are shown in Table 2. 
 
Alignment 
Method 
Num-
ber 
Right 
Num-
ber 
Wrong 
Num-
ber 
Missed 
Preci-
sion 
Recall 
Eng-Chi 
(no DOM 
tree) 
2172 285 563 86.9% 79.4% 
Eng-Chi 
(with DOM 
tree) 
2369 156 366 93.4% 86.6% 
Table 2: sentence alignment accuracy 
 
Table 2 shows that with DOM tree alignment 
support, the sentence alignment accuracy is 
greatly improved by 7% in both precision and 
recall. We also observed that the recall is lower 
than precision. This is because web pages tend to 
contain many short sentences (one or two words 
only) whose alignment is hard to identify due to 
the lack of content information. 
Although Table 2 benchmarks the accuracy of 
sentence aligner, but the quality of the final sen-
tence pair outputs depend on many other mod-
ules as well, e.g. the document level parallelism 
verification, sentence breaker, Chinese word 
breaker, etc. To further measure the quality of 
the mined data, 2,000 sentence pairs are ran-
domly picked from the final output, and are 
manually classified into three categories: (i) ex-
act parallel, (ii) roughly parallel: two parallel 
sentences involving missing words or erroneous 
additions; (iii) not parallel. Two annotators are 
494
assigned for this task with cross-validation. As is 
shown in Table 3, 93.5% of output sentence pairs 
are either exact or roughly parallel. 
 
Corpus Exact 
Parallel 
Roughly 
Parallel 
Not Parallel 
Mined 1703 167 130 
Table 3  Quality of Mined Parallel Sentences 
As we know, the absolute value of mining sys-
tem recall is hard to estimate because it is im-
practical to evaluate all the parallel data held by 
a bilingual website. Instead, we compare mining 
coverage and efficiency between the two systems. 
100 English-Chinese bilingual website are mined 
by both of the system. And the mining efficiency 
comparison is reported in Table 4. 
 
Mining 
System 
Parallel Page 
Pairs found 
& verified 
# of page 
downloads 
# of 
downloads 
per pair 
URL pat-
tern-based 
Mining 
4383 84942 19.38 
DOM Tree 
Align-
ment-
based 
Mining 
5785 13074 2.26 
 Table 4. Mining Efficiency Comparison on 100 
Bilingual Websites 
 
Although it downloads less data, the DOM 
tree based mining scheme increases the parallel 
data acquisition throughput by 32%. Furthermore, 
the ratio of downloaded page count per parallel 
pair is 2.26, which means the bandwidth usage is 
almost optimal. 
Another interesting topic is the complemen-
tarities between both mining systems. As re-
ported in Table (5),  1797 pairs of parallel docu-
ments mined by the new scheme is not covered 
by the URL pattern-based scheme. So if both 
systems are used, the throughput can be further 
increased by 41%. 
 
# of Parallel Page 
Pairs Mined by 
Both Systems  
# of Parallel Page 
Pairs Mined by 
URL Patterns 
only 
# of Parallel Page 
Pairs Mined by 
Tree Alignment 
only 
3988 395 1797 
 Table 5. Mining Results Complementarities on 
100 Bilingual Website 
8 Discussion and Conclusion 
Mining parallel data from web is a promising 
method to overcome the knowledge bottleneck 
faced by machine translation. To build a practical 
mining system, three research issues should be 
fully studied: (i) the quality of mined data, (ii) 
the mining coverage, and (iii) the mining speed. 
Exploiting DOM tree similarities helps in all the 
three issues. 
Motivated by this observation, this paper pre-
sents a new web mining scheme for parallel data 
acquisition. A DOM tree alignment model is pro-
posed to identify translationally equivalent text 
chunks and hyperlinks between two HTML 
documents. Parallel hyperlinks are used to pin-
point new parallel data, and make parallel data 
mining a recursive process. Parallel text chunks 
are fed into sentence aligner to extract parallel 
sentences.  
Benchmarks show that sentence aligner sup-
ported by DOM tree alignment achieves per-
formance enhancement by 7% in both precision 
and recall. Besides, the new mining scheme re-
duce the bandwidth cost by 8~9 times on average 
compared with the URL pattern-based mining 
scheme. In addition, the new mining scheme is 
more general and reliable, and is able to mine 
more data. Using the new mining scheme alone, 
the mining throughput is increased by 32%, and 
when combined with URL pattern-based scheme, 
the mining throughput is increased by 41%. 
References 
Alshawi, H., S. Bangalore, and S. Douglas. 2000. 
Learning Dependency Translation Models as Col-
lections of Finite State Head Transducers. Compu-
tational Linguistics, 26(1).  
Brown, P. F., J. C. Lai and R. L. Mercer. 1991. Align-
ing Sentences in Parallel Corpora. In Proceedings 
of 29th Annual Meeting of the Association for 
Computational Linguistics.  
Brown, P. E., S. A. D. Pietra, V. J. D. Pietra, and R. L. 
Mercer. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. Computa-
tional Linguistics, V19(2).  
Callison-Burch, C. and C. Bannard. 2005. Paraphras-
ing with Bilingual Parallel Corpora. In Proceed-
ings of 43th Annual Meeting of the Association for 
Computational Linguistics. 
Chen, J., R. Chau, and C.-H. Yeh. 1991. Discovering 
Parallel Text from the World Wide Web. In Pro-
ceedings of the second workshop on Australasian 
Information Security, Data Mining and Web Intel-
ligence, and Software Internationalization. 
Chen, S. 1993. Aligning Sentences in Bilingual Cor-
pora Using Lexical Information. In Proceedings of 
31st Annual Meeting of the Association for Compu-
tational Linguistics. 
Church, K. W. 1993. Char_align: A Program for 
Aligning Parallel Texts at the Character Level. In 
495
Proceedings of 31st Annual Meeting of the Asso-
ciation for Computational Linguistics. 
Fung, P. and K. Mckeown. 1994. Aligning Noisy Par-
allel Corpora across Language Groups: Word Pair 
Feature Matching by Dynamic Time Warping. In 
Proceedings of the First Conference of the Asso-
ciation for Machine Translation in the Americas. 
Gale W. A. and K. Church. 1991. A Program for 
Aligning Sentences in Parallel Corpora. In Pro-
ceedings of 29th Annual Meeting of the Association 
for Computational Linguistics. 
Hajic J., et al 2004.  Final Report: Natural Language 
Generation in the Context of Machine Translation. 
Kay M. and M. Roscheisen. 1993. Text-Translation 
Alignment. Computational Linguistics, 19(1).  
Lari K. and S. J. Young. 1990. The Estimation of Sto-
chastic Context Free Grammars using the Inside-
Outside Algorithm. Computer Speech and Lan-
guage, 4:35?56, 1990. 
Ma, X. and M. Liberman. 1999. Bits: A Method for 
Bilingual Text Search over the Web. In Proceed-
ings of Machine Translation Summit VII. 
Ng, H. T., B. Wang, and Y. S. Chan. 2003. Exploiting 
Parallel Texts for Word Sense Disambiguation: An 
Empirical Study. In Proceedings of 41st Annual 
Meeting of the Association for Computational Lin-
guistics. 
Nie, J. Y., M. S. P. Isabelle, and R. Durand. 1999. 
Cross-language Information Retrieval based on 
Parallel Texts and Automatic Mining of Parallel 
Texts from the Web. In Proceedings of the 22nd 
Annual International ACM SIGIR Conference on 
Research and Development.  
Moore, R. C. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Proceedings of 
5th Conference of the Association for Machine 
Translation in the Americas. 
Munteanu D. S, A. Fraser, and D. Marcu. D., 2002.  
Improved Machine Translation Performance via 
Parallel Sentence Extraction from Comparable 
Corpora. In Proceedings of the Human Language 
Technology Conference of the North American 
Chapter of the Association for Computational Lin-
guistics: HLT-NAACL 2004. 
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. In-
ducing Features Of Random Fields. In IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence.  
Resnik, P. and N. A. Smith. 2003. The Web as a Par-
allel Corpus. Computational Linguistics, 29(3)  
Shieber, S. M.  and Y. Schabes. 1990. Synchronous 
tree-adjoining grammars. In Proceedings of the 
13th International Conference on Computational 
linguistics.  
Utiyama, M. and H. Isahara 2003. Reliable Measures 
for Aligning Japanese-English News Articles and 
Sentences.  In Proceedings of 41st Annual Meeting 
of the Association for Computational Linguis-
tics.ACL 2003. 
Wu, D. 1994. Aligning a parallel English-Chinese 
corpus statistically with lexical criterias. In Pro-
ceedings of of 32nd Annual Meeting of the Associa-
tion for Computational Linguistics. 
Wu, D. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3).  
Yamada K. and K. Knight. 2001. A Syntax Based 
Statistical Translation Model. In Proceedings of 
39th Annual Meeting of the Association for Com-
putational Linguistics.  
Zhao B. and S. Vogel. 2002. Adaptive Parallel Sen-
tences Mining From Web Bilingual News Collec-
tion. In 2002 IEEE International Conference on 
Data Mining. 
Zhang, Y., K. Wu, J. Gao, and Phil Vines. 2006. 
Automatic Acquisition of Chinese-English Parallel 
Corpus from the Web. In Proceedings of 28th 
European Conference on Information Retrieval. 
496
Open Text Semantic Parsing Using FrameNet and WordNet
Lei Shi and Rada Mihalcea
Department of Computer Science and Engineering
University of North Texas
leishi@unt.edu, rada@cs.unt.edu
Abstract
This paper describes a rule-based semantic parser
that relies on a frame dataset (FrameNet), and a
semantic network (WordNet), to identify seman-
tic relations between words in open text, as well
as shallow semantic features associated with con-
cepts in the text. Parsing semantic structures al-
lows semantic units and constituents to be ac-
cessed and processed in a more meaningful way
than syntactic parsing, moving the automation of
understanding natural language text to a higher
level.
1 Introduction
The goal of the semantic parser is to analyze the semantic
structure of a natural language sentence. Similar in spirit
with the syntactic parser ? whose goal is to parse a valid nat-
ural language sentence into a parse tree indicating how the
sentence can be syntactically decomposed into smaller syn-
tactic constituents ? the purpose of the semantic parser is to
analyze the structure of sentence meaning. Sentence mean-
ing is composed by entities and interactions between enti-
ties, where entities are assigned semantic roles, and can be
further modified by other modifiers. The meaning of a sen-
tence is decomposed into smaller semantic units connected
by various semantic relations by the principle of composi-
tionality, and the parser represents the semantic structure ?
including semantic units as well as semantic relations, con-
necting them into a formal format.
One major problem faced by many natural language un-
derstanding applications that rely on syntactic analysis of
text, is the fact that similar syntactic patterns may introduce
different semantic interpretations. Likewise, similar mean-
ings can be syntactically realized in many different ways.
The semantic parser attempts to solve this problem, and
produces a syntax-independent representation of sentence
meaning, so that semantic constituents can be accessed and
processed in a more meaningful and flexible way, avoiding
the sometimes rigid interpretations produced by a syntactic
analyzer. For instance, the sentences I boil water and water
boils contain a similar relation between water and boil, even
though they have different syntactic structures.
In this paper, we describe the main components of the se-
mantic parser, and illustrate the basic procedures involved
in parsing semantically open text. Our semantic parser de-
parts from current approaches in statistics-based annotations
of semantic structures. Instead, we are using publicly avail-
able lexical resources (FrameNet and WordNet) as a starting
point to derive rules for a rule-based semantic parser.
2 Semantic Structure
Semantics is the denotation of a string of symbols, either
a sentence or a word. Similar to a syntactic parser, which
shows how a larger string is formed by smaller strings from
a formal point of view, the semantic parser shows how the
denotation of a larger string ? sentence, is formed by deno-
tations of smaller strings ? words. Syntactic relations can be
described using a set of rules about how a sentence string
is formally generated using word strings. Instead, seman-
tic relations between semantic constituents depend on our
understanding of the world, which is across languages and
syntax.
We can model the sentence semantics as describing enti-
ties and interactions between entities. Entities can represent
physical objects, as well as time, places, or ideas, and are
usually formally realized as nouns or noun phrases. Inter-
actions, usually realized as verbs, describe relationships or
interactions between participating entities. Note that a par-
ticipant can also be an interaction, which can be regarded
as an entity nominalized from an interaction. We assign se-
mantic roles to participants and their semantic relations are
identified by the case frame introduced by their interaction.
In a sentence, participants and interactions can be further
modified by various modifiers, including descriptive mod-
ifiers that describe attributes such as drive slowly, restric-
tive modifiers that enforce a general denotation to become
more specific such as musical instrument, referential modi-
fiers that indicate particular instances such as the pizza I or-
dered. Other semantic relations can also be identified, such
as coreference, complement, and others. Based on the prin-
ciple of compositionality, the sentence semantic structure is
recursive, similar to a tree.
Note that the semantic parser analyzes shallow-level se-
mantics, which is derived directly from linguistic knowl-
edge, such as rules about semantic role assignment, lexi-
cal semantic knowledge, and syntactic-semantic mappings,
without taking into account any context or common sense
knowledge. Hence, the parser can be used as an interme-
diate semantic processing level before higher levels of text
understanding.
3 Knowledge Bases for Semantic Parsing
The parser relies on two main types of knowledge ? about
words, and about relations between words. The first type of
knowledge is drawn from WordNet ? a large lexical database
with rich information about words and concepts. We refer
to this as word-level knowledge. The latter is derived from
FrameNet ? a resource that contains information about dif-
ferent situations, called frames, in which semantic relations
are syntactically realized in natural language sentences. We
call this sentence-level knowledge. In addition to these two
lexical knowledge bases, the parser also utilizes a set of man-
ually defined rules, which encode mappings from syntactic
structures to semantic relations, and which are used to han-
dle those structures not explicitly addressed by FrameNet or
WordNet. In this section, we describe the type of informa-
tion extracted from these knowledge bases, and show how
this information is encoded in a format accessible to the se-
mantic parser.
3.1 Sentence Level Knowledge
FrameNet (Johnson et al, 2002) provides the knowl-
edge needed to identify case frames and semantic roles.
FrameNet is based on the theory of frame semantics, and de-
fines a sentence level ontology. In frame semantics, a frame
corresponds to an interaction and its participants, both of
which denote a scenario, in which participants play some
kind of roles. A frame has a name, and we use this name
to identify the semantic relation that groups together the se-
mantic roles. Nouns, verbs and adjectives can be used to
identify frames.
Each annotated sentence in FrameNet exemplifies a pos-
sible syntactic realization for the semantic roles associated
with a frame for a given target word. By extracting the syn-
tactic features and corresponding semantic roles from all an-
notated sentences in the FrameNet corpus, we are able to au-
tomatically build a large set of rules that encode the possible
syntactic realizations of semantic frames.
3.1.1 Rules Learned from FrameNet
FrameNet data ?is meant to be lexicographically relevant,
not statistically representative? (Johnson et al, 2002), and
therefore we are using FrameNet as a starting point to derive
rules for a rule-based semantic parser.
To build the rules, we are extracting several syntactic fea-
tures. Some are explicitly encoded in FrameNet, such as the
grammatical function (GF) and phrase type (PT) features.
In addition, other syntactic features are extracted from the
sentence context. One such feature is the relative position
(RP) to the target word. Another feature is the voice of the
sentence. If the phrase type is prepositional phrase (PP), we
also record the actual preposition that precedes the phrase.
After we extract all these syntactic features, the semantic
role is appended to the rule, which creates a mapping from
syntactic features to semantic roles.
Feature sets are arranged in a list, the order of which is
identical to that in the sentence. Altogether, the rule for a
possible realization of a frame exemplified by a tagged sen-
tence is an ordered sequence of syntactic features with their
semantic roles. For example, the corresponding formalized
rule for the sentence I had chased Selden over the moor is:
[active, [ext,np,before,theme], [obj,np,after,goal],
[comp,pp,after,over,path]]
In FrameNet, there are multiple annotated sentences for
each frame to demonstrate multiple possible syntactic real-
izations. All possible realizations of a frame are collected
and stored in a list for that frame, which also includes the tar-
get word, its syntactic category, and the name of the frame.
All the frames defined in FrameNet are transformed into this
format, so that they can be easily handled by the rule-based
semantic parser.
3.2 Word Level Knowledge
WordNet (Miller, 1995) is the resource used to identify shal-
low semantic features that can be attached to lexical units.
For instance, attribute relations, adjective/adverb classifica-
tions, and others, are semantic features extracted from Word-
Net and stored together with the words, so that they can be
directly used in the parsing process.
All words are uniformly defined, regardless of their class.
Features are assigned to each word, including syntactic and
shallow semantic features, indicating the functions played
by the word. Syntactic features are used by the feature-
augmented syntactic analyzer to identify grammatical errors
and produce syntactic information for semantic role assign-
ment. Semantic features encode lexical semantic informa-
tion extracted from WordNet that is used to determine se-
mantic relations between words in various situations.
Features can be arbitrarily defined, as long as there are
rules to handle them. The features we define encode infor-
mation about the syntactic category of a word, number and
countability for nouns, transitivity and form for verbs, type,
degree, and attribute for adjectives and adverbs, and others.
For example, for the adjective slow, the entry in the lexi-
con is defined as:
lex(slow,W):- W= [parse:slow, cat:adj, attr:speed,
degree:base, type:descriptive].
Here, the category (cat) is defined as adjective, the type
is descriptive, degree is base form. We also record the attr
feature, which is derived from the attribute relation in Word-
Net, and links a descriptive adjective to the attribute (noun)
it modifies, such as slow   speed.
4 The Semantic Parser
The parsing algorithm is implemented as a rule-based sys-
tem. The general procedure of semantic parsing consists of
three main steps: (1) syntactic parsing into an intermedi-
ate format, using a feature-augmented syntactic parser, and
assignment of shallow semantic features; (2) semantic role
assignment; (3) application of default rules.
4.1 Feature Augmented Syntactic/Semantic Analyzer
The semantic parser is based on dependencies between
words that are identified using a structure analyzer. The an-
alyzer generates an intermediate format, where target words
and syntactic arguments are explicitly identified, so that they
can be matched against the rules derived from FrameNet.
The intermediate format also encodes some shallow seman-
tic features, including word level semantics (e.g. attribute,
gender), and semantic relations that have direct syntactic
correspondence (e.g. modifier types). The function of the
sentence is also identified, as assertion, query, yn-query,
command.
The analyzer is based on a feature augmented grammar,
and has the capability of detecting if a sentence is gram-
matically correct (unlike statistical parsers, which attempt to
parse any sentence, regardless of their well-formness). Con-
stituents are assigned with features, and the grammar con-
sists of a set of rules defining how constituents can connect
to each other, based on the values of their features.
Since features can contain both syntactic and semantic in-
formation, the analyzer can reject some grammatically in-
correct sentences such as: I have much apples, You has my
car, or even some semantically incorrect sentences: The
technology is very military1.
4.2 Semantic Role Assignment
In the process of semantic role assignment, we first start by
identifying all possible frames, according to the target word.
Next, a matching algorithm is used to find the most likely
match among all rules derived for these frames, to identify
the correct frame (if several are possible), and assign seman-
tic roles.
In a sentence describing an interaction, we usually select
the verb or predicative adjective as the target word, which
triggers the sentence level frame. A noun can also play the
role of target word, but only within the scope of the noun
phrase it belongs to, and it can be used to assign semantic
roles only to its modifiers.
The matching algorithm relies on a scoring function to
evaluate the similarity between two sequences of syntactic
features. The matching starts from left to right. Whenever
an exact match is found, the score will be increased by 1.
It should be noted that the search sequence is uni-directional
which means that once you find a match, you can go ahead to
check features to the right, but you cannot go back to check
1Since military is not a descriptive adjective, it cannot be con-
nected to the degree modifier very.
rules you have already checked. This guarantees that syntac-
tic features are matched in the right order, and the order of
sequence in the rule is maintained. Since the frame of a tar-
get word may have multiple possible syntactic realizations,
which are exemplified by different sentences in the corpus,
we try to match the syntactic features in the intermediate for-
mat with all the rules available for the target word, and com-
pare their matching scores. The rule with the highest score
is selected, and used for semantic role assignment. Through
this scoring scheme, the matching algorithm tries to maxi-
mize the number of syntactic realizations for semantic roles
defined in FrameNet rules.
Notice that the semantic role assignment is performed re-
cursively, until all roles within frames triggered by all target
words are assigned.
4.2.1 Walk-Through Example
Assume the following two rules, derived from FrameNet for
the target word come:
1:[[ext,np,before,active,theme],
[obj,np,after,active,goal],
[comp,pp,after,active,by,mode_of_transportation]]
2:[[ext,np,before,active,theme],
[obj,np,after,active,goal],
[comp,pp,after,active,from,source]]
And the sentences:
A: I come here by train.
B: I come here from home.
The syntactic features identified by the syntactic analyzer for
these two sentences are:
A?:[[ext,np,before,active], [obj,np,after,active],
$[$comp,pp,after,active,by]]
B?:[[ext,np,before,active], [obj,np,after,active],
$[$comp,pp,after,active,from]]
Using the matching/scoring algorithm, the score for match-
ing A? to rule 1 is determined as 3, and to rule 2 as 2.
Hence, the matching algorithm selects rule 1, and the se-
mantic role for train is mode of transportation. Similarly,
when we match B? to rule 1, we obtain a score of 2, and a
larger score of 3 for matching with rule 2. Therefore, for the
second case, the role assigned to home is source.
4.3 Applying Default Rules
In a sentence, semantic roles are played by the subject, ob-
jects, and the prepositional phrases attached to the inter-
action described by the sentence. However, FrameNet de-
fines roles only for some of these elements, and therefore
the meaning of some sentence constituents cannot be deter-
mined using the rules extracted from FrameNet. In order to
handle these constituents, and allow for a complete seman-
tic interpretation of the sentence, we have defined a set of
default rules that are applied as a last step in the process of
semantic parsing. For example, FrameNet defines a role for
the prepositional phrase on him in ?I depend on him?, but it
does not define a role for the phrase on the street in ?I walk
on the street?. To handle the interpretation of this phrase,
we apply the default rule that ?on something? modifies the
location attribute of an interaction.
We have defined about 100 such default rules, which are
assigned in the last step of the semantic parsing process, if
no other rule could be applied in previous steps. After this
step, the semantic structure of the sentence is produced.
5 Parser Output and Evaluation
The semantic parser is demonstrated in this conference,
which is perhaps the best evaluation we can offer. We
illustrate here the output of the semantic parser on a natural
language sentence, and show the corresponding semantic
structure and tree. For example, for the sentence I like to
eat Mexican food because it is spicy, the semantic parser
produces the following encoding of sentence type, frames,
semantic constituents and roles, and various attributes and
modifiers:
T = assertion
P =
[[experiencer, [[entity, [i], reference(first)],
[modification(attribute), quantity(single)]]],
[interaction(experiencer\_subj),[love]],
[modification(attribute), time(present)],
[content, [
[interaction(ingestion), [eat]],
[ingestibles, [entity, [food]]
[[modification(restriction), [mexican]],
]]]],
[reason, [[agent, [[entity, [it], reference(third)],
[modification(attribute), quantity(single)]]],
[description,
[modification(attribute), time(present)]],
[modification(attribute), taste\_property(spicy)]]]
]
The corresponding semantic tree is shown in Figure 1.
ingestion ), [eat]interaction(
I love to eat Mexican food, because it is spicy.
{[I], reference(first)}
S?[assertion]
interaction( experiencer_subj ), [love]
{[it], reference(third)}
time(present)
quantity(single) {food}
{mexican}
taste_property(spicy)
ingestibles
experiencer
content reason
am am 
sm 
am
Figure 1: Semantic parse tree (am = attributive modifier, rm =
referential modifier, sm = restrictive modifier)
We have conducted evaluations of the semantic role as-
signment algorithm on 350 sentences randomly selected
from FrameNet. The test sentences were removed from
the FrameNet corpus, and the rules-learning procedure de-
scribed earlier in the paper was invoked on this reduced cor-
pus. All test sentences were then semantically parsed, and
full semantic annotations were produced for each sentence.
Notice that the evaluation is conducted only for semantic
role assignment ? since this is the only information avail-
able in FrameNet. The other semantic annotations produced
by the parser (e.g. attribute, gender, countability) are not
evaluated at this point, since there are no hand-validated an-
notations of this kind available in current resources.
Both frames and frame elements are automatically identi-
fied by the parser. Out of all the elements correctly iden-
tified, we found that 74.5% were assigned with the cor-
rect role (this is therefore the accuracy of role assignment),
which compares favorably with previous results reported in
the literature for this task. Notice also that since this is a
rule-based approach, the parser does not need large amounts
of annotated data, but it works well the same for words for
which only one or two sentences are annotated.
6 Related Work
All previous work in semantic parsing has exclusively fo-
cused on labeling semantic roles, rather than analyzing the
full structure of sentence semantics, and is usually based on
statistical models - e.g. (Gildea and Jurafsky, 2000), (Fleis-
chman et al, 2003). To our knowledge, there was no pre-
vious attempt on performing semantic annotations using al-
ternative rule-based algorithms. However, a rule-based ap-
proach is closer to the way humans interpret the semantic
structure of a sentence. Moreover, as mentioned earlier, the
FrameNet data is not meant to be ?statistically representa-
tive?, but rather illustrative for various language constructs,
and therefore a rule-based approach is more suitable for this
lexical resource.
7 Conclusions
We described a rule-based approach to open text seman-
tic parsing. The semantic parser has the capability to an-
alyze the semantic structure of a sentence, and show how
the meaning of the entire sentence is composed of smaller
semantic units, linked by various semantic relations. The
parsing process relies on rules derived from a frame dataset
(FrameNet) and a semantic network (WordNet). We believe
that the semantic parser will prove useful for a range of
language processing applications that require knowledge of
text meaning, including word sense disambiguation, infor-
mation extraction, question answering, machine translation,
and others.
References
M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum en-
tropy models for FrameNet classification. In Proceedings of
2003 Conference on Empirical Methods in Natural Language
Processing EMNLP-2003, Sapporo, Japan.
D. Gildea and D. Jurafsky. 2000. Automatic labeling of semantic
roles. In Proceedings of the 38th Annual Conference of the As-
sociation for Computational Linguistics (ACL-00), pages 512?
520, Hong Kong, October.
C. Johnson, C. Fillmore, M. Petruck, C. Baker, M. Ellsworth,
J. Ruppenhofer, and E. Wood. 2002. FrameNet: Theory and
Practice. http://www.icsi.berkeley.edu/ framenet.
G. Miller. 1995. Wordnet: A lexical database. Communication of
the ACM, 38(11):39?41.
An Algorithm for Open Text Semantic Parsing
Lei Shi and Rada Mihalcea
Department of Computer Science
University of North Texas
leishi@unt.edu, rada@cs.unt.edu
Abstract
This paper describes an algorithm for open text shal-
low semantic parsing. The algorithm relies on a
frame dataset (FrameNet) and a semantic network
(WordNet), to identify semantic relations between
words in open text, as well as shallow semantic fea-
tures associated with concepts in the text. Parsing
semantic structures allows semantic units and con-
stituents to be accessed and processed in a more
meaningful way than syntactic parsing, moving the
automation of understanding natural language text
to a higher level.
1 Introduction
The goal of the semantic parser is to analyze the
semantic structure of a natural language sentence.
Similar in spirit with the syntactic parser ? whose
goal is to parse a valid natural language sentence
into a parse tree indicating how the sentence can
be syntactically decomposed into smaller syntactic
constituents ? the purpose of the semantic parser is
to analyze the structure of sentence meaning. Sen-
tence meaning is composed by entities and interac-
tions between entities, where entities are assigned
semantic roles, and can be further modified by other
modifiers. The meaning of a sentence is decom-
posed into smaller semantic units connected by var-
ious semantic relations by the principle of compo-
sitionality, and the parser represents the semantic
structure ? including semantic units as well as se-
mantic relations, connecting them into a formal for-
mat.
In this paper, we describe the main components
of the semantic parser, and illustrate the basic pro-
cedures involved in parsing semantically open text.
We believe that such structures, reflecting various
levels of semantic interpretation of the text, can be
used to improve the quality of text processing appli-
cations, by taking into account the meaning of text.
The paper is organized as follows. We first de-
scribe the semantic structure of English sentences,
as the basis for semantic parsing. We then intro-
duce the knowledge bases utilized by the parser, and
show how we use this knowledge in the process of
semantic parsing. Next, we describe the parsing
algorithm and elaborate on each of the three main
steps involved in the process of semantic parsing:
(1) syntactic and shallow semantic analysis, (2) se-
mantic role assignment, and (3) application of de-
fault rules. Finally, we illustrate the parsing process
with several examples, and show how the semantic
parsing algorithm can be integrated into other lan-
guage processing systems.
2 Semantic Structure
Semantics is the denotation of a string of symbols,
either a sentence or a word. Similar to a syn-
tactic parser, which shows how a larger string is
formed by smaller strings from a formal point of
view, the semantic parser shows how the denotation
of a larger string ? sentence, is formed by deno-
tations of smaller strings ? words. Syntactic rela-
tions can be described using a set of rules about how
a sentence string is formally generated using word
strings. Instead, semantic relations between seman-
tic constituents depend on our understanding of the
world, which is across languages and syntax.
We can model the sentence semantics as describ-
ing entities and interactions between entities. Enti-
ties can represent physical objects, as well as time,
places, or ideas, and are usually formally realized
as nouns or noun phrases. Interactions, usually real-
ized as verbs, describe relationships or interactions
between participating entities. Note that a partic-
ipant can also be an interaction, which can be re-
garded as an entity nominalized from an interaction.
We assign semantic roles to participants, and their
semantic relations are identified by the case frame
introduced by their interaction. In a sentence, par-
ticipants and interactions can be further modified
by various modifiers, including descriptive modi-
fiers that describe attributes such as drive slowly,
restrictive modifiers that enforce a general denota-
tion to become more specific such as musical in-
strument, referential modifiers that indicate partic-
ular instances such as the pizza I ordered. Other
semantic relations can also be identified, such as
coreference, complement, and others. Based on the
principle of compositionality, the sentence semantic
structure is recursive, similar to a tree.
The semantic parser analyzes shallow-level se-
mantics, which is derived directly from linguis-
tic knowledge, such as rules about semantic
role assignment, lexical semantic knowledge, and
syntactic-semantic mappings, without taking into
account any context or common sense knowledge.
The parser can be used as an intermediate semantic
processing tool before higher levels of text under-
standing.
3 Knowledge Bases for Semantic Parsing
One major problem faced by many natural language
understanding applications that rely on syntactic
analysis of text, is the fact that similar syntactic pat-
terns may introduce different semantic interpreta-
tions. Likewise, similar meanings can be syntac-
tically realized in many different ways. The seman-
tic parser attempts to solve this problem, and pro-
duces a syntax-independent representation of sen-
tence meaning, so that semantic constituents can be
accessed and processed in a more meaningful and
flexible way, avoiding the sometimes rigid interpre-
tations produced by a syntactic analyzer. For in-
stance, the sentences I boil water and water boils
contain a similar relation between water and boil,
even though they have different syntactic structures.
To deal with the large number of cases where the
same syntactic relation introduces different seman-
tic relations, we need knowledge about how to map
syntax to semantics. To this end, we use two main
types of knowledge ? about words, and about rela-
tions between words. The first type of knowledge
is drawn from WordNet ? a large lexical database
with rich information about words and concepts.
We refer to this as word-level knowledge. The lat-
ter is derived from FrameNet ? a resource that con-
tains information about different situations, called
frames, in which semantic relations are syntacti-
cally realized in natural language sentences. We
call this sentence-level knowledge. In addition to
these two lexical knowledge bases, the parser also
utilizes a set of manually defined rules, which en-
code mappings from syntactic structures to seman-
tic relations, and which are also used to handle those
structures not explicitly addressed by FrameNet or
WordNet.
In this section, we describe the type of infor-
mation extracted from these knowledge bases, and
show how this information is encoded in a format
accessible to the semantic parser.
3.1 Frame Identification and Semantic Role
Assignment
FrameNet (Johnson et al, 2002) provides the
knowledge needed to identify case frames and se-
mantic roles. FrameNet is based on the theory of
frame semantics, and defines a sentence level on-
tology. In frame semantics, a frame corresponds to
an interaction and its participants, both of which
denote a scenario, in which participants play some
kind of roles. A frame has a name, and we use this
name to identify the semantic relation that groups
together the semantic roles. In FrameNet, nouns,
verbs and adjectives can be used to identify frames.
Each annotated sentence in FrameNet exempli-
fies a possible syntactic realization for the seman-
tic roles associated with a frame for a given target
word. By extracting the syntactic features and cor-
responding semantic roles from all annotated sen-
tences in the FrameNet corpus, we are able to auto-
matically build a large set of rules that encode the
possible syntactic realizations of semantic frames.
In our implementation, we use only verbs as
target words for frame identification. Currently,
FrameNet defines about 1700 verbs attached to 230
different frames. To extend the parser coverage to
a larger subset of English verbs, we are using Verb-
Net (Kipper et al, 2000), which allows us to handle
a significantly larger set of English verbs.
VerbNet is a verb lexicon compatible with Word-
Net, but with explicitly stated syntactic and se-
mantic information using Levin?s verb classification
(Levin, 1993). The fundamental assumption is that
the syntactic frames of a verb as an argument-taking
element are a direct reflection of the underlying se-
mantics. Therefore verbs in the same VerbNet class
usually share common FrameNet frames, and have
the same syntactic behavior. Hence, rules extracted
from FrameNet for a given verb can be easily ex-
tended to verbs in the same VerbNet class. To en-
sure a correct outcome, we have manually validated
the FrameNet-VerbNet mapping, and corrected the
few discrepancies that were observed between Verb-
Net classes and FrameNet frames.
3.1.1 Rules Learned from FrameNet
FrameNet data ?is meant to be lexicographically rel-
evant, not statistically representative? (Johnson et
al., 2002), and therefore we are using FrameNet as
a starting point to derive rules for a rule-based se-
mantic parser.
To build the rules, we are extracting several syn-
tactic features. Some are explicitly encoded in
FrameNet, such as the grammatical function (GF)
and phrase type (PT) features.
In addition, other syntactic features are extracted
from the sentence context. One such feature is the
relative position (RP) to the target word. Sometimes
the same syntactic constituent may play different se-
mantic roles according to its position with respect
to the target word. For instance the sentences: I pay
you. and You pay me. have different roles assigned
to the same lexical unit you based on the relative
position with respect to the target word pay.
Another feature is the voice of the sentence. Con-
sider these examples: I paid Mary 500 dollars. and
I was paid by Mary 500 dollars. In these two sen-
tences, I has the same values for the features GF, PT
and RP, but it plays completely different roles in the
same frame because of the difference of voice.
If the phrase type is prepositional phrase (PP), we
also record the actual preposition that precedes the
phrase. Consider these examples: I was paid for my
work. and I was paid by Mary. The prepositional
phrases in these examples have the same values for
the features GF, PT, and RP, but different preposi-
tions differentiate the roles they should play.
After we extract all these syntactic features, the
semantic role is appended to the rule, which creates
a mapping from syntactic features to semantic roles.
Feature sets are arranged in a list, the order of
which is identical to that in the sentence. The or-
der of sets within the list is important, as illustrated
by the following example: ?I give the boy a ball.?
Here, the boy and a ball have the same features
as described above, but since the boy occurs be-
fore a ball, then the boy plays the role of recipi-
ent. Altogether, the rule for a possible realization
of a frame exemplified by a tagged sentence is an
ordered sequence of syntactic features with their se-
mantic roles.
For instance, Table 1 lists the syntactic and se-
mantic features extracted from FrameNet for the
sentence I had chased Selden over the moor.
I had chased Selden over the moor
GF Ext obj comp
PT NP Target NP PP
Position before after after
Voice active
PP over
Role Theme Goal Path
Table 1: Example sentence with syntactic and se-
mantic features
The corresponding formalized rule for this sen-
tence is:
[active, [ext,np,before,theme], [obj,np,
after,goal], [comp,pp,after,over,path]]
In FrameNet, there are multiple annotated sen-
tences for each frame to demonstrate multiple pos-
sible syntactic realizations. All possible realizations
of a frame are collected and stored in a list for that
frame, which also includes the target word, its syn-
tactic category, and the name of the frame. All the
frames defined in FrameNet are transformed into
this format, so that they can be easily handled by
the rule-based semantic parser.
3.2 Word Level Knowledge
WordNet (Miller, 1995) is the resource used to iden-
tify shallow semantic features that can be attached
to lexical units. For instance, attribute relations,
adjective/adverb classifications, and others, are se-
mantic features extracted from WordNet and stored
together with the words, so that they can be directly
used in the parsing process.
All words are uniformly defined, regardless of
their class. Features are assigned to each word, in-
cluding syntactic and shallow semantic features, in-
dicating the functions played by the word. Syntactic
features are used by the feature-augmented syntac-
tic analyzer to identify grammatical errors and pro-
duce syntactic information for semantic role assign-
ment. Semantic features encode lexical semantic in-
formation extracted from WordNet that is used to
determine semantic relations between words in var-
ious situations.
Features can be arbitrarily defined, as long as
there are rules to handle them. The features we
define encode information about the syntactic
category of a word, number and countability for
nouns, transitivity and form for verbs, type, degree,
and attribute for adjectives and adverbs, and others.
Table 2 lists the main features used for content
words.
Feature Values
Nouns
Number singular/plural
Countability countable/uncountable
Verbs
Transitivity transitive/intransitive/double transitive
Form normal/infi nitive/present
participle/past participle
Adjectives
Type descriptive/restrictive/referential
Attribute arbitrary
Degree base/comparative/superlative
Adverbs
Type descriptive/restrictive/referential
Attribute arbitrary
Degree base/comparative/superlative
Table 2: Features for content words
For example, for the word dog, the entry in the
lexicon is defined as:
lex(dog,W):- W= [parse:dog, cat:noun,
num:singular, count:countable].
Here, the category (cat) is defined as noun, the
number (num) is singular, and we also record the
countability (count)1.
For adjectives, the value of the attribute feature
is also stored, which is provided by the attribute re-
lation in WordNet. This relation links a descriptive
adjective to the attribute (noun) it modifies, such as
slow ? speed. For example, for the adjective slow,
the entry in the lexicon is defined as:
lex(slow,W):- W= [parse:slow, cat:adj,
attr:speed, degree:base, type:descriptive].
Here, the category (cat) is defined as adjective,
the type is descriptive, degree is base form. We also
record the attr feature, which is derived from the at-
tribute relation in WordNet, and links a descriptive
adjective to the attribute (noun) it modifies, such as
slow? speed.
We are also exploiting the transitional relations
from adverbs to adjectives and to nouns. We noticed
that some descriptive adverbs have correspondence
to descriptive adjectives, which in turn are linked to
nouns by the attribute relation. Using these transi-
tional links, we derive relations like: slowly? slow
? speed. A typical descriptive adverb is defined as
follows:
lex(slowly,W):- W= [parse:slowly, cat:adv,
attr:speed, degree:base, type:descriptive].
In addition to incorporating semantic information
from WordNet into the lexicon, this word level on-
tology is also used to derive default rules, as dis-
cussed later.
3.3 Hand-coded Knowledge
The FrameNet database encodes various syntac-
tic realizations only for semantic roles within a
frame. Syntax-semantics mappings other than se-
mantic roles are manually encoded as rules inte-
grated in the syntactic-semantic analyzer. The an-
alyzer determines the syntactic structure of the sen-
tence, and once a particular syntactic constituent
is identified, its corresponding mapping rules are
immediately applied. The syntactic constituent is
1The value for the feature (countability) is obtained
from word properties stored in the Link parser dictionaries
(http://www.link.cs.cmu.edu/link/). The Link dictionaries are
also used to derive the lists of words to be stored in the lexi-
con. Note however that the Link parser itself is not used in the
parsing process.
then translated into its corresponding semantic con-
stituent, together with the relevant semantic infor-
mation.
Some semantic relations can be directly derived
from syntactic patterns. For example, a restrictive
relative clause such as ?the man that you see? serves
as a referential modifier. An adverbial clause be-
ginning with ?because? is a modifier describing the
?reason? of the interaction. The inflection from
?apple? to ?apples? adds an attributive modifier of
quantity to the entity ?apple?.
However, syntactic relations may often introduce
semantic ambiguity, with multiple possible interpre-
tations. To handle these cases, we encode rules that
describe all possible interpretations of any given
structure, and then use lexical semantic informa-
tion as selectional restrictions for ambiguity reso-
lution. For instance, in ?a book on Chinese his-
tory?, on Chinese history describes the topic of the
book and this interpretation can be uniquely deter-
mined by noting that history is not a physical object,
and thus the interpretation of on Chinese history as
describing location is semantically anomalous. In-
stead, in ?a book on the computer?, on the computer
may describe a location, but it could also describe
the book topic, and hence the correct interpretation
of this sentence cannot be determined without ad-
ditional context. In such cases, the semantic parser
produces all possible interpretations, allowing sys-
tems that use the semantic parser?s output to deter-
mine the right interpretation that best fits the appli-
cation at hand.
Selectional restrictions ? as part of the hand-
coded knowledge ? are used for both semantic
role identification and syntax-semantics translation.
These additional rules are needed to supplement the
information encoded in FrameNet, since FrameNet
only annotates syntactic features, which often times
do not provide enough information for identifying
correct semantic roles.
Consider for example ?I break the window? vs.
?The hammer breaks the window?. According to
our semantic parser, the participants in the interac-
tion ?break? have exactly the same syntactic fea-
tures in both sentences, but they play different se-
mantic roles (?I? plays the agent role while ?ham-
mer? plays the instrument role), since they belong
to different ontological categories: ?I? refers to a
person and ?hammer? refers to a tool. This interpre-
tation is not possible using only FrameNet informa-
tion, and thus we fill the gap by attaching selectional
restrictions to the rules extracted from FrameNet.
The definition of selectional restriction is based
on WordNet 2.0 noun hierarchy. We say that en-
tity E belongs to the ontological category C if the
noun E is a child node of C in the WordNet seman-
tic hierarchy of nouns. For example, if we define
the ontological category for the role ?instrument? as
instrumentality, then all hyponyms of instrumental-
ity can play this role, while other nouns like ?boy?,
which are not part of the instrumentality category
will be rejected. Selectional restrictions are defined
using a Disjunctive Normal Form (DNF) in the fol-
lowing format:
[Onto(ID,P),Onto(ID,P),...],[Onto(ID,P),...],...
Here, ?Onto? is a noun and ID is its Word-
Net sense, which uniquely identifies Onto as a
node in the semantic network. ?P? can be set
to p (positive) or n (negative), denoting if a noun
should belong to the given category or not. For
example, [person(1,n),object(1,p)],[substance(1,p)]
means that the noun should belong to object(sense
#1) but not person(sense #1)2, or it should belong
to substance(sense #1). This information is added
to the rules derived from FrameNet, and therefore
after this step, a complete FrameNet rule entry is:
[Voice,[GF,PT,SelectionalRestriction,Role],...].
4 Semantic Parsing
The general procedure of semantic parsing consists
of three main steps3: (1) The syntactic-semantic
analyzer analyzes the syntactic structure, and uses
hand-coded rules as well as lexical semantic knowl-
edge to identify some semantic relations between
constituents. It also prepares syntactic features for
semantic role assignment in the next step. (2) The
role assigner uses rules extracted from FrameNet,
and assigns semantic roles for identified partici-
pants, based on their syntactic features as produced
in the first step. (3) For those constituents not exem-
plified in FrameNet, we apply default rules to decide
their default meaning.
4.1 Feature Augmented Syntactic-Semantic
Analyzer
The analyzer is implemented as a bottom-up chart
parsing algorithm based on features. We include
rules of syntax-semantics mappings in the unifica-
tion based formalism. The parser analyzes syntac-
tic relations and immediately applies corresponding
mapping rules to obtain semantic relations when a
2person(sense #1) is a child node of object(sense #1) in
WordNet
3The parsing algorithm is implemented as a rule-based sys-
tem using a declarative programming language Prolog.
syntactic relation is identified. Most semantic rela-
tions (e.g. various modifiers) are identified in this
step, except semantic role annotation and applica-
tion of default rules, which are postponed for a later
stage. The analyzer generates an intermediate for-
mat, where target words and arguments are explic-
itly tagged with their syntactic and semantic fea-
tures, so that they can be matched against the rules
derived from FrameNet. We are using a feature-
based analyzer that accomplishes three main tasks:
4.1.1 Check if the sentence is grammatically
correct
The syntactic analyzer is based on a feature aug-
mented grammar, and therefore has the capability of
detecting if a sentence is grammatically correct (un-
like statistical parsers, which attempt to parse any
sentence, regardless of their well-formness). The
grammar consists of a set of rules defining how con-
stituents with different syntactic or semantic fea-
tures can unify with each other.
By defining a grammar in this way, using fea-
tures, once the right features are selected, the an-
alyzer can reject some grammatically incorrect sen-
tences such as: I have much apples., You has my
car., and some semantically anomalous sentences:
The technology is very military.4.
4.1.2 Provide features for semantic role
assignment
Through syntactic-semantic analysis in the first
step, sentences are transformed into a format
in which target words and syntactic constituents
are explicitly tagged with their features. Unlike
FrameNet ? which may also assign roles to adverbs,
we only use the subject, object(s) and prepositional
phrases as potential participants in the interaction
for semantic role labeling5. The analyzer marks
verbs as target words for frame identification, iden-
tifies constituents for semantic role assignment, and
produces features such as GF, PT, Voice, Preposi-
tion, as well as ontological categories for each con-
stituent, in a format identical to the rules extracted
from FrameNet, so that they can be matched with
the frame definitions.
The ontological categories of constituents are
used to match selectional restrictions, and are au-
tomatically derived from the head word of the noun
phrase, or the head word of the noun phrase of the
prepositional phrase. For other constituents that
act like nouns, such as pronouns, infinitive forms,
gerunds, or noun clauses, we have manually defined
4Since military is not a descriptive adjective, it cannot be
modifi ed by very and predicative use is forbidden.
5Adverbs are treated as modifi ers.
ontological categories. For example, ?book? is the
ontological category of the phrase ?the interesting
book? and ?on the book?. ?person? is the ontolog-
ical category we manually define for the pronoun
?he?. We have also defined several special onto-
logical categories that are not in WordNet such as
any, which can be matched to any selectional re-
striction, nonperson, which means everything ex-
cept person, and others. Note that this matching
procedure also plays the role of a word sense dis-
ambiguation tool, by selecting only those categories
that match the current frame constituents. After
this step, target words and syntactic constituents can
be assigned with the corresponding case frame and
semantic roles during the second step of semantic
parsing.
4.1.3 Identify some semantic relations
Some semantic relations can be identified in this
phase. These semantic relations include word level
semantic relations, and some semantic relations
that have direct syntactic correspondence by using
syntax-semantics mapping rules. This phase can
also identify the function of the sentence such as
assertion, query, yn-query, command etc, based on
syntactic patterns of the sentence.
The output of the analyzer is an intermediate for-
mat suitable for the semantic parser, which contains
syntactic features and identified semantic relations.
For example, the output for the sentence ?He kicked
the old dog.? is:
[assertion,
[[tag, ext, np, person,
[[entity, [he], reference(third)],
[modification(attribute), quantity(single)],
[modification(attribute), gender (male)]]],
[target, v, kick, active, [kick]],
[modification(attribute), time (past)],
[tag, obj, np, dog,
[[modification(reference), reference(the)],
[modification(attribute), age(old)],
[target, n, dog, [dog]]]]]
]
4.2 Semantic Role Assignment
In the process of semantic role assignment, we first
start by identifying all possible frames, according
to the target word. Next, a matching algorithm is
used to find the most likely match among all rules
of these frames, to identify the correct frame (or
frames if several are possible), and assign semantic
roles.
In a sentence describing an interaction, we select
the verb as the target word, which triggers the sen-
tence level frame and uses the FrameNet rules of
that target word for matching. If the verb is not
defined in FrameNet and VerbNet, we use Word-
Net synonymy relation to check if any of its syn-
onyms is defined in FrameNet or VerbNet. If such
synonyms exist, their rules are applied to the tar-
get word. This approach is based on the idea in-
troduced by Levin that ?what enables a speaker to
determine the behavior of a verb is its meaning?
(Levin, 1993). Synonymous verbs always intro-
duce the same semantic frame and usually have the
same syntactic behavior. To minimize information
in the verb lexicon, non-frequently used verbs usu-
ally inherit a subset of the syntactic behavior of
their frequently used synonyms. Since VerbNet has
defined a framework of syntactic-semantic behav-
ior for these frequently used verbs, the behavior of
other related verbs can be quite accurately predicted
by using WordNet synonymy relations. Using this
approach, we achieve a coverage of more than 3000
verbal lexical units.
The matching algorithm relies on a scoring
scheme to evaluate the similarity between two se-
quences of features. The matching starts from the
first constituent of the sentence. It looks through
the list of entries in the rule and when a match is
found, it moves to the next constituent looking for
a new match. A match involves match of syntactic
features, as well as match of selectional restrictions.
An exact match means that both syntactic features
and selectional restrictions are matched, which in-
crements the score of matching by 3. We apply
selectional restriction by looking up the WordNet
noun hierarchies. If the node of the ontological cat-
egory is within the areas that the selectional restric-
tion describes, this is regarded as a match. When
applying selectional restrictions, due to polysemy
of the ontological entries, we try all possible senses,
starting from the most frequently used sense accord-
ing to WordNet, until one sense meets the selec-
tional restriction. If the syntactic features match ex-
actly, but none of the possible word senses meet the
selectional restrictions, this is regarded as a partial
match, which increments the score by 2.
Partial matching is also possible, for a relaxed
application of selectional restriction. This enables
anaphora and metaphor resolution, in which the
constituents have either unknown ontological cate-
gory, or inherit features from other ontological cat-
egories (by applying high level knowledge such as
personification). The number of subjects and ob-
jects as well as their relative positions should be
strictly obeyed, since any variations may result in
significant differences for semantic role labeling.
Prepositional phrases are free in their location be-
cause the preposition is already a unique identi-
fier. Finally, after all constituents have found their
match, if there are still remaining entries in the
rule, the total score is decreased by 1. This is a
penalty paid by partial matches, since additional
constituents may indicate different semantic role la-
beling, which may change the interpretation of the
entire sentence.
A polysemous verb may belong to multiple
frames, and a frame pertaining to a given target
word may have multiple possible syntactic realiza-
tions, exemplified by different sentences in the cor-
pus. We try to match the syntactic features in the in-
termediate format with all the rules of all the frames
available for the target word, and compare their
matching scores. The rule with the highest score
is selected, and used for semantic role assignment.
Through this scoring scheme, the matching algo-
rithm tries to maximize the utilization of syntactic
and semantic information available in the sentence,
to correctly identify case frames and semantic roles.
4.2.1 Walk-Through Example
Assume the following two rules, triggered for the
target word break:
1: [active,[ext,np,[[person(1,p)]],agent],
[obj,np,[[object(1,p)]],theme],
[comp,pp,with,[[instrumentality(3,p)]],
instrument]]
2: [[ext,np,[[instrumentality(3,p)]],instrument],
[obj,np,[[person(1,n),object(1,p)]],theme]]
3: [[ext,np,[[person(1,n),object(1,p)]],theme]]
And the sentences:
A: I break the window with a hammer
B: The hammer breaks the window
C: The window breaks on the wall
The features identified by the analyzer are:
A?:[[ext,np,active,person],
[obj,np,active,window],
[comp,pp,active,with,hammer]]
B?:[[ext,np,active,hammer],
[obj,np,active,window]]
C?:[[ext,np,active,window],
[comp,pp,on,wall]]
Using the matching/scoring algorithm, the score
for matching A? to rule 1 is determined as 9 since
there are 3 exact matches, and to rule 2 as 5 since
there is an exact match for ?the window? but a par-
tial match for ?I?. Hence, the matching algorithm
selects rule 1, and the semantic role for ?I? is agent.
Similarly, when we match B? to rule 1, we obtain a
score of 4, since there is an exact match for ?the
window?, a partial match for ?the hammer?, and
rule 1 has an additional entry for a prepositional
phrase, which decrements the score by 1. It makes
a larger score of 6 for matching with rule 2. There-
fore, for the second case, the role assigned to ?the
hammer? is instrument. Rule 3 is not applied to the
first two sentences since they have additional ob-
jects; similarly, rule 1 and 2 cannot be applied to
sentence C for the same reason. The first constituent
in C finds an exact match in rule 3 with a total score
of 3, and hence ?the window? is assigned the correct
role theme. The prepositional phrase ?on the wall?,
for which no entry for labeling a role is found in rule
3, will be handled by default rules (see Section 4.3).
Based on the principle of compositionality, mod-
ifiers and constituents assigned semantic roles can
describe interactions, so the semantic role assign-
ment is performed recursively, until all roles within
frames triggered by all target words are assigned.
4.3 Applying Default Rules
We always assign semantic roles to subjects and ob-
jects6, but only some prepositional phrases can in-
troduce semantic roles, as defined in the FrameNet
case frames. Other prepositional phrases function
as modifiers; in order to handle these constituents,
and allow for a complete semantic interpretation of
the sentence, we have defined a set of default rules
that are applied as the last step of the semantic pars-
ing process. For example, FrameNet defines a role
for the prepositional phrase on him in ?I depend
on him? but not for on the street in ?I walk on the
street?, because it does not play a role, but it is a
modifier describing a location. Since the role for
the prepositional phrase beginning with on is not de-
fined for the target word walk in FrameNet, we ap-
ply the default rule that ?on something? modifies the
location attribute of the interaction walk. Note that
we include selectional restriction in the default rule
since constituents with the same syntactic features
such as ?on Tuesday? and ?on the table? may have
obviously different semantic interpretations. An ex-
ample of a default rule is shown below, indicating
that the interpretation of a prepositional phrase fol-
lowed by a time period (where time period is an
ontological category from WordNet) is that of time
modifier:
DefaultRule([_,pp,_,on,Onto,_],time):-
SelectionalRestriction
(Onto,1,
[[time_period(1,p)]])
We have defined around 100 such default rules,
which are applied during the last step of the seman-
6Where a subject and object are usually realized by noun
phrases, noun clauses, or infi nitive forms.
tic parsing process.
5 Parser Output and Evaluation
We illustrate here the output of the semantic parser
on a natural language sentence, and show the
corresponding semantic structure and tree7. For
example, for the sentence I like to eat Mexican food
because it is spicy, the semantic parser produces the
following encoding of sentence type, frames, se-
mantic constituents and roles, and various attributes
and modifiers:
T = assertion
P =
[[experiencer, [[entity, [i], reference(first)],
[modification(attribute), quantity(single)]]],
[interaction(experiencer_subj),[love]],
[modification(attribute), time(present)],
[content, [
[interaction(ingestion), [eat]],
[ingestibles, [entity, [food]]
[[modification(restriction), [mexican]],
]]]],
[reason, [[agent, [[entity, [it],
reference(third)],
[modification(attribute), quantity(single)]]],
[description,
[modification(attribute), time(present)]],
[modification(attribute),
taste_property(spicy)]]]
]
The corresponding parse tree is shown in Figure 1.
ingestion ), [eat]interaction(
I love to eat Mexican food, because it is spicy.
{[I], reference(first)}
S?[assertion]
interaction( experiencer_subj ), [love]
{[it], reference(third)}
time(present)
quantity(single) {food}
{mexican}
taste_property(spicy)
ingestibles
experiencer content reason
am am
sm
am
Figure 1: Semantic parse tree (am = attributive modifi er,
rm = referential modifi er, sm = restrictive modifi er)
We have conducted evaluations of the semantic
role assignment algorithm on 350 sentences ran-
domly selected from FrameNet. The test sentences
were removed from the FrameNet corpus, and the
rules-extraction procedure described earlier in the
paper was invoked on this reduced corpus. All test
sentences were then semantically parsed, and full
semantic annotations were produced for each sen-
tence. Notice that the evaluation is conducted only
7The semantic parser was demonstrated in a major Natural
Language Processing conference, and can be also demonstrated
during the workshop.
for semantic role assignment ? since this is the only
information available in FrameNet. The other se-
mantic annotations produced by the parser (e.g. at-
tribute, gender, countability) are not evaluated at
this point, since there are no hand-validated anno-
tations of this kind available in current resources.
Both frames and frame elements are automati-
cally identified by the parser. Out of all the elements
correctly identified, we found that 74.5% were as-
signed with the correct role (this is therefore the
accuracy of role assignment), which compares fa-
vorably with previous results reported in the liter-
ature for this task. Notice also that since this is a
rule-based approach, the parser does not need large
amounts of annotated data, and it works well the
same for words for which only one or two sentences
are annotated.
6 Interface and Integration to Other
Systems
The semantic algorithm uses linguistic knowledge,
such as syntactic realization of semantic roles in a
case frame, syntax-semantics mappings, and lexical
semantic knowledge, to parse the semantic structure
of open text. It can be regarded as a shallow se-
mantic analyzer, which provides partial results for
higher level understanding systems that can effec-
tively utilize context, commonsense, and other types
of knowledge, to achieve final accurate meaning in-
terpretations, or use custom defined rules for high
level processing in particular domains.
The matching/scoring scheme integrated in our
algorithm can effectively identify the right semantic
interpretation, but some semantic ambiguity cannot
be resolved without enough context and common-
sense knowledge. For example, although the fa-
mous meaningless sentence ?colorless green ideas
sleep furiously? can be correctly identified as se-
mantically anomalous by the semantic parser, by
analyzing the syntactic behavior of ?sleep? and the
selectional restrictions that we attach to this frame,
the sentence ?I saw the man in the park with the
telescope? has several semantic interpretations. Ac-
cording to the commonsense knowledge that we en-
code in the semantic parser (mostly drawn from
WordNet), telescope is defined as a tool to see some-
thing, and we may infer that ?with telescope? in this
sentence describes an instrument of ?see?. How-
ever, without enough context, not even humans can
rule out the possibility that the ?telescope? is the
man?s possession, rather than an instrument for the
interaction ?see?. The semantic parser maintains all
possible interpretations that cannot be rejected by
their syntactic and shallow semantic patterns, and
rank all of them by their scores as the likelihood of
being the correct interpretation. Other systems can
use high level knowledge such as common sense,
context or user defined rules to choose the right in-
terpretation.
As an integral part of the parsing system, we pro-
vide several interfaces that allow other systems or
additional modules to change the behavior of the
parser based on their rules and knowledge. One
such interface is the ontochg predicate, which is
called whenever the ontological category is identi-
fied for a constituent during the syntactic-semantic
analysis. By default, it outputs the same ontolog-
ical category as identified by the parser, but other
systems can change the content of this predicate to
replace the ontological category identified by the
parser with other categories, according to their rules
and knowledge. This is particularly useful for inte-
grating add-ons capable of anaphora and metaphor
resolution. The adjatt predicate is another interface
for add-ons that can resolve polysemy of descriptive
adjectives and adverbs. Due to polysemy, some de-
scriptive adjectives and adverbs may modify differ-
ent attributes in different situations and sometimes
the resolution requires high level understanding us-
ing commonsense knowledge and context. These
interfaces make the semantic parser more flexible,
robust, and easier to integrate into other systems that
achieve high level meaning processing and under-
standing.
7 Related Work
There are several statistical approaches for auto-
matic semantic role labeling based on PropBank
and FrameNet. (Gildea and Jurafsky, 2000) pro-
posed a statistical approach based on FrameNet I
data for annotation of semantic roles. Fleischman
(Fleischman et al, 2003) used FrameNet annota-
tions in a maximum entropy framework. A more
flexible generative model is proposed in (Thomp-
son et al, 2003), where null-instantiated roles can
be also identified, and frames are not assumed to be
known a-priori. These approaches exclusively focus
on semantic roles labeling based on statistical meth-
ods, rather than analysis of the full structure of sen-
tence semantics. However, a rule-based approach
is closer to the way humans interpret the semantic
structure of a sentence. Moreover, as mentioned
earlier, the FrameNet data is not meant to be ?sta-
tistically representative? (Johnson et al, 2002), but
rather illustrative for various language constructs,
and therefore a rule-based approach is more suitable
for this lexical resource.
8 Conclusions
In this paper, we proposed an algorithm for open
text shallow semantic parsing. The algorithm has
the capability to analyze the semantic structure of
a sentence, and show how the meaning of the en-
tire sentence is composed of smaller semantic units,
linked by various semantic relations. The parsing
process utilizes linguistic knowledge, consisting of
rules derived from a frame dataset (FrameNet), a se-
mantic network (WordNet), as well as hand-coded
rules of syntax-semantics mappings, which encode
natural selectional restrictions. Parsing semantic
structures allows semantic units and constituents to
be accessed and processed in a more meaningful
way than syntactic parsing, and enables higher-level
text understanding applications. We believe that the
semantic parser will prove useful for a range of lan-
guage processing applications that require knowl-
edge of text meaning, including word sense disam-
biguation, information retrieval, question answer-
ing, machine translation, and others.
References
M. Fleischman, N. Kwon, and E. Hovy. 2003.
Maximum entropy models for FrameNet classi-
fication. In Proceedings of 2003 Conference on
Empirical Methods in Natural Language Pro-
cessing EMNLP-2003, Sapporo, Japan.
D. Gildea and D. Jurafsky. 2000. Automatic label-
ing of semantic roles. In Proceedings of the 38th
Annual Conference of the Association for Com-
putational Linguistics (ACL 2000), pages 512?
520, Hong Kong, October.
C. Johnson, C. Fillmore, M. Petruck, C. Baker,
M. Ellsworth, J. Ruppenhofer, and E. Wood.
2002. FrameNet: Theory and Practice.
http://www.icsi.berkeley.edu/ framenet.
K. Kipper, H.T.Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceed-
ings of Seventeenth National Conference on Arti-
ficial Intelligence AAAI 2000, Austin,TX, July.
B. Levin. 1993. English Verb Classes and Alterna-
tion: A Preliminary Investigation. The Univer-
sity of Chicago Press.
G. Miller. 1995. Wordnet: A lexical database.
Communication of the ACM, 38(11):39?41.
C. Thompson, R. Levy, and C. Manning. 2003. A
generative model for FrameNet semantic role la-
beling. In Proceedings of the Fourteenth Euro-
pean Conference on Machine Learning ECML-
2003, Croatia.
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1057?1067,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Cross Language Text Classification by Model Translation and
Semi-Supervised Learning
Lei Shi
Yahoo! Global R&D
Beijing, China
lshi@yahoo-inc.com
Rada Mihalcea
University of North Texas
Denton, TX, U.S.A.
rada@cs.unt.edu
Mingjun Tian
Yahoo! Global R&D
Beijing, China
mingjun@yahoo-inc.com
Abstract
In this paper, we introduce a method that au-
tomatically builds text classifiers in a new lan-
guage by training on already labeled data in
another language. Our method transfers the
classification knowledge across languages by
translating the model features and by using
an Expectation Maximization (EM) algorithm
that naturally takes into account the ambigu-
ity associated with the translation of a word.
We further exploit the readily available un-
labeled data in the target language via semi-
supervised learning, and adapt the translated
model to better fit the data distribution of the
target language.
1 Introduction
Given the accelerated growth of the number of mul-
tilingual documents on the Web and elsewhere, the
need for effective multilingual and cross-lingual text
processing techniques is becoming increasingly im-
portant. There is a growing number of methods that
use data available in one language to build text pro-
cessing tools for another language, for diverse tasks
such as word sense disambiguation (Ng et al, 2003),
syntactic parsing (Hwa et al, 2005), information re-
trieval (Monz and Dorr, 2005), subjectivity analysis
(Mihalcea et al, 2007), and others.
In this paper, we address the task of cross-lingual
text classification (CLTC), which builds text classi-
fiers for multiple languages by using training data in
one language, thereby avoiding the costly and time-
consuming process of labeling training data for each
individual language. The main idea underlying our
approach to CLTC is that although content can be
expressed in different forms in different languages,
there is a significant amount of knowledge that is
shared for similar topics that can be effectively used
to port topic classifiers across languages.
Previous methods for CLTC relied mainly on ma-
chine translation, by translating the training data into
the language of the test data or vice versa, so that
both training and test data belong to the same lan-
guage. Monolingual text classification algorithms
can then be applied on these translated data. Al-
though intuitive, these methods suffer from two ma-
jor drawbacks.
First, most off-the-shelf machine translation sys-
tems typically generate only their best translation for
a given text. Since machine translation is known
to be a notoriously hard problem, applying mono-
lingual text classification algorithms directly on the
erroneous translation of training or test data may
severely deteriorate the classification accuracy.
Second, similar to domain adaptation in statisti-
cal machine learning, due to the discrepancy of data
distribution between the training domain and test do-
main, data distribution across languages may vary
because of the difference of culture, people?s inter-
ests, linguistic expression in different language re-
gions. So even if the translation of training or test
data is perfectly correct, the cross language classi-
fier may not perform as well as the monolingual one
trained and tested on the data from the same lan-
guage.
In this paper, we propose a new approach to
CLTC, which trains a classification model in the
source language and ports the model to the target
language, with the translation knowledge learned us-
ing the EM algorithm. Unlike previous methods
based on machine translation (Fortuna and Shawe-
Taylor, 2005), our method takes into account dif-
1057
ferent possible translations for model features. The
translated model serves as an initial classifier for a
semi-supervised process, by which the model is fur-
ther adjusted to fit the distribution of the target lan-
guage. Our method does not require any labeled
data in the target language, nor a machine transla-
tion system. Instead, the only requirement is a rea-
sonable amount of unlabeled data in the target lan-
guage, which is often easy to obtain.
In the following sections, we first review related
work. In section 3, we introduce our method that
translates the classification model with the trans-
lation knowledge learned using the EM algorithm.
Section 4 describes model adaptation by training the
translated model with unlabeled documents in the
target language. Experiments and evaluations are
presented in section 5 and finally we conclude the
paper in section 6.
2 Related Work
Text classification has rightfully received a lot of at-
tention from both the academic and industry com-
munities, being one of the areas in natural language
processing that has a very large number of practi-
cal applications. Text classification techniques have
been applied to many diverse problems, ranging
from topic classification (Joachims, 1997), to genre
detection (Argamon et al, 1998), opinion identifica-
tion (Pang and Lee, 2004), spam detection (Sahami
et al, 1998), gender and age classification (Schler et
al., 2006).
Text classification is typically formulated as a
learning task, where a classifier learns how to distin-
guish between categories in a given set, using fea-
tures automatically extracted from a collection of
documents. In addition to the learning methodol-
ogy itself, the accuracy of the text classifier also de-
pends to a large extent upon the amount of training
data available at hand. For instance, distinguish-
ing between two categories for which thousands of
manually annotated examples are already available
is expected to perform better than trying to separate
categories that have only a handful of labeled docu-
ments.
Some of the most successful approaches to date
for text classification involve the use of machine
learning methods, which assume that enough an-
notated data is available such that a classification
model can be automatically learned. These include
algorithms such as Naive Bayes (Joachims, 1997;
McCallum and Nigam, 1998), Rocchio classifiers
(Joachims, 1997; Moschitti, 2003), Maximum En-
tropy (Nigam et al, 1999) or Support Vector Ma-
chines (Vapnik, 1995; Joachims, 1998). If only
a small amount of annotated data is available, the
alternative is to use semi-supervised bootstrapping
methods such as co-training or self-training, which
can also integrate raw unlabeled data into the learn-
ing model (Blum and Mitchell, 1998; Nigam and
Ghani, 2000).
Despite the attention that monolingual text clas-
sification has received from the research commu-
nity, there is only very little work that was done
on cross-lingual text classification. The work that
is most closely related to ours is (Gliozzo and Strap-
parava, 2006), where a multilingual domain kernel is
learned from comparable corpora, and subsequently
used for the cross-lingual classification of texts. In
experiments run on Italian and English, Gliozzo and
Strapparava showed that the multilingual domain
kernel exceeds by a large margin a bag-of-words ap-
proach. Moreover, they demonstrated that the use
of a bilingual dictionary can drastically improve the
performance of the models learned from corpora.
(Fortuna and Shawe-Taylor, 2005; Olsson et al,
2005) studied the use of machine translation tools
for the purpose of cross language text classification
and mining. These approaches typically translate
the training data or test data into the same language,
followed by the application of a monolingual classi-
fier. The performance of such classifiers very much
depends on the quality of the machine translation
tools. Unfortunately, the development of statistical
machine translation systems (Brown et al, 1993) is
hindered by the lack of availability of parallel cor-
pora and the quality of their output is often erro-
neous. Several methods were proposed (Shi et al,
2006; Nie et al, 1999) to automatically acquire a
large quantity of parallel sentences from the web,
but such web data is however predominantly con-
fined to a limited number of domains and language
pairs.
(Dai et al, 2007) experimented with the use of
transfer learning for text classification. Although in
this method the transfer learning is performed across
1058
different domains in the same language, the under-
lying principle is similar to CLTC in the sense that
different domains or languages may share a signif-
icant amount of knowledge in similar classification
tasks. (Blum and Mitchell, 1998) employed semi-
supervised learning for training text classifiers. This
method bootstraps text classifiers with only unla-
beled data or a small amount of labeled training data,
which is close to our setting that tries to leverage la-
beled data and unlabeled data in different languages
to build text classifiers.
Finally, also closely related is the work carried out
in the field of sentiment and subjectivity analysis
for cross-lingual classification of opinions. For in-
stance, (Mihalcea et al, 2007) use an English corpus
annotated for subjectivity along with parallel text to
build a subjectivity classifier for Romanian. Sim-
ilarly, (Banea et al, 2008) propose a method based
on machine translation to generate parallel texts, fol-
lowed by a cross-lingual projection of subjectivity
labels, which are used to train subjectivity annota-
tion tools for Romanian and Spanish. A related, yet
more sophisticated technique is proposed in (Wan,
2009), where a co-training approach is used to lever-
age resources from both a source and a target lan-
guage. The technique is tested on the automatic sen-
timent classification of product reviews in Chinese,
and showed to successfully make use of both cross-
language and within-language knowledge.
3 Cross Language Model Translation
To make the classifier applicable to documents in
a foreign language, we introduce a method where
model features that are learned from the training
data are translated from the source language into
the target language. Using this translation process,
a feature associated with a word in the source lan-
guage is transferred to a word in the target language
so that the feature is triggered when the word occurs
in the target language test document.
In a typical translation process, the features would
be translated by making use of a bilingual dictio-
nary. However, this translation method has a major
drawback, due to the ambiguity usually associated
with the entries in a bilingual dictionary: a word in
one language can have multiple translations in an-
other language, with possibly disparate meanings.
If an incorrect translation is selected, it can distort
the classification accuracy, by introducing erroneous
features into the learning model. Therefore, our goal
is to minimize the distortion during the model trans-
lation process, in order to maximize the classifica-
tion accuracy in the target language.
In this paper, we introduce a method that em-
ploys the EM algorithm to automatically learn fea-
ture translation probabilities from labeled text in the
source language and unlabeled text in the target lan-
guage. Using the feature translation probabilities,
we can derive a classification model for the target
language from a mixture model with feature transla-
tions.
3.1 Learning Feature Translation Probabilities
with EM Algorithm
Given a document d from the document collectionD
in the target language, the probability of generating
the document P (d) is the mixture of generating d
with different classes c ? C:
P (d) =
?
c
P (d|c)P (c)
In our cross-lingual setting, we view the generation
of d given a class c as a two step process. In the
first step, a pseudo-document d? is generated in the
source language, followed by a second step, where
d? is translated into the observed document d in the
target language. In this generative model, d? is a la-
tent variable that cannot be directly observed. Since
d could have multiple translations d? in the source
language, the probability of generating d can then
be reformulated as a mixture of probabilities as in
the following equation.
P (d) =
?
c
P (c)
?
d?
P (d|d?, c)P (d?|c)
According to the bag-of-words assumption,
the document translation probability P (d|d?, c) is
the product of the word translation probabilities
P (wi|w?i, c) , where w?i in d? is the source language
word that wi is translated from. P (d?|c) is the prod-
uct of P (w?i|c). The formula is rewritten as:
P (d) =
?
c
P (c)
?
d?
l
?
i=1
P (wi|w?i, c)P (w?i|c)
1059
where wi is the ith word of the document d with l
words. The prior probability P (c) and the proba-
bility of the source language word w? given class c
are estimated using the labeled training data in the
source language, so we use them as known parame-
ters. P (wi|w?i, c) is the probability of translating the
word w?i in the source language to the word wi in
the target language given class c, and these are the
parameters we want to learn from the corpus in the
target language.
Using the Maximum Likelihood Estimation
(MLE) framework, we learn the model parameters ?
? the translation probability P (wi|w?i, c) ? by max-
imizing the log likelihood of a collection of docu-
ments in the target language:
?? = argmax?
m
?
j=1
log(P (dj , ?))
= argmax?
m
?
j=1
log(
?
c
P (c)
?
d?
lj
?
i=1
P (wi|w?i, c)P (w?i|c))
where m is the number of documents in the corpus
in the target language and lj is the number of words
in the document dj .
In order to estimate the optimal values of the pa-
rameters, we use the EM algorithm (Dempster et al,
1977). At each iteration of EM we determine those
values by maximizing the expectation using the pa-
rameters from the previous iteration and this itera-
tive process stops when the change in the parameters
is smaller than a given threshold. We can repeat the
following two steps for the purpose above.
? E-step
P (w?c|w) ? P (cw
?w)
P (w)
= P (w|w
?c)P (w?c)
?
c
?
w? P (w|w?c)P (w?c)
(1)
? M-step
P (w|w?c)? f(w)P (w
?c|w)
?
w?K f(w)P (w?c|w)
(2)
Algorithm 1 EM algorithm for learning translation
probabilities
Dl ? labeled data in the source language
Du ? unlabeled data in the target language
L? bilingual lexicon
1: Initialize P0(w|w?c) = 1nw? , where (w,w
?) ? L,
otherwise P0(w|w?c) = 0;
2: Compute P (w?c) with Dl according to equa-
tion 3
3: repeat
4: Calculate Pt(w?c|w) with Du based on
Pt?1(w|w?c) according to equation 1
5: Calculate Pt(w|w?c) based on Pt?1(w?c|w)
according to equation 2
6: until change of P (w|w?c) is smaller than the
threshold
7: return P (w|w?c)
Here f(w) is the occurrence frequency of the word
w in the corpus. K is the set of translation candi-
dates in the target language for the source language
wordw? according to the bilingual lexicon. P(w?c) is
the probability of occurrence of the source language
word w? under the class c. It can be estimated from
the labeled source language training data available
as follows and it is regarded as a known parameter
of the model.
P (w?c) = f(w
?c)
?
w??V f(w?c)
(3)
where V is the vocabulary of the source language.
Algorithm 1 illustrates the EM learning process,
where nw? denotes the number of translation candi-
dates for w? according to the bilingual lexicon.
Our method requires no labeled training data
in the target language. Many statistical machine
translation systems such as IBM models (Brown
et al, 1993) learn word translation probabilities
from millions of parallel sentences which are mu-
tual translations. However, large scale parallel cor-
pora rarely exist for most language pairs. (Koehn
and Knight, 2000) proposed to use the EM algo-
rithm to learn word translation probabilities from
non-parallel monolingual corpora. However, this
method estimates only class independent transla-
tion probabilities P (wi|w?i), while our approach is
able to learn class specific translation probabilities
1060
P (wi|w?i, c) by leveraging available labeled training
data in the source language. For example, the prob-
ability of translating ?bush? as ???? (small trees)
is higher than translating as ???? (U.S. president)
when the category of the text is ?botany.?
3.2 Model Translation
In order to classify documents in the target language,
a straightforward approach to transferring the classi-
fication model learned from the labeled source lan-
guage training data is to translate each feature from
the bag-of-words model according to the bilingual
lexicon. However, because of the translation ambi-
guity of each word, a model in the source language
could be potentially translated into many different
models in the target language. Thus, we think of
the probability of the class of a target language doc-
ument as the mixture of the probabilities by each
translated model from the source language model,
weighed by their translation probabilities.
P (c|d,mt) ?
?
m?t
P (m?t|ms, c)P (c|d,m?t)
where mt is the target language classification model
and m?t is a candidate model translated from the
model ms trained on the labeled training data in
the source language. This is a very generic rep-
resentation for model translation and the model m
could be any type of text classification. Specifically
in this paper, we take the Maximum Entropy (ME)
model(Berger et al, 1996) as an example for the
model translation across languages, since the ME
model is one of the most widely used text classifica-
tion models. The maximum entropy classifier takes
the form
P (c|d) = 1
Z(d)
?
w?V
e?wf(w,c)
where: V is the vocabulary of the language; f(w, c)
is the feature function associated with the word w
and class c and its value is set to 1 when w occurs in
d and the class is c or otherwise 0. ?w is the feature
weight for f(wi, c) indicating the importance of the
feature in the model. During model translation, the
feature weight for f(wi, c) is transferred to f(w?i, c)
in the target language model, where w?i is the trans-
lation of wi. Z(d) is the normalization factor which
is invariant to c and hence we can omit it for classi-
fication since our objective is to find the best c. Ac-
cording to the formulation of the Maximum Entropy
model, the document can be classified as follows.
c? = argmaxc?C
?
m?t
P (m?t|ms, c)
v
?
i=1
e?wisf(w
i
t,c)
The model translation probability P (m?t|ms, c) can
be modeled as the product of the translation proba-
bilities of each of its individual bag-of-words fea-
tures P (m?t|ms, c) ?
?l
i=1 P (wit|wis, c) and the
classification model can be further written as
c? = argmaxc?C
?
m?t
v
?
i=1
P (wit|wis, c)e
?wisf(w
i
t,c)
where feature translation probabilities P (wit|wis, c)
are estimated with the EM algorithm described in
the previous section. Note that if the average number
of translations for a word w is n and v is the num-
ber of words in the vocabulary there are nv possible
models m?t translated from ms. However, we can
do the following mathematical transformation on the
equation which leads to a polynomial time complex-
ity algorithm. The idea is that instead of enumerat-
ing the exponential number of different translations
of the entire model, we will instead handle one fea-
ture at a time.
?
m?t
v
?
i=1
P (wit|wis, c)e
?wisf(w
i
t,c) =
n1
?
j=1
P (w1jt |w1s , c)e?1f(w
1j
t ,c)
?
m2,vt
v
?
i=2
P (wit|wis, c)e?if(w
i
t,c)
Here w1 is the first word in the vocabulary of the
source language and w1j is a translation of w1 in the
target language with n denoting the number its trans-
lations according to the bilingual lexicon.
?
m2,vt
are all the target language models translated from
the model consisting of the rest of the words w2 ...
wv in the source language. This process is recur-
sive until the last word wvs of the vocabulary and this
transforms the equation into a polynomial form as
1061
follows.
?
m?t
v
?
i=1
P (wit|wis, c)e
?wisf(w
i
t,c)
=
v
?
i=1
ni
?
j=1
P (wijt |wis, c)e
?wisf(w
ij
t ,c)
Based on the above transformation, the class c? for
the target language document d is then calculated
with the following equation.
c? = argmaxc?C
v
?
i=1
ni
?
j=1
P (wijt |wis, c)e
?wisf(w
ij
t ,c)
The time complexity of computing the above equa-
tion is n? v.
4 Model Adaptation with Semi-
Supervised Learning
In addition to translation ambiguity, another chal-
lenge in building a classifier using training data in
a foreign language is the discrepancy of data distri-
bution in different languages. Direct application of a
classifier translated from a foreign model may not fit
well the distribution of the current language. For ex-
ample, a text about ?sports? in (American) English
may talk about ?American football,? ?baseball,? and
?basketball,? whereas Chinese tend to discuss about
?soccer? or ?table tennis.?
To alleviate this problem, we employ semi-
supervised learning in order to adapt the model to
the target language. Specifically, we first start by us-
ing the translated classifier from English as an initial
classifier to label a set of Chinese documents. The
initial classifier is able to correctly classify a num-
ber of unlabeled Chinese documents with the knowl-
edge transferred from English training data. For
instance, words like ?game(??),? ?score(??),?
?athlete(???),? learned from English can still ef-
fectively classify Chinese documents. We then pick
a set of labeled Chinese documents with high con-
fidence to train a new Chinese classifier. The new
classifier can then learn new knowledge from these
Chinese documents. E.g. it can discover that words
like ?soccer(??)? or ?badminton(???)? occur
frequently in the Chinese ?sports? documents, while
words that are frequently occurring in English doc-
uments such as ?superbowl(???)? and ?NHL(?
Algorithm 2 Semi-supervised learning for cross-
lingual text classification
Ls ? labeled data in the source language
Ut ? unlabeled data in the target lan-
guage
1: Cs = train(Ls)
2: Ct = translate(Cs)
3: repeat
4: Label(U,Ct)
5: L? select(confidence(U,Ct))
6: Ct ? train(L)
7: until stopping criterion is met
8: return Ct
?????)? do not occur as often. Re-training the
classifier with the Chinese documents can adjust the
feature weights for these words so that the model fits
better the data distribution of Chinese documents,
and thus it improves the classification accuracy. The
new classifier then re-labels the Chinese documents
and the process is repeated for several iterations. Al-
gorithm 2 illustrates this semi-supervised learning
process.
The confidence score associated with the docu-
ments is calculated based on the probabilities of the
class. For a binary classifier the confidence of clas-
sifying the document d is calculated as:
confidence(d) =
?
?
?
?
log(P (c|d)
P (c|d)
)
?
?
?
?
An unlabeled document is selected as training
data for a new classifier when its confidence score
is above a threshold.
5 Experiments and Evaluation
To evaluate the effectiveness of our method, we
carry out several experiments. First, we compare the
performance of our method on five different cate-
gories, from five different domains, in order to see
its generality and applicability on different domains.
We also run experiments with two different language
pairs - English-Chinese and English-French - to see
if the distance between language families influences
the effectiveness of our method.
To determine the performance of the method with
respect to other approaches, we compare the classi-
fication accuracy with that of a machine translation
1062
approach that translates the training (test) data from
the source language to the target language, as well
as with a classifier trained on monolingual training
data in the target language.
Finally, we evaluate the performance of each of
the two steps of our proposed method. First, we
evaluate the model translated with the parameters
learned with EM, and then the model after the semi-
supervised learning for data distribution adaptation
with different parameters, including the number of
iterations and different amounts of unlabeled data.
5.1 Data Set
Since a standard evaluation benchmark for cross-
lingual text classification is not available, we built
our own data set from Yahoo! RSS news feeds. The
news feed contains news articles from October 1st
2009 to December 31st 2009. We collected a total
of 615731 news articles, categorized by their edi-
tors into topics such as ?sports? or ?business?. We
selected five categories for our experiments, namely
?sports?, ?health?, ?business?, ?entertainment?, ?ed-
ucation?. The Yahoo! RSS news feed includes
news in many languages, including English, Chi-
nese, French, Spanish, and others.
We experimented on two language pairs, English-
Chinese and English-French, selected for their diver-
sity: English and Chinese are disparate languages
with very little common vocabulary and syntax,
whereas English and French are regarded as more
similar. We expect to evaluate the impact of the
distance of languages on the effectiveness of our
method. In both cases, English is regarded as the
source language, where training data are available,
and Chinese and French are the target languages
for which we want to build text classifiers. Note
that regardless of the language, the documents are
assigned with one of the five category labels men-
tioned above. Table 1 shows the distribution of doc-
uments across categories and across languages.
Category English Chinese French
sports 23764 14674 18398
health 15627 11769 12745
business 34619 23692 28740
entertainment 26876 21470 23756
education 16488 14353 15753
Table 1: number of documents in each class
Before building the classification model, several
preprocessing steps are applied an all the docu-
ments. First, the HTML tags are removed, and ad-
vertisements and navigational information are also
eliminated. For the Chinese corpus, all the Chinese
characters with BIG5 encoding are converted into
GB2312 and the Chinese texts are segmented into
words. For the translation, we use the LDC bilin-
gual dictionary1 for Chinese English and ?stardict?
2 for Spanish English.
5.2 Model Translation
To transfer a model learned in one language to an-
other, we can translate all the bag-of-word features
according to a bilingual lexicon. Due to the trans-
lation ambiguity of each feature word, we com-
pare three different ways of model translation. One
method is to equally assign probabilities to all the
translations for a given source language word, and
to translate a word we randomly pick a translation
from all of its translation candidates. We denote this
as ?EQUAL? and it is our baseline method. Another
way is to calculate the translation probability based
on the frequencies of the translation words in the tar-
get language itself. For instance, the English word
?bush? can be translated into ???? , ???? or ??
?? . We can obtain the following unigram counts
of these translation words in our Yahoo! RSS news
corpus.
count translation sense
582 ?? Goerge W. Bush
43 ?? small trees
2 ?? canula
We can estimate that P (??|bush) = 582/(582 +
43+2) = 92.8% and so forth. This method often al-
lows us to estimate reasonable translation probabili-
ties and we use ?UNIGRAM? to denote this method.
And finally the third model translation approach is
to use the translation probability learned with the
EM algorithm proposed in this paper. The initial
parameters of the EM algorithm are set to the prob-
abilities calculated with the ?UNIGRAM? method
and we use 4000 unlabeled documents in Chinese
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002L27
2http://stardict.sourceforge.net/Dictionaries.php
1063
to learn translation probabilities with EM. We first
train an English classification model for the topic of
?sport? and then translate the model into Chinese us-
ing translation probabilities estimated by the above
three different methods. The three translated models
are applied to Chinese test data and we measure the
precision, recall and F-score as shown in Table 2.
Method P R F
EQUAL 71.1 70.6 70.8
UNIGRAM 79.5 77.8 78.6
EM 83.1 84.7 83.9
Table 2: Comparison of different methods for model
translation
From this table we can see that the baseline method
has lowest classification accuracy due to the fact that
it is unable to handle translation ambiguity since
picking any one of the translation word is equally
likely. ?UNIGRAM? shows significant improve-
ment over ?EQUAL? as the occurrence count of the
translation words in the target language can help
disambiguate the translations. However occurrence
count in a monolingual corpus may not always be
the true translation probability. For instance, the
English word ?work? can be translated into ??
?(labor)? and ???(factory)? in Chinese. How-
ever, in our Chinese monolingual news corpus, the
count for ???(factory)? is more than that of ??
?(labor)? even though ???(labor)? should be a
more likely translation for ?work?. The ?EM? algo-
rithm has the best performance as it is able to learn
translation probabilities by looking at documents in
both source language and target language instead of
just a single language corpus.
5.3 Cross Language Text Classification
To evaluate the effectiveness of our method on cross
language text classification, we implement several
methods for comparison. In each experiment, we
run a separate classification for each class, using a
one-versus-all binary classification.
ML (Monolingual). We build a monolingual
text classifier by training and testing the text classi-
fication system on documents in the same language.
This method plays the role of an upper-bound, since
the best classification results are expected when
monolingual training data is available.
MT (Machine Translation). We use the Sys-
tran 5.0 machine translation system to translate
the documents from one language into the other
in two directions. The first direction translates the
training data from the source language into the
target language, and then trains a model in the target
language. This direction is denoted as MTS. The
second direction trains a classifier in the source
language and translates the test data into the source
language. This direction is denoted as MTT. In
our experiments, Systran generates the single best
translation of the text as most off-the-shelf machine
translation tools do.
EM (Model Translation with EM). This is the
first step of our proposed method. We used 4,000
unlabeled documents to learn translation proba-
bilities with the EM algorithm and the translation
probabilities are leveraged to translate the model.
The rest of the unlabeled documents are used for
other experimental purpose.
SEMI (Adapted Model after Semi-Supervised
Learning). This is our proposed method, after both
model translation and semi-supervised learning.
In the semi-supervised learning, we use 6,000
unlabeled target language documents with three
training iterations.
In each experiment, the data consists of 4,000 la-
beled documents and 1,000 test documents (e.g., in
the cross-lingual experiments, we use 4,000 English
annotated documents and 1,000 Chinese or French
test documents). For a given language, the same test
data is used across all experiments.
Table 3 shows the performance of the various
classification methods. The ML (Monolingual)
classifier has the best performance, as it is trained
on labeled data in the target language, so that there
is no information loss and no distribution discrep-
ancy due to a model translation. The MT (ma-
chine translation) based approach scores the lowest
accuracy, probably because the machine translation
software produces only its best translation, which
is often error-prone, thus leading to poor classifi-
cation accuracy. In addition, the direct application
of a classification model from one language to an-
1064
English? Chinese
Category ML MTS MTT EM SEMI
P R F P R F P R F P R F P R F
sports 96.1 94.3 95.2 80.6 81.7 81.2 81.7 83.8 82.7 83.1 84.7 83.9 92.1 91.8 91.9
health 95.1 93.1 94.1 80.8 81.5 81.2 81.6 83.5 82.6 84.5 85.8 85.2 90.2 91.7 90.9
business 91.6 93.1 92.4 81.3 81.9 81.6 80.7 81.0 80.9 81.6 82.0 81.8 87.3 89.3 88.3
entertainment 88.1 88.3 88.2 76.1 78.8 77.5 75.3 78.9 77.1 76.8 79.7 78.2 83.2 83.8 83.5
education 79.1 82.2 80.6 70.2 72.5 71.8 71.1 72.0 71.6 71.2 73.7 72.5 76.2 79.8 78.0
English? French
sports 95.8 95.0 95.4 82.8 83.6 83.2 82.1 83.0 82.5 85.3 87.1 86.2 92.5 92.1 92.3
health 94.2 94.5 94.3 82.6 83.9 83.2 81.8 83.0 82.4 86.2 87.2 86.6 92.0 92.2 92.1
business 90.1 92.2 91.1 81.4 82.1 81.7 81.3 81.8 81.8 84.4 84.3 84.4 88.3 89.2 88.8
entertainment 87.4 87.2 87.3 76.6 79.1 77.8 76.0 78.8 77.4 78.9 81.0 80.0 84.3 85.5 84.9
education 78.8 81.8 80.3 72.1 74.8 73.5 72.3 72.7 72.5 73.8 76.2 75.0 76.3 80.1 78.2
Table 3: Comparison of different methods and different language pairs
other does not adapt to the distribution of the sec-
ond language, even if the documents belong to the
same domain. Comparing the two MT alternatives,
we can see that translating the training data (MTS)
has better performance than translating the test data
(MTT). The reason is that when the model is trained
on the translated training data, the model parame-
ters are learned over an entire collection of translated
documents, which is less sensitive to translation er-
rors than translating a test document on which the
classification is performed individually.
Our EM method for translating model features
outperforms the machine translation approach, since
it does not only rely on the best translation by the
machine translation system, but instead takes into
account all possible translations with knowledge
learned specifically from the target language. Ad-
ditionally, the SEMI (semi-supervised) learning is
shown to further improve the classification accuracy.
The semi-supervised learning is able to not only help
adapt the translated model to fit the words distribu-
tion in the target language, but it also compensates
the distortion or information loss during the model
translation process as it can down-weigh the incor-
rectly translated features.
The improvement in performance for both the
EM and the SEMI methods is consistent across
the five different domains, which indicates that the
methods are robust and they are insensitive to the
domain of the data.
The performance of the two language pairs
English-Chinese and English-French shows a dif-
ference as initially hypothesized. In both the EM
and the SEMI models, the classification accuracy
of English-French exceeds that of English-Chinese,
which is probably explained by the fact that there is
less translation ambiguity in similar languages, and
they have more similar distributions. Note that the
monolingual models in French and Chinese perform
comparably, which means the difficulty of the test
data is similar between the two target languages.
5.4 Model Adaptation with Semi-Supervised
Learning
Finally, to gain further insights into our proposed
adaptation method, we run several experiments with
different parameters for the semi-supervised learn-
ing stage. As these experiments are very time con-
suming, we run them only on Chinese.
For each of the five categories, we train a classi-
fication model using the 4,000 training documents
in English and then translate the model into Chinese
with the translation parameters learned with EM on
20,000 unlabeled Chinese documents. Then we fur-
ther train the translated model on a set of unlabeled
Chinese documents using a different number of it-
erations and a different amount of unlabeled docu-
ments. Figures 1 and 2 show the results of these
evaluations.
As the plots show, the use of unlabeled data in
the target language can improve the cross-language
classification by learning new knowledge in the
target language. Larger amounts of unlabeled
data in general help, although the marginal bene-
fit drops with increasing amounts of data. Regard-
ing the number of iterations, the best performance is
1065
 70
 75
 80
 85
 90
 95
 0  1000  2000  3000  4000  5000  6000
Cla
ssif
ica
tion
 F-
sco
re
Size of unlabeled data
sportshealthbusiness
entertainment
education
Figure 1: Change in classification F-score for an increas-
ing amount of unlabeled data in the target language
 70
 75
 80
 85
 90
 95
 0  1  2  3  4  5  6
Cla
ssif
ica
tion
 F-
sco
re
Number of iterations
sportshealthbusiness
entertainment
education
Figure 2: Change in classification F-score for a different
number of iterations
achieved after 3-4 iterations.
6 Conclusions
In this paper, we proposed a novel method for cross-
lingual text classification. Our method ports a clas-
sification model trained in a source language to a tar-
get language, with the translation knowledge being
learned using the EM algorithm. The model is fur-
ther tuned to fit the distribution in the target language
via semi-supervised learning. Experiments on dif-
ferent datasets covering different languages and dif-
ferent domains show significant improvement over
previous methods that rely on machine translation.
Moreover, the cross-lingual classification accuracy
obtained with our method was found to be close to
the one achieved using monolingual text classifica-
tion.
Acknowledgments
The work of the second author has been partially
supported by National Science Foundation awards
#0917170 and #0747340. Any opinions, findings,
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the National Science
Foundation.
References
S. Argamon, M. Koppel, and G. Avneri. 1998. Style-
based text categorization: What newspaper am i read-
ing? In AAAI-98 Workshop on Learning for Text Cat-
egorization, Madison.
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan. 2008.
Multilingual subjectivity analysis using machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP
2008), Honolulu, Hawaii.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory, Morgan Kaufmann Publishers, June.
P. Brown, S. della Pietra, V. della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: parameter estimation. Computational Linguis-
tics, 19(2).
W. Dai, G. Xue, Q. Yang, and Y. Yu. 2007. Transfer-
ring naive bayes classifiers for text classification. In In
Proceedings of the 22nd AAAI Conference on Artificial
Intell igence, pages 540?545.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the em al-
gorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1).
B. Fortuna and J. Shawe-Taylor. 2005. The use of
machine translation tools for cross-lingual text min-
ing. In Learning With Multiple Views, Workshop at
the 22nd International Conference on Machine Learn-
ing (ICML).
A. Gliozzo and C. Strapparava. 2006. Exploiting com-
parable corpora and bilingual dictionaries for cross-
language text categorization. In Proceedings of the
Conference of the Association for Computational Lin-
guistics, Sydney, Australia.
1066
R. Hwa, P. Resnik, and A. Weinberg. 2005. Bootstrap-
ping parsers via syntactic projection across parallel
texts. Natural Language Engineering. Special issue
on Parallel Texts, editors R. Mihalcea and M. Simard.
T. Joachims. 1997. A probabilistic analysis of the Roc-
chio algorithm with TFIDF for text categorization. In
Proceedings of ICML-97, 14th International Confer-
ence on Machine Learning, Nashville, US.
T. Joachims. 1998. Text categorization with Support
Vector Machines: learning with mny relevant features.
In Proceedings of the European Conference on Ma-
chine Learning, pages 137?142.
P. Koehn and K. Knight. 2000. Estimating word transla-
tion probabilities from unrelated monolingua l corpora
using the em algorithm. In National Conference on
Artificial Intelligence (AAAI 2000) Lang kilde, pages
711?715.
A. McCallum and K. Nigam. 1998. A comparison of
event models for Naive Bayes text classification. In
Proceedings of AAAI-98 Workshop on Learning for
Text Categorization.
R. Mihalcea, C. Banea, and J. Wiebe. 2007. Learning
multilingual subjective language via cross-lingual pro-
jections. In Proceedings of the Association for Com-
putational Linguistics, Prague, Czech Republic.
C. Monz and B.J. Dorr. 2005. Iterative translation dis-
ambiguation for cross-language information retrieval.
In Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, Salvador, Brazil.
A. Moschitti. 2003. A study on optimal paramter tun-
ing for Rocchio text classifier. In Proceedings of the
European Conference on Information Retrieval, Italy.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting par-
allel texts for word sense disambiguation: An empiri-
cal study. In Proceedings of the 41st Annual Meeting
of the Association for Computational Linguistics (ACL
2003), Sapporo, Japan, July.
J.-Y. Nie, M. Simard, P. Isabelle, and R. Durand. 1999.
Cross-language information retrieval based on parallel
texts and automatic mining of parallel texts from the
Web. In Proceedings of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval.
K. Nigam and R. Ghani. 2000. Analyzing the effec-
tiveness and applicability of co-training. In Proceed-
ings of the Conference on Information and Knowledge
Management (CIKM 2000), McLean, VA, November.
K. Nigam, J. Lafferty, and A. McCallum. 1999. Using
maximum entropy for text classification. In IJCAI-99
Workshop on Machine Learning for Information Fil-
tering.
J.S. Olsson, D. W. Oard, and J. Hajic. 2005. Cross-
language text classification. In Proceedings of the 28th
Annual international ACM SIGIR Conference on Re-
search and Development in information Retrieval.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics, Barcelona, Spain, July.
M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz.
1998. A Bayesian approach to filtering junk e-mail.
In AAAI-98 Workshop on Learning for Text Catego-
rization, Madison.
J. Schler, M. Koppel, S. Argamon, and J. Pennebaker.
2006. Effects of age and gender on blogging. In Pro-
ceedings of 2006 AAAI Spring Symposium on Com-
putational Approaches for Analyzing Weblogs, pages
199?204, Stanford.
L. Shi, C. Niu, M. Zhou, and J. Gao. 2006. A dom
tree alignment model for mining parallel data from the
web. In Proceedings of the Annual Meeting of the As-
sociation for Computational Lingusitics (ACL 2006),
Sydney, Australia.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.
X. Wan. 2009. Co-training for cross-lingual sentiment
classification. In Proceedings of the Joint Conference
of the Association of Computational Linguistics and
the International Joint Conference on Natural Lan-
guage Processing, Singapore, August.
1067
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 799?809,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Template Mining for Semantic Category Understanding
Lei Shi
1,2?
, Shuming Shi
3
, Chin-Yew Lin
3
, Yi-Dong Shen
1
, Yong Rui
3
1
State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of Sciences
2
University of Chinese Academy of Sciences
3
Microsoft Research
{shilei,ydshen}@ios.ac.cn
{shumings,cyl,yongrui}@microsoft.com
Abstract
We propose an unsupervised approach to
constructing templates from a large collec-
tion of semantic category names, and use
the templates as the semantic representa-
tion of categories. The main challenge is
that many terms have multiple meanings,
resulting in a lot of wrong templates. Sta-
tistical data and semantic knowledge are
extracted from a web corpus to improve
template generation. A nonlinear scoring
function is proposed and demonstrated to
be effective. Experiments show that our
approach achieves significantly better re-
sults than baseline methods. As an imme-
diate application, we apply the extracted
templates to the cleaning of a category col-
lection and see promising results (preci-
sion improved from 81% to 89%).
1 Introduction
A semantic category is a collection of items shar-
ing common semantic properties. For example,
all cities in Germany form a semantic category
named ?city in Germany? or ?German city?. In
Wikipedia, the category names of an entity are
manually edited and displayed at the end of the
page for the entity. There have been quite a lot of
approaches (Hearst, 1992; Pantel and Ravichan-
dran, 2004; Van Durme and Pasca, 2008; Zhang et
al., 2011) in the literature to automatically extract-
ing category names and instances (also called is-a
or hypernymy relations) from the web.
Most existing work simply treats a category
name as a text string containing one or multiple
words, without caring about its internal structure.
In this paper, we explore the semantic structure
of category names (or simply called ?categories?).
?
This work was performed when the first author was vis-
iting Microsoft Research Asia.
For example, both ?CEO of General Motors? and
?CEO of Yahoo? have structure ?CEO of [com-
pany]?. We call such a structure a category tem-
plate. Taking a large collection of open-domain
categories as input, we construct a list of category
templates and build a mapping from categories to
templates. Figure 1 shows some example semantic
categories and their corresponding templates.
Templates can be treated as additional features
of semantic categories. The new features can be
exploited to improve some upper-layer applica-
tions like web search and question answering. In
addition, by linking categories to templates, it is
possible (for a computer program) to infer the se-
mantic meaning of the categories. For example in
Figure 1, from the two templates linking to cat-
egory ?symptom of insulin deficiency?, it is rea-
sonable to interpret the category as: ?a symptom
of a medical condition called insulin deficiency
which is about the deficiency of one type of hor-
mone called insulin.? In this way, our knowledge
about a category can go beyond a simple string
and its member entities. An immediate application
of templates is removing invalid category names
from a noisy category collection. Promising re-
sults are observed for this application in our ex-
periments.
An intuitive approach to this task (i.e., extract-
ing templates from a collection of category names)
national holiday of South Africa(instances: Heritage Day, Christmas?)
national holiday of Brazil(instances: Carnival, Christmas?) national holiday of [country]
symptom of cortisol deficiency(instances: low blood sugar?)
symptom of insulin deficiency(instances: nocturia, weight loss?) symptom of [hormone] deficiencysymptom of [medical condition]
school in Denverschool in Houston school in [place]school in [city]
Semantic Categories Category templates
football playerbasketball player [sport] player
Figure 1: Examples of semantic categories and
their corresponding templates.
799
contains two stages: category labeling, and tem-
plate scoring.
Category labeling: Divide a category name
into multiple segments and replace some key seg-
ments with its hypernyms. As an example, as-
sume ?CEO of Delphinus? is divided to three seg-
ments ?CEO + of + Delphinus?; and the last seg-
ment (Delphinus) has hypernyms ?constellation?,
?company?, etc. By replacing this segment with
its hypernyms, we get candidate templates ?CEO
of [constellation]? (a wrong template), ?CEO of
[company]?, and the like.
Template scoring: Compute the score of each
candidate template by aggregating the information
obtained in the first phase.
A major challenge here is that many segments
(like ?Delphinus? in the above example) have mul-
tiple meanings. As a result, wrong hypernyms
may be adopted to generate incorrect candidate
templates (like ?CEO of [constellation]?). In this
paper, we focus on improving the template scor-
ing stage, with the goal of assigning lower scores
to bad templates and larger scores to high-quality
ones.
There have been some research efforts (Third,
2012; Fernandez-Breis et al., 2010; Quesada-
Mart?nez et al., 2012) on exploring the structure of
category names by building patterns. However, we
automatically assign semantic types to the pattern
variables (or called arguments) while they do not.
For example, our template has the form of ?city
in [country]? while their patterns are like ?city in
[X]?. More details are given in the related work
section.
A similar task is query understanding, including
query tagging and query template mining. Query
tagging (Li et al., 2009; Reisinger and Pasca,
2011) corresponds to the category labeling stage
described above. It is different from template gen-
eration because the results are for one query only,
without merging the information of all queries to
generate the final templates. Category template
construction are slightly different from query tem-
plate construction. First, some useful features
such as query click-through is not available in cat-
egory template construction. Second, categories
should be valid natural language phrases, while
queries need not. For example, ?city Germany? is
a query but not a valid category name. We discuss
in more details in the related work section.
Our major contributions are as follows.
1) To the best of our knowledge, this is the first
work of template generation specifically for cate-
gories in unsupervised manner.
2) We extract semantic knowledge and statisti-
cal information from a web corpus for improving
template generation. Significant performance im-
provement is obtained in our experiments.
3) We study the characteristics of the scoring
function from the viewpoint of probabilistic evi-
dence combination and demonstrate that nonlinear
functions are more effective in this task.
4) We employ the output templates to clean
our category collection mined from the web, and
get apparent quality improvement (precision im-
proved from 81% to 89%).
After discussing related work in Section 2, we
define the problem and describe one baseline ap-
proach in Section 3. Then we introduce our ap-
proach in Section 4. Experimental results are re-
ported and analyzed in Section 5. We conclude the
paper in Section 6.
2 Related work
Several kinds of work are related to ours.
Hypernymy relation extraction: Hypernymy
relation extraction is an important task in text min-
ing. There have been a lot of efforts (Hearst, 1992;
Pantel and Ravichandran, 2004; Van Durme and
Pasca, 2008; Zhang et al., 2011) in the literature to
extract hypernymy (or is-a) relations from the web.
Our target here is not hypernymy extraction, but
discovering the semantic structure of hypernyms
(or category names).
Category name exploration: Category name
patterns are explored and built in some ex-
isting research work. Third (2012) pro-
posed to find axiom patterns among category
names on an existing ontology. For ex-
ample, infer axiom pattern ?SubClassOf(AB,
B)? from ?SubClassOf(junior school school)?
and ?SubClassOf(domestic mammal mammal)?.
Fernandez-Breis et al. (2010) and Quesada-
Mart?nez et al. (2012) proposed to find lexical pat-
terns in category names to define axioms (in med-
ical domain). One example pattern mentioned in
their papers is ?[X] binding?. They need man-
ual intervention to determine what X means. The
main difference between the above work and ours
is that we automatically assign semantic types to
the pattern variables (or called arguments) while
they do not.
800
Template mining for IE: Some research work
in information extraction (IE) involves patterns.
Yangarber (2003) and Stevenson and Greenwood
(2005) proposed to learn patterns which were in
the form of [subject, verb, object]. The category
names and learned templates in our work are not
in this form. Another difference between our work
and their work is that, their methods need a super-
vised name classifer to generate the candidate pat-
terns while our approach is unsupervised. Cham-
bers and Jurafsky (2011) leverage templates to de-
scribe an event while the templates in our work are
for understanding category names (a kind of short
text).
Query tagging/labeling: Some research work
in recent years focuses on segmenting web search
queries and assigning semantic tags to key seg-
ments. Li et al. (2009) and Li (2010) employed
CRF (Conditional Random Field) or semi-CRF
models for query tagging. A crowdsourcing-
assisted method was proposed by Han et al. (2013)
for query structure interpretation. These super-
vised or semi-supervised approaches require much
manual annotation effort. Unsupervised meth-
ods were proposed by Sarkas et al. (2010) and
Reisinger and Pasca (2011). As been discussed
in the introduction section, query tagging is only
one of the two stages of template generation. The
tagging results are for one query only, without ag-
gregating the global information of all queries to
generate the final templates.
Query template construction: Some existing
work leveraged query templates or patterns for
query understanding. A semi-supervised random
walk based method was proposed by Agarwal et
al. (2010) to generate a ranked templates list which
are relevant to a domain of interest. A predefined
domain schema and seed information is needed for
this method. Pandey and Punera (2012) proposed
an unsupervised method based on graphical mod-
els to mine query templates. The above methods
are either domain-specific (i.e., generating tem-
plates for a specific domain), or have some degree
of supervision (supervised or semi-supervised).
Cheung and Li (2012) proposed an unsupervised
method to generate query templates by the aid of
knowledge bases. An approach was proposed in
(Szpektor et al., 2011) to improve query recom-
mendation via query templates. Query session in-
formation (which is not available in our task) is
needed in this approach for templates generation.
Li et al. (2013) proposed an clustering algorithm
to group existing query templates by search intents
of users.
Compared to the open-domain unsupervised
methods for query template construction, our ap-
proach improves on two aspects. First, we propose
to incorporate multiple types of semantic knowl-
edge (e.g., term peer similarity and term clusters)
to improve template generation. Second, we pro-
pose a nonlinear template scoring function which
is demonstrated to be more effective.
3 Problem Definition and Analysis
3.1 Problem definition
The goal of this paper is to construct a list of cat-
egory templates from a collection of open-domain
category names.
Input: The input is a collection of category
names, which can either be manually compiled
(like Wikipedia categories) or be automatically ex-
tracted. The categories used in our experiments
were automatically mined from the web, by fol-
lowing existing work (Hearst, 1992, Pantel and
Ravichandran 2004; Snow et al., 2005; Talukdar
et al., 2008; Zhang et al., 2011). Specifically,
we applied Hearst patterns (e.g., ?NP [,] (such
as | including) {NP, }
?
{and|or} NP?) and is-
a patterns (?NP (is|are|was|were|being) (a|an|the)
NP?) to a large corpus containing 3 billion En-
glish web pages. As a result, we obtained a
term?hypernym bi-partite graph containing 40
million terms, 74 million hypernyms (i.e., cate-
gory names), and 321 million edges (e.g., one
example edge is ?Berlin???city in Germany?,
where ?Berlin? is a term and ?city in Germany? is
the corresponding hypernym). Then all the multi-
word hypernyms are used as the input category
collection.
Output: The output is a list of templates, each
having a score indicating how likely it is valid. A
template is a multi-word string with one headword
and at least one argument. For example, in tem-
plate ?national holiday of [country]?, ?holiday? is
the headword, and ?[country]? is the argument.
We only consider one-argument templates in this
paper, and the case of multiple arguments is left as
future work. A template is valid if it is syntacti-
cally and semantically correct. ?CEO of [constel-
lation]? (wrongly generated from ?CEO of Del-
phinus?, ?CEO of Aquila?, etc.) is not valid be-
cause it is semantically unreasonable.
801
3.2 Baseline approach
An intuitive approach to this task contains two
stages: category labeling and template scoring.
Figure 2 shows its workflow with simple exam-
ples.
3.2.1 Phase-1: Category labeling
At this stage, each category name is automatically
segmented and labeled; and some candidate tem-
plate tuples (CTTs) are derived based on the la-
beling results. This can be done in the following
steps.
Category segmentation: Divide each cate-
gory name into multiple segments (e.g., ?holi-
day of South Africa? to ?holiday + of + South
Africa?). Each segment is one word or a phrase
appearing in an entity dictionary. The dictionary
used in this paper is comprised of all Freebase
(www.freebase.com) entities.
Segment to hypernym: Find hypernyms for
every segment (except for the headword and some
trivial segments like prepositions and articles), by
referring to a term?hypernym mapping graph.
Following most existing query labeling work, we
derive the term?hypernym graph from a dump of
Freebase. Below are some examples of Freebase
types (hypernyms),
German city (id: /location/de city)
Italian province (id: /location/it province)
Poem character (id: /book/poem character)
Book (id: /book/book)
To avoid generating too fine-grained templates
like ?mayor of [Germany city]? and ?mayor of
[Italian city]? (semantically ?mayor of [city]?
is more desirable), we discard type modifiers
and map terms to the headwords of Freebase
types. For example, ?Berlin? is mapped to
?city?. In this way, we build our basic version of
term?hypernym mapping which contains 16.13
million terms and 696 hypernyms. Since ?South
Africa? is both a country and a book name in Free-
base, hypernyms ?country?, ?book?, and others
are assigned to the segment ?South Africa? in this
step.
CTT generation: Construct CTTs by choosing
one segment (called the target segment) each time
and replacing the segment with its hypernyms. An
CTT is formed by the candidate template (with
one argument), the target segment (as an argument
value), and the tuple score (indicating tuple qual-
ity). Below are example CTTs obtained after the
last segment of ?holiday + of + South Africa? is
processed,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [book], South Africa, w
2
)
3.2.2 Phase-2: Template scoring
The main objective of this stage is to merge all
the CTTs obtained from the previous stage and to
compute a final score for each template. In this
stage, the CTTs are first grouped by the first ele-
ment (i.e., the template string). For example, tu-
ples for ?holiday of [country]? may include,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [country], Brazil, w
2
)
U
3
: (holiday of [country], Germany, w
3
)
...
Then a scoring function is employed to calcu-
late the template score from the tuple scores. For-
mally, given n tuples
~
U=(U
1
, U
2
..., U
n
) for a tem-
plate, the goal is to find a score fusion function
F (
~
U) which yields large values for high-quality
templates and small (or zero) values for invalid
ones.
Borrowing the idea of TF-IDF from information
retrieval, a reasonable scoring function is,
F (
~
U) =
n
?
i=1
w
i
? IDF (h) (1)
where h is the argument type (i.e., the hypernym
of the argument value) of each tuple. TF means
the ?term frequency? and IDF means the ?inverse
document frequency?. An IDF function assigns
lower scores to common hypernyms (like person
and music track which contain a lot of entities).
Let DF (h) be the number of entities having hy-
pernym h, we test two IDF functions in our exper-
iments,
IDF
1
(h) = log
1 +N
1 +DF (h)
IDF
2
(h) = 1/sqrt(DF (h))
(2)
where N is total number of entities in the entity
dictionary.
The next problem is estimating tuple score w
i
.
Please note that there is no weight or score infor-
mation in the term?hypernym mapping of Free-
base. So we have to set w
i
to be constant in the
baseline,
w
i
= 1 (3)
802
Wikipedia
holiday of Brazil
holiday of South Africa
?
Brazil ? country
Brazil ? book
South Africa ? country
South Africa ? book
?
holiday of [country], Brazil, w 1
holiday of [book], Brazil, w 2
holiday of [country], South Africa, w 3
holiday of [book], South Africa, w 4
?
holiday of [country], S 1
holiday of [book], S 2
?
Phase-1: Category labeling
Phase-2: Template scoring
Phase-1 Phase-2
Input: Category names
Term-hypernym mapping
Output: Category templates
head argument
argument
argument value
tuple score
Candidate template tuples (CTTs)
Figure 2: Problem definition and baseline approach.
4 Approach: Enhancing Template
Scoring
In our approach, we follow the same framework
as in the above baseline approach, and focus on
improving the template scoring phase (i.e., phase-
2).
We try three techniques: First, a better tuple
scorew
i
is calculated in Section 4.1 by performing
statistics on a large corpus. The corpus is a collec-
tion of 3 billion web pages crawled in early 2013
by ourselves. During this paper, we use ?our web
corpus? or ?our corpus? to refer to this corpus.
Second, a nonlinear function is adopted in Sec-
tion 4.2 to replace the baseline tuple fusion func-
tion (Formula 1). Third, we extract term peer sim-
ilarity and term clusters from our corpus and use
them as additional semantic knowledge to refine
template scores.
4.1 Enhancing tuple scoring
Let?s examine the following two template tuples,
U
1
: (holiday of [country], South Africa, w
1
)
U
2
: (holiday of [book], South Africa, w
2
)
Intuitively, ?South Africa? is more likely to be
a country than a book when it appears in text. So
for a reasonable tuple scoring formula, we should
have w
1
> w
2
.
The main idea is to automatically calculate
the popularity of a hypernym given a term, by
referring to a large corpus. Then by adding
the popularity information to (the edges of) the
term?hypernym graph of Freebase, we obtain a
weighted term?hypernym graph. The weighted
graph is then employed to enhance the estimation
of w
i
.
For popularity calculation, we apply Hearst
patterns (Hearst, 1992) and is-a patterns (?NP
(is|are|was|were|being) (a|an|the) NP?) to every
sentence of our web corpus. For a (term, hyper-
nym) pair, its popularity F is calculated as the
number of sentences in which the term and the hy-
pernym co-occur and also follow at least one of
the patterns.
For a template tuple U
i
with argument type h
and argument value v, we test two ways of esti-
mating the tuple score w
i
,
w
i
= log (1 + F (v, h)) (4)
w
i
=
F (v, h))
?+
?
h
j
?H
F (v, h
j
)
(5)
where F (v, h) is the popularity of the (v, h) pair
in our corpus, H is the set of all hypernyms for v
in the weighted term?hypernym graph. Parame-
ter ? (=1.0 in our experiments) is introduced for
smoothing purpose. Note that the second formula
is the conditional probability of hypernym h given
term v.
Since it is intuitive to estimate tuple scores with
their frequencies in a corpus, we treat the approach
with the improved w
i
as another baseline (our
strong baseline).
4.2 Enhancing tuple combination function
Now we study the possibility of improving the tu-
ple combination function (Formula 1), by examin-
ing the tuple fusion problem from the viewpoint
of probabilistic evidence combination. We first
demonstrate that the linear function in Formula 1
corresponds to the conditional independence as-
sumption of the tuples. Then we propose to adopt
a series of nonlinear functions for combining tuple
scores.
We define the following events:
T : Template T is a valid template;
T : T is an invalid template;
E
i
: The observation of tuple U
i
.
803
Let?s compute the posterior odds of event T ,
given two tuples U
1
and U
2
. Assuming E
1
and
E
2
are conditionally independent given T or T ,
according to the Bayes rule, we have,
P (T |E
1
, E
2
)
P (T |E
1
, E
2
)
=
P (E
1
, E
2
|T ) ? P (T )
P (E
1
, E
2
|T ) ? P (T )
=
P (E
1
|T )
P (E
1
|T )
?
P (E
2
|T )
P (E
2
|T )
?
P (T )
P (T )
=
P (T |E
1
) ? P (T )
P (T |E
1
) ? P (T )
?
P (T |E
2
) ? P (T )
P (T |E
2
) ? P (T )
?
P (T )
P (T )
(6)
Define the log-odds-gain of T given E as,
G(T |E) = log
P (T |E)
P (T |E)
? log
P (T )
P (T )
(7)
Here G means the gain of the log-odds of T af-
ter E occurs. By combining formulas 6 and 7, we
get
G(T |E
1
, E
2
) = G(T |E
1
) +G(T |E
2
) (8)
It is easy to prove that the above conclusion
holds true when n > 2, i.e.,
G(T |E
1
, ..., E
n
) =
n
?
i=1
G(T |E
i
) (9)
If we treat G(T |E
i
) as the score of template T
when only U
i
is observed, andG(T |E
1
, ..., E
n
) as
the template score after the n tuples are observed,
then the above equation means that the combined
template score should be the sum of w
i
? IDF (h),
which is exactly Formula 1. Please keep in mind
that Equation 9 is based on the assumption that the
tuples are conditional independent. This assump-
tion, however, may not hold in reality. The case
of conditional dependence was studied in (Zhang
et al., 2011), where a group of nonlinear combina-
tion functions were proposed and achieved good
performance in their task of hypernymy extrac-
tion. We choose p-Norm as our nonlinear fusion
functions, as below,
F (
~
U) =
p
?
?
?
?
n
?
i=1
w
p
i
? IDF (h) (p > 1) (10)
where p (=2 in experiments) is a parameter.
Experiments show that the above nonlinear
function performs better than the linear function
of Formula 1. Let?s use an example to show the
intuition. Consider a good template ?city of [coun-
try]? corresponding to CTTs
~
U
A
and a wrong tem-
plate ?city of [book]? having tuples
~
U
B
. Sup-
pose |
~
U
A
| = 200 (including most countries in
the world) and |
~
U
B
| = 1000 (considering that
many place names have already been used as book
names). We observe that each tuple score corre-
sponding to ?city of [country]? is larger than the
tuple score corresponding to ?city of [book]?. For
simplicity, we assume each tuple in
~
U
A
has score
1.0 and each tuple in
~
U
B
has score 0.2. With the
linear and nonlinear (p=2) fusion functions, we
can get,
Linear:
F (
~
U
A
) = 200 ? 1.0 = 200
F (
~
U
B
) = 1000 ? 0.2 = 200
(11)
Nonlinear:
F (
~
U
A
) = 14.1
F (
~
U
B
) = 6.32
(12)
In the above settings the nonlinear function
yields a much higher score for the good template
(than for the invalid template), while the linear one
does not.
4.3 Refinement with term similarity and
term clusters
The above techniques neglect the similarity among
terms, which has a high potential to improve the
template scoring process. Intuitively, for a toy set
{?city in Brazil?, ?city in South Africa?,?city in
China?, ?city in Japan?}, since ?Brazil?, ?South
Africa?, ?China? and ?Japan? are very similar to
each other and they all have a large probability to
be a ?country?, so we have more confidence that
?city in [country]? is a good template. In this sec-
tion, we propose to leverage the term similarity
information to improve the template scoring pro-
cess.
We start with building a large group of small
and overlapped clusters from our web corpus.
4.3.1 Building term clusters
Term clusters are built in three steps.
Mining term peer similarity: Two terms are
peers if they share a common hypernym and they
are semantically correlated. For example, ?dog?
and ?cat? should have a high peer similarity score.
Following existing work (Hearst, 1992; Kozareva
804
et al., 2008; Shi et al., 2010; Agirre et al., 2009;
Pantel et al., 2009), we built a peer similarity graph
containing about 40.5 million nodes and 1.33 bil-
lion edges.
Clustering: For each term, choose its top-30
neighbors from the peer similarity graph and run a
hierarchical clustering algorithm, resulting in one
or multiple clusters. Then we merge highly du-
plicated clusters. The algorithm is similar to the
first part of CBC (Pantel and Lin, 2002), with the
difference that a very high merging threshold is
adopted here in order to generate small and over-
lapped clusters. Please note that one term may be
included in many clusters.
Assigning top hypernyms: Up to two hyper-
nyms are assigned for each term cluster by major-
ity voting of its member terms, with the aid of the
weighted term?hypernym graph of Section 4.1.
To be an eligible hypernym for the cluster, it has
to be the hypernym of at least 70% of terms in the
cluster. The score of each hypernym is the aver-
age of the term?hypernym weights over all the
member terms.
4.3.2 Template score refinement
With term clusters at hand, now we describe the
score refinement procedure for a template T hav-
ing argument type h and supporting tuples
~
U=(U
1
,
U
2
..., U
n
). Denote V = {V
1
, V
2
, ..., V
n
} to be the
set of argument values for the tuples (where V
i
is
the argument value of U
i
).
By computing the intersection of V and every
term cluster, we can get a distribution of the argu-
ment values in the clusters. We find that for a good
template like ?holiday in [country]?, we can often
find at least one cluster (one of the country clus-
ters in this example) which has hypernym h and
also contains many elements in V . However, for
invalid templates like ?holiday of [book]?, every
cluster having hypernym h (=?book? here) only
contains a few elements in V . Inspired by such
an observation, our score refinement algorithm for
template T is as follows,
Step-1. Calculating supporting scores: For
each term cluster C having hypernym h, compute
its supporting score to T as follows:
S(C, T ) = k(C, V ) ? w(C, h) (13)
where k(C, V ) is the number of elements shared
by C and V , and w(C, h) is hypernym score of h
to C (computed in the last step of building clus-
ters).
Step-2. Calculating the final template score:
Let term cluster C
?
has the maximal supporting
score to T , the final template score is computed
as,
S(T ) = F (
~
U) ? S(C
?
, T ) (14)
where F (
~
U) is the template score before refine-
ment.
5 Experiments
5.1 Experimental setup
5.1.1 Methods for comparison
We make a comparison among 10 methods.
SC: The method is proposed in (Cheung and Li,
2012) to construct templates from queries. The
method firstly represents a query as a matrix based
on Freebase data. Then a hierarchical clustering
algorithm is employed to group queries having the
same structure and meaning. Then an intent sum-
marization algorithm is employed to create tem-
plates for each query group.
Base: The linear function in Formula 1 is
adopted to combine the tuple scores. We use
IDF
2
here because it achieves higher precision
than IDF
1
in this setting.
LW: The linear function in Formula 1 is
adopted to combine the tuple scores generated by
Formula 4. IDF
1
is used rather than IDF
2
for
better performance.
LP: The linear function in Formula 1 is adopted
to combine the tuple scores generated by Formula
5. IDF
2
is used rather than IDF
1
for better per-
formance.
NLW: The nonlinear fusion function in For-
mula 10 is used. Other settings are the same as
LW.
NLP: The nonlinear fusion function in Formula
10 is used. Other settings are the same as LP.
LW+C, LP+C, NLW+C, NLP+C: All the set-
tings of LW, LP, NLW, NLP respectively, with the
refinement technology in Section 4.3 applied.
5.1.2 Data sets, annotation and evaluation
metrics
The input category names for experiments are au-
tomatically extracted from a web corpus (Section
3.1). Two test-sets are built for evaluation from the
output templates of various methods.
Subsets: In order to conveniently compare the
performance of different methods, we create 20
sub-collections (called subsets) from the whole in-
put category collection. Each subset contains all
805
the categories having the same headword (e.g.,
?symptom of insulin deficiency? and ?depression
symptom? are in the same subset because they
share the same headword ?symptom?). To choose
the 20 headwords, we first sample 100 at ran-
dom from the set of all headwords; then manu-
ally choose 20 for diversity. The headwords in-
clude symptom, school, food, gem, hero, weapon,
model, etc. We run the 10 methods on these sub-
sets and sort the output templates by their scores.
Top-30 templates from each method on each sub-
set are selected and mixed together for annotation.
Fullset: We run method NLP+C (which has
the best performance according to our subsets
experiments) on the input categories and sort
the output templates by their scores. Then we
split the templates into 9 sections according
to their ranking position. The sections are:
[1?100], (100?1K], (1K?10K], (10K?100K],
(100K,120K], (120K?140K], (140K?160K],
(160K?180K], (180K?200K]. Then 40 templates
are randomly chosen from each section and mixed
together for annotation.
The selected templates (from subsets and the
fullset) are annotated by six annotators, with each
template assigned to two annotators. A template is
assigned a label of ?good?, ?fair?, or ?bad? by an
annotator. The percentage agreement between the
annotators is 80.2%, with kappa 0.624.
For the subset experiments, we adopt
Precision@k (k=10,20,30) to evaluate the
top templates generated by each method. The
scores for ?good?, ?fair?, and ?bad? are 1, 0.5,
and 0. The score of each template is the average
annotation score over two annotators (e.g., if a
template is annotated ?good? by one annotator and
?fair? by another, its score is (1.0+0.5)/2=0.75).
The evaluation score of a method is the average
over the 20 subsets. For the fullset experiments,
we report the precision for each section.
5.2 Experimental results
5.2.1 Results for subsets
The results of each method on the 20 subsets
are presented in Table 1. A few observations
can be made. First, by comparing the per-
formance of baseline-1 (Base) and the methods
adopting term?hypernym weight (LW and LP),
we can see big performance improvement. The
bad performance of baseline-1 is mainly due to
the lack of weight (or frequency) information on
Method P@10 P@20 P@30
Base (baseline-1) 0.359 0.361 0.358
SC (Cheung and Li, 2012) 0.382 0.366 0.371
Weighted LW 0.633 0.582 0.559
(baseline-2) LP 0.771 0.734 0.707
Nonlinear NLW 0.711 0.671 0.638
NLP 0.818 0.791 0.765
LW+C 0.813 0.786 0.754
Term cluster NLW+C 0.854 0.833 0.808
LP+C 0.818 0.788 0.778
NLP+C 0.868 0.839 0.788
Table 1: Performance comparison among the
methods on subset.
term?hypernym edges. The results demonstrate
that edge scores are critical for generating high
quality templates. Manually built semantic re-
sources typically lack such kinds of scores. There-
fore, it is very important to enhance them by de-
riving statistical data from a large corpus. Since
it is relatively easy to have the idea of adopt-
ing a weighted term?hypernym graph, we treat
LW and LP as another (stronger) baseline named
baseline-2.
As the second observation, the results show that
the nonlinear methods (NLP and NLW) achieve
performance improvement over their linear ver-
sions (LW and LP).
Third, let?s examine the methods with template
scores refined by term similarity and term clus-
ters (LW+C, NLW+C, LP+C, NLP+C). It is shown
that the refine-by-cluster technology brings addi-
tional performance gains on all the four settings
(linear and nonlinear, two different ways of calcu-
lating tuple scores). So we can conclude that the
peer similarity and term clusters are quite effective
in improving template generation.
Fourth, the best performance is achieved
when the three techniques (i.e., term?hypernym
weight, nonlinear fusion function, and refine-by-
cluster) are combined together. For instance, by
comparing the P@20 scores of baseline-2 and
NLP+C, we see a performance improvement of
14.3% (from 0.734 to 0.839). Therefore every
technique studied in this paper has its own merit
in template generation.
Finally, by comparing the method SC (Cheung
and Li, 2012) with other methods, we can see that
SC is slightly better than baseline-1, but has much
lower performance than others. The major reason
may be that this method did not employ a weighted
term?hypernym graph or term peer similarity in-
formation in template construction.
806
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@10 NLP > ?? > ?? >
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?? >
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@20 NLP > ?? > ?? > ??
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?? > ??
Base SC LP NLP LP+C
SC ?
LP > ?? > ??
P@30 NLP > ?? > ?? > ??
LP+C > ?? > ?? > ?? ?
NLP+C > ?? > ?? > ?? > ?
Table 2: Paired t-test results on subsets.
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@10 NLW > ?? > ?? > ?
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ?
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@20 NLW > ?? > ?? > ??
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ??
Base SC LW NLW LW+C
SC ?
LW > ?? > ??
P@30 NLW > ?? > ?? > ??
LW+C > ?? > ?? > ?? > ??
NLW+C > ?? > ?? > ?? > ?? > ??
Table 3: Paired t-test results on subsets.
Are the performance differences between meth-
ods significant enough for us to say that one is bet-
ter than the other? To answer this question, we run
paired two-tailed t-test on every pair of methods.
We report the t-test values among methods in ta-
bles 2, 3 and 4.
The meaning of the symbols in the tables are,
?: The method on the row and the one on the
column have similar performance.
>: The method on the row outperforms the
method on the column, but the performance dif-
ference is not statistically significant (0.05 ? P <
0.1 in two-tailed t-test).
> ?: The performance difference is statistically
significant (P < 0.05 in two-tailed t-test).
> ??: The performance difference is statisti-
cally highly significant (P < 0.01 in two-tailed
t-test).
P@10 P@20 P@30
LP V.S. LW > ?? > ?? > ??
NLP V.S. NLW > ?? > ?? > ??
LP+C V.S. LW+C ? ? ?
NLP+C V.S. NLW+C ? ? ?
Table 4: Paired t-test results on subsets.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 4 5 6 7 8 9
Pr
ec
isi
on
Section ID
Figure 3: Precision by section in the fullset.
5.2.2 Fullset results
As described in the Section 5.1.2, for the fullset
experiments, we conduct a section-wise evalua-
tion, selecting 40 templates from each of the 9 sec-
tions of the NLP+C results. The results are shown
in Figure 3. It can be observed that the precision
for each section decreases when the section ID in-
creases. The results indicate the effectiveness of
our approach, since it can rank good templates in
top sections and bad templates in bottom sections.
According to the section-wise precision data, we
are able to determine the template score threshold
for choosing different numbers of top templates in
different applications.
5.2.3 Templates for category collection
cleaning
Since our input category collection is automati-
cally constructed from the web, some wrong or
invalid category names is inevitably contained. In
this subsection, we apply our category templates
to clean the category collection. The basic idea is
that if a category can match a template, it is more
likely to be correct. We compute a new score for
every category name H as follows,
S
new
(H) = log(1 + S(H)) ? S(T
?
) (15)
where S(H) is the existing category score, deter-
mined by its frequency in the corpus. Here S(T
?
)
is the score of template T
?
, the best template (i.e.,
the template with the highest score) for the cate-
gory.
Then we re-rank the categories according to
their new scores to get a re-ranked category list.
We randomly sampled 150 category names from
the top 2 million categories of each list (the old list
and the new list) and asked annotators to judge the
807
quality of the categories. The annotation results
show that, after re-ranking, the precision increases
from 0.81 to 0.89 (i.e., the percent of invalid cate-
gory names decreases from 19% to 11%).
6 Conclusion
In this paper, we studied the problem of build-
ing templates for a large collection of category
names. We tested three techniques (tuple scor-
ing by weighted term?hypernym mapping, non-
linear score fusion, refinement by term clusters)
and found that all of them are very effective and
their combination achieves the best performance.
By employing the output templates to clean our
category collection mined from the web, we get
apparent quality improvement. Future work in-
cludes supporting multi-argument templates, dis-
ambiguating headwords of category names and ap-
plying our approach to general short text template
mining.
Acknowledgments
We would like to thank the annotators for their ef-
forts in annotating the templates. Thanks to the
anonymous reviewers for their helpful comments
and suggestions. This work is supported in part by
China National 973 program 2014CB340301 and
NSFC grant 61379043.
References
Ganesh Agarwal, Govind Kabra, and Kevin Chen-
Chuan Chang. 2010. Towards rich query interpreta-
tion: walking back and forth for mining query tem-
plates. In Proceedings of the 19th international con-
ference on World wide web, pages 1?10. ACM.
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 19?27. Association for Computational Lin-
guistics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 976?986. Association for Computational Lin-
guistics.
Jackie Chi Kit Cheung and Xiao Li. 2012. Sequence
clustering and labeling for unsupervised query intent
discovery. In Proceedings of the fifth ACM interna-
tional conference on Web search and data mining,
pages 383?392. ACM.
Jesualdo Tomas Fernandez-Breis, Luigi Iannone, Ig-
nazio Palmisano, Alan L Rector, and Robert
Stevens. 2010. Enriching the gene ontology via the
dissection of labels using the ontology pre-processor
language. In Knowledge Engineering and Manage-
ment by the Masses, pages 59?73. Springer.
Jun Han, Ju Fan, and Lizhu Zhou. 2013.
Crowdsourcing-assisted query structure interpreta-
tion. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2092?2098. AAAI Press.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics -
Volume 2, COLING ?92, pages 539?545, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048?1056.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 572?579. ACM.
Yanen Li, Bo-June Paul Hsu, and ChengXiang Zhai.
2013. Unsupervised identification of synonymous
query intent templates for attribute intents. In Pro-
ceedings of the 22nd ACM international conference
on Conference on information & knowledge man-
agement, pages 2029?2038. ACM.
Xiao Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1337?1345. Association
for Computational Linguistics.
Sandeep Pandey and Kunal Punera. 2012. Unsuper-
vised extraction of template structure in web search
queries. In Proceedings of the 21st international
conference on World Wide Web, pages 409?418.
ACM.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 613?619.
ACM.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In HLT-
NAACL, volume 4, pages 321?328.
808
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 938?947. Association for Com-
putational Linguistics.
Manuel Quesada-Mart?nez, Jesualdo Tom?as
Fern?andez-Breis, and Robert Stevens. 2012.
Enrichment of owl ontologies: a method for defin-
ing axioms from labels. In Proceedings of the First
International Workshop on Capturing and Refining
Knowledge in the Medical Domain (K-MED 2012),
Galway, Ireland, pages 1?10.
Joseph Reisinger and Marius Pasca. 2011. Fine-
grained class label markup of search queries. In
ACL, pages 1200?1209.
Nikos Sarkas, Stelios Paparizos, and Panayiotis
Tsaparas. 2010. Structured annotations of web
queries. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data,
pages 771?782. ACM.
Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class
mining: distributional vs. pattern-based approaches.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 993?1001. As-
sociation for Computational Linguistics.
Mark Stevenson and Mark A Greenwood. 2005. A
semantic approach to ie pattern induction. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 379?386. As-
sociation for Computational Linguistics.
Idan Szpektor, Aristides Gionis, and Yoelle Maarek.
2011. Improving recommendation for long-tail
queries via templates. In Proceedings of the 20th
international conference on World wide web, pages
47?56. ACM.
Allan Third. 2012. Hidden semantics: what can we
learn from the names in an ontology? In Proceed-
ings of the Seventh International Natural Language
Generation Conference, pages 67?75. Association
for Computational Linguistics.
Benjamin Van Durme and Marius Pasca. 2008. Find-
ing cars, goddesses and enzymes: Parametrizable
acquisition of labeled instances for open-domain in-
formation extraction. In AAAI, volume 8, pages
1243?1248.
Roman Yangarber. 2003. Counter-training in discov-
ery of semantic patterns. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pages 343?350. Association
for Computational Linguistics.
Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, and
Chin-Yew Lin. 2011. Nonlinear evidence fusion
and propagation for hyponymy relation mining. In
ACL, volume 11, pages 1159?1168.
809
