Compound Noun Segmentat ion Based on Lexical Data 
Extracted from Corpus* 
J untae  Yoon  
j t yoon@l inc .c i s .upenn.edu  
IRCS,  Un ivers i ty  of  Pennsy lvan ia ,  
3401 Walnut  St. ,  Su i te  400A,  
Ph i lade lph ia ,  PA  19104-6228,  USA 
Abst ract  
Compound noun analysis is one of the crucial prob- 
lems in Korean language processing because a series 
of nouns in Korean may appear without white space 
in real texts, which makes it difficult to identify the 
morphological constituents. This paper presents an 
effective method of Korean compound noun segmen- 
tation based on lexical data extracted from corpus. 
The segmentation is done by two steps: First, it is 
based on manually constructed built-in dictionary 
for segmentation whose data were extracted from 30 
million word corpus. Second, a segmentation algo- 
rithm using statistical data is proposed, where sim- 
ple nouns and their frequencies are also extracted 
from corpus. The analysis is executed based on CYK 
tabular parsing and min-max operation. By exper- 
iments, its accuracy is about 97.29%, which turns 
out to be very effective. 
1 I n t roduct ion  
Morphological analysis is crucial for processing the 
agglutinative language like Korean since words in 
such languages have lots of morphological variants. 
A sentence is represented by a sequence of eojeols 
which are the syntactic unit~ delimited by spacing 
characters in Korean. Unlike in English, an eojeol 
is not one word but composed of a series of words 
(content words and functional words). In particu- 
lar, since an eojeol can often contain more than one 
noun, we cannot get proper interpretation of the sen- 
tence or phrase without its accurate segmentation. 
The problem in compound noun segmentation is 
that it is not possible to register all compound nouns 
in the dictionary since nouns are in the open set 
of words as well as the number of them is very 
large. Thus, they must be treated as unseen words 
without a segmentation process. Furthermore, ac- 
curate compound noun segmentation plays an im- 
portant role in the application system. Compound 
noun segmentation is necessarily required for im- 
proving recall and precision in Korean information 
* This work was supported by a KOSEF's  postdoctoral fel- 
lowship grant. 
retrieval, and obtaining better translation in ma- 
chine translation. For example, suppose that a 
compound noun 'seol'agsan-gugrib-gongwon(Seol'ag 
Mountain National Park)' appear in documents. 
A user might want to retrieve documents about 
'seol'agsan(Seol'ag Mountain)', and then it is likely 
that the documents with seol'agsan-gugrib-gongwon' 
are also the ones in his interest. Therefore, it 
should be exactly segmented before indexing in or- 
der for the documents to be retrieved with the query 
'seol'agsan'. Also, to translate 'seol'agsan-gugrib- 
gongwon' to Seol'ag Mountain National Park, the 
constituents should be identified first through the 
process of segmentation. 
This paper presents two methods for segmentation 
of compound nouns. First, we extract compound 
nouns from a large size of corpus, manually divide 
them into simple nouns and construct the hand built 
segmentation dictionary with them. The dictionary 
includes compound nouns which are frequently used 
and need exceptional process. The number of data 
are about 100,000. 
Second, the segmentation algorithm is applied if 
the compound noun does not exist in the built-in 
dictionary. Basically, the segmenter is based on fre- 
quency of individual nouns extracted from corpus. 
However, the problem is that it is difficult to dis- 
tinguish proper noun and common noun since there 
is no clue like capital letters in Korean. Thus, just 
a large amount of lexical knowledge does not make 
good results if it contains incorrect data and also it is 
not appropriate to use frequencies obtained by auto- 
matically tagging large corpus. Moreover, sufficient 
lexical data cannot be acquired from small amounts 
of tagged corpus. 
In this paper, we propose a method to get sim- 
ple nouns and their frequencies from frequently oc- 
curring eojeols using repetitiveness of natural lan- 
guage. The amount of eojeols investigated is man- 
ually tractable and frequently used nouns extracted 
from them are crucial for compound noun segmen- 
tation. Furthermore, we propose rain-max compo- 
sition to divide a sequence of syllables, which would 
be proven to be an effective method by experiments. 
l_qF~ 
To briefly show the reason that we select the oper- 
ation, let us consider the following example. Sup- 
pose that a compound noun be composed of four 
syllables 'sl s2s3s4 '. There are several possibilities of 
segmentation i  the sequence of syllables, where we 
consider the following possibilities (Sl/S2S3S4) and 
(sls2/s3s4). Assume that 'sl' is a frequently ap- 
pearing word in texts whereas 's2s3s4' is a rarely 
occurring sequence of syllables as a word. On the 
other hand 'sis2' and 's3s4' occurs frequently but 
although they don't occur as frequently as 'sl'. In 
this case, the more likely segmentation would be 
(sls2/s3s4). It means that a sequence of syllables 
should not be divided into frequently occurring one 
and rarely occurring one. In this sense, min-max is 
the appropriate operation for the selection. In other 
words, rain value is selected between two sequences 
of syllables, and then max is taken from min values 
selected. To apply the operation repetitively, we use 
the CYK tabular parsing style algorithm. 
2 Lexica l  Data  Acquisition 
Since the compound noun consists of a series of 
nouns, the probability model using transition among 
parts of speech is not helpful, and rather lexical in- 
formation is required for the compound noun seg- 
mentation. Our segmentation algorithm is based on 
a large collection of lexical information that consists 
of two kinds of data: One is the hand built seg- 
mentation dictionary (HBSD)  and the other is the 
simple noun dictionary for segmentation (SND). 
2.1 Hand-Built  Segmentation Dictionary 
The first phase of compound noun segmentation uses 
the built-in dictionary (HBSD). The advantage of 
using the built-in dictionary is that the segmenta- 
tion could (1) be very accurate by hand-made data 
and (2) become more efficient. In Korean compound 
noun, one syllable noun is sometimes highly ambigu- 
ous between suffix and noun, but human can easily 
identify them using semantic knowledge. For ex- 
ample, one syllable noun 'ssi' in Korean might be 
used either as a suffix or as a noun which means 
'Mr/Ms' or 'seed' respectively. Without any seman- 
tic information, the best way to distinguish them 
is to record all the compound noun examples con- 
taining the meaning of seed in the dictionary since 
the number of compound nouns containing a mean- 
ing of 'seed' is even smaller. Besides, we can treat 
general spacing errors using the dictionary. By the 
spacing rule for Korean, there should be one content 
word except noun in an eojeol, but it turns out that 
one or more content words of short length sometimes 
appear without space in real texts, which causes the 
lexical ambiguities. It makes the system inefficient 
to deal with all these words on the phase of basic 
morphological analysis. 
compound nouns 
gajuggudu(leather shoes) 
gajugggeun(leather string) 
gaguyong(used for furniture) 
sagwassi(apple seed) 
podossi(graph seed) 
chuggutim (football team) 
analysis information 
gaj ug(leather)-bgudu(shoes) 
gajug(leather) +ggeun(string) 
gagu(furniture)-bxyong(used for) 
sagwa(apple) q-nssi (seed) 
podo(grape)-t-nssi(seed) 
chuggu(foot ball)+tim(team) 
Table 1: Examples of compound noun and analysis 
information in built-in dictionary 
To construct the dictionary, compound nouns axe 
extracted from corpus and manually elaborated. 
First, the morphological analyzer analyzes 30 mil- 
lion eojeol corpus using only simple noun dictionary, 
and the failed results are candidates for compound 
noun. After postpositions, if any, are removed from 
the compound noun candidates of the failure eoje- 
ols, the candidates axe modified and analyzed by 
hand. In addition, a collection of compound nouns 
of KAIST (Korea Advanced Institute of Science & 
Technology) is added to the dictionary in order to 
supplement them. The number of entries contained 
in the built-in dictionary is about 100,000. Table 1 
shows some examples in the built-in dictionary. _The 
italic characters such as 'n' or 'x' in analysis infor- 
mation (right column) of the table is used to make 
distinction between oun and suffix. 
2.2 Extraction of Lexical Information for 
Segmentation from Corpus 
As we said earlier, it is impossible for all compound 
nouns to be registered in the dictionary, and thus the 
built-in dictionary cannot cover all compound nouns 
even though it gives more accurate results. We need 
some good segmentation model for compound noun, 
therefore. 
In compound noun segmentation, the thing that 
we pay attention to was that lexical information is 
crucial for segmenting noun compounds. Since a 
compound noun consists only of a sequence of nouns 
i.e. (noun)+, the transition probability of parts of 
speech is no use. Namely, the frequency of each noun 
plays highly important role in compound noun seg- 
mentation. Besides, since the parameter space is 
huge, we cannot extract enough lexicai information 
from hundreds of thousands of POS tagged corpus 1
even if accurate lexical information can be extracted 
from annotated corpus. Thus, a large size of cor- 
pus should be used to extract proper frequencies of 
nouns. However, it is difficult to look at a large size 
of corpus and to assign analyses to it, which makes 
it difficult to estimate the frequency distribution of 
words. Therefore, we need another approach for ob- 
taining frequencies of nouns. 
~It is the size of POS tagged corpus currently publicized 
by ETRI (Electronics and Telecommunications Research In- 
stitute) project. 
197 
l#l 
l l i i i l l~  
Figure 1: Distribution of eojeols in Korean corpus 
It must be noted here that each noun in compound 
nouns could be easily segmented by human in many 
cases because it has a prominent figure in the sense 
that it is a frequently used word and so familiar with 
him. In other words, nouns prominent in documents 
can be defined as frequently occurred ones, which we 
call distinct nouns. Compound nouns contains these 
distinct nouns in many cases, which makes it easier 
to segment them and to identify their constituents. 
Empirically, it is well-known that too many words 
in the dictionary have a bad influence on morpho- 
logical analysis in Korean. It is because rarely used 
nouns result in oversegmentation f they are included 
in compound noun segmentation dictionary. There- 
fore, it is necessary to select distinct nouns, which 
leads us to use a part of corpus instead of entire 
corpus that consists of frequently used ones in the 
corpus. 
First, we examined istribution of eojeols in cor- 
pus in order to make the subset of corpus to extract 
lexical frequencies of nouns. The notable thing in 
our experiment is that the number of eojeols in cor- 
pus is increased in proportion to the size of corpus, 
but a small portion of eojeols takes most parts of the 
whole corpus. For instance, 70% of the corpus con- 
sists of just 60 thousand types of eojeols which take 
7.5 million of frequency from 10 million eojeol corpus 
and 20.5 million from 30 million eojeols. The lowest 
frequency of the 60,000 eojeols is 49 in 30 million eo- 
jeol corpus. We decided to take 60,000 eojeols which 
are manually tractable and compose most parts of 
corpus (Figure 1). 
Second, we made morphological analyses for the 
60,000 eojeols by hand. Since Korean is an aggluti- 
native language, an eojeol is represented by a se- 
quence of content words and functional words as 
mentioned before. Especially, content words and 
functional words often have different distribution 
of syllables. In addition, inflectional endings for 
predicate and postpositions for nominals also have 
quite different distribution for syllables. Hence we 
can distinguish the constituents of eojeols in many 
cases. Of course, there are also many cases in which 
the result of morphological analysis has ambigui- 
ties. For example, an eojeol 'na-neun' in Korean 
has ambiguity of 'na/N+neun/P', 'na/PN+neun/P' 
and 'nal/V+neun/E'. In this example, the parts 
of speech N, PN, P, V and E mean noun, pro- 
noun, postposition, verb and ending, respectively. 
On the other hand, many eojeols which are ana- 
lyzed as having ambiguities by a morphological n- 
alyzer are actually not ambiguous. For instance, 
'ga-geora' (go/imperative) has ambiguities by most 
morphological nalyzer among 'ga/V+geora/E' and 
'ga/N+i/C+geora/E' (C is copula), but it is actu- 
ally not ambiguous. Such morphological mbiguity 
is caused by overgeneration f the morphological n- 
alyzer since the analyzer uses less detailed rules for 
robustness of the system. Therefore, if we examine 
and correct he results scrupulously, many ambigui- 
ties can be removed through the process. 
As the result of the manual process, only 15% of 
60,000 eojeols remain ambiguous at the mid-level of 
part of speech classification 2. Then, we extracted 
simple nouns and their frequencies from the data. 
Despite of manual correction, there must be ambigu- 
ities left for the reason mentioned above. There may 
be some methods to distribute frequencies in case 
of ambiguous words, but we simply assign the equal 
distribution to them. For instance, gage has two pos- 
sibilities of analysis i.e. 'gage/N' and 'galV+ge/E', 
and its frequency is 2263, in which the noun 'gage' is 
assigned 1132 as its frequency. Table 2 shows exam- 
ples of manually corrected morphological nalyses of 
eojeols containing a noun 'gage' and their frequen- 
cies. We call the nouns extracted in such a way a 
set of distinct nouns. 
In addition, we supplement the dictionary with 
other nouns not appeared in the words obtained 
by the method mentioned above. First, nouns of 
more than three syllables are rare in real texts in 
Korean, as shown in Lee and Ahn (1996). Their 
experiments proved that syllable based bigram in- 
dexing model makes much better result than other 
n-gram model such as trigram and quadragram in 
Korean IR. It follows that two syllable nouns take 
an overwhelming majority in nouns. Thus, there are 
not many such nouns in the simple nouns extracted 
by the manually corrected nouns (a set of distinct 
nouns). In particular, since many nouns of more 
2At the mid-level of part of speech classification, for ex- 
ample, endings and postpositions are represented just by one 
tag e.g. E and P. To identify the sentential or clausal type 
(subordinate or declarative) in Korean, the ending should be 
subclassified for syntactic analysis more detail which can be 
done by statistical process. It is beyond the subject of this 
paper. 
198 
eojeols constituents meaning 
gage gage/N@ga/V+ge/E store@go 2263 
gage-ga gage/N+ga/P store/SUBJ 165 
gage-neun gage/N+neun/P@ga/V+geneun/E store/TOP@go 113 
gage-ro gage/N+ro/P to the store 166 
gage-reul gage/N+reul/P store/OBJ 535 
gage-e gage/N+e/P in the store 312 
gage-eseo gage/N+eseo/P in the store 299 
gage-yi gage/N+yi /P  of the store 132 
frequencies 
extracted noun frequency 
gage store 2797 
Table 2: Example of extraction of distinct nouns. Here N, V, P and E mean tag for noun, verb, postposition 
and ending and '@' is marked for representation f ambiguous analysis 
than three syllables are derived by a word and suf- 
fixes and have some syllable features, they are useful 
for distinguishing the boundaries of constituents in
compound nouns. We select nouns of more than 
three syllables from morphological dictionary which 
is used for basic morphological nalysis and consists 
of 89,000 words (noun, verb, adverb etc). Second, 
simple nouns are extracted from hand-built segmen- 
tation dictionary. We selected nouns which do not 
exist in a set of distinct nouns. 
The frequency is assigned equally with some value 
fq. Since the model is based on min-max composi- 
tion and the nouns extracted in the first phase are 
most important, the value does not take an effect on 
the system performance. 
The nouns extracted in this way are referred to 
as a set of supplementary nouns. And the SND for 
compound noun segmentation is composed of a set 
of distinct nouns and a set of supplementary nouns. 
The number of simple nouns for compound noun seg- 
mentation is about 50,000. 
3 Compound Word  Segmentat ion  
A lgor i thm 
3.1 Basic Idea  
To simply describe the basic idea of our compound 
noun segmentation, we first consider a compound 
noun to be segmented into only two nouns. Given a 
compound noun, it is segmented by the possibility 
that a sequence of syllables inside it forms a word. 
The possibility that a sequence of syllables forms a 
word is measured by the following formula. 
Word(si, . . .  sj) - fq(si , . . ,  sj) Iq~ (1) 
In the formula, fq(s~,...sj) is the frequency of 
the syllable s i . . .s j ,  which is obtained from SND 
constructed on the stages of lexical data extraction. 
And, fqN is the total sum of frequencies of simple 
nouns. Colloquially, the equation (1) estimates how 
much the given sequence of syllables are likely to be 
word. If a sequence of syllables in the set of distinct 
nouns is included in a compound noun, it is more 
probable that it is divided around the syllables. If 
a compound noun consists of, for any combination 
of syllables, sequences of syllables in the set of sup- 
plementary nouns, the boundary of segmentation is 
somewhat fuzzy. Besides, if a given sequence of syl- 
lables is not found in SND, it is not probable that it 
is a noun. 
Consider a compound noun 'hag?gyo-saeng- 
hwal(school life)'. In case that segmentation of 
syllables is made into two, there would be four 
possibilities of segmentation for the example as 
follows: 
1. hag 9yo-saeng-hwal 
2. hag-gyo saeng-hwal 
3. hag-gyo-saeng hwal 
4. hag-gyo-saeng-hwal ? 
As we mentioned earlier, it is desirable that the eo- 
jeol is segmented in the position where each sequence 
of syllables to be divided occurs frequently enough 
in training data. As the length of a sequence of sylla- 
bles is shorter in Korean, it occurs more frequently. 
That is, the shorter part usually have higher fre- 
quency than the other (longer) part when we divide 
syllables into two. Moreover, if the other part is 
the syllables that we rarely see in texts, then the 
part would not be a word. In the first of the above 
example, hag is a sequence of syllable appearing fre- 
quently, but gyo-saeng-hwa! is not. Actually, gyo- 
saeng-hwal is not a word. On the other hand, both 
hag-gyo and saeng-hwal re frequently occurring syl- 
lables, and actually they are all words. Put another 
way, if it is unlikely that one sequence of syllables is 
a word, then it is more likely that the entire syllables 
are not segmented. The min-max composition is a 
suitable operation for this case. Therefore, we first 
199 
take the minimum value from the function Word for 
each possibility of segmentation, and then we choose 
the maximum from the selected minimums. Also, 
the argument taking the maximum is selected as the 
most likely segmentation result. 
Here, Word(si . . .  sj) is assigned the frequency of 
the syllables i . . .  sj from the dictionary SND. Be- 
sides, if two minimums are equal, the entire sylla- 
ble such as hag-gyo-saeng-hwal, if compared, is pre- 
ferred, the values of the other sequence of syllables 
are compared or the dominant pattern has the pri- 
ority. 
3.2 Segmentat ion  A lgor i thm 
In this section, we generalize the word segmentation 
algorithm based on data obtained by the training 
method escribed in the previous ection. The basic 
idea is to apply min-max operation to each sylla- 
ble in a compound noun by the bottom-up strat- 
egy. That is, if the minimum between Words of 
two sequences of syllables is greater than Word of 
the combination of them, the syllables should be 
segmented. For instance, let us suppose a com- 
pound noun consist of two syllable Sl and s2. If 
min(Word(Sl), Word(s2)) > Word(sis2), then the 
compound noun is segmented into Sl and s2. It is 
not segmented, otherwise. That is, we take the max- 
imum among minimums. For example, 'hag' is a fre- 
quently occurring word, but 'gyo' is not in Korean. 
In this case, we can hardly regard the sequence of 
syllable 'hag-gyo' as the combination of two words 
'hag' and 'gyo'. The algorithm can be applied recur- 
sively from individual syllable to the entire syllable 
of the compound noun. 
The segmentation algorithm is effectively imple- 
mented by borrowing the CYK parsing method. 
Since we use the bottom-up strategy, the execu- 
tion looks like composition rather than segmenta- 
tion. After all possible segmentation f syllables be- 
ing checked, the final result is put in the top of the 
table. When a compound noun is composed of n 
syllables, i.e. sis2. . ,  s,~, the composition is started 
from each si (i = 1. . .  n). Thus, the possibility that 
the individual syllable forms a word is recorded in 
the cell of the first row. 
Here, Ci,j is an element of CYK ta- 
ble where the segment result of the sylla- 
bles sj,...j+i-1 is stored (Figure 2). For 
instance, the segmentation result such that 
ar g max(min( W ord( s l ), Word(s2)), Word(s1 s2)) 
is stored in C1,2. What is interesting here is 
that the procedure follows the dynamic pro- 
gramming. Thus, each cell C~,j has the most 
probable segmentation result for a series of syl- 
lables sj ..... j+i-1- Namely, C1,2 and C2,3 have 
the most likely segmentation of sis2 and s2s3 
respectively. When the segmentation of sls2s3 is 
about to be checked, min(value(C2,1), value(C1,3)), 
1 
i 
11-1 
n 
1 ... j ... n -1  n 
\ 
\ 
composition resul t  
for sl""sl+l-1 
Figure 2: Composition Table 
min(value(Cl,1),value(C2,2)) and Word(sls2s3) 
are compared to determine the segmentation for 
the syllables, because all Ci,j have the most likely 
segmentation. Here, value(Ci,j) represents the 
possibility value of Ci,j. 
Then, we can describe the segmentation algorithm 
as follows: 
When it is about to make the segmentation f syl- 
lables s~... sj, the segmentation results of less length 
of syllables like s i . . . s j -1 ,  S~+l... sj and so forth 
would be already stored in the table. In order to 
make analysis of s i . . .  s j, we combine two shorter 
length of analyses and the word generation possibil- 
ities are computed and checked. 
To make it easy to explain the algorithm, let us 
take an example compound noun 'hag-gyo-saeng- 
hwa~ (school ife) which is segmented with 'haggyo' 
(school) and 'saenghwar (life) (Figure 3). When it 
comes up to cell C4,1, we have to make the most 
probable segmentation for 'hag-gyo-saeng-hwal' i.e. 
SlS2S3S4. There are three kinds of sequences of syl- 
lables, i.e. sl in CI,1, sis2 in C2,1 and SlS2S3 in C3,1 
that can construct he word consisting of 8182s384 
which would be put in Ca,1. For instance, the word 
sls2s3s4 (hag-gyo-saeng-hwal) is made with Sl (hag) 
combined with sus3s4 (gyo-saeng-hwal). Likewise, 
it might be made by sis2 combined with s3s4 and 
sls2s3 combined with s4. Since each cell has the 
most probable result and its value, it is simple to 
find the best segmentation for each syllables. In 
addition, four cases, including the whole sequences 
of syllables, are compared to make segmentation of
SlS2SaS4 as follows: 
1. rain(value(C3,1), value(C3,4)) 
2. min(value(C2,1), value(C2,3)) 
3. min(value( Cl,1), value(C3,2)) 
4. Word(SlS2SaS4) = Word(hag-gyo-saeng-hwal) 
Again, the most probable segmentation result is 
put in C4,1 with the likelihood value for its segmen- 
tation. We call it MLS (Most Likely Segmentation) 
200 
hag gyo saeng hwa2 
( ? .... . .  . . . .  . _ _  
arg max(min(w(hag),w(gyo)),w(hag-gyo)) 
Figure 3: State of table when analyzing 'hag-gyo- 
saeng-hwal'. Here, w(si . . . sj) = value(Cij)  
which is found in the following way: 
MLS(C4,z) = 
ar g max(rain(value(C3,1), value( C3,a ) ), 
rain(value(G2,1), value(C2,3)), 
rain(value(C1,1), value(C3,2)), 
Word(sls2s3sa)) 
From the four cases, the maximum value and the 
segmentation result are selected and recorded in 
C4,1. To generalize it, the algorithm is described 
as shown in Figure 4. 
The algorithm is straightforward. Let Word and 
MLS be the likelihood of being a noun and the most 
likely segmentation for a sequence of syllables. In the 
initialization step, each cell of the table is assigned 
Word value for a sequence of syllables sj . . .  sj+i+l 
using its frequency if it is found in SND. In other 
words, if the value of Word for the sequence in each 
cell is greater than zero, the syllables might be as a 
noun a part of a compound noun and so the value is 
recorded as MLS.  It could be substituted by more 
likely one in the segmentation process. 
In order to make it efficient, the segmentation re- 
sult is put as MLS instead of the syllables in case 
the sequence of syllables exists in the HBND. The 
minimum of each Word for constituents of the result 
as Word is recorded. 
Then, the segmenter compares possible analyses 
to make a larger one as shown in Figure 4. When- 
ever Word of the entire syllables is less than that of 
segmented one, the syllables and value are replaced 
with the segmented result and its value. For in- 
stance, sl + s2 and its likelihood substitutes C2,1 
if min(Word(sl) ,  Word(s2)) > Word(sis2). When 
the entire syllables from the first to nth syllable are 
processed, C,~,x has the segmentation result. 
The overall complexity of the algorithm follows 
that of CYK parsing, O(n3). 
3.3 Defau l t  Analys is  and Tun ing  
For the final result, we should take into consideration 
several issues which are related with the syllables 
that left unsegmented. There are several reasons 
that the given string remains unsegmented: 
i .o?1 .. .i i .oo I 
~equence of ~yllabl~ iu diviner gel 
default ~ementatitm pointer 
Figure 5: Default segmentation pointer for 'geon- 
chug-sa-si-heom' where 'si-heom' is a very frequently 
used noun. 
1. The first one is a case where the string consists 
of several nouns but one of them is a unreg- 
istered word. A compound noun 'geon-chug- 
sa-si-heom' is composed of 'geon-chug-sa' and 
'si-heom', which have the meanings of autho- 
rized architect and examination. In this case, 
the unknown noun is caused by the suffix such 
as 'sa' because the suffix derives many words. 
However, it is known that it is very difficult to 
treat the kinds of suffixes since the suffix like 
'sa' is a very frequently used character in Ko- 
rean and thus prone to make oversegmentation 
if included in basic morphological nalysis. 
2. The string might consist of a proper noun alad a 
noun representing a position or geometric infor- 
mation. For instance, a compound noun 'kim- 
dae-jung-dae-tong-ryeong' is composed of 'kim- 
dae-jung' and 'dae-tong-ryeong' where the for- 
mer is personal name and the latter means pres- 
ident respectively. 
3. The string might be a proper noun itself. For 
example, 'willi'amseu' is a transliterated word 
for foreign name 'Williams' and 'hong-gil-dong' 
is a personal name in Korean. Generally, since 
it has a different sequence of syllables from in 
a general Korean word, it often remains unseg- 
mented. 
If the basic segmentation is failed, three proce- 
dures would be executed for solving three problems 
above. For the first issue, we use the set of distinct 
nouns. That is, the offset pointer is stored in the ini- 
tialization step as well as frequency of each noun in 
compound noun is recorded in the table. Attention 
should be paid to non-frequent sequence of syllables 
(ones in the set of supplementary nouns) in the de- 
fault segmentation because it could be found in any 
proper noun such as personal names, place names, 
etc or transliterated words. It is known that the per- 
formance drops if all nouns in the compound noun 
segmentation dictionary are considered for default 
segmentation. We save the pointer to the boundary 
only when a noun in distinct set appears. For the 
above example 'geon-chug-sa-si-heom', the default 
segmentation would be 'geon-chug-sa' nd 'si-heom' 
since 'si-heom' is in the set of distinct nouns and the 
pointer is set before 'si-heom' (Figure 5). 
201 
/* initialization step */ 
for i----1 to n do 
for j= l  to n- i+l do 
value(Ci,j) = Word(s t . . .  sj+i-1); 
MLS(C i , j )  = s t . . .  sj+~-i ; if value(Ci,j) > 0 
?; otherwise 
for i=2 to n do 
for j= 1 to i do 
value( Ci,j ) = max(min( value( Ci_l, j  ), value( C1j+i_ l  ) ), 
min (value( C i -  2,j ) , value( C2,j_ 2 ) ) , 
min(va\[ue(Cl, j) ,value(Ci_,, j+l)),  
Word(st... s~+~)) 
M LS(  Ci,j ) = arg max(min(value( C i _ l j ) ,  value( C i j+ i_ l  ) ), 
min( value( Ci_ 2j  ) , value( C2 j_  2 ) ) , 
Word(s~ . , . si+j)): 
, , , , , ,  , , , ,% . . . . . .  , , , , 
Figure 4: The segmentation algorithm 
If this procedure is failed, the sequence of syllables 
is checked whether it might be proper noun or not. 
Since proper noun in Korean could have a kind of 
nominal suffix such as 'daetongryeong(president)' or 
'ssi(Mr/Ms)'  as mentioned above, we can identify 
it by detaching the nominal suffixes. If there does 
not exist any nominal suffix, then the entire syllables 
would be regarded just as the transliterated foreign 
word or a proper noun like personal or place name. 
4 Experimental Results 
For the test of compound noun segmentation, we 
first extracted compound noun from ETRI POS 
tagged corpus 3. By the processing, 1774 types of 
compound nouns were extracted, which was used as 
a gold standard test set. 
We evaluated our system by two methods: (1) 
the precision and recall rate, and (2) segmentation 
accuracy per compound noun which we refer to as 
SA. They are defined respectively as follows: 
P rec is ion  = 
number of correct constituents in proposed segment results 
total number o\] constituents in proposed segment results 
Reca l l  = 
number of correct constituents in proposed segment results 
total number of constituents in compoundnouns 
SA = 
number of correctly segmented compound nouns  
to ta l  number of compoundnouns 
3The corpus was constructed by the ETRI (Electronics 
and Telecommunications Research Institute) project for stan- 
dardization of natural anguage processing technology and the 
corpus presented consists of about 270,000 eojeols at present. 
What influences on the Korean IR system is 
whether words are appropriately segmented or not. 
The precision and recall estimate how appropriate 
the segmentation results are. They are 98.04% and 
97.80% respectively, which shows that our algorithm 
is very effective (Table 3). 
SA reflects how accurate the segmentation is for a 
compound noun at all. We compared two methods: 
(1) using only the segmentation algorithm with de- 
fault analysis which is a baseline of our system and 
so is needed to estimate the accuracy of the algo- 
rithm. (2) using both the built-in dictionary and the 
segmentation algorithm which reflects system accu- 
racy as a whole. As shown in Table 4, the baseline 
performance using only distinct nouns and the al- 
gorithm is about 94.3% and fairly good. From the 
results, we can find that the distinct nouns has great 
impact on compound noun segmentation. Also, the 
overall segmentation accuracy for the gold standard 
is about 97.29% which is a very good result for the 
application system. In addition, it shows that the 
built-in dictionary supplements he algorithm which 
results in better segmentation. 
Lastly, we compare our system with the previous 
work by (Yun et al , 1997). It is impossible that we 
directly compare our result with theirs, since the test 
set is different. It was reported that the accuracy 
given in the paper is about 95.6%. When comparing 
the performance only in terms of the accuracy, our 
system outperforms theirs. 
Embeded in the morphological nalyzer, the com- 
pound noun segmentater is currently being used for 
some projects on MT and IE which are worked in 
several institutes and it turns out that the system is 
very effective. 
202 
Number of correct constituents 
Rate 
Precision 
3553/3628 
98.04 
Recall 
3553/3637 
97.80 
Table 3: Result 1: Precision and recall rate 
SA 
Number of correct constituents 
Rate 
Whole System Baseline 
1726\]1774 1673/1774 
97.29 94.30 
Table 4: Result 2: Segmentation accuracy for Compound Noun 
5 Conc lus ions  
In this paper, we presented the new method for 
Korean compound noun segmentation. First, we 
proposed the lexical acquisition for compound noun 
analysis, which consists of the manually constructed 
segmentation dictionary (HBSD) and the dictionary 
for applying the segmentation algorithm (SND). The 
hand-built segmentation dictionary was made manu- 
ally for compound nouns extracted from corpus. The 
simple noun dictionary is based on very frequently 
occurring nouns which are called distinct nouns be- 
cause they are clues for identifying constituents of 
compound nouns. Second, the compound noun was 
segmented based on the modification of CYK tab- 
ular parsing and min-max composition, which was 
proven to be the very effective method by exper- 
iments. The bottom up approach using min-max 
operation guarantees the most likely segmentation, 
being applied in the same way as dynamic program- 
ming. 
With our new method, the result for segmenta- 
tion is as accurate as 97.29%. Especially, the al- 
gorithm made results good enough and the built- 
in dictionary supplemented the algorithm. Conse- 
quently, the methodology is promising and the seg- 
mentation system would be helpful for the applica- 
tion system such as machine translation and infor- 
mation retrieval. 
6 Acknowledgement  
We thank Prof. Mansuk Song at Yonsei Univ. and 
Prof. Key-Sun Choi at KAIST to provide data for 
experiments. 
Re ferences  
Cha, J., Lee, G. and Lee, J. 1998. Generalized Un- 
known Morpheme Guessing for Hybrid POS Tag- 
ging of Korean. In Proceedings of the 6th Work- 
shop on Very Large Corpora. 
Choi, K. S., Han, Y. S., Han, Y. G., and Kwon, O. 
W. 1994. KAIST Tree Bank Project for Korean: 
Present and Future Development. In Proceedings 
of the International Workshop on Sharable Natu- 
ral Language Resources. 
Elmi, M. A. and Evens, M. 1998. Spelling Cor- 
rection Using Context. In Proceedings o\] COL- 
ING/A CL 98 
Hopcroft, J. E. and Ullman, J. D. 1979. Introduc- 
tion to Automata Theory, Languages, and Com- 
putation. 
Jin, W. and Chen, L. 1995. Identifying Unknown 
Words in Chinese Corpora In Proceedings of NL- 
PRS 95 
Lee, J. H. and Ahn, J. S. 1996. Using n-grams 
for Korean Text Retrieval. In Proceedings of 19th 
Annual International A CM SIGIR Conference on 
Research and Development in Information Re- 
trieval 
Li, J. and Wang, K. 1995. Study and Implementa- 
tion of Nondictionary Chinese Segmentation. In 
Proceedings of NLPRS 95 
Nagao, M. and Mori, S. 1994. A New Method of 
N-gram Statistics for Large Number of N and Au- 
tomatic Extraction of Words and Phrases from 
Large Text Data of Japanese. In Proceedings of 
COLING 94 
Park, B, R., Hwang, Y. S. and Rim, H. C. 1997. 
Recognizing Korean Unknown Words by Compar- 
atively Analyzing Example Words. In Proceedings 
o\] ICCPOL 97 
Sproat, R. W., Shih, W., Gale, W. and Chang, 
N. 1994. A Stochastic Finite-State Word- 
segmentation Algorithm for Chinese. In Proceed- 
ings of the 32nd Annual Meeting o\] ACL 
Yoon, J., Kang, B. and Choi, K. S. 1999. Informa- 
tion Retrieval Based on Compound Noun Analysis 
for Exact Term Extraction. Submitted in Journal 
of Computer Processing of Orientla Language. 
Yoon, J., Lee, W. and Choi, K. S. 1999. Word Seg- 
mentation Based on Estimation of Words from 
Examples. Technical Report. 
Yun, B. H., Cho, M. C. and Rim, H. C. 1997. Seg- 
menting Korean Compound Nouns Using Statis- 
tical Information and a Preference Rules. In Pro- 
ceedings of PACLING. 
203 
Towards Translingual Information Access 
using Portable Information Extraction 
Michael White, Claire Cardie, Chung-hye Han, Nari Kim, # 
Benoit Lavoie, Martha Palmer, Owen Rainbow,* Juntae Yoon 
CoGenTex, Inc. 
Ithaca, NY, USA 
\[mike,benoit.owen\] 
@cogentex.com 
Institute for Research in 
Cognitive Science 
University of Pennsylvania 
Philadelphia, PA, USA 
chunghye@babel, ling. upenn, edu 
\[ nari, mpalmer, j tyoon } 
@linc. cis.upenn.edu 
Dept. of Computer Science 
Cornell University 
Ithaca, NY, USA 
cardie@cs, cornell, edu 
Abstract 
We report on a small study undertaken to 
demonstrate the feasibility of combining 
portable information extraction with MT in 
order to support translingual information 
access. After describing the proposed 
system's usage scenario and system design, 
we describe our investigation of transferring 
information extraction techniques developed 
for English to Korean. We conclude with a 
brief discussion of related MT issues we plan 
to investigate in future work. 
1 Introduction 
In this paper, we report on a small study 
undertaken to demonstrate the feasibility of 
combining portable information extraction with 
MT in order to support ranslingual information 
access. The goal of our proposed system is to 
better enable analysts to perform information 
filtering tasks on foreign language documents. 
This effort was funded by a SBIR Phase I award 
from the U.S. Army Research Lab, and will be 
pursued further under the DARPA TIDES 
initiative. 
Information extraction (IE) systems are 
designed to extract specific types of information 
from natural language texts. In order to achieve 
acceptable accuracy, IE systems need to be 
tuned for a given topic domain. Since this 
domain tuning can be labor intensive, recent IE 
research has focused on developing learning 
algorithms for training IE system components 
(cf. Cardie, 1997, for a survey). To date, 
however, little work has been done on IE 
systems for languages other than English 
(though cf. MUC-5, 1994, and MUC-7, 1998, 
for Japanese IE systems); and, to our knowledge, 
none of the available techniques for the core task 
of learning information extraction patterns have 
been extended or evaluated for multilingual 
information extraction (though again cf. MUC-7, 
1998, where the use of learning techniques for 
the IE subtasks of named entity recognition and 
coreference r solution are described). 
Given this situation, the primary objective of 
our study was to demonstrate he feasibility of 
using portable--i.e., easily trainable--IE 
technology on Korean documents, focusing on 
techniques for learning information extraction 
patterns. Secondary objectives of the study were 
to elaborate the analyst scenario and system 
design. 
2 Analyst Scenario 
Figure 1 illustrates how an intelligence analyst 
might use the proposed system: 
? The analyst selects one or more Korean 
documents in which to search for 
information (this step not shown). 
# Current affiliation: Konan Technology, Inc., Korea, nari@konantech.co.kr 
* Current affiliation: A'IT Labs-Research, Florham Park, NJ, USA, rambow@research.att.com 
31 
Ouery  
Find Report 
Event: Nest !!lg ........... 
sourcn:l . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ~ 
sate :  I ................. ' ......................... ' ................. i 
Locat Ion: I~u.'~h K..e~.e..a.; ..................................... j I~ 
Part clpant : I .................................................................. i 
Iseun:i~'North Korea" AND "missiles" i 
I 
Response  to  Ouery  
The reports Indicate 2 meetings held In South Korea on the 
issues of North Korea anti missiles: 
Sources Translated Extracts 
Joon,ap~l A ~ ~  
. . . .  I ,4 ~t ln ,  g ~# ,#~=1# o,1 Apf J l  ~ sLYout tP~ I10t 
,!nerF, orea j /ine? ~t~wn Saoul end Tokyo for  the 
Noes / - -  t~Q I ela~gen?? d~/tu~tlons ~uc/t eg Alottl I  Kofgm'~ 
Trans la t ion  o f  Korean  Source  Repor t  
\[Joongang Dally\] 
Korean. Japanese H in i s ters  Discuss NK Po l i cy  
The tmo ministers ~9rsed that any further launching of a 
missile by North Korean would undermine the security of 
~Northeast Asia and the Korea, the United States and Japan 
should take Joint steps against the North Korean missile 
threat. 
}-long requested that Koeura cork to normalize Japan's 
relations with North Korea. rather than cutting channels 
of dialogue bet#men the two countries. 
Koeura said that i f  North Korea continues Its missile 
testing, the Japanese government will definitely stop 
making contributions to KEDO. 
The tee ministers also tentatively agreed that J~anese 
primo minister Kslzo Obuchl should make a state visit  to 
Korea on or around Nerch 20. 
Korean  Source  Repor t  
E t -~ "~I -D lX i '~  ~oo ~ Cll~o" 
oj_a, xd~. ~ ~.\]Ol D IXF~ ~F ~,~FI,,t ~'-9-, ~.~OF ~t~l.~. ~t l  
ud~Otl ~l . : , r t}  ~\]l~i/ ~ol~.-E.II .?-INto ?,,toiSF.~. ~t.-Ol-~ 8-.~01 
~XlI~II= = ~ZISH LDFPI~_ ~C.F~ uH~C3 ~-~-  ~.1-~..~ OF-..It~ 
~01 ~cF.  
x~.~ ~.~OI ~l,.Lt~ EH~=  ~S lO I  ...~CI.~ ~.~_o~ ~It~,/~F 
a~_tOI LO~O KILL= ~0~OPj ~-~/~1 )H~F ~dXl~ 8~9F 
Figure 1 
The analyst selects one or more scenario 
template, to activate in the query. Each 
scenario template corresponds to a specific 
type of event. Available scenario templates 
might include troop movements, acts of 
violence, meetings and negotiathms, 
protests, etc. In Figure 1, the selected event 
is of type meeting (understood broadly). 
The analyst fills in the available slots of the 
selected scenario template in order to restrict 
the search to the information considered to 
be relevant. In Figure 1, the values specified 
in the scenario template indicate that the 
information to f'md is about meetings having 
as location South Korea and as issue North 
Korea and missiles. The analyst also 
32  
specifies what information s/he wants to be 
reported when information matching the 
query is found. In Figure 1, the selected 
boxes under the Report column indicate that 
all information found satisfying the query 
should be reported except for the meeting 
participants. 1 
? Once the analyst submits the query for 
evaluation, the system searches the input 
documents for information matching the 
query. As a result, a hypertext document is 
generated describing the information 
matching the query as well as the source of 
this information. Note that the query 
contains English keywords that are 
automatically translated into Korean prior to 
matching. The extracted information is 
presented in English after being translated 
from Korean. In Figure 1, the generated 
hypertext response indicates two documents 
in the input set that matched the query 
totally or in part. Each summary in the 
response includes just the translations of the 
extracted information that the analyst 
requested to be reported. 
? For each document extract matching the 
analyst query, the analyst can obtain a 
complete machine translation of the Korean 
document where the match was found, and 
where the matched information is 
highlighted. Working with a human 
translator, the analyst can also verify the 
accuracy of the reported information by 
accessing the documents in their original 
language. 
3 System Design 
Figure 2 shows the high-level design of the 
system. It consists of the following components: 
? The User Interface. The browser-based 
interface is for entering queries and 
displaying the resulting presentations. 
? The Portable Information Extractor (PIE) 
component. The PIE component uses the 
While in this example the exclusion of participant 
information in the resulting report is rather artificial, 
in general a scenario template may contain many 
different ypes of information, not all of which are 
likely to interest an analyst at once. 
Extraction Pattem Library - -  which 
contains the set of extraction patterns 
learned in the lab, one set per scenario 
template - -  to extract specific types of 
information from the input Korean 
documents, once parsed. 
? The Ranker component. This component 
ranks the extracted information returned by 
the PIE component according to how well it 
matches the keyword restrictions in the 
query. The MT component's English-to- 
Korean Transfer Lexicon is used to map the 
English keywords to corresponding Korean 
ones. When the match falls below a user- 
? configurable threshold, the extracted 
information is filtered out. 
? The MT component. The MT component 
(cf. Lavoie et al, 2000) translates the 
extracted Korean phrases or sentences into 
corresponding English ones. 
? The Presentation Generator component. 
This component generates well-organized, 
easy-to-read hypertext presentations by 
organizing and formatting the ranked 
extracted information. It uses existing NLG 
components, including the Exemplars text 
planning framework (White and Caldwell, 
1998) and the RealPro syntactic realizer 
(Lavoie and Rainbow, 1997). 
In our feasibility study, the majority of the effort 
went towards developing the PIE component, 
described in the next section. This component 
was implemented in a general way, i.e. in a way 
that we would expect to work beyond the 
specific training/test corpus described below. In 
contrast, we only implemented initial versions of 
the User Interface, Ranker and Presentation 
Generator components, in order to demonstrate 
the system concept; that is, these initial versions 
were only intended.to work with our training/test 
corpus, and will require considerable further 
development prior to reaching operational status. 
For the MT component, we used an early 
version of the lexical transfer-based system 
currently under development in an ongoing 
SBIR Phase II project (cf. Nasr et al, 1997; 
Palmer et al, 1998; Lavoie et al, 2000), though 
with a limited lexicon specifically for translating 
the slot fillers in our training/test corpus. 
33 
Korean Documents 
Parser 
Tagged l 
Korean Documents ( LexiconK?rean 1
~ Syntactic . . . . . .  Eaglish Grammar Structure (English) RealPro 
English Lexicon / ' S~'ntactic Realizer Sentence (English) 
t Parsed Document ~ ::i~i?~'~vii~i? ' .~:Qi~I~:i~-'-iL \[:!::ili:::.:: ~t r~.  :::::::::::::::::::::::: 
Extracted Information \[ 
(Korean) 
Ordered Extracted 
Information(Korean) 
Parsed Document \] Machine "lYanslation I ( 
~l Component (MT) 
Ordered Extracted 
Information (English) 
User Input Data Presentation (E glish) 
Information Extraction 
Query (English) 1 
i : rla0 Inf0rntauonl 
English-Korean 7 
Transfer Lexicon J
Korean-English 
Transfer Lexicon ) 
T 
Miiiiii ii 
? 
Presentation (English) 
End user Document Processing Knowledge base 
component component 
D (C)OTS component 
\[\]Component created in Phase I 
\[\]Component created or improved in Phase II 
Figure 2 
4 Portable Information Extraction 
4.1 Scenario Template and Training/Fest 
Corpus 
For our Phase I feasibility demonstration, we 
chose a minimal scenario template for meeting 
and negotiation events consisting of one or more 
participant slots plus optional date and location 
slots. 2 We then gathered a small corpus of thirty 
articles by searching for articles containing 
"North Korea" and one or more of about 15 
keywords. The first two sentences (with a few 
exceptions) were then annotated with the slots to 
be extracted, leading to a total of 51 sentences 
containing 47 scenario templates and 89 total 
2 In the end, we did not use the 'issue' slot shown in 
Figure 1, as it contained more complex Idlers than 
those that ypically have been handled in IE systems. 
correct slots. Note that in a couple of cases 
more than one template was given for a single 
long sentence. 
When compared to the MUC scenario 
template task, our extraction task was 
considerably simpler, for the following reasons: 
* The answer keys only contained information 
that could be found within a single sentence, 
i.e. the answer keys did not require merging 
information across entences. 
? The answer keys did not require anaphoric 
references to be resolved, and we did not 
deal with conjuncts eparately. 
? We did not attempt o normalize dates or 
remove appositives from NPs. 
4.2 Extraction Pattern Learning 
For our feasibility study, we chose to follow the 
AutoSlog (Lehnert et al, 1992; Riloff, 1993) 
approach to extraction pattern acquisition. In 
this approach, extraction patterns are acquired 
34 
i. E: 
K: 
<target-np>=<subject> <active voice verb> 
<participant> MET 
<target-np>=<subject> <active voice verb> 
<John-i> MANNASSTA 
<John-nom>'MET 
2. E: 
K: 
<target-np>=<subject> <verb> <infinitive> 
<participant> agreed to MEET 
<target-np>=<subject> <verbl-ki- lo> <verb2> 
<John-un> MANNA-ki- lo hapuyhayssta 
<John-nom> MEET-ki- lo agreed 
(-ki: nominalization ending, -io: an adverbial postposition) 
Figure 3 
via a one-shot general-to-specific learning 
algorithm designed specifically for the 
information extraction task. 3 The learning 
algorithm is straightforward and depends only 
on the existence of a (partial) parser and a small 
set of general inguistic patterns that direct the 
creation of specific patterns. As a training 
corpus, it requires a set of texts with noun 
phrases annotated with the slot type to be 
extracted. 
To adapt the AutoSlog approach to Korean, 
we first devised Korean equivalents of the 
English patterns, two of which are shown in 
Figure 3. It turned out that for our corpus, we 
could collapse some of these patterns, though 
some new ones were also needed. In the end we 
used just nine generic patterns. 
Important issues that arose in adapting the 
approach were (1) greater flexibility in word 
order and heavier reliance on morphological 
cues in Korean, and (2) the predominance of 
light verbs (verbs with little semantic ontent of 
their own) and aspectual verbs in the chosen 
domain. We discuss these issues in the next two 
sections. 
4.3 Korean Parser 
We used Yoon's hybrid statistical Korean parser 
(Yoon et al, 1997, 1999; Yoon, 1999) to process 
the input sentences prior to extraction. The 
parser incorporates a POS tagger and 
3 For TIDES, we plan to use more sophisticated 
learning algorithms, as well as active learning 
techniques, such as those described in Thompson et 
al. (1999). 
morphological nalyzer and yields a dependency 
representation as its output? The use of a 
dependency representation e abled us to handle 
the greater flexibility in word order in Korean. 
To facilitate pattern matching, we wrote a 
simple program to convert he parser's output o 
XML form. During the XML conversion, two 
simple heuristics were applied, one to recover 
implicit subjects, and another to correct a 
recurring misanalysis of noun compounds. 
4.4 Trigger Word Filtering and 
Generalization 
In the newswire corpus we looked at, meeting 
events were rarely described with the verb 
'mannata' ('to meet'). Instead, they were 
usually described with a noun that stands for 
'meeting' and a light or aspectual verb, for 
example, 'hoyuy-lul kacta' ('to have a meeting') 
or 'hoyuy-lul machita' ('to finish a meeting'). 
In order to acquire extraction patterns that made 
appropriate use of such collocations, we decided 
to go beyond the AutoSlog approach and 
explicitly group trigger words (such as 'hoyuy') 
into classes, and to likewise group any 
collocations, such as those involving light verbs 
or aspectual verbs. To fmd collocations for the 
trigger words, we reviewed a Korean lexical co- 
occurrence base which was constructed from a 
corpus of 40 million words (Yoon et al, 1997). 
We then used the resulting specification to filter 
the learned patterns to just those containing the 
4 Overall dependency precision is reported to be 
89.4% (Yoon, 1999). 
35 
. - !  
trigger words or trigger word collocations, as 
well as to generalize the patterns to the word 
class level. Because the number of tr:igger 
words is small, this specification can be done 
quickly, and soon pays off in terms of time 
saved in manually filtering the learned patterns. 
4.5 Results 
In testing our approach, we obtained overall 
results of 79% recall and 67% precision in a 
hold-one-out cross validation test. In a cross 
validation test, one repeatedly divides a corpus 
into different raining and test sets, averaging the 
results; in the hold-one-out version, the system 
is tested on a held-out example after being 
trained on the rest. In the IE setting, the recall 
measure is the number of correct slots found 
divided by the total number of correct slots, 
while the precision measure is the number of 
correct slots found divided by the total number 
of slots found. 
While direct comparisons with the MUC 
conference results cannot be made for the 
reasons we gave above, we nevertheless 
consider these results quite promising, as these 
scores exceed the best scores reported at MUC-6 
on the scenario template task. 5 
Table 1: Hold-One-Out Cross Validation 
Slots Recall Precision 
All 79% 67% 
Participant 75% 84% 
Date/Location 86% 54% 
Table2: Hold-One-OutCross Validat~n 
wi~outGeneralizafion 
Slots Recall Precision 
All 61% 64% 
Participant 57% 81% 
Date/Location 67% 52% 
A breakdown by slot is shown in Table 1. We 
may note that precision is low for date and 
location slots because we used a simplistic 
sentence-level merge, rather than dependencies. 
To measure the impact of our approach to 
generalization, we may compare the results in 
5 
http://www.nist.gov/itl/div894/894.02/related_project 
s/tipster/muc.htm 
Table 1 with those shown in Table 2, where 
generalization is not used. As can be seen, the 
generalization step adds substantially to overall 
recall. 
To illustrate the effect of generalization, 
consider the pattern to extract he subject NP of 
the light verb 'kac (hold)' when paired with an 
object NP headed by the noun 'hyepsang 
(negotiation)'. Since this pattern only occurs 
once in our corpus, the slot is not successfully 
extracted in the cross-validation test without 
generalization. However, since this example 
does fall under the more generalized pattern of 
extracting the subject NP of a verb in the light 
verb class when paired with an object NP 
headed by a noun the 'hoytam-hyepsang' class, 
the slot is successfully extracted in the cross- 
validation test using the generalized patterns. 
Cases like these are the source of the 18% boost 
in recall of participant slots, from 57% to 75%. 
5 Discussion 
Our feasibility study has focused our attention 
on several questions concerning the interaction 
of IE and MT, which we hope to pursue under 
the DARPA TIDES initiative. One question is 
the extent o which slot filler translation is more 
practicable than general-purpose MT; one would 
expect to achieve much higher quality on slot 
fillers, as they are typically relatively brief noun 
phrases, and instantiation of a slot implies a 
degree of semantic lassification. On the other 
hand, one might find that higher quality is 
required in order to take translated phrases out 
of their original context. Another question is 
how to automate the construction of bilingual 
lexicons. An important issue here will be how 
to combine information from different sources, 
given that automatically acquired lexical 
information is apt to be less reliable, though 
domain-specific. 
Acknowledgements 
Our thanks go to Richard Kittredge and Tanya 
Korelsky for helpful comments and advice. This 
work was supported by ARL contract DAAD 17- 
99-C-0005. 
36 
References 
Cardie, C. (1997). Empirical Methods in Information 
Extraction. AI Magazine 18(4):65-79. 
Lavoie, B. and Rambow, O. (1997). RealPro - -  A 
fast, portable sentence realizer. In Proceedings of 
the Conference on Applied Natural Language 
Processing (ANLP'97), Washington, DC. 
Lavoie, B., Korelsky, T., and Rambow, O. (2000). A 
Framework for MT and Multilingual NLG Systems 
Based on Uniform Lexico-Structural Processing. 
To appear in Proceedings of the Sixth Conference 
on Applied Natural Language Processing (ANLP- 
2000), Seattle, WA. 
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., 
Riloff, E., and Soderland, S. (1992). University of 
Massachusetts: Description of the CIRCUS system 
as used in MUC-4. In Proceedings of the Fourth 
Message Understanding Conference (MUC-4), 
pages 282-288, San Mateo, CA. Morgan 
Kaufmann. 
MUC-5 (1994). Proceedings of the Fifth Message 
Understanding Conference (MUC-5). Morgan 
Kaufmann, San Mateo, CA. 
MUC-7 (1998). Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Francisco, CA. 
Nasr, A., Rambow, O., Palmer, M., and Rosenzweig, 
J. (1997). Enriching lexical transfer with cross- 
linguistic semantic features. In Proceedings of the 
lnterlingua Workshop at the MT Summit, San 
Diego, CA. 
Palmer, M., Rambow, O., and Nasr, A. (1998). 
Rapid prototyping of domain-specific machine 
translation systems. In Machine Translation and 
the Information Soup - Proceedings of the Third 
Conference of the Association for Machine 
Translation in the Americas AMTA'98, Springer 
Verlag (Lecture Notes in Artificial Intelligence No. 
1529), Berlin. 
Riloff, E. (1993). Automatically constructing a
dictionary for information exlxaction tasks. In 
Proceedings of the Eleventh National Conference 
on Artificial Intelligence, pages 811-816, 
Washington, DC. AAAI Press / MIT Press. 
Thompson, C. A., Califf, M. E., and Mooney, R. J. 
(1999). Active learning for natural language 
parsing and information extraction. In Proceedings 
of the Sixteenth International Machine Learning 
Conference (1CML-99), Bled, Slovenia. 
White, M. and Caldwell, T. (1998). EXEMPLARS: A 
practical, extensible framework for dynamic text 
generation. In Proceedings of the 8th International 
Workshop on Natural Language Generation, 
Niagara-on-the-Lake, Ontario. 
Yoon, J. (1999). Efficient dependency parsing based 
on three types of chunking and lexical association. 
Submitted. 
Yoon, J., Choi, K.-S., and Song, M. (1999). Three 
types of chunking in Korean and dependency 
analysis based on lexical association. In 
Proceedings of lCCPOL. 
Yoon, J., Kim, S., and Song, M. (1997). New parsing 
method using global association table. In 
Proceedings of the 5th International Workshop on 
Parsing Technology. 
37 
St ructura l  Feature  Se lect ion  For Eng l i sh -Korean  Stat is t ica l  
Mach ine  Trans la t ion  
Seonho Kim, Juntae Yoon, Mansuk Song 
{ pobi, j tyoon, mssong} @decemb er.yonsei, ac.kr 
\ ] )ept .  of  Computer  Science, 
Yonsci  Univers i ty ,  Seoul,  Korea  
Abstract 
When aligning texts in very different languages such 
as Korean and English, structural features beyond 
word or phrase give useful intbrmation. In this pa- 
per, we present a method for selecting struetm'al 
features of two languages, from which we construct 
a model that assigns the conditional probabilities 
to corresponding tag sequences in bilingual English- 
Korean corpora. For tag sequence mapl)ing 1)etween 
two langauges, we first, define a structural feature 
fllnction which represents tatistical prol)erties of 
elnpirical distribution of a set of training samples. 
The system, based on maximmn entrol)y coneet)t, se- 
le(:ts only ti;atures that pro(luee high increases in log- 
likelihood of training salnl)les. These structurally 
mat)ped features are more informative knowledge for 
statistical machine translation t)etween English and 
Korean. Also, the inforum.tion can help to reduce the 
1)arameter sl)ace of statisti('al alignment 1)y eliminat- 
ing synta(:tically uiflikely alignmenls. 
1 Introduction 
Aligned texts have been used for derivation of 1)ilin- 
gual dictioimries and terminoh)gy databases which 
are useflfl for nlachine translation and cross lan- 
guages infornmtion retriewfl. Thus, a lot of align- 
ment techniques have been suggested at; the sen- 
tence (Gale et al, 1993), phrase (Shin et al, 1996), 
nomt t)hrase (Kupiec, 1993), word (Brown et al, 
1993; Berger et al, 1996; Melamed, 1997), collo- 
cation (Smadja et al, 1996) and terminology level. 
Seine work has used lexical association measures 
for word alignments. However, the association mea- 
sures could be misled since a word in a source lan- 
guage frequently co-occurs with more titan one word 
in a target language. In other work, iterative re- 
estimation techniques have beets emt)loyed. They 
were usually incorporated with the EM algorithm 
mid dynmnic progranmfing. In that case, the prob- 
al)ilities of aligmnents usually served as 1)arameters 
in a model of statistical machine translation. 
In statistical machine translation, IBM 1~5 mod- 
els (Brown et al, 1993) based on the source-chmmel 
model have been widely used and revised for many 
language donmins and applications. It has also 
shortconfing that it needs much iteration time for 
parameter estimation and high decoding complex- 
ity, however. 
Much work has been done to overcome the prob- 
lem. Wu (1996) adopted chammls that eliminate 
syntactically unlikely alignments and Wang et al 
(1998) presented a model based on structures of two 
la.nguages. Tilhnann et al (1997) suggested the 
dynanfie programming lmsed search to select the 
best alignment and preprocessed bilingual texts to 
remove word order differences. Sate et al (1998) 
and Och et al (1998) proposed a model for learn- 
ing translation rules with morphological information 
mid word category in order to improve statistical 
translation. 
Furthemlore, llla,lly researches assullle(t Olle-to- 
one correspondence due to the coml)lexity and com- 
Imtati(m time of statistical aliglunents. Although 
this assumption Ire'ned out t;o 1)e useful for align- 
ment of close lallguages uch as English and French, 
it is not, applicabh~ to very different languages, in 
particular, Korean and English where there is rarely 
(:lose corresl)ondence in order at the word level. For 
such languages, even phrase level alignment, not to 
mei~tion word aligmnent, does not gives good trans- 
lation due to structural diflbrence. Itence, structural 
features beyond word or t)hrase should t)e consid- 
ered to get t)etter translation 1)etween English and 
Koreml. In addition, the construction of structural 
bilingual texts would be more informative for ex- 
tracting linguistic knowledge. 
In this paper, we suggest a method for structural 
mat)t)ing of bilingual language on the basis of the 
maximum entorl)y and feature induction fl'alnework. 
Our model based on POS tag sequence mapl)ing has 
two advantages: First;, it can reduce a lot of 1)armne- 
ters in statistical machilm translation by eliminating 
syntactically unlikely aligmnents. Second, it: can be 
used as a t)reprocessor for lexical alignments of bilin- 
gual corpora although it; (:an be also exl)loited 1)y it- 
self tbr alignment. In this case, it would serve as the 
first stet) of alignment for reducing the 1)arameter 
sI)ace. 
439 
2 Mot ivat ion  
In order to devise parameters for statistical model- 
ing of translation, we started our research from the 
IBM model which has bee:: widely used by :nany 
researches. The IBM model is represented with the 
formula shown in (1) 
l 17t 
v(f, al ) = I I  I-I t(fJ l%)d(jlaj,m, l) 
i=1 j= l  
(1) 
Here, n is the fertility probability that an English 
word generates n h'end:  words, t is tim aligmnent 
probability that the English word c generates the 
French word f ,  and d is the distortion probability 
that an English word in a certain t)osition will gener- 
ate a lh'ench word in a certain 1)osition. This formula 
is Olm of many ways in which p(f, ale ) can tie writtm. 
as the product of a series of conditional prot)at)ilities. 
In above model, the distortion probability is re-- 
lated with positional preference(word order). Since 
Korean is a free order language, the probability is 
not t~asible in English-Korean translation. 
Furthermore, the difference between two lan- 
guages leads to the discordance between words that 
the one-to-one correst)ondence b tween words gen- 
erally does not keel). The n:odel (1), however, as-- 
sumed that an English word cat: be connected with 
multiple French words, but that each French word 
is connected to exactly one English word inch:ding 
the empty word. hl conclusion, many-to-:nany :nap-- 
pings are not allowed in this model. 
According to our ext)eri:nent, inany-to-nmny 
mappings exceed 40% in English and Korean lexical 
aligninents. Only 25.1% of then: can be explained 
by word for word correspondences. It means that we 
need a statistical model which can lmndle phrasal 
mat) pings. 
In the case of the phrasal mappings, a lot of pa- 
rameters hould be searched eve:: if we restrict the 
length of word strings. Moreover, in order to prop-- 
erly estimate t)arameters we need much larger voI-- 
ume of bilingual aligned text than it in word-for- 
word modeling. Even though such a large corpora 
exist sometimes, they do not come up with the lex-- 
ical alignments. 
For this problem, we here consider syntactic fea- 
tures which are importmlt in determining structures. 
A structural feature means here a mapt)ing between 
tag sequences in bilingual parallel sentences. 
If we are concerned with tag sequence alignments, 
it is possible to estimate statistical t)armneters in 
a relatively small size of corpora. As a result, we 
can remarkably reduce the problem space for possi- 
ble lexical alignments, a sort of t probability in (1), 
which improve the complexity of a statistical ma- 
chine translation model. 
If there are similarities between corresponding tag 
sequences in two language, tile structural features 
would be easily computed or recognized. However, 
a tag sequence in English can be often translated 
into a completely different tag sequence in Korean 
as follows. 
can/MD -+ ~-, cul/ENTR1 su/NNDE1 'iss/ AJMA 
da/ENTE 
It nmans that similarities of tag features between two 
languages are not; kept all the time and it is neces- 
saw to get the most likely tag sequence mappings 
that reflect structural correspondences between two 
languages. 
In this paper, the tag sequence mappings are ob- 
taind by automatic feature selection based on the 
maximum entropy model. 
3 Prob lem Set t ing  
In tiffs ctlat)ter, we describe how the features are 
related to the training data. Let tc be an English 
tag sequence and tk be a Korean tag sequence. Let 
Ts be the set of all possible tag sequence niapI)ings in 
a aligned sentence, S. We define a feature function 
(or a feature) as follows: 
1 pair(t~,tk) C "\]-s 
f(t~,tk) = 0 othcrwi.s'c 
It indicates co-occurrence information l)etween 
tags appeared in Ts. f(t?,tk) expresses the infor- 
mation for predicting that te maps into ta.. A fea- 
ture means a sort of inforination for predicting some- 
thing. In our model, co-occurrence information on 
the same aligned sentence is used for a feature, while 
context is used as a feature in Inost of systems using 
maximum entropy. It can be less informative than 
context. Hence, we considered an initial supervision 
and feature selection. 
Our model starts with initial seed(active) features 
for mapI)ing extracted by SUl)ervision. In the next 
step, thature pool is constructed from training sam- 
ples fro:n filtering and oifly features with a large gain 
to the model are added into active feature set. The 
final outputs of our model are the set of active t'ea- 
tures, their gain values, and conditional probabilities 
of features which maximize the model. Tim results 
can be embedded in parameters of statistical ma- 
chine translation and hell) to construct structural 
bilingual text. 
Most alignment algorithm consists of two steps: 
(1) estimate translation probabilities. 
(2) use these probabilities to search for most t)roba- 
ble alignment path. 
Our study is focused on (1), especially the part of 
tag string alignments. 
Next, we will explain the concept of the model. 
We are concerned with an ot)timal statistical inodel 
which can generate the traiifing samples. Nmnely, 
our task is to construct a stochastic model that pro- 
440 
(1) duces outl)ut tag sequenc0, "~k, given a tag sequence ~+~,-~. 
To The l)roblem of interest is to use Salnt/les of ? --J~\,What .... 
tagged sentences to observe the/)charier of the ran- u~, ,~ 
(loin t)roeess. 'rile model p estinmtes tile conditional tt'2,Y 
probability that tile process will outlmt t,~, given t~.. ~ ,~/~ ,o~!! 
It is chosen out of a set of all allowed probability o~,~ ~ 
e}~..?0,, me (tistributions . . . .  
The fbllowing steps are emt)loyed for ()tit" model, v~ / 
Input: a set L of POS-labeled bilingual aligned 
sentences. 
I. Make a set ~: of corresl)ondence pairs of tag 
sequences, (t~, tk) from a small portion of L by 
supervision. 
2. Set 2F into a set of active features, A. 
3. Maximization of 1)arameters, A of at:tire fea- 
tures 1)y I IS(hnproved Iterative Sealing) algo- 
rithm. 
4. Create a feature pool set ?9 of all possible align- 
nmnts a(t(,, tk) from tag seqllellces of samples. 
5. Filter 7 ) using frequency and sintilarity with M. 
6. Coml)ute the atit)roximate gains of tkmtm:es in 
"p. 
7. Select new features(A/') with a large gain vahle, 
and add A. 
Outt)ut: p(tklt~,)whcrc(t(,, t~.) C M and their Ai. 
We I)egan with training samples comi)osed of 
English-Korean aligned sentence t)airs, (e,k). Since 
they included long sentences, w(', 1)roke them into 
shorter ones. The length of training senl;en(:es was 
limited to ml(h',r 14 on the basis of English. It is 
reasona,bh; \])(',(:&llSe we are interested in not lexical 
alignments lint tag sequence aliglmients. The sam- 
ples were tagged using brill's tagger and qVIorany' 
that we iml)lenmnted as a Korean tagger. Figure \] 
shows the POS tags we considered. For simplicity, 
we adjusted some part of Brill's tag set. 
In the, sut)ervision step, 700 aligned sentences were 
used to construct he tag sequences mal)I)ings wlfich 
are referred to as an active feature set A. As Fig- 
ure 2 shows, there are several ways in constructing 
the corresl)ondem;es. We chose the third mapping 
although (1) can be more useflll to explain Korean 
with I)redieate-argunmnt structure. Since a subject 
of a English sentence is always used for a subject 
tbrln in Korean, we exlcuded a subject case fi'onl ar- 
gulnents of a l/redicate. For examl)le, 'they' is only 
used for a subject form, whereas 'me' is used for a 
object form and a dative form. 
II1 tile next step, training events, (t,:, It.) are con- 
structed to make a feature 1)eel froln training sam- 
pies. The event consists of a tag string t,, of a English 
(2) (31 
? ~ lm-  
~" - -  + ~ '~Whatever  
Figure 2: Tag sequence corresl)ondences at the 
phrase level 
1)OS-tagged sentence and a tag string tL~ of the cor- 
responding Korean POS-tagged sentence and it Call 
be represented with indicator functions fi(t~, tk). 
For a given sequence, the features were drawn 
fl'om all adjacent i)ossible I)airs and sonic interrupted 
pairs. Only features (tci, tfii ) out of the feature pool 
that meet the following conditions are extracted. 
? #(l, ei,t~:i) _> 3, # is count 
? there exist H:.~,, where (t(,i,tt.~.) in A and the 
similarity(sanle tag; colin|;) of lki an(1 tkx _> 0.6 
Table \] shows possible tL'atures, for a given aligned 
sentence , 'take her out - g'mdCOrcul baggcuro 
dcrfleoflara'. 
Since the set of the structural ti;atm'es for align- 
ment modeling is vast, we constructed a maximum 
entrol)y model for p(tkltc) by the iterative model 
growing method. 
4 Maximum Ent ropy  
To explain our method, we l)riefly des(:ribe the con- 
(:ept of maximum entrol)y. Recently, many al)- 
lnoaches l)ased on the maximum entroi)y lnodel have 
t)een applied to natural anguage processing (Berger 
eL al., \]994; Berger et al, 1996; Pietra et al, 1997). 
Suppose a model p which assigns a probability to 
a random variable. If we don't have rely knowledge, 
a reasonal)le solution for p is the most unifbrnl dis- 
tribution. As some knowledge to estilnate the model 
p are added, tile solution st)ace of p are more con- 
strained and the model would lie close to the ol\]timal 
probability model. 
For the t)url/ose of getting tile optimal 1)robability 
model, we need to maxi\]nize the unifl)rnlity under 
some constraints we have. ltere, the constraints are 
related with features. A feature, fi is usually rel/re - 
sented with a binary indicator funct, ion. The inlpor- 
tahoe of a feature, fi can be identified by requiring 
that the model accords with it. 
As a (:onstraint, the expected vahle of fi with re- 
spect to tile model P(fi) is supposed to be the same 
as tile exl)ected value of fi with respect o empiri(:al 
distril)ution in training saml)les, P(fi). 
441 
TAG 
cb 
DT 
PW 
JJ 
JJS 
MD 
NNP 
PDT 
PRP 
RB 
RBS 
SYM 
UH 
VBD 
VBN 
WP$ 
NOT 
BED 
BEG 
HVD 
DOD 
DESCRIPTION 
comma 
conjunction,coordinating 
determiner 
foreign word 
adjective, ordinal 
adjective, superlative 
modal auxiliary 
noun, proper, singular 
pre-determiner 
pronoun, personal 
adverb 
adverb, superlative 
symbol 
interjection 
verb, past tense 
verb, past participle 
WH-pronoun, possessive 
not 
be verb, past tense 
be verb, present participle 
have verb, past participle 
do verb, past tense 
TAG 
CD 
EX 
IN 
J JR 
LS 
NN 
NNPS 
POS 
PRP$ 
RBR 
RP 
TO 
VBP 
VBG 
WDT 
WR8 
BEP 
BEN 
HVP 
DOP 
DON 
DESCRIPTION 
sentence terminator TAG 
numeral, cardinal 
existential there NNIN1 
preposition, subordinating NNIN2 
adjective, comparative NNDE1 NNDE2 list item marker PN 
noun, common NU 
noun, proper, plural VBMA 
genitive marker AJMA 
pronoun, possessive CO 
adverb, comparative AX 
particle ADCO 
to or infinitive marker APSE 
verb, present tense CJ 
verb, present participle ANCO 
WH-determiner ANDE 
WH-adverb ANNU 
be verb. present tense EX 
be verb, past participle LQ 
have verb, present tense RQ 
do verb, present tense SY 
do verb, past participle 
POS 
proper noun 
common noun 
common-dependent noun 
unit-dependent noun 
pronoun 
number 
verb 
adjective 
copula 
auxiliary verb 
constituent adverb 
sentential adverb 
conjunctive adverb 
configurative adnominal 
demonstrative adnominal 
numeral adnominal 
exclamination 
left quotation mark 
right quotation mark 
symbols 
TAG 
PPCA1 
PPCA2 
PPCA3 
PPCA4 
PPAD 
PPCJ 
PPAU 
ENTE 
ENCO1 
ENCO2 
ENCO3 
ENTRt 
ENTR2 
ENTR3 
ENCM 
PE 
SF 
PF 
CM 
SO 
Figure 1: English Tags (left) and Korean Tags (right) 
POS 
nominative postposition 
accusative postposition 
possessive postposition 
vocative postposition 
adverbial postposition 
conjunctive postposition 
auxiliary postposition 
final ending 
coordinate ending 
subordinate ending 
auxiliary ending 
adnominal ending 
nominal ending 
adverbial ending 
ending+postposition 
pre-ending 
suffix 
prefix 
comma 
termination 
~P~II'~ l lq  l l l l  l l~l '~l~t I l l i i LU i I f ( i |  I I  IL'II|II~ ;'~ ;~ l i l | ' t l l l  I I I l l  I LI~M 
\[VBI'+IN\] [take+out\] \[1+3\] 
\[wp\] \[tak(q \[t\] 
\[VBP+PI~P\] \[take+her\] \[1+2\] 
\[W3P+PRP+IN\] \[take+her+out\] \[1+2+3\] 
\[PRP\] [ho,-\] [2\] 
\[IN\] \[out\] \[3\] 
\[I'PCA2+P1)AD-FVBMA\] \[rcul+euro+deryeoga\] \[2+4+5\] 
\[PN\] b.... :/~o\] i l l  
\[PI)AI)+VBIvlA-FENTE\] \[reu/+cure-I- dcrycoga+ra\] \[4+5+6\] 
\[NNIN2\] [bagg \] \[3\] 
\[NNIN2+PPAD\] \[bagg+euro\] \[3+41 
\[ENTE\] \[ra\] [6\] 
\[P1)AD+VBMA\] \[curo+deryeoga\] \[4+5\] 
\[PPAD+VBMA+ENTE\] [euro+deryeoga+ra\] \[4+5+6\] 
\[PPCA2+NNIN2+PPAD+VBMA\] \[reul+bagg+curo+ deryeoga \] \[2+3+4+5\] 
\[PPCA2+NNIN2+PPAI)+VBMA+ENTE\] \[reul+bagg+euro+dcryeoga+ra \] \[2+3+4+5+6\] 
\[P1)CA2+NNIN2+PPAD+VBMA\] \[renl+deryeoga \] \[2+3+4+5\] 
\[PPCA2+NNIN2+Pt)AD+VBMA+ENTE\] \[reul+deryeoga+ra\] \[2+3+4+5+6\] 
Table 1: possible tag sequences 
In sun1, the maxilnunl entropy fralnework finds 
the model which has highest entropy(most uniform)~ 
given constraints. It is related to the constrained 
optimization. To select a model from a constrained 
set, C of allowed l)rol)ability distributions, the model 
p, C C with maximum entropy H(p) is chosen. 
In general, for the constrained optimization prob- 
lem, Lagrange inultipliers of the number of features 
can be used. However, it was proved that the model 
with maximum entropy is equivalent o the model 
that maximizes the log likelihood of the training 
samples like (2) if we can assume it as an exponential 
model. 
hi (2), the left side is Lagrangian of the condi-. 
tional entropy and the right side is inaxilnlHn log-. 
likelihood. We use the right side equation of (2) to 
select I. for the best model p,. 
~,g,,~..~,(- ~.,,. ~(~)v(yl~)logv(vlx)+~,,(v(f,)-~(/,))) (2) 
:a,'9,,,ax~, .,,. ~(x,v)lo.~n,(ylx) 
Since t ,  cannot be tbund analytically, we use 
the tbllowing improved iterative scaling algorithm to 
colnpute I ,  of n active features in .4 in total sam-- 
ples. 
1. Start with l i  = 0 for all i 6 {1 ,2 , . . . ,n}  
2. Do for ca.oh i ~ { \ ] ,2 , . . . ,n}  : 
(a) Let AAi be the solution to the log likeli- 
hood 
(b) Update the value of Ai into l i  + A,h, 
~. ..... ~(.,,v)A(:~,v) 
where AAi = log ~, : ,  ~i.~)v~(?11.~)/~(.~,v) 
px(yl:r) = ~-A--e(~ x'f'("':')) zx (:~.) ' , 
z (x) = E:, c(E ,  
3. Stop if not all the Ai have converged, otherwise 
go to step 2 
Tile exponential model is represented as(3). Here, 
l i  is the weight of feature f i .  In ore" model, since 
only one feature is applied to each pair of x and y, 
it can be represented as (4) and fi is the feature 
related with x and y. 
~(ylx) = ~ i  C'f'(x'Y) (3) 
cAifi(x,Y) 
= (4) 
442 
5 Feature  select ion 
Only a small subset of features will 1)e emph)yed in 
a model by sele(:ting useflfl feal;m'es from (;tie flmture 
1)ool 7 ). Let 1).,4 lie (;tie optimal mo(lel constrained 
by a set of active features M and A U J'i 1)e ,/lfi. Le(; 
PAf~ be the ot)timal model in the space of l)rol)abil- 
ity distribution C(Af i ) .  The optimal model can be 
tel)resented as (5). Here, the optimal model means 
a maxilmnn entropy nlodd. 
1 
v~ :, = z,,.(:,;) p'~ (:11,)::' ("'") 
zo,(: .)  = ~ v.~(::l:,,)c"S'(*'"> (5) 
Y 
The imi)rovement of l;he model regarding the ad- 
dition of a single feature f i  can be estiumted by mea- 
suring the difference of maximmn log-likelihood be- 
tween L(pAf~) and L(pA). We denote the gain of 
t~ature f i t i y  A(~lfi) an(l it can be r(!t/resented in
(G). 
A(A . I 'd  - .,..,:,;,,cAI~(.) 
('A:,(,,,) = J~(>t:,)-- L(v,O 
= _ ~(:~)~, .~( : , / I , . ) :  :'('''') 
x y 
?'~P(.fi) IS) 
Note that a model PA has a, set of t)arameters A 
which means weights of teatures. The m(idel P.Afl 
contains the l )a ra . lnetc . rs  an( I  the  new \[)a.l'a, lllCi;('~r (11 
with l'eSl)ect () the t'eal;ure fi. W'hen adding a new 
feature to A, the optimal values (if all parame(ers of 
probability (listril)u(,ion change. To make th(; (:om- 
i)utation of feature selection tractal)le, we al)l)roxi- 
mate that the addition of a feature f i  affec(;s only 
the single 1)aranxeter a, as shown in (5). 
qShe following a.lgoritlnn is used for (;omputing the 
gain of the model with rest)ect o fi. We referred 
to the studies of (Berger et al, 1996; Pietra e.t al., 
1997). We skip tile detailed contents and 1)root~. 
1. Let 
1 i f  P(fi) <_ PA(J;) 
r = -1  oth, erwise 
2. Set a0 = 0 
3. Repeat the following until GAf f (%, )  has con- 
verged : 
Co i l l l ) l l te  0@1,+ i frOll l  og n l lS i l lg  
a log (1 ! -~6t:' ( ' "~)  ~ ctn+l = (xn + 7" ,. (;~:~(~,,): 
Compute GaV~ (a,~+l) using 
GAA (a)  = - Ea,/3(a,') log Z,,(:,:) + ctf)(fi) , 
c'A:, (,~) = ~(k)  - Ex  ~(~0M( : , . ) ,  
G"  :ct~ A:,, , = -- E.~ P(")V2i:, ((fi -- M(;,;))" la;) 
set description # of disjoint total 
features cvtults 
A active feat m'es 1483 4113 
P feature Calldidat, es 3172 63773 
N new f'eaLures 97 5503 
Table 2: Summery of Features Selected 
where  (:~ ~ (l~n+ 1
A f~ = A u f~,  
M(z)  - p~f~ (fila-) , 
PP4S, (fi l ':) --- E .  ~'~s, (:,?l:r)k(:., ~J) 
d. Set ~ AL(Af i )  <-- GAS,(ct.) 
This algorittun is iteratively comtmted using Net- 
Wel l 'S  method. \?e cmt recognize the iml)ortance of a 
fl;ature with the gain value. As mentioned above, it 
means how much the feature accords with the model. 
We viewed the feature as tile information that Q. and 
t, occur together. 
6 Exper imenta l  resu l t s  
The total saml)les consists of 3,000 aligned Sellteiice 
pairs of English-Korean, which were extracted from 
news on the web site of 'Korea Times' and a. maga- 
zine fl)r English learning. 
In the initial step, we manually constucted (;tie 
correspondences of tag sequences with 700 POS- 
tagged sentence I)airs. hi the SUl)ervision step, 
we extracte(t l.,d83 correct tag sequence corresl)on- 
it(miles its shown in Table 2, and it work as active 
features. As a feature I)OOI, 3,172 (lisjoint %a(;ures 
of tag sequence ma.I)pings were retrieved. 1% is very 
important o make atomic thatures. 
We maxinfized A of active features with resl)ect 
to total smnples using improved the iterative scal- 
ing algoritlun. Figure 3 shows Ai of each feature 
.f(Q31,:P+.m,ttO C A. There a.re nlany corresl)on- 
dence 1)atterns with resl)ect o the Englsh tag string, 
'BEP+J J ' .  
Note that p(tt~lQ) is comtmted by the exponential 
model of (4) mid the conditional probability is the 
saine with empirical probal)ility in (7). Since the 
wflue of p(ylx)  shows the maxinmm likelihood, it is 
proved that each A was converged correctly.  
# of  (.% y) occurs in sam, pie 
P(ylx) - n, um, ber o f  t imes  o f  a: (7) 
hi feature selection step, we chose useflll fea- 
tures with the gain threshold of 0.008. Figure 
4 shows some feaures with a large gain. Anion\ 
then1, tag sequences mapping including 'RB'  are er- 
roneous.  It means that position of adverb in Ko- 
rean is very compl icated  to handle. Also, proper 
noun in English aligned coInmon nouns in Korean 
443 
English 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
Feature(x,y) 
Korean 
VBMA+ENCO3+AX+ENTE 
VBMA 
AJ MA 
AJMA+ENTE 
VBMA+ENTE 
NNIN2+CO 
NNIN2+CO+VBMA 
NNIN2+PPCA1 +VBMA+ENTE 
NNIN2+CO+ENTE 
NNIN2+PPCA2+AX+ENTE 
NNIN2+PPCA1 +VBMA 
1 
10.1369 
8.8520 
8.6787 
8.2628 
7.2379 
7.1372 
6.9909 
8.8402 
6.8308 
6.4256 
6.4250 
Figure 3: A 
\[PRP\] you ~/'\[PN\] cJ~ (dangsin) 
"-\[PPAU\] ~-(eun) 
\[RB\] usually \[ADCO\] EttX41_~(dachero) 
HVP\] have, /\[NNIN2\] ~uf~(ilbanseok) 
TO\] to /~ \[PPAD\] 0tl(e) 
\[VBP\] take ; / \ [VBMA\ ]  N(anj) 
,\[ENCO3\] O~OIE~(ayaman) 
\[J J\] regular . . . . . .  ':-:- \[AX\] "$F(ha) 
\[NN\] seating/' \[ENTE\] L E}(nda) 
Figure 5: Best Lexical alignment 
because of tagging errors. Note that in the case of 
'PN+PPCA2+PPAD+VBMA' ,  it is not an adjacent 
string but an interrupted string. It means ttlat a 
verb in English generally map to a verb taking as 
argument the accusative and adverbial postposition 
in Korean. 
One way of testing usefulness of our method is 
to construct structured aligned bil ingual sentences. 
Table 3 shows lexical al ignments using tag sequence 
al ignments drawn from our algorithm for a given 
sentence, 'you usually have to take regular seating 
- dangsineun dachcw ilbanscokc anjayaman handa' 
and Figure 5 shows the best lexical alignment of the 
sentence. 
We conducted the exi)eriment on 100 sentences 
composed of words in length 14 or less and siln- 
lilY chose the most likely paths. As tim result, the 
accuray was about 71.1~. It shows that we can 
partly use the tag sequence alignments for lexical 
alignments. We will extend tlle structural mapping 
model with consideration to the lexical information. 
The parmneters, the conditional probabilities about 
stuctural mappings will be embedded in a statisti- 
cal model. Table 4 shows conditional probabilities 
of seine features according to 'DT+NN'.  In general, 
determiner is translated into NULL or adnominal 
word in Korean. 
7 Conclus ion 
When aligning Englist>Korean sentences, the differ- 
ences of word order and word unit require structural 
information. For tiffs reason, we tried structural tag 
c(x,y) 
162.0C 
45.00 
39.00 
25.00 
9.00 
8.00 
7.00 
6.00 
6.00 
4.00 
4.00 
P(Ylx) 
0.4247 
0.1180 
0.0996 
0.0655 
0.0236 
0.0210 
0.0183 
0.0157 
0.0157 
0.0105 
0.0105 
Example 
English 
are+prepared 
are+careful 
am+healthy 
is+new 
am+sure 
am+rich 
is+selfish 
is+patriotic 
is+reasonable 
is+reprehensible 
is+helpful 
Korean 
~kllKl+0~+?~+~ LI El- 
0-94 8i 
~ J  8t+ ~ LI C\[ 
~X\[+01 
0191~+01+~LIEt 
011~;Xt+9\[+~+EF 
'NPJ ~ +01 +g~ 
.~N+01 +.El 
of active features in A 
t~ 
I)T-t-NN 
I)Tq-NN 
DT-FNN 
DT+NN 
DT-t-NN 
DT-FNN 
I)T-FNN 
etc 
t~ 
NNIN2 
ANI)E+NNIN2 
ANNUWNNDE2 
NNIN2+PPCA1 
NNIN2+NNIN2 
NNIN2-FPPAU 
ADCO 
eI;c 
p(t~l*~) 
0.524131 
0.15161 
0.091036 
0.063515 
0.058322 
0.05768 
0.049622 
Table 4: Conditional Probability 
string mapping using maximum entropy modeling 
and feature selection concept. We devised a nlodel 
that generates a English tag string given a Korean 
tag string. From initial active structural features, 
useful features are extended by feature selection. 
Tile retrieved features and parameters can be em- 
bedded in statistical maclfine translation and reduce 
the complexity of searching. We showed that they 
can helpful to construct structured aligned bilingual 
sentences. 
References  
Adam L. Berger, Peter F. Brown, Stephen A. 
Della Pietra, Vincent J. Della Pietra, John R. 
Gillett, John D. Lafferty, Robert L. Mercer, Harry 
Printz, and Lubos Ures. 1994. The Calldie sys- 
tem for machine translation, hi Proceedings of the 
ARPA Conference on Human Language Technol- 
ogy, Plainsborough, New Jersey. 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent J. Della Pietra. 1996. A maximum entropy 
approacll to natural anguage processing. Compu- 
tational Linguistics, 22(1):39-73. 
Peter F. Brown, John Cocke, Stephen A. Della 
Pietra, Vincent J. Della Pietra, Fredrick Jelinek, 
John D. Lafferty, Robert L. Mercer, and Paul S. 
Roossin. 1990. A statistical approach to nlachine 
translation. Computational Linguistics, 16(2):79- 
85 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The math- 
enlatics of statistical machine translation: pa- 
444 
Feature(x,y) 
X 
VBP+PRP+TO 
BEP+RBR+IN 
DT4-CD 
JJ+IN 
VBG+TO 
BEP 
BEP 
NNP 
NNP 
TO+PRP 
TO+PRP 
MD+FIB 
MD+RB 
MD+BEP 
NNP+NNS 
VBP+TO+VBP 
BED+VSN+IN 
BED+VSN+IN 
Y 
PNYPPCA2+PPAD+VBMA 
PPAD+AJMA 
NU+NNDE2 
PPAD+AJMA+ENTR1 
PPAD+PPCA2+VBMA 
PPCA1 +AJMA 
PPCA1 +AJMA+ENTE 
NNIN2+PPAU 
NNIN2+NNIN2 
PN+PPAD 
PN+PPCA1 
ENTRI+NNDEI+CO 
ENTRI+NNDEI+CO+ENTE 
CO+ENCO2+VBMA 
NNIN2+SF 
PPCA2+VBMA+ENCO2+VBM~ 
PPAD+VBMA+PE+ENTE 
PPAD+VBMA+PE 
r,~; P(ylx) /~ L(Afi) 
9.8687 0.1722 0.0194 
9.6780 0.3265 0.0192 
9.2799 0.2449 0.0190 
9.6343 0.2450 0.0190 
9.9542 0,3269 0.0189 
9.6720 0.2941 0.0188 
9.2724 0.1961 0.0188 
8.7481 0.1225 0.0182 
9.1397 0.1337 0.0180 
9.5634 0.2307 0.0180 
9.2604 0.1730 0.0180 
9.2445 0.1548 0.0177 
9.2564 0.1548 0.0177 
8.5435 0.0934 0.0177 
9.1597 0.1470 0,0176 
8.9928 0.1278 0.0174 
9.1511 0.1704 0.0173 
9.1636 0.1706 0,0173 
English 
send+Nm+to - - 
is+more+than 
the+two 
smarter+than 
serving+to 
is 
are 
IBM 
Harvard 
to+him 
to+her 
? 
? 
should+be 
English+books 
request+to+send 
was+thrown+to 
were+sent+to 
Example 
Korean 
21+ N+-0tF+-~ L\]I 
-h20+~ 
?+N 
~H+~PA+N+L 
~OII3tI+-~+U~ 
-0I+OA 
-01+~+H 
IBM+~- 
3+OIl)II 
~LI +)F 
? 
? 
Ol+OiOI+N 
-N+~Ll l+Kf~+5/N bF 
~011?t1+c3 ~1XI+~+FA 
Figure d: Some fbatm'es with a large gain 
Tag a l ignment  (km( l i t io lml  Lex ica l  aliglulw, nt 
l ) l{P : PN+I )PAU 0.150109 you : dangs in+cun 
lt\]3 : A I )CO 0.142193 usmt l ly  : dachero  
II, B : NNIN2+PI 'A I )  0.038105 usua l ly  : i lbanseol?-l-e 
I IVP+TO : I~N( JO3-bAX+I"NTE 0,982839 have+to  : ayaman+handa 
VBP  : P1)A I )+VBMA 0.05022,l take  : e+an j  
VBP  : VBMA+F,  NCO3+AX+I , ;NTE  0.011110 take  : an jay+aman-Fha+nda 
V I3P  : P1)A I )+VI~MA+ENCO3-}-AX- I -ENT\ ] ,~  0,001851 take  : e -Fan jaya imm+handa 
V I3P -F J J  : NNIN2+PI )A I ) - \ ] -VBMA 0.057657 take- t - regu lar  : i l bansenk+e+,a l l j  
. I . J+NN : NNIN2 0.581791 regu lar+seat ing  : i l l )anseok 
an(, 3: l,exi(:al aligmnents using tag alignments 
rameter estimation. Computational Linguistics, 
19(2):263-311. 
Stanley F. Chert. 1993. Aligning sentences in bilin- 
gual corpora using lexical information. In l'rocccd- 
ings of ACL ,71, 9-16. 
A. P. Dempster, N. M. Laird and 1). 13. l{ubin. 
1976. Maximum likelihood fi'om incomplet,e data 
via the EM algorithm. The Royal ,S'tatistics Soci- 
ety, 39(B) 205-237. 
Williain A. Gale, Kenneth W. Church. 1993. A pro- 
gram fbr aligning senten(:es in bilingual (-orl)ora. 
Coml)utational Linguistics, \]9:75-102. 
Frederick Jelinek. \]997. Statistical Methods for 
Speech Recognition MIT Press. 
Marin Kay, Martin Roscheisen. 1993. Text- 
translation alignment. Computational Linguis- 
tics, 19:121-142. 
Julian Kupiec. 1993. An algorithm tbr finding noun 
phrase corresl)ondenccs in bilingual corl)ola. In 
Proceedings of ACL 31, 17-22. 
Yuji Matsmno~o, Hiroyuki Ishimoto, Takehito Ut- 
sure. 1993. Structural inatching of para.llel texts. 
In Proceedings of ACL 3I, 23-30. 
I. Dan Melame(l. 1997. A word-to-word model of 
translation equivalence. In PTvcccdings of ACL 
35/EACL 8, 1.6-23. 
Frmlz Josef Och mid Ilans Wel)cr. 1.998. hnt)rov- 
ing Statistical Natural Language Translation with 
Categories and Rules. In Procccdings of ACL 
36/COLING, 985-989. 
Stephen A. Della Pietra, Vincent J. Della Pietra, 
John D. La.tl'erty. 1997. llnducing features of ran- 
dora fields. IEEE ~IYansactions on Pattern Anal- 
ysis and Machine Intelligence, 19(4):380-393. 
Frank Smadja, Kathleen R. McKeown, and Vasileios 
Hatziw~ssiloglou. 1996. Translating collocations 
fi)r bilingual lexicons: A statistical approa(:h. 
Computational Linguistics, 22 (1) :1-38. 
Kengo Sate 1998. Maximum Entrol)y Model Learn- 
ing of the Translation Rules. In Procccdinfls of 
ACL 35/COLING, 1171-1175. 
Jung H. Shin, Y(mng S. Han, and Key-Sun 
Choi. 1996. Bilingual knowledge acquisition from 
Korean-English paralM cort)us using aligmnent 
method. In Proceedings of COLING 96. 
C. Tilhnann, S. Vogel, H. Ney, and A. Zubiaga. 
1997. A I)P t)ased sea.rch using monotone a.lign- 
ments in statistical translation. In Procccdings of 
ACL 35/EACL 8, 289-296. 
Ye-Yi Wa.ng and Alex Waibel. 1997. Decoding algo- 
rithm in statistical machine translation. In Pro- 
cccdinfls of ACL 35/EACL 8, 366-372. 
Ye-Yi Wang and Alex Waibel. 1998. Modeling with 
structures in machine translation. In Procccdings 
of ACL 36/COLING 
Dekai Wu 1996. A t)olynonlial-time algorithm for 
statistical machine translation. In Proceeding of 
A CL 34. 
445 
Identifying Temporal Expression and its Syntactic Role Using 
FST and Lexical Data from Corpus 
Juntae Yoon 
jtyoon@daumcorp.eom 
Daum Communications Corp. 
Kangnam-gu~ Smnsung-dong~ 154-8 
Seoul 135-090~ Korea 
Yoonkwan Kim Mansuk Song 
{general,mssong} @december.yonsei.ac.kr 
Dept. of Computer Selene% Engineering College 
Yonsel Univ. 
Seoul 120-749, Korea 
Abst rac t  
Accurate analysis of the temporal expression is cru- 
cial for Korean text processing applications uch 
as information extraction and clmnking for efficient 
syntactic analysis. It is a complicated problem since 
temporal expressions often have the ambiguity of 
syntactic roles. This t)al)er discusses two problenm: 
(1) representing and identiflying the temporal expres- 
sion (2) distinguishing the syntactic tim(lion of the 
temporal exI)ression in case it has a dual syntac- 
tic role. In this paper, temporal expressions and 
the context for disambiguation which is called local 
context are represented using lexical data extracted 
fiom corlms and the finite state transducer. By ex- 
periments, it; turns out that the method is eflimtive 
for temporal expression analysis. In particular, our 
al)t)roach shows the corI)us-based work could make 
a promising result for the t)roblem in a restricted 
domain in t, hat we can eflbctievely deal with a, large 
size of lexical data. 
1 In t roduct ion  
Accurate analysis of the temporal expression is cru- 
cial tbr text processing aplflications uch as informa- 
tion extraction and for chunking for efficient syntac- 
tic analysis. In information extraction, a user might 
want to get a piece of information about an event. 
Typically, the event is related with (late or time,, 
which is represented by temporal expression. 
Chunking is helpflfl for efficient syntactic analy- 
sis by removing irrelevant intermediate constituents 
generated through parsing. It involves the task to 
divide sentences into non-overlatli)ing segments. As 
a result of chunking, parsing would be a problem of 
analysis inside chunks and between chunks (Yoon, et 
al., 1999). Chunking prevents the parser fl'om pro- 
ducing intermediate structures irrelevant o a final 
output, which makes the parser etticient without los- 
ing accuracy. Thus, it turns out that chunking is an 
essential stage tbr the application system like MT 
that should pursue both efficiency and precision. 
Korean, an agglutinative language, has well- 
developed flmctional words such as postposition 
and ending by which the grammatical fimction of 
a phrase is decisively determined. Besides, because 
it is a head final language and so the head always 
follows its complement, the chunking is relatively 
easy. However, we are also faced with an mnbiguity 
problem in chunking, which is often due to the tem- 
poral expression. This is because inany temporal 
nouns are used as the modifier of noun and vert) in 
a sentence. Let us consider the tbllowing examI)les: 
\[Example\] 
la  jinan(last) :l\]('oFd'll,'llt(SlllillIler) 
-+  
lb 
uv i - t teun  
(we/NOM) hamgge(together) san-c(to moun- 
tain) .qassda( went )
\;Ve went to the mountain together last sum- 
Incr. 
j inan(last ) yeorr;um(summer) banghag-c(in va 
-9  
2a 
cation) 'uri-neun(we/NOM) hamgge(together) 
san-c( to mountain) gassda( went )
We, went to the mountain together in the last; 
SUllllller Va(;atioIl. 
i0 weol(October) 0 il(9th)jeo'nyeo.q(evening) 
-+ 
2b 
"7 sit7 o'clock) daetong'ryeong-yi (presi- 
dent/OEN) damh,'wa-ga(talk/NOM) issda(be) 
The president will give a talk at 7:00pro in 
Oct. 7th. 
10 wool(October) 9 il(9th)jconyeog(evening) 
7 sit7 o'clock) bih, aenggipyo-reul(flight ticket;/ 
ACC) yeyaghal su isssev, bnigga(can reserve) 
-+ Can I reserve the flight ticket tor 7:00pro in 
Oct;. 77 
Ill the examples, each temporal expression plays a 
syntactically different role used as noun phrase or 
adverbial phrase (The undeJ'lined is a phrase) al- 
though they comprise the same phrasal fornls. Tile 
temporal expressions in la  and 2a of the example 
serve as tile temporal adverb to modify predicates. 
On the other hand, the temporal expressions iu lb 
and 2b are used as the modifier of other nouns. That 
is, as a temporal noun either contributes to construc- 
tion of a noun compound or modifies a predicate, it; 
causes a structural aml)iguity. 
One sohltion might be that the POS tagger as- 
signs a different ag to each temporal noun e.g. NN 
954 
mid ADV'. However, since (let/en(len(:ies of teml)oral 
llOllllS .~l, re lexically (lecided, it does not seem that 
their synta('t;ic ta.gs could tie ac(:urately t)redi(:ted 
with a relatively small size of PeG tagged (:orl)uS. 
Also, the siml)le rule based a.1)l/roach Callliol; litake 
sa|;isfa.ctory resuli;s without lexi('al information. As 
such, identification of temi)oral expression is a coin- 
plicate(t l)roI)lem in Korean text mmlysis. 
This 1)aper disc, usses identiticatiol~ of temi)oral ex- 
t)ressions ;m(1 their synta(:tic roles, in this t)aper, 
we wotfld deal with two 1)robleins: (1) re, I)resent- 
ing mM idenl;ifying |;he teinl)oral exl)ression (2) dis- 
t inguishing its syntactic t ime|ion in case it; has a 
drml syntacti(: role. Acl;ually, tile two t)rol)lems are 
(:h)sely related since the identifi(:ation and (lisam- 
biguation proc(~,ss WOll\]d lie done un(ler l;\]w, r(',l)re- 
seul;a|;ion s(:henm of |;emporal exI)ression. Tim pro- 
('ess bases (in lexi(:al data exl;ra('l;e(t \['rOlll (:orl)llS ;Ill(t 
the finite state trans(lu(',(u' (FST). A(:(:(n'(ting to our 
ol,servation of texts, we (:ould see that a fe, w wor(ls 
following a. teml)oral llOllll ha.ve great ef\['ect on the 
syntactic funet, ion of the temt)oral noun. Theretbre, 
we note that the stru(:tura.1 amlliguity (:ould lie re- 
solved in lo(:al (:ont(~xts, mM so obtain lexical in- 
tbcmation for the lo(:a\] (:ontexts from (:orlms. The 
lexi(:al da.ta which (:onl;ain (;onl;exl;s for disambigua- 
| ion are ref)resenl;e(t with |:emt)()ral wor(t transition 
()ver the 1EST. 
l kMly  (l(~scrilling our methodology, we tirsl ex- 
l;r:,mI; (',Oll(;or(lail(:(~ dal,a of each I;emt)oral w(/r(t using 
~/ COllCOrd,:t,llC(~ l)rogr;nn. '\['he CO-OCCllrr(}IlC(~S r(~l)l'e- 
SQIlI; relations ll(d;ween l;e\]nl)ora\] wor(ls and also ex- 
plain how I;(~,llll)Ol'al ll()llllS ;~tll(\] COllllllOll ll()llllS }/17o 
combined to gCllCritl;C .:1 (:Oml)OmM noun. It wouhl 
be the like, lihood of woM (:oral)|nail(m, whMl helps 
disambiguate tim syntacti(: role if a teml)oral word 
have a synta(:|;i(: duality. \]n particular, we (:lassi\[y 
t(mq)oral llOllllS into 26 classes in ac(:ordan(:e with 
their meaning a.nd fmwtion. 'l'hus, the w(n'd (:o- 
OC(;llrr(~,llC0,S l)ecoll le t;\]lose ~llll()ll~ |;ell l i)oral (:\]asses 
or | ;elnl)oral  (;lasses and  el;her ll(/lillS~ whi(:\]l resuli;s 
in re(lu(:ing the 1)m-mn(d;er spa(y_ Se, con(1, l;emi)o- 
ral expl 'essiol ls  ('Ollt}tillillg |;he c()-occ/lrrel lces ()\[" fera- 
l)oral (:lasses an(1 other 1\]OllllS are rel)r(;sented with 
the FST to identit~ temporal  ext)ressions and assign 
their syntactic tags in a Sell|;(}llC0,. it has t/een shown 
|;lint; the FST t)resents a very etlMent way for repre- 
senting 1)hrases with locality. The inlmt of the FST 
is the result from morphological analysis a.n(1 POS 
tagging (here, the teml)oral noun is tagged only as 
nora 0. Its ou|;I)Ul; is the syntaci;i(: tag for each word 
in the senl;ence and temi)oral words are al;l;ached tags 
such as noun and adverl), l:igm'e 1 shows tim over- 
all system fl'om the morI)h(/logical analyzer l;() l;he 
chunker. 
Therefore, the, t)ro(;(~,ss atl;at;ll(~s sylll;a(:l,i(; labels 
to the 1)revious exmnl)les so thai; (:hunking w(ml(1 lie 
safely executed from the results as follows: 
\ [Exmnple \ ]  
la.' \[ j /na'n.(last) y(dol'(;ltfl,.( Sllllllller ) \]T A 'ltf'i-'ll.('lt~l. 
(we/SeMI mou,,- 
rain) gassda(went) 
--> \Ve went to the mountain together last smn- 
l l ler. 
b' ) sum,, ,,r ) \].,, x 
e(in vacation) u'ri-neun(we/NOM) hamg.qe 
to we,ltO 
-+ We. went to tile momltain together in the last 
SllIlllllOr vo~cal;iOll. 
2 1' \[.to o il( o ;h ) eve.i.  ) 
7 si(7 o'clock)\]TA dactongr'ycong-yi (presi- 
dent /GEE)  damh,'wa-ga(tall?/NOM)'i,s',s'da(be) 
-~ The president will Give a talk at 7:0()pro ill 
O(:t. 7th. 
21) ' \[10 ',,,,.ol( ( ),'|~ol ier ) :) ',:l(9a) .#o',.~,,,,9( ev~;,ti,lg) 
7 s/(7 o'clock)\]:rN bih, a(:nq.qipyo-re'u,l(flight 
t i cket /ACe)  ycyagh, al su issscubni99a(can re- 
Se, I'V(~,) 
--~ Can I reserw~ the tlight ticket for 7:0()lml in 
()(:t. 7? 
2 Rela ted  Works  
Almey (\] 991) has proposed texL chunldng as a t)l'e -
l iminary st;e l) to tmrsing on the basis of psycho- 
logical (wi(lence. In his work, tim chunk was (le- 
fine(1 as a. t)artitione(l segnmnt wlfi(:h corl:(~,Sl)Oll(ts in 
some way to ln'osodic lmtt(!rns, l\[n addition, con> 
1)lex }/I;|;tlchlllellt (\](?cisions ;Is occurring in NP or 
VP analysis are 1)OStl)one,(l wit;hour \]icing (leci(led 
in (:hunldng. Rmnshaw and Marcus (1995) intro- 
du(:e(l a 1)aseNl' whi(:h is a non-re(:ursive NIL They 
used trmlsfornmtion-1)ase(l learning to i(lentif~y n(/n- 
recto'sire l)aseNPs in a s(mtence. Also, V-typ(~ (:hullk 
was iifl;roduce(l in their system, and so I;lw, y (aied 
to t)artition sentences into non-overlal)l)ing N-type 
;til(t V-tyl)e ('hunks. Y(/on, et al (\].099) have, de- 
fined ('lmnking in various ways for efficient analysis 
of Korean texts mM shown that the, mt:tho(t is very 
eff(;(:tive for practical al)l)li(:ation. 
l leside, s, th(',r(~ have })een many w(/rks based on the 
finite state ma(:hine. The finite state machine i,~ o f  
ten used for systems u(:h as speech t)r(l('essing, liar- 
tern mat('hing, P()S tagging and so forth becmlse 
of its ei\[i('ien(;y of sl)ee(l mid si)aee and its ('onve- 
nience of rei)resenl;ai;ion. As for parsing, it is not 
suitable \['or flfll parsing based on the grammar  that 
has recurrent property, but for partial parsing re- 
quiring simple, sl;&l;e l;rallsitioll, l~.o('he all(1 S(:hal)es 
(1995) have i;ransformed l;he Brilt's rule based tagger 
to the (ll)timized deterministic FST and imI)roved 
the sl)eed mM sl)a.ce of the tagger. A. nora.tile one re- 
lated t( /this work is about local grammar 1)resented 
in Gross (1993), which is suital)le for rel)resenting 
955 
SOlltelleO 
jinan yeoreum banghag-e uri-neun hamgge san-e gassda 
Morp Anal and POS Tagger 
.LJ.I 
jinan/A yeoreum/N banghag/N-c/P uri/PN-ncun/P hamgge/AI) san/N-c/P ga/V-ss/TE.-da/E 
FST Ibr idefifying temporal expression 
jinan/A_m ycorcum/N_m banghag/N-e/P uri/PN-neun/P hamgge/AD san/N-e/P ga/V-ss/TE-da/E 
Text Clmnkcr 
\[jinan/A_ln ycoreum/N_tn banghag/N-c/P\] \[uri/PN-neun/P\] \[hamgge/Al)\] \[san/N-e/P\] \[ga/V-ss/TE-da/E\] 
Figure 1: System overview fl'om the inort)hological nalyzer from the chunker 
rigid phrases, collocations and idioms unlike global 
grammar for describing sentences of a language in 
a formal level. The temporal expression was repre- 
sented with loc~l grammar in his work, where it, was 
claimed that the formalism of finite automata could 
be easily used to represent them. 
3 Acquiring Co-occurrence of 
Temporal Expression 
3.1 Categorizing Temporal Nouns 
Since many words have in common a similar mean- 
ing and flmction, they can be categorized by their 
features. So do temporal nouns. That is, we say that 
'Sunday' and 'Monday' have the same features and 
so would take the similar behavior patterns uch as 
co-occurring with the similar words in a sentence or 
phrase. Hence, in the frst  place we categorize tem- 
poral nouns according to their meaning and func- 
tion. We first select 259 temporal nouns and divide 
them into 26 classes as shown in Table 1. Among 
them, some temporal words have syntactic duality 
and others play one syntactic role. Thus, the dis- 
ambiguation process would be applied only to the 
words with dual syntactic functions. 
3.2 Acquisition of Temporal Expressions 
f rom Corpus 
Temporal words would be combined with each other 
in order to be made reference to time, which is called 
temporal expression. Since a temt)oral expression is 
typically composed of one or a few temt)oral words, 
it seems to be possible to describe a grammar of 
modifying noun 
jeonlsilll-eUll nlasisseossda 
(hmch/NOM) (was delicious) 
oneul(today) ~ @  . . . . .  @ 
~"X hagayo-c(lo school) gassda(wenl) 
modifying predicate 
Figure 2: Syntactic flmctional ainbiguity of tcnlpo- 
ral expression 
the temporal expression with a simple model like fi- 
nite automata. Ill tile practical system, however, we 
are confronted with a complicated problenl in treat- 
ing teml)oral expressions since many temporal words 
have a functional ambiguity used as both a nominal 
and predicate modifier. For instance, a temporal 
noun oneul(today) could play a different role in the 
similar situation as shown in Figure 2. In the first 
and the second path, the words to follow oneul are all 
noun, but the roles (dependency rela;ions) of oncul 
are different. 
Accurate classitication of their syntactic flmctions 
is crucial for the application system since great dif- 
ference would be made according to accuracy of the 
dependency result. Practically, we therefore should 
take into consideration the structural ambiguity res- 
olution as well as their representation itself in identi- 
956 
word cat(;gory class # l;eml)oral words 
modifier 1 ol(l;his), jinan(lasl;), . . .  gemt)oral 1)refixes 
l l l l l l l \ ] )e r  2 
3 -10 
l lH l l lbe l "  . ? . 
era~ age 
l;emporal unit .Sg(li(Cell~l'y), 7~ygoT~(ye~l.l.'),... 
Lelll l)oral llOllllS 
years 
lno~|l, hs 
19 
20 21 
weeks 
- - - -~  day of week 
(lays I (lay\] 
day2 
time1 
1;line ~ 1;|me2 
~111( \ [  \[ seaso l l  
(lura- I Sl)eeiti e (hlration 
1;lOIl _I edge 
11 ftosaenfldae(1)aleoz(71c), . . .  
12 9eumnyeon(th is  year)-, saehae(new year ) , . . .  
13 ~ month),  jeon.qwol(January), . . .  
14 Oeum:#;,(this week), naeju(nexl; week) , . . .  
15 | lye'/ /(Sunday),  wolyo' il, . . .  
16 17 h, aru(one day), ch,'ascog(Thanksgiving day), . . .  
18 o,~Tia, lgo , lay ) ,  , , , : '  i l ( * ;omorrow) ,  . . . 
.saebyeoo( (lawn ) , achim( morning) , . . .  
yeonmal(year-end~, . . .  
22 ~st ) r ing) ,  yeoreum(smmner) , . . .  
23 hwar~:~eolgi(time of season changing), . . .  
24-25 ch, ogi(early t, ime), j'm~,gban(mid), . . .  
l;emporal suffixes ~eml)oral suffixes 26 dongan(duri l lg),  .~,aenae(l;hrough), . . .  
Table 1: Categorizal.ion of l;eml)oral words 
t~ying l;eml)oral exl)ressions. The poinl; I;hal; we note 
here is thai; we eould pre(liel; the synt;a(:l;ic fllncl;ion of 
I;emt)oral words 1)y looking ahead one or l;wo words. 
Namely, looldng at; a Dw words thai; follows a 1;em- 
poral word we can figure oul; which word the tempo- 
ral expression modifies, and call l;he following words 
local conl, e:rt. 
Unfortunal;ely, it is not easy t() define t, he, local 
conl;exI; for del;ermilfing 1;he synt, ael;i(: flm('l;ion of 
eae.h temporal  word 1)eeause l;hey are lexieally re- 
lai;ed. Thai; is, il; is wholly ditl('.renl; fl:om each wor(t 
wlw,|;her a I;(nnl)oral noun would modit)- ()l;h(w i1()1111 
(:o form ~ (:Oml)Om~d noun or mo(lii~y a 1)re(li(:al;e as 
m). adverbial l)hrase. ()ur al)t)roa/:h is l;o use co l  
pus to acquire informal;ion atmut, l;he local ('oni;exl;. 
Since we could obtain fi'om eorl)uS as many exam- 
ples as needed, rules for comt)ound word generation 
can be (:onstructe(1 from l;\]le examI)les. In l;his 1)al)el', 
we l lSe CO-OCClllTe, II(;O rela|;ions of l;emlmral lO/ l l lS ex -  
t,r,acted froll\] large corpus I;o represent and consi;ru(:t; 
rules for idenl;ifieal;ion of l:emporal expressions. 
As lnenl;ioned before, we would 1)a.y a|;ten(,ion (;o 
two t)oinl;s here,: (l.) In whal; order a tenq)oral ex- 
1)ression would 1)e represenl;ed with temt)oral words, 
i.e. descrit)|;ion of the temporal  exi)ression nel~work. 
(2) how the local context would 1)e described to re- 
solve tile ambiguity of the syntactic t im(l ion of tem- 
1)oral ext)ressions. For this tmrpose, we tirs|; extract 
examl)le sentences containing each of 259 l;eml)oral 
words from eorlms using l;he KA IST  concordance 
progrmn :l (KAIST,  1998). The numl)er of t, elnporal 
words is small and so we could mmmal ly  manii)u- 
late lexieal dal;a ext;racted frOlll corl)us. Figure 3 
1KAIST corlms consists of about 50 million cojeols, l';ojeol 
is a sl)acing unit; comi)oscd of a content word and functional 
words.  
shows exanl l) le sen|;enees at)out, ye.o'reum(sununer) 
ext;ra(:t;ed l)y the. coneor(lanee l)rogram. 
Second, we s(;leet only l, he t)hrases related wit;h 
temporal  words fl'om the examples (Table 4). As 
shown in Table 4, yeoreum is associated wii;h va.ry- 
ing words. Temporal  words like temporal  pre.tixes 
can come before it and coIlllIlOil llOl_lllS C}lll follow ig. 
In (,his stage we describe con(;exts of each temporal  
word and (;he olll;1)U(; (syn(;ae(;ic tag of (;he tOtal)oral 
word) under the given eonl;ex(;. In l)artieular, each 
l;eml)oral word is assigned a (.emporat class. Be.sides, 
or;her nouns serve as local cent;exits for disaml)igua- 
(;ion of syntac(;ic flmc(;ion of t, emporal  words. 
lS:om (;he examl)les , we can see t, lmC if ha're(night), 
byeo(jau, g(villa), ba, ngh, ag(vacal, ion) and so on follows 
it, yeoreum serves as a (:olnponent of a ( 'ompomld 
noun with the following word. On t, he other hand, 
the word naenac wtfich means all the t ime is a tem- 
poral noun and forms a teml)oral adverl)ial l )h l '}/se 
wil;h ()Idler 1)receding temporal  nomL Moreover, yeo- 
7'eum(sllnuner) might represent ime-relal, ed expres- 
sion with t)receding l;eml)oral prefixes. 
4 I dent i fy ing  Tempora l  Express ions  
and Chunk ing  
4.1 Represent ing  Tempora l  Express ion  
Us ing  FST  
The co-occurrence data extracted by tile way de- 
scribed ill the previous section can be represented 
with a finite state machine (Figure 5). For synt;ae- 
t ie :\[inlet;ion disambigual;ion an(t chunking, the au- 
tomata  should produce an out, lm|;, which leads to a 
fiifite st;ate t;ranslhleer. In fact, individual deserip- 
l;ion for each data could be integral,ed into one large 
FST and represented as the right-hand side in Fig- 
ure 5. A finite sta.te transducer is defined with a nix- 
957 
left context word right context 
~ ;._l_~otl = ~1~,~11 ~ .~o,~ ~_ ~,~.. \[ 4~- 
-~1~-  A~I ~ ~-lo~l,g ~-  ~1ol ~t - .  \[o:1-~- 
4~-~ ~-~-~-~- ~1 &~l~ll ~1~,~-. \ [d~ 
0 
~k.  ~1~11 ?' ? 
~,,t~xl ~ ~-~@~1~11 ~ ?4'q ~. \ [x l~  ?4~\] 
~d-~dol~l-q ~t~- ~o1-7-~I-!" ~ \[~ll o:t~-\] 
,'~ ~q-~l 4 o~_ ,? .  \[o I.d oq ~\] 
~\] 'd~ ? lq  ~<-ol- d ~ ;q "d 
'gb~\] ~-<>11 ~.  o_~ -, ~ ~-1~1-~- 
~o~*. ~\] '~ <gl~- V-~?II -~}~ 
.j-q-\] _~ol~x- I , lu&~ o,o~ ,~ 2,2, 2~...J~. 
,~Xor\]Ol~tq, :z~} "J.-*~ls1-~'li ol ~,\] 
-~-~\]~ ~x~lq- ~1~,4 ~. ~'t-l-b 
~'l~xl'4 ~~-~t ~1;'I ~ ~1~~ ~1~ 
Figure 3: Example concordance data of yeorcum(summer) 
befbre temporal noml after outlmt freq 
yeoreum( suulmer ) 
x\] ~.!-/ t~ (ji.,.,,~, ~ant) 
~/tlO (hae,yeal') 
o\] ~/ ld (ibcon,thin) 
o:t ~-/t22 
o:t-~-/t2.,_ 
o-1 ~- / t.2.2 
oq ~-/t,2., 
o~ ~-, /~  
o~ ~-/t._,., 
r~\[(bam,night) TN 2 
vo~,(banghagyacation) TN 7 
'~ ~-( bycoljang,villa ) TN 1 
~( jumal ,  weekends) TN 1 
J,~ 71 (.qam,.qi,flu ) TN 1 
q\] q\]/t2~ (nacnae,all the time) TA 1 
~-F~-(na,,eunJ/TOP) TA \] 
6.25, \]- \] , ('m,a.?rmag, the last) TA 2 
~ (.leo'ntuncun,1)attle/.\[ 0 I  ) TA 1 
Figure 4: Temporal expression phrases elected fronl examl)les 
tuple (Ej, E2, Q, i, F, E) where: E1 is a finite input 
alphabet; E2 in a finite output alphabet; Q in a ti- 
nite net of states or vertices; i E Q in the initial state; 
F C_ (2 is the set of final staten; E C Q ? E~: ~ E.; x (2 
is the set of transitions or edges. 
Although the syntactic function of a temporal ex- 
pression would be nondeterministically selected fl'om 
the context, temporal expressions and the lexical 
data of local context can be represented in a de- 
terministic way due to their finite length. For the 
deterministic FST, we define the partial functions ? 
and ? where q?a = q' iffd(q, a) = {q'} and q,a = w' 
iff ?q' E Q such that q?a = q' and 5(q, a, q') = {w'} 
(R.oche and Schabes, 1995). Then, a nubsequential 
FST is a eight-tui)le (El, E2, Q, i, F, ?, *, p) where: 
E1,E2,Q, i  and F are the smnc as the FST; ? is 
the deterministic state transition fimetion that maps 
() x E1 on Q; ? is the deterministic emission fimction 
Figure 6: 
T = (~,, r,.~, O,i, F, o , . ,p})  
)2~ = {tl, t~2, t~6, wi, wj} 
E2 = {TN, TA,  NT}  
0 = {o, 1,2,3} 
i = 0, F = {3} 
0c4tl =1,  O,t1 =TN,  
1?t22 =2,  l * t~=G 
2?t~< =3, O*twi =TN-NT,  
2 @ t.~6 =3, O * t.2s = TA, 
2 ? twj =3,  O * t, u = TA_NT,  
p(3) = 
Deternfinistic FST resulted from Figure 5 
that  maps Q x E1 on E~; p : F --) 22~ is the final 
outtmt fluiction. 
Our teniporal co-occurrence data can lie relive- 
sented with a deterministic finite state transducer 
958 
} ?~g~'tl\[1111 N , bdl i#~\[  
}~?~CUUltl N I~) colJanglNI 
i , * .  ! N I~mylHL'IN I
/ )c, nc ~I'A ;., ) :"" .( &, 
jin;ulY\[ N /  
~?,w,,,,,n X , j.,,,,,l,x I 
~ c(~etll,l/I N ila ?11 a,'\[ \['A 
j ln;m/I N ?,l?lbrlltl A ilajlclmiN I 
k,e/IN ),,,reumflA ?,25/NI 
kw/ IN  ~?,liclnllJ\[ a ilk ij~tll Ik~\]N I 
\ d ................................................. \, ............. ( ' /  
Figure 5: Fiifite sl;al;e nta('hin(~ (:onsl;rucl;ed wil;h l;he (bd;a in Figure d 
O t'/:"H-( ) 
0 I 
wl /!l 'N, NT' 
t2'-'(' ~ ( ) !~';IT A 
u,j /'I'A, NT 
ll" :: {barn, jumol, t, a l tgha 9 . . . .  } 
mi (i lI" 
wj ~ 1V 
Figure 7: A d(;1;erministic tinil, e sl, aW, (,ransduc('a' 1;o 
i)roce, s,s temi)oral ex\])re,,qsion 
in a similar way. The, sut)s('qu(',ntial FST f()r our 
sysl,em is (l(,,fined as in Figure (i and Figur(~ 7 ilhLs- 
l.ral;es I;he tral~sdu(:(!r in l?igur(~ 6. In L\]m tiI~me, ti 
is a c\]ass 1:o whi(:h lhe (:eml)oral w()rd 1)elongs in l;he 
lx',mporat (:las,qiti('at;ion. wi is a word ol;her l;han l,em- 
1)oral ones 1;hal; has l;he pr(',(:(~(ting t eml)()ral wor(l 1)(', 
il:s modiiier, and wj is not; such a word 1;() make a 
compound noun. TN,  TA and NT  are synt, aciic 
tags. A word t;agged with 5/'N would modify a su('- 
ceeding l lO l l l l  like, barn(night), bangh.ag(vacati(m). A 
word al:t, ached with TA  would lnodify a predica.lx~' 
aim one with NT  nmans ii; is not; a 1;emporal word. 
A(:mally, individual FSTs are coml)in(;d into one aim 
rules for tagging of temporal words are pul; over l;h(; 
.,J. The rule is applied according to the prioriW 
by  f req l le l l cy  il l  case  lllOrl2 t ;hal l  ()lie ()ll{;l)ill; a re  \ ] )os-  
sible for a (:Oilt;ex|;. Nmnely, it; is a rule-l)ased system 
W\]I()I'(I ~\]le r l l l es  al'e, (~xl;ra(;|;(?(l f ro l l l  (;ort)l lS. 
4.2  Chun ldng  
Afl;er the FST of l;enlt)oral (',xt)ressions adds I;o woMs 
syntactic tags such as TN and TA, chunking is con- 
ducted with l'eSlllI;s frolll OllI;l)llI;S 1)y t;h(' FST. As we 
said earlier, (:hunldng in Korean is relal;iv(;ly easy 
only if t;h0, t;eml)oral exi)l'essioll wou\](t be success- 
fully recognized. Act;ua.lly, our ('hunker is also based 
on the, finil;e s(;a,l;e machine. The following is an ex- 
mnl)le for (:hunldng rules. 
iN1,) ~ (NF (NP) I (2V)* (2VIO* (UN) 
('rNI,) ~ {TN)* (N)* (XP) 
\]\](!re: j\r is a noun wil;h(ml, rely \])ost.t)osit.ion , NP  is 
a noun wit.h a. 1)oSl;l)OSil,ion, TN is a t;enll)oral noml 
recogniz(~,d as modifying a suc(:ox'ding n(mn, NU is 
a number and UN is a uni(; n(mn. Afl;e,r t('mporal 
l.a.gging, 1;he ('hunker l;ransforms 'NT '  into N, NP, 
(d,(',. according I,o morl)hologi(:al consi;itueid;s and 
their I)OS. I h'io, tty, t;he, rule says thai; an NP ctmnk is 
mad(', from eil3mr NI '  or l;emporal NIL An NP would 
\])(! (:()llsLrll('l,(',(1 wi i ,h  on(;  o r  lll()l'(} llOllll.q ;ill(1 \[;boil" 
modilie(~ or with a noun (lUanl;ified. A TN\] ), whi(:h 
is r(',lal:ed with lime, is made from n(mns moditied 
by t('ml)oral words wlfi(:h would 1)(', i(t(;nl;itied by the 
FST. By i(l(mtifi(:ation of lx!mporal (',xpressi()n and 
chunking, tlm following (',xmnl)k', senl~elu:e, is chunked 
as  | ) ( ; low.  
? j inan(lasQ ycor~'um(summer) bau, ghag-e(in 
? s * " v,,,:,,.(;io,,) ,,,.,.,,-,,.,;,,,,4,,,~./s un.~) k~o,,,,VV,,- 
t(.~o(,.o,m,,,~;(,F) .,,~,(th~.,,o~)d(.~'-,~',,l(.,,iqOl~.J) 
sassda(bought;) 
-+ \,Vo, bought l;hrce c()mlmt(~'rs in the lasl; 
81111111101" V~IC~I(;i()l l .  
? jin(t?Vl,N ',~l(:OTC'tt'lllq'N balzgha(j-CNl, '~tri-nc'ltlZNl, 
kco'vnl)yUl, eON SCNU dae-reUINl, sassdav 
? \[jinawl'N yCOVCUmTN ban!lh, ag-CNl,\]Nc 
\['uri-ne, wnNP\]NC: \[kcompyuteoN SeN?: dac- 
?'C?tlNI,\] N(; sassdav  
5 Exper imenta l  Resu l t s  
For l;hc ext)erinmnl; a.bouL l;eint)oral expre, ssion, we 
e, xla'aci;ed 300 senl;enc('~s (:onl;a.ining temporal expres- 
sions from E\]?I{I )()S cortms. Table 2 shows the r(',- 
959 
~c'*?unnrL ~ ( 
y~,~cl,,,,ItX { 
ya~cu,,,fl N ( 
) ?,~c,,,n/I N~ ( 
) b'un*N'l ~: : 
) t') ?"II:~'U'/N.I i 
g;,,,,~,lXl~ 
,.?aadD~ 
. xy?l}l?onlfI'A / -  ) 
!+,,+,,,aN )++,.+,,,,,,,A< 2 '':''L"''~'L ,2 
,~,.,ax -( y?,,~,,,,~,L~ /,,~j,,.,,,.xr(; ,~
Figure 5: Finite state machine constructed with the data ill Figure, 4 
wi /TN,  NT  
. .  t i /TN  C~ 1.22/{ F , / "  t2o/'l 'A " " ' '  
0 2 \ - - / "  
wj /TA ,  NT  
IV = {barn, jumal ,  ba~dhao, . . .}  
Wl G IV 
wj  ~ II / 
Figul'e 7: A {lel;('~rministi{: finil;{; stat(; trans{lucer t{} 
process temporal expression 
in a similar way. The subs(xlU{'.ntial FST for our 
sy:stem is detined as in Figure {i and l?igu\]{~ 7 illus- 
trates th{. ~ trans(hl{:er in Figure. (i. In the tigurc, ti 
is a {'lass to which the tc.mporal w{}rd 1}elongs in the. 
temporal classification. "wi is a word {}the.r than tem- 
poral ones that has |;11{; prex:e.ding temporal word be 
its moditier, and 'wj is not such a word to make a 
COml)Oui:d noun. TN,  TA and NT are synta{'ti{: 
tags. A word tagged with TN would mo(lit~y a suc- 
(:ceding noun like barn(night), ban.qh, ag(vacation). A
word attached with TA wouhl mo{lii2y a predicate 
and oi1{; with NT  means it is not a temporal word. 
Actually, individual FSTs arc {:oml}ined int{) one. an(l 
rules for tagging of teml}oral wor{ts are put over the. 
FST. The rule is al}plied according to the priority 
by fro(tllOll{;y ill case m(}re than o11o (}uttmt are pos- 
sible for a context. Namely, it is a rule-based system 
where the rules are extracted fi'om corl}us. 
4 .2  Clmnk ing  
After the FST of temporal exi}ressiolls adds to words 
syntactic tags such as TN and TA, chunking is {:on- 
ducted with results t iom outl)uts 1)y the Fsr\] '. As we 
said earlier, {:lmnking in Kore.an is relatively easy 
only if the teml}oral ext}ression would l)e suc{:e.ss- 
fltlly recognized. Actually, our clmnker is also 1)ased 
on the finite, state lnachine. The tbllowing is an ex- 
ample tbr chunking rule.s. 
(Nl~h,.,~) -? (NP) I (TNP) 
(NP) -~ (N)* (NP) I (N)* (Nu)* (uN) 
('rNu) -~ ('rN)* (N)* (NP) 
Here, N is a noun without any 1)ostl)osition, N/? is 
a noun with a postposition, TN is a temporal noun 
recoglfized as modii~ying a succeeding 1101111, NU is 
a numbe.r and UN is a unit noun. Aft, er tcmI)oral 
tagging, the chunker transforms 'NT'  into N, NP, 
(~tc. according to morphological constituents and 
their POS. Brietly: the rule. says that an NP clmnk is 
made fl'om either NP or temporal NIL An NP would 
1)e (:onst, rll(;te.(1 with one. or lnOl'O llOlllIS and their 
m{}{lifie{~ {)r with a noun quantified. A TNP, whi{:h 
is re, lated with time, is made fr{)m nouns mo(liticd 
by teml}oral words which w{mld be ide, ntitied by the, 
FST. By identification {)f t('mt){}ral ex\]}ression and 
chunldng, the following exami)le sentence is ctmnked 
as below. 
? jinan(last) ycorcum(summer) ban.qhag-c(in 
vacation) 'ari-ncun(we/SUB3) kco'm, pyu- 
.<thr,,{,) 
sa.ssda(bought) 
-+  \VC bOl lght  1;t117o.o comi)uters in the last 
Sll l l l l l ler vacal0io\] l .  
? jinanTN ycoreumTN banghag-cN1, uvi-ncunNp 
kcom, pyuteoN SeNU dac-vculNp sassdav 
? \[jinanTN yeorelt~lZ,l,N ban.qha.q-cNP\]N(~' 
\ [ Iw i -T tC ' l t * tNP \ ]N  C \[kcompyutcoN SCNU dac- 
rculN P\]N(; sassdav 
5 Exper imenta l  Resu l t s  
bbr the {;xi}erinmnt al}out temporal exI}ressi{m, we 
extracted 300 senten{:es containing teml}oral expres- 
si(ms from ETRI  POS corlms. Tal}le 2 shows the r{'.- 
959 
t precision J_?gcall 
rate (%) 97.5 90.56 
Table 2: Results of identifying temporal expression 
no chunking-\[ using chuifldng 
4.8 -\[ 3.3 avg. # of cand \[ 
Table 3: Reduction of candidates resulted from 
chunking 
sults from identit:ying temporal expressions and dis- 
aml/iguatil~g their syntactic functions. From the re- 
sult in the table we see that the method is very effec- 
t, ive in that it very accurately identifies all tile tem- 
poral expressions aim assigns them syntactic tags. 
And, Table 2 shows the reduction resulted from 
chunking after temporal expression identification. 
We take into consideration the average numl)cr of 
head candidates for each word since our parser is 
dependency based one. The test was conducted on 
the tirst file (about 800 sentences) of KAIST tree- 
bank (Choict  al. , 1994). The number was reduced 
by 51% in candidates compared to the system with 
no chunking, whMl makes pa.rsing efficient. 
Most of errors were caused by tile case where tein- 
poral words have different syntactic roles under the 
same context. In this case, the global context such as 
the whole sentence or intcrscnt;cntial infornlation or 
sometimes very soi)histicated processing ixneeded to 
resolve the prol)lem, l~br instance, '82 ~tycoTt(year) 
h, yco'njac-yi(now/Gl;N)' could be used two-.way. If 
the speech time is the year 1982, then h, yeou(fl, e-yi are 
conlbined with 82 nycon to represent time. Other- 
wise, 82 does not ino(til~y hycordac-yi, wlfich cannot 
be recognized only with the local context. Neverthe- 
less, the system is promising in that generally it can 
ilnprove e\[flciency without losing accuracy which is 
crucial for the pracl;ical system. 
6 Conclusions 
hi this paper, we presented a method for identifi- 
cation of temporal exi)ressions and their syntactic 
functions based on FST aim lexical data extracted 
fl:om corpus. Since tenlporal words have the syntac- 
tic ambiguity when used in a sentence, it; is impo> 
tant to identify the syntactic functioll as well as the 
temporal expression itself. 
For the purpose, we manually extracted lexical c(> 
occurrences t?om large corpus aim it was possible as 
the number of temporal nouns is tractable nough 
to manipulate lexical data t)y hand. As shown in 
tile result, lexical co-occurrences are crucial for dis- 
mnbiguating the syntactic flmction of the tenlporal 
expression. Besides, the finite state approach pro- 
vide(l an eftieient model for temporal expression pro- 
cessing. Combined with the clmnker, it helped re- 
nmrkably lessen, by 1)runing irrelewmt candidates, 
intermediate structures generated while parsing. 
References 
Almey, S. 199t. Parsing By Chunks. ill Berwick, 
Abney, and Tenny, editors, Principlc-\]3ascd Par.s- 
ing. Boston: Klnwer Acadenlic Publishers. 
Choi, K. S., tIan, Y. S., Han, Y. G., and Kwon, O. 
W. 1994. KAIST Tree Bank Project for Korean: 
Present and Future Develot)ment. In Proceedings 
of the l~ttcrnational Workshop on ,%ara, blc Natu- 
ral Language Resources. 
Ciravegna, F. and Lavelli, A. 1997. Controlling 
Bottom-Ut) Chart Parsers through Text Clmnk- 
ing. In Proceedings of the 5th International Work- 
shop on Parsing ~);chnology. 
Collins, M. J. 1996. A New Statistical Parser Based 
on Bigram Lexical l)et)endencies. Ill Proceedings 
of the 3~th Annual Meeting of the ACL. 
Elgot, C. C. and Mezei, J. E. 1965. On relations de- 
fined by generalized finite alltonlata. \[B~d r Journal 
of Rcscarc.h and Development, 9, 47-65. 
Gross, M. 1993. Local Grmnmars and their I{ep- 
resen~ation by Finite Automata. Data, Descrip- 
tion, Discourse: l'apcrs on l;hc English language 
i77, \[tor'lto,ur" of John Mc\[\[ Sinclair, Michael Hoey 
(ed). London: HarperCollins Publishers. 
KAIST. KAIST Concordance Program. URL 
htt;p://(:sfive.kaist.ac.kr/kcp/. 
Mohri, M. 1997. Finite-state ~\]5:ans(luccrs in lan- 
guage and Spe(~ch Processing. 6*omp'ul;ational 
Li'n.gui.stics , Vol 23, No (2). 
Ramstmw, L. A. and Marcus, M. \]'. 1995. Text 
Chunking Using Transtbrmation-Based l,earning. 
In Proceedings of the ACL Workshop o'n l/cry 
Large Corpora. 
l{oche, E. and Schabes, Y. 1995. Deterministic 
Part-of-Speech Tagging with Finite-State Trans- 
ducers. Computatiou, al Li'n, guistics, Vol 21, No 
(2). 
Roche, E. and Schabes, Y. 1997. Finite-State Lan- 
guage Processing. The MIT Press. 
Skut, W. and Brants, T. 1999. Chunk Tagger. 
llIl Proceedings of ESSLLI-98 Workshop on Au- 
tomated Acquisition of Syntax and Parsing. 
Sproat, R. W., Shih, W., Gale, W. and Chang, 
N. 1994. A Stochastic Finite-State Word- 
segmentation Algorithm for Chinese. In Pwceed- 
ings of th, c 32rid Annual Meeting of ACL 
Yoon, .J., Choi, K. S. and Song, M. 1999. Three 
Types of Ctmnking in Korean and l)ependency 
Analysis Based on Lexical Association. In l'm- 
cccdings of ICCPOL '99. 
960 
Two-Phase Biomedical Named Entity
Recognition Using A Hybrid Method
Seonho Kim1, Juntae Yoon2, Kyung-Mi Park1, and Hae-Chang Rim1
1 Dept. of Computer Science and Engineering,
Korea University, Seoul, Korea
2 NLP Lab. Daumsoft Inc. Seoul, Korea
Abstract. Biomedical named entity recognition (NER) is a difficult
problem in biomedical information processing due to the widespread am-
biguity of terms out of context and extensive lexical variations. This pa-
per presents a two-phase biomedical NER consisting of term boundary
detection and semantic labeling. By dividing the problem, we can adopt
an effective model for each process. In our study, we use two exponential
models, conditional random fields and maximum entropy, at each phase.
Moreover, results by this machine learning based model are refined by
rule-based postprocessing implemented using a finite state method. Ex-
periments show it achieves the performance of F-score 71.19% on the
JNLPBA 2004 shared task of identifying 5 classes of biomedical NEs.
1 Introduction
Due to dynamic progress in biomedical literature, a vast amount of new infor-
mation and research results have been published and many of them are available
in the electronic form - for example, like the PubMed MedLine database. Thus,
automatic knowledge discovery and efficient information access are strongly de-
manded to curate domain databases, to find out relevant information, and to
integrate/update new information across an increasingly large body of scien-
tific articles. In particular, since most biomedical texts introduce specific no-
tations, acronyms, and innovative names to represent new concepts, relations,
processes, functions, locations, and events, automatic extraction of biomedical
terminologies and mining of their diverse usage are major challenges in biomed-
ical information processing system. In these processes, biomedical named entity
recognition (NER) is the core step to access the higher level of information.
In fact, there has been a wide range of research on NER like the NER task on
the standard newswire domain in the Message Understanding Conference (MUC-
6). In this task, the best system reported 95% accuracy in identifying seven types
of named entities (person, organization, location, time, date, money, and per-
cent). While the performance in the standard domain turned out to be quite good
as shown in the papers, that in the biomedical domain is not still satisfactory,
which is mainly due to the following characteristics of biomedical terminologies:
First, NEs have various naming conventions. For instance, some entities have
descriptive and expanded forms such as ?activated B cell lines, 47 kDa sterol
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 646?657, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Two-Phase Biomedical Named Entity Recognition 647
regulatory element binding factor?, whereas some entities appear in shortened
or abbreviated forms like ?EGFR? and ?EGF receptor? representing epidermal
growth factor receptor. Second, biomedical NEs have the widespread ambiguity
out of context. For instance, ?IL-2? can be doubly classified as ?protein? and
?DNA? according to its context. Third, biomedical NEs often comprise a nested
structure, for example ??DNA??protein?TNF alpha?/protein?gene?/DNA??. Ac-
cording to [13], 16.57% of biomedical terms in GENIA have cascaded construc-
tions. In the case, recognition of the longest terms is the main target in general.
However, in our evaluation task, when the embedded part of a term is regarded
as the meaningful or important class in the context, the term is labeled only with
the class of embedded one. Thus, identification of internal structures of NEs is
helpful to recognize correct NEs. In addition, more than one NE often share the
same head noun with a conjunction/disjunction or enumeration structure, for
instance, ?IFN-gamma and GM-CSF mRNA?, ?CD33+, CD56+, CD16- acute
leukemia?or ?antigen- or cAMP-activated Th2 cell?. Last, there is a lot of inter-
annotator disagreement. [7] reported that the inter-annotator agreement rate of
human experts was just 77.6% when performing gene/protein/mRNA classifica-
tion task manually.
Thus, a lot of term occurrences in real text would not be identified with sim-
ple dictionary look-up, despite the availability of many terminological databases,
as claimed in [12]. That is one of the reasons why machine learning approaches
are more dominant in biomedical NER than rule-based or dictionary-based ap-
proaches [5], even though existence of reliable training resources is very critical.
Accordingly, much work has been done on biomedical NER, based on ma-
chine learning techniques. [3] and [13] have used hidden Markov Model (HMM)
for biomedical NER where state transitions are made by semantic trigger fea-
tures. [4] and [11] have applied maximum entropy plus Markovian sequence based
models such as maximum entropy markov model (MEMM) and conditional ran-
dom fields (CRFs), which present a way for integrating different features such
as internal word spellings and morphological clues within an NE string and con-
textual clues surrounding the string in the sentence.
These works took an one-phase based approach where boundary detection
of named entities and semantic labeling come together. On the other hand, [9]
proposed a two-phase model in which the biomedical named entity recognition
process is divided into two processes of distinguishing biomedical named entities
from general terms and labeling the named entities with semantic classes that
they belong to. They use support vector machines (SVM) for each phase. How-
ever, the SVM does not provide an easy way for labeling Markov sequence data
like B following O and I following B in named entities. Furthermore, since this
system is tested on the GENIA corpus rather than JNLPBA 2004 shared task, we
cannot confirm the effectiveness of this approach on the ground of experiments
for common resources.
In this paper, we present a two-phase named entity recognition model: (1)
boundary detection for NEs and (2) term classification by semantic labeling.
The advantage of dividing the recognition process into two phase is that we can
648 S. Kim et al
select separately a discriminative feature set for each subtask, and moreover can
measure effectiveness of models at each phase. We use two exponential models
for this work, namely conditional random fields for boundary detection having
Markov sequence, and the maximum entropy model for semantic labeling. In ad-
dition, results from the machine learning based model are refined by a rule-based
postprocessing, which is implemented using a finite state transducer (FST). The
FST is constructed with the GENIA 3.02 corpus. We here focus on identification
of five classes of NEs, i.e. ?protein?, ?RNA?, ?DNA?, ?cell line?, and ?cell type?
and experiments are conducted on the training and evaluation set provided by
the shared task in COLING 2004 JNLPBA.
2 Training
2.1 Maximum Entropy and Conditional Random Fields
Before we describe the features used in our model, we briefly introduce the ME
and CRF model which we make use of. In the ME framework, the conditional
probability of predicting an outcome o given a history h is defined as follows:
p?(o|h) =
1
Z?(h)
exp
(
k
?
i=1
?ifi(h, o)
)
(1)
where fi(h, o) is a binary-valued feature function, ?i is the weighting parameter
of fi(h, o), k is the number of features, and Z?(h) is a normalization factor
for ?op?(o|h)=1. That is, the probability p?(o|h) is calculated by the weighted
sum of active features. Given an exponential model with k features and a set
of training data, empirical distribution, weights of the k features are trained to
maximize the model?s log-likelihood:
L(p) =
?
o,h
p?(h, o)log(o|h) (2)
Although the maximum entropy model above provides a powerful tool for
classification by integrating different features, it is not easy to model the Markov
sequence data. In this case, the CRF is used for a task of assigning label sequences
to a set of observation sequences. Based on the principle of maximum entropy,
a CRF has a single exponential model for the joint probability of the entire
sequence of labels given the observation sequence. The CRF is a special case of
the linear chain that corresponds to conditionally trained finite-state machine
and define conditional probability distributions of a particular label sequence s
given observation sequence o
p?(s|o) = 1Z(o)exp(
?k
j=1 ?jFj(s,o))
Fj(s,o) =
?n
i=1 fj(si?1, si,o, i)
(3)
Two-Phase Biomedical Named Entity Recognition 649
where s = s1 . . . sn, and o = o1 . . . on, Z(o) is a normalization factor, and each
feature is a transition function [8]. For example, we can think of the following
feature function.
fj(si?1, si,o, i) =
?
?
?
1 if si?1=B and si=I,
and the observation word at position i is ?gene??
0 otherwise
(4)
Our CRFs for term boundary detection have a first-order Markov dependency
between output tags. The label at position i, si is one of B, I and O. In contrast
to the ME model, since B is the beginning of a term, the transition from O to I
is not possible. CRFs constrain results to consider only reasonable paths. Thus,
total 8 combinations are possible for (si?1,si) and the most likely s can be found
with the Viterbi algorithm. The weights are set to maximize the conditional log
likelihood of labeled sequences in the training set using a quasi-Newton method
called L-BFGS [2].
2.2 Features for Term Boundary Detection
Table 1 shows features for the step of finding the boundary of biomedical terms.
Here, we give a supplementary description of a part of the features.
Table 1. Feature set for boundary detection (+:conjunction)
Model Feature Description
CRF, MEmarkov Word wi?1, wi?2, wi, wi+1, wi+2
CRF, MEmarkov Word Normalization normalization forms of the 5 words
CRF, MEmarkov POS POSwi?1 , POSwi , POSwi+1
CRF, MEmarkov Word Construction form WFwi
CRF, MEmarkov Word Characteristics WCwi?1 , WCwi , WCwi+1
CRF, MEmarkov Contextual Bigrams wi?1 + wi
wi + wi+1
wi+1 + wi+2
CRF, MEmarkov Contextual Trigrams wi?1 + wi + wi+1
CRF, MEmarkov Bigram POS POSwi?1 + POSwi
POSwi + POSwi+1
CRF, MEmarkov Trigram POS POSwi?1 + POSwi + POSwi+1
CRF, MEmarkov Modifier MODI(wi)
CRF, MEmarkov Header HEAD(wi)
CRF, MEmarkov SUFFIX SUFFIX(wi)
CRF, MEmarkov Chunk Type CTypewi
CRF, MEmarkov Chunk Type + Pre POS CTypewi + POSwi?1
MEmarkov Pre label labelwi?1
MEmarkov Pre label + Cur Word labelwi?1 + wi
? word and POS: 5 words(target word(wi), left two words, and right two
words) and three POS(POSwi?1 , POSwi , POSwi+1) are considered.
650 S. Kim et al
? word normalization: This feature contributes to word normalization. We
attempt to reduce a word to its stem or root form with a simple algorithm
which has rules for words containing plural, hyphen, and alphanumeric let-
ters. Specifically, the following patterns are considered.
(1) ?lymphocytes?, ?cells? ? ?lymphocyte?, ?cell?
(2) ?il-2?, ?il-2a?, ?il2a? ? ?il?
(3) ?5-lipoxygenase?, ?v-Abl? ? ?lipoxygenase?, ?abl?
(4) ?peri-kappa?or ?t-cell? has two normalization forms of ?peri?and?kappa?
and ?t? and ?cell? respectively.
(5) ?Ca2+-independent? has two roots of ?ca? and ?independent?.
(6) The root of digits is ?D?.
? informative suffix: This feature appears if a target word has a salient suffix
for boundary detection. The list of salient suffixes is obtained by relative
entropy [10].
? word construction form: This feature indicates how a target word is or-
thographically constructed. Word shapes refer to a mapping of each word
on equivalence classes that encodes with dashes, numerals, capitalizations,
lower letters, symbols, and so on. All spellings are represented with combina-
tions of the attributes1. For instance, the word construction form of ?IL-2?
would become ?IDASH-ALPNUM?.
? word characteristics: This feature appears if a word represents a DNA
sequence of ?A?,?C?,?G?,?T? or Greek letter such as beta or alpha, ordinal
index such as I, II or unit such as BU/ml, micron/mL. It is encoded with
?ACGT?, ?GREEK?, ?INDEX?, ?UNIT?.
? head/modifying information: If a word prefers the rightmost position
of terminologies, we regard it has the property of a head noun. On the
other hand, if a word frequently occurs in other positions, we regard it has
the property of a modifying noun. It can help to establish the beginning
and ending point of multi-word entities. We automatically extract 4,382
head nouns and 7,072 modifying nouns from the training data as shown in
Table 2.
? chunk-type information: This feature is also effective in determining the
position of a word in NEs, ?B?, ?I?, ?O? which means ?begin chunk?, ?in
chunk? and ?others?, respectively. We consider the chunk type of a target
word and the conjunction of the current chunk type and the POS of the
previous word to represent the structure of an NE.
We also tested an ME-based model for boundary detection. For this, we add
two special features : previous state (label) and conjunction of previous label
1 ?IDASH? (inter dash), ?EDASH? (end dash), ?SDASH? (start dash),
?CAP?(capitalization), ?LOW?(lowercase), ?MIX?(lowercase and capitaliza-
tion letters), ?NUM?(digit), ?ALPNUM?(alpha-numeric), ?SYM?(symbol),
?PUNC?(punctuation),and ?COMMA?(comma)
Two-Phase Biomedical Named Entity Recognition 651
Table 2. Examples of Head/Modifying Nouns
Modifying Nouns Head Nouns
nf-kappa cytokines
nuclear elements
activated assays
normal complexes
phorbol macrophages
viral molecules
inflammatory pathways
murine extracts
electrophoretic glucocorticoids
acute levels
intracellular responses
epstein-barr clones
cytoplasmic motifs
and current word to consider state transition. That is, a previous label can be
represented as a feature function in our model as follows:
fi(h, o) =
{
1 if pre label+tw=B+gene,o=I
0 otherwise
(5)
It means that the target word is likely to be inside a term (I), when the word
is ?gene? and the previous label is ?B?. In our model, the current label is de-
terministically assigned to the target word with considering the previous state
with the highest probability.
2.3 Features for Semantic Labeling
Table 3 shows features for semantic labeling with respect to recognized NEs.
? word contextual feature: We make use of three kinds of internal and ex-
ternal contextual features: words within identified NEs, their word normal-
ization forms, and words surrounding the NEs. In Table 3, NEw0 denotes
the rightmost word in an identified NE region. Moreover, the presence of
specific head nouns acting as functional words takes precedence when de-
termining the term class, even though many terms do not contain explicit
term category information. For example, functional words, such as ?factor?,
?receptor?, and ?protein? are very useful in determining protein class, and
?gene?, ?promoter?, and ?motif ? are clues for classifying DNA [5]. In gen-
eral, such functional words are often the last word of an entity. This is the
reason we consider the position where a word occurs in NEs along with the
word. For inside context features, we use non-positional word features as
well. As non-positional features, all words inside NEs are used.
? internal bigrams and trigrams: We consider the rightmost bigrams/
trigrams inside identified NEs and the normalized bigrams/trigrams.
652 S. Kim et al
Table 3. Feature Set for Semantic Classification
Feature description
Word Features (positional) NEwothers , NEw?3 , NEw?2 , NEw?1 , NEw0
Word Features (non-positional) AllNEw
Word Normalization (positional) WFNEw?3 , WFNEw?2 , WFNEw?1 , WFNEw0
Left Context(Words Surrounding an NE) LCW?2, LCW?1
Right Context RCW+1, RCW+2
Internal Bigrams NEw?1 + NEw0
Internal Trigrams NEw?2 + NEw?1 + NEw0
Normalized Internal Bigrams WFNEw?1 + WFNEw0
Normalized Internal Trigrams NEw?2 + NEw?1 + NEw0
IDASH-word related Bigrams/Trigrams
Keyword KEYWORD(NEi)
? IDASH-word related bigrams/trigrams: This feature appears if NEw0
or NEw?1 contains dash characters. In this case, the bigram/trigram are
additionally formed by removing all dashes from the spelling. It is useful to
deal with lexical variants.
? keywords: This feature appears if the identified NE is informative key-
word with respect to a specific class. The keywords set comprises terms
obtained by the relative entropy between general and biomedical domain
corpora.
3 Rule-Based Postprocessing
A rule-based method can be used to correct errors by NER based on machine
learning. For example, the CRFs tag ?IL-2 receptor expression? as ?B I I?,
since the NEs ended with ?receptor expression? in training data almost belong
to ?other name? class even if the NEs ended with ?receptor? belong to ?pro-
tein? class. It should be actually tagged as ?B I O?. That kind of errors is
caused mainly by the cascaded phenomenon in biomedical names. Since our sys-
tem considers all NEs belonging to other classes in the recognition phase, it
tends to recognize the longest ones. That is, in the term classification phase,
such NEs are classified as ?other? class and are ignored. Thus, the system
losts embedded NEs although the training and evaluation set in fact tends to
consider only the embedded NE when the embedded one is more meaningful
or important.
This error correction is conducted by the rule-based method, i.e. If condi-
tion THEN action. For example, the rule ?IF wi?2=IL-2, wi?1=receptor and
wi=expression THEN replace the tag of wi with O? can be applied for the above
case. We use a finite state transducer for this rule-based transformation, which
is easy to understand with given lexical rules, and very efficient. Rules used for
the FST are acquired from the GENIA corpus. We first retrieved all NEs in-
cluding embedded NEs and longest NEs from GENIA 3.02 corpus and change
Two-Phase Biomedical Named Entity Recognition 653
IL-2/B gene/I
IL-2/O gene/O expression/O
Fig. 1. Non-Deterministic FST
IL-2/? gene/ ?
expression/OOO
? /BI
Fig. 2. Deterministic FST
the outputs of all other classes except the target 5 classes to O. That is, the
input of FST is a sequence of words in a sentence and the output is categories
corresponding to the words.
Then, we removed the rules in conflict with NE information from the training
corpus. These rules are non-deterministic (Figure 1), and we can change it to
the deterministic FST (Figure 2) since the lengths of NEs are finite. The deter-
ministic FST is made by defining the final output function for the deterministic
behavior of the transducer, delaying the output. The deterministic FST is de-
fined as follows: (?1, ?2, Q, i, F, ?, ?, ?), where ?1 is a finite input alphabet; ?2
is a finite output alphabet; Q is a finite set of states or vertices; i ? Q is the
initial state; F ? Q is the set of final states; ? is the deterministic state transi-
tion function that maps Q ? ?1 on Q; ? is the deterministic emission function
that maps Q ? ?1 on ??2 and ? : F ? ??2 is the final output function for the
deterministic behavior of the transducer.
4 Evaluation
4.1 Experimental Environments
In the shared task, only biomedical named entities which belong to 5 specific
classes are annotated in the given training data. That is, terms belonging to
other classes in GENIA are excluded from the recognition target. However, we
consider all NEs in the boundary detection step since we separate the NER
task into two phases. Thus, in order to utilize other class terms, we additionally
annotated ?O? class words in the training data where they corresponds to other
classes such as other organic compound, lipid, and multi cell in GENIA 3.02p
version corpus. During the annotation, we only consider the longest NEs on
654 S. Kim et al
Table 4. Number of training examples
RNA DNA cell line cell type protein other
472 5,370 2,236 2,084 16,042 11,475
GENIA. As a consequence, we find all biomedical named entities in text at the
term detection phase. Then, biomedical NEs classified as other class are changed
to O at the semantic labeling phase. The total words that belong to other class
turned out to be 25,987. Table 4 shows the number of NEs with respect to each
class on the training data. In our experiments, a quasi-Newton method called the
L-BFGS with Gaussian Prior smoothing is applied for parameter estimation [2].
4.2 Experimental Results
Table 5 shows the overall performance on the evaluation data. Our system
achieves an F-score of 71.19%. As shown in the table, the performance of NER
for cell line class was not good, because its boundary recognition is not so good
as other classes. Also, Table 6 shows the results of semantic classification. In par-
ticular, the system often confuses protein with DNA, and cell line with cell type.
Among the correctly identified 7,093 terms, 790 terms were misclassified.
Table 7 shows the performance of each phase. Our system obtains 76.88%
F-score in the boundary detection task and, using 100% correctly recognized
terms from annotated test data, 90.54% F-score in the semantic classification
task. Currently, since we cannot directly assess the accuracy of the term detection
process on the evaluation set because of other class words, the 75% of the training
data were used for training and the rest for testing.
Table 5. Overall performance on the evaluation data
Fully Correct Left Correct Right Correct
Class Recall Precision F-score F-score F-score
protein 76.30 69.71 72.85 77.60 79.15
DNA 67.80 64.91 66.33 68.36 74.57
RNA 73.73 63.04 67.97 71.09 74.22
cell line 57.40 54.88 56.11 59.04 65.69
cell type 70.12 77.64 73.69 74.89 81.51
overall 72.77 69.68 71.19 74.75 78.23
Table 6. Confusion matrix over evaluation data
gold/sys protein DNA RNA cell line cell type other
protein 0 72 3 1 4 267
DNA 97 0 0 0 0 49
RNA 11 0 0 0 0 0
cell line 10 1 0 0 63 37
cell type 21 0 0 92 0 57
Two-Phase Biomedical Named Entity Recognition 655
Table 7. Performance of term detection and semantic classification
Recall Precision F-score
term detection (MEMarkov) 74.03 75.31 74.67
term detection (CRF) 76.14 77.64 76.88
semantic classification 87.50 93.81 90.54
overall NER 72.77 69.68 71.19
Table 8. Performance of NE recognition methods (one-phase vs. two-phase)
method Recall Precision F-score
one-phase 64.23 63.13 63.68
two-phase(baseline2) 66.24 64.54 65.38
(only 5 classes)
two-phase(baseline2) 68.51 67.58 68.04
(5 classes+other class)
Also, we compared our model with the one-phase model. The detailed results
are presented in Table 8. Both of them have pros and cons. The best-reported
system presented by [13] uses one-phase strategy. In our evaluation, the two-
phase method shows a better result than the one-phase method, although direct
comparison is not possible since we tested with a maximum entropy based expo-
nential models in all cases. The features for one-phase method are identical with
the recognition features except that the local context of a word is extended as
previous 4 words and next 4 words. In addition, we investigate whether the con-
sideration of ?other? class words is helpful in the recognition performance. Table
8 shows explicit annotations of other NE classes much improve the performance
of existing entity types.
In the next experiment, we test how individual methods have an effect on the
performance in the term detection step. Table 9 shows the results obtained by com-
bining different methods in the NER process. At the semantic labeling phase, all
methods employed the ME model using the features described in 2.3. Baseline1
is the two-phase ME model which restrict the inspection of NE candidates to the
NPs which include at least one biomedical salient word. Baseline2 is the two-phase
ME model considering all words. In order to retrieve domain salient words, we
utilized a relative frequency ratio of word distribution in the domain corpus and
that in the general corpus [10]. We used the Penn II raw corpus as out-of-domain
corpus. Both models do not use the features related to previous labels. As a re-
sult, usage of salient words decrease the performance and it only speeds up the
training process. Baseline2+FST indicates boundary extension/contraction using
FST are applied as postprocessing step in baseline2 recognition. In addition, we
compared use of CRFs and ME with Markov process features. For this, we added
features of previous labels to the feature set for ME. Baseline2+MEMarkov is the
two-phase ME model considering all features including previous label related fea-
tures. Baseline2+CRF is a model exploiting CRFs and baseline2+CRF+FST is a
model using CRFand FST as postprocessing.As shown in Table 9, the CRFs based
656 S. Kim et al
Table 9. F-score for different methods
Method Recall Precision F-score
baseline1(salientNP ) 66.21 66.34 66.27
baseline2(all) 68.51 67.58 68.04
baseline2 + FST 68.89 68.53 68.71
baseline2 + MEMarkov 70.30 67.65 68.95
baseline2 + MEMarkov + FST 70.61 68.40 69.49
baseline2 + CRF 72.44 68.77 70.56
baseline2 + CRF + FST 72.77 69.68 71.19
Table 10. Comparisons with other systems
System Precision Recall F-score
Zhou et. al (2004) 69.42 75.99 72.55
Our system 72.77 69.68 71.19
Finkel et. al (2004) 71.62 68.56 70.06
Settles (2004) 70.0 69.0 69.5
model outperforms the ME based model. Our system reached F-score 71.19% on
the baseline2 + CRF + FST model.
Table 10 shows the comparison with top-ranked systems in JNLPBA 2004
shared task. The top-ranked systems made use of external knowledge from
gazetteers and abbreviation handling routines, which were reported to be ef-
fective. Zhou et. al reported the usage of gazetteers and abbreviation handling
improves the performance of the NER system by 4.8% in F-score [13]. Finkel
et. al made use of a number of external resources, including gazetteers, web-
querying, use of the surrounding abstract, abbreviation handling, and frequency
counts from BNC corpus [4]. Settles utilized semantic domain knowledge of 17
kinds of lexicons [11]. Although the performance of our system is a bit lower than
the best system, the results are very promising since most systems use external
gazetteers, and abbreviation and conjunction/disjunction handling scheme. This
suggests areas for further work.
5 Conclusion and Discussion
We presented a two-phase biomedical NE recognition model, term boundary
detection and semantic labeling. We proposed two exponential models for each
phase. That is, CRFs are used for term detection phase including Markov process
and ME is used for semantic labeling. The benefit of dividing the whole process
into two processes is that, by separating the processes with different characteris-
tics, we can select separately the discriminative feature set for each subtask, and
moreover measure effectiveness of models at each phase. Furthermore, we use
the rule-based method as postprocessing to refine the result. The rules are ex-
tracted from the GENIA corpus, which is represented by the deterministic FST.
The rule-based approach is effective to correct errors by cascading structures
Two-Phase Biomedical Named Entity Recognition 657
of biomedical NEs. The experimental results are quite promising. The system
achieved 71.19% F-score without Gazetteers or abbreviation handling process.
The performance could be improved by utilizing lexical database and testing
various classification models.
Acknowledgements
This work was supported by Korea Research Foundation Grant, KRF-2004-037-
D00017.
References
1. Thorten Brants. TnT A Statistical Part-of-Speech Tagger. In Proceedings of the
6th Applied Natural Language Processing.; 2000.
2. Stanley F. Chen and Ronald Rosenfeld. A Gaussian prior for smoothing maximum
entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.
3. Nigel Collier, Chikashi Nobata and Jun-ichi Tsujii. Extracting the Names of Genes
and Gene Products with a Hidden Markov Model. In Proceedings of COLING 2000;
201-207.
4. Jenny Finkel, Shipra Dingare, and Huy Nguyen. Exploiting Context for Biomedical
Entity Recognition From Syntax to thw Web. In Proceedings of JNLPBA/BioNLP
2004; 88-91.
5. K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. Toward information extrac-
tion: identifying protein names from biological papers. In Proceedins of the Pacific
Symposium on Biocomputing 98; 707-718.
6. Junichi Kazama, Takaki Makino, Yoshihiro Ohta and Junichi Tsujii. Tuning Sup-
port Vector Machines for Biomedical Named Entity Recognition, Proceedings of the
ACL Workshop on Natural Language Processing in the Biomedical Domain 2002;
1-8.
7. Michael Krauthammer and Goran Nenadic. Term Identification in the Biomedical
literature. Journal of Biomedical Informatics. 2004; 37(6):512-526.
8. John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional Random
Fields: probabilistic models for segmenting and labeling sequence data. In Proceed-
ings of ICML-01; 282-289.
9. Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, Hae-Chang Rim. Biomedical
named entity recognition using two-phase model based on SVMs. Journal of
Biomedical Informatics 2004; 37(6):436-447.
10. Kyung-Mi Park, Seonho Kim, Ki-Joong Lee, Do-Gil Lee, and Hae-Chang Rim.
Incorportating Lexical Knowledge into Biomedical NE Recognition. In Proceedings
of Natural Language Processing in Biomedicine and its Applications Post-COLING
Workshop 2004; 76-79.
11. Burr Settles. Biomedical Named Entity Recognition Using Conditional Random
Fields and Rich Feature Sets. In Proceedings of JNLPBA/BioNLP 2004; 104-107.
12. Olivia Tuason, Lifeng Chen, Hongfang Liu, Judith A. Blake, Carol Friedman. Bi-
ological Nomenclatures: A Source of Lexical Knowledge and Ambiguity. In Pacific
Symposium on Biocomputing 2004; 238-249.
13. GuoDong Zhou, Jie Zhang, Jian Su, Chew-Lim Tan. Exploring Deep Knowledge
Resources in Biomedical Name Recognition. In Proceedings of JNLPBA/BioNLP
2004; 99-102.
 
	ff