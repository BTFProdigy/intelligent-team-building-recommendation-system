Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 278?283,
Dublin, Ireland, August 23-24, 2014.
ezDI: A Hybrid CRF and SVM based Model for Detecting and Encoding
Disorder Mentions in Clinical Notes
Parth Pathak, Pinal Patel, Vishal Panchal, Narayan Choudhary, Amrish Patel, Gautam Joshi
ezDI, LLC.
{parth.p,pinal.p,vishal.p,narayan.c,amrish.p,gautam.j}@ezdi.us
Abstract
This paper describes the system used
in Task-7 (Analysis of Clinical Text) of
SemEval-2014 for detecting disorder men-
tions and associating them with their re-
lated CUI of UMLS
1
. For Task-A, a CRF
based sequencing algorithm was used to
find different medical entities and a binary
SVM classifier was used to find relation-
ship between entities. For Task-B, a dic-
tionary look-up algorithm on a customized
UMLS-2012 dictionary was used to find
relative CUI for a given disorder mention.
The system achieved F-score of 0.714 for
Task A & accuracy of 0.599 for Task B
when trained only on training data set, and
it achieved F-score of 0.755 for Task A &
accuracy of 0.646 for Task B when trained
on both training as well as development
data set. Our system was placed 3rd for
both task A and B.
1 Introduction
A clinical document contains plethora of informa-
tion regarding patient?s medical condition in un-
structured format. So a sophisticated NLP sys-
tem built specifically for clinical domain can be
very useful in many different clinical applications.
In recent years, clinical NLP has gained a lot
of significance in research community because it
contains challenging tasks such as medical entity
recognition, abbreviation disambiguation, inter-
conceptual relationship detection, anaphora res-
olution, and text summarization. Clinical NLP
has also gained a significant attraction among the
1
http://www.nlm.nih.gov/research/umls/
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
health care industry because it promises to de-
liver applications like computer assisted coding,
automated data abstraction, core/quality measure
monitoring, fraud detection, revenue loss preven-
tion system, clinical document improvement sys-
tem and so on.
Task-7 of SemEval-2014 was in continuation
of the 2013 ShaRe/CLEF Task-1 (Sameer Prad-
han, et al., 2013). This task was about finding
disorder mentions from the clinical text and as-
sociating them with their related CUIs (concept
unique identifiers) as given in the UMLS (Unified
Medical Language System). UMLS is the largest
available medical knowledge resource. It contains
2,885,877 different CUIs having 6,497,937 differ-
ent medical terms from over 100 different medi-
cal vocabularies. Finding accurate CUIs from free
clinical text can be very helpful in many healthcare
applications. Our aim for participating in this task
was to explore new techniques of finding CUIs
from clinical document.
Over the last few years many different Clin-
ical NLP systems like cTAKES (Savova, Guer-
gana K., et al., 2010), MetaMap (A. Aronson,
2001), MedLEE (C. Friedman et al., 1994) have
been developed to extract medical concepts from
a clinical document. Most of these systems focus
on rule based, medical knowledge driven dictio-
nary look-up approaches. In very recent past, a
few attempts have been made to use supervised or
semi-supervised learning models. In 2009, Yefang
Wang (Wang et al., 2009) used cascading clas-
sifiers on manually annotated data which fetched
F-score of 0.832. In 2010, i2b2 shared task chal-
lenge focused on finding test, treatment and prob-
lem mentions from clinical document.
In 2013, ShARe/CLEF task focused on finding
disorder mentions from clinical document and as-
signing relevant CUI code to it. In both i2b2 task
and ShaRe/CLEF task most of the systems used
either supervised or semi-supervised learning ap-
278
proaches.
In this paper we have proposed a hybrid super-
vised learning approach based on CRF and SVM
to find out disorder mentions from clinical doc-
uments and a dictionary look-up approach on a
customized UMLS meta-thesaurus to find corre-
sponding CUI.
2 Data
The SemEval-2014 corpus comprises of de-
identified plain text from MIMIC
2
version 2.5
database. A disorder mention was defined as any
span of text which can be mapped to a concept
in UMLS and which belongs to the Disorder se-
mantic group. There were 431 notes extracted
from intensive care unit having various clinical
report types (like radiology, discharge summary,
echocardiogram and ECG), out of which 99 notes
were used in development data set, 199 notes were
used in training data set and 133 notes were used
in testing data set.
Preliminary analysis on this data showed that
number of sentences in training documents were
comparatively smaller than the development or
test data set (Table 1). Number of disorder men-
tions were also significantly lower in training data
set than in development data set (Table 1).
Type Dev Train Test
Docuemnts 99 199 133
Sentence 9860 10485 17368
Token 102k 113k 177k
Avg token/sen 10.42 10.79 10.24
Cont. entity 4912 5,165 7,186
Disjoint Entity 439 651 4588
Avg Ent/Doc 54.05 29.22 57.47
Distinct CUI 1007 938 NA
Table 1: Numerical analysis on data.
3 System Design
Analysis of Task-A showed that disorder men-
tions also contain other UMLS semantic types like
findings, anatomical sites and modifiers (Table 2).
So we divided the task of finding disorder men-
tion in to two subtasks. First a CRF based se-
quencing model was used to find different disorder
mentions, modifiers, anatomical sites and findings.
2
http://mimic.physionet.org/database/
releases/70-version-25.html
Then a binary SVM classifier was used to check
if relationship exists between a disorder and other
types of entities or not.
Example D
i
s
o
r
d
e
r
F
i
n
d
i
n
g
s
A
n
a
t
o
m
y
M
o
d
i
fi
e
r
There is persistent
left lower lobe opacity
presumably atelectasis.
3 3 7 7
He had substernal chest
pain, sharp but without
radiation.
3 3 7 7
Patientt also developed
some erythema around
the stoma site on
hospital day two.
3 7 3 7
The tricuspid valve
leaflets are mildly thick-
ened.
7 3 3 7
Please call,if you find
swelling in the wound.
3 3 7 7
She also notes new sharp
pain in left shoulder
blade/back area.
3 7 3 7
An echocardiogram
demonstrated mild
left and right atrial
dilatation
3 7 7 3
Table 2: Entity Types co-relation and examples
For Task-B, we have used a simple dictionary
look up algorithm on a customized UMLS dictio-
nary. A preliminary analysis of UMLS entities in
general show that a single disorder mention may
consist of various types of linguistic phrases. It is
not necesarry that the system to detect these enti-
ties as a single phrase. The entities and their re-
lations may also occur in disjoint phrases as well.
Our analysis of the disorder entities inside UMLS
reveals that out of a total 278,859 disorders (based
on SNOMED-CT library), 96,069 are such that
can be broken down into more than one phrase,
which is roughly 1/3 of total number of disorders
in the UMLS.
3.1 System Workflow
The Work-flow of the system is as follow:
279
Figure 1: System Workflow
3.1.1 Pre-processing
All the clinical documents used in this task
were de-identified. So information related to
hospital name, patient demographics, physician
names/signatures, dates, places, and certain lab-
data were converted into predefined patterns.
These patterns were hindering the flow of natu-
ral language. As a result of it, we were unable to
get accurate results for PoS tagging and chunking
of the sentences. So we replaced all of these de-
identified patterns with some text that appear more
as natural language. There were also some head-
ers and footers associated with all the documents,
which were actually irrelevant to this task. There-
fore all headers and footers were also removed at
the pre-processing level.
3.1.2 openNLP
We have used openNLP
3
to perform basic NLP
tasks like sentence detection, tokenizing, PoS tag-
ging, chunking, parsing and stemming.
3.1.3 Dictionary Lookup
UMLS 2012AA dictionary with Lexical Variant
Generator (LVG)
4
was used to perform dictionary
lookup task. Even though the task was only about
finding disorder mentions, we also identified en-
tities like procedures, finding, lab data, medicine,
anatomical site and medical devices to be used as
features in our CRF model. This was helpful in
decreasing the number of false positive. UMLS
TUI (Type Unique Identifier) used for different
entity type is described in Table 3. A rule-based
approach on the output of the OpenNLP syntac-
tic parser was used to detect possible modifiers for
disorder mentions.
Type Tui list
Disorder
T046,T047,T048,T049,T050,T191,
T037,T019,T184
Anatomical
Sites
T017,T021,T023,T024,T025,T026,
T029,T030
Procedures T059,T060,T061
Medicines T200,T120,T110
Lab Data T196,T119
Modifiers Customized Dictionary
Findings
T033,T034,T041,T084,T032,T201,
T053,T054
Table 3: Entity Types and their related TUI list
from UMLS
3.1.4 CRF Feature Generation
The feature sets were divided into three categories.
1) Clinical Features
i) Section Headers: A clinical note is often di-
vided into relevant segments called Section Head-
ers. These section headers provide very useful
information at the discourse level. Same section
header can have multiple variants. For example
History of Present Illness can also be written as
HPI, HPIS, Brief History etc. We have created a
dictionary of more than 550 different section head-
ers and classified them into more than 40 hierar-
chical categories. But using only section header
dictionary for classification can fetch many false
3
https://opennlp.apache.org/
4
http://lexsrv2.nlm.nih.gov/
280
positives. Section header always appears in a pre-
defined similar sequences. So to remove these
false positives, we have used a Hidden Markov
Model(HMM) (Parth Pathak, et al, 2013). For this
task, we have used unigram section header id as a
feature for all the tokens in CRF.
ii) Dictionary Lookup: A binary feature was
used for all the different entity types detected from
UMLS dictionary from last pipeline.
iii) Abbreviations: Abbreviations Disambigua-
tion is one of the most challenging tasks in clinical
NLP. The primary reason for the same is a lack of
dictionary which contains most of the valid list of
abbreviations. For this task, we have used LRABR
as base dictionary to find out all the possible ab-
breviations and on top of that, a binary SVM clas-
sifier was used to check if the given abbreviation
has medical sense or not.
2) Textual Feature:
Snowball stemmer
5
was used to find out stem
value of all the word tokens. Prefix and suffix of
length 2 to 5 were also used as features. Different
orthographic features like whole word capital, first
char capital, numeric values, dates, words contain-
ing hyphen or slash, medical units (mg/gram/ltr
etc.) were used as features.
3) Syntactic Features:
Different linguistic features like PoS tags and
chunks for each token were used. We have also
used head of the noun phrase as one of the feature
which can be very helpful in detecting the type of
an entity.
3.1.5 CRF toolkit
All the annotated data was converted into BIO
sequencing format. CRF++
6
toolkit was used to
train and predict the model.
3.1.6 SVM
SVM was used to check whether a relationship ex-
ists between two entities or not. For this purpose
all the tokens between these two entities, their part
of speech tags and chunks were used as features.
Rules based on output of a syntactic parser were
also used as a binary feature. Some orthographic
features like all letter capital, contains colon (:),
contains semi colon (;), were also used as features.
LibSVM
7
was used to train as well as predict the
5
http://snowball.tartarus.org/
6
http://crfpp.googlecode.com/
7
http://www.csie.ntu.edu.tw/\
?
cjlin/
libsvm/
model.
3.1.7 Dictionary Look-up for CUI detection
For a better mapping of the entities detected by
NLP inside the given input text, we found it to
be a better approach to divide the UMLS enti-
ties into various phrases. This was done semi-
automatically by splitting the strings based on
function words such as prepositions, particles and
non-nominal word classes such as verbs, adjec-
tives and adverbs. While most of the disorder enti-
ties in UMLS can be contained into a single noun
phrase (NP) there are also quite a few that contain
multiple NPs related with prepositional phrases
(PPs), verb phrases (VPs) and adjectival phrases
(ADJPs).
This task gave us a modified version of the
UMLS disorder entities along with their CUIs.
The following table (Table 4) gives a snapshot
of what this customized UMLS dictionary looked
like.
CUI Text P1 P2 P3
C001
3132
Dribbling
from
mouth
Dribbling from mouth
C001
4591
Bleeding
from nose
Bleeding from nose
C002
9163
Hemorr-
hage from
mouth
Hemo-
rrhage
from mouth
C039
2685
Chest pain
at rest
Chest pain at rest
C026
9678
Fatigue
during
pregnancy
Fatigue during
pregn
ancy
Table 4: An example of the modified UMLS disor-
der entities split as per their linguistic phrase types
Our dictionary look-up algorithm used this cus-
tomized UMLS dictionary as resource to find the
entities and assign the right CUIs.
4 Results & Error Analysis
4.1 Evaluation Calculations
The evaluation measures for Task A are Precision,
Recall and F-Meas, defined as:
Precision =
TP
FP+TP
Recall =
TP
TP+FN
281
F-measure =
2?Precision?Recall
Precision+Recall
where
TP = Disorder mention span matches with gold
standard
FP = Disorder mention span detected by the
system was not present in the gold standard;
FN = Disorder mention span was present in the
gold standard but system was not able detect it
In Task B, the Accuracy was defined as the
number of pre-annotated spans with correctly
generated code divided by the total number of
pre-annotated spans.
Strict Accuracy =
Total correct CUIs
Total annotation in gold standard
Relaxed Accuracy =
Total correct CUIs
Total span detected by system
4.2 System Accuracy
The system results were calculated on two dif-
ferent runs. For the first evaluation, only training
data was used for the training purpose while for
the second evaluation, both the training as well as
the development data sets were used for training
purpose. The results for Task A and B are as
follows:
Precision Recall F-Meas
Strict
(T)
0.750 0.682 0.714
Relaxed
(T)
0.915 0.827 0.869
Strict
(T+D)
0.770 0.740 0.755
Relaxed
(T+D)
0.911 0.887 0.899
Table 5: Task-A Results
where T= Training Data set
D= Development Data set
4.3 Error Analysis
Error Analysis on training data revealed that for
Task-A our system got poor results in detecting
non-contiguous disjoint entities. Our system also
performed very poorly in identifying abbrevia-
tions and misspelled entities. We also observed
Accuracy
Strict
(T)
0.599
Relaxed
(T)
0.878
Strict
(T+D)
0.643
Relaxed
(T+D) 0.868
Table 6: Task-B Results
that the accuracy of the part of speech tagger and
the chunker also contributes a lot towards the final
outcome. For Task-B, we got many false positives.
Many CUIs which we identified from the UMLS
were not actually annotated.
5 Conclusion
In this paper we have proposed a CRF and SVM
based hybrid approach to find Disorder mentions
from a given clinical text and a novel dictio-
nary look-up approach for discovering CUIs from
UMLS meta-thesaurus. Our system did produce
competitive results and was third best among the
participants of this task. In future, we would like
to explore semi-supervised learning approaches to
take advantage of large amount of available un-
annotated free clinical text.
References
Savova, Guergana K., James J. Masanz, Philip V.
Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C.
Kipper-Schuler, and Christopher G. Chute. 2010.
Mayo clinical Text Analysis and Knowledge Ex-
traction System (cTAKES): architecture, component
evaluation and applications. Journal of the Amer-
ican Medical Informatics Association 17, no. 5
(2010): 507-513.
Friedman C, Alderson PO, Austin JH, Cimino JJ, John-
son SB. 1994. A general natural-language text pro-
cessor for clinical radiology. J Am Med Inform As-
soc 1994 Mar-Apr;1(2):16174. [PubMed:7719797]
Aronson, Alan R. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of the AMIA Symposium,
p. 17. American Medical Informatics Association,
2001.
Wang, Yefeng, and Jon Patrick. 2009. Cascading clas-
sifiers for named entity recognition in clinical notes.
In Proceedings of the workshop on biomedical infor-
mation extraction, pp. 42-49. Association for Com-
putational Linguistics, 2009.
282
Suominen, Hanna, Sanna Salanter, Sumithra Velupil-
lai, Wendy W. Chapman, Guergana Savova, Noemie
Elhadad, Sameer Pradhan 2013 Overview of the
ShARe/CLEF eHealth evaluation lab 2013. In In-
formation Access Evaluation. Multilinguality, Mul-
timodality, and Visualization, pp. 212-231. Springer
Berlin Heidelberg, 2013.
. ??
Parth Pathak, Raxit Goswami, Gautam Joshi, Pinal Pa-
tel, and Amrish Patel. 2013 CRF-based Clinical
Named Entity Recognition using clinical Features
283
LAW VIII - The 8th Linguistic Annotation Workshop, pages 87?92,
Dublin, Ireland, August 23-24 2014.
 
 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details:http://creativecommons.org/licenses/by/4.0/ 
  
Annotating a Large Representative Corpus of Clinical Notes for Parts 
of Speech 
Narayan Choudhary, Parth Pathak, Pinal Patel, Vishal Panchal 
ezDI, LLC. 
{narayan.c, parth.p, pinal.p, vishal.p}@ezdi.us 
Abstract 
[We report of the procedures of developing a large representative corpus of 50,000 sentences 
taken from clinical notes. Previous reports of annotated corpus of clinical notes have been 
small and they do not represent the whole domain of clinical notes. The sentences included in 
this corpus have been selected from a very large raw corpus of ten thousand documents. These 
ten thousand documents are sampled from an internal repository of more than 700,000 docu-
ments taken from multiple health care providers. Each of the documents is de-identified to re-
move any PHI data. Using the Penn Treebank tagging guidelines with a bit of modifications, 
we annotate this corpus manually with an average inter-annotator agreement of more than 
98%. The goal is to create a parts of speech annotated corpus in the clinical domain that is 
comparable to the Penn Treebank and also represents the totality of the contemporary text as 
used in the clinical domain. We also report the output of the TnT tagger trained on the initial 
21,000 annotated sentences reaching a preliminary accuracy of above 96%.] 
1 Introduction 
Automated parts of speech (PoS) annotation have been an active field of research for more than 40 
years now. Obviously, there are quite a few of tools already available with an impressive accuracy re-
turns (Toutanova et al, 2003; Shen et al. , 2007; Spoustov?a et al., 2009; S?gaard, 2010). This is true in 
the general domain text such as news reports or general domain articles. But when it comes to a niche 
area like clinical domain, no automated parts of speech taggers are readily available nor has there been 
any report of any such large corpus developed that meet the standards as set out in the general domain.  
Interest has grown now as NLP is sought after in the clinical domain, particularly for the task of in-
formation extraction from clinical notes. 
There have been previous attempts for creating PoS annotated corpus in the clinical domain (Tateisi 
et al., 2004; Pakhomov et al, 2006; Albright et al., 2013). All of these corpora are relatively small and 
the PoS taggers trained on them have not been shown to reach above 96% in the clinical domain. At-
tempts at adapting a general domain PoS tagger to work better for clinical domain include Easy Adapt 
(Daum? H., 2007) and ClinAdapt (Ferraro et al., 2013). But none of these two adaptation methods en-
hance the accuracy levels to more than 95%. 
Given that the text in clinical notes is radically different from what appears in the general domain, 
the general domain English PoS tagger models do not perform well on the clinical text. Our experi-
ments with three such general domain taggers, namely Charniak (Charniak and Johnson, 2005), Stan-
ford (Klein and Manning, 2003) and OpenNLP, yielded not more than 95% accuracy. This motivated 
us to take a radical step of developing a fresh parts of speech annotated corpus comparable to the Penn 
Treebank. Well, we are aware that it is going to take a lot of money, time and effort. But we also be-
lieve that it is necessary if we need better NLP tools for this domain. 
2 The Representative Corpus  
To ensure that we have a representative corpus, we sampled a corpus of more than 750,000 documents 
from 119 providers (hospitals and specialty clinics). The biggest challenge was to take a representative 
sample of documents from various specialties and different work types. Thanks to the metadata infor-
mation available in our internal repository, this was solved in a rather easier way although we did need 
to look for information on classification and sub-classification of the domain manually. 
87
2.1 Sampling Task 
Out of these 750,000 documents, we selected only ten hospitals for document sampling as they were 
large providers with a greater number of note count and provided a diversity of the specialty doc-
tors/providers dictating the clinical notes.  These ten hospitals amounted to a total of 237,110 docu-
ments written by 508 doctors of roughly 97 clinical specialties. A summary of this is given in Appen-
dix A. 
2.2 Sentence Clustering 
We have used 237,110 documents for the process of selecting the sample sentences undergoing the 
PoS annotation. All of these documents were classified into different categories based on their work 
types (operative notes, admission notes, discharge summaries etc.), service line (cardiology, oncology, 
medicine, ambulatory etc.), section headers (History of Present Illness, Chief Complaints, Physical 
Examination, Laboratory Data etc.). Based on this classification, we have selected a sample of 10,000 
documents fairly representing the 237,110 documents selected in the first phase. 
These 10,000 documents were parsed using the Charniak?s full syntactic parser (Charniak & John-
son, 2005). After some modifications, the Charniak parser on clinical data gives an accuracy of about 
95% at the PoS level. A graph based string similarity algorithm was used to find similar sentences 
from these 10,000 documents. A summary of what it yielded is as follows: 
Total Number of Sentences: 704,271 
Total Number of Unique Sentences:  365,518 
Total Number of Unique Patterns:  234,909 
The unique patterns were clustered together using a hierarchical clustering algorithm. Patterns were 
grouped together by calculating the Euclidean distance with a threshold similarity of 80 or more. Fol-
lowing this method, we got a total of 3,768 patterns that represented all of the unique patterns. We call 
them pattern heads. 
By giving a proportional weightage to each of these pattern heads as per their occurrence in the 
unique patterns, we derived a total of 56,632 sentences. While no two sentences selected are same, 
about 41% of the patterns in the sample corpus have a frequency of more than 1. 
Appendix B shows example pairs of sentences having the same tag pattern and Appendix C shows 
example pairs of sentences having similar pattern. 
The final selected candidate sentences also contained quite a  few junk sentences (which came of 
course from the clinical notes themselves) or some very frequent smaller patterns (e.g. date patterns), 
we manually removed them to get a total of 49,278 sentences with a total word count of 491,690 and 
an average per sentence word count of 9.97. The greatest number of token for a sentence was found to 
be 221 in the sampled corpus (while the same in the original, actual corpus is 395). 
3 Annotation Method 
As against the common practice of semi-automatic method of annotating text, we purposely chose to 
annotate the text from scratch. It has been reported that tools do affect the decisions of the annotators 
(Marcus et al., 1993). We asked the annotators to use simple notepad and for each of the tokens they 
had to key in the appropriate PoS label. Tokenization and sentence boundary detection were automati-
cally done before it went to the annotators. 
As against the common practice of engaging annotators with a medical background and training 
them into linguistic annotation (Pakhomov et al., 2006; Fan et al. , 2011), we purposely chose to en-
gage linguists and train them into medical language. The annotators were all graduate level researchers 
in linguistics and had a deep knowledge of theoretical syntax. As next step in linguistics analysis after 
PoS tagging is syntactic parsing or chunking, the linguists were also motivated to learn about the goals 
of this task i.e. we informed them about our interests in developing a chunker and a parser afterwards. 
This information helps the annotators to think in terms of making syntactic tree while assigning a PoS 
tag. For example, there is always confusion among the tag pairs IN/RP, VBN/JJ and so on. But if one 
can try drawing a syntactic tree, the confusion gets cleared. While training annotators with medical 
background in linguistics for the task of PoS tagging may seem rather easy, the same cannot be said 
for syntactic tree formation. Besides, the linguists always had the choice of consulting medical experts 
88
(medical coders, medical transcriptionists with more than 5 years of experience) in case any phrase 
had to be explained in terms of its meaning. 
Training sessions were held for linguists for first 15 days during which differences were brought to 
fore and a consensus was reached. This period was strictly for training purposes and text annotated 
during this period was validated more than thrice before getting included in the final corpus. After this 
training period, an inter-annotator agreement round was run with 10,000 sentences distributed to four 
annotators in turn. Each file was annotated by at least two annotators. The differences were then com-
pared and arbitrated by a third annotator who discussed the conflicting cases with the initial annotators 
and brought a consensus among them. 
Inter-annotator agreement at the start of this phase was 93% to 95%. This after a month increased to 
a consistent 97% to 100%. We are at the end of this phase and the accuracy is consistently close to 
99%. Also of note is the fact that apart from the initial 5 days of face-to-face training session, the an-
notators never sit together and they work remotely from the convenience of their location and have a 
flexible time. We also ensured that they do not work long hours at a stretch doing this job as we know 
that this is a tedious job and cannot be done in a hurry. For a full-time annotator, the target goal was 
annotation of 1600 word per day (8 hours) and for the part-time annotators, it was half of that. They 
were always encouraged to come up with any issues for a weekly discussion on the conflicting or con-
fusing cases. 
For the later phases of annotation process, it is ensured that each annotation is validated by at least 
one other annotator. If disagreements arise, arbitration is done by involving a third annotator following 
a discussion. 
As the text might contain tokenization errors, sentence boundary detection errors and other gram-
matical or typographical mistakes, the annotators are asked to document them in a separate spread-
sheet. The sentences themselves are sacrosanct to the annotators and they can at the most make 
changes in separating the hyphenated words if they are not properly hyphenated by the tokenizer and 
document this change. 
4 Annotation Guideline 
Barring a couple of new tags, the annotation guideline largely follows the Penn Treebank PoS annota-
tion guidelines (Santorini, B., 1990) and takes inputs from various other guidelines such as the Penn 
Treebank II parsing guidelines (Bies et al., 1995) and MiPACQ guidelines (Warner et al., 2012). A 
new tag that we have added on top of the Penn tagset looks for marking a difference between the ex-
pletive ?it? and the pronominal ?it? as it helps in tasks like anaphora resolution. The new tag for the 
expletive use of ?it? is given as ?EXP?. The tagset contains a total of 41 tags. The other four tags are 
HYPH, AFX, GW and XX. These tags are well described in the MiPACQ guideline. 
As we have also seen the PoS labels given to the Penn Treebank data, we find that we are differing 
in assigning the tag to some of the words. For example, for the temporal expressions like ?today?, 
?yesterday? and ?tomorrow?, the tag in Penn corpus is invariably NN while we make a difference in 
their adverbial use and nominal use and assign the tag accordingly as ?RB? or ?NN?. 
5 Initial Training Results 
After 4 months of annotation, we achieved a total of 21 thousand sentences annotated. For an experi-
mental run, we trained a tagger to test how far we can go with this data. We implemented a modified 
version of the TnT (Trigram and Tag) (Brants, 2000) algorithm to train a PoS tagger. This tagger was 
given an input of 17,586 sentences containing a total of 158,330 words and was tested against 3,924 
sentences containing a total of 38,143 words. 
Without giving any extra features apart from the ones mentioned in Brants, we got a total of 2,621 
sentences and 36,234 words annotated correctly. That is the TnT out-of-the-box accuracy was 95.00% 
as against the Charniak out-of-the-box accuracy of 91.36%. 
We also compared the same test data against the Charniak parser (without the resource of tag dic-
tionary and the rules). We find that the current tagger was actually performing better. Results im-
proved by 0.33% if we modified the algorithm to handle unknown words using suffixes from the 
medical domain. These suffixes were collected specifically from the medical domain and were such 
for which a single tag could be given. 
89
We also experimented with another method for improvements. This included using a dictionary of 
unambiguous words (words having single tags invariably, for adjectives and verbs only) and resetting 
the emission probability to 1 for them. These two improvement techniques combined enhanced the 
results by 1.24% to push the accuracy to 96.24%. 
Given that a fraction of our corpus is giving us 95% accuracy which is at par with or better than re-
ported anywhere else for PoS tagging task in the domain of clinical NLP, we believe that the results 
should only improve once we increase the training data and apply the improvement techniques avail-
able in the book. 
6 Conclusion 
There is a paucity of good and large enough annotated corpus in the domain of clinical NLP. The ex-
isting corpora are small although extensive analysis has been done on them. Our effort through this 
project is to fill the gap of having a large corpus comparable to the Penn Treebank. 
In this paper we described an ongoing effort to create a sample corpus of clinical notes across most 
of the sub-domains and including all the different types of linguistic styles in this domain. We have 
also used a novel method for creation of a representative corpus which can be said to represent the 
whole of the clinical text in current practice across providers within United States. 
As compared to semi-automated methods of annotation practiced even in big corpus like the Penn 
Treebank, we are following a fully manual process of annotation where the annotators are only given 
contextual information and no other help or props are provided apart from the guidelines to fasten the 
annotation process. We obtain an inter-annotator agreement of 98.93 and we believe that this is the 
best approach to go for this task. 
Using the basic TnT algorithm we also train a tagger using 30% of our data (17,500 sentences) an-
notated in the initial 3 months of the project and achieve a baseline accuracy of 95%. We expect that 
our accuracy should improve to more than 98% once we train the same algorithm on all the 50,000 
annotated sentences. 
After the completion of the project, we may release this corpus for research use. 
Acknowledgements  
We acknowledge the contribution of the linguists at JNU, New Delhi namely, Oinam Nganthoibi, 
Gayetri Thakur, Shiladitya Bhattacharya and Srishti Singh. Thanks also to Suhas Nair and Hiral Dawe 
for their help in making us understand the nuances of the clinical text. We would also like to thank 
Prof. Pushpak Bhattacharya and Dr. Girish Nath Jha for their advice. 
References 
Ann Bies, Mark Ferguson, Karen Katz, Robert MacIntyre, Victoria Tredinnick, Grace Kim, Mary Ann, Mar-
cinkiewicz and Britta Schasberger. 1995. Bracketing Guidelines for Treebank II Style Penn Treebank Project. 
TR, University of Pennsylvania 
Daniel Albright, Arrick Lanfranchi, Anwen Fredriksen, William F Styler IV, Colin Warner, Jena D Hwang, 
Jinho D Choi, Dmit riy  Dligach, Rodney D Nielsen, James Martin, Wayne Ward, Martha Palmer, Guergana K 
Savova. 2013. Towards Comprehensive Syntactic and Semantic Annotations of the Clinical Narrative. J. Am. 
Med. Inform Assoc. 20:922?930 
Thorsten Brants. 2000. TnT: A Statistical Part-of-Speech Tagger. In: Proceedings of the Sixth Conference on 
Applied Natural Language Processing. Pp. 224-231. 
Hal Daum? III. 2007. Frustratingly Easy Domain Adaptation. In: Proceedings of 45th Ann Meeting of the Assoc 
Computational Linguistics, 2007; 45:256?63. 
Eugene Charniak and Mark Johnson. 2005. Coarse-to-Fine n-best Parsing and MaxEnt Discriminative Rerank-
ing. ACL'05. 
Jung-wei Fan, Rashmi Prasad, Romme M. Yabut, Richard M. Loomis, Daniel S. Zisook, John E. Mattison and 
Yang Huang. 2011. Part-of-Speech Tagging for Clinical Text: Wall or Bridge between Institutions? In: AMIA 
Annu Symp Proc 2011 22; 2011:382-91. 
90
Jeffrey  P Ferraro, Hal Daume III, Scott L DuVall, Wendy W. Chapman, Henk Harkema and Peter J Haug. 2013. 
Improving Performance of Natural Language Processing Part-of-Speech Tagging on Clinical Narratives 
through Domain Adaptation. Journal of the American Medical Informat ics Association. 2013; 20:931?939. 
Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In: Proceedings of the 41st 
Meeting of the Association for Computational Linguistics, pp. 423-430. 
Mitchell P. Marcus, Beatrice Santorin i and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank . Computational Linguistics 19: 313?330 
Serguei V. Pakhomov, Anni Coden and Christopher G. Chute. 2006. Developing a Corpus of Clinical Notes 
Manually Annotated for Part of Speech. International Journal of Medical Informat ics. 75(6):418-429 
Beatrice Santorini. 1990. Part-o f-Speech Tagging Guidelines for the Penn Treebank Project. Technical report 
MS-CIS-90-47, Department of Computer and Information Science, University of Pennsylvania. 
Lu i Shen, Giorgio  Satta and Arv ind Joshi, 2007. Guided Learning for Bidirectional Sequence Classification. In: 
ACL 2007. 
Anders S?gaard. 2010. Simple Semi-Supervised Training of Part-of-Speech Taggers. In : Proceedings of the ACL 
2010 Conference Short Papers. 205?208 
Drahomira J. Spoustova, Jan Hajic, Jan Raab and Miroslav Spousta. 2009. Semi-Supervised Training for the Av-
eraged Perceptron POS Tagger. In: Proceedings of the 12th Conference of the European Chapter o f the ACL 
(EACL 2009). 763?771 
Yuka Tateisi and Jun'ichi Tsujii. 2004. Part-o f-Speech Annotation of Biology Research Abstracts. In: the Pro-
ceedings of 4th International Conference on Language Resource and Evaluation (LREC2004). IV. Lisbon, 
Portugal, pp. 1267-1270 
Kristina Toutanova, Dan Klein, Christopher D. Manning and Yoram Singer. 2003. Feature-rich Part-of-Speech 
Tag-ging with a Cyclic Dependency Network . In: NAACL 73. 252?259 
Colin Warner, Arrick Lanfranchi, Tim O'Gorman, Amanda Howard, Kevin Gould and Michael Regan. 2012. 
Bracketing Biomedical Text: An Addendum to Penn Treebank II Guidelines. 
http://clear.colorado.edu/compsem/documents/treebank_guidelines.pdf (accessed 11 May, 2014) 
Appendix A: Summary of the Medical Sub-Domains Included in the Sample Corpus of 
Clinical Notes 
Domains Co
unt 
Sub-
Special-
ties 
Doctor 
Count 
Top Level Do-
mains 
Note 
Count 
Sub-
Special-
ties 
Doctor 
Count 
Family Medicine 125
15 
9 58 Pathology 4901 1 6 
Vascular and Thoracic 
Surgery 
244
7 
7 11 Obstetrics 3771 1 7 
IM_Card iology 115
55 
6 40 IM_After Hours 
Care 
3072 1 5 
IM_Pulmonology 656
3 
6 21 Urology 2976 1 13 
Emergency Medicine 127
42 
5 28 IM_Neurology 2796 1 12 
Oncology 832
5 
4 11 IM_Hematology 1475 1 1 
IM_Nephrology 568
4 
4 24 IM_General 
Medicine 
1457 1 9 
Unclassified  194
1 
4 4 IM_Pediatrics  1326 1 13 
IM_Infectious Diseases 376 4 11 Anesthesiology 1211 1 1 
Hospitalist 177
67 
3 12 IM_Oncology 1138 1 4 
IM_Internal Medicine 
General 
157
51 
3 56 Psychiatry 827 1 5 
91
Surgery 928
7 
5 33 Neurosurgery 729 1 5 
Otorhinolaryngology  605 3 6 IM_Physician 
Assistant 
437 1 4 
Radio logy 846
35 
2 16 Podiatry 345 1 10 
IM_Gastroenterology 659
2 
2 23 Opthalmology 321 1 3 
IM_Physical Medicnie 
and Rehabilitation 
549
5 
2 6 Nurse Practit io-
ner 
314 1 4 
Orthopedics 548
0 
2 19 IM_Pain Man-
agement 
305 1 2 
Obstetrics & Gynecology 124
3 
2 16 IM_Occupationa
l Medicine 
82 1 1 
IM_Geriatrics  103 2 2 IM_Rheumatolo
gy 
77 1 2 
IM_Hospice Care and 
Palliat ive Medicine 
153 2 2 IM_Endocrinolo
gy 
31 1 2 
Appendix B: Example of Sentences having the same pattern 
Sentence Another Sentence With Same Pattern 
ALLERGIES : He is allergic to procaine . ALLERGIES : HE IS ALLERGIC TO IODINE . 
ABDOMEN : Soft with no tenderness . Abdomen : Soft with no organomegaly . 
He had an unknown syncopalepisode . He underwent a third cardiopulmonary resuscitation . 
There was no significant ST depression . There was no distal pedal edema . 
There was no associated mass shift . There was no apparent air leak . 
The sheath was removed from the sling material . The patient was resuscitated in the emergency room . 
The patient was intubated in the emergency room . The patient was placed on a CPAP mask . 
Appendix C: Example of Sentence Header and Similar Patterns 
Header Sentence Similar Sentence 
The patient was admitted into the hospital under obser-
vation . 
The patient was hospitalized for th is in 04/12 . 
Sodium is 131 , potassium is 3.9 , ch loride is 104 , bi-
carbonate is 23 , glucose is 174 , BUN is 12 , and creati-
nine is 0.82 . 
Total protein is 7.4 , albumin is 4.8 , total bili 0.3 , 
alkphos is 99 , AST is 53 , ALT is 112 , serum os-
mo is 271 . 
LUNGS : Lung sounds reveal still scattered wheezes . LUNGS : Lung reveals some scattered wheezes . 
ALLERGIES : He is allergic to sulfa medications . ALLERGIES : He has no allerg ies to medications . 
Pleasant Caucasian gentleman in no acute distress. She is in no apparent distress. 
Left L5-S1 stenosis with associated left S1 radicu lopa-
thy. 
Left hip impingement syndrome with probable la-
bral tear. 
Lab work today shows the following hemoglobin 11.7 , 
white cell count 9.8 , p latelet count is 59. 
Shows hemoglobin is stable , WBC count is stable , 
and platelet count is stable. 
 
92
