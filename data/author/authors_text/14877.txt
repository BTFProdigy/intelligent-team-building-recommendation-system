A reconfigurable stochastic tagger for languages with complex tag
structure
?ukasz De?bowski
Institute of Computer Science
Polish Academy of Sciences
ldebowsk@ipipan.waw.pl
Abstract
We present a case study of a complex
stochastic disambiguator of alternatives
of morphosyntactic tags which allows
for using incomplete disambiguation,
shorthand tag notation, external tagset
definition and external definition of mul-
tivalued context features. The tagger
bases on Naive Bayes modeling and al-
lows for using almost as general context
features as in classical trigram taggers as
well as more specific ones. Its prelimi-
nary results for Polish still do not meet
our expectations. Possible sources of the
tagger?s failures can be: inhomogene-
ity of the training corpus in preparation,
lack of the automatic search of probabil-
ity models, too general conditional in-
dependence assumptions in defining the
class of interpretable models.
Automatization of high-quality morphosyntac-
tic tagging for strongly inflective languages, such
as Slavic languages, seems to be a much harder
task than so called part-of-speech (POS) tagging
for weaker inflective languages. An important fac-
tor increasing the complexity is the very design of
the tagset. Usually, the tags assigned to word-
like segments in the former task are long lists
of subsequent attribute values, e.g. POS, num-
ber, case, gender, person etc. (Hajic? and Hladk?,
1998; Wolin?ski and Przepi?rkowski, 2001), so
they provide much more information than almost
atomic labels used for POS tagging (Manning and
Sch?tze, 1999). To make the matter harder, many
formal descriptions become easier when the tag
attribute values are allowed to form RSRL-like
type hierarchies (Przepi?rkowski et al, 2002). Al-
lowing the values to be partially ambiguous de-
pending on a context raises questions what is the
accurate level of disambiguation (Wolin?ski and
Przepi?rkowski, 2001) and how to model it prob-
abilistically in terms of random variables taking
disjunctive values (Brew, 1995).
Working for a project aiming at building a large
morphosyntactically tagged corpus of written Pol-
ish (information site http://dach.ipipan.
waw.pl/CORPUS/), we have tried implement-
ing a highly reconfigurable stochastic tagger ad-
dressing some of these problems. The main fea-
tures of our software are as follows:
? The tagger is a contextual disambiguator: It
only prunes the lists of tags admissible for
successive word-segments, given by a sepa-
rate morphological analyzer. Superiority of
this approach over simulating a stochastic
morphological analyzer by a tagger has been
discussed in Hajic? (2000).
? The tags processed by the tagger have form
of short human-readable lists of attribute val-
ues. Especially, non-applicable attributes are
omitted and multiple atomic values can be
given for the same attribute.
? The tagger?s internal representation of dis-
ambiguation alternatives (tagger?s decisions)
is different than the list of all admissible
atomic tags. In this approach, some kind
of contextually-dependent incompleteness of
disambiguation could be learned.
? Special configuration files inform the tag-
ger which tag attributes are to be disam-
biguated and what multivalued context at-
tribute are relevant for that. The tagger?s in-
ference uses a series of Naive-Bayes-like as-
sumptions founded on joint distributions of
disambiguation decisions for one tag attribute
and values of one context attribute.
? The values of the context attributes are au-
tomatically instantiated and smoothed strings
whose templates are given in a hand-made
configuration file. This approach allows to
combine strengths of generality of context at-
tributes as in n-gram models (Brants, 2000;
Megyesi, 2001) with their specificity as for
binary features in MaxEnt taggers (Ratna-
parkhi, 1996; Hajic? and Hladk?, 1998). Pos-
sibility of using alternative files defining the
templates of context attributes eases con-
structive critiques of the particular definition
of them.
? The tagger processes XML-formatted texts
where input and output files have the same
structure. Especially, it can be run on its own
output to disambiguate some attributes in cas-
cade rather than simultaneously.
? The tagger can be used for any other language
supplied with morphological analyzer, train-
ing data and tagset-dependent configuration
files.
In the following bulk of paper, we shall present
the features of our tagger in more detail, we shall
discuss its preliminary results for our Polish tag-
ging project, as well as, we shall share remarks on
possible extensions/improvements of the software
behavior.
Our general feeling is that the tagger as we have
it implemented and configured now for Polish is
not a very practical program: It works very slow
and it makes much more mistakes than state-of-
the-art taggers. In fact, the tagger gives its users so
much freedom of manual configuration and feed-
back information in the error reports that they get
lost. We hope that much accuracy can be earned
when some automatic search for the optimal con-
figuration files is implemented. On the hand, we
are still afraid if we have not underfitted with vari-
ous conditional independence assumptions, which
restrain the tagger from seeing sequences of very
specific tags as something systematic: Especially,
each tag attribute is disambiguated probabilisti-
cally independently and the program does not al-
low for treating tags as atomic entities in proba-
bility modeling. This cannot be overcome with-
out a major change in the program and, worse, in
the already complex structure of its configuration
files.
Despite all these drawbacks, we present some
study of how one can think creatively about tag-
ging and how one cannot find a quick break-even
when trying to implement too many good guide-
lines in one piece. We report on lots of apparently
technical details, hoping that their exposition can
help see the tagger as something more than a black
box and identify the sources of its errors.
1 Human-readable tags and tagger?s
decisions
Before we can define the probabilistic model of
our tagger and the scheme of its training, we need
to define some pretty abstract entities. These en-
tities are tagger?s disambiguating decisions whose
conditional probability is maximized given the ini-
tial state of the text annotation. The decisions dif-
fer from human-readable tags in the corpus. To
explain what they are and why they are there, we
need to clarify how the tagger interprets the struc-
ture of text annotation in a way which allows for
running the tagger on its earlier outputs.
Figure 1 presents a general scheme of using
the tagger. Any plain text (format 0?possibly
with some general XML tags) is first fed through
script analize.plain.text.plwhich iden-
tifies word-like segments and gives them their full
morphological analysis, i.e. the list of all possi-
ble tags (Morfeusz.pm is a library serving the
Polish morphological analyzer, written by our col-
league). After the analysis, the text obtains for-
mat 2 which is conserved through multiple calls of
the tagger (named Cypher.pl). The only thing
which happens during these calls is reducing the
Corpusio.pm
Morfeusz.pm
Morfeuszki.pm
attribute.counts.A
(attribute.weigths.A)
conx.attributes.A
survey.A
analize.plain.text.pl
test.corpus
(format 3)
(format 0)
tag.attributes.full
tag.vtcs.full
Corpusio.pm
(format 2)
Cypher.pl e
Cypher.pl l
test.corpus.A.v
(format 3)
training.corpus
(format 3)
Cypher.pl o
training.corpus.A.v
(format 3)
Cypher.pl o
CorpusioMorfeuszki.pm
(format 2)
Figure 1: Structure of tagger running.
number of alternative tags initially given by the
morphological analysis. Each tagger call can be
used to disambiguate different sets of tag attributes
depending on the given configuration files (such as
conx.attributes.*).
In fact, in format 2, each tagged segment
(roughly orthographic word) is given two identi-
cally formatted lists of potentially ambiguous lin-
guistic annotation: the full morphological analysis
and the reduced morphological analysis as pruned
by the tagger (initially equal to the full morpho-
logical analysis). In format 3, used for training
and test corpora, there is a third list containing the
morphological analysis as pruned by the human
annotators (it shares the same format and it can be
only partially disambiguated). During any kind of
the tagger call (training, testing, or tagging new
texts), it is only the tagger-pruned analyses that
are pruned against values of the call-dependent
tag attributes. Full morphological analyses and
human-pruned analyses remain untouched. Unfor-
tunately, tagger-pruned analyses must be pruned
also in the original training and test data to obtain
the relevant test and training data for the next tag-
ger call run in a pipe.
Each morphological analysis, full or reduced,
appearing in the annotated texts can be ambigu-
ous and it has a form of shorthand human-readable
tags grouped by lemmas. Here is an example of
the full morphological analysis for the Polish word
kurze (dust, cock, hen, hen:Adj):
<l>kurz<t>subst:pl:nom.acc:m3
<l>kur<t>subst:sg:loc.voc:m2
<l>kura<t>subst:sg:dat.loc:f
<l>kurzy<t>adj:sg:nom.acc:n1.n2:
pos<t>adj:pl:nom.acc:m2.m3.f.n1.
n2.p2.p3:pos\n
The tags are lists of values separated by colons (:)
and dots (.). Colons separate tag attributes and
dots separate alternative values for the same at-
tribute. When the dot is used for more than one
attribute, the tag means the full Cartesian product
of alternatives. Values of non-applicable attributes
are omitted.
The omission of attributes is not used in the tag-
ger?s internal representation of the reduced mor-
phological analysis (called tag list). Here, each
attribute has its fixed position, and our example of
full morphological analysis is transformed one-to-
one into list:
[[subst,pl,nom.acc,m3,-,-,...,kurz],
[subst,sg,loc.voc,m2,-,-,...,kur],
[subst,sg,dat.loc,f,-,-,...,kura],
[adj,sg,nom.acc,n1.n2,-,pos,...,
kurzy],
[adj,pl,nom.acc,m2.m3.f.n1.n2.p2.p3,
-,pos,...,kurzy]]
This transformation is controlled by file
tag.attributes.* which specifies names
of consequent tag attributes and enumerations of
their values
<POS> conj prep subst fin ...
<number> sg pl
<case> nom gen dat acc inst loc voc
...
(The last tag attribute is always lemma, which is
not enumerative but formally useful as a kind of
tag attribute.)
Human-reduced analyses in the training data are
allowed to be ambiguous, so the tagger could learn
not to disambiguate in some contexts. In this case,
one needs to differentiate tagger?s lists of disjunc-
tive disambiguation decisions (decision lists) from
the tagger-pruned tag lists. In fact, each deci-
sion list is a very simple functions of the tagger-
reduced tag list. E.g., if it is specified that only
number and case can be disambiguated in a given
tagger call, the list of decisions for our continued
example is:
[[LEAVE,LEAVE,LEAVE,LEAVE,...,LEAVE],
[LEAVE,sg,nom,LEAVE,...,LEAVE],
[LEAVE,sg,dat,LEAVE,...,LEAVE],
[LEAVE,sg,acc,LEAVE,...,LEAVE],
[LEAVE,sg,loc,LEAVE,...,LEAVE],
[LEAVE,sg,voc,LEAVE,...,LEAVE],
[LEAVE,pl,nom,LEAVE,...,LEAVE],
[LEAVE,pl,acc,LEAVE,...,LEAVE]]
Decision list contains all choices of atomic values
of disambiguated attributes plus special decision
[LEAVE,LEAVE,...,LEAVE].
When a decision is eventually selected from the
list, the new reduced tag list becomes former re-
duced tag list unified with the decision against all
tag attributes. LEAVE and any value X unifies
downto X. Atomic value Y and any value X uni-
fies downto Y if X contains Y and downto empty
set otherwise. For learning and testing purposes,
the right decision to be expected from the tagger
is computed as follows: When any manually dis-
ambiguated tag attribute has just one value, the
same attribute value is chosen for the right deci-
sion. When the attribute is ambiguous, same at-
tribute for the right decision equals to LEAVE.
The differentiation between all the tag attributes
and the disambiguated tag attributes was moti-
vated by the occurrence of tag lists close to large
Cartesian products: Some Polish participle forms
contain alternative values for so many tag at-
tributes, that one would get > 90 disjunctive de-
cisions for one word if disambiguating all at-
tributes simultaneously. Sequential disambigua-
tion reduces polynomial complexity downto linear
one and may enable better probability estimation.
Additionally, disambiguation of only 4 out of 14
identified morphosyntactic tag attributes usually
suffices in Polish for the full tag disambiguation.
2 Probability modeling
The general problem of probabilistic modeling of
decision processes is simple to phrase: If we want
a particular decision T to depend on all its con-
text, we find that pairs (decision, context state) are
unique events and we cannot generalize beyond
them. If we group the contexts into too few equiv-
alence classes and make the same decision for all
contexts in a class, then we lose sensitivity of the
decisions. Finding the optimal equivalence classes
can need large amounts of domain knowledge or
extensive search. Probability modeling gives addi-
tional hint: Define several not so sophisticated par-
titions of contexts into equivalence classes. Find
the joint probabilities of pairs (decision, equiva-
lence class of i-th context partition). From these,
compute the joint probability of tuples (decision,
equivalence classes for all context partitions) as-
suming some regularity of this distribution. De-
pending on the size of available training data, it
can give better results than direct estimation of the
tuple distribution.
In stochastic text tagging by our tagger, there is
a string of decisions for successive word segments
(text positions) rather than one decision. Let
Ti be the tagger?s decision at text position i,
i ? I . The admissible disjunctive choices for
Ti are tij , j ? Ji. Both Ti and tij are vectors
of values Tit and tijt for disambiguated tag
attributes t, t ? T (tijt equals to an atomic
tag attribute value or LEAVE). On the other
hand, there is a set of random variables, called
context attributes Citc, c ? Ct, which take values
identifying disjunctive equivalence classes of
successive context partitions. During tagging,
context attribute values are computed using
scheme: Citc = ftc(?, i, {Ti?t?}(i?,t?)?P(i,t)),
where ? represents the whole input text
(list of word-segments and their input tag
lists). P(i, t) = ({i ? k, ..., i ? 1} ? T ) ?
({i} ? ({0, ..., t ? 1} ? T )). ftc are fast-
computable functions taking repeatable values we
describe later.
For a formulated probability model
P
(
Ti = ? | {Citc = ? }t?T ,c?Ct
)
, the tagger
uses Viterbi search to find the optimal string of
decisions tiji , i ? I , maximizing product
?
i?I
P (Ti = tiji|{Citc = ftc(?, i,
{
ti?ji? t?
}
(i?,t?)?P(i,t))}t?T ,c?Ct).
(Viterbi search is done as for a non-stationary hid-
den Markov model of order k). Direct estimation
of P
(
Ti = ? | {Citc = ? }t?T ,c?Ct
)
is assumed un-
feasible. The tagger approximates it as
P
(
Ti = tij| {Citc = ctc}t?T ,c?Ct
)
=
?
t?T
P (T?t = tijt)
?
c?Ct
P (C?tc = ctc|T?t = tijt)
?
j??Ji
?
t?T
P (T?t = tij?t)
?
c?Ct
P (C?tc = ctc|T?t = tij?t)
,
(1)
where numerical values of P (T?t = ? ), P (C?tc =
? |T?t = ? ) do not depend explicitly on i.
If the decision lists were Cartesian products:
?
j?Ji ?t?T tijt = ?t?T
?
j?Ji tijt, then approx-
imation (1) would be equivalent to assuming that
(i) variables {Citc}c?C are independent given Tit
(naive Bayes), and (ii) given all {Citc}t?T ,c?C ,
variables {Tit}t?T are also independent (compare
Hajic? and Hladk? (1998)).
Estimation of probabilities P (T?t = ? ),
P (C?tc = ? |T?t = ? ) is done using formula
P (T?t = tijt|C?tc = ctc) =
# of positions i such that Tit = tijt i Citc = ctc
# of positions i such that Citc = ctc
,
where the counts of positions are faked by smooth-
ing procedure described in the next section.
3 Definition of context partitions
Functions ftc, defining the context partitions, are
specified in file conx.attributes.*. The file
is merely read by the tagger and it can be pre-
pared by hand. Table 1 shows an example of this
file. First column is the name of some tag at-
tribute, say t-th one. Then, the second column is
a string defining ftc. The string defining ftc re-
sembles definition of a decision tree with identi-
cal structure of all branches. It represents a suc-
cession of tests and variable instantiations evalu-
ated at each text position with decision ambigu-
ous for t-th tag attribute. Larger and larger pre-
fixes of the string are taken into account as long
as no false condition is demanded or no instanti-
ation results in disallowed prefixes. (Smoothing
procedure provides a list of allowed instantiated
prefixes, which is substantially smaller than the
list of all syntactically correct instantiations). The
longest achieved prefix with instantiated variables
is returned as ftc(?, i, {Ti?t?}(i?,t?)?P(i,t)).
The meaning of currently recognized com-
mands in the string defining ftc is such:
? rp:INTEGER_NUMBER: sets an auxiliary
text position as current text position plus
INTEGER_NUMBER.
? wd:SOME_STRING: checks if the
segment at auxiliary position equals to
SOME_STRING.
? ac:, ex:, al:, my: followed
by NAME_OF_A_TAG_ATTR: and
SOME_STRING: check if SOME_STRING
for tag attribute NAME_OF_A_TAG_ATTR
at auxiliary position is: the alternative of
available values (ac:), one of available
values (ex:), the only available value (al:),
the value of decision chosen by the tagger
(my:).
? <>: and <0>: are variables replaced for de-
manded kind of entity with its value at the
auxiliary text position (<>:) or at the current
position (<0>:).
For any definition of ftc, such as
rp:-1:wd:nie:rp:0:ac:<POS>:<>:ac:
<lemma>:<>, and each of its maximally long
values, such as C:rp:-1:wd:nie:rp:0:ac:
<POS>:inf.impt:ac:<lemma>:is?c?.
is?cic?, there is a list prefixes representing less
and less specific information on the context:
C:rp:-1:wd:nie:rp:0:ac:<POS>:inf.
impt:ac:<lemma>:is?c?.is?cic?, C:rp:
-1:wd:nie:rp:0:ac:<POS>:inf.impt,
C:rp:-1:wd:nie, C:. For each of these
prefixes, except empty one?C, there is exactly
one prefix immediately shorter. Basing on this
property, we have implemented the following
smoothing of ftc values:
1. During learning, the alphabet of allowed in-
stantiated prefixes is not closed, so for each
ftc, counts of all its maximally long values
are collected.
2. Each ftc value passes all its counts to its im-
mediate prefix if it was seen less than thresh-
old (3 times).
3. The alphabet of allowed values of ftc is fixed
as the set of all prefixes of ftc values currently
counted positively.
4. For each ftc value in the allowed alphabet, it
is faked it was also seen (once) with special
decision value Tit =SMOOTH.
5. During tagging, ftc yields the longest al-
lowed and matching instantiated value ctc.
If ctc has no positive count with particular
asked decision value Tit = tijt, tijt is treated
as it were SMOOTH.
In the adopted disambiguation scheme, the
order k of nonstationary Markov model to be
used in Viterbi search is the negative of min-
imal argument of rp: commands followed by
my: command. Thanks to that, given file
conx.attributes.*, our tagger identifies k
automatically and accepts any value of k.
4 First results
In this section, we would like to present sev-
eral scores of our tagger on processing Polish
texts. The scores should be considered prelimi-
nary due to several causes: (i) we believe that our
conx.attributes.* file is not optimal yet,
(ii) improving the tagger?s code continually, we
have had too little time to test its behavior on large
training data sufficiently, (iii) the training corpus
is still in a mix of two different degrees of dis-
ambiguation of gender values, (iv) morphologi-
cal analyzer contains no guesser and unrecognized
strings are not considered ambiguous by the tag-
ger.
In our current Polish annotation scheme, tags
consist of the following attributes: POS, num-
ber, case, gender, person, degree, aspect, nega-
tion, depreciation, accommodability, accentabil-
ity, post-prepositionality, vocalicity, punctuation,
lemma (Wolin?ski and Przepi?rkowski, 2001). We
have observed that for given morphological anal-
ysis, any atomic tag is almost always identified
given its values for just 4 attributes: POS, number,
case, gender, lemma, and decided to disambiguate
directly only these attributes.
Testing several simple versions of
conx.attributes.* file, we have ob-
served that smaller error on POS is obtained
when POS is disambiguated simultaneously
with number, case, and gender while lemma
is to be disambiguated afterwards.1 The best
conx.attributes.* file we have found so
far is presented in table 1. The general structure of
this model of ftcs resembles a product of trigram
models for each of the disambiguated attributes.
Some modification of conx.attributes.*
file might be adding POS values at positions
-1,0,+1 to ftc values for number, case, and gender.
Theoretically, this modification could be more
sensitive to a regular grammar syntax of simple
phrase structures. Strangely, this modification
yields twice as high error rate as the original
conx.attributes.* file.
Observed error rates for ftcs as in table 1 are:
? Training data 01k (784 word segments)
Error rate: (all tokens) (ambiguous)
Overall 0.25 0.45
on POS 0.06 0.10
on number 0.06 0.10
on case 0.16 0.28
on gender 0.13 0.22
? Training data 10k (8118 word segments)
1Some major ambiguity left for lemma disambiguation in
this case are homonymous present tense forms for musiec? ?
?to have? and music? ? ?to compel?.
<POS> ac:<POS>:<>:ac:<lemma>:<>
<POS> ac:<POS>:<>:rp:-1:my:<POS>:ac:<lemma>:<>
<POS> my:<POS>:<>:rp:+1:ac:<POS>:ac:<lemma>:<>
#
<number> ac:<number>:<>:rp:-1:my:<number>:<>
<number> ac:<number>:<>:rp:+1:ac:<number>:<>
#
<case> rp:-1:my:<case>:<>
<case> rp:+1:ac:<case>:<>
#
<gender> ac:<gender>:<>:rp:-1:my:<gender>:<>
<gender> ac:<gender>:<>:rp:+1:ac:<gender>:<>
Table 1: Preliminary conx.attributes.* file used for Polish.
Error rate: (all tokens) (ambiguous)
Overall 0.23 0.41
on POS 0.06 0.11
on number 0.06 0.10
on case 0.14 0.24
on gender 0.11 0.19
? Training data 50k (41208 word segments)
Error rate: (all tokens) (ambiguous)
Overall 0.20 0.34
on POS 0.05 0.08
on number 0.04 0.08
on case 0.12 0.20
on gender 0.07 0.13
? Full training data (558224 word segments)
Error rate: (all tokens) (ambiguous)
Overall 0.22 0.35
on POS 0.05 0.09
on number 0.05 0.08
on case 0.12 0.21
on gender 0.10 0.17
All trained models were tested on test data 5k
(4117 word segments).
The error rate for full training data is larger than
for 50k training data, especially on gender. This is
probably due to the inhomogenous manual anno-
tation of gender values we had already mentioned.
Comparing with publications on similar tasks, our
minimal overall error rate is twice as big as for
Slovene (D?eroski et al, 2000) and 3 times as big
as for Czech (Hajic? and Hladk?, 1998).
5 Comments and possible extensions
It has been remarked that HMM trigram tag-
gers using single multivalued context features can
perform better and run faster than MaxEnt tag-
gers trying to combine conditional probabilities
for a multitude of binary features (Brants, 2000;
Megyesi, 2001), even for large structured tagsets
and Slavonic free word-order (Hajic? et al, 2001).
Our intention was to try out a hybrid approach in
which a small number of multivalued context fea-
tures is used. We have thought it can help solv-
ing data sparseness problems. Now we do not
know exactly what is the most important cause
of our present high error rate: inhomogenous Pol-
ish corpus annotation, deficiencies of morpholog-
ical analysis, low efficiency of the manual model
search, or the assumption of probabilistic indepen-
dence of different tag attributes (used succesfully
by Hajic? and Hladk? (1998)). Due to the last thing,
we cannot either simulate a simple trigram tagger
interpreting tags as single entities.
So as to overcome the manual model search, we
have thought of writing another program which
could read the error reports of our tagger and
search for the optimal conx.attributes.*
file in a fixed space of context functions ftcs.
For binary context functions, Hajic? and Hladk?
(1998) truncated the length of their definitions and
searched extensively through a very large finite
space of them. We think that optimal ftc defini-
tions may be so long that genetic algorithm search
through the infinite space of ftcs may be a better
approach.
One might also search for more powerful se-
mantics of context functions ftc. As long as ftcs
do not depend on too distant previous tagger de-
cisions (increase the order k of Markov model),
or do not need too much computation on their
own, one can freely decide what functions of mor-
phologically analysed text ftcs are. Better disam-
biguation of Slavic nominative/accusative ambi-
guity may need testing if there is a possible ver-
bal form in the left/right context of a word (Hajic?
and Hladk?, 1998). Disambiguation of complex
gender ambiguities may need testing unifiability
of the attribute values.
Our idea of smoothing ftc values resembles
HMM reconstruction algorithm proposed by Shal-
izi, Shalizi and Crutchfield (2003), which uses ad-
ditionally Kolmogorov-Smirnov test to check if
extending the context depth is necessary for ad-
equate prediction. Similarly, we could replace ftc
value ctc by its prefix c?tc not only when ctc is
rare but also when P (T?t = ? |C?tc = ctc) does not
differ significantly from P (T?t = ? |C?tc = c?tc).
Such technique could decrease memory usage and
slightly speed up the tagger.
Gra?a, Alonso and Vilares (2002) proposed a
modification of Viterbi search for unknown text
segmentation as a common solution for disam-
biguation of segmentation and tagging. Our Pol-
ish tagset introduces some very rare ambiguities
of segmentation but we have consciously decided
not to touch the segmentation since extending the
modification of Viterbi search to multiattribute
tags with independent tag attributes needs a very
different structure of training corpus and proba-
bility estimation. Such tool would be formally
capable of performing functionality of a cascade
phrase parser and would be a good subject of an-
other project.
The research reported here was partly supported
by the KBN grant 8 T11C 043 20.
References
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference (ANLP-
2000). Seattle, WA, USA.
Chris Brew. 1995. Stochastic HPSG. In Proceedings
of the 7th Conference of the European Chapter of
the Association for Computational Linguistics.
Sa?o D?eroski, Toma? Erjavec, and Jakub Zavrel.
2000. Morphosyntactic Tagging of Slovene:
Evaluating PoS Taggers and Tagsets. In Sec-
ond International Conference on Language Re-
sources and Evaluation, LREC?00, pages 1099?
1104, Paris. ELRA. http://nl.ijs.si/et/
Bib/LREC00/lrec-tag-www/.
Jorge Gra?a, Miguel A. Alonso, and Manuel Vilares.
2002. A common solution for tokenization and part-
of-speech tagging. In Proceedings of Text, Speech
and Dialogue 2002. Lecture Notes in Artificial Intel-
ligence 2448. Springer Verlag.
Jan Hajic? and Barbora Hladk?. 1998. Tagging inflec-
tive languages: Prediction of morphological cate-
gories for a rich, structured tagset. In Proceedings
of COLING-ACL Conference. Montr?al.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva, and
Vladim?r Petkevic?. 2001. Serial combination of
rules and statistics: A case study in czech tagging.
In Proceedings of ACL?01. Toulouse, France.
Jan Hajic?. 2000. Morphological tagging: Data vs. dic-
tionaries. In Proceedings of ANLP-NAACL Confer-
ence. Seattle.
Christopher D. Manning and Hinrich Sch?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press.
Be?ta Megyesi. 2001. Comparing data-driven learning
algorithms for PoS tagging of Swedish. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2001). Carnegie
Mellon University, Pittsburgh, PA, USA.
Adam Przepi?rkowski, Anna Kups?c?, Ma?gorzata
Marciniak, and Agnieszka Mykowiecka. 2002. For-
malny opis je?zyka polskiego: Teoria i implemen-
tacja. Akademicka Oficyna Wydawnicza EXIT,
Warszawa.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings of
the First Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-1996). Univer-
sity of Pennsylvania, PA, USA.
Cosma Rohilla Shalizi, Kristina Lisa Shalizi, and
James P. Crutchfield. 2003. An algorithm for
pattern discovery in time series. http://www.
arxiv.org/abs/cs.LG/0210025.
Marcin Wolin?ski and Adam Przepi?rkowski. 2001.
Projekt anotacji morfosynktaktycznej korpusu
je?zyka polskiego. Technical Report 938, Institute of
Computer Science, Polish Academy of Sciences.
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43?50,
Prague, June 2007. c?2007 Association for Computational Linguistics
Towards the Automatic Extraction of Definitions in Slavic
1Adam Przepio?rkowski
2?ukasz Dego?rski
8Beata Wo?jtowicz
Institute of Computer Science PAS
Ordona 21, Warsaw, Poland
adamp@ipipan.waw.pl
ldegorski@bach.ipipan.waw.pl
beataw@bach.ipipan.waw.pl
4Kiril Simov
5Petya Osenova
Institute for Parallel Processing BAS
Bonchev St. 25A, Sofia, Bulgaria
kivs@bultreebank.org
petya@bultreebank.org
3Miroslav Spousta
7Vladislav Kubon?
Charles University
Malostranske? na?me?st?? 25
Prague, Czech Republic
spousta@ufal.ms.mff.cuni.cz
vk@ufal.ms.mff.cuni.cz
6Lothar Lemnitzer
University of Tu?bingen
Wilhelmstr. 19, Tu?bingen, Germany
lothar@sfs.uni-tuebingen.de
Abstract
This paper presents the results of the prelim-
inary experiments in the automatic extrac-
tion of definitions (for semi-automatic glos-
sary construction) from usually unstructured
or only weakly structured e-learning texts
in Bulgarian, Czech and Polish. The ex-
traction is performed by regular grammars
over XML-encoded morphosyntactically-
annotated documents. The results are less
than satisfying and we claim that the rea-
son for that is the intrinsic difficulty of the
task, as measured by the low interannota-
tor agreement, which calls for more sophis-
ticated deeper linguistic processing, as well
as for the use of machine learning classifica-
tion techniques.
1 Introduction
The aim of this paper is to report on the preliminary
results of a subtask of the European Project Lan-
guage Technology for eLearning (http://www.
lt4el.eu/) consisting in the identification of
term definitions in eLearning materials (Learning
Objects; henceforth: LOs), where definitions are
understood pragmatically, as those text fragments
which may, after perhaps some minor editing, be
put into a glossary. Such automatically extracted
term definitions are to be presented to the author or
the maintainer of the LO and, thus, significantly fa-
cilitate and accelerate the creation of a glossary for
a given LO. From this specification of the task it fol-
lows that good recall is much more important than
good precision, as it is easier to reject wrong glos-
sary candidates than to browse the LO for term def-
initions which were not automatically spotted.
The project involves 9 European languages in-
cluding 3 Slavic (and, regrettably, no Baltic) lan-
guages: one South Slavic, i.e., Bulgarian, and two
West Slavic, i.e., Czech and Polish. For all lan-
guages, shallow grammars identifying definitions
have been constructed; after mentioning some previ-
ous work on Information Extraction (IE) for Slavic
languages and on extraction of definitions in sec-
tion 2, we briefly describe the three Slavic grammars
developed within this project in section 3. Section 4
presents the results of the application of these gram-
mars to LOs in respective languages. These results
are evaluated in section 5, where main problems, as
well as some possible solutions, are discussed. Fi-
nally, section 6 concludes the paper.
43
2 Related Work
Definition extraction is an important NLP task,
most frequently a subtask of terminology extraction
(Pearson, 1996), the automatic creation of glossaries
(Klavans and Muresan, 2000; Klavans and Muresan,
2001), question answering (Miliaraki and Androut-
sopoulos, 2004; Fahmi and Bouma, 2006), learning
lexical semantic relations (Malaise? et al, 2004; Stor-
rer and Wellinghoff, 2006) and automatic construc-
tion of ontologies (Walter and Pinkal, 2006). Tools
for definition extraction are invariably language-
specific and involve shallow or deep processing,
with most work done for English (Pearson, 1996;
Klavans and Muresan, 2000; Klavans and Muresan,
2001) and other Germanic languages (Fahmi and
Bouma, 2006; Storrer and Wellinghoff, 2006; Wal-
ter and Pinkal, 2006), as well as French (Malaise? et
al., 2004). To the best of our knowledge, no previ-
ous attempts at definition extraction have been made
for Slavic, with the exception of some work on Bul-
garian (Tanev, 2004; Simov and Osenova, 2005).
Other work on Slavic information extraction has
been carried out mainly for the last 5 years. Prob-
ably the first forum where such work was compre-
hensively presented was the International Workshop
on Information Extraction for Slavonic and Other
Central and Eastern European Languages (IESL),
RANLP, Borovets, 2003, Bulgaria. One of the pa-
pers presented there, (Droz?dz?yn?ski et al, 2003), dis-
cusses shallow SProUT (Becker et al, 2002) gram-
mars for Czech, Polish and Lithuanian. SProUT has
subsequently been extensively used for the informa-
tion extraction from Polish medical texts (Piskorski
et al, 2004; Marciniak et al, 2005).1
3 Shallow Grammars for Definition
Extraction
The input to the task of definition extraction is
XML-encoded morphosyntactically-annotated text,
possibly with some keywords already marked by an
1SProUT has not been seriously considered for the task at
hand for two reasons: first, it was decided that only open source
tools will be used in the current project, if only available, sec-
ond, the input format to the current task is morphosyntactically-
annotated XML-encoded text, rather than raw text, as normally
expected by SProUT. The second obstacle could be removed by
converting input texts to the SProUT-internal XML representa-
tion.
independent process. For example, the representa-
tion of a Polish sentence starting as Konstruktywizm
k?adzie nacisk na (Eng. ?Constructivism puts em-
phasis on?) may be as follows:2
<s id="s9">
<markedTerm id="mt7" kw="y">
<tok base="konstruktywizm" ctag="subst"
id="t253"
msd="sg:nom:m3">Konstruktywizm</tok>
</markedTerm>
<tok base="klasc" ctag="fin" id="t254"
msd="sg:ter:imperf">kladzie</tok>
<tok base="nacisk" ctag="subst" id="t255"
msd="sg:acc:m3">nacisk</tok>
<tok base="na" ctag="prep" id="t256"
msd="acc">na</tok>
[...]
<tok base="." ctag="interp" id="t273">.
</tok>
</s>
For each language, definitions were manually
marked in two batches of texts: the first batch, con-
sulted during the process of grammar development,
contained at least 300 definitions, and the second
batch, held out for evaluation, contained about 150
definitions. All grammars are regular grammars im-
plemented with the use of the lxtransduce tool
(Tobin, 2005), a component of the LTXML2 toolset
developed at the University of Edinburgh.3 An ex-
ample of a simple rule for prepositional phrases is
given below:
<rule name="PP">
<seq>
<query match="tok[@ctag = ?prep?]"/>
<ref name="NP1">
<with-param name="case" value="??"/>
</ref>
</seq>
</rule>
This rule identifies a sequence whose first element
is a token tagged as a preposition and whose subse-
quent elements are identified by a rule called NP1.
This latter rule (not shown here for brevity) is a pa-
rameterised rule which finds a nominal phrase of a
given case, but the way it is called above ensures that
it will find an NP of any case.
2Part of the representation has been replaced by ?[...]?.
3Among the tools considered here were also CLaRK (Simov
et al, 2001), ultimately rejected because it currently does not
work in batch mode, and GATE / JAPE (Cunningham et al,
2002), not used here because we found GATE?s handling of
previously XML-annotated texts rather cumbersome and ill-
documented. Cf. also fn. 1.
44
Currently the grammars show varying degrees of
sophistication, with a small Bulgarian grammar (8
rules in a 2.5-kilobyte file), a larger Polish grammar
(34 rules in a 11KiB file) and a sophisticated Czech
grammar most developed (147 rules in a 28KiB
file). The patterns defined by these three grammars
are similar, but sufficiently different to defy an at-
tempt to write a single parameterised grammar.4 The
remainder of this section briefly describes the gram-
mars.
3.1 Bulgarian
The Bulgarian grammar is manually constructed af-
ter examination of the manually annotated defini-
tions. Here is a list of the rule schemata, together
with the number and percentage of matching defini-
tions:
Pattern # %
NP is NP 140 34.2
NP verb NP 18 29.8
NP - NP 21 5.0
This is NP 15 3.7
It represents NP 4 1.0
other patterns 107 26.2
Table 1: Bulgarian definition types
In the second schema above, ?verb? is a verb or
a verb phrase (not necessarily a constituent) which
is one of the following: ?????????????? (to repre-
sent), ????????? (to show), ?????????? (to mean),
???????? (to describe), ??? ????????? (to be used),
??????????? (to allow), ????? ?????????? ???
(to give opportunity), ??? ??????? (is called),
??????????? (to improve), ??????????? (to ensure),
?????? ??? (to serve as), ??? ???????? (to be under-
stood as), ???????????? (to denote), ????????? (to
contain), ?????????? (to determine), ?????????
(to include), ??? ???????? ????? (is defined as),
??? ???????? ??? (is based on).
We classify the rules in five types: copula defi-
nitions, copula definitions with anaphoric relation,
copula definitions with ellipsis of the copula, defi-
nitions with a verb phrase, definitions with a verb
4Because of this relative language-dependence of definition
patters, which includes, e.g., idiosyncratic case information,
we have not seriously considered re-using rules for other, non-
Slavic, languages.
phrase and anaphoric relation. Each of these types of
definitions defines an NP (sometimes via anaphoric
relation) by another one. There are some variations
of the models where some parenthetical expressions
are presented in the definition.
The grammar contains several most important
rules for each type. The different verb patterns are
encoded as a lexicon. For some of the rules, variants
with parenthetical phrases are also encoded. The rest
of the grammar is devoted to the recognition of noun
phrases and parenthetical phrases. For parentheti-
cal phrases, we have encoded a list of such possible
phrases, extracted on the basis of a bigger corpus.
The NP grammar in our view is the crucial grammar
for recognition of the definitions. Most work now
has to be invested into developing the more complex
and recursive NPs.
3.2 Czech
The Czech grammar for definition context extraction
is constructed to follow both linguistic intuition and
observation of common patterns in manually anno-
tated data.
We adapted a grammar5 based mainly on the ob-
servation of Czech Wikipedia entries. Encyclopedia
definitions are usually clear and very well structured,
but it is quite difficult to find such well-formed defi-
nitions in common texts, including learning objects.
The rules were extended using part of our manually
annotated texts, evaluated and adjusted in several it-
erations, based on the observation of the annotated
data.
Pattern # %
NP is/are NP 52 21.2
NP verb NP 45 18.4
structural 39 15.9
NP (NP) 30 12.2
NP -/:/= NP 20 8.2
other patterns 59 24.1
Table 2: Czech definition types
There are 21 top level rules, divided into five cate-
gories. Most of the correctly marked definitions fall
into the copula verb (?is/are?) category. The sec-
5The grammar was originally developed by Nguyen Thu
Trang.
45
ond most successful rule is the one using selected
verbs like ?definuje? (defines), ?znamen?? (means),
?vymezuje? (delimits), ?pr?edstavuje? (presents) and
several others. The remaining categories make use
of the typical patterns of characters (dash, colon,
equal sign and brackets) or additional structural in-
formation (e.g., HTML tags).
3.3 Polish
The Polish grammar rules are divided into three lay-
ers. Similarly to the Czech grammar, each layer only
refers to itself or lower layers. This allows for ex-
pressing top level rules in a clear and easily man-
ageable way.
The top level layer consists of rules representing
typical patterns found in Polish documents:
Pattern # %
NP (...) are/is NP-INS 40 15.6
NP -/: NP 39 15.2
NP (are/is) to NP-NOM 27 10.6
NP VP-3PERS 25 9.8
NP - i.e./or WH-question 11 4.3
N ADJ - PPAS 8 3.1
NP, i.e./or NP 7 2.7
NP-ACC one may
describe/define as NP-ACC 5 2.0
other patterns
(not in the grammar) 94 36.7
Table 3: Polish definition types
The middle layer consists of rules catching pat-
terns such as ?simple NP in given case, followed by
a sequence of non-punctuation elements? or ?cop-
ula?.
The bottom layer rules basically only refer to
POS markup in the input files (or other bottom layer
rules).
4 Results
As mentioned above, the testing corpus for each lan-
guage consists of about 150 definitions, unseen dur-
ing the construction of the grammar.6
6Obviously, three different corpora had to be used to eval-
uate the grammars for the three languages, but the corpora are
similar in size and character, so any differences in results stem
mostly from the differences in the three grammars.
The Bulgarian test corpus, containing around
76,800 tokens, consists of the third part of the
Calimera guidelines (http://www.calimera.
org/). We view this document as appropriate for
testing because it reflects the chosen domain and it
combines definitions from otherwise different sub-
domains, such as XML language, Internet usage,
etc. There are 203 manually annotated definitions
in this corpus: 129 definitions contained in one sen-
tence, 69 definitions split across 2 sentences, 4 def-
initions in 3 sentences and one definition in 4 sen-
tences. Note that the real test part is the set of the
129 definitions in one sentence, since the Bulgar-
ian grammar does not consider cross-sentence def-
initions in any way.
Czech data used for evaluation consist of several
chapters of the Calimera guidelines and Microsoft
Excel tutorial. The tutorial is a typical text used
in e-learning, consisting of five chapters describing
sheets, tables, formating, graphs and lists. The cor-
pus consists of over 90,000 tokens and contains 162
definitions, out of which 153 are contained in a sin-
gle sentence, 6 span 2 sentences, and 3 definitions
span 3 sentences.
Polish test corpus consists of over 83,200 tokens
containing 157 definitions: 148 definitions are con-
tained within one sentence, while 9 span 2 sen-
tences. The corpus is made up of 10 chapters of a
popular introduction to and history of computer sci-
ence and computer hardware.
Each grammar was quantitatively evaluated by
comparing manually annotated files with the same
files annotated automatically by the grammar. After
considering various ways of quantitative evaluation,
we decided to do the comparison at token level: pre-
cision was calculated as the ratio of the number of
those tokens which were parts of both a manually
marked definition and an automatically discovered
definition to the number of all tokens in automati-
cally discovered definitions, while recall was taken
to be the ratio of the number of tokens simultane-
ously in both kinds of definitions to the number of
tokens in all manually annotated definitions. Since,
for this task, recall is more important than precision,
we used the F2-measure for the combined result.7
7In general, F? = (1 + ?) ? (precision ? recall)/(? ?
precision+recall). Perhaps? larger than 2 could be used, but it
is currently not clear to us what criteria should be assumed when
46
The results for the three grammars are given in
Table 4. Note that the processing model for Czech
precision recall F2
Bulgarian 20.5% 2.2% 3.1
Czech 18.3% 40.7% 28.9
Polish 14.8% 22.2% 19.0
Table 4: Token-based evaluation of shallow gram-
mars
differs from the other two languages, as the input
text is converted to a flat format, as described in sec-
tion 5.3, and grammar rules are sensitive to sentence
boundaries (and may operate over them).
5 Evaluation and Possible Improvements
5.1 Interannotator Agreement
We calculated Cohen?s kappa statistic (1) for the cur-
rent task, where both the relative observed agree-
ment among raters Pr(a) and the probability that
agreement is due to chance Pr(e) where calculated
at token level.
? =
Pr(a) ? Pr(e)
1 ? Pr(e)
(1)
More specifically, we assumed that two annotators
agree on a token if the token belongs to a definition
either according to both annotations or according to
neither. In order to estimate the probability of agree-
ment due to chance Pr(e), we measured, separately
for each annotator, the proportion of tokens found in
definitions to all tokens in text, which resulted in two
probability estimates p1 and p2, and treated Pr(e) as
the probability that the two annotators agree if they
randomly, with their own probability, classify a to-
ken as belonging to a definition, i.e.:
Pr(e) = p1 ? p2 + (1 ? p1) ? (1 ? p2) (2)
The interannotator agreement (IAA) was mea-
sured this way for Czech and Polish, where ? for
each language ? the respective test corpus was an-
notated by two annotators. The results are 0.44 for
Czech and 0.31 for Polish. Such results are very low
for any classification task, and especially low for a
deciding on the exact value of ?. Note that it would not make
sense to use recall alone, as it is trivial to write all-accepting
grammars with 100% recall.
binary classification task. They show that the task of
identifying definitions in running texts and agreeing
on which parts of text count as a definition is intrin-
sically very difficult. They also call for the recon-
sideration of the evaluation and IAA measurement
methodology based on token classification.8
5.2 Evaluation Methodology
To the best of our knowledge, there is no estab-
lished evaluation methodology for the task of def-
inition extraction, where definitions may span sev-
eral sentences.9 For this reason we evaluated the re-
sults again, in a different way: we treated an auto-
matically discovered definition as correct, if it over-
lapped with a manually annotated definition. We
calculated precision as the number of automatic defi-
nitions overlapping with manual definitions, divided
by the number of automatic definitions, while re-
call ? as the number of manual definitions overlap-
ping automatic definitions, divided by the number of
manual definitions.10
The results for the three grammars, given in Ta-
ble 5, are much higher than those in Table 4 above,
although still less than satisfactory.
precision recall F2
Bulgarian 22.5% 8.9% 11.1
Czech 22.3% 46% 33.9
Polish 23.3% 32% 28.4
Table 5: Definition-based evaluation of shallow
grammars
5.3 Definitions and Sentence Boundaries
Regardless of the inherent difficulties of the task and
difficulties with the evaluation of the results, there
is clear room for improvement; one possible path
8A better approximation would be to measure IAA on the
basis of sentence or (as suggested by an anonymous reviewer)
NP classification; we intend to pursue this idea in future work.
9With the assumption that definitions are no longer than
a sentence, usually the task is treated as a classification task,
where sentences are classified as definitional or not, and ap-
propriate precision and recall measures are applied at sentence
level.
10At this stage definition fragments distributed across a num-
ber of different sentences were treated as different definitions,
which negatively affects the evaluation of the Bulgarian gram-
mar, as the Bulgarian test corpus contains a large number of
multi-sentence definitions.
47
to explore concerns multi-sentence definitions. As
noted above, for all languages considered here, there
were definitions which were spanning 2 or more sen-
tences; this turned out to be a problem especially for
Bulgarian, were 36% of definitions crossed a sen-
tence boundary.11
Such multi-sentence definitions are a problem be-
cause in the DTD adopted in this project definitions
are subelements of sentences rather than the other
way round. In case of a multi-sentence definition,
for each sentence there is a separate element en-
capsulating the part of the definition contained in
this sentence. Although these are linked via spe-
cial attributes and the information that they are part
of the same definition can subsequently be recov-
ered, it is difficult to construct an lxtransduce
grammar which would be able to automatically mark
such multi-sentence definitions: an lxtransduce
grammar expects to find a sequence of elements and
wrap them in a single larger element.
A solution to this technical problem has been im-
plemented in the Czech grammar, where first the in-
put text is flattened (via an XSLT script), so that,
e.g.:
<par id="d1p2">
<s id="d1p2s1">
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
</s>
</par>
becomes:
<par id="Sd1p2"/>
<s id="Sd1p2s1"/>
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
<s id="Ed1p2s1"/>
<par id="Ed1p2"/>
11An example of a Polish manually annotated multi-sentence
definition is: . . . opracowano techniki antyspamowe. Tech-
niki te drastycznie zaniz?aja? wartos?c? strony albo ja? banuja?. . .
(Eng. ?. . . anti-spam techniques were developed. Such tech-
niques drastically lower the value of the page or they ban it. . . ?).
The definition is split into two fragments fully contained in re-
spective sentences: techniki antyspamowe and Techniki te. . . .
No attempt at anaphora resolution is made.
This flattened representation is an input to a gram-
mar which is sensitive to the empty s and par ele-
ments and may discover definitions containing such
elements; in such a case, the postprocessing script,
which restores the hierarchical paragraph and sen-
tence structure, splits such definitions into smaller
elements, fully contained in respective sentences.
5.4 Problems Specific to Slavic
At least in case of the two West Slavic languages
considered here, the task of writing a definition
grammar is intrinsically more difficult than for Ger-
manic or Romance languages, mainly for the follow-
ing two reasons.
First, Czech and Polish have very rich nominal
inflection with a large number of paradigm-internal
syncretisms. These syncretisms are a common cause
of tagger errors, which percolate to further stages of
processing. Moreover, the number of cases makes it
more difficult to encode patterns like ?NP verb NP?,
as different verbs may combine with NPs of different
case. In fact, even two different copulas in Polish
take different cases!
Second, the relatively free word order increases
the number of rules that must be encoded, and makes
the grammar writing task more labour-intensive and
error-prone. The current version of the Polish gram-
mar, with 34 rules, is rather basic, and even the 147
rules of the Czech grammar do not take into consid-
eration all possible patterns of grammar definitions.
As Tables 4 and 5 show, there is a positive corre-
lation between the grammar size and the value of
F2, and the Bulgarian and Polish grammars certainly
have room to grow. Moreover, a path that is well
worth exploring is to drastically increase the num-
ber of rules and, hence, the recall, and then deal with
precision via Machine Learning methods (cf. sec-
tion 5.6).
5.5 Levels of Linguistic Processing
The work reported here has been an excercise in
definition extraction using shallow parsing methods.
However, the poor results suggest that this is one
of the tasks that require a much more sophisticated
and deeper approach to language analysis. In fact,
in turns out that virtually all successful attempts at
definition extraction that we are aware of build on
worked-out deep linguistic approaches (Klavans and
48
Muresan, 2000; Fahmi and Bouma, 2006; Walter
and Pinkal, 2006), some of them combining syn-
tactic and semantic information (Miliaraki and An-
droutsopoulos, 2004; Walter and Pinkal, 2006).
Unfortunately, for most Baltic and Slavic lan-
guages, such deep parsers are unavailable or have
not yet been extensively tested on real texts. One
exception is Czech, where a number of parsers were
already described and evaluated (on the Prague De-
pendency Treebank) in (Zeman, 2004, ? 14.2); the
best of these parsers reach 80?85% accuracy.
For Polish, apart from a number of linguistically
motivated toy parsers, there is a possibly wide cov-
erage deep parser (Wolin?ski, 2004), but it has not yet
been evaluated on naturally occurring texts. The sit-
uation is probably most dire for Bulgarian, although
there have been attempts at the induction of a depen-
dency parser from the BulTreeBank (Marinov and
Nivre, 2005; Chanev et al, 2006).
Nevertheless, if other possible paths of improve-
ment suggested in this section do not bring satisfac-
tory results, we plan to make an attempt at adapting
these parsers to the task at hand.
5.6 Postprocessing: Machine Learning and
Keyword Identification
Various approaches to the machine learning treat-
ment of the task of classifying sentences or snippets
as definitions or non-definitions can be found, e.g.,
in (Miliaraki and Androutsopoulos, 2004; Fahmi
and Bouma, 2006) and references therein. In the
context of the present work, such methods may be
used to postprocess apparent definitions found at
earlier processing stages and decide which of them
are genuine definitions. For example, (Fahmi and
Bouma, 2006) report that a system trained on 2299
sentences, including 1366 definition sentences, may
increase the accuracy of a definition extraction tool
from 59% to around 90%.12
Another possible improvement may consist in,
again, aiming at very high recall and then using
an independent keyword detector to mark keywords
(and key phrases) in text and classifying as genuine
definitions those definitions, whose defined term has
been marked as a keyword.
12The numbers are so high ?probably due to the fact that the
current corpus consists of encyclopedic material only? (Fahmi
and Bouma, 2006, fn. 4).
Whatever postprocessing technique or combina-
tion of techniques proves most efficient, it seems that
the linguistic processing should aim at high recall
rather than high precision, which further justifies the
use of the F2 measure for evaluation.13
6 Conclusion
To the best of our knowledge, this paper is the first
report on the task of definition extraction for a num-
ber of Slavic languages. It shows that the task is
intrinsically very difficult, which partially explains
the relatively low results obtained. It also calls atten-
tion to the fact that there is no established evaluation
methodology where possibly multi-sentence defini-
tions are involved and suggests what such method-
ology could amount to. Finally, the paper suggests
ways of improving the results, which we hope to fol-
low and report in the future.
References
Markus Becker et al 2002. SProUT ? shallow process-
ing with typed feature structures and unification. In
Proceedings of the International Conference on NLP
(ICON 2002), Mumbai, India.
Sharon A. Caraballo. 2001. Automatic Construction of a
Hypernym-Labeled Noun Hierarchy from Text. Ph. D.
dissertation, Brown University.
Atanas Chanev, Kiril Simov, Petya Osenova, and Sve-
toslav Marinov. 2006. Dependency conversion and
parsing of the BulTreeBank. In proceedings of the
LREC workshop Merging and Layering Linguistic In-
formation, Genoa, Italy.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Witold Droz?dz?yn?ski, Petr Homola, Jakub Piskorski, and
Vytautas Zinkevic?ius. 2003. Adapting SProUT to
processing Baltic and Slavonic languages. In Infor-
mation Extraction for Slavonic and Other Central and
Eastern European Languages, pp. 18?25.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications.
13Note that the situation here is different than in the task of
acquiring hyponymic relations from texts, where high-precision
manual rules (Hearst, 1992) must be augmented with statistical
clustering methods to increase recall (Caraballo, 2001).
49
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, Nantes, France.
Judith L. Klavans and Smaranda Muresan. 2000.
DEFINDER: Rule-based methods for the extraction of
medical terminology and their associated definitions
from on-line text. In Proceedings of the Annual Fall
Symposium of the American Medical Informatics As-
sociation.
Judith L. Klavans and Smaranda Muresan. 2001. Eval-
uation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of AMIA Sym-
posium 2001.
V?ronique Malais?, Pierre Zweigenbaum, and Bruno
Bachimont. 2004. Detecting semantic relations be-
tween terms in definitions. In S. Ananadiou and
P. Zweigenbaum, editors, COLING 2004 CompuTerm
2004: 3rd International Workshop on Computational
Terminology, pp. 55?62, Geneva, Switzerland. COL-
ING.
Ma?gorzata Marciniak, Agnieszka Mykowiecka, Anna
Kups?c?, and Jakub Piskorski. 2005. Intelligent con-
tent extraction from Polish medical texts. In L. Bolc
et al, editors, Intelligent Media Technology for Com-
municative Intelligence, Second International Work-
shop, IMTCI 2004, Warsaw, Poland, September 13-14,
2004, Revised Selected Papers, volume 3490 of Lec-
ture Notes in Computer Science, pp. 68?78. Springer-
Verlag.
Svetoslav Marinov and Joakim Nivre. 2005. A data-
driven parser for Bulgarian. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, pp. 89?100, Barcelona.
Spyridoula Miliaraki and Ion Androutsopoulos. 2004.
Learning to identify single-snippet answers to defi-
nition questions. In Proceedings of COLING 2004,
pp. 1360?1366, Geneva, Switzerland. COLING.
Jennifer Pearson. 1996. The expression of defini-
tions in specialised texts: a corpus-based analysis.
In M. Gellerstam et al, editors, Proceedings of the
Seventh Euralex International Congress, pp. 817?824,
G?teborg.
Jakub Piskorski et al 2004. Information extraction for
Polish using the SProUT platform. In M. A. K?opotek
et al, editors, Intelligent Information Processing and
Web Mining, pp. 227?236. Springer-Verlag, Berlin.
Kiril Simov and Petya Osenova. 2005. BulQA:
Bulgarian-Bulgarian Question Answering at CLEF
2005. In CLEF, pp. 517?526.
Kiril Simov et al 2001. CLaRK ? an XML-based sys-
tem for corpora development. In P. Rayson et al, edi-
tors, Proceedings of the Corpus Linguistics 2001 Con-
ference, pp. 558?560, Lancaster.
Angelika Storrer and Sandra Wellinghoff. 2006. Auto-
mated detection and annotation of term definitions in
German text corpora. In Proceedings of LREC 2006.
Hristo Tanev. 2004. Socrates: A question answering
prototype for Bulgarian. In Recent Advances in Nat-
ural Language Processing III, Selected Papers from
RANLP 2003, pages 377?386. John Benjamins.
Richard Tobin, 2005. Lxtransduce, a replace-
ment for fsgmatch. University of Edinburgh.
http://www.cogsci.ed.ac.uk/~richard/
ltxml2/lxtransduce-manual.html.
Stephan Walter and Manfred Pinkal. 2006. Automatic
extraction of definitions from German court decisions.
In Proceedings of the Workshop on Information Ex-
traction Beyond The Document, pp. 20?28, Sydney,
Australia. Association for Computational Linguistics.
Marcin Wolin?ski. 2004. Komputerowa weryfikacja gra-
matyki S?widzin?skiego. Ph. D. dissertation, ICS PAS,
Warsaw.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph. D. dissertation, Charles University,
Prague.
50
