Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 657?664,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Extracting loanwords from Mongolian corpora and producing a 
Japanese-Mongolian bilingual dictionary
 
Badam-Osor Khaltar 
Graduate School of Library, 
Information and Media Studies 
University of Tsukuba 
1-2 Kasuga Tsukuba, 305-8550 
Japan 
khab23@slis.tsukuba.ac.jp 
 
 
 
Atsushi Fujii 
Graduate School of Library, 
Information and Media Studies 
University of Tsukuba 
1-2 Kasuga Tsukuba, 305-8550 
Japan 
fujii@slis.tsukuba.ac.jp 
 
 
 
Tetsuya Ishikawa 
The Historiographical Institute 
The University of Tokyo 
3-1 Hongo 7-chome, Bunkyo-ku 
Tokyo, 133-0033 
Japan 
ishikawa@hi.u-tokyo.ac.jp 
 
 
 
Abstract 
This paper proposes methods for extracting 
loanwords from Cyrillic Mongolian corpora 
and producing a Japanese?Mongolian 
bilingual dictionary. We extract loanwords 
from Mongolian corpora using our own 
handcrafted rules. To complement the 
rule-based extraction, we also extract words 
in Mongolian corpora that are phonetically 
similar to Japanese Katakana words as 
loanwords. In addition, we correspond the 
extracted loanwords to Japanese words and 
produce a bilingual dictionary. We propose a 
stemming method for Mongolian to extract 
loanwords correctly. We verify the 
effectiveness of our methods experimentally. 
 
1 Introduction  
Reflecting the rapid growth in science and 
technology, new words and technical terms are being 
progressively created, and these words and terms are 
often transliterated when imported as loanwords in 
another language. 
Loanwords are often not included in dictionaries, 
and decrease the quality of natural language 
processing, information retrieval, machine 
translation, and speech recognition. At the same time, 
compiling dictionaries is expensive, because it relies 
on human introspection and supervision. Thus, a 
number of automatic methods have been proposed to 
extract loanwords and their translations from corpora, 
targeting various languages. 
In this paper, we focus on extracting loanwords in 
Mongolian. The Mongolian language is divided into 
Traditional Mongolian, written using the Mongolian 
alphabet, and Modern Mongolian, written using the 
Cyrillic alphabet. We focused solely on Modern 
Mongolian, and use the word ?Mongolian? to refer 
to Modern Mongolian in this paper. 
There are two major problems in extracting 
loanwords from Mongolian corpora. 
The first problem is that Mongolian uses the 
Cyrillic alphabet to represent both conventional 
words and loanwords, and so the automatic 
extraction of loanwords is difficult. This feature 
provides a salient contrast to Japanese, where the 
Katakana alphabet is mainly used for loanwords and 
proper nouns, but not used for conventional words. 
The second problem is that content words, such as 
nouns and verbs, are inflected in sentences in 
Mongolian. Each sentence in Mongolian is 
segmented on a phrase-by-phase basis. A phrase 
consists of a content word and one or more suffixes, 
such as postpositional particles. Because loanwords 
are content words, then to extract loanwords 
correctly, we have to identify the original form using 
stemming. 
In this paper, we propose methods for extracting 
loanwords from Cyrillic Mongolian and producing a 
Japanese?Mongolian bilingual dictionary. We also 
propose a stemming method to identify the original 
forms of content words in Mongolian phrases. 
657
2 Related work 
To the best of our knowledge, no attempt has been 
made to extract loanwords and their translations 
targeting Mongolian. Thus, we will discuss existing 
methods targeting other languages.  
In Korean, both loanwords and conventional 
words are spelled out using the Korean alphabet, 
called Hangul. Thus, the automatic extraction of 
loanwords in Korean is difficult, as it is in 
Mongolian. Existing methods that are used to extract 
loanwords from Korean corpora (Myaeng and Jeong, 
1999; Oh and Choi, 2001) use the phonetic 
differences between conventional Korean words and 
loanwords. However, these methods require 
manually tagged training corpora, and are expensive. 
A number of corpus-based methods are used to 
extract bilingual lexicons (Fung and McKeown, 
1996; Smadja, 1996). These methods use statistics 
obtained from a parallel or comparable bilingual 
corpus, and extract word or phrase pairs that are 
strongly associated with each other. However, these 
methods cannot be applied to a language pair where 
a large parallel or comparable corpus is not available, 
such as Mongolian and Japanese. 
Fujii et al (2004) proposed a method that does not 
require tagged corpora or parallel corpora to extract 
loanwords and their translations. They used a 
monolingual corpus in Korean and a dictionary 
consisting of Japanese Katakana words. They 
assumed that loanwords in multiple countries 
corresponding to the same source word are 
phonetically similar. For example, the English word 
?system? has been imported into Korean, Mongolian, 
and Japanese. In these languages, the romanized 
words are ?siseutem?, ?sistem?, and ?shisutemu?, 
respectively. 
It is often the case that new terms have been 
imported into multiple languages simultaneously, 
because the source words are usually influential 
across cultures. It is feasible that a large number of 
loanwords in Korean can also be loanwords in 
Japanese. Additionally, Katakana words can be 
extracted from Japanese corpora with a high 
accuracy. Thus, Fujii et al (2004) extracted the 
loanwords in Korean corpora that were phonetically 
similar to Japanese Katakana words. Because each 
of the extracted loanwords also corresponded to a 
Japanese word during the extraction process, a 
Japanese?Korean bilingual dictionary was produced 
in a single framework. 
However, a number of open questions remain 
from Fujii et al?s research. First, their stemming 
method can only be used for Korean. Second, their 
accuracy in extracting loanwords was low, and thus, 
an additional extraction method was required. Third, 
they did not report on the accuracy of extracting 
translations, and finally, because they used Dynamic 
Programming (DP) matching for computing the 
phonetic similarities between Korean and Japanese 
words, the computational cost was prohibitive. 
In an attempt to extract Chinese?English 
translations from corpora, Lam et al (2004) 
proposed a similar method to Fujii et al (2004). 
However, they searched the Web for 
Chinese?English bilingual comparable corpora, and 
matched named entities in each language corpus if 
they were similar to each other. Thus, Lam et al?s 
method cannot be used for a language pair where 
comparable corpora do not exist. In contrast, using 
Fujii et al?s (2004) method, the Katakana dictionary 
and a Korean corpus can be independent. 
In addition, Lam et al?s method requires 
Chinese?English named entity pairs to train the 
similarity computation. Because the accuracy of 
extracting named entities was not reported, it is not 
clear to what extent this method is effective in 
extracting loanwords from corpora. 
 
3 Methodology 
3.1 Overview 
In view of the discussion outlined in Section 2, we 
enhanced the method proposed by Fujii et al (2004) 
for our purpose. Figure 1 shows the method that we 
used to extract loanwords from a Mongolian corpus 
and to produce a Japanese?Mongolian bilingual 
dictionary. Although the basis of our method is 
similar to that used by Fujii et al (2004), 
?Stemming?, ?Extracting loanwords based on rules?, 
and ?N-gram retrieval? are introduced in this paper. 
First, we perform stemming on a Mongolian 
corpus to segment phrases into a content word and  
one or more suffixes. 
658
 
Second, we discard segmented content words if 
they are in an existing dictionary, and extract the 
remaining words as candidate loanwords. 
Third, we use our own handcrafted rules to extract 
loanwords from the candidate loanwords. While the 
rule-based method can extract loanwords with a high 
accuracy, a number of loanwords cannot be extracted 
using predefined rules. 
Fourth, as performed by Fujii et al (2004), we use 
a Japanese Katakana dictionary and extract a 
candidate loanword that is phonetically similar to a 
Katakana word as a loanword. We romanize the 
candidate loanwords that were not extracted using 
the rules. We also romanize all words in the 
Katakana dictionary.  
However, unlike Fujii et al (2004), we use 
N-gram retrieval to limit the number of Katakana 
words that are similar to the candidate loanwords. 
Then, we compute the phonetic similarities between 
each candidate loanword and each retrieved 
Katakana word using DP matching, and select a pair 
whose score is above a predefined threshold. As a 
result, we can extract loanwords in Mongolian and 
their translations in Japanese simultaneously. 
Finally, to identify Japanese translations for the 
loanwords extracted using the rules defined in the 
third step above, we perform N-gram retrieval and 
DP matching.  
We will elaborate further on each step in Sections 
3.2?3.7. 
3.2 Stemming 
A phrase in Mongolian consists of a content word 
and one or more suffixes. A content word can 
potentially be inflected in a phrase. Figure 2 shows 
 
 
Mongolian corpus Katakana dictionary
 Stemming 
 
 Extracting candidate loanwords Romanization 
 
Japanese-Mongolian bilingual dictionaryExtracting loanwords based on rules 
 
 
 
Romanization N-gram retrieval 
 
 
Mongolian loanword dictionary 
High Similarity
Computing phonetic similarity 
Fig  ure 1: Overview of our extraction method.
Type Example 
(a) No inflection. ??? + ?? ? ????? 
Book + Genitive Case 
(b) Vowel elimination. ???? +???+ ??? ???????? 
Work + Ablative Case +Reflexive
(c) Vowel insertion. ?? + ? ? ???? 
Brother + Dative Case 
(d) Consonant insertion. ?????? + ???? ??????????
Building + Genitive Case 
(e) The letter ??? is 
converted to ???, and 
the vowel is eliminated. 
????????+ ???? ?????????? 
School + Ablative Case 
Figure 2: Inflection types of nouns in Mongolian. 
the inflection types of content words in phrases. In 
phrase (a), there is no inflection in the content word 
???? (book)? concatenated with the suffix ??? 
(genitive case)?. 
However, in phrases (b)?(e) in Figure 2, the 
content words are inflected. Loanwords are also 
inflected in all of these types, except for phrase (b). 
Thus, we have to identify the original form of a 
content word using stemming. While most 
loanwords are nouns, a number of loanwords can 
also be verbs. In this paper, we propose a stemming 
method for nouns. Figure 3 shows our stemming 
method. We will explain our stemming method 
further, based on Figure 3. 
First, we consult a ?Suffix dictionary? and 
perform backward partial matching to determine 
whether or not one or more suffixes are concatenated 
at the end of a target phrase. 
Second, if a suffix is detected, we use a ?Suffix 
segmentation rule? to segment the suffix and extract 
659
 
Figure 3: Overview of our noun stemming method. 
 
the noun. The inflection type in phrases (c)?(e) in 
Figure 2 is also determined. 
Third, we investigate whether or not the vowel 
elimination in phrase (b) in Figure 2 occurred in the 
extracted noun. Because the vowel elimination 
occurs only in the last vowel of a noun, we check the 
last two characters of the extracted noun. If both of 
the characters are consonants, the eliminated vowel 
is inserted using a ?Vowel insertion rule? and the 
noun is converted into its original form. 
Existing Mongolian stemming methods (Ehara et 
al., 2004; Sanduijav et al, 2005) use noun 
dictionaries. Because we intend to extract loanwords 
that are not in existing dictionaries, the above 
methods cannot be used. Noun dictionaries have to 
be updated as new words are created. 
Our stemming method does not require a noun 
dictionary. Instead, we manually produced a suffix 
dictionary, suffix segmentation rule, and vowel 
insertion rule. However, once these resources are 
produced, almost no further compilation is required. 
The suffix dictionary consists of 37 suffixes that 
can concatenate with nouns. These suffixes are 
postpositional particles. Table 1 shows the dictionary 
entries, in which the inflection forms of the 
postpositional particles are shown in parentheses. 
The suffix segmentation rule consists of 173 rules. 
We show examples of these rules in Figure 4. Even 
if suffixes are identical in their phrases, the 
segmentation rules can be different, depending on  
the counterpart noun. 
In Figure 4, the suffix ????? matches both the 
noun phrases (a) and (b) by backward partial 
matching. However, each phrase is segmented by a        
Table 1: Entries of the suffix dictionary. 
detect a suffix in
the phrase 
Suffix dictionary Suffix segmentation rule
phrase 
noun 
segment a suffix 
and extract a noun
Yes 
 
insert a vowel 
check if the last two characters of the 
noun are both consonants 
Vowel insertion rule
No 
Case Suffix 
Genitive 
Accusative 
Dative 
Ablative 
Instrumental 
Cooperative 
Reflexive 
Plural 
?, ?, ??, ??, ??, ???, ??? 
??, ???, ? 
?, ? 
??? (???), ??? (???), ???, ??? 
??? (???), ??? (???), ???, ??? 
???, ???, ??? 
?? (??), ?? (??), ??, ?? 
??? (???), ??? (???) 
 
Suffix Noun phrase Noun 
(a) ?????? 
mother?s 
??? 
mother 
 
??? 
Genitive 
 
(b) ????????? 
Haraa?(river name)s 
????? 
Haraa 
Figure 4: Examples of the suffix segmentation rule. 
 
deferent rule independently. The underlined suffixes 
are segmented in each phrase, respectively. In phrase 
(a), there is no inflection, and the suffix is easily 
segmented. However, in phrase (b), a consonant 
insertion has occurred. Thus, both the inserted 
consonant, ???, and the suffix have to be removed. 
 The vowel insertion rule consists of 12 rules. To 
insert an eliminated vowel and extract the original 
form of the noun, we check the last two characters of 
a target noun. If both of these are consonants, we 
determine that a vowel was eliminated. 
However, a number of nouns end with two 
consonants inherently, and therefore, we referred to a 
textbook on Mongolian grammar (Bayarmaa, 2002) 
to produce 12 rules to determine when to insert a 
vowel between two consecutive consonants. 
For example, if any of ???, ???, ???, ???, ???, or 
??? are at the end of a noun, a vowel is inserted. 
However, if any of ???, ???, ???, ???, ???, ???, ???, 
???, or ??? are the second to last consonant in a noun, 
a vowel is not inserted. 
The Mongolian vowel harmony rule is a 
phonological rule in which female vowels and male 
vowels are prohibited from occurring in a single 
word together (with the exception of proper nouns). 
We used this rule to determine which vowel should 
be inserted. The appropriate vowel is determined by 
the first vowel of the first syllable in the target noun. 
660
For example, if there are ??? and ??? in the first 
syllable, the vowel ??? is inserted between the last 
two consonants. 
3.3 Extracting candidate loanwords 
After collecting nouns using our stemming method, 
we discard the conventional Mongolian nouns. We 
discard nouns defined in a noun dictionary 
(Sanduijav et al, 2005), which includes 1,926 nouns. 
We also discard proper nouns and abbreviations. The 
first characters of proper nouns, such as ?????????? 
(Erdenebat)?, and all the characters of abbreviations, 
such as ????? (Nuclear research centre)?, are 
written using capital letters in Mongolian. Thus, we 
discard words that are written using capital 
characters, except those occurring at the beginning of 
sentences. In addition, because ??? and ??? are not 
used to spell out Western languages, words including 
those characters are also discarded. 
3.4 Extracting loanwords based on rules 
We manually produced seven rules to identify 
loanwords in Mongolian. Words that match with one 
of the following rules are extracted as loanwords. 
(a) A word including the consonants ???, ???, ???, 
or ???. 
These consonants are usually used to spell out 
foreign words. 
(b) A word that violated the Mongolian vowel 
harmony rule. 
Because of the vowel harmony rule, a word 
that includes female and male vowels, which is 
not based on the Mongolian phonetic system, is 
probably a loanword. 
(c) A word beginning with two consonants. 
A conventional Mongolian word does not 
begin with two consonants. 
(d) A word ending with two particular consonants. 
A word whose penultimate character is any 
of: ???, ???, ???, ???, ???, ???, or ??? and 
whose last character is a consonant violates 
Mongolian grammar, and is probably a 
loanword. 
(e) A word beginning with the consonant ???. 
In a modern Mongolian dictionary (Ozawa, 
2000), there are 54 words beginning with ???, 
of which 31 are loanwords. Therefore, a word 
beginning with ??? is probably a loanword. 
(f) A word beginning with the consonant ???. 
In a modern Mongolian dictionary (Ozawa, 
2000), there are 49 words beginning with ???, 
of which only four words are conventional 
Mongolian words. Therefore, a word beginning 
with ??? is probably a loanword. 
(g) A word ending with ?<consonant> + ??. 
We discovered this rule empirically. 
3.5 Romanization  
We manually aligned each Mongolian Cyrillic 
alphabet to its Roman representation1. 
In Japanese, the Hepburn and Kunrei systems are 
commonly used for romanization proposes. We used 
the Hepburn system, because its representation is 
similar to that used in Mongolian, compared to the 
Kunrei system. 
However, we adapted 11 Mongolian romanization 
expressions to the Japanese Hepburn romanization. 
For example, the sound of the letter ?L? does not 
exist in Japanese, and thus, we converted ?L? to ?R? 
in Mongolian. 
3.6 N-gram retrieval 
By using a document retrieval method, we efficiently 
identify Katakana words that are phonetically similar 
to a candidate loanword. In other words, we use a 
candidate loanword, and each Katakana word as a 
query and a document, respectively. We call this 
method ?N-gram retrieval?. 
Because the N-gram retrieval method does not 
consider the order of the characters in a target word, 
the accuracy of matching two words is low, but the 
computation time is fast. On the other hand, because 
DP matching considers the order of the characters in 
a target word, the accuracy of matching two words is 
high, but the computation time is slow. We combined 
these two methods to achieve a high matching 
accuracy with a reasonable computation time. 
First, we extract Katakana words that are 
phonetically similar to a candidate loanword using 
N-gram retrieval. Second, we compute the similarity 
between the candidate loanword and each of the 
retrieved Katakana words using DP matching to 
improve the accuracy. 
We romanize all the Katakana words in the 
dictionary and index them using consecutive N 
                                                         
1 http://badaa.mngl.net/docs.php?p=trans_table (May, 2006) 
661
characters. We also romanize each candidate 
loanword when use as a query. We experimentally 
set N = 2, and use the Okapi BM25 (Robertson et al, 
1995) for the retrieval model. 
3.7 Computing phonetic similarity 
Given the romanized Katakana words and the 
romanized candidate loanwords, we compute the 
similarity between the two strings, and select the 
pairs associated with a score above a predefined 
threshold as translations. We use DP matching to 
identify the number of differences (i.e., insertion, 
deletion, and substitution) between two strings on an 
alphabet-by-alphabet basis. 
While consonants in transliteration are usually the 
same across languages, vowels can vary depending 
on the language. The difference in consonants 
between two strings should be penalized more than 
the difference in vowels. We compute the similarity 
between two romanized words using Equation (1). 
         
vc
dvdc
+?
+??? ?
? )(2
1           (1) 
Here, dc and dv denote the number of differences in 
consonants and vowels, respectively, and ? is a 
parametric consonant used to control the importance 
of the consonants. We experimentally set ? = 2. 
Additionally, c and v denote the number of all the 
consonants and vowels in the two strings, 
respectively. The similarity ranges from 0 to 1. 
 
4 Experiments  
4.1 Method 
We collected 1,118 technical reports published in 
Mongolian from the ?Mongolian IT Park?2 and used 
them as a Mongolian corpus. The number of phrase 
types and phrase tokens in our corpus were 110,458 
and 263,512, respectively. 
We collected 111,116 Katakana words from 
multiple Japanese dictionaries, most of which were 
technical term dictionaries. 
We evaluated our method from four perspectives: 
?stemming?, ?loanword extraction?, ?translation 
extraction?, and ?computational cost.? We will 
discuss these further in Sections 4.2-4.5, respectively. 
4.2 Evaluating stemming  
We randomly selected 50 Mongolian technical 
                                                         
2 http://www.itpark.mn/ (May, 2006) 
reports from our corpus, and used them to evaluate 
the accuracy of our stemming method. These 
technical reports were related to: medical 
science (17), geology (10), light industry (14), 
agriculture (6), and sociology (3). In these 50 reports, 
the number of phrase types including conventional 
Mongolian nouns and loanword nouns was 961 and 
206, respectively. We also found six phrases 
including loanword verbs, which were not used in 
the evaluation.  
Table 2 shows the results of our stemming 
experiment, in which the accuracy for conventional 
Mongolian nouns was 98.7% and the accuracy for 
loanwords was 94.6%. Our stemming method is 
practical, and can also be used for morphological 
analysis of Mongolian corpora. 
We analyzed the reasons for any failures, and 
found that for 12 conventional nouns and 11 
loanwords, the suffixes were incorrectly segmented. 
4.3 Evaluating loanword extraction 
We used our stemming method on our corpus and 
selected the most frequently used 1,300 words. We 
used these words to evaluate the accuracy of our 
loanword extraction method. Of these 1,300 words, 
165 were loanwords. We varied the threshold for the 
similarity, and investigated the relationship between 
precision and recall. Recall is the ratio of the number 
of correct loanwords extracted by our method to the 
total number of correct loanwords. Precision is the 
ratio of the number of correct loanwords extracted 
by our method to the total number of words 
extracted by our method. We extracted loanwords 
using rules (a)?(g) defined in Section 3.4. As a result, 
139 words were extracted. 
Table 3 shows the precision and recall of each rule. 
The precision and recall showed high values using 
?All rules?, which combined the words extracted by 
rules (a)?(g) independently. 
We also extracted loanwords using the phonetic 
similarity, as discussed in Sections 3.6 and 3.7. 
 
Table 2: Results of our noun stemming method. 
 No. of each phrase type Accuracy (%) 
Conventional 
nouns 
961 98.7
Loanwords 206 94.6
662
 
 
 
 
 
 
 
We used the N-gram retrieval method to obtain up to 
the top 500 Katakana words that were similar to each 
candidate loanword. Then, we selected up to the top 
five pairs of a loanword and a Katakana word whose 
similarity computed using Equation (1) was greater 
than 0.6. Table 4 shows the results of our 
similarity-based extraction. 
Both the precision and the recall for the 
similarity-based loanword extraction were lower 
than those for the ?All rules? data listed in Table 3. 
 
Table 4: Precision and recall for our similarity-based 
loanword extraction. 
Words extracted 
automatically 
Extracted correct 
loanwords 
Precision 
(%) 
Recall
(%) 
3,479 109 3.1 66.1
 
We also evaluated the effectiveness of a 
combination of the N-gram and DP matching 
methods. We performed similarity-based extraction 
after rule-based extraction. Table 5 shows the results, 
in which the data of the ?Rule? are identical to those 
of the ?All rules? data listed in Table 3. However, the 
?Similarity? data are not identical to those listed in 
Table 4, because we performed similarity-based 
extraction using only the words that were not 
extracted by rule-based extraction.  
When we combined the rule-based and 
similarity-based methods, the recall improved from 
84.2% to 91.5%. The recall value should be high 
when a human expert modifies or verifies the 
resultant dictionary. 
Figure 5 shows example of extracted loanwords in 
Mongolian and their English glosses. 
4.4 Evaluating Translation extraction  
In the row ?Both? shown in Table 5, 151 loanwords 
were extracted, for each of which we selected up to 
the top five Katakana words whose similarity 
computed using Equation (1) was greater than 0.6 as 
 
 
Table 3: Precision and recall for rule-based loanword extraction. 
Rules (a) (b) 
 
(c) (d) (e) (f) (g) All rules 
Words extracted automatically 102 63
 
21 6 4 5 24 150
Extracted correct loanwords 101 60
 
20 5 4
 
5 19 139
Precision (%) 99.0 95.2 95.2 83.3
 
Table 5: Precision and recall of different loanword 
extraction methods. 
 No. of 
words
No. that 
were correct 
Precision 
(%)  
Recall 
(%) 
Rule 150 139 92.7 84.2
Similarity 60 12 20.0 46.2
Both 210 151 71.2 91.5
 
Mongolian English gloss 
???????? 
????????? 
???????? 
????????? 
albumin 
laboratory 
mechanism 
mitochondria 
Figure 5: Example of extracted loanwords. 
 
translations. As a result, Japanese translations were 
extracted for 109 loanwords. Table 6 shows the 
results, in which the precision and recall of 
extracting Japanese?Mongolian translations were 
56.2% and 72.2%, respectively. 
We analyzed the data and identified the reasons 
for any failures. For five loanwords, the N-gram 
retrieval failed to search for the similar Katakana 
words. For three loanwords, the phonetic similarity 
computed using Equation (1) was not high enough 
for a correct translation. For 27 loanwords, the 
Japanese translations did not exist inherently. For 
seven loanwords, the Japanese translations existed, 
but were not included in our Katakana dictionary.  
Figure 6 shows the Japanese translations extracted 
for the loanwords shown in Figure 5. 
 
Table 6: Precision and recall for translation 
extraction.  
No. of translations 
extracted 
automatically 
No. of extracted 
correct 
translations 
Precision 
(%) 
 
Recall 
(%) 
194 109 56.2 72.2
 
100 100 79.2 92.7
Recall (%) 61.2 36.4 12.1 3.0 2.4 3.03 11.5 84.2
663
Japanese Mongolian English gloss 
????? 
?????  ?
????? 
??????? 
???????? 
????????? 
???????? 
????????? 
albumin 
laboratory 
mechanism 
mitochondria 
Figure 6: Japanese translations extracted for the 
loanwords shown in Figure 5. 
 
4.5 Evaluating computational cost 
We randomly selected 100 loanwords from our 
corpus, and used them to evaluate the computational 
cost of the different extraction methods. We 
compared the computation time and the accuracy of 
?N-gram?, ?DP matching?, and ?N-gram + DP 
matching? methods. The experiments were 
performed using the same PC (CPU = Pentium III 1 
GHz dual, Memory = 2 GB). 
Table 7 shows the improvement in computation 
time by ?N-gram + DP matching? on ?DP matching?, 
and the average rank of the correct translations for 
?N-gram?. We improved the efficiency, while 
maintaining the sorting accuracy of the translations. 
 
Table 7: Evaluation of the computational cost. 
Method N-gram DP N-gram + DP
Loanwords 100 
Computation time (sec.) 95 136,815 293
Extracted correct 
translations 
66 66 66
Average rank of correct 
translations 
44.8 2.7 2.7
 
5 Conclusion 
We proposed methods for extracting loanwords from 
Cyrillic Mongolian corpora and producing a 
Japanese?Mongolian bilingual dictionary. Our 
research is the first serious effort in producing 
dictionaries of loanwords and their translations 
targeting Mongolian. We devised our own rules to 
extract loanwords from Mongolian corpora. We also 
extracted words in Mongolian corpora that are 
phonetically similar to Japanese Katakana words as 
loanwords. We also corresponded the extracted 
loanwords to Japanese words, and produced a 
Japanese?Mongolian bilingual dictionary. A noun 
stemming method that does not require noun 
dictionaries was also proposed. Finally, we evaluated 
the effectiveness of the components experimentally. 
 
References  
Terumasa Ehara, Suzushi Hayata, and Nobuyuki Kimura. 2004. 
Mongolian morphological analysis using ChaSen. Proceedings 
of the 10th Annual Meeting of the Association for Natural 
Language Processing, pp. 709-712. (In Japanese). 
Atsushi Fujii, Tetsuya Ishikawa, and Jong-Hyeok Lee. 2004. 
Term extraction from Korean corpora via Japanese. 
Proceedings of the 3rd International Workshop on 
Computational Terminology, pp. 71-74. 
Pascal Fung and Kathleen McKeown. 1996. Finding terminology 
translations from non-parallel corpora. Proceedings of the 5th 
Annual Workshop on Very Large Corpora, pp. 53-87. 
Wai Lam, Ruizhang Huang, and Pik-Shan Cheung. 2004. 
Learning phonetic similarity for matching named entity 
translations and mining new translations. Proceedings of the 
27th Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, pp. 
289-296. 
Sung Hyun Myaeng and Kil-Soon Jeong. 1999. 
Back-Transliteration of foreign words for information retrieval. 
Information Processing and Management, Vol. 35, No. 4, pp. 
523 -540. 
Jong-Hooh Oh and Key-Sun Choi. 2001. Automatic extraction of 
transliterated foreign words using hidden markov model. 
Proceedings of the International Conference on Computer 
Processing of Oriental Languages, 2001, pp. 433-438. 
Shigeo Ozawa. Modern Mongolian Dictionary. Daigakushorin. 
2000. 
Stephen E. Robertson, Steve Walker, Susan Jones, Micheline 
Hancock-Beaulieu, and Mike Gatford. 1995. Okapi at TREC-3,  
Proceedings of the Third Text REtrieval Conference (TREC-3), 
NIST Special Publication 500-226. pp. 109-126. 
Enkhbayar Sanduijav, Takehito Utsuro, and Satoshi Sato. 2005. 
Mongolian phrase generation and morphological analysis 
based on phonological and morphological constraints. Journal 
of Natural Language Processing, Vol. 12, No. 5, pp. 185-205. 
(In Japanese) . 
Frank Smadja, Vasileios Hatzivassiloglou, Kathleen R. McKeown. 
1996. Translating collocations for bilingual lexicons: A 
statistical approach. Computational Linguistics, Vol. 22, No. 1, 
pp. 1-38.  
Bayarmaa Ts. 2002. Mongolian grammar in I-IV grades. (In 
Mongolian). 
664
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 15?22,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A System for Summarizing and Visualizing Arguments in Subjective
Documents: Toward Supporting Decision Making
Atsushi Fujii
Graduate School of Library,
Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba, 305-8550, Japan
fujii@slis.tsukuba.ac.jp
Tetsuya Ishikawa
The Historiographical Institute
The University of Tokyo
3-1 Hongo 7-chome, Bunkyo-ku
Tokyo, 133-0033, Japan
ishikawa@hi.u-tokyo.ac.jp
Abstract
On the World Wide Web, the volume of
subjective information, such as opinions
and reviews, has been increasing rapidly.
The trends and rules latent in a large set of
subjective descriptions can potentially be
useful for decision-making purposes. In
this paper, we propose a method for sum-
marizing subjective descriptions, specifi-
cally opinions in Japanese. We visual-
ize the pro and con arguments for a target
topic, such as ?Should Japan introduce the
summertime system?? Users can summa-
rize the arguments about the topic in order
to choose a more reasonable standpoint for
decision making. We evaluate our system,
called ?OpinionReader?, experimentally.
1 Introduction
On the World Wide Web, users can easily dissem-
inate information irrespective of their own spe-
cialty. Thus, natural language information on the
Web is not restricted to objective and authorized
information, such as news stories and technical
publications. The volume of subjective informa-
tion, such as opinions and reviews, has also been
increasing rapidly.
Although a single subjective description by
an anonymous author is not always reliable, the
trends and rules latent in a large set of subjective
descriptions can potentially be useful for decision-
making purposes.
In one scenario, a user may read customer re-
views before choosing a product. In another sce-
nario, a user may assess the pros and cons of a po-
litical issue before determining their own attitude
on the issue.
The decision making in the above scenarios is
performed according to the following processes:
(1) collecting documents related to a specific
topic from the Web;
(2) extracting subjective descriptions from the
documents;
(3) classifying the subjective descriptions ac-
cording to their polarity, such as posi-
tive/negative or pro/con;
(4) organizing (e.g., summarizing and/or visual-
izing) the classified descriptions so that users
can view important points selectively;
(5) making the decision.
Because it is expensive to perform all of the above
processes manually, a number of automatic meth-
ods have been explored. Specifically, a large num-
ber of methods have been proposed to facilitate
processes (2) and (3).
In this paper, we focus on process (4), and pro-
pose a method for summarizing subjective infor-
mation, specifically opinions in Japanese. Our
method visualizes the pro and con arguments for
a target topic, such as ?Should Japan introduce the
summertime system??
By process (4), users can summarize the argu-
ments about the topic in order to choose a more
reasonable standpoint on it. Consequently, our
system supports decision making by users.
However, process (5) is beyond the scope of this
paper, and remains an intellectual activity for hu-
man beings.
We describe and demonstrate our prototype sys-
tem, called ?OpinionReader?. We also evaluate
the components of our system experimentally.
Section 2 surveys previous research on the pro-
cessing of subjective information. Section 3 pro-
vides an overview of OpinionReader, and Sec-
15
tion 4 describes the methodologies of its compo-
nents. Section 5 describes the experiments and
discusses the results obtained.
2 Related Work
For process (1) in Section 1, existing search en-
gines can be used to search the Web for documents
related to a specific topic. However, not all re-
trieved documents include subjective descriptions
for the topic.
A solution to this problem is to automatically
identify diaries and blogs (Nanno et al, 2004),
which usually include opinionated subjective de-
scriptions.
For process (2), existing methods aim to dis-
tinguish between subjective and objective descrip-
tions in texts (Kim and Hovy, 2004; Pang and Lee,
2004; Riloff and Wiebe, 2003).
For process (3), machine-learning methods are
usually used to classify subjective descriptions
into bipolar categories (Dave et al, 2003; Beineke
et al, 2004; Hu and Liu, 2004; Pang and Lee,
2004) or multipoint scale categories (Kim and
Hovy, 2004; Pang and Lee, 2005).
For process (4), which is the subject of this pa-
per, Ku et al (2005) selected documents that in-
clude a large number of positive or negative sen-
tences about a target topic, and used their head-
lines as a summary of the topic. This is the appli-
cation of an existing extraction-based summariza-
tion method to subjective descriptions.
Hu and Liu (2004) summarized customer re-
views of a product such as a digital camera. Their
summarization method extracts nouns and noun
phrases as features of the target product, (e.g.,
?picture? for a digital camera), and lists positive
and negative reviews on a feature-by-feature basis.
The extracted features are sorted according to
the frequency with which each feature appears in
the reviews. This method allows users to browse
the reviews in terms of important features of the
target product.
Liu et al (2005) enhanced the above method to
allow users to compare different products within a
specific category, on a feature-by-feature basis.
3 Overview of OpinionReader
Figure 1 depicts the process flow in Opinion-
Reader. The input is a set of subjective descrip-
tions for a specific topic, classified according to
their polarity. We assume that processes (1)?(3) in
Section 1 are completed, either manually or auto-
matically, prior to the use of our system. It is of-
ten the case that users post their opinions and state
their standpoints, as exemplified by the websites
used in our experiments (see Section 5).
While our primarily target is a set of opinions
for a debatable issue classified into pros and cons,
a set of customer reviews for a product, classified
as positive or negative, can also be submitted.
extracting points at issue
arranging points at issue
ranking opinions
opinions about a topic
pros cons
Figure 1: Process flow in OpinionReader.
Our purpose is to visualize the pro and con ar-
guments about a target topic, so that a user can de-
termine which standpoint is the more reasonable.
We extract ?points at issue? from the opinions
and arrange them in a two-dimensional space. We
also rank the opinions that include each point at
issue according to their importance, so that a user
can selectively read representative opinions on a
point-by-point basis.
The output is presented via a graphical inter-
face as shown in Figure 2, which is an example
output for the topic ?privatization of hospitals by
joint-stock companies?. The opinions used for this
example are extracted from the website for ?BS
debate?1. This interface is accessible via existing
Web browsers.
In Figure 2, the x and y axes correspond to
the polarity and importance respectively, and each
oval denotes an extracted point at issue, such as
?information disclosure?, ?health insurance?, or
?medical corporation?.
Users can easily see which points at issue are
most important from each standpoint. Points at
issue that are important and closely related to one
particular standpoint are usually the most useful in
users? decision making.
By clicking on an oval in Figure 2, users can
read representative opinions corresponding to that
1http://www.nhk.or.jp/bsdebate/
16
point at issue. In Figure 3, two opinions that in-
clude ?information disclosure? are presented. The
opinions on the right and left sides are selected
from the pros and cons, respectively. While the
pros support information disclosure, the cons in-
sist that they have not recognized its necessity.
As a result, users can browse the pro and con
arguments about the topic in detail. However, for
some points at issue, only opinions from a single
standpoint are presented, because the other side
has no argument about that point.
Given the above functions, users can easily
summarize the main points and how they are used
in arguing about the topic in support of one stand-
point or the other.
If subjective descriptions are classified into
more than two categories with a single axis, we
can incorporate these descriptions into our system
by reclassifying them into just two categories. Fig-
ure 4 is an example of summarizing reviews with a
multipoint scale rating. We used reviews with five-
point star rating for the movie ?Star Wars: Episode
III?2. We reclassified reviews with 1?3 stars as
cons, and reviews with 4?5 stars as pros.
In Figure 4, the points at issue are typical
words used in the movie reviews (e.g. ?story?),
the names of characters (e.g. ?Anakin?, ?Obi-
Wan?, and ?Palpatine?), concepts related to Star
Wars (e.g. ?battle scene? and ?Dark Side?), and
comparisons with other movies (e.g., ?War of the
Worlds?).
Existing methods for summarizing opin-
ions (Hu and Liu, 2004; Liu et al, 2005). extract
the features of a product, which corresponds to
the points at issue in our system, and arrange them
along a single dimension representing the impor-
tance of features. The reviews corresponding to
each feature are not ranked.
However, in our system, features are arranged to
show how the feature relates to each polarity. The
opinions addressing a feature are ranked according
to their importance. We target both opinions and
reviews, as shown in Figures 2 and 4, respectively.
4 Methodology
4.1 Extracting Points at Issue
In a preliminary investigation of political opin-
ions on the Web, we identified that points at issue
can be different language units: words, phrases,
2http://moviessearch.yahoo.co.jp/detail?ty=mv&id=321602
sentences, and combinations of sentences. We
currently target nouns, noun phrases, and verb
phrases, whereas existing summarization meth-
ods (Hu and Liu, 2004; Liu et al, 2005) extract
only nouns and noun phrases.
Because Japanese sentences lack lexical seg-
mentation, we first use ChaSen3 to perform a mor-
phological analysis of each input sentence. As a
result, we can identify the words in the input and
their parts of speech.
To extract nouns and noun phrases, we use
handcrafted rules that rely on the word and part-of-
speech information. We extract words and word
sequences that match these rules. To standard-
ize among the different noun phrases that describe
the same content, we paraphrase specific types of
noun phrases.
To extract verb phrases, we analyze the syntac-
tic dependency structure of each input sentence,
by using CaboCha4. We then use handcrafted rules
to extract verb phrases comprising a noun and a
verb from the dependency structure.
It is desirable that the case of a noun (i.e., post-
positional particles) and the modality of a verb
(i.e., auxiliaries) are maintained. However, if we
were to allow variations of case and modality, verb
phrases related to almost the same meaning would
be regarded as different points at issue and thus the
output of our system would contain redundancy.
Therefore, for the sake of conciseness, we cur-
rently discard postpositional particles and auxil-
iaries in verb phrases.
4.2 Arranging Points at Issue
In our system, the points at issue extracted as
described in Section 4.1 are arranged in a two-
dimensional space, as shown in Figure 2. The x-
axis corresponds to the polarity of the points at is-
sue, that is the degree to which a point is related
to each standpoint. The y-axis corresponds to the
importance of the points at issue.
For a point at issue A, which can be a noun,
noun phrase, or verb phrase, the x-coordinate, xA,
is calculated by Equation (1):
xA = P (pro|A)? P (con|A) (1)
P (S|A), in which S denotes either the pro or con
standpoint, is the probability that an opinion ran-
domly selected from a set of opinions addressing
3http://chasen.naist.jp/hiki/ChaSen/
4http://cl.aist-nara.ac.jp/?taku-ku/software/cabocha/
17
JGCNVJKPUWTCPEG
KPHQTOCVKQPFKUENQUWTG
OGFKECNEQTRQTCVKQP
RTQHKV
EQP RTQ
KORTQXGOGPV
EQUOGVKEUWTIGT[
EWUVQOGTPGGFU
OGFKECNVTGCVOGPV
KPHQTOCVKQP
Figure 2: Example of visualizing points at issue for ?privatization of hospitals by joint-stock companies?.
1RKPKQPUQH2TQ1RKPKQPQH%QP
Figure 3: Example of presenting representative opinions for ?information disclosure?.
1DK9CP
#PCMKP
2CNRCVKPGUVQT[
EQP RTQ DCVVNGUEGPG
&CTM5KFG
9CTQHVJG9QTNFU
Figure 4: Example of summarizing reviews with multipoint scale rating for ?Star Wars: Episode III?.
18
A supports S. We calculate P (S|A) as the num-
ber of opinions that are classified into S and that
include A, divided by the number of opinions that
include A.
xA ranges from ?1 to 1. A is classified into one
of the following three categories depending on the
value of xA:
? if A appears in the pros more frequently than
in the cons, xA is a positive number,
? if A appears in the pros and cons equally of-
ten, xA is zero,
? if A appears in the cons more frequently than
in the pros, xA is a negative number.
The calculation of the y-coordinate of A, yA de-
pends on which of the above categories applies to
A. If A appears in standpoint S more frequently
than in its opposite, we define yA as the probabil-
ity that a point at issue randomly selected from the
opinions classified into S is A.
We calculate yA as the frequency of A in the
opinions classified into S, divided by the total fre-
quencies of points at issue in the opinions classi-
fied into S. Thus, yA ranges from 0 to 1.
However, if A appears in the pros and cons
equally often, we use the average of the values of
yA for both standpoints.
General words, which are usually high fre-
quency words, tend to have high values for yA.
Therefore, we discard the words whose yA is
above a predefined threshold. We empirically set
the threshold at 0.02.
Table 1 shows example points at issue for the
topic ?privatization of hospitals by joint-stock
companies? and their values of xA and yA. In Ta-
ble 1, points at issue, which have been translated
into English, are classified into the three categories
(i.e., pro, neutral, and con) according to xA and
are sorted according to yA in descending order, for
each category.
In Table 1, ?improvement? is the most impor-
tant in the pro category, and ?medical corporation?
is the most important in the con category. In the
pro category, many people expect that the qual-
ity of medical treatment will be improved if joint-
stock companies make inroads into the medical in-
dustry. However, in the con category, many people
are concerned about the future of existing medical
corporations.
Table 1: Examples of points at issue and their co-
ordinates for ?privatization of hospitals by joint-
stock companies?.
Point at issue xA yA
improvement 0.33 9.2?10?3
information disclosure 0.33 7.9?10?3
health insurance 0.60 5.3?10?3
customer needs 0.50 3.9?10?3
cosmetic surgery 0.00 2.6?10?3
medical corporation ?0.69 4.4?10?3
medical institution ?0.64 3.6?10?3
medical cost ?0.60 3.2?10?3
profit seeking ?0.78 3.2?10?3
4.3 Ranking Opinions
Given a set of opinions from which a point at is-
sue has been extracted, our purpose now is to rank
the opinions in order of importance. We assume
that representative opinions contain many content
words that occur frequently in the opinion set. In
our case, content words are nouns, verbs, and ad-
jectives identified by morphological analysis.
We calculate the score of a content word w,
s(w), as the frequency of w in the opinion set. We
calculate the importance of an opinion by the sum
of s(w) for the words in the opinion. However,
we normalize the importance of the opinion by the
number of words in the opinion because long opin-
ions usually include many words.
5 Experiments
5.1 Method
The effectiveness of our system should be evalu-
ated from different perspectives. First, the effec-
tiveness of each component of our system should
be evaluated. Second, the effectiveness of the sys-
tem as a whole should be evaluated. In this second
evaluation, the evaluation measure is the extent to
which the decisions of users can be made correctly
and efficiently.
As a first step in our research, in this paper
we perform only the first evaluation and evaluate
the effectiveness of the methods described in Sec-
tion 4. We used the following Japanese websites
as the source of opinions, in which pros and cons
are posted for specific topics.
(a) BS debate5
(b) ewoman6
5http://www.nhk.or.jp/bsdebate/
6http://www.ewoman.co.jp/
19
(c) Official website of the prime minister of
Japan and his cabinet7
(d) Yomiuri online8
For evaluation purposes, we collected the pros and
cons for five topics. Table 2 shows the five top-
ics, the number of opinions, and the sources. For
topic #4, we used the opinions collected from two
sources to increase the number of opinions.
In Table 2, the background of topic #5 should
perhaps be explained. When using escalators, it
is often customary for passengers to stand on one
side (either left or right) to allow other passen-
gers to walk past them. However, some people
insist that walking on escalators, which are mov-
ing stairs, is dangerous.
Graduate students, none of who was an author
of this paper, served as assessors, and produced
reference data. The output of a method under eval-
uation was compared with the reference data.
For each topic, two assessors were assigned to
enhance the degree of objectivity of the results. Fi-
nal results were obtained by averaging the results
over the assessors and the topics.
5.2 Evaluation of Extracting Points at Issue
For each topic used in the experiments, the asses-
sors read the opinions from both standpoints and
extracted the points at issue. We defined the point
at issue as the grounds for an argument. We did not
restrict the form of the points at issue. Thus, the
assessors were allowed to extract any continuous
language units, such as words, phrases, sentences,
and paragraphs, as points at issue.
Because our method is intended to extract
points at issue exhaustively and accurately, we
used recall and precision as evaluation measures
for the extraction.
Recall is the ratio of the number of correct an-
swers extracted automatically to the total number
of correct answers. Precision is the ratio of the
number of correct answers extracted automatically
to the total number of points at issue extracted au-
tomatically.
Table 3 shows the results for each topic, in
which ?System? denotes the number of points at
issue extracted automatically. In Table 3, ?C?,
?R?, and ?P? denote the number of correct an-
swers, recall, and precision, respectively, on an
assessor-by-assessor basis.
7http://www.kantei.go.jp/
8http://www.yomiuri.co.jp/komachi/forum/
Looking at Table 3, we see that the results
can vary depending on the topic and the assessor.
However, recall and precision were approximately
50% and 4%, respectively, on average.
The ratio of agreement between assessors was
low. When we used the points at issue extracted
by one assessor as correct answers and evaluated
the effectiveness of the other assessor in the ex-
traction, the recall and precision ranged from 10%
to 20% depending on the topic. To increase the ra-
tio of agreement between assessors, the instruction
for assessors needs to be revised for future work.
This was mainly because the viewpoint for a tar-
get topic and the language units to be extracted
were different, depending on the assessor. Be-
cause our automatic method extracted points at is-
sue exhaustively, the recall was high and the pre-
cision was low, irrespective of the assessor.
The ratios of noun phrases (including nouns)
and verb phrases to the number of manually ex-
tracted points at issue were 78.5% and 2.0%, re-
spectively. Although the ratio for verb phrases
is relatively low, extracting both noun and verb
phrases is meaningful.
The recalls of our method for noun phrases and
verb phrases were 60.0% and 44.3%, respectively.
Errors were mainly due to noun phrases that were
not modeled in our method, such as noun phrases
that include a relative clause.
5.3 Evaluation of Arranging Points at Issue
As explained in Section 4.2, in our system the
points at issue are arranged in a two-dimensional
space. The x and y axes correspond to the polarity
and the importance of points at issue, respectively.
Because it is difficult for the assessors to judge
the correctness of coordinate values in the two-
dimensional space, we evaluated the effectiveness
of arranging points at issue indirectly.
First, we evaluated the effectiveness of the cal-
culation for the y-axis. We sorted the points at is-
sue, which were extracted automatically (see Sec-
tion 5.2), according to their importance. We eval-
uated the trade-off between recall and precision
by varying the threshold of yA. We discarded the
points at issue whose yA is below the threshold.
Note that while this threshold was used to de-
termine the lower bound of yA, the threshold ex-
plained in Section 4.2 (i.e., 0.02) was used to de-
termine the upper bound of yA and was used con-
sistently irrespective of the lower bound threshold.
20
Table 2: Topics used for experiments.
#Opinions
Topic ID Topic Pro Con Source
#1 principle of result in private companies 57 29 (a)
#2 privatization of hospitals by joint-stock companies 27 44 (a)
#3 the summertime system in Japan 14 17 (b)
#4 privatization of postal services 28 20 (b), (c)
#5 one side walk on an escalator 29 42 (d)
Table 3: Recall and precision of extracting points at issue (C: # of correct answers, R: recall (%), P:
precision (%)).
Assessor A Assessor B
Topic ID System C R P C R P
#1 1968 194 58.2 5.7 101 44.6 2.3
#2 1864 66 50.0 1.8 194 60.8 6.3
#3 508 43 48.8 4.1 43 60.5 5.1
#4 949 77 64.9 5.3 96 36.5 3.7
#5 711 91 30.0 3.8 75 18.7 2.0
Table 4 shows the results, in which the precision
was improved to 50% by increasing the threshold.
In Figure 2, users can change the threshold of im-
portance by using the panel on the right side to
control the number of points at issue presented in
the interface. As a result, users can choose appro-
priate points at issue precisely.
Second, we evaluated the effectiveness of the
calculation for the x-axis. We evaluated the effec-
tiveness of our method in a binary classification.
For each point at issue extracted by an assessor,
the assessor judged which of the two standpoints
the point supports.
If a point at issue whose x-coordinate calculated
by our method is positive (or negative), it was clas-
sified as pro (or con) automatically. We did not use
the points at issue whose x-coordinate was zero for
evaluation purposes.
Table 5 shows the results. While the number of
target points at issue was different depending on
the topic and the assessor, the difference in classi-
fication accuracy was marginal.
For each topic, we averaged the accuracy deter-
mined by each assessor and averaged the accura-
cies over the topic, which gave 95.6%. Overall,
our method performs the binary classification for
points at issue with a high accuracy.
Errors were mainly due to opinions that in-
cluded arguments for both standpoints. For exam-
ple, a person supporting a standpoint might sug-
gest that he/she would support the other side un-
der a specific condition. Points at issue classified
incorrectly had usually been extracted from such
contradictory opinions.
5.4 Evaluation of Ranking Opinions
To evaluate the effectiveness of our method in
ranking opinions on a point-by-point basis, we
used a method that sorts the opinions randomly
as a control. We compared the accuracy of our
method and that of the control. The accuracy is
the ratio of the number of correct answers to the
number of opinions presented by the method un-
der evaluation.
For each point at issue extracted by an assessor,
the assessor assigned the opinions to one of the
following degrees:
? A: the opinion argues about the point at issue
and is represented,
? B: the opinion argues about the point at issue
but is not represented,
? C: the opinion includes the point at issue but
does not argue about it.
We varied the number of top opinions presented
by changing the threshold for the rank of opinions.
Table 6 shows the results, in which N denotes
the number of top opinions presented. The column
?Answer? refers to two cases: the case in which
only the opinions assigned to ?A? were regarded
as correct answers, and the case in which the opin-
ions assigned to ?A? or ?B? were regarded as cor-
rect answers. In either case, our method outper-
formed the control in ranking accuracy.
Although the accuracy of our method for ?A?
opinions was low, the accuracy for ?A? and ?B?
21
Table 4: Trade-off between recall and precision in extracting points at issue.
Threshold 0 0.002 0.004 0.006 0.008 0.010
Recall 0.48 0.17 0.11 0.04 0.03 0.02
Precision 0.04 0.14 0.21 0.31 0.33 0.50
Table 5: Accuracy for classifying points at issue.
Assessor A Assessor B
Topic ID #Points Accuracy (%) #Points Accuracy (%)
#1 113 98.2 45 97.7
#2 33 91.0 118 94.1
#3 21 95.2 26 100
#4 50 92.0 35 91.4
#5 27 96.3 14 100
Table 6: Accuracy of ranking opinions.
Answer Method N = 1 N = 2 N = 3
A Random 19% 28% 19%
Ours 38% 32% 23%
A+B Random 81% 83% 75%
Ours 87% 87% 83%
opinions was high. This suggests that our method
is effective in distinguishing opinions that argue
about a specific point and opinions that include the
point but do not argue about it.
6 Conclusion
In aiming to support users? decision making, we
have proposed a method for summarizing and vi-
sualizing the pro and con arguments about a topic.
Our prototype system, called ?OpinionReader?,
extracts points at issue from the opinions for both
pro and con standpoints, arranges the points in a
two-dimensional space, and allows users to read
important opinions on a point-by-point basis. We
have experimentally evaluated the effectiveness of
the components of our system.
Future work will include evaluating our system
as a whole, and summarizing opinions that change
over time.
References
Philip Beineke, Trevor Hastie, and Shivakumar
Vaithyanathan. 2004. The sentimental factor: Im-
proving review classification via human-provided
information. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, pages 264?271.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
Proceedings of the 12th International World Wide
Web Conference.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics, pages 1367?1373.
Lun-Wei Ku, Li-Ying Lee, Tung-Ho Wu, and Hsin-Hsi
Chen. 2005. Major topic detection and its appli-
cation to opinion summarization. In Proceedings of
the 28th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 627?628.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opin-
ions on the Web. In Proceedings of the 14th Interna-
tional World Wide Web Conference, pages 324?351.
Tomoyuki Nanno, Toshiaki Fujiki, Yasuhiro Suzuki,
and Manabu Okumura. 2004. Automatically col-
lecting, monitoring, and mining Japanese weblogs.
In The 13th International World Wide Web Confer-
ence, pages 320?321. (poster session).
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity sum-
marization based on minimum cuts. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics, pages 264?271.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 115?124.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the 2003 Conference on Empirical Methods
in Natural Language Processing, pages 105?112.
22
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 242?249,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Modeling Impression in Probabilistic Transliteration into Chinese
LiLi Xu? Atsushi Fujii
Graduate School of Library,
Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba, 305-8550, Japan
fujii@slis.tsukuba.ac.jp
Tetsuya Ishikawa
The Historiographical Institute
The University of Tokyo
3-1 Hongo 7-chome, Bunkyo-ku
Tokyo, 133-0033, Japan
ishikawa@hi.u-tokyo.ac.jp
Abstract
For transliterating foreign words into Chi-
nese, the pronunciation of a source word
is spelled out with Kanji characters. Be-
cause Kanji comprises ideograms, an indi-
vidual pronunciation may be represented
by more than one character. However,
because different Kanji characters convey
different meanings and impressions, char-
acters must be selected carefully. In this
paper, we propose a transliteration method
that models both pronunciation and im-
pression, whereas existing methods do not
model impression. Given a source word
and impression keywords related to the
source word, our method derives possible
transliteration candidates and sorts them
according to their probability. We evalu-
ate our method experimentally.
1 Introduction
Reflecting the rapid growth of science, technology,
and economies, new technical terms and product
names have progressively been created. These
new words have also been imported into different
languages. There are three fundamental methods
for importing foreign words into a language.
In the first method?translation?the meaning
of the source word in question is represented by
an existing or new word in the target language.
In the second method?transliteration?the
pronunciation of the source word is represented by
using the phonetic alphabet of the target language,
such as Katakana in Japanese and Hangul in Ko-
rean.
? This work was done when the first author was a grad-
uate student at University of Tsukuba, who currently works
for Hitachi Construction Machinery Co., Ltd.
In the third method, the source word is spelled
out as it is. However, the misuse of this method
decreases the understandability and readability of
the target language.
While translation is time-consuming, requiring
selection of an existing word or generation of a
new word that correctly represents the meaning of
the source word, transliteration can be performed
rapidly. However, the situation is complicated for
Chinese, where a phonetic alphabet is not used and
Kanji is used to spell out both conventional Chi-
nese words and foreign words.
Because Kanji comprises ideograms, an in-
dividual pronunciation can potentially be repre-
sented by more than one character. However, if
several Kanji strings are related to the same pro-
nunciation of the source word, their meanings will
be different and will therefore convey different im-
pressions.
For example, ?Coca-Cola? can be represented
by different Kanji strings in Chinese. The offi-
cial transliteration is ??????, which comprises
??? (tasty)? and ??? (pleasant)?, and is there-
fore associated with a positive connotation.
However, there are a number of Kanji strings
that represent similar pronunciations to that of
?Coca-Cola?, but which are associated with in-
appropriate impressions for a beverage, such as
??????. This word includes ????, which is
associated with choking.
Therefore, Kanji characters must be selected
carefully during transliteration into Chinese. This
is especially important when foreign companies
intend to introduce their names and products into
China.
In this paper, we propose a method that models
both impression and pronunciation for translitera-
tion into Chinese.
242
Section 2 surveys previous research into auto-
matic transliteration, in order to clarify the mean-
ing and contribution of our research. Section 3
elaborates on our transliteration method. Section 4
evaluates the effectiveness of our method.
2 Related Work
In a broad sense, the term ?transliteration? has
been used to refer to two tasks.
The first task is transliteration in the strict
sense, which creates new words in a target lan-
guage (Haizhou et al, 2004; Wan and Verspoor,
1998).
The second task is back-transliteration (Fujii
and Ishikawa, 2001; Jeong et al, 1999; Knight
and Graehl, 1998; Qu et al, 2003), which iden-
tifies the source word corresponding to an exist-
ing transliterated word. Back-transliteration is in-
tended mainly for cross-lingual information re-
trieval and machine translation.
Both transliteration tasks require methods that
model pronunciation in the source and target lan-
guages.
However, by definition, in back-transliteration,
the word in question has already been transliter-
ated and the meaning or impression of the source
word does not have to be considered. Thus, back-
transliteration is outside the scope of this paper.
In the following, we use the term ?translitera-
tion? to refer to transliteration in the strict sense.
Existing transliteration methods for Chi-
nese (Haizhou et al, 2004; Wan and Verspoor,
1998) aim to spell out foreign names of people
and places, and do not model impression.
However, as exemplified by ?Coca-Cola? in
Section 1, the impression of words needs to be
modeled in the transliteration of proper names,
such as companies and products. The contribu-
tion of our research is to incorporate a model of
impression into automatic transliteration.
3 Methodology
3.1 Overview
Figure 1 shows our transliteration method, which
models both pronunciation and impression when
transliterating foreign words into Chinese. We
will explain the entire process of our translitera-
tion method in terms of Figure 1.
The input for our method is twofold. First, a
source word to be transliterated into Chinese is re-
quested. Second, one or more words that describe
source word Impression keyword(s)
pronunciation model impression model
ranked list of transliteration candidates
ranking candidates
?? (safeguard)?? (another person)?? (live)?? (nutrition)
????(bitamin)
?, ?, ?, ?, ?, ??????????? ?
Transliteration candidates Kanji characters
Figure 1: Overview of our transliteration method
for Chinese.
the impression of the source word, which we call
?impression keywords?, are requested. Currently,
impression keywords must be provided manually
in Chinese. The output of our method is one or
more Kanji strings.
In an example scenario using our method, a user
has a good command of Chinese and intends to
introduce something (e.g., a company or product)
into China. It is reasonable to assume that this user
can provide one or more Chinese impression key-
words to associate with the target object.
Using the pronunciation model, the source word
is converted into a set of Kanji strings whose pro-
nunciation is similar to that of the source word.
Each of these Kanji strings is a transliteration can-
didate.
Currently, we use Japanese Katakana words as
source words, because Katakana words can easily
be converted into pronunciations using the Latin
alphabet. However, in principle, any language that
uses phonetic script can be a source language for
our method. In Figure 1, the Katakana word ?bita-
min (vitamin)? is used as an example source word.
Using the impression model, impression key-
words are converted into a set of Kanji characters.
A simple implementation is to segment each im-
pression keyword into characters.
However, because it is difficult for a user to pro-
vide an exhaustive list of appropriate keywords
and characters, our impression model derives char-
acters that are not included in the impression key-
words.
Because of the potentially large number of se-
lected candidates, we need to rank the candidates.
We model both pronunciation and impression in
243
a probabilistic framework, so that transliteration
candidates are sorted according to their probabil-
ity score.
Transliteration candidates that include many
characters derived from the impression model are
preferred. In other words, the Kanji characters
derived via the impression model are used to re-
rank the candidates derived via the pronunciation
model.
We elaborate on our probabilistic transliteration
model in Section 3.2. We then discuss the pronun-
ciation and impression models in Sections 3.3 and
3.4, respectively.
3.2 Probabilistic Transliteration Model
Given a romanized Japanese Katakana word R
and a set of impression keywords W , our pur-
pose is to select the Kanji string K that maxi-
mizes P (K|R,W ), which is evaluated as shown
in Equation (1), using Bayes? theorem.
P (K|R,W ) = P (R,W |K) ? P (K)P (R,W )
? P (R|K) ? P (W |K) ? P (K)P (R,W )
? P (R|K) ? P (W |K) ? P (K)
(1)
In the second line of Equation (1), we assume the
conditional independence of R and W given K.
In the third line, we omit P (R,W ), which is in-
dependent of K. This does not affect the rela-
tive rank of Kanji strings, when ranked in terms
of P (K|R,W ).
In Figure 1, R and W are ?bitamin? and
?????????????, respectively, and a K
candidate is ?????.
If a user intends to select more than one Kanji
string, those Ks associated with higher probabili-
ties should be selected.
As shown in Equation (1), P (K|R,W ) can
be approximated by the product of P (R|K),
P (W |K), and P (K). We call these three factors
the pronunciation, impression, and language mod-
els, respectively.
The language model, P (K), models the proba-
bility of K irrespective of R and W . In probabilis-
tic natural language processing, P (K) is usually
realized by a word or character N-gram model, and
therefore a K that appears frequently in a corpus
is assigned a high probability.
However, because our purpose is to generate
new words, the use of statistics obtained from ex-
isting corpora is not effective. Therefore, we con-
sider P (K) to be constant for every K.
In summary, P (K|R,W ) is approximated by a
product of P (R|K) and P (W |K). The quality of
our transliteration method will depend on the im-
plementation of the pronunciation and impression
models.
3.3 Pronunciation Model
The pronunciation model, P (R|K), models the
probability that a roman representation R is se-
lected, given a Kanji string K.
In Japanese, the Hepburn and Kunrei systems
are commonly used for romanization purposes.
We use the Hepburn system. We use Pinyin as
a representation for Kanji characters. We decom-
pose K into Kanji characters and associate K with
R on a character-by-character basis. We calculate
P (R|K) as shown in Equation (2).
P (R|K) ? P (R|Y ) ? P (Y |K)
?
N?
i=1
P (ri|yi) ?
N?
j=1
P (yj |kj)
(2)
Y denotes the Pinyin strings representing the pro-
nunciation of K. ki denotes a single Kanji char-
acter. ri and yi denote substrings of R and Y ,
respectively. R, Y , and K are decomposed into
the same number of elements, namely N . We cal-
culate P (ri|yi) and P (yi|ki) as shown in Equa-
tion (3).
P (ri|yi) = F (ri, yi)?
r
F (r, yi)
P (yi|ki) = F (yi, ki)?
y
F (y, ki)
(3)
F (x, y) denotes the co-occurrence frequency of x
and y. We need the co-occurrence frequencies of
ri and yi and the co-occurrence frequencies of yi
and ki in order to calculate P (R|K).
We used a bilingual dictionary comprising 1 140
Katakana words, most of which are technical
terms and proper nouns, and their transliterations
into Chinese, which are annotated with Pinyin. We
manually corresponded 151 pairs of Katakana and
roman characters on a mora-by-mora basis, and
romanized Katakana characters in the dictionary
automatically.
We obtained 1 140 tuples, of the form
< R, Y,K >. Because the number of tuples was
244
manageable, we obtained the element-by-element
R, Y , and K correspondences manually. Finally,
we calculated F (ri, yi) and F (yi, ki).
If there are many tuples, and the process of man-
ual correspondence is expensive, we can automate
the process as performed in existing transliteration
methods, such as the EM algorithm (Knight and
Graehl, 1998) or DP matching (Fujii and Ishikawa,
2001).
The above calculations are performed off-line.
In the online process, we consider all possible seg-
mentations of a single Katakana word. For exam-
ple, the romanized Katakana word ?bitamin (vi-
tamin)? corresponds to two Pinyin strings and is
segmented differently, as follows:
? bi-ta-min: wei-ta-ming,
? bi-ta-mi-n: wei-ta-mi-an.
3.4 Impression Model
The impression model, P (W |K), models the
probability that W is selected as a set of impres-
sion keywords, given Kanji string K. As in the
calculation of P (R|K) in Equation (2), we de-
compose W and K into elements, in calculating
P (W |K).
W is decomposed into a set of words, wi, and
K is decomposed into a set of Kanji characters, kj .
We calculate P (W |K) as a product of P (wi|kj),
which is the probability that wi is selected as an
impression keyword given kj .
However, unlike Equation (2), the numbers of
wi and kj derived from W and K are not always
the same, because users are allowed to provide an
arbitrary number of impression keywords. There-
fore, for each kj we select the wi that maximizes
P (wi|kj) and approximate P (W |K) as shown in
Equation (4).
P (W |K) ?
?
j
maxwi P (wi|kj) (4)
Figure 2 shows an example in which the four Chi-
nese words in the ?wi? column are also used in
Figure 1.
We calculate P (wi|kj) by Equation (5).
P (wi|kj) = F (wi, kj)?
w
F (w, kj)
(5)
As in Equation (3), F (x, y) denotes the co-
occurrence frequency of x and y.
0.6?????? ????0.1??
??0.40.3?? ????0.5??
???iw jk
3?? ?? ?? ??_??? 3??_?h3??_?h3??_? hh
Figure 2: Example calculation of P (W |K).
In summary, we need co-occurrences of each
word and character in Chinese.
These co-occurrences can potentially be col-
lected from existing language resources, such as
corpora in Chinese.
However, it is desirable to collect an association
between a word and a character, not simply their
co-occurrence in corpora. Therefore, we used
a dictionary of Kanji in Chinese, in which each
Kanji character entry is explained via sentences,
and often exemplified by one or more words that
include that character.
We selected 599 entry characters that are often
used to spell out foreign words. Then we collected
the frequencies with which each word is used to
explain each entry character.
Because Chinese sentences lack lexical seg-
mentation, we used SuperMorpho1 to perform a
morphological analysis of explanation sentences
and example words. As a result, 16 943 word types
were extracted. We used all of these words to cal-
culate the co-occurrence frequencies, irrespective
of the parts of speech.
Table 1 shows examples of Kanji characters,
Chinese words, and their co-occurrence frequen-
cies in the dictionary.
However, P (wi|kj) cannot be calculated for the
Kanji characters not modeled in our method (i.e.,
the Kanji characters not included in the 599 entry
characters). Thus, for smoothing purposes, we ex-
perimentally set P (wi|kj) at 0.001 for those kj not
modeled.
4 Experiments
4.1 Method
We evaluated our transliteration method experi-
mentally. Because the contribution of our research
is the incorporation of the impression model in a
transliteration method, we used a method that uses
only the pronunciation model as a control.
1http://www.omronsoft.com/
245
Table 1: Example of characters, words, and their
co-occurrence frequencies.
jk  iw  ),( ji kwF  jk  iw  ),( ji kwF  jk  iw  ),( ji kwF  
? ? 39 ? ? 3 ? ?? 2 
? ?? 8 ? ?? 2 ? ?? 1 
? ? 4 ? ? 43 ? ?? 5 
? ? 4 ? ?? 2 ? ?? 2 
? ?? 2 ? ?? 2 ? ? 51 
? ? 1 ? ?? 2 ? ? 5 
? ? 2 ? ?? 2 ? ? 3 
? ?? 2 ? ?? 4 ? ?? 11 
? ?? 2 ? ?? 2 ? ?? 2 
? ?? 3 ? ?? 1 ? ?? 7 
? ?? 1 ? ?? 2 ? ?? 5 
 
From a Japanese?Chinese dictionary, we se-
lected 210 Katakana words that had been translit-
erated into Chinese, and used these Katakana
words as test words. Each test word can be clas-
sified into one of the following five categories:
products, companies, places, persons, or general
words. Details of the categories of test inputs are
shown in Table 2.
Three Chinese graduate students who had a
good command of Japanese served as assessors
and produced reference data. None of the asses-
sors was an author of this paper. The assessors
performed the same task for the same test words
independently, in order to enhance the objectivity
of the results.
We produced the reference data via the follow-
ing procedure.
First, for each test word, each assessor pro-
vided one or more impression keywords in Chi-
nese. We did not restrict the number of impression
keywords per test word, which was determined by
each assessor.
If an assessor provided more than one impres-
sion keyword for a single test word, he/she was
requested to sort them in order of preference, so
that we could investigate the effect of the number
of impression keywords on the evaluation results,
by changing the number of top keywords used for
transliteration purposes.
We provided the assessors with the descriptions
for the test words from the source dictionary, so
that the assessors could understand the meaning
of each test word.
Second, for each test word, we applied the con-
trol method and our method independently, which
produced two lists of ranked transliteration candi-
dates. Because the impression keywords provided
by the assessors were used only in our method, the
Table 2: Categories of test words.
 
Example word 
Category # Words 
Japanese Chinese English 
Product 63 ???? ?? Audi 
Company 49 ???? ??? Epson 
Place 36 ???? ??? Ohio 
Person 21 ???? ?? Chopin 
General 41 ????? ??? angel 
ranked list produced by the control was the same
for all assessors.
Third, for each test word, each assessor identi-
fied one or more correct transliterations, according
to their impression of the test word. It was impor-
tant not to reveal to the assessors which method
produced which candidates.
By these means, we selected the top 100
transliteration candidates from the two ranked lists
for the control and our method. We merged these
candidates, removed duplications, and sorted the
remaining candidates by the character code.
As a result, the assessors judged the correctness
of up to 200 candidates for each test word. How-
ever, for some test words, assessors were not able
to find correct transliterations in the candidate list.
The resultant reference data was used to eval-
uate the accuracy of a test method in ranking
transliteration candidates. We used the average
rank of correct answers in the list as the evalua-
tion measure. If more than one correct answer was
found for a single test word, we first averaged the
ranks of these answers and then averaged the ranks
over the test words.
Although we used the top 100 candidates for
judgment purposes, the entire ranked list was used
to evaluate each method. Therefore, the average
rank of correct answers can potentially be over
100. The average number of candidates per test
word was 31 779.
Because our method uses the impression model
to re-rank the candidates produced by the pronun-
ciation model, the lists for the control and our
method comprise the same candidates. Therefore,
it is fair to compare these two methods by the av-
erage rank of the correct answers.
For each test word, there is more than one type
of ?correct answer?, as follows:
(a) transliteration candidates judged as correct
by the assessors independently (translitera-
246
tion candidates judged as correct by at least
one assessor);
(b) transliteration candidates judged as correct
by all assessors;
(c) transliterations defined in the source dictio-
nary.
In (a), the coverage of correct answers is the
largest, whereas the objectivity of the judgment is
the lowest.
In (c), the objectivity of the judgment is the
largest, whereas the coverage of correct answers
is the lowest. Although for each Katakana word
the source dictionary gives only one transliteration
that is commonly used, there are a number of ap-
propriate out-of-dictionary transliterations.
In (b), where the assessors did not disagree
about the correctness, the coverage of correctness
and the objectivity are both middle ranked.
Because none of the above answer types is per-
fect, we used all three types independently.
4.2 Results and Analyses
Tables 3?5 show the results of comparative exper-
iments using the answer types (a)?(c) above, re-
spectively.
In Tables 3?5, the column ?# of test words? de-
notes the number of test words for which at least
one correct answer exists. While the values in the
second column of Table 3 are different depending
on the assessor, in Tables 4 and 5 the values of the
second column are the same for all assessors.
The columns ?Avg. # of KW? and ?Avg. # of
answers? denote the number of impression key-
words and the number of correct answers per test
word, respectively. While the values in the fourth
column of Table 3 are different depending on the
assessor, in Tables 4 and 5 the values of the fourth
column are the same for all assessors.
In Tables 4 and 5, the average rank of correct an-
swers for the control is the same for all assessors.
However, the average rank of correct answers for
our method is different depending on the assessor,
because the impression keywords used depended
on the assessor.
The two columns in ?Avg. rank? denote the av-
erage ranks of correct answers for the control and
for our method, respectively. Looking at Tables 3?
5, it can be seen that our method outperformed the
control in ranking transliteration candidates, irre-
spective of the assessor and the answer type.
The average rank of correct answers for our
method in Table 5 was lower than those in Tables 3
and 4. One reason is that the correct answers in the
source dictionary are not always related to the im-
pression keywords provided by the assessors.
Table 6 presents the results in Table 3 on a
category-by-category basis. Because the results
were similar for answer types (b) and (c), we show
only the answer type (a) results, for the sake of
conciseness. Looking at Table 6, it can be seen
that our method outperformed the control in rank-
ing transliteration candidates, irrespective of the
category of test words.
Our method was effective for transliterating
names of places and people, although these types
of words are usually transliterated independently
of their impressions, compared with the names of
products and companies.
One reason is that, in the dictionary of Kanji
used to produce the impression model, the expla-
nation of an entry sometimes includes a phrase,
such as ?this character is often used for a person?s
name?. Assessors provided the word ?person? in
Chinese as an impression keyword for a number
of person names. As a result, transliteration can-
didates that included characters typically used for
a person?s name were highly ranked.
It may be argued that, because the impression
model was produced using Kanji characters that
are often used for transliteration purposes, the im-
pression model could possibly rank correct an-
swers better than the pronunciation model. How-
ever, the pronunciation model was also produced
from Kanji characters used for transliteration pur-
poses.
Figure 3 shows the distribution of correct an-
swers for different ranges of ranks, using answer
type (a). The number of correct answers in the top
10 for our method is approximately twice that of
the control. In addition, by our method, most of
the correct answers can be found in the top 100
candidates. Because the results were similar for
answer types (b) and (c), we show only the answer
type (a) results, for the sake of conciseness.
As explained in Section 4.1, for each test word,
the assessors were requested to sort the impression
keywords in order of preference. We analyzed the
relation between the number of impression key-
words used for the transliteration and the average
rank of correct answers, by varying the threshold
for the number of top impression keywords used.
247
Table 3: Results obtained with answer type (a).
Avg. rank
Assessor # of test words Avg. # of KW Avg. # of answers Control Our method
A 205 5.1 3.8 706 82
B 204 5.8 3.8 728 44
C 199 3.5 2.6 1 130 28
Avg. 203 4.8 3.4 855 51
Table 4: Results obtained with answer type (b).
Avg. rank
Assessor # of test words Avg. # of KW Avg. # of answers Control Our method
A 108 5.1 1.1 297 22
B 108 5.8 1.1 297 23
C 108 3.5 1.1 297 18
Avg. 108 4.8 1.1 297 21
Table 5: Results obtained with answer type (c).
Avg. rank
Assessor # of test words Avg. # of KW Avg. # of answers Control Our method
A 210 5.1 1 1 738 260
B 210 5.8 1 1 738 249
C 210 3.5 1 1 738 103
Avg. 210 4.8 1 1 738 204
Table 6: Results obtained with answer type (a) on a category-by-category basis.
Avg. rank
Category # of test words Avg. # of KW Avg. # of answers Control Our method
Product 144 4.8 3.5 1 527 64
Company 186 4.7 3.6 742 54
Place 102 4.8 3.7 777 46
Person 61 5.0 3.4 766 51
General 115 4.7 2.6 280 38
Avg. 122 4.8 3.4 818 51
?
???
???
???
???
???
???
???
???
???
???? ????
?
????
??
????
???
????
????
????
????
?
????
????
??
????
??
????
???
???
???
???
???
???
??
???????
??????????
Figure 3: Distribution of average rank for correct answers.
248
Table 7 shows the average rank of correct an-
swers for different numbers of impression key-
words, on an assessor-by-assessor basis. By com-
paring Tables 3 and 7, we see that even if a sin-
gle impression keyword was provided, the average
rank of correct answers was higher than that for
the control. In addition, the average rank of correct
answers was generally improved by increasing the
number of impression keywords.
Finally, we investigated changes in the rank of
correct answers caused by our method. Table 8
shows the results, in which ?Higher? and ?Lower?
denote the number of correct answers whose ranks
determined by our method were higher or lower,
respectively, than those determined by the control.
For approximately 30% of the correct answers,
our method decreased the control?s rank. Errors
were mainly caused by correct answers containing
Kanji characters that were not modeled in the im-
pression model. Although we used a smoothing
technique for characters not in the model, the re-
sult was not satisfactory. To resolve this problem,
the number of characters in the impression model
should be increased.
In summary, our method, which uses both the
impression and pronunciation models, ranked cor-
rect transliterations more highly than a method
that used only the pronunciation model. We con-
clude that the impression model is effective for
transliterating foreign words into Chinese. At the
same time, we concede that there is room for im-
provement in the impression model.
5 Conclusion
For transliterating foreign words into Chinese, the
pronunciation of a source word is spelled out with
Kanji characters. Because Kanji characters are
ideograms, a single pronunciation can be repre-
sented by more than one character. However, be-
cause different Kanji characters convey different
meanings and impressions, characters must be se-
lected carefully.
In this paper, we proposed a transliteration
method that models both pronunciation and im-
pression, compared to existing methods that do
not model impression. Given a source word and
impression keywords related to the source word,
our method derives possible transliteration candi-
dates, and sorts them according to their probabil-
ity. We showed the effectiveness of our method
experimentally.
Table 7: Relation between the number of impres-
sion keywords and average rank of correct answers
with answer type (a).
# of KW
Assessor 1 2 3
A 103 94 92
B 64 60 52
C 113 73 34
Table 8: Changes in ranks of correct answers
caused by our method.
Avg. rank
Answer type # of answers Higher Lower
(a) 2 070 1 431 639
(b) 360 250 110
(c) 630 422 208
Future work will include collecting impression
keywords automatically, and adapting the lan-
guage model to the category of source words.
References
Atsushi Fujii and Tetsuya Ishikawa. 2001.
Japanese/English cross-language information
retrieval: Exploration of query translation and
transliteration. Computers and the Humanities,
35(4):389?420.
Li Haizhou, Zhang Min, and Su Jian. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages
160?167.
Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee,
and Key-Sun Choi. 1999. Automatic identification
and back-transliteration of foreign words for infor-
mation retrieval. Information Processing & Man-
agement, 35:523?540.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Yan Qu, Gregory Grefenstette, and David A. Evans.
2003. Automatic transliteration for Japanese-to-
English text retrieval. In Proceedings of the 26th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
pages 353?360.
Stephen Wan and Cornelia Maria Verspoor. 1998. Au-
tomatic English-Chinese name transliteration for de-
velopment of multilingual resources. In Proceed-
ings of the 36th Annual Meeting of the Association
for Computational Linguistics and the 17th Inter-
national Conference on Computational Linguistics,
pages 1352?1356.
249
Summarizing Encyclopedic Term Descriptions on the Web
Atsushi Fujii and Tetsuya Ishikawa
Graduate School of Library, Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba, 305-8550, Japan
{fujii,ishikawa}@slis.tsukuba.ac.jp
Abstract
We are developing an automatic method to
compile an encyclopedic corpus from the
Web. In our previous work, paragraph-style
descriptions for a term are extracted from
Web pages and organized based on domains.
However, these descriptions are independent
and do not comprise a condensed text as
in hand-crafted encyclopedias. To resolve
this problem, we propose a summarization
method, which produces a single text from
multiple descriptions. The resultant sum-
mary concisely describes a term from dif-
ferent viewpoints. We also show the effec-
tiveness of our method by means of experi-
ments.
1 Introduction
Term descriptions, which have been carefully orga-
nized in hand-crafted encyclopedias, are valuable
linguistic knowledge for human usage and compu-
tational linguistics research. However, due to the
limitation of manual compilation, existing encyclo-
pedias often lack new terms and new definitions for
existing terms.
The World Wide Web (the Web), which contains
an enormous volume of up-to-date information, is a
promising source to obtain new term descriptions.
It has become fairly common to consult the Web for
descriptions of a specific term. However, the use of
existing search engines is associated with the follow-
ing problems:
(a) search engines often retrieve extraneous pages
not describing a submitted term,
(b) even if desired pages are retrieved, a user has to
identify page fragments describing the term,
(c) word senses are not distinguished for polyse-
mous terms, such as ?hub (device and center)?,
(d) descriptions in multiple pages are independent
and do not comprise a condensed and coherent
text as in existing encyclopedias.
The authors of this paper have been resolving
these problems progressively. For problems (a) and
(b), Fujii and Ishikawa (2000) proposed an auto-
matic method to extract term descriptions from the
Web. For problem (c), Fujii and Ishikawa (2001)
improved the previous method, so that the multiple
descriptions extracted for a single term are catego-
rized into domains and consequently word senses are
distinguished.
Using these methods, we have compiled an ency-
clopedic corpus for approximately 600,000 Japanese
terms. We have also built a Web site called ?Cy-
clone?1 to utilize this corpus, in which one or more
paragraph-style descriptions extracted from differ-
ent pages can be retrieved in response to a user
input. In Figure 1, three paragraphs describing
?XML? are presented with the titles of their source
pages.
However, the above-mentioned problem (d) re-
mains unresolved and this is exactly what we intend
to address in this paper.
In hand-crafted encyclopedias, a single term is de-
scribed concisely from different ?viewpoints?, such
as the definition, exemplification, and purpose. In
contrast, if the first paragraph in Figure 1 is not
described from a sufficient number of viewpoints
for XML, a user has to read remaining paragraphs.
However, this is inefficient, because the descriptions
are extracted from independent pages and usually
include redundant contents.
To resolve this problem, we propose a summariza-
tion method that produces a concise and condensed
term description from multiple paragraphs. As a re-
sult, a user can obtain sufficient information about a
term with a minimal cost. Additionally, by reducing
the size of descriptions, Cyclone can be used with
mobile devices, such as PDAs.
However, while Cyclone includes various types
of terms, such as technical terms, events, and an-
imals, the required set of viewpoints can vary de-
pending the type of target terms. For example, the
definition and exemplification are necessary for tech-
nical terms, but the family and habitat are necessary
for animals. In this paper, we target Japanese tech-
nical terms in the computer domain.
Section 2 outlines Cyclone. Sections 3 and 4 ex-
plain our summarization method and its evaluation,
respectively. In Section 5, we discuss related work
and the scalability of our method.
1http://cyclone.slis.tsukuba.ac.jp/
Figure 1: Example descriptions for ?XML?.
2 Overview of Cyclone
Figure 2 depicts the overall design of Cyclone,
which produces an encyclopedic corpus by means of
five modules: ?term recognition?, ?extraction?, ?re-
trieval?, ?organization?, and ?related term extrac-
tion?. While Cyclone produces a corpus off-line,
users search the resultant corpus for specific descrip-
tions on-line.
It should be noted that the summarization
method proposed in this paper is not included in
Figure 2 and that the concept of viewpoint has not
been used in the modules in Figure 2.
In the off-line process, the input terms can be ei-
ther submitted manually or collected by the term
recognition module automatically. The term recog-
nition module periodically searches the Web for mor-
pheme sequences not included in the corpus, which
are used as input terms.
The retrieval module exhaustively searches the
Web for pages including an input term, as performed
in existing Web search engines.
The extraction module analyzes the layout (i.e.,
the structure of HTML tags) of each retrieved page
and identifies the paragraphs that potentially de-
scribe the target term. While promising descrip-
tions can be extracted from pages resembling on-line
dictionaries, descriptions can also be extracted from
general pages.
The organization module classifies the multiple
paragraphs for a single term into predefined domains
(e.g., computers, medicine, and sports) and sorts
them according to the score. The score is computed
by the reliability determined by hyper-links as in
Google2 and the linguistic validity determined by a
language model produced from an existing machine-
readable encyclopedia. Thus, different word senses,
which are often associated with different domains,
can be distinguished and high-quality descriptions
can be selected for each domain.
Finally, the related term extraction module
searches top-ranked descriptions for terms strongly
related to the target term (e.g., ?cable? and ?LAN?
for ?hub?). Existing encyclopedias often provide re-
lated terms for each headword, which are effective
to understand the headword. In Cyclone, related
terms can also be used as feedback terms to nar-
row down the user focus. However, this module is
beyond the scope of this paper.
3 Summarization Method
3.1 Overview
Given a set of paragraph-style descriptions for a sin-
gle term in a specific domain (e.g., descriptions for
?hub? in the computer domain), our summarization
2http://www.google.com/
Web
organization
retrieval
extraction
term(s)
encyclopedic
corpus
term recognition
related term extraction
descriptions related terms
Figure 2: Overall design of Cyclone.
method produces a concise text describing the term
from different viewpoints.
These descriptions are obtained by the organiza-
tion module in Figure 2. Thus, the related term
extraction module is independent of our summariza-
tion method.
Our method is multi-document summarization
(MDS) (Mani, 2001). Because a set of input docu-
ments (in our case, the paragraphs for a single term)
were written by different authors and/or different
time, the redundancy and divergence of the topics in
the input are greater than that for single document
summarization. Thus, the recognition of similarity
and difference among multiple contents is crucial.
The following two questions have to be answered:
? by which language unit (e.g., words, phrases, or
sentences) should two contents be compared?
? by which criterion should two contents be re-
garded as ?similar? or ?different??
The answers for these questions can be different de-
pending on the application and the type of input
documents.
Our purpose is to include as many viewpoints as
possible in a concise description. Thus, we com-
pare two contents on a viewpoint-by-viewpoint basis.
In addition, if two contents are associated with the
same viewpoint, we determine that those contents
are similar and that they should not be repeated in
the summary.
Our viewpoint-based summarization (VBS)
method consists of the following four steps:
1. identification, which recognizes the language
unit associated with a viewpoint,
2. classification, which merges the identified units
associated with the same viewpoint into a single
group,
3. selection, which determines one or more repre-
sentative units for each group,
4. presentation, which produces a summary in a
specific format.
The model is similar to those in existing MDS meth-
ods. However, the implementation of each step
varies depending on the application. We elaborate
on the four steps in Sections 3.2-3.5, respectively.
3.2 Identification
The identification module recognizes the language
units, each of which describes a target term from a
specific viewpoint. However, a compound or com-
plex sentence is often associated with multiple view-
points. The following example is an English trans-
lation of a Japanese compound sentence in a Web
page.
XML is an abbreviation for eXtensible
Markup Language, and is a markup lan-
guage.
The first and second clauses describe XML from the
abbreviation and definition viewpoints, respectively.
It should be noted that because ?XML? and ?eX-
tensible Markup Language? are spelled out by the
Roman alphabet in the original sentence, the first
clause does not provide Japanese readers with the
definition of XML.
To extract the language units on a viewpoint-by-
viewpoint basis, we segment Japanese sentences into
simple sentences. However, sentence segmentation
remains a difficult problem and the accuracy is not
100%. First, we analyze the syntactic dependency
structure of an input sentence by CaboCha3. Sec-
ond, we use hand-crafted rules to extract simple sen-
tences using the dependency structure.
The simple sentences excepting the first clause of-
ten lack the subject. To resolve this problem, zero
pronoun detection and anaphora resolution can be
used. However, due to the rudimentary nature of
existing methods, we use hand-crafted rules to com-
plement simple sentences with the subject.
As a result, we can obtain the following two simple
sentences from the above-mentioned input sentence,
in which the complement subject is in parentheses.
? XML is an abbreviation for eXtensible
Markup Language.
? (XML) is a markup language.
3.3 Classification
The classification module merges the simple sen-
tences related to the same viewpoint into a single
group. An existing encyclopedia for technical terms
uses approximately 30 obligatory and optional view-
points. We selected the following 12 viewpoints for
which typical expressions can be coded manually:
3
http://cl.aist-nara.ac.jp/?taku-ku/software/cabocha/
definition, abbreviation, exemplification,
purpose, synonym, reference, product, ad-
vantage, drawback, history, component,
function.
We manually produced 36 linguistic patterns used
to describe terms from a specific viewpoint. These
patterns are regular expressions, in which specific
morphemes are generalized into parts-of-speech or
the special symbol representing the target term.
We use a two-stage classification method. First,
the simple sentences that match with a pattern
are classified into the associated viewpoint group.
A simple sentence that matches with patterns for
multiple viewpoints is classified into every possible
group.
However, the pattern-based method fails to clas-
sify the sentences that do not match with any prede-
fined patterns. Thus, second we classify the remain-
ing sentences into the group in which the most simi-
lar sentence has already been classified. In practice,
we compute the similarity between an unclassified
sentence and each of the classified sentences. The
similarity between two sentences is determined by
the Dice coefficient, i.e., the ratio of content words
commonly included in those sentences. The sen-
tences unclassified through the above method are
classified into the ?miscellaneous? group.
In summary, our two-stage method uses prede-
fined linguistic patterns and statistics of words.
The following examples are English translations
of Japanese sentences extracted in the identification
module. These sentences can be classified into a spe-
cific group on the ground of the underlined expres-
sions, excepting sentence (e). However, in the second
stage, sentence (e) can be classified into the history
group, because sentence (e) is most similar to sen-
tence (c).
(a) XML is an extensible markup language.
? definition
(b) an abbreviation for eXtensible Markup Lan-
guage
? abbreviation
(c) was advised as a standard by W3C in 1998
? history
(d) XML is an abbreviation for Extensible Markup
Language
? abbreviation
(e) the standard of XML was advised by W3C
? ??? ? history
3.4 Selection
The selection module determines one or more rep-
resentative sentences for each viewpoint group. The
number of sentences selected from each group can
vary depending on the desired size of the resultant
summary.
We consider the following factors to compute the
score for each sentence and select sentences with
greater scores in each group.
? the number of common words included (W)
The representative sentences should contain
many words that are common in the group. We
collect the frequencies of words for each group,
and sentences including frequent words are pre-
ferred.
? the rank in Cyclone (R)
As depicted in Figure 2, Cyclone sorts the re-
trieved paragraphs according to the plausibility
as the description. Sentences in highly-ranked
paragraphs are preferred.
? the number of characters included (C)
To minimize the size of a summary, short sen-
tences are preferred.
Because these factors are different in terms of
the dimension, range, and polarity, we normalize
each factor in [0,1] and compute the final score as a
weighed average of the three factors. The weight of
each factor was determined by a preliminary study.
In brief, the relative importance among the three
factors is W>R>C.
However, because the miscellaneous group in-
cludes various viewpoints, we use a different method
from that for the regular groups. First, we select rep-
resentative sentences from the regular groups. Sec-
ond, from the miscellaneous group, we select the sen-
tence that is most dissimilar to the sentences already
selected as representatives. We use the Dice-based
similarity used in Section 3.3 to measure the dis-
similarity between two sentences. If we select more
than one sentence from the miscellaneous group, the
second process is repeated recursively.
3.5 Presentation
The presentation module lists the selected sentences
without any post-editing. Ideally, natural language
generation is required to produce a coherent text by,
for example, complementing conjunctions and gen-
erating anaphoric expressions. However, a simple
list of sentences is also useful to obtain knowledge
about a target term.
Figure 3 depicts an example summary produced
from the top 50 paragraphs for the term ?XML?. In
this figure, six viewpoint groups and the miscella-
neous group were formed and only one sentence was
selected from each group. The order of sentences
presented was determined by the score computed in
the selection module.
While the source paragraphs consist of 11,224
characters, the summary consists of 397 characters,
which is almost the same length as an abstract for a
technical paper.
The following is an English translation of the sen-
tences in Figure 3. Here, the words spelled out by
the Roman alphabet in the original sentences are in
italics.
Figure 3: Example summary for ?XML?.
? definition: XML is an extensible markup lan-
guage (eXtensible Markup Language).
? abbreviation: an abbreviation for Extensible
Markup Language (an extensible markup lan-
guage).
? purpose: Because XML is a standard specifi-
cation for data representation, the data defined
by XML can be reusable, irrespective of the up-
per application.
? advantage: XML is advantageous to develop-
ers of the file maker Pro, which needs to receive
data from the client.
? history: was advised as a standard by W3C
(World Wide Web Consortium: a group stan-
dardizing WWW technologies) in 1998,
? reference: This book is an introduction for
XML, which has recently been paid much at-
tention as the next generation Internet standard
format, and related technologies.
? miscellaneous: In XML, the tags are enclosed
in ?<? and ?>?.
Each viewpoint label or sentence is hyper-linked to
the associated group or the source paragraph, re-
spectively, so that a user can easily obtain more in-
formation on a specific viewpoint. For example, by
the reference sentence, a catalogue page of the book
in question can be retrieved.
Although the resultant summary describes XML
from multiple viewpoints, there is a room for im-
provement. For example, the sentences classified
into the definition and abbreviation viewpoints in-
clude almost the same content.
4 Evaluation
4.1 Methodology
Existing methods for evaluating summarization
techniques can be classified into intrinsic and extrin-
sic approaches.
In the intrinsic approach, the content of a sum-
mary is evaluated with respect to the quality of a
text (e.g., coherence) and the informativeness (i.e.,
the extent to which important contents are in the
summary). In the extrinsic approach, the evaluation
measure is the extent to which a summary improves
the efficiency of a specific task (e.g., relevance judg-
ment in text retrieval).
In DUC4 and NTCIR5, both approaches have
been used to evaluate summarization methods tar-
geting newspaper articles. However, because there
was no public test collections targeting term descrip-
tions in Web pages, we produced our test collection.
4http://duc.nist.gov/
5http://research.nii.ac.jp/ntcir/index-en.html
As the first step of our summarization research, we
addressed only the intrinsic evaluation.
In this paper, we focused on including as many
viewpoints (i.e., contents) as possible in a summary,
but did not address the text coherence. Thus, we
used the informativeness of a summary as the evalu-
ation criterion. We used the following two measures,
which are in the trade-off relation.
? compression ratio
#characters in summary
#characters in Cyclone result
? coverage
#viewpoints in summary
#viewpoints in Cyclone result
Here, ?#viewpoints? denotes the number of view-
point types. Even if a summary contains multiple
sentences related to the same viewpoint, the numer-
ator is increased by 1.
We used 15 Japanese term in an existing computer
dictionary as test inputs. English translations of the
test inputs are as follows:
10BASE-T, ASCII, SQL, XML, accumu-
lator, assembler, binary number, crossing
cable, data warehouse, macro virus, main
memory unit, parallel processing, resolu-
tion, search time, thesaurus.
To calculate the coverage, the simple sentences
in the Cyclone results have to be associated with
viewpoints. To reduce the subjectivity in the evalu-
ation, for each of the 15 terms, we asked two college
students (excluding the authors of this paper) to an-
notate each simple sentence in the top 50 paragraphs
with one or more viewpoints. The two annotators
performed the annotation task independently. The
denominators of the compression ratio and coverage
were calculated by the top 50 paragraphs.
During a preliminary study, the authors and anno-
tators defined 28 viewpoints, including the 12 view-
points targeted in our method. We also defined the
following three categories, which were not considered
as a viewpoint:
? non-description, which were also used to anno-
tate non-sentence fragments caused by errors in
the identification module,
? description for a word sense independent of the
computer domain (e.g., ?hub? as a center, in-
stead of a network device),
? miscellaneous.
It may be argued that an existing hand-crafted
encyclopedia can be used as the standard sum-
mary. However, paragraphs in Cyclone often con-
tain viewpoints not described in existing encyclope-
dias. Thus, we did not use existing encyclopedias in
our experiments.
4.2 Results
Table 1 shows the compression ratio and coverage for
different methods, in which ?#Reps? and ?#Chars?
denote the number of representative sentences se-
lected from each viewpoint group and the number
of characters in a summary, respectively. We always
selected five sentences from the miscellaneous group.
The third column denotes the compression ratio.
The remaining columns denote the coverage on
a annotator-by-annotator basis. The columns ?12
Viewpoints? and ?28 Viewpoints? denote the case
in which we focused only on the 12 viewpoints tar-
geted in our method and the case in which all the
28 viewpoints were considered, respectively.
The columns ?VBS? and ?Lead? denote the cover-
age obtained with our viewpoint-based summariza-
tion method and the lead method. The lead method,
which has often been used as a baseline method in
past literature, systematically extracted the top N
characters from the Cyclone result. Here, N is the
same number in the second column.
In other words, the compression ratio of the VBS
and lead methods was standardized, and we com-
pared the coverage of both methods. The compres-
sion ratio and coverage were averaged over the 15
test terms.
Suggestions which can be derived from Table 1 are
as follows.
First, in the case of ?#Reps=1?, the average
size of a summary was 616 characters, which is
marginally longer than an abstract for a techni-
cal paper. In the case of ?#Reps=3?, the average
summary size was 1309 characters, which is almost
the maximum size for a single description in hand-
crafted encyclopedias. A summary obtained with
four sentences in each group is perhaps too long as
term descriptions.
Second, the compression ratio was roughly 10%,
which is fairly good performance. It may be argued
that the compression ratio is exaggerated. That is,
although paragraphs ranked higher than 50 can po-
tentially provide the sufficient viewpoints, the top 50
paragraphs were always used to calculate the domi-
nator of the compression ratio.
We found that the top 38 paragraphs, on average,
contained all viewpoint types in the top 50 para-
graphs. Thus, the remaining 12 paragraphs did not
provide additional information. However, it is dif-
ficult for a user to determine when to stop reading
a retrieval result. In existing evaluation workshops,
such as NTCIR, the compression ratio is also calcu-
lated using the total size of the input documents.
Third, the VBS method outperformed the lead
method in terms of the coverage, excepting the case
of ?#Reps=1? focusing on the 12 viewpoints by an-
notator B. However, in general the VBS method pro-
duced more informative summaries than the lead
method, irrespective of the compression ratio and
the annotator.
It should be noted that although the VBS method
Table 1: Results of summarization experiments.
Coverage by annotator A (%) Coverage by annotator B (%)
Compression 12 Viewpoints 28 Viewpoints 12 Viewpoints 28 Viewpoints
#Reps #Chars ratio (%) VBS Lead VBS Lead VBS Lead VBS Lead
1 616 5.97 56.62 52.84 49.49 44.84 50.00 53.61 49.49 47.56
2 998 9.61 73.43 57.23 59.26 53.70 64.50 62.96 60.75 57.37
3 1309 12.61 76.04 59.29 63.13 56.44 67.83 64.81 65.22 60.84
targets 12 viewpoints, the sentences selected from
the miscellaneous group can be related to the re-
maining 16 viewpoints. Thus, even if we focus on
the 28 viewpoints, the coverage of the VBS method
can potentially increase.
It should also be noted that all viewpoints are not
equally important. For example, in an existing en-
cyclopedia (Nagao and others, 1990) the definition,
exemplification, and synonym are regarded as the
obligatory viewpoints, and the remaining viewpoints
are optional.
We investigated the coverage for the three obliga-
tory viewpoints. We found that while the coverage
for the definition and exemplification ranged from
60% to 90%, the coverage for the synonym was 50%
or less.
A low coverage for the synonym is partially due
to the fact that synonyms are often described with
parentheses. However, because parentheses are used
for various purposes, it is difficult to identify only
synonyms expressed with parentheses. This problem
needs to be further explored.
5 Discussion
The goal of our research is to automatically compile
a high-quality large encyclopedic corpus using the
Web. Hand-crafted encyclopedias lack new terms
and new definitions for existing terms, and thus the
quantity problem is crucial. The Web contains un-
reliable and unorganized information and thus the
quality problem is crucial. We intend to alleviate
both problems. To the best of our knowledge, no
attempt has been made to intend similar purposes.
Our research is related to question answering
(QA). For example, in TREC QA track, definition
questions are intended to provide a user with the def-
inition of a target item or person (Voorhees, 2003).
However, while the expected answer for a TREC
question is short definition sentences as in a dic-
tionary, we intend to produce an encyclopedic text
describing a target term from multiple viewpoints.
The summarization method proposed in this pa-
per is related to multi-document summarization
(MDS) (Mani, 2001; Radev and McKeown, 1998;
Schiffman et al, 2001). The novelty of our research
is that we applied MDS to producing a condensed
term description from unorganized Web pages, while
existing MDS methods used newspaper articles to
produce an outline of an event and a biography of
a specific person. We also proposed the concept of
viewpoint for MDS purposes.
While we targeted Japanese technical terms in the
computer domain, our method can also be applied to
other types of terms in different languages, without
modifying the model. However, a set of viewpoints
and patterns typically used to describe each view-
point need to be modified or replaced depending the
application. Given annotated data, such as those
used in our experiments, machine learning methods
can potentially be used to produce a set of view-
points and patterns for a specific application.
6 Conclusion
To compile encyclopedic term descriptions from the
Web, we introduced a summarization method to our
previous work. Future work includes generating a
coherent text instead of a simple list of sentences
and performing extensive experiments including an
extrinsic evaluation method.
References
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: Extract-
ing term descriptions from semi-structured texts.
In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages
488?495.
Atsushi Fujii and Tetsuya Ishikawa. 2001. Organiz-
ing encyclopedic knowledge based on the Web and
its application to question answering. In Proceed-
ings of the 39th Annual Meeting of the Association
for Computational Linguistics, pages 196?203.
Inderjeet Mani, 2001. Automatic Summarization,
chapter 7, pages 169?208. John Benjamins.
Makoto Nagao et al, editors. 1990. Encyclope-
dic Dictoinary of Computer Science. Iwanami
Shoten. (In Japanese).
Dragomir R. Radev and Kathleen R. McKeown.
1998. Generating natural language summaries
from multiple on-line sources. Computational Lin-
guistics, 24(3):469?500.
Barry Schiffman, Inderjeet Mani, and Kristian J.
Concepcion. 2001. Producing biographical sum-
maries: Combining linguistic knowledge with cor-
pus statistics. In Proceedings of the 39th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 450?457.
Ellen M. Voorhees. 2003. Evaluating answers to def-
inition questions. In Companion Volume of the
Proceedings of HLT-NAACL 2003, pages 109?111.
Term Extraction from Korean Corpora via Japanese
Atsushi Fujii, Tetsuya Ishikawa
Graduate School of Library,
Information and Media Studies
University of Tsukuba
1-2 Kasuga, Tsukuba
305-8550, Japan
{fujii,ishikawa}@slis.tsukuba.ac.jp
Jong-Hyeok Lee
Division of Electrical and
Computer Engineering,
Pohang University of Science and Technology,
Advanced Information Technology Research Center
San 31 Hyoja-dong Nam-gu,
Pohang 790-784, Republic of Korea
jhlee@postech.ac.kr
Abstract
This paper proposes a method to extract foreign
words, such as technical terms and proper nouns,
from Korean corpora and produce a Japanese-
Korean bilingual dictionary. Specific words have
been imported into multiple countries simultane-
ously, if they are influential across cultures. The
pronunciation of a source word is similar in different
languages. Our method extracts words in Korean
corpora that are phonetically similar to Katakana
words, which can easily be identified in Japanese cor-
pora. We also show the effectiveness of our method
by means of experiments.
1 Introduction
Reflecting the rapid growth in science and tech-
nology, new words have progressively been created.
However, due to the limitation of manual compila-
tion, new words are often out-of-dictionary words
and decrease the quality of human language tech-
nology, such as natural language processing, infor-
mation retrieval, machine translation, and speech
recognition. To resolve this problem, a number
of automatic methods to extract monolingual and
bilingual lexicons from corpora have been proposed
for various languages.
In this paper, we focus on extracting foreign words
(or loanwords) in Korean. Technical terms and
proper nouns are often imported from foreign lan-
guages and are spelled out (or transliterated) by the
Korean alphabet system called Hangul . The similar
trend can be observable in Japanese and Chinese. In
Japanese, foreign words are spelled out by its special
phonetic alphabet (or phonogram) called Katakana.
Thus, foreign words can be extracted from Japanese
corpora with a high accuracy, because the Katakana
characters are seldom used to describe the conven-
tional Japanese words, excepting proper nouns.
However, extracting foreign words from Korean
corpora is more difficult, because in Korean both
the conventional and foreign words are written with
Hangul characters. This problem remains a chal-
lenging issue in computational linguistic research.
It is often the case that specific words have been
imported into multiple countries simultaneously, be-
cause the source words (or concepts) are usually in-
fluential across cultures. Thus, it is feasible that a
large number of foreign words in Korean can also be
foreign words in Japanese.
In addition, the foreign words in Korean and
Japanese corresponding to the same source word are
phonetically similar. For example, the English word
?system? has been imported into both Japanese and
Korean. The romanized words are /sisutemu/ and
/siseutem/ in both countries, respectively.
Motivated by these assumptions, we propose a
method to extract foreign words in Korean corpora
by means of Japanese. In brief, our method per-
forms as follows. First, foreign words in Japanese
are collected, for which Katakana words in corpora
and existing lexicons can be used. Second, from Ko-
rean corpora the words that are phonetically similar
to Katakana words are extracted. Finally, extracted
Korean words are compiled in a lexicon with the cor-
responding Japanese words.
In summary, our method can extract foreign words
in Korean and produce a Japanese-Korean bilingual
lexicon in a single framework.
2 Methodology
2.1 Overview
Figure 1 exemplifies our extraction method, which
produces a Japanese-Korean bilingual lexicon using
a Korean corpus and Japanese corpus and/or lexi-
con. The Japanese and Korean corpora do not have
to be parallel or comparable. However, it is desir-
able that both corpora are associated with the same
domain. For the Japanese resource, the corpus and
lexicon can alternatively be used or can be used to-
gether. Note that compiling Japanese monolingual
lexicon is less expensive than that for a bilingual lex-
icon. In addition, new Katakana words can easily be
extracted from a number of on-line resources, such
as the World Wide Web. Thus, the use of Japanese
lexicons does not decrease the utility of our method.
First, we collect Katakana words from Japanese
resources. This can systematically be performed by
means of a Japanese character code, such as EUC-
JP and SJIS.
Second, we represent the Korean corpus and
Japanese Katakana words by the Roman alphabet
(i.e., romanization), so that the phonetic similarity
can easily be computed. However, we use different
romanization methods for Japanese and Korean.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 71
Third, we extract candidates of foreign words
from the romanized Korean corpus. An alternative
method is to first perform morphological analysis
on the corpus, extract candidate words based on
morphemes and parts-of-speech, and romanize the
extracted words. Our general model does not con-
strain as to which method should be used in the
third step. However, because the accuracy of anal-
ysis often decreases for new words to be extracted,
we experimentally adopt the former method.
Finally, we compute the phonetic similarity be-
tween each combination of the romanized Hangul
and Katakana words, and select the combinations
whose score is above a predefined threshold. As a
result, we can obtain a Japanese-Korean bilingual
lexicon consisting of foreign words.
It may be argued that English lexicons or cor-
pora can be used as source information, instead of
Japanese resources. However, because not all En-
glish words have been imported into Korean, the
extraction accuracy will decrease due to extraneous
words.
Figure 1: Overview of our extraction method.
2.2 Romanizing Japanese
Because the number of phones consisting of Japanese
Katakana characters is limited, we manually pro-
duced the correspondence between each phone
and its Roman representation. The numbers of
Katakana characters and combined phones are 73
and 109, respectively. We also defined a symbol to
represent a long vowel. In Japanese, the Hepbern
and Kunrei systems are commonly used for roman-
ization purposes. We use the Hepburn system, be-
cause its representation is similar to that in Korean,
compared with the Kunrei system.
However, specific Japanese phones, such as /ti/,
do not exist in Korean. Thus, to adapt the Hepburn
system to Korean, /ti/ and /tu/ are converted to
/chi/ and /chu/, respectively.
2.3 Romanizing Korean
The number of Korean Hangul characters is much
greater than that of Japanese Katakana characters.
Each Hangul character is a combination of more
than one consonant. The pronunciation of each char-
acter is determined by its component consonants.
In Korean, there are types of consonant, i.e., the
first consonant, vowel, and last consonant. The
numbers of these consonants are 19, 21, and 27, re-
spectively. The last consonant is optional. Thus, the
number of combined characters is 11,172. However,
to transliterate imported words, the official guide-
line suggests that only seven consonants be used as
the last consonant. In EUC-KR, which is a stan-
dard coding system for Korean text, 2,350 common
characters are coded independent of the pronunci-
ation. Therefore, if we target corpora represented
by EUC-KR, each of the 2,350 characters has to be
corresponded to its Roman representation.
We use Unicode, in which Hangul characters are
sorted according to the pronunciation. Figure 2 de-
picts a fragment of the Unicode table for Korean,
in which each line corresponds to a combination
of the first consonant and vowel and each column
corresponds to the last consonant. The number of
columns is 28, i.e., the number of the last consonants
and the case in which the last consonant is not used.
From this figure, the following rules can be found:
 the first consonant changes every 21 lines, which
corresponds to the number of vowels,
 the vowel changes every line (i.e., 28 characters)
and repeats every 21 lines,
 the last consonant changes every column.
Based on these rules, each character and its pro-
nunciation can be identified by the three consonant
types. Thus, we manually corresponded only the 68
consonants to Roman alphabets.
Figure 2: A fragment of the Unicode table for Ko-
rean Hangul characters.
We use the official romanization system for Ko-
rean, but specific Korean phones are adapted to
Japanese. For example, /j/ and /l/ are converted
to /z/ and /r/, respectively.
It should be noted that the adaptation is not in-
vertible and thus is needed for both J-to-K and K-
to-J directions.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology72
For example, the English word ?cheese?, which
has been imported to both Korean and Japanese as
a foreign word, is romanized as /chiseu/ in Korean
and /ti:zu/ in Japanese. Here, /:/ is the symbol
representing a Japanese long vowel. Using the adap-
tation, these expressions are converted to /chizu/
and /chi:zu/, respectively, which look more similar
to each other, compared with the original strings.
2.4 Extracting term candidates from
Korean corpora
To extract candidates of foreign words from a Ko-
rean corpus, we first extract phrases. This can be
performed systematically, because Korean sentences
are segmented on a phrase-by-phrase basis.
Second, because foreign words are usually nouns,
we use hand-crafted rules to remove post-position
suffixes (e.g., Josa) and extract nouns from phrases.
Third, we discard nouns including the last con-
sonants that are not recommended for translitera-
tion purposes in the official guideline. Although the
guideline suggests other rules for transliteration, ex-
isting foreign words in Korean are not necessarily
regulated by these rules.
Finally, we consult a dictionary to discard exist-
ing Korean words, because our purpose is to extract
new words. For this purpose, we experimentally
use the dictionary for SuperMorph-K morphologi-
cal analyzer1, which includes approximately 50,000
Korean words.
2.5 Computing Similarity
Given romanized Japanese and Korean words, we
compute the similarity between the two strings and
select the pairs associated with the score above a
threshold as translations. We use a DP (dynamic
programming) matching method to identify the
number of differences (i.e., insertion, deletion, and
substitution) between two strings, on a alphabet-
by-alphabet basis.
In principle, if two strings are associated with a
smaller number of differences, the similarity between
them becomes greater. For this purpose, a Dice-style
coefficient can be used.
However, while the use of consonants in translit-
eration is usually the same across languages, the
use of vowels can vary significantly depending on
the language. For example, the English word ?sys-
tem? is romanized as /sisutemu/ and /siseutem/
in Japanese and Korean, respectively. Thus, the dif-
ferences in consonants between two strings should
be penalized more than the differences in vowels.
In view of the above discussion, we compute the
similarity between two romanized words by Equa-
tion (1).
1 ?
2 ? (? ? dc + dv)
? ? c + v (1)
Here, dc and dv denote the numbers of differences
in consonants and vowels, respectively, and ? is a
1http://www.omronsoft.com/
parametric constant used to control the importance
of the consonants. We experimentally set ? = 2. In
addition, c and v denote the numbers of all conso-
nants and vowels in the two strings. The similarity
ranges from 0 to 1.
3 Experimentation
3.1 Evaluating Extraction Accuracy
We collected 111,166 Katakana words (word types)
from multiple Japanese lexicons, most of which were
technical term dictionaries.
We used the Korean document set in the NTCIR-3
Cross-lingual Information Retrieval test collection2.
This document set consists of 66,146 newspaper ar-
ticles of Korean Economic Daily published in 1994.
We randomly selected 50 newspaper articles and
used them for our experiment. We asked a grad-
uate student excluding the authors of this paper to
identify foreign words in the target text. As a result,
124 foreign word types (205 word tokens) were iden-
tified, which were less than we had expected. This
was partially due to the fact that newspaper articles
generally do not contain a large number of foreign
words, compared with technical publications.
We manually classified the extracted words and
used only the words that were imported to both
Japan and Korea from other languages. We dis-
carded foreign words in Korea imported from Japan,
because these words were often spelled out by non-
Katakana characters, such as Kanji (Chinese charac-
ter). A sample of these words includes ?Tokyo (the
capital of Japan)?, ?Heisei (the current Japanese
era name)?, and ?enko (personal connection)?. In
addition, we discarded the foreign proper nouns for
which the human subject was not able to identify
the source word. As a result, we obtained 67 target
word types. Examples of original English words for
these words are as follows:
digital, group, dollar, re-engineering, line,
polyester, Asia, service, class, card, com-
puter, brand, liter, hotel.
Thus, our method can potentially be applied to
roughly a half of the foreign words in Korean text.
We used the Japanese words to extract plausi-
ble foreign words from the target Korean corpus.
We first romanized the corpus and extracted nouns
by removing post-position suffixes. As a result, we
obtained 3,106 words including all the 67 target
words. By discarding the words in the dictionary
for SuperMorph-K, 958 words including 59 target
words were remained.
For each of the remaining 958 words, we computed
the similarity between each of the 111,166 Japanese
words. For evaluation purposes, we varied a thresh-
old for the similarity and investigated the relation
between precision and recall. Recall is the ratio
of the number of target foreign words extracted by
our method and the total number of target foreign
2http://research.nii.ac.jp/ntcir/index-en.html
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 73
words. Precision is the ratio of the number of target
foreign words extracted by our method and the total
number of words obtained by our method.
Table 1 shows the precision and recall for differ-
ent methods. While we varied a threshold of a sim-
ilarity, we also varied the number of Korean words
corresponded to a single Katakana word (N). By
decreasing the value of the threshold and increasing
the number of words extracted, the recall can be im-
proved but the precision decreases. In Table 1, the
precision and recall are in an extreme trade-off rela-
tion. For example, when the recall was 69.5%, the
precision was only 1.2%.
We manually analyzed the words that were not ex-
tracted by our method. Out of the 59 target words,
12 compound words consisting of both conventional
and foreign words were not extracted. However,
our method extracted compound words consisting
of only foreign words. In addition, the three words
that did not have counterparts in the input Japanese
words were not extracted.
Table 1: Precision/Recall for term extraction.
Threshold for similarity
>0.9 >0.7 >0.5
N=1 50.0/8.5 12.7/40.7 4.1/47.5
N=10 50.0/8.5 7.4/47.5 1.2/69.5
3.2 Application-Oriented Evaluation
During the first experiment, we determined a specific
threshold value for the similarity between Katakana
and Hangul words and selected the pairs whose sim-
ilarity was above the threshold. As a result, we ob-
tained 667 Korean words, which were used to en-
hance the dictionary for the SuperMorph-K morpho-
logical analyzer.
We performed morphological analysis on the 50
articles used in the first experiment, which included
1,213 sentences and 9,557 word tokens. We also in-
vestigated the degree to which the analytical accu-
racy is improved by means of the additional dictio-
nary. Here, accuracy is the ratio of the number of
correct word segmentations and the total segmenta-
tions generated by SuperMorph-K. The same human
subject as in the first experiment identified the cor-
rect word segmentations for the input articles.
First, we focused on the accuracy of segmenting
foreign words. The accuracy was improved from
75.8% to 79.8% by means of the additional dictio-
nary. The accuracy for all words was changed from
94.6% to 94.8% by the additional dictionary.
In summary, the additional dictionary was effec-
tive for analyzing foreign words and was not asso-
ciated with side effect for the overall accuracy. At
the same time, we concede that we need larger-scale
experiments to draw firmer conclusions.
4 Related Work
A number of corpus-based methods to extract bilin-
gual lexicons have been proposed (Smadja et al,
1996). In general, these methods use statistics ob-
tained from a parallel or comparable bilingual corpus
and extract word or phrase pairs that are strongly
associated with each other. However, our method
uses a monolingual Korean corpus and a Japanese
lexicon independent of the corpus, which can easily
be obtained, compared with parallel or comparable
bilingual corpora.
Jeong et al (1999) and Oh and Choi (2001) in-
dependently explored a statistical approach to de-
tect foreign words in Korean text. Although the de-
tection accuracy is reasonably high, these methods
require a training corpus in which conventional and
foreign words are annotated. Our approach does not
require annotated corpora, but the detection accu-
racy is not high enough as shown in Section 3.1. A
combination of both approaches is expected to com-
pensate the drawbacks of each approach.
5 Conclusion
We proposed a method to extract foreign words,
such as technical terms and proper nouns, from Ko-
rean corpora and produce a Japanese-Korean bilin-
gual dictionary. Specific words, which have been
imported into multiple countries, are usually spelled
out by special phonetic alphabets, such as Katakana
in Japanese and Hangul in Korean.
Because extracting foreign words spelled out by
Katakana in Japanese lexicons and corpora can be
performed with a high accuracy, our method ex-
tracts words in Korean corpora that are phonetically
similar to Japanese Katakana words. Our method
does not require parallel or comparable bilingual cor-
pora and human annotation for these corpora.
We also performed experiments in which we ex-
tracted foreign words from Korean newspaper arti-
cles and used the resultant dictionary for morpho-
logical analysis. We found that our method did not
correctly extract compound Korean words consist-
ing of both conventional and foreign words. Future
work includes larger-scale experiments to further in-
vestigate the effectiveness of our method.
References
Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee,
and Key-Sun Choi. 1999. Automatic identification
and back-transliteration of foreign words for informa-
tion retrieval. Information Processing & Management,
35:523?540.
Jong-Hoon Oh and Key sun Choi. 2001. Automatic
extraction of transliterated foreign words using hid-
den markov model. In Proceedings of ICCPOL-2001,
pages 433?438.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Computa-
tional Linguistics, 22(1):1?38.
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology74
A Probabilistic Method for Analyzing Japanese Anaphora
Integrating Zero Pronoun Detection and Resolution
Kazuhiro Seki ?, Atsushi Fujii ??, ??? and Tetsuya Ishikawa ??
?National Institute of Advanced Industrial Science and Technology
1-1-1, Chuuou Daini Umezono, Tsukuba 305-8568, Japan
??University of Library and Information Science
1-2, Kasuga, Tsukuba, 305-8550, Japan
???CREST, Japan Science & Technology Corporation
k.seki@aist.go.jp fujii@ulis.ac.jp ishikawa@ulis.ac.jp
Abstract
This paper proposes a method to analyze
Japanese anaphora, in which zero pronouns
(omitted obligatory cases) are used to refer to
preceding entities (antecedents). Unlike the
case of general coreference resolution, zero pro-
nouns have to be detected prior to resolution
because they are not expressed in discourse.
Our method integrates two probability param-
eters to perform zero pronoun detection and
resolution in a single framework. The first pa-
rameter quantifies the degree to which a given
case is a zero pronoun. The second parame-
ter quantifies the degree to which a given entity
is the antecedent for a detected zero pronoun.
To compute these parameters efficiently, we use
corpora with/without annotations of anaphoric
relations. We show the effectiveness of our
method by way of experiments.
1 Introduction
Anaphora resolution is crucial in natural lan-
guage processing (NLP), specifically, discourse
analysis. In the case of English, partially mo-
tivated by Message Understanding Conferences
(MUCs) (Grishman and Sundheim, 1996), a
number of coreference resolution methods have
been proposed.
In other languages such as Japanese and
Spanish, anaphoric expressions are often omit-
ted. Ellipses related to obligatory cases are usu-
ally termed zero pronouns. Since zero pronouns
are not expressed in discourse, they have to be
detected prior to identifying their antecedents.
Thus, although in English pleonastic pronouns
have to be determined whether or not they are
anaphoric expressions prior to resolution, the
process of analyzing Japanese zero pronouns is
different from general coreference resolution in
English.
For identifying anaphoric relations, existing
methods are classified into two fundamental ap-
proaches: rule-based and statistical approaches.
In rule-based approaches (Grosz et al, 1995;
Hobbs, 1978; Mitkov et al, 1998; Nakaiwa
and Shirai, 1996; Okumura and Tamura, 1996;
Palomar et al, 2001; Walker et al, 1994),
anaphoric relations between anaphors and their
antecedents are identified by way of hand-
crafted rules, which typically rely on syntactic
structures, gender/number agreement, and se-
lectional restrictions. However, it is difficult to
produce rules exhaustively, and rules that are
developed for a specific language are not neces-
sarily effective for other languages. For exam-
ple, gender/number agreement in English can-
not be applied to Japanese.
Statistical approaches (Aone and Bennett,
1995; Ge et al, 1998; Kim and Ehara,
1995; Soon et al, 2001) use statistical mod-
els produced based on corpora annotated with
anaphoric relations. However, only a few
attempts have been made in corpus-based
anaphora resolution for Japanese zero pro-
nouns. One of the reasons is that it is costly
to produce a sufficient volume of training cor-
pora annotated with anaphoric relations.
In addition, those above methods focused
mainly on identifying antecedents, and few at-
tempts have been made to detect zero pronouns.
Motivated by the above background, we
propose a probabilistic model for analyzing
Japanese zero pronouns combined with a detec-
tion method. In brief, our model consists of two
parameters associated with zero pronoun detec-
tion and antecedent identification. We focus on
zero pronouns whose antecedents exist in pre-
ceding sentences to zero pronouns because they
are major referential expressions in Japanese.
Section 2 explains our proposed method (sys-
tem) for analyzing Japanese zero pronouns.
Section 3 evaluates our method by way of ex-
periments using newspaper articles. Section 4
discusses related research literature.
2 A System for Analyzing Japanese
Zero Pronouns
2.1 Overview
Figure 1 depicts the overall design of our system
to analyze Japanese zero pronouns. We explain
the entire process based on this figure.
First, given an input Japanese text, our sys-
tem performs morphological and syntactic anal-
yses. In the case of Japanese, morphological
analysis involves word segmentation and part-
of-speech tagging because Japanese sentences
lack lexical segmentation, for which we use
the JUMAN morphological analyzer (Kurohashi
and Nagao, 1998b). Then, we use the KNP
parser (Kurohashi, 1998) to identify syntactic
relations between segmented words.
Second, in a zero pronoun detection phase,
the system uses syntactic relations to detect
omitted cases (nominative, accusative, and da-
tive) as zero pronoun candidates. To avoid zero
pronouns overdetected, we use the IPAL verb
dictionary (Information-technology Promotion
Agency, 1987) including case frames associated
with 911 Japanese verbs. We discard zero pro-
noun candidates unlisted in the case frames as-
sociated with a verb in question.
For verbs unlisted in the IPAL dictionary,
only nominative cases are regarded as obliga-
tory. The system also computes a probability
that case c related to target verb v is a zero
pronoun, P
zero
(c|v), to select plausible zero pro-
noun candidates.
Ideally, in the case where a verb in ques-
tion is polysemous, word sense disambiguation
is needed to select the appropriate case frame,
because different verb senses often correspond
to different case frames. However, we currently
merge multiple case frames for a verb into a sin-
gle frame so as to avoid the polysemous prob-
lem. This issue needs to be further explored.
Third, in a zero pronoun resolution (i.e., an-
tecedent identification) phase, for each zero pro-
noun the system extracts antecedent candidates
from the preceding contexts, which are ordered
according to the extent to which they can be the
antecedent for the target zero pronoun. From
input text
morphological and
sytactic analyses
output text
case frame
dictionary
annotated
corpora
unannotated
corpora
semantic
model
syntactic
model
zero pronoun
detection
zero pronoun
resolution
Figure 1: The overall design of our system to
analyze Japanese zero pronouns.
the viewpoint of probability theory, our task
here is to compute a probability that zero pro-
noun ? refers to antecedent a
i
, P (a
i
|?), and se-
lect the candidate that maximizes the probabil-
ity score. For the purpose of computing this
score, we model zero pronouns and antecedents
in Section 2.2.
Finally, the system outputs texts containing
anaphoric relations. In addition, the number
of zero pronouns analyzed by the system can
optionally be controlled based on the certainty
score described in Section 2.4.
2.2 Modeling Zero Pronouns and
Antecedents
According to past literature associated with
zero pronoun resolution and our preliminary
study, we use the following six features to model
zero pronouns and antecedents.
? Features for zero pronouns
? Verbs that govern zero pronouns (v), which
denote verbs whose cases are omitted.
? Surface cases related to zero pronouns (c),
for which possible values are Japanese case
marker suffixes, ga (nominative), wo (ac-
cusative), and ni (dative). Those values
indicate which cases are omitted.
? Features for antecedents
? Post-positional particles (p), which play
crucial roles in resolving Japanese zero pro-
nouns (Kameyama, 1986; Walker et al,
1994).
? Distance (d), which denotes the distance
(proximity) between a zero pronoun and an
antecedent candidate in an input text. In
the case where they occur in the same sen-
tence, its value takes 0. In the case where
an antecedent occurs in n sentences previ-
ous to the sentence including a zero pro-
noun, its value takes n.
? Constraint related to relative clauses (r),
which denotes whether an antecedent is in-
cluded in a relative clause or not. In the
case where it is included, the value of r
takes true, otherwise false. The rationale
behind this feature is that Japanese zero
pronouns tend not to refer to noun phrases
in relative clauses.
? Semantic classes (n), which represent se-
mantic classes associated with antecedents.
We use 544 semantic classes defined in the
Japanese Bunruigoihyou thesaurus (Na-
tional Language Research Institute, 1964),
which contains 55,443 Japanese nouns.
2.3 Our Probabilistic Model for Zero
Pronoun Detection and Resolution
We consider probabilities that unsatisfied case
c related to verb v is a zero pronoun, P
zero
(c|v),
and that zero pronoun ?
c
refers to antecedent
a
i
, P (a
i
|?
c
). Thus, a probability that case c (?
c
)
is zero-pronominalized and refers to candidate
a
i
is formalized as in Equation (1).
P (a
i
|?
c
) ? P
zero
(c|v) (1)
Here, P
zero
(c|v) and P (a
i
|?
c
) are computed in
the detection and resolution phases, respec-
tively (see Figure 1).
Since zero pronouns are omitted obligatory
cases, whether or not case c is a zero pronoun
depends on the extent to which case c is oblig-
atory for verb v. Case c is likely to be oblig-
atory for verb v if c frequently co-occurs with
v. Thus, we compute P
zero
(c|v) based on the
co-occurrence frequency of ?v, c? pairs, which
can be extracted from unannotated corpora.
P
zero
(c|v) takes 1 in the case where c is ga (nom-
inative) regardless of the target verb, because ga
is obligatory for most Japanese verbs.
Given the formal representation for zero pro-
nouns and antecedents in Section 2.2, the prob-
ability, P (a|?), is expressed as in Equation (2).
P (a
i
|?) = P (p
i
, d
i
, r
i
, n
i
|v, c) (2)
To improve the efficiency of probability estima-
tion, we decompose the right-hand side of Equa-
tion (2) as follows.
Since a preliminary study showed that d
i
and
r
i
were relatively independent of the other fea-
tures, we approximate Equation (2) as in Equa-
tion (3).
P (a
i
|?) ? P (p
i
, n
i
|v, c) ? P (d
i
) ? P (r
i
)
= P (p
i
|n
i
, v, c) ? P (n
i
|v, c)
? P (d
i
) ? P (r
i
)
(3)
Given that p
i
is independent of v and n
i
, we
can further approximate Equation (3) to derive
Equation (4).
P (a
i
|?
c
) ? P (p
i
|c)?P (d
i
)?P (r
i
)?P (n
i
|v, c) (4)
Here, the first three factors, P (p
i
|c) ? P (d
i
) ?
P (r
i
), are related to syntactic properties, and
P (n
i
|v, c) is a semantic property associated with
zero pronouns and antecedents. We shall call
the former and latter ?syntactic? and ?seman-
tic? models, respectively.
Each parameter in Equation (4) is com-
puted as in Equations (5), where F (x) denotes
the frequency of x in corpora annotated with
anaphoric relations.
P (p
i
|c) =
F (p
i
, c)
?
j
F (p
j
, c)
P (d
i
) =
F (d
i
)
?
j
F (d
j
)
P (r
i
) =
F (r
i
)
?
j
F (r
j
)
P (n
i
|v, c) =
F (n
i
, v, c)
?
j
F (n
j
, v, c)
(5)
However, since estimating a semantic model,
P (n
i
|v, c), needs large-scale annotated corpora,
the data sparseness problem is crucial. Thus,
we explore the use of unannotated corpora.
For P (n
i
|v, c), v and c are features for a zero
pronoun, and n
i
is a feature for an antecedent.
However, we can regard v, c, and n
i
as features
for a verb and its case noun because zero pro-
nouns are omitted case nouns. Thus, it is pos-
sible to estimate the probability based on co-
occurrences of verbs and their case nouns, which
can be extracted automatically from large-scale
unannotated corpora.
2.4 Computing Certainty Score
Since zero pronoun analysis is not a stand-alone
application, our system is used as a module in
other NLP applications, such as machine trans-
lation. In those applications, it is desirable that
erroneous anaphoric relations are not generated.
Thus, we propose a notion of certainty to out-
put only zero pronouns that are detected and
resolved with a high certainty score.
We formalize the certainty score, C(?
c
), for
each zero pronoun as in Equation (6), where
P
1
(?
c
) and P
2
(?
c
) denote probabilities com-
puted by Equation (1) for the first and second
ranked candidates, respectively. In addition, t is
a parametric constant, which is experimentally
set to 0.5.
C(?
c
) = t?P
1
(?
c
) + (1?t)(P
1
(?
c
)?P
2
(?
c
)) (6)
The certainty score becomes great in the case
where P
1
(?
c
) is sufficiently great and signifi-
cantly greater than P
2
(?
c
).
3 Evaluation
3.1 Methodology
To investigate the performance of our system,
we used Kyotodaigaku Text Corpus version
2.0 (Kurohashi and Nagao, 1998a), in which
20,000 articles in Mainichi Shimbun newspaper
articles in 1995 were analyzed by JUMAN and
KNP (i.e., the morph/syntax analyzers used in
our system) and revised manually. From this
corpus, we randomly selected 30 general articles
(e.g., politics and sports) and manually anno-
tated those articles with anaphoric relations for
zero pronouns. The number of zero pronouns
contained in those articles was 449.
We used a leave-one-out cross-validation eval-
uation method: we conducted 30 trials in each
of which one article was used as a test input
and the remaining 29 articles were used for pro-
ducing a syntactic model. We used six years
worth of Mainichi Shimbun newspaper arti-
cles (Mainichi Shimbunsha, 1994?1999) to pro-
duce a semantic model based on co-occurrences
of verbs and their case nouns.
To extract verbs and their case noun pairs
from newspaper articles, we performed a mor-
phological analysis by JUMAN and extracted
dependency relations using a relatively simple
rule: we assumed that each noun modifies the
verb of highest proximity. As a result, we
obtained 12 million co-occurrences associated
with 6,194 verb types. Then, we generalized
the extracted nouns into semantic classes in
the Japanese Bunruigoihyou thesaurus. In the
case where a noun was associated with multiple
classes, the noun was assigned to all possible
classes. In the case where a noun was not listed
in the thesaurus, the noun itself was regarded
as a single semantic class.
3.2 Comparative Experiments
Fundamentally, our evaluation is two-fold: we
evaluated only zero pronoun resolution (an-
tecedent identification) and a combination of
detection and resolution. In the former case,
we assumed that all the zero pronouns are cor-
rectly detected, and investigated the effective-
ness of the resolution model, P (a
i
|?). In the
latter case, we investigated the effectiveness of
the combined model, P (a
i
|?
c
) ? P
zero
(c|v).
First, we compared the performance of the
following different models for zero pronoun res-
olution, P (a
i
|?):
? a semantic model produced based on anno-
tated corpora (Sem1),
? a semantic model produced based on unan-
notated corpora, using co-occurrences of
verbs and their case nouns (Sem2),
? a syntactic model (Syn),
? a combination of Syn and Sem1 (Both1),
? a combination of Syn and Sem2 (Both2),
which is our complete model for zero pro-
noun resolution,
? a rule-based model (Rule).
As a control (baseline) model, we took approxi-
mately two man-months to develop a rule-based
model (Rule) through an analysis on ten articles
in Kyotodaigaku Text Corpus. This model uses
rules typically used in existing rule-based meth-
ods: 1) post-positional particles that follow an-
tecedent candidates, 2) proximity between zero
pronouns and antecedent candidates, and 3)
conjunctive particles. We did not use seman-
tic properties in the rule-based method because
they decreased the system accuracy in a prelim-
inary study.
Table 1: Experimental results for zero pronoun resolution.
# of Correct cases (Accuracy)
k Sem1 Sem2 Syn Both1 Both2 Rule
1 25 (6.2%) 119 (29.5%) 185 (45.8%) 30 (7.4%) 205 (50.7%) 162 (40.1%)
2 46 (11.4%) 193 (47.8%) 227 (56.2%) 49 (12.1%) 250 (61.9%) 213 (52.7%)
3 72 (17.8%) 230 (56.9%) 262 (64.9%) 75 (18.6%) 280 (69.3%) 237 (58.6%)
Table 1 shows the results, where we regarded
the k-best antecedent candidates as the final
output and compared results for different values
of k. In the case where the correct answer was
included in the k-best candidates, we judged it
correct. In addition, ?Accuracy? is the ratio be-
tween the number of zero pronouns whose an-
tecedents were correctly identified and the num-
ber of zero pronouns correctly detected by the
system (404 for all the models). Bold figures
denote the highest performance for each value
of k across different models. Here, the average
number of antecedent candidates per zero pro-
noun was 27 regardless of the model, and thus
the accuracy was 3.7% in the case where the
system randomly selected antecedents.
Looking at the results for two different seman-
tic models, Sem2 outperformed Sem1, which
indicates that the use of co-occurrences of verbs
and their case nouns was effective to identify
antecedents and avoid the data sparseness prob-
lem in producing a semantic model.
The syntactic model, Syn, outperformed the
two semantic models independently, and there-
fore the syntactic features used in our model
were more effective than the semantic features
to identify antecedents. When both syntactic
and semantic models were used in Both2, the
accuracy was further improved. While the rule-
based method, Rule, achieved a relatively high
accuracy, our complete model, Both2, outper-
formed Rule irrespective of the value of k. To
sum up, we conclude that both syntactic and
semantic models were effective to identify ap-
propriate anaphoric relations.
At the same time, since our method requires
annotated corpora, the relation between the
corpus size and accuracy is crucial. Thus, we
performed two additional experiments associ-
ated with Both2.
In the first experiment, we varied the number
of annotated articles used to produce a syntactic
model, where a semantic model was produced
25
30
35
40
45
50
55
0 5 10 15 20 25
0 1 2 3 4 5 6
a
cc
u
ra
cy
 (%
)
annotated corpus size for producing a syntactic model (#articles)
unannotated corpus size for producing a semantic model (year)
unannotated
annotated
Figure 2: The relation between the corpus size
and accuracy for a combination of syntactic and
semantic models (Both2).
based on six years worth of newspaper articles.
In the second experiment, we varied the num-
ber of unannotated articles used to produce a
semantic model, where a syntactic model was
produced based on 29 annotated articles. In
Figure 2, we show two independent results as
space is limited: the dashed and solid graphs
correspond to the results of the first and second
experiments, respectively. Given all the articles
for modeling, the resultant accuracy for each ex-
periment was 50.7%, which corresponds to that
for Both2 with k = 1 in Table 1.
In the case where the number of articles was
varied in producing a syntactic model, the ac-
curacy improved rapidly in the first five arti-
cles. This indicates that a high accuracy can
be obtained by a relatively small number of su-
pervised articles. In the case where the amount
of unannotated corpora was varied in produc-
ing a semantic model, the accuracy marginally
improved as the corpus size increases. However,
note that we do not need human supervision to
produce a semantic model.
Finally, we evaluated the effectiveness of the
25
30
35
40
45
50
55
60
65
70
10 20 30 40 50 60 70 80 90
a
cc
u
ra
cy
 (%
)
coverage (%)
P(ai|?c)?Pzero(c|v)
P(ai|?c)
Figure 3: The relation between coverage and
accuracy for zero pronoun detection (Both2).
50
55
60
65
70
75
80
0 10 20 30 40 50 60 70 80 90 100
a
cc
u
ra
cy
 (%
)
coverage (%)
P(ai|?c)?Pzero(c|v)
P(ai|?c)
Figure 4: The relation between coverage and
accuracy for antecedent identification (Both2).
combination of zero pronoun detection and res-
olution in Equation (1). To investigate the con-
tribution of the detection model, P
zero
(c|v), we
used P (a
i
|?
c
) for comparison. Both cases used
Both2 to compute the probability for zero pro-
noun resolution. We varied a threshold for the
certainty score to plot coverage-accuracy graphs
for zero pronoun detection (Figure 3) and an-
tecedent identification (Figure 4).
In Figure 3, ?coverage? is the ratio between
the number of zero pronouns correctly detected
by the system and the total number of zero pro-
nouns in input texts, and ?accuracy? is the ratio
between the number of zero pronouns correctly
detected and the total number of zero pronouns
detected by the system. Note that since our sys-
tem failed to detect a number of zero pronouns,
the coverage could not be 100%.
Figure 3 shows that as the coverage decreases,
the accuracy improved irrespective of the model
used. When compared with the case of P (a
i
|?),
our model, P (a
i
|?)?P
zero
(c|v), achieved a higher
accuracy regardless of the coverage.
In Figure 4, ?coverage? is the ratio between
the number of zero pronouns whose antecedents
were generated and the number of zero pro-
nouns correctly detected by the system. The
accuracy was improved by decreasing the cov-
erage, and our model marginally improved the
accuracy for P (a
i
|?).
According to those above results, our model
was effective to improve the accuracy for zero
pronoun detection and did not have side effect
on the antecedent identification process. As a
result, the overall accuracy of zero pronoun de-
tection and resolution was improved.
4 Related Work
Kim and Ehara (1995) proposed a probabilis-
tic model to resolve subjective zero pronouns
for the purpose of Japanese/English machine
translation. In their model, the search scope
for possible antecedents was limited to the sen-
tence containing zero pronouns. In contrast,
our method can resolve zero pronouns in both
intra/inter-sentential anaphora types.
Aone and Bennett (1995) used a decision tree
to determine appropriate antecedents for zero
pronouns. They focused on proper and definite
nouns used in anaphoric expressions as well as
zero pronouns. However, their method resolves
only anaphors that refer to organization names
(e.g., private companies), which are generally
easier to resolve than our case.
Both above existing methods require anno-
tated corpora for statistical modeling, while we
used corpora with/without annotations related
to anaphoric relations, and thus we can eas-
ily obtain large-scale corpora to avoid the data
sparseness problem.
Nakaiwa (2000) used Japanese/English bilin-
gual corpora to identify anaphoric relations of
Japanese zero pronouns by comparing J/E sen-
tence pairs. The rationale behind this method
is that obligatory cases zero-pronominalized
in Japanese are usually expressed in English.
However, in the case where corresponding En-
glish expressions are pronouns and anaphors,
their method is not effective. Additionally,
bilingual corpora are more expensive to obtain
than monolingual corpora used in our method.
Finally, our method integrates a parameter
for zero pronoun detection in computing the cer-
tainty score. Thus, we can improve the accuracy
of our system by discarding extraneous outputs
with a small certainty score.
5 Conclusion
We proposed a probabilistic model to ana-
lyze Japanese zero pronouns that refer to an-
tecedents in the previous context. Our model
consists of two probabilistic parameters corre-
sponding to detecting zero pronouns and iden-
tifying their antecedents, respectively. The lat-
ter is decomposed into syntactic and semantic
properties. To estimate those parameters ef-
ficiently, we used annotated/unannotated cor-
pora. In addition, we formalized the certainty
score to improve the accuracy. Through exper-
iments, we showed that the use of unannotated
corpora was effective to avoid the data sparse-
ness problem and that the certainty score fur-
ther improved the accuracy.
Future work would include word sense disam-
biguation for polysemous predicate verbs to se-
lect appropriate case frames in the zero pronoun
detection process.
References
Chinatsu Aone and Scott William Bennett. 1995.
Evaluating automated and manual acquisition of
anaphora resolution strategies. In Proceedings of
33th Annual Meeting of the Association for Com-
putational Linguistics, pages 122?129.
Niyu Ge, John Hale, and Eugene Charniak. 1998.
A statistical approach to anaphora resolution. In
Proceedings of the Sixth Workshop on Very Large
Corpora, pages 161?170.
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference - 6: A brief his-
tory. In Proceedings of the 16th International
Conference on Computational Linguistics, pages
466?471.
Barbara J. Grosz, Aravind K. Joshi, and Scott We-
instein. 1995. Centering: A framework for mod-
eling the local coherence of discourse. Computa-
tional Linguistics, 21(2):203?226.
Jerry R. Hobbs. 1978. Resolving pronoun refer-
ences. Lingua, 44:311?338.
Information-technology Promotion Agency, 1987.
IPA Lexicon of the Japanese language for com-
puters (Basic Verbs). (in Japanese).
Megumi Kameyama. 1986. A property-sharing con-
straint in centering. In Proceedings of the 24th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 200?206.
Yeun-Bae Kim and Terumasa Ehara. 1995. Zero-
subject resolution method based on probabilistic
inference with evaluation function. In Proceedings
of the 3rd Natural Language Processing Pacific-
Rim Symposium, pages 721?727.
Sadao Kurohashi and Makoto Nagao. 1998a. Build-
ing a Japanese parsed corpus while improving the
parsing system. In Proceedings of The 1st In-
ternational Conference on Language Resources &
Evaluation, pages 719?724.
Sadao Kurohashi and Makoto Nagao, 1998b.
Japanese morphological analysis system JUMAN
version 3.6 manual. Department of Informatics,
Kyoto University. (in Japanese).
Sadao Kurohashi, 1998. Japanese Dependency/Case
Structure Analyzer KNP version 2.0b6. De-
partment of Informatics, Kyoto University. (in
Japanese).
Mainichi Shimbunsha. 1994?1999. Mainichi Shim-
bun CD-ROM.
Ruslan Mitkov, Lamia Belguith, and Malgorzata
Stys. 1998. Multilingual robust anaphora reso-
lution. In Proceedings of the 3rd Conference on
Empirical Methods in Natural Language Process-
ing, pages 7?16.
Hiromi Nakaiwa and Satoshi Shirai. 1996.
Anaphora resolution of Japanese zero pronouns
with deictic reference. In Proceedings of the
16th International Conference on Computational
Linguistics, pages 812?817.
Hiromi Nakaiwa. 2000. An environment for extract-
ing resolution rules of zero pronouns from corpora.
In COLING-2000 Workshop on Semantic Anno-
tation and Intelligent Content, pages 44?52.
National Language Research Institute. 1964. Bun-
ruigoihyou. Shuei publisher. (in Japanese).
Manabu Okumura and Kouji Tamura. 1996. Zero
pronoun resolution in Japanese discourse based
on centering theory. In Proceedings of the 16th
International Conference on Computational Lin-
guistics, pages 871?876.
Manuel Palomar, Antonio Ferra?ndez, Lidia Moreno,
Patricio Mart??nez-Barco, Jesu?s Peral, Maximil-
iano Saiz-Noeda, and Rafael Mu noz. 2001. An al-
gorithm for anaphora resolution in Spanish texts.
Computational Linguistics, 27(4):545?568.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Marilyn Walker, Masayo Iida, and Sharon Cote.
1994. Japanese discourse and the process of cen-
tering. Computational Linguistics, 20(2):193?233.
 	
  Organizing Encyclopedic Knowledge based on the Web and its
Application to Question Answering
Atsushi Fujii
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
CREST, Japan Science and
Technology Corporation
fujii@ulis.ac.jp
Tetsuya Ishikawa
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
ishikawa@ulis.ac.jp
Abstract
We propose a method to generate large-scale
encyclopedic knowledge, which is valuable
for much NLP research, based on the Web.
We first search the Web for pages contain-
ing a term in question. Then we use lin-
guistic patterns and HTML structures to ex-
tract text fragments describing the term. Fi-
nally, we organize extracted term descrip-
tions based on word senses and domains. In
addition, we apply an automatically gener-
ated encyclopedia to a question answering
system targeting the Japanese Information-
Technology Engineers Examination.
1 Introduction
Reflecting the growth in utilization of the World Wide
Web, a number of Web-based language processing
methods have been proposed within the natural lan-
guage processing (NLP), information retrieval (IR)
and artificial intelligence (AI) communities. A sam-
ple of these includes methods to extract linguistic
resources (Fujii and Ishikawa, 2000; Resnik, 1999;
Soderland, 1997), retrieve useful information in re-
sponse to user queries (Etzioni, 1997; McCallum et
al., 1999) and mine/discover knowledge latent in the
Web (Inokuchi et al, 1999).
In this paper, mainly from an NLP point of view,
we explore a method to produce linguistic resources.
Specifically, we enhance the method proposed by Fu-
jii and Ishikawa (2000), which extracts encyclopedic
knowledge (i.e., term descriptions) from the Web.
In brief, their method searches the Web for pages
containing a term in question, and uses linguistic ex-
pressions and HTML layouts to extract fragments de-
scribing the term. They also use a language model to
discard non-linguistic fragments. In addition, a clus-
tering method is used to divide descriptions into a spe-
cific number of groups.
On the one hand, their method is expected to en-
hance existing encyclopedias, where vocabulary size
is relatively limited, and therefore the quantity prob-
lems has been resolved.
On the other hand, encyclopedias extracted from the
Web are not comparable with existing ones in terms of
quality. In hand-crafted encyclopedias, term descrip-
tions are carefully organized based on domains and
word senses, which are especially effective for human
usage. However, the output of Fujii?s method is simply
a set of unorganized term descriptions. Although clus-
tering is optionally performed, resultant clusters are
not necessarily related to explicit criteria, such as word
senses and domains.
To sum up, our belief is that by combining extrac-
tion and organization methods, we can enhance both
quantity and quality of Web-based encyclopedias.
Motivated by this background, we introduce an or-
ganization model to Fujii?s method and reformalize
the whole framework. In other words, our proposed
method is not only extraction but generation of ency-
clopedic knowledge.
Section 2 explains the overall design of our ency-
clopedia generation system, and Section 3 elaborates
on our organization model. Section 4 then explores
a method for applying our resultant encyclopedia to
NLP research, specifically, question answering. Sec-
tion 5 performs a number of experiments to evaluate
our methods.
2 System Design
2.1 Overview
Figure 1 depicts the overall design of our system,
which generates an encyclopedia for input terms.
Our system, which is currently implemented for
Japanese, consists of three modules: ?retrieval,? ?ex-
traction? and ?organization,? among which the orga-
nization module is newly introduced in this paper. In
principle, the remaining two modules (?retrieval? and
?extraction?) are the same as proposed by Fujii and
Ishikawa (2000).
In Figure 1, terms can be submitted either on-line or
off-line. A reasonable method is that while the system
periodically updates the encyclopedia off-line, terms
unindexed in the encyclopedia are dynamically pro-
cessed in real-time usage. In either case, our system
processes input terms one by one.
We briefly explain each module in the following
three sections, respectively.
domain
model
Web
extraction
rules
organization
encyclopedia
retrieval
extraction
term(s)
description
model
Figure 1: The overall design of our Web-based ency-
clopedia generation system.
2.2 Retrieval
The retrieval module searches the Web for pages con-
taining an input term, for which existing Web search
engines can be used, and those with broad coverage
are desirable.
However, search engines performing query expan-
sion are not always desirable, because they usually re-
trieve a number of pages which do not contain an in-
put keyword. Since the extraction module (see Sec-
tion 2.3) analyzes the usage of the input term in re-
trieved pages, pages not containing the term are of no
use for our purpose.
Thus, we use as the retrieval module ?Google,?
which is one of the major search engines and does not
conduct query expansion1.
2.3 Extraction
In the extraction module, given Web pages containing
an input term, newline codes, redundant white spaces
and HTML tags that are not used in the following pro-
cesses are discarded to standardize the page format.
Second, we approximately identify a region describ-
ing the term in the page, for which two rules are used.
1http://www.google.com/
The first rule is based on Japanese linguistic patterns
typically used for term descriptions, such as ?X toha
Y dearu (X is Y).? Following the method proposed
by Fujii and Ishikawa (2000), we semi-automatically
produced 20 patterns based on the Japanese CD-ROM
World Encyclopedia (Heibonsha, 1998), which in-
cludes approximately 80,000 entries related to various
fields. It is expected that a region including the sen-
tence that matched with one of those patterns can be a
term description.
The second rule is based on HTML layout. In a typ-
ical case, a term in question is highlighted as a heading
with tags such as <DT>, <B> and <Hx> (?x? denotes
a digit), followed by its description. In some cases,
terms are marked with the anchor <A> tag, providing
hyperlinks to pages where they are described.
Finally, based on the region briefly identified by the
above method, we extract a page fragment as a term
description. Since term descriptions usually consist of
a logical segment (such as a paragraph) rather than a
single sentence, we extract a fragment that matched
with one of the following patterns, which are sorted
according to preference in descending order:
1. description tagged with <DD> in the case where
the term is tagged with <DT>2,
2. paragraph tagged with <P>,
3. itemization tagged with <UL>,
4. N sentences, where we empirically set N = 3.
2.4 Organization
As discussed in Section 1, organizing information ex-
tracted from the Web is crucial in our framework. For
this purpose, we classify extracted term descriptions
based on word senses and domains.
Although a number of methods have been proposed
to generate word senses (for example, one based on the
vector space model (Schu?tze, 1998)), it is still difficult
to accurately identify word senses without explicit dic-
tionaries that define sense candidates.
In addition, since word senses are often associated
with domains (Yarowsky, 1995), word senses can be
consequently distinguished by way of determining the
domain of each description. For example, different
senses for ?pipeline (processing method/transportation
pipe)? are associated with the computer and construc-
tion domains (fields), respectively.
To sum up, the organization module classifies term
descriptions based on domains, for which we use do-
main and description models. In Section 3, we elabo-
rate on our organization model.
2<DT> and <DD> are inherently provided to describe
terms in HTML.
3 Statistical Organization Model
3.1 Overview
Given one or more (in most cases more than one)
descriptions for a single input term, the organization
module selects appropriate description(s) for each do-
main related to the term.
We do not need all the extracted descriptions as fi-
nal outputs, because they are usually similar to one
another, and thus are redundant.
For the moment, we assume that we know a priori
which domains are related to the input term.
From the viewpoint of probability theory, our task
here is to select descriptions with greater probability
for given domains. The probability for description d
given domain c, P (d|c), is commonly transformed as
in Equation (1), through use of the Bayesian theorem.
P (d|c) = P (c|d) ? P (d)P (c) (1)
In practice, P (c) can be omitted because this factor is
a constant, and thus does not affect the relative proba-
bility for different descriptions.
In Equation (1), P (c|d) models a probability that d
corresponds to domain c. P (d) models a probability
that d can be a description for the term in question,
disregarding the domain. We shall call them domain
and description models, respectively.
To sum up, in principle we select d?s that are
strongly associated with a specific domain, and are
likely to be descriptions themselves.
Extracted descriptions are not linguistically under-
standable in the case where the extraction process is
unsuccessful and retrieved pages inherently contain
non-linguistic information (such as special characters
and e-mail addresses).
To resolve this problem, Fujii and Ishikawa (2000)
used a language model to filter out descriptions with
low perplexity. However, in this paper we integrated
a description model, which is practically the same as
a language model, with an organization model. The
new framework is more understandable with respect
to probability theory.
In practice, we first use Equation (1) to compute
P (d|c) for all the c?s predefined in the domain model.
Then we discard such c?s whose P (d|c) is below a spe-
cific threshold. As a result, for the input term, related
domains and descriptions are simultaneously selected.
Thus, we do not have to know a priori which domains
are related to each term.
In the following two sections, we explain methods
to realize the domain and description models, respec-
tively.
3.2 Domain Model
The domain model quantifies the extent to which de-
scription d is associated with domain c, which is fun-
damentally a categorization task. Among a number
of existing categorization methods, we experimentally
used one proposed by Iwayama and Tokunaga (1994),
which formulates P (c|d) as in Equation (2).
P (c|d) = P (c) ?
?
t
P (t|c) ? P (t|d)
P (t) (2)
Here, P (t|d), P (t|c) and P (t) denote probabilities
that word t appears in d, c and all the domains, respec-
tively. We regard P (c) as a constant. While P (t|d) is
simply a relative frequency of t in d, we need prede-
fined domains to compute P (t|c) and P (t). For this
purpose, the use of large-scale corpora annotated with
domains is desirable.
However, since those resources are prohibitively
expensive, we used the ?Nova? dictionary for
Japanese/English machine translation systems3, which
includes approximately one million entries related to
19 technical fields as listed below:
aeronautics, biotechnology, business, chem-
istry, computers, construction, defense,
ecology, electricity, energy, finance, law,
mathematics, mechanics, medicine, metals,
oceanography, plants, trade.
We extracted words from dictionary entries to esti-
mate P (t|c) and P (t), which are relative frequencies
of t in c and all the domains, respectively. We used
the ChaSen morphological analyzer (Matsumoto et al,
1997) to extract words from Japanese entries. We also
used English entries because Japanese descriptions of-
ten contain English words.
It may be argued that statistics extracted from dic-
tionaries are unreliable, because word frequencies in
real word usage are missing. However, words that are
representative for a domain tend to be frequently used
in compound word entries associated with the domain,
and thus our method is a practical approximation.
3.3 Description Model
The description model quantifies the extent to which a
given page fragment is feasible as a description for the
input term. In principle, we decompose the description
model into language and quality properties, as shown
in Equation (3).
P (d) = P
L
(d) ? P
Q
(d) (3)
Here, P
L
(d) and P
Q
(d) denote language and quality
models, respectively.
3Produced by NOVA, Inc.
It is expected that the quality model discards in-
correct or misleading information contained in Web
pages. For this purpose, a number of quality rating
methods for Web pages (Amento et al, 2000; Zhu and
Gauch, 2000) can be used.
However, since Google (i.e., the search engine used
in our system) rates the quality of pages based on
hyperlink information, and selectively retrieves those
with higher quality (Brin and Page, 1998), we tenta-
tively regarded P
Q
(d) as a constant. Thus, in practice
the description model is approximated solely with the
language model as in Equation (4).
P (d) ? P
L
(d) (4)
Statistical approaches to language modeling have
been used in much NLP research, such as machine
translation (Brown et al, 1993) and speech recogni-
tion (Bahl et al, 1983). Our model is almost the same
as existing models, but is different in two respects.
First, while general language models quantify the
extent to which a given word sequence is linguisti-
cally acceptable, our model also quantifies the extent
to which the input is acceptable as a term description.
Thus, we trained the model based on an existing ma-
chine readable encyclopedia.
We used the ChaSen morphological analyzer to
segment the Japanese CD-ROM World Encyclope-
dia (Heibonsha, 1998) into words (we replaced head-
words with a common symbol), and then used the
CMU-Cambridge toolkit (Clarkson and Rosenfeld,
1997) to model a word-based trigram.
Consequently, descriptions in which word se-
quences are more similar to those in the World En-
cyclopedia are assigned greater probability scores
through our language model.
Second, P (d), which is a product of probabilities
for N -grams in d, is quite sensitive to the length of d.
In the cases of machine translation and speech recog-
nition, this problem is less crucial because multiple
candidates compared based on the language model are
almost equivalent in terms of length.
However, since in our case length of descriptions are
significantly different, shorter descriptions are more
likely to be selected, regardless of the quality. To avoid
this problem, we normalize P (d) by the number of
words contained in d.
4 Application
4.1 Overview
Encyclopedias generated through our Web-based
method can be used in a number of applications, in-
cluding human usage, thesaurus production (Hearst,
1992; Nakamura and Nagao, 1988) and natural lan-
guage understanding in general.
Among the above applications, natural language un-
derstanding (NLU) is the most challenging from a sci-
entific point of view. Current practical NLU research
includes dialogue, information extraction and question
answering, among which we focus solely on question
answering (QA) in this paper.
A straightforward application is to answer inter-
rogative questions like ?What is X?? in which a QA
system searches the encyclopedia database for one or
more descriptions related to X (this application is also
effective for dialog systems).
In general, the performance of QA systems are eval-
uated based on coverage and accuracy. Coverage is
the ratio between the number of questions answered
(disregarding their correctness) and the total number
of questions. Accuracy is the ratio between the num-
ber of correct answers and the total number of answers
made by the system.
While coverage can be estimated objectively and
systematically, estimating accuracy relies on human
subjects (because there is no absolute description for
term X), and thus is expensive.
In view of this problem, we targeted Information
Technology Engineers Examinations4, which are bian-
nual (spring and autumn) examinations necessary for
candidates to qualify to be IT engineers in Japan.
Among a number of classes, we focused on the
?Class II? examination, which requires fundamental
and general knowledge related to information technol-
ogy. Approximately half of questions are associated
with IT technical terms.
Since past examinations and answers are open to the
public, we can evaluate the performance of our QA
system with minimal cost.
4.2 Analyzing IT Engineers Examinations
The Class II examination consists of quadruple-choice
questions, among which technical term questions can
be subdivided into two types.
In the first type of question, examinees choose
the most appropriate description for a given technical
term, such as ?memory interleave? and ?router.?
In the second type of question, examinees choose
the most appropriate term for a given question, for
which we show examples collected from the exami-
nation in the autumn of 1999 (translated into English
by one of the authors) as follows:
1. Which data structure is most appropriate for
FIFO (First-In First-Out)?
a) binary trees, b) queues, c) stacks, d) heaps
2. Choose the LAN access method in which mul-
tiple terminals transmit data simultaneously and
4Japan Information-Technology Engineers Examination
Center. http://www.jitec.jipdec.or.jp/
thus they potentially collide.
a) ATM, b) CSM/CD, c) FDDI, d) token ring
In the autumn of 1999, out of 80 questions, the num-
ber of the first and second types were 22 and 18, re-
spectively.
4.3 Implementing a QA system
For the first type of question, human examinees would
search their knowledge base (i.e., memory) for the de-
scription of a given term, and compare that description
with four candidates. Then they would choose the can-
didate that is most similar to the description.
For the second type of question, human examinees
would search their knowledge base for the description
of each of four candidate terms. Then they would
choose the candidate term whose description is most
similar to the question description.
The mechanism of our QA system is analogous to
the above human methods. However, unlike human
examinees, our system uses an encyclopedia generated
from the Web as a knowledge base.
In addition, our system selectively uses term de-
scriptions categorized into domains related to infor-
mation technology. In other words, the description
of ?pipeline (transportation pipe)? is irrelevant or mis-
leading to answer questions associated with ?pipeline
(processing method).?
To compute the similarity between two descriptions,
we used techniques developed in IR research, in which
the similarity between a user query and each document
in a collection is usually quantified based on word fre-
quencies. In our case, a question and four possible
answers correspond to query and document collection,
respectively. We used a probabilistic method (Robert-
son and Walker, 1994), which is one of the major IR
methods.
To sum up, given a question, its type and four
choices, our QA system chooses one of four candi-
dates as the answer, in which the resolution algorithm
varies depending on the question type.
4.4 Related Work
Motivated partially by the TREC-8 QA collec-
tion (Voorhees and Tice, 2000), question answering
has of late become one of the major topics within the
NLP/IR communities.
In fact, a number of QA systems targeting
the TREC QA collection have recently been pro-
posed (Harabagiu et al, 2000; Moldovan and
Harabagiu, 2000; Prager et al, 2000). Those sys-
tems are commonly termed ?open-domain? systems,
because questions expressed in natural language are
not necessarily limited to explicit axes, including who,
what, when, where, how and why.
However, Moldovan and Harabagiu (2000) found
that each of the TREC questions can be recast as ei-
ther a single axis or a combination of axes. They also
found that out of the 200 TREC questions, 64 ques-
tions (approximately one third) were associated with
the what axis, for which the Web-based encyclopedia
is expected to improve the quality of answers.
Although Harabagiu et al (2000) proposed a
knowledge-based QA system, most existing systems
rely on conventional IR and shallow NLP methods.
The use of encyclopedic knowledge for QA systems,
as we demonstrated, needs to be further explored.
5 Experimentation
5.1 Methodology
We conducted a number of experiments to investigate
the effectiveness of our methods.
First, we generated an encyclopedia by way of our
Web-based method (see Sections 2 and 3), and evalu-
ated the quality of the encyclopedia itself.
Second, we applied the generated encyclopedia to
our QA system (see Section 4), and evaluated its per-
formance. The second experiment can be seen as a
task-oriented evaluation for our encyclopedia genera-
tion method.
In the first experiment, we collected 96 terms from
technical term questions in the Class II examination
(the autumn of 1999). We used as test inputs those 96
terms and generated an encyclopedia, which was used
in the second experiment.
For all the 96 test terms, Google (see Section 2.2)
retrieved a positive number of pages, and the average
number of pages for one term was 196,503. Since
Google practically outputs contents of the top 1,000
pages, the remaining pages were not used in our ex-
periments.
In the following two sections, we explain the first
and second experiments, respectively.
5.2 Evaluating Encyclopedia Generation
For each test term, our method first computed P (d|c)
using Equation (1) and discarded domains whose
P (d|c) was below 0.05. Then, for each remaining do-
main, descriptions with higher P (d|c) were selected as
the final outputs.
We selected the top three (not one) descriptions for
each domain, because reading a couple of descriptions,
which are short paragraphs, is not laborious for human
users in real-world usage. As a result, at least one de-
scription was generated for 85 test terms, disregarding
the correctness. The number of resultant descriptions
was 326 (3.8 per term). We analyzed those descrip-
tions from different perspectives.
First, we analyzed the distribution of the Google
ranks for the Web pages from which the top three de-
scriptions were eventually retained. Figure 2 shows
the result, where we have combined the pages in
groups of 50, so that the leftmost bar, for example, de-
notes the number of used pages whose original Google
ranks ranged from 1 to 50.
Although the first group includes the largest number
of pages, other groups are also related to a relatively
large number of pages. In other words, our method
exploited a number of low ranking pages, which are
not browsed or utilized by most Web users.
0
10
20
30
40
50
60
70
0 100 200 300 400 500 600 700 800 900 1000
# 
of
 p
ag
es
ranking
Figure 2: Distribution of rankings for original pages in
Google.
Second, we analyzed the distribution of domains
assigned to the 326 resultant descriptions. Figure 3
shows the result, in which, as expected, most descrip-
tions were associated with the computer domain.
However, the law domain was unexpectedly asso-
ciated with a relatively great number of descriptions.
We manually analyzed the resultant descriptions and
found that descriptions for which appropriate domains
are not defined in our domain model, such as sports,
tended to be categorized into the law domain.
computers (200), law (41), electricity (28),
plants (15), medicine (10), finance (8),
mathematics (8), mechanics (5), biotechnology (4),
construction (2), ecology (2), chemistry (1),
energy (1), oceanography (1)
Figure 3: Distribution of domains related to the 326
resultant descriptions.
Third, we evaluated the accuracy of our method,
that is, the quality of an encyclopedia our method gen-
erated. For this purpose, each of the resultant descrip-
tions was judged as to whether or not it is a correct de-
scription for a term in question. Each domain assigned
to descriptions was also judged correct or incorrect.
We analyzed the result on a description-by-
description basis, that is, all the generated descriptions
were considered independent of one another. The ratio
of correct descriptions, disregarding the domain cor-
rectness, was 58.0% (189/326), and the ratio of cor-
rect descriptions categorized into the correct domain
was 47.9% (156/326).
However, since all the test terms are inherently re-
lated to the IT field, we focused solely on descriptions
categorized into the computer domain. In this case,
the ratio of correct descriptions, disregarding the do-
main correctness, was 62.0% (124/200), and the ratio
of correct descriptions categorized into the correct do-
main was 61.5% (123/200).
In addition, we analyzed the result on a term-by-
term basis, because reading only a couple of descrip-
tions is not crucial. In other words, we evaluated
each term (not description), and in the case where at
least one correct description categorized into the cor-
rect domain was generated for a term in question, we
judged it correct. The ratio of correct terms was 89.4%
(76/85), and in the case where we focused solely on the
computer domain, the ratio was 84.8% (67/79).
In other words, by reading a couple of descriptions
(3.8 descriptions per term), human users can obtain
knowledge of approximately 90% of input terms.
Finally, we compared the resultant descriptions with
an existing dictionary. For this purpose, we used the
?Nichigai? computer dictionary (Nichigai Associates,
1996), which lists approximately 30,000 Japanese
technical terms related to the computer field, and con-
tains descriptions for 13,588 terms. In the Nichigai
dictionary, 42 out of the 96 test terms were described.
Our method, which generated correct descriptions as-
sociated with the computer domain for 67 input terms,
enhanced the Nichigai dictionary in terms of quantity.
These results indicate that our method for generat-
ing encyclopedias is of operational quality.
5.3 Evaluating Question Answering
We used as test inputs 40 questions, which are related
to technical terms collected from the Class II exami-
nation in the autumn of 1999.
The objective here is not only to evaluate the perfor-
mance of our QA system itself, but also to evaluate the
quality of the encyclopedia generated by our method.
Thus, as performed in the first experiment (Sec-
tion 5.2), we used the Nichigai computer dictionary as
a baseline encyclopedia. We compared the following
three different resources as a knowledge base:
? the Nichigai dictionary (?Nichigai?),
? the descriptions generated in the first experiment
(?Web?),
? combination of both resources (?Nichigai +
Web?).
Table 1 shows the result of our comparative exper-
iment, in which ?C? and ?A? denote coverage and ac-
curacy, respectively, for variations of our QA system.
Since all the questions we used are quadruple-
choice, in case the system cannot answer the question,
random choice can be performed to improve the cov-
erage to 100%. Thus, for each knowledge resource we
compared cases without/with random choice, which
are denoted ?w/o Random? and ?w/ Random? in Ta-
ble 1, respectively.
Table 1: Coverage and accuracy (%) for different ques-
tion answering methods.
w/o Random w/ Random
Resource C A C A
Nichigai 50.0 65.0 100 45.0
Web 92.5 48.6 100 46.9
Nichigai + Web 95.0 63.2 100 61.3
In the case where random choice was not per-
formed, the Web-based encyclopedia noticeably im-
proved the coverage for the Nichigai dictionary, but
decreased the accuracy. However, by combining both
resources, the accuracy was noticeably improved, and
the coverage was comparable with that for the Nichi-
gai dictionary.
On the other hand, in the case where random choice
was performed, the Nichigai dictionary and the Web-
based encyclopedia were comparable in terms of both
the coverage and accuracy. Additionally, by combin-
ing both resources, the accuracy was further improved.
We also investigated the performance of our QA
system where descriptions related to the computer do-
main are solely used. However, coverage/accuracy did
not significantly change, because as shown in Figure 3,
most of the descriptions were inherently related to the
computer domain.
6 Conclusion
The World Wide Web has been an unprecedentedly
enormous information source, from which a number
of language processing methods have been explored
to extract, retrieve and discover various types of infor-
mation.
In this paper, we aimed at generating encyclopedic
knowledge, which is valuable for many applications
including human usage and natural language under-
standing. For this purpose, we reformalized an exist-
ing Web-based extraction method, and proposed a new
statistical organization model to improve the quality of
extracted data.
Given a term for which encyclopedic knowledge
(i.e., descriptions) is to be generated, our method se-
quentially performs a) retrieval of Web pages contain-
ing the term, b) extraction of page fragments describ-
ing the term, and c) organizing extracted descriptions
based on domains (and consequently word senses).
In addition, we proposed a question answering sys-
tem, which answers interrogative questions associated
with what, by using a Web-based encyclopedia as a
knowledge base. For the purpose of evaluation, we
used as test inputs technical terms collected from the
Class II IT engineers examination, and found that the
encyclopedia generated through our method was of
operational quality and quantity.
We also used test questions from the Class II exam-
ination, and evaluated the Web-based encyclopedia in
terms of question answering. We found that our Web-
based encyclopedia improved the system coverage ob-
tained solely with an existing dictionary. In addition,
when we used both resources, the performance was
further improved.
Future work would include generating information
associated with more complex interrogations, such as
ones related to how and why, so as to enhance Web-
based natural language understanding.
Acknowledgments
The authors would like to thank NOVA, Inc. for their
support with the Nova dictionary and Katunobu Itou
(The National Institute of Advanced Industrial Science
and Technology, Japan) for his insightful comments on
this paper.
References
Brian Amento, Loren Terveen, and Will Hill. 2000.
Does ?authority? mean quality? predicting expert
quality ratings of Web documents. In Proceedings
of the 23rd Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 296?303.
Lalit. R. Bahl, Frederick Jelinek, and Robert L. Mer-
cer. 1983. A maximum linklihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
5(2):179?190.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1?7):107?117.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statisti-
cal language modeling using the CMU-Cambridge
toolkit. In Proceedings of EuroSpeech?97, pages
2707?2710.
Oren Etzioni. 1997. Moving up the information food
chain. AI Magazine, 18(2):11?18.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: Extract-
ing term descriptions from semi-structured texts.
In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages
488?495.
Sanda M. Harabagiu, Marius A. Pas?ca, and Steven J.
Maiorano. 2000. Experiments with open-domain
textual question answering. In Proceedings of the
18th International Conference on Computational
Linguistics, pages 292?298.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 14th International Conference on Computa-
tional Linguistics, pages 539?545.
Hitachi Digital Heibonsha. 1998. CD-ROM World
Encyclopedia. (In Japanese).
Akihiro Inokuchi, Takashi Washio, Hiroshi Motoda,
Kouhei Kumasawa, and Naohide Arai. 1999. Bas-
ket analysis for graph structured data. In Proceed-
ings of the 3rd Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining, pages 420?431.
Makoto Iwayama and Takenobu Tokunaga. 1994. A
probabilistic model for text categorization: Based
on a single random variable with multiple values. In
Proceedings of the 4th Conference on Applied Nat-
ural Language Processing, pages 162?167.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki
Imamura. 1997. Japanese morphological analysis
system ChaSen manual. Technical Report NAIST-
IS-TR97007, NAIST. (In Japanese).
Andrew McCallum, Kamal Nigam, Jason Rennie, and
Kristie Seymore. 1999. A machine learning ap-
proach to building domain-specific search engines.
In Proceedings of the 16th International Joint Con-
ference on Artificial Intelligence, pages 662?667.
Dan Moldovan and Sanda Harabagiu. 2000. The
structure and performance of an open-domain ques-
tion answering system. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 563?570.
Jun?ichi Nakamura and Makoto Nagao. 1988. Extrac-
tion of semantic information from an ordinary En-
glish dictionary and its evaluation. In Proceedings
of the 10th International Conference on Computa-
tional Linguistics, pages 459?464.
Nichigai Associates. 1996. English-Japanese com-
puter terminology dictionary. (In Japanese).
John Prager, Eric Brown, and Anni Coden. 2000.
Question-answering by predictive annotation. In
Proceedings of the 23rd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 184?191.
Philip Resnik. 1999. Mining the Web for bilingual
texts. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 527?534.
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 232?241.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Stephen Soderland. 1997. Learning to extract text-
based information from the World Wide Web. In
Proceedings of 3rd International Conference on
Knowledge Discovery and Data Mining.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 200?207.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189?196.
Xiaolan Zhu and Susan Gauch. 2000. Incorporating
quality metrics in centralized/distributed informa-
tion retrieval on the World Wide Web. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 288?295.
Question Answering Using Encyclopedic Knowledge
Generated from the Web
Atsushi Fujii
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
CREST, Japan Science and
Technology Corporation
fujii@ulis.ac.jp
Tetsuya Ishikawa
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
ishikawa@ulis.ac.jp
Abstract
We propose a question answering sys-
tem which uses an encyclopedia as a
knowledge base. However, since ex-
isting encyclopedias lack technical/new
terms, we use an encyclopedia automat-
ically generated from the World Wide
Web. For this purpose, we first search
the Web for pages containing a term
in question. Then linguistic patterns
and HTML structures are used to ex-
tract text fragments describing the term.
Finally, extracted term descriptions are
organized based on word senses and
domains. We also evaluate our sys-
tem by way of experiments, where the
Japanese Information-Technology En-
gineers Examination is used as a test
collection.
1 Introduction
Motivated partially by the TREC-8 QA collec-
tion (Voorhees and Tice, 2000), question answer-
ing has of late become one of the major topics
within the natural language processing and infor-
mation retrieval communities, and a number of
QA systems targeting the TREC collection have
been proposed (Harabagiu et al, 2000; Moldovan
and Harabagiu, 2000; Prager et al, 2000).
Although Harabagiu et al (2000) proposed a
knowledge-based QA system, most existing sys-
tems rely on conventional IR and shallow NLP
methods. However, question answering is inher-
ently a more complicated procedure that usually
requires explicit knowledge bases.
In this paper, we propose a question answering
system which uses an encyclopedia as a knowl-
edge base. However, since existing (published)
encyclopedias usually lack technical/new terms,
we generate one based on the World Wide Web,
which includes a number of technical and recent
information. For this purpose, we use a modified
version of our method to extract term descriptions
from Web pages (Fujii and Ishikawa, 2000).
Intuitively, our system answers interrogative
questions like ?What is X?? in which a QA sys-
tem searches an encyclopedia database for one or
more descriptions related to term X.
The performance of QA systems can be evalu-
ated based on coverage and accuracy. Coverage
is the ratio between the number of questions an-
swered (disregarding their correctness) and the to-
tal number of questions. Accuracy is the ratio be-
tween the number of correct answers and the total
number of answers made by the system. While
coverage can be estimated objectively and sys-
tematically, estimating accuracy relies on human
subjects (because it is difficult to define the abso-
lute description for term X), and thus is expensive.
In view of this problem, we use as a test col-
lection Information Technology Engineers Exam-
inations1, which are biannual examinations nec-
essary for candidates to qualify to be IT engineers
in Japan.
Among a number of classes, we focus on the
?Class II? examination, which requires funda-
1Japan Information-Technology Engineers Examination
Center. http://www.jitec.jipdec.or.jp/
mental and general knowledge related to informa-
tion technology. Approximately half of questions
are associated with IT technical terms. Since past
examinations and answers are open to the pub-
lic, we can objectively evaluate the performance
of our QA system with minimal cost.
Our system is not categorized into ?open-
domain? systems, where questions expressed in
natural language are not limited to explicit axes
including who, what, when, where, how and why.
However, Moldovan and Harabagiu (2000)
found that each of the TREC questions can be re-
cast as either a single axis or a combination of
axes. They also found that out of the 200 TREC
questions, 64 questions (approximately one third)
were associated with the what axis, for which
our encyclopedia-based system is expected to im-
prove the quality of answers.
Section 2 analyzes the Japanese IT Engineers
Examination, and Section 3 explains our question
answering system. Then, Sections 4 and 5 elab-
orate on our Web-base method for encyclopedia
generation. Finally, Section 6 evaluates our sys-
tem by way of experiments.
2 IT Engineers Examinations
The Class II examination consists of quadruple-
choice questions, among which technical term
questions can be subdivided into two types.
In the first type of question, examinees choose
the most appropriate description for a given tech-
nical term, such as ?memory interleave? and
?router.?
In the second type of question, examinees
choose the most appropriate term for a given
question, for which we show examples collected
from the examination in the autumn of 1999
(translated into English by one of the authors) as
follows:
1. Which data structure is most appropriate for
FIFO (First-In First-Out)?
a) binary trees, b) queues, c) stacks, d) heaps
2. Choose a LAN access method where mul-
tiple terminals transmit data simultaneously
and thus they potentially collide.
a) ATM, b) CSM/CD, c) FDDI, d) token ring
In the autumn of 1999, out of 80 question, the
number of the first and second types were 22 and
18, respectively.
3 Overview of our QA system
For the first type of question (see Section 2),
human examinees would search their knowledge
base (i.e., memory) for the description of a given
term, and compare that description with four can-
didates. Then they would choose the candidate
that is most similar to the description.
For the second type of question, human exam-
inees would search their knowledge base for the
description of each of four candidate terms. Then
they would choose the candidate term whose de-
scription is most similar to the question.
The mechanism of our QA system is analogous
to the above human methods. However, our sys-
tem uses as a knowledge base an encyclopedia
generated from the Web.
To compute the similarity between two de-
scriptions, we use techniques developed in IR re-
search, in which the similarity between a user
query and each document in a collection is usu-
ally quantified based on word frequencies. In our
case, a question and four possible answers corre-
spond to query and document collection, respec-
tively. We use one of the major probabilistic IR
method (Robertson and Walker, 1994).
To sum up, given a question, its type and four
choices, our QA system chooses as the answer
one of four candidates, in which resolution algo-
rithm varies depending on the question type.
4 Encyclopedia Generation
4.1 Overview
Figure 1 depicts the overall design of our method
to generate an encyclopedia for input terms. This
figure consists of three modules: ?retrieval,? ?ex-
traction? and ?organization,? among which the
organization module is newly introduced in this
paper. In principle, the remaining two modules
(?retrieval? and ?extraction?) are the same as pro-
posed by Fujii and Ishikawa (2000).
In Figure 1, terms can be submitted either on-
line or off-line. A reasonable method is that while
the system periodically updates the encyclopedia
off-line, terms unindexed in the encyclopedia are
dynamically processed in real-time usage. In ei-
ther case, our system processes input terms one
by one. We briefly explain each module in the
following three sections, respectively.
domain
model
Web
extraction
rules
organization
encyclopedia
retrieval
extraction
term(s)
description
model
Figure 1: The overview of our Web-based ency-
clopedia generation process.
4.2 Retrieval
The retrieval module searches the Web for pages
containing an input term, for which existing Web
search engines can be used, and those with broad
coverage are desirable.
However, search engines performing query ex-
pansion are not always desirable, because they
usually retrieve a number of pages which do not
contain a query keyword. Since the extraction
module (see Section 4.3) analyzes the usage of
the input term in retrieved pages, pages not con-
taining the term are of no use for our purpose.
Thus, we use as the retrieval module ?Google,?
which is one of the major search engines and does
not conduct query expansion2.
4.3 Extraction
In the extraction module, given Web pages con-
taining an input term, newline codes, redundant
white spaces and HTML tags that are not used in
the following process are discarded so as to stan-
dardize the page format.
Second, we (approximately) identify a region
describing the term in the page, for which two
rules are used.
2http://www.google.com/
The first rule is based on Japanese linguis-
tic patterns typically used for term descrip-
tions, such as ?X toha Y dearu (X is Y).?
Following the method proposed by Fujii and
Ishikawa (2000), we semi-automatically pro-
duced 20 patterns based on the Japanese CD-
ROM World Encyclopedia (Heibonsha, 1998),
which includes approximately 80,000 entries re-
lated to various fields.
It is expected that a region including the sen-
tence that matched with one of those patterns can
be a term description.
The second rule is based on HTML layout. In
a typical case, a term in question is highlighted
as a heading with tags such as <DT>, <B> and
<Hx> (?x? denotes a digit), followed by its de-
scription. In some cases, terms are marked with
the anchor <A> tag, providing hyperlinks to pages
where they are described.
Finally, based on the region briefly identified
by the above method, we extract a page frag-
ment as a term description. Since term descrip-
tions usually consist of a logical segment (such
as a paragraph) rather than a single sentence, we
extract a fragment that matched with one of the
following patterns, which are sorted according to
preference in descending order:
1. description tagged with <DD> in the case
where the term is tagged with <DT>3,
2. paragraph tagged with <P>,
3. itemization tagged with <UL>,
4. N sentences, where we empirically set
N = 3.
4.4 Organization
For the purpose of organization, we classify ex-
tracted term descriptions based on word senses
and domains.
Although a number of methods have been pro-
posed to generate word senses (for example, one
based on the vector space model (Schu?tze, 1998)),
it is still difficult to accurately identify word
senses without explicit dictionaries that predefine
sense candidates.
3<DT> and <DD> are inherently provided to describe
terms in HTML.
Since word senses are often associated with
domains (Yarowsky, 1995), word senses can be
consequently distinguished by way of determin-
ing the domain of each description. For ex-
ample, different senses for ?pipeline (processing
method/transportation pipe)? are associated with
computer and construction domains (fields), re-
spectively.
To sum up, the organization module classifies
term descriptions based on domains, for which we
use domain and description models. In Section 5,
we elaborate on the organization model.
5 Statistical Organization Model
5.1 Overview
Given one or more (in most cases more than one)
descriptions for a single term, the organization
module selects appropriate description(s) for each
domain related to the term.
We do not need all the extracted descriptions
as final outputs, because they are usually similar
to one another, and thus are redundant. For the
moment, we assume that we know a priori which
domains are related to the input term.
From the viewpoint of probability theory, our
task here is to select descriptions with greater
probability for given domains. The probability
for description d given domain c, P (d|c), is com-
monly transformed as in Equation (1), through
use of the Bayesian theorem.
P (d|c) = P (c|d) ? P (d)P (c) (1)
In practice, P (c) can be omitted because this fac-
tor is a constant, and thus does not affect the rela-
tive probability for different descriptions.
In Equation (1), P (c|d) models a probability
that d corresponds to domain c. P (d) models a
probability that d can be a description for the term
in question, disregarding the domain. We shall
call them domain and description models, respec-
tively.
To sum up, in principle we select d?s that are
strongly associated with a certain domain, and are
likely to be descriptions themselves.
Extracted descriptions are not linguistically un-
derstandable in the case where the extraction pro-
cess is unsuccessful and retrieved pages inher-
ently contain non-linguistic information (such as
special characters and e-mail addresses).
To resolve this problem, we previously used
a language model to filter out descriptions with
low perplexity (Fujii and Ishikawa, 2000). How-
ever, in this paper we integrated a description
model, which is practically the same as a lan-
guage model, with an organization model. The
new framework is more understandable with re-
spect to probability theory.
In practice, we first use Equation (1) to com-
pute P (d|c) for all the c?s predefined in the do-
main model. Then we discard such c whose
P (d|c) is below a specific threshold. As a result,
for the input term, related domains and descrip-
tions are simultaneously selected. Thus, we do
not have to know a priori which domains are re-
lated to each term.
In the following two sections, we explain meth-
ods to realize the domain and description models,
respectively.
5.2 Domain Model
The domain model quantifies the extent to which
description d is associated with domain c, which
is fundamentally a categorization task.
Among a number of existing categorization
methods, we experimentally used one proposed
by Iwayama and Tokunaga (1994), which formu-
lates P (c|d) as in Equation (2).
P (c|d) = P (c) ?
?
t
P (t|c) ? P (t|d)
P (t) (2)
Here, P (t|d), P (t|c) and P (t) denote probabili-
ties that word t appears in d, c and all the domains,
respectively. We regard P (c) as a constant. While
P (t|d) is simply a relative frequency of t in d, we
need predefined domains to compute P (t|c) and
P (t). For this purpose, the use of large-scale cor-
pora annotated with domains is desirable.
However, since those resources are pro-
hibitively expensive, we used the ?Nova? dic-
tionary for Japanese/English machine translation
systems4, which includes approximately one mil-
lion entries related to 19 technical fields as listed
below:
4Produced by NOVA, Inc.
aeronautics, biotechnology, business,
chemistry, computers, construction, de-
fense, ecology, electricity, energy, fi-
nance, law, mathematics, mechan-
ics, medicine, metals, oceanography,
plants, trade.
We extracted words from dictionary entries
to estimate P (t|c) and P (t). For Japanese en-
tries, we used the ChaSen morphological ana-
lyzer (Matsumoto et al, 1997) to extract words.
We also used English entries because Japanese
descriptions often contain English words.
It may be argued that statistics extracted from
dictionaries are unreliable, because word frequen-
cies in real word usage are missing. However,
words that are representative for a domain tend
to be frequently used in compound word entries
associated with the domain, and thus our method
is a practical approximation.
5.3 Description Model
The description model quantifies the extent to
which a given page fragment is feasible as a de-
scription for the input term. In principle, we de-
compose the description model into language and
quality properties, as shown in Equation (3).
P (d) = P
L
(d) ? P
Q
(d) (3)
Here, P
L
(d) and P
Q
(d) denote language and
quality models, respectively.
It is expected that the quality model discards
incorrect or misleading information contained in
Web pages. For this purpose, a number of qual-
ity rating methods for Web pages (Amento et al,
2000; Zhu and Gauch, 2000) can be used.
However, since Google (i.e., the search engine
we used in the retrieval module) rates the quality
of pages based on hyperlink information, and se-
lectively retrieves those with higher quality (Brin
and Page, 1998), we tentatively regarded P
Q
(d)
as a constant. Thus, in practice the description
model is approximated solely with the language
model as in Equation (4).
P (d) ? P
L
(d) (4)
Statistical approaches to language modeling
have been used in much NLP research, such
as machine translation (Brown et al, 1993) and
speech recognition (Bahl et al, 1983). Our lan-
guage model is almost the same as existing mod-
els, but is different in two respects.
First, while general language models quantify
the extent to which a given word sequence is lin-
guistically acceptable, our model also quantifies
the extent to which the input is acceptable as a
term description. Thus, we trained the model
based on an existing machine readable encyclo-
pedia.
We used the ChaSen morphological analyzer
to segment the Japanese CD-ROM World Ency-
clopedia (Heibonsha, 1998) into words (we re-
placed headwords with a common symbol), and
then used the CMU-Cambridge toolkit (Clark-
son and Rosenfeld, 1997) to model a word-based
trigram. Consequently, descriptions in which
word sequences are more similar to those in the
World Encyclopedia are assigned greater proba-
bility scores through our language model.
Second, P (d), which is generally a product
of probabilities for N -grams in d, is quite sen-
sitive to the length of d. In the cases of machine
translation and speech recognition, this problem
is less crucial because multiple candidates com-
pared based on the language model are almost
equivalent in terms of length. For example, in the
case of machine translation, candidates are trans-
lations for a single input, which are usually com-
parable with respect to length.
However, since in our case length of descrip-
tions are significantly different, shorter descrip-
tions are more likely to be selected, regardless of
the quality. To avoid this problem, we normalize
P (d) by the number of words contained in d.
6 Experimentation
6.1 Methodology
We evaluated the performance of our question an-
swering system, for which we used as test in-
puts 40 technical term questions collected from
the Class II examination (the autumn of 1999).
First, we generated an encyclopedia including
96 terms that are associated with those 40 ques-
tions. For all the 96 test terms, Google retrieved a
positive number of pages, and the average num-
ber of pages for one term was 196,503. Since
Google practically outputs contents of the top
1,000 pages, the remaining pages were not used
in our experiments.
For each test term, we computed P (d|c) us-
ing Equation (1) and discarded domains whose
P (d|c) was below 0.05. Then, for each remain-
ing domain, the top three descriptions with higher
P (d|c) values were selected as the final outputs,
because a preliminary experiment showed that a
correct description was generally found in the top
three candidates.
In addition, to estimate a baseline perfor-
mance, we used the ?Nichigai? computer dictio-
nary (Nichigai Associates, 1996). This dictio-
nary lists approximately 30,000 Japanese techni-
cal terms related to the computer field, and con-
tains descriptions for 13,588 terms. In this dictio-
nary 42 out of 96 test terms were described.
We compared the following three different re-
sources as a knowledge base:
? the Nichigai dictionary (?Nichigai?),
? the descriptions generated in the first experi-
ment (?Web?),
? combination of both resources (?Nichigai +
Web?).
6.2 Results
Table 1 shows the result of our comparative ex-
periment, in which ?C? and ?A? denote coverage
and accuracy, respectively, for variations of our
QA system.
Since all the questions we used are quadruple-
choice, in case the system cannot answer the
question, random choice can be performed to im-
prove the coverage to 100%.
Thus, for each knowledge resource we com-
pared cases without/with random choice, which
are denoted ?w/o Random? and ?w/ Random? in
Table 1, respectively.
Table 1: Coverage and accuracy (%) for different
question answering methods.
w/o Random w/ Random
Resource C A C A
Nichigai 50.0 65.0 100 45.0
Web 92.5 48.6 100 46.9
Nichigai + Web 95.0 63.2 100 61.3
In the case where random choice was not per-
formed, the Web-based encyclopedia noticeably
improved the coverage for the Nichigai dictio-
nary, but decreased the accuracy. However, by
combining both resources, the accuracy was no-
ticeably improved, and the coverage was compa-
rable with that for the Nichigai dictionary.
On the other hand, in the case where random
choice was performed, the Nichigai dictionary
and the Web-based encyclopedia were compara-
ble in terms of both the coverage and accuracy.
Additionally, by combining both resources, the
accuracy was further improved.
We also investigated the performance of our
QA system where descriptions related to the com-
puter domain are solely used. For example, the
description of ?pipeline (transportation pipe)? is
in principle irrelevant or misleading to answer
questions associated with ?pipeline (processing
method).?
However, coverage/accuracy did not change,
because approximately one third of the resultant
descriptions were inherently related to the com-
puter domain, and thus those related to minor do-
mains did not affect the result.
7 Conclusion
In this paper, we proposed a question answering
system which uses an encyclopedia as a knowl-
edge base. For this purpose, we reformalized
our Web-based extraction method, and proposed
a new statistical organization model to improve
the quality of extracted data.
Given a term for which encyclopedic knowl-
edge (i.e., descriptions) is to be generated, our
method sequentially performs a) retrieval of Web
pages containing the term, b) extraction of page
fragments describing the term, and c) organiz-
ing extracted descriptions based on domains (and
consequently word senses).
For the purpose of evaluation, we used as test
questions the Japanese Information-Technology
Engineers Examination, and found that our Web-
based encyclopedia was comparable with an ex-
isting dictionary in terms of the application to
question answering. In addition, by using the both
resources the performance of question answering
was further improved.
Acknowledgments
The authors would like to thank NOVA, Inc.
for their support with the Nova dictionary and
Katunobu Itou (The National Institute of Ad-
vanced Industrial Science and Technology, Japan)
for his insightful comments on this paper.
References
Brian Amento, Loren Terveen, and Will Hill. 2000.
Does ?authority? mean quality? predicting expert
quality ratings of Web documents. In Proceedings
of the 23rd Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 296?303.
Lalit. R. Bahl, Frederick Jelinek, and Robert L. Mer-
cer. 1983. A maximum linklihood approach to
continuous speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
5(2):179?190.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1?7):107?117.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statisti-
cal language modeling using the CMU-Cambridge
toolkit. In Proceedings of EuroSpeech?97, pages
2707?2710.
Atsushi Fujii and Tetsuya Ishikawa. 2000. Utilizing
the World Wide Web as an encyclopedia: Extract-
ing term descriptions from semi-structured texts.
In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, pages
488?495.
Sanda M. Harabagiu, Marius A. Pas?ca, and Steven J.
Maiorano. 2000. Experiments with open-domain
textual question answering. In Proceedings of the
18th International Conference on Computational
Linguistics, pages 292?298.
Hitachi Digital Heibonsha. 1998. CD-ROM World
Encyclopedia. (In Japanese).
Makoto Iwayama and Takenobu Tokunaga. 1994. A
probabilistic model for text categorization: Based
on a single random variable with multiple values.
In Proceedings of the 4th Conference on Applied
Natural Language Processing, pages 162?167.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki
Imamura. 1997. Japanese morphological analysis
system ChaSen manual. Technical Report NAIST-
IS-TR97007, NAIST. (In Japanese).
Dan Moldovan and Sanda Harabagiu. 2000. The
structure and performance of an open-domain ques-
tion answering system. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 563?570.
Nichigai Associates. 1996. English-Japanese com-
puter terminology dictionary. (In Japanese).
John Prager, Eric Brown, and Anni Coden. 2000.
Question-answering by predictive annotation. In
Proceedings of the 23rd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 184?191.
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 232?241.
Hinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building
a question answering test collection. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 200?207.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 189?
196.
Xiaolan Zhu and Susan Gauch. 2000. Incorporating
quality metrics in centralized/distributed informa-
tion retrieval on the World Wide Web. In Proceed-
ings of the 23rd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 288?295.
A Method for Open-Vocabulary Speech-Driven Text Retrieval
Atsushi Fujii  
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
fujii@ulis.ac.jp
Katunobu Itou
National Institute of
Advanced Industrial
Science and Technology
1-1-1 Chuuou Daini Umezono
Tsukuba, 305-8568, Japan
itou@ni.aist.go.jp
Tetsuya Ishikawa
University of Library and
Information Science
1-2 Kasuga, Tsukuba
305-8550, Japan
ishikawa@ulis.ac.jp
Abstract
While recent retrieval techniques do not
limit the number of index terms, out-of-
vocabulary (OOV) words are crucial in
speech recognition. Aiming at retrieving
information with spoken queries, we fill
the gap between speech recognition and
text retrieval in terms of the vocabulary
size. Given a spoken query, we gener-
ate a transcription and detect OOV words
through speech recognition. We then cor-
respond detected OOV words to terms in-
dexed in a target collection to complete the
transcription, and search the collection for
documents relevant to the completed tran-
scription. We show the effectiveness of
our method by way of experiments.
1 Introduction
Automatic speech recognition, which decodes hu-
man voice to generate transcriptions, has of late
become a practical technology. It is feasible that
speech recognition is used in real-world human lan-
guage applications, such as information retrieval.
Initiated partially by TREC-6, various methods
have been proposed for ?spoken document retrieval
(SDR),? in which written queries are used to search
speech archives for relevant information (Garo-
folo et al, 1997). State-of-the-art SDR methods,
where speech recognition error rate is 20-30%, are

The first and second authors are also members of CREST,
Japan Science and Technology Corporation.
comparable with text retrieval methods in perfor-
mance (Jourlin et al, 2000), and thus are already
practical. Possible rationales include that recogni-
tion errors are overshadowed by a large number of
words correctly transcribed in target documents.
However, ?speech-driven retrieval,? where spo-
ken queries are used to retrieve (textual) informa-
tion, has not fully been explored, although it is re-
lated to numerous keyboard-less applications, such
as telephone-based retrieval, car navigation systems,
and user-friendly interfaces.
Unlike spoken document retrieval, speech-driven
retrieval is still a challenging task, because recogni-
tion errors in short queries considerably decrease re-
trieval accuracy. A number of references addressing
this issue can be found in past research literature.
Barnett et al (1997) and Crestani (2000) indepen-
dently performed comparative experiments related
to speech-driven retrieval, where the DRAGON
speech recognition system was used as an input in-
terface for the INQUERY text retrieval system. They
used as test queries 35 topics in the TREC col-
lection, dictated by a single male speaker. How-
ever, these cases focused on improving text retrieval
methods and did not address problems in improv-
ing speech recognition. As a result, errors in recog-
nizing spoken queries (error rate was approximately
30%) considerably decreased the retrieval accuracy.
Although we showed that the use of target docu-
ment collections in producing language models for
speech recognition significantly improved the per-
formance of speech-driven retrieval (Fujii et al,
2002; Itou et al, 2001), a number of issues still re-
main open questions.
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 188-195.
                         Proceedings of the Conference on Empirical Methods in Natural
Section 2 clarifies problems addressed in this pa-
per. Section 3 overviews our speech-driven text
retrieval system. Sections 4-6 elaborate on our
methodology. Section 7 describes comparative ex-
periments, in which an existing IR test collection
was used to evaluate the effectiveness of our method.
Section 8 discusses related research literature.
2 Problem Statement
One major problem in speech-driven retrieval is re-
lated to out-of-vocabulary (OOV) words.
On the one hand, recent IR systems do not limit
the vocabulary size (i.e., the number of index terms),
and can be seen as open-vocabulary systems, which
allow users to input any keywords contained in a tar-
get collection. It is often the case that a couple of
million terms are indexed for a single IR system.
On the other hand, state-of-the-art speech recog-
nition systems still need to limit the vocabulary size
(i.e., the number of words in a dictionary), due
to problems in estimating statistical language mod-
els (Young, 1996) and constraints associated with
hardware, such as memories. In addition, compu-
tation time is crucial for a real-time usage, including
speech-driven retrieval. In view of these problems,
for many languages the vocabulary size is limited to
a couple of ten thousands (Itou et al, 1999; Paul and
Baker, 1992; Steeneken and van Leeuwen, 1995),
which is incomparably smaller than the size of in-
dexes for practical IR systems.
In addition, high-frequency words, such as func-
tional words and common nouns, are usually in-
cluded in dictionaries and recognized with a high
accuracy. However, those words are not necessarily
useful for retrieval. On the contrary, low-frequency
words appearing in specific documents are often ef-
fective query terms.
To sum up, the OOV problem is inherent in
speech-driven retrieval, and we need to fill the gap
between speech recognition and text retrieval in
terms of the vocabulary size. In this paper, we pro-
pose a method to resolve this problem aiming at
open-vocabulary speech-driven retrieval.
3 System Overview
Figure 1 depicts the overall design of our speech-
driven text retrieval system, which consists of
speech recognition, text retrieval and query com-
pletion modules. Although our system is cur-
rently implemented for Japanese, our methodology
is language-independent. We explain the retrieval
process based on this figure.
Given a query spoken by a user, the speech
recognition module uses a dictionary and acous-
tic/language models to generate a transcription of
the user speech. During this process, OOV words,
which are not listed in the dictionary, are also de-
tected. For this purpose, our language model in-
cludes both words and syllables so that OOV words
are transcribed as sequences of syllables.
For example, in the case where ?kankitsu (cit-
rus)? is not listed in the dictionary, this word
should be transcribed as /ka N ki tsu/. How-
ever, it is possible that this word is mistak-
enly transcribed, such as /ka N ke tsu/ and
/ka N ke tsu ke ko/.
To improve the quality of our system, these sylla-
ble sequences have to be transcribed as words, which
is one of the central issues in this paper. In the case
of speech-driven retrieval, where users usually have
specific information needs, it is feasible that users
utter contents related to a target collection. In other
words, there is a great possibility that detected OOV
words can be identified as index terms that are pho-
netically identical or similar.
However, since a) a single sound can potentially
correspond to more than one word (i.e., homonyms)
and b) searching the entire collection for phoneti-
cally identical/similar terms is prohibitive, we need
an efficient disambiguation method. Specifically, in
the case of Japanese, the homonym problem is mul-
tiply crucial because words consist of different char-
acter types, i.e., ?kanji,? ?katakana,? ?hiragana,? al-
phabets and other characters like numerals1.
To resolve this problem, we use a two-stage re-
trieval method. In the first stage, we delete OOV
words from the transcription, and perform text re-
trieval using remaining words, to obtain a specific
number of top-ranked documents according to the
degree of relevance. Even if speech recognition is
not perfect, these documents are potentially associ-
ated with the user speech more than the entire col-
1In Japanese, kanji (or Chinese character) is the idiogram,
and katakana and hiragana are phonograms.
lection. Thus, we search only these documents for
index terms corresponding to detected OOV words.
Then, in the second stage, we replace detected
OOV words with identified index terms so as to
complete the transcription, and re-perform text re-
trieval to obtain final outputs. However, we do not
re-perform speech recognition in the second stage.
In the above example, let us assume that the user
also utters words related to ?kankitsu (citrus),? such
as ?orenji (orange)? and ?remon (lemon),? and that
these words are correctly recognized as words. In
this case, it is possible that retrieved documents
contain the word ?kankitsu (citrus).? Thus, we re-
place the syllable sequence /ka N ke tsu/ in the
query with ?kankitsu,? which is additionally used as
a query term in the second stage.
It may be argued that our method resembles the
notion of pseudo-relevance feedback (or local feed-
back) for IR, where documents obtained in the first
stage are used to expand query terms, and final out-
puts are refined in the second stage (Kwok and Chan,
1998). However, while relevance feedback is used to
improve only the retrieval accuracy, our method im-
proves the speech recognition and retrieval accuracy.
Dictionary
Text retrieval Collection
Acoustic
model
Language
model
Speech recognition
user speech
transcription
top-ranked documents
Query completion
completed
transcription
Figure 1: The overall design of our speech-driven
text retrieval system.
4 Speech Recognition
The speech recognition module generates word se-
quence

, given phone sequence  . In a stochastic
speech recognition framework (Bahl et al, 1983),
the task is to select the

maximizing 


	 ,
which is transformed as in Equation (1) through the
Bayesian theorem.





	



 
	ff