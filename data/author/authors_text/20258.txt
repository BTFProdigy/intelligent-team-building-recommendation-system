Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 171?177, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
HLTDI: CL-WSD Using Markov Random Fields for SemEval-2013 Task 10
Alex Rudnick, Can Liu and Michael Gasser
Indiana University, School of Informatics and Computing
{alexr,liucan,gasser}@indiana.edu
Abstract
We present our entries for the SemEval-
2013 cross-language word-sense disambigua-
tion task (Lefever and Hoste, 2013). We
submitted three systems based on classifiers
trained on local context features, with some
elaborations. Our three systems, in increasing
order of complexity, were: maximum entropy
classifiers trained to predict the desired target-
language phrase using only monolingual fea-
tures (we called this system L1); similar clas-
sifiers, but with the desired target-language
phrase for the other four languages as features
(L2); and lastly, networks of five classifiers,
over which we do loopy belief propagation to
solve the classification tasks jointly (MRF).
1 Introduction
In the cross-language word-sense disambiguation
(CL-WSD) task, given an instance of an ambigu-
ous word used in a context, we want to predict the
appropriate translation into some target language.
This setting for WSD has an immediate application
in machine translation, since many words have mul-
tiple possible translations. Framing the resolution of
lexical ambiguities as an explicit classification task
has a long history, and was considered in early SMT
work at IBM (Brown et al, 1991). More recently,
Carpuat and Wu have shown how to use CL-WSD
techniques to improve modern phrase-based SMT
systems (Carpuat and Wu, 2007), even though the
language model and phrase-tables of these systems
mitigate the problem of lexical ambiguities some-
what.
In the SemEval-2013 CL-WSD shared task
(Lefever and Hoste, 2013), entrants are asked to
build a system that can provide translations for
twenty ambiguous English nouns, given appropri-
ate contexts ? here the particular usage of the am-
biguous noun is called the target word. The five tar-
get languages of the shared task are Spanish, Dutch,
German, Italian and French. In the evaluation, for
each of the twenty ambiguous nouns, systems are to
provide translations for the target word in each of
fifty sentences or short passages. The translations
of each English word may be single words or short
phrases in the target language, but in either case,
they should be lemmatized.
Following the work of Lefever and Hoste (2011),
we wanted to make use of multiple bitext corpora
for the CL-WSD task. ParaSense, the system of
Lefever and Hoste, takes into account evidence from
all of the available parallel corpora. Let S be the set
of five target languages and t be the particular target
language of interest at the moment; ParaSense cre-
ates bag-of-words features from the translations of
the target sentence into the languages S?{t}. Given
corpora that are parallel over many languages, this
is straightforward at training time. However, at test-
ing time it requires a complete MT system for each
of the four other languages, which is computation-
ally prohibitive. Thus in our work, we learn from
several parallel corpora but require neither a locally
running MT system nor access to an online transla-
tion API.
We presented three systems in this shared task,
all of which were variations on the theme of a max-
imum entropy classifier for each ambiguous noun,
trained on local context features similar to those
used in previous work and familiar from the WSD
literature. The first system, L1 (?layer one?), uses
maximum entropy classifiers trained on local con-
171
text features. The second system, L2 (?layer two?),
is the same as the L1 system, with the addition
of the correct translations into the other target lan-
guages as features, which at testing time are pre-
dicted with L1 classifiers. The third system, MRF
(?Markov random field?) uses a network of inter-
acting classifiers to solve the classification problem
for all five target languages jointly. Our three sys-
tems are all trained from the same data, which we
extracted from the Europarl Intersection corpus pro-
vided by the shared task organizers.
At the time of the evaluation, our simplest sys-
tem had the top results in the shared task for the
out-of-five evaluation for three languages (Spanish,
German, and Italian). However, after the evaluation
deadline, we fixed a simple bug in our MRF code,
and the MRF system then achieved even better re-
sults for the oof evaluation. For the best evaluation,
our two more sophisticated systems posted better re-
sults than the L1 version. All of our systems beat the
?most-frequent sense? baseline in every case.
In the following sections, we will describe our
three systems1, our training data extraction process,
the results on the shared task, and conclusions and
future work.
2 L1
The ?layer one? classifier, L1, is a maximum en-
tropy classifier that uses only monolingual features
from English. Although this shared task is described
as unsupervised, the L1 classifiers are trained with
supervised learning on instances that we extract pro-
grammatically from the Europarl Intersection cor-
pus; we describe the preprocessing and training data
extraction in Section 5.
Having extracted the relevant training sentences
from the aligned bitext for each of the five lan-
guage pairs, we created training instances with local
context features commonly used in WSD systems.
These are described in Figure 1. Each instance is
assigned the lemma of the translation that was ex-
tracted from the training sentence as its label.
We trained one L1 classifier for each target lan-
guage and each word of interest, resulting in 20?5 =
1Source is available at
http://github.iu.edu/alexr/semeval2013
? target word features
? literal word form
? POS tag
? lemma
? window unigram features (within 3 words)
? word form
? POS tag
? word with POS tag
? word lemma
? window bigram features (within 5 words)
? bigrams
? bigrams with POS tags
Figure 1: Features used in our classifiers
100 classifiers. Classifiers were trained with the
MEGA Model optimization package 2 and its corre-
sponding NLTK interface (Bird et al, 2009). Upon
training, we cache these classifiers with Python
pickles, both to speed up L1 experiments and also
because they are used as components of the other
models.
We combined the word tokens with their tags
in some features so that the classifier would not
treat them independently, since maximum entropy
classifiers learn a single weight for each feature.
Particularly, the ?POS tag? feature is distinct from
the ?word with tag? feature; for the tagged word
?house/NN?, the ?POS tag? feature would be NN ,
and the ?word with tag? feature is house NN .
3 L2
The ?layer two? classifier, L2, is an extension to
the L1 approach, with the addition of multilingual
features. Particularly, L2 makes use of the trans-
lations of the target word into the four target lan-
guages other than the one we are currently trying to
predict. At training time, since we have the transla-
tions of each of the English sentences into the other
target languages, the appropriate features are ex-
tracted from the corresponding sentences in those
languages. This is the same as the process by which
labels are given to training instances, described in
Section 5. At testing time, since translations of the
2http://www.umiacs.umd.edu/?hal/megam/
172
es de
nl
fr it
Figure 2: The network structure used in the MRF
system: a complete graph with five nodes where
each node represents a variable for the translation
into a target language
test sentences are not given, we estimate the transla-
tions for the target word in the four other languages
using the cached L1 classifiers.
Lefever and Hoste (2011) used the Google Trans-
late API to translate the source English sentences
into the four other languages, and extracted bag-of-
words features from these complete sentences. The
L2 classifiers make use of a similar intuition, but
they do not rely on a complete MT system or an
available online MT API; we only include the trans-
lations of the specific target word as features.
4 MRF
Our MRF model builds a Markov network (often
called a ?Markov random field?) of L1 classifiers
in an effort to find the best translation into all five
target languages jointly. This network has nodes
that correspond to the distributions produced by the
L1 classifiers, given an input sentence, and edges
with pairwise potentials that are derived from the
joint probabilities of target-language labels occur-
ring together in the training data. Thus the task of
finding the optimal translations into five languages
jointly is framed as a MAP (Maximum A Posteriori)
inference problem, where we try to maximize the
joint probability P (wfr, wes, wit, wde, wnl), given
the evidence of the features extracted from the
source-language sentence. The inference process is
performed using loopy belief propagation (Murphy
et al, 1999), which is an approximate but tractable
inference algorithm that, while it gives no guaran-
tees, often produces good solutions in practice.
The intuition behind using a Markov network for
this task is that, since we must make five decisions
for each source-language sentence, we should make
use of the correlations between the target-language
words. Correlations might occur in practice due to
cognates ? the languages in the shared task are fairly
closely related ? or they may simply reflect ambigu-
ities in the source language that are resolved in two
target languages.
So by building a Markov network in which all of
the classifiers can communicate (see Figure 2), we
allow nodes to influence the translation decisions of
their neighbors, but only proportionally to the cor-
relation between the translations that we observe in
the two languages.
We frame the MAP inference task as a minimiza-
tion problem; we want to find an assignment that
minimizes the sum of all of our penalty functions,
which we will describe next. First, we have a unary
function from each of the five L1 classifiers, which
correspond to nodes in the network. These func-
tions each assign a penalty to each possible label for
the target word in the corresponding language; that
penalty is simply the negative log of the probability
of the label, as estimated by the classifier.
Formally, a unary potential ?i, for some fixed set
of features f and a particular language i, is a func-
tion from a label l to some positive penalty value.
?i(l) = ?logP (Li = l|F = f)
Secondly, for each unordered pair of classifiers
(i, j) (i.e., each edge in the graph) there is a pairwise
potential function ?(i,j) that assigns a penalty to any
assignment of that pair of variables.
?(i,j)(li, lj) = ?logP (Li = li, Lj = lj)
Here by P (Li = li, Lj = lj), we mean the prob-
ability that, for a fixed ambiguous input word, lan-
guage i takes the label li and language j takes the
label lj . These joint probabilities are estimated from
the training data; we count the number of times
each pair of labels li and lj co-occurs in the train-
173
ing sentences and divide, with smoothing to avoid
zero probabilities and thus infinite penalties.
When it comes time to choose translations, we
want to find a complete assignment to the five vari-
ables that minimizes the sum of all of the penal-
ties assigned by the ? functions. As mentioned ear-
lier, we do this via loopy belief propagation, using
the formulation for pairwise Markov networks that
passes messages directly between the nodes rather
than first constructing a cluster graph (Koller and
Friedman, 2009, ?11.3.5.1).
As we are trying to compute the minimum-
penalty assignment to the five variables, we use the
min-sum version of loopy belief propagation. The
messages are mappings from the possible values
that the recipient node could take to penalty values.
At each time step, every node passes to each of
its neighbors a message of the following form:
?ti?j(Lj) = min
li?Li
[
?i(li) + ?(i,j)(li, lj)
+
?
k?S?{i,j}
?t?1k?i(li)
]
By this expression, we mean that the message
from node i to node j at time t is a function from
possible labels for node j to scalar penalty values.
Each penalty value is determined by minimizing
over the possible labels for node i, such that we find
the label li that minimizes sum of the unary cost for
that label, the binary cost for li and lj taken jointly,
and all of the penalties in the messages that node i
received at the previous time step, except for the one
from node j.
Intuitively, these messages inform a given neigh-
bor about the estimate, from the perspective of the
sending node and what it has heard from its other
neighbors, of the minimum penalty that would be
incurred if the recipient node were to take a given
label. As a concrete example, when the nl node
sends a message to the fr node at time step 10, this
message is a table mapping from all possible French
translations of the current target word to their as-
sociated penalty values. The message depends on
three things: the function ?nl (itself dependent on
the probability distribution output by the L1 classi-
fier), the binary potential function ?(nl,fr), and the
messages from es, it and de from time step 9. Note
that the binary potential functions are symmetric be-
cause they are derived from joint probabilities.
Loopy belief propagation is an approximate infer-
ence algorithm, and it is neither guaranteed to find
a globally optimal solution, nor even to converge
at all, but it does often find good solutions in prac-
tice. We run it for twenty iterations, which empir-
ically works well. After the message-passing iter-
ations, each node chooses the value that minimizes
the sum of the penalties from messages and from its
own unary potential function. To avoid accumulat-
ing very large penalties, we normalize the outgoing
messages at each time step and give a larger weight
to the unary potential functions. These normaliza-
tion and weighting parameters were set by hand, but
seem to work well in practice.
5 Training Data Extraction
For simplicity and comparability with previous
work, we worked with the Europarl Intersection
corpus provided by the task organizers. Europarl
(Koehn, 2005) is a parallel corpus of proceedings of
the European Parliament, currently available in 21
European languages, although not every sentence is
translated into every language. The Europarl Inter-
section is the intersection of the sentences from Eu-
roparl that are available in English and all five of the
target languages for the task.
In order to produce the training data for the classi-
fiers, we first tokenized the text for all six languages
with the default NLTK tokenizer and tagged the En-
glish text with the Stanford Tagger (Toutanova et
al., 2003). We aligned the untagged English with
each of the target languages using the Berkeley
Aligner (DeNero and Klein, 2007) to get one-to-
many alignments from English to target-language
words, since the target-language labels may be
multi-word phrases. We used nearly the default set-
tings for Berkeley Aligner, except that we ran 20
iterations each of IBM Model 1 and HMM align-
ment.
We used TreeTagger (Schmid, 1995) to lemma-
tize the text. At first this caused some confusion in
our pipeline, as TreeTagger by default re-tokenizes
input text and tries to recognize multi-word expres-
174
sions. Both of these, while sensible behaviors, were
unexpected, and resulted in a surprising number of
tokens in the TreeTagger output. Once we turned off
these behaviors, TreeTagger provided useful lem-
mas for all of the languages.
Given the tokenized and aligned sentences, with
their part-of-speech tags and lemmas, we used
a number of heuristics to extract the appropriate
target-language labels for each English-language in-
put sentence. For each target word, we extracted a
sense inventory Vi from the gold standard answers
from the 2010 iteration of this task (Lefever and
Hoste, 2009). Then, for each English sentence that
contains one of the target words used as a noun,
we examine the alignments to determine whether
that word is aligned with a sense present in Vi , or
whether the words aligned to that noun are a sub-
sequence of such a sense. The same check is per-
formed both on the lemmatized and unlemmatized
versions of the target-language sentence. If we do
find a match, then that sense from the gold stan-
dard Vi is taken to be the label for this sentence.
While a gold standard sense inventory will clearly
not be present for general translation systems, there
will be some vocabulary of possible translations for
each word, taken from a bilingual dictionary or the
phrase table in a phrase-based SMT system.
If a label from Vi is not found with the align-
ments, but some other word or phrase is aligned
with the ambiguous noun, then we trust the output
of the aligner, and the lemmatized version of this
target-language phrase is assigned as the label for
this sentence. In this case we used some heuristic
functions to remove stray punctuation and attached
articles (such as d? from French or nell? from Ital-
ian) that were often left appended to the tokens by
the default NLTK English tokenizer.
We dropped all of the training instances with
labels that only occurred once, considering them
likely alignment errors or other noise.
6 Results
There were two settings for the evaluation, best and
oof. In either case, systems may present multiple
possible answers for a given translation, although
in the best setting, the first answer is given more
weight in the evaluation, and the scoring encour-
ages only returning the top answer. In the oof set-
ting, systems are asked to return the top-five most
likely translations. In both settings, the answers are
compared against translations provided by several
human annotators for each test sentence, who pro-
vided a number of possible target-language transla-
tions in lemmatized form, and more points are given
for matching the more popular translations given by
the annotators. In the ?mode? variant of scoring,
only the one most common answer for a given test
sentence is considered valid. For a complete ex-
planation of the evaluation and its scoring, please
see the shared task description (Lefever and Hoste,
2013).
The scores for our systems3 are reported in Figure
3. In all of the settings, our systems posted some of
the top results among entrants in the shared task,
achieving the best scores for some evaluations and
some languages. For every setting and language,
our systems beat the most-frequent sense baseline,
and our best results usually came from either the L2
or MRF system, which suggests that there is some
benefit in using multilingual information from the
parallel corpora, even without translating the whole
source sentence.
For the best evaluation, considering only the
mode gold-standard answers, our L2 system
achieved the highest scores in the competition for
Spanish and German. For the oof evaluation, our
MRF system ? with its post-competition bug fix ?
posted the best results for Spanish, German and Ital-
ian in both complete and mode variants. Also, cu-
riously, our L1 system posted the best results in the
competition for Dutch in the oof variant.
For the best evaluation, our results were lower
than those posted by ParaSense, and in the stan-
dard best setting, they were also lower than those
from the c1lN system (van Gompel and van den
Bosch, 2013) and adapt1 (Carpuat, 2013). This,
combined with the relatively small difference be-
tween our simplest system and the more sophisti-
cated ones, suggests that there are many improve-
ments that could be made to our system; perhaps
3The oof scores for the MRF system reflect a small bug fix
after the competition.
175
system es nl de it fr
MFS 23.23 20.66 17.43 20.21 25.74
best 32.16 23.61 20.82 25.66 30.11
PS 31.72 25.29 24.54 28.15 31.21
L1 29.01 21.53 19.5 24.52 27.01
L2 28.49 22.36 19.92 23.94 28.23
MRF 29.36 21.61 19.76 24.62 27.46
(a) best evaluation results: precision
system es nl de it fr
MFS 53.07 43.59 38.86 42.63 51.36
best 62.21 47.83 44.02 53.98 59.80
L1 61.69 46.55 43.66 53.57 57.76
L2 59.51 46.36 42.32 53.05 58.20
MRF 62.21 46.63 44.02 53.98 57.83
(b) oof evaluation results: precision
system es nl de it fr
MFS 27.48 24.15 15.30 19.88 20.19
best 37.11 27.96 24.74 31.61 26.62
PS 40.26 30.29 25.48 30.11 26.33
L1 36.32 25.39 24.16 26.52 21.24
L2 37.11 25.34 24.74 26.65 21.07
MRF 36.57 25.72 24.01 26.26 21.24
(c) best evaluation results: mode precision
system es nl de it fr
MFS 57.35 41.97 44.35 41.69 47.42
best 65.10 47.34 53.75 57.50 57.57
L1 64.65 47.34 53.50 56.61 51.96
L2 62.52 44.06 49.03 54.06 53.57
MRF 65.10 47.29 53.75 57.50 52.14
(d) oof evaluation results: mode precision
Figure 3: Task results for our systems. Scores in bold are the best result for that language and evaluation
out of our systems, and those in bold italics are the best posted in the competition. For comparison, we
also give scores for the most-frequent-sense baseline (?MFS?), ParaSense (?PS?), the system developed by
Lefever and Hoste, and the best posted score for competing systems this year (?best?).
we could integrate ideas from the other entries in
the shared task this year.
7 Conclusions and future work
Our systems had a strong showing in the compe-
tition, always beating the MFS baseline, achiev-
ing the top score for three of the five languages in
the oof evaluation, and for two languages in the
best evaluation when considering the mode gold-
standard answers. The systems that took into ac-
count evidence from multiple sources had better
performance than the one using monolingual fea-
tures: our top result in every language came from
either the L2 or the MRF classifier for both eval-
uations. This suggests that it is possible to make
use of the evidence in several parallel corpora in a
CL-WSD task without translating every word in a
source sentence into many target languages.
We expect that the L2 classifier could be im-
proved by adding features derived from more classi-
fiers and making use of information from many dis-
parate sources. We would like to try adding classi-
fiers trained on the other Europarl languages, as well
as completely different corpora. The L2 classifier
approach only requires that the first-layer classifiers
make some prediction based on text in the source
language. They need not be trained from the same
source text, depend on the same features, or even
output words as labels. In future work we will ex-
plore all of these variations. One could, for exam-
ple, train a monolingual WSD system on a sense-
tagged corpus and use this as an additional informa-
tion source for an L2 classifier.
There remain a number of avenues that we would
like to explore for the MRF system; thus far, we
have used the joint probability of two labels to set
the binary potentials. We would like to investigate
other functions, especially ones that do not incur
large penalties for rare labels, as the joint probabil-
ity of two labels that often co-occur but are both rare
will be low. Also, in the current system, the relative
weights of the binary potentials and the unary po-
tentials were set by hand, with a very small amount
of empirical tuning. We could, in the future, tune the
176
weights with a more principled optimization strat-
egy, using a development set.
As with the L2 classifiers, it would be helpful in
the future for the MRF system to not require many
mutually parallel corpora for training ? however, the
current approach for estimating the edge potentials
requires the use of bitext for each edge in the net-
work. Perhaps these correlations could be estimated
in a semi-supervised way, with high-confidence au-
tomatic labels being used to estimate the joint dis-
tribution over target-language phrases. We would
also like to investigate approaches to jointly disam-
biguate many words in the same sentence, since lex-
ical ambiguity is not just a problem for a few nouns.
Aside from improvements to the design of our
CL-WSD system itself, we want to use it in a practi-
cal system for translating into under-resourced lan-
guages. We are now working on integrating this
project with our rule-based MT system, L3 (Gasser,
2012). We had experimented with a similar, though
less sophisticated, CL-WSD system for Quechua
(Rudnick, 2011), but in the future, L3 with the inte-
grated CL-WSD system should be capable of trans-
lating Spanish to Guarani, either as a standalone
system, or as part of a computer-assisted translation
tool.
References
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-Sense Dis-
ambiguation Using Statistical Methods. In Proceed-
ings of the 29th Annual Meeting of the Association for
Computational Linguistics, pages 264?270.
Marine Carpuat and Dekai Wu. 2007. How Phrase
Sense Disambiguation Outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation.
Marine Carpuat. 2013. NRC: A Machine Translation
Approach to Cross-Lingual Word Sense Disambigua-
tion (SemEval-2013 Task 10). In Proceedings of the
7th International Workshop on Semantic Evaluation
(SemEval 2013), Atlanta, USA.
John DeNero and Dan Klein. 2007. Tailoring Word
Alignments to Syntactic Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Michael Gasser. 2012. Toward a Rule-Based Sys-
tem for English-Amharic Translation. In LREC-2012:
SALTMIL-AfLaT Workshop on Language technology
for normalisation of less-resourced languages.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
The Tenth Machine Translation Summit, Phuket, Thai-
land.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
Els Lefever and Ve?ronique Hoste. 2009. SemEval-2010
Task 3: Cross-lingual Word Sense Disambiguation.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 82?87, Boulder, Colorado, June.
Association for Computational Linguistics.
Els Lefever and Ve?ronique Hoste. 2013. SemEval-2013
Task 10: Cross-Lingual Word Sense Disambiguation.
In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), Atlanta, USA.
Els Lefever, Ve?ronique Hoste, and Martine De Cock.
2011. ParaSense or How to Use Parallel Corpora for
Word Sense Disambiguation. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 317?322, Portland, Oregon, USA, June. Asso-
ciation for Computational Linguistics.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan.
1999. Loopy Belief Propagation for Approximate In-
ference: An Empirical Study. In UAI ?99: Proceed-
ings of the Fifteenth Conference on Uncertainty in Ar-
tificial Intelligence, Stockholm, Sweden.
Alex Rudnick. 2011. Towards Cross-Language Word
Sense Disambiguation for Quechua. In Proceedings
of the Second Student Research Workshop associated
with RANLP 2011, pages 133?138, Hissar, Bulgaria,
September. RANLP 2011 Organising Committee.
Helmut Schmid. 1995. Improvements In Part-of-Speech
Tagging With an Application To German. In Proceed-
ings of the ACL SIGDAT-Workshop, pages 47?50.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
In PROCEEDINGS OF HLT-NAACL, pages 252?259.
Maarten van Gompel and Antal van den Bosch. 2013.
WSD2: Parameter optimisation for Memory-based
Cross-Lingual Word-Sense Disambiguation. In Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation (SemEval 2013), Atlanta, USA.
177
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 356?360,
Dublin, Ireland, August 23-24, 2014.
IUCL: Combining Information Sources for SemEval Task 5
Alex Rudnick, Levi King, Can Liu, Markus Dickinson, Sandra K?ubler
Indiana University
Bloomington, IN, USA
{alexr,leviking,liucan,md7,skuebler}@indiana.edu
Abstract
We describe the Indiana University sys-
tem for SemEval Task 5, the L2 writ-
ing assistant task, as well as some exten-
sions to the system that were completed
after the main evaluation. Our team sub-
mitted translations for all four language
pairs in the evaluation, yielding the top
scores for English-German. The system
is based on combining several information
sources to arrive at a final L2 translation
for a given L1 text fragment, incorporating
phrase tables extracted from bitexts, an L2
language model, a multilingual dictionary,
and dependency-based collocational mod-
els derived from large samples of target-
language text.
1 Introduction
In the L2 writing assistant task, we must translate
an L1 fragment in the midst of an existing, nearly
complete, L2 sentence. With the presence of this
rich target-language context, the task is rather dif-
ferent from a standard machine translation setting,
and our goal with our design was to make effec-
tive use of the L2 context, exploiting collocational
relationships between tokens anywhere in the L2
context and the proposed fragment translations.
Our system proceeds in several stages: (1) look-
ing up or constructing candidate translations for
the L1 fragment, (2) scoring candidate transla-
tions via a language model of the L2, (3) scoring
candidate translations with a dependency-driven
word similarity measure (Lin, 1998) (which we
call SIM), and (4) combining the previous scores
in a log-linear model to arrive at a final n-best
list. Step 1 models transfer knowledge between
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
the L1 and L2; step 2 models facts about the L2
syntax, i.e., which translations fit well into the lo-
cal context; step 3 models collocational and se-
mantic tendencies of the L2; and step 4 gives dif-
ferent weights to each of the three sources of in-
formation. Although we did not finish step 3 in
time for the official results, we discuss it here, as
it represents the most novel aspect of the system ?
namely, steps towards the exploitation of the rich
L2 context. In general, our approach is language-
independent, with accuracy varying due to the size
of data sources and quality of input technology
(e.g., syntactic parse accuracy). More features
could easily be added to the log-linear model, and
further explorations of ways to make use of target-
language knowledge could be promising.
2 Data Sources
The data sources serve two major purposes for our
system: For L2 candidate generation, we use Eu-
roparl and BabelNet; and for candidate ranking
based on L2 context, we use Wikipedia and the
Google Books Syntactic N-grams.
Europarl The Europarl Parallel Corpus (Eu-
roparl, v7) (Koehn, 2005) is a corpus of pro-
ceedings of the European Parliament, contain-
ing 21 European languages with sentence align-
ments. From this corpus, we build phrase tables
for English-Spanish, English-German, French-
English, Dutch-English.
BabelNet In the cases where the constructed
phrase tables do not contain a translation for a
source phrase, we need to back off to smaller
phrases and find candidate translations for these
components. To better handle sparsity, we extend
look-up using the multilingual dictionary Babel-
Net, v2.0 (Navigli and Ponzetto, 2012) as a way to
find translation candidates.
356
Wikipedia For German and Spanish, we use re-
cent Wikipedia dumps, which were converted to
plain text with the Wikipedia Extractor tool.
1
To
save time during parsing, sentences longer than 25
words are removed. The remaining sentences are
POS-tagged and dependency parsed using Mate
Parser with its pre-trained models (Bohnet, 2010;
Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013).
To keep our English Wikipedia dataset to a man-
ageable size, we choose an older (2006), smaller
dump. Long sentences are removed, and the re-
maining sentences are POS-tagged and depen-
dency parsed using the pre-trained Stanford Parser
(Klein and Manning, 2003; de Marneffe et al.,
2006). The resulting sizes of the datasets are
(roughly): German: 389M words, 28M sentences;
Spanish: 147M words, 12M sentences; English:
253M words, 15M sentences. Dependencies ex-
tracted from these parsed datasets serve as training
for the SIM system described in section 3.3.
Google Books Syntactic N-grams For English,
we also obtained dependency relationships for our
word similarity statistics using the arcs dataset of
the Google Books Syntactic N-Grams (Goldberg
and Orwant, 2013), which has 919M items, each
of which is a small ?syntactic n-gram?, a term
Goldberg and Orwant use to describe short de-
pendency chains, each of which may contain sev-
eral tokens. This data set does not contain the ac-
tual parses of books from the Google Books cor-
pus, but counts of these dependency chains. We
converted the longer chains into their component
(head, dependent, label) triples and then collated
these triples into counts, also for use in the SIM
system.
3 System Design
As previously mentioned, at run-time, our system
decomposes the fragment translation task into two
parts: generating many possible candidate transla-
tions, then scoring and ranking them in the target-
language context.
3.1 Constructing Candidate Translations
As a starting point, we use phrase tables con-
structed in typical SMT fashion, built with the
training scripts packaged with Moses (Koehn et
al., 2007). These scripts preprocess the bitext, es-
timate word alignments with GIZA++ (Och and
1
http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
Ney, 2000) and then extract phrases with the
grow-diag-final-and heuristic.
At translation time, we look for the given
source-language phrase in the phrase table, and if
it is found, we take all translations of that phrase
as our candidates.
When translating a phrase that is not found in
the phrase table, we try to construct a ?synthetic
phrase? out of the available components. This
is done by listing, combinatorially, all ways to
decompose the L1 phrase into sub-phrases of at
least one token long. Then for each decomposi-
tion of the input phrase, such that all of its compo-
nents can be found in the phrase table, we gen-
erate a translation by concatenating their target-
language sides. This approach naively assumes
that generating valid L2 text requires no reorder-
ing of the components. Also, since there are 2
n?1
possible ways to split an n-token phrase into sub-
sequences (i.e., each token is either the first token
in a new sub-sequence, or it is not), we perform
some heuristic pruning at this step, taking only
the first 100 decompositions, preferring those built
from longer phrase-table entries. Every phrase in
the phrase table, including these synthetic phrases,
has both a ?direct? and ?inverse? probability score;
for synthetic phrases, we estimate these scores by
taking the product of the corresponding probabili-
ties for the individual components.
In the case that an individual word cannot be
found in the phrase table, the system attempts to
look up the word in BabelNet, estimating the prob-
abilities as uniformly distributed over the available
BabelNet entries. Thus, synthetic phrase table
entries can be constructed by combining phrases
found in the training data and words available in
BabelNet.
For the evaluation, in cases where an L1 phrase
contained words that were neither in our train-
ing data nor BabelNet (and thus were simply out-
of-vocabulary for our system), we took the first
translation for that phrase, without regard to con-
text, from Google Translate, through the semi-
automated Google Docs interface. This approach
is not particularly scalable or reproducible, but
simulates what a user might do in such a situation.
3.2 Scoring Candidate Translations via a L2
Language Model
To model how well a phrase fits into the L2 con-
text, we score candidates with an n-gram lan-
357
guage model (LM) trained on a large sample of
target-language text. Constructing and querying
a large language model is potentially computa-
tionally expensive, so here we use the KenLM
Language Model Toolkit and its Python interface
(Heafield, 2011). Here our models were trained
on the Wikipedia text mentioned previously (with-
out filtering long sentences), with KenLM set to
5-grams and the default settings.
3.3 Scoring Candidate Translations via
Dependency-Based Word Similarity
The candidate ranking based on the n-gram lan-
guage model ? while quite useful ? is based on
very shallow information. We can also rank the
candidate phrases based on how well each of the
components fits into the L2 context using syntactic
information. In this case, the fitness is measured in
terms of dependency-based word similarity com-
puted from dependency triples consisting of the
the head, the dependent, and the dependency la-
bel. We slightly adapted the word similarity mea-
sure by Lin (1998):
SIM(w
1
, w
2
) =
2 ? c(h, d, l)
c(h,?, l) + c(?, d, l)
(1)
where h = w
1
and d = w
2
and c(h, d, l)
is the frequency with which a particular
(head, dependent, label) dependency triple
occurs in the L2 corpus. c(h,?, l) is the fre-
quency with which a word occurs as a head
in a dependency labeled l with any dependent.
c(?, d, l) is the frequency with which a word
occurs as a dependent in a dependency labeled
l with any head. In the measure by Lin (1998),
the numerator is defined as the information of all
dependency features that w
1
and w
2
share, com-
puted as the negative sum of the log probability of
each dependency feature. Similarly, the denom-
inator is computed as the sum of information of
dependency features for w
1
and w
2
.
To compute the fitness of a word w
i
for its
context, we consider a set D of all words that are
directly dependency-related to w
i
. The fitness of
w
i
is thus computed as:
FIT (w
i
) =
?
D
w
j
SIM(w
i
, w
j
)
|D|
(2)
The fitness of a phrase is the average word sim-
ilarity over all its components. For example, the
fitness of the phrase ?eat with chopsticks? would
be computed as:
FIT (eat with chopsticks) =
FIT (eat) + FIT (with) + FIT (chopsticks)
3
(3)
Since we consider the heads and dependents
of a target phrase component, these may be situ-
ated inside or outside the phrase. Both cases are
included in our calculation, thus enabling us to
consider a broader, syntactically determined local
context of the phrase. By basing the calculation on
a single word?s head and dependents, we attempt
to avoid data sparseness issues that we might get
from rare n-gram contexts.
Back-Off Lexical-based dependency triples suf-
fer from data sparsity, so in addition to computing
the lexical fitness of a phrase, we also calculate the
POS fitness. For example, the POS fitness of ?eat
with chopsticks? would be computed as follows:
FIT (eat/VBG with/IN chopsticks/NNS) =
FIT (VBG) + FIT (IN) + FIT (NNS)
3
(4)
Storing and Caching The large vocabulary
and huge number of combinations of our
(head, dependent, label) triples poses an effi-
ciency problem when querying the dependency-
based word similarity values. Thus, we stored
the dependency triples in a database with a
Python programming interface (SQLite3) and
built database indices on the frequent query types.
However, for frequently searched dependency
triples, re-querying the database is still inefficient.
Thus, we built a query cache to store the recently-
queried triples. Using the database and cache sig-
nificantly speeds up our system.
This database only stores dependency triples
and their corresponding counts; the dependency-
based similarity value is calculated as needed, for
each particular context. Then, these FIT scores
are combined with the scores from the phrase ta-
ble and language model, using weights tuned by
MERT.
358
system acc wordacc oofacc oofwordacc
run2 0.665 0.722 0.806 0.857
SIM 0.647 0.706 0.800 0.852
nb 0.657 0.717 0.834 0.868
Figure 1: Scores on the test set for English-
German; here next-best is CNRC-run1.
system acc wordacc oofacc oofwordacc
run2 0.633 0.72 0.781 0.847
SIM 0.359 0.482 0.462 0.607
best 0.755 0.827 0.920 0.944
Figure 2: Scores on the test set for English-
Spanish; here best is UEdin-run2.
3.4 Tuning Weights with MERT
In order to rank the various candidate translations,
we must combine the different sources of infor-
mation in some way. Here we use a familiar log-
linear model, taking the log of each score ? the di-
rect and inverse translation probabilities, the LM
probability, and the surface and POS SIM scores ?
and producing a weighted sum. Since the original
scores are either probabilities or probability-like
(in the range [0, 1]), their logs are negative num-
bers, and at translation time we return the trans-
lation (or n-best) with the highest (least negative)
score.
This leaves us with the question of how to
set the weights for the log-linear model; in this
work, we use the ZMERT package (Zaidan, 2009),
which implements the MERT optimization algo-
rithm (Och, 2003), iteratively tuning the feature
weights by repeatedly requesting n-best lists from
the system. We used ZMERT with its default
settings, optimizing our system?s BLEU scores
on the provided development set. We chose, for
convenience, BLEU as a stand-in for the word-
level accuracy score, as BLEU scores are maxi-
mized when the system output matches the refer-
ence translations.
4 Experiments
In figures 1-4, we show the scores on this year?s
test set for running the two variations of our sys-
tem: run2, the version without the SIM exten-
sions, which we submitted for the evaluation, and
SIM, with the extensions enabled. For compar-
ison, we also include the best (or for English-
German, next-best) submitted system. We see here
system acc wordacc oofacc oofwordacc
run2 0.545 0.682 0.691 0.800
SIM 0.549 0.687 0.693 0.800
best 0.733 0.824 0.905 0.938
Figure 3: Scores on the test set for French-English;
here best is UEdin-run1.
system acc wordacc oofacc oofwordacc
run2 0.544 0.679 0.634 0.753
SIM 0.540 0.676 0.635 0.753
best 0.575 0.692 0.733 0.811
Figure 4: Scores on the test set for Dutch-English;
here best is UEdin-run1.
that the use of the SIM features did not improve
the performance of the base system, and in the
case of English-Spanish caused significant degra-
dation, which is as of yet unexplained, though we
suspect difficulties parsing the Spanish test set, as
for all of the other language pairs, the effects of
adding SIM features were small.
5 Conclusion
We have described our entry for the initial run-
ning of the ?L2 Writing Assistant? task and ex-
plained some possible extensions to our base log-
linear model system.
In developing the SIM extensions, we faced
some interesting software engineering challenges,
and we can now produce large databases of depen-
dency relationship counts for various languages.
Unfortunately, these extensions have not yet led
to improvements in performance on this particu-
lar task. The databases themselves seem at least
intuitively promising, capturing interesting infor-
mation about common usage patterns of the tar-
get language. Finding a good way to make use
of this information may involve computing some
measure that we have not yet considered, or per-
haps the insights captured by SIM are covered ef-
fectively by the language model.
We look forward to future developments around
this task and associated applications in helping
language learners communicate effectively.
359
References
Bernd Bohnet and Jonas Kuhn. 2012. The best of
both worlds ? A graph-based completion model for
transition-based parsers. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics (EACL), pages
77?87, Avignon, France.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (COLING), pages 89?97, Beijing,
China.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC-06, Genoa, Italy.
Yoav Goldberg and Jon Orwant. 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
pages 241?247, Atlanta, GA.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-2003,
pages 423?430, Sapporo, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, Prague, Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In International Conference on
Machine Learning (ICML), volume 98, pages 296?
304.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 440?447, Hong
Kong.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July.
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23?55.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
360
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 2?11,
Dublin, Ireland, August 24 2014.
Feature Selection for Highly Skewed Sentiment Analysis Tasks
Can Liu
Indiana University
Bloomington, IN, USA
liucan@indiana.edu
Sandra K?ubler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Ning Yu
University of Kentucky
Lexington, KY, USA
ning.yu@uky.edu
Abstract
Sentiment analysis generally uses large feature sets based on a bag-of-words approach, which
results in a situation where individual features are not very informative. In addition, many data
sets tend to be heavily skewed. We approach this combination of challenges by investigating
feature selection in order to reduce the large number of features to those that are discriminative.
We examine the performance of five feature selection methods on two sentiment analysis data
sets from different domains, each with different ratios of class imbalance.
Our finding shows that feature selection is capable of improving the classification accuracy only
in balanced or slightly skewed situations. However, it is difficult to mitigate high skewing ratios.
We also conclude that there does not exist a single method that performs best across data sets and
skewing ratios. However we found that TF ? IDF
2
can help in identifying the minority class
even in highly imbalanced cases.
1 Introduction
In recent years, sentiment analysis has become an important area of research (Pang and Lee, 2008;
Bollen et al., 2011; Liu, 2012). Sentiment analysis is concerned with extracting opinions or emotions
from text, especially user generated web content. Specific tasks include monitoring mood and emotion;
differentiating opinions from facts; detecting positive or negative opinion polarity; determining opinion
strength; and identifying other opinion properties. At this point, two major approaches exists: lexicon
and machine learning based. The lexicon-based approach uses high quality, often manually generated
features. The machine learning-based approach uses automatically generated feature sets, which are from
various sources of evidence (e.g., part-of-speech, n-grams, emoticons) in order to capture the nuances of
sentiment. This means that a large set of features is extracted, out of which only a small subset may be
good indicators for the sentiment.
One major problem associated with sentiment analysis of web content is that for many topics, these
data sets tend to be highly imbalanced. There is a general trend that users are willing to submit positive
reviews, but they are much more hesitant to submit reviews in the medium to low ranges. For example,
for the YouTube data set that we will use, we collected comments for YouTube videos from the comedy
category, along with their ratings. In this data set, more than 3/4 of all ratings consist of the highest rating
of 5. For other types of user generated content, they opposite may be true.
Heavy skewing in data sets is challenging for standard classification algorithms. Therefore, the data
sets generally used for research on sentiment analysis are balanced. Researchers either generate balanced
data sets during data collection, by sampling a certain number of positive and negative reviews, or by se-
lecting a balanced subset for certain experiments. Examples for a balanced data set are the movie review
data set (Pang and Lee, 2004) or the IMDB review data set (Maas et al., 2011). Using a balanced data set
allows researchers to focus on finding robust methods and feature sets for the problem. Particularly, the
movie review data set has been used as a benchmark data set that allows for comparisons of various sen-
timent analysis models. For example, Agarwal and Mittal (2012), Agarwal and Mittal (2013), Kummer
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2
and Savoy (2012), O?Keefe and Koprinska (2009), and Paltoglou and Thelwall (2010) all proposed com-
petitive feature selection methods evaluated on the movie review data set. However, the generalizability
of such feature selection methods to imbalanced data sets, which better represent real world situations,
has not been investigated in much detail. Forman (2003) provides an extensive study of feature selection
methods for highly imbalanced data sets, but he uses document classification as task.
This current paper investigates the robustness of three feature selection methods that Forman (2003)
has shown to be successful, as well as two variants of TF ? IDF . The three methods are Odds-Ratio
(OR), Information Gain (IG), and Binormal Separation (BNS). BNS has been found to perform signif-
icantly better than other methods in more highly skewed tasks (Forman, 2003). The two variants of
TF ? IDF differ in the data set used for calculating document frequency. We investigate the behavior
of these methods on a subtask of sentiment analysis, namely the prediction of user ratings. For this, we
will use data sets from two different domains in order to gain insight into whether or not these feature
selection methods are robust across domains and across skewing ratios: One set consists of user reviews
from Epicurious
1
, an online community where recipes can be exchanged and reviewed, the other set
consists of user reviews of YouTube comedy videos.
The remainder of this paper is organized as follows: In section 2, we explain the rationale for applying
feature selection and introduce the feature selection methods that are examined in this paper. Section
3 introduces the experimental settings, including a description of the two data sets, data preprocessing,
feature representation, and definition of the binary classification tasks. In section 4, we present and
discuss the results for the feature selection methods, and in section 5, we conclude.
2 Feature Selection and Class Skewing
In a larger picture, feature selection is a method (applicable both in regression and classification prob-
lems) to identify a subset of features to achieve various goals: 1) to reduce computational cost, 2) to
avoid overfitting, 3) to avoid model failure, and 4) to handle skewed data sets for classification tasks.
We concentrate on the last motivation, even though an improvement of efficiency and the reduction of
overfitting are welcome side effects. The feature selection methods studied in this paper have been used
in text classification as well, which is a more general but similar task using n-gram features. However,
since all measures are intended for binary classification problems, we reformulate the rating prediction
into a binary classification problem (see section 3.5).
Feature selection methods can be divided into wrapper and filter methods. Wrapper methods use
the classification outcome on a held-out data set to score feature subsets. Standard wrapper methods
include forward selection, backward selection, and genetic algorithms. Filter methods, in contrast, use
an independent measure rather than the error rate on the held-out data. This means that they can be
applied to larger feature sets, which may be unfeasible with wrapper methods. Since sentiment analysis
often deals with high dimensional feature representation, we will concentrate on filter methods for our
feature selection experiments.
Previous research (e.g. (Brank et al., 2002b; Forman, 2003)) has shown that Information Gain and
Odds Ratio have been used successfully across different tasks and that Binormal Separation has good
recall for the minority class under skewed class distributions. So we will investigate them in this paper.
Other filter methods are not investigated in this paper due to two main concerns: We exclude Chi-
squared and Z-score, statistical tests because they require a certain sample size. Our concern is that
their estimation for rare words may not be accurate. We also exclude Categorical Proportion Difference
and Probability Proportion Difference since they do not normalize over the sample of size of positive
and negative classes. Thus, our concern is that they may not provide a fair estimate for features from a
skewed data sets.
2.1 Notation
Following Zheng et al. (2004), feature selection methods can be divided into two groups: one-sided and
two-sided measures. One-sided measures assign a high score to positively-correlated features and a low
1
www.epicurious.com
3
score to negative features while two-sided measures prefer highly distinguishing features, independent
of whether they are positively or negatively correlated. Zheng et al. (2004) note that the ratio of positive
and negative features affects precision and recall of the classification, especially for the minority class.
For one-sided methods, we have control over this ratio by selecting a specified number of features one
each side; for two-sided methods, however, we do not have this control. In this paper, we will keep a 1:1
ratio for one-sided methods. For example, if we select 1 000 features, we select the 500 highest ranked
features for the positive class, and the 500 highest ranked features for the negative class. When using
two-sided methods, the 1 000 highest ranked features are selected.
For the discussion of the feature selection methods, we use the following notations:
? S: target or positive class.
? S: negative class.
? D
S
: The number of documents in class S.
? D
S
: The number of documents in class S.
? D
Sf
: The number of documents in class S where feature f occurs.
? D
Sf
: The number of documents in class S where feature f occurs.
? T
Sf
: The number of times feature f occurs in class S.
2.2 Feature Selection Methods
In addition to Information Gain, Odds Ratio and Bi-Normal Separation, TF ? IDF is included for
comparison purposes. We define these measures for binary classification as shown below.
Information Gain (IG): IG is a two-sided measure that estimates how much is known about an unob-
served random variable given an observed variable. It is defined as the entropy of one random variable
minus the conditional entropy of the observed variable. Thus, IG is the reduced uncertainty of class S
given a feature f :
IG = H(S)?H(S|f) =
?
f?{0,1}
?
S?{0,1}
P (f, S)log
P (f, S)
P (f)P (S)
Brank et al. (2002b) analyzed feature vector sparsity and concluded that IG prefers common features
over extremely rare ones. IG can be regarded as the weighted average of Mutual Information, and rare
features are penalized in the weighting. Thus they are unlikely to be chosen (Li et al., 2009). Forman
(2003) observed that IG performs better when only few features (100-500) are used. Both authors agreed
that IG has a high precision with respect to the minority class.
Odds Ratio (OR): OR (Mosteller, 1968) is a one-sided measure that is defined as the ratio of the odds
of feature f occurring in class S to the odds of it occurring in class S. A value larger than 1 indicates that
a feature is positively correlated with class S, a value smaller than 1 indicates it is negatively correlated:
OR = log
P (f, S)(1? P (f, S))
P (f, S)(1? P (f, S))
Brank et al. (2002b) showed that OR requires a high number of features to achieve a given feature
vector sparsity because it prefers rare terms. Features that occur in very few documents of class S and
do not occur in S have a small denominator, and thus a rather large OR value.
4
Bi-Normal Separation (BNS): BNS (Forman, 2003) is a two-sided measure that regards the proba-
bility of feature f occurring in class S as the area under the normal distribution bell curve. The whole
area under the bell curve corresponds to 1, and the area for a particular feature has a corresponding
threshold along the x-axis (ranging from negative infinite to positive infinite). For a feature f , one can
find the threshold that corresponds to the probability of occurring in the positive class, and the threshold
corresponding to the probability of occurring in S. BNS measures the separation in these two thresholds:
BNS = |F
?1
(
D
Sf
D
S
)? F
?1
(
D
Sf
D
S
)|
where F
?1
is the inverse function of the standard normal cumulative probability distribution. As we can
see, the F
?1
function exaggerates an input more dramatically when the input is close to 0 or 1 which
means that BNS perfers rare words.
Term Frequency * Inverse Document Frequency (TF*IDF): TF ? IDF was originally proposed
for information retrieval tasks, where it measures how representative a term is for the document in which
it occurs. When TF ? IDF is adopted for binary classification, we calculate the TF ? IDF of a feature
w.r.t. the positive class (normalized) and the TF ? IDF w.r.t. the negative class (normalized). We obtain
the absolute value of the difference of these two measures. If a feature is equally important in both
classes and thus would not contribute to classification, it receives a small value. The larger the value,
the more discriminative the feature. We apply two variants of TF ? IDF , depending on how IDF is
calculated:
TF ? IDF
1
= (0.5 +
0.5? T
Sf
max
i
(T
Sf
i
)
)? log(
D
S
+ D
S
D
Sf
)
TF ? IDF
2
= (0.5 +
0.5? T
Sf
max
i
(T
Sf
i
)
)? log(
D
S
D
Sf
)
In the first variant, TF ? IDF
1
, document frequency is based on the whole set of examples while in
the second variant, TF ? IDF
2
, document frequency is based only on the class under consideration, S.
3 Experimental Setup
3.1 Data Sets
Epicurious Data Set: We developed a web crawler to scrape user reviews for 10 146 recipes, published
on the Epicurious website before and on April 02, 2013. On the website, each recipe is assigned a rating
of 1 to 4 forks, including the intermediate values of 1.5, 2.5, and 3.5. This is an accumulated rating over
all user reviews. (Reviews with ratings of 0 were excluded, they usually indicate that recipes have not
received any ratings.) We rounded down all the half ratings, e.g., 1.5 forks counts as 1 fork, based on
the observation that users are generous when rating recipes. Our experiments classify each recipe by
aggregating over all its reviews. While a little more than half of the recipes received 1 to 10 reviews,
there are recipes with more than 100 reviews. To avoid an advantage for highly reviewed recipes, we
randomly selected 10 reviews if a recipe has more than 10 reviews. Recipes with less than 3 reviews
were eliminated since they do not provide enough information. After these clean-up steps, the data set
has the distribution of ratings shown in table 1.
YouTube Data Set: Using the Google YouTube Data API, we collected average user ratings and user
comments for a set of YouTube videos in the category Comedy. Each video is rated from 1 to 5. The
distribution of ratings among all YouTube videos is very skewed, as illustrated in figure 1. Most videos
are rated highly; very few are rated poorly. The 1% quantile is 1.0; the 6.5% quantile is 3.0; the 40%
quantile is 4.75; the 50% quantile is 4.85; and the 77% quantile is 5.0. We selected a set of 3 000 videos.
Videos with less than 5 comments or with non-English comments are discarded.
5
rating no.
1 fork 44 recipes
2 forks 304 recipes
3 forks 1416 recipes
4 forks 1368 recipes
Table 1: The distribution of ratings in the Epicurious data set.
1 2 3 4 5
0.0
0.5
1.0
1.5
2.0
2.5
Video Rating
Den
sity
Figure 1: Skewing in the YouTube data set.
3.2 Data Preprocessing
Before feature extraction, basic preprocessing is conducted for both data sets individually. For the Epi-
curious data set, we perform stemming using the Porter Stemmer (Porter, 1980) to normalize words, and
rare words (? 4 occurrences) are removed. On the YouTube data set, we perform spelling correction
and normalization because the writing style is rather informal. Our normalizer collapses repetitions into
their original forms plus a suffix ?RPT?, thus retaining this potentially helpful clue for reviewer?s strong
emotion without increasing the features due to creative spelling. For example, ?loooooove? is changed
to ?loveRPT? and?lolololol? to ?lolRPT?. The normalizer also replaces all emoticons by either?EMOP?
for positive emoticons or ?EMON? for negative emoticons. Besides a standard English dictionary, we
also use the Urban Dictionary
2
since it has a better coverage of online abbreviations.
We do not filter stop words for two reasons: 1) Stop words are domain dependent, and some En-
glish stop words may be informative for sentiment analysis, and 2) uninformative words that are equally
common in both classes will be excluded by feature selection if the method is successful.
3.3 Feature representation
Since our focus is on settings with high numbers of features, we use a bag-of-words approach, in which
every word represents one feature, and its term frequency serves as its value. Different feature weighting
methods, including binary weighting, term frequency, and TF?IDF have been adopted in past sentiment
analysis studies (e.g., (Pang et al., 2002; Paltoglou and Thelwall, 2010)). (Pang et al., 2002) found that
simply using binary feature weighting performed better than using more complicated weightings in a
task of classifying positive and negative movie reviews. However, movie reviews are relatively short, so
there may not be a large difference between binary features and others. Topic classification usually uses
term frequency as feature weighting. TF ? IDF and variants were shown to perform better than binary
weighting and term frequency for sentiment analysis (Paltoglou and Thelwall, 2010).
Since our user rating prediction tasks aggregate all user comments into large documents and predict
ratings per recipe/YouTube video, term frequency tends to capture richer information than binary fea-
2
http://www.urbandictionary.com/
6
Epicurious YouTube
ratio no. NEG no. POS ratio no. NEG no. POS
1:8 348 2 784 1:10 56 559
1:1.57 348 547 1:1.57 356 559
1:1 348 348 1:1 559 559
Table 2: Skewing ratios and sizes of positive and negative classes for both data sets.
tures. Thus, we use term frequency weighting for simplicity, not to deviate from the focus of feature
selection methods. Since there is a considerable variance in term frequency in the features, we normalize
the feature values to [0,1] to avoid large feature values from dominating the vector operations in classifier
optimization.
For the Epicurious data, the whole feature set consists of 10 677 unigram features. For YouTube, the
full feature set of features consists of 23 232 unigram features. We evaluate the performance of feature
selection methods starting at 500 features, at a step-size of 500. For the Epicurious data, we include up to
10 500 features. For the YouTube data, we stop at 15 000 features due to prohibitively long classification
times.
3.4 Classifier
The classifier we use in this paper is Support Vector Machines (SVMs) in the implementation of
SVM
light
(Joachims, 1999). Because algorithm optimization is not the focus of this study, we use the
default linear kernel and other default parameter values. Classification results are evaluated by accuracy
as well as precision and recall for individual classes.
3.5 Binary Classification
Since all feature selection methods we use in our experiments are defined under a binary classification
scenario, we need to redefine the rating prediction task. For both data sets, this means, we group the
recipes and videos into a positive and a negative class. A baseline classifier predicts every instance as the
majority class. For both data sets, the majority class is positive.
For the Epicurious data set, 1-fork and 2-fork recipes are grouped into the negative class (NEG), and
3-fork and 4-fork recipes are grouped into the positive class (POS), yielding a data set of 348 NEG and
2 784 POS recipes (skewing ratio: 1:8). The different skewing ratios we use are shown in table 2. 2/3 of
the data is used for training, and 1/3 for testing, with the split maintaining the class ratio. Note that for
the less skewed settings, all NEG instances were kept while POS instances were sampled randomly.
For the YouTube data set, we sample from all videos with rating 5 for the positive class and from
all videos with ratings between and including 1 and 3 for the negative class. This yields 559 POS and
559 NEG videos. The different skewing ratios we use are shown in table 2. 7/8 of the data is used for
training, and 1/8 for testing, with the split maintaining the class ratio.
4 Results
4.1 Results for the Epicurious Data Set
The results for the Epicurious data set with different skewing ratios are shown in figure 2. The accuracy
of the baseline is 50% for the 1:1 ratio, 61% for 1:1.57, and 88.9% for 1:8.
The results show that once we use a high number of features, all the feature selection methods perform
the same. This point where they conflate is reached at around 4 000 features for the ratios of 1:1 and
1:1.57. There, accuracy reaches around 71%. For the experiment with the highest skewing, this point is
reached much later, at around 8 000 features. For this setting, we also reach a higher accuracy of around
89%, which is to be expected since we have a stronger majority class. Note that once the conflation point
is reached, the accuracy also corresponds to the accuracy when using the full feature set. This accuracy
is always higher than that of the baseline.
7
2000 4000 6000 8000 10000
0.66
0.68
0.70
0.72
0.74
0.76
Number of Features Chosen
Accura
cy l
l
l
l l l
l l l l l l l l l l l l l l l
l
l l l
l
l
l
l
l l
Epicurious 1:1? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline = 0.5
2000 4000 6000 8000 100000
.60
0.65
0.70
0.75
0.80
Number of Features Chosen
Accura
cy
l
l
l
l
l
l
l l l l l l l l l l l l l l l
l
l
l
l
l
l l
l l l
l l l l
Epicurious 1:1.57 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
2000 4000 6000 8000 100000
.880
0.885
0.890
0.895
0.900
Number of Features Chosen
Accura
cy
l l l l l l l l l l
l l l l l l l l l l
l
l l
l l
l
l l l
l
l
l l l
l l l
Epicurious 1:8 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
Figure 2: The results for the Epicurious data set.
2000 4000 6000 8000 10000
0.0
0.2
0.4
0.6
0.8
1.0
Number of Features Chosen
Prec
ision
l l l l l l l l l l l l l l l l l l l l l
l
l
l
Epicurious 1:8 ? Precision NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
2000 4000 6000 8000 10000
0.00
0.02
0.04
0.06
0.08
0.10
Number of Features Chosen
Reca
ll
l l l l l l l l l l l l l l l l l l l l l
l
l
l
Epicurious 1:8 ? Recall NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
Figure 3: Precision and recall for the negative cases in the Epicurious set, given a 1:8 skewing.
The results also show that the most pronounced differences between feature selection methods occur in
the balanced data set. In the set with the highest skewing, the differences are minor, and only TF ?IDF
2
improves over the baseline when using 1 000 features.
Another surprising result is that TF ?IDF
2
, OR, and BNS have a tendency to fluctuate between higher
and lower results than the setting using all features. This means that it is difficult to find a good cut-off
point for these methods. TF ? IDF
1
and IG show clear performance gains for the balanced setting, but
they also show more fluctuation in settings with higher skewing.
From these results, we can conclude that for sentiment analysis tasks, feature selection is useful only
in a balanced or slightly skewed cases if we are interested in accuracy. However, a look at the precision
and recall given the highest skewing (see figure 3) shows that TF ? IDF
2
in combination with a small
number of features is the only method that finds at least a few cases of the minority class. Thus if a
good performance on the minority class examples is more important than overall accuracy, TF ? IDF
2
is a good choice. One explanation is that TF ? IDF
2
concentrates on one class and can thus ignore the
otherwise overwhelming positive class completely. TF ? IDF
1
and OR have the lowest precision, and
BNS fluctuates. Where recall is concerned, TF ? IDF
1
and IG reach the highest recall given a small
feature set.
4.2 Results for the YouTube Data Set
The results for the YouTube data set with different skewing ratios are shown in figure 4. The accuracy of
the baseline is 50% for the 1:1 ratio, 61.08% for 1:1.57, and 90.9% for 1:10.
The results show that even though the YouTube data set is considerably smaller than the Epicurious
one, is does profit from larger numbers of selected features: For the balanced and the low skewing, there
8
0 5000 10000 15000
0.50
0.55
0.60
0.65
0.70
0.75
Number of Features Chosen
Accura
cy
l
l l l
l
l l l
l l l l l
l l l l l l l l l l l l l l l l l
l l
l l l l
l
l
l
l
l l l
l l l l l l
l l
l l
l
l l l
l
l l
l
l l
l l l l l l l
l
Youtube Equal ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
0 5000 10000 150000
.60
0.65
0.70
0.75
Number of Features Chosen
Accura
cy
l
l l l l
l l
l l
l l l l
l
l l l l
l
l l l
l l l
l l l l l
l
l l
l l l l l l
l l l
l
l l
l l
l
l l
l l
l
l l l l
l l
l l
l
l l l l l l l l l l l l l l l
Youtube 1:1.57 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
0 5000 10000 150000
.80
0.85
0.90
0.95
Number of Features Chosen
Accura
cy
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
l
l
Youtube 1:10 ? Accuracy
l
ll
IGBNSORTFIDF1TFIDF2baseline
Figure 4: The results for the YouTube set.
0 5000 10000 15000
0.70
0.75
0.80
0.85
0.90
0.95
1.00
Number of Features Chosen
Prec
ision
l
l
l
l
l
l
l l
l l l l l l l l l l
l l l l
l
l l l l l l l
l l l l l l l l l
l l l l l l l l
l l
l l
l
l
l
l
l l l l l l l l l l
Youtube 1:1.57 ? Precision NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
0 5000 10000 15000
0.0
0.1
0.2
0.3
0.4
0.5
Number of Features Chosen
Reca
ll
l l
l l l
l l
l l
l l l l
l
l l l l
l
l l l l l l l l l l l
l
l l
l l l l l l
l l l
l
l l
l l
l
l l
l l
l
l l l l l l l l l l l l l l l l l l l l l l l l
Youtube 1:1.57 ? Recall NEG class
l
l
l
IGBNSORTFIDF1TFIDF2
Figure 5: Precision and recall for the negative cases in the YouTube set, given a 1:1.57 skewing.
is no point at which the methods conflate. The results for the highly skewed case show that no feature
selection method is capable of finding cases of the minority class: all methods consistently identify only
one instance of the negative class. However, this may be a consequence of the small data set. In terms of
accuracy, we see that a combination of a small number of features with either IG or TF ? IDF
1
provides
the best results. For the YouTube data set, BNS also performs well but requires a larger set of features for
reaching its highest accuracy. We assume that this is the case because BNS has a tendency to prefer rare
words. Note that we did not test for number of features greater than 15 000 because of the computation
cost, but we can see that the performance curve for different feature selection methods tends to conflate
to the point that represents the full feature set.
If we look at the performance of different feature selection methods on identification of minority class
instances, we find that TF ? IDF
2
again manages to increase recall in the highly skewed case, but this
time at the expense of precision. For the 1:1.57 ratio, all methods reach a perfect precision when a high
number of features is selected, see figure 5. TF ? IDF
2
is the only method that reaches this precision
with small numbers of features, too. However, this is not completely consistent.
4.3 Discussion
If we compare the performance curves across data sets and skewing ratios and aim for high accuracy,
we see that there is no single feature selection method that is optimal in all situations. Thus, we have to
conclude that the choice of feature selection method is dependent on the task. In fact, the performance
of a feature selection method could depend on many factors, such as the difficulty of the classification
task, the preprocessing step, the feature generation decision, the data representation scheme (Brank et
al., 2002a), or the classification model (e.g., SVM, Maximum Entropy, Naive Bayes).
9
We have also shown on two different data sets, each with three skewing ratios, that it is difficult for
feature selection methods to mitigate the effect of highly skewed class distributions while we can still
improve performance by using a reduced feature set for slightly skewed cases. Thus, the higher the skew-
ing of the data set is, the more difficult it is to find a feature selection method that has a positive effect,
and parameters, such as feature set size, have to be optimized very carefully. Thus, feature selection is
much less effective in highly skewed user rating tasks than in document classification.
However, if the task requires recall of the minority class, our experiments have shown that TF ?IDF
2
is able to increase this measure with a small feature set, even for highly imbalanced cases.
5 Conclusion and Future Work
In this paper, we investigated whether feature selection methods reported to be successful for document
classification perform robustly in sentiment classification problems with a highly skewed class distribu-
tion. Our findings show that feature selection methods are most effective when the data sets are balanced
or moderately skewed, while for highly imbalanced cases, we only saw an improvement in recall for the
minority class.
In the future, we will extend feature selection methods ? originally defined for binary classification
scenarios ? to handle multi-class classification problems. A simple way of implementing this is be to
break multi-class classification into several 1-vs.-all or 1-vs.-1 tasks, perform feature selection on these
binary tasks and then aggregate them. Another direction that we want to take is integrating more complex
features, such as parsing or semantic features, into the classification task to investigate how they influence
feature selection. In addition, we will compare other approaches against feature selection for handling
highly skewed data sets, including classification by rank, ensemble learning methods, and memory-
based learning with a instance-specific weighting. Finally, a more challenging task is to select features
for sentiment analysis problems where no annotation (feature selection under unsupervised learning) or
few annotations (feature selection under semi-supervised learning) are available.
References
Basant Agarwal and Namita Mittal. 2012. Categorical probability proportion difference (CPPD): A feature selec-
tion method for sentiment classification. In Proceedings of the 2nd Workshop on Sentiment Analysis where AI
meets Psychology (SAAIP), pages 17?26.
Basant Agarwal and Namita Mittal. 2013. Sentiment classification using rough set based hybrid feature selection.
In Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media
Analysis (WASSA), pages 115?119, Atlanta, GA.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011. Twitter mood predicts the stock market. Journal of Compu-
tational Science, 2:1?8.
Janez Brank, Marko Grobelnik, Nata?sa Milic-Frayling, and Dunja Mladenic. 2002a. An extensive empirical study
of feature selection metrics for text classification. In Proceedings of the Third International Conference on Data
Mining Methods and Databases for Engineering, Finance and Other Fields, Bologna, Italy.
Janez Brank, Marko Grobelnik, Nata?sa Milic-Frayling, and Dunja Mladenic. 2002b. Feature selection using
Linear Support Vector Machines. Technical Report MSR-TR-2002-63, Microsoft Research.
George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Thorsten Joachims. 1999. Making large-scale SVM learning practical. In B. Sch?olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector Learning. MIT-Press.
Olena Kummer and Jaques Savoy. 2012. Feature selection in sentiment analysis. In Proceeding of the Conf?erence
en Recherche d?Infomations et Applications (CORIA), pages 273?284, Bordeaux, France.
Shoushan Li, Rui Xia, Chengqing Zong, and Chu-Ren Huang. 2009. A framework of feature selection methods
for text categorization. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Language Processing of the AFNLP, pages 692?700, Suntec,
Singapore.
10
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pages 142?150, Portland, OR.
Frederick Mosteller. 1968. Association and estimation in contingency tables. Journal of the American Statistical
Association, 63(321):1?28.
Tim O?Keefe and Irena Koprinska. 2009. Feature selection and weighting methods in sentiment analysis. In Pro-
ceedings of the 14th Australasian Document Computing Symposium (ADCS), pages 67?74, Sydney, Australia.
Georgios Paltoglou and Mike Thelwall. 2010. A study of information retrieval weighting schemes for sentiment
analysis. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages
1386?1395, Uppsala, Sweden.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Lin-
guistics, Barcelona, Spain.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information
Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 79?86, Philadelphia, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130?137.
Zhaohui Zheng, Xiayun Wu, and Rohini Srihari. 2004. Feature selection for text categorization on imbalanced
data. ACM SIGKDD Explorations Newsletter, 6(1):80?89.
11
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 12?21,
Dublin, Ireland, August 24 2014.
?My Curiosity was Satisfied, but not in a Good Way?:
Predicting User Ratings for Online Recipes
Can Liu, Chun Guo, Daniel Dakota , Sridhar Rajagopalan, Wen Li, Sandra K
?
ubler
Indiana University
{liucan, chunguo, ddakota, srrajago, wl9, skuebler}@indiana.edu
Ning Yu
University of Kentucky
ning.yu@uky.edu
Abstract
In this paper, we develop an approach to automatically predict user ratings for recipes at Epicuri-
ous.com, based on the recipes? reviews. We investigate two distributional methods for feature se-
lection, Information Gain and Bi-Normal Separation; we also compare distributionally selected
features to linguistically motivated features and two types of frameworks: a one-layer system
where we aggregate all reviews and predict the rating vs. a two-layer system where ratings of
individual reviews are predicted and then aggregated. We obtain our best results by using the
two-layer architecture, in combination with 5 000 features selected by Information Gain. This
setup reaches an overall accuracy of 65.60%, given an upper bound of 82.57%.
1 Introduction
Exchanging recipes over the internet has become popular over the last decade. There are numerous sites
that allow us to upload our own recipes, to search for and to download others, as well as to rate and
review recipes. Such sites aggregate invaluable information. This raises the question how such sites can
select good recipes to present to users. Thus, we need to automatically predict their ratings.
Previous work (Yu et al., 2013) has shown that the reviews are the best rating predictors, in comparison
to ingredients, preparation steps, and metadata. In this paper, we follow their approach and investigate
how to use the information contained in the reviews to its fullest potential. Given that the rating classes
are discrete and that the distances between adjacent classes are not necessarily equivalent, we frame this
task as a classification problem, in which the class distribution is highly skewed, posing the question of
how to improve precision and recall especially for the minority classes to achieve higher overall accuracy.
One approach is to identify n-gram features of the highest discriminating power among ratings, from a
large number of features, many of which are equally distributed over ratings. An alternative strategy is
to select less surface-oriented, but rather linguistically motivated features. Our second question concerns
the rating predictor architecture. One possibility is to aggregate all reviews for a recipe, utilizing rich
textual information at one step (one-layer architecture). The other possibility is to rate individual reviews
first, using shorter but more precise language clues, and then aggregate them (two-layer). The latter
approach avoids the problem of contradictory reviews for a given review, but it raises the question on
how to aggregate over individual ratings. We will investigate all these approaches.
The remainder of the paper is structured as follows: First, we review related work in section 2. Then, in
section 3, we motivate our research questions in more detail. Section 4 describes the experimental setup,
including the data preparation, feature extraction, classifier, and evaluation. In section 5, we present
the results for the one-layer experiments, and in section 6 for the two-layer experiments. Section 7
investigates a more realistic gold standard. We then conclude in section 8.
2 Related Work
This section provides a brief survey for sentiment analysis on online reviews.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
12
During the last decade or more, there has been significant body of sentiment analysis studies on online
reviews. Two major approaches exist: lexicon-based and machine learning. A lexicon-based approach
requires prior knowledge of important sentiment features to build a list of sentiment-bearing words (or
phrases), which are often domain independent. Examples of such lexicons include the Multi-Perspective
Question Answering (MPQA) subjectivity lexicon (Wilson et al., 2005) and the Linguistic Inquiry and
Word Count (LIWC) dictionary (Pennebaker et al., 2014). The sentiment of a review is determined
by various ways of aggregating information about the sentiment-bearing words (phrases), such as their
frequency and sentiment scores. The machine learning approach dominantly adopts supervised learning
algorithms, which treat sentiment analysis as a text classification task. In this case, sentiment features
are generated from a pre-labeled corpus. Given the lack of annotated data, semi-supervised learning is
adopted (Yu, 2014; Yu and Ku?bler, 2011). For this study, we focus on a specific language domain of
online recipe reviews, which has user ratings, thus we choose supervised learning. We also adopt one
existing linguistic lexicon to provide extra features for our classification models.
The earliest sentiment analysis on online reviews was done by Pang et al. (2002); they applied several
supervised learning algorithms to classify online movie reviews into a positive and a negative class.
This study found that machine learning methods outperformed human annotators. It also found that
bigrams did not improve the classification performance, whether used alone or with unigrams, which has
been confirmed by many following studies. However, Cui et al. (2006) later conjectured that when the
training corpus is large enough, adding bigrams to unigrams improved the accuracy of binary product
review classification. A great number of diverse features were proven to be beneficial to capture subtle
sentiments across studies and a ?kitchen sink? approach is often adopted for sentiment analysis (Yang
et al., 2008). However, when features are noisy and redundant, researcher have found it beneficial to
identify the most telling ones (Gamon, 2004; Ng et al., 2006).
While it is useful to differentiate positive and negative reviews, a finer level of distinction can help
users better compare online reviews. As a matter of fact, even extra half star ratings can have dramatic
economic impact (Anderson and Magruder, 2012). To predict multi-level ratings, either multiclass clas-
sification or regression methods can be applied (Koppel and Schler, 2006; Yu et al., 2013). Pang and Lee
(2005) have also proposed an alternative meta-algorithm based on metric labeling for predicting three or
four sentiment classes for movie reviews. In their experiments, the meta-algorithm outperformed SVMs
in either one-versus-all or regression mode. In order to adopt this meta-algorithm, however, one needs to
determine an effective review similarity measure, which is not always straightforward.
If an item receives multiple reviews and/or comes from multiple sources, an overall rating needs to be
generated for this item. Yu et al. (2013) generated this overall rating by treating all the reviews from one
recipe as one long review. In this study, we are going to investigate how to integrate review-level rating
predictions to generate a recipe-level prediction. Rating aggregation has been studied intensively for
collaborative filtering, where the user/rater?s bias is adjusted (e.g., the trustworthy user?s rating has more
influence than others (McGlohon et al., 2010)). Since our current study does not take raters? information
into consideration, we are going to stay with the sample aggregation method. A study by Garcin et al.
(2009) suggests that among mean, median, and mode, the median is often a better choice as it is not as
sensitive to outliers as the mean.
3 Research Questions
As described in the previous section, many studies use only word unigrams or bigrams. We use word
and part-of-speech (POS) n-grams, with n ranging from 1 to 3. This approach generates a large number
of features, creating a very noisy and high dimensional data set, which also makes classifier training
and testing slow. For this reason, we first investigate the effect of feature selection. The next question
concerns the usefulness of linguistically and socio-linguistically motivated features. This results in a
small, but ideally meaningful set of features. The last research question that we approach in this paper
concerns whether classifying recipes on the recipe level is too coarse. In general, we have a wide range
of reviews, each of which is accompanied by a user rating. Thus, it is possible to conduct review-level
classification and then aggregate the ratings.
13
3.1 Feature Selection
Our primary feature set is based on word and POS n-grams. This results in an extremely large feature
set of 449 144 features, many of which do not serve any discriminatory function. A common first step to
trimming the feature set is to delete stop words. However, in the cooking domain, it is unclear whether
stop words would help. Feature selection is used to identify n-grams tightly associated with individual
ratings. Additionally, a extremely high dimensional feature representation makes model training and
testing more time consuming, and is likely to suffer from overfitting - given a large number of parameters
needed to describe the model. Due to the exponential computation time required by wrapper approaches
for feature selection, we use filtering approaches which are based on statistics about the distribution of
features. Previous research (Liu et al., 2014) indicates that Bi-Normal Separation (BNS) (Forman, 2003)
and Information Gain (IG) yield best results for this task. Information Gain is defined as follows:
IG = H(S) ? H(S|f) =
?
f?{0,1}
?
S?{0,1}
P (f, S)log
P (f, S)
P (f)P (S)
where S is the positive class, f a feature, and P (f, S) the joint probability of the feature f occurring
with class S. Bi-Normal Separation finds the separation of the probability of a feature occurring in the
positive class vs. the negative class, normalized by F
?1
, which is the inverse function of the standard
normal cumulative probability distribution. Bi-Normal Separation is defined as follows:
BNS = |F
?1
(
D
Sf
D
S
) ? F
?1
(
D
Sf
D
S
)|
where D
S
is the number of documents in class S, D
S
the number of documents in class S, D
Sf
the
number of documents in class S where feature f occurs, and D
S,f
the number of documents in class
S where feature f occurs. The F
?1
function exaggerates an input more dramatically when the input is
close to 0 or 1, which means that BNS prefers rare words.
Since both metrics are defined for binary classification, the features are chosen in terms of a separation
of the recipes into ?bad? ratings (1-fork and 2-fork) versus ?good? ratings (3-fork and 4-fork), on the
assumption that the selected features will be predictive for the more specific classes as well. For
review-based experiments, the features are chosen with regard to ?good? and ?bad? individual reviews.
3.2 Linguistically Motivated Features
Linguistic features In order to examine whether linguistic information can improve prediction accu-
racy, linguistically motivated features were extracted from the data. We selected seven features based
on the assumption that they reveal a sense of involvedness or distance of the reviewer, i.e., that authors
distance themselves from a recipe to indicate negative sentiment and show more involvedness to indicate
positive sentiment. These seven features are:
1. The percentage of personal pronouns per sentence.
2. The number of words per sentence.
3. The total number of words in the review.
4. The percentage of passive sentences per review.
5. The number of punctuation marks per sentence.
6. The number of capitalized characters per sentence.
7. The type/token ratio per review.
Features such as words per sentence, total words, and the type/token ratio are seen as indicating the
complexity of the review.
14
Our hypothesis is that the longer the review, the more likely it indicates a negative sentiment as the
review may go at lengths to indicate why something was negative.
Similarly, using the passive voice can be viewed as distancing oneself from the review indicating a
sense of impartial judgement, most likely associated with negativity, as one tends to actively like some-
thing (i.e. ?We liked it? versus ?It wasn?t well seasoned.?). Since some reviews with strong emotions are
written in all capital letters as well as contain many punctuation marks (particularly ?!?), these features
are also collected as possible indicators of sentiment.
Lexicon-based features In addition, we used an existing lexicon, the Linguistic Inquiry and Word
Count (LIWC) dictionary (Pennebaker et al., 2014), to analyze several emotional and cognitive dimen-
sions in the recipe reviews. This lexicon is chosen over other sentiment lexicons because it covers a broad
range of categories beyond simply positive and negative emotions. Briefly, it contains general descriptor
categories (e.g., percentage of words captured by the dictionary), standard linguistic dimensions (e.g.,
percentage of words in the text that are pronouns), word categories tapping psychological constructs
(e.g., biological processes), personal concern categories (e.g., work), spoken language dimensions (e.g.,
accent), and punctuation categories. Details of these dimensions can be found in the LIWC 2007 manual.
For our study, we first extracted all the features from a review set independent from our training/test
set. We then selected the LIWC features with highest power to differentiate four rating classes based
on Information Gain. Below are the 15 selected features. Note that the linguistic features here are
document-level features, not sentence-level features, as proposed above.
? Linguistic Processes
? Negations (e.g., never, no): 57 words
? 1st person plural (e.g., we, us): 12 words
? Exclamation mark
? Psychological Processes
? Affective process: this high level category contains 915 positive/negative emotions, anxiety,
anger and sadness related terms.
? Positive emotion (e.g., love, nice, sweet): 406 words
? Negative emotion (e.g., hurt, ugly, nasty): 499 words
? Sadness (e.g., crying, grief): 101 words
? Exclusive (e.g., but, exclude): 17 words
? Tentative (e.g., maybe, perhaps, guess): 155 words
? Causation (e.g., because, hence): 108 words
? Discrepancy (e.g., should, would, could) : 76 words
? Certainty (e.g., always, never): 83 words
? Sexual (e.g., love): 96 words
? Feel (e.g., feel, touch): 75 words
? Personal Concerns
? Leisure (e.g, cook, chat, movie): 229 words
It is not surprising that emotion related features are selected, but it is interesting to see that cognitive
processes features (i.e., causation, tentative, discrepancy, certainty and exclusive) are also highly related
to ratings. Taking a close look at the means of feature values across four ratings, we observe that people
tend to use words in the tentative, discrepancy, exclusive categories when they write negative recipe
reviews. For terms in causation, however, it is the opposite: People write about reasons when writing
positive reviews. Some further investigation is needed to explain why this is the case. We also see that the
higher the rating, the more likely it is that people use first person plural pronouns. This may be due to the
fact that only when people like a recipe, they will tend to share the food with others. Other observations
15
1 fork 108
2 fork 787
3 fork 5 648
4 fork 3 546
Table 1: The distribution of ratings in the Epicurious data.
include: The sexual features are positively correlated with high ratings, which is mainly due to the word
?love? in its non-sexual meaning. People tend to use more words from the perception processes category
feel when they complain about a recipe.
3.3 One-Layer Prediction Versus Two-Layer Prediction
The one-layer or recipe-based approach consider all reviews per recipe as a single document. This
approach has rich textual information, especially when a large number of reviews exist for a recipe.
However, the concern with this approach is that the reviews in themselves may be varied. There are
recipes whose reviews range from the lowest to the highest rating. Given such a range of individual
ratings, we can assume that the recipe-based approach will be faced with a contradictory feature set for
certain recipes. For this reason, we also investigate a two-layer or review-based approach. Here, every
individual review is rated automatically. In a second step, we aggregate over all reviews per recipe.
Aggregation can either take the form of majority voting, average, or of a second classifier which takes
the aggregated ratings as features to make a final decision. However, this approach will suffer from often
very short reviews, which do not allow the extraction of sufficient features as well as from the inequality
in the number of reviews per recipe.
4 Experimental Setup
4.1 Data Set
We scraped user reviews for 10 089 recipes, published on the Epicurious website
1
before and on April 02,
2013. Typically, a recipe contains three parts: ingredients, cooking instructions, and user reviews. In our
experiments, we focus exclusively on the reviews. Each user review has a rating for this recipe, ranging
from 1 fork to 4 forks. There is also an overall rating per recipe, which is the average of all reviews
ratings as well as ratings submitted without reviews. Half forks are possible for recipe rating but not for
review ratings. These recipes were pre-processed to remove reviews with zero ratings. Recipes that had
no reviews were then subsequently removed. In order to counter the effect of the wide variance in the
number of reviews per recipe, we randomly sampled 10 reviews from recipes with more than 10 reviews.
We had performed initial experiments with all reviews, which resulted in only minor differences. At the
review level, rare words (unigrams occurring less than four times) were removed for two reasons: 1)
Extremely rare words are likely to be noise rather than sentiment-bearing clues; 2) the feature selection
method BNS is biased towards rare words; 3) such words do not generalize well. The recipes were then
tagged using the Stanford POS Tagger (Toutanova et al., 2003).
The data set is severely skewed with regard to the number of recipes per fork: Users seem to be more
willing to review good recipes. To lessen the effect of imbalance in the rating classifier, all half fork
reviews were added to their corresponding full star reviews (i.e., 1.5 fork was added to the 1 fork data).
This resulted in the data split of 10 089 recipes shown in table 1. Even after collapsing the half stars,
there is still a very large skewing of the data towards the higher ratings. This means, feature selection is
important to mitigate the imbalance to a certain degree.
4.2 Features
In addition to the linguistic features described in section 3.2, we also extracted n-gram features: word
unigrams, bigrams, and trigrams as well as POS tag unigrams, bigrams, and trigrams. Since the data
1
http://www.epicurious.com
16
Method 750 900 1 000 1 500 3 000 6 000
BNS ? ? 31.33 ? 42.00 50.67
IG 62.00 62.00 62.33 62.33 61.00 58.67
Table 2: Results for feature selection based on Bi-Normal Separation (BNS) and Information Gain (IG).
includes tokens particular to the web, modifications were made to the data to help with the processing of
these types of tokens. URLs were replaced with a single URL token and tagged with a unique ?URL? tag.
Emoticons were defined as either positive or negative and subsequently replaced by EMOP or EMON
respectively. Since it is unclear for this task whether more frequent feature should receive a larger weight,
we normalized features values to a range of [0,1].
4.3 Classifiers
Preliminary experiments were run to determine the best classifier for the task. We decided on Support
Vector Machines in the implementation of SVM multi-class V1.01 (Crammer and Singer, 2002) for both
review-level and recipe-level rating prediction. Initial experiments showed that SVM multi-class V1.01
reaches higher results on our skewed data set than the current V2.20. For this reason, all experiments
reported in this paper are based on V1.01 with its default settings, i.e., using a linear kernel.
To aggregate review-level ratings into a recipe-level prediction, we experimented with both the max-
imum entropy classifier in the implementation of the Stanford Classifier (Manning and Klein, 2003)
and the SVM multi-class classifier. We included Maxent Classifier because given the small number of
features it is no longer clear whether SVM is advantageous.
4.4 Baseline
The baseline was established following Yu et al. (2013) as selecting the label of majority class (3-fork)
to tag all recipes, producing an accuracy of 56.00% for both one-layer and two-layer systems.
4.5 Evaluation
Evaluation was performed using 3-fold cross validation. Since the data is skewed, we report Precision
(P), Recall (R), and F-Scores (F) for all classes across each experiment, along with standard accuracy.
5 Results for One-Layer Prediction
5.1 Feature Selection
We first investigated the effect of feature selection, varying the number of included features from 750 to
6 000. Results for the two methods and different feature thresholds are shown in table 2. Since previous
work (Liu et al., 2014) showed that BNS has a tendency to select infrequent n-grams and would need
a larger number of features than IG to achieve good performance, we tested the higher ranges of 1 000,
3 000, 6 000 features. None of these experiments yields an accuracy higher than the baseline of 56.00%.
On the other hand, the performance of Information Gain peaks at 1 000 and 1 500 features, and we reach
an absolute increase in accuracy of 6.33%. Given these experiments, for all following experiments, we
use the combination of Information Gain and 1 000 n-gram features.
5.2 Linguistically Motivated Features
Here, we test the contribution of the linguistically motivated features introduced in section 3.2. To allow
a comparison to previous experiments, we report the baseline and the results for using Information Gain.
For the two sets of linguistically motivated features, we used the following combination of features:
1. Lexicon-based features (Lex) combined with linguistic features (Ling) (22 features).
2. Lexicon-based features (Lex) combined with the 1 000 features selected by Information Gain (IG)
(1015 features).
17
1 fork 2 fork 3 fork 4 fork
P R F P R F P R F P R F Acc.
Base 0.00 0.00 0.00 0.00 0.00 0.00 56.00 100.00 72.00 0.00 0.00 0.00 56.00
IG 33.33 1.00 2.00 31.33 12.00 17.33 66.00 73.67 69.67 58.33 58.00 58.00 62.33
Lex+Ling 0.00 0.00 0.00 0.00 0.00 0.00 56.00 100.00 72.00 0.00 0.00 0.00 56.00
IG+Lex 39.00 2.00 3.67 31.67 10.00 15.00 65.33 75.33 69.67 59.67 55.67 57.33 62.67
IG+Ling 0.00 0.00 0.00 32.00 3.33 6.00 63.67 81.00 71.33 62.67 49.67 55.33 63.33
IG+Lex+Ling 0.00 0.00 0.00 32.00 3.33 6.00 63.67 81.00 71.33 62.67 49.67 55.33 63.33
Table 3: Results for manually selected features.
1 fork 2 fork 3 fork 4 fork
no. feat. P R F P R F P R F P R F Acc.
1000 61.57 58.11 59.79 53.70 37.42 44.11 63.04 42.14 50.51 71.30 89.08 79.20 67.80
2000 61.65 58.27 59.91 52.96 39.40 45.18 63.37 43.19 51.37 71.86 88.70 79.40 68.11
3000 62.50 58.51 60.44 52.98 40.88 46.15 62.90 44.49 52.12 72.45 88.10 79.51 68.34
4000 62.45 58.45 60.38 52.38 41.05 46.03 62.99 45.54 52.86 72.83 87.70 79.58 68.46
5000 62.32 57.00 59.54 51.66 41.17 45.82 62.21 46.15 52.99 73.05 87.24 79.52 68.31
Table 4: Results on individual reviews for the two-layer experiments.
3. Linguistic features (Ling) combined with the 1 000 features selected by Information Gain (IG)
(1007 features).
4. A combination of all three sets of features (IG+Lex+Ling) (1022 features).
The results for these experiments are reported in table 3. These results show that a combination of
the two sets of linguistically motivated features does not increase accuracy over the baseline. In fact, the
classification is identical to the baseline, i.e., all recipes are grouped into the majority class of 3-fork. We
assume that the linguistically motivated features are too rare to be useful. If we add the lexicon-based
features to the ones selected by Information Gain, we reach a minimal improvement over only the IG
features: accuracy increases from 62.33% to 62.67%. This increase is mostly due to a better performance
on the minority class of 1 fork. If we add the 7 linguistic features to the IG features, we reach the highest
accuracy of 63.33%. However, this is due to a more pronounced preference for selecting the majority
class. Adding the lexicon-based features to this feature set does not give any further improvements.
6 Results for Two-Layer Prediction
In this section, we investigate the two-layer or review-based prediction. For these experiments, we per-
formed feature selection on the individual reviews using IG. Adding the linguistically motivated features
considerably decreased performance. We assume that these features do not generalize well on the shorter
reviews.
Note that the task approached here is a difficult task since the recipe rating on Epicurious is not the
average over all the ratings associated to the individual reviews but also includes ratings by user who did
not write a review. If we average over all the sampled gold standard review ratings per recipe, we reach
an accuracy of 82.57%. This is the upper bound that we can reach in these experiments.
6.1 Classifying Individual Reviews
First, we look at the phase in which individual reviews are classified. The results of this set of experiments
is shown in table 4. Note that there are three important trends here: 1) The accuracy of the SVM classifier
is higher than for classifying recipes. The comparison needs to be taken with a grain of salt because
these are two different tasks. However, this is an indication that it is possible to reach higher results
based on aggregating over individual reviews. 2) For this task, we reach the highest results by using
4 000 features, i.e., a considerably higher number of features than the optimal set for the recipe-based
experiments, where 1 000 features sufficed. We suspect that we need more features in this setting because
the individual reviews are shorter so that individual features do not generalize as well as for complete
recipes. 3) The classification of individual reviews is less skewed than for complete recipes. The F-scores
18
1 fork 2 fork 3 fork 4 fork
no. f. sys. P R F P R F P R F P R F Acc.
1000 avg 44.64 36.98 40.45 60.00 23.03 33.28 75.72 51.91 61.59 52.38 86.01 65.11 61.48
maxent 43.87 33.33 37.88 58.30 19.73 29.48 73.90 58.77 65.47 55.03 81.40 65.67 63.41
svm 62.21 62.21 62.21 56.18 16.03 24.94 72.24 58.14 64.43 53.92 80.82 68.68 62.21
2000 avg 44.61 38.83 41.52 61.45 24.29 34.82 76.12 53.96 63.15 53.45 85.53 65.79 62.58
maxent 43.17 34.27 38.21 61.47 21.23 31.56 74.43 60.93 67.01 56.27 80.93 66.38 64.58
svm 63.29 63.29 63.29 56.53 16.79 25.89 72.65 60.32 65.91 55.14 80.26 65.37 63.29
3000 avg 42.71 38.86 40.69 61.08 26.57 37.03 75.62 54.33 63.23 53.71 84.60 65.71 62.64
maxent 42.40 35.20 38.47 61.90 23.40 33.96 74.00 61.90 67.41 56.83 79.73 66.36 64.84
svm 63.53 63.53 63.53 54.09 17.41 26.34 72.14 61.45 66.37 55.80 79.02 65.41 63.53
4000 avg 38.64 34.22 36.30 61.09 24.89 35.37 75.23 55.91 64.15 54.34 83.81 65.93 63.07
maxent 38.37 31.47 34.58 60.67 22.47 32.79 73.80 63.03 67.99 57.47 79.00 66.54 65.16
svm 64.03 64.03 64.03 52.92 17.53 26.33 72.24 62.85 67.22 56.51 78.17 65.60 64.03
5000 avg 39.23 37.05 38.11 59.20 24.89 35.05 75.38 56.25 64.42 54.54 83.64 66.03 63.23
maxent 38.03 33.40 35.56 58.37 22.00 31.96 73.97 64.17 68.72 58.13 78.57 66.82 65.60
svm 64.68 64.68 64.68 50.19 17.40 25.84 72.52 64.18 68.10 57.45 77.95 66.15 64.68
Table 5: Results on aggregating reviews for the two-layer experiments.
for the non-majority classes are considerably higher than in the recipe-based setting. Thus, we expect to
obtain more balanced results across classes in the aggregation as well.
6.2 Predicting Recipe Ratings by Aggregating Reviews
When aggregating review predictions to recipe rating, we use three methods: 1) Taking the average of
the review ratings from the previous step; 2) using SVM; and 3) using a maximum entropy classifier
(Maxent), the Stanford Classifier. When calculating the average over review rating predictions, the final
average is rounded up. The results are reported in table 5. When using SVM and the maximum entropy
classifier, we use four features, corresponding to the four ratings. The feature values are calculated as
the percentage of reviews from the target recipe that were assigned to this fork rating by our review-level
classifier.
Overall, the maximum entropy classifier yields the best performance, independent of the number of
features used for the review-level classifier. The highest performance we reach by using 5 000 features
and the maximum entropy classifier. Calculating the average results in the worst performance. Although
Epicurious calculates the average user ratings based on review ratings and singular ratings, keep in mind
that we use at most 10 reviews per recipe, hence only capture part of the image. This may explain why
simply calculating the average does not work well. When looking at the F-scores for each fork in table 5,
however, the maximum entropy classifier produces lower performance than average and SVM classifier
for the 1 fork and 2 fork classes. For 1 fork, SVM has the highest F-scores for different numbers of
features, followed by the averaging approach while for 2 fork, the average approach produced the highest
F-scores. One possible explanation is that recipes with lower ratings have relatively small numbers of
reviews and thus may be less impacted by our sampling.
7 Towards a More Realistic Gold Standard
When we aggregate over the individual review rating using the average, the results are only slightly better
than the one-layer results. For example, the best performance using the average reaches an accuracy of
63.23%, as opposed to the one-layer accuracy of 62.33% in table 2 (note that these settings use only IG
features). One reason for this low performance is that Epicurious averages all review ratings to generate
a recipe rating, independent of whether there is review attached to the rating or not. Since our text-based
classifiers make their decisions only based on the reviews, the question is how well we actually predict
the average rating if only ratings attached to reviews were used in the calculation. In this way, we can
evaluate how well our approach works if we assume that all the information is available to the classifier.
Consequently, we calculated a new gold standard, averaging gold ratings of individual reviews in the
recipe sample. We investigate this effect based on the two-layer setting where reviews are aggregated
via averaging. The results of this set of experiments are shown in table 6 for the two-layer approach and
in table 7 for the one-liner approach. We report results using the gold label based on the ratings from
19
1 fork 2 fork 3 fork 4 fork
sys. P R F P R F P R F P R F Acc.
EPI 39.23 37.05 38.11 59.20 24.89 35.05 75.38 56.25 64.42 54.54 83.64 66.03 63.23
EPI-AVG 56.41 51.16 53.66 62.73 62.73 62.73 76.89 65.66 70.83 67.10 85.39 75.15 71.10
Table 6: Evaluation on a more realistic gold standard for two-layer experiments.
1 fork 2 fork 3 fork 4 fork
P R F P R F P R F P R F Acc.
Base 0.00 0.00 0.00 0.00 0.00 0.00 52.00 100.00 68.00 0.00 0.00 0.00 52.00
IG 8.33 1.00 1.67 29.67 11.00 16.00 64.33 70.00 67.00 64.00 66.00 64.67 63.33
Lex+Ling 0.00 0.00 0.00 0.00 0.00 0.00 51.33 99.33 67.67 41.33 1.00 2.00 51.00
IG+Lex 11.00 1.00 2.00 29.00 9.67 14.67 64.00 70.33 67.00 64.00 65.33 64.67 63.33
IG+Ling 16.67 1.00 2.00 31.00 6.33 10.67 63.00 72.67 67.67 65.00 63.33 64.00 63.33
IG+Lex+Ling 16.67 1.00 2.00 31.33 6.67 11.00 63.00 72.67 67.33 65.00 63.33 64.00 63.33
Table 7: Evaluation on a more realistic gold standard for one-layer experiments.
Epicurious (EPI) and based on the new gold standard (EPI-AVG). These results show that based on this
more realistic gold standard, averaging over the individual reviews results in an accuracy of 71.10%,
however with an upper bound of 100% instead of 82.57%. The results for the on-layer experiments are
not as sensitive to this new gold standard. The baseline, which loses 4%, shows that now, the task is
more difficult. All combinations involving IG selected features reach an accuracy of 63.33%, the same
as for the Epicurious gold standard (see table 3).
8 Conclusion and Future Work
In this study, we have explored various strategies for predicting recipe ratings based on user reviews.
This is a difficult task due to systemic reasons, user bias, as well as exogenous factors: 1) There are
user ratings that do not come with reviews, which means that they constitute hidden information for our
classifiers (so that we have an upper bound of 82.57% in overall accuracy). 2) Ratings are not entirely
supported by text, i.e., some ratings seem to be independent from the review text, due to user behavior
(e.g., people tend to give higher ratings in good weather than in bad weather (Bakhshi et al., 2014)).
Our experiments suggest that a two-layer approach, which predicts review-level ratings and aggregates
them for the recipe-level rating, reaches a higher accuracy than the one-layer approach that aggregates
all reviews and predicts on the recipe level directly, with a 3.6% absolute improvement in accuracy. If
we evaluate the two-layer results on a more realistic gold standard, we achieve an even higher increase
of 12.3%.
Our experiments also suggest that with feature selection, automatically generated n-gram features can
produce reasonable results without manually generated linguistic cues and lexicons, although the latter
does show a slight improvement, especially for minority classes.
A few directions can be taken for our future study: 1) Handling short reviews with better methods for
dealing with sparse features. 2) The feature selection is conducted within a binary classification scenario
(1- and 2-forks vs. 3- and 4-forks). It is worth exploring the effect of feature selection within four 1 vs.
all scenarios (i.e., 1-fork against the rest, etc.). 3) We will explore aspect-level sentiment classification
to provide a finer-grained summary of the recipes.
References
Michael Anderson and Jeremy Magruder. 2012. Learning from the crowd: Regression discontinuity estimates of
the effects of an online review database. The Economic Journal, 122:957?989.
Saeideh Bakhshi, Partha Kanuparthy, and Eric Gilbert. 2014. Demographics, weather and online reviews: A study
of restaurant recommendations. In Proceedings of the WWW conference, Seoul, Korea.
Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector
machines. Journal of Machine Learning Research, 2:265?292.
20
Hang Cui, Vibhu Mittal, and Mayur Datar. 2006. Comparative experiments on sentiment classification for online
product reviews. In Proceedings of the 21st National Conference on Artificial Intelligence, AAAI?06, pages
1265?1270, Boston, Massachusetts.
George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. Journal of
Machine Learning Research, 3:1289?1305.
Michael Gamon. 2004. Sentiment classification on customer feedback data: Noisy data, large feature vectors,
and the role of linguistic analysis. In Proceedings of the 20th International Conference on Computational
Linguistics (COLING), pages 841?847, Geneva, Switzerland.
Florent Garcin, Boi Faltings, Radu Jurca, and Nadine Joswig. 2009. Rating aggregation in collaborative filtering
systems. In Proceedings of the Third ACM Conference on Recommender Systems, pages 349?352, New York,
NY.
Moshe Koppel and Jonathan Schler. 2006. The importance of neutral examples in learning sentiment. Computa-
tional Intelligence Journal, 22:100?109. Special Issue on Sentiment Analysis.
Can Liu, Sandra Ku?bler, and Ning Yu. 2014. Feature selection for highly skewed sentiment analysis tasks. In
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), Dublin,
Ireland.
Christopher Manning and Dan Klein. 2003. Optimization, maxent models, and conditional estimation without
magic. Tutorial at HLT-NAACL 2003 and ACL 2003.
Mary McGlohon, Natalie Glance, and Zach Reiter. 2010. Star quality: Aggregating reviews to rank products and
merchants. In Proceedings of Fourth International Conference on Weblogs and Social Media (ICWSM), pages
114?121, Washington, DC.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources
in the automatic identification and classification of reviews. In Proceedings of COLING/ACL, pages 611?618,
Sydney, Australia.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguistics,
ACL, pages 115?124, Ann Arbor, MI.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine
learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,
EMNLP, pages 79?86, Philadelphia, PA.
James Pennebaker, Roger Booth, and Martha Francis, 2014. Linguistic inqury and word count: LIWC 2007
operator?s manual. http://homepage.psy.utexas.edu/HomePage/Faculty/Pennebaker/
Reprints/LIWC2007_OperatorManual.pdf.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, pages 252?259, Edmonton,
Canada.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
347?354, Vancouver, Canada.
Kiduk Yang, Ning Yu, and Hui Zhang. 2008. WIDIT in TREC2007 blog track: Combining lexicon-basedmethods
to detect opinionated blogs. In Proceedings of the 16th Text Retrieval Conference, Gaithersburg, MD.
Ning Yu and Sandra Ku?bler. 2011. Filling the gap: Semi-supervised learning for opinion detection across domains.
In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, CoNLL, pages 200?
209, Portland, OR.
Ning Yu, Desislava Zhekova, Can Liu, and Sandra Ku?bler. 2013. Do good recipes need butter? Predicting user
ratings of online recipes. In Proceedings of the IJCAI Workshop on Cooking with Computers, Beijing, China.
Ning Yu. 2014. Exploring co-training strategies for opinion detection. Journal of the Association for Information
Science and Technology.
21
