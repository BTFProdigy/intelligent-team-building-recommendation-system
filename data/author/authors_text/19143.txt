Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181?186,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
I?m a Belieber:
Social Roles via Self-identification and Conceptual Attributes
Charley Beller, Rebecca Knowles, Craig Harman
Shane Bergsma
?
, Margaret Mitchell
?
, Benjamin Van Durme
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
?
University of Saskatchewan, Saskatoon, Saskatchewan Canada
?
Microsoft Research, Redmond, Washington USA
charleybeller@jhu.edu, rknowles@jhu.edu, craig@craigharman.net,
shane.a.bergsma@gmail.com, memitc@microsoft.com, vandurme@cs.jhu.edu
Abstract
Motivated by work predicting coarse-
grained author categories in social me-
dia, such as gender or political preference,
we explore whether Twitter contains infor-
mation to support the prediction of fine-
grained categories, or social roles. We
find that the simple self-identification pat-
tern ?I am a ? supports significantly
richer classification than previously ex-
plored, successfully retrieving a variety of
fine-grained roles. For a given role (e.g.,
writer), we can further identify character-
istic attributes using a simple possessive
construction (e.g., writer?s ). Tweets
that incorporate the attribute terms in first
person possessives (my ) are confirmed
to be an indicator that the author holds the
associated social role.
1 Introduction
With the rise of social media, researchers have
sought to induce models for predicting latent au-
thor attributes such as gender, age, and politi-
cal preferences (Garera and Yarowsky, 2009; Rao
et al, 2010; Burger et al, 2011; Van Durme,
2012b; Zamal et al, 2012). Such models are
clearly in line with the goals of both computa-
tional advertising (Wortman, 2008) and the grow-
ing area of computational social science (Conover
et al, 2011; Nguyen et al, 2011; Paul and Dredze,
2011; Pennacchiotti and Popescu, 2011; Moham-
mad et al, 2013) where big data and computa-
tion supplement methods based on, e.g., direct hu-
man surveys. For example, Eisenstein et al (2010)
demonstrated a model that predicted where an au-
thor was located in order to analyze regional dis-
tinctions in communication. While some users ex-
plicitly share their GPS coordinates through their
Twitter clients, having a larger collection of au-
tomatically identified users within a region was
preferable even though the predictions for any
given user were uncertain.
We show that media such as Twitter can sup-
port classification that is more fine-grained than
gender or general location. Predicting social roles
such as doctor, teacher, vegetarian, christian,
may open the door to large-scale passive surveys
of public discourse that dwarf what has been pre-
viously available to social scientists. For exam-
ple, work on tracking the spread of flu infections
across Twitter (Lamb et al, 2013) might be en-
hanced with a factor based on aggregate predic-
tions of author occupation.
We present two studies showing that first-
person social content (tweets) contains intuitive
signals for such fine-grained roles. We argue that
non-trivial classifiers may be constructed based
purely on leveraging simple linguistic patterns.
These baselines suggest a wide range of author
categories to be explored further in future work.
Study 1 In the first study, we seek to determine
whether such a signal exists in self-identification:
we rely on variants of a single pattern, ?I am a ?,
to bootstrap data for training balanced-class binary
classifiers using unigrams observed in tweet con-
tent. As compared to prior research that required
actively polling users for ground truth in order to
construct predictive models for demographic in-
formation (Kosinski et al, 2013), we demonstrate
that some users specify such properties publicly
through direct natural language.
Many of the resultant models show intuitive
strongly-weighted features, such as a writer be-
ing likely to tweet about a story, or an ath-
lete discussing a game. This demonstrates self-
identification as a viable signal in building predic-
tive models of social roles.
181
Role Tweet
artist I?m an Artist..... the last of a dying breed
belieber @justinbieber I will support you in ev-
erything you do because I am a belieber
please follow me I love you 30
vegetarian So glad I?m a vegetarian.
Table 1: Examples of self-identifying tweets.
# Role # Role # Role
29,924 little 5,694 man 564 champion
21,822 big ... ... 559 teacher
18,957 good 4,007 belieber 556 writer
13,069 huge 3,997 celebrity 556 awful
13,020 bit 3,737 virgin ... ...
12,816 fan 3,682 pretty 100 cashier
10,832 bad ... ... 100 bro
10,604 girl 2,915 woman ... ...
9,981 very 2,851 beast 10 linguist
... ... ... ... ... ...
Table 2: Number of self-identifying users per ?role?. While
rich in interesting labels, cases such as very highlight the pur-
poseful simplicity of the current approach.
Study 2 In the second study we exploit a com-
plementary signal based on characteristic con-
ceptual attributes of a social role, or concept
class (Schubert, 2002; Almuhareb and Poesio,
2004; Pas?ca and Van Durme, 2008). We identify
typical attributes of a given social role by collect-
ing terms in the Google n-gram corpus that occur
frequently in a possessive construction with that
role. For example, with the role doctor we extract
terms matching the simple pattern ?doctor?s ?.
2 Self-identification
All role-representative users were drawn from
the free public 1% sample of the Twitter Fire-
hose, over the period 2011-2013, from the sub-
set that selected English as their native language
(85,387,204 unique users). To identify users of
a particular role, we performed a case-agnostic
search of variants of a single pattern: I am a(n)
, and I?m a(n) , where all single tokens filling
the slot were taken as evidence of the author self-
reporting for the given ?role?. Example tweets can
be seen in Table 1, examples of frequency per role
in Table 2. This resulted in 63,858 unique roles
identified, of which 44,260 appeared only once.
1
We manually selected a set of roles for fur-
ther exploration, aiming for a diverse sample
across: occupation (e.g., doctor, teacher), family
(mother), disposition (pessimist), religion (chris-
1
Future work should consider identifying multi-word role
labels (e.g., Doctor Who fan, or dog walker).
0.60
0.65
0.70
0.75
0.80
ll
l
l
l
l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
l
l
l
l
l
l
direc
tione
r
belie
ber
optim
ist
sold
ier
soph
omo
re
pess
imis
t
ran
dom
.0
danc
er
hips
ter
ran
dom
.2
sing
er
fresh
man
mo
ther
ran
dom
.1
chee
rlead
er
rapp
er
chris
tian
artis
t
sm
oker acto
r
vege
taria
n
wo
ma
n
athle
te
geek engi
neer
wa
itres
s
nur
se
ma
n
stud
ent doct
or poet writ
er
athe
ist
gran
dmalawy
er
teac
her
Role
Cha
nce 
of S
ucce
ss
Figure 1: Success rate for querying a user. Random.0,1,2
are background draws from the population, with the mean of
those three samples drawn horizontally. Tails capture 95%
confidence intervals.
tian), and ?followers? (belieber, directioner).
2
We filtered users via language ID (Bergsma et al,
2012) to better ensure English content.
3
For each selected role, we randomly sampled up
to 500 unique self-reporting users and then queried
Twitter for up to 200 of their recent publicly
posted tweets.
4
These tweets served as represen-
tative content for that role, with any tweet match-
ing the self-reporting patterns filtered. Three sets
of background populations were extracted based
on randomly sampling users that self-reported En-
glish (post-filtered via LID).
Twitter users are empowered to at any time
delete, rename or make private their accounts.
Any given user taken to be representative based on
a previously posted tweet may no longer be avail-
able to query on. As a hint of the sort of user stud-
ies one might explore given access to social role
prediction, we see in Figure 1 a correlation be-
tween self-reported role and the chance of an ac-
count still being publicly visible, with roles such
as belieber and directioner on the one hand, and
doctor and teacher on the other.
The authors examined the self-identifying tweet
of 20 random users per role. The accuracy of the
self-identification pattern varied across roles and
is attributable to various factors including quotes,
e.g. @StarTrek Jim, I?m a DOCTOR not a down-
load!. While these samples are small (and thus
estimates of quality come with wide variance), it
2
Those that follow the music/life of the singer Justin
Bieber and the band One Direction, respectively.
3
This removes users that selected English as their primary
language, used a self-identification phrase, e.g. I am a be-
lieber, but otherwise tended to communicate in non-English.
4
Roughly half of the classes had less than 500 self-
reporting users in total, in those cases we used all matches.
182
actorartistatheist
athletebeliebercheerleader
christiandancerdirectioner
doctorengineerfreshman
geekgrandmahipster
lawyermanmother
nurseoptimistpessimist
poetrappersinger
smokersoldiersophomore
studentteachervegetarian
waitresswomanwriter
0 5 10 15
Figure 2: Valid self-identifying tweets from sample of 20.
is noteworthy that a non-trivial number for each
were judged as actually self-identifying.
Indicative Language Most work in user clas-
sification relies on featurizing language use,
most simply through binary indicators recording
whether a user did or did not use a particular word
in a history of n tweets. To explore whether lan-
guage provides signal for future work in fine-grain
social role prediction, we constructed a set of ex-
periments, one per role, where training and test
sets were balanced between users from a random
background sample and self-reported users. Base-
line accuracy in these experiments was thus 50%.
Each training set had a target of 600 users (300
background, 300 self-identified); for those roles
with less than 300 users self-identifying, all users
were used, with an equal number background. We
used the Jerboa (Van Durme, 2012a) platform
to convert data to binary feature vectors over a un-
igram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al, 2008). We used the pos-
itively annotated data to form test sets, balanced
with data from the background set. Each test set
had a theoretical maximum size of 40, but for sev-
eral classes it was in the single digits (see Fig-
ure 2). Despite the varied noisiness of our simple
pattern-bootstrapped training data, and the small
size of our annotated test set, we see in Figure 3
that we are able to successfully achieve statisti-
cally significant predictions of social role for the
majority of our selected examples.
Table 3 highlights examples of language indica-
tive of role, as determined by the most positively
weighted unigrams in the classification experi-
0.2
0.4
0.6
0.8
1.0
l
l l l
l
ll
l ll
l
l l
l
ll
l ll
l
l
ll l
l
l
l
l l l
l
l
l
sold
ier
wo
ma
n
pess
imis
t
chris
tian
gran
dma
nur
se
rapp
er
ma
n poet
chee
rlead
er
stud
ent
engi
neer acto
r
teac
her
vege
taria
n
mo
ther sing
er
lawy
er
optim
ist
wa
itres
s
sm
oker hips
ter doct
or
danc
er
artis
t
fresh
man
direc
tione
r
geek
soph
omo
re
athe
ist
athle
te
writ
er
belie
ber
Role
Acc
urac
y
Figure 3: Accuracy in classifying social roles.
Role :: Feature ( Rank)
artist morning, summer, life, most, amp, studio
atheist fuck, fucking, shit, makes, dead, ..., religion
19
athlete lol, game, probably, life, into, ..., team
9
belieber justin, justinbeiber, believe, beliebers, bieber
cheerleader cheer, best, excited, hate, mom, ..., prom
16
christian lol, ..., god
12
, pray
13
, ..., bless
17
, ..., jesus
20
dancer dance, since, hey, never, been
directioner harry, d, follow, direction, never, liam, niall
doctor sweet, oh, or, life, nothing
engineer (, then, since, may, ), test
9
, -
17
, =
18
freshman summer, homework, na, ..., party
19
, school
20
geek trying, oh, different, dead, been
grandma morning, baby, around, night, excited
hipster fucking, actually, thing, fuck, song
lawyer did, never, his, may, pretty, law, even, office
man man, away, ai, young, since
mother morning, take, fuck, fucking, trying
nurse lol, been, morning, ..., night
10
, nursing
11
, shift
13
optimist morning, enough, those, everything, never
poet feel, song, even, say, yo
rapper fuck, morning, lol, ..., mixtape
8
, songs
15
singer sing, song, music, lol, never
smoker fuck, shit, fucking, since, ass, smoke, weed
20
solider ai, beautiful, lol, wan, trying
sophmore summer, >, ..., school
11
, homework
12
student anything, summer, morning, since, actually
teacher teacher, morning, teach, ..., students
7
, ..., school
20
vegetarian actually, dead, summer, oh, morning
waitress man, try, goes, hate, fat
woman lol, into, woman, morning, never
writer write, story, sweet, very, working
Table 3: Most-positively weighted features per role, along
with select features within the top 20. Surprising mother
features come from ambigious self-identification, as seen in
tweets such as: I?m a mother f!cking starrrrr.
ment. These results qualitatively suggest many
roles under consideration may be teased out from a
background population by focussing on language
that follows expected use patterns. For example
the use of the term game by athletes, studio by
artists, mixtape by rappers, or jesus by Christians.
3 Characteristic Attributes
Bergsma and Van Durme (2013) showed that the
183
task of mining attributes for conceptual classes can
relate straightforwardly to author attribute predic-
tion. If one views a role, in their case gender, as
two conceptual classes, male and female, then ex-
isting attribute extraction methods for third-person
content (e.g., news articles) can be cheaply used to
create a set of bootstrapping features for building
classifiers over first-person content (e.g., tweets).
For example, if we learn from news corpora that:
a man may have a wife, then a tweet saying: ...my
wife... can be taken as potential evidence of mem-
bership in the male conceptual class.
In our second study, we test whether this idea
extends to our wider set of fine-grained roles. For
example, we aimed to discover that a doctor may
have a patient, while a hairdresser may have a
salon; these properties can be expressed in first-
person content as possessives like my patient or my
salon. We approached this task by selecting target
roles from the first experiment and ranking charac-
teristic attributes for each using pointwise mutual
information (PMI) (Church and Hanks, 1990).
First, we counted all terms matching a target
social role?s possessive pattern (e.g., doctor?s )
in the web-scale n-gram corpus Google V2 (Lin
et al, 2010)
5
. We ranked the collected terms
by computing PMI between classes and attribute
terms. Probabilities were estimated from counts of
the class-attribute pairs along with counts match-
ing the generic possessive patterns his and
her which serve as general background cate-
gories. Following suggestions by Bergsma and
Van Durme, we manually filtered the ranked list.
6
We removed attributes that were either (a) not
nominal, or (b) not indicative of the social role.
This left fewer than 30 attribute terms per role,
with many roles having fewer than 10.
We next performed a precision test to identify
potentially useful attributes in these lists. We ex-
amined tweets with a first person possessive pat-
tern for each attribute term from a small corpus
of tweets collected over a single month in 2013,
discarding those attribute terms with no positive
matches. This precision test is useful regardless
of how attribute lists are generated. The attribute
5
In this corpus, follower-type roles like belieber and di-
rectioner are not at all prevalent. We therefore focused on
occupational and habitual roles (e.g., doctor, smoker).
6
Evidence from cognitive work on memory-dependent
tasks suggests that such relevance based filtering (recogni-
tion) involves less cognitive effort than generating relevant
attributes (recall) see (Jacoby et al, 1979). Indeed, this filter-
ing step generally took less than a minute per class.
term chart, for example, had high PMI with doc-
tor; but a precision test on the phrase my chart
yielded a single tweet which referred not to a med-
ical chart but to a top ten list (prompting removal
of this attribute). Using this smaller high-precision
set of attribute terms, we collected tweets from the
Twitter Firehose over the period 2011-2013.
4 Attribute-based Classification
Attribute terms are less indicative overall than
self-ID, e.g., the phrase I?m a barber is a clearer
signal than my scissors. We therefore include a
role verification step in curating a collection of
positively identified users. We use the crowd-
sourcing platform Mechanical Turk
7
to judge
whether the person tweeting held a given role
Tweets were judged 5-way redundantly. Me-
chanical Turk judges (?Turkers?) were presented
with a tweet and the prompt: Based on this
tweet, would you think this person is a BAR-
BER/HAIRDRESSER? along with four response
options: Yes, Maybe, Hard to tell, and No.
We piloted this labeling task on 10 tweets per
attribute term over a variety of classes. Each an-
swer was associated with a score (Yes = 1, Maybe
= .5, Hard to tell = No = 0) and aggregated across
the five judges. We found in development that an
aggregate score of 4.0 (out of 5.0) led to an ac-
ceptable agreement rate between the Turkers and
the experimenters, when the tweets were randomly
sampled and judged internally. We found that
making conceptual class assignments based on a
single tweet was often a subtle task. The results of
this labeling study are shown in Figure 4, which
gives the percent of tweets per attribute that were
4.0 or above. Attribute terms shown in red were
manually discarded as being inaccurate (low on
the y-axis) or non-prevalent (small shape).
From the remaining attribute terms, we identi-
fied users with tweets scoring 4.0 or better as posi-
tive examples of the associated roles. Tweets from
those users were scraped via the Twitter API to
construct corpora for each role. These were split
intro train and test, balanced with data from the
same background set used in the self-ID study.
Test sets were usually of size 40 (20 positive, 20
background), with a few classes being sparse (the
smallest had only 16 instances). Results are shown
in Figure 5. Several classes in this balanced setup
can be predicted with accuracies in the 70-90%
7
https://www.mturk.com/mturk/
184
l l l l ll lll l l l
l l l l l l
l
l l ll
l
l l l l l
l
l l l
Actor/Actress Athlete Barber/Hairdresser Bartender Blogger Cheerleader
Christian College Student Dancer Doctor/Nurse Drummer Hunter
Jew Mom Musician Photographer Professor Rapper/Songwriter
Reporter Sailor Skier Smoker Soldier Student
Swimmer Tattoo Artist Waiter/Waitress Writer
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
0.00.2
0.40.6
0.8
rehear
sal theater directo
r lines
conc
ussionplayin
gprotein sport squadcondit
ioningjerseypositioncoach calves clien
t
scissor
s
shears salon bar blog bloggi
ng pom
hope testimo
ny
church bible schola
rship
syllabu
s
adviso
r
tuition campu
s
univer
sity
college tu
tu
scrub patient stethos
cope drum stand
shul angel deliver
y kid parent
ing set alum guitar piano shoot shutter lecture faculty studen
t lyrics
cove
rage editor article shi
p
goggle
s pipe smokin
g
tobacc
o
smoke cigaret
te
billet comba
t duffel orders bunk deploy
mentbarrac
ks stats cap lab philoso
phy
pool ink station tip apron script memo
ir poemKeyword
Above
 Thres
hold
log10(Count)
l l l l1 2 3 4Keep
l FALSE TRUE
Figure 4: Turker judged quality of attributes selected as
candidate features for bootstrapping positive instances of the
given social role.
0.5
0.6
0.7
0.8
acto
r
athle
te
barbe
r
blogg
er
chee
rlead
er
chris
tian docto
r
drum
mer
mo
m
mu
sicia
n
photo
graph
er
profe
ssor
repor
ter
smo
ker
soldi
er
stude
nt
waite
r
write
r
Accu
racy
Figure 5: Classifier accuracy on balanced set contrasting
agreed upon Twitter users of a given role against users pulled
at random from the 1% stream.
range, supporting our claim that there is discrimi-
nating content for a variety of these social roles.
Conditional Classification How accurately we
can predict membership in a given class when a
Twitter user sends a tweet matching one of the tar-
geted attributes? For example, if one sends a tweet
saying my coach, then how likely is it that author
Figure 6: Results of positive vs negative by attribute term.
Given that a user tweets . . . my lines . . . we are nearly 80%
accurate in identifying whether or not the user is an actor.
is an athlete?
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given attribute term. Positive instances were taken
to be those with a score of 4.0 or higher, with neg-
ative instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in Figure 6.
5 Conclusion
We have shown that Twitter contains sufficiently
robust signal to support more fine-grained au-
thor attribute prediction tasks than have previously
been attempted. Our results are based on simple,
intuitive search patterns with minimal additional
filtering: this establishes the feasibility of the task,
but leaves wide room for future work, both in the
sophistication in methodology as well as the diver-
sity of roles to be targeted. We exploited two com-
plementary types of indicators: self-identification
and self-possession of conceptual class (role) at-
tributes. Those interested in identifying latent de-
mographics can extend and improve these indica-
tors in developing ways to identify groups of inter-
est within the general population of Twitter users.
Acknowledgements This material is partially
based on research sponsored by the NSF un-
der grants DGE-123285 and IIS-1249516 and by
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
185
References
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an eval-
uation. In Proceedings of EMNLP.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clay Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the NAACL Workshop
on Language and Social Media.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of EMNLP.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Jacob Eisenstein, Brendan O?Connor, Noah Smith, and
Eric P. Xing. 2010. A latent variable model of
geographical lexical variation. In Proceedings of
EMNLP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Michal Kosinski, David Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. Proceed-
ings of the National Academy of Sciences.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In Proceedings of NAACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Saif M. Mohammad, Svetlana Kiritchenko, and Joel
Martin. 2013. Identifying purpose behind elec-
toral tweets. In Proceedings of the Second Interna-
tional Workshop on Issues of Sentiment Discovery
and Opinion Mining, WISDOM ?13, pages 1?9.
Dong Nguyen, Noah A Smith, and Carolyn P Ros?e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 115?123. Association for Computational Lin-
guistics.
Marius Pas?ca and Benjamin Van Durme. 2008.
Weakly-Supervised Acquisition of Open-Domain
Classes and Class Attributes from Web Documents
and Query Logs. In Proceedings of ACL.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 430?
438. ACM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Lenhart K. Schubert. 2002. Can we derive general
world knowledge from texts? In Proceedings of
HLT.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Jennifer Wortman. 2008. Viral marketing and the
diffusion of trends on social networks. Technical
Report MS-CIS-08-19, University of Pennsylvania,
May.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
186
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 50?55,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Predicting Fine-grained Social Roles with Selectional Preferences
Charley Beller Craig Harman Benjamin Van Durme
charleybeller@jhu.edu craig@craigharman.net vandurme@cs.jhu.edu
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
Abstract
Selectional preferences, the tendencies of
predicates to select for certain semantic
classes of arguments, have been success-
fully applied to a number of tasks in
computational linguistics including word
sense disambiguation, semantic role label-
ing, relation extraction, and textual infer-
ence. Here we leverage the information
encoded in selectional preferences to the
task of predicting fine-grained categories
of authors on the social media platform
Twitter. First person uses of verbs that se-
lect for a given social role as subject (e.g.
I teach ... for teacher) are used to quickly
build up binary classifiers for that role.
1 Introduction
It has long been recognized that linguistic pred-
icates preferentially select arguments that meet
certain semantic criteria (Katz and Fodor, 1963;
Chomsky, 1965). The verb eat for example se-
lects for an animate subject and a comestible ob-
ject. While the information encoded by selectional
preferences can and has been used to support nat-
ural language processing tasks such as word sense
disambiguation (Resnik, 1997), syntactic disam-
biguation (Li and Abe, 1998) and semantic role
labeling (Gildea and Jurafsky, 2002), much of
the work on the topic revolves around developing
methods to induce selectional preferences from
data. In this setting, end-tasks can be used for
evaluation of the resulting collection. Ritter et al.
(2010) gave a recent overview of this work, break-
ing it down into class-based approaches (Resnik,
1996; Li and Abe, 1998; Clark and Weir, 2002;
Pantel et al., 2007), similarity-based approaches
(Dagan et al., 1999; Erk, 2007), and approaches
using discriminative (Bergsma et al., 2008) or gen-
erative probabilistic models (Rooth et al., 1999)
like their own.
One of our contributions here is to show that
the literature on selectional preferences relates to
the analysis of the first person content transmitted
through social media. We make use of a ?quick
and dirty? method for inducing selectional pref-
erences and apply the resulting collections to the
task of predicting fine-grained latent author at-
tributes on Twitter. Our method for inducing se-
lectional preferences is most similar to class-based
approaches, though unlike approaches such as by
Resnik (1996) we do not require a WordNet-like
ontology.
The vast quantity of informal, first-person text
data made available by the rise of social me-
dia platforms has encouraged researchers to de-
velop models that predict broad user categories
like age, gender, and political preference (Garera
and Yarowsky, 2009; Rao et al., 2010; Burger et
al., 2011; Van Durme, 2012b; Zamal et al., 2012).
Such information is useful for large scale demo-
graphic research that can fuel computational social
science advertising.
Similarly to Beller et al. (2014), we are inter-
ested in classification that is finer-grained than
gender or political affiliation, seeking instead to
predict social roles like smoker, student, and
artist. We make use of a light-weight, unsuper-
vised method to identify selectional preferences
and use the resulting information to rapidly boot-
strap classification models.
2 Inducing Selectional Preferences
Consider the task of predicting social roles in more
detail: For a given role, e.g. artist, we want a way
to distinguish role-bearing from non-role-bearing
users. We can view each social role as being a
fine-grained version of a semantic class of the sort
required by class-based approaches to selectional
preferences (e.g. the work by Resnik (1996) and
those reviewed by Light and Greiff (2002)). The
goal then is to identify a set of verbs that preferen-
50
tially select that particular class as argument. Once
we have a set of verbs for a given role, simple pat-
tern matches against first person subject templates
like I can be used to identify authors that bear
that social role.
In order to identify verbs that select for a given
role r as subject we use an unsupervised method
inspired by Bergsma and Van Durme (2013) that
extracts features from third-person content (i.e.
newswire) to build classifiers on first-person con-
tent (i.e. tweets). For example, if we read in a
news article that an artist drew ..., we can take a
tweet saying I drew ... as potential evidence that
the author bears the artist social role.
We first count all verbs v that appear with role r
as subject in the web-scale, part-of-speech tagged
n-gram corpus, Google V2 (Lin et al., 2010).
The resulting collection of verbs is then ranked
by computing their pointwise mutual information
(Church and Hanks, 1990) with the subject role r.
The PMI of a given role r and a verb v that takes
r as subject is given as:
PMI(r, v) = log
P (r, v)
P (r)P (v)
Probabilities are estimated from counts of the
role-verb pairs along with counts matching the
generic subject patterns he and she which
serve as general background cases. This gives us a
set of verbs that preferentially select for the subset
of persons filling the given role.
The output of the PMI ranking is a high-recall
list of verbs that preferentially select the given so-
cial role as subject over a background population.
Each such list then underwent a manual filtering
step to rapidly remove non-discriminative verbs
and corpus artifacts. One such artifact from our
corpus was the term wannabe which was spuri-
ously elevated in the PMI ranking based on the
relative frequency of the bigram artist wannabe as
compared to she wannabe. Note that in the first
case wannabe is best analyzed as a noun, while in
the second case a verbal analysis is more plausi-
ble. The filtering was performed by one of the au-
thors and generally took less than two minutes per
list. The rapidity of the filtering step is in line with
findings such as by Jacoby et al. (1979) that rele-
vance based filtering involves less cognitive effort
than generation. After filtering the lists contained
fewer than 40 verbs selecting each social role.
In part because of the pivot from third- to first-
person text we performed a precision test on the
remaining verbs to identify which of them are
likely to be useful in classifying twitter users. For
each remaining verb we extracted all tweets that
contained the first person subject pattern I from
a small corpus of tweets drawn from the free pub-
lic 1% sample of the Twitter Firehose over a single
month in 2013. Verbs that had no matches which
appeared to be composed by a member of the as-
sociated social role were discarded. Using this
smaller high-precision set of verbs, we collected
tweets from a much larger corpus drawn from 1%
sample over the period 2011-2013.
One notable feature of the written English in
social media is that sentence subjects can be op-
tionally omitted. Subject-drop is a recognized fea-
ture of other informal spoken and written registers
of English, particularly ?diary dialects? (Thrasher,
1977; Napoli, 1982; Haegeman and Ihsane, 2001;
Weir, 2012; Haegeman, 2013; Scott, 2013). Be-
cause of the prevalence of subjectless cases we
collected two sets of tweets: those matching the
first person subject pattern I and those where
the verb was tweet initial. Example tweets for each
of our social roles can be seen in Table 2.
3 Classification via selectional
preferences
We conducted a set of experiments to gauge the
strength of the selectional preference indicators
for each social role. For each experiment we used
balanced datasets for training and testing with half
of the users taken from a random background sam-
ple and half from a collection of users identified
as belonging to the social role. Base accuracy was
thus 50%.
To curate the collections of positively identified
users we crowdsourced a manual verification pro-
cedure. We use the popular crowdsourcing plat-
form Mechanical Turk
1
to judge whether, for a
tweet containing a given verb, the author held the
role that verb prefers as subject. Each tweet was
judged using 5-way redundancy.
Mechanical Turk judges (?Turkers?) were pre-
sented with a tweet and the prompt: Based on this
tweet, would you think this person is a ARTIST?
along with four response options: Yes, Maybe,
Hard to tell, and No. An example is shown in Fig-
ure 1.
We piloted this labeling task with a goal of
20 tweets per verb over a variety of social roles.
1
https://www.mturk.com/mturk/
51
Artist
draw Yeaa this a be the first time I draw my
shit onn
Athlete
play @[user] @[user] i have got the night off
tonight because I played last night and I
am going out for dinner so won?t be able
to come?
Blogger
blogged @[user] I decided not to renew. I
blogged about it on the fan club. a bit
shocked no neg comments back to me
Cheerleader
cheer I really dont wanna cheer for this game
I have soo much to do
Christian
thank Had my bday yesterday 3011 nd had a
good night with my friends. I thank God
4 His blessings in my life nd praise Him
4 adding another year.
DJ
spin Quick cut session before I spin tonight
Filmmaker
film @[user] apparently there was no au-
dio on the volleyball game I filmed
so...there will be no ?NAT sound? cause
I have no audio at all
Media Host
interview Oh. I interviewed her on the @[user] .
You should listen to the interview. Its
awesome! @[user] @[user] @[user]
Performer
perform I perform the flute... kareem shocked...
Producer
produce RT @[user]: Wow 2 films in Urban-
world this year-1 I produced ... [URL]
Smoker
smoke I smoke , i drank .. was my shit bra !
Stoner
puff I?m a cigarello fiend smokin weed like
its oxygen Puff pass, nigga I puff grass
till I pass out
Student
finish I finish school in March and my friend
birthday in March ...
Teacher
teach @[user] home schooled I really wanna
find out wat it?s like n making new
friends but home schooling is cool I
teach myself mums ill
Table 1: Example verbs and sample tweets collected using
them in the first person subject pattern (I ).
Each answer was associated with a score (Yes = 1,
Maybe = .5, Hard to tell = No = 0) and aggregated
across the five judges, leading to a range of pos-
sible scores from 0.0 to 5.0 per tweet. We found
in development that an aggregate score of 4.0 led
to an acceptable agreement rate between the Turk-
ers and the experimenters, when the tweets were
randomly sampled and judged internally.
Verbs were discarded for being either insuffi-
ciently accurate or insufficiently prevalent in the
corpus. From the remaining verbs, we identified
users with tweets scoring 4.0 or better as the posi-
tive examples of the associated social roles. These
positively identified user?s tweets were scraped us-
ing the Twitter API in order to construct user-
specific corpora of positive examples for each role.
Figure 1: Mechanical Turk presentation
0.5
0.6
0.7
0.8
Art
ist
Ath
lete
Blo
gge
r
Che
erle
ade
r
Chr
istia
n
DJ
Film
mak
er
Ho
st
Per
form
er
Pro
duc
er
Sm
oke
r
Sto
ner
Stu
den
t
Tea
che
r
Acc
ura
cy
Figure 2: Accuracy of classifier trained and tested on bal-
anced set contrasting agreed upon Twitter users of a given
role, against users pulled at random from the 1% stream.
3.1 General Classification
The positively annotated examples were balanced
with data from a background set of Twitter users
to produce training and test sets. These test sets
were usually of size 40 (20 positive, 20 back-
ground), with a few classes being sparser (the
smallest test set had only 28 instances). We used
the Jerboa (Van Durme, 2012a) platform to con-
vert our data to binary feature vectors over a uni-
gram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al., 2008). Results are shown
in Figure 2. As can be seen, a variety of classes in
this balanced setup can be predicted with accura-
cies in the range of 80%. This shows that the in-
formation encoded in selectional preferences con-
tains discriminating signal for a variety of these
social roles.
3.2 Conditional Classification
How accurately can we predict membership in a
given class when a Twitter user sends a tweet
matching one of the collected verbs? For exam-
ple, if one sends a tweet saying I race ..., then how
likely is it that the author is an athlete?
52
0.5
0.6
0.7
0.8
Art
ist :
 dra
w
Ath
lete
 : ra
ce
Ath
lete
 : ru
n
Blo
gge
r : b
log
ged
Che
erle
ade
r : c
hee
r
Chr
istia
n : 
pra
y
Chr
istia
n : 
serv
e
Chr
istia
n : 
than
k
DJ 
: sp
in
Film
mak
er 
: fil
m
Ho
st : 
inte
rvie
w
Per
form
er :
 per
form
Pro
duc
er :
 pro
duc
e
Sm
oke
r : 
sm
oke
Sto
ner
 : p
uff
Sto
ner
 : sp
ark
Stu
den
t : e
nro
ll
Stu
den
t : f
inis
h
Tea
che
r : t
eac
h
Acc
ura
cy
Figure 3: Results of positive vs negative by verb. Given
that a user writes a tweet containing I interview . . . or Inter-
viewing . . . we are about 75% accurate in identifying whether
or not the user is a Radio/Podcast Host.
# Users # labeled # Pos # Neg Attribute
199022 516 63 238 Artist-draw
45162 566 40 284 Athlete-race
1074289 1000 54 731 Athlete-run
9960 15 14 0 Blogger-blog
2204 140 57 18 College Student-enroll
247231 1000 85 564 College Student-finish
60486 845 61 524 Cheerleader-cheer
448738 561 133 95 Christian-pray
92223 286 59 180 Christian-serve
428337 307 78 135 Christian-thank
17408 246 17 151 DJ-spin
153793 621 53 332 Filmmaker-film
36991 554 42 223 Radio Host-interview
43997 297 81 97 Performer-perform
69463 315 71 100 Producer-produce
513096 144 74 8 Smoker-smoke
5542 124 49 15 Stoner-puff
5526 229 59 51 Stoner-spark
149244 495 133 208 Teacher-teach
Table 2: Numbers of positively and negatively identified
users by indicative verb.
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given verb term. Positive instances were taken to
be those with a score of 4.0 or higher, with nega-
tive instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in figure 3.
Note that for a number of verb terms these thresh-
olds left very sparse collections of users. There
were only 8 users, for example, that tweeted the
phrase I smoke ... but were labeled as negative in-
stances of Smokers. Counts are given in Table 2.
Despite the sparsity of some of these classes,
many of the features learned by our classifiers
make intuitive sense. Highlights of the most
highly weighted unigrams from the classification
Verb Feature ( Rank)
draw drawing, art, book
4
, sketch
14
, paper
19
race race, hard, winter, won
11
, training
16
, run
17
run awesome, nike
6
, fast
9
, marathon
20
blog notes, boom, hacked
4
, perspective
9
cheer cheer, pictures, omg, text, literally
pray through, jesus
3
, prayers
7
, lord
14
, thank
17
serve lord, jesus, church, blessed, pray, grace
thank [ ], blessed, lord, trust
11
, pray
12
enroll fall, fat, carry, job, spend, fail
15
finish hey, wrong, may
8
, move
9
, officially
14
spin show, dj, music, dude, ladies, posted, listen
film please, wow, youtube, send, music
8
perform [ ], stuck, act, song, tickets
7
, support
16
produce follow, video
8
, listen
10
, single
11
, studio
13
,
interview fan, latest, awesome, seems
smoke weakness, runs, ti, simply
puff bout, $
7
, smh
9
, weed
10
spark dont, fat
5
, blunt
6
, smoke
11
teach forward, amazing, students, great, teacher
7
Table 3: Most-highly indicative features that a user holds
the associated role given that they used the phrase I VERB
along with select features within the top 20.
experiments are shown in Table 3. Taken together
these features suggest that several of our roles can
be distinguished from the background population
by focussing on typical language use. The use of
terms like, e.g., sketch by artists, training by ath-
letes, jesus by Chrisitians, and students by teach-
ers conforms to expected pattern of language use.
4 Conclusion
We have shown that verb-argument selectional
preferences relates to the content-based classifica-
tion strategy for latent author attributes. In particu-
lar, we have presented initial studies showing that
mining selectional preferences from third-person
content, such as newswire, can be used to inform
latent author attribute prediction based on first-
person content, such as that appearing in social
media services like Twitter.
Future work should consider the question of
priors. Our study here relied on balanced class
experiments, but the more fine-grained the social
role, the smaller the subset of the population we
might expect will possess that role. Estimating
these priors is thus an important point for future
work, especially if we wish to couple such demo-
graphic predictions within a larger automatic sys-
tem, such as the aggregate prediction of targeted
sentiment (Jiang et al., 2011).
Acknowledgements This material is partially based
on research sponsored by the NSF under grant IIS-1249516
and by DARPA under agreement number FA8750-13-2-0017
(DEFT).
53
References
Charley Beller, Rebecca Knowles, Craig Harman,
Shane Bergsma, Margaret Mitchell, and Benjamin
Van Durme. 2014. I?m a belieber: Social roles via
self-identification and conceptual attributes. In Pro-
ceedings of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 59?68. Association for
Computational Linguistics.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of EMNLP.
Noam Chomsky. 1965. Aspects of the Theory of Syn-
tax. Number 11. MIT press.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2).
Ido Dagan, Lillian Lee, and Fernando CN Pereira.
1999. Similarity-based models of word cooccur-
rence probabilities. Machine Learning, 34(1-3):43?
69.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceeding of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, volume 45, page 216.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational linguis-
tics, 28(3):245?288.
Liliane Haegeman and Tabea Ihsane. 2001. Adult null
subjects in the non-pro-drop languages: Two diary
dialects. Language acquisition, 9(4):329?346.
Liliane Haegeman. 2013. The syntax of registers: Di-
ary subject omission and the privilege of the root.
Lingua, 130:88?110.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585?600.
Long Jiang, Mo Yu, Xiaohua Liu, and Tiejun Zhao.
2011. Target-dependent twitter sentiment classifi-
cation. In Proceedings of ACL.
Jerrold J Katz and Jerry A Fodor. 1963. The structure
of a semantic theory. language, pages 170?210.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDL principle.
Computational linguistics, 24(2):217?244.
Marc Light and Warren Greiff. 2002. Statistical mod-
els for the induction and use of selectional prefer-
ences. Cognitive Science, 26(3):269?281.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221?
2227.
Donna Jo Napoli. 1982. Initial material deletion in
English. Glossa, 16(1):5?111.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,
Timothy Chklovski, and Eduard H Hovy. 2007.
ISP: Learning inferential selectional preferences. In
HLT-NAACL, pages 564?571.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61(1):127?159.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52?57. Washington,
DC.
Alan Ritter, Masaum, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 424?434. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th annual meeting of the Asso-
ciation for Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
54
Kate Scott. 2013. Pragmatically motivated null sub-
jects in English: A relevance theory perspective.
Journal of Pragmatics, 53:68?83.
Randolph Thrasher. 1977. One way to say more by
saying less: A study of so-called subjectless sen-
tences. Kwansei Gakuin University Monograph Se-
ries, 11.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Andrew Weir. 2012. Left-edge deletion in English and
subject omission in diaries. English Language and
Linguistics, 16(01):105?129.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
55
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 51?60,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Quantifying Mental Health Signals in Twitter
Glen Coppersmith Mark Dredze Craig Harman
Human Language Technology Center of Excellence
Johns Hopkins University
Balitmore, MD, USA
Abstract
The ubiquity of social media provides a
rich opportunity to enhance the data avail-
able to mental health clinicians and re-
searchers, enabling a better-informed and
better-equipped mental health field. We
present analysis of mental health phe-
nomena in publicly available Twitter data,
demonstrating how rigorous application of
simple natural language processing meth-
ods can yield insight into specific disor-
ders as well as mental health writ large,
along with evidence that as-of-yet undis-
covered linguistic signals relevant to men-
tal health exist in social media. We present
a novel method for gathering data for
a range of mental illnesses quickly and
cheaply, then focus on analysis of four in
particular: post-traumatic stress disorder
(PTSD), depression, bipolar disorder, and
seasonal affective disorder (SAD). We in-
tend for these proof-of-concept results to
inform the necessary ethical discussion re-
garding the balance between the utility of
such data and the privacy of mental health
related information.
1 Introduction
While mental health issues pose a significant
health burden on the general public, mental health
research lacks the quantifiable data available to
many physical health disciplines. This is partly
due to the complexity of the underlying causes
of mental illness and partly due to longstanding
societal stigma making the subject all but taboo.
Lack of data has hampered mental health research
in terms of developing reliable diagnoses and ef-
fective treatment for many disorders. Moreover,
population-level analysis via traditional methods
is time consuming, expensive, and often comes
with a significant delay.
In contrast, social media is plentiful and has
enabled diverse research on a wide range of top-
ics, including political science (Boydstun et al.,
2013), social science (Al Zamal et al., 2012), and
health at an individual and population level (Paul
and Dredze, 2011; Dredze, 2012; Aramaki et al.,
2011; Hawn, 2009). Of the numerous health top-
ics for which social media has been considered,
mental health may actually be the most appropri-
ate. A major component of mental health research
requires the study of behavior, which may be man-
ifest in how an individual acts, how they com-
municate, what activities they engage in and how
they interact with the world around them includ-
ing friends and family. Additionally, capturing
population level behavioral trends from Web data
has previously provided revolutionary capabilities
to health researchers (Ayers et al., 2014). Thus,
social media seems like a perfect fit for study-
ing mental health in both individual and overall
trends in the population. Such topics have already
been the focus of several studies (Coppersmith et
al., 2014; De Choudhury et al., 2014; De Choud-
hury et al., 2013d; De Choudhury et al., 2013b;
De Choudhury et al., 2013c; Ayers et al., 2013).
What can we expect to learn about mental health
by studying social media? How does a service like
Twitter inform our knowledge in this area? Nu-
merous studies indicate that language use, social
expression and interaction are telling indicators of
mental health. The well-known Linguistic Inquiry
Word Count (LIWC), a validated tool for the psy-
chometric analysis of language data (Pennebaker
et al., 2007), has been repeatedly used to study
language associated with all types of disorders
(Resnik et al., 2013; Alvarez-Conrad et al., 2001;
Tausczik and Pennebaker, 2010). Furthermore, so-
cial media is by nature social, which means that
social patterns, a critical part of mental health and
illness, may be readily observable in raw Twitter
data. Thus, Twitter and other social media provide
51
a unique quantifiable perspective on human behav-
ior that may otherwise go unobserved, suggesting
it as a powerful tool for mental health researchers.
The main vehicle for studying mental health in
social media has been the use of surveys, e.g.,
depression battery (De Choudhury, 2013) or per-
sonality test (Schwartz et al., 2013), to deter-
mine characteristics of a user coupled with analyz-
ing their corresponding social media data. Work
in this area has mostly focused on depression
(De Choudhury et al., 2013d; De Choudhury et al.,
2013b; De Choudhury et al., 2013c), and the num-
ber of users is limited by those that can complete
the appropriate survey. For example, De Choud-
hury et al. (2013d) solicited Twitter users to take
the CES-D and to share their public Twitter pro-
file, analyzing linguistic and behavioral patterns.
While this type of study has produced high qual-
ity data, it is limited in size (by survey respon-
dents) and scope (to diagnoses which have a bat-
tery amenable to administration over the internet).
In this paper we examine a range of mental
health disorders using automatically derived sam-
ples from large amounts of Twitter data. Rather
than rely on surveys, we automatically identify
self-expressions of mental illness diagnoses and
leverage these messages to construct a labeled data
set for analysis. Using this dataset, we make the
following contributions:
? We demonstrate the effectiveness of our au-
tomatically derived data by showing that sta-
tistical classifiers can differentiate users with
four different mental health disorders: de-
pression, bipolar, post traumatic stress disor-
der and seasonal affective disorder.
? We conduct a LIWC analysis of each dis-
order to measure deviations in each illness
group from a control group, replicating pre-
vious findings for depression and providing
new findings for bipolar, PTSD and SAD.
? We conduct an open-vocabulary analysis that
captures language use relevant to mental
health beyond what is captured with LIWC.
Our results open the door to a range of large scale
analysis of mental health issues using Twitter.
2 Related Work
For a good retrospective and prospective sum-
mary of the role of social media in mental health
research, we refer the reader to De Choudhury
(2013). De Choudhury identifies ways in which
NLP has and can be used on social media data to
produce what the relevant mental health literature
would predict, both at an individual level and a
population level. She proceeds to identify ways
in which these types of analyses can be used in
the near and far term to influence mental health
research and interventions alike.
Differences in language use have been observed
in the personal writing of students who score
highly on depression scales (Rude et al., 2004),
forum posts for depression (Ramirez-Esparza et
al., 2008), self narratives for PTSD (He et al.,
2012; D?Andrea et al., 2011; Alvarez-Conrad et
al., 2001), and chat rooms for bipolar (Kramer
et al., 2004). Specifically in social media, dif-
ferences have previously been observed between
depressed and control groups (as assessed by
internet-administered batteries) via LIWC: de-
pressed users more frequently use first person pro-
nouns (Chung and Pennebaker, 2007) and more
frequently use negative emotion words and anger
words on Twitter, but show no differences in posi-
tive emotion word usage (Park et al., 2012). Simi-
larly, an increase in negative emotion and first per-
son pronouns, and a decrease in third person pro-
nouns, (via LIWC) is observed, as well as many
manifestations of literature findings in the pattern
of life of depressed users (e.g., social engagement,
demographics) (De Choudhury et al., 2013d). Dif-
ferences in language use in social media via LIWC
have also been observed between PTSD and con-
trol groups (Coppersmith et al., 2014).
For population-level analysis, surveys such as
the Behavioral Risk Factor Surveillance System
(BRFSS) are conducted via telephone (Centers
for Disease Control and Prevention (CDC), 2010).
Some of these surveys cover relatively few par-
ticipants (often in the thousands), have significant
cost, and have long delays between data collec-
tion and dissemination of the findings. However,
De Choudhury et al. (2013c) presents a promising
population-level analysis of depression that high-
lights the role of NLP and social media.
3 Data
All data we obtain is public, posted between
2008 and 2013, and made available from Twitter
via their application programming interface (API).
Specifically, this does not include any data that has
52
Genuine Statements of Diagnosis
In loving memory my mom, she was only 42, I was 17 & taken away from me. I was diagnosed with having P.T.S.D LINK
So today I started therapy, she diagnosed me with anorexia, depression, anxiety disorder, post traumatic stress disorder and
wants me to
@USER The VA diagnosed me with PTSD, so I can?t go in that direction anymore
I wanted to share some things that have been helping me heal lately. I was diagnosed with severe complex PTSD and... LINK
Disingenuous Statements of Diagnosis
?I think I?m I?m diagnosed with SAD. Sexually active disorder? -anonymous
LOL omg my bro the ?psychologist? just diagnosed me with seasonal ADHD AHAHAHAAAAAAAAAAA IM DYING.
The winter blues: Yesterday I was diagnosed with seasonal affective disorder. Now, this sounds a lot more dramat... LINK
Table 1: Examples found via regular expression keyword search for diagnosis tweets.
been marked as ?private? by the author or any di-
rect messages.
Diagnosed Group We seek users who publicly
state that they have been diagnosed with various
mental illnesses. Users may make such a state-
ment to seek support from others in their social
network, to fight the taboo of mental illness, or
perhaps as an explanation of some of their behav-
ior. Tweets were obtained using regular expres-
sions on a large multi-year health related collec-
tion, e.g. ?I was diagnosed with X.? We searched
for four conditions: depression, bipolar disorder,
post traumatic stress disorder (PTSD) and sea-
sonal affective disorder (SAD). The matched diag-
nosis tweets were manually labeled as to whether
the tweet contained a genuine statement of a men-
tal health diagnosis. Table 1 shows examples of
both genuine statements of diagnosis and disin-
genuous statements (often jokes or quotes).
Next, we retrieved the most recent tweets (up
to 3200) for each user with a genuine diagnosis
tweet. We then filtered the users to remove those
with fewer than 25 tweets and those whose tweets
were not at least 75% in English (measured using
the Compact Language Detector
1
). These filter-
ing steps left us with users that were considered
positive examples. Table 2 indicates the number
of users and tweets found for each of the mental
health categories examined. We manually exam-
ined and annotated only half the diagnosis state-
ments for depression ? indicating there are likely
800-900 depression users available via these auto-
matic methods from our collection, compared to
the 117 obtained via the methods of De Choud-
hury et al. (2013d). Additionally, we emphasize
the low cost and effort of our automated effort
as compared to their crowdsourced survey meth-
1
https://code.google.com/p/cld2/
ods. The difference in collection methods also
suggests that the two have a reasonable chance of
being complementary. This is especially signif-
icant when considering disorders with lower in-
cidence rates than depression (arguably the high-
est), where respondents to crowdsourced surveys
or self-stated diagnoses alike are rare.
This method is similar in spirit to that of De
Choudhury et al. (2013c), where they inferred
a tweet-level classifier for depression from user-
level labels (specifically, tweets from the past three
months from users scoring highly on CES-D for
the positive class and conversely for the negative).
Control Group To build models for analysis
and to validate the data, we also need a sample of
the general population to use as an approximation
of community controls. We follow a similar pro-
cess: randomly select 10k usernames from a list
of Twitter users who posted to a separate random
historical collection within a selected two week
window, downloaded the 3200 most recent tweets
from these users, and apply our two filters: at least
25 tweets and 75% English. This yields a control
group of 5728 random users, whose 13.7 million
tweets were used as negative examples.
Caveats Our method for finding users with
mental health diagnoses has significant caveats: 1)
the method may only capture a subpopulation of
each disorder (i.e., those who are speaking pub-
licly about what is usually a very private mat-
ter), which may not truly represent all aspects of
the population as a whole. 2) This method in
no way verifies whether this diagnosis is genuine
(i.e., people are not always truthful in self-reports).
However, given the stigma often associated with
mental illness, it seems unlikely users would tweet
that they are diagnosed with a condition they do
not have. 3) The control group is likely contami-
53
Match Users Tweets
Bipolar 6k 394 992k
Depression 5k 441 1.0m
PTSD 477 244 573k
SAD 389 159 421k
Control 10k 5728 13.7m
Table 2: Number of users matching the diagnosis regular
expression, users labeled with genuine diagnoses and tweets
retrieved from diagnosed users for each mental health condi-
tion.
nated by the presence of users that are diagnosed
with the various conditions investigated. We make
no attempt to remove these users, and if we as-
sume that the prevalence of each disorder in the
general population is similar in our control groups,
we likely have hundreds of such diagnosed users
contaminating our control training data. 4) Twitter
users are not an entirely representative sample of
the population as a whole. Despite these caveats,
we find that this method yielded promising results
as discussed in the next sections.
Comorbidity Since some of these disorders
have high comorbidity, there are some users in
more than one class (e.g., those that state a diagno-
sis for PTSD and depression): Bipolar and depres-
sion have 19 users in common (4.8% of the bipo-
lar users, 4.3% of the depression users), PTSD and
depression share 10 (4.0% of PTSD, 2.2% of de-
pression), and bipolar and PTSD share 9 (2.2% of
bipolar, 3.6% of PTSD). Two users state diagnosis
of bipolar, PTSD and depression (less than 1% of
each set). No users stated diagnoses of both SAD
and any other condition investigated.
4 Methods
We quantify various aspects of each user?s lan-
guage usage and pattern of life via automated
methods, extracting features for subsequent ma-
chine learning. We use these to (1) replicate pre-
vious findings, (2) build classifiers to separate di-
agnosed from control users, and (3) introspect on
those classifiers. Introspection here shows us what
quantified signals in the content the classifiers base
their decision on, and thus we can gain intuition
about what signals are present in the content rele-
vant to mental health.
4.1 Linguistic Inquiry Word Count (LIWC)
LIWC provides clinicians with a tool for gather-
ing quantitative data regarding the state of a pa-
tient from the patient?s writing (Pennebaker et al.,
2007). Previous work has found signal in the ?pos-
itive affect? and ?negative affect? categories of the
LIWC when applied to social media (including
Twitter), so we examine their correlations sepa-
rately, as well as in the context of other LIWC
categories (De Choudhury et al., 2013a). In all,
we examine some of the LIWC categories directly
(Swear, Anger, PosEmo, NegEmo, Anx) and com-
bine pronoun classes by linguistic form: I and We
classes are combined to form Pro1, You becomes
Pro2 and SheHe and They become Pro3. Each of
these classes provides one feature used by subse-
quent machine learning and our other analyses.
4.2 Language Models (LMs)
Language models are commonly used to estimate
how likely a given sequence of words is. Gener-
ally, an n-gram language model refers to a model
that examines strings of up to n words long. This
is less than ideal for applications in social me-
dia: spelling errors, shortenings, space removal,
and other aspects of social media data (especially
Twitter) confounds many traditional word-based
approaches. Thus, we employ two LMs, first a
traditional 1-gram LM (ULM) that examines the
probability of each whole word. Second, a char-
acter 5-gram LM (CLM) to examine sequences of
up to 5 characters.
LMs model the likelihood of sequences from
training data. In our case, we build one of each
model from the positive class (tweets from one
class of diagnosed users ? e.g., PTSD), yield-
ing ULM
+
and CLM
+
. We also build one of
each model from the negative class (control users),
yielding ULM
?
and CLM
?
. We score each tweet
by computing these probabilities and classifying it
according to which model has a higher probability
(e.g., for a given tweet, is ULM
+
> ULM
?
?).
4.3 Pattern of Life Analytics
For brevity, we only briefly discuss the pattern of
life analytics, since they do not depend on sig-
nificant NLP. They examine how correlates found
to be significant in the mental health literature
may manifest and be measured in social media
data. These are all imperfect proxies for the find-
ings from the literature, but our experiments will
demonstrate that they do collectively provide in-
formation relevant to mental health.
For each of the following analytics we extract
one feature to use in subsequent machine learn-
ing. Social engagement has been correlated with
54
??
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?? ??
?
?
?
?
?
?
??
??
???
?
?
?
?
?
??
?
?
?
?
?
?
??
??
?
?
?
??? ??
?
? ?
?
??
?
? ?
???
?
?
????
?
???
?
?
?
???
?
?? ?
?
??
? ?
??
?
?
?
??
??
?
?
?
?
?????
?
?
?
?
?
?
?
?
??
?
?
?
??
?
?
?
?
?
?
?
?
?
???
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
????
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
0.00
0.05
0.10
0.15 Pro1* * Pro2* Pro3* Swear* Anger* PosEmo NegEmo* Anxiety****
0
0.00
5
0.01
0.01
5
Figure 1: Box and whiskers plot of proportion of tweets each user has (y-axis) matching various LIWC categories. Each
bar represents one LIWC category for one condition ? PTSD in purple, depression in blue, SAD in orange, bipolar in red and
control in gray. Anxiety occurs an order of magnitude less often than the others, so its proportion is on the right y-axis (and thus
not comparable to the others). Statistically significant deviations from control users are denoted by asterisks.
positive mental health outcomes (Greetham et al.,
2011; Berkman et al., 2000; Organization, 2001;
De Choudhury et al., 2013d), which is difficult
to measure directly so we examine various ways
in which this may be manifest in a user?s tweet
stream: Tweet rate measures how often a twit-
ter user posts (a measure of overall engagement
with this social media platform) and Proportion
of tweets with @mentions measures how often
a user posts ?in conversation? (for lack of better
terms) with other users. Number of @mentions is
a measure of how often the user in question en-
gages other users, while Number of self @men-
tions is a measure of how often the user responds
to mentions of themselves (since users rarely in-
clude their own username in a tweet). To estimate
the size of a user?s social network, we calculate
Number of unique users @mentioned and Number
of users @mentioned at least 3 times, respectively.
For each of the following analytics, we calcu-
late the proportion of a user?s tweets that the ana-
lytic finds evidence in: Insomnia and sleep distur-
bance is often a symptom of mental health disor-
ders (Weissman et al., 1996; De Choudhury et al.,
2013d), so we calculate the proportion of tweets
that a user makes between midnight and 4am ac-
cording to their local timezone. Exercise has
also been correlated with positive mental health
outcomes (Penedo and Dahn, 2005; Callaghan,
2004), so we examine tweets mentioning one of a
small set of exercise-related terms. We also use an
English sentiment analysis lexicon from Mitchell
et al. (2013) to score individual tweets according
to the presence and valence of sentiment words.
We apply no thresholds, so any tweet with a senti-
ment score above 0 was considered positive, below
0 was considered negative, and those with score 0
were considered to have no sentiment. Thus we
use the proportion of Insomnia, Exercise, Positive
Sentiment and Negative Sentiment tweets as fea-
tures in subsequent machine learning and analysis.
5 Results
We present three types of experiments to evalu-
ate the quality and character of these data, and to
demonstrate some quantifiable mental health sig-
nals in Twitter. First, we validate our method for
obtaining data by replicating previous findings us-
ing LIWC. Next, we build classifiers to distinguish
each group from the control group, demonstrating
that there is useful signal in the language of each
group, and compare these classifiers. Finally, we
analyze the correlations between our analytics and
classifiers to uncover relationships between them
and derive insight into quantifiable and relevant
mental health signals in Twitter.
Validation First, we provide some validation
for our novel method for gathering samples. We
demonstrate that language use, as measured by
LIWC, is statistically significantly different be-
tween control and diagnosed users. Figure 1
shows the proportion of tweets from each user
that scores positively on various LIWC categories
(i.e., have at least one word from that category).
Box-and-whiskers plots (Tukey, 1977)
2
summa-
rize a distribution of observations and ease com-
2
For a modern implementation see Wickham (2009).
55
False Alarm: 0.1 0.2
Bipolar 0.64 0.82
Depression 0.48 0.68
PTSD 0.67 0.81
SAD 0.42 0.65
Figure 2: ROC curves for separating diagnosed from con-
trol users, compared across disorders: bipolar in red, depres-
sion in blue, PTSD in purple, SAD in orange. The preci-
sion (diagnosed, correctly labeled) for each disorder at false
alarm (control, labeled as diagnosed) rates of 10% and 20%
are shown to the right of the ROC curve. Chance performance
is indicated by the dotted black line.
parison between them (here, each observation is
the proportion of a user?s tweets that score posi-
tively on LIWC). The median of the distribution
is the black horizontal line in the middle of the
bar, the bar covers the inter quartile range (where
50% of the observations lie), the whiskers are a
robust estimate of the extent of the data, with out-
liers plotted as circles beyond the whiskers. An
approximation of statistical significance is indi-
cated by the pinched in notches on each bar. If
the notches on the bars do not overlap, the dif-
ferences between those distributions is different
(?<0.05, 95% confidence interval). Each bar is
colored according to diagnosis, and each group
of 5 bars notes the scores for one LIWC cat-
egory. Differences that reach statistical signifi-
cance from the control group are noted with as-
terisks (e.g., Pro1, Swear, Anger, NegEmo and
Anxiety are statistically significantly different for
the depression group). Importantly, this repli-
cates previous findings of significant differences
between depressed users (according to an internet-
administered diagnostic battery): significant in-
creases are expected in NegEmo, Anger, Pro1 and
Pro3 and no change in PosEmo, given all previous
work (Park et al., 2012; Chung and Pennebaker,
2007; De Choudhury et al., 2013d). We repli-
cate all these findings except the increase in Pro3
(which only De Choudhury et al. (2013d) found),
which validates our data collection methods.
Classification We next explore the ability of
the various analytics to separate diagnosed from
control users and assess performance on a leave-
one-out cross-validation task. We train a log lin-
ear classifier on the features described in ?4 using
scikit-learn (Pedregosa et al., 2011).
Bipolar Depression
PTSD SAD
Figure 3: ROC curves of performance of individual analyt-
ics for each disorder: LIWC in blue, pattern of life in yellow,
CLM in red, ULM in green, all in black. Chance performance
is indicated by the dotted black line.
The receiver operating characteristic (ROC)
curves in Figures 2 and 3 demonstrate perfor-
mance of the various classifiers at the task of sepa-
rating diagnosed from control groups. In all cases,
the correct detections (or hits) are on the y-axis
and the false detections (or false alarms) are on
the x-axis. Figure 2 compares performance across
diagnoses, one line per disorder.
Figure 3 shows one plot per mental health con-
dition, with the performance of the various an-
alytics, individually and in concert as individual
ROC curves. A few trends emerge ? 1) All an-
alytics show some ability to separate the classes,
indicating they are finding useful signals. 2) The
LMs provide superior performance to the other an-
alytics, indicating there are more signals present
in the language than are captured by LIWC and
pattern-of-life analytics. For readability we do not
show the performance of all combinations of an-
alytics, but they perform as expected: any set of
them perform equal to or better than their indi-
vidual components. Taken together, this indicates
that there is information relevant to separating di-
agnosed users from controls in all the analytics
discussed here. Furthermore, this highlights that
there remains significant signals to be uncovered
and understood in the language of social media.
These trends also allow us to compare the dis-
orders as manifest in language usage, though this
56
tends to raise more questions than it answers. Gen-
erally, the pattern-of-life analytics and LIWC are
on par, but this is decidedly not true for depres-
sion, where pattern-of-life seems to perform espe-
cially poorly, and for SAD, where pattern-of-life
seems to perform especially well. This indicates
that the depression users have patterns-of-life that
look more similar to the controls than is the case
for the other disorders (perhaps especially surpris-
ing given the inclusion of the sentiment lexicon)
and that there may be significant correlation be-
tween pattern-of-life factors and SAD.
5.1 Analytic Introspection
To examine correlations between the analytics and
the linguistic content they depend on, we scored
a random subset of 1 million tweets from control
users with each of the linguistic analytics, and plot
their Pearson?s correlation coefficients (r) in Fig-
ure 4. A simple overlap of wordlists is not suf-
ficient to assess the true utility of these methods
since it does not take into account the frequency
of occurrence of each word, nor the correlation be-
tween these words in real data (e.g., does a classi-
fier based on the LIWC category Swear provide
redundant information to the sentiment analysis).
Each row and column in Figure 4 represents one of
the 17 analytics, in the same order. Colors denote
Bonferroni-corrected Pearson?s r for statistically
significant correlations between the analytic on the
row and column. Correlations that do not reach
statistical significance are in aquamarine (corre-
sponding to r=0). Excluded for brevity is a sanity
check of a ?
2
test between the analytics to assert
they were scoring significantly differently.
The strong correlations between the various
LIWC analytics, notably Swear, Anger and
NegEmo, likely indicates that the analytics are
triggered by the same word(s) ? in this case pro-
fanity. Similarly for LIWC?s PosEmo and the sen-
timent lexicon ? ?happy? for example. The corre-
lation between CLM for various diagnoses is par-
ticularly intriguingly, as it is in line with known
patterns of comorbidity: major depressive disor-
der, PTSD, and bipolar all have observed comor-
bidity (Brady et al., 2000; Campbell et al., 2007;
McElroy et al., 2001) while SAD is currently con-
sidered a specifier of major depressive disorder or
bipolar disorder (American Psychiatric Associa-
tion, 2013; Lurie et al., 2006), without published
findings indicating comorbidity. Indeed our small
Figure 4: Pearson?s r correlations between various analyt-
ics, color indicates the strength of statistically significant cor-
relations, or 0 (aquamarine) otherwise. Bonferroni corrected,
each comparison is significant only if ?<0.0002). Rows and
columns represent the analytics in the same order, so the di-
agonal is self-correlation.
sample dataset follows the same trends, where
we observed users with multiple diagnoses exist
within depression, PTSD, and bipolar, but none
exist with SAD. The correlation observed is too
large to be solely attributed to those users shared
between the groups, though (correlations at most
r = 0.05 would be attributable to that alone). Fur-
thermore, when taken in combination with the dif-
ferent patterns exhibited by the groups as seen in
Figure 1, this correlation is not solely attributable
to LIWC categories either. At its core, these cor-
relations seem to suggest that similar language
is employed by users diagnosed with these occa-
sionally comorbid disorders, and dissimilar lan-
guage by users with SAD. This should be taken as
merely suggestive of the type of analysis one could
do, though, since the literature does not present a
strong and clear prediction for the comorbidity and
exhibited symptoms (to include language use).
Interestingly, the lack of (or negative) correla-
tion between most of the analytics again highlights
the complexity of the mental illnesses and the di-
vergent signals it presents. Additionally, the lack
of correlation between ULM and the other models
is to be expected, since they are basing their scores
on significantly more words (or different signals as
is the case for CLM). Each one of these analytics is
highly imperfect, and often give contradictory ev-
idence, but when combined, the machine learning
algorithms are able to sort through the conflicting
signals with some success.
57
Analytic Example Tweet Text
Bipolar LM I?m insecure because being around your ex of 4 years little sister, makes me feel a slight bit uncom-
fortable. Ok.
Depression LM Pain has a weird way of working. You?re still the same person from before the pain, but that person is
underneath & doesn?t come out.
PTSD LM Don?t wanna get out my bed but I really need to get up & prepare myself for work
Sentiment(+) NAME is absolutely unbelievable, he just gets better and better every time I see him. The best play in
the world, no doubt about it.
Sentiment(-) I hate losing people in my life. I try so hard to not let it happen
PosEmo Wowee...that was a hectic day... Got more done than expected but so glad to be in bed now. Grateful
for my supportive husband & loving pooch
Functioning if i had a dollar for all the grammatical errors ive ever typed, my college tuition, book cost, and dorm
rent would be paid in full
NegEmo My tooth hurts, my neck hurts, my mouth hurts, my toungue hurts, my head hurts...kill me now.
Anx don?t stress over someone who is going to stress over you..
Anger Ugly n arrogant sums everytin up.shdnt hv ffd her seff
Table 3: Example high scoring tweets from each analytic.
6 Conclusion
We demonstrate quantifiable signals in Twitter
data relevant to bipolar disorder, major depres-
sive disorder, post-traumatic-stress disorder and
seasonal affective disorder. We introduce a novel
method for automatic data collection and validate
its veracity by 1) replicating observations of sig-
nificant differences between depressed and control
user groups and 2) constructing classifiers capa-
ble of separating diagnosed from control users for
each disorder. This data allows us to demonstrate
equivalent differences in language use (according
to LIWC) for bipolar, PTSD, and SAD. Further-
more, we provide evidence that more information
relevant to mental health is encoded in language
use in social media (above and beyond that cap-
tured by methods based on the mental health lit-
erature). By examining correlations between the
various analytics investigated, we provide some
insight into what quantifiable linguistic informa-
tion is captured by our classifiers. We finally
demonstrate the utility of examining multiple dis-
orders simultaneously and other larger analyses,
difficult or impossible with other methods.
Crucially, we expect that these novel data col-
lection methods can provide complementary infor-
mation to existing survey-based methods, rather
than supplant them. For many disorders rarer
than depression (which has comparatively high in-
cidence rates), we suspect that finding any data
will be a challenge, in which case combining
these methods with the existing survey collection
methods may be the best way to obtain sufficient
amounts of data for statistical analyses.
Since the LMs take more information into ac-
count when modeling the language usage of di-
agnosed and control users, it is unsurprising that
they outperform LIWC and pattern-of-life analy-
ses alone, but this is evidence of as-of-yet undis-
covered linguistic differences between diagnosed
and control users for all disorders investigated.
Uncovering and interpreting these signals can be
best accomplished through collaboration between
NLP and mental health researchers.
Naturally, some caveats come with these re-
sults: while identifying genuine self-statements of
diagnosis in Twitter works well for some condi-
tions, others exist for which there were few or
no diagnoses stated. For Alzheimer?s, the demo-
graphic with the majority of diagnoses does not
frequently use Twitter (or likely any social me-
dia). Eating disorders are also elusive via this
method, though related automatic methods (e.g.,
using disorder-related hashtags) may address this.
Finally, those willing to publicly reveal a mental
health diagnosis may not be representative of the
population suffering from that mental illness.
All these experiments, taken together, indicate
that there are a diverse set of quantifiable signals
relevant to mental health observable in Twitter.
They indicate that individual- and population-level
analyses can be made cheaper and more timely
than current methods, yet there remains as-of-yet
untapped information encoded in language use ?
promising a rich collaboration between the fields
of natural language processing and mental health.
Acknowledgments: The authors would like to
thank Kristy Hollingshead for thoughtful com-
ments and contributions throughout this research.
58
References
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Jennifer Alvarez-Conrad, Lori A. Zoellner, and
Edna B. Foa. 2001. Linguistic predictors of trauma
pathology and physical health. Applied Cognitive
Psychology, 15(7):S159?S170.
American Psychiatric Association. 2013. Diagnostic
Statistical Manual 5. American Psychiatric Associ-
ation.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza
epidemics using twitter. In Empirical Natural Lan-
guage Processing Conference (EMNLP).
John W. Ayers, Benjamin M. Althouse, Jon-Patrick
Allem, J. Niels Rosenquist, and Daniel E. Ford.
2013. Seasonality in seeking mental health infor-
mation on google. American journal of preventive
medicine, 44(5):520?525.
John W. Ayers, Benjamin M. Althouse, and Mark
Dredze. 2014. Could behavioral medicine lead the
web data revolution? Journal of the American Med-
ical Association (JAMA), February 27.
Lisa F. Berkman, Thomas Glass, Ian Brissette, and
Teresa E. Seeman. 2000. From social integration
to health: Durkheim in the new millennium? Social
Science & Medicine, 51(6):843?857, September.
Amber Boydstun, Rebecca Glazier, Timothy Jurka, and
Matthew Pietryka. 2013. Examining debate effects
in real time: A report of the 2012 React Labs: Ed-
ucate study. The Political Communication Report,
23(1), February. [Online; accessed 25-February-
2014].
Kathleen T. Brady, Therese K. Killeen, Tim Brewerton,
and Sylvia Lucerini. 2000. Comorbidity of psy-
chiatric disorders and posttraumatic stress disorder.
Journal of Clinical Psychiatry.
Patrick Callaghan. 2004. Exercise: a neglected inter-
vention in mental health care? Journal of Psychi-
atric and Mental Health Nursing, 11:476?483.
Duncan G. Campbell, Bradford L. Felker, Chuan-Fen
Liu, Elizabeth M. Yano, JoAnn E. Kirchner, Domin
Chan, Lisa V. Rubenstein, and Edmund F. Chaney.
2007. Prevalence of depression-PTSD comorbidity:
Implications for clinical practice guidelines and pri-
mary care-based interventions. Journal of General
Internal Medicine, 22(6):711?718.
Centers for Disease Control and Prevention (CDC).
2010. Behavioral risk factor surveillance system
survey data.
Cindy Chung and James Pennebaker. 2007. The psy-
chological functions of function words. Social com-
munication, pages 343?359.
Glen A. Coppersmith, Craig T. Harman, and Mark
Dredze. 2014. Measuring post traumatic stress
disorder in Twitter. In Proceedings of the Interna-
tional AAAI Conference on Weblogs and Social Me-
dia (ICWSM).
Wendy D?Andrea, Pearl H. Chiu, Brooks R. Casas,
and Patricia Deldin. 2011. Linguistic predictors of
post-traumatic stress disorder symptoms following
11 September 2001. Applied Cognitive Psychology,
26(2):316?323, October.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013a. Major life changes and behav-
ioral markers in social media: Case of childbirth. In
Proceedings of the ACM Conference on Computer
Supported Cooperative Work and Social Computing
(CSCW).
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013b. Predicting postpartum changes in
emotion and behavior via social media. In Proceed-
ings of the ACM Annual Conference on Human Fac-
tors in Computing Systems (CHI), pages 3267?3276.
ACM.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013c. Social media as a measurement
tool of depression in populations. In Proceedings of
the Annual ACM Web Science Conference.
Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013d. Predicting de-
pression via social media. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).
Munmun De Choudhury, Andres Monroy-Hernandez,
and Gloria Mark. 2014. ? narco? emotions: Affect
and desensitization in social media during the mexi-
can drug war.
Munmun De Choudhury. 2013. Role of social media
in tackling challenges in mental health. In Proceed-
ings of the 2nd International Workshop on Socially-
Aware Multimedia, pages 49?52.
Mark Dredze. 2012. How social media will change
public health. IEEE Intelligent Systems, 27(4):81?
84.
Danica Vukadinovic Greetham, Robert Hurling,
Gabrielle Osborne, and Alex Linley. 2011. Social
networks and positive and negative affect. Procedia
- Social and Behavioral Sciences, 22:4?13, January.
Carleen Hawn. 2009. Take Two Aspirin And Tweet
Me In The Morning: How Twitter, Facebook, And
Other Social Media Are Reshaping Health Care.
Health Affairs, 28(2):361?368.
59
Qiwei He, Bernard P. Veldkamp, and Theo de Vries.
2012. Screening for posttraumatic stress disorder
using verbal features in self narratives: A text min-
ing approach. Psychiatry Research.
Adam D. I. Kramer, Susan R. Fussell, and Leslie D.
Setlock. 2004. Text analysis as a tool for analyz-
ing conversation in online support groups. In Pro-
ceedings of the ACM Annual Conference on Human
Factors in Computing Systems (CHI).
Stephen J. Lurie, Barbara Gawinski, Deborah Pierce,
and Sally J. Rousseau. 2006. Seasonal affective dis-
order. American family physician, 74(9).
Susan L. McElroy, Lori L. Altshuler, Trisha Suppes,
Paul E. Keck, Mark A. Frye, Kirk D. Denicoff,
Willem A. Nolen, Ralph W. Kupka, Gabriele S. Lev-
erich, Jennifer R. Rochussen, A. John Rush Rush,
and Robert M. Post Post. 2001. Axis I psychi-
atric comorbidity and its relationship to historical ill-
ness variables in 288 patients with bipolar disorder.
American Journal of Psychiatry, 158(3):420?426.
Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-
son, and Benjamin Van Durme. 2013. Open domain
targeted sentiment. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1643?1654.
World Health Organization. 2001. The world health
report 2001 - Mental health: New understanding,
new hope. Technical report, Genf, Schweiz.
Minsu Park, Chiyoung Cha, and Meeyoung Cha. 2012.
Depressive moods of users portrayed in Twitter. In
Proceedings of the ACM SIGKDD Workshop on
Healthcare Informatics (HI-KDD).
Michael J. Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing Twitter for public health. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
and Matthieu Perrot
?
Edouard Duchesnay. 2011.
scikit-learn: Machine learning in Python. The Jour-
nal of Machine Learning Research, 12:2825?2830.
Frank J. Penedo and Jason R. Dahn. 2005. Exer-
cise and well-being: a review of mental and phys-
ical health benefits associated with physical activ-
ity. Current Opinion in Psychiatry, 18(2):189?193,
March.
James W. Pennebaker, Cindy K. Chung, Molly Ire-
land, Amy Gonzales, and Roger J. Booth. 2007.
The development and psychometric properties of
LIWC2007.
Nairan Ramirez-Esparza, Cindy K. Chung, Ewa
Kacewicz, and James W. Pennebaker. 2008. The
psychology of word use in depression forums in En-
glish and in Spanish: Testing two text analytic ap-
proaches. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM).
Philip Resnik, Anderson Garron, and Rebecca Resnik.
2013. Using topic modeling to improve prediction
of neuroticism and depression. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural, pages 1348?1353.
Stephanie S. Rude, Eva-Maria Gortner, and James W.
Pennebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133, December.
H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M.
Ramones, Megha Agrawal, Achal Shah, Michal
Kosinski, David Stillwell, Martin E. P. Seligman,
and Lyle H. Ungar. 2013. Personality, gender,
and age in the language of social media: The open-
vocabulary approach. PLOS One, 8(9).
Yla R. Tausczik and James W. Pennebaker. 2010. The
psychological meaning of words: LIWC and com-
puterized text analysis methods. Journal of Lan-
guage and Social Psychology, 29(1):24?54.
John W. Tukey. 1977. Box-and-whisker plots. Ex-
ploratory Data Analysis, pages 39?43.
Myrna M. Weissman, Roger C. Bland, Glorisa J.
Canino, Carlo Faravelli, Steven Greenwald, Hai-
Gwo Hwu, Peter R. Joyce, Eile G. Karam, Chung-
Kyoon Lee, Joseph Lellouch, Jean-Pierre L?epine,
Stephen C. Newman, Maritza Rubio-Stipec, J. Elis-
abeth Wells, Priya J. Wickramaratne, Hans-Ulrich
Wittchen, and Eng-Kung Yeh. 1996. Cross-national
epidemiology of major depression and bipolar dis-
order. Journal of the American Medical Association
(JAMA), 276(4):293?299.
Hadley Wickham. 2009. ggplot2: elegant graphics for
data analysis. Springer.
60
