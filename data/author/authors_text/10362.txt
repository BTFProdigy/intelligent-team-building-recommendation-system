Determining the Unithood of Word Sequences using a Probabilistic
Approach
Wilson Wong, Wei Liu and Mohammed Bennamoun
School of Computer Science and Software Engineering
University of Western Australia
Crawley WA 6009
{wilson,wei,bennamou}@csse.uwa.edu.au
Abstract
Most research related to unithood were con-
ducted as part of a larger effort for the deter-
mination of termhood. Consequently, nov-
elties are rare in this small sub-field of term
extraction. In addition, existing work were
mostly empirically motivated and derived.
We propose a new probabilistically-derived
measure, independent of any influences of
termhood, that provides dedicated measures
to gather linguistic evidence from parsed
text and statistical evidence from Google
search engine for the measurement of unit-
hood. Our comparative study using 1, 825
test cases against an existing empirically-
derived function revealed an improvement in
terms of precision, recall and accuracy.
1 Introduction
Automatic term recognition, also referred to as term
extraction or terminology mining, is the process of
extracting lexical units from text and filtering them
for the purpose of identifying terms which charac-
terise certain domains of interest. This process in-
volves the determination of two factors: unithood
and termhood. Unithood concerns with whether or
not a sequence of words should be combined to
form a more stable lexical unit. On the other hand,
termhood measures the degree to which these sta-
ble lexical units are related to domain-specific con-
cepts. Unithood is only relevant to complex terms
(i.e. multi-word terms) while termhood (Wong et
al., 2007a) deals with both simple terms (i.e. single-
word terms) and complex terms. Recent reviews by
(Wong et al, 2007b) show that existing research on
unithood are mostly carried out as a prerequisite to
the determination of termhood. As a result, there
is only a small number of existing measures dedi-
cated to determining unithood. Besides the lack of
dedicated attention in this sub-field of term extrac-
tion, the existing measures are usually derived from
term or document frequency, and are modified as
per need. As such, the significance of the different
weights that compose the measures usually assume
an empirical viewpoint. Obviously, such methods
are at most inspired by, but not derived from formal
models (Kageura and Umino, 1996).
The three objectives of this paper are (1) to sepa-
rate the measurement of unithood from the determi-
nation of termhood, (2) to devise a probabilistically-
derived measure which requires only one thresh-
old for determining the unithood of word se-
quences using non-static textual resources, and (3)
to demonstrate the superior performance of the new
probabilistically-derived measure against existing
empirical measures. In regards to the first objective,
we will derive our probabilistic measure free from
any influence of termhood determination. Follow-
ing this, our unithood measure will be an indepen-
dent tool that is applicable not only to term extrac-
tion, but many other tasks in information extraction
and text mining. Concerning the second objective,
we will devise our new measure, known as the Odds
of Unithood (OU), which are derived using Bayes
Theorem and founded on a few elementary probabil-
ities. The probabilities are estimated using Google
page counts in an attempt to eliminate problems re-
lated to the use of static corpora. Moreover, only
103
one threshold, namely, OUT is required to control
the functioning of OU . Regarding the third objec-
tive, we will compare our new OU against an ex-
isting empirically-derived measure called Unithood
(UH) (Wong et al, 2007b) in terms of their preci-
sion, recall and accuracy.
In Section 2, we provide a brief review on some of
existing techniques for measuring unithood. In Sec-
tion 3, we present our new probabilistic approach,
the measures involved, and the theoretical and in-
tuitive justification behind every aspect of our mea-
sures. In Section 4, we summarize some findings
from our evaluations. Finally, we conclude this pa-
per with an outlook to future work in Section 5.
2 Related Works
Some of the most common measures of unithood
include pointwise mutual information (MI) (Church
and Hanks, 1990) and log-likelihood ratio (Dunning,
1994). In mutual information, the co-occurrence fre-
quencies of the constituents of complex terms are
utilised to measure their dependency. The mutual
information for two words a and b is defined as:
MI(a, b) = log
2
p(a, b)
p(a)p(b) (1)
where p(a) and p(b) are the probabilities of occur-
rence of a and b. Many measures that apply sta-
tistical techniques assuming strict normal distribu-
tion, and independence between the word occur-
rences (Franz, 1997) do not fare well. For handling
extremely uncommon words or small sized corpus,
log-likelihood ratio delivers the best precision (Kurz
and Xu, 2002). Log-likelihood ratio attempts to
quantify how much more likely one pair of words is
to occur compared to the others. Despite its poten-
tial, ?How to apply this statistic measure to quan-
tify structural dependency of a word sequence re-
mains an interesting issue to explore.? (Kit, 2002).
(Seretan et al, 2004) tested mutual information, log-
likelihood ratio and t-tests to examine the use of re-
sults from web search engines for determining the
collocational strength of word pairs. However, no
performance results were presented.
(Wong et al, 2007b) presented a hybrid approach
inspired by mutual information in Equation 1, and
C-value in Equation 3. The authors employ Google
page counts for the computation of statistical evi-
dences to replace the use of frequencies obtained
from static corpora. Using the page counts, the au-
thors proposed a function known as Unithood (UH)
for determining the mergeability of two lexical units
ax and ay to produce a stable sequence of words s.
The word sequences are organised as a set W =
{s, ax, ay} where s = axbay is a term candidate,
b can be any preposition, the coordinating conjunc-
tion ?and? or an empty string, and ax and ay can
either be noun phrases in the form Adj?N+ or an-
other s (i.e. defining a new s in terms of other s).
The authors define UH as:
UH(ax, ay) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1 if (MI(ax, ay) > MI+) ?
(MI+ ? MI(ax, ay)
? MI??
ID(ax, s) ? IDT ?
ID(ay, s) ? IDT ?
IDR+ ? IDR(ax, ay)
? IDR?)
0 otherwise
(2)
where MI+, MI?, IDT , IDR+ and IDR?
are thresholds for determining mergeability deci-
sions, and MI(ax, ay) is the mutual information be-
tween ax and ay, while ID(ax, s), ID(ay, s) and
IDR(ax, ay) are measures of lexical independence
of ax and ay from s. For brevity, let z be either ax or
ay, and the independence measure ID(z, s) is then
defined as:
ID(z, s) =
{
log
10
(nz ? ns) if(nz > ns)
0 otherwise
where nz and ns is the Google page count for z and
s respectively. On the other hand, IDR(ax, ay) =
ID(ax,s)
ID(ay ,s) . Intuitively, UH(ax, ay) states that the two
lexical units ax and ay can only be merged in two
cases, namely, 1) if ax and ay has extremely high
mutual information (i.e. higher than a certain thresh-
old MI+), or 2) if ax and ay achieve average mu-
tual information (i.e. within the acceptable range of
two thresholds MI+ and MI?) due to both of their
extremely high independence (i.e. higher than the
threshold IDT ) from s.
(Frantzi, 1997) proposed a measure known as
Cvalue for extracting complex terms. The measure
104
is based upon the claim that a substring of a term
candidate is a candidate itself given that it demon-
strates adequate independence from the longer ver-
sion it appears in. For example, ?E. coli food poi-
soning?, ?E. coli? and ?food poisoning? are accept-
able as valid complex term candidates. However,
?E. coli food? is not. Therefore, some measures
are required to gauge the strength of word combina-
tions to decide whether two word sequences should
be merged or not. Given a word sequence a to be
examined for unithood, the Cvalue is defined as:
Cvalue(a) =
{
log
2
|a|fa if |a| = g
log
2
|a|(fa ?
?
l?La fl
|La| ) otherwise
(3)
where |a| is the number of words in a, La is the
set of longer term candidates that contain a, g is
the longest n-gram considered, fa is the frequency
of occurrence of a, and a /? La. While certain re-
searchers (Kit, 2002) consider Cvalue as a termhood
measure, others (Nakagawa and Mori, 2002) accept
it as a measure for unithood. One can observe that
longer candidates tend to gain higher weights due to
the inclusion of log
2
|a| in Equation 3. In addition,
the weights computed using Equation 3 are purely
dependent on the frequency of a.
3 A Probabilistically-derived Measure for
Unithood Determination
We propose a probabilistically-derived measure for
determining the unithood of word pairs (i.e. po-
tential term candidates) extracted using the head-
driven left-right filter (Wong, 2005; Wong et al,
2007b) and Stanford Parser (Klein and Manning,
2003). These word pairs will appear in the form of
(ax, ay) ? A with ax and ay located immediately
next to each other (i.e. x + 1 = y), or separated
by a preposition or coordinating conjunction ?and?
(i.e. x + 2 = y). Obviously, ax has to appear before
ay in the sentence or in other words, x < y for all
pairs where x and y are the word offsets produced by
the Stanford Parser. The pairs in A will remain as
potential term candidates until their unithood have
been examined. Once the unithood of the pairs in
A have been determined, they will be referred to as
term candidates. Formally, the unithood of any two
lexical units ax and ay can be defined as
Definition 1 The unithood of two lexical units is the
?degree of strength or stability of syntagmatic com-
binations and collocations? (Kageura and Umino,
1996) between them.
It is obvious that the problem of measuring the
unithood of any pair of words is the determination
of their ?degree? of collocational strength as men-
tioned in Definition 1. In practical terms, the ?de-
gree? mentioned above will provide us with a way to
determine if the units ax and ay should be combined
to form s, or left alone as separate units. The collo-
cational strength of ax and ay that exceeds a certain
threshold will demonstrate to us that s is able to form
a stable unit and hence, a better term candidate than
ax and ay separated. It is worth pointing that the
size (i.e. number of words) of ax and ay is not lim-
ited to 1. For example, we can have ax=?National
Institute?, b=?of? and ay=?Allergy and Infectious
Diseases?. In addition, the size of ax and ay has no
effect on the determination of their unithood using
our approach.
As we have discussed in Section 2, most of
the conventional practices employ frequency of oc-
currence from local corpora, and some statistical
tests or information-theoretic measures to determine
the coupling strength between elements in W =
{s, ax, ay}. Two of the main problems associated
with such approaches are:
? Data sparseness is a problem that is well-
documented by many researchers (Keller et al,
2002). It is inherent to the use of local corpora
that can lead to poor estimation of parameters
or weights; and
? Assumption of independence and normality of
word distribution are two of the many problems
in language modelling (Franz, 1997). While
the independence assumption reduces text to
simply a bag of words, the assumption of nor-
mal distribution of words will often lead to in-
correct conclusions during statistical tests.
As a general solution, we innovatively employ re-
sults from web search engines for use in a proba-
bilistic framework for measuring unithood.
As an attempt to address the first problem, we
utilise page counts by Google for estimating the
probability of occurrences of the lexical units in W .
105
We consider the World Wide Web as a large general
corpus and the Google search engine as a gateway
for accessing the documents in the general corpus.
Our choice of using Google to obtain the page count
was merely motivated by its extensive coverage. In
fact, it is possible to employ any search engines on
the World Wide Web for this research. As for the
second issue, we attempt to address the problem of
determining the degree of collocational strength in
terms of probabilities estimated using Google page
count. We begin by defining the sample space, N as
the set of all documents indexed by Google search
engine. We can estimate the index size of Google,
|N | using function words as predictors. Function
words such as ?a?, ?is? and ?with?, as opposed to
content words, appear with frequencies that are rel-
atively stable over many different genres. Next, we
perform random draws (i.e. trial) of documents from
N . For each lexical unit w ? W , there will be a cor-
responding set of outcomes (i.e. events) from the
draw. There will be three basic sets which are of
interest to us:
Definition 2 Basic events corresponding to each
w ? W :
? X is the event that ax occurs in the document
? Y is the event that ay occurs in the document
? S is the event that s occurs in the document
It should be obvious to the readers that since the doc-
uments in S have to contain all two units ax and ay,
S is a subset of X ? Y or S ? X ? Y . It is worth
noting that even though S ? X ? Y , it is highly
unlikely that S = X ? Y since the two portions
ax and ay may exist in the same document without
being conjoined by b. Next, subscribing to the fre-
quency interpretation of probability, we can obtain
the probability of the events in Definition 2 in terms
of Google page count:
P (X) = nx|N | (4)
P (Y ) = ny|N |
P (S) = ns|N |
where nx, ny and ns is the page count returned as
the result of Google search using the term [+?ax?],
[+?ay?] and [+?s?], respectively. The pair of
quotes that encapsulates the search terms is the
phrase operator, while the character ?+? is the re-
quired operator supported by the Google search en-
gine. As discussed earlier, the independence as-
sumption required by certain information-theoretic
measures and other Bayesian approaches may not al-
ways be valid, especially when we are dealing with
linguistics. As such, P (X ? Y ) 6= P (X)P (Y )
since the occurrences of ax and ay in documents are
inevitably governed by some hidden variables and
hence, not independent. Following this, we define
the probabilities for two new sets which result from
applying some set operations on the basic events in
Definition 2:
P (X ? Y ) = nxy|N | (5)
P (X ? Y \ S) = P (X ? Y )? P (S)
where nxy is the page count returned by Google
for the search using [+?ax? +?ay?]. Defining
P (X?Y ) in terms of observable page counts, rather
than a combination of two independent events will
allow us to avoid any unnecessary assumption of in-
dependence.
Next, referring back to our main problem dis-
cussed in Definition 1, we are required to estimate
the strength of collocation of the two units ax and
ay. Since there is no standard metric for such mea-
surement, we propose to address the problem from
a probabilistic perspective. We introduce the proba-
bility that s is a stable lexical unit given the evidence
s possesses:
Definition 3 Probability of unithood:
P (U |E) = P (E|U)P (U)P (E)
where U is the event that s is a stable lexical unit
and E is the evidences belonging to s. P (U |E) is
the posterior probability that s is a stable unit given
the evidence E. P (U) is the prior probability that s
is a unit without any evidence, and P (E) is the prior
probability of evidences held by s. As we shall see
later, these two prior probabilities will be immaterial
in the final computation of unithood. Since s can
either be a stable unit or not, we can state that,
P (U? |E) = 1? P (U |E) (6)
106
where U? is the event that s is not a stable lexical unit.
Since Odds = P/(1 ? P ), we multiply both sides
of Definition 3 by (1? P (U |E))?1 to obtain,
P (U |E)
1? P (U |E) =
P (E|U)P (U)
P (E)(1? P (U |E)) (7)
By substituting Equation 6 in Equation 7 and later,
applying the multiplication rule P (U? |E)P (E) =
P (E|U?)P (U?) to it, we will obtain:
P (U |E)
P (U? |E) =
P (E|U)P (U)
P (E|U?)P (U?) (8)
We proceed to take the log of the odds in Equation 8
(i.e. logit) to get:
log P (E|U)
P (E|U?) = log
P (U |E)
P (U? |E) ? log
P (U)
P (U?) (9)
While it is obvious that certain words tend to co-
occur more frequently than others (i.e. idioms and
collocations), such phenomena are largely arbitrary
(Smadja, 1993). This makes the task of deciding
on what constitutes an acceptable collocation dif-
ficult. The only way to objectively identify sta-
ble lexical units is through observations in samples
of the language (e.g. text corpus) (McKeown and
Radev, 2000). In other words, assigning the apri-
ori probability of collocational strength without em-
pirical evidence is both subjective and difficult. As
such, we are left with the option to assume that
the probability of s being a stable unit and not be-
ing a stable unit without evidence is the same (i.e.
P (U) = P (U?) = 0.5). As a result, the second term
in Equation 9 evaluates to 0:
log P (U |E)
P (U? |E) = log
P (E|U)
P (E|U?) (10)
We introduce a new measure for determining the
odds of s being a stable unit known as Odds of Unit-
hood (OU):
Definition 4 Odds of unithood
OU(s) = log P (E|U)
P (E|U?)
Assuming that the evidences in E are independent
of one another, we can evaluate OU(s) in terms of:
OU(s) = log
?
i P (ei|U)
?
i P (ei|U?)
(11)
=
?
i
log P (ei|U)
P (ei|U?)
(a) The area with darker
shade is the set X ? Y \ S.
Computing the ratio of P (S)
and the probability of this area
will give us the first evidence.
(b) The area with darker
shade is the set S?. Comput-
ing the ratio of P (S) and the
probability of this area (i.e.
P (S?) = 1? P (S)) will give
us the second evidence.
Figure 1: The probability of the areas with darker
shade are the denominators required by the evi-
dences e
1
and e
2
for the estimation of OU(s).
where ei are individual evidences possessed by s.
With the introduction of Definition 4, we can ex-
amine the degree of collocational strength of ax
and ay in forming s, mentioned in Definition 1 in
terms of OU(s). With the base of the log in Def-
inition 4 more than 1, the upper and lower bound
of OU(s) would be +? and ??, respectively.
OU(s) = +? and OU(s) = ?? corresponds to
the highest and the lowest degree of stability of the
two units ax and ay appearing as s, respectively. A
high1 OU(s) would indicate the suitability for the
two units ax and ay to be merged to form s. Ulti-
mately, we have reduced the vague problem of the
determination of unithood introduced in Definition
1 into a practical and computable solution in Defini-
tion 4. The evidences that we propose to employ for
determining unithood are based on the occurrences
of s, or the event S if the readers recall from Defini-
tion 2. We are interested in two types of occurrences
of s, namely, the occurrence of s given that ax and
ay have already occurred or X ? Y , and the occur-
rence of s as it is in our sample space, N . We refer
to the first evidence e
1
as local occurrence, while
the second one e
2
as global occurrence. We will
discuss the intuitive justification behind each type of
occurrences. Each evidence ei captures the occur-
rences of s within a different confinement. We will
estimate these evidences in terms of the elementary
probabilities already defined in Equations 4 and 5.
The first evidence e
1
captures the probability of
occurrences of s within the confinement of ax and ay
1A subjective issue that may be determined using a threshold
107
or X?Y . As such, P (e
1
|U) can be interpreted as the
probability of s occurring within X ? Y as a stable
unit or P (S|X ? Y ). On the other hand, P (e
1
|U?)
captures the probability of s occurring in X ? Y not
as a unit. In other words, P (e
1
|U?) is the probability
of s not occurring in X ? Y , or equivalently, equal
to P ((X ? Y \ S)|(X ? Y )). The set X ? Y \ S is
shown as the area with darker shade in Figure 1(a).
Let us define the odds based on the first evidence as:
OL =
P (e
1
|U)
P (e
1
|U?) (12)
Substituting P (e
1
|U) = P (S|X ? Y ) and
P (e
1
|U?) = P ((X ? Y \ S)|(X ? Y )) into Equa-
tion 12 will give us:
OL =
P (S|X ? Y )
P ((X ? Y \ S)|(X ? Y ))
= P (S ? (X ? Y ))P (X ? Y )
P (X ? Y )
P ((X ? Y \ S) ? (X ? Y ))
= P (S ? (X ? Y ))P ((X ? Y \ S) ? (X ? Y ))
and since S ? (X?Y ) and (X?Y \S) ? (X?Y ),
OL =
P (S)
P (X ? Y \ S) if(P (X ? Y \ S) 6= 0)
and OL = 1 if P (X ? Y \ S) = 0.
The second evidence e
2
captures the probability
of occurrences of s without confinement. If s is a
stable unit, then its probability of occurrence in the
sample space would simply be P (S). On the other
hand, if s occurs not as a unit, then its probability of
non-occurrence is 1?P (S). The complement of S,
which is the set S? is shown as the area with darker
shade in Figure 1(b). Let us define the odds based
on the second evidence as:
OG =
P (e
2
|U)
P (e
2
|U?) (13)
Substituting P (e
2
|U) = P (S) and P (e
2
|U?) = 1 ?
P (S) into Equation 13 will give us:
OG =
P (S)
1? P (S)
Intuitively, the first evidence attempts to capture
the extent to which the existence of the two lexical
units ax and ay is attributable to s. Referring back
to OL, whenever the denominator P (X ?Y \S) be-
comes less than P (S), we can deduce that ax and
ay actually exist together as s more than in other
forms. At one extreme when P (X ? Y \ S) = 0,
we can conclude that the co-occurrence of ax and
ay is exclusively for s. As such, we can also refer to
OL as a measure of exclusivity for the use of ax and
ay with respect to s. This first evidence is a good
indication for the unithood of s since the more the
existence of ax and ay is attributed to s, the stronger
the collocational strength of s becomes. Concerning
the second evidence, OG attempts to capture the ex-
tent to which s occurs in general usage (i.e. World
Wide Web). We can consider OG as a measure of
pervasiveness for the use of s. As s becomes more
widely used in text, the numerator in OG will in-
crease. This provides a good indication on the unit-
hood of s since the more s appears in usage, the like-
lier it becomes that s is a stable unit instead of an oc-
currence by chance when ax and ay are located next
to each other. As a result, the derivation of OU(s)
using OL and OG will ensure a comprehensive way
of determining unithood.
Finally, expanding OU(s) in Equation 11 using
Equations 12 and 13 will give us:
OU(s) = logOL + logOG (14)
= log P (S)P (X ? Y \ S) + log
P (S)
1? P (S)
As such, the decision on whether ax and ay should
be merged to form s can be made based solely on
the Odds of Unithood (OU) defined in Equation 14.
We will merge ax and ay if their odds of unithood
exceeds a certain threshold, OUT .
4 Evaluations and Discussions
For this evaluation, we employed 500 news arti-
cles from Reuters in the health domain gathered be-
tween December 2006 to May 2007. These 500 arti-
cles are fed into the Stanford Parser whose output is
then used by our head-driven left-right filter (Wong,
2005; Wong et al, 2007b) to extract word sequences
in the form of nouns and noun phrases. Pairs of word
sequences (i.e. ax and ay) located immediately next
to each other, or separated by a preposition or the
conjunction ?and? in the same sentence are mea-
108
sured for their unithood. Using the 500 news arti-
cles, we managed to obtain 1, 825 pairs of words to
be tested for unithood.
We performed a comparative study of our
new probabilistic approach against the empirically-
derived unithood function described in Equation 2.
Two experiments were conducted. In the first one,
we assessed our probabilistically-derived measure
OU(s) as described in Equation 14 where the de-
cisions on whether or not to merge the 1, 825 pairs
are done automatically. These decisions are known
as the actual results. At the same time, we inspected
the same list manually to decide on the merging of
all the pairs. These decisions are known as the ideal
results. The threshold OUT employed for our evalu-
ation is determined empirically through experiments
and is set to ?8.39. However, since only one thresh-
old is involved in deciding mergeability, training al-
gorithms and data sets may be employed to automat-
ically decide on an optimal number. This option is
beyond the scope of this paper. The actual and ideal
results for this first experiment are organised into
a contingency table (not shown here) for identify-
ing the true and the false positives, and the true and
the false negatives. In the second experiment, we
conducted the same assessment as carried out in the
first one but the decisions to merge the 1, 825 pairs
are based on the UH(ax, ay) function described in
Equation 2. The thresholds required for this func-
tion are based on the values suggested by (Wong et
al., 2007b), namely, MI+ = 0.9, MI? = 0.02,
IDT = 6, IDR+ = 1.35, and IDR? = 0.93.
Table 1: The performance of OU(s) (from Exper-
iment 1) and UH(ax, ay) (from Experiment 2) in
terms of precision, recall and accuracy. The last
column shows the difference in the performance of
Experiment 1 and 2.
Using the results from the contingency tables,
we computed the precision, recall and accuracy for
the two measures under evaluation. Table 1 sum-
marises the performance of OU(s) and UH(ax, ay)
in determining the unithood of 1, 825 pairs of lex-
ical units. One will notice that our new measure
OU(s) outperformed the empirically-derived func-
tion UH(ax, ay) in all aspects, with an improvement
of 2.63%, 3.33% and 2.74% for precision, recall and
accuracy, respectively. Our new measure achieved a
100% precision with a lower recall at 95.83%. As
with any measures that employ thresholds as a cut-
off point in accepting or rejecting certain decisions,
we can improve the recall of OU(s) by decreasing
the threshold OUT . In this way, there will be less
false negatives (i.e. pairs which are supposed to be
merged but are not) and hence, increases the recall
rate. Unfortunately, recall will improve at the ex-
pense of precision since the number of false pos-
itives will definitely increase from the existing 0.
Since our application (i.e. ontology learning) re-
quires perfect precision in determining the unithood
of word sequences, OU(s) is the ideal candidate.
Moreover, with only one threshold (i.e. OUT ) re-
quired in controlling the function of OU(s), we are
able to reduce the amount of time and effort spent
on optimising our results.
5 Conclusion and Future Work
In this paper, we highlighted the significance of unit-
hood and that its measurement should be given equal
attention by researchers in term extraction. We fo-
cused on the development of a new approach that
is independent of influences of termhood measure-
ment. We proposed a new probabilistically-derived
measure which provide a dedicated way to deter-
mine the unithood of word sequences. We refer to
this measure as the Odds of Unithood (OU). OU is
derived using Bayes Theorem and is founded upon
two evidences, namely, local occurrence and global
occurrence. Elementary probabilities estimated us-
ing page counts from the Google search engine are
utilised to quantify the two evidences. The new
probabilistically-derived measure OU is then eval-
uated against an existing empirical function known
as Unithood (UH). Our new measure OU achieved a
precision and a recall of 100% and 95.83% respec-
tively, with an accuracy at 97.26% in measuring the
unithood of 1, 825 test cases. OU outperformed UH
by 2.63%, 3.33% and 2.74% in terms of precision,
109
recall and accuracy, respectively. Moreover, our new
measure requires only one threshold, as compared to
five in UH to control the mergeability decision.
More work is required to establish the coverage
and the depth of the World Wide Web with regards
to the determination of unithood. While the Web has
demonstrated reasonable strength in handling gen-
eral news articles, we have yet to study its appropri-
ateness in dealing with unithood determination for
technical text (i.e. the depth of the Web). Similarly,
it remains a question the extent to which the Web
is able to satisfy the requirement of unithood deter-
mination for a wider range of genres (i.e. the cov-
erage of the Web). Studies on the effect of noises
(e.g. keyword spamming) and multiple word senses
on unithood determination using the Web is another
future research direction.
Acknowledgement
This research was supported by the Australian En-
deavour International Postgraduate Research Schol-
arship, and the Research Grant 2006 by the Univer-
sity of Western Australia.
References
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
T. Dunning. 1994. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
K. Frantzi. 1997. Incorporating context information for
the extraction of terms. In Proceedings of the 35th An-
nual Meeting on Association for Computational Lin-
guistics, Spain.
A. Franz. 1997. Independence assumptions considered
harmful. In Proceedings of the 8th Conference on Eu-
ropean Chapter of the Association for Computational
Linguistics, Madrid, Spain.
K. Kageura and B. Umino. 1996. Methods of automatic
term recognition: A review. Terminology, 3(2):259?
289.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Using
the web to overcome data sparseness. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia.
C. Kit. 2002. Corpus tools for retrieving and deriving
termhood evidence. In Proceedings of the 5th East
Asia Forum of Terminology, Haikou, China.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proceedings of the 41st Meeting of the As-
sociation for Computational Linguistics.
D. Kurz and F. Xu. 2002. Text mining for the extrac-
tion of domain relevant terms and term collocations.
In Proceedings of the International Workshop on Com-
putational Approaches to Collocations, Vienna.
K. McKeown and D. Radev. 2000. Collocations. In
R. Dale, H. Moisl, and H. Somers, editors, Handbook
of Natural Language Processing. Marcel Dekker.
H. Nakagawa and T. Mori. 2002. A simple but powerful
automatic term extraction method. In Proceedings of
the International Conference On Computational Lin-
guistics (COLING).
V. Seretan, L. Nerima, and E. Wehrli. 2004. Using
the web as a corpus for the syntactic-based colloca-
tion identification. In Proceedings of the International
Conference on on Language Resources and Evaluation
(LREC), Lisbon, Portugal.
F. Smadja. 1993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143?177.
W. Wong, W. Liu, and M. Bennamoun. 2007a. Deter-
mining termhood for learning domain ontologies in a
probabilistic framework. In Proceedings of the 6th
Australasian Conference on Data Mining (AusDM),
Gold Coast.
W. Wong, W. Liu, and M. Bennamoun. 2007b. Deter-
mining the unithood of word sequences using mutual
information and independence measure. In Proceed-
ings of the 10th Conference of the Pacific Associa-
tion for Computational Linguistics (PACLING), Mel-
bourne, Australia.
W. Wong. 2005. Practical approach to knowledge-
based question answering with natural language un-
derstanding and advanced reasoning. Master?s thesis,
National Technical University College of Malaysia,
arXiv:cs.CL/0707.3559.
110
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 486?496, Dublin, Ireland, August 23-29 2014.
Fast Tweet Retrieval with Compact Binary Codes
Weiwei Guo
?
Wei Liu
?
Mona Diab
?
?
Computer Science Department, Columbia University, New York, NY, USA
?
IBM T. J. Watson Research Center, Yorktown Heights, NY, USA
?
Department of Computer Science, George Washington University, Washington, D.C., USA
weiwei@cs.columbia.edu weiliu@us.ibm.com mtdiab@gwu.edu
Abstract
The most widely used similarity measure in the field of natural language processing may be co-
sine similarity. However, in the context of Twitter, the large scale of massive tweet data inevitably
makes it expensive to perform cosine similarity computations among tremendous data samples.
In this paper, we exploit binary coding to tackle the scalability issue, which compresses each data
sample into a compact binary code and hence enables highly efficient similarity computations via
Hamming distances between the generated codes. In order to yield semantics sensitive binary
codes for tweet data, we design a binarized matrix factorization model and further improve it in
two aspects. First, we force the projection directions employed by the model nearly orthogonal to
reduce the redundant information in their resulting binary bits. Second, we leverage the tweets?
neighborhood information to encourage similar tweets to have adjacent binary codes. Evaluated
on a tweet dataset using hashtags to create gold labels in an information retrieval scenario, our
proposed model shows significant performance gains over competing methods.
1 Introduction
Twitter is rapidly gaining worldwide popularity, with 500 million active users generating more than
340 million tweets daily
1
. Massive-scale tweet data which is freely available on the Web contains rich
linguistic phenomena and valuable information, therefore making it one of most favorite data sources
used by a variety of Natural Language Processing (NLP) applications. Successful examples include
first story detection (Petrovic et al., 2010), local event detection (Agarwal et al., 2012), Twitter event
discovery (Benson et al., 2011) and summarization (Chakrabarti and Punera, 2011), etc.
In these NLP applications, one of core technical components is tweet similarity computing to search
for the desired tweets with respect to some sample tweets. For example, in first story detection (Petrovic
et al., 2010), the purpose is to find an incoming tweet that is expected to report a novel event not revealed
by the previous tweets. This is done by measuring cosine similarity between the incoming tweet and
each previous tweet.
One obvious issue is that cosine similarity computations among tweet data will become very slow once
the scale of tweet data grows drastically. In this paper, we investigate the problem of searching for most
similar tweets given a query tweet. Specifically, we propose a binary coding approach to render com-
putationally efficient tweet comparisons that should benefit practical NLP applications, especially in the
face of massive data scenarios. Using the proposed approach, each tweet is compressed into short-length
binary bits (i.e., a compact binary code), so that tweet comparisons can be performed substantially faster
through measuring Hamming distances between the generated compact codes. Crucially, Hamming
distance computation only involves very cheap NOR and popcount operations instead of floating-point
operations needed by cosine similarity computation.
Compared to other genres of data, similarity search in tweet data is very challenging due to the short
nature of Twitter messages, that is, a tweet contains too little information for traditional models to extract
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://en.wikipedia.org/wiki/Twitter
486
Symbol Definition
n Number of tweets in the corpus.
d Dimension of a tweet vector, i.e., the vocabulary size.
x
i
The sparse tf-idf vector corresponding to the i-th tweet in the corpus.
?
x
i
The vector subtracted by the mean ? of the tweet corpus:
?
x
i
= x
i
??.
X,
?
X The tweet corpus in a matrix format, and the zero-centered tweet data.
r The number of binary coding functions, i.e., the number of latent topics.
f
k
The k-th binary coding function.
Table 1: Symbols used in binary coding.
latent topical semantics. For instance, in our collected dataset, there exist only 11 words per tweet on
average. We address the sparsity issue pertaining to tweet data by converting our previously proposed
topic modelWeighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) to a binarized version.
WTMF maps a tweet to a low-dimensional semantic vector which can easily be transformed to a binary
code by virtue of a sign function. We consider WTMF a good baseline for the task of tweet retrieval, as
it has achieved state-of-the-art performance among unsupervised systems on two benchmark short-text
datasets released by Li et al. (2006) and Agirre et al. (2012).
In this paper, we improve WTMF in two aspects. The first drawback of the WTMF model is that it
focuses on exhaustively encoding the local context, and hence introduces some overlapping information
that is reflected in its associated projections. In order to remove the redundant information and meanwhile
discover more distinct topics, we employ a gradient descent method to make the projection directions
nearly orthogonal.
The second aspect is to enrich each tweet by its neighbors. Because of the short context, most tweets
do not contain sufficient information of an event, as noticed by previous work (Agarwal et al., 2012; Guo
et al., 2013). Ideally, we would like to learn a model such that the tweets related to the same event are
mapped to adjacent binary codes. We fulfill this purpose by augmenting each tweet in a given training
dataset with its neighboring tweets within a temporal window, and assuming that these neighboring
(or similar) tweets are triggered by the same event. We name the improved model Orthogonal Matrix
Factorization with Neighbors (OrMFN).
In our experiments, we use Twitter hashtags to create the gold (i.e., groundtruth) labels, where tweets
with the same hashtag are considered semantically related, hence relevant. We collect a tweet dataset
which consists of 1.35 million tweets over 3 months where each tweet has exactly one hashtag. The
experimental results show that our proposed model OrMFN significantly outperforms competing binary
coding methods.
2 Background and Related Work
2.1 Preliminaries
We first introduce some notations used in this paper to formulate our problem. Suppose that we are
given a dataset of n tweets and the size of the vocabulary is d. A tweet is represented by all the words it
contains. We use notationx ? R
d
to denote a sparse d-dimensional tf-idf vector corresponding to a tweet,
where each word stands for a dimension. For ease of notation, we represent all n tweets in a matrix X =
[x
1
,x
2
, ? ? ? ,x
n
] ? R
d?n
. For binary coding, we seek r binarization functions
{
f
k
: R
d
? {1,?1}
}
r
k=1
so that a tweet x
i
is encoded into an r-bit binary code (i.e., a string of r binary bits). Table 1 illustrates
the symbols used in this paper for notation.
Hamming Ranking: In the paper we evaluate the quality of binary codes in terms of Hamming ranking.
Given a query tweet, all data items are ranked in an ascending order according to the Hamming distances
between their binary codes and the query?s binary code, where a Hamming distance is the number of
bit positions in which bits of two codes differ. Compared with cosine similarity, computing Hamming
distance can be substantially efficient. This is because fixed-length binary bits enable very cheap logic
operations for Hamming distance computation, whereas real-valued vectors require floating-point op-
487
erations for cosine similarity computation. Since logic operations are much faster than floating-point
operations, Hamming distance computation is typically much faster than cosine similarity computation
2
2.2 Binary Coding
Early explorations of binary coding focused on using random permutations or random projections to ob-
tain binary coding functions (aka, hash functions), such as Min-wise Hashing (MinHash) (Broder et al.,
1998) and Locality-Sensitive Hashing (LSH) (Indyk and Motwani, 1998). MinHash and LSH are gen-
erally considered data-independent approaches, as their coding functions are generated in a randomized
fashion. In the context of Twitter, the simple LSH scheme proposed in (Charikar, 2002) is of particular
interest. Charikar proved that the probability of two data points colliding is proportional to the angle
between them, and then employed a random projection w ? R
d
to construct a binary coding function:
f(x) = sgn
(
w
>
x
)
=
{
1, if w
>
x > 0,
?1, otherwise.
(1)
The current held view is that data-dependent binary coding can lead to better performance. A data-
dependent coding scheme typically includes two steps: 1) learning a series of binary coding functions
with a small amount of training data; 2) applying the learned functions to larger scale data to produce
binary codes.
In the context of tweet data, Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997) can di-
rectly be used for data-dependent binary coding. LSA reduces the dimensionality of the data in X by
performing singular value decomposition (SVD) over X: X = U?V
>
. Let
?
X be the zero-centered data
matrix, where each tweet vector x
i
is subtracted by the mean vector ?, resulting in
?
x
i
= x
i
? ?. The r
coding functions are then constructed by using the r eigenvectors u
1
,u
2
, ? ? ? ,u
r
associated with the r
largest eigenvalues, that is, f
k
(x) = sgn
(
u
>
k
?
x
)
= sgn
(
u
>
k
(x ? ?)
)
(k = 1, ? ? ? , r). The goal of using
zero-centered data
?
X is to balance 1 bits and ?1 bits.
Iterative Quantization (ITQ) (Gong and Lazebnik, 2011) is another popular unsupervised binary cod-
ing approach. ITQ attempts to find an orthogonal rotation matrix R ? R
r?r
to minimize the squared
quantization error: ?B?RV ?
2
F
, whereB ? {1,?1}
r?n
contains the binary codes of all data, V ? R
r?n
contains the LSA-projected and zero-centered vectors, and ? ? ?
F
denotes Frobenius norm. After R is
optimized, the binary codes are simply obtained by B = sgn(RV ).
Much recent work learns nonlinear binary coding functions, including Spectral Hashing (Weiss et
al., 2008), Anchor Graph Hashing (Liu et al., 2011), Bilinear Hashing (Liu et al., 2012b), Kernelized
LSH (Kulis and Grauman, 2012), etc. Concurrently, supervised information defined among training data
samples was incorporated into coding function learning such as Minimal Loss Hashing (Norouzi and
Fleet, 2011) and Kernel-Based Supervised Hashing (Liu et al., 2012a). Our proposed method falls into
the category of unsupervised, linear, data-dependent binary coding.
2.3 Applications in NLP
The NLP community has successfully applied LSH in several tasks such as first story detection (Petrovic
et al., 2010), and paraphrase retrieval for relation extraction (Bhagat and Ravichandran, 2008), etc. This
paper shows that our proposed data-dependent binary coding approach is superior to data-independent
LSH in terms of the quality of generated binary codes.
Subercaze et al. (2013) proposed a binary coding approach to encode user profiles for recommenda-
tions. Compared to (Subercaze et al., 2013) in which a data unit is a whole user profile consisting of all
his/her Twitter posts, we tackle a more challenging problem, since our data units are extremely short ?
namely, a single tweet.
2
We recognize that different hardware exploiting techniques such as GPU or parallelization accelerate cosine similarity.
However, they don?t change the inherent nature of the data representation. They can be equally applied to Hamming distance
and we anticipate significant speed gains. We relegate this exploration of different implementations of Hamming distance to
future work.
488
X	 ? P	 ? Q
T	 ??	 ? ?	 ?
Figure 1: Weighted Textual Matrix Factorization. The d ? n matrix X is approximated by the product
of a d? r matrix P and an n? r matrix Q. Note in the figure we used the transpose of the Q matrix.
3 Weighted Textual Matrix Factorization
The WTMF model proposed by Guo and Diab (2012) is designed to extract latent semantic vectors for
short textual data. The low-dimensional semantic vectors can be used to represent the tweets in the
original high-dimensional space. WTMF achieved state-of-the-art unsupervised performance on two
short text similarity datasets, which can be attributed to the fact that WTMF carefully handles missing
words (the missing words of a text are the words with 0 values in a data vector x).
Assume that there are r latent dimensions/topics in the data, the matrix X is approximated by the
product of a d?r matrix P and an n?r matrixQ, as in Figure 1. Accordingly, a tweet x
j
is represented
by an r-dimensional vector Q
j,?
; similarly, a word w
i
is generalized by the r-dimensional vector P
i,?
(the
ith row in matrix P ). The matrix factorization scheme has an intuitive explanation: the inner-product
of a word profile vector P
i,?
and a tweet profile vector Q
j,?
is to approximate the TF-IDF value X
ij
:
P
i,?
>
Q
j,?
? X
ij
(as illustrated by the shaded parts in Figure 1).
Intuitively, X
ij
= 0 suggests that the latent topics of the text x
j
are not relevant to the word w
i
.
Note that 99% of the cells in X are 0 because of the short contexts, which significantly diminishes the
contribution of the observed words to the searching of optimal P andQ. To reduce the impact of missing
words, a small weight w
m
is assigned to each 0 cell of X in the objective function:
?
i
?
j
W
ij
(
P
i,?
>
Q
j,?
?X
ij
)
2
+ ?||P ||
2
2
+ ?||Q||
2
2
,
W
i,j
=
{
1, if X
ij
6= 0,
w
m
, if X
ij
= 0.
(2)
where ? is the regularization parameter. Alternating Least Squares (Srebro and Jaakkola, 2003) is used
to iteratively compute the latent semantic vectors in P and Q:
P
i,?
=
(
Q
>
?
W
(i)
Q+ ?I
)
?1
Q
>
?
W
(i)
X
>
i,?
,
Q
j,?
=
(
P
>
?
W
(j)
P + ?I
)
?1
P
>
?
W
(j)
X
?,j
(3)
where
?
W
(i)
= diag(W
i,?
) is a n ? n diagonal matrix containing the i-th row of the weight matrix W .
Similarly,
?
W
(j)
= diag(W
?,j
) is a d? d diagonal matrix containing the j-th column of W .
As in Algorithm 1 line 6-9, P andQ are computed iteratively, i.e., in a iteration each P
i,?
(i = 1, ? ? ? , d)
is calculated based on Q, then each Q
j,?
(j = 1, ? ? ? , n) is calculated based on P . This can be computed
efficiently since: (1) all P
i,?
share the same Q
>
Q; similarly all Q
j,?
share the same P
>
P ; (2) X is very
sparse. More details can be found in (Steck, 2010).
Adapting WTMF to binary coding is straightforward. Following LSA, we use the matrix P to linearly
project tweets into low-dimensional vectors, and then apply the sign function. The k-th binarization
function uses the k-th column of the P matrix (P
?,k
) as follows
f
k
(x) = sgn (P
?,k
?
x) =
{
1, if P
?,k
?
x > 0,
?1, otherwise.
(4)
4 Removing Redundant Information
It is worth noting that there are two explanations of the d? r matrix P . The rows of P , denoted by P
i,?
,
may be viewed as the collection of r-dimensional latent profiles of words, which we observe frequently
489
Algorithm 1: OrMF
1 Procedure P = OrMF(X,W, ?, n itr, ?)
2 n words, n docs? size(X);
3 randomly initialize P,Q;
4 itr ? 1;
5 while itr < n itr do
6 for j ? 1 to n docs do
7 Q
j,?
=
(
P
>
?
W
(j)
P + ?I
)
?1
P
>
?
W
(j)
X
?,j
8 for i? 1 to n words do
9 P
i,?
=
(
Q
>
?
W
(i)
Q+ ?I
)
?1
Q
>
?
W
(i)
X
>
i,?
10 c = mean(diag(P
>
P ));
11 P ? P ? ?P (P
>
P ? cI);
12 itr ? itr + 1;
in the WTMF model. Meanwhile, columns of P are projection vectors, denoted by P
?,k
, which are
similar to eigenvectors U obtained by LSA. The projection vector P
?,k
is employed to multiply to a zero
centered data vector
?
x to generate a binary string: sgn(P
?,k
>
?
x). In this section, we focus on the property
of the P matrix columns.
As in equation 3, each row in matrices P and Q is iteratively optimized to approximate the data:
P
i,?
>
Q
j,?
? X
ij
. While it does a good job at preserving the existence/relevance of each word in a short
text, it might encode repetitive information by means of the dimensionality reduction or the projection
vectors P
?,k
(the columns of P ). For example, the first dimension P
?,1
may be 90% about the politics
topic and 10% about the economics topic, and the second dimension P
?,2
is 95% on economics and 5%
on technology topics, respectively.
Ideally we would like the dimensions to be uncorrelated, so that more distinct topics of data could
be captured. One way to ensure the uncorrelatedness is to force P to be orthogonal, i.e., P
>
P = I . It
implies P
?,j
>
P
?,k
= 0 if k 6= j.
4.1 Implementation of Orthogonal Projections
To produce nearly orthogonal projections in the current framework, we could add a regularizer ?(P
>
P?
I)
2
with the weight ? in the objective function of the WTMF model (equation 6). However, in practice
this method does not lead to the convergence of P . This is mainly caused by the phenomenon that any
word profile P
i,?
becomes dependent of all other word profiles after an iteration.
Therefore, we adopt a simpler method, gradient descent, in which P is updated by taking a small step
in the direction of the negative gradient of (P
>
P ? I)
2
. It is also worth noting that (P
>
P ? I)
2
requires
each projection P
?,k
to be a unit vector because of P
?,k
>
P
?,k
= 1, which is infeasible when the nonzero
values in X are large. Therefore, we multiply the matrix I by a coefficient c, which is calculated from
the mean of the diagonal of P
>
P in the current iteration. The following two lines are added at the end
of an iteration:
c? mean(diag(P
>
P )),
P ?P ? ?P (P
>
P ? cI).
(5)
This procedure is presented in Algorithm 1. Accordingly, the magnitude of P is not affected. The step
size ? is fixed to 0.0001. We refer to this model as Orthogonal Matrix Factorization (OrMF).
5 Exploiting Nearest Neighbors for Tweets
We observe that tweets triggered by the same event do not have very high cosine similarity scores among
them. This is caused by the inherent short length of tweets such that usually a tweet only describes one
490
aspect of an event (Agarwal et al., 2012; Guo et al., 2013). Our objective is to find the relevant tweets
given a tweet, and then learn a model that assigns similar binary bits to these relevant tweets.
5.1 Modeling Neighboring Tweets
Given a tweet, we treat its nearest neighbors in a temporal window as its most relevant tweets. We
assume that the other aspects of an event can be found in its nearest neighbors. Accordingly, we extract
t neighbors for a tweet from 10,000 most chronologically close tweets. In this current implementation,
we set t = 5.
Under the weighted matrix factorization framework, we extend each tweet by its t nearest neighbors.
Specifically, for each tweet, we incorporate additional words from its neighboring tweets. The values
of the new words are averaged. Moreover, these new words are treated differently by assigning a new
weight w
n
to them, since we believe that the new words are not as informative as the original words in
the tweet.
We present an illustrative example of how to use neighbors to extend the tweets. Let x
1
be a tweet
with the following words (the numbers after the colon are TF-IDF values):
x
1
= {obama:5.5, medicare:8.3, website:3.8}
which has two nearest neighbors:
x
27
= {obama:5.5, medicare:8.3, website:3.8, down:5.4}
x
356
= {obama:5.5, medicare:8.3, website:3.8, problem:7.0}
Then there are two additional words added in x
1
whose values are averaged. The new data vector x
?
1
is:
x
?
1
= {obama:5.5, medicare:8.3, website:3.8, down:2.7, problem:3.5}
Therefore, the algorithm is run on the new neighbor-augmented data matrix, denoted by X
?
, and the
weight matrix W becomes
W
i,j
=
?
?
?
1, if X
?
ij
6= 0 &j is an original word,
w
n
, if X
?
ij
6= 0, &j is from neighbor tweets,
w
m
, if X
?
ij
= 0.
(6)
This model is referred to as Orthogonal Matrix Factorization with Neighbors (OrMFN).
5.2 Binary coding without Neighbors
It is important to point out that the data used by OrMFN, X
?
, could be a very small subset of the whole
dataset. Therefore we only need to find neighbors for a small portion of the data. After the P matrix
is learned, the neighborhood information is implicitly encoded in the matrix P , and we still apply the
same binarization function sgn(P
?,k
>
?
x) on the whole dataset (in large scale) without neighborhood
information. We randomly sample 200,000 tweets for OrMFN to learn P ; neighbors are extracted only
for these 200,000 tweets (note that the neighbors are from the 200,000 tweets as well), and then we use
the learned P to generate binary codes for the whole dataset 1.35 million tweets without searching for
their nearest neighbors.
3
Our scheme has a clear advantage: the binary coding remains very efficient. During binarization for
any data, there is no need to compare 10,000 most recent tweets to find nearest neighbors, which could
be time-consuming. An opposite example is the method presented in (Guo et al., 2013), where t most
nearest neighbor tweets were extracted, and a tweet profile Q
j,?
was explicitly forced to be similar to its
neighbors? profiles. However, for each new data, the approach proposed in (Guo et al., 2013) requires
computing its nearest neighbors.
6 Experiments
6.1 Tweet Data
We crawled English tweets spanning three months from October 5th 2013 to January 5th 2014 using the
Twitter API.
4
We cleaned the data such that each hashtag appears at least 100 times in the corpus, and
3
When generating the binary codes for the 200,000 tweets, these tweets are not augmented with neighbor words.
4
https://dev.twitter.com
491
each word appears at least 10 times. This data collection consists of 1,350,159 tweets, 15 million word
tokens, 30,608 unique words, and 3,214 unique hashtags.
One of main reasons to use hashtags is to enhance accessing topically similar tweets (Efron, 2010).
In a large-scale data setting, it is impossible to manually identify relevant tweets. Therefore, we use
Twitter hashtags to create groundtruth labels, which means that tweets marked by the same hashtag
as the query tweet are considered relevant. Accordingly, in our experiments all hashtags are removed
from the original data corpus. We chose a subset of hashtags from the most frequent hashtags to create
groundtruth labels: we manually removed some tags from the subset that are not topic-related (e.g.,
#truth, #lol) or are ambiguous; we also removed all the tags that are referring to TV series (the relevant
tweets can be trivially obtained by named entity matching). The resulting subset contains 18 hashtags.
5
100 tweets are randomly selected as queries (test data) for each of the 18 hashtags. The median
number of relevant tweets per query is 5,621. The small size of gold standard makes the task relatively
challenging. We need to identify 5,621 (0.42% of the whole dataset) tweets out of 1.35 million tweets.
200,000 tweets are randomly selected (not including the 1,800 queries) as training data for the data
dependent models to learn binarization functions.
6
The functions are subsequently applied on all the
1.35 million tweets, including the 1,800 query tweets.
6.2 Evaluation
We evaluate a model by the search quality: given a tweet as query, we would like to rank the relevant
tweets as high as possible. Following previous work (Weiss et al., 2008; Liu et al., 2011), we use mean
precision among top 1000 returned list (MP@1000) to measure the ranking quality. Let pre@k be the
precision among top k return data, then MP@1000 is the average value of pre@1, pre@2...pre@1000.
Obviously MP gives more reward on the systems that can rank relevant data in the top places, e.g., if
the highest ranked tweet is a relevant tweet, then all the precision values (pre@2, pre@3, pre@4...) are
increased. We also calculate the precision and recall curve at varying values of top k returned list.
6.3 Methods
We evaluate the proposed unsupervised binary coding models OrMF and OrMFN, whose performance is
compared against 5 other unsupervised methods, LSH, SH, LSA, ITQ, and WTMF. All the binary coding
functions except LSH are learned on the 200,000 tweet set. All the methods have the same form of binary
coding functions: sgn(P
?,k
>
?
x), where they differ only in the projection vector P
?,k
. The retrieved tweets
are ranked according to their Hamming distance to the query, where Hamming distance is the number of
different bit positions between the binary codes of a tweet and the query.
For ITQ and SH, we use the code provided by the authors. Note that the dense matrix
?
X
?
X
>
is
impossible to compute due the large vocabulary, therefore we replace it by sparse matrix XX
>
. For the
three matrix factorization based methods (WTMF, OrMF, OrMFN) we run 10 iterations. The regularizer
? in equation 6 is fixed at 20 as in (Guo and Diab, 2012). A small set of 500 tweets is selected from
the training set as tuning set to choose the missing word weight w
m
in the baseline WTMF, and then its
value is fixed for OrMF and OrMFN. The same 500 tweets tuning set is used to choose the neighbor word
weight w
n
. In fact these models are very stable, consistently outperforming the baselines regardless of
different values of w
m
and w
n
, as later shown in Figure 4 and 5.
We also present the results of cosine similarity on the original word space (COSINE) as an upper
bound of the binary coding methods. We implemented an efficient algorithm for COSINE, which is the
algorithm 1 in (Petrovic et al., 2010). It firstly normalizes each data to a unit vector, then cosine similarity
is calculated by traversing only once the tweets via inverted word index.
6.4 Results
Table 2 summarizes the ranking performance measured by MP@1000 (the mean precision at top 1000
returned list). Figures 2 and 3 illustrate the corresponding precision and recall curve for the Hamming
5
The tweet dataset and their associated list of hashtags will be available upon request.
6
Although we use the word ?training?, the hashtags are never seen by the models. The training data is used for the models
to learn the word co-occurrence, and construct binary coding functions.
492
Models Parameters r=64 r=96 r=128
LSH ? 19.21% 21.84% 23.75%
SH ? 18.29% 19.32% 19.95%
LSA ? 21.04% 22.07% 22.67%
ITQ ? 20.8% 22.06% 22.86%
WTMF w
m
= 0.1 26.64% 29.39% 30.38%
OrMF w
m
= 0.1 27.7% 30.48% 31.26%
OrMFN w
m
= 0.1, w
n
= 0.5 29.73% 31.73% 32.55%
COSINE ? 33.68%
Table 2: Mean precision among top 1000 returned list
0 200 400 600 800 1000
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Pre
cisio
n
# of samples
 
 LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(a) r = 64
0 200 400 600 800 1000
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Pre
cisio
n
# of samples
 
 
LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(b) r = 96
0 200 400 600 800 1000
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Pre
cisio
n
# of samples
 
 
LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(c) r = 128
Figure 2: Hamming ranking: precision curve under top 1000 returned list
distance ranking. The number of r binary coding functions corresponds to the number of dimensions in
the 6 data-dependent models LSA, SH, ITQ, WTMF, OrMF and OrMFN. The missing words weight w
m
is fixed as 0.1 based on the tuning set in the three weighted matrix factorization based models WTMF,
OrMF and OrMFN. The neighbor word weight w
n
is chosen as 0.5 for OrMFN. Later in Section 6.4.1
we show that the performance is robust using varying values of w
m
and w
n
.
As the number of bits increases, all binary coding models yield better results. This is understandable
since the binary bits really record very tiny bits of information from each tweet, and more bits, the more
they are able to capture more semantic information.
SH has the worst MP@1000 performance. The reason might be it is designed for vision data where
the data vector is relatively dense. ITQ yields comparable results to LSA in terms of MP@1000, yet the
recall curve in Figure 3b,c clearly shows the superiority of ITQ over LSA.
WTMF outperforms LSA by a large margin (around 5% to 7%) through properly modeling missing
words, which is also observed in (Guo and Diab, 2012). Although WTMF already reaches a very high
MP@1000 performance level, OrMF can still achieve around 1% improvement over WTMF, which can
be attributed to orthogonal projections that captures more distinct topics. At last, leveraging neighbor-
hood information, OrMFN is the best performing model (around 1% improvement over OrMF). The
trend holds consistently across all conditions. The precision and recall curves in Figures 2 and 3 confirm
the trend observed in Table 2 as well.
All the binary coding models yield worse performance than COSINE baseline. This is expected, as the
binary bits are employed to gain efficiency at the cost of accuracy: the 128 bits significantly compress
the data losing a lot of nuanced information, whereas in the high dimensional word space 128 bits can
be only used to record two words (32 bits for two word indices and 32 bits for two TF-IDF values). We
manually examined the ranking list. We found in the binary coding models, there exist a lot of ties (128
bits only result in 128 possible Hamming distance values), whereas the COSINE baseline can correctly
rank them by detecting the subtle difference signaled by the real-valued TF-IDF values.
493
0 2 4 6 8 10 12
x 104
0
0.05
0.1
0.15
0.2
0.25
0.3
Rec
all
# of samples
 
 LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(a) r = 64
0 2 4 6 8 10 12
x 104
0
0.05
0.1
0.15
0.2
0.25
0.3
Rec
all
# of samples
 
 LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(b) r = 96
0 2 4 6 8 10 12
x 104
0
0.05
0.1
0.15
0.2
0.25
0.3
Rec
all
# of samples
 
 LSH
SH
LSA
ITQ
WTMF
OrMF
OrMFN
(c) r = 128
Figure 3: Hamming ranking: recall curve under top 100,000 returned list
0.05 0.08 0.1 0.15 0.20.25
0.26
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
MP@
1000
wm
 
 WTMFOrMFOrMFN
(a) r = 64
0.05 0.08 0.1 0.15 0.20.25
0.26
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
MP@
1000
wm
 
 WTMFOrMFOrMFN
(b) r = 96
0.05 0.08 0.1 0.15 0.20.25
0.26
0.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
MP@
1000
wm
 
 
WTMFOrMFOrMFN
(c) r = 128
Figure 4: Weighted matrix factorization based models: MP@1000 vs. missing word weight w
m
6.4.1 Analysis
We are interested in whether other values of w
m
and w
n
can generate good results ? in other words,
whether the performance is robust to the two parameter values. Accordingly, we present their im-
pact on MP@1000 in Figure 4 and 5. In Figure 4, the missing word weight w
m
is chosen from
{0.05, 0.08, 0.1, 0.15, 0.2}, where in OrMFN the neighbor weight w
n
is fixed as 0.5. The figure in-
dicates we can achieve even better MP@1000 around 33.2% when selecting the optimal w
m
= 0.05.
In general, the curves for all the code length are very smooth; the chosen value of w
m
does not have a
negative impact, e.g., the gain from OrMF over WTMF is always positive.
Figure 5 demonstrates the impact of varying the values of neighbor word weight w
n
from
{0, 0.25, 0.5, 0.75, 1} on OrMFN tested in different r conditions. Note that when w
n
= 0 indicating
that no neighbor information is exploited, the OrMFN model is simply reduced to the OrMF model.
Based on the Figure illustration we can conclude that integrating neighboring word information always
yields a positive effect, since any value of w
n
> 0 yields a performance gain over w
n
= 0 which is
OrMF.
6.5 Computation Cost
The data-dependent models involve 2 steps: 1) learning coding functions from a small dataset, and 2)
binary coding for the large scale whole dataset.
7
In real-time scenarios, the time is only spent on the
2nd step that involves no matrix factorization. The computation cost of binary coding for all models
(LSH, ITQ, LSA, WTMF, OrMF and OrMFN) are roughly the same: sgn(P
?,k
>
?
x). Note that P
?,k
>
?
x =
P
?,k
>
x? P
?,k
>
? where x is a very sparse vector (with 11 non-zeros values on average) and P
?,k
>
? can
be precomputed. On the other hand, calculating Hamming distance on binary codes is also very fast
using the logic operations.
7
Learning the binarization functions can be always done on a small dataset, for example in this paper all the data dependent
models are run on the 200,000 tweets, hence it performs very fast. In addition, in the OrMFN model, there is no need to find
nearest neighbors for the whole dataset in the 2nd step (the binary coding step).
494
0 0.2 0.4 0.6 0.8 10.27
0.28
0.29
0.3
0.31
0.32
0.33
0.34
MP
@1
00
0
wn
 
 
r=64
r=96
r=128
Figure 5: OrMFN model: MP@1000 vs. neighbor word weight w
n
7 Conclusion
In this paper, we proposed a novel unsupervised binary coding model which provides efficient similarity
search in massive tweet data. The proposed model, OrMFN, improves an existing matrix factorization
model through learning nearly orthogonal projection directions and leveraging the neighborhood infor-
mation hidden in tweet data. We collected a dataset whose groundtruth labels are created from Twitter
hashtags. Our experiments conducted on this dataset showed significant performance gains of OrMFN
over the competing methods.
Acknowledgements
We thank Boyi Xie and three anonymous reviewers for their valuable comments. This project is sup-
ported by the DARPA DEFT Program.
References
Puneet Agarwal, Rajgopal Vaithiyanathan, Saurabh Sharma, and Gautam Shroff. 2012. Catching the long-tail: Ex-
tracting local news events from twitter. In Proceedings of the Sixth International AAAI Conference on Weblogs
and Social Media.
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on
semantic textual similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM).
Edward Benson, Aria Haghighi, and Regina Barzilay. 2011. Event discovery in social media feeds. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.
Rahul Bhagat and Deepak Ravichandran. 2008. Large scale acquisition of paraphrases for learning surface pat-
terns. In Proceedings of ACL-08: HLT.
Andrei Z Broder, Moses Charikar, Alan M Frieze, and Michael Mitzenmacher. 1998. Min-wise independent
permutations. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing.
Deepayan Chakrabarti and Kunal Punera. 2011. Event summarization using tweets. In Proceedings of the Fifth
International AAAI Conference on Weblogs and Social Media.
Moses S. Charikar. 2002. Similarity estimation techniques from rounding algorithms. In Proceedings of the
Thiry-fourth Annual ACM Symposium on Theory of Computing.
Miles Efron. 2010. Information search and retrieval in microblogs. In Journal of the American Society for
Information Science and Technology.
Yunchao Gong and Svetlana Lazebnik. 2011. Iterative quantization: A procrustean approach to learning binary
codes. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.
495
Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics.
Weiwei Guo, Hao Li, Heng Ji, and Mona Diab. 2013. Linking tweets to news: A framework to enrich online
short text data in social media. In Proceedings of the 51th Annual Meeting of the Association for Computational
Linguistics.
Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimen-
sionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing.
Brian Kulis and Kristen Grauman. 2012. Kernelized locality-sensitive hashing. IEEE Transactions On Pattern
Analysis and Machine Intelligence, 34(6):1092?1104.
Thomas K. Landauer and Susan T. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis
theory of acquisition, induction and representation of knowledge. In Psychological review.
Yuhua Li, David McLean, Zuhair A. Bandar, James D. O?Shea, and Keeley Crockett. 2006. Sentence similarity
based on semantic nets and corpus statistics. IEEE Transaction on Knowledge and Data Engineering, 18.
Wei Liu, Jun Wang, Sanjiv Kumar, and Shih-Fu Chang. 2011. Hashing with graphs. In Proceedings of the 28th
International Conference on Machine Learning.
Wei Liu, Jun Wang, Rongrong Ji, Yu-Gang Jiang, and Shih-Fu Chang. 2012a. Supervised hashing with kernels.
In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.
Wei Liu, Jun Wang, Yadong Mu, Sanjiv Kumar, and Shih-Fu Chang. 2012b. Compact hyperplane hashing with
bilinear functions. In Proceedings of the 29th International Conference on Machine Learning.
Mohammad Norouzi and David J. Fleet. 2011. Minimal loss hashing for compact binary codes. In Proceedings
of the 28th International Conference on Machine Learning.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to
twitter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the Twentieth
International Conference on Machine Learning.
Harald Steck. 2010. Training and testing of recommender systems on data missing not at random. In Proceedings
of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
Julien Subercaze, Christophe Gravier, and Frederique Laforest. 2013. Towards an expressive and scalable twitter?s
users profiles. In IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent
Technologies.
Yair Weiss, Antonio Torralba, and Rob Fergus. 2008. Spectral hashing. In Advances in Neural Information
Processing Systems.
496
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 495?500,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Two-Stage Hashing for Fast Document Retrieval
Hao Li
?
Wei Liu
?
Heng Ji
?
?
Computer Science Department,
Rensselaer Polytechnic Institute, Troy, NY, USA
{lih13,jih}@rpi.edu
?
IBM T. J. Watson Research Center, Yorktown Heights, NY, USA
weiliu@us.ibm.com
Abstract
This work fulfills sublinear time Near-
est Neighbor Search (NNS) in massive-
scale document collections. The primary
contribution is to propose a two-stage
unsupervised hashing framework which
harmoniously integrates two state-of-the-
art hashing algorithms Locality Sensitive
Hashing (LSH) and Iterative Quantization
(ITQ). LSH accounts for neighbor candi-
date pruning, while ITQ provides an ef-
ficient and effective reranking over the
neighbor pool captured by LSH. Further-
more, the proposed hashing framework
capitalizes on both term and topic similar-
ity among documents, leading to precise
document retrieval. The experimental re-
sults convincingly show that our hashing
based document retrieval approach well
approximates the conventional Informa-
tion Retrieval (IR) method in terms of re-
trieving semantically similar documents,
and meanwhile achieves a speedup of over
one order of magnitude in query time.
1 Introduction
A Nearest Neighbor Search (NNS) task aims at
searching for top K objects (e.g., documents)
which are most similar, based on pre-defined sim-
ilarity metrics, to a given query object in an ex-
isting dataset. NNS is essential in dealing with
many search related tasks, and also fundamen-
tal to a broad range of Natural Language Pro-
cessing (NLP) down-stream problems including
person name spelling correction (Udupa and Ku-
mar, 2010), document translation pair acquisition
(Krstovski and Smith, 2011), large-scale similar
noun list generation (Ravichandran et al, 2005),
lexical variants mining (Gouws et al, 2011), and
large-scale first story detection (Petrovic et al,
2010).
Hashing has recently emerged to be a popular
solution to tackling fast NNS, and been success-
fully applied to a variety of non-NLP problems
such as visual object detection (Dean et al, 2013)
and recognition (Torralba et al, 2008a; Torralba
et al, 2008b), large-scale image retrieval (Kulis
and Grauman, 2012; Liu et al, 2012; Gong et al,
2013), and large-scale machine learning (Weiss et
al., 2008; Liu et al, 2011; Liu, 2012). However,
hashing has received limited attention in the NLP
field to the date. The basic idea of hashing is to
represent each data object as a binary code (each
bit of a code is one digit of ?0? or ?1?). When
applying hashing to handle NLP problems, the ad-
vantages are two-fold: 1) the capability to store
a large quantity of documents in the main mem-
ory. for example, one can store 250 million doc-
uments with 1.9G memory using only 64 bits for
each document while a large news corpus such as
the English Gigaword fifth edition
1
stores 10 mil-
lion documents in a 26G hard drive; 2) the time
efficiency of manipulating binary codes, for ex-
ample, computing the hamming distance between
a pair of binary codes is several orders of magni-
tude faster than computing the real-valued cosine
similarity over a pair of document vectors.
The early explorations of hashing focused on
using random permutations or projections to con-
struct randomized hash functions, e.g., the well-
known Min-wise Hashing (MinHash) (Broder et
al., 1998) and Locality Sensitive Hashing (LSH)
(Andoni and Indyk, 2008). In contrast to such
data-independent hashing schemes, recent re-
search has been geared to studying data-dependent
hashing through learning compact hash codes
from a training dataset. The state-of-the-art unsu-
pervised learning-based hashing methods include
Spectral Hashing (SH) (Weiss et al, 2008), An-
chor Graph Hashing (AGH) (Liu et al, 2011),
and Iterative Quantization (ITQ) (Gong et al,
1
http://catalog.ldc.upenn.edu/LDC2011T07
495
2013), all of which endeavor to make the learned
hash codes preserve or reveal some intrinsic struc-
ture, such as local neighborhood structure, low-
dimensional manifolds, or the closest hypercube,
underlying the training data. Despite achieving
data-dependent hash codes, most of these ?learn-
ing to hash? methods cannot guarantee a high suc-
cess rate of looking a query code up in a hash ta-
ble (referred to as hash table lookup in literature),
which is critical to the high efficacy of exploit-
ing hashing in practical uses. It is worth noting
that we choose to use ITQ in the proposed two-
stage hashing framework for its simplicity and ef-
ficiency. ITQ has been found to work better than
SH by Gong et al (2013), and be more efficient
than AGH in terms of training time by Liu (2012).
To this end, in this paper we propose a novel
two-stage unsupervised hashing framework to si-
multaneously enhance the hash lookup success
rate and increase the search accuracy by integrat-
ing the advantages of both LSH and ITQ. Further-
more, we make the hashing framework applicable
to combine different similarity measures in NNS.
2 Background and Terminology
? Binary Codes: A bit (a single bit is ?0? or
?1?) sequence assigned to represent a data
object. For example, represent a document
as a 8-bit code ?11101010?.
? Hash Table: A linear table in which all bi-
nary codes of a data set are arranged to be
table indexes, and each table bucket contains
the IDs of the data items sharing the same
code.
? Hamming Distance: The number of bit po-
sitions in which bits of the two codes differ.
? Hash Table Lookup: Given a query q with
its binary code h
q
, find the candidate neigh-
bors in a hash table such that the Hamming
distances from their codes to h
q
are no more
than a small distance threshold . In practice
 is usually set to 2 to maintain the efficiency
of table lookups.
? Hash Table Lookup Success Rate: Given a
query q with its binary code h
q
, the probabil-
ity to find at least one neighbor in the table
buckets whose corresponding codes (i.e., in-
dexes) are within a Hamming ball of radius 
centered at h
q
.
? Hamming Ranking: Given a query q with
its binary code h
q
, rank all data items accord-
ing to the Hamming distances between their
codes and h
q
; the smaller the Hamming dis-
tance, the higher the data item is ranked.
3 Document Retrieval with Hashing
In this section, we first provide an overview of ap-
plying hashing techniques to a document retrieval
task, and then introduce two unsupervised hash-
ing algorithms: LSH acts as a neighbor-candidate
filter, while ITQ works towards precise reranking
over the candidate pool returned by LSH.
3.1 Document Retrieval
The most traditional way of retrieving nearest
neighbors for documents is to represent each docu-
ment as a term vector of which each element is the
tf-idf weight of a term. Given a query document
vector q, we use the Cosine similarity measure to
evaluate the similarity between q and a document
x in a dataset:
sim(q,x) =
q
>
x
?q??x?
. (1)
Then the traditional document retrieval method
exhaustively scans all documents in the dataset
and returns the most similar ones. However, such
a brute-force search does not scale to massive
datasets since the search time complexity for each
query is O(n); additionally, the computational
cost spent on Cosine similarity calculation is also
nontrivial.
3.2 Locality Sensitive Hashing
The core idea of LSH is that if two data points are
close, then after a ?projection? operation they will
remain close. In other words, similar data points
are more likely to be mapped into the same bucket
with a high collision probability. In a typical LSH
setting of k bits and L hash tables, a query point
q ? R
d
and a dataset point x ? R
d
collide if and
only if
h
ij
(q) ? h
ij
(x), i ? [1 : L], j ? [1 : k], (2)
where the hash function h
ij
(?) is defined as
h
ij
(x) = sgn
(
w
>
ij
x
)
, (3)
in which w
ij
? R
d
is a random projection di-
rection with components being independently and
identically drawn from a normal distribution, and
the sign function sgn(x) returns 1 if x > 0 and -1
otherwise. Note that we use ?1/-1? bits for deriva-
tions and training, and ?1/0? bits for the hashing
496
implementation including converting data to bi-
nary codes, arranging binary codes into hash ta-
bles, and hash table lookups.
3.3 Iterative Quantization
The central idea of ITQ is to learn the binary codes
achieving the lowest quantization error that en-
coding raw data to binary codes incurs. This is
pursued by seeking a rotation of the zero-centered
projected data. Suppose that a set of n data points
X = {x
i
? R
d
}
n
i=1
are provided. The data matrix
is defined as X = [x
1
,x
2
, ? ? ? ,x
n
]
>
? R
n?d
.
In order to reduce the data dimension from d to
the desired code length c < d, Principal Compo-
nent Analysis (PCA) or Latent Semantic Analy-
sis (LSA) is first applied to X. We thus obtain
the zero-centered projected data matrix as V =
(I ?
1
n
11
>
)XU where U ? R
d?c
is the projec-
tion matrix.
After the projection operation, ITQ minimizes
the quantization error as follows
Q(B,R) = ?B?VR?
2
F
, (4)
where B ? {1,?1}
n?c
is the code matrix each
row of which contains a binary code, R ? R
c?c
is the target orthogonal rotation matrix, and ? ? ?
F
denotes the Frobenius norm. Finding a local min-
imum of the quantization error in Eq. (4) begins
with a random initialization of R, and then em-
ploys a K-means clustering like iterative proce-
dure. In each iteration, each (projected) data point
is assigned to the nearest vertex of the binary hy-
percube, and R always satisfying RR
>
= I is
subsequently updated to minimize the quantiza-
tion loss given the current assignment; the two
steps run alternatingly until a convergence is en-
countered. Concretely, the two updating steps are:
1. Fix R and update B: minimize the follow-
ing quantization loss
Q(B,R) = ?B?
2
F
+ ?VR?
2
F
? 2tr
(
R
>
V
>
B
)
= nc+ ?V?
2
F
? 2tr
(
R
>
V
>
B
)
= constant? 2tr
(
R
>
V
>
B
)
,
(5)
achieving B = sgn(VR);
2. Fix B and update R: perform the SVD of
the matrix V
>
B ? R
c?c
to obtain V
>
B =
S?
?
S
>
, and then set R = S
?
S
>
.
Figure 1: The two-stage hashing framework.
3.4 Two-Stage Hashing
There are three main merits of LSH. (1) It tries to
preserve the Cosine similarity of the original data
with a probabilistic guarantee (Charikar, 2002).
(2) It is training free, and thus very efficient in
hashing massive databases to binary codes. (3) It
has a very high hash table lookup success rate. For
example, in our experiments LSH with more than
one hash table is able to achieve a perfect 100%
hash lookup success rate. Unfortunately, its draw-
back is the low search precision that is observed
even with long hash bits and multiple hash tables.
ITQ tries to minimize the quantization error of
encoding data to binary codes, so its advantage
is the high quality (potentially high precision of
Hamming ranking) of the produced binary codes.
Nevertheless, ITQ frequently suffers from a poor
hash lookup success rate when longer bits (e.g.,
? 48) are used (Liu, 2012). For example, in
our experiments ITQ using 384 bits has a 18.47%
hash lookup success rate within Hamming radius
2. Hence, Hamming ranking (costing O(n) time)
must be invoked for the queries for which ITQ
fails to return any neighbors via hash table lookup,
which makes the searches inefficient especially on
very large datasets.
Taking into account the above advantages and
disadvantages of LSH and ITQ, we propose a two-
stage hashing framework to harmoniously inte-
grate them. Fig. 1 illustrates our two-stage frame-
work with a toy example where identical shapes
denote ground-truth nearest neighbors.
In this framework, LSH accounts for neigh-
bor candidate pruning, while ITQ provides an ef-
ficient and effective reranking over the neighbor
pool captured by LSH. To be specific, the pro-
497
posed framework enjoys two advantages:
1. Provide a simple solution to accomplish both
a high hash lookup success rate and high precision,
which does not require scanning the whole list of
the ITQ binary codes but scanning the short list
returned by LSH hash table lookup. Therefore, a
high hash lookup success rate is attained by the
LSH stage, while maintaining high search preci-
sion due to the ITQ reranking stage.
2. Enable a hybrid hashing scheme combining
two similarity measures. The term similarity is
used during the LSH stage that directly works
on document tf-idf vectors; during the ITQ stage,
the topic similarity is used since ITQ works on
the topic vectors obtained by applying Latent se-
mantic analysis (LSA) (Deerwester et al, 1990)
to those document vectors. LSA (or PCA), the
first step in running ITQ, can be easily acceler-
ated via a simple sub-selective sampling strategy
which has been proven theoretically and empiri-
cally sound by Li et al (2014). As a result, the
nearest neighbors returned by the two-stage hash-
ing framework turns out to be both lexically and
topically similar to the query document. To sum-
marize, the proposed two-stage hashing frame-
work works in an unsupervised manner, achieves a
sublinear search time complexity due to LSH, and
attains high search precision thanks to ITQ. After
hashing all data (documents) to LSH and ITQ bi-
nary codes, we do not need to save the raw data in
memory. Thus, our approach can scale to gigan-
tic datasets with compact storage and fast search
speed.
4 Experiments
Data and Evaluations
For the experiments, we use the English portion
of the standard TDT-5 dataset, which consists of
278, 109 documents from a time spanning April
2003 to September 2003. 126 topics are anno-
tated with an average of 51 documents per topic,
and other unlabeled documents are irrelevant to
them. We select six largest topics for the top-K
NNS evaluation, with each including more than
250 documents. We randomly select 60 docu-
ments from each of the six topics for testing. The
six topics are (1). Bombing in Riyadh, Saudi Ara-
bia (2). Mad cow disease in North America (3).
Casablanca bombs (4). Swedish Foreign Minister
killed (5). Liberian former president arrives in ex-
ile and (6). UN official killed in attack. For each
document, we apply the Stanford Tokenizer
2
for
tokenization; remove stopwords based on the stop
list from InQuery (Callan et al, 1992), and apply
Porter Stemmer (Porter, 1980) for stemming.
If one retrieved document shares the same topic
label with the query document, they are true neigh-
bors. We evaluate the precision of the top-K candi-
date documents returned by each method and cal-
culate the average precision across all queries.
Results
We first evaluate the quality of term vectors and
ITQ binary codes by conducting the whole list
Cosine similarity ranking and hamming distance
ranking, respectively. For each query document,
the top-K candidate documents with highest Co-
sine similarity scores and shortest hamming dis-
tances are returned, then we calculate the average
precision for each K. Fig. 2(a) demonstrates that
ITQ binary codes could preserve document simi-
larities as traditional term vectors. It is interesting
to notice that ITQ binary codes are able to outper-
form traditional term vectors. It is mainly because
some documents are topically related but share
few terms thus their relatedness can be captured by
LSA. Fig. 2(a) also shows that the NNS precision
keep increasing as longer ITQ code length is used
and is converged when ITQ code length equals to
384 bits. Therefore we set ITQ code length as 384
bits in the rest of the experiments.
Fig. 2(b) - Fig. 2(e) show that our two-stage
hashing framework surpasses LSH with a large
margin for both small K (e.g., K ? 10) and
large K (e.g., K ? 100) in top-K NNS. It also
demonstrates that our hashing based document re-
trieval approach with only binary codes from LSH
and ITQ well approximates the conventional IR
method. Another crucial observation is that with
ITQ reranking, a small number of LSH hash ta-
bles is needed in the pruning step. For example,
LSH(40bits) + ITQ(384bits) and LSH(48bits) +
ITQ(384bits) are able to reach convergence with
only four LSH hash tables. In that case, we can
alleviate one main drawback of LSH as it usually
requires a large number of hash tables to maintain
the hashing quality.
Since the LSH pruning time can be ignored,
the search time of the two-stage hashing scheme
equals to the time of hamming distance rerank-
ing in ITQ codes for all candidates produced from
LSH pruning step, e.g., LSH(48bits, 4 tables) +
2
http://nlp.stanford.edu/software/corenlp.shtml
498
(a)
0 50 100 150
0.65
0.7
0.75
0.8
0.85
0.9
0.95
number of top?K returned documents
Precis
ion
 
 Traditional IRITQ(448bits)ITQ(384bits)ITQ(320bits)ITQ(256bits)ITQ(192bits)
(b)
1 2 3 4 5 6 7 8 9 100
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
number of hash tables
Top?1
0 Prec
ision
 
 LSH(64bits)LSH(56bits)LSH(48bits)LSH(40bits)
(c)
1 2 3 4 5 6 7 8 9 100.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
number of hash tables
Top?1
0 Prec
ision
 
 
Traditional IRLSH(40bits)+ITQ(384bits)LSH(48bits)+ITQ(384bits)LSH(56bits)+ITQ(384bits)LSH(64bits)+ITQ(384bits)
(d)
1 2 3 4 5 6 7 8 9 100
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
number of hash tables
Top?1
00 Pre
cision
 
 LSH(64bits)LSH(56bits)LSH(48bits)LSH(40bits)
(e)
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
number of hash tables
Top?1
00 Pre
cision
 
 
Traditional IRLSH(40bits)+ITQ(384bits)LSH(48bits)+ITQ(384bits)LSH(56bits)+ITQ(384bits)LSH(64bits)+ITQ(384bits)
(f)
1 2 3 4 5 6 7 8 9 100
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
number of hash tables
Perce
ntage
 
 LSH(40bits)LSH(48bits)LSH(56bits)LSH(64bits)
Figure 2: (a) ITQ code quality for different code length, (b) LSH Top-10 Precision, (c) LSH +
ITQ(384bits) Top-10 Precision, (d) LSH Top-100 Precision, (e) LSH + ITQ(384bits) Top-100 Precision,
(f) The percentage of visited data samples by LSH hash lookup.
ITQ(384bits) takes only one thirtieth of the search
time as the traditional IR method. Fig. 2 (f)
shows the ITQ data reranking percentage for dif-
ferent LSH bit lengths and table numbers. As the
LSH bit length increases or the hash table num-
ber decreases, a lower percentage of the candidates
will be selected for reranking, and thus costs less
search time.
The percentage of visited data samples by LSH
hash lookup is a key factor that influence the
NNS precision in the two-stage hashing frame-
work. Generally, higher rerank percentage results
in better top-K NNS Precision. Further more, by
comparing Fig. 2 (c) and (e), it shows that our
framework works better for small K than for large
K. For example, scanning 5.52% of the data is
enough for achieving similar top-10 NNS result
as the traditional IR method while 36.86% of the
data is needed for top-100 NNS. The reason of the
lower performance with large K is that some true
neighbors with the same topic label do not share
high term similarities and may be filtered out in
the LSH step when the rerank percentage is low.
5 Conclusion
In this paper, we proposed a novel two-stage un-
supervised hashing framework for efficient and ef-
fective nearest neighbor search in massive docu-
ment collections. The experimental results have
shown that this framework achieves not only com-
parable search accuracy with the traditional IR
method in retrieving semantically similar docu-
ments, but also an order of magnitude speedup in
search time. Moreover, our approach can com-
bine two similarity measures in a hybrid hashing
scheme, which is beneficial to comprehensively
modeling the document similarity. In our future
work, we plan to design better data representa-
tion which can well fit into the two-stage hash-
ing theme; we also intend to apply the proposed
hashing approach to more informal genres (e.g.,
tweets) and other down-stream NLP applications
(e.g., first story detection).
Acknowledgements
This work was supported by the U.S. ARL
No. W911NF-09-2-0053 (NSCTA), NSF IIS-
0953149, DARPA No. FA8750- 13-2-0041, IBM,
Google and RPI. The views and conclusions con-
tained in this document are those of the authors
and should not be interpreted as representing the
official policies, either expressed or implied, of the
U.S. Government. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernment purposes notwithstanding any copyright
notation here on.
499
References
A. Andoni and P. Indyk. 2008. Near-optimal hash-
ing algorithms for approximate nearest neighbor in
high dimensions. Communications of the ACM,
51(1):117?122.
A. Z. Broder, M. Charikar, A. M. Frieze, and
M. Mitzenmacher. 1998. Min-wise independent
permutations. In Proc. STOC.
J. P. Callan, W. B. Croft, and S. M. Harding. 1992. The
inquery retrieval system. In Proc. the Third Interna-
tional Conference on Database and Expert Systems
Applications.
M. Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proc. STOC.
T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-
narasimhan, and J. Yagnik. 2013. Fast, accurate
detection of 100,000 object classes on a single ma-
chine. In Proc. CVPR.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. JASIS, 41(6):391?407.
Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.
2013. Iterative quantization: A procrustean ap-
proach to learning binary codes for large-scale im-
age retrieval. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 35(12):2916?2929.
S. Gouws, D. Hovy, and D. Metzler. 2011. Unsuper-
vised mining of lexical variants from noisy text. In
Proc. EMNLP.
K. Krstovski and D. A. Smith. 2011. A minimally su-
pervised approach for detecting and ranking docu-
ment translation pairs. In Proc. the sixth ACL Work-
shop on Statistical Machine Translation.
B. Kulis and K. Grauman. 2012. Kernelized locality-
sensitive hashing. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 34(6):1092?
1104.
Y. Li, C. Chen, W. Liu, and J. Huang. 2014. Sub-
selective quantization for large-scale image search.
In Proc. AAAI Conference on Artificial Intelligence
(AAAI).
W. Liu, J. Wang, S. Kumar, and S.-F. Chang. 2011.
Hashing with graphs. In Proc. ICML.
W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang.
2012. Supervised hashing with kernels. In Proc.
CVPR.
W. Liu. 2012. Large-scale machine learning for classi-
fication and search. In PhD Thesis, Graduate School
of Arts and Sciences, Columbia University.
S. Petrovic, M. Osborne, and V. Lavrenko. 2010.
Streaming first story detection with application to
twitter. In Proc. HLT-NAACL.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
D. Ravichandran, P. Pantel, and E. H. Hovy. 2005.
Randomized algorithms and nlp: Using locality sen-
sitive hash functions for high speed noun clustering.
In Proc. ACL.
A. Torralba, R. Fergus, and W. T. Freeman. 2008a. 80
million tiny images: A large data set for nonpara-
metric object and scene recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
30(11):1958?1970.
A. Torralba, R. Fergus, and Y. Weiss. 2008b. Small
codes and large image databases for recognition. In
Proc. CVPR.
R. Udupa and S. Kumar. 2010. Hashing-based ap-
proaches to spelling correction of personal names.
In Proc. EMNLP.
Y. Weiss, A. Torralba, and R. Fergus. 2008. Spectral
hashing. In NIPS 21.
500
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 5?6,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Detecting Word Misuse in Chinese
Wei Liu
Department of Computer Science
University of Sheffield
W.Liu@dcs.shef.ac.uk
Abstract
Social Network Service (SNS) and personal
blogs have become the most popular platform
for online communication and sharing infor-
mation. However because most modern com-
puter keyboards are Latin-based, Asian lan-
guage speakers (such as Chinese) has to rely
on a input system which accepts Romanisation
of the characters and convert them into charac-
ters or words in that language. In Chinese this
form of Romanisation (usually called Pinyin)
is highly ambiguous, word misuses often oc-
cur because the user choose a wrong candi-
date or deliverately substitute the word with
another character string that has the identical
Romanisation to convey certain semantics, or
to achieve a sarcasm effect. In this paper we
aim to develop a system that can automati-
cally identify such word misuse, and suggest
the correct word to be used.
1 Introduction
A certain kind of derogatory opinion is being con-
veyed in Chinese chat forums and SNS sites through
the use of Chinese Hanzi (hieroglyphic) characters.
There is potential for this to happen whenever two
expressions are pronounced in a similar way in Chi-
nese. For exmaple, irate readers have used ????
(?Ji Zhe?) for ???? (?Ji Zhe?). While ????
means reporter or journalist, ???? can be inter-
preted as prostitute.
There are 5000 commonly used characters. While
the number of distinct Pinyin (toneless) is only 412.
Therefore Pinyin to character conversion is highly
ambigurous and is a active research topic (Zhou
et al, 2007), (Lin and Zhang, 2008), (Chen and
Lee, 2000). On the other hand, automatic Pinyin
generation is considered a solved task, (Liu and
Guthrie, 2009) shows that using the most frequent
Pinyin approach to assign Pinyin to each character
can achieve 98% accuracy. In fact, we test on the Gi-
gaword Chinese (Verson 2) corpus and find out that
only about 15% of the characters have ambigurous
Pinyin.
2 Automatically Detecting Word Misuse
We divided the detection process into three steps as
below:
? Segmentation: Given a piece of Chinese text,
we first feed it into an automatic word seg-
menter (Zhang et al, 2003) to break the text
into semantic units. Because we consider only
multiple-character anomaly cases, anomalies
can only be contained within sequences of sin-
gle characters.
? Character sequence extraction: After segmen-
tation, we are interested in sequences of sin-
gle characters, because anomalies will occur
only within those sequences. Once we obtain
these sequences, we generate all possible sub-
strings for each sequence because any anoma-
lous words can be part of a character sequence.
? Detection: We assume the anomaly shares
many phonetic similarities with the ?true?
word. As a result we need a method for
comparing pronunciations of two character se-
quences. Here we use the Pinyin to represent
phonetics of a Chinese character, and we de-
fine two pronunciations to be similar when they
both have identical Pinyin (not including the
tone). We use character-to-pinyin conversion
tool1 to create a Pinyin-to-Word hash table us-
ing the machine-segmented Chinese Gigaword
1http://pinyin4j.sourceforge.net/
5
ver. 2. Once we have the resources, we first
produce all possible Pinyin sequences of each
character sequence.Next we do a Pinyin-word
look up in the hash table we created; if there
exists any entries, we know that the Pinyin se-
quence maps to one or more ?real? words. Con-
sequently, we consider any character sequences
whose Pinyin maps to these words to be possi-
ble anomalies.
3 Data and Experiments
We have conducted preliminary experiments to test
our algorithm. To start with, we manually gath-
ered a small number of documents which contain
anomalous phrases of the type described above. The
documents are gathered from internet chat-rooms
and contain 3,797 Chinese characters: the anoma-
lies herein are shown in table 1.
Intended Misused Pinyin Freq
word character seq.
?? ?? Mei guo 43
(The U.S.)
?? ?? Jiao shou 23
(Professor)
?? ??or?? Ou xiang 12
(Role model)
Table 1: Testing document
3.1 Results and Discussions
We evaluate our identification/correction perfor-
mance using standard measures of standard preci-
sion and recall. We tested our performance using
bigram thresholds of 0, 1 and 2.
Table 2 shows the performances of our method.
No. of misused chararcter sequence 78
Total identified 130
Correctly identified 78
Precision 60%
Recall 100%
F-measure 75%
Table 2: Result for word misuse identification
The initial experiments showed that our method
can successfully identify and correct the three ex-
amples of non-word anomalies with reasonable pre-
cision and recall. The method obtains 100% recall
however it generates a lot of false positives; this can
be seen in a relatively low precision of 60%.
In summary, our method is successful at iden-
tifying genuine anomalous non-word character se-
quences; however the method also retrieves some
false positives, due to the highly ambiguous Pinyin
to word mappings.
4 Future Work
Our experiments shows that our preliminary method
can detect word misuses due to the Pinyin sequence
being idential but with a relatively high false posi-
tives. In the future we plan to use other contextual
evidence, such as pointwise mutual information to
model whether the candidate sequence generated by
our method is a better fit than the original sequence.
We also plan to gather more real data that contain
misuse of our interests.
References
Chen, Z. and Lee, K.-F. (2000). A new statistical ap-
proach to chinese pinyin input. In In Proceedings of
the 38th Annual Meeting on Association for Computa-
tional Linguistics, pages 241?247, Hong Kong.
Lin, B. and Zhang, J. (2008). A novel statistical chi-
nese language model and its application in pinyin-to-
character conversion. In CIKM ?08: Proceeding of the
17th ACM conference on Information and knowledge
management, pages 1433?1434, New York, NY, USA.
ACM.
Liu, W. and Guthrie, L. (2009). Chinese pinyin-text
conversion on segmented text. In TSD ?09: Pro-
ceedings of the 12th International Conference on Text,
Speech and Dialogue, pages 116?123, Berlin, Heidel-
berg. Springer-Verlag.
Zhang, H.-P., Liu, Q., Cheng, X.-Q., Zhang, H., and Yu,
H.-K. (2003). Chinese lexical analysis using hierar-
chical hidden markov model. In Proceedings of the
second SIGHAN workshop on Chinese language pro-
cessing, pages 63?70, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Zhou, X., Hu, X., Zhang, X., and Shen, X. (2007). A
segment-based hidden markov model for real-setting
pinyin-to-chinese conversion. In CIKM ?07: Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
pages 1027?1030, New York, NY, USA. ACM.
6
