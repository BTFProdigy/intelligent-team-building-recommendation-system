61
A Computational Model of Emergent Simple Syntax: Supporting the Natural 
Transition from the One-Word Stage to the Two-Word Stage.  
Kris Jack, Chris Reed, and Annalu Waller 
Division of Applied Computing,  
University of Dundee, 
Dundee, Scotland, DD1 4HN 
[kjack | creed | awaller]@computing.dundee.ac.uk 
 
Abstract 
This paper introduces a system that simulates 
the transition from the one-word stage to the 
two-word stage in child language production.  
Two-word descriptions are syntactically 
generated and compete against one-word 
descriptions from the outset.  Two-word 
descriptions become dominant as word 
combinations are repeatedly recognised, 
forming syntactic categories; resulting in an 
emergent simple syntax.  The system 
demonstrates a similar maturation as children 
as evidenced by phenomena such as 
overextensions and mismatching, and the use 
of one-word descriptions being replaced by 
two-word descriptions over time. 
1 Introduction 
Studies of first language acquisition in children 
have documented general stages in linguistic 
development.  Neither the trigger nor the 
mechanism that takes a child from one stage to the 
next are known.  Stages arise gradually with no 
precise start  or end points, overlapping one 
another (Ingram, 1989). 
The aim of this research is to develop a system 
that autonomously acquires conceptual 
representations of individual words (the ?one-word 
stage?) and also, simultaneously, is capable of 
developing representations of valid multi-word 
structures i.e. simple syntax (the ?two-word 
stage?).  Two-word descriptions are expected to 
emerge as a result of the system state and not be 
artificially triggered. 
The system accepts sentences containing a 
maximum of two words.  It is designed to be 
scalable, allowing larger, more natural sentence 
sizes also.  System input is therefore a mixture of 
both one-word and two-word sentences.  The 
system is required to produce valid descriptions, 
particularly in the two-word stage.  Rules that 
enforce syntactic order, and allow for the 
production of semantically correct descriptions 
from novel concepts, are desirable. 
This paper is sectioned as follows;  pre-one-
word stage linguistic abilities in children are 
briefly discussed to explain why initial system 
functionality assumptions are made; the defining 
characteristics of both the one-word stage and two-
word stage in children are introduced as possible 
benchmarks for the system; a detailed description 
of system design and implementation with 
examples of the learning process and games played 
by the system are presented; a discussion of current 
results along with their possible implications 
follows; a brief review of related works that have 
influenced this research, citing major influences; 
the direction and aims of future research is 
described briefly; and finally, conclusions are 
drawn. 
2 Pre-One-Word Stage Children 
Linguistic abilities can be found in children prior 
to word production.  In terms of comprehension, 
children can distinguish between their mother?s 
voice and a stranger?s voice, male and female 
voices, and sentences spoken in their mother?s 
native language and sentences spoken in a different 
language.  They also show categorical perception 
to voice, can use formant transition information to 
mark articulation, and show intonation sensitivity 
(Pinker, 1994, Jusczyk, 1999). 
In terms of production, children produce noises, 
such as discomfort noises (0-2 months), comfort 
noises (2-4 months), and ?play? vocally with pitch 
and loudness variations (4-7 months) (Pinker, 
1994).  The babbling stage (6-8 months) is 
characterised with the production of recognisable 
syllables.  The syllables are often repeated, such as 
[mamama] and [papapa], with the easiest to 
produce sounds often being associated with 
members of the family (Jakobson, 1971). 
From this evidence it is reasonable to draw 
conclusions about linguistic abilities in the young 
child that can be used to frame assumptions for use 
in the system.  It is assumed that the system can 
receive and produce strings that can be broken 
down into their component words.  These words 
can be compared and equalities can be detected. 
62
3 One-Word Stage and Two-Word Stages 
The system is required to produce one-word 
descriptions in early stages that develop into two-
word descriptions, where appropriate, in latter 
stages..  The recognition of each stage is based on 
the number of words that the system uses at a 
particular point.  In children, the one and two-word 
stages have notable features. 
The one-word, or holophrastic, stage (9-18 
months), is characterised by one-word 
vocalisations that are consistently associated with 
concepts.  These concepts can be either concrete or 
abstract, such as ?mama?, referring to the concrete 
concept of the child?s mother, and ?more?, an 
abstract concept which can be applied in a variety 
of situations (Piaget, 1960). 
Two phenomena that occur during this stage are 
underextensions and overextensions.  An 
underextension is the formation of a word to 
concept association that is too narrow, such as 
?dog? referring to only the family dog.  
Overextension, similarly, is an association that is 
too broad, such as ?dog? referring to all four 
legged animals.  Mismatches, or idiosyncratic 
referencing also occur, resulting in a word being 
associated with an unrelated concept, such as 
?dog? referring to a table (Pinker, 1994).  These 
associations change over time. 
The two-word stage (18-24 months) introduces 
simple syntax into the child?s language faculty.  
Children appear to determine the most important 
words in a sentence and, almost all of the time, use 
them in the same order as an adult would 
(Gleitman and Newport, 1995).  Brown (1973) 
defines a typology to express semantic relations in 
the two-word stage.  It contains ten sets of 
relations, but only one will be considered in this 
paper; attribute  + entity (?red circle?).  During this 
stage, children already demonstrate a three word 
comprehension level (Tomasello and Kruger, 
1992).  The concepts relating to their sentences 
may therefore be more detailed than the phrases 
themselves. 
The system is expected to make the transition 
from the one-word stage to the two-word stage 
without changes to the functionality of the system.  
Once the system begins to run, input is restricted to 
that of sensory (concept based) and vocal (string 
representation) data. 
4 System Design and Implementation 
4.1 Introduction 
The system is designed to learn phrase-to-
concept associations and demonstrate it through 
playing games: a guessing game and a naming 
game.  Games are often used to test, and encourage 
system learning (Steels and Kaplan, 2001).  The 
learning process involves a user selecting an object 
in a scene and naming it.  The guessing game 
involves a user saying a phrase, and the system 
pointing to the object that the phrase refers to.  The 
naming game involves a user pointing to an object 
and the system naming it  The system is not 
physically grounded, so all games are simulated. 
The learning process allows the system to 
acquire associations between phrases and concepts 
while the games test system comprehension and 
system production respectively.  The learning 
process takes a string and concept as input, and 
produces no output.  Comprehension takes a string 
as input, and produces a concept as output, 
whereas production takes a concept as input, and 
produces a string as output. 
4.2 Strings and Concepts 
A string is a list of characters with a fixed order.  
A blank space is used to separate words within the 
string, of which there can be either one or two.  
The system can break strings down into their 
component words. 
A concept is a list of feature values.  The 
system recognises six feature values; red, blue, 
green, white, circle, and square.  There are no in-
built associations between any of the feature 
values.  This form of learning is supported by the 
imageability theory (Paiviom 1971).  No claims 
concerning concept acquisition and formation are 
made in this paper.  All concepts are hard coded 
from the outset. 
The full list of objects used in the games are 
derived from shape and colour combinations; red 
square, red circle, blue square, blue circle, green 
square, green circle, white square, and white 
circle.  Individual feature values can also act as 
concepts, therefore the full list is concepts is the 
list of object plus the list of feature values. 
4.3 Groups 
To associate a string with a concept, the system 
stores a list of groups.  Each group contains an ID, 
one or more description pairs, an observed 
frequency, and zero or more occurrence 
supporter links. 
The ID acts as a unique identifier, allowing the 
group to be found.  A description pair is a string 
and a concept.  Groups must have at least one 
description pair since their primary function is to 
relate a string to a concept.  The observed 
frequency represents the number of times that the 
description pair?s components have been 
associated through system input. 
The occurrence supporter links are a set of group 
IDs.  Each ID in the set refers to a group that 
63
contains a superset of either the description pair, or 
the same value for one component of the 
description pair and a superset of the other e.g. The 
description pair [?red?; red] 1 would be supported 
by the description pair [?red square?; red square].  
A worked example is provided in the next section.  
The links therefore record the number of 
occurrences of the group?s description pair.  The 
occurrence supporter link reinforces the 
description pair?s association and increases the 
total frequency of the group.  The total frequency 
is the group?s observed frequency plus the 
observed frequency of all of its supporters, never 
including a supporter more than once.   
Finally, group equality is defined by groups 
sharing the same description pair. 
4.4 The Learning Process 
At each stage in the learning process, a 
description pair is entered into the system.  The 
system does not attempt to parse the correctness of 
the description.  All data is considered to be 
positive.  The general learning process algorithm is 
detailed in the rest of this section.  Specific 
examples are also provided in Table 1, showing the 
groups? values; ID, description pair, occurrence 
frequency (OF), occurrence supporter links 
(OSLs), and total frequency (TF).  Five steps are 
followed to incorporate the new data: 
1. Identify the description pair. 
2. Find equal and unequal parts. 
3. Update system based on equal parts.. 
4. Update system based on unequal parts. 
5. Re-enter new groups into the system. 
4.4.1 Identify the description pair 
If the description pair exists in a group that is 
already in the system, then that group?s observed 
frequency is incremented.  Otherwise, the system 
creates a new group containing the new 
description.  It is given a unique ID and an 
observed frequency of one.  Assume that the 
system already contains a group based on the 
description pair [?red circle?; red circle].  This has 
an ID of one.  Assume also that the new 
description pair entered is [?red square?; red 
square].  Its group has an ID of two (group #2). 
All description pairs entered into the system are 
called concrete description pairs, this is, the 
system has encountered them directly as input.  
The new group is referred to as a concrete group, 
since it contains a concrete description pair. 
 
 
                                                   
1
 The convention of strings appearing in quotes 
(? red? ), and concepts appearing in italics (red) is 
adopted throughout this paper. 
 
ID Description Pair OF OSLs TF 
#1 [? red circle? ; red circle] 1 [] 1 
#2 [? red square? ; red 
square]
1 [] 1 
#3 [? red? ; red] 0 [#1,# 2] 2 
#4 [? #3 circle? ; #3 circle] 0 [#1] 1 
#5 [? #3 square? ; #3
square]
0 [#2] 1 
#6 [? circle? ; circle],
[? square? ; square]
0 [] 0 
#7 [? #3 #6? ; #3 #6] 0 [#2] 1 
Table 1: Sample data 
4.4.2 Find equal and unequal parts 
The new group is compared to all of the groups 
in the system.  Comparisons are based on the 
groups? description pairs alone.  Strings are 
compared separately from concepts.  A string 
match is found if one of the strings is a subset, or 
exact match, of the other.  Subsets of strings must 
contain complete words.  Words are regarded as 
atomic units.  Concepts are compared in the same 
fashion as strings, where feature values are the 
atomic units.  Successful comparisons create a set 
of equal parts and unequal parts.  Comparison 
results are only used when equal parts exist.  This 
approach is similar to alignment based learning, 
but with the additional component of concepts (van 
Zaanen, 2000). 
In comparing the new group, group #2, to the 
existing group, group #1, the equal part [?red?; 
red] and the unequal part [?circle?; circle], 
[?square?; square] are found.  The comparison 
algorithm is essential to the operation of the 
system.  It is used in the learning process and in the 
games.  Without it, no string or concept relations 
could be drawn2. 
4.4.3 Update system based on equal parts 
When an equal part is found, a new group is 
created.  In the example, an equal part is found 
between group #1 and group #2.  Group #3 is 
created as a result.  The new group is given an 
observed frequency of zero.  The IDs of the groups 
that were compared (group #1 and group #2) are 
added to the new group?s (group #3) occurrence 
supporter links.  If the group already exists, then as 
well as the existing group?s observed frequency 
being incremented, the IDs of the groups that were 
compared are added to the occurrence supporter 
links.  IDs can only appear once in the set of 
occurrence supporters links, so if an ID is already 
in it, then it is not added. 
                                                   
2
 The system assumes full compositionality.  Idioms 
and metaphors are not considered at this stage. 
64
Up until this point, all groups? description pairs 
have contained a string and concept.  Description 
pairs can also contain links to other groups? strings 
and groups? concepts.  These description pairs are 
referred to as abstract description pairs.  If all 
elements of the abstract description pair are links 
to other groups then it is fully abstract, else it is 
partially abstract.  A group that contains an 
abstract description pair is called an abstract 
group.  The group is fully abstract if its abstract 
description pair is fully abstract, else it is a 
partially abstract group.  Once a group has been 
created (as group #3 was), based on a description 
comparison, the system attempts to make two 
abstract groups. 
The new abstract groups (group #4 and group 
#5) are based on substitutions of the new group?s 
ID (group #3) into each of the groups that were  
originally compared.  Group #4 is therefore created 
by substituting group #3 into group #1.  Similarly, 
group #5 is created by substituting group #3 into 
group #2. 
The new abstract groups are given an observed 
frequency of zero (ID?s equal four and five).  Note 
that abstract groups always have an observed 
frequency of zero as they can never been directly 
observed.  The ID of the appropriate group used in 
comparison and later creation is added to the 
occurrence supporters links.  Each abstract group 
therefore has a total frequency equal to that of the 
group of which it is an abstract form. 
4.4.4 Update system based on unequal parts 
Unequal parts are only considered if equal parts 
are found in the comparison.  Otherwise, the 
unequal parts would be the complete set of data 
from both groups, which does not provide useful 
information for comparisson.  For every set of 
unequal parts that is found, a new group is created.  
If there is more than one unequal part then the 
group will contain more than one description pair.  
Such a group is referred to as a multi-group.  Two 
unequal parts were found earlier in comparing 
group #1 and group #2.  They are [?circle?; circle] 
and [?square?; square].  Group #6 is therefore 
created using these two description pairs. 
The creation of a multi-group allows for a fully 
abstract group to be created.  The system uses the 
data from the new multi-group (group #6) and the 
group created through equal parts (group #3).  
Both groups are substituted back into the group 
that was originally being compared (group #1).  
The resulting group (group #7) is fully abstract as 
both equal parts and unequal parts have been used 
to reconstruct the original group (group #1). 
4.4.5 Re-enter new groups into the system 
All groups that have been created through steps 
3 and 4 are compared to all other groups in the 
system.  Results of comparisons are dealt with by 
repeating steps 3-5 with the new results.  By use a 
recursive step like this, all groups are compared to 
one another in the system.  All group equalities are 
therefore created when the round is complete.  The 
amount of information available from every new 
group entered into the system is therefore 
maximised. 
4.5 The Significance of Groups Types 
Four different types of group have been 
identified in the previous section.  Although all 
groups share the same properties, they can be seen 
to represent difference aspects of language.  It is 
the combination and interaction of these groups 
that gives rise to emergent simple syntax.  This 
syntax is bi-gram collocations, but since the system 
is scalable, it is referred to as simple syntax. 
4.5.1 Concrete Groups 
Concrete groups acquire the meaning of 
individual lexemes (associate concepts with 
strings).  They are verifiable in the real world 
through the use of scene based games. 
4.5.2 Multi-Groups 
Multi-groups form syntactic categories based on 
similarities between description pair usage.  Under 
the current system, groups can only have a 
maximum of two description pairs.  If this were to 
be expanded, it is clear that large syntactic 
categories such as noun and verb equivalents 
would arise. 
4.5.3 Partially and Fully Abstract Groups 
Partially and fully abstract groups act as phrasal 
rules in the system.  Abstract values contained 
within the group?s description pairs can relate to 
both concrete groups and multi-groups.  Abstract 
groups that relate to multi-groups offer a choice of 
substitutions. 
For example, group #7 (Table 1) relates a single 
group to a multi-group.  By substitution of groups 
#3 and #6 into group #7, the concrete pairings of 
[?red circle?; red circle] and [?red square?; red 
square] are produced.  The string data are directly 
equivalent to: 
S -> Adj. N, 
where Adj. = {?red?} 
and N = {?circle?, ?square?} 
When a description pair is entered into the 
system, the process of semantic bootstrapping 
takes place.  Lexical items (strings) are associated 
with their meanings (concepts). When group 
65
comparisons are made, syntactic bootstrapping 
begins.  Associations are made between all 
combinations of lexical items throughout the 
system, and all combinations of meanings 
throughout the system. 
The system stores lexical item-meaning 
associations, lexical item-lexical item associations 
and meaning-meaning associations.  This basic 
framework allows for the production of complex 
phrasal rules. 
4.6 Comprehension and Production Through 
Games 
The guessing game tests comprehension while 
the naming game test production.  Comprehension 
takes a string as input, and produces a concept as 
output, whereas production takes a concept as 
input, and produces a string as output.  The 
comprehension and the production algorithms are 
the same, except the first is string based, and the 
second is concept based.  
The algorithm performs two tasks: finding 
concrete groups with exact matches to the input, 
and finding abstract groups with possible matches 
to the input.  Holophrastic matching uses only 
concrete groups.  Syntactic matching performs 
holophrastic matching, followed by further 
matches using abstract groups.  Note that the 
system only performs syntactic matching, which 
includes holophrastic matching.  Holophrastic 
matching is never performed alone, unless in 
testing stages. 
For holophrastic matches, the system searches 
through its list of groups.  Their description pairs 
are compared to the input being searched for.  
There is therefore re-use of the comparison 
algorithm introduced in the learning process.  
When a match is found, the group is added to a list 
of possible results. 
If holophrastic matching is being performed 
alone, then this list of possible results is sorted by 
total frequency.  The group with the highest total 
frequency is output by the system. 
Syntactic matching begins by performing 
holophrastic matching, but does not output a result 
until all abstract groups have been matched too.  It 
is therefore an extension of holophrastic matching.  
Once a first fun of holophrastic matching is 
performed, the input is converted into abstract 
form.  This is performed at the word/feature value 
level.  The most likely element is found by 
searching through the groups, comparing it to the 
description pair, and selecting the group with the 
highest total frequency from those found. 
The group IDs replace the appropriate element in 
the input (just as substitutions were made during 
the learning process).  All multi-groups that 
contain any of the abstract forms are found.  Each 
multi-group?s description pair becomes a 
replacement for the appropriate input?s abstract 
value. 
The new input, which is still in abstract form, is 
searched for, using holophrastic matching again.  
Since the groups found are not exact matches of 
the original input, their total frequency is 
multiplied by an abstract factor.  The abstract 
factor is a value between zero and one inclusive.  
The higher the factor, the greater the effect that 
abstract groups have on the results.  Syntactic 
matches can therefore  produce different results 
based on the value of abstract factor.  The abstract 
factor is not changed from the initiation to 
termination of the system. 
Groups found during the search are added to a 
new list of possible results.  The appropriate 
elements are substituted into the groups abstract 
values to make them concrete.  If an abstract value 
is acting as a substitute (by being found originally 
in a multi-group) then the original input value is 
used, not the replacement element.  This allows the 
abstract group to act as a syntactic rule, but it is 
penalised by the abstract factor so it does not have 
as much influence as concrete groups, that have 
been found to occur through direct input 
associations. 
The groups found throughout the entire syntactic 
search are now contained in a second list of 
possible results.  This list is reduced by removing 
duplicate groups.  For each group that is removed, 
its observed frequency and occurrence supporter 
links are added to the duplicate that is kept in the 
list. 
The two lists from each matching routine are 
merged and sorted by total frequency.  The 
string\concept of the group with the highest total 
frequency is outputted by the system. 
5 Testing and Results 
The system is tested within the following areas: 
1. Comprehension and production of all 
fourteen concepts.  The rate at which full 
comprehension and full production are 
achieved is compared. 
2. Correctness of production matches for 
compound concepts.  The correctness of 
production matches are studied over a 
number of rounds. 
3. Type of production matches for compound 
concepts.  The type of production matches 
favoured, holophrastic or syntactic, are 
compared over a number of rounds 
A match of concept to word or word to concept 
is considered correct if the string describes the 
concept fully.  For example, [?red?; red] and [?red 
66
square?;  red square] are correct, but [?red?; red 
square] and [?red square?; red] are incorrect.  One 
point is given for each correct match, zero for each 
incorrect match. 
Note that all test results are based on the average 
of ten different system trials.  Each result shows a 
broad tendency that will likely be smoothed if 
more trials are run.  All input is randomly 
generated.  The abstract factor is set to 0.4 for all 
tests. 
5.1 Comprehension Vs. Production 
Full comprehension occurs much sooner (see 
Figure 1), on average, than full production.  This 
result is found in children also.  Although 
production and comprehension compete quite 
steadily in early stages of the system, 
comprehension reaches its maximum, on average, 
in 20% of the time that production takes to reach 
its maximum. 
0
2
4
6
8
10
12
14
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49
Production
Comprehension
 
Figure 1: Shows number of correct 
comprehension and production matches 
Full comprehension (fourteen points) is 
achieved, on average, by round 50, while full 
production comes at round 250.  Both holophrastic 
data and syntactic data contribute to the successes.  
Underextensions are found during comprehension.  
For example,  in early rounds, ?green? is used to 
describe only green squares.  This phenomena is 
quickly eliminated in the trials but with a larger set 
of concepts and vocabulary, it is likely to persist 
for more than a few rounds. 
5.2 Correctness of Holophrastic Vs Syntactic 
Matches 
At the end of each round, production is tested 
using the eight compound concepts alone.  These 
are based on the eight observable objects in the 
simulated scene.  Only compound concepts can 
demonstrate simple syntax in this system, as 
singular concepts have associations to single word 
strings. 
 The system uses syntactic matching alone, but 
syntactic matching includes holophrastic matching, 
as discussed earlier.  To determine whether 
holophrastic data is being used, or syntactic data 
when a syntactic match is run, the matching 
algorithm has been split.  The number of correct 
strings produced using holophrastic data and the 
number of correct strings produced using syntactic 
data alone are compared (see Figure 2). 
The data demonstrate that the system uses 
mostly holophrastic matches in early rounds 
(comparable to the one-word stage).  This is 
eliminated in further rounds, in favour or syntactic 
matches alone (the two-word stage).  Note that 
although the holophrastic stage may appear to be 
producing two-words, these words are considered 
to be one-word.  For example, ?allgone? is 
considered to be one-word in early stages of 
linguistic development, as opposed to ?all gone? 
(Ingram, 1989). 
0
1
2
3
4
5
6
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73
Holophrastic
Syntactic
 
Figure 2: Shows number of correct holophrastic 
and syntactic matches. 
The syntactic data continues to rise, until it 
achieves full production.  The holophrastic stage 
never achieves full production, but peaks, then 
reduces to zero.  This trend occurs as holophrastic 
underextensions such as ?red? representing red 
square become more likely than ?red square? 
representing red square. 
Early syntactic matches are based on novel 
string productions for novel string concepts.  
Holophrastic matching is incapable of producing 
novel strings from novel concepts, as it deals with 
concrete concepts.  Abstract concepts however, 
allow new string combinations to be produced, 
such as ?blue square?, from blue square even 
though neither then string nor concept have been 
encountered before.  Such an abstraction may 
come from a multi-group that associates ?blue? 
with ?red?, while containing a group that contains 
?red square? also.  The novel string ?blue square? 
is therefore abstracted. 
5.3 Use of Holophrastic Vs Syntactic Matches 
The system does not always produce the correct 
strings when a concept is entered.  The strings that 
are produced are a result of either holophrastic or 
syntactic matching.  Regardless of correctness, the 
amount of times that holophrastic matches are 
made over syntactic matches can be compared (see 
Figure 3). 
67
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73
Syntactic
Holophrastic
 
Figure 3: Shows distribution of holophrastic and 
syntactic matches. 
The system relies completely on one-word 
descriptions at the outset, but soon syntactically 
derived two-word descriptions become prevalent.  
It is likely that the one-word stage will last longer 
if larger concept and vocabulary sets are in use. 
The system shows the same form of transition as 
can be seen in children from the one-word stage to 
the two-word stage, without the use of an artificial 
trigger.  The shift is gradual although the use of 
larger concept and vocabulary sets, plus different 
abstract factor values will affect the transition.  
The greater the number of words in multi-groups 
(the greater the size of syntactic categories), the 
lower the abstract factor is required to encourage 
the emergence of simple syntax. 
6 Related Works 
Supporters of computational modelling in 
language acquisition, often promote the practical 
importance of running simulations, where 
evolutionary effects can be recreated in short time 
periods (Zuidema, 2001). 
Although this paper is focussed on an individual 
system, or agent, acquiring language, it is been 
influenced by research into social learning 
(Oliphant and Batali, 1997; Kirby, 1999; Steels 
and Kaplan, 2002).  Social learning demonstrates 
the convergence upon a common language, or set 
of languages, from an uncoordinated proto-
language, within a population of agents.  Social 
learning allows for the playing of games between 
agents, similar to those in this paper, with the 
results being used as further system input, to 
support, or deny associations.  This research can be 
viewed as a form of social learning with one agent 
(string and concept generator) performing the 
teacher role, and the other agent (the system) 
performing the learner role. 
Simulations of both the babbling stage and the 
one-word stage have been developed (Scheler, 
1997; Abidi, 1999).  ACCLAIM, a one-word stage 
simulator, demonstrates that systems can react 
appropriately to changes in situations.  For 
example, when a cessation event is triggered, it 
produces ?Stop?, and when an object is requested, 
it produces ?More?.  Both examples are typical of 
children during the one-word stage (Bloom, 1973). 
Several systems exist that use perceptions to 
encourage language acquisition (Howell, Becker, 
and Jankowicz,, 2001; Roy, 2001).  ELBA learns 
both nouns and verbs from video scenes, starting 
with a blank lexicon.  Such systems have helped in 
the selection of both appropriate input sources and 
feature values to use in this research.  This system 
will also be physically grounded in future. 
The research presented in this paper describes a 
system that drives linguistic development.  Other 
systems have used similar techniques, based on 
syntactic and semantic bootstrapping (Howell and 
Becker, 2001), but have not explained how 
multiple word acquisition is achieved from a single 
word basis. 
Steels (1998) introduces frames that group 
lexical elements together by the roles that they 
play, very similar to groups in this paper.  Frames 
are more dynamic than groups however, 
structurally adapting when words reoccur.  Groups 
do not adapt in this way.  New groups are created 
to describe similarities rather than adapting 
existing ones.  Steels also introduces multiple word 
sentences, but it is unclear as to why agents invent 
a multiple word description over creating a new 
single word description.  The invention is triggered 
and does not emerge.  This research is based on 
real multiple word inputs, so the reason for 
invention is not necessary, unlike the reason for 
adoption i.e. why the system adopts two-word 
descriptions. 
The comparison algorithm, as previously noted, 
is similar to alignment based learning (van Zaanen, 
2000).  The system in this research performs 
perfect alignment requiring exact word matches 
when finding equal parts and unequal parts.  This 
system also uses concepts, reducing the number of 
incorrect groupings, or constituents, when there is 
ambiguity in text.  Unsupervised grammar 
induction can also be found in EMILE (van Zaanen 
and Adriaans, 2001).  EMILE identifies 
substitution classes by means of clustering.  These 
classes are comparable to this system?s groups 
although no concepts are used. 
7 Future Research 
As the system stands, it uses a small input set.  
Further developments are focussed on expanding 
the system.  All ten of Brown?s relations should be 
implemented.  Larger concept and vocabulary sets 
are therefore required.  Extensions to these sets are 
likely to affect underextensions, mismatches, the 
length of pre-syntactic usage time, and the overall 
growth pattern of simple syntax. 
68
8 Conclusion 
This paper offers a potential explanation of the 
mechanism by which the two-word stage emerges 
from the one-word stage.  It suggests that syntactic 
data is sought out from the beginning of language 
acquisition.  This syntactic data is always 
competing with the associations of holophrastic 
data.  Syntax is strengthened when patterns are 
consistently found between strings and concepts, 
and is used in favour of holophrastic data when it 
is sufficiently frequent.  The simple syntax 
continues to grow in strength, ultimately being 
used in favour of holophrastic data in all 
production and comprehension tasks. 
This system provides the foundation for more 
complex, hierarchical, syntax to emerge.  The type 
and volume of input is the only constraint upon the 
system.  The entry into post two-word stages is 
predicted from the system?s robust architecture. 
9 Acknowledgements 
The first author is sponsored by a studentship 
from the EPSRC. 
Thanks to the workshop reviewers for their 
helpful and much appreciated advice. 
References  
S. Abidi, 1996.  A Neural Network Simulation of Child 
Language Development at the One-word Stage.  In
proceedings of IASTED Int. Conf. on Modelling,
Simulation and Optimization, Gold Coast, Australia. 
L. Bloom, 1973.  One Word at a Time.  The use of 
single-word utterances before syntax  The Hague, 
Mouton. 
R.W. Brown, 1986.  Language and categories. In ? A
Study of Thinking? , ed. J.S. Bruner, J.J. Goodnow, 
and G.A. Austin, pages 247-312. New York: John 
Wiley, 1956. Reprint, New Brunswick: Transaction. 
L.R.. Gleitman and Elissa L. Newport, 1995.  The 
Invention of Language by Children: Environmental 
and Biological Influences on the Acquisition 
Language.  In ? An Invitation to Cognitive Science? , 
L.R. Gleitman and M. Liberman, 2nd ed., Vol.1, 
Cambridge, Mass., London, MIT Press. 
S.R. Howell and S. Becker, 2001.  Modelling language 
acquisition: Grammar from the Lexicon?  In 
Proceedings of the Cognitive Science Society..
S.R. Howell, S. Becker, and D. Jankowicz, 2001.  
Modelling Language Acquisition: Lexical Grounding 
Through Perceptual Features.  In Proceedings of the 
2001 Workshop on Developmental Embodied 
Cognition 
J.R. Hurford, M. Studdert-Kennedey, and C. Knight, 
1998. The Emergence of Syntax.  In ? Approaches to 
the evolution of language: social and cognitive 
bases? , Cambridge, Cambridge University Press. 
D. Ingram, 1989. First Language Acquisition.  Method, 
Description and Explanation.  Cambridge: Cambridge 
University Press. 
R. Jakobson, 1971.  Why ?mama? and ?papa?? In 
? Child Language: A Book of Readings? , by A. Bar-
Adon and W. F. Leopold, ed., pages 213-217. 
Englewood Cliffs, NJ:Prentice-Hall.
P.W. Jusczyk, 1999  How infants begin to extract words 
from speech.  Trends in Cognitive Science, 3 (9, 
September):323-328. 
S. Kirby, 1999.  Syntax out of learning: The cultural 
evolution of structured communication in a 
population of induction algorithms. In Proceedings of 
ECAL99 European Conference on Artificial Life, D. 
Floreano et al ed. pages 694-703, Berlin: Springer-
Verlag, 
M. Oliphant and J. Batali 1997. Learning and the 
emergence of coordinated communication. Centre for 
Research in Language Newsletter, 11(1). 
A. Paivio, 1971, Imagery and Verbal Processes. New 
York: Holt, Rinehart & Winston. 
J. Piaget, 1960.  The Language and Thought of the 
Child.  Routledge and K. Paul, 3rd ed.,.  Routledge 
Paperbacks.
S. Pinker, 1994. The Language Instinct.  The New 
Science of Language and Mind. Allen Lane, Penguin 
Press. 
D. Roy, 2001.  Grounded spoken language acquisition: 
Experiments in word learning.  IEEE Transactions on 
Multimedia. 
G. Scheler, 1997d. The transition from babbling to the 
one-word stage: A computational model. In 
Proceedings of GALA '97. 
L. Steels and F. Kaplan, 2001.  AIBO's first words: The 
social learning of language and meaning. Evolution of 
Communication, vol. 4(1):3-32.  John Benjamin?s 
Publishing Company, Amsterdam, Holland. 
L. Steels, 1998.  The Origins of Syntax in visually 
grounded robotic agents. AI 103, 1-24. 
M. Tomasello, and A.C. Kruger, 1992. Joint attention in 
action: Acquiring verbs in ostensive and non-
ostensive contexts. Journal of Child Language 
19:311-333. 
M. van Zaanen, 2000.  Learning structure using 
alignment based learning.  In Proceedings of the 
Third Annual Doctoral Research Colloquium 
(CLUK), pages 75-82. 
M. van Zaanen and P. Adriaans, 2001.  Alignment-
based learning versus EMILE: A comparison.  In 
Proceedings of the Belgian-Dutch Conference on AI 
(BNAIC).
W.H. Zuidema, 2001.  Emergent syntax: the unremitting 
value of computational modelling for understanding 
the origins of complex language. ECAL01, 641-644. 
Springer, Prague, Sept. 10-14, 2001. 
Proceedings of the First Workshop on Argumentation Mining, pages 79?87,
Baltimore, Maryland USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
Mining Arguments From 19th Century Philosophical Texts Using Topic
Based Modelling
John Lawrence and Chris Reed
School of Computing,
University of Dundee, UK
Colin Allen
Dept of History & Philosophy of Science,
Indiana University, USA
Simon McAlister and Andrew Ravenscroft
Cass School of Education & Communities,
University of East London, UK
David Bourget
Centre for Digital Philosophy,
University of Western Ontario, Canada
Abstract
In this paper we look at the manual anal-
ysis of arguments and how this compares
to the current state of automatic argument
analysis. These considerations are used to
develop a new approach combining a ma-
chine learning algorithm to extract propo-
sitions from text, with a topic model to de-
termine argument structure. The results
of this method are compared to a manual
analysis.
1 Introduction
Automatic extraction of meaningful information
from natural text remains a major challenge fac-
ing computer science and AI. As research on spe-
cific tasks in text mining has matured, it has been
picked up commercially and enjoyed rapid suc-
cess. Existing text mining techniques struggle,
however, to identify more complex structures in
discourse, particularly when they are marked by a
complex interplay of surface features rather than
simple lexeme choice.
The difficulties in automatically identifying
complex structure perhaps suggest why there has
been, to date, relatively little work done in the area
of argument mining. This stands in contrast to the
large number of tools and techniques developed
for manual argument analysis.
In this paper we look at the work which has
been done to automate argument analysis, as well
as considering a range of manual methods. We
then apply some of the lessons learnt from these
manual approaches to a new argument extraction
technique, described in section 3. This technique
is applied to a small sample of text extracted from
three chapters of ?THE ANIMAL MIND: A Text-
Book of Comparative Psychology? by Margaret
Floy Washburn, and compared to a high level man-
ual analysis of the same text. We show that de-
spite the small volumes of data considered, this
approach can be used to produce, at least, an ap-
proximation of the argument structure in a piece
of text.
2 Existing Approaches to Extracting
Argument from Text
2.1 Manual Argument Analysis
In most cases, manual argument analysis can be
split into four distinct stages as shown in Figure 1.
Text segmentation
Argument /
Non-Argument
Simple Structure
Refined Structure
Figure 1: Steps in argument analysis
Text segmentation This involves selecting frag-
ments of text from the original piece that
will form the parts of the resulting argument
structure. This can often be as simple as high-
lighting the section of text required, for ex-
ample in OVA (Bex et al., 2013). Though in
some cases, such as the AnalysisWall
1
, this is
a separate step carried out by a different user.
1
http://arg.dundee.ac.uk/analysiswall
79
Argument / Non-Argument This step involves
determining which of the segments previ-
ously identified are part of the argument be-
ing presented and which are not. For most
manual analysis tools this step is performed
as an integral part of segmentation: the an-
alyst simply avoids segmenting any parts of
the text that are not relevant to the argument.
This step can also be performed after deter-
mining the argument structure by discarding
any segments left unlinked to the rest.
Simple Structure Once the elements of the argu-
ment have been determined, the next step is to
examine the links between them. This could
be as simple as noting segments that are re-
lated, but usually includes determining sup-
port/attack relations.
Refined Structure Having determined the basic
argument structure, some analysis tools al-
low this to be refined further by adding de-
tails such as the argumentation scheme.
2.2 Automatic Argument Analysis
One of the first approaches to argument mining,
and perhaps still the most developed, is the work
carried out by Moens et al. beginning with (Moens
et al., 2007), which attempts to detect the argu-
mentative parts of a text by first splitting the text
into sentences and then using features of these sen-
tences to classify each as either ?Argument? or
?Non-Argument?. This approach was built upon
in (Palau and Moens, 2009) where an additional
machine learning technique was implemented to
classify each Argument sentence as either premise
or conclusion.
Although this approach produces reasonable
results, with a best accuracy of 76.35% for
Argument/Non-Argument classification and f-
measures of 68.12% and 74.07% for classifica-
tion as premise or conclusion, the nature of the
technique restricts its usage in a broader context.
For example, in general it is possible that a sen-
tence which is not part of an argument in one sit-
uation may well be in another. Similarly, a sen-
tence which is a conclusion in one case is often a
premise in another.
Another issue with this approach is the original
decision to split the text into sentences. While this
may work for certain datasets, the problem here is
that, in general, multiple propositions often occur
within the same sentence and some parts of a sen-
tence may be part of the argument while others are
not.
The work of Moens et al. focused on the first
three steps of analysis as mentioned in section 2.1,
and this was further developed in (Feng and Hirst,
2011), which looks at fitting one of the top five
most common argumentation schemes to an argu-
ment that has already undergone successful extrac-
tion of conclusions and premises, achieving accu-
racies of 63-91% for one-against-others classifica-
tion and 80-94% for pairwise classification.
Despite the limited work carried out on argu-
ment mining, there has been significant progress
in the related field of opinion mining (Pang and
Lee, 2008). This is often performed at the doc-
ument level, for example to determine whether a
product review is positive or negative. Phrase-
level sentiment analysis has been performed in a
small number of cases, for example (Wilson et al.,
2005) where expressions are classified as neutral
or polar before determining the polarity of the po-
lar expressions.
Whilst it is clear that sentiment analysis alone
cannot give us anything close to the results of man-
ual argument analysis, it is certainly possible that
the ability to determine the sentiment of a given
expression may help to fine-tune any discovered
argument structure.
Another closely related area is Argumentative
Zoning (Teufel et al., 1999), where scientific pa-
pers are annotated at the sentence level with labels
indicating the rhetorical role of the sentence (criti-
cism or support for previous work, comparison of
methods, results or goals, etc.). Again, this infor-
mation could assist in determining structure, and
indeed shares some similarities to the topic mod-
elling approach as described in section 3.2 .
3 Methodology
3.1 Text Segmentation
Many existing argument mining approaches, such
as (Moens et al., 2007), take a simple approach
to text segmentation, for example, simply splitting
the input text into sentences, which, as discussed,
can lead to problems when generally applied.
There have been some more refined attempts
to segment text, combining the segmentation step
with Argument/Non-Argument classification. For
example, (Madnani et al., 2012) uses three meth-
ods: a rule-based system; a supervised probabilis-
80
tic sequence model; and a principled hybrid ver-
sion of the two, to separate argumentative dis-
course into language used to express claims and
evidence, and language used to organise them
(?shell?). Whilst this approach is instructive, it
does not necessarily identify the atomic parts of
the argument required for later structural analysis.
The approach that we present here does not con-
sider whether a piece of text is part of an argu-
ment, but instead simply aims to split the text into
propositions. Proposition segmentation is carried
out using a machine learning algorithm to identify
boundaries, classifying each word as either the be-
ginning or end of a proposition. Two Naive Bayes
classifiers, one to determine the first word of a
proposition and one to determine the last, are gen-
erated using a set of manually annotated training
data. The text given is first split into words and a
list of features calculated for each word. The fea-
tures used are given below:
word The word itself.
length Length of the word.
before The word before.
after The word after. Punctuation is treated as a
separate word so, for example, the last word
in a sentence may have an after feature of ?.?.
pos Part of speech as identified by the Python
Natural Language Toolkit POS tagger
2
.
Once the classifiers have been trained, these
same features can then be determined for each
word in the test data and each word can be clas-
sified as either ?start? or ?end?. Once the classi-
fication has taken place, we run through the text
and when a ?start? is reached we mark a proposi-
tion until the next ?end?.
3.2 Structure identification
Having extracted propositions from the text we
next look at determining the simple structure of
the argument being made and attempt to establish
links between propositions. We avoid distinguish-
ing between Argument and Non-Argument seg-
ments at this stage, instead assuming that any seg-
ments left unconnected are after the structure has
been identified are Non-Argument.
2
http://www.nltk.org/
In order to establish these links, we first con-
sider that in many cases an argument can be repre-
sented as a tree. This assumption is supported by
around 95% of the argument analyses contained in
AIFdb (Lawrence et al., 2012) as well as the fact
that many manual analysis tools including Arau-
caria (Reed and Rowe, 2004), iLogos
3
, Rationale
(Van Gelder, 2007) and Carneades (Gordon et al.,
2007), limit the user to a tree format.
Furthermore, we assume that the argument tree
is generated depth first, specifically that the con-
clusion is presented first and then a single line
of supporting points is followed as far as possi-
ble before working back up through the points
made. The assumption is grounded in work in
computational linguistics that has striven to pro-
duce natural-seeming argument structures (Reed
and Long, 1997). We aim to be able to construct
this tree structure from the text by looking at the
topic of each proposition. The idea of relating
changes in topic to argument structure is supported
by (Cardoso et al., 2013), however, our approach
here is the reverse, using changes in topic to de-
duce the structure, rather than using the structure
to find topic boundaries.
Based on these assumptions, we can determine
structure by first computing the similarity of each
proposition to the others using a Latent Dirich-
let Allocation (LDA) model. LDA is a genera-
tive model which conforms to a Bayesian infer-
ence about the distributions of words in the docu-
ments being modelled. Each ?topic? in the model
is a probability distribution across a set of words
from the documents.
To perform the structure identification, a topic
model is first generated for the text to be stud-
ied and then each proposition identified in the test
data is compared to the model, giving a similar-
ity score for each topic. The propositions are then
processed in the order in which they appear in the
test data. Firstly, the distance between the propo-
sition and its predecessor is calculated as the Eu-
clidean distance between the topic scores. If this
is below a set threshold, the proposition is linked
to its predecessor. If the threshold is exceeded, the
distance is then calculated between the proposition
and all the propositions that have come before, if
the closest of these is then within a certain dis-
tance, an edge is added. If neither of these criteria
3
http://www.phil.cmu.edu/projects/
argument_mapping/
81
is met, the proposition is considered unrelated to
anything that has gone before.
By adjusting the threshold required to join a
proposition to its predecessor we can change how
linear the structure is. A higher threshold will in-
crease the chance that a proposition will instead be
connected higher up the tree and therefore reduce
linearity. The second threshold can be used to alter
the connectedness of the resultant structure, with
a higher threshold giving more unconnected sec-
tions.
It should be noted that the edges obtained do
not have any direction, and there is no further de-
tail generated at this stage about the nature of the
relation between two linked propositions.
4 Manual Analysis
In order to train and test our automatic analysis ap-
proach, we first required some material to be man-
ually analysed. The manual analysis was carried
out by an analyst who was familiar with manual
analysis techniques, but unaware of the automatic
approach that we would be using. In this way we
avoided any possibility of fitting the data to the
technique. He also chose areas of texts that were
established as ?rich? in particular topics in animal
psychology through the application of the mod-
elling techniques above, the assumption being that
these selections would also contain relevant argu-
ments.
The material chosen to be analysed was taken
from ?THE ANIMAL MIND: A TextBook of
Comparative Psychology by Margaret Floy Wash-
burn, 1908? made available to us through the Hathi
Trust.
The analyst began with several selected pas-
sages from this book and in each case generated an
analysis using OVA
4
, an application which links
blocks of text using argument nodes. OVA pro-
vides a drag-and-drop interface for analysing tex-
tual arguments. It is reminiscent of a simplified
Araucaria, except that it is designed to work in an
online environment, running as an HTML5 canvas
application in a browser.
The analyst was instructed only to capture the
argument being made in the text as well as they
could. Arguments can be mapped at different lev-
els depending upon the choices the analyst priori-
tises. This is particularly true of volumes such
as those analysed here, where, in some cases, the
4
http://ova.computing.dundee.ac.uk
same topic is pursued for a complete chapter and
so there are opportunities to map the extended ar-
gument.
In this case the analyst chose to identify discrete
semantic passages corresponding to a proposition,
albeit one that may be compound. An example is
shown in Figure 2. A section of contiguous text
from the volume has been segmented and marked
up using OVA, where each text box corresponds to
such a passage. It is a problem of the era in which
the chosen volume is written that there is a ver-
bosity and indirectness of language, so a passage
may stretch across several sentences. The content
of each box was then edited to contain only ar-
gumentative content and a simple structure pro-
posed by linking supporting boxes towards con-
cluding or sub-concluding boxes. Some fifteen
OVA maps were constructed to represent the argu-
ments concerned with animal consciousness and
with anthropomorphism.
In brief, this analysis approach used OVA as a
formal modelling tool, or lens, to characterise and
better understand the nature of argument within
the texts that were considered, as well as produc-
ing a large set of argument maps. Therefore, it
represented a data-driven and empirically authen-
tic approach and set of data against which the au-
tomated techniques could be considered and com-
pared.
5 Automatic Analysis Results
As discussed in section 4, the manual analysis is
at a higher level of abstraction than is carried out
in typical approaches to critical thinking and argu-
ment analysis (Walton, 2006; Walton et al., 2008),
largely because such analysis is very rarely ex-
tended to arguments presented at monograph scale
(see (Finocchiaro, 1980) for an exception). The
manual analysis still, however, represents an ideal
to which automatic processing might aspire. In or-
der to train the machine learning algorithms, how-
ever, a large dataset of marked propositions is re-
quired. To this end, the manual analysis conducted
at the higher level is complemented by a more fine-
grained analysis of the same text which marks only
propositions (and not inter-proposition structure).
In this case a proposition was considered to cor-
respond to the smallest span of text containing a
single piece of information. It is this detailed anal-
ysis of the text which is used as training data for
text segmentation.
82
Figure 2: Sample argument map from OVA
5.1 Text segmentation
An obvious place to start, then, is to assess the per-
formance of the proposition identification ? that is,
using discourse indicators and other surface fea-
tures as described in section 3.1, to what extent do
spans of text automatically extracted match up to
spans annotated manually described in section 4?
There are four different datasets upon which the
algorithms were trained, with each dataset com-
prising extracted propositions from: (i) raw data
directly from Hathi Trust taken only from Chap-
ter 1 ; (ii) cleaned data (with these errors manually
corrected) taken only from Chapter 1; (iii) cleaned
data from Chapters 1 and 2; and (iv) cleaned data
from Chapters 1, 2 and 4. All the test data is taken
from Chapter 1, and in each case the test data was
not included in the training dataset.
It is important to establish a base line using the
raw text, but it is expected that performance will
be poor since randomly interspersed formatting ar-
tifacts (such as the title of the chapter as a run-
ning header occurring in the middle of a sentence
that runs across pages) have a major impact on the
surface profile of text spans used by the machine
learning algorithms.
The first result to note is the degree of corre-
spondence between the fine-grained propositional
analysis (which yielded, in total, around 1,000
propositions) and the corresponding higher level
analysis. As is to be expected, the atomic argu-
ment components in the abstract analysis typically
cover more than one proposition in the less ab-
stract analysis. In total, however, 88.5% of the
propositions marked by the more detailed anal-
ysis also appear in the more abstract. That is
to say, almost nine-tenths of the material marked
as argumentatively relevant in the detailed analy-
sis was also marked as argumentatively relevant
in the abstract analysis. This result not only
lends confidence to the claim that the two lev-
els are indeed examining the same linguistic phe-
nomena, but also establishes a ?gold standard? for
the machine learning ? given that manual analysis
achieves 88.5% correspondence, and it is this anal-
ysis which provides the training data, we would
not expect the automatic algorithms to be able to
perform at a higher level.
Perhaps unsurprisingly, only 11.6% of the
propositions automatically extracted from the raw,
uncleaned text exactly match spans identified as
propositions in the manual analysis. By running
the processing on cleaned data, this figure is im-
proved somewhat to 20.0% using training data
from Chapter 1 alone. Running the algorithms
trained on additional data beyond Chapter 1 yields
performance of 17.6% (for Chapters 1 and 2) and
13.9% (for 1, 2 and 4). This dropping off is quite
surprising, and points to a lack of homogeneity in
83
the book as a whole ? that is, Chapters 1, 2 and
4 do not provide a strong predictive model for a
small subset. This is an important observation, as
it suggests the need for careful subsampling for
training data. That is, establishing data sets upon
which machine learning algorithms can be trained
is a highly labour-intensive task. It is vital, there-
fore, to focus that effort where it will have the most
effect. The tailing-off effect witnessed on this
dataset suggests that it is more important to sub-
sample ?horizontally? across a volume (or set of
volumes), taking small extracts from each chapter,
rather than subsampling ?vertically,? taking larger,
more in-depth extracts from fewer places across
the volume.
This first set of results is determined using
strong matching criteria: that individual proposi-
tions must match exactly between automatic and
manual analyses. In practice, however, artefacts
of the text, including formatting and punctuation,
may mean that although a proposition has indeed
been identified automatically in the correct way,
it is marked as a failure because it is including
or excluding a punctuation mark, connective word
or other non-propositional material. To allow for
this, results were also calculated on the basis of a
tolerance of ?3 words (i.e. space-delimited char-
acter strings). On this basis, performance with un-
formatted text was 17.4% ? again, rather poor as is
to be expected. With cleaned text, the match rate
between manually and artificially marked propo-
sition boundaries was 32.5% for Chapter 1 text
alone. Again, performance drops over a larger
training dataset (reinforcing the observation above
regarding the need for horizontal subsampling), to
26.5% for Chapters 1 and 2, and 25.0% for Chap-
ters 1, 2 and 4.
A further liberal step is to assess automatic
proposition identification in terms of argument rel-
evance ? i.e. to review the proportion of automat-
ically delimited propositions that are included at
all in manual analysis. This then stands in direct
comparison to the 88.5% figure mentioned above,
representing the proportion of manually identi-
fied propositions at a fine-grained level of analy-
sis that are present in amongst the propositions at
the coarse-grained level. With unformatted text,
the figure is still low at 27.3%, but with cleaned
up text, results are much better: for just the text of
Chapter 1, the proportion of automatically identi-
fied propositions which are included in the man-
ual, coarse-grained analysis is 63.6%, though this
drops to 44.4% and 50.0% for training datasets
corresponding to Chapters 1 and 2, and to Chap-
ters 1, 2 and 4, respectively. These figures com-
pare favourably with the 88.5% result for human
analysis: that is, automatic analysis is relatively
good at identifying text spans with argumentative
roles.
These results are summarised in Table 1, below.
For each of the four datasets, the table lists the
proportion of automatically analysed propositions
that are identical to those in the (fine-grained level)
manual analysis, the proportion that are within
three words of the (fine-grained level) manual
analysis, and the proportion that are general sub-
strings of the (coarse-grained level) manual analy-
sis (i.e. a measure of argument relevance).
Identical ?3Words Substring
Unformated 11.6 17.4 27.3
Ch. 1 20.0 32.5 63.6
Ch. 1&2 17.6 26.5 44.4
Ch. 1,2&4 13.9 25.0 50.0
Table 1: Results of automatic proposition process-
ing
5.2 Structure identification
Clearly, identifying the atoms from which argu-
ment ?molecules? are constructed is only part of
the problem: it is also important to recognise the
structural relations. Equally clearly, the results
described in section 5.1 have plenty of room for
improvement in future work. They are, however,
strong enough to support further investigation of
automatic recognition of structural features (i.e.,
specifically, features relating to argument struc-
ture).
In order to tease out both false positives and
false negatives, our analysis here separates preci-
sion and recall. Furthermore, all results are given
with respect to the coarse-grained analysis of sec-
tion 4, as no manual structure identification was
performed on the fine-grained analysis.
As described in section 3.2, the automatic struc-
ture identification currently returns connectedness,
not direction (that is, it indicates two argument
atoms that are related together in an argument
structure, but do not indicate which is premise
and which conclusion). The system uses propo-
sitional boundaries as input, so can run equally on
manually segmented propositions (those used as
84
training data in section 5.1) or automatically seg-
mented propositions (the results for which were
described in Table 1). In the results which follow,
we compare performance between manually an-
notated and automatically extracted propositions.
Figures 3 and 4 show sample extracts from the au-
tomatic structure recognition algorithms running
on manually segmented and automatically seg-
mented propositions respectively.
For all those pairs of (manually or automati-
cally) analysed propositions which the automatic
structure recognition algorithms class as being
connected, we examine in the manual structural
analysis connectedness between propositions in
which the text of the analysed propositions ap-
pears. Thus, for example, if our analysed propo-
sitions are the strings xxx and yyy, and the auto-
matic structure recognition system classes them as
connected, we first identify the two propositions
(P1 and P2) in the manual analysis which include
amongst the text with which they are associated
the strings xxx and yyy. Then we check to see if P1
and P2 are (immediately) structurally related. For
automatically segmented propositions, precision is
33.3% and recall 50.0%, whilst for manually seg-
mented propositions, precision is 33.3% and re-
call 18.2%. For automatically extracted proposi-
tions, the overlap with the coarse-grained analy-
sis was small ? just four propositions ? so the re-
sults should be treated with some caution. Preci-
sion and recall for the manually extracted proposi-
tions however is based on a larger dataset (n=26),
so the results are disappointing. One reason is that
with the manual analysis at a significantly more
coarse-grained level, propositions that were identi-
fied as being structurally connected were quite of-
ten in the same atomic unit in the manual analysis,
thus being rejected as a false positive by the anal-
ysis engine. As a result, we also consider a more
liberal definition of a correctly identified link be-
tween propositions, in which success is recorded
if either:
(a) for any two manually or automatically anal-
ysed propositions (p1, p2) that the automatic struc-
ture recognition indicates as connected, there is a
structural connection between manually analysed
propositions (P1, P2) where p1 is included in P1
and p2 included in P2
or
(b) for any two manually or automatically anal-
ysed propositions (p1, p2) that the automatic struc-
ture recognition indicates as connected, there is a
single manually analysed propositions (P1) where
p1 and p2 are both included in P1
Under this rubric, automatic structure recog-
nition with automatically segmented propositions
has precision of 66.6% and recall of 100% (but
again, only on a dataset of n=4), and more signif-
icantly, automatic structure recognition with man-
ually segmented propositions has precision 72.2%
and recall 76.5% These results are summarised in
Table 2.
Automatically
segmented
propositions
Manually seg-
mented propo-
sitions
In separate
propositions
n=4, P=33.3%,
R=50.0%
n=26,
P=33.3%,
R=18.2%
In separate
or the same
proposition
n=4, P=66.6%,
R=100.0%
n=26,
P=72.2%,
R=76.5%
Table 2: Results of automatic structure generation
The results are encouraging, but larger scale
analysis is required to further test the reliability of
the extant algorithms.
6 Conclusion
With fewer than one hundred atomic argument
components analysed at the coarse-grained level,
and barely 1,000 propositions at the fine-grained
level, the availability of training data is a ma-
jor hurdle. Developing these training sets is de-
manding and extremely labour intensive. One
possibility is to increasingly make available and
reuse datasets between projects. Infrastructure
efforts such as aifdb.org make this more
realistic, with around 15,000 analysed propo-
sitions in around 1,200 arguments, though as
scale increases, quality management (e.g. over
crowdsourced contributions) becomes an increas-
ing challenge.
With sustained scholarly input, however, in con-
junction with crossproject import and export, we
would expect these datasets to increase 10 to 100
fold over the next year or two, which will sup-
port rapid expansion in training and test data sets
for the next generation of argument mining algo-
rithms.
Despite the lack of training data currently avail-
able, we have shown that automatic segmentation
of propositions in a text on the basis of relatively
simple features at the surface and syntactic levels
85
Figure 3: Example of automated structure recognition using manually identified propositions
Figure 4: Example of automated structure recognition using automatically identified propositions
is feasible, though generalisation between chap-
ters, volumes and, ultimately, genres, is extremely
demanding.
Automatic identification of at least some struc-
tural features of argument is surprisingly robust,
even at this early stage, though more sophisticated
structure such as determining the inferential direc-
tionality and inferential type is likely to be much
more challenging.
We have also shown that automatic segmenta-
tion and automatic structure recognition can be
connected to determine at least an approximation
of the argument structure in a piece of text, though
much more data is required to test its applicability
at scale.
6.1 Future Work
Significantly expanded datasets are crucial to fur-
ther development of these techniques. This will
require collaboration amongst analysts as well as
the further development of tools for collaborating
on and sharing analyses.
Propositional segmentation results could be im-
proved by making more thorough use of syntactic
information such as clausal completeness. Com-
bining a range of techniques to determine proposi-
tions would counteract weaknesses that each may
face individually.
With a significant foundation for argument
structure analysis, it is hoped that future work can
focus on extending and refining sets of algorithms
and heuristics based on both statistical and deep
learning mechanisms for exploiting not just topi-
cal information, but also the logical, semantic, in-
ferential and dialogical structures latent in argu-
mentative text.
7 Acknowledgements
The authors would like to thank the Digging Into
Data challenge funded by JISC in the UK and
NEH in the US under project CIINN01, ?Digging
By Debating? which in part supported the research
reported here.
86
References
F. Bex, J. Lawrence, M. Snaith, and C.A. Reed. 2013.
Implementing the argument web. Communications
of the ACM, 56(10):56?73.
P.C. Cardoso, M. Taboada, and T.A. Pardo. 2013.
On the contribution of discourse structure to topic
segmentation. In Proceedings of the Special Inter-
est Group on Discourse and Dialogue (SIGDIAL),
pages 92?96. Association for Computational Lin-
guistics.
V.W. Feng and G. Hirst. 2011. Classifying argu-
ments by scheme. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 987?996. Association for Computational Lin-
guistics.
Maurice A. Finocchiaro. 1980. Galileo and the art of
reasoning. rhetorical foundations of logic and scien-
tific method. Boston Studies in the Philosophy of
Science New York, NY, 61.
Thomas F Gordon, Henry Prakken, and Douglas
Walton. 2007. The carneades model of argu-
ment and burden of proof. Artificial Intelligence,
171(10):875?896.
John Lawrence, Floris Bex, Chris Reed, and Mark
Snaith. 2012. Aifdb: Infrastructure for the argu-
ment web. In COMMA, pages 515?516.
N. Madnani, M. Heilman, J. Tetreault, and
M. Chodorow. 2012. Identifying high-level
organizational elements in argumentative discourse.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 20?28. Association for Computational
Linguistics.
M.F. Moens, E. Boiy, R.M. Palau, and C. Reed. 2007.
Automatic detection of arguments in legal texts. In
Proceedings of the 11th international conference
on Artificial intelligence and law, pages 225?230.
ACM.
R.M. Palau and M.F. Moens. 2009. Argumentation
mining: the detection, classification and structure of
arguments in text. In Proceedings of the 12th in-
ternational conference on artificial intelligence and
law, pages 98?107. ACM.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Now Pub.
Chris Reed and Derek Long. 1997. Content ordering
in the generation of persuasive discourse. In IJCAI
(2), pages 1022?1029. Morgan Kaufmann.
Chris Reed and Glenn Rowe. 2004. Araucaria: Soft-
ware for argument analysis, diagramming and repre-
sentation. International Journal on Artificial Intelli-
gence Tools, 13(04):961?979.
S. Teufel, J. Carletta, and M. Moens. 1999. An anno-
tation scheme for discourse-level argumentation in
research articles. In Proceedings of the ninth con-
ference on European chapter of the Association for
Computational Linguistics, pages 110?117. Associ-
ation for Computational Linguistics.
Tim Van Gelder. 2007. The rationale for rationale.
Law, probability and risk, 6(1-4):23?42.
D Walton, C Reed, and F Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
D Walton. 2006. Fundamentals of critical argumenta-
tion. Cambridge University Press.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on human language technology and empiri-
cal methods in natural language processing, pages
347?354. Association for Computational Linguis-
tics.
87
