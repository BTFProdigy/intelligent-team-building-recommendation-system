Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 712?717,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Applying Grammar Induction to Text Mining 
 
Andrew Salway 
Uni Research Computing 
Thorm?hlensgt. 55 
N-5008 Bergen 
Norway 
andrew.salway@uni.no 
Samia Touileb 
Information Science and Media Studies 
University of Bergen 
N-5020 Bergen 
Norway 
samia.touileb@gmail.com 
 
  
 
Abstract 
We report the first steps of a novel 
investigation into how a grammar induction 
algorithm can be modified and used to 
identify salient information structures in a 
corpus. The information structures are to be 
used as representations of semantic content 
for text mining purposes.  We modify the 
learning regime of the ADIOS algorithm 
(Solan et al, 2005) so that text is presented as 
increasingly large snippets around key terms, 
and instances of selected structures are 
substituted with common identifiers in the 
input for subsequent iterations. The technique 
is applied to 1.4m blog posts about climate 
change which mention diverse topics and 
reflect multiple perspectives and different 
points of view. Observation of the resulting 
information structures suggests that they 
could be useful as representations of semantic 
content. Preliminary analysis shows that our 
modifications had a beneficial effect for 
inducing more useful structures. 
1 Introduction 
There is an obvious need for text mining 
techniques to deal with large volumes of very 
diverse material, especially since the advent of 
social media and user-generated content which 
includes dynamic discussions of wide-ranging 
and controversial topics. 
In order to be portable across domains, text 
genres and languages, current techniques tend to 
treat texts as bags of words when analyzing 
semantic content, e.g. for keyword-based 
retrieval, summarization with word clouds, and 
topic modelling. Such techniques capture the 
general ?aboutness? of texts, but they do little to 
elucidate the actual statements that are made 
about key terms in the material. More structured 
and deeper semantic representations can be 
generated by information extraction systems for 
relatively restricted text genres and domains, but 
even then they are costly to port. 
We see one particular area of application in 
elucidating the semantic content of social media 
debates about controversial topics, like climate 
change, both for casual users, and for social 
scientists studying online discourses. The 
complex, diverse and dynamic nature of the text 
content in such material presents a significant 
challenge for elucidating semantics. On the one 
hand, keywords alone will not convey what is 
said about important concepts, nor different 
points of view. On the other hand, modelling the 
semantics for information extraction purposes 
does not seem feasible given the breadth and 
diversity of the material.  
Thus, we are motivated to develop a portable 
technique that generates representations of 
semantic content that are richer than keywords, 
and that can be applied to broad domains. 
Specifically, we seek to extract important 
information structures from an unannotated 
corpus comprising texts of the same genre and 
relating to the same domain.  
Rather than using language-specific or 
domain-specific resources, we assume that 
important information structures in such a corpus 
will be reflected by patterning in the surface 
form of texts, such that they can be identified 
automatically through a distributional analysis 
(Section 2). Our approach is to induce 
information structures from an unannotated 
corpus by modifying and applying the ADIOS 
grammar induction algorithm (Solan et al, 
2005): the modifications serve to focus the 
algorithm on what is typically written about key-
terms (Section 3). To date we have implemented 
the approach to process 1.4m English-language 
blog posts about climate change: proper 
evaluation is ongoing but we are able to show 
712
examples of the semantic representations 
generated, discuss how they elucidate semantic 
content, and suggest how they might be used for 
various NLP tasks (Section 4). In closing, we 
make tentative conclusions and describe ongoing 
work (Section 5). 
2 Background 
Harris (1954; 1988) demonstrated how linguistic 
units and structures can be identified (manually) 
through a distributional analysis of partially 
aligned sentential contexts. His work suggests 
that it should be possible to induce syntactic 
descriptions from samples of unannotated text.  
An early attempt to apply this thinking to 
computational linguistics was made by Lamb 
(1961) who described procedures for identifying 
?H-groups? and ?V-groups?. An H-group is a 
horizontal grouping of items (words and groups) 
that tend to appear sequentially, cf. a syntagmatic 
linguistic unit. A V-group is a vertical grouping 
of items that occur in similar linguistic contexts 
in a corpus, cf. a paradigmatic linguistic unit. As 
a toy example, take the H-group ?(the (woman 
| man) went to the (pub | shop | 
park))?, with V-groups ?(woman | man)? 
and ?(pub | shop | park)?. 
In more recent times, Harris? insights have 
become a cornerstone for some of the work in 
the field of grammatical inference, where 
researchers attempt to induce grammatical 
structures from raw text, e.g. ADIOS (Solan et 
al., 2005). In this field the emphasis is on 
generating complete grammatical descriptions 
for text corpora in order to understand the 
processes of language learning, rather than text 
mining; see D?Ulizia et al (2011) for a review.  
The unsupervised ADIOS algorithm 
recursively induces hierarchically structured 
patterns from sequential data, e.g. sequences of 
words in unannotated text, using statistical 
information in the sequential data. Each 
sequence (sentence) is loaded onto a directed 
pseudograph with one vertex for each vocabulary 
item: this means that partially aligned sequences 
share sub-paths across the graph.  
In each iteration, the most significant pattern 
is identified with a statistical criterion that favors 
frequent sequences that occur in a variety of 
contexts. Then, the algorithm looks for possible 
equivalence classes within the context of the 
pattern, i.e. it identifies positions in the pattern 
that could be filled by different items and forms 
an equivalence class with those items. At the end 
of the iteration, the new pattern and equivalence 
class become vocabulary items in the graph, so 
that they can become part of further patterns and 
equivalence classes, and hence hierarchical 
structures are formed. For us, the terms ?pattern? 
and ?equivalence class? equate to the previously 
mentioned ?H-group? and ?V-group?: we prefer 
the simplicity and literalness of these terms and 
use them henceforth. 
3 Approach 
For text mining purposes we do not see the need 
to induce a complete grammar for the corpus that 
we are mining. Rather, we are struck by Harris? 
further observation that the linguistic structures 
derived from a distributional analysis may reflect 
information structures, especially in the 
?sublanguages? of specialist domains (Harris, 
1988). Thus, we propose to use a grammar 
induction algorithm to identify the most salient 
information structures in a corpus and take these 
as representations of important semantic content. 
ADIOS has been evaluated on an interesting 
range of text corpora, and other kinds of 
sequential data. However, to the best of our 
knowledge, it has not been shown to successfully 
process a corpus with the scale and diversity of 
material that we envisage, e.g. 1.4m blog posts 
relating to climate change. This, along with our 
objective of identifying salient information 
structures rather than a complete grammatical 
description, led us to modify the learning regime 
to ADIOS. In the rest of this section we explain 
the modifications: please see 4.2.1 for a detailed 
description of how they were implemented. 
To address the large scale and complexity of 
language use in social media, we modify the way 
in which text is presented to ADIOS by focusing 
separately on text around key terms of interest, 
rather than processing all sentences en masse. 
Our thinking here is in part influenced by the 
theory of local grammar (Gross, 1997), i.e. the 
idea that language is best described with word 
classes that are specific to local contexts, rather 
than general across the language.  
Firstly, for each key term, we present only text 
snippets that contain that term: we expect there 
to be more salient patterning in snippets around a 
single key term because of repetition in the kinds 
of things written about it. Secondly, blog posts 
contain long and complex sentences so we 
process the clause containing a key term, and 
ignore the rest of the sentence. Thirdly, since we 
expect the key term to form more significant 
713
units with words in its close proximity, we 
present the clauses in increasingly large snippets 
around the key term. 
A further modification targets the most 
frequent and meaningful structures. After each 
iteration in which H-groups and V-groups are 
induced, the most frequent H-groups are filtered 
to remove any containing large V-groups which 
are likely to be more semantically nebulous. 
Instances of the selected H-groups are replaced 
with common identifiers in the input file so that 
patterning around them is more explicit in 
subsequent iterations. 
4 Implementation 
Here we report our first attempt to apply 
grammar induction to text mining. We chose to 
work with a corpus of blogs relating to climate 
change because they provide a challenging 
scenario with complex semantics, in which 
diverse topics ? causes, effects, solutions, etc. ?   
are discussed from multiple perspectives ? 
scientific, political, personal, etc. ? and with 
different beliefs (section 4.1).  
We describe how we modified the learning 
regime of the ADIOS algorithm in order to 
induce H-groups and V-groups from an 
unannotated corpus (4.2.1). At this stage in our 
work, our focus is on observing the kinds of 
information structures that can be identified in 
this way, and in considering their potential 
applications as representations of semantic 
content (4.2.2). We also analyzed how results 
were affected by our modifications, i.e. the use 
of incrementally bigger snippets rather than 
complete clauses, and the iterative selection and 
substitution of frequent H-groups (4.2.3). 
4.1 Input data 
We used a corpus of about 1.4m unannotated 
English-language blog posts from 3,000 blogs 
related to climate change (Salway et al, 2013). 
Based on the relative frequency of words 
compared with a general language corpus, and 
the use of n-grams, we identified a set of domain 
key terms, e.g. ?climate change?, ?greenhouse 
gases?, ?carbon tax?, ?sea levels?. From these we 
selected 17, with a mix of high (10,000?s), 
medium (1,000?s) and low (100?s) frequencies. 
For each key term we crudely extracted every 
clause it occurred in by taking a clause to be a 
sequence of words between punctuation. Pre-
processing involved conversion to lower case, 
joining the words of key terms to make single 
items, e.g. ?greenhouse_gases?, and substituting 
?dddd? with ?YEAR?, and other digit sequences 
with ?NUMBER?: these changes all serve to 
make patterning more explicit.  
Then, from the clauses for each key term, 
snippets of varying sizes were created. A snippet 
file for a key term is defined by (min-max) 
where there must be at least min words to one 
side of the key term, and no more than max 
words either side. Sets of snippet files were 
created for three different increment values: i = 2 
(0-2, 3-4, 5-6, 7-8, 9-10, 11-12); i = 3 (0-3, 4-6, 
7-9, 10-12); and, i = 4 (0-4, 5-8, 9-12). 
4.2 Modifying the ADIOS learning regime 
4.2.1 Method 
In Section 3 we explained the rationale for our 
modifications to the ADIOS learning regime. 
They are detailed in steps 1 and 3-5 below. 
 
For one key term and one increment value: 
 
(1) INITIALIZE. Set the current input file to be 
the first snippet file for the key term and 
increment value, i.e. the smallest snippets. 
(2) INDUCE CANDIDATE H-GROUPS AND V-
GROUPS. Run the ADIOS algorithm over the 
current input file with default parameter 
values, except E=0.9 (cf. Solan et al 2005). 
(3) SELECTION. Filter the 5 most frequent H-
groups to keep those that meet the following 
criterion: if the H-group contains a V-group 
then the V-group must contain < 6 elements. 
If none of the 5 most frequent H-groups 
remain then go to (5). 
(4) SUBSTITUTION.  For each selected H-group, 
replace all instances of it in the current input 
file with a common identifier. Iterate 10 
times from (2). 
(5) TRANSITION. Until the final snippet file is 
reached, set the current input file to be the 
next largest snippet file and substitute 
identifiers for the instances of all H-groups 
selected so far. Go to (2). 
This process was executed for 17 key terms, with 
three increment values (i = 2, 3, 4). For further 
comparison, for each key term it was executed 
with complete clauses (ten iterations with 
selection and substitution) and with complete 
clauses (one iteration). 
714
1. (((to (combat|fight))| (to (battle|slow|minimise|mitigate|tackle))) 
climate_change) 
2. (climate_change (summit|adaptation|talks|meetings|convention))  
3. (((greenhouse gases)|emissions|gases|(carbon emissions)|pollution) blamed 
((for|to) global_warming)) 
4. ((cause|causes) (of global_warming)) 
5. ((dangers|signs|effect|consequences|perils) (of global_warming)) 
6. (to (confuse|mislead|educate) the public) // from global_warming snippets 
7. ((anthropogenic|manmade|(man made)) global_warming)  
8. ((would|should|to|must) (control|reduce|regulate|regulating|release) 
greenhouse_gases) 
9. ((source|emitter|emitters|producers) of greenhouse_gases) 
10. (the (effects|impact) ((under|of) ((a|its|the) carbon_tax))) 
11. (a (modest|$_NUMBER a tonne|global|simple) carbon_tax) 
12. ((will|would|to) (push|raise|elevate) (sea_levels (around|by))) 
13. (((due to)|(caused by)) ((climate change)|(global warming))) //from 
sea_levels snippets 
14. ((((the|global|some|sophisticated|complex) climate_models) 
(hint|show|indicate) that) 
Table 1. A small selection of H-groups induced from snippets for a variety of key terms (in bold). 
 
 
4.2.2 Results and potential applications 
Table 1 presents a small selection of 14 H-groups 
that were induced from snippets with various key 
terms and increment values. Here, H-groups and 
V-groups are bracketed and nested. The elements 
of H-groups are separated by white space and the 
elements of V-groups are separated by ?|?. Recall 
that the induction process selects frequent H-
groups which, based on our assumptions, should 
reflect important semantic content.  
This output would benefit from some post-
processing, which is part of ongoing work. For 
example, in 1 there are two V-groups containing 
verbs that would be more elegantly expressed as 
a single V-group. There are also H-groups in 
which not all V-group alternatives make sense 
with the rest of the containing H-group due to 
over-generalization, e.g. ?to? in ??blamed 
((for|to) global warming)? in 3. Despite 
these issues, some interesting and potentially 
useful structures are induced. 
 Some H-groups, we assume those resulting 
from the most stylized use of language in blogs, 
could perhaps be taken as the basis for 
information extraction templates, e.g. 11 where 
?$_NUMBER? is a slot for different amounts of 
tax, and 12 which captures various ways in 
which predictions about the amount of sea level 
rise can be written.  
Other H-groups highlight some of the things 
typically written about key terms by grouping 
together different expressions of canonical 
statements, e.g. 3, 8 and 13. These could be used 
as a basis for summarizing the most important 
points of a topic, i.e. by taking 10,000?s 
sentences and reducing them to 10?s H-groups. 
For broad topics it is desirable to perform 
finer-grained text classification and retrieval. The 
induction of H-groups such as 4 and 5 helps to 
identify different facets of a topic. In this case, 
the H-groups flag the causes of global warming 
and the effects of global warming as sub-topics, 
and show different ways in which they may be 
expressed. 
The alternation in V-groups contained by H-
groups may reflect different beliefs and opinions 
which could be used for text classification and 
opinion mining. In 14, the V-group 
?hint|show|indicate? reflects different 
degrees of confidence that bloggers have in 
climate models. In 6, the alternatives in 
?confuse|mislead|educate? reflect 
positive and negative views about public 
communication in the climate debate. 
Semantically related terms, such as those 
captured in 1 and 5, have very different 
connotations and as such reflect different beliefs: 
consider the difference between someone writing 
about the ?effect of global warming? and 
the ?perils of global warming?. In other 
cases, alternation reflects different ways to say 
the same thing, e.g. the more or less synonymous 
terms that are captured in 2, 7 and 9 which would 
be useful for query expansion. 
715
Key Term Clauses Number of different H-groups and total instances 
i=2 i=3 i=4 clauses-10 clauses-1 
climate change 48241 198 47000 105 52745 86 57799 8 31611 698 123531 
global warming  27582 191 25998 155 30001 104 31850 40 32315 397 57388 
greenhouse gases 20345 174 30148 136 34009 94 33846 28 25213 552 65167 
carbon tax 7751 106 6727 84 8341 80 9859 36 11393 128 14988 
sea levels 6448 138 8322 121 10246 118 11020 55 12090 240 16752 
climate models 6276 98 5041 91 6020 74 6399 26 6061 142 11058 
emissions trading scheme 2989 86 2243 65 3802 68 3140 50 7680 96 8118 
Table 2. Numbers of different H-groups and total instances generated from different input data. 
 
4.2.3 The effects of our modifications 
The numbers of H-groups generated by different 
executions of the induction process for each key 
term are shown in Table 2, i.e. three executions 
using snippets with different values of i, and two 
executions using clauses for comparison (cf. 
4.2.1). The 10 omitted key terms (less than 1,000 
clauses each) generated less than 25 H-groups 
for each value of i.  
The high frequencies for clauses-1 are because 
no selection of H-groups took place, i.e. we 
simply take the normal ADIOS output. Based on 
our own inspections, some potentially useful H-
groups were found in this output but, compared 
with other outputs, it was more common to see 
H-groups with large and semantically nebulous 
V-groups. This observation supports the iterative 
selection and substitution of H-groups with a 
limit on the size of V-groups. We also looked at 
the average number of V-groups in H-groups for 
each execution, as a way to compare the amount 
of structure in H-groups. This number was 
consistently lowest in results for clauses-1 which 
further supports our modifications. 
A few potentially useful H-groups were 
observed in results for clauses-10, for which 
selection and substitution were applied. However 
the low numbers of different H-groups compared 
with all values of i suggests that it is better to use 
snippets as input rather than clauses. 
The way in which the ratio of different H-
groups and total instances varies for values of i 
suggests that starting with larger snippets (i=4) 
results in fewer H-groups but that these will 
capture more instances, i.e. they are more 
general. Whilst the H-groups for clauses-10 have 
many instances these tend not to capture useful 
patterning, i.e. they tended to describe 
combinations of key terms and function words.  
5 Closing Remarks 
At this stage in the research any conclusions 
must be tentative. However, it seems to us that 
the use of grammar induction to elucidate 
semantic content for text mining purposes shows 
promise. The H-groups shown in Table 1 provide 
richer semantic descriptions of the domain than 
keywords do, and we noted potential applications 
for high-level summarization of a whole corpus, 
the creation of information extraction templates 
and finer-grained text classification and retrieval. 
Importantly, the technique for generating H-
groups would not require adaptation for use on a 
different corpus. The analysis in 4.2.3 suggests 
that the modifications that we made to the 
ADIOS learning regime had a beneficial effect.  
Without a thorough evaluation we cannot 
make strong claims. In particular, we have little 
sense of the technique?s recall, i.e. we do not 
know what information structures it missed. That 
said, it might be argued that since we expect the 
technique to be consistent in identifying 
patterning in the surface form of texts then its 
success will depend on the extent to which key 
terms are written about in consistent ways. This 
will of course vary between text genres and 
domains. Work has started on another corpus 
with more restricted language use and richer 
structuring was induced (Salway et al 2014). 
In other ongoing work we are looking more 
into the effects of the various parameters of 
ADIOS, and the necessity for our modifications. 
We are also seeking a deeper understanding of 
how the statistical information exploited by 
ADIOS relates to that which is captured by n-
gram language models to describe sequences of 
words (cf. H-groups), and by established 
techniques to form semantic classes based on 
shared linguistic contexts (cf. V-groups). 
Acknowledgments 
We are very grateful to Zach Solan for providing 
an implementation of the ADIOS algorithm, and 
to Knut Hofland and Lubos Steskal for their roles 
in creating the NTAP blog corpus. This research 
was supported by a grant from The Research 
Council of Norway?s VERDIKT program. 
716
References 
Arianna D?Ulizia, Fernando Ferri and Patrizia 
Grifoni. 2011. A survey of grammatical inference 
methods for natural language learning. Artificial 
Intelligence Review 36(1):1-27. 
Maurice Gross. 1997. The Construction of Local 
Grammars. In: E. Roche and Y. Schabes (eds.), 
Finite-State Language Processing. The MIT Press, 
Cambridge MA: 329-354. 
Zellig Harris. 1954. Distributional Structure. Word 
10(2/3):146-162. 
Zellig Harris. 1988. Language and Information. 
Columbia University Press, New York. 
Sydney Lamb. 1961. On the Mechanization of 
Syntactic Analysis. Int. Conf. Machine Translation 
of Languages and Applied Language Analysis. 
Andrew Salway, Knut Hofland and Samia Touileb. 
2013. Applying Corpus Techniques to Climate 
Change Blogs. Procs. Corpus Linguistics 2013, 
Lancaster University. 
Andrew Salway, Samia Touileb and Endre 
Tvinnereim. 2014. Inducing Information Structures 
for Data-driven Text Analysis. To appear in: Procs. 
ACL Workshop on Language Technologies and 
Computational Social Science. 
Zach Solan, David Horn, Eytan Ruppin, and Shimon 
Edelman. 2005. Unsupervised learning of natural 
languages. Procs. of the National Academy of 
Sciences 102(33):11629-11634. 
717
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 28?32,
Baltimore, Maryland, USA, June 26, 2014. c?2014 Association for Computational Linguistics
Inducing Information Structures for Data-driven Text Analysis 
 
 
Andrew Salway 
Uni Research Computing 
N-5008 Bergen 
Norway 
andrew.salway@uni.no 
Samia Touileb 
University of Bergen 
N-5020 Bergen 
Norway 
samia.touileb@gmail.com 
Endre Tvinnereim 
Uni Research Rokkansenteret  
N-5015 Bergen 
Norway 
endre.tvinnereim@uni.no 
 
  
 
Abstract 
We report ongoing work that is aiming to 
develop a data-driven approach to text 
analysis for computational social science. 
The novel feature is the use of a grammar 
induction algorithm to identify salient 
information structures from an unannotated 
text corpus. The structures provide richer 
representations of text content than 
keywords, by capturing patterning related to 
what is written about key terms. Here we 
show how information structures were 
induced from texts that record political 
negotiations, and how the structures were 
used in analyzing relations between countries 
and negotiation positions.   
1 Introduction 
There is a widespread need for automated text 
analysis to be integrated into research methods 
for computational social science (e.g. Grimmer 
and Stewart, 2013).  In order to analyze highly 
diverse content, techniques tend to treat texts as 
bags of words, e.g. for search, to summarize 
content with word clouds, and to model topics.  
Such techniques capture the general ?aboutness? 
of texts, but they do little to elucidate the actual 
statements that are made about key concepts. 
Conversely, structured representations of 
statements can be generated, up to a point, by 
information extraction systems but these are 
costly to port to new languages and domains. 
Thus, we are motivated to develop a portable 
technique that can generate richer representations 
of text content than keywords. Our idea is to 
adapt and apply a grammar induction algorithm 
to identify salient information structures in the 
surface form of texts. It seems to us that, to the 
extent that there is patterning, information 
structures may be induced from an unannotated 
text corpus with little or no need for language-
specific and domain-specific resources. Unlike 
approaches under the rubrics of unsupervised and 
open information extraction (e.g. Riloff, 1996; 
Sekine, 2006; Etzioni et al., 2008), we avoid the 
use of parsers, part-of-speech taggers, and pre-
specified entities for which relations are sought.  
The approach that we envisage fits with the 
ethos of exploratory ?data-driven? research. 
Rather than approaching a corpus with a 
hypothesis and an a priori coding scheme, a 
researcher is given an overview of the content in 
terms of computationally tractable information 
structures that were induced from the corpus. 
Such structures map to surface forms in text and 
can hence be used directly in quantitative 
analyses for further exploration and to test 
hypotheses, once they have been interpreted as 
interesting by a researcher. Working in this way 
will avoid the expense and bottleneck of manual 
coding, and reduce the potential for biases. 
In the following we motivate our use of the 
ADIOS algorithm for grammar induction (2.1), 
and introduce the Earth Negotiations Bulletin 
(2.2). Section 3 describes our method and 
discusses the information structures identified in 
ENB texts. Section 4 takes some preliminary 
steps in using these information structures to 
identify dyads of (dis-) agreement and to extract 
markers of quantifiable negotiation positions.  In 
closing, Section 5 offers some tentative 
conclusions and ideas for future work. 
2 Background 
2.1 Grammar induction for text mining 
Harris (1954; 1988) demonstrated how linguistic 
units and structures can be identified manually 
through a distributional analysis of partially 
aligned sentential contexts. We are struck by 
Harris? insight that the linguistic structures 
derived from a distributional analysis may reflect 
28
domain-specific information structures, 
especially in the ?sublanguages? of specialist 
domains (Harris, 1988). Whilst the textual 
material typically analyzed by social scientists 
may not be as restricted in content and style as 
that analyzed by Harris, our work proceeds on 
the assumption that, at least in some domains, it 
is restricted enough such that there is sufficient 
patterning for an inductive approach. 
Harris? ideas about distributional analysis have 
become a cornerstone for some of the work in 
the field of automated grammatical inference, 
where researchers attempt to induce grammatical 
structures from raw text. In this field the 
emphasis is on generating complete grammatical 
descriptions for text corpora in order to 
understand the processes of language learning; 
see D?Ulizia et al. (2011) for a review. 
For example, the unsupervised ADIOS 
algorithm (Solan et al., 2005) recursively induces 
hierarchically structured patterns from sequential 
data, e.g. sequences of words in sentences of 
unannotated text, using statistical information in 
the sequential data. Patterns may include 
equivalence classes comprising items that share 
similar distributional properties, where items 
may be words or other patterns. As a toy 
example of a pattern, take ?(the (woman|man) 
went to the (house|shop|pub))?, with equivalence 
classes ?(woman|man)? and ?(house|shop|pub)?. 
2.2 The Earth Negotiations Bulletin 
Within political science, text corpora provide a 
valuable resource for the analysis of political 
struggle and structures. For international climate 
negotiations, the Earth Negotiation Bulletin 
(ENB) constitutes an online record of the 
positions and proposals of different countries, 
their agreements and disagreements, and changes 
over time. As such it can provide insights into, 
e.g., how institutional structures and bargaining 
strategies affect policy outcomes. Since 1995, 
every day of formal climate negotiations under 
the UN Framework Convention on Climate 
Change (UN FCCC) and the Kyoto Protocol has 
been summarized in a separate 2-4 page issue of 
the ENB1. The ENB seeks to cover the major 
topics of discussion and which negotiators 
(referred to by country name) said what. The 
publication is used by scholars to address 
research questions such as whether countries 
with more extreme positions have more or less 
success (Weiler, 2012) and whether democracies 
                                               
1 http://www.iisd.ca/linkages/vol12/ 
and autocracies (Bailer, 2012) or developed and 
developing countries (Castro et al., 2014) behave 
differently in negotiations. From our perspective, 
the ENB?s restricted content and style makes it 
appropriate to test our inductive approach.  
3 Inducing Information Structures 
We are investigating how the ADIOS algorithm 
(Solan et al., 2005) can be adapted and applied 
for mining the content of unannotated corpora; 
cf. Salway and Touileb (2014). Our objective of 
identifying salient information structures, rather 
than generating a complete grammatical 
description, leads us to modify the learning 
regime of ADIOS. Firstly, we modify the way in 
which text is presented to ADIOS by presenting 
sentences containing terms of interest (for the 
ENB texts these were country names), rather 
than processing all sentences: we expect more 
relevant patterning in these sentences, and think 
the patterning will be more explicit if not diluted 
by the rest of the corpus. Secondly, as described 
in more detail below, we focus the algorithm on 
frequent structures through an iterative process 
of selection and substitution.  
3.1 Method 
Our data set comprised all texts from the ENB 
volume 12, numbers 1-594, which cover the 
period 1995-2013. Preprocessing involved 
removing boilerplate text, sentence 
segmentation, and making all text lowercase. 
Then, all sentences mentioning one or more 
countries were selected. Every mention of a 
country, or a list of countries, was replaced with 
the token ?COUNTRY?: this serves to make 
patterning around mentions of countries more 
explicit. A list of country names was the only 
domain- and language-specific resource required 
for the processing described below. 
The resulting file of 32,288 sentences was 
processed by an implementation of the ADIOS 
algorithm, in which we modified the original 
learning regime to bias it towards frequent 
structures. After one execution of ADIOS we 
selected the five most frequent patterns (and any 
patterns contained within them) and replaced all 
instances of them in the input file with a unique 
identifier for each pattern: as with the 
?COUNTRY? token, we believe that this serves 
to make relevant patterning more explicit. We 
executed ADIOS and selected and substituted 
frequent patterns nine more times.  
29
3.2 Results 
In this way 53 patterns were identified, some of 
which are shown in Table 1 (patterns 1-7). Here 
patterns and equivalence classes are bracketed 
and nested. The sequential items in a pattern are 
separated by whitespace and the alternative items 
in an equivalence class are separated by ?|?. 
?COUNTRY? stands for a mention of a country, 
or a list of countries. In some cases we have 
manually merged and simplified patterns for 
clarity, but the structuring that they describe was 
all induced automatically. 
Pattern 1 captures a simple relation between 
countries that appears frequently in sentences 
like ?China supported by Saudi Arabia said??. It 
could thus be used as a simple template for 
extracting data about how countries align with 
one another (see section 4.1). Patterns 2-4 
represent a multitude of ways in which a 
country?s stated positions on issues can be 
reported. These patterns do not describe the 
issues, but could be used as cues to locate text 
fragments that do, e.g. by taking the text that 
follows ?COUNTRY said|noted|recommended| 
(etc.)?? (see section 4.2). Patterns 5 and 6 
appear to have captured a wide variety of verbs 
and noun phrases respectively. Presumably these 
verbs relate to things that countries say that they 
will do, or that they think should be done. The 
noun phrases appear to raise topics for 
discussion; consider how pattern 6 appears as 
part of 7. There were other patterns that did not 
contain any equivalence classes: these often 
captured domain terminology, e.g. ?(developing 
countries)?, ?(commitment period)?. 
Patterns 1-6 all have a relatively shallow 
structure. In order to induce further structure we 
made new input files, based on what we saw in 
the initial patterns.  We chose the most frequent 
?speech acts? from patterns 2-4, and for each one 
made a separate file containing only sentences 
that contained ?COUNTRY SPEECH_ACT?, e.g. 
one file that contained all the sentences matching 
?COUNTRY expressed?. Each speech act file 
was processed with 10 iterations of selection and 
substitution (cf. section 3.1). The resulting 
patterns, including 8-10 in Table 1, do indeed 
have richer structures and show in a more 
nuanced way how countries? positions are 
reported in the ENB texts. 
These results are encouraging for the idea of 
inducing information structures from an 
unannotated text corpus. The examples shown in 
Table 1 would not surprise anybody who was 
familiar with the ENB material. However, they 
provide a useful summary view of what is 
typically written about countries. Further, since 
they relate directly to surface forms in the text, 
they may be valuable for guiding further 
quantitative analyses, e.g. by pinpointing where 
significant expressions of country positions, 
arguments and affinities are to be found. 
 
1. (COUNTRY ((supported|opposed) by) COUNTRY) 
2. (COUNTRY (said|noted|recommended|explained|responded|stressed| questioned|addressed|reiterated| 
reported|urged|amended|invited?)); the equivalence class contains 51 words 
3. (COUNTRY ((clarified|urged|reported) that) 
4. (COUNTRY ((presented|demanded|outlined|favored (the|a))  
5. (to (apply|safeguard|undertake|link|deliver?)); the equivalence class contains 63 words 
6. (the (merit|cost effectiveness| merits|importance|idea?) of); the equivalence class contains 84 words 
7. ((COUNTRY (noted|said|questioned|?)) (the (merit|cost effectiveness| merits|importance|idea?) 
of)) 
8. (COUNTRY expressed ((disappointment|concern) that)|((support|appreciation) 
for)|((readiness|willingness) to)|(satisfaction (with the) (outcome|reconstitution|functioning|work) (of 
the))) 
9. (COUNTRY called (((for |on) (parties|(developed countries)) to)|((for a) (cautious|three 
phased|common|phased|bottom up|budget|global) approach to)|(for an 
(overview|elaboration|analysis|evaluation|examination) of))) 
10. (COUNTRY highlighted ((the (need|basis) for)|(the (benefits|possibility|establishment) of)|(the 
(consideration|impact|impacts) of)|(the (use|involvement) of)|((the need to) (err|focus) on)|(the 
(role|importance) (of the)))) 
Table 1: Ten of the patterns automatically induced from Earth Negotiations Bulletin texts. 
30
4 Using selected information structures  
Here we describe our first steps in using some of 
the induced structures to infer coalitions (4.1) 
and to scale negotiation positions (4.2). 
4.1  Dyads of support and opposition  
The pattern ?(COUNTRY ((supported|opposed) 
by) COUNTRY)?, cf. Table 1,  was used as a 
regular expression to extract instances where 
relations between countries were recorded with 
respect to stated positions. This gave 1145 
instances of support, and 592 of opposition, often 
involving multiple countries; recall that 
?COUNTRY? may stand for a list of countries. A 
count was made for each pair of countries in 
support and opposition, with a distinction made 
between ?C1 supported by C2? and ?C2 
supported by C1?. Figure 1 is a scatterplot made 
from these counts. It shows, for example, that the 
US very often finds its statements supported by 
Canada. Further, whilst the US tends to support 
the EU relatively often, the EU supports the US 
only about as often as it opposes the US.  
 
Figure 1: Dyads of support and opposition 
4.2 Scaling negotiation positions 
Patterns 2-4 from Table 1 were combined into a 
regular expression to extract instances of the 
statements made by countries. For each country a 
file was made with the text following every 
instance of ?COUNTRY said | noted | 
recommended | (etc.)?, until the end of the 
sentence. The collection of country files was then 
analyzed with Wordfish (Slapin and Proksch, 
2008): this tool, which implements a scaling 
model, positions texts (here reflecting countries) 
on a political/ideological dimension based on the 
relative frequency of discriminating words.  
For the 40 countries with the most statements, 
the parameter indicating country position on the 
induced dimension ranged in ascending order 
from Austria (-2.38) via Belgium, Germany, the 
UK, Switzerland, the US, Canada, Australia, 
Norway, France, Russia, New Zealand to Japan 
(-.62) and on to Papua New Guinea (-.26), 
Tuvalu, Peru, Mexico, Brazil, Argentina, 
Malaysia, South Korea, Colombia, Saudi Arabia, 
Chile, Kuwait, Nigeria, Grenada, Uganda, 
Bangladesh, China, Egypt, the Philippines, South 
Africa, Indonesia, Venezuela, Iran, Bolivia, 
Barbados, India and Algeria (1.44).  
The method thus perfectly identifies the main 
cleavage in international climate negotiations 
between developed and developing countries (cf. 
Castro et al., 2014). The bifurcation is robust to 
alternative specifications. Among the ten most 
discriminating lemmas used by developing 
countries are ?equal?, ?distribut?, ?resourc?, 
?histor?, and ?equiti?, suggesting an emphasis on 
fairness and rich countries? historical emissions. 
5 Closing Remarks 
The novel use of a grammar induction algorithm 
was successful in elucidating the content of a 
corpus in a complementary way to bag-of-words 
techniques: some of the induced structures were 
useful for guiding subsequent analyses as part of 
a data-driven approach to computational social 
science. Specifically, in this case, the structures 
facilitated text analysis at the statement level, i.e. 
statements about country relations and countries? 
positions. This meant we could plot country 
relations and scale country positions even though 
our source texts were not organized by country. 
Given its inherent portability, we see the 
potential for applying the grammar induction 
approach to many other corpora, most obviously 
the other 32 ENB volumes, and other texts with 
similarly restricted content and style, e.g. 
parliamentary proceedings. It remains a largely 
open question as to what happens when the text 
input becomes more heterogeneous, but see 
Salway and Touileb (2014) regarding the 
processing of blog posts. 
In ongoing work we are seeking to understand 
more about how the parameters of the ADIOS 
algorithm, and the modifications we make, affect 
the set of structures that it identifies. Also we are 
considering evaluation metrics to validate the 
induced patterns and to measure recall.  
31
Acknowledgements 
We are very grateful to Zach Solan for providing 
an implementation of the ADIOS algorithm, and 
to Knut Hofland for his help in creating our 
corpus of ENB texts. This research was 
supported by a grant from The Research Council 
of Norway?s VERDIKT program. 
References 
Stefanie Bailer. 2012. Strategy in the climate change 
negotiations: do democracies negotiate differently? 
Climate Policy 12(5): 534-551. 
Paula Castro, Lena H?rnlein, and Katharina 
Michaelowa. 2014. Constructed peer groups and 
path dependence in international organizations. 
Global Environmental Change. 
Arianna D?Ulizia, Fernando Ferri and Patrizia 
Grifoni. 2011. A survey of grammatical inference 
methods for natural language learning. Artificial 
Intelligence Review 36(1):1-27. 
Oren Etzioni, Michele Banko, Stephen Soderland and 
Daniel S. Weld. Open Information Extraction from 
the Web. Comms. of the ACM 51(12): 68-74. 
Justin Grimmer and Brandon M. Stewart. 2013. Text 
as Data: The Promise and Pitfalls of Automatic 
Content Analysis Methods for Political Texts. 
Political Analysis 21(3):267-297. 
Zellig Harris. 1954. Distributional Structure. Word 
10(2/3):146-162. 
Zellig Harris. 1988. Language and Information. 
Columbia University Press, New York. 
Eileen Riloff. 1996. Automatically Generating 
Extraction Patterns from Untagged Text. Procs. 
13th National Conference on Artificial Intelligence 
(AAAI-96):1044-1049. 
Andrew Salway and Samia Touileb. 2014. Applying 
Grammar Induction to Text Mining. To appear in 
Procs. ACL 2014. 
Satoski Sekine. 2006. On-Demand Information 
Extraction. Procs. COLING/ACL 2006: 731-738. 
Jonathan Slapin and Sven-Oliver Proksch. 2008. A 
Scaling Model for Estimating Time-Series Party 
Positions from Texts. American Journal of 
Political Science 52(3):705-722. 
Zach Solan, David Horn, Eytan Ruppin, and Shimon 
Edelman. 2005. Unsupervised learning of natural 
languages. PNAS 102(33):11629-11634. 
Florian Weiler. 2012. Determinants of bargaining 
success in the climate change negotiations. Climate 
Policy 12(5):552-574.  
 
32
