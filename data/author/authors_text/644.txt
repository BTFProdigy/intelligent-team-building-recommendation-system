The Annotation Graph Toolkit:
Software Components for
Building Linguistic Annotation Tools
Kazuaki Maeda, Steven Bird, Xiaoyi Ma and Haejoong Lee
Linguistic Data Consortium, University of Pennsylvania
3615 Market St., Philadelphia, PA 19104-2608 USA
fmaeda, sb, xma, haejoongg@ldc.upenn.edu
ABSTRACT
Annotation graphs provide an efficient and expressive data model
for linguistic annotations of time-series data. This paper reports
progress on a complete software infrastructure supporting the rapid
development of tools for transcribing and annotating time-series
data. This general-purpose infrastructure uses annotation graphs
as the underlying model, and allows developers to quickly create
special-purpose annotation tools using common components. An
application programming interface, an I/O library, and graphical
user interfaces are described. Our experience has shown us that it
is a straightforward task to create new special-purpose annotation
tools based on this general-purpose infrastructure.
Keywords
transcription, coding, annotation graph, interlinear text, dialogue
annotation
1. INTRODUCTION
Annotation graphs (AGs) provide an efficient and expressive
data model for linguistic annotations of time-series data [2]. This
paper reports progress on a complete software infrastructure sup-
porting the rapid development of tools for transcribing and anno-
tating time-series data. This general-purpose infrastructure uses
annotation graphs as the underlying model, and allows developers
to quickly create special-purpose annotation tools using common
components. This work is being done in cooperation with the
developers of other widely used annotation systems, Transcriber
and Emu [1, 3].
The infrastructure is being used in the development of a series
of annotation tools at the Linguistic Data Consortium. Several
such tools are shown in the paper: one for dialogue annotation,
one for telephone conversation transcription, and one for interlinear
transcription aligned to speech.
This paper will cover the following points: the application pro-
gramming interfaces for manipulating annotation graph data and
importing data from other formats; the model of inter-component
.
communication which permits easy reuse of software components;
and the design of the graphical user interfaces, which have been
tailored to be maximally ergonomic for the tasks.
The project homepage is: [http://www.ldc.upenn.edu/
AG/]. The software tools and software components described in
this paper are available through a CVS repository linked from this
homepage.
2. ARCHITECTURE
2.1 General Architecture
Existing annotation tools are based on a two level model (Fig-
ure 1 Top). The systems we demonstrate are based around a three
level model, in which annotation graphs provide a logical level
independent of application and physical levels (Figure 1 Bottom).
The application level represents special-purpose tools built on top
of the general-purpose infrastructure at the logical level.
The system is built from several components which instantiate
this model. Figure 2 shows the architecture of the tools currently
being developed. Annotation tools, such as the ones discussed
below, must provide graphical user interface components for signal
visualization and annotation. The communication between compo-
nents is handled through an extensible event language. An appli-
cation programming interface for annotation graphs (AG-API) has
been developed to support well-formed operations on annotation
graphs. This permits applications to abstract away from file format
issues, and deal with annotations purely at the logical level.
2.2 The Annotation Graph API
The complete IDL definition of the AG-API is provided in the
appendix (also online). Here we describe a few salient features of
the API.
The API provides access to internal objects (signals, anchors,
annotations etc) using identifiers. Identifiers are strings which con-
tain internal structure. For example, an AG identifier is quali-
fied with an AGSet identifier: AGSetId:AGId. Annotations and
anchors are doubly qualified: AGSetId:AGId:AnnotationId,
AGSetId:AGId:AnchorId. Thus, it is possible to determine from
any given identifiers, its membership in the overall data structure.
The functioning of the API will now be illustrated with a series
of examples. Suppose we have already constructed an AG and now
wish to create a new anchor. We might have the following API call:
CreateAnchor( "agSet12:ag5", 15.234, "sec" );
This call would construct a new anchor object and return its
identifier: agSet12:ag5:anchor34. Alternatively, if we already
Physical
Level
Application
Level
Query
Systems
Evaluation
Software
Annotation
ToolsExtraction
Systems
Visualization
& Exploration
Conversion
Tools
RDB
Format XML Tab delimited
flat files
Automatic
Aligners
Physical
Level
Application
Level
Logical
Level
Tab delimited
flat files
RDB
Format
XML
Query
Systems
Automatic
Aligners
Conversion
Tools
Extraction
Systems
Visualization
& Exploration
Evaluation
Software
Annotation
Tools
AG-API
Figure 1: The Two and Three-Level Architectures for Speech
Annotation
Figure 2: Architecture for Annotation Systems
have an anchor identifier that we wish to use for this new anchor
(e.g. because we are reading previously created annotation data
from a file and do not wish to assign new identifiers), then we could
have the following API call:
CreateAnchor( "agset12:ag5:anchor34", 15.234, "sec" );
This call will return agset12:ag5:anchor34.
Once a pair of anchors have been created it is possible to create
an annotation which spans them:
CreateAnnotation( "agSet12:ag5",
"agSet12:ag5:anchor34",
"agSet12:ag5:anchor35",
"phonetic" );
This call will construct an annotation object and return an iden-
tifier for it, e.g. agSet12:ag5:annotation41. We can now add
features to this annotation:
SetFeature( "agSet12:ag5:annotation41",
"date", "1999-07-02" );
The implementation maintains indexes on all the features, and
also on the temporal information and graph structure, permitting
efficient search using a family of functions such as:
GetAnnotationSetByFeature( "agSet12:ag5",
"date", "1999-07-02" );
2.3 A File I/O Library
A file I/O library (AG-FIO) to support creation and export of AG
data has been developed. This will eventually handle all widely
used annotation formats. Formats currently supported by the AG-
FIO library include the TIMIT, BU, Treebank, AIF (ATLAS Inter-
change Format), Switchboard and BAS Partitur formats.
2.4 Inter-component Communication
Figure 3 shows the structure of an annotation tool in terms of
components and their inter-communications.
Main program - a small script
Waveform
display
Transcription
editor
Internal
representation
File input
/ output
AG-GUI-API
AG-GUI-API AG-API
AG-FIO-API
Figure 3: The Structure of an Annotation Tool
The main program is typically a small script which sets up the
widgets and provides callback functions to handle widget events.
In this example there are four other components which are reused
by several annotation tools. The AG and AG-FIO components
have already been described. The waveform display component
(of which there may be multiple instances) receives instructions to
pan and zoom, to play a segment of audio data, and so on. The tran-
scription editor is an annotation component which is specialized for
a particular coding task. Most tool customization is accomplished
by substituting for this component.
Both GUI components and the main program support a com-
mon API for transmitting and receiving events. For example, GUI
components have a notion of a ?current region? ? the timespan
which is currently in focus. A waveform component can change
an annotation component?s idea of the current region by sending a
SetRegion event (Figure 4). The same event can also be used in
the reverse direction. The main program routes the events between
GUI components, calling the AG-API to update the internal repre-
sentation as needed. With this communication mechanism, it is a
straightforward task to add new commands, specific to the annota-
tion task.
Main program
Waveform display AG-API Transcription editor
User types Control-G Update Display
SetRegion t1 t2 AG::SetAnchorOffset SetRegion t1 t2
Update
Internal Representation
Figure 4: Inter-component Communication
2.5 Reuse of Software Components
The architecture described in this paper allows rapid develop-
ment of special-purpose annotation tools using common compo-
nents. In particular, our model of inter-component communica-
tion facilitates reuse of software components. The annotation tools
described in the next section are not intended for general purpose
annotation/transcription tasks; the goal is not to create an ?emacs
for linguistic annotation?. Instead, they are special-purpose tools
based on the general purpose infrastructure. These GUI com-
ponents can be modified or replaced when building new special-
purpose tools.
3. GRAPHICAL USER INTERFACES
3.1 A Spreadsheet Component
The first of the annotation/transcription editor components we
describe is a spreadsheet component. In this section, we show two
tools that use the spreadsheet component: a dialogue annotation
tool and a telephone conversation transcription tool.
Dialogue annotation consists of assigning a field-structured record
to each utterance in each speaker turn. A key challenge is to
handle overlapping speaker turns and back-channel cues without
disrupting the structure of individual speaker contributions. The
tool solves these problems and permits annotations to be aligned
to a (multi-channel) recording. The records are displayed in a
spreadsheet. Clicking on a row of the spreadsheet causes the corre-
sponding extent of audio signal to be highlighted. As an extended
recording is played back, annotated sections are highlighted (both
waveform and spreadsheet displays).
Figure 5 shows the tool with a section of the TRAINS/DAMSL
corpus [4]. Figure 6 shows another tool designed for transcribing
telephone conversations. This latter tool is a version of the dialogue
annotation tool, with the columns changed to accommodate the
needed fields: in this case, speaker turns and transcriptions. Both
of these tools are for two-channel audio files. The audio channel
corresponding to the highlighted annotation in the spreadsheet is
also highlighted.
3.2 An Interlinear Transcription Component
Interlinear text is a kind of text in which each word is anno-
tated with phonological, morphological and syntactic information
(displayed under the word) and each sentence is annotated with a
free translation. Our tool permits interlinear transcription aligned
to a primary audio signal, for greater accuracy and accountability.
Whole words and sub-parts of words can be easily aligned with
the audio. Clicking on a piece of the annotation causes the corre-
sponding extent of audio signal to be highlighted. As an extended
recording is played back, annotated sections are highlighted (both
waveform and interlinear text displays).
The following screenshot shows the tool with some interlinear
text from Mawu (a Manding language of the Ivory Coast, West
Africa).
Figure 7: Interlinear Transcription Tool
3.3 A Waveform Display Component
The tools described above utilize WaveSurfer and Snack devel-
oped by Ka?re Sjo?lander and Jonas Beskow [7, 8]. WaveSurfer
allows developers to specify event callbacks through a plug-in
architecture. We have developed a plug-in for WaveSurfer that
enables the inter-component communication described in this paper.
In addition to waveforms, it is also possible to show spectrograms
and pitch contours of a speech file if the given annotation task
requires phonetic analysis of the speech data.
4. FUTURE WORK
4.1 More GUI Components
In addition to the software components discussed in this paper,
we plan to develop more components to support various annotation
tasks. For example, a video component is being developed, and it
will have an associated editor for gestural coding. GUI components
for Conversation Analysis (CA) [6] and CHAT [5] are also planned.
4.2 An Annotation Graph Server
We are presently designing a client-side component which presents
the same AG-API to the annotation tool, but translates all calls
Figure 5: Dialogue Annotation Tool for the TRAINS/DAMSL Corpus
Figure 6: Telephone Conversation Transcription Tool for the CALLFRIEND Spanish Corpus
into SQL and then transmits them to a remote SQL server (see
Figure 8). A centralized server could house a potentially large
quantity of annotation data, permitting multiple clients to collabo-
ratively construct annotations of shared data. Existing methods for
authentication and transaction processing will be be used to ensure
the integrity of the data.
AG-API
Mapping to SQL
SQL
RDB server and
persistent storage
Main program - a small script
Waveform
display
Transcription
editor
File input
/ output
AG-GUI-API
AG-GUI-API
AG-FIO-API
network
Figure 8: Annotation Tool Connecting to Annotation Server
4.3 Timeline for Development
A general distribution (Version 1.0) of the tools is planned for the
early summer, 2001. Additional components and various improve-
ments will be added to future releases. Source code will be
available through a source code distribution service, SourceForge
([http://sourceforge.net/projects/agtk/]). Further
schedule for updates will be posted on our web site: [http:
//www.ldc.upenn.edu/AG/].
5. CONCLUSION
This paper has described a comprehensive infrastructure for
developing annotation tools based on annotation graphs. Our expe-
rience has shown us that it is a simple matter to construct new
special-purpose annotation tools using high-level software compo-
nents. The tools can be quickly created and deployed, and replaced
by new versions as annotation tasks evolve. The components and
tools reported here are all being made available under an open
source license.
6. ACKNOWLEDGMENT
This material is based upon work supported by the National
Science Foundation under Grant No. 9978056 and 9983258.
7. REFERENCES
[1] C. Barras, E. Geoffrois, Z. Wu, and M. Liberman. Transcriber:
development and use of a tool for assisting speech corpora
production. Speech Communication, 33:5?22, 2001.
[2] S. Bird and M. Liberman. A formal framework for linguistic
annotation. Speech Communication, 33:23?60, 2001.
[3] S. Cassidy and J. Harrington. Multi-level annotation of
speech: An overview of the emu speech database management
system. Speech Communication, 33:61?77, 2001.
[4] D. Jurafsky, E. Shriberg, and D. Biasca. Switchboard
SWBD-DAMSL Labeling Project Coder?s Manual, Draft 13.
Technical Report 97-02, University of Colorado Institute of
Cognitive Science, 1997. [http://stripe.colorado.
edu/?jurafsky/manual.august1.html].
[5] B. MacWhinney. The CHILDES Project: Tools for Analyzing
Talk. Mahwah, NJ: Lawrence Erlbaum., second edition, 1995.
[http://childes.psy.cmu.edu/].
[6] E. Schegloff. Reflections on studying prosody in
talk-in-interaction. Language and Speech, 41:235?60, 1998.
[http://www.sscnet.ucla.edu/soc/faculty/
schegloff/prosody/].
[7] K. Sjo?lander. The Snack sound toolkit, 2000.
[http://www.speech.kth.se/snack/].
[8] K. Sjo?lander and J. Beskow. WaveSurfer ? an open source
speech tool. In Proceedings of the 6th International
Conference on Spoken Language Processing, 2000.
[http://www.speech.kth.se/wavesurfer/].
APPENDIX
A. IDL DEFINITION FOR FLAT AG API
interface AG {
typedef string Id; // generic identifier
typedef string AGSetId; // AGSet identifier
typedef string AGId; // AG identifier
typedef string AGIds;
// AG identifiers (space separated list)
typedef string AnnotationId;
// Annotation identifier
typedef string AnnotationType; // Annotation type
typedef string AnnotationIds;
// Annotation identifiers (list)
typedef string AnchorId; // Anchor identifier
typedef string AnchorIds;
// Anchor identifiers (list)
typedef string TimelineId; // Timeline identifier
typedef string SignalId; // Signal identifier
typedef string SignalIds;
// Signal identifiers (list)
typedef string FeatureName; // feature name
typedef string FeatureNames; // feature name (list)
typedef string FeatureValue; // feature value
typedef string Features;
// feature=value pairs (list)
typedef string URI;
// a uniform resource identifier
typedef string MimeClass; // the MIME class
typedef string MimeType; // the MIME type
typedef string Encoding; // the signal encoding
typedef string Unit; // the unit for offsets
typedef string AnnotationRef;
// an annotation reference
typedef float Offset; // the offset into a signal
//// AGSet ////
// Id is AGSetId or AGId
AGId CreateAG( in Id id
in TimelineId timelineId );
boolean ExistsAG( in AGId agId );
void DeleteAG( in AGId agId );
AGIds GetAGIds( in AGSetId agSetId );
//// Signals ////
TimelineId CreateTimeline( in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track );
TimelineId CreateTimeline( in TimelineId timelineId,
in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track);
boolean ExistsTimeline( in TimelineId timelineId );
void DeleteTimeline( in TimelineId timelineId );
// Id may be TimelineId or SignalId
SignalId CreateSignal( in Id id,
in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track );
boolean ExistsSignal( in SignalId signalId );
void DeleteSignal( in SignalId signalId );
SignalIds GetSignals( in TimelineId timelineId );
MimeClass
GetSignalMimeClass( in SignalId signalId );
MimeType
GetSignalMimeType( in SignalId signalId );
Encoding GetSignalEncoding( in SignalId signalId );
string GetSignalXlinkType( in SignalId signalId );
string GetSignalXlinkHref( in SignalId signalId );
string GetSignalUnit( in SignalId signalId );
Track GetSignalTrack( in SignalId signalId );
//// Annotation ////
// Id may be AGId or AnnotationId
AnnotationId CreateAnnotation( in Id id,
in AnchorId anchorId1,
in AnchorId anchorId2,
in AnnotationType annotationType );
boolean ExistsAnnotation
(in AnnotationId annotationId );
void DeleteAnnotation
(in AnnotationId annotationId );
AnnotationId CopyAnnotation
(in AnnotationId annotationId );
AnnotationIds SplitAnnotation
(in AnnotationId annotationId );
AnnotationIds NSplitAnnotation(
in AnnotationId annotationId, in short N );
AnchorId
GetStartAnchor( in AnnotationId annotationId);
AnchorId GetEndAnchor(
in AnnotationId annotationId);
void SetStartAnchor( in AnnotationId annotationId,
in AnchorId anchorId );
void SetEndAnchor( in AnnotationId annotationId,
in AnchorId anchorId );
Offset
GetStartOffset( in AnnotationId annotationId );
Offset GetEndOffset(
in AnnotationId annotationId );
void SetStartOffset( in AnnotationId annotationId,
in Offset offset );
void SetEndOffset( in AnnotationId annotationId,
in Offset offset );
// this might be necessary to package up an id
// into a durable reference
AnnotationRef GetRef( in Id id );
//// Features ////
// this is for both the content of an annotation,
// and for the metadata associated with AGSets,
// AGs, Timelines and Signals.
void SetFeature( in Id id,
in FeatureName featureName,
in FeatureValue featureValue );
boolean ExistsFeature( in Id id,
in FeatureName featureName );
void DeleteFeature( in Id id,
in FeatureName featureName );
string GetFeature( in Id id,
in FeatureName featureName );
void UnsetFeature( in Id id,
in FeatureName featureName );
FeatureNames GetFeatureNames( in Id id );
void SetFeatures( in Id id,
in Features features );
Features GetFeatures( in Id id );
void UnsetFeatures( in Id id );
//// Anchor ////
// Id may be AGId or AnchorId
AnchorId CreateAnchor( in Id id,
in Offset offset,
in Unit unit,
in SignalIds signalIds );
AnchorId CreateAnchor( in Id id,
in SignalIds signalIds );
AnchorId CreateAnchor( in Id id );
boolean ExistsAnchor( in AnchorId anchorId );
void DeleteAnchor( in AnchorId anchorId );
void SetAnchorOffset( in AnchorId anchorId,
in Offset offset );
Offset GetAnchorOffset( in AnchorId anchorId );
void UnsetAnchorOffset( in AnchorId anchorId );
AnchorId SplitAnchor( in AnchorId anchorId );
AnnotationIds GetIncomingAnnotationSet(
in AnchorId anchorId );
AnnotationIds GetOutgoingAnnotationSet(
in AnchorId anchorId );
//// Index ////
AnchorIds GetAnchorSet( in AGId agId );
AnchorIds GetAnchorSetByOffset( in AGId agId,
in Offset offset,
in float epsilon );
AnchorIds GetAnchorSetNearestOffset(
in AGId agId,
in Offset offset );
AnnotationIds
GetAnnotationSetByFeature( in AGId agId,
in FeatureName featureName );
AnnotationIds
GetAnnotationSetByOffset( in AGId agId,
in Offset offset );
AnnotationIds
GetAnnotationSetByType( in AGId agId,
in AnnotationType annotationType );
//// Ids ////
// Id may be AGId, AnnotationId, AnchorId
AGSetId GetAGSetId( in Id id );
// Id may be AnnotationId or AnchorId
AGId GetAGId( in Id id );
// Id may be AGId or SignalId
TimelineId GetTimelineId( in Id id );
};
The OLAC Metadata Set and Controlled Vocabularies
Steven Bird
Linguistic Data Consortium
University of Pennsylvania
3615 Market Street, Suite 200
Philadelphia, PA 19104-2608, USA
sb@ldc.upenn.edu
Gary Simons
SIL International
7500 West Camp Wisdom Road
Dallas, TX 75236, USA
Gary Simons@sil.org
Abstract
As language data and associated
technologies proliferate and as the
language resources community rapidly
expands, it has become difficult to
locate and reuse existing resources.
Are there any lexical resources for
such-and-such a language? What
tool can work with transcripts in
this particular format? What is a
good format to use for linguistic data
of this type? Questions like these
dominate many mailing lists, since
web search engines are an unreliable
way to find language resources.
This paper describes a new digital
infrastructure for language resource
discovery, based on the Open Archives
Initiative, and called OLAC ? the
Open Language Archives Community.
The OLAC Metadata Set and the
associated controlled vocabularies
facilitate consistent description and
focussed searching. We report progress
on the metadata set and controlled
vocabularies, describing current issues
and soliciting input from the language
resources community.
1 Introduction
Language technology and the linguistic sciences
are confronted with a vast array of language
resources, richly structured, large and diverse.
Multiple communities depend on language
resources, including linguists, engineers,
teachers and actual speakers. Many individuals
and institutions provide key pieces of the
infrastructure, including archivists, software
developers, and publishers. Today we have
unprecedented opportunities to connect these
communities to the language resources they
need. First, inexpensive mass storage technology
permits large resources to be stored in digital
form, while the Extensible Markup Language
(XML) and Unicode provide flexible ways
to represent structured data and ensure its
long-term survival. Second, digital publication
? both on and off the world wide web ? is the
most practical and efficient means of sharing
language resources. Finally, a standard resource
description model, the Dublin Core Metadata Set,
together with an interchange method provided
by the Open Archives Initiative (OAI), make
it possible to construct a union catalog over
multiple repositories and archives.
In December 2000, an NSF-funded workshop
on Web-Based Language Documentation and
Description, held in Philadelphia, brought
together a group of nearly 100 language
software developers, linguists, and archivists
who are responsible for creating language
resources in North America, South America,
Europe, Africa, the Middle East, Asia and
Australia http://www.ldc.upenn.edu/
exploration/expl2000/. The outcome
of the workshop was the founding of the Open
Language Archives Community (OLAC), an
application of the OAI to digital archives of
language resources, with the following purpose:
OLAC, the Open Language Archives
Community, is an international
partnership of institutions and
individuals who are creating a
worldwide virtual library of language
resources by: (i) developing consensus
on best current practice for the digital
archiving of language resources,
and (ii) developing a network of
interoperating repositories and services
for housing and accessing such
resources.
This paper will describe the leading ideas
that motivate OLAC, before focussing on
the metadata set and the controlled vocabularies
which implement part (ii) of OLAC?s statement of
purpose. Metadata elements of special interest to
the language resources community include such
things as language identification and language
resource type. The corresponding controlled
vocabularies ensure consistent description.
For example, French language resources are
specified using an official RFC-3066 designation
(Alvestrand, 2001), instead of multiple distinct
text strings like ?French?, ?Francais? and
?Franc?ais?. A separate controlled vocabulary
exists for resource type, and has items such
as annotation/phonetic and description/grammar.
Services for end-users can map controlled
vocabularies onto convenient terminology for
any target language. (A live demonstration
accompanies this presentation.)
2 Locating Data, Tools and Advice
We can observe that the individuals who use
and create language resources are looking
for three things: data, tools, and advice. By
DATA we mean any information that documents
or describes a language, such as a published
monograph, a computer data file, or even
a shoebox full of hand-written index cards.
The information could range in content from
unanalyzed sound recordings to fully transcribed
and annotated texts to a complete descriptive
grammar. By TOOLS we mean computational
resources that facilitate creating, viewing,
querying, or otherwise using language data.
Tools include not just software programs, but also
Figure 1: In reality the user can?t always get there
from here
the digital resources that the programs depend
on, such as fonts, stylesheets, and document
type definitions. By ADVICE we mean any
information about what data sources are reliable,
what tools are appropriate in a given situation,
what practices to follow when creating new data,
and so forth. In the context of OLAC, the term
language resource is broadly construed to include
all three of these: data, tools and advice.
Unfortunately, today?s user does not have ready
access to the resources that are needed. Figure 1
offers a diagrammatic view of the reality. Some
archives (e.g. Archive 1) do have a site on the
internet which the user is able to find, so the
resources of that archive are accessible. Other
archives (e.g. Archive 2) are on the internet, so
the user could access them in theory, but the user
has no idea they exist so they are not accessible in
practice. Still other archives (e.g. Archive 3) are
not even on the internet. And there are potentially
hundreds of archives (e.g. Archive n) that the user
needs to know about. Tools and advice are out
there as well, but are at many different sites.
There are many other problems inherent in the
current situation. For instance, the user may not
be able to find all the existing data about the
language of interest because different sites have
called it by different names (low recall). The
user may be swamped with irrelevant resources
because search terms have important meanings in
other domains (low precision). The user may not
be able to use an accessible data file for lack of
being able to match it with the right tools. The
user may locate advice that seems relevant but
have no basis for judging its merits.
2.1 Bridging the gap
2.1.1 Why improved web-indexing is not
enough
As the internet grows and web-indexing
technologies improve one might hope that
a general-purpose search engine should be
sufficient to bridge the gap between people
and the resources they need, but this is a vain
hope. The first reason is that many language
resources, such as audio files and software, are
not text-based. The second reason concerns
language identification, the single most important
property for describing language resources.
If a language has a canonical name which is
distinctive as a character string, then the user
has a chance of finding any online resources
with a search engine. However, the language
may have multiple names, possibly due to the
vagaries of Romanization, such as a language
known variously as Fadicca, Fadicha, Fedija,
Fadija, Fiadidja, Fiyadikkya, and Fedicca (giving
low recall). The language name may collide with
a word which has other interpretations that are
vastly more frequent, e.g. the language names
Mango and Santa Cruz (giving low precision).
The third reason why general-purpose search
engines are inadequate is the simple fact that
much of the material is not, and will not, be
documented in free prose on the web. Either
people will build systematic catalogues of their
resources, or they won?t do it at all. Of course,
one can always export a back-end database
as HTML and let the search engines index
the materials. Indeed, encouraging people to
document resources and make them accessible
to search engines is part of our vision. However,
despite the power of web search engines, there
remain many instances where people still prefer
to use more formal databases to house their data.
This last point bears further consideration. The
challenge is to build a system for ?bringing like
things together and differentiating among them?
(Svenonius, 2000). There are two dominant
storage and indexing paradigms, one exemplified
by traditional databases and one exemplified
by the web. In the case of language resources,
the metadata is coherent enough to be stored in
a formal database, but sufficiently distributed
and dynamic that it is impractical to maintain it
centrally. Language resources occupy the middle
ground between the two paradigms, neither of
which will serve adequately. A new framework
is required that permits the best of both worlds,
namely bottom-up, distributed initiatives, along
with consistent, centralized finding aids. The
Dublin Core (DC) and the Open Archives
Initiative provide the framework we need to
?bridge the gap.?
2.1.2 The Dublin Core Metadata Initiative
The Dublin Core Metadata Initiative began in
1995 to develop conventions for resource discov-
ery on the web [dublincore.org]. The Dublin
Core metadata elements represent a broad, inter-
disciplinary consensus about the core set of ele-
ments that are likely to be widely useful to sup-
port resource discovery. The Dublin Core consists
of 15 metadata elements, where each element
is optional and repeatable: Title, Creator, Subject,
Description, Publisher, Contributor, Date, Type, For-
mat, Identifier, Source, Language, Relation, Coverage,
Rights. This set can be used to describe resources
that exist in digital or traditional formats.
In ?Dublin Core Qualifiers? (DCMI, 2000a)
two kinds of qualifications are allowed: encoding
schemes and refinements. An encoding scheme
specifies a particular controlled vocabulary or
notation for expressing the value of an element.
The encoding scheme serves to aid a client system
in interpreting the exact meaning of the element
content. A refinement makes the meaning of the
element more specific. For example, a Language
element can be encoded using the conventions
of RFC 3066 to unambiguously identify the
language in which the resource is written (or
spoken). A Subject element can be given a
language refinement to restrict its interpretation
to concern the language the resource is about.
2.1.3 The Open Archives Initiative
The Open Archives Initiative (OAI) was
launched in October 1999 to provide a common
framework across electronic preprint archives,
and it has since been broadened to include digital
repositories of scholarly materials regardless of
their type [www.openarchives.org] (Lagoze and
de Sompel, 2001).
Figure 2: Bridging the gap through community
infrastructure
In the OAI infrastructure, each participating
archive implements a repository ? a network
accessible server offering public access to
archive holdings. The primary object in an
OAI-conformant repository is called an item,
having a unique identifier and being associated
with one or more metadata records. Each
metadata record describes an archive holding,
which is any kind of primary resource such as
a document, raw data, software, a recording,
a physical artifact, a digital surrogate, and so
forth. Each metadata record will usually contain
a reference to an entry point for the holding, such
as a URL or a physical location, as shown in
Figure 2.
To implement the OAI infrastructure, a
participating archive must comply with two
standards: the OAI shared metadata set (Dublin
Core), which facilitates interoperability across
all repositories participating in the OAI, and the
OAI metadata harvesting protocol, which allows
software services to query a repository using
HTTP requests.
OAI archives are called ?data providers,?
though they are strictly just metadata providers.
Typically, data providers will also have a
submission procedure, together with a long-term
storage system, and a mechanism permitting
users to obtain materials from the archive. An
OAI ?service provider? is a third party that
provides end-user services (such as search
functions over union catalogs) based on metadata
harvested from one or more OAI data providers.
Figure 3 illustrates a single service provider
accessing three data providers (using the OAI
metadata harvesting protocol). End-users only
interact with service providers.
Figure 3: A Service Provider Accessing Multiple
Data Providers
Over the past decade, the Linguist List has
become the primary source of online informa-
tion for the linguistics community, reaching out
to over 13,000 subscribers worldwide, and having
four complete mirror sites. The Linguist List will
be augmenting its service by hosting the primary
service provider for OLAC, and permitting end-
users to browse distributed language resources at
a single place.
2.2 Applying the OAI to language resources
The OAI infrastructure is a new invention; it has
the bottom-up, distributed character of the web,
while simultaneously having the efficient, struc-
tured nature of a centralized database. This com-
bination is well-suited to the language resource
community, where the available data is growing
rapidly and where a large user-base is fairly con-
sistent in how it describes its resource needs.
The primary outcome of the Philadelphia work-
shop was the founding of the Open Language
Archives Community, and with it the identifica-
tion of an advisory board, alpha testers and mem-
ber archives. Details of these groups are available
from the OLAC site [www.language-archives.
org].
Recall that the OAI community is defined by
the archives which comply with the OAI metadata
harvesting protocol and that register with the OAI.
Any compliant repository can register as an Open
Archive, and the metadata provided by an Open
Archive is open to the public. OAI data providers
may support metadata standards in addition to
the Dublin Core. Thus, a specialist community
can define a metadata format which is specific
to its domain. Service providers, data providers
and users that employ this specialized metadata
format constitute an OAI subcommunity. The
workshop participants agreed unanimously that
the OAI provides a significant piece of the infras-
tructure needed for the language resources com-
munity.
In the same way that OLAC represents a
specialized subcommunity with respect to the
entire Open Archives community, there are
specialized subcommunities within the scope
of OLAC. For instance, the ISLE Meta Data
Initiative is developing a detailed metadata
scheme for corpora of recorded speech events
and their associated descriptions (MPI ISLE
Team, 2000). Similarly, the language data centers
? the Linguistic Data Consortium (LDC) and
the European Language Resources Association
(ELRA) ? are using OLAC metadata as the
basis of a joint catalog, and will add elements
and vocabularies for their specialized needs
(price, rights, and categories of membership
and use). For archived language resources that
are of this kind, such a metadata scheme would
support a richer description. This specialized
subcommunity can implement its own service
provider that offers focused searching based on
its own rich metadata set. At the same time, the
data providers will exposing OLAC and Dublin
Core versions of the metadata, permitting the
resources to be discovered by users of OLAC and
OAI service providers.
2.3 Federation and integration of language
resource archives
The OAI framework permits archives to
interoperate. OAI archives support the Dublin
Core metadata format and metadata harvesting
protocol. OLAC archives additionally support the
OLAC metadata format. Widespread adoption
of these standards will permit language resource
archives to be federated and integrated.
First, a collection of archives which support
the same metadata format can be federated, in
the sense that a virtual meta-archive can collect
all the information into a single place, and end-
users can query multiple archives simultaneously.
To demonstrate this, the Linguistic Data Consor-
tium has harvested the catalogs of three language
resource archives (LDC, ELRA, DFKI) and cre-
ated a prototype service provider. A search for
language=Bulgarian returns records from all three
archives, as shown in Figure 4 (Ba?nik and Bird,
2001).
Second, a collection of archives which support
the same metadata format can be integrated, in the
sense that relational joins can be performed across
different archives. This permits queries such as:
?find all lexicon tools that understand a format for
which Hungarian data is available.?
3 A Core Metadata Set for Language
Resources
The OLAC Metadata Set extends the Dublin
Core set only to the minimum degree required
to express basic properties of language resources
which are useful as finding aids.
All fifteen Dublin Core elements are used in the
OLAC Metadata Set. In order to suit the specific
needs of the language resources community, the
elements have been qualified following principles
articulated in ?Dublin Core Qualifiers? (DCMI,
2000a) and exemplified in (DCMI, 2000b).
This section describes some of the attributes,
elements and controlled vocabularies of the
OLAC Metadata Set. Before launching into
this discussion, we first review some XML
terminology and explain some aspects of the
OLAC representation which follow directly from
our choice of XML.
3.1 Aside: XML representation
The Extensible Markup Language (XML) is the
universal format for structured documents and
data on the Web [www.w3.org/XML]. The key
building block of an XML document is the ele-
ment. An element has a name, attributes and con-
tent. Here is an example of an element Language
with attributes refine and code, and free-text con-
tent:
<Language refine="OLAC" code="x-sil-BAN">
Foreke Dschang</Language>
In general, XML elements may contain other
elements, or they may be empty. XML Docu-
ment Type Definitions (DTDs) and XML schemas
are grammars that define the structure of a valid
XML document, and they limit the arrangement
of XML elements in a document. We believe it
oai:ldc:LDC94T5
Date: 1994
Title: ECI Multilingual Text
Type: text
Identifier: 1-58563-033-3
Subject.language: Albanian, Bulgarian, Chinese, Czech, Dutch, English, Estonian,
French, Gaelic, German, Greek, Italian, Japanese, Latin, Lithuanian,
Malay, Spanish, Danish, Uzbek, Norwegian, Portuguese, Russian,
Serbian, Swedish, Turkish, Tibetan
Identifier: http://www.ldc.upenn.edu/Catalog/LDC94T5.html
Description: Recommended Applications: information retrieval, machine transla-
tion, language modeling
oai:elra:L0030
Title: Bulgarian Morphological Dictionary
Date: 1998
Subject.language: Bulgarian
Description: 67,500 entries divided into 242 inflectional types (including proper
nouns), morphosyntactic information for each entry, and a morpho-
logical engine (MS DOS and WINDOWS 95/NT) for morphological
analysis and generation
Identifier: http://www.icp.inpg.fr/ELRA/cata/text det.html#bulmodic
oai:dfki:KPML
Title: KPML
Creator: Bateman and many others
Subject.language: Spanish, Russian, Japanese, Greek, German, French, English, Czech,
Bulgarian
Format.os: Windows NT, Windows 98, Windows 95/98, Solaris
Type.functionality: Software: Annotation Tools, Grammars, Lexica, Development Tools,
Formalisms, Theories, Deep Generation, Morphological Generation,
Shallow Generation
Description: Natural Language Generation Linguistic Resource Development and
Maintenance workbench for large scale generation grammar devel-
opment, teaching, and experimental generation. Based on systemic-
functional linguistics. Descendent of the Penman NLG system.
Identifier: http://www.purl.org/net/kpml
Description: Contact: bateman@uni-bremen.de
Relation.requires: Windows: none; Solaris: CommonLisp + CLIM
Figure 4: Querying the Prototype Service Provider for Bulgarian Resources
is important to use a formal mechanism for vali-
dating a metadata record. Following the OAI, we
use XML schemas to specify the OLAC metadata
format.
XML schemas make it possible for element
content and attribute values to be constrained
according to the element name. However, XML
schemas do not permit element content to be
constrained on the basis of the attribute value.
Accordingly, in implementing qualified Dublin
Core using XML, we are limited to using one
encoding scheme (or controlled vocabulary) per
element.
There are two cases we need to consider here.
In the case where all refinements of an element
employ the same encoding scheme, we use the
element name as is and add a refine attribute with
a fixed value. This documents that the particu-
lar encoding scheme has been used, and ensures
that the element cannot be confused with a cor-
responding unqualified Dublin Core element (see
the above example). In the case where differ-
ent refinements of an element employ different
encoding schemes, then a unique element must
be defined. Following (DCMI, 2000b), we define
such elements by concatenating the Dublin Core
element name and the refinement name with an
intervening dot. An example is shown below:
<Format.encoding code="iso-8859-1"/>
3.2 Attributes used in implementing the
OLAC Metadata Set
Three attributes ? refine, code, and lang ? are
used throughout the metadata set to handle most
qualifications to Dublin Core. Some elements in
the OLAC Metadata Set use the refine attribute
to identify element refinements. These quali-
fiers make the meaning of an element narrower
or more specific. A refined element shares the
meaning of the unqualified element, but with a
more restricted scope (DCMI, 2000a).
Some elements in the OLAC Metadata Set use
the code attribute to hold metadata values that are
taken from a specific encoding scheme. When an
element may take this attribute, the attribute value
specifies a precise value for the element taken
from a controlled vocabulary or formal notation
(x3.4). In such cases, the element content may
also be used to specify a freeform elaboration of
the coded value.
Every element in the OLAC Metadata Set may
use the lang attribute. It specifies the language
in which the text in the content of the element is
written. The value for the attribute comes from
a controlled vocabulary OLAC-Language. By
default, the lang attribute has the value ?en?, for
English. Whenever the language of the element
content is other than English, the lang attribute
should be used to identify the language. By
using multiple instances of the metadata elements
tagged for different languages, data providers
may offer their metadata records in multiple
languages.
In addition, there is a lang attribute on the
<olac> element that contains the metadata
elements for a given metadata record. It lists
the languages in which the metadata record
is designed to be read. This attribute holds
a space-delimited list of language codes. By
default, this attribute has the value ?en?, for
English, indicating that the record is aimed only
at English readers. If an explicit value is given for
the attribute, then the record is aimed at readers
of all the languages listed.
Service providers should use this information
in order to offer multilingual views of the meta-
data. When a metadata record lists only one alter-
native language, then all elements are displayed
(regardless of their individual languages), unless
the user has requested to suppress all records in
that language. When a metadata record has mul-
tiple alternative languages, the user should be able
to select one and have display of elements in the
other languages suppressed. An element in a
language not included in the list of alternatives
should always be displayed (for instance, the ver-
nacular title of a work).
3.3 The elements of the OLAC Metadata Set
In this section we present a synopsis of the
elements of the OLAC metadata set. For each
element, we provide a one sentence definition
followed by a brief discussion, systematically
borrowing and adapting the definitions provided
by the Dublin Core Metadata Initiative (DCMI,
1999). Each element is optional and repeatable.
Contributor: An entity responsible for making
contributions to the content of the
resource. Examples of a Contributor
include a person, an organization, or a
service. The refine attribute is optionally
used to specify the role played by the named
entity in the creation of the resource, using
the controlled vocabulary OLAC-Role.
Coverage: The extent or scope of the content
of the resource. Coverage will typically
include spatial location or temporal period.
Where the geographical information is pre-
dictable from the language identification, it
is not necessary to specify geographic cov-
erage.
Creator: An entity primarily responsible for
making the content of the resource. The
refine attribute is optionally used to specify
the role played by the named entity in the
creation of the resource, using the controlled
vocabulary OLAC-Role.
Date: A date associated with an event in the life
cycle of the resource. The refine attribute is
optionally used to refine the meaning of the
date using values from a controlled vocab-
ulary (for instance, date of creation versus
date of issue versus date of modification, and
so on). The vocabulary for refinements to
Date is defined in (DCMI, 2000a).
Description: An account of the content of the
resource. Description may include but is not
limited to: an abstract, table of contents, ref-
erence to a graphical representation of con-
tent, or a free-text account of the content.
Format: The physical or digital manifestation
of the resource. Typically, Format may
include the media-type or dimensions of the
resource. Format may be used to determine
the software, hardware or other equipment
needed to use the resource. The code
attribute identifies the format using the
controlled vocabulary OLAC-Format.
Format.cpu: The CPU required to use a soft-
ware resource. The code attribute identi-
fies the CPU using the controlled vocabulary
OLAC-CPU.
Format.encoding: An encoded character set
used by a digital resource. For a digitally
encoded text, Format.encoding names the
encoded character set it uses. For a font,
Format.encoding names an encoded character
set that it is able to render. For a software
application, Format.encoding names an
encoded character set that it can read or
write. The code attribute is used to identify
the character set using the controlled
vocabulary OLAC-Encoding.
Format.markup: The OAI identifier for the
definition of the markup format.
Format.markup provides an OAI identifier
for an XML DTD, schema or some other
definition of the markup format. (This has
the side-effect of ensuring that the format
definition is archived somewhere). For a
software resource, Format.markup names a
markup scheme that it can read or write.
The code attribute identifies the markup
scheme using the controlled vocabulary
OLAC-Markup.
Format.os: The operating system required to
use a software resource. The code attribute
is used to identify the operating system
using the controlled vocabulary OLAC-OS.
Additional restrictions for operating system
version, may be specified using the element
content.
Format.sourcecode: The programming lan-
guage(s) of software distributed in source
form. The code attribute identifies the
language using the controlled vocabulary
OLAC-Sourcecode.
Identifier: An unambiguous reference to
the resource within a given context.
Recommended best practice is to identify
the resource by means of a string or number
conforming to a globally-known formal
identification system (e.g. URIs, ISBNs).
For non-digital archives, Identifier may use
the existing scheme for locating a resource
within the collection.
Language: A language of the intellectual
content of the resource. Language is
used for a language the resource is in, as
opposed to the language it describes (see
Subject.language). It identifies a language
that the creator of the resource assumes
that its eventual user will understand. The
code attribute is used to make a precise
identification of the language using the
controlled vocabulary OLAC-Language.
Publisher: An entity responsible for making the
resource available. Examples of a publisher
include a person, an organization, or a ser-
vice.
Relation: A reference to a related resource. This
element is used to document relationships
between resources. The refine attribute is
used to refine the nature of the relationship
using values from a controlled vocabulary
(for instance, is replaced by, requires, is
part of, and so on). The vocabulary for
refinements to Relation is defined in (DCMI,
2000a).
Rights: Information about rights held in and
over the resource. Typically, a Rights ele-
ment will contain a rights management state-
ment for the resource, or reference a service
providing such information. Rights informa-
tion often encompasses intellectual property
rights (IPR), copyright, and various property
rights. The code attribute is used to make
a summary statement about rights using the
controlled vocabulary OLAC-Rights.
Rights.software: Information about rights held
in and over a software resource. A rights
statement pertaining to software, using the
controlled vocabulary OLAC-Software-
Rights.
Source: A reference to a resource from which
the present resource is derived. For
instance, it may be the bibliographic
information about a printed book of which
this is the electronic encoding or from which
the information was extracted.
Subject: The topic of the content of the
resource. Typically, a Subject will be
expressed as keywords, key phrases or
classification codes that describe a topic of
the resource. Recommended best practice
is to select a value from a controlled
vocabulary or formal classification scheme.
Subject.language: A language which the content
of the resource describes or discusses. As
with the Language element, a code attribute
is used to identify the language precisely.
Title: A name given to the resource. Typically,
a title will be a name by which the resource
is formally known. A translation of the title
can be supplied in a second Title element.
The lang attribute is used to identify the lan-
guage of these elements.
Type: The nature or genre of the content of
the resource. The code attribute is used to
identify the type using the Dublin Core con-
trolled vocabulary DC-Type.
Type.data: The nature or genre of the content of
the resource, from a linguistic standpoint.
Type includes terms describing general cate-
gories, functions, genres, or aggregation lev-
els for content. The code attribute is used to
identify the type using the controlled vocab-
ulary OLAC-Data.
Type.functionality: The functionality of a
software resource. The code attribute
is used to identify the type using the
controlled vocabulary OLAC-Functionality.
Observe that some elements, such as Format,
Format.encoding and Format.markup are applicable
to software as well as to data. Service providers
can exploit this feature to match data with appro-
priate software tools.
3.4 The controlled vocabularies
Controlled vocabularies are enumerations of legal
values for the code attribute. In some cases, more
than one value applies, in which case the corre-
sponding element must be repeated, once for each
applicable value. In other cases, no value is appli-
cable ands the corresponding element is simply
omitted. In yet other cases, the controlled vocab-
ulary may fail to provide a suitable item, in which
case a similar item can be optionally specified and
a prose comment included in the element content.
3.4.1 OLAC-Language
Language identification is an important
dimension of language resource classification.
However, the character-string representation
of language names is problematic for several
reasons: different languages (in different parts
of the world) may have the same name; the
same language may have a different name in
each country where it is spoken; within the
same country, the preferred name for a language
may change over time; in the early history of
discovering new languages (before names were
standardized), different people referred to the
same language by different names; and for
languages having non-Roman orthographies,
the language name may have several possible
romanizations. Together, these facts suggest that
a standard based on names will not work. Instead,
we need a standard based on unique identifiers
that do not change, combined with accessible
documentation that clarifies the particular speech
variety denoted by each identifier.
The information technology community has a
standard for language identification, namely, ISO
639 (ISO, 1998). Part 1 of this standard lists
two-letter codes for identifying 160 of the world?s
major languages; part 2 of the standard lists three-
letter codes for identifying about 400 languages.
ISO 639 in turn forms the core of another stan-
dard, RFC 3066 (formerly RFC 1766), which is
the standard used for language identification in
the xml:lang attribute of XML and in the language
element of the Dublin Core metadata set. RFC
3066 provides a mechanism for users to register
new language identification codes for languages
not covered by ISO 639, but very few additional
languages have been registered.
Unfortunately, the existing standard falls
far short of meeting the needs of the language
resources community since it fails to account for
more than 90% of the world?s languages, and
it fails to adequately document what languages
the codes refer to (Simons, 2000). However,
SIL?s Ethnologue (Grimes, 2000) provides a
complete system of language identifiers which
is openly available on the Web. OLAC will
employ the RFC 3066 extension mechanism
to build additional language identifiers based
on the Ethnologue codes. For the 130-plus
ISO-639-1 codes having a one-to-one mapping
onto Ethnologue codes, OLAC will support both.
Where an ISO code is ambiguous ? such as mhk
for ?other Mon Khmer languages? ? OLAC will
require the Ethnologue code. New identifiers
for ancient languages, currently being developed
by LINGUIST List, will be incorporated. These
language identifiers are expressed using the code
attribute of the Language and Subject.language
elements. The free-text content of these elements
may be used to specify an alternative human-
readable name for the language (where the name
specified by the standard is unacceptable for
some reason) or to specify a dialect (where the
resource is dialect-specific).
3.4.2 OLAC-Data
After language identification, another dimen-
sion of central importance is the linguistic type of
a resource. Notions such as ?lexicon? and ?gram-
mar? are fundamental to OLAC, and the discourse
of the language resources community depends on
shared assumptions about what these types mean.
We believe that it is helpful to distinguish at
least four top-level types: transcription, annota-
tion, description and lexicon, each defined broadly
as proposed below. A transcription is any time-
ordered symbolic representation of a linguistic
event. An annotation is any kind of structured
linguistic information that is explicitly aligned to
some spatial and/or temporal extent of a linguistic
record (such as a recorded signal or an image).
A description is any description or analysis of a
language; unlike a transcription or an annotation,
the structure of a description is independent of the
structure of the linguistic events that it describes.
A lexicon is any record-structured inventory of lin-
guistic forms.
For each of these top-level types we envision a
more specific vocabulary to facilitate greater pre-
cision. For example, an orthographic transcrip-
tion would have the code transcription/orthographic.
Other subtypes could include: phonetic, prosodic,
morphological, gestural, part-of-speech, syntactic, dis-
course, musical. The annotation type would include
these subtypes, and add others to cover spatial
annotation of images (e.g. for OCR annotation
of textual images or for isogloss maps).
The description type could have subtypes for
grammatical, phonological, orthographic, paradigms,
pedagogical, dialectal and comparative. The lexi-
con type could also carry subtypes to distinguish
wordlists, wordnets, thesauri and so forth.
3.4.3 Other controlled vocabularies
OLAC-CPU: A vocabulary for identifying the
CPU(s) for which the software is available,
in the case of binary distributions: x86, mips,
alpha, ppc, sparc, 680x0.
OLAC-Encoding: A vocabulary for identifying
the character encoding used by a digital
resource, e.g. iso-8859-1, ...
<?xml version="1.0" encoding="UTF-8"?>
<olac
xmlns="http://www.language-archives.org/OLAC/0.3/"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.language-archives.org/OLAC/0.3/
http://www.language-archives.org/OLAC/olac-0.3b1.xsd">
<Title>KPML</Title>
<Identifier>http://www.purl.org/net/kpml/</Identifier>
<Creator refine="Author">Bateman, John</Creator>
<Subject.language code="es"/> <Subject.language code="ru"/>
<Subject.language code="ja"/> <Subject.language code="el"/>
<Subject.language code="de"/> <Subject.language code="fr"/>
<Subject.language code="en"/> <Subject.language code="cs"/>
<Subject.language code="bg"/>
<Format.os code="MSWindows/winNT"/> <Format.os code="MSWindows/win95"/>
<Format.os code="MSWindows/win98"/> <Format.os code="Unix/Solaris"/>
<Type.functionality>Annotation Tools, Grammars, Lexica, Development Tools,
Formalisms, Theories, Deep Generation, Morphological Generation,
Shallow Generation</type.functionality>
<Relation refine="Requires">Windows: none; Solaris: CommonLisp + CLIM</Relation>
<Description>Natural Language Generation Linguistic Resource Development and
Maintenance workbench for large scale generation grammar development,
teaching, and experimental generation. Based on systemic-functional
linguistics. Descendent of the Penman NLG system.</Description>
</olac>
Figure 5: OLAC Metadata Record for KPML
OLAC-Format: A vocabulary for identifying
the manifestation of the resource. The
representation is inspired by MIME types,
e.g. text/sf for SIL standard format.
(Format.markup is used to identify the
particular tagset.) It may be necessary
to add new types and subtypes to cover
non-digital holdings, such as manuscripts,
microforms, and so forth and we expect to be
able to incorporate an existing vocabulary.
OLAC-Functionality: A vocabulary for clas-
sifying the functionality of software, again
using the MIME style of representation,
and using the HLT Survey as a source of
categories (Cole, 1997) as advocated by
the ACL/DFKI Natural Language Software
Registry. For example, written/OCR would
cover ?written language input, print or
handwriting optical character recognition.?
OLAC-OS: A vocabulary for identifying the
operating system(s) for which the software
is available: Unix, MacOS, OS2, MSDOS,
MSWindows. Each of these has optional
subtypes, e.g. Unix/Linux, MSWindows/winNT.
OLAC-Rights: A vocabulary for classifying
the rights held over a resource, e.g.: open,
restricted, ...
OLAC-Role: A vocabulary for identifying
the role of a contributor or creator of the
resource, e.g.: author, editor, translator,
transcriber, sponsor, ...
OLAC-Software-Rights: A vocabulary for
classifying the rights held over a resource,
e.g.: open-source, royalty-free-library, royalty-
free-binary, commercial, ...
OLAC-Sourcecode: A vocabulary for identify-
ing the programming language(s) used by
software which is distributed in source form,
e.g.: C++, Java, Python, Tcl, VB, ...
4 XML Representation
The OLAC metadata format consists of an XML
schema for the element set, and a set of schemas
for the controlled vocabularies. The latest ver-
sions are available from the OLAC website.
Figure 5 shows the OLAC metadata record cor-
responding to the KPML display from Figure 4.
The top element is olac; this references the XML
namespace for version 0.3b1 of the schema. The
contents of the olac element are the OLAC meta-
data elements, which are optional and repeatable,
and can occur in any order, as in Dublin Core.
Some elements employ the optional code or
refine attributes, and/or free-text content. The
third attribute, lang, is not used here since the free-
text content is in English (specified in the XML
schema as the default). For the Creator element,
the refine attribute narrows the meaning of cre-
ator to Author. For the Subject.language elements,
the code attribute specifies nine languages using
Ethnologue codes. A service provider would map
these codes to human-readable names.
The Format.os element illustrates a two-level
coding scheme, consisting of an OS ?family?,
followed by a specific operating system. Further
details can be included in the free-text content
if necessary. If a piece of software runs on all
members of an OS family, then the more detailed
designation can be omitted, e.g. code=?Unix?.
The Type.functionality element is specified using
free-text content, since the details of the
controlled vocabulary OLAC-Functionality are
still being worked out.
5 Conclusions
The OLAC Metadata Set and controlled vocabu-
laries are works in progress, and are continuing to
be revised with input from participating archives
and members of the wider language resources
community. We hope to have provided sufficient
motivation and exemplification for our choices so
that readers will easily be able to contribute to
ongoing developments.
Even once OLAC is completely in place, there
will still be documentation tasks which the cre-
ators of language resources will have to under-
take, and new habits to acquire. It will always be
necessary to identify and manually correct incon-
sistent or erroneous metadata. The OLAC con-
trolled vocabularies will need to be refined indef-
initely in response to changes in the world around
us. The creators of language resources will need
to generate metadata with each new resource and
place the resource in a suitable archive. The
communities will need to adopt best practices for
archival storage formats.
Despite these intrinsic limitations, the OLAC
Metadata Set and controlled vocabularies offer a
template for resource description, providing two
clear benefits over traditional full-text descrip-
tion and retrieval. First, the template guides the
resource creator in giving a complete description
of the resource, in contrast to prose descriptions
which may omit important details. And second,
the template associates a resource with standard
labels, such as creator and title, permitting users
to do focussed searching. Resources and reposi-
tories can proliferate, yet common metadata and
vocabularies will support centralized services giv-
ing users easy access to language resources.
References
Harald Alvestrand. 2001. RFC 3066: Tags for the
identification of languages (replaces 1766).
ftp://ftp.isi.edu/in-notes/rfc3066.txt.
?Eva Ba?nik and Steven Bird. 2001. LDC experimental
OLAC service provider. http://wave.ldc.upenn.
edu/OLAC/sp-0.2/sp.php4.
Ronald Cole, editor. 1997. Survey of the State of the Art in
Human Language Technology. Studies in Natural Language
Processing. Cambridge University Press.
http://cslu.cse.ogi.edu/HLTsurvey/.
DCMI. 1999. Dublin Core Metadata Element Set, version
1.1: Reference description. http://dublincore.
org/documents/1999/07/02/dces/.
DCMI. 2000a. Dublin Core qualifiers.
http://dublincore.org/documents/2000/07/
11/dcmes-qualifiers/.
DCMI. 2000b. Recording qualified Dublin Core metadata
in HTML. http://dublincore.org/documents/
2000/08/15/dcq-html/.
Barbara F. Grimes, editor. 2000. Ethnologue: Languages of
the World. Dallas: Summer Institute of Linguistics, 14th
edition. http//www.sil.org/ethnologue/.
ISO. 1998. ISO 639: Codes for the representation of names
of languages-part 2: Alpha-3 code.
http://lcweb.loc.gov/standards/
iso639-2/langhome.html.
Carl Lagoze and Herbert Van de Sompel. 2001. The Open
Archives Initiative: Building a low-barrier interoperability
framework. http://www.cs.cornell.edu/
lagoze/papers/oai-jcdl.pdf.
MPI ISLE Team. 2000. ISLE meta data elements for
session descriptions proposal.
http://www.mpi.nl/world/ISLE/documents/
draft/ISLE_Metadata_2.0.pdf.
Gary Simons. 2000. Language identification in metadata
descriptions of language archive holdings. In Steven Bird
and Gary Simons, editors, Proceedings of the Workshop on
Web-Based Language Documentation and Description.
http://www.ldc.upenn.edu/exploration/
expl2000/papers/simons/.
Elaine Svenonius. 2000. The Intellectual Foundation of
Information Organization. The MIT Press.
Annotation Graphs and Servers and Multi-Modal Resources: 
Infrastructure for Interdisciplinary Education, Research and 
Development 
Christopher Cieri 
University of Pennsylvania 
Linguistic Data Consortium 
3615 Market Street 
Philadelphia, PA. 19104-2608 USA 
ccieri@ldc.upenn.edu 
Steven Bird  
University of Pennsylvania 
Linguistic Data Consortium 
3615 Market Street 
Philadelphia, PA. 19104-2608 USA 
sb@ldc.upenn.edu 
 
 
Abstract 
Annotation graphs and annotation 
servers offer infrastructure to support 
the analysis of human language 
resources in the form of time-series 
data such as text, audio and video. This 
paper outlines areas of common need 
among empirical linguists and 
computational linguists. After 
reviewing examples of data and tools 
used or under development for each of 
several areas, it proposes a common 
framework for future tool development, 
data annotation and resource sharing 
based upon annotation graphs and 
servers. 
1 Introduction 
Despite different methodologies, goals and 
traditions, researchers in a variety of specialties 
in linguistics and computational linguistics share 
a core of assumptions and needs. Research 
communities in empirical linguistics, natural 
language processing, speech recognition, 
information retrieval and language teaching 
have a common need for language resources 
such as observations of linguistic performance, 
annotations encoding human judgment, 
standards for maintaining consistency among 
distributed resources and processes for 
extracting relevant observations. Where needs 
overlap, there is the opportunity to reuse 
existing resources and coordinate new initiatives 
so that communities share the burden of 
development while benefiting from the results. 
Where computational linguistics interacts with 
other areas of language research and teaching, 
there are additional opportunities for symbiosis. 
Natural language technology may offer greater 
access and robustness to empirical linguistic 
research that in turn may offer new data 
necessary to develop new technologies. This 
paper discusses common infrastructure for the 
annotation of linguistic data and the application 
of that infrastructure to several traditionally very 
diverse fields of inquiry.  
2 Common Assumptions, Needs and 
Goals in Natural Language Studies 
Human language resources, expensive to 
create and maintain, are in increasing demand 
among a growing number of research 
communities. One solution to this expanding 
need is to reannotate and reuse language 
resources created for other purposes. The now 
classic example is that of the Switchboard-1 
Corpus (ISBN: 1-58563-121-3), a collection of 
2400 two-sided telephone conversations among 
543 U.S. speakers, created by Texas Instruments 
in 1991. Although collected for speaker 
identification and topic spotting research, 
Switchboard has been widely used to support 
large vocabulary conversational speech 
recognition. It has been extensively corrected 
twice, once at Penn and NIST, and once at 
Mississippi State. Two excerpts have been 
published as test corpora for government-
sponsored projects. At least 6 other annotations 
have been created at various times and more-or-
less widely distributed among research sites: 
part-of-speech annotation (Penn); syntactic 
structure annotation (Penn); dysfluency 
annotation (Penn); partial phonetic transcription 
(independently at UCLA and at Berkeley); and 
discourse function annotation (Colorado). These 
annotations use different ?editions? of the 
underlying corpus and have sometimes silently 
introduced their own corrections or modified the 
data format to suit their needs. Thus the 
Colorado discourse function annotation was 
based on phrase structures introduced by the 
Penn dysfluency annotation, which in turn was 
based on the Penn/NIST corrections, which in 
turn were based on the original TI transcriptions 
of the underlying (and largely unchanging) 
audio files. Switchboard and its derivatives 
remain in active use worldwide, and new 
derivatives continue to be produced, along with 
(published and unpublished) corrections of old 
ones. This worsens the already acute problem of 
establishing and maintaining coherent relations 
among the derivatives in common use today. 
The Switchboard-1 case is by no means 
isolated (Graff & Bird 2000). The Topic 
Detection and Tracking Corpus, TDT-2 (ISBN: 
1-58563-157-4) was created in 1998 by LDC 
and contains newswire and more than 600 hours 
of transcribed broadcast news from 8 English 
and 3 Chinese sources sampled daily over six 
months with annotations to indicate story 
boundaries and relevance of those stories to 100 
randomly selected topics. Since its release, 
TDT-2 has been used as training, development-
test and evaluation data in the TDT evaluations; 
the audio has been used in TREC SDR 
evaluations (Garofalo, Auzanne and Voorhees 
2000), TDT text has been partially re-annotated 
for entity detection in the Automatic Content 
Extraction project (Przybocki 2000) and 
portions have been used for the Center for 
Spoken Language Processing?s workshops in 
Novel Information Detection (Allan et. al. 
1999), Mandarin-English Information (Meng et. 
al. 2000) and Audio-Visual Speech Recognition 
(Chalapati 2000). 
Switchboard and TDT are just two examples 
of a growing trend toward reannotation and 
reuse of language resources, a trend that is not 
limited to language engineering. Miller and 
Walker (2001) have demonstrated the value of 
the CALLHOME German corpus (ISBN: 1-
58563-117-5), developed to support speech 
recognition research, for language teaching. 
Deckert & Yaeger-Dror (2000) have used 
Switchboard to study regional syntactic 
variation in American English. 
Reannotation and reuse of linguistic data 
highlight the need for common infrastructure to 
support resource development across disciplines 
and specialties. 
3 Overlaps between Human Language 
Technology and Other Linguistic 
Research 
Many specialties in empirical linguistics and 
language engineering require large volumes of 
language data and tools for browsing and 
searching the data efficiently. The sections that 
follow provide examples of recent efforts to 
address emerging needs for language resources.  
Interlinear Texts and Linguistic Exploration  
Interlinear text is a product of linguistic 
fieldwork often in low-density languages. The 
physical appearance of interlinear text typically 
consists of a main text line annotated with 
linguistic transcriptions and analyses, such as 
morphological representations, glosses at 
various levels, part-of-speech tags, and a free 
translation at the sentence level. Fragments of 
these annotation lines are vertically aligned with 
the corresponding fragments of text. Phrasal 
translations and footnotes are often presented on 
other lines. Interlinear texts come in many forms 
and can be represented digitally in many ways, 
e.g. plain text with hard spacing, tables, special 
markup, and special-purpose data structures. 
There are various methods for linking to audio 
data and lexical entries, and for including 
footnotes and other marginalia. This diversity of 
form presents problems for general-purpose 
software for searching, exchanging, displaying 
and enriching interlinear texts. Nonetheless 
interlinear text is a precious resource with 
multiple uses in natural language processing. Its 
various components can be used in the 
development of lexical and morphological 
resources, can support tagging and parsing and 
can provide training material for machine 
translation. Maeda and Bird (2000, 2001) 
demonstrated a tool for creating interlinear text. 
A screenshot appears in Figure 1. 
Figure 1: Interlinear text tool   using the AG 
Toolkit 
Sociolinguistic Annotation  
The quantitative analysis of linguistic 
variation begins with empirical observation and 
statistical description of linguistic behavior. 
Although general computer technology 
encourages the collection, annotation, analysis 
and discussion of linguistic behavior wholly 
within the digital domain, few tools exist to help 
the sociolinguist in this effort. The project on 
Data and Annotations for Sociolinguistics 
(DASL) is investigating best practices via a case 
study of well-documented sociolinguistic 
phenomena in several large speech corpora: 
TIMIT , Switchboard-1, CallHome and Hub-4. 
Researchers are currently annotating the corpora 
for t/d deletion, the process by which [t] and [d] 
sometimes fail to be realized under certain 
phonological, morphological and social 
conditions. The case study is also a means to 
address broader questions: How do the specified 
corpora compare with the interview data 
typically used in sociolinguistics? Will the study 
of corpus data reveal new patterns not evident in 
the more common studies conducted within the 
framework of the speech community? Can 
empirical research on language variation be 
organized on a large scale with teams of non-
specialist annotators? 
All of the data used in DASL were originally 
created to support human language technology 
development; the datasets are currently being 
reannotated to support empirical studies of 
linguistic variation. A custom annotation tool 
allows users to query each corpus for tokens of 
potential interest greatly reducing effort relative 
to traditional approaches. Annotators can read or 
listen to each token, access demographic data 
and encode their observations in formats 
compatible with other analytical software used 
in the community. The web-based interface in 
Figure 2 promotes multi-site annotation and the 
study of inter-annotator consistency (Cieri and 
Strassel, 2001). 
Authoring Resources and Tools for Language 
Learning 
Although current information technology 
encourages new approaches in computer assisted 
language learning and teaching, progress in this 
area is hampered by an inadequate supply of 
language resources. The SMART (Source Media 
Authoring Resources and Tools) pilot project is 
addressing this problem by providing 
appropriately licensed data and software 
resources for preparing language-learning 
material. The Linguistic Data Consortium, a 
partner in this effort, is contributing several of 
its large data sets including conversational and 
broadcast data in Arabic, English, French and 
German. The language resources overlap almost 
completely with those used in language 
engineering. SMART is building upon the 
distribution model established in LDC Online, a 
service that provides network-based access to 
hundreds of gigabytes of text and audio data and 
annotations. Audio data are available digitally in 
files corresponding to a conversation, broadcast 
or other linguistic event. To facilitate searching, 
LDC Online includes, according to their 
availability, human- and machine-generated 
Figure 2: Sociolinguistic Annotation Tool 
transcripts time-aligned to permit more fine-
grained access. For example, where a time-
aligned transcript of a conversation exists, users 
may extract, reformat and play any segment 
specified by the time stamps in the transcript. 
SMART is building upon this foundation by 
providing additional data resources, browsing 
and search customized to the needs of language 
teachers and additional output formats to 
accommodate courseware authoring tools 
available in the commercial market. 
SMART promises to benefit a wide range of 
language teachers and learners but only to the 
extent that its resources are readily available. 
The volume of SMART data exceeds that which 
can be easily transferred over a network. Even 
small video clips consume hundreds of megabits 
of bandwidth. Instead SMART data will be 
delivered via servers that maintain raw data and 
associated annotations, permit browsing and 
queries and allow the user to specify the format 
and granularity of the response. The user will 
have the option of downloading the data for 
local use or adding annotations that may be kept 
privately or made public via the annotation 
server. The technology of the annotation server 
coupled with the extensibility of annotation 
graphs described below will enables nearly 
unconstrained access to SMART data. 
These efforts to support interlinear text, 
sociolinguistic annotation and multimodal data 
in language teaching each require flexible access 
to signal data and associated annotations. The 
sections that follow describe an architecture that 
provides such access. 
4 Annotation Graphs, Annotation 
Servers and a Query Language: 
Common Infrastructure for 
Coordinated Research, Resource 
Development 
Storing and serving large amounts of 
annotated data via the web requires 
interoperable data representations and tools 
along with methods for handling external 
formats and protocols for querying and 
delivering annotations. Annotation graphs were 
presented by Bird and Liberman (1999) as a 
general purpose model for representing and 
manipulating annotations of time series data, 
regardless of their physical storage format.  An 
annotation graph is a labeled, directed, acyclic 
graph with time offsets on some of its nodes. 
The formalism is illustrated below by 
application to the TIMIT Corpus (Garofolo et al 
1986). The original TIMIT word file contains 
starting and ending offsets (in 16KHz samples) 
and transcripts of each word in the audio file  
 
train/dr1/fjsp0/sa1.wrd: 
 2360    5200   she 
 5200    9680   had  
 9680   11077   your 
11077   16626   dark 
16626   22179   suit 
22179   24400   in 
24400   30161   greasy 
30161   36150   wash 
36720   41839   water 
41839   44680   all 
44680   49066   year  
 
The phone file provides the same information 
for each sound in the audio file. This is the 
phonetic transcription for ?she had?.  
 
train/dr1/fjsp0/sa1.phn: 
    0    2360    h# 
 2360    3720    sh 
 3720    5200    iy 
 5200    6160    hv 
 6160    8720    ae 
 8720    9680    dcl 
 9680   10173    y 
10173   11077    axr 
11077   12019    dcl 
12019   12257    d 
 
A section of the corresponding annotation 
graph appears in Figure 3. Each node displays 
the node identifier and the time offset.  The arcs 
are decorated with type and label information. 
Type W is for words and the type P is for 
phonetic transcriptions. 
Figure 3: A TIMIT annotation graph 
Since an annotation graph is just a set of 
(timed) nodes, arcs and labels, it can be trivially 
represented using three relational tables: 
 
Time:       Arc:               Label: 
 N     T      A   X   Y  T      A  L 
--------     -------------     ------- 
 0     0      1   0   1  P      1  h# 
 1  2360      2   1   2  P      2  sh 
 2  3270      3   2   3  P      3  iy 
 3  5200      4   3   4  P      4  hv 
 4  6160      5   4   5  P      5  ae 
 5  8720      6   5   6  P      6  dcl 
 6  9680      7   6   7  P      7  y 
 7 10173      8   7   8  P      8  axr 
 8 11077      9   8   9  P      9  dcl 
 9 12019     10   9  10  P     10  d 
 
10 12257     19   3   6  W     18  she 
14 16626     20   6   8  W     19  had 
17 22179     21   8  14  W     20  your 
             22  14  17  W     21  dark  
                               22  suit 
 
A large amount of annotation can be 
efficiently represented and indexed in this 
manner.  This brings us to the question of 
converting (or loading) existing data into such a 
database. The LDC's catalog alone includes 
nearly 200 publications, where each typically 
has its own format (often more than one). The 
sheer quantity and diversity of the data presents 
a significant challenge to the conversion 
process.  In addition, some corpora exist in 
multiple versions, or include uncorrected, 
corrected and re-corrected parts. 
The Annotation Graph Toolkit, version 1.0, 
contains a complete implementation of the 
annotation graph model, import filters for 
several formats, loading/storing data to an 
annotation server (MySQL), application 
programming interfaces in C++ and Tcl/tk, and 
example annotation tools for dialogue, ethology 
and interlinear text.  The supported formats are: 
xlabel, TIMIT, BAS Partitur, Penn Treebank, 
Switchboard, LDC Callhome, CSV and AIF 
level 0.  Future work will provide Python and 
Perl interfaces, more supported formats, a query 
language and interpreter, and a multi-channel 
transcription tool.  All software is distributed 
under an open source license, and is available 
from http://www.ldc.upenn.edu/AG/. 
Given that the annotation data can be stored 
in a relational database, it can be queried 
directly in SQL.  More convenient, a domain-
specific query language will be developed (see 
Cassidy and Bird 2000 and the work cited 
there). Query expressions will be transmitted 
over the web in the form of a CGI request, and 
translated into SQL by the annotation server. 
The resulting annotation data will be returned in 
the form of an XML document.  An example for 
the TIMIT database, using the language 
proposed by Cassidy and Bird (2000), will serve 
to illustrate: 
Find word arcs spanning a sequence of 
segments beginning with hv and containing ae: 
http://BASE-URL/cgi-bin/query? 
X.[].Y<timit/word; 
X.[:hv].[]*.[:ae].[]*.Y<-timit/ph 
Executed on the above annotation data, this 
query would return the XML document in 
Figure 4. 
Neither the query nor the returned document 
are intended for human consumption. A client-
side annotation tool will initiate queries and 
display annotation content on behalf of an end-
user.  
<?xml version="1.0"?> 
<!DOCTYPE AGSet SYSTEM "ag.dtd"> 
<AGSet id="Timit" version="1.0" xmlns="http://www.ldc.upenn.edu/atlas/ag/"  
        xmlns:xlink="http://www.w3.org/1999/xlink" 
        xmlns:dc="http://purl.org/DC/documents/rec-dces-19990702.htm"> 
<Timeline id="T1"> 
<Signal id="S1" mimeClass="audio" mimeType="wav" encoding="wav" 
        unit="16kHz" xlink:href="TIMIT/train/dr1/fjsp0/sa1.wav"/> 
</Timeline>       
<AG id="t1" type="transcription" timeline="T1"> 
<Anchor id="A3" offset="5200" unit="16kHz"/> 
<Anchor id="A6" offset="9680" unit="16kHz"/> 
<Annotation id="Ann10" type="W" start="A3" end="A6"> 
<Feature name="label">had</Feature> 
</Annotation> 
</AG> 
</AGSet> 
Figure 4: Document returned by AG query 
This annotation tool and server are integrated 
using the model shown below. A simplified 
client-server model, working at the level of 
annotation files is already available with the 
current distribution of the Annotation Graph 
Toolkit. Significantly, a networked annotation 
tool is identical to a standalone version, except 
that the AG library fetches its data from a 
remote server instead of local disk.  
 
The annotation graph formalism, annotation 
servers and the emerging query language will 
provide basic infrastructure to store, process and 
deliver essentially arbitrary amounts and types 
of signal annotations for a wide variety of 
research and teaching tasks including those 
described above. This infrastructure will enable 
reuse of existing resources and coordinated 
development of new resources both within and 
across research communities working with 
annotated linguistic datasets. 
5 Remaining Challenges to Language 
Resource Development 
We have described a process whereby 
annotated data in a variety of formats can be 
loaded into a central database server that 
interacts directly with annotation tools. The 
Annotation Graph Toolkit, version 1.0, is the 
first implementation of this architecture. As the 
toolkit undergoes future development, it will 
need to deal continually with conversion issues. 
Annotation data will continue to be created and 
manipulated by multiple tools and to be stored 
in incompatible file formats.  Data will continue 
to be mapped between different formats so that 
appropriate tools can be used, and appropriately 
managed to keep inconsistencies from arising.  
There will still be times when we need to trace 
the provenance of a particular item, back 
through a history involving several formats. 
These will always be hard problems; the 
proposed infrastructure will address them but no 
infrastructure is likely to eliminate conversion, 
integrity and provenance issues. 
Annotation graphs focus on the problems of 
dealing with time series. They do not directly 
address paradigmatic data such as lexicons and 
demographic tables. One should note however, 
that time series data and paradigmatic data can 
be united efficiently. As already mentioned, 
annotation graphs may be stored trivially in 
relational tables, technology routinely  used for 
paradigmatic data. In this way, conventional 
?joins? of relational table can convolve time-
series annotations with paradigms (e.g. texts 
with dictionaries or utterances with speaker 
demographics). 
Through judicious compromises - such as 
one-time computer-assisted conversion of 
legacy annotation data and creating once-off 
interfaces to existing useful tools - and through 
the judicious combination of simple and well-
supported formalisms and technologies as 
described above, we believe that the 
management problems can be substantially 
reduced in scale and severity. 
We can illustrate the advantages of AG with 
a example of the annotation of the Switchboard 
corpus for ?t/d deletion. Switchboard contains 
two-channel audio of thousands of 5-minute 
conversations among pairs of speakers that have 
been transcribed with the transcripts time-
aligned to the audio. A single utterance is 
written: 
 
274.35 279.50 A.119 Uh, he, 
uh, carves out different figures 
in the, in the plants, 
 
giving the start and stop time of the utterance, 
channel, speaker ID and the transcript of the 
utterance. This can be converted trivially into 
AG format as above. 
Figure 5:  Interactions among annotation tools 
and the annotation server 
The DASL tool concordances audio 
transcripts and identifies utterances in which the 
target phenomenon (eg. ?t/d deletion) may 
occur. A line of the concordance file contains 
two IDs one to identify the utterance within the 
concordance, the other to link back to the 
original corpus. The <annotate> tags identify a 
potential environment for the phenomenon 
under study.  
 
<sample id="1" senid="10194">uh 
he uh carves out <annotate> 
different figures </annotate> in 
the in the p[lants]- plants 
shrubs </sample> 
 
The link between the concordance and the 
original corpus is maintained through a table 
containing: Sentence_ID, File_ID, Start_Time, 
Stop_Time, Channel and Speaker. 
 
10194 2141 274.35  279.50 A 1139 
 
Speakers? demographic data appears in 
another table containing: Speaker_ID, Sex, Age, 
Region, Education_Level 
 
1139, MALE, 50, NORTHERN, 2 
 
The DASL interface embeds the concordance 
results in a template containing input fields for 
each parameter to be annotated (see Figure 2). 
The linguist?s annotation of the utterance can be 
stored in AG formalism as in Figure 5. Note that 
although AGs provide an elegant and general 
solution to the annotation of time series data, 
they do not remove the need to deal with the ad 
hoc formats one may encounter in various 
corpora. Nor do they remove the need to track 
the relations among elements in time-series data 
and paradigmatic material. 
6 Conclusions  
Researchers in human language share 
assumptions and needs within and across 
research communities. Each group feels an acute 
need for language resources including data, 
annotations, formats and processes. This paper 
has summarized some common needs and 
described an architecture for encoding 
annotations and delivering them via annotation 
servers using SQL or a custom query language. 
Much of the architecture discussed has already 
been created and is available in the Annotation 
Graphic Toolkit. Other components, especially 
the query language, are currently under 
development. It is hoped that tools based on 
annotations graphs and annotation servers will 
encourage greater levels of resource sharing and 
the coordination of future resource development. 
Figure 5: A sociolinguistic annotation in AG format 
<?xml version="1.0"?> 
<!DOCTYPE AGSet SYSTEM "http://www.ldc.upenn.edu/AG/doc/xml/ag.dtd"> 
<AGSet id="DASL" version="1.0" 
xmlns="http://www.ldc.upenn.edu/atlas/ag/" xmlns:xlink="http://www.w3.org/1999/xlink" 
xmlns:dc="http://purl.org/DC/documents/rec-dces-19990702.htm"> 
<Metadata></Metadata> 
<Timeline id="DASL:Timeline1"> <Signal id="DASL:Timeline1:Signal1" mimeClass="audio" 
mimeType="wav" encoding="mu-law" unit="8kHz" xlink:type="simple"  
xlink:href="LDC93S7:sw2141.wav"> 
</Signal></Timeline>  
<AG id="DASL:AG1" timeline="DASL:Timeline1"> 
<Anchor id="DASL:AG1:Anchor1" offset="274.595" signals="DASL:Timeline1:Signal1"></Anchor> 
<Anchor id="DASL:AG1:Anchor2" offset="280.671" signals="DASL:Timeline1:Signal1"></Anchor> 
<Annotation id="DASL:AG1:Annotation1" type="csv" start="DASL:AG1:Anchor1" 
 end="DASL:AG1:Anchor2"> 
<Feature name="td">Deleted</Feature> <Feature name="Morphological">Monomorpheme</Feature> 
<Feature name="EPreceding">AlveolarNasal</Feature> <Feature name="EFollowing">Obstruent</Feature> 
<Feature name="Same_Prec_Foll">N/A</Feature>  <Feature name="Stress">Unstressed</Feature> 
<Feature name="Cluster_complexity">Two_elements</Feature> 
<Feature name="Sentence_id">1</Feature> <Feature name="Corpus_name">swb</Feature>  
<Feature name="WPreceding">uh he uh carves out </Feature> 
<Feature name="WMatched">different figures</Feature>  
<Feature name="WFollowing"> in the in the p[lants]- plants shrubs</Feature> 
<Feature name="File_name">/speech/swb0/sw2141.wav</Feature>  
<Feature name="Speech_channel">1</Feature> <Feature name="Speaker_id">1139</Feature> 
<Feature name="Sex">MALE</Feature>   <Feature name="Birth_year">1956</Feature> 
<Feature name="Dialect">NORTHERN</Feature> <Feature name="Edu">2</Feature>  
</Annotation></AG></AGSet> 
References 
James Allan, et. al., (1999) Topic Based Novelty 
Detection 1999 Summer Workshop at CLSP Final 
Report, http://www.clsp.jhu.edu/ws99/final/Topic-
based.pdf 
Steven Bird & Mark Liberman (2001) A Formal 
Framework for Linguistic Annotation, Speech 
Communication 33(1,2) pp 23-60, 
http://arxiv.org/abs/cs/0010033 
Steven Bird, Peter Buneman & Wang-Chiew Tan 
(2000) Towards a query language for annotation 
graphs, Proceedings of the Second International 
Conference on Language Resources and 
Evaluation, pp. 807-814.  
Steve Cassidy & Steven Bird (2000) Querying 
databases of annotated speech, Proceedings of the 
Eleventh Australasian Database Conference, 
http://www.ldc.upenn.edu/Papers/ADC2000/adc0
0.pdf 
Center for Language and Speech Processing  (2000) 
Summer Workshop Pages, 
http://www.clsp.jhu.edu/workshops/. 
Chalapati, Neti, et. al. (2000) Audio Visual Speech 
Recognition, Summer Workshop at CLSP Final 
Report, 
http://www.clsp.jhu.edu/workshops/ws2000/final_
reports/avsr/. 
Lea Christiansen, Christopher Cieri, Kathleen Egan, 
Anita Kulman, Milton Paul (2001) Getting 
SMART about Authoring, Presented at CALICO 
2001: Computer Aided Language Instruction 
Conference, Orlando, University of Central 
Florida. 
Christopher Cieri and Stephanie Strassel (2001) 
DASL Project Pages, 
http://www.ldc.upenn.edu/Projects/DASL. 
Deckert & Yaeger-Dror (2000) Dialect variation in 
negation strategies in the LDC Switchboard 
corpus Corpus Linguistics Conference 2, Boston 2, 
pp. 49-59. 
Garofalo, John, Cedric Auzanne and Ellen Voorhees 
(2000) The TREC Spoken Document Retrieval 
Track: A Success Story, 
http://www.nist.gov/speech/tests/sdr/sdr2000/pape
rs/01plenary1.pdf 
Garofalo, John S., Lori F. Lamel, William M. Fisher, 
Jonathan G. Fiscus, David S. Pallett, and Nancy L. 
Dahlgren, "The DARPA TIMIT Acoustic-
Phonetic Continuous Speech Corpus CDROM" 
(printed documentation; available on request from 
the LDC). 
David Graff, Steven Bird (2000) Many uses, many 
annotations for large speech corpora, Proceedings 
of the Second 2nd Language Resources and 
Evaluation Conference, Athens, Greece, pp. 427-
433. 
http://www.ldc.upenn.edu/Papers/LREC2000/mult
iuse.pdf. 
Kazuaki Maeda and Steven Bird (2000) A Formal 
Framework for Interlinear Text, Web-Based 
Language Documentation and Description 
Workshop, University of Pennsylvania, 
Philadelphia, December 2000 
http;//www.ldc.upenn.edu/exploration/expl2000/. 
Kazuaki Maeda and Steven Bird (2001), Annotation 
Tools Based on the Annotation Graph API, 
Proceedings of this workshop. 
Meng, Helen, et. al., (2000) Mandarin English 
Information (MEI) : Investigation Translingual 
Speech Retrieval, 1999 Summer Workshop at 
CLSP Final Report 
http://www.clsp.jhu.edu/ws2000/final_reports/mei
/ws00mei.pdf 
David Miller and Kevin Walker (2001) Telephone 
Speech in the Foreign Language Classroom: 
Applications Methods and Technology, Presented 
at CALICO 2001: Computer Aided Language 
Instruction Conference, Orlando, University of 
Central Florida. 
Przybocki, Mark (2000) Automatic Content 
Extraction Web Page, 
http://www.nist.gov/speech/tests/ace/ 
Annotation Tools Based on the Annotation Graph API
Steven Bird, Kazuaki Maeda, Xiaoyi Ma and Haejoong Lee
Linguistic Data Consortium, University of Pennsylvania
3615 Market Street, Suite 200, Philadelphia, PA 19104-2608, USA
fsb,maeda,xma,haejoongg@ldc.upenn.edu
Abstract
Annotation graphs provide an efficient
and expressive data model for linguistic
annotations of time-series data. This
paper reports progress on a complete
open-source software infrastructure
supporting the rapid development of
tools for transcribing and annotating
time-series data. This general-
purpose infrastructure uses annotation
graphs as the underlying model, and
allows developers to quickly create
special-purpose annotation tools using
common components. An application
programming interface, an I/O library,
and graphical user interfaces are
described. Our experience has shown
us that it is a straightforward task to
create new special-purpose annotation
tools based on this general-purpose
infrastructure.
1 Introduction
In the past, standardized file formats and coding
practices have greatly facilitated data sharing and
software reuse. Yet it has so far proved impossible
to work out universally agreed formats and codes
for linguistic annotation. We contend that this is a
vain hope, and that the interests of sharing and
reuse are better served by agreeing on the data
models and interfaces.
Annotation graphs (AGs) provide an efficient
and expressive data model for linguistic anno-
tations of time-series data (Bird and Liberman,
Figure 1: Architecture for Annotation Systems
2001). Recently, the LDC has been develop-
ing a complete software infrastructure supporting
the rapid development of tools for transcribing
and annotating time-series data, in cooperation
with NIST and MITRE as part of the ATLAS
project, and with the developers of other widely
used annotation systems, Transcriber and Emu
(Bird et al, 2000; Barras et al, 2001; Cassidy and
Harrington, 2001).
The infrastructure is being used in the devel-
opment of a series of annotation tools at the Lin-
guistic Data Consortium. Two tools are shown in
the paper: one for dialogue annotation and one
for interlinear transcription. In both cases, the
transcriptions are time-aligned to a digital audio
signal.
This paper will cover the following points: the
application programming interfaces for manipu-
lating annotation graph data and importing data
from other formats; the model of inter-component
communication which permits easy reuse of soft-
ware components; and the design of the graphical
user interfaces.
2 Architecture
2.1 General architecture
Figure 1 shows the architecture of the tools
currently being developed. Annotation tools,
such as the ones discussed below, must provide
graphical user interface components for signal
visualization and annotation. The communication
between components is handled through an
extensible event language. An application
programming interface for annotation graphs
has been developed to support well-formed
operations on annotation graphs. This permits
applications to abstract away from file format
issues, and deal with annotations purely at the
logical level.
2.2 The annotation graph API
The application programming interface provides
access to internal objects (signals, anchors,
annotations etc) using identifiers, represented
as formatted strings. For example, an AG
identifier is qualified with an AGSet identifier:
AGSetId:AGId. Annotations and anchors are
doubly qualified: AGSetId:AGId:AnnotationId,
AGSetId:AGId:AnchorId. Thus, the identifier
encodes the unique membership of an object in
the containing objects.
We demonstrate the behavior of the API with
a series of simple examples. Suppose we have
already constructed an AG and now wish to create
a new anchor. We might have the following API
call:
CreateAnchor("agSet12:ag5", 15.234, "sec");
This call would construct a new anchor object
and return its identifier: agSet12:ag5:anchor34.
Alternatively, if we already have an anchor iden-
tifier that we wish to use for the new anchor (e.g.
because we are reading previously created anno-
tation data from a file and do not wish to assign
new identifiers), then we could have the following
API call:
CreateAnchor("agset12:ag5:anchor34",
15.234, "sec");
This call will return agset12:ag5:anchor34.
Once a pair of anchors have been created it
is possible to create an annotation which spans
them:
CreateAnnotation("agSet12:ag5",
"agSet12:ag5:anchor34",
"agSet12:ag5:anchor35",
"phonetic" );
This call will construct an annotation
object and return an identifier for it, e.g.
agSet12:ag5:annotation41. We can now add
features to this annotation:
SetFeature("agSet12:ag5:annotation41",
"date", "1999-07-02" );
The implementation maintains indexes on all
the features, and also on the temporal information
and graph structure, permitting efficient search
using a family of functions such as:
GetAnnotationSetByFeature(
"agSet12:ag5", "date", "1999-07-02");
2.3 A file I/O library
A file I/O library (AG-FIO) supports input and
output of AG data to existing formats. Formats
currently supported by the AG-FIO library
include the TIMIT, BU, Treebank, AIF (ATLAS
Interchange Format), Switchboard and BAS
Partitur formats. In time, the library will handle
all widely-used signal annotation formats.
2.4 Inter-component communication
Figure 2 shows the structure of an annotation tool
in terms of components and their communication.
The main program is typically a small script
which sets up the widgets and provides callback
functions to handle widget events. In this
example there are four other components which
Main program - a small script
Waveform
display
Transcription
editor
Internal
representation
File input
/ output
AG-GUI-API
AG-GUI-API AG-API
AG-FIO-API
Figure 2: The Structure of an Annotation Tool
Main program
Waveform display AG-API Transcription editor
User types Control-G Update Display
SetRegion t1 t2 AG::SetAnchorOffset SetRegion t1 t2
Update
Internal Representation
Figure 3: Inter-component Communication
are reused by several annotation tools. The AG
and AG-FIO components have already been
described. The waveform display component (of
which there may be multiple instances) receives
instructions to pan and zoom, to play a segment
of audio data, and so on. The transcription
editor is an annotation component which is
specialized for a particular coding task. Most tool
customization is accomplished by substituting for
this component.
Both GUI components and the main program
support a common API for transmitting and
receiving events. For example, GUI components
have a notion of a ?current region? ? the
timespan which is currently in focus. A
waveform component can change an annotation
component?s idea of the current region by
sending a SetRegion event (Figure 3). The
same event can also be used in the reverse
direction. The main program routes the events
between GUI components, calling the annotation
graph API to update the internal representation as
needed. With this communication mechanism, it
is straightforward to add new commands, specific
to the annotation task.
2.5 Reuse of software components
The architecture described in this paper allows
rapid development of special-purpose annotation
tools using common components. In particular,
our model of inter-component communication
facilitates reuse of software components.
The annotation tools described in the next
section are not intended for general purpose
annotation/transcription tasks; the goal is not
to create an ?emacs for linguistic annotation?.
Instead, they are special-purpose tools based on
the general purpose infrastructure. These GUI
Figure 4: Dialogue Annotation Tool for the
TRAINS/DAMSL Corpus
components can be modified or replaced when
building new special-purpose tools.
3 Graphical User Interfaces
3.1 A spreadsheet component
Dialogue annotation typically consists of assign-
ing a field-structured record to each utterance in
each speaker turn. A key challenge is to handle
overlapping turns and back-channel cues without
disrupting the structure of individual speaker con-
tributions. The tool side-steps these problems by
permitting utterances to be independently aligned
to a (multi-channel) recording. The records are
displayed in a spreadsheet; clicking on a row of
the spreadsheet causes the corresponding extent
of audio signal to be highlighted. As an extended
recording is played back, annotated sections are
highlighted, in both the waveform and spread-
sheet displays.
Figure 4 shows the tool with a section of the
TRAINS/DAMSL corpus (Jurafsky et al, 1997).
Note that the highlighted segment in the audio
channel corresponds to the highlighted annotation
in the spreadsheet.
3.2 An interlinear transcription component
Interlinear text is a kind of text in which
each word is annotated with phonological,
morphological and syntactic information
(displayed under the word) and each sentence
is annotated with a free translation. Our tool
Figure 5: Interlinear Transcription Tool
permits interlinear transcription aligned to a
primary audio signal, for greater accuracy and
accountability. Whole words and sub-parts of
words can be easily aligned with the audio.
Clicking on a piece of the annotation causes
the corresponding extent of audio signal to be
highlighted. As an extended recording is played
back, annotated sections are highlighted (both
waveform and interlinear text displays).
The screenshot in Figure 5 shows the tool with
some interlinear text from Mawu (a Manding lan-
guage of the Ivory Coast, West Africa).
3.3 A waveform display component
The tools described above utilize WaveSurfer
and Snack (Sjo?lander, 2000; Sjo?lander and
Beskow, 2000). We have developed a plug-in
for WaveSurfer to support the inter-component
communication described in this paper.
4 Available Software and Future Work
The Annotation Graph Toolkit, version 1.0, con-
tains a complete implementation of the annota-
tion graph model, import filters for several for-
mats, loading/storing data to an annotation server
(MySQL), application programming interfaces in
C++ and Tcl/tk, and example annotation tools for
dialogue, ethology and interlinear text. The sup-
ported formats are: xlabel, TIMIT, BAS Parti-
tur, Penn Treebank, Switchboard, LDC Callhome,
CSV and AIF level 0. All software is distributed
under an open source license, and is available
from http://www.ldc.upenn.edu/AG/.
Future work will provide Python and Perl inter-
faces, more supported formats, a query language
and interpreter, a multichannel transcription tool,
and a client/server model.
5 Conclusion
This paper has described a comprehensive infras-
tructure for developing annotation tools based on
annotation graphs. Our experience has shown us
that it is a simple matter to construct new special-
purpose annotation tools using high-level soft-
ware components. The tools can be quickly cre-
ated and deployed, and replaced by new versions
as annotation tasks evolve.
Acknowledgements
This material is based upon work supported by the
National Science Foundation under Grant Nos.
9978056, 9980009, and 9983258.
References
Claude Barras, Edouard Geoffrois, Zhibiao Wu, and Mark
Liberman. 2001. Transcriber: development and use of a
tool for assisting speech corpora production. Speech
Communication, 33:5?22.
Steven Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech
Communication, 33:23?60.
Steven Bird, David Day, John Garofolo, John Henderson,
Chris Laprun, and Mark Liberman. 2000. ATLAS: A
flexible and extensible architecture for linguistic annotation.
In Proceedings of the Second International Conference on
Language Resources and Evaluation. Paris: European
Language Resources Association.
Steve Cassidy and Jonathan Harrington. 2001. Multi-level
annotation of speech: An overview of the emu speech
database management system. Speech Communication,
33:61?77.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL Labeling Project
Coder?s Manual, Draft 13. Technical Report 97-02,
University of Colorado Institute of Cognitive Science.
[stripe.colorado.edu/?jurafsky/manual.august1.html].
Ka?re Sjo?lander and Jonas Beskow. 2000. Wavesurfer ? an
open source speech tool. In Proceedings of the 6th
International Conference on Spoken Language Processing.
http://www.speech.kth.se/wavesurfer/.
Ka?re Sjo?lander. 2000. The Snack sound toolkit.
http://www.speech.kth.se/snack/.
NLTK: The Natural Language Toolkit
Edward Loper and Steven Bird
Department of Computer and Information Science
University of Pennsylvania, Philadelphia, PA 19104-6389, USA
Abstract
NLTK, the Natural Language Toolkit,
is a suite of open source program
modules, tutorials and problem sets,
providing ready-to-use computational
linguistics courseware. NLTK covers
symbolic and statistical natural lan-
guage processing, and is interfaced to
annotated corpora. Students augment
and replace existing components, learn
structured programming by example,
and manipulate sophisticated models
from the outset.
1 Introduction
Teachers of introductory courses on compu-
tational linguistics are often faced with the
challenge of setting up a practical programming
component for student assignments and
projects. This is a difficult task because
different computational linguistics domains
require a variety of different data structures
and functions, and because a diverse range of
topics may need to be included in the syllabus.
A widespread practice is to employ multiple
programming languages, where each language
provides native data structures and functions
that are a good fit for the task at hand. For
example, a course might use Prolog for pars-
ing, Perl for corpus processing, and a finite-state
toolkit for morphological analysis. By relying
on the built-in features of various languages, the
teacher avoids having to develop a lot of software
infrastructure.
An unfortunate consequence is that a
significant part of such courses must be devoted
to teaching programming languages. Further,
many interesting projects span a variety of
domains, and would require that multiple
languages be bridged. For example, a student
project that involved syntactic parsing of corpus
data from a morphologically rich language might
involve all three of the languages mentioned
above: Perl for string processing; a finite state
toolkit for morphological analysis; and Prolog
for parsing. It is clear that these considerable
overheads and shortcomings warrant a fresh
approach.
Apart from the practical component, compu-
tational linguistics courses may also depend on
software for in-class demonstrations. This con-
text calls for highly interactive graphical user
interfaces, making it possible to view program
state (e.g. the chart of a chart parser), observe
program execution step-by-step (e.g. execu-
tion of a finite-state machine), and even make
minor modifications to programs in response to
?what if? questions from the class. Because
of these difficulties it is common to avoid live
demonstrations, and keep classes for theoreti-
cal presentations only. Apart from being dull,
this approach leaves students to solve important
practical problems on their own, or to deal with
them less efficiently in office hours.
In this paper we introduce a new approach to
the above challenges, a streamlined and flexible
way of organizing the practical component
of an introductory computational linguistics
course. We describe NLTK, the Natural
Language Toolkit, which we have developed in
conjunction with a course we have taught at
the University of Pennsylvania.
The Natural Language Toolkit is avail-
able under an open source license from
http://nltk.sf.net/. NLTK runs on all
platforms supported by Python, including
Windows, OS X, Linux, and Unix.
2 Choice of Programming Language
The most basic step in setting up a practical
component is choosing a suitable programming
language. A number of considerations
influenced our choice. First, the language must
have a shallow learning curve, so that novice
programmers get immediate rewards for their
efforts. Second, the language must support
rapid prototyping and a short develop/test
cycle; an obligatory compilation step is a
serious detraction. Third, the code should be
self-documenting, with a transparent syntax and
semantics. Fourth, it should be easy to write
structured programs, ideally object-oriented but
without the burden associated with languages
like C++. Finally, the language must have
an easy-to-use graphics library to support the
development of graphical user interfaces.
In surveying the available languages, we
believe that Python offers an especially good
fit to the above requirements. Python is an
object-oriented scripting language developed
by Guido van Rossum and available on all
platforms (www.python.org). Python offers
a shallow learning curve; it was designed to
be easily learnt by children (van Rossum,
1999). As an interpreted language, Python is
suitable for rapid prototyping. Python code is
exceptionally readable, and it has been praised
as ?executable pseudocode.? Python is an
object-oriented language, but not punitively
so, and it is easy to encapsulate data and
methods inside Python classes. Finally, Python
has an interface to the Tk graphics toolkit
(Lundh, 1999), and writing graphical interfaces
is straightforward.
3 Design Criteria
Several criteria were considered in the design
and implementation of the toolkit. These design
criteria are listed in the order of their impor-
tance. It was also important to decide what
goals the toolkit would not attempt to accom-
plish; we therefore include an explicit set of non-
requirements, which the toolkit is not expected
to satisfy.
3.1 Requirements
Ease of Use. The primary purpose of the
toolkit is to allow students to concentrate on
building natural language processing (NLP) sys-
tems. The more time students must spend learn-
ing to use the toolkit, the less useful it is.
Consistency. The toolkit should use consis-
tent data structures and interfaces.
Extensibility. The toolkit should easily
accommodate new components, whether those
components replicate or extend the toolkit?s
existing functionality. The toolkit should
be structured in such a way that it is obvious
where new extensions would fit into the toolkit?s
infrastructure.
Documentation. The toolkit, its data
structures, and its implementation all need to
be carefully and thoroughly documented. All
nomenclature must be carefully chosen and
consistently used.
Simplicity. The toolkit should structure the
complexities of building NLP systems, not hide
them. Therefore, each class defined by the
toolkit should be simple enough that a student
could implement it by the time they finish an
introductory course in computational linguis-
tics.
Modularity. The interaction between differ-
ent components of the toolkit should be kept
to a minimum, using simple, well-defined inter-
faces. In particular, it should be possible to
complete individual projects using small parts
of the toolkit, without worrying about how they
interact with the rest of the toolkit. This allows
students to learn how to use the toolkit incre-
mentally throughout a course. Modularity also
makes it easier to change and extend the toolkit.
3.2 Non-Requirements
Comprehensiveness. The toolkit is not
intended to provide a comprehensive set of
tools. Indeed, there should be a wide variety of
ways in which students can extend the toolkit.
Efficiency. The toolkit does not need to
be highly optimized for runtime performance.
However, it should be efficient enough that
students can use their NLP systems to perform
real tasks.
Cleverness. Clear designs and implementa-
tions are far preferable to ingenious yet inde-
cipherable ones.
4 Modules
The toolkit is implemented as a collection of
independent modules, each of which defines a
specific data structure or task.
A set of core modules defines basic data
types and processing systems that are used
throughout the toolkit. The token module
provides basic classes for processing individual
elements of text, such as words or sentences.
The tree module defines data structures for
representing tree structures over text, such
as syntax trees and morphological trees. The
probability module implements classes that
encode frequency distributions and probability
distributions, including a variety of statistical
smoothing techniques.
The remaining modules define data structures
and interfaces for performing specific NLP tasks.
This list of modules will grow over time, as we
add new tasks and algorithms to the toolkit.
Parsing Modules
The parser module defines a high-level inter-
face for producing trees that represent the struc-
tures of texts. The chunkparser module defines
a sub-interface for parsers that identify non-
overlapping linguistic groups (such as base noun
phrases) in unrestricted text.
Four modules provide implementations
for these abstract interfaces. The srparser
module implements a simple shift-reduce
parser. The chartparser module defines a
flexible parser that uses a chart to record
hypotheses about syntactic constituents. The
pcfgparser module provides a variety of
different parsers for probabilistic grammars.
And the rechunkparser module defines a
transformational regular-expression based
implementation of the chunk parser interface.
Tagging Modules
The tagger module defines a standard interface
for augmenting each token of a text with supple-
mentary information, such as its part of speech
or its WordNet synset tag; and provides several
different implementations for this interface.
Finite State Automata
The fsa module defines a data type for encod-
ing finite state automata; and an interface for
creating automata from regular expressions.
Type Checking
Debugging time is an important factor in the
toolkit?s ease of use. To reduce the amount of
time students must spend debugging their code,
we provide a type checking module, which can
be used to ensure that functions are given valid
arguments. The type checking module is used
by all of the basic data types and processing
classes.
Since type checking is done explicitly, it can
slow the toolkit down. However, when efficiency
is an issue, type checking can be easily turned
off; and with type checking is disabled, there is
no performance penalty.
Visualization
Visualization modules define graphical
interfaces for viewing and manipulating
data structures, and graphical tools for
experimenting with NLP tasks. The draw.tree
module provides a simple graphical inter-
face for displaying tree structures. The
draw.tree edit module provides an interface
for building and modifying tree structures.
The draw.plot graph module can be used to
graph mathematical functions. The draw.fsa
module provides a graphical tool for displaying
and simulating finite state automata. The
draw.chart module provides an interactive
graphical tool for experimenting with chart
parsers.
The visualization modules provide interfaces
for interaction and experimentation; they do
not directly implement NLP data structures or
tasks. Simplicity of implementation is therefore
less of an issue for the visualization modules
than it is for the rest of the toolkit.
Text Classification
The classifier module defines a standard
interface for classifying texts into categories.
This interface is currently implemented by two
modules. The classifier.naivebayes module
defines a text classifier based on the Naive Bayes
assumption. The classifier.maxent module
defines the maximum entropy model for text
classification, and implements two algorithms
for training the model: Generalized Iterative
Scaling and Improved Iterative Scaling.
The classifier.feature module provides
a standard encoding for the information that
is used to make decisions for a particular
classification task. This standard encoding
allows students to experiment with the
differences between different text classification
algorithms, using identical feature sets.
The classifier.featureselection module
defines a standard interface for choosing which
features are relevant for a particular classifica-
tion task. Good feature selection can signifi-
cantly improve classification performance.
5 Documentation
The toolkit is accompanied by extensive
documentation that explains the toolkit, and
describes how to use and extend it. This
documentation is divided into three primary
categories:
Tutorials teach students how to use the
toolkit, in the context of performing specific
tasks. Each tutorial focuses on a single domain,
such as tagging, probabilistic systems, or text
classification. The tutorials include a high-level
discussion that explains and motivates the
domain, followed by a detailed walk-through
that uses examples to show how NLTK can be
used to perform specific tasks.
Reference Documentation provides precise
definitions for every module, interface, class,
method, function, and variable in the toolkit. It
is automatically extracted from docstring com-
ments in the Python source code, using Epydoc
(Loper, 2002).
Technical Reports explain and justify the
toolkit?s design and implementation. They are
used by the developers of the toolkit to guide
and document the toolkit?s construction. Stu-
dents can also consult these reports if they would
like further information about how the toolkit is
designed, and why it is designed that way.
6 Uses of NLTK
6.1 Assignments
NLTK can be used to create student assign-
ments of varying difficulty and scope. In the
simplest assignments, students experiment with
an existing module. The wide variety of existing
modules provide many opportunities for creat-
ing these simple assignments. Once students
become more familiar with the toolkit, they can
be asked to make minor changes or extensions to
an existing module. A more challenging task is
to develop a new module. Here, NLTK provides
some useful starting points: predefined inter-
faces and data structures, and existing modules
that implement the same interface.
Example: Chunk Parsing
As an example of a moderately difficult
assignment, we asked students to construct
a chunk parser that correctly identifies base
noun phrase chunks in a given text, by
defining a cascade of transformational chunking
rules. The NLTK rechunkparser module
provides a variety of regular-expression
based rule types, which the students can
instantiate to construct complete rules.
For example, ChunkRule(?<NN.*>?) builds
chunks from sequences of consecutive nouns;
ChinkRule(?<VB.>?) excises verbs from
existing chunks; SplitRule(?<NN>?, ?<DT>?)
splits any existing chunk that contains a
singular noun followed by determiner into
two pieces; and MergeRule(?<JJ>?, ?<JJ>?)
combines two adjacent chunks where the first
chunk ends and the second chunk starts with
adjectives.
The chunking tutorial motivates chunk pars-
ing, describes each rule type, and provides all
the necessary code for the assignment. The pro-
vided code is responsible for loading the chun-
ked, part-of-speech tagged text using an existing
tokenizer, creating an unchunked version of the
text, applying the chunk rules to the unchunked
text, and scoring the result. Students focus on
the NLP task only ? providing a rule set with
the best coverage.
In the remainder of this section we reproduce
some of the cascades created by the students.
The first example illustrates a combination of
several rule types:
cascade = [
ChunkRule(?<DT><NN.*><VB.><NN.*>?),
ChunkRule(?<DT><VB.><NN.*>?),
ChunkRule(?<.*>?),
UnChunkRule(?<IN|VB.*|CC|MD|RB.*>?),
UnChunkRule("<,|\\.|??|??>"),
MergeRule(?<NN.*|DT|JJ.*|CD>?,
?<NN.*|DT|JJ.*|CD>?),
SplitRule(?<NN.*>?, ?<DT|JJ>?)
]
The next example illustrates a brute-force sta-
tistical approach. The student calculated how
often each part-of-speech tag was included in
a noun phrase. They then constructed chunks
from any sequence of tags that occurred in a
noun phrase more than 50% of the time.
cascade = [
ChunkRule(?<\\$|CD|DT|EX|PDT
|PRP.*|WP.*|\\#|FW
|JJ.*|NN.*|POS|RBS|WDT>*?)
]
In the third example, the student constructed
a single chunk containing the entire text, and
then excised all elements that did not belong.
cascade = [
ChunkRule(?<.*>+?)
ChinkRule(?<VB.*|IN|CC|R.*|MD|WRB|TO|.|,>+?)
]
6.2 Class demonstrations
NLTK provides graphical tools that can be used
in class demonstrations to help explain basic
NLP concepts and algorithms. These interactive
tools can be used to display relevant data struc-
tures and to show the step-by-step execution of
algorithms. Both data structures and control
flow can be easily modified during the demon-
stration, in response to questions from the class.
Since these graphical tools are included with
the toolkit, they can also be used by students.
This allows students to experiment at home with
the algorithms that they have seen presented in
class.
Example: The Chart Parsing Tool
The chart parsing tool is an example of a
graphical tool provided by NLTK. This tool can
be used to explain the basic concepts behind
chart parsing, and to show how the algorithm
works. Chart parsing is a flexible parsing algo-
rithm that uses a data structure called a chart to
record hypotheses about syntactic constituents.
Each hypothesis is represented by a single edge
on the chart. A set of rules determine when new
edges can be added to the chart. This set of rules
controls the overall behavior of the parser (e.g.,
whether it parses top-down or bottom-up).
The chart parsing tool demonstrates the pro-
cess of parsing a single sentence, with a given
grammar and lexicon. Its display is divided into
three sections: the bottom section displays the
chart; the middle section displays the sentence;
and the top section displays the partial syntax
tree corresponding to the selected edge. But-
tons along the bottom of the window are used
to control the execution of the algorithm. The
main display window for the chart parsing tool
is shown in Figure 1.
This tool can be used to explain several dif-
ferent aspects of chart parsing. First, it can be
used to explain the basic chart data structure,
and to show how edges can represent hypothe-
ses about syntactic constituents. It can then
be used to demonstrate and explain the indi-
vidual rules that the chart parser uses to create
new edges. Finally, it can be used to show how
Figure 1: Chart Parsing Tool
these individual rules combine to find a complete
parse for a given sentence.
To reduce the overhead of setting up demon-
strations during lecture, the user can define a
list of preset charts. The tool can then be reset
to any one of these charts at any time.
The chart parsing tool allows for flexible con-
trol of the parsing algorithm. At each step of
the algorithm, the user can select which rule or
strategy they wish to apply. This allows the user
to experiment with mixing different strategies
(e.g., top-down and bottom-up). The user can
exercise fine-grained control over the algorithm
by selecting which edge they wish to apply a rule
to. This flexibility allows lecturers to use the
tool to respond to a wide variety of questions;
and allows students to experiment with different
variations on the chart parsing algorithm.
6.3 Advanced Projects
NLTK provides students with a flexible frame-
work for advanced projects. Typical projects
involve the development of entirely new func-
tionality for a previously unsupported NLP task,
or the development of a complete system out of
existing and new modules.
The toolkit?s broad coverage allows students
to explore a wide variety of topics. In our intro-
ductory computational linguistics course, topics
for student projects included text generation,
word sense disambiguation, collocation analysis,
and morphological analysis.
NLTK eliminates the tedious infrastructure-
building that is typically associated with
advanced student projects by providing
students with the basic data structures, tools,
and interfaces that they need. This allows the
students to concentrate on the problems that
interest them.
The collaborative, open-source nature of the
toolkit can provide students with a sense that
their projects are meaningful contributions, and
not just exercises. Several of the students in our
course have expressed interest in incorporating
their projects into the toolkit.
Finally, many of the modules included in the
toolkit provide students with good examples
of what projects should look like, with well
thought-out interfaces, clean code structure, and
thorough documentation.
Example: Probabilistic Parsing
The probabilistic parsing module was created
as a class project for a statistical NLP course.
The toolkit provided the basic data types and
interfaces for parsing. The project extended
these, adding a new probabilistic parsing inter-
face, and using subclasses to create a prob-
abilistic version of the context free grammar
data structure. These new components were
used in conjunction with several existing compo-
nents, such as the chart data structure, to define
two implementations of the probabilistic parsing
interface. Finally, a tutorial was written that
explained the basic motivations and concepts
behind probabilistic parsing, and described the
new interfaces, data structures, and parsers.
7 Evaluation
We used NLTK as a basis for the assignments
and student projects in CIS-530, an introduc-
tory computational linguistics class taught at
the University of Pennsylvania. CIS-530 is a
graduate level class, although some advanced
undergraduates were also enrolled. Most stu-
dents had a background in either computer sci-
ence or linguistics (and occasionally both). Stu-
dents were required to complete five assign-
ments, two exams, and a final project. All class
materials are available from the course website
http://www.cis.upenn.edu/~cis530/.
The experience of using NLTK was very pos-
itive, both for us and for the students. The
students liked the fact that they could do inter-
esting projects from the outset. They also liked
being able to run everything on their computer
at home. The students found the extensive doc-
umentation very helpful for learning to use the
toolkit. They found the interfaces defined by
NLTK intuitive, and appreciated the ease with
which they could combine different components
to create complete NLP systems.
We did encounter a few difficulties during the
semester. One problem was finding large clean
corpora that the students could use for their
assignments. Several of the students needed
assistance finding suitable corpora for their
final projects. Another issue was the fact that
we were actively developing NLTK during the
semester; some modules were only completed
one or two weeks before the students used
them. As a result, students who worked at
home needed to download new versions of the
toolkit several times throughout the semester.
Luckily, Python has extensive support for
installation scripts, which made these upgrades
simple. The students encountered a couple of
bugs in the toolkit, but none were serious, and
all were quickly corrected.
8 Other Approaches
The computational component of computational
linguistics courses takes many forms. In this sec-
tion we briefly review a selection of approaches,
classified according to the (original) target audi-
ence.
Linguistics Students. Various books intro-
duce programming or computing to linguists.
These are elementary on the computational side,
providing a gentle introduction to students hav-
ing no prior experience in computer science.
Examples of such books are: Using Computers
in Linguistics (Lawler and Dry, 1998), and Pro-
gramming for Linguistics: Java Technology for
Language Researchers (Hammond, 2002).
Grammar Developers. Infrastructure
for grammar development has a long history
in unification-based (or constraint-based)
grammar frameworks, from DCG (Pereira
and Warren, 1980) to HPSG (Pollard and
Sag, 1994). Recent work includes (Copestake,
2000; Baldridge et al, 2002a). A concurrent
development has been the finite state toolkits,
such as the Xerox toolkit (Beesley and
Karttunen, 2002). This work has found
widespread pedagogical application.
Other Researchers and Developers.
A variety of toolkits have been created for
research or R&D purposes. Examples include
the CMU-Cambridge Statistical Language
Modeling Toolkit (Clarkson and Rosenfeld,
1997), the EMU Speech Database System
(Harrington and Cassidy, 1999), the General
Architecture for Text Engineering (Bontcheva
et al, 2002), the Maxent Package for Maximum
Entropy Models (Baldridge et al, 2002b), and
the Annotation Graph Toolkit (Maeda et al,
2002). Although not originally motivated by
pedagogical needs, all of these toolkits have
pedagogical applications and many have already
been used in teaching.
9 Conclusions and Future Work
NLTK provides a simple, extensible, uniform
framework for assignments, projects, and class
demonstrations. It is well documented, easy to
learn, and simple to use. We hope that NLTK
will allow computational linguistics classes to
include more hands-on experience with using
and building NLP components and systems.
NLTK is unique in its combination of three
factors. First, it was deliberately designed as
courseware and gives pedagogical goals primary
status. Second, its target audience consists of
both linguists and computer scientists, and it
is accessible and challenging at many levels of
prior computational skill. Finally, it is based on
an object-oriented scripting language support-
ing rapid prototyping and literate programming.
We plan to continue extending the breadth
of materials covered by the toolkit. We are
currently working on NLTK modules for Hidden
Markov Models, language modeling, and tree
adjoining grammars. We also plan to increase
the number of algorithms implemented by some
existing modules, such as the text classification
module.
Finding suitable corpora is a prerequisite for
many student assignments and projects. We are
therefore putting together a collection of corpora
containing data appropriate for every module
defined by the toolkit.
NLTK is an open source project, and we wel-
come any contributions. Readers who are inter-
ested in contributing to NLTK, or who have
suggestions for improvements, are encouraged to
contact the authors.
10 Acknowledgments
We are indebted to our students for feedback
on the toolkit, and to anonymous reviewers, Jee
Bang, and the workshop organizers for com-
ments on an earlier version of this paper. We are
grateful to Mitch Marcus and the Department of
Computer and Information Science at the Uni-
versity of Pennsylvania for sponsoring the work
reported here.
References
Jason Baldridge, John Dowding, and Susana Early.
2002a. Leo: an architecture for sharing resources
for unification-based grammars. In Proceedings
of the Third Language Resources and Evaluation
Conference. Paris: European Language Resources
Association.
http://www.iccs.informatics.ed.ac.uk/
~jmb/leo-lrec.ps.gz.
Jason Baldridge, Thomas Morton, and Gann
Bierner. 2002b. The MaxEnt project.
http://maxent.sourceforge.net/.
Kenneth R. Beesley and Lauri Karttunen. 2002.
Finite-State Morphology: Xerox Tools and Tech-
niques. Studies in Natural Language Processing.
Cambridge University Press.
Kalina Bontcheva, Hamish Cunningham, Valentin
Tablan, Diana Maynard, and Oana Hamza. 2002.
Using GATE as an environment for teaching NLP.
In Proceedings of the ACL Workshop on Effective
Tools and Methodologies for Teaching NLP and
CL. Somerset, NJ: Association for Computational
Linguistics.
Philip R. Clarkson and Ronald Rosenfeld.
1997. Statistical language modeling using
the CMU-Cambridge Toolkit. In Proceedings
of the 5th European Conference on Speech
Communication and Technology (EUROSPEECH
?97). http://svr-www.eng.cam.ac.uk/~prc14/
eurospeech97.ps.
Ann Copestake. 2000. The (new) LKB system.
http://www-csli.stanford.edu/~aac/doc5-2.
pdf.
Michael Hammond. 2002. Programming for Linguis-
tics: Java Technology for Language Researchers.
Oxford: Blackwell. In press.
Jonathan Harrington and Steve Cassidy. 1999. Tech-
niques in Speech Acoustics. Kluwer.
John M. Lawler and Helen Aristar Dry, editors.
1998. Using Computers in Linguistics. London:
Routledge.
Edward Loper. 2002. Epydoc.
http://epydoc.sourceforge.net/.
Fredrik Lundh. 1999. An introduction to tkinter.
http://www.pythonware.com/library/
tkinter/introduction/index.htm.
Kazuaki Maeda, Steven Bird, Xiaoyi Ma, and Hae-
joong Lee. 2002. Creating annotation tools with
the annotation graph toolkit. In Proceedings of
the Third International Conference on Language
Resources and Evaluation. http://arXiv.org/
abs/cs/0204005.
Fernando C. N. Pereira and David H. D. Warren.
1980. Definite clause grammars for language anal-
ysis ? a survey of the formalism and a comparison
with augmented transition grammars. Artificial
Intelligence, 13:231?78.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Chicago University
Press.
Guido van Rossum. 1999. Computer program-
ming for everybody. Technical report, Corpo-
ration for National Research Initiatives. http:
//www.python.org/doc/essays/cp4e.html.
Last Words
Natural Language Processing and
Linguistic Fieldwork
Steven Bird?
University of Melbourne
March 2009 marked an important milestone: the First International Conference on
Language Documentation and Conservation, held at the University of Hawai?i.1 The
scale of the event was striking, with five parallel tracks running over three days. The
organizers coped magnificently with three times the expected participation (over 300).
The buzz among the participants was that we were at the start of something big, that
we were already part of a significant and growing community dedicated to supporting
small languages together, the conference subtitle.
The event was full of computation and linguistics, yet devoid of computational lin-
guistics. The language documentation community uses technology to process language,
but is largely ignorant of the field of natural language processing. I pondered what we
have to offer this community: ?Send us your 10 million words of Nahuatl-English bitext
and we?ll do you a machine translation system!? ?Show us your Bambara WordNet and
we?ll use it to train a word sense disambiguation tool!? ?Write up the word-formation
rules of Inuktitut in this arcane format and we?ll give you a morphological analyzer!? Is
there not some more immediate contribution we could offer?
Over the past 15 years, the field of computational linguistics has been revolution-
ized by the ready availability of large corpora. Landmark dates are the founding of
the Linguistic Data Consortium (1992) and the first Workshop on Very Large Corpora
(1993). While the CL community has been pre-occupied with the new-found technical
capabilities for collecting and processing large amounts of data, the field of linguistics
has been undergoing a revolution of its own. It is also dominated with the use of
new-found technical capabilities for collecting and processing large amounts of data.
However, in this case, the data comes from languages that are facing extinction.
Back in 1992, Michael Krauss, of the Alaska Native Language Center, issued the
world?s linguists with a wake-up call, calculating that ?at the rate things are going?
the coming century will see either the death or the doom of 90 per cent of mankind?s
languages? (Krauss 1992, page 7). He exhorted linguists to document these languages
?lest linguistics go down in history as the only science that presided obliviously over
the disappearance of 90 per cent of the very field to which it is dedicated? (page 10).
This message was delivered at the 15th International Congress of Linguists in Quebec,
and also in Language, the journal of the Linguistic Society of America.2
? Department of Computer Science and Software Engineering, University of Melbourne, Victoria 3010,
Australia. E-mail: sb@csse.unimelb.edu.au.
1 http://nflrc.hawaii.edu/icldc09/.
2 The LSA has posted an FAQ containing an accessible description of the problem and its scale at
http://www.lsadc.org/info/ling-faqs-endanger.cfm.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
Today, endangered language documentation is part of mainstream linguistics, sup-
ported with several book-length treatments of the subject,3 the online journal Language
Documentation and Conservation,4 numerous graduate courses, and funding programs in
many countries. Here is the description of the U.S. NSF/NEH program, Documenting
Endangered Languages, emphases added:5
This multi-year funding partnership between the National Science Foundation (NSF)
and the National Endowment for the Humanities (NEH) supports projects to develop
and advance knowledge concerning endangered human languages. Made urgent by
the imminent death of an estimated half of the 6000?7000 currently used human
languages, this effort aims also to exploit advances in information technology. Funding will
support fieldwork and other activities relevant to recording, documenting, and
archiving endangered languages, including the preparation of lexicons, grammars, text
samples, and databases. Funding will be available in the form of one- to three-year
project grants as well as fellowships for up to twelve months. At least half the available
funding will be awarded to projects involving fieldwork.
What does computational linguistics offer to a community that is exploiting advances
in information technology for projects involving linguistic fieldwork with endangered
languages?
The genesis of the field of computational linguistics out of the field ofmachine trans-
lation is well-known; this journal had a previous existence under the title Mechanical
Translation and Computational Linguistics. The relationship between CL and MT over the
past half-century has just come full circle: In March 2009 the ACL Executive Committee
accepted a proposal for a newACL Special Interest Group inMachine Translation. There
can be no doubt that the multilingual information society is driving many important
challenges in our discipline. However, relatively few languages have the necessary
resources to participate.
Over the same half-century another strand of research has sought to use compu-
tational techniques to support linguistic fieldwork. For example, Joseph Grimes?ACL
Vice President (1975)?has devoted much of his long career to studies at the interface
between computational linguistics and linguistic fieldwork.6 His NSF project with Gary
Simons, called Language Variation and Limits to Communication (Cornell University, 1976?
1978), involved building a suitcase-sized ?portable? computer and lugging it around
the Pacific to capture and analyze wordlists. Two decades later, my own fieldwork on
tone languages of Cameroon involved a laptop computer powered by a car battery, and
led to a series of ?Grassfields Bantu Fieldwork? corpora published by the LDC. While
the technology had improved, the modus operandi was the same: Take technology to a
remote field location and bring back data, and do enough linguistic analysis in the field
to ensure that the right variety and quality of data is being collected.
As if this were not challenging enough, the subsequent curation of the data is
fraught with technical difficulties. It?s easy to generate ?endangered data? when for-
mats, encodings, and media are so quickly obsolete (Bird and Simons 2003). Existing
fieldwork tools use incompatible formats, and it is often necessary to convert data
between the native formats of various tools. The experience of writing 10k lines of
3 Crystal 2000; Fishman 2001; Gippert, Himmelmann, and Mosel 2006; Grenoble and Whaley 2006;
Harrison 2007.
4 http://nflrc.hawaii.edu/ldc/.
5 http://www.nsf.gov/publications/pub summ.jsp?ods key=nsf06577.
6 For example, Grimes (1968), http://www.ethnologue.com/show author.asp?auth=2961.
470
Bird Natural Language Processing and Linguistic Fieldwork
Figure 1
Tone data from Bamileke Dschang, a Grassfields Bantu language of Cameroon.
Perl scripts for manipulating fieldwork data in Cameroon was the backdrop to the
development of the Natural Language Toolkit.7 Clearly, with enough effort we can use
computational techniques to represent and manipulate linguistic field data. Is there
more we can offer?
For example, consider the tone language data in Figure 1. It represents a slice
through part of an 8-dimensional tone paradigm containing 1,350 cells (Bird 2003). The
address of each cell in this data cube is just a vector specifying properties like tense,
mood, noun class, and lexical tones. The content of each cell is just a vector specifying a
surface tone pattern using abstract pitch numbers, like 31144442. What structure could
NLP techniques discover in this data? Could such analysis take place early enough to
guide the data collection work?
For a long time, fieldwork has been regarded as a style of elicitation and analysis that
involves an exotic language, an extended period, and an extreme location (cf. Hyman
[2001]). In contrast, a new, cyber-fieldworkmay be on the rise, in which the data is what-
ever one wants to treat as data, and where the ?fieldworker? elicits data via Skype, by
interrogating a sound archive, or by analyzing linguistic materials found on YouTube.
However, it is hard to find cases of fieldwork that fit these stereotypes of purism and
pragmatism, or what detractors might label paternalism and postmodernism. Thank-
fully, the real situation is more interesting. Regardless of location, language, and mode
of elicitation, linguistic fieldworkers are usually immersing themselves in data, in close
contact with a speech community. This may happen in the ancestral location or among
a well-organized diaspora of speakers. In places where the Internet is reaching into
remote places, scattered speakers of endangered languages are able to form online
communities,8 and in time this may provide another context for elicitation.
Linguistic fieldworkers are often pushing the limits of current theoretical machin-
ery, while simultaneously experiencing the bleeding edge of digital recording and
annotation technology. In the midst of this, they are eliciting and exploring a substantial
quantity of primary data, wheremany of the descriptive categories are simply unknown
or subject to revision. Theymay be transcribing speech when there is no existing writing
system and when we don?t know which sound contrasts are significant. They might be
guessing word breaks and testing hunches about what particular morphemes mean.
They could be puzzling over apparent inconsistencies in data from different speakers.
7 http://www.nltk.org/. See especially Bird, Klein, and Loper (2009, ch. 11).
8 For example, http://www.firstvoices.com/.
471
Computational Linguistics Volume 35, Number 3
When the data is not systematized, when there is no established body of knowledge
about the language, when many analytical options are available, and when every
conclusion is open to question, the task becomes one of managing uncertainty?and
in the meantime, avoiding an existential crisis.
(It?s hard for a field linguist to explain this ?fieldwork state of mind? to a computer
scientist. What comes closest is the experience of debugging someone else?s program.
An undergraduate computing laboratory is ripe with ?freely occurring programs,? each
one arising from a different?sometimes unrecognizable?view of a specified problem.
To help someone fix their program requires that you briefly enter their world, and
align your conceptual model of the problem with theirs, and point the way forward.
However, this is made more difficult by the fact that you must puzzle over their code
and their verbal statements, both of which may contain subtle errors. Now, scale up this
experience from minutes to months!)
Migrating early pen-and-paper fieldwork onto computer is difficult, and probably
fruitless. The technology gets in the way of the elicitation, and pre-occupation with
systematizating the data prevents us from noticing the patterns: ?premature mathema-
tization keeps Nature?s surprises hidden? (Lenat and Feigenbaum 1987, page 1177).
It?s probably best not to bother with linguistic software in the early stages of linguistic
description.
However, things change once the descriptive notation has stabilized, and a ?lin-
guistic exploration? workflow is established. The discovery of a newword in a text may
require an update to the lexicon and the construction of a new paradigm (e.g., in order to
correctly classify the word). Such updates may occasion the creation of some field notes,
the extension of a descriptive report, and possibly even the revision of the manuscript
for an analytical paper. Progress on description and analysis gives fresh insights about
how to organize existing data and it informs the quest for new data. Whether one is
sorting data, or generating helpful tabulations, or gathering statistics, or searching for
a (counter-)example, or verifying the transcriptions used in a manuscript, the principal
challenge is computational.
Documenting and describing endangered languages presents computational lin-
guistics with some difficult challenges. The most immediate challenge concerns lin-
guistic data management: representing structured annotations such as interlinear text,
supporting collaborative annotation, handling uncertain data, validating structure,
tracking data provenance, combining human and automatic methods, and so forth. NLP
techniquesmay enter the picture in unexpectedways. For instance, most documentation
projects collect wordlists, and these typically evolve into full-fledged lexicons over
time. The organization of fields within an entry is often inconsistent, yet we can recog-
nize the structure using standard robust parsing techniques, then transform the data
into a consistent structure, potentially saving months of manual effort in the process.
Once the data has some basic level of organization, the next challenge is one of
simultaneously downscaling and upscaling. First, we need new techniques that work on
small data sets (downscaling), with the consequence that fewer resources are spent on
data collection, while permitting many more languages to be analyzed in the same
timeframe (upscaling). What methods do we have that can detect structure in small,
noisy data sets, while being directly applicable to a wide variety of languages? This
represents uncharted territory for NLP.9
9 See (Palmer et al 2009) for a promising pilot study.
472
Bird Natural Language Processing and Linguistic Fieldwork
This dual perspective applies to the data collection work itself. If we have just one
week in a location where a language is spoken, to collect all the data we will ever
have for this language, what will we do? I write this on the eve of a one-week visit
to the Usarufa language area in the Eastern Highlands of Papua NewGuinea, under the
auspices of SIL. The language has about 1,000 speakers, and is no longer being learned
by children. We will give out digital voice recorders to have people record linguistic
interactions, narratives, and songs. Later, we will meet in a classroom where others
will augment these recordings with voice annotations, phrase by phrase, providing
a careful speech version along with translation into Tok Pisin, the language of wider
communication. A handful of speakers who are literate in the language will transcribe
a small portion of the collection. The resulting corpus, it is hoped, will be adequate
to support future analysis and revitalization work. If it is possible to collect a useful
corpus in the space of a week (downscaling) then it will also be possible to apply
such methods to many other languages (upscaling). In this way, limited resources are
deployed efficiently in a breadth-first approach to language documentation.
Apart from technical challenges, there is also an important sociological challenge
to create maximally interoperable language analysis software. To imagine this can be
done simply by adopting common file formats, or by operating an in-house software
development lifecycle using project funds, or by invoking the XML family of buzzwords
is to misunderstand the nature of the problem. Instead, we need to foster new research
collaborations involving computational linguists and field linguists, leading to new
understanding about how to collect and analyze corpora of data from endangered
languages. We need to nurture a community to share in the development of tools,
formats, interfaces, data repositories, query systems, machine learning techniques, visu-
alization methods, and so forth. We need to collaborate on a global federated database
of language data, permitting Web-based collaborative annotation of primary linguistic
data, continuously expandible and fully exportable for local processing.10 Everything
should be available under open source and open content licenses, fostering a Web-scale
ecosystem in which geographically distributed computational linguists, field linguists,
and the speakers of endangered languages themselves are united in their efforts to
document and describe the world?s languages.
We live during a brief period of overlap between the mass extinction of the world?s
languages and the advent of the digital age. What can we do?as individuals and as
a professional association?as we wake up to this global linguistic crisis? Recently, we
have seen that national bureaucracies have been able to take unprecedented steps in
the face of the global economic crisis; are we less agile? If the economic motivation for
language technology research has lost some of its luster, what do we have to lose?
So, shall we eke out an incremental existence, parasitic on linguistic theories, lan-
guage corpora, and machine learning algorithms developed by others? Are we content
to tweak parameters and deliver results that are surpassed at next year?s meeting, while
important sources of new data are falling silent? It?s time that we focused some of our
efforts on a new kind of computational linguistics, one that accelerates the documenta-
tion and description of the world?s endangered linguistic heritage, and delivers tangible
and intangible value to future generations. Who knows, we may even postpone the day
when these languages utter their last words.
10 The Open Language Archives Community (http://language-archives.org), the World Atlas of
Language Structures (http://wals.info), and the Rosetta Project (http://rosettaproject.org)
represent significant early steps in this direction.
473
Computational Linguistics Volume 35, Number 3
References
Bird, Steven, editor. 2003. Grassfields Bantu
Fieldwork: Dschang Tone Paradigms.
Linguistic Data Consortium. LDC2003S02,
ISBN 1-58563-254-6.
Bird, Steven, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with
Python. O?Reilly Media, Sebastopol, CA.
http://www.nltk.org/book.
Bird, Steven and Gary Simons. 2003. Seven
dimensions of portability for language
documentation and description. Language,
79:557?582.
Crystal, David. 2000. Language Death.
Cambridge University Press, Cambridge,
UK.
Fishman, Joshua A., editor. 2001. Can
Threatened Languages be Saved?: Reversing
Language Shift, Revisited: A 21st Century
Perspective. Multilingual Matters,
Clevedon, UK.
Gippert, Jost, Nikolaus Himmelmann, and
Ulrike Mosel, editors. 2006. Essentials of
Language Documentation. Mouton de
Gruyter, Berlin and New York.
Grenoble, Lenore and Lindsay Whaley. 2006.
Saving Languages: An Introduction to
Language Revitalization. Cambridge
University Press, Cambridge, UK.
Grimes, Joseph E. 1968. Computer backup
for field work in phonology.Mechanical
Translation and Computational Linguistics,
11:73?74.
Harrison, K. David. 2007.When Languages
Die: The Extinction of the World?s Languages
and the Erosion of Human Knowledge.
Cambridge University Press,
Cambridge, UK, pages 15?33.
Hyman, Larry M. 2001. Fieldwork as a
state of mind. In Paul Newman and
Martha Ratliff, editors, Linguistic
Fieldwork. Cambridge University Press,
Cambridge, UK.
Krauss, Michael E. 1992. The world?s
languages in crisis. Language, 68:4?10.
Lenat, Douglas B. and Edward A.
Feigenbaum. 1987. On the thresholds
of knowledge. In Proceedings of
the 10th International Conference
on Artificial Intelligence,
pages 1173?1182.
Palmer, Alexis, Taesun Moon, and Jason
Baldridge. 2009. Evaluating automation
strategies in language documentation.
In Proceedings of the NAACL HLT 2009
Workshops on Active Learning for Natural
Language Processing, pages 36?44,
Boulder, CO.
474
NLTK: The Natural Language Toolkit
Steven Bird
Department of Computer Science
and Software Engineering
University of Melbourne
Victoria 3010, Australia
sb@csse.unimelb.edu.au
Edward Loper
Department of Computer
and Information Science
University of Pennsylvania
Philadelphia PA 19104-6389, USA
edloper@gradient.cis.upenn.edu
Abstract
The Natural Language Toolkit is a suite of program mod-
ules, data sets, tutorials and exercises, covering symbolic
and statistical natural language processing. NLTK is
written in Python and distributed under the GPL open
source license. Over the past three years, NLTK has
become popular in teaching and research. We describe
the toolkit and report on its current state of development.
1 Introduction
The Natural Language Toolkit (NLTK) was
developed in conjunction with a computational
linguistics course at the University of Pennsylvania
in 2001 (Loper and Bird, 2002). It was designed
with three pedagogical applications in mind:
assignments, demonstrations, and projects.
Assignments. NLTK supports assignments of
varying difficulty and scope. In the simplest assign-
ments, students experiment with existing compo-
nents to perform a wide variety of NLP tasks. As
students become more familiar with the toolkit, they
can be asked to modify existing components, or
to create complete systems out of existing compo-
nents.
Demonstrations. NLTK?s interactive graphical
demonstrations have proven to be very useful
for students learning NLP concepts. The
demonstrations give a step-by-step execution
of important algorithms, displaying the current
state of key data structures. A screenshot of the
chart parsing demonstration is shown in Figure 1.
Projects. NLTK provides students with a flexible
framework for advanced projects. Typical projects
might involve implementing a new algorithm,
developing a new component, or implementing a
new task.
We chose Python because it has a shallow learn-
ing curve, its syntax and semantics are transparent,
and it has good string-handling functionality. As
an interpreted language, Python facilitates interac-
tive exploration. As an object-oriented language,
Python permits data and methods to be encapsulated
and re-used easily. Python comes with an extensive
standard library, including tools for graphical pro-
gramming and numerical processing. The recently
added generator syntax makes it easy to create inter-
active implementations of algorithms (Loper, 2004;
Rossum, 2003a; Rossum, 2003b).
Figure 1: Interactive Chart Parsing Demonstration
2 Design
NLTK is implemented as a large collection of
minimally interdependent modules, organized
into a shallow hierarchy. A set of core modules
defines basic data types that are used throughout the
toolkit. The remaining modules are task modules,
each devoted to an individual natural language
processing task. For example, the nltk.parser
module encompasses to the task of parsing, or
deriving the syntactic structure of a sentence;
and the nltk.tokenizer module is devoted to
the task of tokenizing, or dividing a text into its
constituent parts.
2.1 Tokens and other core data types
To maximize interoperability between modules, we
use a single class to encode information about nat-
ural language texts ? the Token class. Each Token
instance represents a unit of text such as a word,
sentence, or document, and is defined by a (partial)
mapping from property names to values. For exam-
ple, the TEXT property is used to encode a token?s
text content:1
>>> from nltk.token import *
>>> Token(TEXT="Hello World!")
<Hello World!>
The TAG property is used to encode a token?s part-
of-speech tag:
>>> Token(TEXT="python", TAG="NN")
<python/NN>
The SUBTOKENS property is used to store a tok-
enized text:
>>> from nltk.tokenizer import *
>>> tok = Token(TEXT="Hello World!")
>>> WhitespaceTokenizer().tokenize(tok)
>>> print tok[?SUBTOKENS?])
[<Hello>, <World!>]
In a similar fashion, other language processing tasks
such as word-sense disambiguation, chunking and
parsing all add properties to the Token data struc-
ture.
In general, language processing tasks are formu-
lated as annotations and transformations involving
Tokens. In particular, each processing task takes
a token and extends it to include new information.
These modifications are typically monotonic; new
information is added but existing information is not
deleted or modified. Thus, tokens serve as a black-
board, where information about a piece of text is
collated. This architecture contrasts with the more
typical pipeline architecture where each processing
task?s output discards its input information. We
chose the blackboard approach over the pipeline
approach because it allows more flexibility when
combining tasks into a single system.
In addition to the Token class and its derivatives,
NLTK defines a variety of other data types. For
instance, the probability module defines classes
for probability distributions and statistical smooth-
ing techniques; and the cfg module defines classes
for encoding context free grammars and probabilis-
tic context free grammars.
1Some code samples are specific to NLTK version 1.4.
2.2 The corpus module
Many language processing tasks must be developed
and tested using annotated data sets or corpora.
Several such corpora are distributed with NLTK,
as listed in Table 1. The corpus module defines
classes for reading and processing many of these
corpora. The following code fragment illustrates
how the Brown Corpus is accessed.
>>> from nltk.corpus import brown
>>> brown.groups()
[?skill and hobbies?, ?popular lore?,
?humor?, ?fiction: mystery?, ...]
>>> brown.items(?humor?)
(?cr01?, ?cr02?, ?cr03?, ?cr04?, ?cr05?,
?cr06?, ?cr07?, ?cr08?, ?cr09?)
>>> brown.tokenize(?cr01?)
<[<It/pps>, <was/bedz>, <among/in>,
<these/dts>, <that/cs>, <Hinkle/np>,
<identified/vbd>, <a/at>, ...]>
A selection of 5% of the Penn Treebank corpus is
included with NLTK, and it is accessed as follows:
>>> from nltk.corpus import treebank
>>> treebank.groups()
(?raw?, ?tagged?, ?parsed?, ?merged?)
>>> treebank.items(?parsed?)
[?wsj_0001.prd?, ?wsj_0002.prd?, ...]
>>> item = ?parsed/wsj_0001.prd?
>>> sentences = treebank.tokenize(item)
>>> for sent in sentences[?SUBTOKENS?]:
... print sent.pp() # pretty-print
(S:
(NP-SBJ:
(NP: <Pierre> <Vinken>)
(ADJP:
(NP: <61> <years>)
<old>
)
...
2.3 Processing modules
Each language processing algorithm is implemented
as a class. For example, the ChartParser and
RecursiveDescentParser classes each define
a single algorithm for parsing a text. We imple-
ment language processing algorithms using classes
instead of functions for three reasons. First, all
algorithm-specific options can be passed to the con-
structor, allowing a consistent interface for applying
the algorithms. Second, a number of algorithms
need to have their state initialized before they can
be used. For example, the NthOrderTagger class
Corpus Contents and Wordcount Example Application
20 Newsgroups (selection) 3 newsgroups, 4000 posts, 780kw text classification
Brown Corpus 15 genres, 1.15Mw, tagged training & testing taggers, text classification
CoNLL 2000 Chunking Data 270kw, tagged and chunked training & testing chunk parsers
Project Gutenberg (selection) 14 texts, 1.7Mw text classification, language modelling
NIST 1999 IEER (selection) 63kw, named-entity markup training & testing named-entity recognizers
Levin Verb Index 3k verbs with Levin classes parser development
Names Corpus 8k male & female names text classification
PP Attachment Corpus 28k prepositional phrases, tagged parser development
Roget?s Thesaurus 200kw, formatted text word-sense disambiguation
SEMCOR 880kw, POS & sense tagged word-sense disambiguation
SENSEVAL 2 Corpus 600kw, POS & sense tagged word-sense disambiguation
Stopwords Corpus 2,400 stopwords for 11 lgs text retrieval
Penn Treebank (sample) 40kw, tagged & parsed parser development
Wordnet 1.7 180kw in a semantic network WSD, NL understanding
Wordlist Corpus 960kw and 20k affixes for 8 lgs spell checking
Table 1: Corpora and Corpus Samples Distributed with NLTK
must be initialized by training on a tagged corpus
before it can be used. Third, subclassing can be used
to create specialized versions of a given algorithm.
Each processing module defines an interface
for its task. Interface classes are distinguished by
naming them with a trailing capital ?I,? such as
ParserI. Each interface defines a single action
method which performs the task defined by the
interface. For example, the ParserI interface
defines the parse method and the Tokenizer
interface defines the tokenize method. When
appropriate, an interface defines extended action
methods, which provide variations on the basic
action method. For example, the ParserI interface
defines the parse n method which finds at most n
parses for a given sentence; and the TokenizerI
interface defines the xtokenize method, which
outputs an iterator over subtokens instead of a list
of subtokens.
NLTK includes the following modules:
cfg, corpus, draw (cfg, chart, corpus,
featurestruct, fsa, graph, plot, rdparser,
srparser, tree), eval, featurestruct,
parser (chart, chunk, probabilistic),
probability, sense, set, stemmer (porter),
tagger, test, token, tokenizer, tree, and
util. Please see the online documentation for
details.
2.4 Documentation
Three different types of documentation are avail-
able. Tutorials explain how to use the toolkit, with
detailed worked examples. The API documentation
describes every module, interface, class, method,
function, and variable in the toolkit. Technical
reports explain and justify the toolkit?s design and
implementation. All are available from http://
nltk.sf.net/docs.html.
3 Installing NLTK
NLTK is available from nltk.sf.net, and is
packaged for easy installation under Unix, Mac
OS X and Windows. The full distribution consists
of four packages: the Python source code (nltk);
the corpora (nltk-data); the documentation
(nltk-docs); and third-party contributions
(nltk-contrib). Before installing NLTK, it is
necessary to install Python version 2.3 or later,
available from www.python.org. Full installation
instructions and a quick start guide are available
from the NLTK homepage.
As soon as NLTK is installed, users can run the
demonstrations. On Windows, the demonstrations
can be run by double-clicking on their Python
source files. Alternatively, from the Python
interpreter, this can be done as follows:
>>> import nltk.draw.rdparser
>>> nltk.draw.rdparser.demo()
>>> nltk.draw.srparser.demo()
>>> nltk.draw.chart.demo()
4 Using and contributing to NLTK
NLTK has been used at the University of Pennsylva-
nia since 2001, and has subsequently been adopted
by several NLP courses at other universities, includ-
ing those listed in Table 2.
Third party contributions to NLTK include:
Brill tagger (Chris Maloof), hidden Markov model
tagger (Trevor Cohn, Phil Blunsom), GPSG-style
feature-based grammar and parser (Rob Speer, Bob
Berwick), finite-state morphological analyzer (Carl
de Marcken, Beracah Yankama, Bob Berwick),
decision list and decision tree classifiers (Trevor
Cohn), and Discourse Representation Theory
implementation (Edward Ivanovic).
NLTK is an open source project, and we wel-
come any contributions. There are several ways
to contribute: users can report bugs, suggest fea-
tures, or contribute patches on Sourceforge; users
can participate in discussions on the NLTK-Devel
mailing list2 or in the NLTK public forums; and
users can submit their own NLTK-based projects
for inclusion in the nltk contrib directory. New
code modules that are relevant, substantial, orig-
inal and well-documented will be considered for
inclusion in NLTK proper. All source code is dis-
tributed under the GNU General Public License, and
all documentation is distributed under a Creative
Commons non-commercial license. Thus, poten-
tial contributors can be confident that their work
will remain freely available to all. Further infor-
mation about contributing to NLTK is available at
http://nltk.sf.net/contrib.html.
5 Conclusion
NLTK is a broad-coverage natural language toolkit
that provides a simple, extensible, uniform frame-
work for assignments, demonstrations and projects.
It is thoroughly documented, easy to learn, and sim-
ple to use. NLTK is now widely used in research
and teaching. Readers who would like to receive
occasional announcements about NLTK are encour-
aged to sign up for the low-volume, moderated mail-
ing list NLTK-Announce.3
6 Acknowledgements
We are indebted to our students and colleagues for
feedback on the toolkit, and to many contributors
listed on the NLTK website.
2http://lists.sourceforge.net/
lists/listinfo/nltk-devel
3http://lists.sourceforge.net/
lists/listinfo/nltk-announce
Graz University of Technology, Austria
Information Search and Retrieval
Macquarie University, Australia
Intelligent Text Processing
Massachusetts Institute of Technology, USA
Natural Language Processing
National Autonomous University of Mexico, Mexico
Introduction to Natural Language Processing
in Python
Ohio State University, USA
Statistical Natural Language Processing
University of Amsterdam, Netherlands
Language Processing and Information Access
University of Colorado, USA
Natural Language Processing
University of Edinburgh, UK
Introduction to Computational Linguistics
University of Magdeburg, Germany
Natural Language Systems
University of Malta, Malta
Natural Language Algorithms
University of Melbourne, Australia
Human Language Technology
University of Pennsylvania, USA
Introduction to Computational Linguistics
University of Pittsburgh, USA
Artificial Intelligence Application Development
Simon Fraser University, Canada
Computational Linguistics
Table 2: University Courses using NLTK
References
Edward Loper and Steven Bird. 2002. NLTK:
The Natural Language Toolkit. In Proceedings
of the ACL Workshop on Effective Tools and
Methodologies for Teaching Natural Language
Processing and Computational Linguistics, pages
62?69. Somerset, NJ: Association for Computa-
tional Linguistics. http://arXiv.org/abs/
cs/0205028.
Edward Loper. 2004. NLTK: Building a pedagogi-
cal toolkit in Python. In PyCon DC 2004. Python
Software Foundation. http://www.python.
org/pycon/dc2004/papers/.
Guido Van Rossum. 2003a. An Introduction to
Python. Network Theory Ltd.
Guido Van Rossum. 2003b. The Python Language
Reference. Network Theory Ltd.
*ULG(QDEOLQJ1DWXUDO/DQJXDJH(QJLQHHULQJ%\6WHDOWK
 
%DGHQ+XJKHVDQG6WHYHQ%LUG'HSDUWPHQWRI&RPSXWHU6FLHQFHDQG6RIWZDUH(QJLQHHULQJ8QLYHUVLW\RI0HOERXUQH9LFWRULD$XVWUDOLD
{badenh, sb}@cs.mu.oz.au 
 

$EVWUDFW
:HGHVFULEHDSURSRVDOIRUDQH[WHQVLEOHFRPSRQHQWEDVHGVRIWZDUHDUFKLWHFWXUHIRUQDWXUDOODQJXDJHHQJLQHHULQJDSSOLFDWLRQV2XUPRGHOOHYHUDJHVH[LVWLQJOLQJXLVWLFUHVRXUFHGHVFULSWLRQDQGGLVFRYHU\PHFKDQLVPVEDVHGRQH[WHQGHG'XEOLQ&RUHPHWDGDWD,QDGGLWLRQDSSOLFDWLRQGHVLJQLVIOH[LEOHDOORZLQJGLVSDUDWHFRPSRQHQWVWREHFRPELQHGWRVXLWWKHRYHUDOODSSOLFDWLRQIXQFWLRQDOLW\$QDSSOLFDWLRQVSHFLILFDWLRQODQJXDJHSURYLGHVDEVWUDFWLRQIURPWKHSURJUDPPLQJHQYLURQPHQWDQGDOORZVHDVHRILQWHUIDFHZLWKFRPSXWDWLRQDOJULGVYLDDEURNHU
 ,QWURGXFWLRQ
&RPSXWDWLRQDOJULGVDUHDQHPHUJLQJLQIUDVWUXFWXUH IUDPHZRUN IRU FRQGXFWLQJ UHVHDUFK ZKHUHSUREOHPV DUH RIWHQ GDWD RU SURFHVVRU LQWHQVLYH$FRPSXWDWLRQDOJULGDOORZVIRU ODUJHVFDOHDQDO\VLVGLVWULEXWHGUHVRXUFHVDQGSURFHVVLQJLQDGGLWLRQWRHQJHQGHULQJQHZPRGHOVIRUFROODERUDWLRQDQGDSSOLFDWLRQ GHYHORSPHQW )RVWHU HW DO  SURYLGHV D SK\VLRORJLFDO DQG DQ DQDWRPLFDO RYHUYLHZ RI JULG FRPSXWLQJ VHUYLFHV DQG SURYLGHVIRXQGDWLRQDO DUFKLWHFWXUHV IRU DSSOLFDWLRQ GHYHORSPHQWLQWKHJULGVSDFH*LYHQWKHSUHYDOHQFHRIODUJHGDWDVRXUFHVLQWKHQDWXUDOODQJXDJHHQJLQHHULQJ GRPDLQ DQG WKH QHHG IRU UDZ FRPSXWDWLRQDOSRZHU LQ WKHDXWRPDWHGDQDO\VLVRI VXFKGDWD WKHJULG FRPSXWLQJ SDUDGLJP SURYLGHV HIILFLHQFLHVRWKHUZLVH XQDYDLODEOH WR QDWXUDO ODQJXDJH HQJLQHHULQJ/DQJXDJHHQJLQHHULQJDSSOLFDWLRQVDUHW\SLFDOO\FRQVWUXFWHGRXWRIVHYHUDOSURFHVVLQJFRPSRQHQWVHDFK UHVSRQVLEOH IRU D VSHFLDOL]HG WDVN  7\SLFDOFRPSRQHQWV LQFOXGH VSHHFK UHFRJQLWLRQ WDJJLQJHQWLW\ GHWHFWLRQ DQDSKRUD UHVROXWLRQ SDUVLQJ HWF(DFKFRPSRQHQWLVKHDYLO\SDUDPHWHUL]HGDQGPXVWEHWUDLQHGRQYHU\ODUJHGDWDVHWVHJWKH/'&*LJDZRUGFRUSXV*UDII'LVFRYHULQJRSWLPDOSDUDPHWHUL]DWLRQV LV ERWK GDWD DQG FRPSXWDWLRQ
DOO\LQWHQVLYH  %XLOGLQJ FRPSOH[ DSSOLFDWLRQVVXFKDVVSRNHQGLDORJXHV\VWHPVGHSHQGVRQLGHQWLI\LQJ DQG LQWHJUDWLQJ VXLWDEOH FRPSRQHQWV RIWHQIURPDUDQJHRIVRXUFHV,QWKLVSDSHUZHGHVFULEHDSURSRVDOIRUDQH[WHQVLEOH FRPSRQHQWEDVHG VRIWZDUH DUFKLWHFWXUHIRU QDWXUDO ODQJXDJH HQJLQHHULQJ DSSOLFDWLRQVZKLFK OHYHUDJHV OLQJXLVWLF UHVRXUFH GLVFRYHU\PHFKDQLVPV DQG DOORZV VWDQGDUG LQWHUIDFHV ZLWKFRPSXWDWLRQDOJULGVIRUGDWDDQDO\VLV$QRYHUYLHZRIWKHPRGHOLVGLVSOD\HGEHORZLQ)LJXUH

)LJXUH$UFKLWHFWXUDO0RGHO

5HFHQWDGYDQFHVLQWKHDXWRPDWHGH[SRVXUHDQGKDUYHVWLQJ RI GDWD UHVRXUFH FDWDORJXHV DUH SURSRVHG WREHH[WHQGHG IRU WKHQDWXUDO ODQJXDJHHQJLQHHULQJ GRPDLQ %DVHG RQ WKHVH H[WHQVLRQV ZHFDQ SURJUDPPDWLFDOO\ DQG LQWHOOLJHQWO\ GLVFRYHUGDWD VRXUFHV FRPSRQHQWV DSSOLFDWLRQV DQG JULGQRGHVRIIHULQJVSHFLILFVHUYLFHV7KHPRGHODGYRFDWHGKHUH LVEURDGO\EDVHGRQWKHFRQFHSWRIFORXGFRPSXWLQJ6LHJHOH,QWKLVPRGHOD VHULHVRIGLVWULEXWHG LQGLYLGXDOFRPSRQHQWV DUH DVVHPEOHG YLD DQ DSSOLFDWLRQ IUDPHZRUN ZLWK LQWHUQDO FRPPXQLFDWLRQ UHTXLUHPHQWVDGGUHVVHGE\DFRPPRQLQWHUIDFHVSHFLILFDWLRQ$QDSSOLFDWLRQ FRQVLVWV RI RQH RU PRUH GDWD VRXUFHVWRJHWKHUZLWKDQXPEHURI LQGLYLGXDOFRPSRQHQWVFRRUGLQDWHGZLWKLQDQRYHUDOO IUDPHZRUN&RPSRQHQWVDUH IXQFWLRQVSHFLILF LPSOHPHQWDWLRQVZKLFKDGKHUH WR D FRUH VHULHV RI VWDQGDUGV IRU LQWHUFRPSRQHQW FRRUGLQDWLRQ ZKLFK LV RSHQ WR H[WHQVLRQE\WKLUGSDUWLHV&ROOHFWLRQVRIFRPSRQHQWVZKLFKIRUPDSSOLFDWLRQV DUH WKHQ H[SUHVVHG XVLQJ D JULG DSSOLFDWLRQVSHFLILFDWLRQODQJXDJHZKLFKLVSDUVHGE\DEURNHUWKDW LQWHUIDFHV ZLWK WKH FRPSXWDWLRQDO JULG LQIUDVWUXFWXUH$Q\DSSOLFDWLRQFRPSRVHGZLWKLQRURXWVLGH WKLV PRGHO FDQ DGGUHVV VXFK D EURNHU DQGKHQFHDFFHVVWKHSRZHUDQGVFDODELOLW\RIFRPSXWDWLRQDOJULGVIRUSURFHVVLQJ7KLVPRGHOKDVQXPHURXVEHQHILWV)LUVWH[LVWLQJ QDWXUDO ODQJXDJH HQJLQHHULQJ DSSOLFDWLRQV FDQEHQHILW IURP JULG VHUYLFHV ZLWKRXW EHLQJ JULGDZDUHVLPSO\E\LQVWUXFWLQJWKHEURNHUWRSHUIRUPDVSHFLILHGWDVNDVLI LWZHUHDVLQJOHVHUYHU 6HFRQGDSSOLFDWLRQGHILQLWLRQVDUHDGHFODUDWLYHVSHFLILFDWLRQRIDSURFHVVLQJWDVNDQGRIWKHUHODWLRQVKLSEHWZHHQ SURFHVVLQJ FRPSRQHQWV IDFLOLWDWLQJ VXEVWLWXWLRQRIHTXLYDOHQWFRPSRQHQWVDQGDJJUHJDWLRQRIVLPSOHDSSOLFDWLRQVLQWRPRUHFRPSOH[DSSOLFDWLRQV7KLUGDSSOLFDWLRQGHILQLWLRQVDQGUHVXOWVHWVFDQ EH VWRUHG GHVFULEHG XVLQJ VWDQGDUGPHWDGDWDDQG EH GLVFRYHUHG DQG UHXVHG E\ RWKHU DSSOLFDWLRQVDWDODWHUGDWH,QWKLVSDSHUZHGLVFXVVWKHDUFKLWHFWXUDOIRXQGDWLRQVRIRXUSURSRVDOUHVRXUFHGLVFRYHU\PHFKDQLVPV FRPSRQHQW LGHQWLILFDWLRQPXOWLFRPSRQHQWDSSOLFDWLRQ GHVLJQ LQFOXGLQJ VHYHUDO VDPSOHDSSOLFDWLRQV WKH JULG VHUYLFHV LQWHUIDFH DQG WKHJULG DSSOLFDWLRQ VSHFLILFDWLRQ ODQJXDJH )LQDOO\VRPHGLUHFWLRQVIRUIXWXUHZRUNDUHLGHQWLILHG
 %DVLF3URFHVVLQJ3DUDGLJPV
$ UDQJH RI VRIWZDUH DUFKLWHFWXUHV IRU QDWXUDOODQJXDJH HQJLQHHULQJ KDYH EHHQ GHYHORSHG LQ WKHODVWGHFDGHDQGPDWXUHGWRWKHH[WHQWWKDWWKHUHDUHDZLGH UDQJH RI DSSOLFDWLRQV EXLOW RQ IRXQGDWLRQVVXFKDV*$7(&XQQLQJKDPHWDO&XQQLQJKDP  DQG$7/$6 %LUG HW DO  +RZHYHUDOORIWKHVHVRIWZDUHDUFKLWHFWXUHVHQYLVDJHDORFDOSURFHVVLQJPRGHOZKHUHGDWDVRXUFHVHQWLUHDSSOLFDWLRQV DQG FRPSXWDWLRQDO UHVRXUFHV DUH FRORFDWHG $GGLWLRQDOO\ SURFHVVLQJ LV JHQHUDOO\ DVVXPHG WR EH VHULDOL]HGZLWK OLWWOH RSSRUWXQLW\ IRUVLPXOWDQHRXVSURFHVVHVWDNLQJDGYDQWDJHRIDQDO\VHVSURYLGHGE\HDFKRWKHU(TXDOO\ WKHUH KDYH EHHQ QXPHURXV GHYHORSPHQWV IRU WKH GHSOR\PHQW RI FRPSXWDWLRQDO JULGVUDQJLQJ IURPPLGGOHZDUH$3,?V OLEUDULHV DQG LQIUDVWUXFWXUDO PDQDJHPHQW DSSURDFKHV :LWKLQ WKHFRPSXWDWLRQDO JULG DUFKLWHFWXUHV FXUUHQWO\ GHSOR\HG WKHUH DUH ERWK SURFHVVRUFHQWULF DQG GDWDFHQWULFDUFKLWHFWXUHVRIZKLFKWKHPRVWFRPPRQLVWKH SURFHVVRUFHQWULF JULG ,Q WKLV FRQWH[W JULGDZDUHDSSOLFDWLRQVW\SLFDOO\DGRSWWKHDSSURDFKRIWUDQVSRUWLQJWKHGDWDIURPDVWRUDJHVLWHWRWKHVLWHRI DYDLODEOH &38 FDSDFLW\ +RZHYHU LQ QDWXUDOODQJXDJHHQJLQHHULQJWKLVLVOHVVDWWUDFWLYHEHFDXVHWKHVL]HRIGDWDVRXUFHVLVFRPPRQO\ODUJHHQRXJKIRU DQ\ QHWZRUN EDVHG WUDQVSRUW WR EH UHODWLYHO\LQHIILFLHQW DQGRU FRVWSURKLELWLYH 7KH LPSOLFDWLRQV IRU WKHGHYHORSPHQWRIDEURNHUDUFKLWHFWXUHIRUGDWDFHQWULFJULGVDUHRIRUWKRJRQDOLQWHUHVWEXWDUHGLVFXVVHGEULHIO\ODWHU7KHPRGHODGYRFDWHGLQWKLVSDSHUZRXOGZRUNHTXDOO\ ZHOO LQ HLWKHU FRQWH[W SURYLGLQJ D VXIILFLHQWO\ LQWHOOLJHQW EURNHU DUFKLWHFWXUH ZDV DYDLODEOH$W WKH WLPHRIZULWLQJ LW LVHQYLVDJHG WKDWDSURFHVVRUFHQWULF DUFKLWHFWXUH ZLOO EH XWLOL]HG DWOHDVWLQWKHPHGLXPWHUP
 5HVRXUFH'LVFRYHU\
:LWKLQ DQ\ GLVWULEXWHG QDWXUDO ODQJXDJH HQJLQHHULQJ V\VWHP WKHUH FOHDUO\ H[LVWV DQHHG WRGLVFRYHU GDWD VRXUFHV FRPSRQHQWV DSSOLFDWLRQV DQGSURFHVVLQJVHUYLFHVRYHUWKHQHWZRUN7KLVQHHGLVH[SUHVVHG ERWK E\ WKH DSSOLFDWLRQ GHYHORSHUZKRGHVLUHV WR EXLOG DQ DSSOLFDWLRQ IURP D UDQJH RIDYDLODEOHFRPSRQHQWVDVZHOODVE\ WKHEURNHULQJDJHQW ZKLFK QHHGV WR DOLJQ DSSOLFDWLRQ UHTXLUHPHQWVZLWKDYDLODEOHFRPSXWDWLRQDOUHVRXUFHV
7KHUHDUHDUDQJHRIPHWKRGVDYDLODEOHWRIDFLOLWDWH VXFK GLVFRYHU\ 2I QRWH DUH WKH SRWHQWLDO IRUWKH XVH RI D:HE 6HUYLFHV LPSOHPHQWDWLRQ XVLQJ'$0/6RUIRUPDO5')RUHPEHGGLQJ WKLV WDVNZLWKLQ WKH ZLGHU 6HPDQWLF :HE LPSOHPHQWDWLRQVHJ E\:&:KLOVW WKH DXWKRUV DUH DSSUHFLDWLYHRI WKH GHYHORSPHQWV ZLWKLQ WKHVH DUHDV WKHUH DOUHDG\ H[LVWV D ODQJXDJH UHVRXUFH VSHFLILF IUDPHZRUNIRUUHVRXUFHGHVFULSWLRQDQGGLVFRYHU\ZLWKLQWKH2SHQ/DQJXDJH$UFKLYHV&RPPXQLW\2/$&ZKLFKKDVVLJQLILFDQWEHQHILWDQGLWLVRQWKLVEDVLVZHSURFHHG:H SURSRVH WR H[WHQG WKH2/$& UHVRXUFH GHVFULSWLRQ DQG GLVFRYHU\PHFKDQLVPV 7KLVZLOO EHDFKLHYHG E\ DGRSWLQJ DQG H[WHQGLQJ VWDQGDUGVEDVHGRQ WKH IRXQGDWLRQDOZRUNRI+XJKHVIRU WKH HQFRGLQJRI LQIRUPDWLRQ VXFK DV&38DQGPHPRU\UHTXLUHPHQWVDQGFRPSRQHQWDQGDSSOLFDWLRQIXQFWLRQDOLW\(OHFWLQJ WRH[WHQG WKLVVWDQGDUGDOORZV XV WR OHYHUDJH GLVFRYHU\ WRROV DQG WHFKQLTXHV DOUHDG\ LQ H[LVWHQFH DQG FRQVLGHUHG WKHEHQFKPDUN IRU VXFK SURFHVVHV 7KH2/$&0HWDGDWD6HW%LUGDQG6LPRQVDLVDQH[WHQVLRQRI WKH 'XEOLQ &RUH 0HWDGDWD 6HW 'XEOLQ &RUH0HWDGDWD ,QLWLDWLYH  WKXV HQVXULQJ ZLGHVSUHDG DFFHVVLELOLW\ 7KH 2/$& LQLWLDWLYH KDV DVWDQGDUGL]HG PHFKDQLVP IRU H[WHQGLQJ 2/$&PHWDGDWD%LUGDQG6LPRQVEZKLFKZHZLOODGRSW WR GHVFULEH GDWD VRXUFHV DSSOLFDWLRQV DQGSURFHVVLQJ QRGHV+DYLQJ EDVHG UHVRXUFH GHVFULSWLRQV RQ 2/$& VWDQGDUGV ZH FDQ WKHQ TXHU\ DQ2/$& DJJUHJDWRU WR GLVFRYHU WKH H[LVWHQFH DQGVWDWXVRIUHVRXUFHVRILQWHUHVW
 &RPSRQHQW,GHQWLILFDWLRQ
,Q RUGHU WR H[SORUH WKH JULG FRPSXWLQJ SDUDGLJP WKURXJK D QDWXUDO ODQJXDJH HQJLQHHULQJIUDPHZRUN D FRPSRQHQW EDVHG PRGHO KDV EHHQDGRSWHG :LWKLQ WKLV PRGHO ZH LGHQWLI\ DQG GHVFULEH D QXPEHU RI UHXVDEOH FRPSRQHQW W\SHVZKLFKFDQEHFRPELQHGWRFUHDWHPXOWLFRPSRQHQWDSSOLFDWLRQV WR EH H[HFXWHG LQ WKH JULG HQYLURQPHQW (DFK RI WKHVH FRPSRQHQW W\SHV KDV DV DFRPPRQ IXQFWLRQDO FRUH WKH DELOLW\ WR FRPPXQLFDWHZLWK D FHQWUDOPDQDJHPHQW LQIUDVWUXFWXUH XVLQJ VWDQGDUG PHVVDJLQJ LQWHUIDFHV ,Q GHYHORSLQJSURWRW\SH LPSOHPHQWDWLRQVZH LQWHQG WRZUDSH[LVWLQJFRPSRQHQWVZKHUHYHUSRVVLEOH
 $QQRWDWLRQ6HUYHU
$ODUJHFODVVRI ODQJXDJHHQJLQHHULQJWDVNVLQYROYLQJ WLPHVHULHV GDWD FDQ EH FRQVWUXHG DV DGGLQJDQHZOD\HURIDQQRWDWLRQWRH[LVWLQJGDWD)RUH[DPSOH D SURFHVVZKLFK WDNHV VSHHFK LQSXW DQGSURGXFHVWH[WRXWSXWHJDVSHHFKUHFRJQL]HUFDQEH YLHZHG DV DGGLQJ WH[WXDO DQQRWDWLRQ WR DXGLRGDWD ZLWK WKH UHVXOW WKDW ERWK GDWD W\SHV UHPDLQDFFHVVLEOHIRUIXUWKHUDQDO\VLV:KHQVHYHUDOWDVNVRSHUDWHLQWKLVPRGHWKH\FROOHFWLYHO\EXLOGDULFKVWRUH RI OLQJXLVWLF LQIRUPDWLRQ  )RU H[DPSOH DSURVRG\ UHFRJQLWLRQ FRPSRQHQW FRXOG LGHQWLI\PDMRU SKUDVH ERXQGDULHV LQ VSRNHQ LQSXW DQG DSDUVHU FRXOG HPSOR\ ERWK WKH $65 DQG SURVRG\RXWSXWLQFRQVWUXFWLQJSDUVHWUHHV$QQRWDWLRQ JUDSKV FDQ EH XVHG WR UHSUHVHQW DGLYHUVHUDQJHRIWLPHVHULHVDQQRWDWLRQVLQFOXGLQJ$65 RXWSXW 326 WDJV QDPHG HQWLWLHV V\QWDFWLFFKXQNVRUWUHHVDOLJQHGWUDQVODWLRQVGLDORJXHDFWVDQGVRRQ,PSRUWDQWO\WKHLQWHUPHGLDWHVWDJHVRIPDQ\SURFHVVLQJ WDVNVDUHZHOOIRUPHGDVDQQRWDWLRQJUDSKV VRFHUWDLQRXWSXWVPD\EHFRPHDYDLODEOH VKRUWO\ DIWHU WKH SURFHVV VWDUWV IDFLOLWDWLQJHIILFLHQWSLSHOLQLQJDQGVWUHDPLQJ7KH$QQRWDWLRQ*UDSK 7RRONLW $*7. /LQJXLVWLF 'DWD &RQVRUWLXP  LV DQ RSHQ VRXUFH LPSOHPHQWDWLRQ RI DQQRWDWLRQ JUDSKV ZKLFK ZRUNV ZLWK DPDWXUH$3,DQGDZLGHUDQJHRIVXSSRUWHGPRGHOVDQG IRUPDWV 7KH $QQRWDWLRQ 6HUYHU LV D FRPSRQHQW WKDW FROOHFWV FROODWHV DQG VWRUHV DQQRWDWLRQJUDSKV ZKLFK PD\ EH JHQHUDWHG RU DFFHVVHG E\RWKHUFRPSRQHQWVDQGDSSOLFDWLRQVDQGH[WHQGVRQSURSRVDOV SUHYLRXVO\ GHVFULEHG E\ &LHUL DQG %LUGDQGDYDLODEOHDVDSURWRW\SHLQWKHFXUUHQW$*7.GLVWULEXWLRQ
 $OLJQPHQW
$Q $OLJQPHQW FRPSRQHQW LV XVHG WR IRUFLEO\DOLJQ GLJLWL]HG VSHHFK ZLWK D VXSSOLHG WUDQVFULSW7KLVFDQEHSHUIRUPHGDWYDULRXV OHYHOVRIJUDQXODULW\ DQG SDUWLDO UHVXOWV FDQ EH QDWXUDOO\ UHSUHVHQWHGDVDQDQQRWDWLRQJUDSK
 $XWRPDWLF6SHHFK5HFRJQLWLRQ
$Q $65 FRPSRQHQW FDQ EH XVHG WR FUHDWH DWLPHDOLJQHG WUDQVFULSW DJDLQ UHSUHVHQWHG GLUHFWO\DVDQDQQRWDWLRQJUDSK
 'DWD6RXUFH3DFNDJLQJ
$ 'DWD 6RXUFH 3DFNDJLQJ FRPSRQHQW GLYLGHVGDWD VRXUFHV LQWR ORJLFDO XQLWV ZKLFK FDQ EH GLVWULEXWHG DFURVV WKH JULG DV LQGLYLGXDO SURFHVVLQJWDVNV$VDQH[DPSOHFRQVLGHUDGLJLWDODXGLRILOHRI0E LQ VL]H$V WKLVZRXOGEHQRQWULYLDO WRWUDQVIHU WR D UHPRWH SURFHVVLQJ QRGH D 3DFNDJHUFRXOGGLYLGHWKLVGDWDVRXUFHLQWRILIW\0EGLJLWDODXGLRILOHVHDFKRIZKLFKFRXOGEHFRPSUHVVHGWUDQVSRUWHGDQGSURFHVVHGVHSDUDWHO\
 &RQYHUVLRQ
$&RQYHUVLRQFRPSRQHQWLVXVHGWRFRQYHUWEHWZHHQPHGLDIRUPDWVFKDUDFWHUHQFRGLQJVFKHPHVDQGDQQRWDWLRQW\SHV7KHH[SHFWHGLQSXWDQGRXWSXW IRUPDWVRIH[LVWLQJQDWXUDO ODQJXDJHHQJLQHHULQJ FRPSRQHQWV DUH RIWHQ LQFRPSDWLEOH DQG WKLVFRPSRQHQW ZLOO IDFLOLWDWH FRPSRQHQW LQWHJUDWLRQVLPSOLI\LQJ WKH WDVN RIZUDSSLQJ H[LVWLQJ FRPSRQHQWV  ,Q WKH FDVH RIPHGLD IRUPDWV WKLV FRPSRQHQWZLOOEHDEOHWRVHUYHDVWDWLFGDWDVRXUFHDVDVWUHDPVRXUFHDQGYLFHYHUVD,QWKHFDVHRIDQQRWDWLRQ FRQYHUVLRQ WKLV FRPSRQHQW ZLOO EH DEOH WRFRQYHUWEHWZHHQWKHZLGHYDULHW\RIH[LVWLQJWLPHVHULHVDQQRWDWLRQIRUPDWVXQGHUVWRRGE\$*7.
 7H[W$QQRWDWLRQ
$7H[W$QQRWDWLRQFRPSRQHQWDXJPHQWVH[LVWLQJDQQRWDWHG WH[WZLWKDQHZOD\HURIDQQRWDWLRQHJ326WDJVVHQVHWDJVQDPHGHQWLWLHVV\QWDFWLFFKXQNVDQGVRIRUWK
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 49?56,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Dynamic Path Prediction and Recommendation in a Museum Environment
Karl Grieser??, Timothy Baldwin? and Steven Bird?
? CSSE
University of Melbourne
VIC 3010, Australia
? DIS
University of Melbourne
VIC 3010, Australia
{kgrieser,tim,sb}@csse.unimelb.edu.au
Abstract
This research is concerned with making
recommendations to museum visitors
based on their history within the physical
environment, and textual information
associated with each item in their history.
We investigate a method of providing
such recommendations to users through
a combination of language modelling
techniques, geospatial modelling of
the physical space, and observation of
sequences of locations visited by other
users in the past. This study compares
and analyses different methods of path
prediction including an adapted naive
Bayes method, document similarity, visitor
feedback and measures of lexical similarity.
1 Introduction
Visitors to an information rich environment such as
a museum, are invariably there for a reason, be it
entertainment or education. The visitor has paid
their admission fee, and we can assume they intend
to get the most out of their visit. As with other
information rich environments and systems, first-
time visitors to the museum are at a disadvantage as
they are not familiar with every aspect of the collec-
tion. Conversely, the museum is severely restricted
in the amount of information it can convey to the
visitor in the physical space.
The use of a dynamic, intuitive interface can over-
come some of these issues (Filippini, 2003; Benford
et al, 2001). Such an interface would convention-
ally take the form of a tour guide, audio tour, or a
curator stationed at points throughout the museum.
This research is built around the assumption that the
museum visitor has access to a digital device such
as a PDA and that it is possible for automatic sys-
tems to interact with the user via this device. In this
way we aim to be able to deliver relevant content
to the museum visitor based on observation of their
movements within the physical museum space, as
well as make recommendations of what exhibits they
might like to visit next and why. At present, we are
focusing exclusively on the task of recommendation.
Recommendations can be used to convey predic-
tions about what theme or topic a given visitor is
interested in. They can also help to communicate
unexpected connections between exhibits (Hitzeman
et al, 1997), or explicitly introduce variety into the
visit. For the purposes of this research, we focus
on this first task of providing recommendations con-
sistent with the visitor?s observed behaviour to that
point. We investigate different factors which we
hypothesise impact on the determination of what
exhibits a given visitor will visit, namely: the phys-
ical proximity of exhibits, the conceptual similarity
of exhibits, and the relative sequence in which other
visitors have visited exhibits.
Recommendation systems in physical environ-
ments are notoriously hard to evaluate, as the
recommendation system is only one of many stimuli
which go to determine the actual behaviour of the
visitor. In order to evaluate the relative impact
of different factors in determining actual visitor
behaviour, we separate the stimuli present into
a range of predictive methods. In this paper we
target the task of user prediction, that is prediction
of what exhibit a visitor will visit next based on
49
their previous history. Language based models are
intended to simulate a potentially unobservable
source of information: the visitor?s thought process.
In order to identify the reason for the visitor?s
interest in the multiple part exhibits we parallel this
problem with the task of word sense disambiguation
(WSD). Determining the visitor?s reason for visiting
an exhibit allows a predictive system to more
accurately model the visitor?s future path.
This study aims to arrive at accurate methods of
predicting how a user will act in an information-rich
museum. The space focused on in this research is
the Australia Gallery Collection of the Melbourne
Museum, at Carlton Gardens in Melbourne,
Australia. The predictions take the form of which
exhibits a visitor will visit given a history of
previously visited exhibits. This study analyses
and compares the effectiveness of supervised and
unsupervised learning methods in the museum
domain, drawing on a range of linguistic and
geospatial features. A core contribution of
this study is its focus on the relative import of
heterogeneous information sources a user makes
use of in selecting the next exhibit to visit.
2 Problem Description
In order to recommend exhibits to visitors while they
are going through a museum, the recommendations
need to be accurate/pertinent to the goals that the
visitor has in mind. Without accurate recomme nda-
tions, recommendations given to a visitor are essen-
tially useless, and might as well not have been rec-
ommended at all.
Building a recommender system based on contex-
tual information (Resnick and Varian, 1997) is the
ultimate goal of this research. However the envi-
ronment in this circumstance is physical, and the
actions of visitors are expected to vary within such
a space, as opposed to the usual online or digital
domain of recommender systems. Studies such as
HIPS (Benelli et al, 1999) and the Equator project1
have analysed the importance and difficulty of inte-
grating the virtual environment into the physical, as
well as identifying how non-physical navigation sys-
tems can relate to similar physical systems. For the
purpose of this study, it is sufficient to acknowledge
1http://www.equator.ac.uk
the effect of the physical environment by scaling all
recommendations against their distances from one
another.
The common information that museum exhibits
contain is key in determining how each individual
relates to each other exhibit in the collection. At
the most basic level, the exhibits are simply isolated
elements that share no relationship with one another,
their only similarity being that they occur together
in visitor paths. This interpretation disregards any
meaning or content that each exhibit contains. But
museum exhibits are created with the goal of pro-
viding information, and to disregard the content of
an exhibit is to disregard its purpose.
An exhibit in a museum may be many kinds of
things, and hence most exhibits will differ in presen-
tation and content. The target audience of a museum
is one indicator of the type of content that can be
expected within each exhibit. An art gallery is com-
prised of mainly paintings and sculptures: single
component exhibits with brief descriptions. A chil-
dren?s museum will contain a high proportion of
interactive exhibits, and much audio and visual con-
tent. In these two cases the reason for visiting the
exhibit differs greatly.
Given the diversity of information contained
within each exhibit and the greater diversity of a
museum collection, it can be difficult to see why
visitors only examine certain exhibits during their
tours. It is very difficult to perceive what a visitor?s
intention is without constant feedback, making the
problem of providing relevant recommendations a
question of predicting what a visitor is interested in
based on characteristics of exhibits the visitor has
already seen. The use of both physical attributes and
exhibit information content are used in conjunction
in an effort to account for multiple possible reasons
for visiting as exhibit. Connections between
physical attributes of an exhibit are easier to identify
than connections based on information content.
This is due to the large quantity of information
associated with each exhibit, and the difficulty in
determining what the visitor liked (or disliked)
about the exhibit.
In order to make prediction based on a visitor?s
history, the importance of the exhibits in the visi-
tors path must be known. This is difficult to obtain
directly without the aid of real-time feedback from
50
the user themselves. In an effort to emulate the
difficulty of observing mental processes adopted by
each visitor, language based predictive models are
employed.
3 Resources
The domain in which all experimentation takes place
is the Australia Gallery of the Melbourne Museum.
This exhibition provides a history of the city of Mel-
bourne Melbourne, from its settlement up to the
present day, and includes such exhibits as the taxi-
dermised coat of Phar Lap (Australia?s most famous
race horse) and CSIRAC (Australia?s first, and the
world?s fourth, computer). The Gallery contains
enough variation so that not all exhibits can be clas-
sified into a single category, but is sufficiently spe-
cialised to offer much interaction and commonality
between the exhibits.
The exhibits within the Australia Gallery take
a wide variety of forms, from single items with
a description plaque, to multiple component dis-
plays with interactivity and audio-visual enhance-
ment; note, for our purposes in experimentation,
we do not differentiate between exhibit types or
modalities. The movement of visitors within an
exhibition can be restricted if the positioning of the
exhibits require visitors to take a set path (Peponis
et al, 2004), which can alter how a visitor chooses
between exhibits to view. In the case of the Australia
Gallery, however, the collection is spread out over
a sizeable area, and has an open plan design such
that visitor movement is not restricted or funnelled
through certain areas and there is no predetermined
sequence or selection of exhibits that a given visitor
can be expected to spend time at.
We used several techniques to represent the dif-
ferent aspects of each exhibit. We categorised each
exhibit by way of its physical attributes (e.g. size)
and taxonomic information about the exhibit con-
tent (e.g. clothing or animal). We also described
each exhibit by way of its physical location within
the Australia Gallery, relative to a floorplan of the
Gallery.
The Melbourne Museum also has a sizable
web-site2 which contains much detailed information
about the exhibits within the Australia Gallery. This
2http://www.museum.vic.gov.au/
data is extremely useful in that it provides a rich
vocabulary of information based on the content
of each exhibit. Each exhibit identified within the
Australia Gallery has a corresponding web-page
describing it. The information content of an exhibit
is made up of the text in its corresponding web-page
combined with its attributes. By having a large
source of natural language information associated
with the exhibit, linguistic based predictive methods
can more accurately identify the associations made
by visitors.
The dataset that forms that basis of this research
is a database of 60 visitor paths through the Aus-
tralia Gallery, which was collected by Melbourne
Museum staff over a period of four months towards
the end of 2001. The Australia Gallery contains a
total of fifty-three exhibits. This data is used to eval-
uate both physical and conceptual predictive meth-
ods. If predictive methods are able to accurately
describe how a visitor travels in a museum, then
the predictive method creates an accurate model of
visitor behaviour.
Exhibit components can be combined to form a
description for each exhibit. For this purpose, the
Natural Language Toolkit 3 (Bird, 2005) was used
to analyse and compare the lexical content associ-
ated with each exhibit, so that relationships between
exhibits can be identified.
4 Methodology
Analysis of user history as a method of prediction
(or recommendation) has been examined in
Chalmers et al (1998). Also discussed is the
role that user history plays in anticipating user
goals. This approach can be adapted to a physical
environment by simply substituting in locations
visited in place of web pages visited. Data gathered
from the paths of previous visitors also forms a valid
means of predicting other visitors? paths (Zukerman
and Albrecht, 2001). This approach operates under
the assumption that all visitors behave in a similar
fashion when visiting a museum. However visitors?
goals in visiting a museum can differ widely. For
example, the goals of a student researching a project
will differ to those of a family with young children
on a weekend outing.
3http://nltk.sourceforge.net/
51
A conceptual model of the exhibition space is cre-
ated by visitors with a specific task in mind. Inter-
pretation of this conceptual model is key to creating
accurate recommendations. The building of such a
conceptual model takes place from the moment a
visitor enters an exhibition, until the time they leave,
and skews the visitor towards groups of conceptual
locations and categories.
The representation of these intrinsically dynamic
models is directly related to the task the visitor has
in mind. Students will form a conceptual model
based around their course requirements, children
around the most visually attractive exhibits, and
so forth. This necessitates the need for multiple
exhibit similarity measures, however in the absence
of express knowledge of the ?type? of each visitor in
the sample data, a broad-coverage recommendation
system that functions best in all circumstances is the
desired goal. It is hoped that in future, reevaluation
of the data to classify visitors into broad categories
(e.g. information seeking, entertainment seeking)
will allow for the development of specialised
models tailored to visitor types.
The models of exhibit representation we exam-
ine in this research are exhibit proximity, text-based
exhibit information content, and exhibit popularity
(based on the previous visitor data provided by the
Melbourne Museum), as well as combinations of the
three. Exhibit information content is a two part rep-
resentation: primarily each exhibit has a large body
of text describing the exhibit drawn from the Mel-
bourne Museum website. It is fortunate that this
information is curated, and managed from a cen-
tral source, so that inconsistencies between exhibit
information are extremely rare. The authors were
unable to find any contradictory information in the
web-pages used for experimentation, as may be the
case with larger non-curated document bodies. The
second component of the information content is a
small set of key terms describing the attributes of
the exhibit. Textual content as a means of deter-
mining exhibit similarity has been analysed previ-
ously (Green et al, 1999), both in terms of keyword
attributes and bodies of explanatory text.
In order to form a prediction about which exhibit
a visitor will next visit, the probability of the tran-
sition of the visitor from their current location to
every other exhibit in the collection must be known.
Prediction of the next exhibit by proximity simply
means choosing the closest not-yet-visited exhibit to
the visitor?s current location. In terms of information
content, each exhibit is related to all other exhibits to
a certain degree. To express this we use the attribute
keywords as a query to find the exhibit most simi-
lar. We use the attribute keywords associated with
each document to search the document space of the
exhibits to find the exhibit that is most similar to the
exhibit the visitor is currently located at. To do this
we use a simple tf?idf scheme, using the attribute
keywords as the queries, and the exhibit associated
web pages as the document space. The score of each
query over each document is normalised into a tran-
sitional probability array such that
?
j P (q|dj) = 1
for a query (q) over the j exhibit documents (dj).
In order to determine the popularity of an
exhibit, the visitor paths provided by the Melbourne
Museum were used to form another matrix of
transitional probabilities based on the likelihood
that a visitor will travel to an exhibit from the
exhibit they are currently at. I.e. for each exhibit e
an array of transitional probabilities is formed such
that
?
j P (e|cj) = 1 where cj ? C ? = C/{e}, i.e.
all exhibits other than e. In both cases Laplacian
smoothing was used to remove zero probabilities.
The methods of exhibit popularity and physical
proximity are superficial in scope and do not extend
into the conceptual space adopted by the visitors.
They do however give insight into how a physical
space affects a visitors? mental representation of the
conceptual areas associated with specific exhibit col-
lections, and are more easily observable. Visitor
reaction to exhibit information content is harder to
observe and more problematic to predict. Any accu-
rate recommender systems produced in this fashion
will need to take into account the limitations these
two methods place on the thought processes of visi-
tors.
Connections that visitors make between exhibits
are more fluid, and are harder to represent in terms
of similarity measures. Specifically it is difficult to
see why visitors make connections between exhibits
as there can be multiple similarities between two
exhibits. To this end we have equated this prob-
lem with the task of Word Sense Disambiguation
(WSD). The path that a visitor takes can be seen
as a sentence of exhibits, and each exhibit in the
52
sentence has an associated meaning. WSD is used
to determine the meaning of the next exhibit based
on the meanings of previous exhibits in the path. For
each word in the keyword set of each exhibit, the
WordNet (Fellbaum, 1998) similarity is calculated
against each word in another exhibit. The similar-
ity is the sum of the WordNet similarities between
all attribute keywords in the two exhibits (K1, K2),
normalised over the length of both keyword sets:
?
k1?K1
?
k2?K2 WNsim(k1, k2)
|K1||K2|
For the purposes of this experiment we have
chosen to use three WordNet similarity/relatedness
measures to simulate the conceptual connections
that visitors make between exhibits. The Lin (Lin,
1998) and Leacock-Chodorow (Leacock et al,
1998) similarity measures and the Banerjee-
Pedersen (Patwardhan and Pedersen, 2003)
relatedness measures were used. The similarities
were normalised and transformed into probability
matrices such that
?
j PWNsim(e|cj) = 1 for each
next exhibit ci. The use of WordNet measures is
intended to simulate the mental connections that
visitors make between exhibit content, given that
each visit can interpret content in a number of
different ways.
The history of the visitor at any given time is
essential in keeping the visitor?s conceptual model
of the exhibit space current. The recency of a given
exhibit within a visitor?s history is inversely propor-
tional to how long ago the exhibit was encountered.
To take into account the visitor history, the col-
laborative data, proximity, document vectors, and
conceptual WordNet similarity, we adapt the naive
Bayes approach. The conditional probabilities of
each method are combined along with the temporal
recency of an exhibit to produce a predictive exhibit
recommender. The resultant recommendation to a
visitor can be described as follows:
c? = arg max
ci
P (ci)
t
?
j=1
P (Aj |ci) ? 2?(t?j+1) +
2?t
t
where t is the length of the visitor?s history, Aj ? C
is an exhibit at time j in the visitor history (and C
is the full set of exhibits), and ci ? C ? = C/{Aj}
is each unvisited exhibit. The most probable next
exhibit (c?) is selected from all possible next exhibits
(ci). Any selections made must be compared against
the visitor?s history. In this, we assume that a pre-
viously visited exhibit has already been seen, and
hence should not be recommended again.
The effectiveness of these methods was tested in
multiple combinations, both with history modeling
and without (only the exhibit the visitor is currently
at is considered). Testing was carried out using
the sixty visitor paths supplied by the Melbourne
Museum. For each method two tests were carried
out:
? Predict the next exhibit in the visitor?s path.
? Only make a prediction if the probability of the
prediction is above a given threshold.
Each path was analysed independently of the oth-
ers, and the resulting recommendations evaluated as
a whole. The measures of precision and recall in
the evaluation of recommender systems has been
applied effectively in previous studies (Raskutti et
al., 1997; Basu et al, 1998). In the second test
precision is the measure we are primarily concerned
with: it is not the aim of this recommender system to
predict all elements of a visitor?s path in the correct
order. The correctness of the exhibits predicted is
more important than the quantity of the predictions
the visitor visits, hence only exhibits predicted with
a (relatively) high probability are included in the
final list of predicted exhibits for that visitor.
The thresholds are designed to increase the cor-
rectness of the predictions, by only making a pre-
diction if there is a high probability of the visitor
travelling to the exhibit. As all predictive methods
choose the most probable transition from all possible
transitions, the transition with the highest probabil-
ity is always selected. The threshold values simply
cut off all probabilities below a certain value.
5 Results and Evaluation
The first tests carried out were done only using the
simple probability matrices described in Section 4,
and hence only use the information associated with
the visitor?s current location and not the entirety of
their history. The baseline method being used in all
testing is the naive method of moving to the closest
not-yet-visited exhibit.
53
Method BOE Accuracy
Proximity (baseline) 0.270 0.192
Popularity 0.406 0.313
Tf?Idf 0.130 0.018
Lin 0.129 0.039
Leacock-Chodorow 0.116 0.024
Banerjee-Pedersen 0.181 0.072
Popularity - Tf?Idf 0.196 0.093
Popularity - Lin 0.225 0.114
Popularity - Leacock-Chodorow 0.242 0.130
Popularity - Banerjee-Pedersen 0.163 0.064
Proximity - Tf?Idf 0.205 0.084
Proximity - Lin 0.180 0.114
Proximity - Leacock-Chodorow 0.220 0.151
Proximity - Banerjee-Pedersen 0.205 0.105
Proximity - Popularity 0.232 0.129
Table 1: Single exhibit history using individual and
combined transitional probabilities
In order to prevent specialisation of the methods
over the training data (the aforementioned 60 visitor
paths), 60 fold cross-validation was used. With the
path being used as the test case removed from the
training data at each iteration.
The results of prediction using only the current
exhibit as information can be seen in Table 1. Com-
binations of predictive methods are also included to
add physical environment factors to conceptual sim-
ilarity methods. For example, if two exhibits may
be highly related conceptually but on opposite sides
of the exhibit space, a visitor may forgo the distant
exhibit in favour of a closer exhibit that is slightly
less relevant.
Due to the lengths of the recommendation sets
made for each visitor (a recommendation is made
for each exhibit visited), precision and recall are
identical. The measure of Bag Of Exhibits (BOE)
describes the percentage of exhibits that were visited
by the visitor, but not necessarily in the same order
as they were recommended. The BOE measure is
the same as measuring precision and recall for the
purposes of this evaluation. With the introduction of
thresholds to improve precision, precision and recall
are measured as separate entities.
As seen in Table 1 the performance of the
conceptual or information similarity methods
(the tf?idf method, Lin, Leacock-Chodorow and
Banerjee-Pedersen) is worse than that of the
methods based on static features of the exhibits,
and all perform worse than the baseline. In
order to produce a higher percentage of correct
recommendations, thresholds were introduced.
Using thresholds, a recommendation is only made
if the probability of a visitor visiting an exhibit next
is above a given percentage. The thresholds used
in Table 2 are arbitrary, and were arrived at after
experimentation.
It is worth noting that in both tests, with and
without thresholds, the method of exhibit popularity
based on visitor paths is the most successful. One
expects this trend to continue with the introduction
of the history based model described in Section 4.
Each transitional probability matrix was used in con-
junction with the history model, the results of this
experimentation can be seen in Table 3.
Only single transitional probability matrices are
used in conjunction with the history model. The
physical distance to an exhibit is only relevant to the
current prediction, the distance travelled in the past
from exhibit to exhibit is irrelevant, and so physical
conceptual combinations are not necessary. A model
such as this describes the evolution of a thought pro-
cess, or is able to identify the common conceptual
thread linking the exhibits in a visitor?s path. This
is only true if the visitor has a conceptual model in
mind when touring the museum. Without the aid of
a common information thread, conceptual predictive
methods based on exhibit information content will
always perform poorly.
6 Discussion
The visitor paths supplied by the Melbourne
Museum represent sequential lists of exhibits, and
each visitor is a black box travelling from exhibit
to exhibit. It is this token vs. type problem that
does not allow us to select an appropriate predictive
method with which to make recommendations.
Instead a broad coverage method is necessary. Use
of history models to analyse entire visitor paths are
less successful than analysis of solely the current
location of the visitor. This can be attributed to the
fact that a majority of the visitors tracked may not
have had preconceived tasks in mind when they
entered the museum space, and just moved from
one visually impressive exhibit to the next. The
visitors do not consider their entire history as being
relevant, and only take into account their current
54
Method Threshold Precision Recall F-score
Proximity 0.03 0.271 0.270 0.270
Popularity 0.06 0.521 0.090 0.153
Tf?Idf 0.06 0.133 0.122 0.128
Lin 0.01 0.129 0.129 0.129
Leacock-Chodorow 0.01 0.117 0.117 0.117
Banerjee-Pedersen 0.01 0.182 0.180 0.181
Popularity - Tf?Idf 0.001 0.176 0.154 0.164
Popularity - Lin 0.0005 0.383 0.316 0.348
Popularity - Leacock-Chodorow 0.0005 0.430 0.349 0.385
Popularity - Banerjee-Pedersen 0.001 0.236 0.151 0.184
Proximity - Tf?Idf 0.001 0.189 0.174 0.181
Proximity - Lin 0.0005 0.239 0.237 0.238
Proximity - Leacock-Chodorow 0.0005 0.252 0.250 0.251
Proximity - Banerjee-Pedersen 0.0005 0.182 0.180 0.181
Proximity - Popularity 0.001 0.262 0.144 0.186
Table 2: Single exhibit history predictive methods using thresholds
Method BOE Accuracy
Proximity 0.066 0.0
Popularity 0.016 0.0
Tf?Idf 0.033 0.0
Lin 0.064 0.0
Leacock-Chodorow 0.036 0.0
Banerjee-Pedersen 0.036 0.0
Table 3: Entire visitor history predictive methods.
context. This also explains the relative success of
the predictive method built from analysis of the
visitor paths, presenting a marked improvement
over the baseline of nearest exhibit. In the best case
(as seen in Table 2) the exhibit popularity predictive
method was able to give relevant recommendations
52% of the time.
The interaction between predictive methods here
is highly simplified. The assumption made is that all
aspects of the visitor?s conceptual model are inde-
pendent, or only interact on a superficial level (see
the lower halves of Tables 1?2). More complex
methods of prediction need to be explored fully
take into account the interaction between predictive
methods.
Representations based on physical proximity take
into account little of how a visitor conceptualises a
museum space. They do however describe the fact
that closer exhibits are more visible to visitors, and
are hence more likely to be visited. Proximity can
be used as an augmentation to a conceptual model
designed to be used within a physical space.
Any exhibit is best described by the information it
contains. Visitors with a specific task in mind when
entering an exhibition already have a pre-initialised
conceptual model, relating to a theme. The visitors
seek out content related to their conceptual model,
and separate the bulk of the collection content from
the information they require. The representation of
the content within each exhibit as a vocabulary of
terms allows us to find similarity between exhibits.
The data available at the time of this testing does not
make the distinction between user types, and so only
broad coverage methods result in a improvements.
With the introduction of user types to the data sup-
plied by the museum, specific predictive methods
can be applied to each individual user. This addi-
tional information can be significantly beneficial as
the specialisation of predictive types to visitors is
expected to produce much more accurate predictions
and recommendations. Currently the only method
available to discern the user type is to analyse the
length of time the visitor spends at each each exhibit.
This data is yet to be adapted and annotated from the
raw data supplied by the Melbourne Museum.
7 Conclusion
The above methods are intended to represent base-
line components of possible conceptual models that
represent how a visitor is able to selectively assess
the dynamic context of museum visits. The model
that a visitor generates for themselves is unique, and
is difficult to represent in terms of physical attributes
of exhibits.
55
Being able to predict future actions of a user
within a given environment allows a recommender
system to influence a user?s choices. Key to the pre-
diction of future actions, is the idea that a user has
a conceptual model of how they see content within
the environment in relation to a task. With respect to
a museum environment, the majority of users have
no preconceived conceptual model upon entering an
exhibition and must build one as they explore the
environment. Users with a preconceived task will
more often than not stick to exhibits surrounding
a particular theme. Use of a language-based con-
ceptual model based on the information contained
within an exhibit can be combined with conceptual
models based on geospatial attributes of the exhibit
to create a representation of how a user will react
to an exhibit. The use of heterogeneous information
contained within the exhibit space is only relevant
when the visitor has an information-centric task in
mind.
7.1 Future Work
The methods dealing with a language-based concep-
tual model given here are very basic, and the overall
accuracy and precision of the recommender system
components require improvement. Additional anno-
tation of the paths of visitors to the museum will
enable proper evaluation of conceptual information
based predictive methods. On-site testing of predic-
tive methods at the Melbourne Museum is the ulti-
mate goal of this project, and testing the effects of
visitor feedback on recommendations will also be
analysed. In order to gain more insight into vis-
itor behaviour, the current small-scale set of visi-
tors needs to be expanded to include multiple visitor
types, as well as tasks.
Acknowledgments
This research was supported by Australian Research Council
DP grant no. DP0770931. The authors wish to thank the staff
of the Melbourne Museum for their help in this study. Special
thanks goes to Carolyn Meehan and Alexa Reynolds for their
gathering of data, and helpful suggestions throughout this study.
Thanks also goes to Ingrid Zukerman and Liz Sonenberg for
their input on this research.
References
Chumki Basu, Haym Hirsh, and William Cohen. 1998. Rec-
ommendations as classification: Using social and content-
based information in recommendation. In Proceedings of the
National Conference of Artificial Intelligence, pages 714?
720, Madison, United States.
Giuliano Benelli, Alberto Bianchi, Patrizia Marti, David Sen-
nati, and Elena Not. 1999. HIPS: Hyper-Interaction within
Physical Space. In ICMCS ?99: Proceedings of the IEEE
International Conference on Multimedia Computing and
Systems, volume 2, page 1075. IEEE Computer Society.
Steve Benford, John Bowers, Paul Chandler, Luigina Ciolfi,
Martin Flintham, Mike Fraser, Chris Greenhalgh, Tony Hall,
Sten-Olof Hellstrom, Shahram Izadi, Tom Rodden, Holger
Schnadelbach, and Ian Taylor. 2001. Unearthing virtual
history: using diverse interfaces to reveal hidden worlds. In
Proc Ubicomp, pages 1?6. ACM.
Steven Bird. 2005. NLTK-Lite: Efficient scripting for natural
language processing. In Proceedings of the 4th International
Conference on Natural Language Processing (ICON), pages
11?18, Kanpur, India.
Matthew Chalmers, Kerry Rodden, and Dominique Brodbeck.
1998. The Order of Things: Activity-Centred Information
Access. Computer Networks and ISDN Systems, 30:1?7.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Silvia Filippini. 2003. Personalisation through IT in museums:
Does it really work? Presentation at ICHIM 2003.
Stephen J. Green, Maria Milosavljevic, Robert Dale, and Cecile
Paris. 1999. When virtual documents meet the real world.
In Proc. of WWW8 Workshop: Virtual Documents, Hypertext
Functionality and the Web.
Janet Hitzeman, Chris Mellish, and Jon Oberlander. 1997.
Dynamic generation of museum web pages: The intelli-
gent labelling explorer. Archives and Museum Informatics,
11(2):117?115.
Claudia Leacock, Martin Chodorow, and George A Miller.
1998. Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics, 24(1):147?
65.
Dekang Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In (CoLING)-(ACL), pages 768?774, Montreal,
Canada.
Siddharth Patwardhan and Ted Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In Interna-
tional Joint Conference on Artificial Intelligence, pages 805?
810, Acapulco, Mexico.
John Peponis, Ruth Conroy Dalton, Jean Wineman, and Nick
Dalton. 2004. Measuring the effect of layout on visitors?
spatial behaviors in open plan exhibition settings. Environ-
ment and Planning B: Planning and Design, 31:453?473.
Bhavani Raskutti, Anthony Beitz, and Belinda Ward. 1997. A
feature-based approach to recommending selections based
on past preferences. User Modelling and User Adaption,
7(3):179?218.
Paul Resnick and Hal R Varian. 1997. Recommender systems.
Commun. ACM, 40(3):56?58.
Ingrid Zukerman and David W Albrecht. 2001. Predictive
statistical models for user modeling. User Modeling and
User-Adapted Interaction, 11(1?2):5?18.
56
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 27?35,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Defining a Core Body of Knowledge for the
Introductory Computational Linguistics Curriculum
Steven Bird
Department of Computer Science and Software Engineering
University of Melbourne, Victoria 3010, Australia
sb@csse.unimelb.edu.au
Abstract
Discourse in and about computational linguis-
tics depends on a shared body of knowledge.
However, little content is shared across the
introductory courses in this field. Instead,
they typically cover a diverse assortment of
topics tailored to the capabilities of the stu-
dents and the interests of the instructor. If the
core body of knowledge could be agreed and
incorporated into introductory courses several
benefits would ensue, such as the proliferation
of instructional materials, software support,
and extension modules building on a com-
mon foundation. This paper argues that it is
worthwhile to articulate a core body of knowl-
edge, and proposes a starting point based on
the ACM Computer Science Curriculum. A
variety of issues specific to the multidisci-
plinary nature of computational linguistics are
explored.
1 Introduction
Surveys of introductory courses in computational
linguistics and natural language processing provide
evidence of great diversity.1 Regional variation is
stark: courses may emphasise theory over program-
ming (or vice versa), rule-based approaches over
statistical approaches (or vice versa), tagging and
parsing over semantic interpretation (or vice versa),
and so on. The diversity is hardly surprising given
the particular background of a student cohort and the
particular expertise of an instructor.
1http://aclweb.org/aclwiki/index.php?
title=List_of_NLP/CL_courses
In spite of this variation, the introductory course
needs to serve some common, basic needs. For some
students, it will be the first step in a pathway leading
to specialised courses, graduate research, or employ-
ment in this field. These students should receive a
solid technical foundation and should come away
with an accurate picture of the many opportunities
that lie ahead. For students who do not continue, the
introductory course will be their main exposure to
the field. In addition to the technical content, these
students need to understand how the field draws
from and contributes back to its parent disciplines
of linguistics and computer science, along with tech-
nological applications that are helping to shape the
future information society. Naturally, this course
is also a prime opportunity to promote the field to
newcomers and encourage them to pursue advanced
studies in this area. In all cases, the introductory
course needs to cover a core body of knowledge.
The fact that a core body of knowledge exists
in computational linguistics is demonstrated anec-
dotally: a doctoral student is told to curtail her
extended discussions of basic POS tagging and CFG
parsing algorithms since they are part of the pre-
sumed knowledge of the audience; a researcher pre-
senting work to a general linguistics or computer sci-
ence audience discovers to his surprise that certain
methodologies or algorithms need to be explicated
and defended, even though they was uncontroversial
when presented at a conference; a keynote speaker at
a computational linguistics conference can presume
that certain theoretical programs and practical goals
of the field are widely accepted. These three areas
? terminology, methodology, ideology ? constitute
27
part of the core body of knowledge of computational
linguistics. They provide us with the starting point
for identifying the concepts and skills to be covered
in the introductory course.
Identifying a core body of knowledge would bring
three major benefits. First, technical support would
be consolidated: instructional materials together
with implementations of standard algorithms would
be available in several programming paradigms and
languages. Second, colleagues without a research
specialization in computational linguistics would
have a non-controversial curriculum with external
support, a standard course that could be promoted
to a broad range of students as a mainstream option,
in both linguistics and computer science. Similarly,
new graduates beginning a teaching career would
be better equipped to push for the adoption of a
new computational linguistics or natural language
processing course at institutions where it is not
yet established. Third, employers and graduate
schools would be able to make assumptions about
the knowledge and skills of a new graduate.
The goal of this paper is to advocate the idea of
consensus around a body of knowledge as a promis-
ing way to coordinate the introductory computa-
tional linguistics curriculum, without attempting to
mandate the structure of individual courses or the
choice of textbooks. The paper is organised as fol-
lows: section 2 sets the scene by describing a vari-
ety of contexts in which computational linguistics
is taught, drawing on the author?s first-hand experi-
ence; section 3 sets out a possible organization for
the introductory topics in computational linguistics,
modelled on the ACM Computer Science Curricu-
lum; section 4 explores some implications of this
approach for curriculum and assessment. The paper
closes with remarks about next steps.
2 Contexts for Teaching and Learning in
Computational Linguistics
In this section a variety of scenarios are described
in which the author has had direct first-hand experi-
ence. All cases involve entry-level courses in com-
putational linguistics. They provide the back-drop
to the current proposal, exemplifying a range of
contexts in which a core body of knowledge would
need to be delivered, contexts imposing different
constraints on implementation.
Before embarking on this discussion it is helpful
to be reminded of the differing backgrounds and
goals of new students. Some want to use com-
putational techniques in the analysis of language,
while others want to use linguistic knowledge in the
development of language technologies. These back-
grounds and goals are orthogonal, leading to the grid
shown in Table 1.
I will begin with the most common context of a
graduate-level course, before progressing to upper-
level undergraduate, lower-level undergraduate, and
secondary levels.
2.1 Graduate-Level Courses
Dozens of graduate programs in computer science
and in linguistics have an introductory course on
computational linguistics or natural language pro-
cessing. In most cases, this is all the formal train-
ing a student will receive, and subsequent training
is happens in private study or on the job. In some
universities this is the entry point into a suite of more
advanced courses in such areas as lexical semantics,
statistical parsing, and machine translation. Even so,
it is important to consider the shared assumptions of
these specialised courses, and the needs of a student
who only undertakes the introductory course.
There are two principal challenges faced by
instructors at this level. The first is to adequately
cover the theoretical and practical sides of the field
in a single semester. A popular solution is not to try,
i.e. to focus on theory to the exclusion of practical
exercises, or to simply teach ?programming for
linguists.? The former deprives students of the
challenge and excitement of writing programs to
automatically process language. The latter fails to
cover any significant domain-specific theories or
algorithms.
The second challenge is to address the diverse
backgrounds of students, ranging from those with a
computer science background to a linguistics back-
ground, with a scattering of students who have a
background in both or in neither.
The author taught at this level at the University
of Pennsylvania over a period of three years. Per-
haps the most apt summary of the experience is
triage. Cohorts fell into three groups: (i) students
28
Background: Arts and Humanities Background: Science and Engineering
Language
Analysis
Programming to manage language data,
explore linguistic models, and test empir-
ical claims
Language as a source of interesting prob-
lems in data modeling, data mining, and
knowledge discovery
Language
Technology
Knowledge of linguistic algorithms and
data structures for high quality, maintain-
able language processing software
Learning to program, with applications
to familiar problems, to work in language
technology or other technical field
Table 1: Summary of Students? Backgrounds and Goals, from (Bird et al, 2008a)
who are well prepared in either linguistics or com-
puter science but not both (the majority) who will
perform well given appropriate intervention; (ii) stu-
dents who are well-prepared in both linguistics and
computer science, able to complete learning tasks
on their own with limited guidance; and (iii) stu-
dents with minimal preparation in either linguis-
tics or computer science, who lack any foundational
knowledge upon which to build. Resources targetted
at the first group invariably had the greatest impact.
2.2 Specialised Upper-Level Undergraduate
Courses
In contrast with graduate-level courses, a specialised
upper-level undergraduate course will typically be
an elective, positioned in the later stages of an
extended sequence of courses (corresponding to
ACM unit IS7 Natural Language Processing, see
?3). Here it is usually possible to make reliable
assumptions about background knowledge and
skills, and to provide training that is pitched at
exactly the right level.
The author taught at this level in the Computer
Science and Linguistics departments at the
University of Melbourne during the past five
years (five times in Computer Science, once in
Linguistics). In the Linguistics department, the
course began by teaching programming, with
illustrations drawn from linguistic domains,
before progressing to topics in text processing
(tokenization, tagging), grammars and parsing, and
data management. Laboratory sessions focussed on
the acquisition of programming skills, and we found
that a 1:5 staff-student ratio was insufficient.
In the Computer Science department, the first
approach was to introduce linguistics for 2-3 weeks
before looking at algorithms for linguistic process-
ing. This was unpopular with many students, who
did not see the motivation for learning about such
topics as morphology and verb subcategorization in
isolation from practical applications. A revised ver-
sion of the course opened with topics in text process-
ing, including tokenization, extracting text from the
web, and moving on to topics in language engineer-
ing. (Bird et al (2008b) provide a more extended
discussion of opening topics.)
A third option is to teach computational linguistic
topics in the context of a specialised course in an
allied field. Thus a course on morphology could
include a module on finite-state morphology, and a
course on machine learning could include a mod-
ule on text mining. In the former case, a linguistic
domain is presupposed and the instructor needs to
teach the linguist audience about a particular corpus
to be processed or an algorithm to be implemented
or tested. In the latter case, a family of algorithms
and data structures is presupposed and the instructor
needs to teach a computer science audience about
linguistic data, structures, and processes that can
serve as a domain of application.
2.3 Cross-Disciplinary Transition
People entering computational linguistics from
either a linguistics or computer science background
are faced with a daunting challenge of learning
the fundamentals of the other field before they
can progress very far with the study of the target
domain. A major institution with a long history
of teaching computational linguistics will have
a cadre of graduate students and post-doctoral
researchers who can support an instructor in
teaching a course. However, one measure of the
success of the approach being advocated here are
that such institutions will be in the minority of those
where computational linguistics is taught. In such
contexts, a computational linguistics course will be
29
a lone offering, competing for enrolments with a
variety of more established electives. To compound
the problem, a newcomer to the field may be faced
with taking a course in a department other than
their host department, a course which presumes
background knowledge they lack. Additional
support and self-paced learning materials are
crucial. Efforts on filling out the computational
linguistics content in Wikipedia ? by instructors and
students alike ? will help the entire community.
2.4 Lower-Level Undergraduate Courses
An intriguing option for delivery of an introduc-
tion to computational linguistics is in the context
of entry-level courses in linguistics and computer
science. In some cases, this may help to address
the declining interest of students in these individual
disciplines.
As computer science finds a broader role in ser-
vice teaching, rather than in training only those stu-
dents doing a major, the curriculum needs to be
driven by topics of broad appeal. In the author?s cur-
rent first year teaching, such topics include climate
change, population health, social anthropology, and
finance. Many fundamental concepts in data struc-
tures and algorithms can be taught from such start-
ing points. It is possible to include language pro-
cessing as one of the drivers for such a course.
Many possibilities for including computational
linguistics exist in the second-level computer sci-
ence curriculum. For example, algorithmic methods
involving time-space trade-offs and dynamic pro-
gramming can be motivated by the task of building a
simple web search engine (Bird and Curran, 2006).
Concrete tasks involve web crawling, text extrac-
tion, stemming, and indexing. Spelling correction
can be used as a driver for teaching core computer
science concepts in associative arrays, linked lists,
and sorting by a secondary key.
An analogous opportunity exists in the context of
entry-level courses in linguistics. Linguistics stu-
dents will readily agree that most human knowledge
and communication is represented and expressed
using language. But it will come as a surprise that
language technologies can process language auto-
matically, leading to more natural human-machine
interfaces, and more sophisticated access to stored
information. In this context, a linguistics student
may grasp a broader vision for his/her role in the
multilingual information society of the future.
In both cases, the hope is that students are inspired
to do further undergraduate study spanning linguis-
tics and computer science, and to enter industry
or graduate school with a solid preparation and a
suitable mix of theoretical knowledge and technical
skills.
The major obstacle is the lack of resources avail-
able to the typical instructor, who is not a specialist
in computational linguistics, and who has to deliver
the course to a large audience having no prior inter-
est or knowledge in this area. They need simple
packages and modules that can be incorporated into
a variety of teaching contexts.
2.5 Secondary School
Programming and Information Technology have
found a place in the secondary curriculum in many
countries. The coursework is typically animated
with projects involving games, databases, and
dynamic websites. In contrast, the curriculum
involving the grammar and literature of a major
world language typically only uses information
technology skills for such mundane tasks as word
processing and web-based research. However, as
innovators in the language curriculum look for
new ways to enliven their classes with technology,
computational linguistics offers a ready-made
source of interesting problems and methods.
In Australia, the English Language curriculum of
the Victorian Certificate of Education is a linguistics
program offered as part of the last two years of
secondary education (VCAA, 2006; Mulder et al,
2001). This course provides a promising host for
computational linguistics content in the Victorian
secondary curriculum. The author has delivered an
?Electronic Grammar? module2 in an English class
in a Victorian secondary school over a three week
period, jointly with a teacher who has a double
degree in linguistics and computer science. Students
were taught the elements of programming together
with some simple applications involving taggers,
parsers and annotated corpora. These activities
served to reinforce students? understanding of
lexical categories, lexical semantics, and syntactic
2http://nltk.org/electronic_grammar.html
30
ambiguity (i.e. prepositional phrase attachment).
Similar methods could be applied in second
language learning classes to locate common words
and idioms in corpora.
In this context, key challenges are the installa-
tion of specialised software (even a programming
language interpreter), overcoming the impenetrable
nature of standard part-of-speech tagsets by map-
ping them to simplified tagsets, and providing suit-
able training for teachers. A promising solution
is to provide a self-paced web-based programming
and testing environment, side-stepping issues with
school infrastructure and teacher training.3
3 Defining the CL Body of Knowledge
A promising approach for identifying the CL body
of knowledge is to begin with the ACM Computing
Curricula 2001 Computer Science Volume (ACM,
2001). In this scheme, the body of knowledge within
computer science is organised in a three-level hierar-
chy: subfields, units and topics. Each subfield has a
two-letter designator, such as OS for operating sys-
tems. Subfields are divided into several units, each
being a coherent theme within that particular area,
and each identified with a numeric suffix. Within
each unit, individual topics are identified. We can
select from this body of knowledge the areas that
are commonly assumed in computational linguistics
(see the Appendix), and then expect them to be part
of the background of an incoming computer science
student.
The field of linguistics is less systematised, and no
professional linguistics body has attempted to devise
an international curriculum standard. Helpful com-
pendia of topics exist, such as the Language Files
(Stewart and Vaillette, 2008). However, this does
not attempt to define the curriculum but to provide
supporting materials for introductory courses.
Following the ACM scheme, one could try to
establish a list of topics comprising the body of
knowledge in computational linguistics. This is not
an attempt to create a comprehensive ontology for
the field (cf. Cole (1997), Uszkoreit et al (2003)),
but rather a simple practical organization of intro-
ductory topics.
3This is a separate activity of the author and colleagues,
available via ivle.sourceforge.net
CL. Computational Linguistics
CL1. Goals of computational linguistics
roots, philosophical underpinnings,
ideology, contemporary divides
CL2. Introduction to Language
written vs spoken language; linguistic levels;
typology, variation and change
CL3. Words, morphology and the lexicon
tokenization, lexical categories, POS-tagging,
stemming, morphological analysis, FSAs
CL4. Syntax, grammars and parsing
grammar formalisms, grammar development,
formal complexity of natural language
CL5. Semantics and discourse
lexical semantics, multiword expressions,
discourse representation
CL6. Generation
text planning, syntactic realization
CL7. Language engineering
architecture, robustness, evaluation paradigms
CL8. Language resources
corpora, web as corpus, data-intensive linguistics,
linguistic annotation, Unicode
CL9. Language technologies
named entity detection, coreference, IE, QA,
summarization, MT, NL interfaces
Following the ACM curriculum, we would expect
to designate some of these areas as core (e.g.
CL1-3), while expecting some number of additional
areas to be taken as electives (e.g. three from the
remaining six areas). A given curriculum would
then consist of three components: (a) bridging
studies so students can access the core knowledge;
(b) the core body of knowledge itself; and (c)
a selection of electives chosen to give students
a balance of linguistic models, computational
methodologies, and application domains. These
issues involve fleshing out the body of knowledge
into a sequential curriculum, the topic of the next
section.
4 Implications for the Curriculum
The curriculum of an introductory course builds out
from the body of knowledge of the field by lin-
earizing the topic areas and adding bridging studies
and electives. The result is a pathway that medi-
ates between students? backgrounds and their goals
as already schematised in Table 1. Figure 1 dis-
plays two hypothetical pathways, one for students
31
Parsing
Computational Linguistics Core Body of Knowledge
LinguisticsPREPARATION
FOUNDATION
EXTENSION
Computer 
Science
Mathematics, 
Statistics Psychology
Discourse Generation
...
Language 
Engineering ...
"LING-380 Language Technology""CS-390 Natural Language Processing"
Figure 1: Curriculum as a Pathway Through the Core Body of Knowledge, with Two Hypothetical Courses
from a computer science background wanting to
learn about natural language processing, and one for
students from a linguistics background wanting to
learn about language technology. These could serve
as templates for individual advanced undergraduate
courses with names that are driven by local market-
ing needs rather than the need to emphasise the com-
putational linguistics content. However, they could
also serve as a guide for a whole series of course
selections in the context of a coursework masters
program. Clearly, the adoption of a core body of
knowledge has rather limited implications for the
sequence of an individual curriculum.
This section explores these implications for the
curriculum and raises issues for wider discussion
and exploration.
4.1 Diverse Entry Points
An identified body of knowledge is not yet a cur-
riculum. It must sit in the context of the background
and goals of a particular audience. An analysis of
the author?s experience in teaching computational
linguistics to several types of audience has led to
a four-way partitioning of the possible entry points,
shown in Figure 2.
The approaches in the top half of the figure are
driven by applications and skills, while those in the
bottom half are driven by theoretical concerns both
inside and outside computational linguistics. The
entry points in the top-left and bottom-right of the
diagram seem to work best for a computer science
audience, while the other two seem to work best
for a linguistics audience (though further work is
required to put such impressionistic observations on
a sound footing).
By definition, all students would have to cover
the core curriculum regardless of their entry point.
Depending on the entry point and the other courses
taken, different amounts of the core curriculum
would already be covered. For students with
minimal preparation, it might actually take more
than one course to cover the core curriculum.
4.2 Bridging Studies
One approach to preparation, especially suitable at
the graduate level, is to mandate bridging studies
for students who are not adequately prepared for the
introductory course. This could range from an indi-
vidual program of preparatory readings, to a sum-
mer intensive course, to a full semester course (e.g.
auditing a first or second year undergraduate course
such as Introduction to Language or Algorithms and
Data Structures).
It is crucial to take seriously the fact that some
students may be learning to program for the first
time in their lives. Apart from learning the syntax
of a particular programming language, they need to
learn a new and quite foreign algorithmic approach
to problem solving. Students often report that they
understand the language constructs and follow the
examples provided by the instructor, but find they
are unable to write new programs from scratch.
32
Programming First:
skills and problem-solving 
focus, with CL for motivation, 
illustrations, and applications
Text Processing First:
application focus, token-
ization, tagging, evaluation, 
language engineering
Linguistics First:
syntax, semantics, morph-
ology, with CL for testing a 
theory, exploring corpora
Algorithms First:
CL algorithms or CL as 
application for an allied field 
(e.g. AI, IR, ML, DB, HCI)
Language Computing
Application
Theory
Figure 2: Approaches to Teaching NLP
This accords with the finding that the way in
which programmers understand or write programs
differs greatly between the novice and the expert
(Lister et al, 2006). The issue is independent of
the computational linguistics context, and fits the
more general pattern that students completing an
introductory programming course do not perform as
well as expected (McCracken et al, 2001).
Bridging studies can also overlap with the course
itself, as already indicated in Figure 1. For example,
in the first week of classes one could run a quiz that
identifies students who are not sufficiently prepared
for the programming component of the course. Such
a quiz could include a diagnostic non-programming
task, like articulating the search process for looking
up a name in a telephone book, which is a predictor
of programming skill (Simon et al, 2006). Early
intervention could include extra support, readings,
classes, and so on. Some students could be alerted
to the fact that they will find the course very chal-
lenging. Some students in this category may opt
to switch to a less demanding course, which might
actually be the best outcome for all concerned.
4.3 Organizational Models
Linguistics Model: A natural way to structure the
computational linguistics curriculum is to adopt
organizational patterns from linguistics courses.
This could involve progression up through the
linguistic levels from phonology to discourse, or
a focus on the analysis of a particular language or
language family, the implementation of a particular
linguistic theory, or skills development in corpus
linguistics or field methods. In this way, content can
be packaged to meet local needs, while retaining
latitude to enter and exit the core body of knowledge
in computational linguistics.
Computer Science Model: The curriculum
could adopt organizational patterns from other
computer science courses. This could involve
progression through families of algorithms, or
navigating the processing pipeline of speech
understanding, or exploring the pieces of a
multi-component system (e.g. question answering).
As with the linguistics model, the course would be
badged to appeal to students in the local context,
while covering the core body of knowledge in
computational linguistics.
Vocational Model: In some contexts, established
theoretical courses dominate, and there is room to
promote a course that is focussed on building pro-
gramming skills in a new language or for some new
application area. This may result in a popular elec-
tive that gives students a readily marketable skill.4
This approach may also work at the secondary level
in the form of an after-school club. The course is
structured according to the features of a particular
programming language, but examples and projects
on text processing succeed in covering the core body
4The author found this approach to be successful in the case
of a database theory course, in which a semester project on
building a web database using PHP and MySQL added signifi-
cant appeal to an otherwise dry subject.
33
of knowledge in computational linguistics.
Dialectic Model: As discussed above, a major
goal for any curriculum is to take students from one
of the entry points in Figure 2 into the core body of
knowledge. One approach is to consider transitions
to topics covered in one of the other entry points:
the entry point is a familiar topic, but from there the
curriculum goes across to the other side, attempt-
ing to span the divide between computer science
and linguistics. Thus, a computational linguistics
curriculum for a computer science audience could
begin with algorithms (bottom-left) before applying
these to a range of problems in text processing (top-
left) only to discover that richer sources of linguistic
knowledge were required (bottom-right). Similarly
a curriculum for a linguistics audience could begin
with programming (top-right), then seek to apply
these skills to corpus processing for a particular lin-
guistic domain (bottom-left).
This last approach to the curriculum criss-crosses
the divide between linguistics and computer science.
Done well, it will establish a dialectic between the
two fields, one in which students reach a mature
understanding of the contrasting methodologies
and ideologies that exist within computational
linguistics including: philosophical assumptions
(e.g. rationalism vs empiricism); the measurement
of success (e.g. formal evaluation vs linguistic
explanation); and the role of observation (e.g.
a single datum as a valuable nugget vs massive
datasets as ore to be refined).
5 Conclusion
A core body of knowledge is presumed background
to just about any communication within the field
of computational linguistics, spanning terminology,
methodology, and ideology. Consensus on this body
of knowledge would serve to underpin a diverse
range of introductory curricula, ensuring they cover
the core without imposing much restriction on the
details of any particular course. Curricula beginning
from four very different starting points can progress
towards this common core, and thence to specialised
topics that maximise the local appeal of the course
and its function of attracting newcomers into the
field of computational linguistics.
There is enough flexibility in the curriculum of
most existing introductory computational linguis-
tics courses to accommodate a core body of knowl-
edge, regardless of the aspirations of students or the
research interests of an instructor. If the introductory
course is part of a sequence of courses, a larger body
of knowledge is in view and there will be scope for
switching content into and out of the first course. If
the introductory course stands alone as an elective
that leads to no other courses, there will also be
scope for adding or removing content.
The preliminary discussion of this paper leaves
many areas open for discussion and exploration.
The analyses and recommendations remain at the
level of folk pedagogy and need to be established
objectively. The various pathways have only been
described schematically, and still need to be fleshed
out into complete syllabuses, down to the level of
week-by-week topics. Support for skill development
is crucial, especially in the case of students learn-
ing to program for the first time. Finally, obsta-
cles to conceptual learning and skill development
need to be investigated systematically, with the help
of more sophisticated and nuanced approaches to
assessment.
Acknowledgments
The experiences and ideas discussed in this paper
have arisen during my computational linguistics
teaching at the Universities of Edinburgh,
Pennsylvania and Melbourne. I?m indebted to
several co-teachers who have accompanied me on
my journey into teaching computational linguistics,
including Edward Loper, Ewan Klein, Baden
Hughes, and Selina Dennis. I am also grateful
to many students who have willingly participated
in my explorations of ways to bridge the divide
between linguistics and computer science over the
past decade. This paper has benefitted from the
feedback of several anonymous reviewers.
34
References
ACM. 2001. Computing Curricula 2001: Computer Sci-
ence Volume. Association for Computing Machinery.
http://www.sigcse.org/cc2001/.
Steven Bird and James Curran. 2006. Building a
search engine to drive problem-based learning. In
Proceedings of the Eleventh Annual Conference on
Innovation and Technology in Computer Science Edu-
cation. http://eprints.unimelb.edu.au/
archive/00001618/.
Steven Bird, Ewan Klein, and Edward Loper. 2008a.
Natural Language Processing in Python. http://
nltk.org/book.html.
Steven Bird, Ewan Klein, Edward Loper, and Jason
Baldridge. 2008b. Multidisciplinary instruction with
the Natural Language Toolkit. In Proceedings of the
Third Workshop on Issues in Teaching Computational
Linguistics. Association for Computational Linguis-
tics.
Ronald Cole, editor. 1997. Survey of the State of the Art
in Human Language Technology. Studies in Natural
Language Processing. Cambridge University Press.
Raymond Lister, Beth Simon, Errol Thompson, Jacque-
line L. Whalley, and Christine Prasad. 2006. Not
seeing the forest for the trees: novice programmers
and the SOLO taxonomy. In Proceedings of the 11th
Annual SIGCSE Conference on Innovation and Tech-
nology in Computer Science Education, pages 118?
122.
Michael McCracken, Vicki Almstrum, Danny Diaz,
Mark Guzdial, Dianne Hagan, Yifat Ben-David
Kolikant, Cary Laxer, Lynda Thomas, Ian Utting, and
Tadeusz Wilusz. 2001. A multi-national, multi-
institutional study of assessment of programming
skills of first-year CS students. SIGCSE Bulletin,
33:125?180.
Jean Mulder, Kate Burridge, and Caroline Thomas.
2001. Macmillan English Language: VCE Units 1 and
2. Melbourne: Macmillan Education Australia.
Simon Simon, Quintin Cutts, Sally Fincher, Patricia
Haden, Anthony Robins, Ken Sutton, Bob Baker, Ilona
Box, Michael de Raadt, John Hamer, Margaret Hamil-
ton, Raymond Lister, Marian Petre, Denise Tolhurst,
and Jodi Tutty. 2006. The ability to articulate strategy
as a predictor of programming skill. In Proceedings of
the 8th Australian Conference on Computing Educa-
tion, pages 181?188. Australian Computer Society.
Thomas W. Stewart and Nathan Vaillette, editors. 2008.
Language Files: Materials for an Introduction to Lan-
guage and Linguistics. Ohio State University Press.
Hans Uszkoreit, Brigitte Jo?rg, and Gregor Erbach. 2003.
An ontology-based knowledge portal for language
technology. In Proceedings of ENABLER/ELSNET
Workshop ?International Roadmap for Language
Resources?.
VCAA. 2006. English Language: Victorian
Certicate of Education Study Design. Victorian
Curriculum and Assessment Authority. http:
//www.vcaa.vic.edu.au/vce/studies/
englishlanguage/englangindex.htm%l.
Appendix: Selected Topics from ACM CS
Body of Knowledge Related to
Computational Linguistics
DS. Discrete Structures
DS1. Functions, relations and sets
DS2. Basic logic
DS5. Graphs and trees
DS6. Discrete probability
PF. Programming Fundamentals
PF1. Fundamental programming constructs
PF2. Algorithms and problem solving
PF3. Fundamental data structures
PF4. Recursion
AL. Algorithms and Complexity
AL1. Basic algorithmic analysis
AL2. Algorithmic strategies
IS. Intelligent Systems
IS1. Fundamental issues in intelligent systems
IS2. Search and constraint satisfaction
IS3. Knowledge representation and reasoning
IS7. (Natural language processing)
IM. Information Management
IM1. Information models and systems
IM3. Data modeling
SP. Social and Professional Issues
SP4. Professional and ethical responsibilities
SP5. Risks and liabilities of computer-based systems
SE. Software Engineering
SE1. Software design
SE2. Using application programming interfaces
SE9. Component-based computing
35
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62?70,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multidisciplinary Instruction with the Natural Language Toolkit
Steven Bird
Department of Computer Science
University of Melbourne
sb@csse.unimelb.edu.au
Ewan Klein
School of Informatics
University of Edinburgh
ewan@inf.ed.ac.uk
Edward Loper
Computer and Information Science
University of Pennsylvania
edloper@gradient.cis.upenn.edu
Jason Baldridge
Department of Linguistics
University of Texas at Austin
jbaldrid@mail.utexas.edu
Abstract
The Natural Language Toolkit (NLTK) is
widely used for teaching natural language
processing to students majoring in linguistics
or computer science. This paper describes
the design of NLTK, and reports on how
it has been used effectively in classes that
involve different mixes of linguistics and
computer science students. We focus on three
key issues: getting started with a course,
delivering interactive demonstrations in the
classroom, and organizing assignments and
projects. In each case, we report on practical
experience and make recommendations on
how to use NLTK to maximum effect.
1 Introduction
It is relatively easy to teach natural language pro-
cessing (NLP) in a single-disciplinary mode to a uni-
form cohort of students. Linguists can be taught to
program, leading to projects where students manip-
ulate their own linguistic data. Computer scientists
can be taught methods for automatic text processing,
leading to projects on text mining and chatbots. Yet
these approaches have almost nothing in common,
and it is a stretch to call either of these NLP: more
apt titles for such courses might be ?linguistic data
management? and ?text technologies.?
The Natural Language Toolkit, or NLTK, was
developed to give a broad range of students access
to the core knowledge and skills of NLP (Loper
and Bird, 2002). In particular, NLTK makes it
feasible to run a course that covers a substantial
amount of theory and practice with an audience
consisting of both linguists and computer scientists.
NLTK is a suite of Python modules distributed
under the GPL open source license via nltk.org.
NLTK comes with a large collection of corpora,
extensive documentation, and hundreds of exercises,
making NLTK unique in providing a comprehensive
framework for students to develop a computational
understanding of language. NLTK?s code base of
100,000 lines of Python code includes support
for corpus access, tokenizing, stemming, tagging,
chunking, parsing, clustering, classification,
language modeling, semantic interpretation,
unification, and much else besides. As a measure of
its impact, NLTK has been used in over 60 university
courses in 20 countries, listed on the NLTK website.
Since its inception in 2001, NLTK has undergone
considerable evolution, based on the experience
gained by teaching courses at several universities,
and based on feedback from many teachers and
students.1 Over this period, a series of practical
online tutorials about NLTK has grown up into a
comprehensive online book (Bird et al, 2008). The
book has been designed to stay in lock-step with the
NLTK library, and is intended to facilitate ?active
learning? (Bonwell and Eison, 1991).
This paper describes the main features of
NLTK, and reports on how it has been used
effectively in classes that involve a combination
of linguists and computer scientists. First we
discuss aspects of the design of the toolkit that
1(Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst,
2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005;
Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk,
2008)
62
arose from our need to teach computational
linguistics to a multidisciplinary audience (?2). The
following sections cover three distinct challenges:
getting started with a course (?3); interactive
demonstrations (?4); and organizing assignments
and projects (?5).
2 Design Decisions Affecting Teaching
2.1 Python
We chose Python2 as the implementation language
for NLTK because it has a shallow learning curve, its
syntax and semantics are transparent, and it has good
string-handling functionality. As an interpreted
language, Python facilitates interactive exploration.
As an object-oriented language, Python permits
data and methods to be encapsulated and re-used
easily. Python comes with an extensive standard
library, including tools for graphical programming
and numerical processing, which means it can be
used for a wide range of non-trivial applications.
Python is ideal in a context serving newcomers and
experienced programmers (Shannon, 2003).
We have taken the step of incorporating a detailed
introduction to Python programming in the NLTK
book, taking care to motivate programming con-
structs with linguistic examples. Extensive feedback
from students has been humbling, and revealed that
for students with no prior programming experience,
it is almost impossible to over-explain. Despite the
difficulty of providing a self-contained introduction
to Python for linguists, we nevertheless have also
had very positive feedback, and in combination with
the teaching techniques described below, have man-
aged to bring a large group of non-programmer stu-
dents rapidly to a point where they could carry out
interesting and useful exercises in text processing.
In addition to the NLTK book, the code in the
NLTK core is richly documented, using Python doc-
strings and Epydoc3 support for API documenta-
tion.4 Access to the code documentation is available
using the Python help() command at the interac-
tive prompt, and this can be especially useful for
checking the parameters and return type of func-
tions.
2http://www.python.org/
3http://epydoc.sourceforge.net/
4http://nltk.org/doc/api/
Other Python libraries are useful in the NLP con-
text: NumPy provides optimized support for linear
algebra and sparse arrays (NumPy, 2008) and PyLab
provides sophisticated facilities for scientific visual-
ization (Matplotlib, 2008).
2.2 Coding Requirements
As discussed in Loper & Bird (2002), the priorities
for NLTK code focus on its teaching role. When code
is readable, a student who doesn?t understand the
maths of HMMs, smoothing, and so on may benefit
from looking at how an algorithm is implemented.
Thus consistency, simplicity, modularity are all vital
features of NLTK code. A similar importance is
placed on extensibility, since this helps to ensure that
the code grows as a coherent whole, rather than by
unpredictable and haphazard additions.
By contrast, although efficiency cannot be
ignored, it has always taken second place to
simplicity and clarity of coding. In a similar vein,
we have tried to avoid clever programming tricks,
since these typically hinder intelligibility of the
code. Finally, comprehensiveness of coverage has
never been an overriding concern of NLTK; this
leaves open many possibilities for student projects
and community involvement.
2.3 Naming
One issue which has absorbed a considerable
amount of attention is the naming of user-oriented
functions in NLTK. To a large extent, the system of
naming is the user interface to the toolkit, and it is
important that users should be able to guess what
action might be performed by a given function.
Consequently, naming conventions need to be
consistent and semantically transparent. At the same
time, there is a countervailing pressure for relatively
succinct names, since excessive verbosity can also
hinder comprehension and usability. An additional
complication is that adopting an object-oriented
style of programming may be well-motivated for
a number of reasons but nevertheless baffling to
the linguist student. For example, although it is
perfectly respectable to invoke an instance method
WordPunctTokenizer().tokenize(text)
(for some input string text), a simpler version is
also provided: wordpunct tokenize(text).
63
2.4 Corpus Access
The scope of exercises and projects that students
can perform is greatly increased by the inclusion
of a large collection of corpora, along with easy-to-
use corpus readers. This collection, which currently
stands at 45 corpora, includes parsed, POS-tagged,
plain text, categorized text, and lexicons.5
In designing the corpus readers, we emphasized
simplicity, consistency, and efficiency. Corpus
objects, such as nltk.corpus.brown and
nltk.corpus.treebank, define common
methods for reading the corpus contents, abstracting
away from idiosyncratic file formats to provide a
uniform interface. See Figure 1 for an example of
accessing POS-tagged data from different tagged
and parsed corpora.
The corpus objects provide methods for loading
corpus contents in various ways. Common meth-
ods include: raw(), for the raw contents of the
corpus; words(), for a list of tokenized words;
sents(), for the same list grouped into sentences;
tagged words(), for a list of (word, tag) pairs;
tagged sents(), for the same list grouped into
sentences; and parsed sents(), for a list of parse
trees. Optional parameters can be used to restrict
what portion of the corpus is returned, e.g., a partic-
ular section, or an individual corpus file.
Most corpus reader methods return a corpus view
which acts as a list of text objects, but maintains
responsiveness and memory efficiency by only load-
ing items from the file on an as-needed basis. Thus,
when we print a corpus view we only load the first
block of the corpus into memory, but when we pro-
cess this object we load the whole corpus:
>>> nltk.corpus.alpino.words()
[?De?, ?verzekeringsmaatschappijen?,
?verhelen?, ...]
>>> len(nltk.corpus.alpino.words())
139820
2.5 Accessing Shoebox Files
NLTK provides functionality for working with
?Shoebox? (or ?Toolbox?) data (Robinson et
al., 2007). Shoebox is a system used by many
documentary linguists to produce lexicons and
interlinear glossed text. The ability to work
5http://nltk.org/corpora.html
straightforwardly with Shoebox data has created a
new incentive for linguists to learn how to program.
As an example, in the Linguistics Department at
the University of Texas at Austin, a course has been
offered on Python programming and working with
corpora,6 but so far uptake from the target audience
of core linguistics students has been low. They usu-
ally have practical computational needs and many of
them are intimidated by the very idea of program-
ming. We believe that the appeal of this course can
be enhanced by designing a significant component
with the goal of helping documentary linguistics stu-
dents take control of their own Shoebox data. This
will give them skills that are useful for their research
and also transferable to other activities. Although
the NLTK Shoebox functionality was not originally
designed with instruction in mind, its relevance to
students of documentary linguistics is highly fortu-
itous and may prove appealing for similar linguistics
departments.
3 Getting Started
NLP is usually only available as an elective course,
and students will vote with their feet after attending
one or two classes. This initial period is important
for attracting and retaining students. In particular,
students need to get a sense of the richness of lan-
guage in general, and NLP in particular, while gain-
ing a realistic impression of what will be accom-
plished during the course and what skills they will
have by the end. During this time when rapport
needs to be rapidly established, it is easy for instruc-
tors to alienate students through the use of linguistic
or computational concepts and terminology that are
foreign to students, or to bore students by getting
bogged down in defining terms like ?noun phrase?
or ?function? which are basic to one audience and
new for the other. Thus, we believe it is crucial
for instructors to understand and shape the student?s
expectations, and to get off to a good start. The best
overall strategy that we have found is to use succinct
nuggets of NLTK code to stimulate students? interest
in both data and processing techniques.
6http://comp.ling.utexas.edu/courses/
2007/corpora07/
64
>>> nltk.corpus.treebank.tagged_words()
[(?Pierre?, ?NNP?), (?Vinken?, ?NNP?), (?,?, ?,?), ...]
>>> nltk.corpus.brown.tagged_words()
[(?The?, ?AT?), (?Fulton?, ?NP-TL?), ...]
>>> nltk.corpus.floresta.tagged_words()
[(?Um?, ?>N+art?), (?revivalismo?, ?H+n?), ...]
>>> nltk.corpus.cess_esp.tagged_words()
[(?El?, ?da0ms0?), (?grupo?, ?ncms000?), ...]
>>> nltk.corpus.alpino.tagged_words()
[(?De?, ?det?), (?verzekeringsmaatschappijen?, ?noun?), ...]
Figure 1: Accessing Different Corpora via a Uniform Interface
3.1 Student Expectations
Computer science students come to NLP expecting
to learn about NLP algorithms and data structures.
They typically have enough mathematical prepara-
tion to be confident in playing with abstract for-
mal systems (including systems of linguistic rules).
Moreover, they are already proficient in multiple
programming languages, and have little difficulty in
learning NLP algorithms by reading and manipulat-
ing the implementations provided with NLTK. At the
same time, they tend to be unfamiliar with the termi-
nology and concepts that linguists take for granted,
and may struggle to come up with reasonable lin-
guistic analyses of data.
Linguistics students, on the other hand, are
interested in understanding NLP algorithms and
data structures only insofar as it helps them to
use computational tools to perform analytic tasks
from ?core linguistics,? e.g. writing a set of CFG
productions to parse some sentences, or plugging
together NLP components in order to derive the
subcategorization requirements of verbs in a corpus.
They are usually not interested in reading significant
chunks of code; it isn?t what they care about and
they probably lack the confidence to poke around in
source files.
In a nutshell, the computer science students typ-
ically want to analyze the tools and synthesize new
implementations, while the linguists typically want
to use the tools to analyze language and synthe-
size new theories. There is a risk that the former
group never really gets to grips with natural lan-
guage, while the latter group never really gets to
grips with processing. Instead, computer science
students need to learn that NLP is not just an applica-
tion of techniques from formal language theory and
compiler construction, and linguistics students need
to understand that NLP is not just computer-based
housekeeping and a solution to the shortcomings of
office productivity software for managing their data.
In many courses, linguistics students or computer
science students will dominate the class numeri-
cally, simply because the course is only listed in
one department. In such cases it is usually enough
to provide additional support in the form of some
extra readings, tutorials, and exercises in the open-
ing stages of the course. In other cases, e.g. courses
we have taught at the universities of Edinburgh, Mel-
bourne, Pennsylvania, and Texas-Austin or in sum-
mer intensive programs in several countries, there is
more of an even split, and the challenge of serving
both cohorts of students becomes acute. It helps to
address this issue head-on, with an early discussion
of the goals of the course.
3.2 Articulating the Goals
Despite an instructor?s efforts to add a cross-
disciplinary angle, students easily ?revert to
type.? The pressure of assessment encourages
students to emphasize what they do well. Students?
desire to understand what is expected of them
encourages instructors to stick to familiar
assessment instruments. As a consequence,
the path of least resistance is for students to
remain firmly monolingual in their own discipline,
while acquiring a smattering of words from a
foreign language, at a level we might call ?survival
linguistics? or ?survival computer science.? If they
ever get to work in a multidisciplinary team they are
65
likely only to play a type-cast role.
Asking computer science students to write their
first essay in years, or asking linguistics students
to write their first ever program, leads to stressed
students who complain that they don?t know what
is expected of them. Nevertheless, students need
to confront the challenge of becoming bilingual, of
working hard to learn the basics of another disci-
pline. In parallel, instructors need to confront the
challenge of synthesizing material from linguistics
and computer science into a coherent whole, and
devising effective methods for teaching, learning,
and assessment.
3.3 Entry Points
It is possible to identify several distinct pathways
into the field of Computational Linguistics. Bird
(2008) identifies four; each of these are supported
by NLTK, as detailed below:
Text Processing First: NLTK supports variety of
approaches to tokenization, tagging, evaluation, and
language engineering more generally.
Programming First: NLTK is based on Python
and the documentation teaches the language and
provides many examples and exercises to test and
reinforce student learning.
Linguistics First: Here, students come with a
grounding in one or more areas of linguistics, and
focus on computational approaches to that area by
working with the relevant chapter of the NLTK book
in conjunction with learning how to program.
Algorithms First: Here, students come with a
grounding in one or more areas of computer sci-
ence, and can use, test and extend NLTK?S reference
implementations of standard NLP algorithms.
3.4 The First Lecture
It is important that the first lecture is effective at
motivating and exemplifying NLP to an audience
of computer science and linguistics students. They
need to get an accurate sense of the interesting
conceptual and technical challenges awaiting them.
Fortunately, the task is made easier by the simple
fact that language technologies, and language itself,
are intrinsically interesting and appealing to a wide
audience. Several opening topics appear to work
particularly well:
The holy grail: A long term challenge,
mythologized in science fiction movies, is to
build machines that understand human language.
Current technologies that exhibit some basic level
of natural language understanding include spoken
dialogue systems, question answering systems,
summarization systems, and machine translation
systems. These can be demonstrated in class
without too much difficulty. The Turing test is a
linguistic test, easily understood by all students, and
which helps the computer science students to see
NLP in relation to the field of Artificial Intelligence.
The evolution of programming languages has
brought them closer to natural language, helping
students see the essentially linguistic purpose of
this central development in computer science.
The corresponding holy grail in linguistics is full
understanding of the human language faculty;
writing programs and building machines surely
informs this quest too.
The riches of language: It is easy to find
examples of the creative richness of language in its
myriad uses. However, linguists will understand
that language contains hidden riches that can only
be uncovered by careful analysis of large quantities
of linguistically annotated data, work that benefits
from suitable computational tools. Moreover, the
computational needs for exploratory linguistic
research often go beyond the capabilities of the
current tools. Computer scientists will appreciate
the cognate problem of extracting information from
the web, and the economic riches associated with
state-of-the-art text mining technologies.
Formal approaches to language: Computer sci-
ence and linguistics have a shared history in the area
of philosophical logic and formal language theory.
Whether the language is natural or artificial, com-
puter scientists and linguists use similar logical for-
malisms for investigating the formal semantics of
languages, similar grammar formalisms for model-
ing the syntax of languages, and similar finite-state
methods for manipulating text. Both rely on the
recursive, compositional nature of natural and arti-
ficial languages.
3.5 First Assignment
The first coursework assignment can be a significant
step forwards in helping students get to grips with
66
the material, and is best given out early, perhaps
even in week 1. We have found it advisable for
this assignment to include both programming and
linguistics content. One example is to ask students
to carry out NP chunking of some data (e.g. a section
of the Brown Corpus). The nltk.RegexpParser
class is initialized with a set of chunking rules
expressed in a simple, regular expression-oriented
syntax, and the resulting chunk parser can be run
over POS-tagged input text. Given a Gold Standard
test set like the CoNLL-2000 data,7 precision
and recall of the chunk grammar can be easily
determined. Thus, if students are given an existing,
incomplete set of rules as their starting point, they
just have to modify and test their rules.
There are distinctive outcomes for each set of stu-
dents: linguistics students learn to write grammar
fragments that respect the literal-minded needs of
the computer, and also come to appreciate the noisi-
ness of typical NLP corpora (including automatically
annotated corpora like CoNLL-2000). Computer
science students become more familiar with parts
of speech and with typical syntactic structures in
English. Both groups learn the importance of formal
evaluation using precision and recall.
4 Interactive Demonstrations
4.1 Python Demonstrations
Python fosters a highly interactive style of teaching.
It is quite natural to build up moderately complex
programs in front of a class, with the less confi-
dent students transcribing it into a Python session
on their laptop to satisfy themselves it works (but
not necessarily understanding everything they enter
first time), while the stronger students quickly grasp
the theoretical concepts and algorithms. While both
groups can be served by the same presentation, they
tend to ask quite different questions. However, this
is addressed by dividing them into smaller clusters
and having teaching assistants visit them separately
to discuss issues arising from the content.
The NLTK book contains many examples, and
the instructor can present an interactive lecture that
includes running these examples and experiment-
ing with them in response to student questions. In
7http://www.cnts.ua.ac.be/conll2000/
chunking/
early classes, the focus will probably be on learning
Python. In later classes, the driver for such interac-
tive lessons can be an externally-motivated empiri-
cal or theoretical question.
As a practical matter, it is important to consider
low-level issues that may get in the way of students?
ability to capture the material covered in interactive
Python sessions. These include choice of appropri-
ate font size for screen display, avoiding the prob-
lem of output scrolling the command out of view,
and distributing a log of the instructor?s interactive
session for students to study in their own time.
4.2 NLTK Demonstrations
A significant fraction of any NLP syllabus covers
fundamental data structures and algorithms. These
are usually taught with the help of formal notations
and complex diagrams. Large trees and charts are
copied onto the board and edited in tedious slow
motion, or laboriously prepared for presentation
slides. It is more effective to use live demonstrations
in which those diagrams are generated and updated
automatically. NLTK provides interactive graphical
user interfaces, making it possible to view program
state and to study program execution step-by-step.
Most NLTK components have a demonstration
mode, and will perform an interesting task without
requiring any special input from the user. It is
even possible to make minor modifications to
programs in response to ?what if? questions. In this
way, students learn the mechanics of NLP quickly,
gain deeper insights into the data structures and
algorithms, and acquire new problem-solving skills.
An example of a particularly effective set
of demonstrations are those for shift-reduce
and recursive descent parsing. These make
the difference between the algorithms glaringly
obvious. More importantly, students get a concrete
sense of many issues that affect the design of
algorithms for tasks like parsing. The partial
analysis constructed by the recursive descent
parser bobs up and down as it steps forward and
backtracks, and students often go wide-eyed as the
parser retraces its steps and does ?dumb? things
like expanding N to man when it has already
tried the rule unsuccessfully (but is now trying
to match a bare NP rather than an NP with a PP
modifier). Linguistics students who are extremely
67
knowledgeable about context-free grammars and
thus understand the representations gain a new
appreciation for just how naive an algorithm can be.
This helps students grasp the need for techniques
like dynamic programming and motivates them to
learn how they can be used to solve such problems
much more efficiently.
Another highly useful aspect of NLTK is the abil-
ity to define a context-free grammar using a sim-
ple format and to display tree structures graphically.
This can be used to teach context-free grammars
interactively, where the instructor and the students
develop a grammar from scratch and check its cov-
erage against a testbed of grammatical and ungram-
matical sentences. Because it is so easy to modify
the grammar and check its behavior, students readily
participate and suggest various solutions. When the
grammar produces an analysis for an ungrammatical
sentence in the testbed, the tree structure can be dis-
played graphically and inspected to see what went
wrong. Conversely, the parse chart can be inspected
to see where the grammar failed on grammatical sen-
tences.
NLTK?s easy access to many corpora greatly facil-
itates classroom instruction. It is straightforward to
pull in different sections of corpora and build pro-
grams in class for many different tasks. This not
only makes it easier to experiment with ideas on the
fly, but also allows students to replicate the exer-
cises outside of class. Graphical displays that show
the dispersion of terms throughout a text also give
students excellent examples of how a few simple
statistics collected from a corpus can provide useful
and interesting views on a text?including seeing the
frequency with which various characters appear in a
novel. This can in turn be related to other resources
like Google Trends, which shows the frequency with
which a term has been referenced in news reports or
been used in search terms over several years.
5 Exercises, Assignments and Projects
5.1 Exercises
Copious exercises are provided with the NLTK book;
these have been graded for difficulty relative to the
concepts covered in the preceding sections of the
book. Exercises have the tremendous advantage of
building on the NLTK infrastructure, both code and
documentation. The exercises are intended to be
suitable both for self-paced learning and in formally
assigned coursework.
A mixed class of linguistics and computer sci-
ence students will have a diverse range of program-
ming experience, and students with no programming
experience will typically have different aptitudes for
programming (Barker and Unger, 1983; Caspersen
et al, 2007). A course which forces all students
to progress at the same rate will be too difficult for
some, and too dull for others, and will risk alien-
ating many students. Thus, course materials need
to accommodate self-paced learning. An effective
way to do this is to provide students with contexts
in which they can test and extend their knowledge at
their own rate.
One such context is provided by lecture or lab-
oratory sessions in which students have a machine
in front of them (or one between two), and where
there is time to work through a series of exercises to
consolidate what has just been taught from the front,
or read from a chapter of the book. When this can be
done at regular intervals, it is easier for students to
know which part of the materials to re-read. It also
encourages them to get into the habit of checking
their understanding of a concept by writing code.
When exercises are graded for difficulty, it is
easier for students to understand how much effort
is expected, and whether they even have time to
attempt an exercise. Graded exercises are also good
for supporting self-evaluation. If a student takes
20 minutes to write a solution, they also need to
have some idea of whether this was an appropriate
amount of time.
The exercises are also highly adaptable. It is com-
mon for instructors to take them as a starting point
in building homework assignments that are tailored
to their own students. Some instructors prefer to
include exercises that do not allow students to take
advantage of built-in NLTK functionality, e.g. using
a Python dictionary to count word frequencies in the
Brown corpus rather than NLTK?s FreqDist (see
Figure 2). This is an important part of building
facility with general text processing in Python, since
eventually students will have to work outside of
the NLTK sandbox. Nonetheless, students often use
NLTK functionality as part of their solutions, e.g.,
for managing frequencies and distributions. Again,
68
nltk.FreqDist(nltk.corpus.brown.words())
fd = nltk.FreqDist()
for filename in corpus_files:
text = open(filename).read()
for w in nltk.wordpunct_tokenize(text):
fd.inc(w)
counts = {}
for w in nltk.corpus.brown.words():
if w not in counts:
counts[w] = 0
counts[w] += 1
Figure 2: Three Ways to Build up a Frequency Distribu-
tion of Words in the Brown Corpus
this flexibility is a good thing: students learn to
work with resources they know how to use, and can
branch out to new exercises from that basis. When
course content includes discussion of Unix com-
mand line utilities for text processing, students can
furthermore gain a better appreciation of the pros
and cons of writing their own scripts versus using
an appropriate Unix pipeline.
5.2 Assignments
NLTK supports assignments of varying difficulty and
scope: experimenting with existing components to
see what happens for different inputs or parameter
settings; modifying existing components and
creating systems using existing components;
leveraging NLTK?s extensible architecture by
developing entirely new components; or employing
NLTK?s interfaces to other toolkits such as Weka
(Witten and Frank, 2005) and Prover9 (McCune,
2008).
5.3 Projects
Group projects involving a mixture of linguists
and computer science students have an initial
appeal, assuming that each kind of student can
learn from the other. However, there?s a complex
social dynamic in such groups, one effect of which
is that the linguistics students may opt out of the
programming aspects of the task, perhaps with
view that their contribution would only hurt the
chances of achieving a good overall project mark.
It is difficult to mandate significant collaboration
across disciplinary boundaries, with the more
likely outcome being, for example, that a parser is
developed by a computer science team member,
then thrown over the wall to a linguist who will
develop an appropriate grammar.
Instead, we believe that it is generally more pro-
ductive in the context of a single-semester introduc-
tory course to have students work individually on
their own projects. Distinct projects can be devised
for students depending on their background, or stu-
dents can be given a list of project topics,8 and
offered option of self-proposing other projects.
6 Conclusion
We have argued that the distinctive features of
NLTK make it an apt vehicle for teaching NLP
to mixed audiences of linguistic and computer
science students. On the one hand, complete
novices can quickly gain confidence in their ability
to do interesting and useful things with language
processing, while the transparency and consistency
of the implementation also makes it easy for
experienced programmers to learn about natural
language and to explore more challenging tasks.
The success of this recipe is borne out by the
wide uptake of the toolkit, not only within tertiary
education but more broadly by users who just want
try their hand at NLP. We also have encouraging
results in presenting NLTK in classrooms at the
secondary level, thereby trying to inspire the
computational linguists of the future!
Finally, we believe that NLTK has gained much
by participating in the Open Source software move-
ment, specifically from the infrastructure provided
by SourceForge.net and from the invaluable
contributions of a wide range of people, including
many students.
7 Acknowledgments
We are grateful to the members of the NLTK com-
munity for their helpful feedback on the toolkit and
their many contributions. We thank the anonymous
reviewers for their feedback on an earlier version of
this paper.
8http://nltk.org/projects.html
69
References
Jason Baldridge and Katrin Erk. 2008. Teaching com-
putational linguistics to a large, diverse student body:
courses, tools, and interdepartmental interaction. In
Proceedings of the Third Workshop on Issues in Teach-
ing Computational Linguistics. Association for Com-
putational Linguistics.
Ricky Barker and E. A. Unger. 1983. A predictor for
success in an introductory programming class based
upon abstract reasoning development. ACM SIGCSE
Bulletin, 15:154?158.
Steven Bird and Edward Loper. 2004. NLTK: The Nat-
ural Language Toolkit. In Companion Volume to the
Proceedings of 42st Annual Meeting of the Association
for Computational Linguistics, pages 214?217. Asso-
ciation for Computational Linguistics.
Steven Bird, Ewan Klein, and Edward Loper. 2008.
Natural Language Processing in Python. http://
nltk.org/book.html.
Steven Bird. 2005. NLTK-Lite: Efficient scripting
for natural language processing. In 4th International
Conference on Natural Language Processing, Kanpur,
India, pages 1?8.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006
Interactive Presentation Sessions, pages 69?72, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Steven Bird. 2008. Defining a core body of knowledge
for the introductory computational linguistics curricu-
lum. In Proceedings of the Third Workshop on Issues
in Teaching Computational Linguistics. Association
for Computational Linguistics.
Charles C. Bonwell and James A. Eison. 1991. Active
Learning: Creating Excitement in the Classroom.
Washington, D.C.: Jossey-Bass.
Michael Caspersen, Kasper Larsen, and Jens Benned-
sen. 2007. Mental models and programming aptitude.
SIGCSE Bulletin, 39:206?210.
Marti Hearst. 2005. Teaching applied natural language
processing: Triumphs and tribulations. In Proceedings
of the Second ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 1?8,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Ewan Klein. 2006. Computational semantics in the Nat-
ural Language Toolkit. In Proceedings of the Aus-
tralasian Language Technology Workshop, pages 26?
33.
Elizabeth Liddy and Nancy McCracken. 2005. Hands-on
NLP for an interdisciplinary audience. In Proceedings
of the Second ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 62?
68, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: The Nat-
ural Language Toolkit. In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Computa-
tional Linguistics, pages 62?69. Association for Com-
putational Linguistics.
Edward Loper. 2004. NLTK: Building a pedagogical
toolkit in Python. In PyCon DC 2004. Python Soft-
ware Foundation.
Nitin Madnani and Bonnie Dorr. 2008. Combining
open-source with research to re-engineer a hands-on
introductory NLP course. In Proceedings of the Third
Workshop on Issues in Teaching Computational Lin-
guistics. Association for Computational Linguistics.
Nitin Madnani. 2007. Getting started on natural lan-
guage processing with Python. ACM Crossroads,
13(4).
Matplotlib. 2008. Matplotlib: Python 2D plotting
library. http://matplotlib.sourceforge.
net/.
William McCune. 2008. Prover9: Automated
theorem prover for first-order and equational logic.
http://www.cs.unm.edu/?mccune/mace4/
manual-examples.html.
NumPy. 2008. NumPy: Scientific computing with
Python. http://numpy.scipy.org/.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolkit. Language Documentation and
Conservation, 1:44?57.
Christine Shannon. 2003. Another breadth-first
approach to CS I using Python. In Proceedings of
the 34th SIGCSE Technical Symposium on Computer
Science Education, pages 248?251. ACM.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann.
70
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69?72,
Sydney, July 2006. c?2006 Association for Computational Linguistics
NLTK: The Natural Language Toolkit
Steven Bird
Department of Computer Science and Software Engineering
University of Melbourne, Victoria 3010, AUSTRALIA
Linguistic Data Consortium, University of Pennsylvania,
Philadelphia PA 19104-2653, USA
Abstract
The Natural Language Toolkit is a suite of
program modules, data sets and tutorials
supporting research and teaching in com-
putational linguistics and natural language
processing. NLTK is written in Python
and distributed under the GPL open source
license. Over the past year the toolkit has
been rewritten, simplifying many linguis-
tic data structures and taking advantage
of recent enhancements in the Python lan-
guage. This paper reports on the simpli-
fied toolkit and explains how it is used in
teaching NLP.
1 Introduction
NLTK, the Natural Language Toolkit, is a suite
of Python modules providing many NLP data
types, processing tasks, corpus samples and
readers, together with animated algorithms,
tutorials, and problem sets (Loper and Bird,
2002). Data types include tokens, tags, chunks,
trees, and feature structures. Interface definitions
and reference implementations are provided for
tokenizers, stemmers, taggers (regexp, ngram,
Brill), chunkers, parsers (recursive-descent,
shift-reduce, chart, probabilistic), clusterers, and
classifiers. Corpus samples and readers include:
Brown Corpus, CoNLL-2000 Chunking Corpus,
CMU Pronunciation Dictionary, NIST IEER
Corpus, PP Attachment Corpus, Penn Treebank,
and the SIL Shoebox corpus format.
NLTK is ideally suited to students who are
learning NLP or conducting research in NLP
or closely related areas. NLTK has been used
successfully as a teaching tool, as an individual
study tool, and as a platform for prototyping and
building research systems (Liddy and McCracken,
2005; S?tre et al, 2005).
We chose Python for its shallow learning curve,
transparent syntax, and good string-handling.
Python permits exploration via its interactive
interpreter. As an object-oriented language,
Python permits data and code to be encapsulated
and re-used easily. Python comes with an
extensive library, including tools for graphical
programming and numerical processing (Beasley,
2006).
Over the past four years the toolkit grew rapidly
and the data structures became significantly more
complex. Each new processing task added new
requirements on input and output representations.
It was not clear how to generalize tasks so they
could be applied independently of each other.
As a simple example, consider the independent
tasks of tagging and stemming, which both oper-
ate on sequences of tokens. If stemming is done
first, we lose information required for tagging. If
tagging is done first, the stemming must be able
to skip over the tags. If both are done indepen-
dently, we need to be able to align the results.
As task combinations multiply, managing the data
becomes extremely difficult.
To address this problem, NLTK 1.4 introduced
a blackboard architecture for tokens, unifying
many data types, and permitting distinct tasks
to be run independently. Unfortunately this
architecture also came with a significant overhead
for programmers, who were often forced to use
?rather awkward code structures? (Hearst, 2005).
It was clear that the re-engineering done in NLTK
1.4 unduly complicated the programmer?s task.
This paper presents a brief overview and tutorial
on a new, simplified toolkit, and describes how it
is used in teaching.
69
2 Simple Processing Tasks
2.1 Tokenization and Stemming
The following three-line program imports the
tokenize package, defines a text string, and
tokenizes the string on whitespace to create a list
of tokens. (NB. ?>>>? is Python?s interactive
prompt; ?...? is the continuation prompt.)
>>> text = ?This is a test.?
>>> list(tokenize.whitespace(text))
[?This?, ?is?, ?a?, ?test.?]
Several other tokenizers are provided. We can
stem the output of tokenization using the Porter
Stemmer as follows:
>>> text = ?stemming is exciting?
>>> tokens = tokenize.whitespace(text)
>>> porter = stem.Porter()
>>> for token in tokens:
... print porter.stem(token),
stem is excit
The corpora included with NLTK come with
corpus readers that understand the file structure
of the corpus, and load the data into Python data
structures. For example, the following code reads
part a of the Brown Corpus. It prints a list of
tuples, where each tuple consists of a word and
its tag.
>>> for sent in brown.tagged(?a?):
... print sent
[(?The?, ?at?), (?Fulton?, ?np-tl?),
(?County?, ?nn-tl?), (?Grand?, ?jj-tl?),
(?Jury?, ?nn-tl?), (?said?, ?vbd?), ...]
NLTK provides support for conditional
frequency distributions, making it easy to count
up items of interest in specified contexts. Such
information may be useful for studies in stylistics
or in text categorization.
2.2 Tagging
The simplest possible tagger assigns the same tag
to each token:
>>> my_tagger = tag.Default(?nn?)
>>> list(my_tagger.tag(tokens))
[(?John?, ?nn?), (?saw?, ?nn?),
(?3?, ?nn?), (?polar?, ?nn?),
(?bears?, ?nn?), (?.?, ?nn?)]
On its own, this will tag only 10?20% of the
tokens correctly. However, it is a reasonable tag-
ger to use as a default if a more advanced tagger
fails to determine a token?s tag.
The regular expression tagger assigns a tag to a
token according to a series of string patterns. For
instance, the following tagger assigns cd to cardi-
nal numbers, nns to words ending in the letter s,
and nn to everything else:
>>> patterns = [
... (r?\d+(.\d+)?$?, ?cd?),
... (r?\.*s$?, ?nns?),
... (r?.*?, ?nn?)]
>>> simple_tagger = tag.Regexp(patterns)
>>> list(simple_tagger.tag(tokens))
[(?John?, ?nn?), (?saw?, ?nn?),
(?3?, ?cd?), (?polar?, ?nn?),
(?bears?, ?nns?), (?.?, ?nn?)]
The tag.Unigram class implements a sim-
ple statistical tagging algorithm: for each token,
it assigns the tag that is most likely for that token.
For example, it will assign the tag jj to any occur-
rence of the word frequent, since frequent is used
as an adjective (e.g. a frequent word) more often
than it is used as a verb (e.g. I frequent this cafe).
Before a unigram tagger can be used, it must be
trained on a corpus, as shown below for the first
section of the Brown Corpus.
>>> unigram_tagger = tag.Unigram()
>>> unigram_tagger.train(brown(?a?))
Once a unigram tagger has been trained, it can
be used to tag new text. Note that it assigns
the default tag None to any token that was not
encountered during training.
>>> text = "John saw the books on the table"
>>> tokens = list(tokenize.whitespace(text))
>>> list(unigram_tagger.tag(tokens))
[(?John?, ?np?), (?saw?, ?vbd?),
(?the?, ?at?), (?books?, None),
(?on?, ?in?), (?the?, ?at?),
(?table?, None)]
We can instruct the unigram tagger to back off
to our default simple_tagger when it cannot
assign a tag itself. Now all the words are guaran-
teed to be tagged:
>>> unigram_tagger =
... tag.Unigram(backoff=simple_tagger)
>>> unigram_tagger.train(train_sents)
>>> list(unigram_tagger.tag(tokens))
[(?John?, ?np?), (?saw?, ?vbd?),
(?the?, ?at?), (?books?, ?nns?),
(?on?, ?in?), (?the?, ?at?),
(?table?, ?nn?)]
We can go on to define and train a bigram tagger,
as shown below:
>>> bigram_tagger =\
... tag.Bigram(backoff=unigram_tagger)
>>> bigram_tagger.train(brown.tagged(?a?))
We can easily evaluate this tagger against
some gold-standard tagged text, using the
tag.accuracy() function.
NLTK also includes a Brill tagger (contributed
by Christopher Maloof) and an HMM tagger (con-
tributed by Trevor Cohn).
70
3 Chunking and Parsing
Chunking is a technique for shallow syntactic
analysis of (tagged) text. Chunk data can be
loaded from files that use the common bracket or
IOB notations. We can define a regular-expression
based chunk parser for use in chunking tagged
text. NLTK also supports simple cascading of
chunk parsers. Corpus readers for chunked data
in Penn Treebank and CoNLL-2000 are provided,
along with comprehensive support for evaluation
and error analysis.
NLTK provides several parsers for context-free
phrase-structure grammars. Grammars can be
defined using a series of productions as follows:
>>> grammar = cfg.parse_grammar(???
... S -> NP VP
... VP -> V NP | V NP PP
... V -> "saw" | "ate"
... NP -> "John" | Det N | Det N PP
... Det -> "a" | "an" | "the" | "my"
... N -> "dog" | "cat" | "ball"
... PP -> P NP
... P -> "on" | "by" | "with"
... ???)
Now we can tokenize and parse a sentence with
a recursive descent parser. Note that we avoided
left-recursive productions in the above grammar,
so that this parser does not get into an infinite loop.
>>> text = "John saw a cat with my ball"
>>> sent = list(tokenize.whitespace(text))
>>> rd = parse.RecursiveDescent(grammar)
Now we apply it to our sentence, and iterate
over all the parses that it generates. Observe
that two parses are possible, due to prepositional
phrase attachment ambiguity.
>>> for p in rd.get_parse_list(sent):
... print p
(S:
(NP: ?John?)
(VP:
(V: ?saw?)
(NP:
(Det: ?a?)
(N: ?cat?)
(PP: (P: ?with?)
(NP: (Det: ?my?) (N: ?ball?))))))
(S:
(NP: ?John?)
(VP:
(V: ?saw?)
(NP: (Det: ?a?) (N: ?cat?))
(PP: (P: ?with?)
(NP: (Det: ?my?) (N: ?ball?)))))
The same sentence can be parsed using a grammar
with left-recursive productions, so long as we
use a chart parser. We can invoke NLTK?s chart
parser with a bottom-up rule-invocation strategy
with chart.ChartParse(grammar,
chart.BU STRATEGY). Tracing can be turned
on in order to display each step of the process.
NLTK also supports probabilistic context free
grammars, and provides a Viterbi-style PCFG
parser, together with a suite of bottom-up
probabilistic chart parsers.
4 Teaching with NLTK
Natural language processing is often taught within
the confines of a single-semester course, either
at advanced undergraduate level or at postgradu-
ate level. Unfortunately, it turns out to be rather
difficult to cover both the theoretical and practi-
cal sides of the subject in such a short span of
time. Some courses focus on theory to the exclu-
sion of practical exercises, and deprive students of
the challenge and excitement of writing programs
to automatically process natural language. Other
courses are simply designed to teach programming
for linguists, and do not manage to cover any sig-
nificant NLP content. NLTK was developed to
address this problem, making it feasible to cover
a substantial amount of theory and practice within
a single-semester course.
A significant fraction of any NLP course is
made up of fundamental data structures and
algorithms. These are usually taught with the
help of formal notations and complex diagrams.
Large trees and charts are copied onto the board
and edited in tedious slow motion, or laboriously
prepared for presentation slides. A more
effective method is to use live demonstrations
in which those diagrams are generated and
updated automatically. NLTK provides interactive
graphical user interfaces, making it possible
to view program state and to study program
execution step-by-step (e.g. see Figure 1).
Most NLTK components have a demonstration
mode, and will perform an interesting task without
requiring any special input from the user. It is even
possible to make minor modifications to programs
in response to ?what if? questions. In this way,
students learn the mechanics of NLP quickly,
gain deeper insights into the data structures and
algorithms, and acquire new problem-solving
skills. Since these demonstrations are distributed
with the toolkit, students can experiment on their
own with the algorithms that they have seen
presented in class.
71
Figure 1: Two Parser Demonstrations: Shift-Reduce and Recursive Descent Parsers
NLTK can be used to create student assign-
ments of varying difficulty and scope. In the sim-
plest assignments, students experiment with one of
the existing modules. Once students become more
familiar with the toolkit, they can be asked to make
minor changes or extensions to an existing module
(e.g. build a left-corner parser by modifying the
recursive descent parser). A bigger challenge is to
develop one or more new modules and integrate
them with existing modules to perform a sophis-
ticated NLP task. Here, NLTK provides a useful
starting point with its existing components and its
extensive tutorials and API documentation.
NLTK is a unique framework for teaching nat-
ural language processing. NLTK provides com-
prehensive support for a first course in NLP which
tightly couples theory and practice. Its extensive
documentation maximizes the potential for inde-
pendent learning. For more information, including
documentation, download pointers, and links to
dozens of courses that have adopted NLTK, please
see: http://nltk.sourceforge.net/ .
Acknowledgements
I am grateful to Edward Loper, co-developer of
NLTK, and to dozens of people who have con-
tributed code and provided helpful feedback.
References
Marti Hearst. 2005. Teaching applied natural language
processing: Triumphs and tribulations. In Proc 2nd
ACL Workshop on Effective Tools and Methodolo-
gies for Teaching NLP and CL, pages 1?8, ACL
Elizabeth Liddy and Nancy McCracken. 2005. Hands-
on NLP for an interdisciplinary audience. In Proc
2nd ACL Workshop on Effective Tools and Method-
ologies for Teaching NLP and CL, pages 62?68,
ACL
Edward Loper and Steven Bird. 2002. NLTK: The
Natural Language Toolkit. In Proc ACL Workshop
on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational
Linguistics, pages 62?69. ACL.
David Beasley. 2006. Python Essential Reference, 3rd
Edition. Sams.
Rune S?tre, Amund Tveit, Tonje S. Steigedal, and
Astrid L?greid. 2005. Semantic annotation of
biomedical literature using Google. In Data Min-
ing and Bioinformatics Workshop, volume 3482 of
Lecture Notes in Computer Science. Springer.
72
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1015?1024, Dublin, Ireland, August 23-29 2014.
Collecting Bilingual Audio in Remote Indigenous Communities
Steven Bird
Dept of Computing and
Information Systems,
University of Melbourne;
Linguistic Data Consortium,
University of Pennsylvania
Lauren Gawne
Department of Linguistics
and Multilingual Studies,
Nanyang Technological
University, Singapore
Katie Gelbart
School of Oriental
and African Studies,
University of London
Isaac McAlister
Department of Languages,
Literatures, and Cultures,
University of Massachusetts,
Amherst
Abstract
Most of the world?s languages are under-resourced, and most under-resourced languages lack
a writing system and literary tradition. As these languages fall out of use, we lose important
sources of data that contribute to our understanding of human language. The first, urgent step is
to collect and orally translate a large quantity of spoken language. This can be digitally archived
and later transcribed, annotated, and subjected to the full range of speech and language process-
ing tasks, at any time in future. We have been investigating a mobile application for recording
and translating unwritten languages. We visited indigenous communities in Brazil and Nepal and
taught people to use smartphones for recording spoken language and for orally interpreting it into
the national language, and collected bilingual phrase-aligned speech recordings. In spite of sev-
eral technical and social issues, we found that the technology enabled an effective workflow for
speech data collection. Based on this experience, we argue that the use of special-purpose soft-
ware on smartphones is an effective and scalable method for large-scale collection of bilingual
audio, and ultimately bilingual text, for languages spoken in remote indigenous communities.
1 Introduction
Past the top one to three hundred economically significant languages, there are few prospects for re-
sourcing the production of annotated corpora. Advances in natural language processing have relied on
such corpora ? including treebanks and wordnets ? though they are expensive to produce and depend on
substantial prior scholarship on the language. An alternative is to collect bilingual aligned text, relating
a low-resource language to a high-resource language, and then infer lexical and syntactic information
from the high-resource language via alignments (Abney and Bird, 2010; Baldwin et al., 2010; Palmer et
al., 2010; Das and Petrov, 2011).
This approach only works for written languages. Over half the world?s languages lack a literary
tradition. In some cases they have a writing system, but it is not in regular use and so these languages
remain effectively unwritten. Collecting data for unwritten languages necessarily involves speech.
w
1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9
w
10
w
11
w
12
f
1
f
2
f
3
f
4
f
5
f
6
f
7
f
8
f
9
f
10
f
11
f
12
f
13
f
14
f
15
f
16
f
17
f
18
f
19
f
20
f
21
f
22
f
23
f
24
f
25
f
26
f
27
f
1
f
2
f
5
f
6
f
7
f
8
w
3
w
2
f
9
f
10
f
11
f
15
f
16
f
22
f
23
f
12
f
13
f
14
w
1
w
6
w
5
f
17
f
18
f
19
f
20
f
21
w
8
w
7
w
9
w
10
f
24
f
25
w
12
w
11
w'
1    
w'
2    
w'
3    
w'
4    
w'
5    
w'
6    
w'
7    
w'
8    
w'
8    
w'
10    
w'
11    
w'
12
f
26
f
27
f
3
f
4
w
4
Figure 1: The Vision: phrase-aligned bilingual audio from an unwritten language to a language of wider
communication, along with extracted acoustic features and crowdsourced transcription (left); interlinear
glossed text with word segmentation, word-level glosses, and sentence-level translations (right).
This work is licensed under a Creative Commons Attribution 4.0 International Licence.
1015
While the physical isolation of these languages presents a logistical challenge, it is still possible to
collect hundreds of hours of speech using mobile devices (de Vries et al., 2014). Furthermore, there
are promising signs that natural language processing methods and speech processing methods can be
integrated (Zhang et al., 2004; Dredze et al., 2010; Vu et al., 2011; Siniscalchi et al., 2012; Lee and
Glass, 2012). Thus, the challenge is to collect substantial quantities of bilingual aligned audio, transcribe
the translations, extract phonetic features from the source language and, ultimately, produce bilingual
aligned text (see Figure 1).
We have chosen to focus on endangered languages because of the interesting and difficult challenges
that are faced in collecting data. However, the resource problem exists even for vital languages having
large speaker populations. For example, Shanghainese (Wu) is spoken by 77 million people in China,
but is almost never written down because written Chinese is based on Mandarin; Oromo is spoken by
17 million people in Ethiopia, but few of its speakers know how to write it. Such languages are collec-
tively spoken by billions, yet remain seriously under-resourced. Thus, while our focus is on endangered
languages, the approach applies to under-resourced languages in general.
Several other promising approaches to the problems raised by endangered languages are being actively
pursued in computational linguistics, however they typically focus on written language with annotations,
often with the goal of making optimal use of human expertise (Probst et al., 2002; Levin et al., 2006;
Clark et al., 2008; Palmer et al., 2010; Bender et al., 2012; Beale, 2012; Bender et al., 2013). The
research reported here is unique in its focus on securing spoken language data in a form and on a scale
that will be usable even once the languages in question are no longer spoken.
This paper explores ways that networked smartphones can be used for collecting bilingual aligned
audio. We have used a prototype Android application for collecting audio and phrase-aligned translations
(or consecutive interpretations). We took a set of phones to villages in Brazil and Nepal, and worked
with languages Temb?e, Nhengatu and Kagate. We visited at the invitation of the local communities and
collaborated closely with them in each stage of the process, including setting the goals and agreeing
on the form of dissemination, cf. (Rice, 2011). We compiled small collections of recorded texts and
translations in each language.
We describe and evaluate this novel resource-creation activity, and argue that it can be used effectively
for large-scale collection of bilingual aligned audio. This paper is organised as follows. In section 2,
we give an overview of the mobile software. The next three sections report the activities in the three
communities. We reflect on the work in section 6.
2 Mobile applications for recording and translating endangered languages
Smartphones are proliferating: they are part of the vanguard of technologies that make it into many
isolated communities. Even in the most remote villages, many people own a mobile phone, keep it on
their person, and are able to get it charged when mains electricity is unreliable or non-existent. These
phones can be inexpensive (US$100-200) and some models have sufficient audio quality to be useful for
speech data collection. With suitable software it is possible to collect metadata along with recordings,
including location, date, the identity of the speaker, and possibly some information about the content
such as the title and genre. The networking capability of a smartphone facilitates wireless sharing and
backup.
The speech collection task calls for a variety of individual contributions. The best speakers of the
language are not necessarily the best translators; they may be monolingual. Similarly, the best translators
may not be the best transcribers; they may be illiterate. Thus, for reasons of skill, not just scale, we need
to involve a whole team of people in the data collection activity. In the medium term, we assume that this
work would take place under the supervision of a linguist who provides hardware and training, and who
monitors the balance of the collection, including coverage of various discourse types, getting everything
translated, and so forth).
Aikuma is open source software that supports recording of speech directly to phone storage (Hanke
and Bird, 2013; Bird et al., 2014). Recordings are synchronized with other phones that are connected
to the same WiFi LAN, so that any user can listen to recordings made on any phone in the same local
1016
network. A user can ?like? a recording to improve its overall ranking. A user can also provide a phrase-
by-phrase spoken translation of the recording, using the interface shown in Figure 2. This functionality
is based on the protocol of ?Basic Oral Language Documentation? (Reiman, 2010; Bird, 2010).
(a) Recording a Temb?e narrative (b) Translating Temb?e into Portuguese
Figure 2: Recording and translating using the Aikuma Android app
Users press and hold the left play button to hear the next segment of audio source. They can press
it multiple times to hear the same segment over again. Once ready, they press the right record button
and speak a translation of the source. This process continues until the source has been fully translated.
It generates a second audio file that is time-aligned with the source (cf Figure 1). The app supports
playback of the source, the translation, and the source with interleaved translation.
Aikuma maintains a database of speakers and synchronizes this to the other phones along with the
recordings and titles, and keeps track of which speaker provided which recording. In this way, basic
metadata resides with the recordings, and recordings are effectively backed up several times over. If the
contents of one phone are periodically archived, then we have a permanent copy of all the recordings and
metadata from all of the phones.
We used HTC Desire C and HTC Desire 200 phones which cost US$160 each. We chose these phones
for their support of Android 4 and their recording quality. Unlike a professional audio set-up, mobile
phone audio recording includes built-in noise suppression that is optimised for near-field voice sources
and attenuates background noise. The software stores audio in uncompressed 16kHz 16-bit mono. The
quality of the audio from these phones is more than sufficient to support phonetic analysis (Bettinson,
2013). We expect these materials to be considered of archival quality in those cases where the original
recording environment was quiet and where the content itself has linguistic and cultural value.
Another advantage of smartphones compared with professional recording equipment is ease of
recharging. Many remote indigenous communities without mains electricity are still able to keep phones
charged with the help of generators and car batteries. By choosing to use mobile phones, we can piggy-
back on the existing infrastructure.
The cost and usability of smartphones relative to professional recording equipment makes it easy
to consider giving them out to people to make recordings in their own time. Apart from significantly
increasing the amount of recorded and translated material that a linguist can collect, this gives speakers
direct control over the content and context of the recordings, and it may lead to the collection of more
naturalistic materials. In some cases, speakers already own an Android phone and can simply install the
software and get started.
In the following three sections we report on our experience of using these phones with indigenous
communities in the Amazon and the Himalayas.
3 Temb
?
e, Par
?
a State, Brazil
The Temb?e language is spoken by approximately 150 people amongst a larger community numbering
about 1,500, in a group of villages in the Reserva Alto Rio Guam in the vicinity of Paragominas in the
Par?a state of Brazil. Bird, Gelbart, and McAlister spent five days in the village of Cajueiro (Akazu?yw),
1017
the gateway to several other Temb?e villages that can only be accessed by river. Like many Indian villages,
Cajueiro is laid out around a soccer field. The village was connected to the electricity grid ten years ago.
We recorded 14 texts from 8 speakers, mostly personal narratives but also a song and a dialogue. Most
texts were orally translated; some were translated twice. Of two hours of source audio, 35 minutes were
orally translated, producing an extra 25 minutes of audio.
Our visit to Cajueiro is mostly interesting for the great variety of unanticipated challenges, and how
we were still able to use the platform to collect data.
Previous contact with the Temb?e community was mediated by staff at the Goeldi Museum in Bel?em.
The Temb?e community had been discussing prospects for installing an antenna in Cajueiro to enable
an Internet connection. On arrival, the chief asked about our plans to set up Internet access, and we
explained that we were not able to do this because there was no signal for 100km. After this, the chief
lost interest in our activities and we were not able to hold a village meeting as we had hoped, in order
to discuss our work, invite participation, and demonstrate the use of the technology for recording and
translation. Instead, we could only work one-on-one.
Our first 24 hours in the village was spent on video documentation of a coming-of-age ceremony. More
elaborate versions of this ceremony had been filmed in the past, so there was minimal documentary value
in recording this event. However, it was the basis for our invitation to the village, cf. (Crowley, 2007,
80), and it enabled us to meet the whole community and to observe the limited social interaction, almost
exclusively conducted in Portuguese.
In the following days, we went around the village showing the app to people, explaining our work,
playing existing recordings in Temb?e and other languages, and trying to find fluent speakers who were
motivated to preserve Temb?e linguistic heritage. Few people claimed to be fluent and we only found six
who were willing to be recorded, all men. No women would consent to being recorded until a Temb?e
man, trained as a computer technician, learnt how to use the app and took a handset and found two female
speakers and recorded them. They were in their thirties, less confident with the language, and could only
read haltingly from a small storybook. For the fluent speakers we were able to find, the documentary
activity proceeded naturally; they easily recounted histories and gave phrase-by-phrase translations. We
prepared a selection from our recordings and made audio CDs to give away for people to play on their
personal stereos.
We experienced a variety of technical difficulties with the smartphones, none of which had been appar-
ent during lab testing. The most obvious were due to people?s unfamiliarity with smartphones. Signing
in required entering the participant?s name using a touchscreen keyboard, then selecting an ISO 639 lan-
guage code via a search interface, then taking a photo using the phone. The photo could not be taken
easily by the participant as the phones lacked a front-facing camera. Consequently, we generally took
care of these tasks on behalf of speakers. Similarly, upon completion of a recording, the participant was
prompted to enter a name for the recording, and we would reclaim the phone and enter a title after a brief
discussion with the participant about a suitable choice.
Further problems concerned the translation task. A couple of participants began to give a Portuguese
paraphrase immediately on finishing a story. Despite the obvious value of capturing an immediate para-
phrase from the same speaker, the software was not designed for this and we had no way to capture
the paraphrase as a separate audio file and link it back to the original. The thumb-controlled interface
(Figure 2b) was also slightly problematic. Often a speaker would still be holding down the play button
with his left thumb at the moment he went to press the record button with his right thumb. Sometimes,
speakers would begin to speak and then notice that playback was still continuing, and only then release
the play button. By the time they had pressed the record button again, they had already spoken a word or
two, and this speech was not captured by the app. This problem happened often enough to interfere with
the flow of the translation task. Possible solutions are to have the controls operated by a single thumb,
or else to change the behavior of the app so that the most recent thumb gesture overrides a existing but-
ton press. Several other interactional issues with the software were identified and resolved with similar
minor changes to behavior.
A final set of issues concerned dissemination. Many Indian villages are now equipped with computer
1018
rooms and have desktop machines with CD burners, though mains electricity may be intermittent, or else
depend on a generator. We were able to transfer files from the phones to a local machine using a USB
connection, though it was a slow process to identify the recordings of interest to the participants and to
compile an audio CD. Instead, we realised that any user of a phone should be able to export selected
recordings to a local folder that could be burnt to CD.
The key problem for us, however, was lack of participation. The main reason for this, we believe, was
the limited local interest in the Temb?e language. A secondary factor was the misunderstanding about our
contribution (?bringing the Internet?) and the fact that the product, a CD of stories, was not necessarily
something that the community wanted.
4 Nhengatu, Amazonas State, Brazil
The Nhengatu language is a creole spoken by 10,000 people across a wide area, including the village of
Terra Preta, 50km NW of Manaus. Nhengatu used to be the language of communication amongst Indians
from different tribes along the Rio Negro, and between Indians and non-Indians in the Brazilian Amazon.
Although most of the inhabitants of Terra Preta are ethnically Bar?e, the only indigenous language spoken
in the village is Nhengatu. Younger generations are monolingual in Portuguese. Unusually, there are
also some non-Indians living in the village. The villagers were open to receiving us, partly due to their
proximity to Manaus and the fact they were accustomed to meeting tourists and showing white people
around and selling handcrafts. Compared with Cajueiro, there was a stronger sense of community in
Terra Preta: on weekends they would have breakfast together in a communal meeting place, and agree
on community service tasks for the weekend.
We made a preliminary visit and presented our work at a public meeting. We called for a volunteer to
tell a story to the group and then invited another volunteer to provide an oral translation. Both individuals
did a perfect job even though neither one had used the software before. One of them, a former village
chief, addressed the group and explained the significance of our work. He then asked if we would help
in the preparation of a DVD. Since we did not have the necessary equipment, we offered to create a
bilingual storybook instead. They agreed, and said this could be used in their local school. We had
already intended to propose this as our contribution to the community after our experience with Temb?e,
where most people did not grasp the value of us only leaving audio recordings. A booklet would be a
natural extension to our documentary goals, and it offered to draw in the whole community including the
children who could provide illustrations.
Three weeks later, once the necessary approvals had been obtained, we arrived in Terra Preta and
launched our activities with another public meeting. At this meeting, and again at public meetings on
the following two mornings, we invited anyone who was interested to take a phone and record a story.
Sometimes a storyteller held a phone while addressing a small group (often involving children), and
recounted a folktale.
After three days, we recorded 35 texts from nine speakers (including two children), mostly folklore
and personal narratives. Most texts were orally translated. Of 2.5 hours of source audio, approximately
one hour of recordings were orally translated (some two or more times), producing an extra two hours of
audio. Seven short texts by children or directed at children were delivered in Portuguese, and we did not
translate these back into Nhengatu.
During the second half of the visit, four men who were literate in Nhengatu joined us in the task of
transcribing the stories, focussing on those that would be most interesting for inclusion in the storybook.
They worked in parallel, playing back the recordings on the phones, transcribing them on paper, then
bringing the sheets back to be typed and proof-read. This work was arduous, continuing through the heat
of the day, but they were keen to process as many stories as possible.
Two weeks after our visit, we published a small booklet of stories and translations and sent copies
back to the village, and posted a digital copy in the Internet Archive (Bird et al., 2013).
We encountered some additional technical difficulties that we had not experienced in Cajueiro. First,
a bug in the recording app which appeared on the last day caused one recording to overrun and produced
a three hour (350MB) file. After this, WiFi synchronisation was too slow to be effective, and it was
1019
necessary to perform synchronisation manually, copying the files from all phones onto a laptop, then
copying the collection back onto each phone. Second, the presence of an audience for some stories
encouraged the storyteller to speak loudly. Since speakers were holding the phone close to their mouths,
this resulted in clipped audio. Third, at the height of our intensive transcription and translation process,
we needed to keep track of the activities of several participants, and created a checklist. Finally, there
was an issue with the power supply. Unlike Cajueiro, Terra Preta is not attached to the electricity grid, but
it has a generator which is turned on for four hours every evening, and sometimes during the mornings
for brief periods. We could use this to keep the phones charged and to power the router for long enough
to synchronise the phones a couple of times each day. But the village became very noisy when power
was available, thanks to an abundance of stereo systems and power tools, and this made it difficult to get
good quality recordings during these times.
Figure 3: Transcribing a spoken translation
In spite of these problems, there were some suc-
cesses. The most notable was that participants took
no more than a minute to become adept with the
recording functionality and the thumb-controlled oral
translation functionality (Figure 2b). Second, the
availability of multiple networked recording devices
meant that we could collect materials in parallel. For
example, we could discuss a story we wanted to
record and then send several people off at the same
time to record their own versions. Then they could
synchronise their recordings and hear what each other
said. Finally, automatic synchronisation greatly facil-
itated concurrent transcription activities. We could
assign people to transcribe or translate a particular
source recording without having to keep track of device it had been recorded with: it was already avail-
able on all of the phones.
5 Kagate, Ramechhap district, Nepal
A third field test with a later version of the app was undertaken in Nepal. Kagate, known to its speakers
as Syuba, is a Tibeto-Burman language spoken by around 1,500 people in the Ramechhap district, east
of Kathmandu. Handsets with the Aikuma app were taken by Gawne and were deployed in parallel, in
the context of a project to video record traditional folk narratives and history. Twelve original recordings
were made, totalling 80 minutes. Four of these recordings were translated into Nepali, and two record-
ings were also carefully ?respoken? to aid later written transcription (Woodbury, 2003). Although the
recordings represent a more modest total than at other fieldsites, this field test demonstrates that Aikuma
can operate in conjunction with, and to the benefit of, more traditional field methods. A number of
challenges were addressed.
The first challenge was the lack of mains electricity, with the village only having a number of small
solar panels for charging mobile phones and running small lights. Much like at the Nhengatu site, mobile
phones enabled work to proceed in the absence of mains electricity. Indeed, this was greatly beneficial
because it meant that more recordings could be made without rapidly depleting the video camera battery,
which required charging at a village a one hour walk away. The lack of proximal mains electricity meant
that it was not possible to run the router and synchronise the data on each phone. As a result of this (and
participation issues discussed below) the researcher only kept two devices in use at a time, making it
easier to keep track of what was on each device. This field trip demonstrated that even without the data
synchronization feature Aikuma is still a useful fieldwork tool.
The second challenge was fostering participation. As a number of anthropologists working in re-
lated communities have observed, the centre of village life for Kagate people is the household (F?urer-
Haimendorf, 1964; Desjarlais, 1992). Relationships beyond this are negotiated through extended famil-
ial relations of reciprocity. Therefore, there were no opportunities to arrange community meetings as in
1020
Terra Preta, or even to find an individual who was an officially designated leader. As a result, much time
was spent engaging a small number of enthusiastic participants and working with them to engage other
members of the community through existing social networks. The benefit of the mobile devices was that
they could be carried about and then demonstrated to people during a lull in other activities. Because
of this portability and ease of demonstration, the mobile phones became a key part of negotiations with
all participants, even those who the community members wanted to video record. Having the handsets
meant that we could immediately show people the outcome of a recording session. Sometimes, even
after this demonstration, people were reluctant to participate in recording with video cameras or phones.
We took this as a positive sign that participants had a better level of informed consent with which to
make this choice than they otherwise would have. Many community members were reluctant to take
the phones, as even basic smartphones that we chose for their affordability are an expensive commodity
and out of the price bracket of many. A small number of people became comfortable enough to take the
phones away to work with, but would return them immediately after a specific task had been completed.
With a longer period of presence in the village it is likely more people would become more comfortable
with the process.
The final issue, like at other sites, concerned the process of saving recordings once they had been
made. Processes that are taken for granted with some audiences, like naming a recording, presume a
great deal of cultural knowledge about iconography, the layout of keyboards, and spelling conventions.
It was only on the final day that one of the more frequent participants saved a file without assistance.
Fortunately, an import feature had been built into the app, which meant that when participants returned
with files that they had not managed to save they could still be loaded into the list. While some of the
issues faced can be overcome through further refining the design, others are useful educational tools to
help familiarize participants with key features of digital literacy.
Throughout the above discussion we have touched on some benefits to using Aikuma at this field
site. There are some other advantages that are also worth noting. The first is that the portability of the
handsets meant that there was a wider range of participants recorded. The limited electricity available
for the parallel video documentation, and community attitudes about who was a suitable participant in
that work, meant that only a small section of the community (mostly older males) would have been
documented. The lower formality of using the phones, compared with a bulky video camera, meant that
people also felt quite relaxed, often telling stories with an audience present.
The use of phones also meant that there were fewer missed opportunities for recording. One evening
we used the phones when the light was too poor for video. Another morning when the researcher was
unwell, she gave one of the handsets to a member of the community who recorded some traditional stories
with an older man who had not been able to remember them the day before. On yet another occasion, a
man took one of the handsets away and recorded a translation while the researcher was filming a video
with another participant. Although the linguist was still needed for the saving of recordings, people
became less dependent on her presence to do their own documentation work.
6 Discussion
Reflecting on our experience in the Temb?e, Nhengatu, and Kagate communities, further issues warrant
discussion.
The mobile device was a major attraction. People gathered round to see how it was used, then ex-
plained it to others in their native language. They brought elders to see the work, and encouraged them
to tell stories. This impact convinced us that the mobile phone is an effective platform for engaging
with participants and helping them quickly grasp the collection and dissemination aspects of language
documentation work, cf. (Rice, 2011). Note that the phones were not equipped with SIM cards, and so
there was no distraction of them being used for voice calls or for downloading extraneous software.
However, the device was also an obstacle. Although some people had used smartphones, few had ex-
perienced touchscreens. Creating a user profile required entering a name using the touchscreen keyboard.
It seemed like overkill to train individuals to use a keyboard and to go through a process they would only
perform once. Moreover, the language selection process displayed a searchable list of 7,000 languages,
1021
and it would have been easier to have a small selection of local languages to choose from. In Temb?e,
the man who was trained as a computer technician learned to create user profiles for other people. By
the time of the Kagate experiment, we added support for default languages, and set these as Kagate and
Nepali. This simplified the task, though it also meant that we did not capture information about people?s
competencies in other languages. These issues with the device only occurred at the outset, and highlight
the need to simplify the metadata collection process. The impact of the problem would be reduced with
improved software design.
The device helped with the process of obtaining informed consent. We played an existing recording,
either one collected during an earlier phase of documenting the language, or one from another endangered
or extinct language. In this way we communicated the idea that language recordings can be preserved
and transmitted over distance and time, even once the language is no longer spoken. We also asked what
people thought about the idea of others hearing their language, and they were generally enthusiastic.
In the case of a further Brazilian language, one community leader asked for substantial donations of
hardware and another cited intellectual property concerns, and so we did not record this language. A
related open issue concerns the process for documenting informed consent, particularly when working
with monolingual speakers.
Most of the collected material consisted of personal narratives, folklore, and a limited amount of
singing. Other discourse types that we did not collect include dialogue, oratory, and procedural discourse,
cf. (Johnson and Aristar Dry, 2002). On many occasions, people listened to a traditional narrative and
then asked to recount their own version. Consequently, we see the possibility for achieving substantial
lexical overlap in recordings by different speakers, which could help with speech modelling, dialect
identification, and lexicon production.
7 Conclusions
We have investigated the use of Aikuma, an Android app designed for recording and translating unwritten
languages. We taught members of indigenous communities in Brazil and Nepal to use smartphones for
recording spoken language and for orally interpreting it into the national language, and we collected
a sample of bilingual phrase-aligned speech in the languages. We collected approximately 8.5 hours of
audio, approximately 100,000 words, and in the process, we demonstrated that the platform is an effective
way to engage indigenous communities in the task of building phrase-aligned bilingual speech corpora.
The built-in networking capability of the phone was used to good effect in Nhengatu for leveraging the
contribution of multiple members of the community who have differing linguistic aptitudes.
We identified several areas for additional functionality: support for adding a paraphrase as soon as a
story has been told; support for exporting playlists to CD; a checklist that shows which recordings have
been translated; permitting handwritten transcriptions to be photographed and linked back to the original
audio; and redesigning the interface to remove some remaining English prompts and confusing icons.
These and other enhancements are being developed in our open source project.
1
Above all, we have found that this approach to linguistic data collection greatly facilitates work on
indigenous languages that are falling out of use. It bypasses the need for expensive equipment by pig-
gybacking on the burgeoning adoption of mobile phones and wireless broadband networks. We are
optimistic about the prospects of using this approach to collect substantial new corpora for supporting
linguistic research and language technology development, even for some of the most isolated linguistic
communities in the world.
Acknowledgments
This research was supported by NSF Award 1160639 Language Preservation 2.0: Crowdsourcing Oral
Language Documentation using Mobile Devices (Bird and Liberman), ARC Award 120101712 Lan-
guage Engineering in the Field (Bird), and Firebird Foundation project Documenting the Traditional
Songs and Stories in Kagate, a language of Nepal (Gawne). Bird, Gelbart, and McAlister are grateful to
Dr Denny Moore and the Goeldi Museum (Bel?em) for facilitating their work in Brazil.
1
https://github.com/aikuma
1022
References
Steven Abney and Steven Bird. 2010. The Human Language Project: building a universal corpus of the world?s
languages. In Proceedings of the 48th Meeting of the Association for Computational Linguistics, pages 88?97.
Association for Computational Linguistics.
Timothy Baldwin, Jonathan Pool, and Susan Colowick. 2010. PanLex and LEXTRACT: Translating all words of
all languages of the world. In Proceedings of the 23rd International Conference on Computational Linguistics,
pages 37?40, Beijing, China.
Stephen Beale. 2012. Documenting endangered languages with Linguist?s Assistant. Language Documentation
and Conservation, 6:104?134.
Emily Bender, Robert Schikowski, and Balthasar Bickel. 2012. Deriving a lexicon for a precision grammar
from language documentation resources: A case study of Chintang. In Proceedings of the 25th International
Conference on Computational Linguistics, pages 247?262.
Emily Bender, Michael Wayne Goodman, Joshua Crowgey, and Fei Xia. 2013. Towards creating precision
grammars from interlinear glossed text: Inferring large-scale typological properties. In Proceedings of the
7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 74?83.
Association for Computational Linguistics.
Mat Bettinson. 2013. The effect of respeaking on transcription accuracy. Honours Thesis, Dept of Linguistics,
University of Melbourne.
Steven Bird, Katie Gelbart, and Isaac McAlister, editors. 2013. F?abulas de Terra Preta. Internet Archive.
Steven Bird, Florian R. Hanke, Oliver Adams, and Haejoong Lee. 2014. Aikuma: A mobile app for collaborative
language documentation. In Proceedings of the Workshop on the Use of Computational Methods in the Study of
Endangered Languages. Association for Computational Linguistics.
Steven Bird. 2010. A scalable method for preserving oral literature from small languages. In Proceedings of the
12th International Conference on Asia-Pacific Digital Libraries, pages 5?14.
Jonathan Clark, Robert Frederking, and Lori Levin. 2008. Toward active learning in data selection: Automatic
discovery of language features during elicitation. In Proceedings of the Sixth International Conference on
Language Resources and Evaluation.
Terry Crowley. 2007. Field Linguistics: A Beginner?s Guide. Oxford University Press.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies, pages 600?609. Association for Computational Linguistics.
Nic de Vries, Marelie Davel, Jaco Badenhorst, Willem Basson, Febe de Wet, Etienne Barnard, and Alta de Waal.
2014. A smartphone-based ASR data collection tool for under-resourced languages. Speech Communication,
56:119?131.
Robert R. Desjarlais. 1992. Body and emotion: the aesthetics of illness and healing in the Nepal Himalayas.
Philadelphia: University of Pennsylvania Press.
Mark Dredze, Aren Jansen, Glen Coppersmith, and Ken Church. 2010. NLP on spoken documents without ASR.
In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 460?470.
Association for Computational Linguistics.
Christoph von F?urer-Haimendorf. 1964. The Sherpas of Nepal: Buddhist highlanders. London: John Murray.
Florian R. Hanke and Steven Bird. 2013. Large-scale text collection for unwritten languages. In Proceedings of
the 6th International Joint Conference on Natural Language Processing, pages 1134?1138. Asian Federation
of Natural Language Processing.
Heidi Johnson and Helen Aristar Dry. 2002. OLAC discourse type vocabulary. http://www.
language-archives.org/REC/discourse.html.
Chia-ying Lee and James Glass. 2012. A nonparametric bayesian approach to acoustic model discovery. In Pro-
ceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 40?49. Association
for Computational Linguistics.
Lori Levin, Jeff Good, Alison Alvarez, and Robert Frederking. 2006. Parallel reverse treebanks for the discovery
of morpho-syntactic markings. In Jan Haji?c and Joakim Nivre, editors, Proceedings of the Fifth Workshop on
Treebanks and Linguistic Theories, pages 103?114.
Alexis Palmer, Taesun Moon, Jason Baldridge, Katrin Erk, Eric Campbell, and Telma Can. 2010. Computational
strategies for reducing annotation effort in language documentation. Linguistic Issues in Language Technology,
3:1?42.
1023
Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie, and Jaime Carbonell. 2002. MT for resource-poor
languages using elicitation-based learning of syntactic transfer rules. Machine Translation, 17(4):225?270.
Will Reiman. 2010. Basic oral language documentation. Language Documentation and Conservation, 4:254?268.
Keren Rice. 2011. Documentary linguistics and community relations. Language Documentation and Conserva-
tion, 5:187?207.
S.M. Siniscalchi, Dau-Cheng Lyu, T. Svendsen, and Chin-Hui Lee. 2012. Experiments on cross-language at-
tribute detection and phone recognition with minimal target-specific training data. IEEE Transactions on Audio,
Speech, and Language Processing, 20:875?887.
Ngoc Thang Vu, Franziska Kraus, and Tanja Schultz. 2011. Rapid building of an ASR system for under-resourced
languages based on multilingual unsupervised training. In Interspeech, pages 3145?3148.
Anthony C. Woodbury. 2003. Defining documentary linguistics. In Peter Austin, editor, Language Documentation
and Description, volume 1, pages 35?51. London: SOAS.
Ruiqiang Zhang, Genichiro Kikui, Hirofumi Yamamoto, Taro Watanabe, Frank Soong, and Wai Kit Lo. 2004.
A unified approach in speech-to-speech translation: integrating features of speech recognition and machine
translation. In Proceedings of the 20th International Conference on Computational Linguistics, pages 1168?
1174.
1024
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 886?897,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
What Can We Get From 1000 Tokens?
A Case Study of Multilingual POS Tagging For Resource-Poor Languages
Long Duong,
12
Trevor Cohn,
1
Karin Verspoor,
1
Steven Bird,
1
and Paul Cook
1
1
Department of Computing and Information Systems,
The University of Melbourne
2
National ICT Australia, Victoria Research Laboratory
lduong@student.unimelb.edu.au
{t.cohn, karin.verspoor, sbird, paulcook}@unimelb.edu.au
Abstract
In this paper we address the problem
of multilingual part-of-speech tagging for
resource-poor languages. We use par-
allel data to transfer part-of-speech in-
formation from resource-rich to resource-
poor languages. Additionally, we use a
small amount of annotated data to learn to
?correct? errors from projected approach
such as tagset mismatch between lan-
guages, achieving state-of-the-art perfor-
mance (91.3%) across 8 languages. Our
approach is based on modest data require-
ments, and uses minimum divergence clas-
sification. For situations where no uni-
versal tagset mapping is available, we
propose an alternate method, resulting
in state-of-the-art 85.6% accuracy on the
resource-poor language Malagasy.
1 Introduction
Part-of-speech (POS) tagging is a crucial task for
natural language processing (NLP) tasks, provid-
ing basic information about syntax. Supervised
POS tagging has achieved great success, reach-
ing as high as 95% accuracy for many languages
(Petrov et al., 2012). However, supervised tech-
niques need manually annotated data, and this
is either lacking or limited in most resource-
poor languages. Fully unsupervised POS tagging
is not yet useful in practice due to low accu-
racy (Christodoulopoulos et al., 2010). In this pa-
per, we propose a semi-supervised method to nar-
row the gap between supervised and unsupervised
approaches. We demonstrate that even a small
amount of supervised data leads to substantial im-
provement.
Our method is motivated by the availability of
parallel data. Thanks to the development of mul-
tilingual documents from government projects,
book translations, multilingual websites, and so
forth, parallel data between resource-rich and
resource-poor languages is relatively easy to ac-
quire. This parallel data provides the bridge that
permits us to transfer POS information from a
resource-rich to a resource-poor language.
Systems that make use of cross-lingual tag
projection typically face several issues, includ-
ing mismatches between the tagsets used for the
languages, artifacts from noisy alignments and
cross-lingual syntactic divergence. Our approach
compensates for these issues by training on a
small amount of annotated data on the target side,
demonstrating that only 1k tokens of annotated
data is sufficient to improve performance.
We first tag the resource-rich language using a
supervised POS tagger. We then project POS tags
from the resource-rich language to the resource-
poor language using parallel word alignments.
The projected labels are noisy, and so we use
various heuristics to select only ?good? training
examples. We train the model in two stages.
First, we build a maximum entropy classifier T
on the (noisy) projected data. Next, we train
a supervised classifier P on a small amount of
annotated data (1,000 tokens) in the target lan-
guage, using a minimum divergence technique
to incorporate the first model, T . Compared
with the state of the art (T?ackstr?om et al., 2013),
we make more-realistic assumptions (e.g. relying
on a tiny amount of annotated data rather than
a huge crowd-sourced dictionary) and use less
parallel data, yet achieve a better overall result.
We achieved 91.3% average accuracy over 8 lan-
guages, exceeding T?ackstr?om et al. (2013)?s result
of 88.8%.
The test data we employ makes use of map-
pings from language-specific POS tag inventories
to a universal tagset (Petrov et al., 2012). How-
ever, such a mapping might not be available for
resource-poor languages. Therefore, we also pro-
886
pose a variant of our method which removes the
need for identical tagsets between the projection
model T and the correction model P , based on
a two-output maximum entropy model over tag
pairs. Evaluating on the resource-poor language
Malagasy, we achieved 85.6% accuracy, exceed-
ing the state-of-the-art of 81.2% (Garrette et al.,
2013).
2 Background and Related Work
There is a wealth of prior work on multilingual
POS tagging. The simplest approach takes advan-
tage of the typological similarities that exist be-
tween languages pairs such as Czech and Russian,
or Serbian and Croatian. They build the tagger
? or estimate part of the tagger ? on one lan-
guage and apply it to the other language (Reddy
and Sharoff, 2011, Hana et al., 2004).
Yarowsky and Ngai (2001) pioneered the use of
parallel data for projecting tag information from
a resource-rich language to a resource-poor lan-
guage. Duong et al. (2013b) used a similar method
on using sentence alignment scores to rank the
goodness of sentences. They trained a seed model
from a small part of the data, then applied this
model to the rest of the data using self-training
with revision.
Das and Petrov (2011) also used parallel data
but additionally exploited graph-based label prop-
agation to expand the coverage of labelled tokens.
Each node in the graph represents a trigram in the
target language. Each edge connects two nodes
which have similar context. Originally, only some
nodes received a label from direct label projection,
and then labels were propagated to the rest of the
graph. They only extracted the dictionary from
the graph because the labels of nodes are noisy.
They used the dictionary as the constraints for a
feature-based HMM tagger (Berg-Kirkpatrick et
al., 2010). Both Duong et al. (2013b) and Das and
Petrov (2011) achieved 83.4% accuracy on the test
set of 8 European languages.
Goldberg et al. (2008) pointed out that, with the
presence of a dictionary, even an incomplete one,
a modest POS tagger can be built using simple
methods such as expectation maximization. This
is because most of the time, words have a very
limited number of possible tags, thus a dictionary
that specifies the allowable tags for a word helps
to restrict the search space. With a gold-standard
dictionary, Das and Petrov (2011) achieved an ac-
curacy of approximately 94% on the same 8 lan-
guages. The effectiveness of a gold-standard dic-
tionary is undeniable, however it is costly to build
one, especially for resource-poor languages. Li et
al. (2012) used the dictionary from Wiktionary,
1
a
crowd-sourced dictionary. They scored 84.8% ac-
curacy on the same 8 languages. Currently, Wik-
tionary covers over 170 languages, but the cov-
erage varies substantially between languages and,
unsurprisingly, it is poor for resource-poor lan-
guages. Therefore, relying on Wiktionary is not
effective for building POS taggers for resource-
poor languages.
T?ackstr?om et al. (2013) combined both token
information (from direct projected data) and type
constraints (from Wiktionary?s dictionary) to form
the state-of-the-art multilingual tagger. They built
a tag lattice and used these token and type con-
straints to prune it. The remaining paths are the
training data for a CRF tagger. They achieved
88.8% accuracy on the same 8 languages.
Table 1 summarises the performance of the
above models across all 8 languages. Note that
these methods vary in their reliance on external
resources. Duong et al. (2013b) use the least, i.e.
only the Europarl Corpus (Koehn, 2005). Das and
Petrov (2011) additionally use the United Nation
Parallel Corpus. Li et al. (2012) didn?t use any par-
allel text but used Wiktionary instead. T?ackstr?om
et al. (2013) exploited more parallel data than Das
and Petrov (2011) and also used a dictionary
from Li et al. (2012).
Another approach for resource-poor languages
is based on the availability of a small amount
of annotated data. Garrette et al. (2013) built a
POS tagger for Kinyarwanda and Malagasy. They
didn?t use parallel data but instead exploited four
hours of manual annotation to build?4,000 tokens
or ?3,000 word-types of annotated data. These
tokens or word-types were used to build a tag dic-
tionary. They employed label propagation for ex-
panding the coverage of this dictionary in a sim-
ilar vein to Das and Petrov (2011), but they also
used an external dictionary. They built training
examples using the combined dictionary and then
trained the tagger on this data. They achieved
81.9% and 81.2% accuracy for Kinyarwanda and
Malagasy respectively. Note that their usage of an
external dictionary compromises their claim of us-
ing only 4 hours of annotation.
1
http://www.wiktionary.org/
887
da nl de el it pt es sv Average
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Duong et al. (2013b) 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Li et al. (2012) 83.3 86.3 85.4 79.2 86.5 84.5 86.4 86.1 84.8
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages
? Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish
(sv) ? evaluated on CoNLL data (Buchholz and Marsi, 2006).
The method we propose in this paper is similar
in only using a small amount of annotation. How-
ever, we directly use the annotated data to train
the model rather than using a dictionary. We argue
that with a proper ?guide?, we can take advantage
of very limited annotated data.
2.1 Annotated data
Our annotated data mainly comes from CoNLL
shared tasks on dependency parsing (Buchholz
and Marsi, 2006). The language specific tagsets
are mapped into the universal tagset. We will
use this annotated data mainly for evaluation. Ta-
ble 2 shows the size of annotated data for each
language. The 8 languages we are considering
in this experiment are not actually resource-poor
languages. However, running on these 8 lan-
guages makes our system comparable with pre-
viously proposed methods. Nevertheless, we try
to use as few resources as possible, in order to
simulate the situation for resource-poor languages.
Later in Section 6 we adapt the approach for Mala-
gasy, a truly resource-poor language.
2.2 Universal tagset
We employ the universal tagset from (Petrov et
al., 2012) for our experiment. It consists of 12
common tags: NOUN, VERB, ADJ (adjective),
ADV (adverb), PRON (pronoun), DET (deter-
miner and article), ADP (preposition and post-
position), CONJ (conjunctions), NUM (numeri-
cal), PRT (particle), PUNC (punctuation) and X
(all other categories including foreign words and
abbreviations). Petrov et al. (2012) provide the
mapping from each language-specific tagset to the
universal tagset.
The idea of using the universal tagset is of great
use in multilingual applications, enabling compar-
ison across languages. However, the mapping is
not always straightforward. Table 2 shows the size
of the annotated data for each language, the num-
ber of tags presented in the data, and the list of
tags that are not matched. We can see that only 8
tags are presented in the annotated data for Dan-
ish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are
missing.
2
Thus, a classifier using all 12 tags will
be heavily penalized in the evaluation.
Li et al. (2012) considered this problem and
tried to manually modify the Danish mappings.
Moreover, PRT is not really a universal tag since
it only appears in 3 out of the 8 languages. Plank
et al. (2014) pointed out that PRT often gets con-
fused with ADP even in English. We will later
show that the mapping problem causes substantial
degradation in the performance of a POS tagger
exploiting parallel data. The method we present
here is more target-language oriented: our model
is trained on the target language, in this way, only
relevant information from the source language is
retained. Thus, we automatically correct the map-
ping, and other incompatibilities arising from in-
correct alignments and syntactic divergence be-
tween the source and target languages.
Lang Size(k) # Tags Not Matched
da 94 8 DET, PRT, PUNC, NUM
nl 203 11 PRT
de 712 12
el 70 12
it 76 11 PRT
pt 207 11 PRT
es 89 11 PRT
sv 191 11 DET
AVG 205
Table 2: The size of annotated data from
CoNLL (Buchholz and Marsi, 2006), and the
number of tags included and missing for 8 lan-
guages.
2
Many of these are mistakes in the mapping, however,
they are indicative of the kinds of issues expected in low-
resource languages.
888
3 Directly Projected Model (DPM)
In this section we describe a maximum entropy
tagger that only uses information from directly
projected data.
3.1 Parallel data
We first collect Europarl data having English as
the source language, an average of 1.85 million
parallel sentences for each of the 8 language pairs.
In terms of parallel data, we use far less data com-
pared with other recent work. Das and Petrov
(2011) used Europarl and the ODS United Na-
tion dataset, while T?ackstr?om et al. (2013) addi-
tionally used parallel data crawled from the web.
The amount of parallel data is crucial for align-
ment quality. Since DPM uses alignments to trans-
fer tags from source to target language, the per-
formance of DPM (and other models that exploit
projection) largely depends on the quantity of par-
allel data. The ?No LP? model of Das and Petrov
(2011), which only uses directly projected labels
(without label propagation), scored 81.3% for 8
languages. However, using the same model but
with more parallel data, T?ackstr?om et al. (2013)
scored 84.9% on the same test set.
3.2 Label projection
We use the standard alignment tool Giza++ (Och
and Ney, 2003) to word align the parallel data. We
employ the Stanford POS tagger (Toutanova et al.,
2003) to tag the English side of the parallel data
and then project the label to the target side. It has
been confirmed in many studies (T?ackstr?om et al.,
2013, Das and Petrov, 2011, Toutanova and John-
son, 2008) that directly projected labels are noisy.
Thus we need a method to reduce the noise. We
employ the strategy of Yarowsky and Ngai (2001)
of ranking sentences using a their alignment scores
from IBM model 3.
Firstly, we want to know how noisy the pro-
jected data is. Thus, we use the test data to build
a simple supervised POS tagger using the TnT
tagger (Brants, 2000) which employs a second-
order Hidden Markov Model (HMM). We tag the
projected data and compare the label from direct
projection and from the TnT tagger. The labels
from the TnT Tagger are considered as pseudo-
gold labels. Column ?Without Mapping? from Ta-
ble 3 shows the average accuracy for the first n-
sentences (n = 60k, 100k, 200k, 500k) for 8 lan-
guages according to the ranking. Column ?Cov-
erage? shows the percentages of projected label
(the other tokens are Null aligned). We can see
that when we select more data, both coverage and
accuracy fall. In other words, using the sentence
alignment score, we can rank sentences with high
coverage and accuracy first. However, even after
ranking, the accuracy of projected labels is less
than 80% demonstrating how noisy the projected
labels are.
Table 3 (column ?With Mapping?) additionally
shows the accuracy using simple tagset mapping,
i.e. mapping each tag to the tag it is assigned most
frequently in the test data. For example DET, PRT,
PUNC, NUM, missing from Danish gold data, will
be matched to PRON, X, X, ADJ respectively. This
simple matching yields a ? 4% (absolute) im-
provement in average accuracy. This illustrates the
importance of handling tagset mapping carefully.
3.3 The model
In this section, we introduce a maximum entropy
tagger exploiting the projected data. We select the
first 200k sentences from Table 3 for this experi-
ment. This number represents a trade-off between
size and accuracy. More sentences provide more
information but at the cost of noisier data. Duong
et al. (2013b) also used sentence alignment scores
to rank sentences. Their model stabilizes after us-
ing 200k sentences. We conclude that 200k sen-
tences is enough and capture most information
from the parallel data.
Features Descriptions
W@-1 Previous word
W@+1 Next word
W@0 Current word
CAP First character is capitalized
NUMBER Is number
PUNCT Is punctuation
SUFFIX@k Suffix up to length 3 (k <= 3)
WC Word class
Table 4: Feature template for a maximum entropy
tagger
We ignore tokens that don?t have labels, which
arise from null alignments and constitute approxi-
mately 14% of the data. The remaining data (?1.4
million tokens) are used to train a maximum en-
tropy (MaxEnt) model. MaxEnt is one of the
simplest forms of probabilistic classifier, and is
appropriate in this setting due to the incomplete
889
Data Size (k) Coverage (%) Without Mapping With Mapping
60 91.5 79.9 84.2
100 89.1 79.4 83.6
200 86.1 79.1 82.9
500 82.4 78.0 81.5
Table 3: The coverage, and POS tagging accuracy with and without tagset mapping of directly projected
labels, averaged over 8 languages for different data sizes
Model da nl de el it pt es sv Avg
All features 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
- Word Class 64.7 82.6 86.6 79.0 82.8 84.6 82.2 76.9 79.9
- Suffix 64.0 82.8 86.3 78.1 81.0 85.9 82.3 76.2 79.6
- Prev, Next Word 62.6 82.5 87.4 79.0 81.9 86.5 82.2 74.8 79.6
- Cap, Num, Punct 64.0 81.9 84.0 78.0 79.1 86.3 81.8 75.6 78.8
Table 5: The accuracy of Directed Project Model (DPM) with different feature sets, removing one feature
set at a time
sequence data. While sequence models such as
HMMs or CRFs can provide more accurate mod-
els of label sequences, they impose a more strin-
gent training requirement.
3
We also experimented
with a first-order linear chain CRF trained on con-
tiguous sub-sequences but observed ? 4% (abso-
lute) drop in performance.
The maximum entropy classifier estimates the
probability of tag t given a word w as
P (t|w) =
1
Z(w)
exp
D
?
j=1
?
j
f
j
(w, t) ,
where Z(w) =
?
t
exp
?
D
j=1
?
j
f
j
(w, t) is the
normalization factor to ensure the probabilities
P (t|w) sum to one. Here f
j
is a feature function
and ?
j
is the weight for this feature, learned as
part of training. We use Maximum A Posteriori
(MAP) estimation to maximize the log likelihood
of the training data, D = {w
i
, t
i
}
N
i=1
, subject to a
zero-mean Gaussian regularisation term,
L = logP (?)
N
?
i=1
P (t
(i)
|w
(i)
)
= ?
D
?
j=1
?
2
j
2?
2
+
N
?
i=1
D
?
j=1
?
j
f
j
(w
i
, t
i
)? logZ(w
i
)
where the regularisation term limits over-fitting,
an important concern when using large feature
3
T?ackstr?om et al. (2013) train a CRF on incomplete data,
using a tag dictionary heuristic to define a ?gold standard?
lattice over label sequences.
sets. For our experiments we set ?
2
= 1. We use
L-BFGS which performs gradient ascent to maxi-
mize L. Table 4 shows the features we considered
for building the DPM. We use mkcls, an unsu-
pervised method for word class induction which is
widely used in machine translation (Och, 1999).
We run mkcls to obtain 100 word classes, using
only the target language side of the parallel data.
Table 5 shows the accuracy of the DPM evalu-
ated on 8 languages (?All features model?). DPM
performs poorly on Danish, probably because of
the tagset mapping issue discussed above. The
DPM result of 80.2% accuracy is encouraging,
particularly because the model had no explicit su-
pervision.
To see what features are meaningful for our
model, we remove features in turn and report
the result. The result in Table 5 disagrees with
T?ackstr?om et al. (2013) on the word class features.
They reported a gain of approximately 3% (ab-
solute) using the word class. However, it seems
to us that these features are not especially mean-
ingful (at least in the present setting). Possible
reasons for the discrepancy are that they train the
word class model on a massive quantity of exter-
nal monolingual data, or their algorithms for word
clustering are better (Uszkoreit and Brants, 2008).
We can see that the most informative features are
Capitalization, Number and Punctuation. This
makes sense because in languages such as Ger-
man, capitalization is a strong indicator of NOUN.
Number and punctuation features ensure that we
classify NUM and PUNCT tags correctly.
890
4 Correction Model
In this section we incorporate the directly pro-
jected model into a second correction model
trained on a small supervised sample of 1,000 an-
notated tokens. Our DPM model is not very accu-
rate; as we have discussed it makes many errors,
due to invalid or inconsistent tag mappings, noisy
alignments, and cross-linguistic syntactic diver-
gence. However, our aim is to see how effectively
we can exploit the strengths of the DPM model
while correcting for its inadequacies using direct
supervision. We select only 1,000 annotated to-
kens to reflect a low resource scenario. A small
supervised training sample is a more realistic form
of supervision than a tag dictionary (noisy or oth-
erwise). Although used in most prior work, a tag
dictionary for a new language requires significant
manual effort to construct. Garrette and Baldridge
(2013) showed that a 1,000 token dataset could be
collected very cheaply, requiring less than 2 hours
of non-expert time.
Our correction model makes use of a mini-
mum divergence (MD) model (Berger et al., 1996),
a variant of the maximum entropy model which
biases the target distribution to be similar to a
static reference distribution. The method has been
used in several language applications including
machine translation (Foster, 2000) and parsing
(Plank and van Noord, 2008, Johnson and Riezler,
2000). These previous approaches have used var-
ious sources of reference distribution, e.g., incor-
porating information from a simpler model (John-
son and Riezler, 2000) or combining in- and out-
of-domain models (Plank and van Noord, 2008).
Plank and van Noord (2008) concluded that this
method for adding prior knowledge only works
with high quality reference distributions, other-
wise performance suffers.
In contrast to these previous approaches, we
consider the specific setting where both the
learned model and the reference model s
o
=
P (t|w) are both maximum entropy models. In this
case we show that the MD setup can be simplified
to a regularization term, namely a Gaussian prior
with a non-zero mean. We model the classification
probability, P
?
(t|w) as the product between a base
model and a maximum entropy classifier,
P
?
(t|w) ? P (t|w) exp
D
?
j=1
?
j
f
j
(w, t)
where here we use the DPM model as base model
P (t|w). Under this setup, where P
?
uses the same
features as P , and both are log-linear models, this
simplifies to
P
?
(t|w) ? exp
?
?
D
?
j=1
?
j
f
j
(w, t) +
D
?
j=1
?
j
f
j
(w, t)
?
?
? exp
D
?
j=1
(?
j
+ ?
j
) f
j
(w, t) (1)
where the constant of proportionality is Z
?
(w) =
?
t
exp
?
D
j=1
(?
j
+ ?
j
) f
j
(w, t). It is clear that
Equation (1) also defines a maximum entropy clas-
sifier, with parameters ?
j
= ?
j
+ ?
j
, and conse-
quently this might seem to be a pointless exercise.
The utility of this approach arises from the prior:
MAP training with a zero mean Gaussian prior
over ? is equivalent to a Gaussian prior over the
aggregate weights, ?
j
? N (?
j
, ?
2
). This prior
enforces parameter sharing between the two mod-
els by penalising parameter divergence from the
underlying DPM model ?. The resulting training
objective is
L
corr
= logP (t|w, ?)?
1
2?
2
D
?
j=1
(?
j
? ?
j
)
2
which can be easily optimised using standard
gradient-based methods, e.g., L-BFGS. The con-
tribution of the regulariser is scaled by the constant
1
2?
2
.
4.1 Regulariser sensitivity
Careful tuning of the regularisation term ?
2
is crit-
ical for the correction model, both to limit over-
fitting on the very small training sample of 1,000
tokens, and to control the extent of the influence
of the DPM model over the correction model.
A larger value of ?
2
lessens the reliance on the
DPM and allows for more flexible modelling of
the training set, while a small value of ?
2
forces
the parameters to be close to the DPM estimates at
the expense of data fit. We expect the best value
to be somewhere between these extremes, and use
line-search to find the optimal value for ?
2
. For
this purpose, we hold out 100 tokens from the
1,000 instance training set, for use as our devel-
opment set for hyper-parameter selection.
From Figure 1, we can see that the model per-
forms poorly on small values of ?
2
. This is under-
standable because the small ?
2
makes the model
891
ll
l
l
l l l l l
l l
0.0
1 0.1 1 10 70 100 100
0
100
00
1e+
05
1e+
06
1e+
07
Variance
80
84
88
Acc
ura
cy (%
) 
l Average Acc
Figure 1: Sensitivity of regularisation parameter
?
2
against the average accuracy measured on 8
languages on the development set
too similar to DPM, which is not very accurate
(80.2%). At the other extreme, if ?
2
is large, the
DPM model is ignored, and the correction model
is equivalent with the supervised model (? 88%
accuracy). We select the value of ?
2
= 70, which
maximizes the accuracy on the development set.
4.2 The model
Using the value of ?
2
= 70, we retrain the model
on the whole 1,000-token training set and evalu-
ate the model on the rest of the annotated data.
Table 6 shows the performance of DPM, Super-
vised model, Correction model and the state-of-
the-art model (T?ackstr?om et al., 2013). The super-
vised model trains a maximum entropy tagger us-
ing the same features as in Table 4 on this 1000 to-
kens. The only difference between the supervised
model and the correction model is that in the cor-
rection model we additionally incorporate DPM as
the prior.
The supervised model performs surprisingly
well confirming that our features are meaning-
ful in distinguishing between tags. This model
achieves high accuracy on Danish compared with
other languages probably because Danish is eas-
ier to learn since it contains only 8 tags. Despite
the fact that the DPM is not very accurate, the cor-
rection model consistently outperforms the super-
vised model on all considered languages, approx-
imately 4.3% (absolute) better on average. This
shows that our method of incorporating DPM to
the model is efficient and robust.
The correction model performs much bet-
ter than the state-of-the-art for 7 languages but
l
l l
l l l
l l
l l l
100 300 500 700 100
0
150
0
200
0
500
0
100
00
150
00
500
00
Data Size
65
75
85
95
Acc
urac
y (%
) 
l Correction ModelSupervised Model
Figure 2: Learning curve for correction model and
supervised model: the x-axis is the size of data
(number of tokens); the y-axis is the average ac-
curacy measured on 8 languages; the dashed line
shows the data condition reported in Table 6
slightly worse for 1 language. On average we
achieve 91.3% accuracy compared with 88.8%
for the state-of-the-art, an error rate reduction of
22.3%. This is despite using fewer resources and
only modest supervision.
5 Analysis
Tagset mismatch In the correction model, we
implicitly resolve the mismatched tagset issue.
DPM might contain tags that don?t appear in the
target language or generally are errors in the map-
ping. However, when incorporating DPM into the
correction model, only the feature weight of tags
that appear in the target language are retained. In
general, because we don?t explicitly do any map-
ping between languages, we might have trouble if
the tagset size of the target language is bigger than
the source language tagset. However, this is not
the case for our experiment because we choose En-
glish as the source-side and English has the full 12
tags.
Learning curve We investigate the impact of
the number of available annotated tokens on the
correction model. Figure 2 shows the learning
curve of the correction model and the supervised
model. We can clearly see the differences be-
tween 2 models when the size of training data is
small. For example, at 100 tokens, the difference
is very large, approximately 18% (absolute), it is
also 6% (absolute) better than DPM. This differ-
ence diminishes as we add more data. This make
sense because when we add more data, the super-
vised model become stronger, while the effective-
892
Model da nl de el it pt es sv Avg
DPM 64.4 83.3 86.3 79.7 82.0 86.5 82.5 76.5 80.2
T?ackstr?om et al. (2013) 88.2 85.9 90.5 89.5 89.3 91.0 87.1 88.9 88.8
Supervised model 90.1 84.6 89.6 88.2 81.4 87.6 88.9 85.4 87.0
Correction Model 92.1 91.1 92.5 92.1 89.9 92.5 91.6 88.7 91.3
DPM (with dict) 65.2 83.9 87.0 79.1 83.5 87.1 83.0 77.5 80.8
Correction Model (with dict) 93.3 92.2 93.7 93.2 92.2 93.1 92.8 90.0 92.6
Table 6: The comparison of our Directly Projected Model, Supervised Model, Correction Model and the
state-of-the-art system (T?ackstr?om et al., 2013). The best performance for each language is shown in
bold. The models that are built with a dictionary are provided for reference.
ness of the DPM prior on the correction model is
wearing off. An interesting observation is that the
correction model is always better, even when we
add massive amounts of annotated data. At 50,000
tokens, when the supervised model reaches 96%
accuracy, the correction model is still 0.3% (abso-
lute) better, reaching 96.3%. It means that even
at that high level of confidence, some informa-
tion can still be added from DPM to the correc-
tion model. This improvement probably comes
from the observation that the ambiguity in one
language is explained through the alignment. It
also suggests that this method could improve the
performance of a supervised POS tagger even for
resource-rich languages.
Our methods are also relevant for annotation
projects for resource-poor languages. Assuming
that it is very costly to annotate even 100 tokens,
applying our methods can save annotation effort
but maintain high performance. For example, we
just need 100 tokens to match the accuracy of a su-
pervised method trained on 700 tokens, or we just
need 500 tokens to match the performance with
nearly 2,000 tokens of supervised learning.
Our method is simple, but particularly suitable
for resource-poor languages. We need a small
amount of annotated data for a high performance
POS tagger. For example, we need only around
300 annotated tokens to reach the same accuracy
as the state-of-the-art unsupervised POS tagger
(88.8%).
Tag dictionary Although, it is not our objec-
tive to rely on the dictionary, we are interested
in whether the gains from the correction model
still persist when the DPM performance is im-
proved. We attempt to improve DPM, following
the method of Li et al. (2012) by building a tag dic-
tionary using Wiktionary. This dictionary is then
used as a feature which fires for word-tag pairings
present in the dictionary. We expect that when we
add this additional supervision, the DPM model
should perform better. Table 6 shows the perfor-
mance of DPM and the correction model when in-
corporating the dictionary. The DPM model only
increases 0.6% absolute but the correction model
increases 1.3%. Additionally, it shows that our
model can improve further by incorporating exter-
nal information where available.
CRF Our approach of using simple classifiers
begs the question of whether better results could
be obtained using sequence models, such as con-
ditional random fields (CRFs). As mentioned pre-
viously, a CRF is not well suited for incomplete
data. However, as our second ?correction? model
is trained on complete sequences, we now con-
sider using a CRF in this stage. The training al-
gorithm is as follows: first we estimate the DPM
feature weights on the incomplete data as before,
and next we incorporate the feature weights into a
CRF trained on the 1,000 annotated tokens. This is
complicated by the different feature sets between
the MaxEnt classifier and the CRF, however the
classifier uses a strict subset of the CRF features.
Thus, we use the minimum divergence prior for
the token level features, and a standard zero-mean
prior for the sequence features. That is, the ob-
jective function of the CRF correction model be-
comes:
L
corr
crf
= logP (t|w, ?)
?
1
2?
2
1
?
j?F
1
(?
j
? ?
j
)
2
?
1
2?
2
2
?
j?F
2
?
2
j
(2)
where F
1
is the set of features referring to only
one label as in the DPM maxent model and F
2
is the set of features over label pairs. The union
of F = F
1
? F
2
is the set of all features for
the CRF. We perform grid search using held out
893
data as before for ?
2
1
and ?
2
2
. The CRF correc-
tion model scores 88.1% compared with 86.5% of
the supervised CRF model trained on the 1,000
tokens. Clearly, this is beneficial, however, the
CRF correction model still performs worse than
the MaxEnt correction model (91.3%). We are not
sure why but one reason might be overfitting of
the CRF, due to its large feature set and tiny train-
ing sample. Moreover, this CRF approach is or-
thogonal to T?ackstr?om et al. (2013): we could use
their CRF model as the DPM model and train the
CRF correction model using the same minimum
divergence method, presumably resulting in even
higher performance.
6 Two-output model
Garrette and Baldridge (2013) also use only a
small amount of annotated data, evaluating on
two resource-poor languages Kinyarwanda (KIN)
and Malagasy (MLG). As a simple baseline, we
trained a maxent supervised classifier on this data,
achieving competitive results of 76.4% and 80.0%
accuracy compared with their published results
of 81.9% and 81.2% for KIN and MLG, respec-
tively. Note that the Garrette and Baldridge (2013)
method is much more complicated than this base-
line, and additionally uses an external dictionary.
We want to further improve the accuracy of
MLG using parallel data. Applying the technique
from Section 4 will not work directly, due to the
tagset mismatch (the Malagasy tagset contains 24
tags) which results in highly different feature sets.
Moreover, we don?t have the language expertise
to manually map the tagset. Thus, in this section,
we propose a method capable of handling tagset
mismatch. For data, we use a parallel English-
Malagasy corpus of ?100k sentences,
4
and the
POS annotated dataset developed by Garrette and
Baldridge (2013), which comprises 4230 tokens
for training and 5300 tokens for testing.
6.1 The model
Traditionally, MaxEnt classifiers are trained us-
ing a single label.
5
The method we propose is
trained with pairs of output labels: one for the
4
http://www.ark.cs.cmu.edu/global-voices/
5
Or else a sequence of labels, in the case of a conditional
random field (Lafferty et al., 2001). However, even in this
case, each token is usually assigned a single label. An excep-
tion is the factorial CRF (Sutton et al., 2007), which models
several co-dependent sequences. Our approach is equivalent
to a factorial CRF without edges between tags for adjacent
tokens in the input.
Malagasy tag (t
M
) and one for the universal tag
(t
U
), which are both predicted conditioned on a
Malagasy word (w
M
) in context. Our two-output
model is defined as
P (t
M
, t
U
|w
M
) =
1
Z(w
M
)
exp
(
D
?
j=1
?
j
f
M
j
(w, t
M
)
+
E
?
j=1
?
j
f
U
j
(w, t
U
) +
F
?
j=1
?
j
f
B
j
(w, t
M
, t
U
)
)
(3)
where f
M
, f
U
, f
B
are the feature functions con-
sidering t
M
only, t
U
only, and over both outputs
t
M
and t
U
respectively, and Z(w
M
) is the parti-
tion function. We can think of Eq. (3) as the com-
bination of 3 models: the Malagasy maxent super-
vised model, the DPM model, and the tagset map-
ping model. The central idea behind this model is
to learn to predict not just the MLG tags, as in a
standard supervised model, but also to learn the
mapping between MLG and the noisy projected
universal tags. Framing this as a two output model
allows for information to flow both ways, such that
confident taggings in either space can inform the
other, and accordingly the mapping weights ? are
optimised to maximally exploit this effect.
One important question is how to obtain la-
belled data for training the two-output model, as
our small supervised sample of MLG text is only
annotated for MLG labels t
M
. We resolve this
by first learning the DPM model on the projected
labels, after which we automatically label our
correction training set with predicted tags from
the DPM model. That is, we augment the an-
notated training data from (t
M
, w
M
) to become
(t
M
, t
U
, w
M
). This is then used to train the two-
output maxent classifier, optimising a MAP ob-
jective using standard gradient descent. Note that
it would be possible to apply the same minimum
divergence technique for the two-output maxent
model. In this case the correction model would
include a regularization term over the ? to bias to-
wards the DPM parameters, while ? and ? would
use a zero-mean regularizer. However, we leave
this for future work.
Table 7 summarises the performance of the
state-of-the-art (Garrette et al., 2013), the super-
vised model and the two-output maxent model
evaluated on the Malagasy test set. The two-output
maxent model performs much better than the su-
pervised model, achieving ?5.3% (absolute) im-
894
Model Accuracy (%)
Garrette et al. (2013) 81.2
MaxEnt Supervised 80.0
2-output MaxEnt (Universal tagset) 85.3
2-output MaxEnt (Penn tagset) 85.6
Table 7: The performance of different models for
Malagasy.
provement. An interesting property of this ap-
proach is that we can use different tagsets for the
DPM. We also tried the original Penn treebank
tagset which is much larger than the universal
tagset (48 vs. 12 tags). We observed a small im-
provement reaching 85.6%, suggesting that some
pertinent information is lost in the universal tagset.
All in all, this is a substantial improvement over
the state-of-the-art result of 81.2% (Garrette et al.,
2013) and an error reduction of 23.4%.
7 Conclusion
In this paper, we thoroughly review the work on
multilingual POS tagging of the past decade. We
propose a simple method for building a POS tag-
ger for resource-poor languages by taking advan-
tage of parallel data and a small amount of anno-
tated data. Our method also efficiently resolves
the tagset mismatch issue identified for some lan-
guage pairs. We carefully choose and tune the
model. Comparing with the state-of-the-art, we
are using the more realistic assumption that a
small amount of labelled data can be made avail-
able rather than requiring a crowd-sourced dic-
tionary. We use less parallel data which as we
pointed out in section 3.1, could have been a huge
disadvantage for us. Moreover, we did not exploit
any external monolingual data. Importantly, our
method is simpler but performs better than previ-
ously proposed methods. With only 1,000 anno-
tated tokens, less than 1% of the test data, we can
achieve an average accuracy of 91.3% compared
with 88.8% of the state-of-the-art (error reduction
rate ?22%). Across the 8 languages we are sub-
stantially better at 7 and slightly worse at one. Our
method is reliable and could even be used to im-
prove the performance of a supervised POS tagger.
Currently, we are building the tagger and eval-
uating through several layers of mapping. Each
layer might introduce some noise which accumu-
lates and leads to a biased model. Moreover,
the tagset mappings are not available for many
resource-poor languages. We therefore also pro-
posed a method to automatically match between
tagsets based on a two-output maximum entropy
model. On the resource-poor language Mala-
gasy, we achieved the accuracy of 85.6% com-
pared with the state-of-the-art of 81.2% (Garrette
et al., 2013). Unlike their method, we didn?t use an
external dictionary but instead use a small amount
of parallel data.
In future work, we would like to improve the
performance of DPM by collecting more parallel
data. Duong et al. (2013a) pointed out that using
a different source language can greatly alter the
performance of the target language POS tagger.
We would like to experiment with different source
languages other than English. We assume that we
have 1,000 tokens for each language. Thus, for the
8 languages we considered we will have 8,000 an-
notated tokens. Currently, we treat each language
independently, however, it might also be interest-
ing to find some way to incorporate information
from multiple languages simultaneously to build
the tagger for a single target language.
Acknowledgments
We would like to thank Dan Garreette, Jason
Baldridge and Noah Smith for Malagasy and Kin-
yarwanda datasets. This work was supported by
the University of Melbourne and National ICT
Australia (NICTA). NICTA is funded by the Aus-
tralian Federal and Victoria State Governments,
and the Australian Research Council through the
ICT Centre of Excellence program. Dr Cohn is the
recipient of an Australian Research Council Fu-
ture Fellowship (project number FT130101105).
895
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceeding of
HLT-NAACL, pages 582?590.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. COMPU-
TATIONAL LINGUISTICS, 22:39?71.
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing
(ANLP ?00), pages 224?231, Seattle, Washington,
USA.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsuper-
vised pos induction: How far have we come? In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 575?584.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Volume
1, HLT ?11, pages 600?609.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013a. Increasing the quality and quan-
tity of source language data for Unsupervised Cross-
Lingual POS tagging. Proceedings of the Sixth In-
ternational Joint Conference on Natural Language
Processing, pages 1243?1249. Asian Federation of
Natural Language Processing.
Long Duong, Paul Cook, Steven Bird, and Pavel
Pecina. 2013b. Simpler unsupervised POS tagging
with bilingual projections. Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
634?639. Association for Computational Linguis-
tics.
George Foster. 2000. A maximum entropy/minimum
divergence translation model. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 45?52.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
pages 138?147, June.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of pos-
taggers for low-resource languages. pages 583?592,
August.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start. In In Proc. ACL, pages
746?754.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphology:
Tagging Russian using Czech resources. In Pro-
ceedings of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?04),
pages 222?229, Barcelona, Spain, July.
Mark Johnson and Stefan Riezler. 2000. Exploit-
ing auxiliary distributions in stochastic unification-
based grammars. In Proceedings of the 1st North
American Chapter of the Association for Computa-
tional Linguistics Conference, NAACL 2000, pages
154?161.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summit
X), pages 79?86, Phuket, Thailand. AAMT.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289.
Shen Li, Jo?ao V. Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 1389?1398.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In Proceedings of the
Ninth Conference on European Chapter of the As-
sociation for Computational Linguistics, EACL ?99,
pages 71?76.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey, may. European Language Resources
Association (ELRA).
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach
to domain adaptation of a syntactic disambigua-
tion model. In Coling 2008: Proceedings of the
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 9?16.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
896
Computational Linguistics, pages 742?751, Gothen-
burg, Sweden, April.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS taggers (and other tools) for Indian lan-
guages: An experiment with Kannada using Telugu
resources. In Proceedings of IJCNLP workshop on
Cross Lingual Information Access: Computational
Linguistics and the Information Need of Multilin-
gual Societies. (CLIA 2011 at IJNCLP 2011), Chi-
ang Mai, Thailand, November.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. J. Mach. Learn.
Res., 8:693?723, May.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Kristina Toutanova and Mark Johnson. 2008. A
bayesian lda-based model for semi-supervised part-
of-speech tagging. In J.C. Platt, D. Koller, and
Y. Singer a nd S.T. Roweis, editors, Advances in
Neural Information Processing Systems 20, pages
1521?1528. Curran Associates, Inc.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1 (NAACL ?03), pages 173?180, Edmonton,
Canada.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In In
ACL International Conference Proceedings.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Chapter
of the Association for Computational Linguistics on
Language technologies, NAACL ?01, pages 1?8.
897
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 267?275,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Fast Query for Large Treebanks
Sumukh Ghodke?
?Department of Computer Science
and Software Engineering,
University of Melbourne
Victoria 3010, Australia
Steven Bird??
?Linguistic Data Consortium,
University of Pennsylvania
3600 Market Street, Suite 810
Philadelphia PA 19104, USA
Abstract
A variety of query systems have been devel-
oped for interrogating parsed corpora, or tree-
banks. With the arrival of efficient, wide-
coverage parsers, it is feasible to create very
large databases of trees. However, existing ap-
proaches that use in-memory search, or rela-
tional or XML database technologies, do not
scale up. We describe a method for storage,
indexing, and query of treebanks that uses an
information retrieval engine. Several experi-
ments with a large treebank demonstrate ex-
cellent scaling characteristics for a wide range
of query types. This work facilitates the cu-
ration of much larger treebanks, and enables
them to be used effectively in a variety of sci-
entific and engineering tasks.
1 Introduction
The problem of representing and querying linguistic
annotations has been an active area of research for
several years. Much of the work has grown from
efforts to curate large databases of annotated text
such as treebanks, for use in developing and testing
language technologies (Marcus et al, 1993; Abeille?,
2003; Hockenmaier and Steedman, 2007). At least
a dozen linguistic tree query languages have been
developed for interrogating treebanks (see ?2).
While high quality syntactic parsers are able to
efficiently annotate large quantities of English text
(Clark and Curran, 2007), existing approaches to
query do not work on the same scale. Many exist-
ing systems load the entire corpus into memory and
check a user-supplied query against every tree. Oth-
ers avoid the memory limitation, and use relational
or XML database systems. Although these have
built-in support for indexes, they do not scale up ei-
ther (Ghodke and Bird, 2008; Zhang et al, 2001)).
The ability to interrogate large collections of
parsed text has important practical applications.
First, it opens the way to a new kind of information
retrieval (IR) that is sensitive to syntactic informa-
tion, permitting users to do more focussed search.
At the simplest level, an ambiguous query term like
wind or park could be disambiguated with the help
of a POS tag (e.g. wind/N, park/V). (Existing IR
engines already support query with part-of-speech
tags (Chowdhury and McCabe, 1998)). More com-
plex queries could stipulate the syntactic category of
apple is in subject position.
A second benefit of large scale tree query is for
natural language processing. For example, we might
compute the likelihood that a given noun appears as
the agent or patient of a verb, as a measure of an-
imacy. We can use features derived from syntactic
trees in order to support semantic role labeling, lan-
guage modeling, and information extraction (Chen
and Rambow, 2003; Collins et al, 2005; Hakenberg
et al, 2009). A further benefit for natural language
processing, though not yet realized, is for a treebank
and query engine to provide the underlying storage
and retrieval for a variety of linguistic applications.
Just as a relational database is present in most busi-
ness applications, providing reliable and efficient ac-
cess to relational data, such a system would act as a
repository of annotated texts, and expose an expres-
sive API to client applications.
A third benefit of large scale tree query is to
support syntactic investigations, e.g. for develop-
267
ing syntactic theories or preparing materials for lan-
guage learners. Published treebanks will usually not
attest particular words in the context of some in-
frequent construction, to the detriment of syntactic
studies that make predictions about such combina-
tions, and language learners wanting to see instances
of some construction involving words from some
specialized topic. A much larger treebank allevi-
ates these problems. To improve recall performance,
multiple parses for a given sentence could be stored
(possibly derived from different parsers).
A fourth benefit for large scale tree query is to
support the curation of treebanks, a major enter-
prise in its own right (Abeille?, 2003). Manual selec-
tion and correction of automatically generated parse
trees is a substantial part of the task of preparing a
treebank. At the point of making such decisions, it
is often helpful for an annotator to view existing an-
notations of a given construction which have already
been manually validated (Hiroshi et al, 2005). Oc-
casionally, an earlier annotation decision may need
to be reconsidered in the light of new examples,
leading to further queries and to corrections that are
spread across the whole corpus (Wallis, 2003; Xue
et al, 2005).
This paper explores a new methods for scaling up
tree query using an IR engine. In ?2 we describe ex-
isting tree query systems, elaborating on the design
decisions, and on key aspects of their implementa-
tion and performance. In ?3 we describe a method
for indexing trees using an IR engine, and discuss
the details of our open source implementation. In
?4 we report results from a variety of experiments
involving two data collections. The first collection
contains of 5.5 million parsed trees, two orders of
magnitude larger than those used by existing tree
query systems, while the second collection contains
26.5 million trees.
2 Treebank Query
A tree query system needs to be able to identify trees
having particular properties. On the face of it, this
should be possible to achieve by writing simple pro-
grams over treebank files on disk. The programs
would match tree structures using regular expression
patterns, possibly augmented with syntax for match-
ing tree structure. However, tree query is a more
complex and interesting task, due to several factors
which we list below.
Structure of the data: There are many varieties
of treebank. Some extend the nested bracketing
syntax to store morphological information. Oth-
ers store complex attribute-value matrices in tree
nodes or have tree-valued attributes (Oepen et al,
2002), or store dependency structures ( ?Cmejrek et
al., 2004), or categorial grammar derivations (Hock-
enmaier and Steedman, 2007). Others store multiple
overlapping trees (Cassidy and Harrington, 2001;
Heid et al, 2004; Volk et al, 2007).
Form of results: Do we want entire trees, or
matching subtrees, or just a count of the number of
results? Do we need some indication of why the
query matched a particular tree, perhaps by show-
ing how query terms relate to a hit, cf. document
snippets and highlighted words in web search re-
sults? Do we want to see multiple hits when a query
matches a particular tree in more than one place?
Do we want to see tree diagrams, or some machine-
readable tree representation that can be used in ex-
ternal analysis? Can a query serve to update the tree-
bank, cf. SQL update queries?
Number of results: Do we want all results, or the
first n results in document order, or the ?best? n re-
sults, where our notion of best might be based on
representativeness or distinctiveness.
Description language: Do we prefer to describe
trees by giving examples of tree fragments, replac-
ing some nodes replaced with wildcards (Hiroshi et
al., 2005; Ichikawa et al, 2006; M??rovsky?, 2006)?
Or do we prefer a path language (Rohde, 2005; Lai
and Bird, 2010)? Or perhaps we prefer a language
involving variables, quantifiers, boolean operators,
and negation (Ko?nig and Lezius, 2001; Kepser,
2003; Pajas and ?Ste?pa?nek, 2009)? What built-in
tree relations are required, beyond the typical par-
ent/child, ancestor/descendent, sibling and temporal
relations? (E.g. last child, leftmost descendent, par-
ent?s following sibling, pronoun?s antecedent.) Do
we need to describe tree nodes using regular expres-
sions, or attributes and values? Do we need a type
system, a pattern language, or boolean logic for talk-
ing about attribute values? The expressive require-
ments of the query language have been discussed
268
at length elsewhere (Lai and Bird, 2004; M??rovsky?,
2008), and we will not consider them further here.
Performance: What performance is acceptable,
especially as the data size grows? Do we want
to optimize multiple reformulations of a query, for
users who iteratively refine a query based on query
results? Do we want to optimize certain query
types? Are queries performed interactively or in
batch mode? Is the treebank stable, or being actively
revised, in which case indexes need to be easily up-
datable? Do we expect logically identical queries
to have the same performance, so that users do not
have to rewrite their queries for efficiency? Key per-
formance measures are index size and search times.
Architecture: Is the query system standalone, or
does it exist in a client-server architecture? Is there
a separate user-interface layer that interacts with a
data server using a well-defined API, or is it a mono-
lithic system? Should it translate queries into an-
other language, such as SQL (Bird et al, 2006;
Nakov et al, 2005), or XQuery (Cassidy, 2002;
Mayo et al, 2006), or to automata (Maryns and
Kepser, 2009), in order to benefit from the perfor-
mance optimizations they provide
Indexing. The indexing methods used in individ-
ual systems are usually not reported. Many systems
display nearly constant time for querying a database,
regardless of the selectivity of a query, a strong in-
dicator that no indexes are being used. For exam-
ple, Emu performs all queries in memory with no
indexes, and several others are likely to be the same
(Cassidy and Harrington, 2001; Ko?nig and Lezius,
2001; Heid et al, 2004). TGrep2 (Rohde, 2005) uses
a custom corpus file and processes it sentence by
sentence at query execution time. Other tree query
systems use hashed indexes or other types of in-
memory indexes. However, a common drawback of
these systems is that they are designed for treebanks
that are at most a few million words in size, and do
not scale well to much larger treebanks.
There are many positions to be taken on the above
questions. Our goal is not to argue for a particu-
lar data format or query style, but rather to demon-
strate a powerful technique for indexing and query-
ing treebanks which should be applicable to most of
the above scenarios.
3 Indexing Trees
In this section we discuss two methods of stor-
ing and indexing trees. The first uses a relational
database and linguistic queries are translated into
SQL, while the second uses an inverted index ap-
proach based on an open source IR engine, Lucene.1
Relational databases are a mature technology and
are known to be efficient at performing joins and
accessing data using indexes. Information retrieval
engines using term vectors, on the other hand, ef-
ficiently retrieve documents relevant to a query. IR
engines are known to scale well, but they do not sup-
port complex queries. A common feature of both
the IR and database approaches is the adoption of
so-called ?tree labeling? schemes.
3.1 Tree labeling schemes
Tree queries specify node labels (?value con-
straints?) and structural relationships between nodes
of interest (?structural constraints?). A simple value
constraint could look for a wh noun phrase by spec-
ifying the WHNP; such queries are efficiently im-
plemented using indexes. Structural relationships
cannot be indexed like node labels. A term in a
sentence will have multiple relationships with other
terms in the same sentence. Indexing all pairs of
terms that exist in a given structural relationship re-
sults in an explosion in the index size. Instead, the
standard approach is to store position information
with each occurrence of a term, using a table or a
term vector, and then use the position information
to find structural matches. Many systems use this
approach, from early object databases such as Lore
(McHugh et al, 1997), to relational representation
of tree data (Bird et al, 2006) and XISS/R (Hard-
ing et al, 2003), and native XML databases such as
eXist (Meier, 2003). Here, the position is encoded
via node labeling schemes, and is designed so it can
support efficient testing of a variety of structural re-
lations.
A labeling scheme based on pre-order and post-
order labeling of nodes is the foundation for several
extended schemes. It can be used for efficiently de-
tecting that two nodes are in a hierarchical (or inclu-
sion) relationship. Other labeling schemes are based
on the Dewey scheme, in which each node contains
1http://lucene.apache.org/
269
Figure 1: Generating node labels
a hierarchical label in which numbers are separated
by periods (Tatarinov et al, 2002). A child node gets
its label by appending its position relative to its sib-
lings to its parent?s label. This scheme can be used
for efficiently detecting that two nodes are in a hier-
archical or sequential (temporal) relationship.
The LPath numbering scheme assigns four integer
labels to each node (Bird et al, 2006). The genera-
tion of these labels is explained with the help of an
example. Figure 1 is the graphical representation of
a parse tree for a sentence with 7 words, w1 ? ? ?w7.
Let A, B, C, D, E, and S represent the annotation
tags. Some nodes at different positions in the tree
share a common name.
The first step in labeling is to identify the sequen-
tial positions between words, as shown beneath the
parse tree in Figure 1. The left id of a terminal node
is the sequence position immediately to the left of a
node, while its right id is the one to its immediate
right. The left id of a non-terminal node is the left
id of its leftmost descendant, and the right id is the
right id of its rightmost descendant. In most cases
the ancestor-descendant and preceding-following re-
lationships between two elements can be evaluated
using the left and right ids alone. The sequential ids
do not differentiate between two nodes where one is
the lone child of the other. The depth id is therefore
required in such cases and to identify the child node
(depth values are shown on the left side of Figure 1).
In order to check if two given nodes are siblings,
the above three ids will not suffice. We therefore
assign a common parent id label to siblings. These
four identifiers together enable us to identify rela-
tionships between elements without traversing trees.
Node Left Right Depth Parent
A 2 4 3 2
A 1 4 2 6
A 5 8 3 8
B 3 4 4 4
B 4 5 3 8
B 7 8 4 10
Table 1: Node labels
Table 1 illustrates the node labels assigned to A
and B nodes in Figure 1. We can see that the parent
id of the third A and second B are equal because they
are siblings.
Once these numbers are assigned to each node,
the nodes can be stored independently without loss
of any structural information (in either a relational
database or an inverted index). At query execution
time, the set of elements on either side of an opera-
tor are extracted and only those node numbers that
satisfy the operator?s specification are selected as the
result. For example, if the operator is the child rela-
tion, and the operands are A and B, then there are two
matches: B{3, 4, 4, 4}, child of A{2, 4, 3, 2} and,
B{7, 8, 4, 10}, child of A{5, 8, 3, 8}.2 This process
of finding the elements of a document that match op-
erators is nothing other than the standard join oper-
ation (and it is implemented differently in relational
databases and IR engines).
3.2 Relational database approach
Tree nodes can be stored in a relational database us-
ing a table structure (Bird et al, 2006). Each tree-
bank would have a single table for all nodes where
each node?s information is stored in a tuple. The
node name is stored along with other position infor-
mation and the sentence id. Every node tuple also
has a unique primary key. The parent id column
is a foreign key, referencing the parent node?s id,
speeding up parent/child join operations. In prac-
tice, queries are translated from higher level linguis-
tic query languages such as LPath into SQL auto-
matically, allowing users to use a convenient syntax,
rather than query using SQL.
Previous research on a similar database structure
for containment queries in XML databases showed
2The node labels are represented as an ordered set here for
brevity. Their positions match the headings in Table 1.
270
that databases are generally slower than specialised
IR indexes (Zhang et al, 2001). In that work, the
authors provide results comparing their IR join algo-
rithm, the multi-predicate merge join (MPMGJN),
with two standard relational join algorithms. They
consider the number of comparisons performed in
the standard merge join and the index nested loop
join, and contrast these with their IR join algorithm.
They show that the IR algorithm performs fewer
comparisons than a standard merge join but greater
than the index nested loop join.
The multi-predicate merge join exploits the fact
that nodes are encountered in document order (i.e. a
node appears before its descendents). Search within
a document can be aborted as soon as it is clear that
further searching will not yield further results. Im-
portantly, this IR join algorithm is faster than both
relational join algorithms in practice, since it makes
much better use of the hardware disk cache. Our
own experiments with a large treebank stored in an
Oracle database have demonstrated that this short-
coming of relational query relative to IR query exists
in the linguistic domain (Ghodke and Bird, 2008).
3.3 IR engine approach
We transform the task of searching treebanks into a
conventional document retrieval task in which each
sentence is treated as a document. Tree node labels
are stored in inverted indexes just like words in a
text index. We require two types of indexes, for fre-
quency and position. The frequency index for a node
label contains a list of sentence ids and, for each one,
a count indicating the frequency of the node label
in the sentence. (Labels with a frequency of zero
do not appear in this index.) The position index is
used to store node numbers for each occurrence of
the node label. The numbers at each position are
read into memory as objects only when required (at
other times, the byte numbers are skipped over for
efficiency). During query processing, the frequency
indexes are first traversed sequentially to find a doc-
ument that contains all the required elements in the
query. Once a document is found, the structural con-
straints are checked using the data stored in the po-
sition index for that document. The document itself
does not need to be loaded.
Using an inverted index for searching structured
data is not new, and several XML databases already
use this method to index XML elements (Meier,
2003). However, linguistic query systems are spe-
cial purpose applications where the unit of retrieval
is usually a sentence. A given tree may satisfy a
query in multiple places, but we only identify which
sentences are relevant. Finding all matches within a
sentence requires further processing. 3
Our approach has been to process each sentence
as a document. By fixing the unit of retrieval to be
the sentence, we are able to greatly reduce the size
of intermediate results when performing a series of
joins. The task is then to simply check whether a
sentence satisfies a query or not. This can be done
using substantially less resources than is needed for
finding sets of nodes, the unit of retrieval for rela-
tional and XML databases. When processing a se-
ries of joins, we use a single buffer to store the node
positions required to perform the next join in the se-
ries. After computing that join and processing an-
other operator in the query, the buffer contents is re-
placed with a new set of nodes, discarding the inter-
mediate information.
4 Experiments with IR Engine
4.1 Data
We used two data collections in our experiments.
The first collection is a portion of the English Giga-
word Corpus, parsed in the Penn Treebank format.
We used the TnT tagger and the DBParser trained
on the Wall Street Journal section of the Penn Tree-
bank to parse sentences in the corpus. The total size
of the corpus is about 5.5 million sentences. The
TGrep2 corpus file for this corpus is about 1.8 GB
and the Lucene index is 4 GB on disk. The second
data collection is a portion of English Wikipedia,
again tagged and parsed using TnT tagger and DB-
Parser, respectively. This collection contains 26.5
million parsed sentences. The TGrep2 corpus file
corresponding to this collection is about 6.6 GB and
the Lucene index is 14 GB on disk.
3Several alternate path joins and improvements to the
MPMGJN algorithm have been proposed over the years to over-
come the problem of large number of intermediate nodes and to
reduce unnecessary joins (Al-Khalifa et al, 2002; Li and Moon,
2001). Bruno et al?s work on twig joins further improved on
those efforts by processing an entire query twig in a holistic
fashion (Bruno et al, 2002), and has since been further opti-
mized.
271
Query Selectivity Data Collection 1 (5.5M sentences) Data Collection 2 (26.5M sentences)
Full search First 10 Full search First 10
(//N1 op N2) N1-op-N2 cold warm hits cold warm cold warm hits cold warm
NP/NN L-L-L 7.326 5.533 4,814,540 0.059 0.0003 24.680 20.256 21,906,349 0.260 0.0003
VP/DT L-H-L 4.576 3.593 17,328 0.140 0.004 13.865 11.363 91,070 0.301 0.003
NP/LST L-L-H 4.454 0.043 6,808 0.083 0.001 16.864 0.077 2.974 0.270 0.003
VP/WHPP L-H-H 2.445 0.034 32 1.012 0.014 8.834 0.066 29 3.653 0.015
LST\NP H-L-L 4.444 0.043 6,808 0.080 0.001 16.814 0.077 2,974 0.271 0.003
WHPP\VP H-H-L 2.461 0.034 32 0.990 0.013 8.726 0.065 29 3.611 0.015
LST/LS H-L-H 0.181 0.005 10,432 0.071 0.0001 0.294 0.008 8,977 0.238 0.0002
LST/FW H-H-H 0.123 0.009 4 0.103 0.011 0.348 0.012 9 0.408 0.012
Table 2: Execution times (in seconds) for queries of varying selectivity
4.2 Types of queries
Query performance depends largely on the nature of
the individual queries, therefore we present a de-
tailed analysis of the query types and their corre-
sponding results in this section.
Selectivity: A query term that has few correspond-
ing hits in the corpus will be considered to have high
selectivity. The selectivity of whole queries depends
not only on the selectivity of their individual ele-
ments, but also on how frequently these terms satisfy
the structural constraints specified by the query.
Table 2 gives execution times for queries with
varying selectivity, using our system. We assign a
selectivity measure for the operator based on how
often the two operands satisfy the structural condi-
tion. It is clear that when elements are very common
and they frequently satisfy the structural constraints
of the operator, there are bound to be more run-time
structural checks and the performance deteriorates.
This is demonstrated by the time taken by the first
query. Note the relatively small difference in the ex-
ecution time between the second and third queries.
The third query contains a high selectivity element
and even returns fewer matches compared to the sec-
ond, but takes almost as long. This may be due to the
relative frequency of the tags within each sentence,
which we have not controled in this experiment. If
there are several LST tags in the sentences where
it appears, there are likely to be greater number of
searches within each sentence. A better join algo-
rithm would improve the performance in such cases.
A multiple regression analysis of the full search
(cold start) times for collection 2 shows that low-
selectivity labels contribute 9.5 seconds, and a low-
selectivity operator contributes 6.7 seconds, and that
this accounts for most of the variability in the timing
data (t = ?1.53 + 9.51 ? N1 + 6.72 ? op + 9.44 ?
N2, R2 = 0.8976). This demonstrates that the dis-
tribution of full search (cold start) times is mostly
accounted for by the index load time, with the time
for computing a large join being a secondary cost.
The full search (warm start) times in Table 2 pay a
lesser index loading cost.
Query length: It is evident that the system must
retrieve and process more term vectors as we in-
crease the number of elements in a query. To find
out exactly how the query length affects processing,
we ran tests with three sets of queries. In each set we
varied the number of elements in a dominance rela-
tionship with another node of the same name. The
number of terms in the dominance relationship was
varied from 1 to 6, where the first case is equiva-
lent to just finding all terms with that name. In the
first set, queries search for nested noun phrases (NP),
while the second and third look for adjective phrases
(ADJP) and list elements (LST) respectively.
These terms have been chosen to simultaneously
study the effects of selectivity and query length, with
NP being the least selective (or most common), fol-
lowed by ADJP, then with LST being the most selec-
tive (or least common). NP is also more frequently
self-nested than the others. Figure 2 plots query
length (x-axis) against query execution time (y-axis,
log scale) for the three sets, using our system. With
272
//NP
//ADJP
//LST
Number of elements
Ti
m
e 
(se
co
nd
s) 
1 2 3 4 5 6
0.
00
1
0.
01
0.
1
1
10
10
0
Figure 2: Variation of query execution time with query length in data collection 1
//NP
//ADJP
//LST
Number of elements
Ti
m
e 
(se
co
nd
s) 
1 2 3 4 5 6
0.
00
1
0.
01
0.
1
1
10
10
0
Figure 3: Variation of query execution time with query length in data collection 2
each step on the x-axis, a query will have an extra
descendant node. For example, at position 3 for ele-
ment A, the query would be //A//A//A.
The circles on the plot are proportional to the log
of the result set size. The biggest circle is for //NP
which is of the order of 5.4 million, while there are
only 4 trees in which LST is nested 4 times. LST is
not nested 5 or more times. Similarly, ADJP returns
0 results for the 6th test query and hence there are no
circles at these locations. The thick lines on the plot
indicate the average cold start run time over three
runs, while the dashed line shows the minimum av-
erage run time of 4 sets, with the query executed 5
times in each set. Together, the pairs of unbroken
and dashed lines indicate the variation in run time
depending on the state of the system. 4
4We can observe from the results that the variation be-
4.3 Measurement techniques
The measurement techniques vary for TGrep and
the IR based approach. In TGrep the corpus file is
loaded each time during query processing, but in the
IR approach an index once loaded can operate faster
than a cold start.
In order to understand the variations in the operat-
ing speed we plot the variation in times from a cold
start to a repeat query, as shown in Table 3.
tween cold start and warm start correlates with query length.
The length experiment here use a single term repeated multi-
ple times. However, there is a possibility that the results may
vary when the terms are different, because it would involve ad-
ditional time to load the term vectors of distinct elements into
memory.
273
Query Data collection 1 Data collection 2TGrep2 IR TGrep2 IR
//NP 25.28 8.15 89.35 15.53
//NP//NP 25.44 10.42 88.36 35.95
//NP//NP//NP 25.45 14.96 87.48 52.81
//NP. . . //NP (4 times) 25.34 18.38 88.28 66.80
//NP. . . //NP (5 times) 25.46 20.94 87.38 70.80
//NP. . . //NP (6 times) 25.41 23.23 86.92 75.05
//ADJP 25.48 0.69 86.83 1.03
//ADJP//ADJP 25.36 0.73 86.42 1.61
//ADJP//ADJP//ADJP 25.29 0.84 86.89 1.89
//ADJP. . . //ADJP (4 times) 25.45 0.90 87.39 2.11
//ADJP. . . //ADJP (5 times) 25.23 1.03 86.50 2.49
//ADJP. . . //ADJP (6 times) 25.74 1.11 89.24 2.79
//LST 25.29 0.17 87.73 0.26
//LST//LST 25.49 0.20 87.09 0.27
//LST//LST//LST 25.38 0.20 87.66 0.28
//LST. . . //LST (4 times) 25.43 0.19 87.17 0.29
//LST. . . //LST (5 times) 25.40 0.19 88.02 0.31
//LST. . . //LST (6 times) 25.32 0.19 89.01 0.32
//NP/NN 25.66 7.33 87.63 24.68
//VP/DT 25.53 4.58 89.85 13.86
//NP/LST 25.62 4.45 86.39 16.86
//VP/WHPP 25.09 2.97 87.43 8.83
//WHPP/IN 25.75 4.44 88.48 16.81
//LST/JJ 25.46 2.46 86.57 8.73
//LST/LS 25.38 0.18 87.40 0.29
//LST/FW 25.51 0.12 87.27 0.35
Table 3: Comparison of TGrep2 and IR Engine cold start
query times (seconds)
5 Conclusions
We have shown how an IR engine can be used to
build a high performance tree query system. It
outperforms existing approaches using indexless in-
memory search, or custom indexes, or relational
database systems, or XML database systems. We
reported the results of a variety of experiments to
demonstrate the efficiency of query for a variety of
query types on two treebanks consisting of around
5 and 26 million sentences, more than two orders
of magnitude larger than what existing systems sup-
port. The approach is quite general, and not limited
to particular treebank formats or query languages.
This work suggests that web-scale tree query may
soon be feasible. This opens the door to some in-
teresting possibilities: augmenting web search with
syntactic constraints, the ability discover rare exam-
ples of particular syntactic constructions, and as a
technique for garnering better statistics and more
sensitive features for the purpose of constructing
language models.
Acknowledgments
We gratefully acknowledge support from Microsoft
Research India and the University of Melbourne.
References
Anne Abeille?, editor. 2003. Treebanks: Building and
Using Parsed Corpora. Text, Speech and Language
Technology. Kluwer.
Shurug Al-Khalifa, H.V. Jagadish, Nick Koudas, Jig-
nesh M. Patel, Divesh Srivastava, and Yuqing Wu.
2002. Structural joins: A primitive for efficient XML
query pattern matching. In ICDE ?02: Proc. 18th Intl
Conf on Data Engineering, page 141. IEEE Computer
Society.
Steven Bird, Yi Chen, Susan B. Davidson, Haejoong Lee,
and Yifeng Zheng. 2006. Designing and evaluating
an XPath dialect for linguistic queries. In ICDE ?06:
Proc. 22nd Intl Conf on Data Engineering, page 52.
IEEE Computer Society.
Nicolas Bruno, Nick Koudas, and Divesh Srivastava.
2002. Holistic twig joins: optimal XML pattern
matching. In SIGMOD ?02: Proc. 2002 ACM SIG-
MOD Intl Conf on Management of Data, pages 310?
321. ACM.
Steve Cassidy and Jonathan Harrington. 2001. Multi-
level annotation of speech: an overview of the Emu
Speech Database Management System. Speech Com-
munication, 33:61?77.
Steve Cassidy. 2002. Xquery as an annotation query lan-
guage: a use case analysis. In Proc. 3rd LREC.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Empirical Methods in Natural
Language Processing, pages 41?48.
Abdur Chowdhury and M. Catherine McCabe. 1998.
Performance improvements to vector space informa-
tion retrieval systems with POS. U Maryland.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493?552.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proc. 43rd ACL, pages 507?
514. ACL.
Sumukh Ghodke and Steven Bird. 2008. Querying lin-
guistic annotations. In Proc. 13th Australasian Docu-
ment Computing Symposium, pages 69?72.
Jo?rg Hakenberg, Illes Solt, Domonkos Tikk, Luis Tari,
Astrid Rheinla?nder, Nguyen Quang Long, Graciela
Gonzalez, and Ulf Leser. 2009. Molecular event
extraction from Link Grammar parse trees. In Proc.
BioNLP 2009 Workshop, pages 86?94. ACL.
274
Philip J Harding, Quanzhong Li, and Bongki Moon.
2003. XISS/R: XML indexing and storage system us-
ing RDBMS. In Proc. 29th Intl Conf on Very Large
Data Bases, pages 1073?1076. Morgan Kaufmann.
Ulrich Heid, Holger Voormann, Jan-Torsten Milde, Ul-
rike Gut, Katrin Erk, and Sebastian Pado. 2004.
Querying both time-aligned and hierarchical corpora
with NXT search. In Proc. 4th LREC.
Ichikawa Hiroshi, Noguchi Masaki, Hashimoto Taiichi,
Tokunaga Takenobu, and Tanaka Hozumi. 2005.
eBonsai: An integrated environment for annotating
treebanks. In Proc. 2nd IJCNLP, pages 108?113.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33:355?396.
Hiroshi Ichikawa, Keita Hakoda, Taiichi Hashimoto, and
Takenobu Tokunaga. 2006. Efficient sentence re-
trieval based on syntactic structure. In COLING/ACL,
pages 399?406.
Stephan Kepser. 2003. Finite Structure Query: A tool
for querying syntactically annotated corpora. In Proc.
10th EACL, pages 179?186.
Esther Ko?nig and Wolfgang Lezius. 2001. The TIGER
language: a description language for syntax graphs.
part 1: User?s guidelines. Technical report, University
of Stuttgart.
Catherine Lai and Steven Bird. 2004. Querying and up-
dating treebanks: A critical survey and requirements
analysis. In Proc. Australasian Language Technology
Workshop, pages 139?146.
Catherine Lai and Steven Bird. 2010. Querying linguis-
tic trees. Journal of Logic, Language and Information,
19:53?73.
Quanzhong Li and Bongki Moon. 2001. Indexing and
querying XML data for regular path expressions. In
VLDB ?01: Proc. 27th Intl Conf on Very Large Data
Bases, pages 361?370. Morgan Kaufmann.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?30.
Hendrik Maryns and Stephan Kepser. 2009.
Monasearch: Querying linguistic treebanks with
monadic second-order logic. In The 7th International
Workshop on Treebanks and Linguistic Theories.
Neil Mayo, Jonathan Kilgour, and Jean Carletta. 2006.
Towards an alternative implementation of nxts query
language via xquery. In Proc. 5th Workshop on NLP
and XML: Multi-Dimensional Markup in Natural Lan-
guage Processing, pages 27?34. ACL.
J. McHugh, S. Abiteboul, R. Goldman, D. Quass, and
J. Widom. 1997. Lore: A database management sys-
tem for semistructured data. SIGMOD Rec., 26:54?66.
Wolfgang Meier. 2003. eXist: An open source native
XML database. In Revised Papers from the NODe
2002 Web and Database-Related Workshops on Web,
Web-Services, and Database Systems, pages 169?183.
Springer-Verlag.
Jir??? M??rovsky?. 2006. Netgraph: a tool for searching
in Prague Dependency Treebank 2.0. In Proc. 5th
Intl Conf on Treebanks and Linguistic Theories, pages
211?222.
Jir??? M??rovsky?. 2008. PDT 2.0 requirements on a query
language. In Proc. 46th ACL, pages 37?45. ACL.
Preslav Nakov, Ariel Schwartz, Brian Wolf, and Marti
Hearst. 2005. Supporting annotation layers for natural
language processing. In Proc. 43rd ACL, pages 65?68.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten
Brants. 2002. The LinGO Redwoods Treebank: Mo-
tivation and preliminary applications. In Proc. 19th
COLING, pages 1253?57.
Petr Pajas and Jan ?Ste?pa?nek. 2009. System for querying
syntactically annotated corpora. In Proc. 47th ACL,
pages 33?36. ACL.
Douglas L. T. Rohde, 2005. TGrep2 User Manual Ver-
sion 1.15. http://tedlab.mit.edu/ dr/TGrep2/tgrep2.pdf.
Igor Tatarinov, Stratis D. Viglas, Kevin Beyer, Jayavel
Shanmugasundaram, Eugene Shekita, and Chun
Zhang. 2002. Storing and querying ordered XML
using a relational database system. In SIGMOD ?02:
Proc. 2002 ACM SIGMOD Intl Conf on Management
of Data, pages 204?215. ACM.
M. ?Cmejrek, J. Cur???n, and J. Havelka. 2004. Prague
czech-english dependency treebank: Any hopes for a
common annotation scheme? In A. Meyers, editor,
HLT-NAACL 2004 Workshop: Frontiers in Corpus An-
notation, pages 47?54. ACL.
Martin Volk, Joakim Lundborg, and Mae?l Mettler. 2007.
A search tool for parallel treebanks. In Proc. Linguis-
tic Annotation Workshop, pages 85?92. ACL.
Sean Wallis. 2003. Completing parsed corpora. In
Anne Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, Text, Speech and Language Technol-
ogy, pages 61?71. Kluwer.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11:207?238.
Chun Zhang, Jeffrey Naughton, David DeWitt, Qiong
Luo, and Guy Lohman. 2001. On supporting contain-
ment queries in relational database management sys-
tems. In SIGMOD ?01: Proc. ACM SIGMOD inter-
national Conference on Management of Data, pages
425?436, New York. ACM.
275
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 88?97,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
The Human Language Project:
Building a Universal Corpus of the World?s Languages
Steven Abney
University of Michigan
abney@umich.edu
Steven Bird
University of Melbourne and
University of Pennsylvania
sbird@unimelb.edu.au
Abstract
We present a grand challenge to build a
corpus that will include all of the world?s
languages, in a consistent structure that
permits large-scale cross-linguistic pro-
cessing, enabling the study of universal
linguistics. The focal data types, bilin-
gual texts and lexicons, relate each lan-
guage to one of a set of reference lan-
guages. We propose that the ability to train
systems to translate into and out of a given
language be the yardstick for determin-
ing when we have successfully captured a
language. We call on the computational
linguistics community to begin work on
this Universal Corpus, pursuing the many
strands of activity described here, as their
contribution to the global effort to docu-
ment the world?s linguistic heritage before
more languages fall silent.
1 Introduction
The grand aim of linguistics is the construction of
a universal theory of human language. To a com-
putational linguist, it seems obvious that the first
step is to collect significant amounts of primary
data for a large variety of languages. Ideally, we
would like a complete digitization of every human
language: a Universal Corpus.
If we are ever to construct such a corpus, it must
be now. With the current rate of language loss, we
have only a small window of opportunity before
the data is gone forever. Linguistics may be unique
among the sciences in the crisis it faces. The next
generation will forgive us for the most egregious
shortcomings in theory construction and technol-
ogy development, but they will not forgive us if we
fail to preserve vanishing primary language data in
a form that enables future research.
The scope of the task is enormous. At present,
we have non-negligible quantities of machine-
readable data for only about 20?30 of the world?s
6,900 languages (Maxwell and Hughes, 2006).
Linguistics as a field is awake to the crisis. There
has been a tremendous upsurge of interest in doc-
umentary linguistics, the field concerned with the
the ?creation, annotation, preservation, and dis-
semination of transparent records of a language?
(Woodbury, 2010). However, documentary lin-
guistics alone is not equal to the task. For example,
no million-word machine-readable corpus exists
for any endangered language, even though such a
quantity would be necessary for wide-ranging in-
vestigation of the language once no speakers are
available. The chances of constructing large-scale
resources will be greatly improved if computa-
tional linguists contribute their expertise.
This collaboration between linguists and com-
putational linguists will extend beyond the con-
struction of the Universal Corpus to its exploita-
tion for both theoretical and technological ends.
We envisage a new paradigm of universal linguis-
tics, in which grammars of individual languages
are built from the ground up, combining expert
manual effort with the power tools of probabilis-
tic language models and grammatical inference.
A universal grammar captures redundancies which
exist across languages, constituting a ?universal
linguistic prior,? and enabling us to identify the
distinctive properties of specific languages and
families. The linguistic prior and regularities due
to common descent enable a new economy of scale
for technology development: cross-linguistic tri-
angulation can improve performance while reduc-
ing per-language data requirements.
Our aim in the present paper is to move beyond
generalities to a concrete plan of attack, and to
challenge the field to a communal effort to cre-
ate a Universal Corpus of the world?s languages,
in consistent machine-readable format, permitting
large-scale cross-linguistic processing.
88
2 Human Language Project
2.1 Aims and scope
Although language endangerment provides ur-
gency, the corpus is not intended primarily as
a Noah?s Ark for languages. The aims go be-
yond the current crisis: we wish to support cross-
linguistic research and technology development at
the largest scale. There are existing collections
that contain multiple languages, but it is rare to
have consistent formats and annotation across lan-
guages, and few such datasets contain more than a
dozen or so languages.
If we think of a multi-lingual corpus as con-
sisting of an array of items, with columns repre-
senting languages and rows representing resource
types, the usual focus is on ?vertical? processing.
Our particular concern, by contrast, is ?horizontal?
processing that cuts indiscriminately across lan-
guages. Hence we require an unusual degree of
consistency across languages.
The kind of processing we wish to enable is
much like the large-scale systematic research that
motivated the Human Genome Project.
One of the greatest impacts of having
the sequence may well be in enabling
an entirely new approach to biological
research. In the past, researchers stud-
ied one or a few genes at a time. With
whole-genome sequences . . . they can
approach questions systematically and
on a grand scale. They can study . . .
how tens of thousands of genes and pro-
teins work together in interconnected
networks to orchestrate the chemistry of
life. (Human Genome Project, 2007)
We wish to make it possible to investigate human
language equally systematically and on an equally
grand scale: a Human Linguome Project, as it
were, though we have chosen the ?Human Lan-
guage Project? as a more inviting title for the un-
dertaking. The product is a Universal Corpus,1 in
two senses of universal: in the sense of including
(ultimately) all the world?s languages, and in the
sense of enabling software and processing meth-
ods that are language-universal.
However, we do not aim for a collection that
is universal in the sense of encompassing all lan-
guage documentation efforts. Our goal is the con-
struction of a specific resource, albeit a very large
1http://universalcorpus.org/
resource. We contrast the proposed effort with
general efforts to develop open resources, stan-
dards, and best practices. We do not aim to be all-
inclusive. The project does require large-scale col-
laboration, and a task definition that is simple and
compelling enough to achieve buy-in from a large
number of data providers. But we do not need and
do not attempt to create consensus across the en-
tire community. (Although one can hope that what
proves successful for a project of this scale will
provide a good foundation for future standards.)
Moreover, we do not aim to collect data
merely in the vague hope that it will prove use-
ful. Although we strive for maximum general-
ity, we also propose a specific driving ?use case,?
namely, machine translation (MT), (Hutchins and
Somers, 1992; Koehn, 2010). The corpus pro-
vides a testing ground for the development of MT
system-construction methods that are dramatically
?leaner? in their resource requirements, and which
take advantage of cross-linguistic bootstrapping.
The large engineering question is how one can
turn the size of the task?constructing MT systems
for all the world?s languages simultaneously?to
one?s advantage, and thereby consume dramati-
cally less data per language.
The choice of MT as the use case is also driven
by scientific considerations. To explain, we re-
quire a bit of preamble.
We aim for a digitization of each human lan-
guage. What exactly does it mean to digitize an
entire language? It is natural to think in terms
of replicating the body of resources available for
well-documented languages, and the pre-eminent
resource for any language is a treebank. Producing
a treebank involves a staggering amount of man-
ual effort. It is also notoriously difficult to obtain
agreement about how parse trees should be defined
in one language, much less in many languages si-
multaneously. The idea of producing treebanks for
6,900 languages is quixotic, to put it mildly. But
is a treebank actually necessary?
Let us suppose that the purpose of a parse
tree is to mediate interpretation. A treebank, ar-
guably, represents a theoretical hypothesis about
how interpretations could be constructed; the pri-
mary data is actually the interpretations them-
selves. This suggests that we annotate sentences
with representations of meanings instead of syn-
tactic structures. Now that seems to take us out of
the frying pan into the fire. If obtaining consen-
89
sus on parse trees is difficult, obtaining consensus
on meaning representations is impossible. How-
ever, if the language under consideration is any-
thing other than English, then a translation into
English (or some other reference language) is for
most purposes a perfectly adequate meaning rep-
resentation. That is, we view machine translation
as an approximation to language understanding.
Here is another way to put it. One measure of
adequacy of a language digitization is the abil-
ity of a human?already fluent in a reference
language?to acquire fluency in the digitized lan-
guage using only archived material. Now it would
be even better if we could use a language digiti-
zation to construct an artificial speaker of the lan-
guage. Importantly, we do not need to solve the AI
problem: the speaker need not decide what to say,
only how to translate from meanings to sentences
of the language, and from sentences back to mean-
ings. Taking sentences in a reference language as
the meaning representation, we arrive back at ma-
chine translation as the measure of success. In
short, we have successfully captured a language if
we can translate into and out of the language.
The key resource that should be built for each
language, then, is a collection of primary texts
with translations into a reference language. ?Pri-
mary text? includes both written documents and
transcriptions of recordings. Large volumes of pri-
mary texts will be useful even without translation
for such tasks as language modeling and unsuper-
vised learning of morphology. Thus, we antici-
pate that the corpus will have the usual ?pyrami-
dal? structure, starting from a base layer of unan-
notated text, some portion of which is translated
into a reference language at the document level to
make the next layer. Note that, for maximally au-
thentic primary texts, we assume the direction of
translation will normally be from primary text to
reference language, not the other way around.
Another layer of the corpus consists of sentence
and word alignments, required for training and
evaluating machine translation systems, and for
extracting bilingual lexicons. Curating such anno-
tations is a more specialized task than translation,
and so we expect it will only be done for a subset
of the translated texts.
In the last and smallest layer, morphology is an-
notated. This supports the development of mor-
phological analyzers, to preprocess primary texts
to identify morpheme boundaries and recognize
allomorphs, reducing the amount of data required
for training an MT system. This most-refined
target annotation corresponds to the interlinear
glossed texts that are the de facto standard of anno-
tation in the documentary linguistics community.
We postulate that interlinear glossed text is suf-
ficiently fine-grained to serve our purposes. It
invites efforts to enrich it by automatic means:
for example, there has been work on parsing the
English translations and using the word-by-word
glosses to transfer the parse tree to the object lan-
guage, effectively creating a treebank automati-
cally (Xia and Lewis, 2007). At the same time, we
believe that interlinear glossed text is sufficiently
simple and well-understood to allow rapid con-
struction of resources, and to make cross-linguistic
consistency a realistic goal.
Each of these layers?primary text, translations,
alignments, and morphological glosses?seems to
be an unavoidable piece of the overall solution.
The fact that these layers will exist in diminishing
quantity is also unavoidable. However, there is an
important consequence: the primary texts will be
permanently subject to new translation initiatives,
which themselves will be subject to new align-
ment and glossing initiatives, in which each step
is an instance of semisupervised learning (Abney,
2007). As time passes, our ability to enhance the
quantity and quality of the annotations will only
increase, thanks to effective combinations of auto-
matic, professional, and crowd-sourced effort.
2.2 Principles
The basic principles upon which the envisioned
corpus is based are the following:
Universality. Covering as many languages as
possible is the first priority. Progress will be
gauged against concrete goals for numbers of lan-
guages, data per language, and coverage of lan-
guage families (Whalen and Simons, 2009).
Machine readability and consistency. ?Cover-
ing? languages means enabling machine process-
ing seamlessly across languages. This will sup-
port new types of linguistic inquiry and the devel-
opment and testing of inference methods (for mor-
phology, parsers, machine translation) across large
numbers of typologically diverse languages.
Community effort. We cannot expect a single
organization to assemble a resource on this scale.
It will be necessary to get community buy-in, and
90
many motivated volunteers. The repository will
not be the sole possession of any one institution.
Availability. The content of the corpus will be
available under one or more permissive licenses,
such as the Creative Commons Attribution Li-
cense (CC-BY), placing as few limits as possible
on community members? ability to obtain and en-
hance the corpus, and redistribute derivative data.
Utility. The corpus aims to be maximally use-
ful, and minimally parochial. Annotation will be
as lightweight as possible; richer annotations will
will emerge bottom-up as they prove their utility
at the large scale.
Centrality of primary data. Primary texts and
recordings are paramount. Secondary resources
such as grammars and lexicons are important, but
no substitute for primary data. It is desirable that
secondary resources be integrated with?if not de-
rived from?primary data in the corpus.
2.3 What to include
What should be included in the corpus? To some
extent, data collection will be opportunistic, but
it is appropriate to have a well-defined target in
mind. We consider the following essential.
Metadata. One means of resource identification
is to survey existing documentation for the lan-
guage, including bibliographic references and lo-
cations of web resources. Provenance and proper
citation of sources should be included for all data.
For written text. (1) Primary documents in
original printed form, e.g. scanned page images or
PDF. (2) Transcription. Not only optical charac-
ter recognition output, but also the output of tools
that extract text from PDF, will generally require
manual editing.
For spoken text. (1) Audio recordings. Both
elicited and spontaneous speech should be in-
cluded. It is highly desirous to have some con-
nected speech for every language. (2) Slow speech
?audio transcriptions.? Carefully respeaking a
spoken text can be much more efficient than writ-
ten transcription, and may one day yield to speech
recognition methods. (3) Written transcriptions.
We do not impose any requirements on the form
of transcription, though orthographic transcription
is generally much faster to produce than phonetic
transcription, and may even be more useful as
words are represented by normalized forms.
For both written and spoken text. (1) Trans-
lations of primary documents into a refer-
ence language (possibly including commentary).
(2) Sentence-level segmentation and transla-
tion. (3) Word-level segmentation and glossing.
(4) Morpheme-level segmentation and glossing.
All documents will be included in primary
form, but the percentage of documents with man-
ual annotation, or manually corrected annotation,
decreases at increasingly fine-grained levels of an-
notation. Where manual fine-grained annotation is
unavailable, automatic methods for creating it (at a
lower quality) are desirable. Defining such meth-
ods for a large range of resource-poor languages is
an interesting computational challenge.
Secondary resources. Although it is possible to
base descriptive analyses exclusively on a text cor-
pus (Himmelmann, 2006, p. 22), the following
secondary resources should be secured if they are
available: (1) A lexicon with glosses in a reference
language. Ideally, everything should be attested in
the texts, but as a practical matter, there will be
words for which we have only a lexical entry and
no instances of use. (2) Paradigms and phonol-
ogy, for the construction of a morphological ana-
lyzer. Ideally, they should be inducible from the
texts, but published grammatical information may
go beyond what is attested in the text.
2.4 Inadequacy of existing efforts
Our key desideratum is support for automatic pro-
cessing across a large range of languages. No data
collection effort currently exists or is proposed, to
our knowledge, that addresses this desideratum.
Traditional language archives such as the Audio
Archive of Linguistic Fieldwork (UC Berkeley),
Documentation of Endangered Languages (Max
Planck Institute, Nijmegen), the Endangered Lan-
guages Archive (SOAS, University of London),
and the Pacific And Regional Archive for Digi-
tal Sources in Endangered Cultures (Australia) of-
fer broad coverage of languages, but the majority
of their offerings are restricted in availability and
do not support machine processing. Conversely,
large-scale data collection efforts by the Linguis-
tic Data Consortium and the European Language
Resources Association cover less than one percent
of the world?s languages, with no evident plans for
major expansion of coverage. Other efforts con-
cern the definition and aggregation of language
resource metadata, including OLAC, IMDI, and
91
CLARIN (Simons and Bird, 2003; Broeder and
Wittenburg, 2006; Va?radi et al, 2008), but this is
not the same as collecting and disseminating data.
Initiatives to develop standard formats for lin-
guistic annotations are orthogonal to our goals.
The success of the project will depend on con-
tributed data from many sources, in many differ-
ent formats. Converting all data formats to an
official standard, such as the RDF-based models
being developed by ISO Technical Committee 37
Sub-committee 4 Working Group 2, is simply im-
practical. These formats have onerous syntactic
and semantic requirements that demand substan-
tial further processing together with expert judg-
ment, and threaten to crush the large-scale collab-
orative data collection effort we envisage, before
it even gets off the ground. Instead, we opt for a
very lightweight format, sketched in the next sec-
tion, to minimize the effort of conversion and en-
able an immediate start. This does not limit the
options of community members who desire richer
formats, since they are free to invest the effort in
enriching the existing data. Such enrichment ef-
forts may gain broad support if they deliver a tan-
gible benefit for cross-language processing.
3 A Simple Storage Model
Here we sketch a simple approach to storage of
texts (including transcribed speech), bitexts, inter-
linear glossed text, and lexicons. We have been
deliberately schematic since the goal is just to give
grounds for confidence that there exists a general,
scalable solution.
For readability, our illustrations will include
space-separated sequences of tokens. However,
behind the scenes these could be represented as
a sequence of pairs of start and end offsets into a
primary text or speech signal, or as a sequence of
integers that reference an array of strings. Thus,
when we write (1a), bear in mind it may be imple-
mented as (1b) or (1c).
(1) a. This is a point of order .
b. (0,4), (5,7), (8,9), (10,15), (16,18), . . .
c. 9347, 3053, 0038, 3342, 3468, . . .
In what follows, we focus on the minimal re-
quirements for storing and disseminating aligned
text, not the requirements for efficient in-memory
data structures. Moreover, we are agnostic about
whether the normalized, tokenized format is stored
entire or computed on demand.
We take an aligned text to be composed of a
series of aligned sentences, each consisting of a
small set of attributes and values, e.g.:
ID: europarl/swedish/ep-00-01-17/18
LANGS: swd eng
SENT: det ga?ller en ordningsfra?ga
TRANS: this is a point of order
ALIGN: 1-1 2-2 3-3 4-4 4-5 4-6
PROVENANCE: pharaoh-v1.2, ...
REV: 8947 2010-05-02 10:35:06 leobfld12
RIGHTS: Copyright (C) 2010 Uni...; CC-BY
The value of ID identifies the document and sen-
tence, and any collection to which the document
belongs. Individual components of the identi-
fier can be referenced or retrieved. The LANGS
attribute identifies the source and reference lan-
guage using ISO 639 codes.2 The SENT attribute
contains space-delimited tokens comprising a sen-
tence. Optional attributes TRANS and ALIGN
hold the translation and alignment, if these are
available; they are omitted in monolingual text.
A provenance attribute records any automatic or
manual processes which apply to the record, and
a revision attribute contains the version number,
timestamp, and username associated with the most
recent modification of the record, and a rights at-
tribute contains copyright and license information.
When morphological annotation is available, it
is represented by two additional attributes, LEX
and AFF. Here is a monolingual example:
ID: example/001
LANGS: eng
SENT: the dogs are barking
LEX: the dog be bark
AFF: - PL PL ING
Note that combining all attributes of these
two examples?that is, combining word-by-word
translation with morphological analysis?yields
interlinear glossed text.
A bilingual lexicon is an indispensable re-
source, whether provided as such, induced from
a collection of aligned text, or created by merg-
ing contributed and induced lexicons. A bilin-
gual lexicon can be viewed as an inventory of
cross-language correspondences between words
or groups of words. These correspondences are
just aligned text fragments, albeit much smaller
than a sentence. Thus, we take a bilingual lexicon
to be a kind of text in which each record contains
a single lexeme and its translation, represented us-
ing the LEX and TRANS attributes we have already
introduced, e.g.:
2http://www.sil.org/iso639-3/
92
ID: swedishlex/v3.2/0419
LANGS: swd eng
LEX: ordningsfra?ga
TRANS: point of order
In sum, the Universal Corpus is represented as
a massive store of records, each representing a
single sentence or lexical entry, using a limited
set of attributes. The store is indexed for effi-
cient access, and supports access to slices identi-
fied by language, content, provenance, rights, and
so forth. Many component collections would be
?unioned? into this single, large Corpus, with only
the record identifiers capturing the distinction be-
tween the various data sources.
Special cases of aligned text and wordlists,
spanning more than 1,000 languages, are Bible
translations and Swadesh wordlists (Resnik et al,
1999; Swadesh, 1955). Here there are obvious
use-cases for accessing a particular verse or word
across all languages. However, it is not neces-
sary to model n-way language alignments. In-
stead, such sources are implicitly aligned by virtue
of their structure. Extracting all translations of
a verse, or all cognates of a Swadesh wordlist
item, is an index operation that returns monolin-
gual records, e.g.:
ID: swadesh/47 ID: swadesh/47
LANGS: fra LANGS: eng
LEX: chien LEX: dog
4 Building the Corpus
Data collection on this scale is a daunting
prospect, yet it is important to avoid the paraly-
sis of over-planning. We can start immediately by
leveraging existing infrastructure, and the volun-
tary effort of interested members of the language
resources community. One possibility is to found
a ?Language Commons,? an open access reposi-
tory of language resources hosted in the Internet
Archive, with a lightweight method for commu-
nity members to contribute data sets.
A fully processed and indexed version of se-
lected data can be made accessible via a web ser-
vices interface to a major cloud storage facility,
such as Amazon Web Services. A common query
interface could be supported via APIs in multi-
ple NLP toolkits such as NLTK and GATE (Bird
et al, 2009; Cunningham et al, 2002), and also
in generic frameworks such as UIMA and SOAP,
leaving developers to work within their preferred
environment.
4.1 Motivation for data providers
We hope that potential contributors of data will
be motivated to participate primarily by agree-
ment with the goals of the project. Even some-
one who has specialized in a particular language
or language family maintains an interest, we ex-
pect, in the universal question?the exploration of
Language writ large.
Data providers will find benefit in the availabil-
ity of volunteers for crowd-sourcing, and tools for
(semi-)automated quality control, refinement, and
presentation of data. For example, a data holder
should be able to contribute recordings and get
help in transcribing them, through a combination
of volunteer labor and automatic processing.
Documentary linguists and computational lin-
guists have much to gain from collaboration. In re-
turn for the data that documentary linguistics can
provide, computational linguistics has the poten-
tial to revolutionize the tools and practice of lan-
guage documentation.
We also seek collaboration with communities of
language speakers. The corpus provides an econ-
omy of scale for the development of literacy mate-
rials and tools for interactive language instruction,
in support of language preservation and revitaliza-
tion. For small languages, literacy in the mother
tongue is often defended on the grounds that it pro-
vides the best route to literacy in the national lan-
guage (Wagner, 1993, ch. 8). An essential ingredi-
ent of any local literacy program is to have a sub-
stantial quantity of available texts that represent
familiar topics including cultural heritage, folk-
lore, personal narratives, and current events. Tran-
sition to literacy in a language of wider commu-
nication is aided when transitional materials are
available (Waters, 1998, pp. 61ff). Mutual bene-
fits will also flow from the development of tools
for low-cost publication and broadcast in the lan-
guage, with copies of the published or broadcast
material licensed to and archived in the corpus.
4.2 Roles
The enterprise requires collaboration of many in-
dividuals and groups, in a variety of roles.
Editors. A critical group are people with suffi-
cient engagement to serve as editors for particular
language families, who have access to data or are
able to negotiate redistribution rights, and oversee
the workflow of transcription, translation, and an-
notation.
93
CL Research. All manual annotation steps need
to be automated. Each step presents a challeng-
ing semi-supervised learning and cross-linguistic
bootstrapping problem. In addition, the overall
measure of success?induction of machine trans-
lation systems from limited resources?pushes the
state of the art (Kumar et al, 2007). Numerous
other CL problems arise: active learning to im-
prove the quality of alignments and bilingual lex-
icons; automatic language identification for low-
density languages; and morphology learning.
Tool builders. We need tools for annotation, for-
mat conversion, spidering and language identifica-
tion, search, archiving, and presentation. Innova-
tive crowd-sourcing solutions are of particular in-
terest, e.g. web-based functionality for transcrib-
ing audio and video of oral literature, or setting up
a translation service based on aligned texts for a
low-density language, and collecting the improved
translations suggested by users.
Volunteer annotators. An important reason for
keeping the data model as lightweight as possible
is to enable contributions from volunteers with lit-
tle or no linguistic training. Two models are the
volunteers who scan documents and correct OCR
output in Project Gutenberg, or the undergraduate
volunteers who have constructed Greek and Latin
treebanks within Project Perseus (Crane, 2010).
Bilingual lexicons that have been extracted from
aligned text collections might be corrected using
crowd-sourcing, leading to improved translation
models and improved alignments. We also see the
Universal Corpus as an excellent opportunity for
undergraduates to participate in research, and for
native speakers to participate in the preservation of
their language.
Documentary linguists. The collection proto-
col known as Basic Oral Language Documentation
(BOLD) enables documentary linguists to collect
2?3 orders of magnitude more oral discourse than
before (Bird, 2010). Linguists can equip local
speakers to collect written texts, then to carefully
?respeak? and orally translate the texts into a refer-
ence language. With suitable tools, incorporating
active learning, local speakers could further curate
bilingual texts and lexicons. An early need is pi-
lot studies to determine costings for different cat-
egories of language.
Data agencies. The LDC and ELRA have a cen-
tral role to play, given their track record in obtain-
ing, curating, and publishing data with licenses
that facilitate language technology development.
We need to identify key resources where negoti-
ation with the original data provider, and where
payment of all preparation costs plus compensa-
tion for lost revenue, leads to new material for the
Corpus. This is a new publication model and a
new business model, but it can co-exist with the
existing models.
Language archives. Language archives have a
special role to play as holders of unique materi-
als. They could contribute existing data in its na-
tive format, for other participants to process. They
could give bilingual texts a distinct status within
their collections, to facilitate discovery.
Funding agencies. To be successful, the Human
Language Project would require substantial funds,
possibly drawing on a constellation of public and
private agencies in many countries. However, in
the spirit of starting small, and starting now, agen-
cies could require that sponsored projects which
collect texts and build lexicons contribute them to
the Language Commons. After all, the most effec-
tive time to do translation, alignment, and lexicon
work is often at the point when primary data is
first collected, and this extra work promises direct
benefits to the individual project.
4.3 Early tasks
Seed corpus. The central challenge, we believe,
is getting critical mass. Data attracts data, and if
one can establish a sufficient seed, the effort will
snowball. We can make some concrete proposals
as to how to collect a seed. Language resources
on the web are one source?the Cru?bada?n project
has identified resources for 400 languages, for ex-
ample (Scannell, 2008); the New Testament of the
Bible exists in about 1200 languages and contains
of the order of 100k words. We hope that exist-
ing efforts that are already well-disposed toward
electronic distribution will participate. We partic-
ularly mention the Language and Culture Archive
of the Summer Institute of Linguistics, and the
Rosetta Project. The latter is already distributed
through the Internet Archive and contains material
for 2500 languages.
Resource discovery. Existing language re-
sources need to be documented, a large un-
94
dertaking that depends on widely distributed
knowledge. Existing published corpora from the
LDC, ELRA and dozens of other sources?a total
of 85,000 items?are already documented in the
combined catalog of the Open Language Archives
Community,3 so there is no need to recreate this
information. Other resources can be logged by
community members using a public access wiki,
with a metadata template to ensure key fields are
elicited such as resource owner, license, ISO 639
language code(s), and data type. This information
can itself be curated and stored in the form of an
OLAC archive, to permit search over the union of
the existing and newly documented items. Work
along these lines has already been initiated by
LDC and ELRA (Cieri et al, 2010).
Resource classification. Editors with knowl-
edge of particular language families will catego-
rize documented resources relative to the needs of
the project, using controlled vocabularies. This
involves examining a resource, determining the
granularity and provenance of the segmentation
and alignment, checking its ISO 639 classifi-
cations, assigning it to a logarithmic size cate-
gory, documenting its format and layout, collect-
ing sample files, and assigning a priority score.
Acquisition. Where necessary, permission will
be sought to lodge the resource in the repository.
Funding may be required to buy the rights to the
resource from its owner, as compensation for lost
revenue from future data sales. Funding may be
required to translate the source into a reference
language. The repository?s ingestion process is
followed, and the resource metadata is updated.
Text collection. Languages for which the avail-
able resources are inadequate are identified, and
the needs are prioritized, based on linguistic and
geographical diversity. Sponsorship is sought
for collecting bilingual texts in high priority lan-
guages. Workflows are developed for languages
based on a variety of factors, such as availability
of educated people with native-level proficiency
in their mother tongue and good knowledge of
a reference language, internet access in the lan-
guage area, availability of expatriate speakers in a
first-world context, and so forth. A classification
scheme is required to help predict which work-
flows will be most successful in a given situation.
3http://www.language-archives.org/
Audio protocol. The challenge posed by lan-
guages with no written literature should not be
underestimated. A promising collection method
is Basic Oral Language Documentation, which
calls for inexpensive voice recorders and net-
books, project-specific software for transcription
and sentence-aligned translation, network band-
width for upload to the repository, and suitable
training and support throughout the process.
Corpus readers. Software developers will in-
spect the file formats and identify high priority for-
mats based on information about resource priori-
ties and sizes. They will code a corpus reader, an
open source reference implementation for convert-
ing between corpus formats and the storage model
presented in section 3.
4.4 Further challenges
There are many additional difficulties that could
be listed, though we expect they can be addressed
over time, once a sufficient seed corpus is estab-
lished. Two particular issues deserve further com-
ment, however.
Licenses. Intellectual property issues surround-
ing linguistic corpora present a complex and
evolving landscape (DiPersio, 2010). For users, it
would be ideal for all materials to be available un-
der a single license that permits derivative works,
commercial use, and redistribution, such as the
Creative Commons Attribution License (CC-BY).
There would be no confusion about permissible
uses of subsets and aggregates of the collected cor-
pora, and it would be easy to view the Universal
Corpus as a single corpus. But to attract as many
data contributors as possible, we cannot make such
a license a condition of contribution.
Instead, we propose to distinguish between:
(1) a digital Archive of contributed corpora that
are stored in their original format and made avail-
able under a range of licenses, offering preserva-
tion and dissemination services to the language
resources community at large (i.e. the Language
Commons); and (2) the Universal Corpus, which
is embodied as programmatic access to an evolv-
ing subset of materials from the archive under
one of a small set of permissive licenses, licenses
whose unions and intersections are understood
(e.g. CC-BY and its non-commercial counterpart
CC-BY-NC). Apart from being a useful service in
its own right, the Archive would provide a staging
95
ground for the Universal Corpus. Archived cor-
pora having restrictive licenses could be evaluated
for their potential as contributions to the Corpus,
making it possible to prioritize the work of nego-
tiating more liberal licenses.
There are reasons to distinguish Archive and
Corpus even beyond the license issues. The Cor-
pus, but not the Archive, is limited to the formats
that support automatic cross-linguistic processing.
Conversely, since the primary interface to the Cor-
pus is programmatic, it may include materials that
are hosted in many different archives; it only needs
to know how to access and deliver them to the user.
Incidentally, we consider it an implementation is-
sue whether the Corpus is provided as a web ser-
vice, a download service with user-side software,
user-side software with data delivered on physical
media, or a cloud application with user programs
executed server-side.
Expenses of conversion and editing. We do not
trivialize the work involved in converting docu-
ments to the formats of section 3, and in manu-
ally correcting the results of noisy automatic pro-
cesses such as optical character recognition. In-
deed, the amount of work involved is one moti-
vation for the lengths to which we have gone to
keep the data format simple. For example, we have
deliberately avoided specifying any particular to-
kenization scheme. Variation will arise as a con-
sequence, but we believe that it will be no worse
than the variability in input that current machine
translation training methods routinely deal with,
and will not greatly injure the utility of the Corpus.
The utter simplicity of the formats also widens the
pool of potential volunteers for doing the manual
work that is required. By avoiding linguistically
delicate annotation, we can take advantage of mo-
tivated but untrained volunteers such as students
and members of speaker communities.
5 Conclusion
Nearly twenty years ago, the linguistics commu-
nity received a wake-up call, when Hale et al
(1992) predicted that 90% of the world?s linguis-
tic diversity would be lost or moribund by the year
2100, and warned that linguistics might ?go down
in history as the only science that presided oblivi-
ously over the disappearance of 90 per cent of the
very field to which it is dedicated.? Today, lan-
guage documentation is a high priority in main-
stream linguistics. However, the field of computa-
tional linguistics is yet to participate substantially.
The first half century of research in compu-
tational linguistics?from circa 1960 up to the
present?has touched on less than 1% of the
world?s languages. For a field which is justly
proud of its empirical methods, it is time to apply
those methods to the remaining 99% of languages.
We will never have the luxury of richly annotated
data for these languages, so we are forced to ask
ourselves: can we do more with less?
We believe the answer is ?yes,? and so we chal-
lenge the computational linguistics community to
adopt a scalable computational approach to the
problem. We need leaner methods for building
machine translation systems; new algorithms for
cross-linguistic bootstrapping via multiple paths;
more effective techniques for leveraging human
effort in labeling data; scalable ways to get bilin-
gual text for unwritten languages; and large scale
social engineering to make it all happen quickly.
To believe we can build this Universal Corpus is
certainly audacious, but not to even try is arguably
irresponsible. The initial step parallels earlier ef-
forts to create large machine-readable text collec-
tions which began in the 1960s and reverberated
through each subsequent decade. Collecting bilin-
gual texts is an orthodox activity, and many alter-
native conceptions of a Human Language Project
would likely include this as an early task.
The undertaking ranks with the largest data-
collection efforts in science today. It is not achiev-
able without considerable computational sophis-
tication and the full engagement of the field of
computational linguistics. Yet we require no fun-
damentally new technologies. We can build on
our strengths in corpus-based methods, linguis-
tic models, human- and machine-supplied annota-
tions, and learning algorithms. By rising to this,
the greatest language challenge of our time, we
enable multi-lingual technology development at a
new scale, and simultaneously lay the foundations
for a new science of empirical universal linguis-
tics.
Acknowledgments
We are grateful to Ed Bice, Doug Oard, Gary
Simons, participants of the Language Commons
working group meeting in Boston, students in
the ?Digitizing Languages? seminar (University of
Michigan), and anonymous reviewers, for feed-
back on an earlier version of this paper.
96
References
Steven Abney. 2007. Semisupervised Learning for
Computational Linguistics. Chapman & Hall/CRC.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media. http://nltk.org/book.
Steven Bird. 2010. A scalable method for preserving
oral literature from small languages. In Proceedings
of the 12th International Conference on Asia-Pacific
Digital Libraries, pages 5?14.
Daan Broeder and Peter Wittenburg. 2006. The IMDI
metadata framework, its current application and fu-
ture direction. International Journal of Metadata,
Semantics and Ontologies, 1:119?132.
Christopher Cieri, Khalid Choukri, Nicoletta Calzo-
lari, D. Terence Langendoen, Johannes Leveling,
Martha Palmer, Nancy Ide, and James Pustejovsky.
2010. A road map for interoperable language re-
source metadata. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Eval-
uation (LREC).
Gregory R. Crane. 2010. Perseus Digital Library:
Research in 2008/09. http://www.perseus.
tufts.edu/hopper/research/current.
Accessed Feb. 2010.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE: an
architecture for development of robust HLT appli-
cations. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 168?175. Association for Computational
Linguistics.
Denise DiPersio. 2010. Implications of a permis-
sions culture on the development and distribution
of language resources. In FLaReNet Forum 2010.
Fostering Language Resources Network. http:
//www.flarenet.eu/.
Hale, M. Krauss, L. Watahomigie, A. Yamamoto, and
C. Craig. 1992. Endangered languages. Language,
68(1):1?42.
Nikolaus P. Himmelmann. 2006. Language documen-
tation: What is it and what is it good for? In
Jost Gippert, Nikolaus Himmelmann, and Ulrike
Mosel, editors, Essentials of Language Documenta-
tion, pages 1?30. Mouton de Gruyter.
Human Genome Project. 2007. The science
behind the Human Genome Project. http:
//www.ornl.gov/sci/techresources/
Human_Genome/project/info.shtml.
Accessed Dec. 2007.
W. John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 42?50,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Mike Maxwell and Baden Hughes. 2006. Frontiers
in linguistic annotation for lower-density languages.
In Proceedings of the Workshop on Frontiers in Lin-
guistically Annotated Corpora 2006, pages 29?37,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999. The Bible as a parallel corpus: Annotating
the ?book of 2000 tongues?. Computers and the Hu-
manities, 33:129?153.
Kevin Scannell. 2008. The Cru?bada?n Project: Corpus
building for under-resourced languages. In Cahiers
du Cental 5: Proceedings of the 3rd Web as Corpus
Workshop.
Gary Simons and Steven Bird. 2003. The Open Lan-
guage Archives Community: An infrastructure for
distributed archiving of language resources. Liter-
ary and Linguistic Computing, 18:117?128.
Morris Swadesh. 1955. Towards greater accuracy
in lexicostatistic dating. International Journal of
American Linguistics, 21:121?137.
Tama?s Va?radi, Steven Krauwer, Peter Wittenburg,
Martin Wynne, and Kimmo Koskenniemi. 2008.
CLARIN: common language resources and technol-
ogy infrastructure. In Proceedings of the Sixth Inter-
national Language Resources and Evaluation Con-
ference. European Language Resources Association.
Daniel A. Wagner. 1993. Literacy, Culture, and Devel-
opment: Becoming Literate in Morocco. Cambridge
University Press.
Glenys Waters. 1998. Local Literacies: Theory and
Practice. Summer Institute of Linguistics, Dallas.
Douglas H. Whalen and Gary Simons. 2009. En-
dangered language families. In Proceedings of the
1st International Conference on Language Docu-
mentation and Conservation. University of Hawaii.
http://hdl.handle.net/10125/5017.
Anthony C. Woodbury. 2010. Language documenta-
tion. In Peter K. Austin and Julia Sallabank, edi-
tors, The Cambridge Handbook of Endangered Lan-
guages. Cambridge University Press.
Fei Xia and William D. Lewis. 2007. Multilingual
structural projection across interlinearized text. In
Proceedings of the Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL). Association for Computational
Linguistics.
97
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1506?1515,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Collective Classification of Congressional Floor-Debate Transcripts
Clinton Burfoot, Steven Bird and Timothy Baldwin
Department of Computer Science and Software Engineering
University of Melbourne, VIC 3010, Australia
{cburfoot, sb, tim}@csse.unimelb.edu.au
Abstract
This paper explores approaches to sentiment
classification of U.S. Congressional floor-
debate transcripts. Collective classification
techniques are used to take advantage of the
informal citation structure present in the de-
bates. We use a range of methods based on
local and global formulations and introduce
novel approaches for incorporating the outputs
of machine learners into collective classifica-
tion algorithms. Our experimental evaluation
shows that the mean-field algorithm obtains
the best results for the task, significantly out-
performing the benchmark technique.
1 Introduction
Supervised document classification is a well-studied
task. Research has been performed across many
document types with a variety of classification tasks.
Examples are topic classification of newswire ar-
ticles (Yang and Liu, 1999), sentiment classifica-
tion of movie reviews (Pang et al, 2002), and satire
classification of news articles (Burfoot and Baldwin,
2009). This and other work has established the use-
fulness of document classifiers as stand-alone sys-
tems and as components of broader NLP systems.
This paper deals with methods relevant to super-
vised document classification in domains with net-
work structures, where collective classification can
yield better performance than approaches that con-
sider documents in isolation. Simply put, a network
structure is any set of relationships between docu-
ments that can be used to assist the document clas-
sification process. Web encyclopedias and scholarly
publications are two examples of document domains
where network structures have been used to assist
classification (Gantner and Schmidt-Thieme, 2009;
Cao and Gao, 2005).
The contribution of this research is in four parts:
(1) we introduce an approach that gives better than
state of the art performance for collective classifica-
tion on the ConVote corpus of congressional debate
transcripts (Thomas et al, 2006); (2) we provide a
comparative overview of collective document classi-
fication techniques to assist researchers in choosing
an algorithm for collective document classification
tasks; (3) we demonstrate effective novel approaches
for incorporating the outputs of SVM classifiers into
collective classifiers; and (4) we demonstrate effec-
tive novel feature models for iterative local classifi-
cation of debate transcript data.
In the next section (Section 2) we provide a for-
mal definition of collective classification and de-
scribe the ConVote corpus that is the basis for our
experimental evaluation. Subsequently, we describe
and critique the established benchmark approach for
congressional floor-debate transcript classification,
before describing approaches based on three alterna-
tive collective classification algorithms (Section 3).
We then present an experimental evaluation (Sec-
tion 4). Finally, we describe related work (Section 5)
and offer analysis and conclusions (Section 6).
2 Task Definition
2.1 Collective Classification
Given a network and an object o in the network,
there are three types of correlations that can be used
1506
to infer a label for o: (1) the correlations between
the label of o and its observed attributes; (2) the cor-
relations between the label of o and the observed at-
tributes and labels of nodes connected to o; and (3)
the correlations between the label of o and the un-
observed labels of objects connected to o (Sen et al,
2008).
Standard approaches to classification generally
ignore any network information and only take into
account the correlations in (1). Each object is clas-
sified as an individual instance with features derived
from its observed attributes. Collective classification
takes advantage of the network by using all three
sources. Instances may have features derived from
their source objects or from other objects. Classifi-
cation proceeds in a joint fashion so that the label
given to each instance takes into account the labels
given to all of the other instances.
Formally, collective classification takes a graph,
made up of nodes V = {V1, . . . , Vn} and edges
E. The task is to label the nodes Vi ? V from
a label set L = {L1, . . . , Lq}, making use of the
graph in the form of a neighborhood function N =
{N1, . . . , Nn}, where Ni ? V \ {Vi}.
2.2 The ConVote Corpus
ConVote, compiled by Thomas et al (2006), is a
corpus of U.S. congressional debate transcripts. It
consists of 3,857 speeches organized into 53 debates
on specific pieces of legislation. Each speech is
tagged with the identity of the speaker and a ?for?
or ?against? label derived from congressional voting
records. In addition, places where one speaker cites
another have been annotated, as shown in Figure 1.
We apply collective classification to ConVote de-
bates by letting V refer to the individual speakers in a
debate and populatingN using the citation graph be-
tween speakers. We set L = {y, n}, corresponding
to ?for? and ?against? votes respectively. The text
of each instance is the concatenation of the speeches
by a speaker within a debate. This results in a corpus
of 1,699 instances with a roughly even class distri-
bution. Approximately 70% of these are connected,
i.e. they are the source or target of one or more cita-
tions. The remainder are isolated.
3 Collective Classification Techniques
In this section we describe techniques for perform-
ing collective classification on the ConVote cor-
pus. We differentiate between dual-classifier and
iterative-classifier approaches.
Dual-classifier approach: This approach uses
a collective classification algorithm that takes inputs
from two classifiers: (1) a content-only classifier that
determines the likelihood of a y or n label for an in-
stance given its text content; and (2) a citation clas-
sifier that determines, based on citation information,
whether a given pair of instances are ?same class? or
?different class?.
Let ? denote a set of functions representing the
classification preferences produced by the content-
only and citation classifiers:
? For each Vi ? V , ?i ? ? is a function ?i: L ?
R+ ? {0}.
? For each (Vi, Vj) ? E, ?ij ? ? is a function
?ij : L ? L ? R+ ? {0}.
Later in this section we will describe three collec-
tive classification algorithms capable of performing
overall classification based on these inputs: (1) the
minimum-cut approach, which is the benchmark for
collective classification with ConVote, established
by Thomas et al; (2) loopy belief propagation; and
(3) mean-field. We will show that these latter two
techniques, which are both approximate solutions
for Markov random fields, are superior to minimum-
cut for the task.
Figure 2 gives a visual overview of the dual-
classifier approach.
Iterative-classifier approach: This approach
incorporates content-only and citation features into
a single local classifier that works on the assump-
tion that correct neighbor labels are already known.
This approach represents a marked deviation from
the dual-classifier approach and offers unique ad-
vantages. It is fully described in Section 3.4.
Figure 3 gives a visual overview of the iterative-
classifier approach.
For a detailed introduction to collective classifica-
tion see Sen et al (2008).
1507
Debate 006
Speaker 400378 [against]
Mr. Speaker, . . . all over Washington and in the country, people are talking today about the
majority?s last-minute decision to abandon . . .
. . .
Speaker 400115 [for]
. . .
Mr. Speaker, . . . I just want to say to the gentlewoman from New York that every single member
of this institution . . .
. . .
Figure 1: Sample speech fragments from the ConVote corpus. The phrase gentlewoman from New York by speaker
400115 is annotated as a reference to speaker 400378.
Debate content
Citation vectorsContent-only vectors
Content-only classifications Citation classifications
Content-only and
citation scores
Overall classifications
Extract features Extract features
SVM SVM
NormaliseNormalise
MF/LBP/Mincut
Figure 2: Dual-classifier approach.
Debate content
Content-only vectors
Content-only classifications
Local vectors
Local classifications
Overall classifications
Extract features
SVM
Combine content-only
and citation features
SVM
Update citation features
Terminate iteration
Figure 3: Iterative-classifier approach.
3.1 Dual-classifier Approach with
Minimum-cut
Thomas et al use linear kernel SVMs as their base
classifiers. The content-only classifier is trained to
predict y or n based on the unigram presence fea-
tures found in speeches. The citation classifier is
trained to predict ?same class? or ?different class?
labels based on the unigram presence features found
in the context windows (30 tokens before, 20 tokens
after) surrounding citations for each pair of speakers
in the debate.
The decision plane distance computed by the
content-only SVM is normalized to a positive real
number and stripped of outliers:
?i(y) =
?
??
??
1 di > 2?i;(
1 + di2?i
)
/2 |di| ? 2?i;
0 di < ?2?i
where ?i is the standard deviation of the decision
plane distance, di, over all of the instances in the
debate and ?i(n) = 1??i(y). The citation classifier
output is processed similarly:1
?ij(y, y) =
?
?
?
0 dij < ?;
? ? dij/4?ij ? ? dij ? 4?ij ;
? dij > 4?ij
where ?ij is the standard deviation of the decision
plane distance, dij over all of the citations in the de-
bate and ?ij(n, n) = ?ij(y, y). The ? and ? vari-
ables are free parameters.
A given class assignment v is assigned a cost that
is the sum of per-instance and per-pair class costs
derived from the content-only and citation classifiers
respectively:
c(v) =
?
Vi?V
?i(v?i) +
?
(Vi,Vj)?E:vi 6=vj
?ij(vi, vi)
where vi is the label of node Vi and v?i denotes the
complement class of vi.
1Thomas et al classify each citation context window sep-
arately, so their ? values are actually calculated in a slightly
more complicated way. We adopted the present approach for
conceptual simplicity and because it gave superior performance
in preliminary experiments.
1508
The cost function is modeled in a flow graph
where extra source and sink nodes represent the y
and n labels respectively. Each node in V is con-
nected to the source and sink with capacities ?i(y)
and ?i(n) respectively. Pairs classified in the ?same
class? class are linked with capacities defined by ?.
An exact optimum and corresponding overall
classification is efficiently computed by finding the
minimum-cut of the flow graph (Blum and Chawla,
2001). The free parameters are tuned on a set of
held-out data.
Thomas et al demonstrate improvements over
content-only classification, without attempting to
show that the approach does better than any alter-
natives; the main appeal is the simplicity of the flow
graph model. There are a number of theoretical lim-
itations to the approach, which we now discuss.
As Thomas et al point out, the model has no way
of representing the ?different class? output from the
citation classifier and these citations must be dis-
carded. This, to us, is the most significant problem
with the model. Inspection of the corpus shows that
approximately 80% of citations indicate agreement,
meaning that for the present task the impact of dis-
carding this information may not be large. However,
the primary utility in collective approaches lies in
their ability to fill in gaps in information not picked
up by content-only classification. All available link
information should be applied to this end, so we
need models capable of accepting both positive and
negative information.
The normalization techniques used for converting
SVM outputs to graph weights are somewhat arbi-
trary. The use of standard deviations appears prob-
lematic as, intuitively, the strength of a classification
should be independent of its variance. As a case in
point, consider a set of instances in a debate all clas-
sified as similarly weak positives by the SVM. Use
of ?i as defined above would lead to these being er-
roneously assigned the maximum score because of
their low variance.
The minimum-cut approach places instances in
either the positive or negative class depending on
which side of the cut they fall on. This means
that no measure of classification confidence is avail-
able. This extra information is useful at the very
least to give a human user an idea of how much to
trust the classification. A measure of classification
confidence may also be necessary for incorporation
into a broader system, e.g., a meta-classifier (An-
dreevskaia and Bergler, 2008; Li and Zong, 2008).
Tuning the ? and ? parameters is likely to become
a source of inaccuracy in cases where the tuning and
test debates have dissimilar link structures. For ex-
ample, if the tuning debates tend to have fewer, more
accurate links the ? parameter will be higher. This
will not produce good results if the test debates have
more frequent, less accurate links.
3.2 Heuristics for Improving Minimum-cut
Bansal et al (2008) offer preliminary work describ-
ing additions to the Thomas et al minimum-cut ap-
proach to incorporate ?different class? citation clas-
sifications. They use post hoc adjustments of graph
capacities based on simple heuristics. Two of the
three approaches they trial appear to offer perfor-
mance improvements:
The SetTo heuristic: This heuristic works
through E in order and tries to force Vi and Vj into
different classes for every ?different class? (dij < 0)
citation classifier output where i < j. It does this by
altering the four relevant content-only preferences,
?i(y), ?i(n), ?j(y), and ?j(n). Assume without
loss of generality that the largest of these values is
?i(y). If this preference is respected, it follows that
Vj should be put into class n. Bansal et al instanti-
ate this chain of reasoning by setting:
? ??i(y) = max(?, ?i(y))
? ??j(n) = max(?, ?j(n))
where ?? is the replacement content-only function,
? is a free parameter ? (.5, 1], ??i(n) = 1 ? ?
?
i(y),
and ??j(y) = 1? ?
?
j(y).
The IncBy heuristic: This heuristic is a more
conservative version of the SetTo heuristic. Instead
of replacing the content-only preferences with fixed
constants, it increments and decrements the previous
values so they are somewhat preserved:
? ??i(y) = min(1, ?i(y) + ?)
? ??j(n) = min(1, ?j(n) + ?)
There are theoretical shortcomings with these ap-
proaches. The most obvious problem is the arbitrary
nature of the manipulations, which produce a flow
1509
graph that has an indistinct relationship to the out-
puts of the two classifiers.
Bensal et al trial a range of ? values, with vary-
ing impacts on performance. No attempt is made to
demonstrate a method for choosing a good ? value.
It is not clear that the tuning approach used to set ?
and ? would be successful here. In any case, having
a third parameter to tune would make the process
more time-consuming and increase the risks of in-
correct tuning, described above.
As Bansal et al point out, proceeding through E
in order means that earlier changes may be undone
for speakers who have multiple ?different class? ci-
tations.
Finally, we note that the confidence of the cita-
tion classifier is not embodied in the graph structure.
The most marginal ?different class? citation, classi-
fied just on the negative side of the decision plane, is
treated identically to the most confident one furthest
from the decision plane.
3.3 Dual-classifier Approach with Markov
Random Field Approximations
A pairwise Markov random field (Taskar et al,
2002) is given by the pair (G,?), where G and ?
are as previously defined, ? being re-termed as a set
of clique potentials. Given an assignment v to the
nodes V , the pairwise Markov random field is asso-
ciated with the probability distribution:
P (v) =
1
Z
?
Vi?V
?i(vi)
?
(Vi,Vj)?E
?ij(vi, vj)
where:
Z =
?
v?
?
Vi?V
?i(v
?
i)
?
(Vi,Vj)?E
?ij(v
?
i, v
?
j)
and v?i denotes the label of Vi for an alternative as-
signment in v?.
In general, exact inference over a pairwise
Markov random field is known to be NP-hard. There
are certain conditions under which exact inference
is tractable, but real-world data is not guaranteed to
satisfy these. A class of approximate inference al-
gorithms known as variational methods (Jordan et
al., 1999) solve this problem by substituting a sim-
pler ?trial? distribution which is fitted to the Markov
random field distribution.
Loopy Belief Propagation: Applied to a pair-
wise Markov random field, loopy belief propagation
is a message passing algorithm that can be concisely
expressed as the following set of equations:
mi?j(vj) = ?
?
vi?L
{?ij(vi, vj)?i(vi)
?
Vk?Ni?V\Vj
mk?i(vi),?vj ? L}
bi(vi) = ??i(vi)
?
Vj?Ni?V
mj?i(vi),?vi ? L
where mi?j is a message sent by Vi to Vj and ? is
a normalization constant that ensures that each mes-
sage and each set of marginal probabilities sum to 1.
The algorithm proceeds by making each node com-
municate with its neighbors until the messages sta-
bilize. The marginal probability is then derived by
calculating bi(vi).
Mean-Field: The basic mean-field algorithm can
be described with the equation:
bj(vj) = ??j(vj)
?
Vi?Nj?V
?
vi?L
?bi(vi)ij (vi, vj), vj ? L
where ? is a normalization constant that ensures
?
vj
bj(vj) = 1. The algorithm computes the fixed
point equation for every node and continues to do so
until the marginal probabilities bj(vj) stabilize.
Mean-field can be shown to be a variational
method in the same way as loopy belief propagation,
using a simpler trial distribution. For details see Sen
et al (2008).
Probabilistic SVM Normalisation: Unlike
minimum-cut, the Markov random field approaches
have inherent support for the ?different class? out-
put of the citation classifier. This allows us to ap-
ply a more principled SVM normalisation technique.
Platt (1999) describes a technique for converting the
output of an SVM classifier to a calibrated posterior
probability. Platt finds that the posterior can be fit
using a parametric form of a sigmoid:
P (y = 1|d) =
1
1 + exp(Ad+B)
This is equivalent to assuming that the output of
the SVM is proportional to the log odds of a positive
example. Experimental analysis shows error rate is
1510
improved over a plain linear SVM and probabilities
are of comparable quality to those produced using a
regularized likelihood kernel method.
By applying this technique to the base classifiers,
we can produce new, simpler ? functions, ?i(y) =
Pi and ?ij(y, y) = Pij where Pi is the probabilis-
tic normalized output of the content-only classifier
and Pij is the probabilistic normalized output of the
citation classifier.
This approach addresses the problems with the
Thomas et al method where the use of standard
deviations can produce skewed normalizations (see
Section 3.1). By using probabilities we also open
up the possibility of replacing the SVM classifiers
with any other model than can be made to produce
a probability. Note also that there are no parameters
to tune.
3.4 Iterative Classifier Approach
The dual-classifier approaches described above rep-
resent global attempts to solve the collective classifi-
cation problem. We can choose to narrow our focus
to the local level, in which we aim to produce the
best classification for a single instance with the as-
sumption that all other parts of the problem (i.e. the
correct labeling of the other instances) are solved.
The Iterative Classification Algorithm (Bilgic et
al., 2007), defined in Algorithm 1, is a simple tech-
nique for performing collective classification using
such a local classifier. After bootstrapping with a
content-only classifier, it repeatedly generates new
estimates for vi based on its current knowledge of
Ni. The algorithm terminates when the predictions
stabilize or a fixed number of iterations is com-
pleted. Each iteration is completed using a newly
generated ordering O, over the instances V .
We propose three feature models for the local
classifier.
Citation presence and Citation count: Given
that the majority of citations represent the ?same
class? relationship (see Section 3.1), we can an-
ticipate that content-only classification performance
will be improved if we add features to represent the
presence of neighbours of each class.
We define the function c(i, l) =
?
vj?Ni?V
?vj ,l
giving the number of neighbors for node Vi with la-
bel l, where ? is the Kronecker delta. We incorporate
these citation count values, one for the supporting
Algorithm 1 Iterative Classification Algorithm
for each node Vi ? V do {bootstrapping}
compute ~ai using only local attributes of node
vi ? f(~ai)
end for
repeat {iterative classification}
randomly generate ordering O over nodes in V
for each node Vi ? O do
{compute new estimate of vi}
compute ~ai using current assignments to Ni
vi ? f(~ai)
end for
until labels have stabilized or maximum iterations
reached
class and one for the opposing class, obtaining a new
feature vector (u1i , u
2
i , . . . , u
j
i , c(i, y), c(i, n)) where
u1i , u
2
i , . . . , u
j
i are the elements of ~ui, the binary un-
igram feature vector used by the content-only clas-
sifier to represent instance i.
Alternatively, we can represent neighbor labels
using binary citation presence values where any
non-zero count becomes a 1 in the feature vector.
Context window: We can adopt a more nu-
anced model for citation information if we incor-
porate the citation context window features into the
feature vector. This is, in effect, a synthesis of
the content-only and citation feature models. Con-
text window features come from the product space
L ? C, where C is the set of unigrams used in ci-
tation context windows and ~ci denotes the context
window features for instance i. The new feature vec-
tor becomes: (u1i , u
2
i , . . . , u
j
i , c
1
i , c
2
i , . . . , c
k
i ). This
approach implements the intuition that speakers in-
dicate their voting intentions by the words they use
to refer to speakers whose vote is known. Because
neighbor relations are bi-directional the reverse is
also true: Speakers indicate other speakers? voting
intentions by the words they use to refer to them.
As an example, consider the context window fea-
ture AGREE-FOR, indicating the presence of the
agree unigram in the citation window I agree with
the gentleman from Louisiana, where the label for
the gentleman from Louisiana instance is y. This
feature will be correctly correlated with the y label.
Similarly, if the unigram were disagree the feature
would be correlated with the n label.
1511
4 Experiments
In this section we compare the performance of our
dual-classifier and iterative-classifier approaches.
We also evaluate the performance of the three fea-
ture models for local classification.
All accuracies are given as the percentages of
instances correctly classified. Results are macro-
averaged using 10 ? 10-fold cross validation, i.e.
10 runs of 10-fold cross validation using different
randomly assigned data splits.
Where quoted, statistical significance has been
calculated using a two-tailed paired t-test measured
over all 100 pairs with 10 degrees of freedom. See
Bouckaert (2003) for an experimental justification
for this approach.
Note that the results presented in this section
are not directly comparable with those reported by
Thomas et al and Bansal et al because their exper-
iments do not use cross-validation. See Section 4.3
for further discussion of experimental configuration.
4.1 Local Classification
We evaluate three models for local classification: ci-
tation presence features, citation count features and
context window features. In each case the SVM
classifier is given feature vectors with both content-
only and citation information, as described in Sec-
tion 3.4.
Table 1 shows that context window performs the
best with 89.66% accuracy, approximately 1.5%
ahead of citation count and 3.5% ahead of citation
presence. All three classifiers significantly improve
on the content-only classifier.
These relative scores seem reasonable. Knowing
the words used in citations of each class is better
than knowing the number of citations in each class,
and better still than only knowing which classes of
citations exist.
These results represent an upper-bound for the
performance of the iterative classifier, which re-
lies on iteration to produce the reliable information
about citations given here by oracle.
4.2 Collective Classification
Table 2 shows overall results for the three collective
classification algorithms. The iterative classifier was
run separately with citation count and context win-
Method Accuracy (%)
Majority 52.46
Content-only 75.29
Citation presence 85.01
Citation count 88.18
Context window 89.66
Table 1: Local classifier accuracy. All three local
classifiers are significant over the in-isolation classifier
(p < .001).
dow citation features, the two best performing local
classification methods, both with a threshold of 30
iterations.
Results are shown for connected instances, iso-
lated instances, and all instances. Collective clas-
sification techniques can only have an impact on
connected instances, so these figures are most im-
portant. The figures for all instances show the per-
formance of the classifiers in our real-world task,
where both connected and isolated instances need to
be classified and the end-user may not distinguish
between the two types.
Each of the four collective classifiers outperform
the minimum-cut benchmark over connected in-
stances, with the iterative classifier (context win-
dow) (79.05%) producing the smallest gain of less
than 1% and mean-field doing best with a nearly
6% gain (84.13%). All show a statistically signif-
icant improvement over the content-only classifier.
Mean-field shows a statistically significant improve-
ment over minimum-cut.
The dual-classifier approaches based on loopy
belief propagation and mean-field do better than
the iterative-classifier approaches by an average of
about 3%.
Iterative classification performs slightly better
with citation count features than with context win-
dow features, despite the fact that the context win-
dow model performs better in the local classifier
evaluation. We speculate that this may be due to ci-
tation count performing better when given incorrect
neighbor labels. This is an aspect of local classi-
fier performance we do not otherwise measure, so a
clear conclusion is not possible. Given the closeness
of the results it is also possible that natural statistical
variation is the cause of the difference.
1512
The performance of the minimum-cut method is
not reliably enhanced by either the SetTo or IncBy
heuristics. Only IncBy(.15) gives a very small im-
provement (0.14%) over plain minimum-cut. All
of the other combinations tried diminished perfor-
mance slightly.
4.3 A Note on Error Propagation and
Experimental Configuration
Early in our experimental work we noticed that per-
formance often varied greatly depending on the de-
bates that were allocated to training, tuning and test-
ing. This observation is supported by the per-fold
scores that are the basis for the macro-average per-
formance figures reported in Table 2, which tend
to have large standard deviations. The absolute
standard deviations over the 100 evaluations for the
minimum-cut and mean-field methods were 11.19%
and 8.94% respectively. These were significantly
larger than the standard deviation for the content-
only baseline, which was 7.34%. This leads us to
conclude that the performance of collective classifi-
cation methods is highly variable.
Bilgic and Getoor (2008) offer a possible expla-
nation for this. They note that the cost of incor-
rectly classifying a given instance can be magnified
in collective classification, because errors are prop-
agated throughout the network. The extent to which
this happens may depend on the random interaction
between base classification accuracy and network
structure. There is scope for further work to more
fully explain this phenomenon.
From these statistical and theoretical factors we
infer that more reliable conclusions can be drawn
from collective classification experiments that use
cross-validation instead of a single, fixed data split.
5 Related work
Somasundaran et al (2009) use ICA to improve sen-
timent polarity classification of dialogue acts in a
corpus of multi-party meeting transcripts. Link fea-
tures are derived from annotations giving frame re-
lations and target relations. Respectively, these re-
late dialogue acts based on the sentiment expressed
and the object towards which the sentiment is ex-
pressed. Somasundaran et al provides another ar-
gument for the usefulness of collective classification
(specifically ICA), in this case as applied at a dia-
logue act level and relying on a complex system of
annotations for link information.
Somasundaran and Wiebe (2009) propose an un-
supervised method for classifying the stance of each
contribution to an online debate concerning the mer-
its of competing products. Concessions to other
stances are modeled, but there are no overt citations
in the data that could be used to induce the network
structure required for collective classification.
Pang and Lee (2005) use metric labeling to per-
form multi-class collective classification of movie
reviews. Metric labeling is a multi-class equiva-
lent of the minimum-cut technique in which opti-
mization is done over a cost function incorporat-
ing content-only and citation scores. Links are con-
structed between test instances and a set of k near-
est neighbors drawn only from the training set. Re-
stricting the links in this way means the optimization
problem is simple. A similarity metric is used to find
nearest neighbors.
The Pang and Lee method is an instance of im-
plicit link construction, an approach which is be-
yond the scope of this paper but nevertheless an im-
portant area for future research. A similar technique
is used in a variation on the Thomas et al experi-
ment where additional links between speeches are
inferred via a similarity metric (Burfoot, 2008). In
cases where both citation and similarity links are
present, the overall link score is taken as the sum of
the two scores. This seems counter-intuitive, given
that the two links are unlikely to be independent. In
the framework of this research, the approach would
be to train a link meta-classifier to take scores from
both link classifiers and output an overall link prob-
ability.
Within NLP, the use of LBP has not been re-
stricted to document classification. Examples of
other applications are dependency parsing (Smith
and Eisner, 2008) and alignment (Cromires and
Kurohashi, 2009). Conditional random fields
(CRFs) are an approach based on Markov random
fields that have been popular for segmenting and
labeling sequence data (Lafferty et al, 2001). We
rejected linear-chain CRFs as a candidate approach
for our evaluation on the grounds that the arbitrar-
ily connected graphs used in collective classification
can not be fully represented in graphical format, i.e.
1513
Connected Isolated All
Majority 52.46 46.29 50.51
Content only 75.31 78.90 76.28
Minimum-cut 78.31 78.90 78.40
Minimum-cut (SetTo(.6)) 78.22 78.90 78.32
Minimum-cut (SetTo(.8)) 78.01 78.90 78.14
Minimum-cut (SetTo(1)) 77.71 78.90 77.93
Minimum-cut (IncBy(.05)) 78.14 78.90 78.25
Minimum-cut (IncBy(.15)) 78.45 78.90 78.46
Minimum-cut (IncBy(.25)) 78.02 78.90 78.15
Iterative-classifier (citation count) 80.07? 78.90 79.69?
Iterative-classifier (context window) 79.05 78.90 78.93
Loopy Belief Propagation 83.37? 78.90 81.93?
Mean-Field 84.12? 78.90 82.45?
Table 2: Speaker classification accuracies (%) over connected, isolated and all instances. The marked results are
statistically significant over the content only benchmark (? p < .01, ? p < .001). The mean-field results are statistically
significant over minimum-cut (p < .05).
linear-chain CRFs do not scale to the complexity of
graphs used in this research.
6 Conclusions and future work
By applying alternative models, we have demon-
strated the best recorded performance for collective
classification of ConVote using bag-of-words fea-
tures, beating the previous benchmark by nearly 6%.
Moreover, each of the three alternative approaches
trialed are theoretically superior to the minimum-cut
approach approach for three main reasons: (1) they
support multi-class classification; (2) they support
negative and positive citations; (3) they require no
parameter tuning.
The superior performance of the dual-classifier
approach with loopy belief propagation and mean-
field suggests that either algorithm could be consid-
ered as a first choice for collective document classi-
fication. Their advantage is increased by their abil-
ity to output classification confidences as probabili-
ties, while minimum-cut and the local formulations
only give absolute class assignments. We do not dis-
miss the iterative-classifier approach entirely. The
most compelling point in its favor is its ability to
unify content only and citation features in a single
classifier. Conceptually speaking, such an approach
should allow the two types of features to inter-relate
in more nuanced ways. A case in point comes from
our use of a fixed size context window to build a
citation classifier. Future approaches may be able
to do away with this arbitrary separation of features
by training a local classifier to consider all words in
terms of their impact on content-only classification
and their relations to neighbors.
Probabilistic SVM normalization offers a conve-
nient, principled way of incorporating the outputs of
an SVM classifier into a collective classifier. An op-
portunity for future work is to consider normaliza-
tion approaches for other classifiers. For example,
confidence-weighted linear classifiers (Dredze et al,
2008) have been shown to give superior performance
to SVMs on a range of tasks and may therefore be a
better choice for collective document classification.
Of the three models trialled for local classifiers,
context window features did best when measured in
an oracle experiment, but citation count features did
better when used in a collective classifier. We con-
clude that context window features are a more nu-
anced and powerful approach that is also more likely
to suffer from data sparseness. Citation count fea-
tures would have been the less effective in a scenario
where the fact of the citation existing was less infor-
mative, for example, if a citation was 50% likely to
indicate agreement rather than 80% likely. There is
much scope for further research in this area.
1514
References
Alina Andreevskaia and Sabine Bergler. 2008. When
specialists and generalists work together: Overcom-
ing domain dependence in sentiment tagging. In ACL,
pages 290?298.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The
power of negative thinking: Exploiting label disagree-
ment in the min-cut classification framework. In COL-
ING, pages 15?18.
Mustafa Bilgic and Lise Getoor. 2008. Effective label
acquisition for collective classification. In KDD, pages
43?51.
Mustafa Bilgic, Galileo Namata, and Lise Getoor. 2007.
Combining collective classification and link predic-
tion. In ICDM Workshops, pages 381?386. IEEE
Computer Society.
Avrim Blum and Shuchi Chawla. 2001. Learning from
labeled and unlabeled data using graph mincuts. In
ICML, pages 19?26.
Remco R. Bouckaert. 2003. Choosing between two
learning algorithms based on calibrated tests. In
ICML, pages 51?58.
Clint Burfoot and Timothy Baldwin. 2009. Automatic
satire detection: Are you having a laugh? In ACL-
IJCNLP Short Papers, pages 161?164.
Clint Burfoot. 2008. Using multiple sources of agree-
ment information for sentiment classification of polit-
ical transcripts. In Australasian Language Technology
Association Workshop 2008, pages 11?18. ALTA.
Minh Duc Cao and Xiaoying Gao. 2005. Combining
contents and citations for scientific document classifi-
cation. In 18th Australian Joint Conference on Artifi-
cial Intelligence, pages 143?152.
Fabien Cromires and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In EACL, pages
166?174.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML, pages 264?271.
Zeno Gantner and Lars Schmidt-Thieme. 2009. Auto-
matic content-based categorization of Wikipedia ar-
ticles. In 2009 Workshop on The People?s Web
Meets NLP: Collaboratively Constructed Semantic
Resources, pages 32?37.
Michael Jordan, Zoubin Ghahramani, Tommi Jaakkola,
Lawrence Saul, and David Heckerman. 1999. An in-
troduction to variational methods for graphical mod-
els. Machine Learning, 37:183?233.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Shoushan Li and Chengqing Zong. 2008. Multi-domain
sentiment classification. In ACL, pages 257?260.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In ACL, pages 115?124.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using ma-
chine learning techniques. In EMNLP, pages 79?86.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized likeli-
hood methods. In A. Smola, P. Bartlett, B. Scholkopf,
and D. Schuurmans, editors, Advances in Large Mar-
gin Classifiers, pages 61?74. MIT Press.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,
Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29:93?106.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In EMNLP, pages 145?
156.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In ACL-IJCNLP,
pages 226?234.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In EMNLP, pages
170?179.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In UAI.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In EMNLP, pages
327?335.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In Proceedings ACM SI-
GIR, pages 42?49.
1515
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 634?639,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Simpler unsupervised POS tagging with bilingual projections
Long Duong, 12 Paul Cook, 1 Steven Bird, 1 and Pavel Pecina2
1 Department of Computing and Information Systems, The University of Melbourne
2 Charles University in Prague, Czech Republic
lduong@student.unimelb.edu.au, paulcook@unimelb.edu.au,
sbird@unimelb.edu.au, pecina@ufal.mff.cuni.cz
Abstract
We present an unsupervised approach to
part-of-speech tagging based on projec-
tions of tags in a word-aligned bilingual
parallel corpus. In contrast to the exist-
ing state-of-the-art approach of Das and
Petrov, we have developed a substantially
simpler method by automatically identi-
fying ?good? training sentences from the
parallel corpus and applying self-training.
In experimental results on eight languages,
our method achieves state-of-the-art re-
sults.
1 Unsupervised part-of-speech tagging
Currently, part-of-speech (POS) taggers are avail-
able for many highly spoken and well-resourced
languages such as English, French, German, Ital-
ian, and Arabic. For example, Petrov et al (2012)
build supervised POS taggers for 22 languages us-
ing the TNT tagger (Brants, 2000), with an aver-
age accuracy of 95.2%. However, many widely-
spoken languages ? including Bengali, Javanese,
and Lahnda ? have little data manually labelled
for POS, limiting supervised approaches to POS
tagging for these languages.
However, with the growing quantity of text
available online, and in particular, multilingual
parallel texts from sources such as multilin-
gual websites, government documents and large
archives of human translations of books, news, and
so forth, unannotated parallel data is becoming
more widely available. This parallel data can be
exploited to bridge languages, and in particular,
transfer information from a highly-resourced lan-
guage to a lesser-resourced language, to build un-
supervised POS taggers.
In this paper, we propose an unsupervised ap-
proach to POS tagging in a similar vein to the
work of Das and Petrov (2011). In this approach,
a parallel corpus for a more-resourced language
having a POS tagger, and a lesser-resourced lan-
guage, is word-aligned. These alignments are ex-
ploited to infer an unsupervised tagger for the tar-
get language (i.e., a tagger not requiring manually-
labelled data in the target language). Our ap-
proach is substantially simpler than that of Das
and Petrov, the current state-of-the art, yet per-
forms comparably well.
2 Related work
There is a wealth of prior research on building un-
supervised POS taggers. Some approaches have
exploited similarities between typologically simi-
lar languages (e.g., Czech and Russian, or Telugu
and Kannada) to estimate the transition probabil-
ities for an HMM tagger for one language based
on a corpus for another language (e.g., Hana et al,
2004; Feldman et al, 2006; Reddy and Sharoff,
2011). Other approaches have simultaneously
tagged two languages based on alignments in a
parallel corpus (e.g., Snyder et al, 2008).
A number of studies have used tag projection
to copy tag information from a resource-rich to
a resource-poor language, based on word align-
ments in a parallel corpus. After alignment, the
resource-rich language is tagged, and tags are pro-
jected from the source language to the target lan-
guage based on the alignment (e.g., Yarowsky and
Ngai, 2001; Das and Petrov, 2011). Das and
Petrov (2011) achieved the current state-of-the-art
for unsupervised tagging by exploiting high con-
fidence alignments to copy tags from the source
language to the target language. Graph-based la-
bel propagation was used to automatically produce
more labelled training data. First, a graph was
constructed in which each vertex corresponds to
a unique trigram, and edge weights represent the
syntactic similarity between vertices. Labels were
then propagated by optimizing a convex function
to favor the same tags for closely related nodes
634
Model Coverage Accuracy
Many-to-1 alignments 88% 68%
1-to-1 alignments 68% 78%
1-to-1 alignments: Top 60k sents 91% 80%
Table 1: Token coverage and accuracy of many-
to-one and 1-to-1 alignments, as well as the top
60k sentences based on alignment score for 1-to-1
alignments, using directly-projected labels only.
while keeping a uniform tag distribution for un-
related nodes. A tag dictionary was then extracted
from the automatically labelled data, and this was
used to constrain a feature-based HMM tagger.
The method we propose here is simpler to that
of Das and Petrov in that it does not require con-
vex optimization for label propagation or a feature
based HMM, yet it achieves comparable results.
3 Tagset
Our tagger exploits the idea of projecting tag infor-
mation from a resource-rich to resource-poor lan-
guage. To facilitate this mapping, we adopt Petrov
et al?s (2012) twelve universal tags: NOUN,
VERB, ADJ, ADV, PRON (pronouns), DET (de-
terminers and articles), ADP (prepositions and
postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), ?.? (punctuation), and X
(all other categories, e.g., foreign words, abbrevia-
tions). These twelve basic tags are common across
taggers for most languages.
Adopting a universal tagset avoids the need
to map between a variety of different, language-
specific tagsets. Furthermore, it makes it possi-
ble to apply unsupervised tagging methods to lan-
guages for which no tagset is available, such as
Telugu and Vietnamese.
4 A Simpler Unsupervised POS Tagger
Here we describe our proposed tagger. The key
idea is to maximize the amount of information
gleaned from the source language, while limit-
ing the amount of noise. We describe the seed
model and then explain how it is successively re-
fined through self-training and revision.
4.1 Seed Model
The first step is to construct a seed tagger from
directly-projected labels. Given a parallel corpus
for a source and target language, Algorithm 1 pro-
vides a method for building an unsupervised tag-
ger for the target language. In typical applications,
the source language would be a better-resourced
language having a tagger, while the target lan-
guage would be lesser-resourced, lacking a tagger
and large amounts of manually POS-labelled data.
Algorithm 1 Build seed model
1: Tag source side.
2: Word align the corpus with Giza++ and re-
move the many-to-one mappings.
3: Project tags from source to target using the re-
maining 1-to-1 alignments.
4: Select the top n sentences based on sentence
alignment score.
5: Estimate emission and transition probabilities.
6: Build seed tagger T.
We eliminate many-to-one alignments (Step 2).
Keeping these would give more POS-tagged to-
kens for the target side, but also introduce noise.
For example, suppose English and French were
the source and target language, respectively. In
this case alignments such as English laws (NNS)
to French les (DT) lois (NNS) would be expected
(Yarowsky and Ngai, 2001). However, in Step 3,
where tags are projected from the source to target
language, this would incorrectly tag French les as
NN. We build a French tagger based on English?
French data from the Europarl Corpus (Koehn,
2005). We also compare the accuracy and cov-
erage of the tags obtained through direct projec-
tion using the French Melt POS tagger (Denis and
Sagot, 2009). Table 1 confirms that the one-to-one
alignments indeed give higher accuracy but lower
coverage than the many-to-one alignments. At
this stage of the model we hypothesize that high-
confidence tags are important, and hence eliminate
the many-to-one alignments.
In Step 4, in an effort to again obtain higher
quality target language tags from direct projection,
we eliminate all but the top n sentences based on
their alignment scores, as provided by the aligner
via IBM model 3. We heuristically set this cutoff
to 60k to balance the accuracy and size of the seed
model.1 Returning to our preliminary English?
French experiments in Table 1, this process gives
improvements in both accuracy and coverage.2
1We considered values in the range 60?90k, but this
choice had little impact on the accuracy of the model.
2We also considered using all projected labels for the top
60k sentences, not just 1-to-1 alignments, but in preliminary
experiments this did not perform as well, possibly due to the
previously-observed problems with many-to-one alignments.
635
The number of parameters for the emission prob-
ability is |V | ? |T | where V is the vocabulary and
T is the tag set. The transition probability, on the
other hand, has only |T |3 parameters for the tri-
gram model we use. Because of this difference
in number of parameters, in step 5, we use dif-
ferent strategies to estimate the emission and tran-
sition probabilities. The emission probability is
estimated from all 60k selected sentences. How-
ever, for the transition probability, which has less
parameters, we again focus on ?better? sentences,
by estimating this probability from only those sen-
tences that have (1) token coverage > 90% (based
on direct projection of tags from the source lan-
guage), and (2) length > 4 tokens. These cri-
teria aim to identify longer, mostly-tagged sen-
tences, which we hypothesize are particularly use-
ful as training data. In the case of our preliminary
English?French experiments, roughly 62% of the
60k selected sentences meet these criteria and are
used to estimate the transition probability. For un-
aligned words, we simply assign a random POS
and very low probability, which does not substan-
tially affect transition probability estimates.
In Step 6 we build a tagger by feeding the es-
timated emission and transition probabilities into
the TNT tagger (Brants, 2000), an implementation
of a trigram HMM tagger.
4.2 Self training and revision
For self training and revision, we use the seed
model, along with the large number of target lan-
guage sentences available that have been partially
tagged through direct projection, in order to build
a more accurate tagger. Algorithm 2 describes
this process of self training and revision, and as-
sumes that the parallel source?target corpus has
been word aligned, with many-to-one alignments
removed, and that the sentences are sorted by
alignment score. In contrast to Algorithm 1, all
sentences are used, not just the 60k sentences with
the highest alignment scores.
We believe that sentence alignment score might
correspond to difficulty to tag. By sorting the sen-
tences by alignment score, sentences which are
more difficult to tag are tagged using a more ma-
ture model. Following Algorithm 1, we divide
sentences into blocks of 60k.
In step 3 the tagged block is revised by com-
paring the tags from the tagger with those ob-
tained through direct projection. Suppose source
Algorithm 2 Self training and revision
1: Divide target language sentences into blocks
of n sentences.
2: Tag the first block with the seed tagger.
3: Revise the tagged block.
4: Train a new tagger on the tagged block.
5: Add the previous tagger?s lexicon to the new
tagger.
6: Use the new tagger to tag the next block.
7: Goto 3 and repeat until all blocks are tagged.
language word wsi is aligned with target language
word wtj with probability p(wtj |wsi ), T si is the tag
for wsi using the tagger available for the source
language, and T tj is the tag for wtj using the tagger
learned for the target language. If p(wtj |wsi ) > S,
where S is a threshold which we heuristically set
to 0.7, we replace T tj by T si .
Self-training can suffer from over-fitting, in
which errors in the original model are repeated
and amplified in the new model (McClosky et al,
2006). To avoid this, we remove the tag of
any token that the model is uncertain of, i.e., if
p(wtj |wsi ) < S and T tj ?= T si then T tj = Null. So,
on the target side, aligned words have a tag from
direct projection or no tag, and unaligned words
have a tag assigned by our model.
Step 4 estimates the emission and transition
probabilities as in Algorithm 1. In Step 5, emis-
sion probabilities for lexical items in the previous
model, but missing from the current model, are
added to the current model. Later models therefore
take advantage of information from earlier mod-
els, and have wider coverage.
5 Experimental Results
Using parallel data from Europarl (Koehn, 2005)
we apply our method to build taggers for the same
eight target languages as Das and Petrov (2011)
? Danish, Dutch, German, Greek, Italian, Por-
tuguese, Spanish and Swedish ? with English as
the source language. Our training data (Europarl)
is a subset of the training data of Das and Petrov
(who also used the ODS United Nations dataset
which we were unable to obtain). The evaluation
metric and test data are the same as that used by
Das and Petrov. Our results are comparable to
theirs, although our system is penalized by having
less training data. We tag the source language with
the Stanford POS tagger (Toutanova et al, 2003).
636
Danish Dutch German Greek Italian Portuguese Spanish Swedish Average
Seed model 83.7 81.1 83.6 77.8 78.6 84.9 81.4 78.9 81.3
Self training + revision 85.6 84.0 85.4 80.4 81.4 86.3 83.3 81.0 83.4
Das and Petrov (2011) 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Table 2: Token-level POS tagging accuracy for our seed model, self training and revision, and the method
of Das and Petrov (2011). The best results on each language, and on average, are shown in bold.
0 5 10 15 20 25 30
Iteration
50
60
70
80
Pe
rce
nta
ge
s Overall Acc
Know Acc
OOV Acc
Know tkn
0 5 10 15 20 25 30
Iteration
70
75
80
85
90
Pe
rce
nta
ge
s Overall Acc
Know Acc
OOV Acc
Know tkn
Figure 1: Overall accuracy, accuracy on known tokens, accuracy on unknown tokens, and proportion of
known tokens for Italian (left) and Dutch (right).
Table 2 shows results for our seed model, self
training and revision, and the results reported by
Das and Petrov. Self training and revision im-
prove the accuracy for every language over the
seed model, and gives an average improvement
of roughly two percentage points. The average
accuracy of self training and revision is on par
with that reported by Das and Petrov. On individ-
ual languages, self training and revision and the
method of Das and Petrov are split ? each per-
forms better on half of the cases. Interestingly, our
method achieves higher accuracies on Germanic
languages ? the family of our source language,
English?while Das and Petrov perform better on
Romance languages. This might be because our
model relies on alignments, which might be more
accurate for more-related languages, whereas Das
and Petrov additionally rely on label propagation.
Compared to Das and Petrov, our model per-
forms poorest on Italian, in terms of percentage
point difference in accuracy. Figure 1 (left panel)
shows accuracy, accuracy on known words, accu-
racy on unknown words, and proportion of known
tokens for each iteration of our model for Italian;
iteration 0 is the seed model, and iteration 31 is
the final model. Our model performs poorly on
unknown words as indicated by the low accuracy
on unknown words, and high accuracy on known
words compared to the overall accuracy. The poor
performance on unknown words is expected be-
cause we do not use any language-specific rules
to handle this case. Moreover, on average for the
final model, approximately 10% of the test data
tokens are unknown. One way to improve the per-
formance of our tagger might be to reduce the pro-
portion of unknown words by using a larger train-
ing corpus, as Das and Petrov did.
We examine the impact of self-training and re-
vision over training iterations. We find that for
all languages, accuracy rises quickly in the first
5?6 iterations, and then subsequently improves
only slightly. We exemplify this in Figure 1 (right
panel) for Dutch. (Findings are similar for other
languages.) Although accuracy does not increase
much in later iterations, they may still have some
benefit as the vocabulary size continues to grow.
6 Conclusion
We have proposed a method for unsupervised POS
tagging that performs on par with the current state-
of-the-art (Das and Petrov, 2011), but is substan-
tially less-sophisticated (specifically not requiring
convex optimization or a feature-based HMM).
The complexity of our algorithm is O(nlogn)
compared to O(n2) for that of Das and Petrov
637
(2011) where n is the size of training data.3 We
made our code are available for download.4
In future work we intend to consider using a
larger training corpus to reduce the proportion of
unknown tokens and improve accuracy. Given
the improvements of our model over that of Das
and Petrov on languages from the same family
as our source language, and the observation of
Snyder et al (2008) that a better tagger can be
learned from a more-closely related language, we
also plan to consider strategies for selecting an ap-
propriate source language for a given target lan-
guage. Using our final model with unsupervised
HMM methods might improve the final perfor-
mance too, i.e. use our final model as the ini-
tial state for HMM, then experiment with differ-
ent inference algorithms such as ExpectationMax-
imization (EM), Variational Bayers (VB) or Gibbs
sampling (GS).5 Gao and Johnson (2008) compare
EM, VB and GS for unsupervised English POS
tagging. In many cases, GS outperformed other
methods, thus we would like to try GS first for our
model.
7 Acknowledgements
This work is funded by Erasmus Mundus
European Masters Program in Language and
Communication Technologies (EM-LCT) and
by the Czech Science Foundation (grant no.
P103/12/G084). We would like to thank Prokopis
Prokopidis for providing us the Greek Treebank
and Antonia Marti for the Spanish CoNLL 06
dataset. Finally, we thank Siva Reddy and Span-
dana Gella for many discussions and suggestions.
References
Thorsten Brants. 2000. TnT: A statistical part-of-
speech tagger. In Proceedings of the sixth con-
ference on Applied natural language processing
(ANLP ?00), pages 224?231. Seattle, Washing-
ton, USA.
Dipanjan Das and Slav Petrov. 2011. Unsu-
pervised part-of-speech tagging with bilingual
graph-based projections. In Proceedings of
3We re-implemented label propagation from Das and
Petrov (2011). It took over a day to complete this step on
an eight core Intel Xeon 3.16GHz CPU with 32 Gb Ram, but
only 15 minutes for our model.
4https://code.google.com/p/universal-tagger/
5We in fact have tried EM, but it did not help. The overall
performance dropped slightly. This might be because self-
training with revision already found the local maximal point.
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies - Volume 1 (ACL 2011), pages
600?609. Portland, Oregon, USA.
Pascal Denis and Beno??t Sagot. 2009. Coupling
an annotated corpus and a morphosyntactic lex-
icon for state-of-the-art POS tagging with less
human effort. In Proceedings of the 23rd Pa-
cific Asia Conference on Language, Information
and Computation, pages 721?736. Hong Kong,
China.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of
new morpho-syntactically annotated resources.
In Proceedings of the Eight International Con-
ference on Language Resources and Evaluation
(LREC?06), pages 549?554. Genoa, Italy.
Jianfeng Gao and Mark Johnson. 2008. A com-
parison of bayesian estimators for unsupervised
hidden markov model pos taggers. In Proceed-
ings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?08,
pages 344?352. Association for Computational
Linguistics, Stroudsburg, PA, USA.
Jiri Hana, Anna Feldman, and Chris Brew. 2004.
A resource-light approach to Russian morphol-
ogy: Tagging Russian using Czech resources.
In Proceedings of the 2004 Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP ?04), pages 222?229. Barcelona,
Spain.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Proceed-
ings of the Tenth Machine Translation Summit
(MT Summit X), pages 79?86. AAMT, Phuket,
Thailand.
David McClosky, Eugene Charniak, and Mark
Johnson. 2006. Effective self-training for pars-
ing. In Proceedings of the main conference on
Human Language Technology Conference of the
North American Chapter of the Association of
Computational Linguistics (HLT-NAACL ?06),
pages 152?159. New York, USA.
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012. A universal part-of-speech tagset. In
Proceedings of the Eight International Confer-
ence on Language Resources and Evaluation
(LREC?12), pages 2089?2096. Istanbul, Turkey.
Siva Reddy and Serge Sharoff. 2011. Cross lan-
guage POS Taggers (and other tools) for Indian
638
languages: An experiment with Kannada using
Telugu resources. In Proceedings of the IJC-
NLP 2011 workshop on Cross Lingual Infor-
mation Access: Computational Linguistics and
the Information Need of Multilingual Societies
(CLIA 2011). Chiang Mai, Thailand.
Benjamin Snyder, Tahira Naseem, Jacob Eisen-
stein, and Regina Barzilay. 2008. Unsupervised
multilingual learning for POS tagging. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP
?08), pages 1041?1050. Honolulu, Hawaii.
Kristina Toutanova, Dan Klein, Christopher D.
Manning, and Yoram Singer. 2003. Feature-
rich part-of-speech tagging with a cyclic de-
pendency network. In Proceedings of the
2003 Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics on Human Language Technology - Vol-
ume 1 (NAACL ?03), pages 173?180. Edmon-
ton, Canada.
David Yarowsky and Grace Ngai. 2001. Induc-
ing multilingual POS taggers and NP bracketers
via robust projection across aligned corpora. In
Proceedings of the Second Meeting of the North
American Chapter of the Association for Com-
putational Linguistics on Language technolo-
gies (NAACL ?01), pages 1?8. Pittsburgh, Penn-
sylvania, USA.
639
Towards a Data Model for the Universal Corpus
Steven Abney
University of Michigan
abney@umich.edu
Steven Bird
University of Melbourne and
University of Pennsylvania
sbird@unimelb.edu.au
Abstract
We describe the design of a comparable cor-
pus that spans all of the world?s languages and
facilitates large-scale cross-linguistic process-
ing. This Universal Corpus consists of text
collections aligned at the document and sen-
tence level, multilingual wordlists, and a small
set of morphological, lexical, and syntactic an-
notations. The design encompasses submis-
sion, storage, and access. Submission pre-
serves the integrity of the work, allows asyn-
chronous updates, and facilitates scholarly ci-
tation. Storage employs a cloud-hosted file-
store containing normalized source data to-
gether with a database of texts and annota-
tions. Access is permitted to the filestore, the
database, and an application programming in-
terface. All aspects of the Universal Corpus
are open, and we invite community participa-
tion in its design and implementation, and in
supplying and using its data.
1 Introduction
We have previously proposed a community dataset
of annotated text spanning a very large number of
languages, with consistent annotation and format
that enables automatic cross-linguistic processing
on an unprecedented scale (Abney and Bird, 2010).
Here we set out the data model in detail, and invite
members of the computational linguistics commu-
nity to begin work on the first version of the dataset.
The targeted annotation generalizes over three
widely-used kinds of data: (1) simple bitexts, that
is, tokenized texts and their translations, which are
widely used for training machine translation sys-
tems; (2) interlinear glossed text (IGT), which adds
lemmas, morphological features and parts of speech,
and is the de facto standard in the documentary lin-
guistics literature; and (3) dependency parses, which
add a head pointer and relation name for each word,
and are gaining popularity as representations of syn-
tactic structure. We do not expect all texts to have
equal richness of annotation; rather, these are the
degrees of annotation we wish to explicitly accom-
modate. Keeping the annotation lightweight is a pri-
mary desideratum.
We strive for inclusion of as many languages as
possible. We are especially interested in languages
outside of the group of 30 or so for which there
already exist non-trivial electronic resources. Op-
timistically, we aim for a universal corpus, in the
sense of one that covers a widely representative set
of the world?s languages and supports inquiry into
universal linguistics and development of language
technologies with universal applicability.
We emphasize, however, that even if completely
successful, it will be a universal corpus and not the
universal corpus. The term ?universal? should em-
phatically not be understood in the sense of encom-
passing all language annotation efforts. We are not
proposing a standard or a philosophy of language
documentation, but rather a design for one partic-
ular resource. Though the goals with regard to lan-
guage coverage are unusually ambitious, for the sake
of achievability we keep the targeted annotation as
simple as possible. The result is intended to be a sin-
gle, coherent dataset that is very broad in language
coverage, but very thin in complexity of annotation.
120
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 120?127,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Finally, the development of the corpus is an un-
funded, all-volunteer effort. It will only come about
if it wins community buy-in, in the spirit of collab-
orative efforts like Project Gutenberg. We formulate
it as a cooperation among data providers and host-
ing services to provide data in a manner that creates
a single, seamless dataset from the user perspective.
This paper is a first draft of a ?cooperative agree-
ment? that could achieve that goal.
2 A lightweight model for multilingual text
2.1 Media and annotation
In documentary linguistics, a distinction is made
between language documentation, whose concern
is the collection of primary documentation such as
speech recordings and indigenous written works,
and language description, whose concern is the an-
notation and organization of the primary material
(Himmelmann, 1998). We make a similar distinc-
tion between media files and annotation, where ?an-
notation? is understood broadly to include all pro-
cessing steps that make the linguistic contents more
explicit, including plain text rendering, sentence
segmentation, and alignment of translations.
The Corpus consists of annotated documents, in
the sense of primary documents with accompany-
ing annotation. There are many efforts at collecting
documentation for a broad range of languages; what
makes this Corpus distinct is its focus on annotation.
Accordingly, we assume that media files and anno-
tation are handled separately.
For media, the Language Commons collection in
the Internet Archive is a recently-established repos-
itory for redistributable language data that we view
as the primary host.1 For the annotation database, a
primary data host remains to be established, but we
have identified some options. For example, Amazon
Web Services and the Talis Connected Commons
have free hosting services for public data sets.
2.2 The data model in brief
In order to keep the barriers to participation as low as
possible, we have made our target for annotation as
simple as possible. The data model is summarized
in Figure 1. We distinguish between aligned texts
1http://www.archive.org/details/
LanguageCommons
(or parallel texts) and analyzed texts (comparable
texts).
Semantically, the entire collection of aligned texts
constitutes a matrix whose columns are languages
and whose rows are texts. We limit attention to three
levels of granularity: document, sentence, and word.
Each cell is occupied by a string, the typical length
of the string varying with the granularity. We expect
the matrix to be quite sparse: most cells are empty.
The collection of analyzed texts consists, semanti-
cally, of one table per language. The rows represent
words and the columns are properties of the words.
The words may either be tokens in a sentence anal-
ysis, as suggested by the examples, or types repre-
senting dictionary information. The tables are com-
parable, in the sense that they have a common format
and are conducive to language-independent process-
ing, but they are not parallel: the i-th word in the
German table has nothing to do with the i-th word
in the Spanish table.
The tables in Figure 1 constitute the bulk of the
data model. In addition, we assume some auxiliary
information (not depicted) that is primarily organi-
zational. It includes an association between docu-
ments and sentences, the location of documents and
sentences within media files (if applicable), a group-
ing of table rows into ?files,? and a grouping of files
into ?works.? Metadata such as revision information
is attached to files and works. We return below to
the characterization of this auxiliary information.
In contrast to current standard practice, we wish
to emphasize the status of aligned and analyzed text
as annotation of primary documents represented by
media files such as speech recordings or page im-
ages, and we wish to maintain explicit connections
between annotations and primary documents. We
do not insist that the underlying media files be avail-
able in all cases, but we hope to identify them when
possible. However, we focus on storage of the anno-
tation; we assume that media files are in a separate
store, and referenced by external URIs.
2.3 Two implementations: filestore and
database
The data model is abstract, and is implemented in a
couple of ways for different purposes. For distribu-
tion on physical medium or by download, it is most
convenient to implement the data model as actual
121
Aligned Texts Analyzed Texts
deu spa fra eng . . .
d1 sie.. ella.. elle.. she..
d2
...
s1
s2
...
w1
w2
...
deu
sent form lemma morph pos gloss head rel
w1 s1 Ku?he Kuh PL N cow 2 SBJ
w2 s1 sind sein PL V be 0 ROOT
...
spa
sent form lemma morph pos gloss head rel
w1 s2 estas este F.PL D this 2 SPC
w2 s2 floras flora F.PL N flower 3 SBJ
...
...
Figure 1: An overview of the targeted annotation: Aligned Texts in a single matrix having three levels of granularity
(document, sentence, word), and Analyzed Texts grouped by language and annotated down to the word level with
morphological, lexical and syntactic information.
files. Each file contains information corresponding
to some slice of a table, and the structure of the table
is encoded in the file format. On the other hand,
web services are often implemented as databases,
making an implementation of the abstract model as
a database desirable.
A file-based implementation is most familiar, and
most existing resources are available as file collec-
tions. However, even when different existing re-
sources have similar semantics, such as different
parallel text collections, there is considerable variety
in the organization and representation of the infor-
mation. In order to work with multiple such sources,
a substantial amount of housekeeping is required.
One can view our proposed filestore as a normal-
ized form that removes the diversity that only gets in
the way of efficient cross-language processing. In-
deed, our proposed format for analyzed text hews
intentionally close to the format used in the CoNLL
dependency-parsing shared tasks, which provided
a normal form into which data from multiple tree-
banks was mapped (Buchholz et al, 2006).
When an existing resource is included in the Cor-
pus, we assume that it remains externally available
in its original form, but a copy is imported into the
Corpus filestore in which every file has been pre-
processed into one of a set of simple file formats
implementing the model of Figure 1, following a
consistent scheme for filenames, with utf8 charac-
ter encoding, and capturing any available alignment
information in an auxiliary table. Distribution of the
Corpus via physical media or download simply in-
volves copying the filestore.
The filestore is organized around material pro-
vided by individual data providers, or ?authors,? and
maintains the identity of a data provider?s contribu-
tion as a distinct intellectual ?work.? Works provide
an appropriate unit to which to attach edition and
rights metadata.
In addition to the filestore, the texts and align-
ments are imported into a collection of database ta-
bles that can be queried efficiently.
In section 3 we describe a simple file-based im-
plementation of the data model, and show the variety
of familiar file types that find a natural place in the
model. In section 4 we describe the tabular storage
model.
3 Filestore implementation
Despite the simplicity of the data model, it captures
a substantial, even surprising, variety of commonly-
used textual data file types.
Document-aligned text. Parallel corpora are most
commonly aligned at the document level. Typically,
each translation of a document is contained in a file,
and there is some way of indicating which files are
mutual translations of the same document. The con-
122
tents of a file, as a single string, represents one cell
in the Aligned Text matrix in Figure 1 (at the ?doc-
ument? level of granularity). A document, compris-
ing a collection of mutual translations, corresponds
to a row of the matrix.
As normal form, we propose the convention of
using filenames that incorporate a language iden-
tifier and a document identifier. For example,
1001-eng.txt and 1001-deu.txt are the En-
glish and German files representing mutual transla-
tions of some hypothetical document 1001.
Language identifiers are ISO 639-3 language
code, supplemented by the Linguist List local-use
codes and subgroup and dialect identifiers.
Sentence-aligned text. At a finer grain, paral-
lel corpora may be aligned at the sentence level.
Each file contains the translation of one document,
segmented into one sentence per line. Our nor-
mal form uses the same filename convention as
for document-aligned text, to indicate which files
are mutual translations. We use the file suffix
?.snt? to indicate a file with one sentence per
line. This incidentally indicates which document
a set of sentences came from, since the filenames
share a document identifier. For example, the file
1001-deu.snt contains the sentence-segmented
version of 1001-deu.txt.
In the canonical case, each file in a group of
aligned files contains the same number of sentences,
and the sentences line up one-to-one. The group
of aligned files corresponds to a set of rows in the
Aligned Text matrix, at the ?sentence? level of gran-
ularity.
There are cases in which the sentence alignment
between documents is not one-to-one. Even in this
case, we can view the alignment as consisting of a
sequence of ?beads? that sometimes contain multi-
ple sentences in one language. If we normalize the
file to one in which the group of sentences belong-
ing to a single bead are concatenated together as a
?translational unit,? we reduce this case to the one-
to-one case, though we do lose the information about
orthographic sentence boundaries internal to a bead.
Preserving the original sentences would necessi-
tate an extension to the data model. A typical ap-
proach is to store the alignments in a table, where
n-way alignments are indicated using n-tuples of in-
tegers. We leave this as a point for future consider-
ation. We also put aside consideration of word-level
document alignment.
Translation dictionaries. A translation dictionary
contains word translations in multiple languages.
One representation looks just like sentence-aligned
text, except that each file contains one entry per line
instead of one sentence per line. Each file in an
aligned set contains the same number of entries, and
the entries line up one-to-one across files. This is
the representation we take as our normal form. We
also use the same filename convention, but with suf-
fix .tdi for translation dictionary.
A translation dictionary corresponds to a set of
rows in the Aligned Text matrix, at the ?word? level
of granularity. A translation dictionary would typ-
ically be derived from a large number of text doc-
uments, so each translation dictionary will typically
have a unique document identifier, and will not align
with files at the sentence or document granularity.
Transcriptions and segmentations. When one
begins with a sound recording or with page images
from a print volume that has been scanned, a first
step is conversion to plain text. We will call this a
?transcription? both for the case where the original
was a sound file and for the case where the origi-
nal was a page image. Transcriptions fit into our
data model as the special case of ?document-aligned
text? in which only one language is involved. We
assume that the Aligned Text matrix is sparse, and
this is the extreme case in which only one cell in a
row is occupied. The connection between the tran-
script?s document identifier and the original media
file is recorded in an auxiliary metadata file.
After transcription, the next step in processing is
to identify the parts of the text that are natural lan-
guage (as opposed to markup or tables or the like),
and to segment the natural language portion into
sentences. The result is sentence-segmented text.
Again, we treat this as the special case of sentence-
aligned text in which only one language is involved.
Analyzed text. A variety of different text file types
can be grouped together under the heading of an-
alyzed text. The richest example we consider is
dependency parse structure. One widely-used file
representation has one word token per line. Each
123
line consists of tab-separated fields containing at-
tributes of the word token. There is some varia-
tion in the attributes that are specified, but the ones
used in the Analyzed Text tables of our data model
are typical, namely: sentence identifier, wordform,
lemma, morphological form, gloss, part of speech,
head (also called governor), and relation (also called
role). Sentence boundaries are not represented as to-
kens; rather, tokens belonging to the same sentence
share the same value for sentence identifier. We con-
tinue with the same filename convention as before;
for Analyzed Text files, the suffix is .tab.
Many different linguistic annotations are natu-
rally represented as special cases of Analyzed Text.
? Tokenized text in ?vertical format? is the spe-
cial case in which the only column is the word-
form column. We include the sentence ID col-
umn as well, in lieu of sentence-boundary to-
kens.
? POS-tagged text adds the part of speech col-
umn.
? The information in the word-by-word part of
interlinear glossed text (IGT) typically includes
the wordform, lemma, morph, and gloss; again
we also include the sentence ID column.
? A dependency parse, as already indicated, is the
case in which all columns are present.
In addition, the format accommodates a variety
of monolingual and multilingual lexical resources.
Such lexical resources are essential, whether manu-
ally curated or automatically extracted.
? A basic dictionary consists of a sequence of en-
tries, each of which contains a lemma, part of
speech, and gloss. Hence a dictionary is nat-
urally represented as analyzed text containing
just those three columns. The entries in a dic-
tionary are word types rather than word tokens,
so the wordform and sentence ID columns are
absent.
? If two or more lexicons use the same glosses,
the lexicons are implicitly aligned by virtue of
the glosses and there is no need for overt align-
ment information. This is a more flexible repre-
sentation than a translation dictionary: unlike a
translation dictionary, it permits multiple words
to have the same gloss (synonyms), and it adds
parts of speech.
4 Database implementation
An alternative implementation, appropriate for de-
ployment of the Corpus as a web service, is as a
normalized, multi-table database. In this section
we drill down and consider the kinds of tables and
records that would be required in order to represent
our abstract data model. We will proceed by way
of example, for each of the kinds of data we would
like to accommodate. Each example is displayed as
a record consisting of a series of named fields.
Note that we make no firm commitment as to the
physical format of these records. They could be se-
rialized as XML when the database is implemented
as a web service. Equally, they could be represented
using dictionaries or tuples when the database is ac-
cessed via an application program interface (API).
We will return to this later.
4.1 The Aligned Text matrix
The Aligned Text matrix is extremely sparse. We
use the more flexible representation in which each
matrix cell is stored using a separate record, where
the record specifies (index, column) pairs. For ex-
ample, the matrix row
deu spa fra
d1 Sie... Ella...
d2 Mein... Mon...
is represented as
Document Table
DID LANG TEXT
1 deu Sie...
1 spa Ella...
2 deu Mein...
2 fra Mon...
(The ellipses are intended to indicate that each cell
contains the entire text of a document.) We have also
added an explicit document ID.
When we consider entries at the sentence and
word levels, we require both a document ID and sen-
tence or word IDs within the document. Figure 2
shows an example of two sentences from the same
document, translated into two languages. Note that
we can think of DID + LANG as an identifier for a
monolingual document instance, and DID + LANG +
SID identifies a particular sentence in a monolingual
document.
124
DID LANG SID TEXT
1 deu 1 Der Hund bellte.
1 eng 1 the dog barked.
1 deu 2 Mein Vater ist Augenarzt.
1 eng 2 My father is an optometrist.
Figure 2: Two sentences with two translations. These are
sentence table records.
In short, we implement the Aligned Text matrix as
three database tables. All three tables have columns
DID, LANG, and TEXT. The sentence table adds SID,
and the word table adds WID instead of SID. (The
words are types, not tokens, hence are not associated
with any particular sentence.)
4.2 The Analyzed Text tables
The implementation of the Analyzed Text tables is
straightforward. We add a column for the document
ID, and we assume that sentence ID is relative to
the document. We also represent the word token ID
explicitly, and take it to be relative to the sentence.
Finally, we add a column for LANG, so that we have
a single table rather than one per language.
The first record from the German table in Figure 1
is implemented as in Figure 3. This is a record from
a dependency parse. Other varieties of analyzed text
leave some of the columns empty, as discussed in the
previous section.
There is a subtlety to note. In the sentence table,
the entry with DID 1, SID 1, and LANG ?deu? is un-
derstood to be a translation of the entry with DID 1,
SID 1, and LANG ?eng.? That is not the case with
records in the analyzed-text table. Word 1 in the En-
glish sentence 1 of document 1 is not necessarily a
translation of word 1 in the German sentence 1 of
document 1.
A few comments are in order about the meanings
of the columns. The wordform is the attested, in-
flected form of the word token. The LEMMA pro-
vides the lexical form, which is the headword un-
der which one would find the word in a dictionary.
The MORPH field provides a symbolic indicator of
the relationship between the lemma and the word-
form. For example, ?Ku?he? is the PL form of the
lemma ?Kuh.?
This approach encompasses arbitrary morpholog-
ical processes. For example, Hebrew lomedet may
be represented as the PRESPTC.FEM.SG form of
lmd, (?to learn?).
When we represent dictionaries, the records are
word types rather than word tokens. We assign a
document ID to the dictionary as a whole, but by
convention take the SID to be uniformly 0.
Ultimately, the POS and GLOSS fields are in-
tended to contain symbols from controlled vocab-
ularies. For the present, the choice of controlled
vocabulary is up to the annotator. For the GLOSS
field, an option that has the benefit of simplicity is
to use the corresponding word from a reference lan-
guage, but one might equally well use synset identi-
fiers from WordNet, or concepts in some ontology.
4.3 The auxiliary tables
The auxiliary tables were not shown in the abstract
data model as depicted in Figure 1. They primar-
ily include metadata. We assume a table that asso-
ciates each document ID with a work, and a table
that provides metadata for each work. The Corpus
as a whole is the sum of the works.
In the spirit of not duplicating existing efforts, we
?outsource? the bulk of the metadata to OLAC (Si-
mons and Bird, 2003). If a work has an OLAC entry,
we only need to associate the internal document ID
to the OLAC identifier.
There is some metadata information that we
would like to include for which we cannot refer to
OLAC.
? Provenance: how the annotation was con-
structed, e.g., who the annotator was, or what
software was used if it was automatically cre-
ated.
? Rights: copyright holder, license category cho-
sen from a small set of interoperable licenses.
? Standards: allows the annotator to indicate
which code sets are used for the MORPH, POS,
and GLOSS fields. We would like to be able
to specify a standard code set for each, in the
same way that we have specified ISO 639-3 for
language codes. Consensus has not yet crystal-
lized around any one standard, however.
The auxiliary tables also associate documents
with media files. We assume a table associating
document IDs with a media files, represented by
125
DID LANG SID WID FORM LEMMA MORPH POS GLOSS HEAD REL
123 deu 1 1 Ku?he Kuh PL N cow 2 SBJ
Figure 3: A single word from a dependency parse. This is a record from the analyzed-text table.
their URLs, and a table associating sentences (DID
+ SID) with locations in media files.
Note that, as we have defined the file and tabu-
lar implementations, there is no need for an explicit
mapping between document IDs and filenames. A
filename is always of the form did-lang.suffix,
where the suffix is .txt for the document table,
.snt for the sentence table, .tdi for the word ta-
ble, and .tab for the analyzed-text table. Each file
corresponds to a set of records in one of the tables.
5 Cloud Storage and Interface
A third interface to the Corpus is via an applica-
tion programming interface. We illustrate a possi-
ble Python API using Amazon SimpleDB, a cloud-
hosted tuple store accessed via a web services in-
terface.2 An ?item? is a collection of attribute-
value pairs, and is stored in a ?domain.? Items,
attributes, and domains are roughly equivalent to
records, fields, and tables in a relational database.
Unlike relational databases, new attributes and do-
mains can be added at any time.
Boto is a Python interface to Amazon Web Ser-
vices that includes support for SimpleDB.3 The fol-
lowing code shows an interactive session in which a
connection is established and a domain is created:
>>> import boto
>>> sdb = boto.connect_sdb(PUBLIC_KEY, PRIVATE_KEY)
>>> domain = sdb.create_domain(?analyzed_text?)
We can create a new item, then use Python?s dic-
tionary syntax to create attribute-value pairs, before
saving it:
>>> item = domain.new_item(?123?)
>>> item[?DID?] = ?123?
>>> item[?LANG?] = ?deu?
>>> item[?FORM?] = ?Ku?he?
>>> item[?GLOSS?] = ?cow?
>>> item[?HEAD?] = ?2?
>>> item.save()
Finally, we can retrieve an item by name, or submit
a query using SQL-like syntax.
2http://aws.amazon.com/simpledb/
3http://code.google.com/p/boto/
>>> sdb.get_attributes(domain, ?123?)
?LANG?: ?deu?, ?HEAD?: ?2?, ?DID?: ?123?,
?FORM?: ?Ku?he?, ?GLOSS?: ?cow?
>>> sdb.select(domain,
... ?select DID, FORM from analyzed_text
... where LANG = "deu"?)
[?DID?: ?123?, ?FORM?: ?Ku?he?]
We have developed an NLTK ?corpus reader?
which understands the Giza and NAACL03 formats
for bilingual texts, and creates a series of records for
insertion into SimpleDB using the Boto interface.
Other formats will be added over time.
Beyond the loading of corpora, a range of query
and report generation functions are needed, as illus-
trated in the following (non-exhaustive) list:
? lookup(lang=ENG, rev="1.2b3", ...): find all
items which have the specified attribute val-
ues, returning a list of dictionaries; following
Python syntax, we indicate this variable num-
ber of keyword arguments with **kwargs.
? extract(type=SENT, lang=[ENG, FRA, DEU],
**kwargs): extract all aligned sentences involv-
ing English, French, and German, which meet
any further constraints specified in the keyword
arguments. (When called extract(type=SENT)
this will extract all sentence alignments across
all 7,000 languages, cf Figure 1.)
? dump(type=SENT, format="giza", lang=[ENG,
FRA], **kwargs): dump English-French bitext
in Giza format.
? extract(type=LEX, lang=[ENG, FRA, ...],
**kwargs): produce a comparative wordlist for
the specified languages.
? dump(type=LEX, format="csv", lang=[ENG,
FRA, ...], **kwargs): produce the wordlist in
comma-separated values format.
Additional functions will be required for discov-
ery (which annotations exist for an item?), naviga-
tion (which file does an item come from?), citation
(which publications should be cited in connection
with these items?), and report generation (what type
and quantity of material exists for each language?).
126
Further functionality could support annotation.
We do not wish to enable direct modification of
database fields, since everything in the Corpus
comes from contributed corpora. Instead, we could
foster user input and encourage crowdsourcing of
annotations by developing software clients that ac-
cess the Corpus using methods such as the ones al-
ready described, and which save any new annota-
tions as just another work to be added to the Corpus.
6 Further design considerations
Versioning. When a work is contributed, it comes
with (or is assigned) a version, or ?edition.? Multi-
ple editions of a work may coexist in the Corpus, and
each edition will have distinct filenames and identi-
fiers to avoid risk of collision. Now, it may hap-
pen that works reference each other, as when a base
text from one work is POS-tagged in another. For
this reason, we treat editions as immutable. Modi-
fications to a work are accumulated and released as
a new edition. When a new edition of a base text
is released, stand-off annotations of that text (such
as the POS-tagging in our example) will need to be
updated in turn, a task that should be largely auto-
mated. A new edition of the annotation, anchored to
the new edition of the base text, is then released. The
old editions remain unchanged, though they may be
flagged as obsolete and may eventually be deleted.
Licensing. Many corpora come with license con-
ditions that prevent them from being included. In
some cases, this is due to license fees that are paid
by institutional subscription. Here, we need to ex-
plore a new subscription model based on access. In
some cases, corpus redistribution is not permitted,
simply in order to ensure that all downloads occur
from one site (and can be counted as evidence of
impact), and so that users agree to cite the scholarly
publication about the corpus. Here we can offer data
providers a credible alternative: anonymized usage
tracking, and an automatic way for authors to iden-
tify the publications associated with any slice of the
Corpus, facilitating comprehensive citation.
Publication. The Corpus will be an online publi-
cation, with downloadable dated snapshots, evolv-
ing continually as new works and editions are added.
An editorial process will be required, to ensure that
contributions are appropriate, and to avoid spam-
ming. A separate staging area would facilitate
checking of incoming materials prior to release.
7 Conclusion
We have described the design and implementation
of a Universal Corpus containing aligned and anno-
tated text collections for the world?s languages. We
follow the same principles we set out earlier (Abney
and Bird, 2010, 2.2), promoting a community-level
effort to collect bilingual texts and lexicons for as
many languages as possible, in a consistent format
that facilitates machine processing across languages.
We have proposed a normalized filestore model that
integrates with current practice on the supply side,
where corpora are freestanding works in a variety
of formats and multiple editions. We have also de-
vised a normalized database model which encom-
passes the desired range of linguistic objects, align-
ments, and annotations. Finally, we have argued that
this model scales, and enables a view of the Univer-
sal Corpus as a vast matrix of aligned and analyzed
texts spanning the world?s languages, a radical de-
parture from existing resource creation efforts in lan-
guage documentation and machine translation.
We invite participation by the community in elab-
orating the design, implementing the storage model,
and populating it with data. Furthermore, we seek
collaboration in using such data as the basis for
large-scale cross-linguistic analysis and modeling,
and in facilitating the creation of easily accessible
language resources for the world?s languages.
References
Steven Abney and Steven Bird. 2010. The Human
Language Project: building a universal corpus of the
world?s languages. In Proc. 48th ACL, pages 88?97.
Association for Computational Linguistics.
Sabine Buchholz, Erwin Marsi, Yuval Krymolowski, and
Amit Dubey. 2006. CoNLL-X shared task: Multi-
lingual dependency parsing. http://ilk.uvt.
nl/conll/. Accessed May 2011.
Nikolaus P. Himmelmann. 1998. Documentary and de-
scriptive linguistics. Linguistics, 36:161?195.
Gary Simons and Steven Bird. 2003. The Open Lan-
guage Archives Community: An infrastructure for dis-
tributed archiving of language resources. Literary and
Linguistic Computing, 18:117?128.
127
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 1?5,
Baltimore, Maryland, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Aikuma: A Mobile App for Collaborative Language Documentation
Steven Bird
1,2
, Florian R. Hanke
1
, Oliver Adams
1
, and Haejoong Lee
2
1
Dept of Computing and Information Systems, University of Melbourne
2
Linguistic Data Consortium, University of Pennsylvania
Abstract
Proliferating smartphones and mobile
software offer linguists a scalable, net-
worked recording device. This paper de-
scribes Aikuma, a mobile app that is de-
signed to put the key language documen-
tation tasks of recording, respeaking, and
translating in the hands of a speech com-
munity. After motivating the approach we
describe the system and briefly report on
its use in field tests.
1 Introduction
The core of a language documentation consists
of primary recordings along with transcriptions
and translations (Himmelmann, 1998; Woodbury,
2003). Many members of a linguistic community
may contribute to a language documentation, play-
ing roles that depend upon their linguistic com-
petencies. For instance, the best person to pro-
vide a text could be a monolingual elder, while the
best person to translate it could be a younger bilin-
gual speaker. Someone else again may be the best
choice for performing transcription work. What-
ever the workflow and degree of collaboration,
there is always the need to manage files and cre-
ate secondary materials, a data management prob-
Figure 1: Phrase-aligned bilingual audio
lem. The problem is amplified by the usual prob-
lems that attend linguistic fieldwork: limited hu-
man resources, limited communication, and lim-
ited bandwidth.
The problem is not to collect large quantities of
primary audio in the field using mobile devices (de
Vries et al., 2014). Rather, the problem is to en-
sure the long-term interpretability of the collected
recordings. At the most fundamental level, we
want to know what words were spoken, and what
they meant. Recordings made in the wild suf-
fer from the expected range of problems: far-field
recording, significant ambient noise, audience par-
ticipation, and so forth. We address these prob-
lems via the ?respeaking? task (Woodbury, 2003).
Recordings made in an endangered language may
not be interpretable once the language falls out of
use. We address this problem via the ?oral trans-
lation? task. The result is relatively clean source
audio recordings with phrase-aligned translations
(see Figure 1). NLP methods are applicable to
such data (Dredze et al., 2010), and we can hope
that ultimately, researchers working on archived
bilingual audio sources will be able to automati-
cally extract word-glossed interlinear text.
We describe Aikuma, an open source Android
app that supports recording along with respeaking
Figure 2: Adding a time-aligned translation
1
and oral translation, while capturing basic meta-
data. Aikuma supports local networking so that
a set of mobile phones can be synchronized, and
anyone can listen to and annotate the recordings
made by others. Key functionality is provided
via a text-less interface (Figure 2). Aikuma in-
troduces social media and networked collabora-
tion to village-based fieldwork, all on low-cost de-
vices, and this is a boon for scaling up the quan-
tity of documentary material that can be collected
and processed. Field trials in Papua New Guinea,
Brazil, and Nepal have demonstrated the effective-
ness of the approach (Bird et al., 2014).
2 Thought Experiment: The Future
Philologist
A typical language documentation project is
resource-bound. So much documentation could be
collected, yet the required human resources to pro-
cess it all adequately are often not available. For
instance, some have argued that it is not effective
to collect large quantities of primary recordings
because there is not the time to transcribe it.
1
Estimates differ about the pace of language loss.
Yet it is uncontroversial that ? for hundreds of lan-
guages ? only the oldest living speakers are well-
versed in traditional folklore. While a given lan-
guage may survive for several more decades, the
opportunity to document significant genres may
pass much sooner. Ideally, a large quantity of these
nearly-extinct genres would be recorded and given
sufficient further treatment in the form of respeak-
ings and oral translations, in order to have archival
value. Accordingly, we would like to determine
what documentary materials would be of greatest
practical value to the linguist working in the fu-
ture, possibly ten to a hundred or more years in
future. Given the interest of classical philology in
ancient languages, we think of this researcher as
the ?future philologist.?
Our starting point is texts, as the least processed
item of the so-called ?Boasian trilogy.? A substan-
tial text corpus can serve as the basis for the prepa-
ration of grammars and dictionaries even once a
language is extinct, as we know from the cases of
the extinct languages of the Ancient Near East.
1
E.g. Paul Newman?s 2013 seminar The Law of Un-
intended Consequences: How the Endangered Languages
Movement Undermines Field Linguistics as a Scientific
Enterprise, https://www.youtube.com/watch?v=
xziE08ozQok
Our primary resource is the native speaker com-
munity, both those living in the ancestral home-
land and the members of the diaspora. How
can we engage these communities in the tasks
of recording, respeaking, and oral interpretation,
in order to generate the substantial quantity of
archival documentation?
Respeaking involves listening to an original
recording and repeating what was heard carefully
and slowly, in a quiet recording environment It
gives archival value to recordings that were made
?in the wild? on low-quality devices, with back-
ground noise, and by people having no training in
linguistics. It provides much clearer audio content,
facilitating transcription. Bettinson (2013) has
shown that human transcribers, without knowl-
edge of the language under study, can generally
produce phonetic transcriptions from such record-
ings that are close enough to enable someone who
knows the language to understand what was said,
and which can be used as the basis for phonetic
analysis. This means we can postpone the tran-
scription task ? by years or even decades ? un-
til such time as the required linguistic expertise is
available to work with archived recordings.
By interpretation, we mean listening to a
recording and producing a spoken translation of
what was heard. Translation into another language
obviates the need for the usual resource-intensive
approaches to linguistic analysis that require syn-
tactic treebanks along with semantic annotations,
at the cost of a future decipherment effort (Xia and
Lewis, 2007; Abney and Bird, 2010).
3 Design Principles
Several considerations informed the design of
Aikuma. First, to facilitate use by monolingual
speakers, the primary recording functions need to
be text free.
Second, to facilitate collaboration and guard
against loss of phones, it needs to be possible
to continuously synchronise files between phones.
Once any information has been captured on a
phone, it is synchronized to the other phones on
the local network. All content from any phone is
available from any phone, and thus only a single
phone needs to make it back from village-based
work. After a recording is made, it needs to be
possible to listen to it on the other phones on the
local network. This makes it easy for people to
annotate each other?s recordings. This also en-
2
ables users to experience the dissemination of their
recordings, and to understand that a private activ-
ity of recording a narrative is tantamount to public
speaking. This is useful for establishing informed
consent in communities who have no previous ex-
perience of the Internet or digital archiving.
Third, to facilitate trouble-shooting and future
digital archaeology, the file format of phones
needs to be transparent. We have devised an
easily-interpretable directory hierarchy for record-
ings and users, which permits direct manipulation
of recordings. For instance, all the metadata and
recordings that involve a particular speaker could
be extracted from the hierarchy with a single file-
name pattern.
4 Aikuma
Thanks to proliferating smartphones, it is now rel-
atively easy and cheap for untrained people to col-
lect and share many sorts of recordings, for their
own benefit and also for the benefit of language
preservation efforts. These include oral histories,
oral literature, public speaking, and discussion of
popular culture. With inexpensive equipment and
minimal training, a few dozen motivated people
can create a hundred hours of recorded speech (ap-
prox 1M words) in a few weeks. However, adding
transcription and translation by a trained linguist
introduces a bottleneck: most languages will be
gone before linguists will get to them.
Aikuma puts this work in the hands of language
speakers. It collects recordings, respeakings, and
interpretations, and organizes them for later syn-
chronization with the cloud and archival storage.
People with limited formal education and no prior
experience using smartphones can readily use the
app to record their stories, or listen to other peo-
ple?s stories to respeak or interpret them. Literate
users can supply written transcriptions and trans-
lations. Items can be rated by the linguist and
language workers and highly rated items are dis-
played more prominently, and this may be used to
influence the documentary workflow. Recordings
are stored alongside a wealth of metadata, includ-
ing language, GPS coordinates, speaker, and off-
sets on time-aligned translations and comments.
4.1 Listing and saving recordings
When the app is first started, it shows a list of
available recordings, indicating whether they are
respeakings or translations (Figure 3(a)). These
recordings could have been made on this phone, or
synced to this phone from another, or downloaded
from an archive. The recording functionality is
accessed by pressing the red circle, and when the
user is finished, s/he is prompted to add metadata
to identify the person or people who were recorded
(Figure 3(b)) and the language(s) of the recording
(Figure 3(c)).
(a) Main list (b) Adding speaker metadata (c) Adding language metadata
Figure 3: Screens for listing and saving recordings
3
4.2 Playback and commentary
When a recording is selected, the user sees a dis-
play for the individual recording, with its name,
date, duration, and images of the participants,
cf. Figure 4.
Figure 4: Recording playback screen
The availability of commentaries is indicated by
user images beneath the timeline. Once an orig-
inal recording has commentaries, their locations
are displayed within the playback slider. Playback
interleaves the original recording with the spoken
commentary, cf Figure 5.
Figure 5: Commentary playback screen
4.3 Gesture vs voice activation
Aikuma provides two ways to control any record-
ing activity, using gesture or voice activation. In
the gesture-activated mode, playback is started,
paused, or stopped using on-screen buttons. For
commentary, the user presses and holds the play
button to listen to the source, and presses and holds
the record button to supply a commentary, cf Fig-
ure 2. Activity is suspended when neither button
is being pressed.
In the voice-activated mode, the user puts the
phone to his or her ear and playback begins au-
tomatically. Playback is paused when the user
lifts the phone away from the ear. When the user
speaks, playback stops and the speech is recorded
and aligned with the source recording.
4.4 File storage
The app supports importing of external audio files,
so that existing recordings can be put through the
respeaking and oral translation processes. Stor-
age uses a hierarchical file structure and plain text
metadata formats which can be easily accessed di-
rectly using command-line tools. Files are shared
using FTP. Transcripts are stored using the plain
text NIST HUB-4 transcription format and can be
exported in Elan format.
4.5 Transcription
Aikuma incorporates a webserver and clients can
connect using the phone?s WiFi, Bluetooth, or
USB interfaces. The app provides a browser-based
transcription tool that displays the waveform for
a recording along with the spoken annotations.
Users listen to the source recording along with any
available respeakings and oral translations, and
then segment the audio and enter his or her own
written transcription and translation. These are
saved to the phone?s storage and displayed on the
phone during audio playback.
5 Deployment
We have tested Aikuma in Papua New Guinea,
Brazil, and Nepal (Bird et al., 2014). We taught
members of remote indigenous communities to
record narratives and orally interpret them into a
language of wider communication. We collected
approximately 10 hours of audio, equivalent to
100k words. We found that the networking capa-
bility facilitated the contribution of multiple mem-
bers of the community who have a variety of lin-
guistic aptitudes. We demonstrated that the plat-
form is an effective way to engage remote indige-
nous speech communities in the task of building
phrase-aligned bilingual speech corpora. To sup-
port large scale deployment, we are adding sup-
port for workflow management, plus interfaces to
the Internet Archive and to SoundCloud for long
term preservation and social interaction.
Acknowledgments
We gratefully acknowledge support from the Aus-
tralian Research Council, the National Science
Foundation, and the Swiss National Science Foun-
dation. We are also grateful to Isaac McAlister,
Katie Gelbart, and Lauren Gawne for field-testing
work. Aikuma development is hosted on GitHub.
4
References
Steven Abney and Steven Bird. 2010. The Human
Language Project: building a universal corpus of the
world?s languages. In Proceedings of the 48th Meet-
ing of the Association for Computational Linguis-
tics, pages 88?97. Association for Computational
Linguistics.
Mat Bettinson. 2013. The effect of respeaking on tran-
scription accuracy. Honours Thesis, Dept of Lin-
guistics, University of Melbourne.
Steven Bird, Isaac McAlister, Katie Gelbart, and Lau-
ren Gawne. 2014. Collecting bilingual audio in re-
mote indigenous villages. under review.
Nic de Vries, Marelie Davel, Jaco Badenhorst, Willem
Basson, Febe de Wet, Etienne Barnard, and Alta
de Waal. 2014. A smartphone-based ASR data col-
lection tool for under-resourced languages. Speech
Communication, 56:119?131.
Mark Dredze, Aren Jansen, Glen Coppersmith, and
Ken Church. 2010. NLP on spoken documents
without ASR. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 460?470. Association for Com-
putational Linguistics.
Florian R. Hanke and Steven Bird. 2013. Large-
scale text collection for unwritten languages. In Pro-
ceedings of the 6th International Joint Conference
on Natural Language Processing, pages 1134?1138.
Asian Federation of Natural Language Processing.
Nikolaus P. Himmelmann. 1998. Documentary and
descriptive linguistics. Linguistics, 36:161?195.
Anthony C. Woodbury. 2003. Defining documentary
linguistics. In Peter Austin, editor, Language Docu-
mentation and Description, volume 1, pages 35?51.
London: SOAS.
Fei Xia and William D. Lewis. 2007. Multilingual
structural projection across interlinearized text. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics, pages
452?459. Association for Computational Linguis-
tics.
5
