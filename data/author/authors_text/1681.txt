Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning Expressive Models for Word Sense Disambiguation 
Lucia Specia 
NILC/ICMC 
University of S?o Paulo 
Caixa Postal 668, 13560-970 
S?o Carlos, SP, Brazil 
lspecia@icmc.usp.br 
Mark Stevenson 
Department of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St. 
Sheffield, S1 4DP, UK  
marks@dcs.shef.ac.uk 
Maria das Gra?as V. Nunes 
NILC/ICMC 
University of S?o Paulo 
Caixa Postal 668, 13560-970  
S?o Carlos, SP, Brazil 
gracan@icmc.usp.br 
 
 
Abstract 
We present a novel approach to the word 
sense disambiguation problem which 
makes use of corpus-based evidence com-
bined with background knowledge. Em-
ploying an inductive logic programming 
algorithm, the approach generates expres-
sive disambiguation rules which exploit 
several knowledge sources and can also 
model relations between them. The ap-
proach is evaluated in two tasks: identifica-
tion of the correct translation for a set of 
highly ambiguous verbs in English-
Portuguese translation and disambiguation 
of verbs from the Senseval-3 lexical sam-
ple task. The average accuracy obtained for 
the multilingual task outperforms the other 
machine learning techniques investigated. 
In the monolingual task, the approach per-
forms as well as the state-of-the-art sys-
tems which reported results for the same 
set of verbs. 
1 Introduction 
Word Sense Disambiguation (WSD) is concerned 
with the identification of the meaning of ambi-
guous words in context. For example, among the 
possible senses of the verb ?run? are ?to move fast 
by using one's feet? and ?to direct or control?. 
WSD can be useful for many applications, includ-
ing information retrieval, information extraction 
and machine translation. Sense ambiguity has been 
recognized as one of the most important obstacles 
to successful language understanding since the ear-
ly 1960?s and many techniques have been pro-
posed to solve the problem. Recent approaches 
focus on the use of various lexical resources and 
corpus-based techniques in order to avoid the sub-
stantial effort required to codify linguistic know-
ledge. These approaches have shown good results; 
particularly those using supervised learning (see 
Mihalcea et al, 2004 for an overview of state-of-
the-art systems). However, current approaches rely 
on limited knowledge representation and modeling 
techniques: traditional machine learning algorithms 
and attribute-value vectors to represent disambigu-
ation instances. This has made it difficult to exploit 
deep knowledge sources in the generation of the 
disambiguation models, that is, knowledge that 
goes beyond simple features extracted directly 
from the corpus, like bags-of-words and colloca-
tions, or provided by shallow natural language 
tools like part-of-speech taggers.  
In this paper we present a novel approach for 
WSD that follows a hybrid strategy, i.e. combines 
knowledge and corpus-based evidence, and em-
ploys a first-order formalism to allow the represen-
tation of deep knowledge about disambiguation 
examples together with a powerful modeling tech-
nique to induce theories based on the examples and 
background knowledge. This is achieved using 
Inductive Logic Programming (ILP) (Muggleton, 
1991), which has not yet been applied to WSD.  
Our hypothesis is that by using a very expres-
sive representation formalism, a range of (shallow 
and deep) knowledge sources and ILP as learning 
technique, it is possible to generate models that, 
when compared to models produced by machine 
learning algorithms conventionally applied to 
41
WSD, are both more accurate for fine-grained dis-
tinctions, and ?interesting?, from a knowledge ac-
quisition point of view (i.e., convey potentially 
new knowledge that can be easily interpreted by 
humans).  
WSD systems have generally been more suc-
cessful in the disambiguation of nouns than other 
grammatical categories (Mihalcea et al, 2004). A 
common approach to the disambiguation of nouns 
has been to consider a wide context around the 
ambiguous word and treat it as a bag of words or 
limited set of collocates. However, disambiguation 
of verbs generally benefits from more specific 
knowledge sources, such as the verb?s relation to 
other items in the sentence (for example, by ana-
lysing the semantic type of its subject and object). 
Consequently, we believe that the disambiguation 
of verbs is task to which ILP is particularly well-
suited. Therefore, this paper focuses on the disam-
biguation of verbs, which is an interesting task 
since much of the previous work on WSD has con-
centrated on the disambiguation of nouns.  
WSD is usually approached as an independent 
task, however, it has been argued that different 
applications may have specific requirements (Res-
nik and Yarowsky, 1997). For example, in machine 
translation, WSD, or translation disambiguation, is 
responsible for identifying the correct translation 
for an ambiguous source word. There is not always 
a direct relation between the possible senses for a 
word in a (monolingual) lexicon and its transla-
tions to a particular language, so this represents a 
different task to WSD against a (monolingual) 
lexicon (Hutchins and Somers, 1992). Although it 
has been argued that WSD does not yield better 
translation quality than a machine translation 
system alone, it has been recently shown that a 
WSD module that is developed following specific 
multilingual requirements can significantly im-
prove the performance of a machine translation 
system (Carpuat et al, 2006). 
This paper focuses on the application of our ap-
proach to the translation of verbs in English to Por-
tuguese translation, specifically for a set of 10 
mainly light and highly ambiguous verbs. We also 
experiment with a monolingual task by using the 
verbs from Senseval-3 lexical sample task. We 
explore knowledge from 12 syntactic, semantic 
and pragmatic sources. In principle, the proposed 
approach could also be applied to any lexical dis-
ambiguation task by customizing the sense reposi-
tory and knowledge sources. 
In the remainder of this paper we first present 
related approaches to WSD and discuss their limi-
tations (Section 2). We then describe some basic 
concepts on ILP and our application of this tech-
nique to WSD (Section 3). Finally, we described 
our experiments and their results (Section 4).  
2 Related Work 
WSD approaches can be classified as (a) know-
ledge-based approaches, which make use of lin-
guistic knowledge, manually coded or extracted 
from lexical resources (Agirre and Rigau, 1996; 
Lesk 1986); (b) corpus-based approaches, which 
make use of shallow knowledge automatically ac-
quired from corpus and statistical or machine 
learning algorithms to induce disambiguation 
models (Yarowsky, 1995; Sch?tze 1998); and (c) 
hybrid approaches, which mix characteristics from 
the two other approaches to automatically acquire 
disambiguation models from corpus supported by 
linguistic knowledge (Ng and Lee 1996; Stevenson 
and Wilks, 2001). 
Hybrid approaches can combine advantages 
from both strategies, potentially yielding accurate 
and comprehensive systems, particularly when 
deep knowledge is explored. Linguistic knowledge 
is available in electronic resources suitable for 
practical use, such as WordNet (Fellbaum, 1998), 
dictionaries and parsers. However, the use of this 
information has been hampered by the limitations 
of the modeling techniques that have been ex-
plored so far: using deep sources of domain know-
ledge is beyond the capabilities of such techniques, 
which are in general based on attribute-value vec-
tor representations. 
Attribute-value vectors consist of a set of 
attributes intended to represent properties of the 
examples. Each attribute has a type (its name) and 
a single value for a given example. Therefore, 
attribute-value vectors have the same expressive-
ness as propositional formalisms, that is, they only 
allow the representation of atomic propositions and 
constants. These are the representations used by 
most of the machine learning algorithms conven-
tionally employed to WSD, for example Na?ve 
Bayes and decision-trees. First-order logic, a more 
expressive formalism which is employed by ILP, 
allows the representation of variables and n-ary 
predicates, i.e., relational knowledge.  
42
In the hybrid approaches that have been ex-
plored so far, deep knowledge, like selectional pre-
ferences, is either pre-processed into a vector 
representation to accommodate machine learning 
algorithms, or used in previous steps to filter out 
possible senses e.g. (Stevenson and Wilks, 2001). 
This may cause information to be lost and, in addi-
tion, deep knowledge sources cannot interact in the 
learning process. As a consequence, the models 
produced reflect only the shallow knowledge that 
is provided to the learning algorithm.  
Another limitation of attribute-value vectors is 
the need for a unique representation for all the ex-
amples: one attribute is created for every knowl-
edge feature and the same structure is used to 
characterize all the examples. This usually results 
in a very sparse representation of the data, given 
that values for certain features will not be available 
for many examples. The problem of data sparse-
ness increases as more knowledge is exploited and 
this can cause problems for the machine learning 
algorithms. 
A final disadvantage of attribute-value vectors 
is that equivalent features may have to be bounded 
to distinct identifiers. An example of this occurs 
when the syntactic relations between words in a 
sentence are represented by attributes for each pos-
sible relation, sentences in which there is more 
than one instantiation for a particular grammatical 
role cannot be easily represented.  For example, the 
sentence ?John and Anna gave Mary a present.? 
contains a coordinate subject and, since each fea-
ture requires a unique identifier, two are required 
(subj1-verb1, subj2-verb1). These will be treated as 
two independent pieces of knowledge by the learn-
ing algorithm.  
First-order formalisms allow a generic predicate 
to be created for every possible syntactic role, re-
lating two or more elements. For example 
has_subject(verb, subject), which could then have 
two instantiations: has_subject(give, john) and 
has_subject(give, anna). Since each example is 
represented independently from the others, the data 
sparseness problem is minimized. Therefore, ILP 
seems to provide the most general-purpose frame-
work for dealing with such data: it does not suffer 
from the limitations mentioned above since there 
are explicit provisions made for the inclusion of 
background knowledge of any form, and the repre-
sentation language is powerful enough to capture 
contextual relationships. 
3 A hybrid relational approach to WSD 
In what follows we provide an introduction to ILP 
and then outline how it is applied to WSD by pre-
senting the sample corpus and knowledge sources 
used in our experiments. 
3.1 Inductive Logic Programming 
Inductive Logic Programming (Muggleton, 1991) 
employs techniques from Machine Learning and 
Logic Programming to build first-order theories 
from examples and background knowledge, which 
are also represented by first-order clauses. It allows 
the efficient representation of substantial know-
ledge about the problem, which is used during the 
learning process, and produces disambiguation 
models that can make use of this knowledge. The 
general approach underlying ILP can be outlined 
as follows:  
Given: 
-  a set of positive and negative examples E = 
E+ ? E- 
- a predicate p specifying the target relation to 
be learned 
- knowledge ? of the domain, described ac-
cording to a language Lk, which specifies which 
predicates qi can be part of the definition of p. 
The goal is: to induce a hypothesis (or theory) 
h for p, with relation to E and ?, which covers 
most of the E+, without covering the E-, i.e., K ? h 
 E+ and K ? h  E-.  
 
We use the Aleph ILP system (Srinivasan, 2000), 
which provides a complete inference engine and 
can be customized in various ways. The default 
inference engine induces a theory iteratively using 
the following steps: 
1. One instance is randomly selected to be gen-
eralized.  
2. A more specific clause (the bottom clause) is 
built using inverse entailment (Muggleton, 1995), 
generally consisting of the representation of all the 
knowledge about that example. 
3. A clause that is more generic than the bottom 
clause is searched for using a given search (e.g., 
best-first) and evaluation strategy (e.g., number of 
positive examples covered). 
4. The best clause is added to the theory and the 
examples covered by that clause are removed from 
the sample set. Stop if there are more no examples 
in the training set, otherwise return to step 1. 
43
3.2 Sample data 
This approach was evaluated using two scenarios: 
(1) an English-Portuguese multilingual setting ad-
dressing 10 very frequent and problematic verbs 
selected in a previous study (Specia et. al., 2005); 
and (2) an English setting consisting of 32 verbs 
from Senseval-3 lexical sample task (Mihalcea et. 
al. 2004). 
For the first scenario a corpus containing 500 
sentences for each of the 10 verbs was constructed. 
The text was randomly selected from corpora of 
different domains and genres, including literary 
fiction, Bible, computer science dissertation ab-
stracts, operational system user manuals, newspa-
pers and European Parliament proceedings. This 
corpus was automatically annotated with the trans-
lation of the verb using a tagging system based on 
parallel corpus, statistical information and transla-
tion dictionaries (Specia et al, 2005), followed by 
a manual revision. For each verb, the sense reposi-
tory was defined as the set of all the possible trans-
lations of that verb in the corpus. 80% of the 
corpus was randomly selected and used for train-
ing, with the remainder retained for testing. The 10 
verbs, number of possible translations and the per-
centage of sentences for each verb which use the 
most frequent translation are shown in Table 1. 
For the monolingual scenario, we use the sense 
tagged corpus and sense repositories provided for 
verbs in Senseval-3. There are 32 verbs with be-
tween 40 and 398 examples each. The number of 
senses varies between 3 and 10 and the average 
percentage of examples with the majority (most 
frequent) sense is 55%.  
 
 Verb # Translations Most frequent 
translation - % 
ask 7 53 
come 29 36 
get 41 13 
give 22 72 
go 30 53 
live 8 66 
look 12 41 
make 21 70 
take 32 25 
tell 8 66 
Table 1. Verbs and possible senses in our corpus 
 
Both corpora were lemmatized and part-of-speech 
(POS) tagged using Minipar (Lin, 1993) and 
Mxpost (Ratnaparkhi, 1996), respectivelly. Addi-
tionally, proper nouns identified by the tagger were 
replaced by a single identifier (proper_noun) and 
pronouns replaced by identifiers representing 
classes of pronouns (relative_pronoun, etc.).  
3.3 Knowledge sources 
We now describe the background knowledge 
sources used by the learning algorithm, having as 
an example sentence (1), in which the word ?com-
ing? is the target verb being disambiguated. 
 
(1) "If there is such a thing as reincarnation, I 
would not mind coming back as a squirrel". 
 
KS1. Bag-of-words consisting of 5 words to the 
right and left of the verb (excluding stop words), 
represented using definitions of the form 
has_bag(snt, word): 
has_bag(snt1, mind). 
has_bag(snt1, not). ? 
 
KS2. Frequent bigrams consisting of pairs of adja-
cent words in a sentence (other than the target 
verb) which occur more than 10 times in the cor-
pus, represented by has_bigram(snt, word1, 
word2): 
has_bigram(snt1, back, as). 
has_bigram(snt1, such, a). ? 
 
KS3. Narrow context containing 5 content words to 
the right and left of the verb, identified using POS 
tags, represented by has_narrow(snt, 
word_position, word): 
has_narrow(snt1, 1st_word_left, mind). 
has_narrow(snt1, 1st_word_right, back). ? 
 
KS4. POS tags of 5 words to the right and left of 
the verb, represented by has_pos(snt, 
word_position, pos): 
has pos(snt1, 1st_word_left, nn). 
has pos(snt1, 1st_word_right, rb). ? 
 
KS5. 11 collocations of the verb: 1st preposition to 
the right, 1st and 2nd words to the left and right, 
1st noun, 1st adjective, and 1st verb to the left and 
right. These are represented using definitions of the 
form has_collocation(snt, type, collocation): 
has_collocation(snt1, 1st_prep_right, back). 
has_collocation(snt1, 1st_noun_left, mind).? 
44
KS6. Subject and object of the verb obtained using 
Minipar and represented by has_rel(snt, type, 
word): 
has_rel(snt1, subject, i). 
has_rel(snt1, object, nil). ? 
 
KS7. Grammatical relations not including the tar-
get verb also identified using Minipar. The rela-
tions (verb-subject, verb-object, verb-modifier, 
subject-modifier, and object-modifier) occurring 
more than 10 times in the corpus are represented 
by has_related_pair(snt, word1, word2): 
has_related_pair(snt1, there, be). ? 
 
KS8. The sense with the highest count of overlap-
ping words in its dictionary definition and in the 
sentence containing the target verb (excluding stop 
words) (Lesk, 1986), represented by 
has_overlapping(sentence, translation): 
has_overlapping(snt1, voltar). 
 
KS9. Selectional restrictions of the verbs defined 
using LDOCE (Procter, 1978). WordNet is used 
when the restrictions imposed by the verb are not 
part of the description of its arguments, but can be 
satisfied by synonyms or hyperonyms of those ar-
guments. A hierarchy of feature types is used to 
account for restrictions established by the verb that 
are more generic than the features describing its 
arguments in the sentence. This information is 
represented by definitions of the form satis-
fy_restriction(snt, rest_subject, rest_object): 
satisfy_restriction(snt1, [human], nil). 
satisfy_restriction(snt1, [animal, human], nil). 
 
KS1-KS9 can be applied to both multilingual and 
monolingual disambiguation tasks. The following 
knowledge sources were specifically designed for 
multilingual applications: 
 
KS10. Phrasal verbs in the sentence identified using 
a list extracted from various dictionaries. (This 
information was not used in the monolingual task 
because phrasal constructions are not considered 
verb senses in Senseval data.) These are 
represented by definitions of the form 
has_expression(snt, verbal_expression):  
has_expression(snt1, ?come back?). 
 
KS11. Five words to the right and left of the target 
verb in the Portuguese translation. This could be 
obtained using a machine translation system that 
would first translate the non-ambiguous words in 
the sentence. In our experiments it was extracted 
using a parallel corpus and represented using defi-
nitions of the form has_bag_trns(snt, portu-
guese_word): 
has_bag_trns(snt1, coelho). 
has_bag_trns(snt1, reincarna??o). ? 
 
KS12. Narrow context consisting of 5 collocations 
of the verb in the Portuguese translation, which 
take into account the positions of the words, 
represented by has_narrow_trns(snt, 
word_position, portuguese_word): 
has_narrow_trns(snt1, 1st_word_right, como). 
has_narrow_trns(snt1, 2nd_word_right, um). ? 
 
In addition to background knowledge, the system 
learns from a set of examples. Since all knowledge 
about them is expressed as background knowledge, 
their representation is very simple, containing only 
the sentence identifier and the sense of the verb in 
that sentence, i.e. sense(snt, sense): 
sense(snt1,voltar).  
sense(snt2,ir). ? 
 
Based on the examples, background knowledge 
and a series of settings specifying the predicate to 
be learned (i.e., the heads of the rules), the predi-
cates that can be in the conditional part of the 
rules, how the arguments can be shared among dif-
ferent  predicates and several other parameters, the 
inference engine produces a set of symbolic rules. 
Figure 1 shows examples of the rules induced for 
the verb ?to come? in the multilingual task.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Examples of rules produced for the verb 
?come? in the multilingual task 
 
Rule_1. sense(A, voltar) :- 
    has_collocation(A, 1st_prep_right, back). 
Rule_2. sense(A, chegar) :- 
   has_rel(A, subj, B), has_bigram(A, today, B), 
   has_bag_trans(A, hoje). 
Rule_3. sense(A, chegar) :- 
    satisfy_restriction(A, [animal, human], [concrete]); 
    has_expression(A, 'come at'). 
Rule_4. sense(A, vir) :- 
    satisfy_restriction(A, [animate], nil);  
    (has_rel(A, subj, B), 
    (has_pos(A, B, nnp); has_pos(A, B, prp))). 
 
45
Models learned with ILP are symbolic and can be 
easily interpreted. Additionally, innovative knowl-
edge about the problem can emerge from the rules 
learned by the system. Although some rules simply 
test shallow features such as collocates, others pose 
conditions on sets of knowledge sources, including 
relational sources, and allow non-instantiated ar-
guments to be shared amongst them by means of 
variables. For example, in Figure 1, Rule_1 states 
that the translation of the verb in a sentence A will 
be ?voltar? (return) if the first preposition to the 
right of the verb in that sentence is ?back?. Rule_2 
states that the translation of the verb will be 
?chegar? (arrive) if it has a certain subject B, 
which occurs frequently with the word ?today? as a 
bigram, and if the partially translated sentence con-
tains the word ?hoje? (the translation of ?today?). 
Rule_3 says that the translation of the verb will be 
?chegar? (reach) if the subject of the verb has the 
features ?animal? or ?human? and the object has 
the feature ?concrete?, or if the verb occurs in the 
expression ?come at?. Rule_4 states that the trans-
lation of the verb will be ?vir? (move toward) if the 
subject of the verb has the feature ?animate? and 
there is no object, or if the verb has a subject B that 
is a proper noun (nnp) or a personal pronoun (prp). 
4 Experiments and results 
To assess the performance of the approach the 
model produced for each verb was tested on the 
corresponding set of test cases by applying the 
rules in a decision-list like approach, i.e., retaining 
the order in which they were produced and backing 
off to the most frequent sense in the training set to 
classify cases that were not covered by any of the 
rules. All the knowledge sources were made avail-
able to be used by the inference engine, since pre-
vious experiments showed that they are all relevant 
(Specia, 2006). In what follows we present the re-
sults and discuss each task.  
4.1 Multilingual task 
Table 2 shows the accuracies (in terms of percen-
tage of corpus instances which were correctly dis-
ambiguated) obtained by the Aleph models. 
Results are compared against the accuracy that 
would be obtained by using the most frequent 
translation in the training set to classify all the ex-
amples of the test set (in the column labeled ?Ma-
jority sense?). For comparison, we ran experiments 
with three learning algorithms frequently used for 
WSD, which rely on knowledge represented as 
attribute-value vectors: C4.5 (decision-trees), 
Naive Bayes and Support Vector Machine (SVM)1. 
In order to represent all knowledge sources in 
attribute-value vectors, KS2, KS7, KS9 and KS10 
had to be pre-processed to be transformed into bi-
nary attributes. For example, in the case of selec-
tional restrictions (KS9), one attribute was created 
for each possible sense of the verb and a true/false 
value was assigned to it depending on whether the 
arguments of the verb satisfied any restrictions re-
ferring to that sense. Results for each of these algo-
rithms are also shown in Table 2. 
As we can see in Table 2, the accuracy of the 
ILP approach is considerably better than the most 
frequent sense baseline and also outperforms the 
other learning algorithms. This improvement is 
statistically significant (paired t-test; p < 0.05). As 
expected, accuracy is generally higher for verbs 
with fewer possible translations.  
The models produced by Aleph for all the verbs 
are reasonably compact, containing 50 to 96 rules. 
In those models the various knowledge sources 
appear in different rules and all are used. This 
demonstrates that they are all useful for the disam-
biguation of verbs. 
 
Verb Majori- 
ty sense 
C4.5 Na?ve  
Bayes 
SVM Aleph 
ask 0.68 0.68 0.82 0.88 0.92 
come 0.46 0.57 0.61 0.68 0.73 
get 0.03 0.25 0.46 0.47 0.49 
give 0.72 0.71 0.74 0.74 0.74 
go 0.49 0.61 0.66 0.66 0.66 
live 0.71 0.72 0.64 0.73 0.87 
look 0.48 0.69 0.81 0.83 0.93 
make 0.64 0.62 0.60 0.64 0.68 
take 0.14 0.41 0.50 0.51 0.59 
tell 0.65 0.67 0.66 0.68 0.82 
Average 0.50 0.59 0.65 0.68 0.74 
Table 2. Accuracies obtained by Aleph and other 
learning algorithms in the multilingual task 
 
These results are very positive, particularly if we 
consider the characteristics of the multilingual sce-
nario: (1) the verbs addressed are highly ambi-
guous; (2) the corpus was automatically tagged and 
thus distinct synonym translations were sometimes 
                                                          
1
 The implementations provided by Weka were used. Weka is 
available from http://www.cs.waikato.ac.nz/ml/weka/ 
46
used to annotate different examples (these count as 
different senses for the inference engine); and (3) 
certain translations occur very infrequently (just 1 
or 2 examples in the whole corpus). It is likely that 
a less strict evaluation regime, such as one which 
takes account of synonym translations, would re-
sult in higher accuracies. 
It is worth noticing that we experimented with a 
few relevant parameters for both Aleph and the 
other learning algorithms. Values that yielded the 
best average predictive accuracy in the training 
sets were assumed to be optimal and used to eva-
luate the test sets.  
4.2 Monolingual task 
Table 3 shows the average accuracy obtained by 
Aleph in the monolingual task (Senseval-3 verbs 
with fine-grained sense distinctions and using the 
evaluation system provided by Senseval). It also 
shows the average accuracy of the most frequent 
sense and accuracies reported on the same set of 
verbs by the best systems submitted by the sites 
which participated in this task. Syntalex-3 (Mo-
hammad and Pedersen, 2004) is based on an en-
semble of bagged decision trees with narrow 
context part-of-speech features and bigrams. 
CLaC1 (Lamjiri et al, 2004) uses a Naive Bayes 
algorithm with a dynamically adjusted context 
window around the target word. Finally, MC-WSD 
(Ciaramita and Johnson, 2004) is a multi-class av-
eraged perceptron classifier using syntactic and 
narrow context features, with one component 
trained on the data provided by Senseval and other 
trained on WordNet glosses.  
 
System % Average accuracy 
Majority sense 0.56 
Syntalex-3 0.67 
CLaC1 0.67 
MC-WSD 0.72 
Aleph 0.72 
Table 3. Accuracies obtained by Aleph and other 
approaches in the monolingual task 
 
As we can see in Table 3, results are very encour-
aging: even without being particularly customized 
for this monolingual task, the ILP approach signif-
icantly outperforms the majority sense baseline and 
performs as well as the state-of-the-art system re-
porting results for the same set of verbs. As with 
the multilingual task, the models produced contain 
a small number of rules (from 6, for verbs with a 
few examples, to 88) and all knowledge sources 
are used across different rules and verbs. 
In general, results from both multilingual and 
monolingual tasks demonstrate that the hypothesis 
put forward in Section 1, that ILP?s ability to gen-
erate expressive rules which combine and integrate 
a wide range of knowledge sources is beneficial for 
WSD systems, is correct.  
5 Conclusion 
We have introduced a new hybrid approach to 
WSD which uses ILP to combine deep and shallow 
knowledge sources. ILP induces expressive disam-
biguation models which include relations between 
knowledge sources. It is an interesting approach to 
learning which has been considered promising for 
several applications in natural language processing 
and has been explored for a few of them, namely 
POS-tagging, grammar acquisition and semantic 
parsing (Cussens et al, 1997; Mooney, 1997). This 
paper has demonstrated that ILP also yields good 
results for WSD, in particular for the disambigua-
tion of verbs.  
We plan to further evaluate our approach for 
other sets of words, including other parts-of-speech 
to allow further comparisons with other approach-
es. For example, Dang and Palmer (2005) also use 
a rich set of features with a traditional learning al-
gorithm (maximum entropy). Currently, we are 
evaluating the role of the WSD models for the 10 
verbs of the multilingual task in an English-
Portuguese statistical machine translation system. 
References 
Eneko Agirre and German Rigau. 1996. Word Sense 
Disambiguation using Conceptual Density. Proceed-
ings of the 15th Conference on Computational Lin-
guistics (COLING-96). Copenhagen, pages 16-22. 
Marine Carpuat, Yihai Shen, Xiaofeng Yu, and Dekai 
WU. 2006. Toward Integrating Word Sense and Enti-
ty Disambiguation into Statistical Machine Transla-
tion. Proceedings of the Third International 
Workshop on Spoken Language Translation,. Kyoto, 
pages 37-44. 
Massimiliano Ciaramita and Mark Johnson. 2004. Mul-
ti-component Word Sense Disambiguation. Proceed-
ings of Senseval-3: 3rd International Workshop on 
the Evaluation of Systems for the Semantic Analysis 
of Text, Barcelona, pages 97-100. 
47
James Cussens, David Page, Stephen Muggleton, and 
Ashwin Srinivasan. 1997. Using Inductive Logic 
Programming for Natural Language Processing. 
Workshop Notes on Empirical Learning of Natural 
Language Tasks, Prague, pages 25-34. 
Hoa T. Dang and Martha Palmer. 2005. The Role of 
Semantic Roles in Disambiguating Verb Senses. 
Proceedings of the 43rd Meeting of the Association 
for Computational Linguistics (ACL-05), Ann Arbor, 
pages 42?49. 
Christiane Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. MIT Press, Massachusetts.  
W. John Hutchins and Harold L. Somers. 1992. An In-
troduction to Machine Translation. Academic Press, 
Great Britain. 
Abolfazl K. Lamjiri, Osama El Demerdash, Leila Kos-
seim. 2004. Simple features for statistical Word 
Sense Disambiguation. Proceedings of Senseval-3: 
3rd International Workshop on the Evaluation of Sys-
tems for the Semantic Analysis of Text, Barcelona, 
pages 133-136. 
Michael Lesk. 1986. Automatic sense disambiguation 
using machine readable dictionaries: how to tell a 
pine cone from an ice cream cone. ACM SIGDOC 
Conference, Toronto, pages 24-26. 
Dekang Lin. 1993. Principle based parsing without 
overgeneration. Proceedings of the 31st Meeting of 
the Association for Computational Linguistics (ACL-
93), Columbus, pages 112-120. 
Rada Mihalcea, Timothy Chklovski and Adam Kilga-
riff. 2004. The Senseval-3 English Lexical Sample 
Task. Proceedings of Senseval-3: 3rd International 
Workshop on the Evaluation of Systems for Semantic 
Analysis of Text, Barcelona, pages 25-28. 
Saif Mohammad and Ted Pedersen. 2004. Complemen-
tarity of Lexical and Simple Syntactic Features: The 
SyntaLex Approach to Senseval-3. Proceedings of 
Senseval-3: 3rd International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text, 
Barcelona, pages 159-162. 
Raymond J. Mooney. 1997. Inductive Logic Program-
ming for Natural Language Processing. Proceedings 
of the 6th International Workshop on ILP, LNAI 
1314, Stockolm, pages 3-24. 
Stephen Muggleton. 1991. Inductive Logic Program-
ming. New Generation Computing, 8(4):295-318. 
Stephen Muggleton. 1995. Inverse Entailment and Pro-
gol. New Generation Computing, 13:245-286. 
 
Hwee T. Ng and Hian B. Lee. 1996. Integrating mul-
tiple knowledge sources to disambiguate word sense: 
an exemplar-based approach. Proceedings of the 34th 
Meeting of the Association for Computational 
Linguistics (ACL-96), Santa Cruz, CA, pages 40-47. 
Paul Procter (editor). 1978. Longman Dictionary of 
Contemporary English. Longman Group, Essex. 
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. Proceedings of the Conference on 
Empirical Methods in Natural Language Processing, 
New Jersey, pages 133-142. 
Phillip Resnik and David Yarowsky. 1997. A Perspec-
tive on Word Sense Disambiguation Methods and 
their Evaluating. Proceedings of the ACL-SIGLEX 
Workshop Tagging Texts with Lexical Semantics: 
Why, What and How?, Washington. 
Hinrich Sch?tze. 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics, 24(1): 97-123. 
Lucia Specia, Maria G.V. Nunes, and Mark Stevenson. 
2005. Exploiting Parallel Texts to Produce a 
Multilingual Sense Tagged Corpus for Word Sense 
Disambiguation. Proceedings of the Conference on 
Recent Advances on Natural Language Processing 
(RANLP-2005), Borovets, pages 525-531. 
Lucia Specia. 2006. A Hybrid Relational Approach for 
WSD - First Results. Proceedings of the 
COLING/ACL 06 Student Research Workshop, Syd-
ney, pages 55-60.  
Ashwin Srinivasan. 2000. The Aleph Manual. Technical 
Report. Computing Laboratory, Oxford University. 
Mark Stevenson and Yorick Wilks. 2001. The Interaction 
of Knowledge Sources for Word Sense Disambiguation. 
Computational Linguistics, 27(3):321-349. 
Yorick Wilks and Mark Stevenson. 1998. The Grammar 
of Sense: Using Part-of-speech Tags as a First Step in 
Semantic Disambiguation. Journal of Natural Lan-
guage Engineering, 4(1):1-9 
David Yarowsky. 1995. Unsupervised Word-Sense Dis-
ambiguation Rivaling Supervised Methods. 
Proceedings of the 33rd Meeting of the Association 
for Computational Linguistics (ACL-05), Cambridge, 
MA, pages 189-196.  
 
48
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 111?114,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
LIHLA: Shared task system description
Helena M. Caseli, Maria G. V. Nunes
NILC ? ICMC ? Univ. Sa?o Paulo
CP 668P, 13560-970 Sa?o Carlos?SP, Brazil
{helename,gracan}@icmc.usp.br
Mikel L. Forcada
Transducens ? DLSI ? Univ. d?Alacant
E-03071 Alacant, Spain
mlf@dlsi.ua.es
Abstract
In this paper we describe LIHLA, a lexical
aligner which uses bilingual probabilis-
tic lexicons generated by a freely availa-
ble set of tools (NATools) and language-
independent heuristics to find links be-
tween single words and multiword units
in sentence-aligned parallel texts. The
method has achieved an alignment error
rate of 22.72% and 44.49% on English?
Inuktitut and Romanian?English parallel
sentences, respectively.
1 Introduction
Alignment of words and multiword units plays an
important role in many natural language processing
(NLP) applications, such as example-based machine
translation (EBMT) (Somers, 1999) and statistical
machine translation (SMT) (Ayan et al, 2004; Och
and Ney, 2000), transfer rule learning (Carl, 2001;
Menezes and Richardson, 2001), bilingual lexi-
cography (Go?mez Guinovart and Sacau Fontenla,
2004), and word sense disambiguation (Gale et al,
1992), among others.
Aligning two (or more) texts means finding
correspondences (translation equivalences) between
segments (paragraphs, sentences, words, etc.) of the
source text and segments of its translation (the tar-
get text). Following the same idea of many recently
proposed approaches on lexical alignment (e.g., Wu
and Wang (2004) and Ayan et al (2004)), the
method described in this paper, LIHLA (Language-
Independent Heuristics Lexical Aligner) starts from
statistical alignments between single words (de-
fined in bilingual lexicons) and applies language-
independent heuristics to them, aiming at finding the
best alignments between words or multiword units.
Although the most frequent alignment category is
1 : 1 (in which one source word is translated exactly
as one target word), other categories such as omis-
sions (1 : 0 or 0 : 1) or those involving multiword
units (n : m, with n and/or m ? 1) are also possible.
This paper is organized as follows: section 2 ex-
plains how LIHLA works; section 3 describes some
experiments carried out with LIHLA together with
their results and, in section 4, some concluding re-
marks are presented.
2 How LIHLA works
As the first step, LIHLA uses alignments between
single words defined in two bilingual lexicons
(source?target and target?source) generated from
sentence-aligned parallel texts using NATools.1
Given two sentence-aligned corpus files, the NA-
Tools word aligner ?based on the Twenty-One sys-
tem (Hiemstra, 1998)? counts the co-occurrences
of words in all aligned sentence pairs and builds a
sparse matrix of word-to-word probabilities (Model
A) using an iterative expectation-maximization al-
gorithm (5 iterations by default). Finally, the ele-
ments with higher values in the matrix are cho-
sen to compose two probabilistic bilingual lexi-
cons (source?target and target?source) (Simo?es and
Almeida, 2003). For each word in the corpus, each
1NATools is a set of tools developed to work with parallel
corpora, which is freely available in http://natura.di.
uminho.pt/natura/natura/.
111
bilingual lexicon gives: the number of occurrences
of that word in the corpus (its absolute frequency)
and its most likely translations together with their
probabilities.
The construction of the bilingual lexicons is an
independent prior step for the alignment performed
by LIHLA and the same bilingual lexicons can be
used several times to align parallel sentences.
So, using the two bilingual lexicons generated
by NATools and some language-independent heuris-
tics, LIHLA tries to find the best alignment between
source and target tokens (words, numbers, special
characters, etc.) in a pair of parallel sentences. For
each source token sj in source sentence S, LIHLA
will look for the best token ti in the target parallel
sentence T applying these heuristics in sequence:
1. Exact match
LIHLA creates a 1 : 1 alignment between sj
and ti if they are identical. This heuristic stays
for exact matches, for instance, between proper
names and numbers.
2. Best candidate according to the bilingual
lexicon
LIHLA looks for possible translations of sj in
the source?target bilingual lexicon (BS) and
makes an intersection between them and the
words in T . In this intersection, if no candi-
date word identical to those in BS is found,
then LIHLA tries to look for cognates for
those words using the longest common subse-
quence ratio (LCSR).2 By doing this, LIHLA
can deal with small changes in possible trans-
lations such as different forms of the same verb,
changes in gender and/or number of nouns,
adjectives, and so on.
Then, LIHLA selects the best target candidate
word ti for sj ?the best candidate word accor-
ding to BS among those in a position which
is favorably situated in relation to sj? and
looks for multiword units involving sj and ti
?those words that occur immediately before
and/or after sj (for source multiword units) or
2The LCSR of two words is computed by dividing the length
of their longest common subsequence by the length of the
longer word. For example, the LCSR of Portuguese word alin-
hamento and Spanish word alineamiento is 1012 ' 0.83 as their
longest common subsequence is a-l-i-n-a-m-e-n-t-o.
ti (for target multiword units) and are not pos-
sible translations for other words in T and S,
respectively. According to the multiword units
that have (or not) been found, a 1 : 1, 1 : n,
m : 1 or m : n alignment is established. An
omission alignment for sj (1 : 0) can also be
established if no target candidate word ti that
satisfies this heuristic is available.
3. Cognates
If no possible translation for sj is found in the
bilingual lexicon and the target sentence (T ) at
the same time, LIHLA uses the LCSR to look
for cognates for sj in T and sets a 1 : 1 align-
ment between sj and its best cognate or a 1 : 0
alignment if there is no cognate available.
These heuristics are applied while alignments can
still be produced and a maximum number of itera-
tions is not reached (see section 3 for the number
of iterations performed in the experiments described
in this paper). Furthermore, at the first iteration,
all words with a frequency higher than a set thres-
hold are ignored to avoid erroneous alignments since
all subsequent alignments are based on the previous
ones.
In its last step (which is optional and has not
been performed in the experiments described in
this paper), LIHLA aligns the remaining unaligned
source and target tokens between two pairs of al-
ready aligned tokens establishing several 1 : 1 align-
ments when there are the same number of source
and target tokens, or just one alignment involving
all source and target tokens if they exist in different
quantities. The decision of creating n 1 : 1 align-
ments in spite of just one n : n alignment when there
is the same number of source and target tokens is due
to the fact that a 1 : 1 alignment is more likely to be
found than a n : n one.
3 Experiments
In this section we present the experiments carried
out with LIHLA for the ?Shared task on word align-
ment? in the Workshop on Building and Using Pa-
rallel Texts during ACL2005. Systems participa-
ting in this shared task were provided with training
data (consisting of sentence-aligned parallel texts)
for three pairs of languages: English?Inuktitut,
112
Romanian?English and English?Hindi. Further-
more, the systems would choose to participate in one
or both subtasks of ?limited resources? (where sys-
tems were allowed to use only the resources pro-
vided) and ?unlimited resources? (where systems
were allowed to use any resources in addition to
those provided). The system described in this pa-
per, LIHLA, participated in the subtask of limited re-
sources aligning English?Inuktitut and Romanian?
English test sets.
The training sets ?composed of 338,343
English?Inuktitut aligned sentences (omission cases
were excluded from the whole set of 340,526 pairs)
and 48,478 Romanian?English aligned ones? were
used to build the bilingual lexicons. Then,
without changing any default parameter (threshold
for LCSR, maximum number of iterations, etc.),
LIHLA aligned the 75 English?Inuktitut and the 203
Romanian?English parallel sentences on test sets.
The whole alignment process (bilingual lexicon ge-
neration and alignment itself) did not take more than
17 minutes for English?Inuktitut (3 iterations per
sentence, on average) and 7 minutes for Romanian?
English (4 iterations per sentence, on average).
The evaluation was run with respect to precision,
recall, F -measure, and alignment error rate (AER)
considering sure and probable alignments but not
NULL ones (Mihalcea and Pedersen, 2003). Tables
1 and 2 present metric values for English?Inuktitut
and Romanian?English alignments, respectively, as
provided by the organization of the shared task.
Metric Sure Probable
Precision 46.55% 79.53%
Recall 73.72% 18.71%
F -measure 57.07% 30.30%
AER 22.72%
Table 1: LIHLA results for English?Inuktitut
Metric Sure Probable
Precision 57.68% 57.68%
Recall 53.51% 53.51%
F -measure 55.51% 55.51%
AER 44.49%
Table 2: LIHLA results for Romanian?English
The results obtained in these experiments were
not so good as those achieved by LIHLA on the
language pairs for which it was developed, that
is, 92.48% of precision and 88.32% of recall on
Portuguese?Spanish parallel texts and 84.35% of
precision and 76.39% of recall on Portuguese?
English ones.3
The poor performance in the English?Inuktikut
task may be partly due to the fact that Inuktikut is
a polysynthetic language, that is, one in which, un-
like in English, words are formed by long strings of
concatenated morphemes. This makes it difficult for
NATools to build reasonable dictionaries and lead
to a predominance of n : 1 alignments, which are
harder to determine ?this fact can be confirmed by
the better precision of LIHLA when probable align-
ments were considered (see table 1). The perfor-
mance in the English?Romanian task, not very far
from the English?Portuguese task used to tune up
the parameters of the algorithm, is harder to explain
without further analysis.
The difference in precision and recall between
the two language pairs is due to the fact that on
the English?Inuktitut reference corpus in addition to
sure alignments the probable ones were also anno-
tated while in Romanian?English only sure align-
ments are found. This indicates that evaluating
alignment systems is not a simple task since their
performance depends not only on the language pairs
and the quality of parallel corpora (constant criteria
in this shared task) but also the way the reference
corpus is built.
So, at this moment, it would be unfair to blame
the worse performance of LIHLA on its alignment
methodology since it has been applied to the new
language pairs without changing any of its default
parameters. Maybe a simple optimization of para-
meters for each pair of languages could bring better
results and also the impact of size and quality of
training and reference corpora used in these experi-
ments should be investigated. Then, the only conclu-
sion that can be taken at this moment is that LIHLA,
with its heuristics and/or default parameters, can not
be indistinctly applied to any pair of languages.
Despite of its performance, LIHLA has some
3For more details of these experiments see (Caseli et al, ac-
cepted paper).
113
advantages when compared to other lexical align-
ment methods found in the literature, such as: it
does not need to be trained for a new pair of lan-
guages (as in Och and Ney (2000)) and neither does
it require pre-processing steps to handle texts (as
in Go?mez Guinovart and Sacau Fontenla (2004)).
Furthermore, the whole alignment process (bilingual
lexical generation and alignment itself) has proved
to be very fast as mentioned previously.
4 Concluding remarks
This paper has presented a lexical alignment
method, LIHLA, which aligns words and multi-
word units based on initial statistical word-to-word
correspondences and language-independent heuris-
tics.
In the experiments carried out at the ?Shared
task on word alignment? which took place at the
Workshop on Building and Using Parallel Texts
during ACL2005, LIHLA has been evaluated on
English?Inuktitut and Romanian?English parallel
texts achieving an AER of 22.72% and 44.49%,
respectively.
As future work, we aim at investigating the impact
of using additional linguistic information (such as
part-of-speech tags) on LIHLA?s performance. Also,
as a long-term goal, LIHLA will be part of a system
implemented to learn transfer rules from sequences
of aligned words.
Acknowledgments
We thank FAPESP, CAPES, CNPq and the
Spanish Ministry of Science & Technology (Project
TIC2003-08681-C02-01) for financial support.
References
Necip F. Ayan, Bonnie J. Dorr, and Nizar Habash. 2004.
Multi-Align: Combining linguistic and statistical tech-
niques to improve alignments for adaptable MT. In
R. E. Frederking and K. B. Taylor, editors, Proceed-
ings of the 6th Conference of the AMTA (AMTA-2004),
number 3265 in Lecture Notes in Artificial Inteligence
(LNAI), pages 17?26. Springer-Verlag Berlin Heidel-
berg.
Michael Carl. 2001. Inducing probabilistic invertible
translation grammars from aligned texts. In Pro-
ceedings of CoNLL-2001, pages 145?151, Toulouse,
France.
Helena M. Caseli, Maria das Grac?as V. Nunes, and
Mikel L. Forcada. (accepted paper). LIHLA: A
lexical aligner based on language-independent heuris-
tics. In Proceedings of the V Encontro Nacional de
Intelige?ncia Artificial (ENIA05), Sa?o Leopoldo, RS,
Brazil.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. Using bilingual materials to develop
word sense disambiguation methods. In Proceedings
of the 4th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI
1992), pages 101?112, Montreal, Canada, June.
Xavier Go?mez Guinovart and Elena Sacau Fontenla.
2004. Me?todos de optimizacio?n de la extraccio?n de
le?xico bilingu?e a partir de corpus paralelos. Proce-
samiento del Lenguaje Natural, 33:133?140.
Djoerd Hiemstra. 1998. Multilingual domain modeling
in Twenty-One: automatic creation of a bi-directional
translation lexicon from a parallel corpus. In Pe-
ter Arno Coppen, Hans van Halteren, and Lisanne Te-
unissen, editors, Proceedings of the 8th CLIN meeting,
pages 41?58.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the Workshop on Data-driven Machine Trans-
lation at 39th Annual Meeting of the ACL (ACL-2001),
pages 39?46, Toulouse, France.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In HLT-NAACL 2003
Workshop: Building and Using Parallel Texts Data
Driven Machine Translation and Beyond, pages 1?10,
Edmonton, May?June.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the ACL (ACL-2000), pages 440?
447, Hong Kong, China, October.
Alberto M. Simo?es and Jose? J. Almeida. 2003. NA-
Tools ? A statistical word aligner workbench. Pro-
cessamiento del Lenguaje Natural, 31:217?224.
Harold Somers. 1999. Review article: Example-based
machine translation. Machine Translation, 14(2):113?
157.
Hua Wu and Haifeng Wang. 2004. Improving domain-
specific word alignment with a general bilingual cor-
pus. In R. E. Frederking and K. B. Taylor, editors, Pro-
ceedings of the 6th Conference of the AMTA (AMTA-
2004), number 3265 in Lecture Notes in Artificial
Inteligence (LNAI), pages 262?271. Springer-Verlag
Berlin Heidelberg.
114
Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 41?50,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Capturing Disjunction in Lexicalization
with Extensible Dependency Grammar
Jorge Marques Pelizzoni
ICMC - Univ. de S?o Paulo - Brazil
Langue & Dialogue - LORIA - France
Jorge.Pelizzoni@loria.fr
Maria das Gra?as Volpe Nunes
ICMC - Univ. de S?o Paulo - Brazil
gracan@icmc.usp.br
Abstract
In spite of its potential for bidirectionality,
Extensible Dependency Grammar (XDG)
has so far been used almost exclusively
for parsing. This paper represents one of
the first steps towards an XDG-based inte-
grated generation architecture by tackling
what is arguably the most basic among
generation tasks: lexicalization. Herein
we present a constraint-based account of
disjunction in lexicalization, i.e. a way
to enable an XDG grammar to generate
all paraphrases ? along the lexicalization
axis, of course ? realizing a given in-
put semantics. Our model is (i) efficient,
yielding strong propagation, (ii) modu-
lar and (iii) favourable to synergy inas-
much as it allows collaboration between
modules, notably semantics and syntax.
We focus on constraints ensuring well-
formedness and completeness and avoid-
ing over-redundancy.
1 Introduction
In text generation the term lexicalization (Reiter
and Dale, 2000) refers to deciding which among a
choice of potentially applicable lexical items real-
izing a given intended meaning are actually going
to take part in a generated utterance. It can be re-
garded as a general, necessary generation task ?
especially if one agrees that the term task does not
necessarily imply pipelining ? and remarkably
pervasive at that. For instance, even though the
realization of such a phrase as ?a ballerina? owes
much to referring expression generation, a com-
plementary task, it is still a matter of lexicaliza-
tion whether to prioritize that specific phrase over
all its possible legitimate alternates, e.g. ?a female
dancer?, ?a dancing woman? or ?a dancing female
person?. However, prior to the statement of prior-
itizing criteria or selection preferences and rather
as the very substratum thereto, the ultimate matter
of lexicalization is exactly alternation, choice ?
in one word, disjunction.
Given the combinatorial nature of language
and specifically the interchangeability of lexical
items yielding hosts of possible valid solutions to
one same instance lexicalization task, disjunction
may well become a major source of (combinato-
rial) complexity. Our subject matter in this pa-
per is solely disjunction in lexicalization as a ba-
sis for more advanced lexicalization models, and
our purpose is precisely to describe a constraint-
based model that (i) captures the disjunctive po-
tential of lexicalization, i.e. allows the generation
of all mutually paraphrasing solutions (according
to a given language model) to any given lexical-
ization task, (ii) ensures well-formedness, espe-
cially ruling out over-redundancy (such as found
in ??a dancing female dancer/ballerina/woman?)
and syntactic anomalies (??a dancer woman?), and
does so (iii) modularly, in that not only are con-
cerns neatly separated (e.g. semantics vs. syntax),
but also solutions are reusable, and future exten-
sions, likely to be developed with no change to
current modules, (iv) efficiently, having an imple-
mentation yielding strong propagation and thus
prone to keep complexity at acceptable levels, and
(v) synergicly, inasmuch as it promotes the inter-
play between modules (namely syntax and seman-
tics) and seems compatible with the concept of in-
tegrated generation architectures (Reiter and Dale,
2000), i.e. those in which tasks are not executed in
pipeline, but are rather interleaved so as to avoid
failed or suboptimal choices during search.
We build upon the Extensible Dependency
Grammar (XDG) (Debusmann et al, 2004b; De-
busmann et al, 2004a; Debusmann et al, 2005)
model and its CP implementation in Oz (Van Roy
and Haridi, 2004), namely the XDG Develop-
ment Toolkit1 (XDK) (Debusmann et al, 2004c).
1http://www.ps.uni-sb.de/~rade/xdg.
41
In fact, all those appealing goals of modularity,
efficiency and synergy are innate to XDG and
the XDK, and our work can most correctly be
regarded as the very first attempts at equipping
XDG for generation and fulfilling its bidirectional
promise.
The paper proceeds as follows. Section 2 pro-
vides background information on XDG and the
XDK. Section 3 motivates our lexicalization dis-
junction model and describes it both intuitively
and formally, while Section 4 presents implemen-
tation highlights, assuming familiarity with the
XDK and focusing on the necessary additions and
modifications to it, as well as discussing perfor-
mance. Finally, in Section 5 we conclude and dis-
cuss future work.
2 Extensible Dependency Grammar
An informal overview of XDG?s core concepts is
in order; for a formal description of XDG, how-
ever, see (Debusmann and Smolka, 2006; Debus-
mann et al, 2005). Strictly speaking, XDG is
not a grammatical framework, but rather a descrip-
tion language over finite labelled multigraphs that
happens to show very convenient properties for
the modeling of natural language, among which a
remarkable reconciliation between monostratality,
on one side, and modularity and extensibility, on
the other.
Most of XDG?s strengths stem from its multi-
dimensional metaphor (see Fig. 1), whereby an
(holistic or multidimensional) XDG analysis con-
sists of a set of concurrent, synchronized, comple-
mentary, mutually constraining one-dimensional
analyses, each of which is itself a graph sharing
the same set of nodes as the other analyses, but
having its own type or dimension, i.e., its own
edge label and lexical feature types and its own
well-formedness constraints. In other words, each
1D analysis has a nature and interpretation of its
own, associates each node with one respective in-
stance of a data type of its own (lexical features)
and establishes its own relations/edges between
nodes using labels and principles of its own.
That might sound rather autistic at first, but the
1D components of an XDG analysis interact in
fact. It is exactly their sharing one same set of
nodes, whose sole intrinsic property is identity,
that provides the substratum for interdimensional
communication, or rather, mutual constraining.
html
Figure 1: Three concurrent one-dimensional
analyses. It is the sharing of one same set of
nodes that co-relates and synchronizes them into
one holistic XDG analysis.
ID
1
Mary
2
wants
3
to
4
laugh
5
.
root
part
vinfsubj
LP
1
Mary
2
wants
3
to
4
laugh
5
.
nounf
finf
partf
infinf
root
root
partf
vinffsubjf
DS
1
Mary
2
wants
3
to
4
laugh
5
.
root del
subjdsu
bjd
subd
PA
1
Mary
2
wants
3
to
4
laugh
5
.
roo
troot del
arg1
arge
arg1
SC
1
Mary
2
wants
3
to
4
laugh
5
.
root del
a
s
Figure 2: A 5D XDG analysis for ?Mary wants to
laugh.? according to grammar Chorus.ul deployed
with the XDK
42
That is chiefly achieved by means of two devices,
namely: multidimensional principles and lexical
synchronization.
Multidimensional principles. Principles are
reusable, usually parametric constraint predicates
used to define grammars and their dimensions.
Those posing constraints between two or more 1D
analyses are said multidimensional. For example,
the XDK library provides a host of linking princi-
ples, one of whose main applications is to regu-
late the relationship between semantic arguments
and syntactic roles according to lexical specifica-
tions. The framework allows lexical entries to con-
tain features of the type lab(D1) ? {lab(D2)},
i.e. mappings from edge labels in dimension D1
to sets of edge labels in D2. Therefore, lexi-
cal entries specifying {pat ? {subj}} might be
characteristic of unaccusative verbs, while those
with {agt ? {subj} , pat ? {obj}} would suit
a class of transitive ones. Linking principles pose
constraints taking this kind of features into ac-
count.
Lexical synchronization. The lexicon compo-
nent in XDG is specified in two steps: first, each
dimension declares its own lexicon entry type;
next, once all dimensions have been declared, lex-
icon entries are provided, each specifying the val-
ues for features on all dimensions. Finally, at run-
time it is required of well-formed analyses that
there should be at least one valid assignment of
lexicon entries to nodes such that all principles are
satisfied. In other words, every node must be as-
signed a lexicon entry that simultaneously satisfies
all principles on all dimensions, for which reason
the lexicon is said to synchronize all 1D compo-
nents of an XDG analysis. Lexical synchroniza-
tion is a major source of propagation.
Figure 2 presents a sample 5D XDG analysis
involving the most standard dimensions in XDG
practice and jargon, namely (i) PA, capturing
predicate argument structure; (ii) SC, captur-
ing the scopes of quantifiers; (iii) DS, for deep
syntax, i.e. syntactic structure modulo control and
raising phenomena; (iv) ID, for immediate domi-
nance in surface syntax (as opposed to DS); and
(v) LP, for linear precedence, i.e. a structure
tightly related to ID working as a substratum for
constraints on the order of utterance of words. In
fact, among these dimensions LP is the only one
actually to involve a concept of order. PA and DS,
in turn, are the only ones not constrained to be
trees, but directed acyclic graphs instead. Further
details on the meaning of all these dimensions, as
well as the interactions between them, would be
beyond the scope of this paper and have been dealt
with elsewhere. From Section 3 on we shall focus
on PA and, to a lesser extent, the dimension with
which it interfaces directly: DS.
Emulating deletion. Figure 2 also illustrates the
rather widespread technique of deletion, there ap-
plied to infinitival ?to? on dimensions DS, PA,
and SC. As XDG is an eminently monostratal and
thus non-transformational framework, ?deletion?
herein refers to an emulation thereof. According
to this technique, whenever a node has but one in-
coming edge with a reserved label, say del, on di-
mension D it is considered as virtually deleted on
D. In addition, one artificial root node is postu-
lated from which emerge as many del edges as re-
quired on all dimensions. The trick also comes in
handy when tackling, for instance, multiword ex-
pressions (Debusmann, 2004), which involve wor-
thy syntactic nodes that conceptually have no se-
mantic counterparts.
3 Modelling Lexicalization Disjunction
in XDG
Generation input. Having revised the basics of
XDG, it is worth mentioning that so far it has
been used mostly for parsing, in which case the in-
put type is usually rather straightforward, namely
typewritten sentences or possibly other text units.
Model creation is also very simple in parsing and
consists of (i) creating exactly one node for each
input token, all nodes being instances of one sin-
gle homogeneous feature structure type automat-
ically inferred from the grammar definition, (ii)
making each node select from all the lexical en-
tries indexed by its respective token, (iii) posing
constraints automatically generated from the prin-
ciples found in the grammar definition and (iv)
deterministically assigning values to the order-
related variables in nodes so as to reflect the actual
order of tokens in input.
As concerns generation, things are not so clear,
though. For a start, take input, which usually
varies across applications and systems, not to
mention the fact that representability and com-
putability of meaning in general are open issues.
Model creation should follow closely, as it is a di-
rect function of input. Notwithstanding, we can
43
to some extent and advantage tell what genera-
tion input is not. Under the hypothesis of an
XDG-based generation system tackling lexicaliza-
tion, input is not likely to contain some direct rep-
resentation of fully specified PA analyses, much
though this is usually regarded as a satisfactory
output for a parsing system (!). What happens
here is that generating an input PA analysis would
presuppose lexicalization having already been car-
ried out. In other words, PA analyses accounting
for e.g. ?a ballerina? and ?a dancing female hu-
man being? have absolutely nothing to do with
each other whereas what we wish is exactly to
feed input allowing both realizations. Therefore,
PA analyses are themselves part of generation out-
put and are acceptable as parsing output inasmuch
as ?de-lexicalization? is considered a trivial task,
which is not necessarily true, however.
Although our system still lacks a comprehen-
sive specification of input format and semantics,
we have already established on the basis of the
above rationale that our original PA predicates
must be decomposed into simpler, primitive predi-
cates that expose their inter-relations. For the pur-
pose of the present discussion, we understand that
it suffices to specify that our input will contain flat
first-order logic-like conjunctions such as
?x (dance(x) ? female(x) ? human(x)) ,
in order to characterize entities, even if the fi-
nal accepted language is sure to have a stricter
logic component than first-order logic and might
involve crossings with yet other formalisms. Pred-
icates, fortunately, are not necessarily unary; and,
for example, ?A ballerina tapped a lovely she-dog?
might well be generated from the following input:
?e, x, y
?
?????
dance(x) ? female(x)?
?human(x) ? event(e)?
? past(e) ? tap(e, x, y)?
?female(y) ? dog(y)?
lovely(y)
?
?????
. (1)
Deletion as the substance of disjunction. Nat-
urally, simply creating one node for each input
semantic literal is not at all the idea behind our
model. For example, if ?woman? is to be ac-
tually employed in a specific lexicalization task,
then it should continue figuring as one single node
in XDG analyses as usual in spite of potentially
covering a complex of literals. In fact, XDG and,
in specific, PA analyses should behave and resem-
ble much the same as they used to.
However, one remarkable difference of analyses
in our generation model as compared to parsing
lies in the role and scope of deletion, which in-
deed constitutes the very substance of disjunction
now. By assigning all nodes but the root one ex-
tra lexical entry synchronizing deletion on all di-
mensions2, we build an unrestrained form of dis-
junction whereby whole sets of nodes may as well
act as if not taking part in the solution. Now it
is possible to create nodes at will, even one for
each applicable lexical item, and rely on the fact
that, many ill-formed outputs as the set of all so-
lutions may contain, it still covers all correct para-
phrases, i.e. those in which all and only the right
nodes have been deleted. For example, should one
node be created for each of ?ballerina?, ?woman?,
?dancer?, ?dancing?, ?female? and ?person?, all
possible combinations of these words, including
the correct ones, are sure to be generated.
Our design obviously needs further constrain-
ing, yet the general picture should be visible by
now that we really intend to finish model creation
? or rather, start search ? with (i) a bunch of per-
fectly floating nodes in that not one edge is given
at this time, all of which are equally willing and
often going to be deleted, and (ii) a bunch of con-
straints to rule out ill-formed output and provide
for efficiency. There are two main gaps in this
summary, namely:
? what these constraints are and
? how exactly nodes are to be created.
This paper restricts itself to the first question. The
second one involves issues beyond lexicalization,
actually permeating all generation tasks, and is
currently our research priority. Consequently, in
all our experiments most of model creation was
handcrafted.
In the name of clarity, we shall hereafter ab-
stract over deletion, that is to say we shall in all re-
spects adhere to the illusion of deletion, that nodes
may cease to exist. In specific, whenever we refer
to the sisters, daughters and mothers of a node, we
mean those not due to deletion. In other words, all
happens as if deleted nodes had no relation what-
soever to any other node. This abstraction is ex-
tremely helpful and is actually employed in our
implementation, as shown in Section 4.
2That is, specifying valencies such that an incoming del
edge is required on all dimensions simultaneously.
44
3.1 How Nodes Relate
In the following description, we shall mostly re-
strict ourselves to what is novel in our model as
compared to current practice in XDG modelling.
Therefore, we shall emphasize dimension PA and
the new constraints we had to introduce in order
to have only the desired PA analyses emerge. Ex-
cept for sparse remarks on dimension DS and its
relationship with PA, which we shall also discuss
briefly, we assume without further mention the
concurrence of other XDG dimensions, principles
and concepts (e.g. lexical synchronization) in any
actual application of our model.
Referents, arguments and so nodes meet. For
the most part, ruling out ill-formed output con-
cerns posing constraints on acceptable edges, es-
pecially when one takes into account that all we
have is some floating nodes to start with. Let us
first recall that dimension PA is all about predicate
arguments, which are necessarily variables thanks
to the flat nature of our input semantics. Roughly
speaking, each PA edge relates a predicate with
one of its arguments and thus ?is about? one sin-
gle variable. Therefore, our first concern must be
to ensure that every PA edge should land on a node
that ?is (also) about? the same variable as the edge
itself.
In order to provide for such an ?aboutness?
agreement, so to speak, one must first provide for
?aboutness? itself. Thus, we postulate that every
node should now have two new features, namely
(i) hook, identifying the referent of the node,
i.e. the variable it is about, and (ii) holes, map-
ping every PA edge label ` into the argument (a
variable) every possible `-labelled outgoing edge
should be about. Normally these features should
be lexicalized. The coincidence with Copestake et
al.?s terminology (Copestake et al, 2001) is not
casual; in fact, our formulation can be regarded as
a decoupled fragment of theirs, since neither our
holes involves syntactic labels nor are scopal is-
sues ever touched. As usual in XDG, we leave it
for other modules such as mentioned in the previ-
ous section to take charge of scope and the rela-
tionship between semantic arguments and syntac-
tic roles. The role of these new features is depicted
in Figure 3, in which an arrow does not mean an
edge but the possibility of establishing edges.
Completeness and compositionality. Next we
proceed to ensure completeness, i.e. that every so-
Figure 3: For every node v and on top of e.g. va-
lency constraints, features hook and holes further
constrain the set of nodes able to receive edges
from v for each specific edge label.
lution should convey the whole intended seman-
tic content. To this end, nodes must have fea-
tures holding semantic information, the most ba-
sic of which is bsem, standing for base seman-
tic content, or rather, the semantic contribution a
lexical entry may make on its own to the whole.
For example, ?woman? might be said to contribute
?x.female(x)?human(x), while ?female?, only
?x. female(x). Normally bsem should be lexi-
calized.
In addition, we postulate feature sem for hold-
ing the actual semantic content of nodes, which
should not be lexicalized, but rather calculated
by a principle imposing semantic composition-
ality. In our rather straightforward formulation,
for every node v, sem(v) is but the conjunction
of bsem(v) and the sems of all its PA daughters
thus:
sem(v)
=
bsem(v) ?? {sem(u) : v ??PA u} ,
(2)
where v ?`?D u denotes that node u is a daughter of
v on dimension D through an edge labelled ` (the
absence of the label just denotes that it does not
matter).
Finally, completeness is imposed by means of
node feature axiom, upon which holds the invari-
ant
sem(v) ? axiom(v) , (3)
for every node v. The idea is to have axiom as
a lexicalized feature and consistently assign it the
neutralizing constant true for all lexical entries
but those meant for the root node, in which case
the value should equal the intended semantic con-
tent.
Coreference classes, concentrators and revi-
sions to dimensions PA and DS. The unavoid-
45
able impediment to propagation is intrinsic choice,
i.e. that between things equivalent and that we
wish to remain so. That is exactly what we would
like to capture for lexicalization while attempting
to make the greatest amount of determinacy avail-
able to minimize failure. To this end, our strategy
is to make PA analyses as flat as possible, with
coreferent nodes ? i.e. having the same ref-
erent or hook ? organizing in plexuses around,
or rather, directly below hopefully one single node
per plexus, thus said to be a concentrator. This
offers advantages such as the following:
1. the number of leaf nodes is maximized,
whose sem features are determinate and
equals their respective bsems;
2. coreferent nodes tend to be either potential
sisters below a concentrator or deleted. This
allows most constraints to be stated in terms
of direct relationships of mother-, daughter-
or sisterhood. Such proximity and concen-
tration is rather opportune because we are
dealing simply with potential relationships as
nodes will usually be deleted. In other words,
our constraints aim mostly at ruling out un-
desired relations rather than establishing cor-
rect ones. The latter must remain a matter of
choice.
It is in order to define which are the best candi-
dates for concentrators. Having different concen-
trators in equivalent alternative realizations, such
as ?a ballerina?, ?a female dancer? or ?a danc-
ing woman? (hypothetical concentrators are un-
derlined), would be rather hampering, since the
task of assigning ?concentratingness? would then
be fatally bound to lexicalization disjunction it-
self and not much determinacy could possibly be
derived ahead of committing to this or that re-
alization. In face of that, the natural candidate
must be something that remains constant all along,
namely the article. Certainly, what specific arti-
cle and, among others, whether to generate a defi-
nite/anaphoric or indefinite/first-time referring ex-
pression is also a matter of choice, but not pertain-
ing to lexicalization. For the sake of simplicity and
scope, let us stick to the case of indefinite articles,
keeping in mind that possible extensions to our
model to cope with (especially definite anaphoric)
referring expression generation shall certainly re-
quire some revisions.
DS
a dancing female person ?
npd
modd modd
root
PA
a dancing female person ?
apply apply apply
root
Figure 4: new PA and DS analyses for ?a dancing
female person?. An asterisk stands for the root
node
Electing articles for concentrators means that
they now directly dominate their respective nouns
and accompanying modifiers on dimension PA as
shown in Figure 4 for ?a dancing female person?.
One new edge label apply is postulated to connect
concentrators with their complements, the follow-
ing invariants holding:
1. for every node v, hook(v) =
holes(v)(apply), i.e. only coreferent
nodes are linked by apply edges;
2. every concentrator lexical entry provides a
valency allowing any number of outgoing
apply edges, though requiring at least one.
Roughly speaking, the intuition behind this new
PA design is that the occurrence of a lexical (as
opposed to grammatical) word corresponds to the
evaluation of a lambda expression, resulting in
a fresh unary predicate built from the basesem
of the word/node and the sems of its children.
In turn, every apply edge denotes the applica-
tion of one such predicate to the variable/referent
of a concentrator. In fact, even verbs might be
treated analogously if Infl constituents were mod-
elled, constituting the concentrators of verb base
forms. Also useful is the intuition that PA abstracts
over most morphosyntactic oppositions, such as
that between nouns and adjectives, which figure
as equals there. The subordination of the latter
word class to the former becomes a strictly syntac-
tic phenomenon or, in any case, other dimensions?
affairs.
Dimension DS is all about such oppositions,
however, and should remain much the same ex-
cept that the design is rather simplified if DS main-
tains concentrator dominance. As a result, arti-
cles must stand as heads of noun ? or rather, de-
46
Figure 5: Starting conditions with perfectly float-
ing nodes in the lexicalization of ?a ballerina? and
its paraphrases
terminer ? phrases, which is not an unheard-of
approach, just unprecedented in XDG. Naturally,
standard syntactic structures should appear below
determiners, as exemplified in Figure 4. Granted
this, the flatness of PA and its relation to DS can
straightforwardly be accomplished by the applica-
tion of XDK library principles Climbing, whereby
PA is constrained to be a flattening of DS, and Bar-
riers, whereby concentrators are set as obstacles to
climbing by means of special lexical features. Fig-
ure 5 thus illustrates the starting conditions for the
lexicalization of ?a ballerina? and its paraphrases,
including the bsems of nodes. Notice that we have
created distinct nodes for different parts of speech
of one same word, ?female?. The relevance of this
measure shall be clarified along this section as we
develop this example.
Fighting over-redundancy. We currently em-
ploy two constraints to avoid over-redundancy.
The first is complete in that its declarative seman-
tics already sums up all we desire to express in that
matter, while the other is redundant, incomplete,
but supplied to improve propagation.
The complete constraint is imposed between
every node and each of its potential daughters.
Apart from overhead reasons, it might as well be
imposed between every pair of nodes. However,
the set of potential daughters of a node v is best
approximated by function dcands thus:
dcands(v)
=
(? {?x? : x ? ran(holes(v))})? {v} ,
where ?x? denotes the coreference class of vari-
able x; and ran(f), the range of function f . It is
worth noticing that in generation dcands is known
at model creation.
Given a node u and a potential daughter v ?
dcands(u), this constraint involves hypothesiz-
ing what the actual semantic content of u would
be like if v were not among its daughters.
Let hdsv(u) and hsemv(u) be respectively the
hypothetical set of daughters of u counting v out
and its ?actual? semantic content in that case,
which can be defined thus:
hdsv(u) = {w : u ??PA w} ? {v}
and
hsemv(u) = bsem(u)?
?? {sem(w) : w ? hdsv(u)} . (4)
The constraint consists of ensuring that, if the ac-
tual semantic content of the potential daughter v
would be subsumed by the hypothetical semantic
content of u, then v can never be a daughter of u.
In other words, each daughter of u must make a
difference. Formally, we have the following:
(hsemv(u) ? sem(v)) ? ? (u ??PA v) (5)
where the two implication symbols, ? and ?
have the same interpretation in this logic state-
ment, but are nonetheless distinguished because
their implementations are radically different as
shall be discussed in Section 4. Constraint (5)
is especially active after some choices have been
made. Suppose, in our ?a ballerina? example,
that ?dancing? is the only word selected so far
for lexicalization. Let u and v be respectively
the nodes for ?a? and ?dancing?. In this case,
the consequent in (5) is false and so must be the
antecedent hsemv(u) ? dance(x), which im-
plies that hsemv(u) can never ?contain? the literal
dance(x). From (4) and the fact that articles have
neutral base semantics ? i.e. bsem(u) = true ?
it follows that all further daughters of u must not
imply dance(x). As that does not hold for ?bal-
lerina? and ?dancer?, these nodes are ruled out as
daughters of u and thus deleted for lack of moth-
ers. Conversely, if ?ballerina? had been selected
47
in the first place, (5) would trivially detect the re-
dundancy of all other words and analogously en-
tail their deletion.
In turn, the redundant constraint ensures that,
for every pair of coreferent nodes u and v ?
?upvar(u)?, if the actual semantic content of v
is subsumed by u, then they can never be sisters.
Formally:
(sem(u) ? sem(v)) ?
(v /? sistersPA(u)) . (6)
This constraint is remarkable for being active
even in the absence of choice since it is established
between potential sisters, which usually have their
sems sufficiently, if not completely, determined.
Surprisingly enough, the main effect of (6) is on
syntax, by constraining alliances on DS. As our
new version of the XDK?s Climbing principle is
now aware of sisterhood constraints, it will con-
strain every node on PA to have as a mother on
DS either its current PA mother or some node be-
longing to one of its PA sister trees3. In ground
terms, when (6) detects that e.g. ?woman? sub-
sumes ?female (adj./n.)? and constrains them not
to be sisters on PA, the Climbing principle will
rule out ?woman? as a potential DS mother of
?female (adj.)?. It is worth mentioning that once
v /? sistersD(u) is imposed, our sisterhood con-
straints entail u /? sistersD(v).
Redundant compositionality constraints. Al-
though a complete statement of semantic compo-
sitionality is given by Equation 2, we introduce
two redundant constraints to improve propagation.
The first of them attempts to advance detection of
nodes whose semantic contribution is strictly re-
quired even before the sem features of their moth-
ers become sufficiently constrained. It does so
by means of an strategy analogous to that of (5),
namely by hypothesizing, for every node v, what
the total semantic content would be like if v were
deleted. Let root, hdownv(u) and htotsemv be
respectively the root node, the set of nodes directly
or indirectly below u counting v out, and the to-
tal semantic content supposing v is deleted, which
can be defined thus:
hdownv(u) = downPA(u)? {v}
and
htotsemv =
?
{sem(u) : u ? hdownv(root)} .
3If the subgraphs option is active, which is the case here.
The constraint can be formally expressed thus:
deleted(v) ? (htotsemv ? sem(v)) . (7)
Unfortunately, (7) is not of much use in our current
example, better applying to cases where there are a
greater number of alternative inner nodes. For ex-
ample, in the lexicalization of (1), this constraint
was immediately able to infer that ?lovely? must
not be deleted since it was the sole node contribut-
ing lovely(y).
The second redundant compositionality con-
straint attempts to advance detection of nodes not
counting on enough potential sisters to fulfill the
actual semantic content of their mothers. To this
end, for every node v, the following constraint is
imposed:
? {sem(u) : u ??PA v}
=?
?
? {bsem(u) : u ??PA v}
?? {sem(u) : u ? eqsisPA(v)}
?
? ,
(8)
where
eqsisD(v) =
{ ?, iff v is deleted on D
sistersD(v) ? {v} , else. (9)
which reads ?the actual semantic content of the
mothers of a node is equal to their base seman-
tic content in conjunction with the actual se-
mantic content of this node and its sisters?. It
is worth noticing that, when v is deleted, both
{u : u ??PA v} and eqsisPA(v) become empty so
that (8) still holds. This constraint is especially
interesting because our new versions of principles
Climbing and Barriers, which hold between DS
and PA, propagate sisters constraints in both di-
rections. In association with (6) and (8), these
principles promote an interesting interplay be-
tween syntax and semantics. Resuming our ex-
ample, let v be node ?female (n.)?. Before any
selection is performed, constraint (6) infers that
only ?dancing?, ?person? and ?dancer? can be sis-
ters to v on PA and thus (now due to Climbing)
daughters to v on DS. They cannot be mothers
to v because its valency on DS and Climbing are
enough to establish that, if v has any mother at
all on DS, it is ?a?. Again taking the DS valency
of v into account, it is possible to infer that, if v
has any daughter at all on DS, it is ?dancing?, i.e.
the only adjective in the original set of candidate
48
daughters. It is the new sisterhood-aware version
of Barriers that propagates this new piece of in-
formation back to PA. This principle now knows
that the sisters of v on PA must come from ei-
ther (i) the tree below v on DS, (ii) one of its DS
sister trees or (iii) some DS tree whose root be-
longs to eqsisDS(inter) for some node inter ap-
pearing ? on DS ? between v and one of its
mothers on PA. In our example, (ii) and (iii) are
known to be empty sets, while (i) is at most ?danc-
ing?. Consequently, ?dancing? is the only poten-
tial PA sister of v. Now (8) is finally able to con-
tribute. As ?a? is the only possible DS mother
of v and any article has empty basic semantics,
one is entitled to equate ? {sem(u) : u ??PA v} to? {sem(u) : u ? eqsisPA(v)}. Even though it is
not known whether v will ever have mothers or
daughters, (8) knows that the left-hand side of the
equation yields either the whole intended seman-
tics or nothing, while the right-hand side yields ei-
ther nothing or at most dance(x) ? female(x) .
Therefore, the only solution to the equation is
nothing on both sides, implying that eqsisPA(v)
is empty and thus v is deleted by definition (9).
Such strong interplay is only possible because
we have created distinct nodes for the different
parts of speech ? or rather, the two different DS
valencies ? of ?female?. With somewhat more
complicated, heavier constraints it would be pos-
sible to have the same propagation for one sin-
gle node selecting from different parts of speech.
Notwithstanding, that does not seem worth the ef-
fort because a model creation algorithm would be
perfectly able to detect the diverging DS valencies,
create as many nodes as needed and distribute the
right lexical entries among them.
4 Implementation and Performance
Remarks
The ideas presented in Section 3 were fully im-
plemented in a development branch of the XDK.
As with the original XDK, all development is
based on the multiparadigm programming system
Mozart4.
The implementation closely follows the original
CP approach of the XDK and strongly reflects the
constraints we have presented after some rather
standard transformations to CP, namely:
? variable identifiers in hooks and holes, as
well as all semantic input literals such as
4http://www.mozart-oz.org
human(x) and tap(e, x, y), are encoded as
integer values. Features bsem/sem are im-
plemented as set constants/variables of such
integers;
? logic conjunction ? is thus modelled by set
union ?. Each ?big? conjunction is re-
duced to the form ? {f(v) : v ? V }, where
V is a set variable of integer-encoded node
identifiers, and modelled by a union selec-
tion constraint ? ?f(1) f(2) ... f(M)? [V ],
where M is the maximum node identifier and
which constrains its result ? a set variable
? to be the union of f(v) for all v ? V ;
? implications of the form x ? y are imple-
mented as y ? x, while those of the form
x ? y as reify(x) ? reify(y) , where
the result of reify(x) is an integer-encoded
boolean variable constrained to coincide with
the truth-value of expression x.
Our branch of the XDK now counts on two new
principles, namely (i) Delete, which requires the
Graph principle, creates doubles for the node at-
tributes introduced by the latter, providing the il-
lusion of deletion, and introduces features for sis-
terhood constraints; and (ii) Compsem, imposing
all constraints described in Section 3.
A few preliminary proof-of-concept experi-
ments were carried out with input similar to (1)
and linguistically and combinatorially analogous
to our ?ballerina? example. In all of them, the sys-
tem was able to generate all paraphrases with no
failed state (backtracking) in search, which means
that propagation was maximal for all cases. Al-
though our design supports more complex linguis-
tic constructs such as relative clauses and prepo-
sition phrases and is expected to behave similarly
for those cases, we have not made any such exper-
iments so far. This is so because we are currently
prioritizing the issue of model creation and cover-
age of other generation tasks.
5 Conclusions and Future Work
In this paper we have presented the results of the
very first steps towards the application of XDG to
Natural Language Generation, hopefully in an in-
tegrated architecture. Our main contribution and
focus was a formulation of lexicalization disjunc-
tion in XDG terms, preserving the good properties
of modularity and extensibility while achieving
49
good propagation. We also hope to have demon-
strated how strong the interplay between linguis-
tic dimensions can be in XDG. As basic issues
as the very nature of input were discussed also as
an evidence that there is still a long way to go.
We are currently working on extending our design
to cover other generation tasks than lexicalization
and perform model creation.
Acknowledgements
We would like to thank Claire Gardent and Denys
Duchier, for all their invaluable insights and com-
ments, and group Langue & Dialogue and Brazil-
ian agencies CNPq and CAPES, for funding this
research project and travel to COLING/ACL.
References
Copestake, A. A., Lascarides, A., and Flickinger, D.
(2001). An algebra for semantic construction in
constraint-based grammars. In Meeting of the As-
sociation for Computational Linguistics, pages 132?
139.
Debusmann, R. (2004). Multiword expressions as de-
pendency subgraphs. In Proceedings of the ACL
2004 Workshop on Multiword Expressions: Integrat-
ing Processing, Barcelona/ESP.
Debusmann, R., Duchier, D., Koller, A., Kuhlmann,
M., Smolka, G., and Thater, S. (2004a). A rela-
tional syntax-semantics interface based on depen-
dency grammar. In Proceedings of the COLING
2004 Conference, Geneva/SUI.
Debusmann, R., Duchier, D., and Kruijff, G.-J. M.
(2004b). Extensible dependency grammar: A new
methodology. In Proceedings of the COLING
2004 Workshop on Recent Advances in Dependency
Grammar, Geneva/SUI.
Debusmann, R., Duchier, D., and Niehren, J. (2004c).
The xdg grammar development kit. In Proceedings
of the MOZ04 Conference, volume 3389 of Lec-
ture Notes in Computer Science, pages 190?201,
Charleroi/BEL. Springer.
Debusmann, R., Duchier, D., and Rossberg, A. (2005).
Modular Grammar Design with Typed Parametric
Principles. In Proceedings of FG-MOL 2005, Ed-
inburgh/UK.
Debusmann, R. and Smolka, G. (2006). Multi-
dimensional dependency grammar as multigraph de-
scription. In Proceedings of FLAIRS-19, Melbourne
Beach/US. AAAI.
Reiter, E. and Dale, R. (2000). Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
Van Roy, P. and Haridi, S. (2004). Concepts, Tech-
niques, and Models of Computer Programming.
MIT Press.
50
Multilingual versus Monolingual WSD 
Lucia Specia 
ICMC ? University of S?o Paulo 
Av. do Trabalhador S?o-Carlense, 400 
S?o Carlos, 13560-970, Brazil 
lspecia@icmc.usp.br
Maria das Gra?asVolpe Nunes 
ICMC ? University of S?o Paulo 
Av. do Trabalhador S?o-Carlense, 400 
S?o Carlos, 13560-970, Brazil 
gracan@icmc.usp.br
Mark Stevenson 
Computer Science ? University of Sheffield 
Regent Court, 211 Portobello Street 
Sheffield, S1 4DP, UK  
M.Stevenson@dcs.shef.ac.uk  
Gabriela Castelo Branco Ribeiro 
DL - Pontificial Catholic University - Rio 
R. Marqu?s de S?o Vicente, 225 - G?vea 
Rio de Janeiro, RJ, Brazil. CEP: 22.453-900 
gabrielacastelo@globo.com
  
Abstract 
Although it is generally agreed that Word 
Sense Disambiguation (WSD) is an ap-
plication dependent task, the great major-
ity of the efforts has aimed at the devel-
opment of WSD systems without consid-
ering their application. We argue that this 
strategy is not appropriate, since some 
aspects, such as the sense repository and 
the disambiguation process itself, vary 
according to the application. Taking Ma-
chine Translation (MT) as application 
and focusing on the sense repository, we 
present evidence for this argument by ex-
amining WSD in English-Portuguese MT 
of eight sample verbs. By showing that 
the traditional monolingual WSD strate-
gies are not suitable for multilingual ap-
plications, we intend to motivate the de-
velopment of WSD methods for particu-
lar applications.  
1 Introduction 
Word Sense Disambiguation (WSD) is con-
cerned with the choice of the most appropriate 
sense of an ambiguous word given its context. 
The applications for which WSD has been 
thought to be helpful include Information Re-
trieval, Information Extraction, and Machine 
Translation (MT) (Ide and Ver?nis, 1998). The 
usefulness of WSD for MT, particularly, has 
been recently subject of debate, with conflicting 
results. Vickrey et al (2005), e.g., show that the 
inclusion of a WSD module significantly im-
proves the performance of their statistical MT 
system. Conversely, Carpuat and Wu (2005) 
found that WSD does not yield significantly bet-
ter translation quality than a statistical MT sys-
tem alone. In this latter work, however, the WSD 
module was not specifically designed for MT: it 
is based on the use of monolingual methods to 
identify the source language senses, which are 
then mapped into the target language transla-
tions. 
In fact, although it has been agreed that WSD 
is more useful when it is meant for a specific ap-
plication (Wilks and Stevenson, 1998; Kilgarriff, 
1997; Resnik and Yarowsky, 1997), little has 
been done on the development of WSD modules 
specifically for particular applications. WSD 
models in general are application independent, 
and focus on monolingual contexts, particularly 
English. 
Approaches to WSD as an application-
independent task usually apply standardised 
sense repositories, such as WordNet (Miller, 
1990). For multilingual applications, a popular 
approach is to carry out monolingual WSD and 
then map the source language senses into the cor-
responding target word translations (Carpuat and 
Wu, 2005; Montoyo et al, 2002). Although this 
strategy can yield reasonable results for certain 
pairs of languages, especially those which have a 
common sense repository, such as EuroWordNet 
(Vossen, 1998), mapping senses between lan-
guages is a very complex issue (cf. Section 2).  
33
We believe that WSD is an intermediate, applica-
tion dependent task, and thus WSD modules for 
particular applications must be developed fol-
lowing the requirements of such applications. 
Many key factors of the process are application-
dependent. The main factor is the sense inven-
tory. As emphasized by Kilgarriff (1997), no 
sense inventory is suitable for all applications. 
Even for the same application there is often little 
consensus about the most appropriate sense in-
ventory. For example, the use of WordNet, al-
though very frequent, has been criticized due to 
characteristics such as the level sense granularity 
and the abstract criteria used for the sense dis-
tinctions in that resource (e.g., Palmer 1998). In 
particular, it is generally agreed that the granular-
ity in WordNet is too refined for MT.  
In addition to requiring different sense inven-
tories (Hutchins and Somers, 1992), the disam-
biguation process itself often can be varied ac-
cording to the application. For instance, in mono-
lingual WSD, the main information source is the 
context of the ambiguous word, that is, the sur-
rounding words in a sentence or paragraph. For 
MT purposes, the context can be also that of the 
translation in the target language, i.e., words 
which have been already translated.  
In this paper we focus on the differences in the 
sense inventory, contrasting the WordNet inven-
tory for English disambiguation, which was cre-
ated according to psycholinguistics principles, 
with the Portuguese translations assigned to a set 
of eight verbs in a corpus, simulating MT as a 
Computational Linguistics application.  
We show that the relation between the number 
of senses and translations is not a one-to-one, 
and that it is not only a matter of the level of re-
finement of WordNet. The number of transla-
tions can be either smaller or larger, i.e., either 
two or more senses can be translated as the same 
word, or the same sense can be translated using 
different words. With that, we present evidence 
that employing a monolingual WSD method for 
the task of MT is not appropriate, since monolin-
gual information offers little help to multilingual 
disambiguation. In other words, we argue that 
multilingual WSD is different from monolingual 
WSD, and thus requires specific strategies. We 
start by presenting approaches that show cognate 
results for different pairs of languages, and also 
approaches developed with the reverse goal of 
using multilingual information to help monolin-
gual WSD (Section 2). We then present our ex-
periments (Sections 3 and 4) and their results 
(Section 5).  
2 Related work  
Recently, others have also investigated the dif-
ferences between sense repositories for monolin-
gual and multilingual WSD. Chatterjee et al 
(2005), e.g., investigated the ambiguity in the 
translation of the English verb ?to have? into 
Hindi. 11 translation patterns were identified for 
the 19 senses of the verb, according to the vari-
ous target syntactic structures and/or target 
words for the verb. They argued that differences 
in both these aspects do not depend only on the 
sense of the verb. Out of the 14 senses analyzed, 
six had 2-5 different translations each.  
Bentivogli et al (2004) proposed an approach 
to create an Italian sense tagged corpus (Mul-
tiSemCor) based on the transference of the anno-
tations from the English sense tagged corpus 
SemCor (Miller et al, 1994), by means of word-
alignment methods. A gold standard corpus was 
created by manually transferring senses in Sem-
Cor to the Italian words in a translated version of 
that corpus. From a total of 1,054 English words, 
155 annotations were considered non-
transferable to their corresponding Italian words, 
mainly due to the lack of synonymy at the lexical 
level.  
Mih?ltz (2005) manually mapped senses from 
the English in a sense tagged corpus to Hungar-
ian translations, in order to carry out WSD be-
tween these languages. Out of 43 ambiguous 
nouns, 38 had all or most of their English senses 
mapped into the same Hungarian translation. 
Some senses of the remaining nouns had to be 
split into different Hungarian translations. On 
average, the sense mapping decreased the ambi-
guity from 3.97 English senses to 2.49 Hungar-
ian translations. 
As we intend to show with this work, differ-
ences like those mentioned above in the sense 
inventories make it inappropriate to use mono-
lingual WSD strategies for multilingual disam-
biguation. Nevertheless, some approaches have 
successfully employed multilingual information, 
especially parallel corpora, to support monolin-
gual WSD. They are motivated by the argument 
that the senses of a word should be determined 
based on the distinctions that are lexicalized in a 
second language (Resnik and Yarowsky, 1997). 
In general, the assumptions behind these ap-
proaches are the following:  
(1) If a source language word is translated dif-
ferently into a second language, it might be am-
biguous and the different translations can indi-
cate the senses in the source language.  
34
(2) If two distinct source language words are 
translated as the same word into a second lan-
guage, it often indicates that the two are being 
used with similar senses.  
Ide (1999), for example, analyzes translations 
of English words into four different languages, in 
order to check if the different senses of an Eng-
lish word are lexicalized by different words in all 
the other languages. A parallel aligned corpus is 
used and the translated senses are mapped into 
WordNet senses. She uses this information to 
determine a set of monolingual sense distinctions 
that is potentially useful for NLP applications. In 
subsequent work (Ide et al, 2002), seven lan-
guages and clustering techniques are employed 
to create sense groups based on the translations.  
Diab and Resnik (2002) use multilingual in-
formation to create an English sense tagged cor-
pus to train a monolingual WSD approach. An 
English sense inventory and a parallel corpus 
automatically produced by an MT system are 
employed. Sentence and word alignment systems 
are used to assign the word correspondences be-
tween the two languages. After grouping all the 
words that correspond to translations of a single 
word in the target language, all their possible 
senses are considered as candidates. The sense 
that maximizes the semantic similarity of the 
word with the others in the group is chosen.  
Similarly, Ng et al (2003) employ English-
Chinese parallel word aligned corpora to identify 
a repository of senses for English. The English 
word senses are manually defined, based on the 
WordNet senses, and then revised in the light of 
the Chinese translations. For example, if two oc-
currences of a word with two different senses in 
WordNet are translated into the same Chinese 
word, they will be considered to have the same 
English sense.  
In general, these approaches rely on the two 
previously mentioned assumptions about the in-
teraction between translations and word senses. 
Although these assumptions can be useful when 
using cross-language information as an approxi-
mation to monolingual disambiguation, they are 
not very helpful in the opposite direction, i.e., 
using monolingual information for cross-
language disambiguation, as we will show in 
Section 4.  
3 Experimental setting  
We focused our experiments on verbs, which 
represent difficult cases for WSD. In particular, 
we experimented with five frequent and highly 
ambiguous verbs identified as problematic for 
MT systems in a previous study (Specia, 2005): 
?to come?, ?to get?, ?to give?, ?to look?, and ?to 
make?; and other three frequent verbs that are 
not so ambiguous: ?to ask?, ?to live?, and ?to 
tell?. The inclusion of the additional verbs allows 
us to analyze the effect of the ambiguity level in 
the experiment. These verbs will then be trans-
lated into Portuguese so that the resulting transla-
tions can be contrasted to the English senses. 
3.1 Corpus selection 
We collected all the sentences containing one of 
the eight verbs and their corresponding phrasal 
verbs from SemCor, Senseval-2 and Senseval-3 
corpora1. These corpora were chosen because 
they are both widely used and easily available. In 
each of these corpora, ambiguous words are an-
notated with WordNet 2.0 senses. Occurrences 
which did not identify a unique sense were not 
used. The numbers of sentences selected for each 
verb and its phrasal verbs are shown in Table 1. 
Verb # Verb  
Occurrences
# Phrasal Verb 
Occurrences 
ask 414 8
come 674 330
get 683 267
give 740 79
live 242 5 
look 370 213 
make 1463 105 
tell 509 3 
Table 1. Number of verbs and phrasal verbs ex-
tracted from SemCor and Senseval corpora 
It is worth mentioning that the phrasal verbs in-
clude simple verb-particle constructions, such as 
?give up?, and more complex multi-word expres-
sions, e.g., ?get in touch with?, ?make up for?, 
?come to mind?, etc. 
In order to avoid biasing the experiment due to 
possible misunderstandings of the verb uses, and 
to make the experiment feasible, with a reason-
able number of occurrences to be analyzed, we 
selected a subset of the total number of sentences 
in Table 1, which were distributed among five 
professional English-Portuguese translators (T1, 
T2, T3, T4, T5), according to the following crite-
ria:  
- The meaning of the verb/phrasal verb in the 
context of the sentence should be understandable 
and non-ambiguous (for human translators). 
                                                
1
 Available at http://www.cs.unt.edu/~rada/downloads.html. 
35
- The experiment should be the most compre-
hensive possible, with the largest possible num-
ber of senses for each verb/phrasal. 
- Each translator should be given two occur-
rences (when available) of all the distinct senses 
of each verb/phrasal verb, in order to make it 
possible to contrast different uses of the verb.  
- The translators should not be given any in-
formation other than the sentence to select the 
translation.  
To meet these criteria, a professional translator, 
who was not involved in the translation task, 
post-processed the selected sentences, filtering 
them according to the criteria specified above. 
Due to both the scarce number of occurrences of 
each phrasal verb sense and the large number of 
different phrasal verbs for certain verbs, the post-
selection of phrasal verbs was different from the 
post-selection of verbs. In the case of verbs, the 
translator scanned the sentences in order to get 
10 distinct occurrences of each sense (two for 
each translator), eliminating those sentences 
which were too complex to understand or used 
the verb in an ambiguous way. This process did 
not eliminate any senses, and thus did not reduce 
the coverage of the experiment. When there were 
fewer than 10 occurrences of a given sense, sen-
tences were repeated among translators to guar-
antee that each translator would be given exam-
ples of all the senses of the verb. For instance, if 
a sense had only four occurrences, the first two 
occurrences were given to T1, T3 and T5, while 
the other two occurrences were given to T2 and 
T4. If a sense occurred only once for a verb, it 
was repeated for all five translators. 
For phrasal verbs, the same process was used 
to eliminate the complex and ambiguous sen-
tences. Two occurrences (when available) of 
each sense of a phrasal verb were then selected. 
Due to the large number of different phrasal 
verbs for certain verbs, they were divided among 
translators, so that each translator was given two 
occurrences of only some phrasal verbs of each 
verb. Sentences were distributed so that all trans-
lators had a similar number of cases, as shown in 
Table 2. 
In order to avoid biasing the translations ac-
cording to the English senses, the original sense 
annotations were not shown to the translators and 
the sentences for each of the verbs, together with 
their phrasal verbs, were randomly ordered. 
Additionally, we gave the same set of selected 
sentences to another group of five translators, so 
that we could analyze the reliability of the ex-
periment by investigating the agreement between 
the groups of translators on the same data. 
Translator
Verb 
# T1 # T2 # T3 # T4 # T5
ask 13 13 13 10 10
come 53 52 52 51 47
get 59 59 56 59 57
give 46 50 48 47 48
live 11 11 11 16 16
look 15 19 17 19 14
make 47 45 44 46 41
tell 14 12 12 15 10
Total  258 261 253 263 243
Table 2. Number of selected sentences and its 
distribution among the five translators 
3.2 English senses and Portuguese transla-
tions 
As mentioned above, the corpora used are tagged 
with WordNet senses. Although this may not be 
the optimal sense inventory for many purposes, it 
is the best option in terms of availability and 
comprehensiveness. Moreover, it is the most fre-
quently used repository for monolingual WSD 
systems, making it possible to generalize, to a 
certain level, our results to most of the monolin-
gual work. The number of senses for the eight 
selected verbs (and their phrasal verbs) in 
WordNet 2.0, along with the number of their 
possible translations in bilingual dictionaries2, is 
shown in Table 3. 
Verb # Senses # Translations 
ask  12 16
come 108 226
get  147 242
give  92 128
live  15 15
look  34 63
make 96 239
tell  12 28
Table 3. Verbs, possible senses and translations 
As we can see, the number of possible transla-
tions is different from the number of possible 
senses, which already shows that there is not a 
one-to-one correspondence between senses and 
translations (although there is a high correlation 
between the number of senses and translations: 
Pearson?s Correlation = 0.955). In general, the 
number of possible translations is greater than 
                                                
2
 For example, DIC Pratico Michaelis?, version 5.1. 
36
the number of possible senses, in part because 
synonyms are considered as different transla-
tions. As we will show in Section 5 (Table 4), we 
eliminate the use of synonyms as possible trans-
lations. Moreover, we are dealing with a limited 
set of possible senses, provided by the SemCor 
and Senseval data. As a consequence, the num-
ber of translations pointed out by the human 
translators for our corpus will be considerably 
smaller than the total number of possible transla-
tions. 
4 Contrasting senses and translations 
In order to contrast the English senses with the 
Portuguese translations, we submitted the se-
lected sentences (cf. Section 3.1) to two groups 
of five translators (T1, T2, T3, T4, and T5), all 
native speakers of Portuguese. We asked the 
translators to assign the appropriate translation to 
each of the verb occurrences, which we would 
then compare to the original English senses. 
They were not told what their translations were 
going to be used for. 
The translators were provided with entire sen-
tences, but for practical reasons they were asked 
to translate only the verb and were allowed to 
use any bilingual resource to search for possible 
translations, if needed. They were asked to avoid 
considering synonyms as different translations.  
The following procedure was defined to ana-
lyze the results returned by the translators, for 
each verb and its phrasal verbs separately: 
1) We grouped all the occurrences of an Eng-
lish sense and looked at all the translations used 
by the translators in order to identify synonyms 
(in those specific uses), using a dictionary of 
Portuguese synonyms. Synonyms were consid-
ered as unique translations.  
2) We then analyzed the sentences which had 
been given to multiple translators of the same 
group (when there were not enough occurrences 
of certain senses, as mentioned in Section 3.1), in 
order to identify a single translation for the oc-
currence and eliminate redundancies. The trans-
lation chosen was the one pointed out by the ma-
jority of the translators. When it was not possible 
to elect only one translation, the n equally most 
used were kept, and thus the sentence was re-
peated n times. 
3) Finally, we examined the relation between 
senses and translations, focusing on two cases: 
(1) if a sense had only one or many translations; 
and (2) if a translation referred to only one or 
many senses, i.e., whether the sense was shared 
by many translations. We placed each sense into 
two of the following categories, explained be-
low: (a) or (b), mutually exclusive, representing 
the first case; and (c), (d) or (e), also mutually 
exclusive, representing the second case. 
(a) 1 sense  1 translation: all the occur-
rences of the same sense being translated as 
the same Portuguese word. For example, ?to 
ask?, in the sense of ?inquire, enquire?, is al-
ways translated as ?perguntar?. 
(b) 1 sense  n translations: different oc-
currences of the same sense being translated as 
different, non-synonyms, Portuguese words. 
For example, ?to look?, in the sense of ?per-
ceive with attention; direct one's gaze to-
wards? can be translated as ?olhar?, ?assistir?, 
and ?voltar-se?. 
(c) n senses  1 translation (ambiguous): 
Different senses of a word being translated as 
the same Portuguese word, which encom-
passes all the English senses. For example, 
?make?, in the sense of ?engage in?, ?create?, 
and ?give certain properties to something?, is 
translated as ?fazer?, which carries the three 
senses. 
(d) n senses  1 translation (non-
ambiguous): different senses of a word being 
translated using the same Portuguese word, 
which has only one sense. For example, ?take 
advantage? in both the senses of ?draw advan-
tages from? and ?make excessive use of?, be-
ing translated as ?aproveitar-se?.  
(e) n senses  n translations: different 
senses of a word being translated as different 
Portuguese words. For example, the ?move 
fast? and ?carry out a process or program? 
senses of the verb ?run? being translated re-
spectively as ?correr? and ?executar?. 
Items (a) and (e) represent cases where multilin-
gual ambiguity only reflects the monolingual 
one, that is, to all the occurrences of every sense 
of an English word corresponds a specific Portu-
guese translation. On the other hand, items (b), 
(c) and (d) provide evidence that multilingual 
ambiguity is different from monolingual ambigu-
ity. Item (b) means that different criteria are 
needed for the disambiguation, as ambiguity 
arises only during the translation, due to specific 
principles used to distinguish senses in Portu-
guese. Items (c) and (d) mean that disambigua-
tion is not necessary, as either the Portuguese 
37
translation is also ambiguous, embracing the 
same senses of the English word, or Portuguese 
has a less refined sense distinction. 
5 Results and discussion 
Table 4 presents the number of different sen-
tences analyzed for each of the verbs (after 
grouping and eliminating the repeated sen-
tences), the English (E) senses and (non-
synonyms) Portuguese (P) translations in our 
corpus, followed by the percentage of occur-
rences of each of the categories outlined in Sec-
tion 4 (a ? e) with respect to the number of 
senses (# Senses) for that verb. Items (c) and (d) 
were grouped, since for practical purposes it is 
not important to tell if the P word translating the 
various E senses encompasses one or many 
senses. For items (b) and (c&d) we also present 
the average of P translations per E sense ((b) av-
erage), and the average of E senses per P transla-
tion, respectively ((c&d) average).  
We divided the analysis of these results ac-
cording to our two cases (cf. Section 4): the first 
covers items (c&d) and (e) (light grey in Table 
4), while the second covers items (a) and (b) 
(dark grey in Table 4). 
1) Items (c), (d) and (e): n senses ? ? transla-
tion(s) 
The number of senses in the corpus is almost 
always greater than the number of translations, 
suggesting that the level of sense distinctions in 
WordNet can be too fine-grained for translation 
applications The numbers of senses and transla-
tions are in an opposite relation comparing to the 
one shown in Table 3, where the number of pos-
sible translations was larger than the number of 
possible senses. This shows that indeed many of 
the possible translations are synonyms.  
On average, the level of ambiguity decreased 
from 40.3 (possible senses) to 24.4 (possible 
translations), if the monolingual and multilingual 
ambiguity are compared in the corpus. If we con-
sider the five most ambiguous verbs, the level of 
ambiguity decreased from 58.8 to 35. For the 
other three less ambiguous verbs, the level of 
ambiguity decreased from 9.3 to 6.7.  
Column % (c&d) shows the percentage of 
senses, with respect to the total shown in the 
third column (# Senses), which share translations 
with other senses. A shared translation means 
that several senses of the verb have the same 
translation. (c&d) average indicates the average 
number of E senses per P translation, for those 
cases where translations are shared. For all verbs, 
on average translations cover more than two 
senses. The level of variation in the number of 
shared translations among senses is high, e.g., 
from 2 (translation = ?organizar?) to 27 (transla-
tion = ?dar?) for the verb ?to give?. Contrasting 
the percentage of senses that share translations, 
in % (c), with the percentages in % (d), which 
refers to the senses for which translations are not 
shared, we can see that the great majority of 
senses have translations in common with other 
senses, and thus the disambiguation among these 
senses would not be necessary in most of the 
cases. In fact, it could result in errors, since an 
incorrect sense could be chosen. 
2) Items (a) and (b): 1 sense ? ? translation(s) 
As previously mentioned, the differences in the 
sense inventory for monolingual and multilingual 
WSD are not only due to the fact that sense dis-
tinctions in WordNet are too refined. That would 
only indicate that using monolingual WSD for 
multilingual purposes implies unnecessary work. 
However, we consider that the most important 
problem is the one evidenced by item (b) in the 
sixth column in Table 4. For all the verbs except 
?to ask? (the least ambiguous), there were cases 
in which different occurrences of the same sense 
were translated into different, non-synonyms 
words. Although the proportion of senses with 
only one translation is greater, as shown by item 
(a) in the fifth column, the percentage of senses 
with more than one translation is impressive, 
especially for the five most ambiguous verbs. In 
face of this, the lack of disambiguation of a word 
during translation based on the fact that the word 
is not ambiguous in the source language can re-
sult in very serious translation errors when 
monolingual methods are employed for multilin-
gual WSD. Therefore, this also shows that, for 
these verbs, sense inventories that are specific to 
the translation between the pair of languages un-
der consideration would be more appropriate to 
achieve effective WSD. 
5.1 Agreement between translators 
In an attempt to quantify the agreement between 
the two groups of translators, we computed the 
Kappa coefficient for annotation tasks, as de-
fined by Carletta (1996). Kappa was calculated 
separately for our two areas of inquiry, i.e., cases 
(1) and (2) discussed in Section 5.  
In the experiment referring to case (1), groups 
were considered to agree about a sense of a verb 
if they both judged that the translation of such 
38
Verb # Sen-
tences
# Senses # Transla-
tions 
% (a)  % (b) (b) av-
erage 
%  
(c&d) 
(c&d) av-
erage 
% (e) 
ask 83 8 3 100 0 0 87.5 3.5 12.5 
come 202 68 42 62 38 3.1 73.2 6.3 26.8 
get 226 90 61 70 30 2.6 61.1 3.4 38.9 
give 241 57 12 48.7 51.3 3.3 84.2 6.3 15.8 
live 55 10 7 83.3 16.7 3.0 70 2.7 30 
look 82 26 18 63.2 36.8 2.4 84.6 2.7 15.4 
make 225 53 42 51.4 48.6 2.9 77.4 4.1 22.6 
tell 73 10 10 37.5 62.5 2.8 60 4.0 40 
Table 4. Results of the procedure contrasting senses and translations 
verb was or was not shared by other senses. For 
example, both groups agreed that the word 
?fazer? should be used to translate occurrences 
of many senses of the verb ?to make?, including 
?engage in?, ?give certain properties to some-
thing?, and ?make or cause to be or to become?. 
On the other hand, the groups disagreed about 
the sense ?go off or discharge? of the phrasal 
verb ?to go off?: the first group found that the 
translation of that sense, ?disparar?, did not refer 
to any other sense, while the second group used 
that word to translate also the sense ?be dis-
charged or activated? of the same phrasal verb.  
In the experiment with case (2), groups were 
considered to agree about a sense if they both 
judged that the sense had or had not more than 
one translation. For example, both groups agreed 
that the sense ?reach a state, relation, or condi-
tion? of the verb ?to come? should be translated 
by more than one Portuguese word, including 
?terminar?, ?vir?, and ?chegar?. They also 
agreed that the sense ?move toward, travel to-
ward something or somebody or approach some-
thing or somebody? of the same verb had only 
one translation, namely ?vir?.  
The average Kappa coefficient obtained was 
0.66 for item (1), and 0.65 for item (2). There is 
not a reference value for this particular annota-
tion task (translation annotation), but the levels 
of agreement pointed by Kappa here can be con-
sidered satisfactory. The agreement levels are 
close to the coefficient suggested by Carletta as 
indicative of a good agreement level for dis-
course annotation (0.67), and which has been 
adopted as a cutoff in Computational Linguistics. 
6 Conclusions and future work 
We presented experiments contrasting monolin-
gual and multilingual WSD. It was found that, in 
fact, monolingual and multilingual disambigua-
tion differ in many respects, particularly the 
sense repository, and therefore specific strategies 
could be more appropriate to achieve effective 
multilingual WSD. We investigated the differ-
ences in sense repositories considering English-
Portuguese translation, using a set of eight am-
biguous verbs collected from sentences in Sem-
Cor and Senseval corpora. The English sense 
tags given by WordNet were compared to the 
Portuguese translations assigned by two groups 
of five human translators.  
Results corroborate previous cognate work, 
showing that there is not a one-to-one mapping 
between the English senses and their translations 
(to Portuguese, in this study). In most of the 
cases, many different senses were translated into 
the same Portuguese word. In many other cases, 
different, non-synonymous, words were neces-
sary to translate occurrences of the same sense of 
the source language, showing that differences 
between monolingual and multilingual WSD are 
not only a matter of the highly refined sense dis-
tinction criterion adopted in WordNet. Therefore, 
these results reinforce our argument that apply-
ing monolingual methods for multilingual WSD 
can either imply unnecessary work, or result in 
disambiguation errors.  
As future work we plan to carry out further in-
vestigation of the differences between monolin-
gual and multilingual WSD contrasting the Eng-
lish senses and translations into other languages, 
and analyzing other grammatical categories, par-
ticularly nouns.  
References  
Bentivogli, L., Forner, P., and Pianta, E. (2004). 
Evaluating Cross-Language Annotation Trans-
fer in the MultiSemCor Corpus. COLING-
2004, Geneva, pp. 364-370. 
Carletta, J. (1996). Assessing agreement on clas-
sification tasks: the kappa statistic. Computa-
tional Linguistics, 22(2), pp. 249-254. 
Carpuat, M. and Wu, D. (2005). Word sense dis-
ambiguation vs. statistical machine translation. 
43rd ACL Meeting, Ann Arbor, pp. 387?394. 
39
 Chatterjee, N., Goyal, S., and Naithani, A. 
(2005). Pattern Ambiguity and its Resolution 
in English to Hindi Translation. RANLP-2005, 
Borovets, pp. 152-156. 
Diab, M. and Resnik, P. (2002). An Unsuper-
vised Method for Word Sense Tagging using 
Parallel Corpora. 40th ACL Meeting, Philadel-
phia. 
Hutchins, W.J. and Somers H.L. (1992) An In-
troduction to Machine Translation. Academic 
Press, Great Britain. 
Ide, N. and V?ronis, J. (1998). Word Sense Dis-
ambiguation: The State of the Art. Computa-
tional Linguistics, 24 (1).  
Ide, N. (1999). Parallel Translations as Sense 
Discriminators. SIGLEX99 Workshop: Stan-
dardizing Lexical Resources, Maryland, pp. 
52-61. 
Ide, N., Erjavec, T., and Tufi, D. (2002). Sense 
Discrimination with Parallel Corpora. ACL'02 
Workshop on Word Sense Disambiguation: 
Recent Successes and Future Directions, 
Philadelphia, pp. 54-60.  
Kilgarriff, A. (1997). I Don't Believe in Word 
Senses. Computers and the Humanities, 31 
(2):91-113. 
Mih?ltz, M. (2005). Towards A Hybrid Ap-
proach to Word-Sense Disambiguation in Ma-
chine Translation. RANLP-2005 Workshop: 
Modern Approaches in Translation Technolo-
gies, Borovets.  
Miller, G.A., Beckwith, R.T., Fellbaum, C.D., 
Gross, D., and Miller, K. (1990). WordNet: 
An On-line Lexical Database. International 
Journal of Lexicography, 3(4):235-244. 
Miller, G.A., Chorodow, M., Landes, S., Lea-
cock, C., and Thomas, R.G. (1994). Using a 
Semantic Concordancer for Sense Identifica-
tion. ARPA Human Language Technology
Workshop - ACL, Washington, pp. 240-243. 
Montoyo, A., Romero, R., Vazquez, S., Calle, 
M., and Soler, S. (2002). The Role of WSD 
for Multilingual Natural Language Applica-
tions.  TSD?2002, Czech Republic, pp. 41-48. 
Ng, H.T., Wang, B., and Chan, Y.S. (2003). Ex-
ploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. 41st ACL 
Meeting, Sapporo, pp. 455-462.  
Palmer, M. (1998). Are WordNet sense distinc-
tions appropriate for computational lexicons? 
Senseval, Siglex98, Brighton.  
Resnik, P. and Yarowsky, D. (1997). A Perspec-
tive on Word Sense Disambiguation Methods 
and their Evaluating. ACL-SIGLEX Workshop 
Tagging Texts with Lexical Semantics: Why, 
What and How?, Washington. 
Specia, L. (2005). A Hybrid Model for Word 
Sense Disambiguation in English-Portuguese 
Machine Translation. 8th CLUK, Manchester, 
pp. 71-78. 
Vickrey, D., Biewald, L., Teyssier, M., and 
Koller, D. (2005). Word-Sense Disambigua-
tion for Machine Translation. HLT/EMNLP, 
Vancouver.
Vossen, P. (1998). EuroWordNet: Building a 
Multilingual Database with WordNets for 
European Languages. The ELRA Newsletter, 
3(1). 
Wilks, Y. and Stevenson, M. (1998). The Gram-
mar of Sense: Using Part-of-speech Tags as a 
First Step in Semantic Disambiguation. Natu-
ral Language Engineering, 4(1):1-9.  
40
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 17?24,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Extractive Automatic Summarization: Does more linguistic knowledge 
make a difference? 
 
Daniel S. Leite1, Lucia H. M. Rino1, Thiago A. S. Pardo2, Maria das Gra?as V. Nunes2 
N?cleo Interinstitucional de Ling??stica Computacional (NILC) 
http://www.nilc.icmc.usp.br 
1Departamento de Computa??o, UFSCar  
CP 676, 13565-905 S?o Carlos - SP, Brazil 
2Instituto de Ci?ncias Matem?ticas e de Computa??o, Universidade de S?o Paulo 
CP 668, 13560-970 S?o Carlos - SP, Brazil 
{daniel_leite; lucia}@dc.ufscar.br , {taspardo,gracan}@icmc.usp.br 
 
 
Abstract 
In this article we address the usefulness of 
linguistic-independent methods in extrac-
tive Automatic Summarization, arguing 
that linguistic knowledge is not only useful, 
but may be necessary to improve the in-
formativeness of automatic extracts. An as-
sessment of four diverse AS methods on 
Brazilian Portuguese texts is presented to 
support our claim. One of them is Mihal-
cea?s TextRank; other two are modified 
versions of the former through the inclusion 
of varied linguistic features. Finally, the 
fourth method employs machine learning 
techniques, tackling more profound and 
language-dependent knowledge. 
1 Introduction 
Usually, automatic summarization involves produc-
ing a condensed version of a source text through 
selecting or generalizing its relevant content. As a 
result, either an extract or an abstract will be pro-
duced. An extract is produced by copying text seg-
ments and pasting them into the final text preserving 
the original order. An abstract instead is produced 
by selecting and restructuring information from the 
source text. The resulting structure is thus linguisti-
cally realized independently of the surface choices 
of the source text. This comprises, thus, a rewriting 
task. 
This article focuses solely on extracts of source 
texts written in Brazilian Portuguese. For extrac-
tive Automatic Summarization (AS), several meth-
ods have been suggested that are based upon 
statistics or data readily available in the source 
text. Word frequency (Luhn, 1958) and sentence 
position (Edmundson, 1969) methods are classic 
examples of that. Usually, extractive AS does not 
take into account linguistic and semantic knowl-
edge in order to be portable to distinct domains or 
languages (Mihalcea, 2005). Graph-based methods 
aim at the same and have been gaining a lot of in-
terest because they usually do not rely on any lin-
guistic resource and run pretty fast. Exemplars of 
those are LexRank (Erkan and Radev, 2004) and 
TextRank (Mihalcea and Tarau, 2004). In spite of 
their potentialities, we claim that there is a com-
promise in pursuing a language-free setting: how-
ever portable a system may be, it may also produce 
extracts that lack the degree of informativeness 
needed for use. Informativeness, in the current 
context, refers to the ability of an automatic sum-
marizer to produce summaries that convey most 
information of reference, or ideal, summaries. Our 
assessment thus aimed at verifying if parsimonious 
use of linguistic knowledge could improve extrac-
tive AS. 
We argue that the lack of linguistic knowledge 
in extractive AS can be the reason for weak per-
formance regarding informativeness. This argu-
ment follows from acknowledging that 
improvements on the scores usually obtained in 
that field have not been expressive lately. The most 
common metrics used to date, precision and recall, 
signal average results, suggesting that it is not 
enough to pursue completely language-free sys-
tems, no matter the current demands for portability 
in the global communication scenario. We focus 
here on TextRank, which can be used for summa-
17
rizing Brazilian Portuguese texts due to its lan-
guage independence. To show that linguistic 
knowledge does make a difference in extractive 
AS, we compared four automatic summarizers: 
TextRank itself, two other modified versions of 
that, and SuPor-2 (Leite and Rino, 2006). 
TextRank works in a completely unsupervised 
way. Our two variations, although still 
unsupervised, include diverse linguistic knowledge 
in the preprocessing phase. SuPor-2 is the only 
machine learning-based system amongst the four 
ones, and it was built to summarize texts in 
Brazilian Portuguese, although it may be 
customized to other languages. Unlike the others, it 
embeds more sophisticated decision features that 
rely on varied linguistic resources. Some of them 
correspond to full summarization methods by 
themselves: Lexical Chaining (Barzilay and 
Elhadad, 1997), Relationship Mapping (Salton et 
al., 1997), and Importance of Topics (Larocca Neto 
et al, 2000). This is its unique and distinguishing 
characteristic.   
In what follows we first review the different lev-
els of processing in extractive AS (Section 2), then 
we describe TextRank and its implementation to 
summarize Brazilian Portuguese texts (Section 3). 
Our suggested modifications of TextRank are pre-
sented in Section 4, whilst SuPor-2 is described in 
Section 5. Finally, we compare the results of the 
four automatic summarizers when running on Bra-
zilian Portuguese texts (Section 6), and make some 
remarks on linguistic independence for extractive 
AS in Section 7. 
2 A Review of Automatic Summarization 
Mani (2001) classifies AS methods based upon 
three levels of linguistic processing to summarize a 
text, namely: 
 
? Shallow level. At this level only features at the 
surface of the text are explored. For example, 
location (Edmunson, 1969), sentence length 
and presence of signaling phrases (e.g., Kupiec 
et al, 1995). Combined, such features may 
yield a salience function that drives selection 
of sentences of the source text to include in a 
summary. 
 ? Entity level. The aim here is to build an inter-
nal representation of the source text that con-
veys its entities and corresponding 
relationships. These amount to the information 
that allows identifying important text seg-
ments. Examples of such relations are word 
cooccurrence (e.g., Salton et al, 1997), syno-
nyms and antonyms (e.g., Barzilay and Elha-
dad, 1997), logical relations, such as 
concordance or contradiction, and syntactic 
relations. 
 ? Discourse level. At this level the whole struc-
ture of the source text is modeled, provided 
that its communicative goals can be grasped 
from the source text. The discourse structure is 
intended to help retrieving, e.g., the main top-
ics of the document (e.g, Barzilay and Elha-
dad, 1997; Larocca Neto et al, 2000) or its 
rhetorical structure (e.g., Marcu, 1999), in or-
der to provide the means for AS. 
 
In this work we mainly focus on the entity level. 
Special entities and their relations thus provide the 
means to identify important sentences for building 
an extract. In turn, there is a loss of independence 
from linguistic knowledge, when compared to shal-
lower approaches. Actually, apart from TextRank, 
the other systems described in this paper target en-
tity level methods, as we shall see shortly.  
3 The TextRank Method 
The unsupervised TextRank method (Mihalcea and 
Tarau, 2004) takes after Google?s PageRank (Brin 
and Page, 1998), a graph-based system that helps 
judge the relevance of a webpage through incoming 
and outgoing links. PageRank directed graphs repre-
sent webpages as nodes and their linking to other 
webpages as edges. A random walk model is thus 
applied to build a path between the nodes, in order 
to grade the importance of a webpage in the graph. 
Similarly to grading webpages through travers-
ing a graph, TextRank attempts to weight sentences 
of a text by building an undirected graph. Nodes are 
now sentences, and edges express their similarity 
degrees to other sentences in the text. Actually, the 
degree of similarity is based upon content overlap. 
As such, similarity degrees help assess the overall 
cohesive structure of a text. The more content over-
lap a sentence has with other sentences, the more 
important it is and more likely it is to be included in 
the extract.. Similarity is calculated through equa-
tion [1] (Mihalcea and Tarau, 2004), where Si and Sj 
are sentences and wk is a common token between 
18
them. The numerator is the sum of common words 
between Si and Sj. To reduce bias, normalization of 
the involved sentences length takes place, as shows 
the denominator. 
 
|)log(||)log(|
|}|{|),(
ji
jkikk
ji SS
SwSwwSSSim +
???=  [1] 
 
Once the graph and all similarity degrees are 
produced, sentence importance is calculated by the 
random walk algorithm shown in equation [2]. 
TR(Vi) signals sentence importance, d is an arbitrary 
parameter in the interval [0,1], and N is the number 
of sentences in the text. Parameter d integrates the 
probability of jumping from one vertex to another 
randomly chosen. Thus, it is responsible for random 
walking. This parameter is normally set to 0.85 (this 
value is also used in TextRank). 
 
? ?
-
= -
=
??
??
?
?
??
??
?
?
??+-=
1
0
1
0
),(
),()()1()(
N
j
N
k
kj
ji
ji
SSSim
SSSimVTRddVTR  [2] 
 
Initial TR similarity values are randomly set in 
the [0,1] interval. After successive calculations, 
those values converge to the targeted importance 
value. After calculating the importance of the verti-
ces, the sentences are sorted in reverse order and the 
top ones are selected to compose the extract. As 
usual, the number of sentences of the extract is de-
pendent upon a given compression rate. 
  Clearly, TextRank is not language dependent. 
For this reason Mihalcea (2005) could use it to 
evaluate AS on texts in Brazilian Portuguese, be-
sides reporting results on texts in English. She also 
explored distinct means of representing a text with-
out considering linguistic knowledge, emphasizing 
TextRank language and domain independence. She 
varies, e.g., the ways the graphs could be traversed 
using both directed and undirected graphs. Once a 
sentence is chosen to compose an extract, having 
undirected graphs makes possible, to look forward ? 
from the sentence to its outgoing edges (i.e., focus-
ing on the set of its following sentences in the text) 
? or to look backward, considering that sentence 
incoming edges and, thus, the set of its preceding 
sentences in the text. 
Another variation proposed by Mihalcea is to 
replace the PageRank algorithm (Equation [2]) by 
HITS (Kleinberg, 1999). This works quite simi-
larly to PageRank. However, instead of aggregat-
ing the scores for both incoming and outgoing 
links of a node in just one final score, it produces 
two independent scores. These are correspondingly 
named ?authority? and ?hub? scores. 
4 Improving TextRank through varia-
tions on linguistic information 
To improve the similarity scores between sen-
tences in TextRank we fed it with more linguistic 
knowledge, yielding its two modified versions. The 
first variation focused just upon basic preprocess-
ing; the second one, on the use of a thesaurus to 
calculate semantic similarity to promote AS deci-
sions. However, we did not modify the main ex-
tractive algorithm of TextRank: we kept the graph 
undirected and used PageRank as the score deter-
miner. Actually, we modified only the method of 
computing the edges weights. 
4.1 Using Basic Preprocessing Methods 
In applying Equation 1 for similarity scores, only 
exact matches between two words are allowed. 
Since in Brazilian Portuguese there are many mor-
phological and inflexional endings for most words, 
this process becomes troublesome: important 
matches may be ignored. To overcome that, we used 
a stemmer for Brazilian Portuguese (Caldas Jr. et 
al., 2001) based upon Porter?s algorithm (1980). We 
also removed stopwords from the source text, be-
cause they are not useful in determining similarity. 
The resulting version of TextRank is named hereaf-
ter ?TextRank+Stem+StopwordsRem?. 
4.2 Using a Thesaurus 
Our second TextRank variation involved plugging 
into the system a Brazilian Portuguese thesaurus 
(Dias-da-Silva et al, 2003). Our hypothesis here is 
that semantic similarity of the involved words is 
also important to improve the informativeness of 
the extracts under production. Thus, an extractive 
summarizer should consider not only word repeti-
tion in the source text, but also synonymy and an-
tonymy.  
Although plugging the thesaurus into the 
automatic summarizer did not imply changing its 
main method of calculating similarity, there were 
some obstacles to overcome concerning the follow-
ing:  
19
 
 
 
 
 
 Figure 1. SuPor-2 training phase 
 
  
Figure 2. SuPor-2 extraction phase 
 
a) Should we consider only synonyms or both 
synonyms and antonyms in addition to term 
repetition (reiteration)? 
 
b) How to acknowledge, and disentangle, se-
mantic similarity, when polissemy, for ex-
ample, is present? 
 
c) Once the proper relations have been 
determined, how should they be weighted? 
Just considering all thesaural relations to be 
equally important might not be the best ap-
proach. 
Concerning (a), synonyms, antonyms, and term 
repetition were all considered, as suggested by oth-
ers (e.g., Barzilay and Elhadad, 1997). We did not 
tackle (b) to choose the right sense of a word be-
cause of the lack of an effective disambiguation 
procedure for Brazilian Portuguese. Finally, in 
tackling (c) and, thus, grading the importance of 
the relations for sentence similarity, we adopted 
the same weights proposed by Barzilay and Elha-
dad (1997) in their lexical chaining method, which 
is discussed in more detail below. For both reitera-
tion and synonymy, they assume a score of 10 for 
the considered lexical chain; for antonymy, they 
suggest a score of 7. The resulting version of Tex-
tRank is named here ?TextRank+Thesaurus?. 
5 The SuPor-2 System 
SuPor-2 is an extractive summarizer built from 
scratch for Brazilian Portuguese. It embeds differ-
ent features in order to identify and extract relevant 
sentences of a source text. To configure SuPor-2 
for an adequate combination of such features we 
employ a machine learning approach. Figures 1 
and 2 depict the training and extraction phases, 
respectively. 
For training, machine learning is carried out by a 
Na?ve-Bayes classifier that employs Kernel meth-
ods for numeric feature handling, known as Flexi-
ble Bayes (John and Langley, 1995). This 
environment is provided by WEKA1 (Witten and 
Frank, 2005), which is used within SuPor-2 itself. 
The training corpus comprises both source texts 
and corresponding reference extracts. Every sen-
tence from a source text is represented in the train-
                                                        
1 Waikato Environment for Knowledge Analysis. Available at 
http://www.cs.waikato.ac.nz/ml/weka/ (December, 2006) 
20
ing dataset as a tuple of the considered features. 
Each tuple is labeled with its class, which signals if 
the sentence appears in a reference extract. The 
class label will be true if the sentence under focus 
matches a sentence of the reference extract and 
false otherwise. 
Once produced, the training dataset is used by 
the Bayesian classifier to depict the sentences that 
are candidates to compose the extract (Figure 2). In 
other words, the probability for the ?true? class is 
computed and the top-ranked sentences are se-
lected, until reaching the intended compression 
rate. 
When computing features, three full methods 
(M) and four corpus-based parameters (P) are con-
sidered. Both methods and parameters are mapped 
onto the feature space and are defined as follows: 
 
(M) Lexical Chaining (Barzilay and Elhadad, 
1997). This method computes the connectedness 
between words aiming at determining lexical 
chains in the source text. The stronger a lexical 
chain, the more important it is considered for ex-
traction. Both an ontological resource and Word-
Net (Miller et al, 1990) are used to identify 
different relations, such as synonymy or antonym, 
hypernymy or hyponymy, that intervene to com-
pute connectedness. The lexical chains are then 
used to produce three sets of sentences. To identify 
and extract sentences from those sets, three heuris-
tics are  made available, namely: (H1) selecting 
every sentence s of the source text based on each 
member m of every strong lexical chain of the text. 
In this case, s is the sentence that contains the first 
occurrence of m; (H2) this heuristics is similar to 
the former one, but instead of considering all the 
members of a strong lexical chain, it uses only the 
representative ones. A representative member is 
one whose frequency is greater than the average 
frequency of all words in the chain; (H3) a sen-
tence s is chosen by focusing only on representa-
tive lexical chains of every topic of the source text. 
In SuPor-2, the mapping of this method onto a 
nominal feature is accomplished by signaling 
which heuristics have recommended the sentence. 
Thus, features in the domain may range over the 
values {?None?, ?H1?, ?H2?, ?H3?, ?H1H2?, 
?H1H3?, ?H2H3?, ?H1H2H3?}. 
 
(M) Relationship Mapping (Salton et al, 
1997). This method performs similarly to the pre-
vious one and also to TextRank in that it builds up 
a graph interconnecting text segments. However, it 
considers paragraphs instead of sentences as verti-
ces. Hence, graph edges signal the connectiveness 
of the paragraphs of the source text. Similarity 
scores between two paragraphs are thus related to 
the degree of connectivity of the nodes. Similarly 
to Lexical Chaining, Salton et al also suggest three 
different ways of producing extracts. However, 
they now depend on the way the graph is traversed. 
The so-called dense or bushy path (P1), deep path 
(P2), and segmented path (P3) aim at tackling dis-
tinct textual problems that may damage the quality 
of the resulting extracts. The dense path considers 
that paragraphs are totally independent from each 
other, focusing on the top-ranked ones (i.e., the 
ones that are denser). As a result, it does not guar-
antee that an extract will be cohesive. The deep 
path is intended to overcome the former problem 
by choosing paragraphs that may be semantically 
inter-related. Its drawback is that only one topic, 
even one that is irrelevant, may be conveyed in the 
extract. Thus, it may lack proper coverage of the 
source text. Finally, the segmented path aims at 
overcoming the limitations of the former ones, ad-
dressing all the topics at once. Similarly to Lexical 
Chaining, features in the Relationship method 
range over the set {?None?,?P1?,?P2?,?P3?, ?P1P2?, 
?P1P3?, ?P2P3?, ?P1P2P3?}. 
 
(M) Importance of Topics (Larocca Neto et 
al., 2000). This method also aims at identifying the 
main topics of the source text, however through the 
TextTiling algorithm (Hearst, 1993). Once the top-
ics of the source text have been determined, the 
first step is to select sentences that better express 
the importance of each topic. The amount of sen-
tences, in this case, is proportional to the topic im-
portance. The second step is to determine the 
sentences that will actually be included in the ex-
tract. This is carried out by measuring their simi-
larity to their respective topic centroids (Larocca 
Neto et al, 2000). The method thus signals how 
relevant a sentence is to a given topic. In SuPor-2 
this method yields a numeric feature whose value 
conveys the harmonic mean between the sentence 
similarity to the centroid of the topic in which it 
appears and the importance of that topic.  
(P) Sentence Length (Kupiec et al, 1995). 
This parameter just signals the normalized count of 
words of a sentence. 
21
(P) Sentence Location (Edmundson, 1969). 
This parameter takes into account the position of a 
sentence in the text. It is valued, thus, in 
{?II?,?IM?,?IF?,?MI?,?MM?,?MF?,?FI?,?FM?,?FF?}. 
In this set the first letter of each label signals the 
position of the sentence within a paragraph (Initial, 
Medium, or Final). Similarly, the second letter sig-
nals the position of the paragraph within the text. 
(P) Occurrence of proper nouns (e.g., Kupiec 
et al, 1995). This parameter accounts for the num-
ber of proper nouns in a sentence.  
(P) Word Frequency (Luhn, 1958). This pa-
rameter mirrors the normalized sum of the word 
frequency in a sentence. 
SuPor-2 provides a flexible way of combining 
linguistic and non-linguistic features for extraction. 
There are profound differences from TextRank. 
First, it is clearly language-dependent. Also, its 
graph-based methods do not assign weights to their 
vertices in order to select sentences for extraction. 
Instead, they traverse a graph in very specific  and 
varied ways that mirror both linguistic interde-
pendencies and important connections between the 
nodes. 
6 Assessing the Four Systems 
To assess the degree of informativeness of the sys-
tems previously described, we adopt ROUGE2 (Lin 
and Hovy, 2003), whose recall rate mirrors the in-
formativeness degree of automatically generated 
extracts by correlating automatic summaries with 
ideal ones. 
The two modified versions of TextRank require 
linguistic knowledge but at a low cost. This is cer-
tainly due to varying only preprocessing, while the 
main decision procedure is kept unchanged and 
language-independent. Those three systems do not 
need training, one of the main arguments in favor 
of TextRank (Mihalcea and Tarau, 2004). In con-
trast, SuPor-2 relies on training and this is certainly 
one of its main bottlenecks. It also employs lin-
guistic knowledge for both preprocessing and ex-
traction, which TextRank purposefully avoids. 
However, using WEKA has made its adjustments 
less demanding and more consistent, indicating 
that scaling up the system is feasible.  
                                                        
2 Recall-Oriented Understudy for Gisting Evaluation. Avail-
able at http://haydn.isi.edu/ROUGE/ (January, 2007). 
In our assessment, the same single-document 
summarization scenario posed by Mihalcea (2005) 
was adopted, namely: (a) we considered the Brazil-
ian Portuguese TeM?rio corpus (Pardo and Rino, 
2003); (b) we used the same baseline, which se-
lects top-first sentences to include in the extract; 
(c) we adopted a 70-75% compression rate, making 
it compatible with the compression rate of the ref-
erence summaries; and (d) ROUGE was used for 
evaluation in its Ngram(1,1) 95% confidence rate 
setting, without stopwords removal. TeM?rio com-
prises 100 newspaper articles from online Brazilian 
newswire. A set of corresponding manual summa-
ries produced by an expert in Brazilian Portuguese 
is also included in TeM?rio. These are our refer-
ence summaries. 
For training and testing SuPor-2, we avoided 
building an additional training corpus by using a 
10-fold cross-validation procedure. Finally, we 
produced three sets of extracts using ?TextRank +  
Stem + StopwordsRem?, ?TextRank + Thesaurus?, 
and SuPor-2 on the TeM?rio source texts. Results 
for informativeness are shown in Table 1. Since 
Mihalcea?s setting was kept unchanged, we just 
included in that table the same results presented in 
(Mihalcea, 2005), i.e., we did not run her systems 
all over again. We also reproduced for comparison 
the TextRank variations reported by Mihalcea, es-
pecially regarding graph-based walks by PageRank 
and HITS. Shaded lines correspond to our sug-
gested methods presented in Sections 4 and 5, 
which involve differing degrees of dependence on 
linguistic knowledge. 
It can be seen that ?TextRank+Thesaurus? and 
?TextRank+Stem+StopwordsRem? considerably 
outperformed all other versions of TextRank. 
Compared with Mihalcea's best version, i.e., with 
'TextRank (PageRank - backward)', those two 
methods represented a 6% and 9% improvement, 
respectively. We can conclude that neither the way 
the graph is built nor the choice of the graph-based 
ranking algorithm affects the results as signifi-
cantly as do the linguistic-based methods. Clearly, 
both variations proposed in this paper signal that 
linguistic knowledge, even if only used at the pre-
processing stage, provides more informative ex-
tracts than those produced when no linguistic 
knowledge at all is considered. Moreover, at that 
stage little modeling and computational effort is 
demanded, since lexicons, stoplists, and thesauri 
22
are quite widely available nowadays for several 
Romance languages. 
Even the baseline outperformed most versions 
of TextRank, showing that linguistic independence 
in a random walk model for extractive AS should 
be reconsidered. Actually, this shows that linguis-
tic knowledge does make a difference, at least for 
summarizing newswire texts in Brazilian Portu-
guese. 
In addition, SuPor-2 performance exceeds the 
best version of TextRank that uses no linguistic 
knowledge ? ?TextRank (PageRank - backward)? ? 
by about 14%. 
 
 
System ROUGE NGram(1,1) 
SuPor-2 0,5839 
TextRank+Thesaurus 0,5603 
TextRank+Stem+StopwordsRem 0,5426 
TextRank (PageRank - backward) 0,5121 
TextRank (HIT hub - forward) 0,5002 
TextRank (HITS authority - backward) 0,5002 
Baseline 0,4963 
TextRank (PageRank - undirected) 0,4939 
TextRank (HITS authority - forward) 0,4834 
TextRank (HIT hub - backward) 0,4834 
TextRank (HITS authority - undirected) 0,4814 
TextRank (HIT hub - undirected) 0,4814 
TextRank (PageRank - forward) 0,4574 
 
Table 1. Informativeness comparison between ex-
tractive summarizers 
7 Final Remarks 
A critical issue in the comparison presented above 
is the contrast between having an unsupervised or 
supervised summarizer, which is related to the is-
sue on having linguistic-independent extractive 
summarizers. Perhaps the question that we should 
pose here is how interesting and useful an extrac-
tive automatic summarizer that is totally independ-
ent from linguistic knowledge can actually be. To 
our view, the more non-informative an extract, the 
less useful it may be. So, summarizers that do not 
reach a minimum threshold concerning informa-
tiveness are deemed to failure nowadays. Clearly, 
SuPor-2 requires language-dependent resources, 
but its main extraction procedure is still general 
enough to make it portable and adaptable to new 
domains and languages. Hence, SuPor-2 assess-
ment suggests that it may be interesting to scale up 
SuPor-2. 
Considering that SuPor-2 is one of the best ex-
tractive summarizers for Brazilian Portuguese texts 
(Leite and Rino, 2006) and ?TextRank+Thesaurus? 
performed only 4% below it, we can also argue  in 
favor of providing even simple linguistic proce-
dures for extractive AS. The latter system shows 
that TextRank can yield extracts nearly as informa-
tive as those produced by the former, when em-
bedding stemming and stopwords removal. It can 
also perform AS with little computational effort 
and no training, when compared to the supervised 
SuPor-2. As a conclusion, we see that some lin-
guistic knowledge may boost TextRank perform-
ance without too much effort, since language-
dependent resources for preprocessing texts in 
natural language are usually available and easy to 
handle, concerning our addressed approach. 
There are many experiments that may be derived 
from our discussion in this paper (1) Although the 
reported results suggest that linguistic knowledge 
does make a difference when embedded in lan-
guage-free extractive summarizers, the perform-
ance of the top systems assessed through ROUGE 
should be more comprehensively licensed through 
additional assessment tasks. (2) These could also 
incorporate other graph-based algorithms than 
TextRank, such as the LexRank one, aiming at re-
assuring our claim and scaling up graph-based ap-
proaches. (3) Since we addressed language-
independence (thus portability) versus language-
dependence for informativeness, it would also be 
interesting to explore other domains or languages 
to support our claim or, at least, to look for other 
findings to confirm if linguistic knowledge indeed 
makes a difference. (4) Other TextRank variations 
could also be explored, to see if adding more fea-
tures would make TextRank closer to SuPor-2. 
Acknowledgements 
This work has been supported by the Brazilian re-
search funding agencies CNPq, CAPES and 
FAPESP.
23
References 
B. C. Dias-da-Silva, M. F. Oliveira, H. R. Moraes, C. 
Paschoalino, R. Hasegawa, D. Amorin and A. C. 
Nascimento. 2000. Constru??o de um Thesaurus Ele-
tr?nico para o Portugu?s do Brasil. In Proceedings of 
the V Encontro para o Processamento Computacio-
nal da L?ngua Portuguesa Escrita e Falada 
(PROPOR 2000), S?o Carlos, Brasil , 1-11. 
C. Lin and E. H. Hovy. 2003. Automatic Evaluation of 
Summaries Using N-gram Co-occurrence Statistics. 
In Proceedings of Language Technology Conference 
(HLT-NAACL 2003), Edmonton, Canada.  
D. Marcu. 1999. Discourse Trees Are Good Indicators 
of Importance in Text. In Mani, I., Maybury, M. T. 
(Eds.). 1999. Advances in Automatic Text Summari-
zation. MIT Press. 
D. S. Leite and L. H. M. Rino. 2006. Selecting a Feature 
Set to Summarize Texts in Brazilian Portuguese. In J. 
S. Sichman et al (eds.): Proceedings of 18th. Brazil-
ian Symposium on Artificial Intelligence (SBIA'06) 
and 10th. Ibero-American Artificial Intelligence Con-
ference (IBERAMIA'06). Lecture Notes on Artificial 
Intelligence, No. 4140, Springer-Verlag, 462-471. 
G. Erkan and D R. Radev. 2004. LexRank: Graph-based 
Lexical Centrality as Salience in Text Summariza-
tion. Journal of Artificial Intelligence Research 
22:457-479 
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross and 
K. Miller. 1990. Introduction to WordNet: An On-
line Lexical Database. International Journal of Lexi-
cography 3(4):235-244 
G. Salton, and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information 
Processing & Management 24 : 513-523.. Reprinted 
in: K. Sparck-Jones and P. Willet (eds.). 1997. Read-
ings in Information Retrieval, Morgan Kaufmann, 
323-328.  
H. Luhn. 1958. The automatic creation of literature ab-
stracts. IBM Journal of Research and Development 
2:159-165 
H. P. Edmundson. 1969. New methods in automatic 
extracting. Journal of the Association for Computing 
Machinery 16:264-285. 
I. Witten and E. Frank. 2005. Data Mining: Practical 
machine learning tools and techniques, 2nd ed. Mor-
gan Kaufmann, San Francisco. 
I. Mani. 2001. Automatic Summarization. John Benja-
min?s Publishing Company.  
I. Mani and M. T. Maybury. 1999. Advances in Auto-
matic Text Summarization. MIT Press. 
J. Caldas Junior, C. Y. M. Imamura and S. O. Rezende. 
Avalia??o de um Algoritmo de Stemming para a 
L?ngua Portuguesa. In Proceedings of the 2nd Con-
gress of Logic Applied to Technology 
(LABTEC?2001), vol. II. Faculdade SENAC de Ci?n-
cias Exatas e Tecnologia, S?o Paulo, Brasil (2001), 
267-274. 
J. M. Kleinberg. 1999. Authoritative sources in hyper-
linked environment. Journal of the ACM, 46(5):604-
632. 
J. Kupiec, J. Pedersen and F. Chen. 1995. A trainable 
document summarizer. In Proceedings of the 18th 
ACM-SIGIR Conference on Research & Develop-
ment in Information Retrieval, 68-73. 
J. Larocca Neto, A. D. Santos, C. A. A. Kaestner and A. 
A. Freitas. 2000. Generating Text Summaries 
through the Relative Importance of Topics. Lecture 
Notes in Artificial Intelligence, No. 1952. Springer-
Verlag, 200-309 
M. A. Hearst. 1993. TextTiling: A Quantitative Ap-
proach to Discourse Segmentation. Technical Report 
93/24. University of California, Berkeley. 
M. F. Porter. 1980. An Algorithm for Suffix Stripping. 
Program, 14 (3) : 130-137 
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing 
Order into Texts. In  Proceedings of the Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP 2004), Barcelona, Spain, July.  
R. Mihalcea. 2005. Language Independent Extractive 
Summarization. In Proceedings of the 43th Annual 
Meeting of the Association for Computational Lin-
guistics, Companion Volume (ACL2005), Ann Ar-
bor, MI, June. 
R. Barzilay and M. Elhadad. 1997. Using lexical chains 
for text summarization. In Proceedings of the Intelli-
gent Scalable Text Summarization Workshop 
(ISTS'97), ACL, Madrid, Spain.  
S. Brin and L. Page. 1998. The anatomy of a large-scale 
hypertextual Web search engine. Computer Networks 
and ISDN Systems 30:1-7. 
T. A. S. Pardo and L.H.M. Rino. 2003. TeM?rio: A cor-
pus for automatic text summarization (in Portu-
guese). NILC Tech. Report NILC-TR-03-09  
24
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 442?445,
Prague, June 2007. c?2007 Association for Computational Linguistics
USP-IBM-1 and USP-IBM-2: The ILP-based Systems for Lexical Sample
WSD in SemEval-2007
Lucia Specia, Maria das Grac?as Volpe Nunes
ICMC - University of Sa?o Paulo
Trabalhador Sa?o-Carlense, 400, Sa?o Carlos, 13560-970, Brazil
{lspecia, gracan}@icmc.usp.br
Ashwin Srinivasan, Ganesh Ramakrishnan
IBM India Research Laboratory
Block 1, Indian Institute of Technology, New Delhi 110016, India
{ashwin.srinivasan, ganramkr}@in.ibm.com
Abstract
We describe two systems participating of the
English Lexical Sample task in SemEval-
2007. The systems make use of Inductive
Logic Programming for supervised learning
in two different ways: (a) to build Word
Sense Disambiguation (WSD) models from
a rich set of background knowledge sources;
and (b) to build interesting features from
the same knowledge sources, which are then
used by a standard model-builder for WSD,
namely, Support Vector Machines. Both sys-
tems achieved comparable accuracy (0.851
and 0.857), which outperforms considerably
the most frequent sense baseline (0.787).
1 Introduction
Word Sense Disambiguation (WSD) aims to iden-
tify the correct sense of ambiguous words in context.
Results from the last edition of the Senseval com-
petition (Mihalcea et al, 2004) have shown that, for
supervised learning, the best accuracies are obtained
with a combination of various types of features, to-
gether with traditional machine learning algorithms
based on feature-value vectors, such as Support Vec-
tor Machines (SVMs) and Naive Bayes. While the
features employed by these approaches are mostly
considered to be ?shallow?, that is, extracted from
corpus or provided by shallow syntactic tools like
part-of-speech taggers, it is generally thought that
significant progress in automatic WSD would re-
quire a ?deep? approach in which access to substan-
tial body of linguistic and world knowledge could
assist in resolving ambiguities. Although the ac-
cess to large amounts of knowledge is now possi-
ble due to the availability of lexicons like WordNet,
parsers, etc., the incorporation of such knowledge
has been hampered by the limitations of the mod-
elling techniques usually employed for WSD. Using
certain sources of information, mainly relational in-
formation, is beyond the capabilities of such tech-
niques, which are based on feature-value vectors.
Arguably, Inductive Logic Programming (ILP) sys-
tems provide an appropriate framework for dealing
with such data: they make explicit provisions for the
inclusion of background knowledge of any form; the
richer representation language used, based on first-
order logic, is powerful enough to capture contextual
relationships; and the modelling is not restricted to
being of a particular form (e.g., classification).
We describe the investigation of the use of ILP
for WSD in the Lexical Sample task of SemEval-
2007 in two different ways: (a) the construction of
models that can be used directly to disambiguate
words; and (b) the construction of interesting fea-
tures to be used by a standard feature-based algo-
rithm, namely, SVMs, to build disambiguation mod-
els. We call the systems resulting of the two differ-
ent approaches ?USP-IBM-1? and ?USP-IBM-2?,
respectively. The background knowledge is from 10
different sources of information extracted from cor-
pus, lexical resources and NLP tools.
In the rest of this paper we first present the spec-
ification of ILP implementations that construct ILP
models and features (Section 2) and then describe
the experimental evaluation on the SemEval-2007
Lexical Sample task data (Section 3).
442
2 Inductive Logic Programming
Inductive Logic Programming (ILP) (Muggleton,
1991) employs techniques from Machine Learning
and Logic Programming to build first-order theo-
ries or descriptions from examples and background
knowledge, which are also represented by first-order
clauses. Functionally, ILP can be characterised by
two classes of programs. The first, predictive ILP,
is concerned with constructing models (in this case,
sets of rules) for discriminating accurately amongst
positive and negative examples. The partial spec-
ifications provided by (Muggleton, 1994) form the
basis for deriving programs in this class:
? B is background knowledge consisting of a fi-
nite set of clauses = {C1, C2, . . .}
? E is a finite set of examples = E+?E? where:
? Positive Examples. E+ = {e1, e2, . . .} is
a non-empty set of definite clauses
? Negative Examples. E? = {f1, f2 . . .} is
a set of Horn clauses (this may be empty)
? H , the output of the algorithm given B and E,
is acceptable if these conditions are met:
? Prior Satisfiability. B ? E? 6|= 2
? Posterior Satisfiability. B ?H ?E? 6|= 2
? Prior Necessity. B 6|= E+
? Posterior Sufficiency. B ? H |= e1 ? e2 ?
. . .
The second category of ILP programs, descriptive
ILP, is concerned with identifying relationships that
hold amongst the background knowledge and exam-
ples, without a view of discrimination. The partial
specifications for programs in this class are based
on the description in (Muggleton and Raedt, 1994):
? B is background knowledge
? E is a finite set of examples (this may be
empty)
? H , the output of the algorithm given B and E
is acceptable if the following condition is met:
? Posterior Sufficiency. B ? H ? E 6|= 2
The intuition behind the idea of exploiting a
feature-based model constructor that uses first-order
features is that certain sources of structured infor-
mation that cannot be represented by feature vectors
can, by a process of ?propositionalization?, be iden-
tified and converted in a way that they can be accom-
modated in such vectors, allowing for traditional
learning techniques to be employed. Essentially, this
involve two steps: (1) a feature-construction step
that identifies all the features, that is, a set of clauses
H , that are consistent with the constraints provided
by the background knowledge B (descriptive ILP);
and (2) a feature-selection step that retains some of
the features based on their utility in classifying the
examples, for example, each clause must entail at
least one positive example (predictive ILP). In order
to be used by SVMs, each clause hi in H is con-
verted into a boolean feature fi that takes the value
1 (or 0) for any individual for which the body of
the clause is true (if the body is false). Thus, the
set of clauses H gives rise to a boolean vector for
each individual in the set of examples. The fea-
tures constructed may express conjunctions on dif-
ferent knowledge sources. For example, the follow-
ing boolean feature built from a clause for the verb
?ask? tests whether the sentence contains the expres-
sion ?ask out? and the word ?dinner?. More details
on the specifications of predictive and descriptive
ILP for WSD can be found in (Specia et al, 2007):
f1(X) =
{
1 expr(X, ?ask out?) ? bag(X,dinner) = 1
0 otherwise
3 Experiments
We investigate the performance of two kinds of ILP-
based models for WSD:
1. ILP models (USP-IBM-1 system): models con-
structed by an ILP system for predicting the
correct sense of a word.
2. ILP-assisted models (USP-IBM-2 system):
models constructed by SVMs for predicting the
correct sense of a word that, in addition to ex-
isting shallow features, use features built by an
ILP system according to the specification for
feature construction in Section 2.
443
The data for the English Lexical Sample task in
SemEval-2007 consists of 65 verbs and 35 nouns.
Examples containing those words were extracted
from the WSJ Penn Treebank II and Brown corpus.
The number of training / test examples varies from
19 / 2 to 2,536 / 541 (average = 222.8 / 48.5). The
senses of the examples were annotated according to
OntoNotes tags, which are groupings of WordNet
senses, and therefore are more coarse-grained. The
number of senses used in the training examples for
a given word varies from 1 to 13 (average = 3.6).
First-order clauses representing the following
background knowledge sources, which were au-
tomatically extracted from corpus and lexical re-
sources or provided by NLP tools, were used to de-
scribe the target words in both systems:
B1. Unigrams consisting of the 5 words to the
right and left of the target word.
B2. 5 content words to the right and left of the
target word.
B3. Part-of-speech tags of 5 words to the right and
left of the target word.
B4. Syntactic relations with respect to the target
word. If that word is a verb, subject and object syn-
tactic relations are represented. If it is a noun, the
representation includes the verb of which it is a sub-
ject or object, and the verb / noun it modifies.
B5. 12 collocations with respect to the target
word: the target word itself, 1st preposition to the
right, 1st and 2nd words to the left and right, 1st
noun, 1st adjective, and 1st verb to the left and right.
B6. A relative count of the overlapping words in
the sense inventory definitions of each of the pos-
sible senses of the target word and the words sur-
rounding that target word in the sentence, according
to the sense inventories provided.
B7. If the target word is a verb, its selectional
restrictions, defined in terms of the semantic fea-
tures of its arguments in the sentence, as given by
LDOCE. WordNet relations are used to make the
verification more generic and a hierarchy of feature
types is used to account for different levels of speci-
ficity in the restrictions.
B8. If the target word is a verb, the phrasal verbs
possibly occurring in a sentence, according to the
list of phrasal verbs given by dictionaries.
B9. Pairs of words in the sentence that occur fre-
quently in the corpus related by verb-subject/object
or subject/verb/object-modifier relations.
B10. Bigrams consisting of adjacent words in a
sentence occurring frequently in the corpus.
Of these 10 sources, B1?B6 correspond to the so
called ?shallow features?, in the sense that they can
be straightforwardly represented by feature vectors.
A feature vector representation of these sources is
built to be used by the feature-based model construc-
tor. Clausal definitions for B1?B10 are directly used
by the ILP system.
We use the Aleph ILP system (Srinivasan, 1999)
to construct disambiguation models in USP-IBM-1
and to construct features to be used in USP-IBM-
2. Feature-based model construction in USP-IBM-
2 system is performed by a linear SVM (the SMO
implementation in WEKA).
In the USP-IBM-1 system, for each target word,
equipped with examples and background knowl-
edge definitions (B1?B10), Aleph constructs a set
of clauses in line with the specifications for predic-
tive ILP described in Section 2. Positive examples
are provided by the correct sense of the target word.
Negative examples are generated automatically us-
ing all the other senses. 3-fold cross-validation on
the training data was used to obtain unbiased esti-
mates of the predictive accuracy of the models for a
set of relevant parameters. The best average accura-
cies were obtained with the greedy induction strat-
egy, in conjunction with a minimal clause accuracy
of 2. The constructed clauses were used to predict
the senses in the test data following the order of their
production, in a decision-list like manner, with the
addition to the end of a default rule assigning the
majority sense for those cases which are not covered
by any other rule.
In the USP-IBM-2 system, for constructing the
?good? features for each target word from B1?
B10 (the ?ILP-based features?), we first selected, in
Aleph, the clauses covering at least 1 positive exam-
ple. 3-fold cross-validation on the training data was
performed in order to obtain the best model possi-
ble using SVM with features in B1?B6 and the ILP-
based features. A feature selection method based
on information gain with various percentages of fea-
tures to be selected (1/64, ..., 1/2) was used, which
resulted in different numbers of features for each tar-
get word.
444
Baseline USP-IBM-1 USP-IBM-2
Nouns 0.809 0.882 0.882
Verbs 0.762 0.817 0.828
All 0.787 0.851 0.857
Table 1: Average accuracies of the ILP-based mod-
els for different part-of-speeches
Table 1 shows the average accuracy of a base-
line classifier that simply votes for the most frequent
sense of each word in the training data against the
accuracy of our ILP-based systems, USP-IBM-1 and
USP-IBM-2, according to the part-of-speech of the
target word, and for all words. Clearly, the ?ma-
jority class? classifier performs poorest, on average.
The difference between both ILP-based systems and
the baseline is statistically significant according to
a paired t-test with p < 0.01. The two ILP-based
models appear to be comparable in their average ac-
curacy. Discarding ties, IBM-USP-2 outperforms
IBM-USP-1 for 31 of the words, but the advantage
is not statistically significant (cf. paired t-test).
The low accuracy of the ILP-based systems for
certain words may be consequence of some charac-
teristics of the data. In particular, the sense distri-
butions are very skewed in many cases, with differ-
ent distributions in the training and test data. For
example, in the case of ?care? (accuracy = 0.428),
the majority sense in the training data is 1 (78.3%),
while in the test data the majority sense is 2 (71%).
In cases like this, many of the test examples remain
uncovered by the rules produced by the ILP system
and backing off to the majority sense also results in
a mistake, since the majority sense in the training
data does not apply for most of the test examples.
The same goes for the feature-based system: fea-
tures which are relevant for the test examples will
not be built or selected.
One relevant feature of ILP is its ability to pro-
duce expressive symbolic models. These models
can reproduce any kind of background knowledge
using sets of rules testing conjunctions of different
types of knowledge, which may include variables
(intensional clauses). This is valid both for the con-
struction of predictive models and for the construc-
tion of features (which are derived from the clauses).
Examples of rules induced for the verb ?come? are
given in Figure 1. The first rule states that the sense
sense(X, 3) :-
expr(X, ?come to?).
sense(X, 1) :-
satisfy restrictions(X, [animate], nil);
(relation(X, subj, B), pos(X, B, nnp)).
Figure 1: Examples of rules learned for ?come?
of the verb in a sentence X will be 3 (progress to a
state) if that sentence contains the expression ?come
to?. The second rule states that the sense of the verb
will be 1 (move, travel, arrive) if its subject is ?ani-
mate? and there is no object, or if it has has a subject
B that is a proper noun (nnp).
4 Concluding Remarks
We have investigated the use of ILP as a mech-
anism for incorporating shallow and deep knowl-
edge sources into the construction of WSD mod-
els for the Semeval-2007 Lexical Sample Task data.
Results consistently outperform the most frequent
sense baseline. It is worth noticing that the knowl-
edge sources used here were initially designed for
the disambiguation of verbs (Specia et al, 2007)
and therefore we believe that further improvements
could be achieved with the identification and speci-
fication of other sources which are more appropriate
for the disambiguation of nouns.
References
R. Mihalcea, T. Chklovski, A. Kilgariff. 2004.
The SENSEVAL-3 English Lexical Sample Task.
SENSEVAL-3: 3rd Int. Workshop on the Evaluation of
Systems for Semantic Analysis of Text, 25?28.
S. Muggleton. 1991. Inductive Logic Program-ming.
New Generation Computing, 8(4):29-5-318.
S. Muggleton. 1994. Inductive Logic Programming:
derivations, successes and shortcomings. SIGART Bul-
letin, 5(1):5?11.
S. Muggleton and L. D. Raedt. 1994. Inductive logic
programming: Theory and methods. Journal of Logic
Programming, 19,20:629?679.
L. Specia, M.G.V. Nunes, A. Srinivasan, G. Ramakrish-
nan. 2007. Word Sense Disambiguation using Induc-
tive Logic Programming. Proceedings of the 16th In-
ternational Conference on ILP, Springer-Verlag.
A. Srinivasan. 1999. The Aleph Manual. Computing
Laboratory, Oxford University.
445
An interlingua aiming at communication on the Web: 
How language-independent can it be? 
Ronaldo Teixeira Martins 
ronaIdo @nilc.icmsc.sc.usp.br 
Lucia Helena Machado Rino 
Iucia @ dc. uf scar.br 
Maria das Graqas Volpe Nunes 
md gvnune @ icmc.sc, usp.br 
Gisele Montilha 
gisele @nilc. icmsc, sc. usp. br 
Osvaldo Novais de Oliveira Jr. 
chu@if.sc.usp.br 
Ndcleo Interinstitucional de Lingiiistica Computacional (NILC/Sio Carlos) 
http://nilc.icmsc.sc.usp.br 
CP 668 - ICMC-USP, 13560-970 Silo Carlos, SP, Brazil 
Abstract 
In this paper, we describe the Universal Networking Language, an interlingua 
to be plugged in a Web environment aiming at allowing for many-to-many 
information exchange, 'many' here referring to many natural anguages. The 
interlingua is embedded in a Knowledge-Base MT system whose language- 
dependent modules comprise an encoder, a decoder, and linguistic resources 
that have been developed by native speakers of each language involved in the 
project. Issues concerning both the interlingua formalism and its foundational 
issues are discussed. 
1. Introduction 
The widespread use of the Web and the 
growing Intemet facilities have sparked 
enormous interest in improving the ways 
people use to communicate. In this context 
multilingual Machine Translation systems 
become prominent, for they allow for a huge 
information flow. To date, MT systems have 
been built under limited conditions, of which 
we highlight two: i) in general, they mirror 
one-to-many(languages) or many(languages)- 
to-one approaches, often involving English at 
the "one" end; ii) communication is reduced 
to basic information exchange, ignoring 
richness and flexibility implied by human 
mind. The first limitation has been seldom 
overcome, since it requires a robust 
environment and research teams that can 
cope with knowledge of several anguages 1, 
to derive precise automatic language 
analyzers and synthesizers. The second 
limitation follows up the first: adding up 
communicative issues to linguistic 
processing/modeling makes still harder to 
overcome MT limitations. 
In this article, we elaborate on work using 
an interlingua conceived to overcome the first 
limitation, i.e., to allow for a many-to-many 
information exchange environment, which 
shall be plugged in a nontraditional Internet 
platform. The goal is to allow interlocutors to 
entangle communication even if they do not 
share the same mother tongue or the English 
Standing, most often, for natural language, or NL. 
24 
language, unlike MT systems that have just 
one language at one of their edges. As the 
main component of a Knowledge-Base MT 
system (hereafter, KBMT), the interlingua 
approach has been developed under the 
Universal Networking Language Project, or 
simply UNL Project. What makes the 
interlingua UNL special is its intended use: 
as an electronic language for networks, it has 
to allow for high quality 2 conversation 
systems involving many languages. As the 
main component of a KBMT system, it has to 
be sufficiently robust o ground research and 
development (R&D) of the language-specific 
modules to be attached to the system. It is 
this latter perspective that is undertaken here: 
from the viewpoint of R&D, we discuss how 
broad, or language-independent, he 
interlingua UNL is, especially focusing on its 
syntax and coverage. In addition to being 
consistent and complete to represent 
meaning, we also consider its sharing by 
researchers all around the world, which is an 
important bottleneck of the UNL Project, 
since information exchange by researchers 
during R&D brings about the problems 
introduced by the interlingua UNL itself, 
concerning both its formalism and 
foundational issues. Before discussing this 
topic in Section 5, we present an overview of 
the UNL Project (Section 2) and describe the 
main features of the interlingua UNL 
(Section 3). In Section 4, we describe the 
UNL system architecture. Hereafter, 
'interlingua UNL' will be simply referred to 
as UNL, the acronym for Universal 
Networking Language. Also, the viewpoint 
presented here is that of interlingua users 
who experience R&D for a given NL, and not 
of its authors. 
2. The UNL Project 
The UNL Project 3 has been launched by 
the United Nations University to foster and 
ease international web communication by 
means of NLP systems. Its main strength lies 
on the development of the UNL, as a unique 
semantic (or meaning) representation that can 
be interchanged with the various languages to 
be integrated in the KBMT system. In the 
UNL Project, plug-in software to encode NL 
texts onto UNL ones (NL-UNL encoders) 
and to decode UNL into NL texts (UNL-NL 
decoders) have been developed by R&D 
groups in their own native languages. The 
modules to process Brazilian Portuguese 4, for 
example, have been developed by a team of 
Portuguese native speakers that comprises 
linguists, computational linguists, and 
computer experts. Such packages will be 
made available in WWW servers and will be 
accessible by browsing through Internet, thus 
overcoming the need for people all around 
the world tO learn the language of their 
interlocutors. Several inguistic groups have 
signed to the. Project, namely: the Indo- 
European (Portuguese, Spanish, French, 
Italian, English, German, Russian, Latvian 
and Hindi), the Semitic (Arabic), the Sino- 
Tibetan (Chinese), the Ural-Altaic 
(Mongolian), the Malayan-Polynesian 
(Indonesian), and the Japanese. 
On the one hand, the main strength of 
the Project is that knowledgeable specialists 
address language-dependent issues of their 
mother tongue, most of which are related to 
R&D of the encoding and decoding modules 
and to the specification of the NL-UNL 
lexicon. On the other hand, this also 
represents a crucial problem faced by the 
project participants, for distinct groups may 
interpret the interlingua specification 
differently. There is thus the need for a 
consensus about the UNL formalism, 
2 By 'high quality' we mean 'at least allowing for 
readability and understandability by any user'. 
3 A description of both, the Project and the UNL itself, 
can be found in http://www.unl.ias.unu.edu/. 
4 Hereafter referred to as Portuguese or by its acronym, 
BP. 
25 
bringing about an assessment of its coverage, 
completeness, and consistency, all features 
that will be discussed shortly. 
3. The Universal Networking Language 
The UNL is a formal language designed 
for rendering automatic multilingual 
information exchange. It is intended to be a 
cross-linguistic semantic representation of 
NL sentence meaning, being the core of the 
UNL System, the KBMT system developed 
by H. Uchida (1996) at the Institute of 
Advanced- Studies, United Nations 
University, Tokyo; Japan. 
UNL subsumes a tridimensional theory of 
(sentence) meaning, whose components are 
defined according to one of the following sets 
(Martins et al, 1998a): concepts (e.g., "cat", 
"sit", "on", or "mat"), concept relations (e.g., 
"agent", "place", or "object"), and concept 
predicates (e.g., "past" or "definite"). Such 
components are formally and 
correspondingly represented by three 
different kinds of entities, namely: Universal 
Words (UWs), Relation Labels (RLs), and 
Attribute Labels (ALs). According to the 
UNL syntax, information conveyed by each 
sentence can be represented by a hypergraph 
whose nodes represent UWs and whose arcs 
represent RLs. To make symbol processing 
simpler, hypergraphs are often reduced to 
lists of ordered binary relations between 
concepts, as it is shown in Figure 1 for the 
sentence (1) The cat sat  on the mat. 5 
'sit', 'cat', 'on' and 'mat' are UWs; 'agt' (agent), 
'pie' (place) and 'obj' (object) are RLs; '@def, 
'@entry' and '@past' are ALs. 
Figure la: UNL hypergraph representation f the 
English sentence "The cat sat on the mat" 
agt(sit. @entry. @past,cat. @def) 
plc(sit. @entry. @past,on) 
obj(on,mat. @def) 
Figure lb :  UNL linear representation of the 
English sentence "The cat sat on the mat." 
UWs are labels for concept-like 
information, roughly corresponding to the 
lexical level in the sentence structure. They 
comprise an open large inventory, virtually 
capable of denoting every non-compositional 
meaning to be conveyed by any speaker of 
any language. For the sake of representation, 
these atomic semantic ontents are associated 
to English words and expressions, which play 
the role of semantic labels. However, there is 
no one-to-one mapping between the English 
vocabulary and the UNL lexicon, for UNL, 
as a multilingual representation code, is 
larger than the English vocabulary. To avoid 
unnecessary proliferation of the UNL 
vocabulary and to certify that standards be 
observed by UNL teams, control over the 
specification of the UW set is centered at the 
UNL Center, in Japan. 
Several semantic relationships hold 
between UWs, namely synonymy, antonymy, 
hyponymy, hypemymy and meronymy, 
which compose the UNL Ontology. Steady 
semantic valencies (such as agent and object 
features) can also be represented, forming the 
UNL Knowledge-Base. Both Ontology and 
Knowledge-Base aim at constraining the 
scope of UW labels, whenever ambiguity is 
to be avoided. The. UNL representation f 
sentence (1), for example, can be ambiguous 
26 
in Romance languages, for the translation of 
'cat' should make explicit the animal sex: if 
male, it would be "gato" (Portuguese and 
Spanish), "gatto" (Italian), "chat" (French), 
whereas different names would have to be 
used for the female cat. Instead of having a 
unique UW 'cat', it is thus quite feasible to 
have a whole structure in which 'cat' is only 
the hyper-ordinate option. 
For the English-UNL association ot to 
undermine the intended universality of the 
UW inventory, its semantic-orthograpical 
correspondence has to be considered rather 
incidental, or even. approximated. It is not 
always the case that extensions 6 of a UW 
label and of its corresponding English word 
coincide. The extension of the English word 
"mat", for example, does not exactly coincide 
with the extension of any Portuguese word, 
although we can find many overlaps between 
"mat" and, e.g., "capacho" (Portuguese). 
Portuguese speakers, however, would not say 
"capacho" for the ornamental dishmat, as 
would not English speakers use the word 
"mat" for a fawner (still "capacho" in 
Portuguese). Since each language categorizes 
the world in a very idiosyncratic way, it 
would be misleading to impose a 
straightforward correspondence between 
lexical items of two different languages. In 
UNL, this problem has been overcome by 
proposing a rather analogic lexicon, instead 
of a digital one. Although discrete, UWs 
convey continuous entities, in the sense that 
semantic gaps between concepts are fulfilled 
by the UNL Knowledge-Base, as it is shown 
for the UW 'mat' in Figure 2. Granularity 
thus plays an important role in UNL lexical 
organization and brings flexibility into cross- 
linguistic lexical matching. 
Cf. (Frege, 1892), extension here is used to establish 
the relationship between a word and the world, 
opposed to intension, referring to the relationship 
between aword and its meaning. 
icl 
Figure 2a: UNL hypergraph artial representation for 
the meaning denoted by the English word "mat" 
"mat" 
"mat(aoj>entity)" 
"mat(icl>event)" 
"mat(icl>frame)" 
"mat(icl>rug)" 
"mat(icl>state)" 
"mat(obi>entitv)" 
Figure 2b: UNL partial inear epresentation for 
the meaning denoted by the English word "mat" 
While lexical representation in UNL 
comprises a set of universal concepts 
signaled by UWs, the cross-lexical level 
involves a set of ordered binary relations 
between UWs, which are the Relation Labels 
(RLs). RLs specification are similar to 
Fillmore's semantic ases (1968), with RLs 
corresponding to semantic-value relations 
linking concept-like information. There are 
currently 44 RLs, but this set has been 
continuously modified by empirical evidence 
of lack, or redundancy, of relations. The 
inventory of RLs can be divided into three 
parts, according to the functional aspects of 
the related concepts: ontological, event-like 
and logical relations. Ontological relations 
are used as UW constraints in reducing 
lexical granularity or avoiding ambiguity as 
shown above, and they help positioning UWs 
in a UNL lexical structure. Five different 
labels are used to convey ontological 
relations: icl (hyponymy), equ (synonymy), 
ant (antonymy), pof (meronymy), and fld 
(semantic field). 
2"7 
UNL depicts sentence meaning as a fact 
composed by either a simple or a complex 
event, which is considered here the starting 
point of a UNL representation, i.e., its 
minimal complete semantic unit. Event-like 
relations are assigned by an event external or 
internal structure, or by both. An event 
external structure has to do nearly always 
with time and space boundaries. It can be 
referred to by a set of RLs signaling the event 
co-occurrent meanings, such as 7 its 
environment (scn); starting place (pl0, 
finishing p!ace (pit), or, simply, place (plc); 
range (fmt); starting time (tmf), finishing 
time (tmt), or, simply, time (tim); and 
duration (dur). Action modifiers, such as 
manner (man) and method (met) can also 
qualify this structure. An event internal 
structure is associated to one of the following 
simple frames: action, activity, movement, 
state, and process, each expressing different 
RLs in the event itself, including its actors 
and circumstances. 
Event actors are any animate or inanimate 
character playing any role in events, which 
can be the main or the coadjutant actors. 
There can be up to eight actors, signaled by 
the following RLs: agent (agt), co-agent 
(cag), object (obj), co-object (cob), object 
place (opl), beneficiary (ben), partner (ptn) 
and instrument (ins). They can also be 
coordinated through the RLs conjunction 
(and) and disjunction (or), or subordinated to 
each other by possession (pos), content (cnt), 
naming (nam), comparison (bas), proportion 
(per), and modification (mod). They can still 
be quantified (qua) or qualified by the RLs 
"property attribution" (aoj) and co-attribution 
(cao). It is possible to refer to an "initial 
actor" (src), a "final actor" (gol), or an 
"intermediary actor" (via). Finally, spatial 
relationships can also hold between actors: 
current place (plc), origin (firm), destination 
(to), and path (via). Besides single events, 
there can still be complex cross-event 
relationships which express either paralleled 
events - co-occurrence (coo), conjunction 
(and), and disjunction (or) - or hierarchically 
posed events - purpose (pur), reason (rsn), 
condition (con), and sequence (seq). They 
can all be referred to as logical relations, 
since they are often isomorphic to first-order 
logic predicates. 
According to the UNL authors, it is 
possible to codify any sentence written in any 
NL into a corresponding UNL text expressing 
the sentence meaning through the use of the 
above RLs. This is still a claim to be verified, 
since cases of superposition and competition 
between different RLs have been observed, 
as it is discussed in Section 5. 
In addition to UWs and RLs, UNL 
makes use of predicate-like information, or 
Attribute Labels (ALs), which are names for 
event and concept "transformations", in a 
sense very close to that intended by Chomsky 
(1957, 1965). They are not explicitly 
represented in a UNL hypergraph, although 
they are used to modify its nodes. ALs can 
convey information about concept intensions 
and extensions. In the former case, ALs name 
information about utterers' intensions over 
either specific parts of a sentence (focus, 
topic, emphasis, theme) or the whole 
structure (exclamation, interrogation, 
invitation, recommendation, obligation, etc.). 
In the latter case, ALs refer to spatial 
(definite, indefinite, generic, plural) or 
temporal (past, present, future)information, 
or still, temporal external (begin-soon, begin- 
just, end-soon, end-just) or intemal 
(perfecfive, progressive, imperfective, 
iterative) structures. To differentiate ALs 
from UWs, ALs are attached to UWs by the 
symbol ".@". The cOncept expressed by the 
UW 'sit' in "sit. @entry. @past", for example, 
is taken as the starting point (. @entry) of the 
corresponding hypergraph and it is to be 
modified by temporal information (.@past). 
7 RLs names are bracketed.  
28 
4. The UNL System 
The UNL system architecture consists of 
two main processes, the encoder and 
decoder, and several linguistic resources, 
each group of these corresponding to a NL 
embedded in the system, as depicted in 
Figure 3. 
~U~qL e language-to-~ dictionary 
UNL-t0-target-~ uage dictionary 
~source  I 
language I 
Encoder 
? r I 
1 
Decoder 
language I 
 s~t~CNL e language-to-~ grammar J 
Figure 3: The UNL System Architecture 
A source document (SLD) conveys 
written text on any subject, in any of the NLs 
considered. There is no constraint in the 
domain or structure of the SLD, but there is 
necessarily a loss of semantic expressiveness 
during NL-UNL encoding. The goal of the 
UNL is not, in principle, to fully preserve text 
meaning, but only its main components, i.e., 
those considered to be essential. However, 
there is no measurable account as to what is 
essential in the UNL Project. By convention, 
this is linked to what has been called the 
literal meaning, whi.ch is directly derived 
from interpreting the sentence surface 
structure. Therefore, there is no room to 
represent content hat is not directly mapped 
onto the NL syntactic-semantic licensed 
structures. 
The NL-UNL encoding tool, or UNL 
Encoder, is generic enough to handle all the 
29 
languages included in the Project. Apart from 
the (supposedly) universal knowledge-base, 
used to fill-in possible interlexical gaps when 
mapping is not precise, all other linguistic 
resources are language-dependent. The 
source grammar essentially guides the 
elicitation of the sentence semantic structure 
into its corresponding UNL structure, by 
determining RLs and ALs, always giving 
priority to information content. 
The UNL-NL decoding tool, or UNL 
Decoder, works in the opposite way to the 
Encoder. Besides the lexicon and the 
grammar, a cooccurrence dictionary is also 
used at this stage, to disentangle lexical 
choice. The target grammar is responsible for 
the semantic-syntactic mapping, now 
resolving semantic organization by making 
syntactic and dependence choices between 
UWs, taking RLs and ALs into account. 
5. Remarks on language-independence 
The main strength of the UNL Project 
rests on human expertise: language-specific 
aspects to be included in the multilingual 
KBMT system are handled by native 
speakers of that language, in an attempt o 
overcome the need of representing 
knowledge across several languages or 
cultures. It has been successful in developing 
NL-driven resources and processes by 
researchers all around the world. For 
example, the BP UNL lexicon has over 
65,000 entries that are categorized according 
to grammatical and some semantic features, 
and this will be extended considerably in the 
future to cover the Portuguese vocabulary to 
a greater extent. Up to the present ime, only 
decoding systems customized to each NL 
have been plugged into a general decoder 
skeleton (provided by the UNL Center) and 
have already been assessed, producing 
promising results. The BP decoder, for 
example, is able to produce outputs whose 
literal meaning is preserved in most cases 
(Martins et al, 1998b), using handcoded 
UNL expressions. Actually, to decode any 
UNL text, NL-UNL encoding has to be 
handmade, since customization of the UNL 
Encoder to each NL has not yet been 
undertaken in the project. In spite of the 
promising decoding results, a) output quality 
varies enormously with UNL sentences 
encoding, which can be different across 
distinct research groups; b) communicative 
aspects of information exchange on the web 
are not explored in depth, as it can be seen 
through the list of RLs or ALs. UNL is not 
knowledge intensive and there are no 
guidelines as to consistently recognize or 
extract such kind of information from the 
surface of the source texts. 
There are several reasons why 
interpretation and use of the UNL among the 
various teams are not uniform, including 
cultural aspects and syntax differences of the 
languages involved. Using English as the 
lingua franca for communication and 
cooperation among the research groups and 
as the knowledge representation language has 
also brought limitations into the Project, 
since it implies a non-desirable level of 
language-dependence. This is inevitable, 
however, for limitations definitely come 
along with the choice made. For example, 
attaching a NL word to a UW may be 
difficult, owing to the cross-references 
introduced by using English to convey UNL 
symbols. Resuming the example shown in 
Figure 1, this is the case of the UW "on" in 
(lb): the preposition 'on' fills in the position 
feature of the verb 'sit' and, thus, is 
represented in UNL correspondingly as the 
second term of the binary relation 'plc' and 
the first term of 'obj'. This, undoubtedly, is 
critical, for 'sit' can be juxtaposed to other 
prepositions leading to different meanings, 
which, in turn, may introduce different sets of 
binary relations, implying a high-level 
complexity in the UNL representation. As a 
result, languages whose syntactic structures 
deeply differ from the English ones may 
30 
present an additional level of complexity that 
makes mapping to/from UNL impossible or 
unrealistic. In this respect, we have not been 
facing many problems in fitting Portuguese 
structures with UNL ones, since Portuguese, 
like English, is an inflectional anguage that 
also employs prepositional constructions. 
However, prepositions in Portuguese may 
play considerably different roles compared to 
English. Various extensions of the English 
spatial prepositions "on", "over" and 
"above", for example, are subsumed in 
Portuguese by a single form "sobre" (which 
may also mean ..about). Therefore, in 
Portuguese, cats could be, at the same time, 
not only "on" but also "over" and "above" 
mats. Only world knowledge, associated to 
contextual indexes, both absent in the 
referred UNL hypergraph, could avoid the 
unsuited encodings The cat sat over the mat. 
or The cat sat above the mat. from the 
Portuguese sentence "O gato sentou sobre o 
tapete". 
Another problem related to the sentence 
The cat sat on the mat. refers to the existence 
of competing analyses: it is quite plausible 
that a UNL representation suggesting a noun 
phrase instead of a full sentence holds for this 
sentence. It so happens when the arc between 
'sitting' and 'cat' concepts are labeled by the 
RL 'obj', instead of the RL 'agt' in (1), as it 
is shown in Figure 1 a', yielding the UNL text 
shown in Figure lb'. 
o 
Figure la': UNL hypergraph representation f 
the English sentence "The cat sat on the mat." 
obj(sit. @entry. @past,cat. @def) 
plc(sit. @ entry. @ past,on) 
obj(on,mat. @def) 
Figure lb ' :  UNL linear representation f the 
English sentence "The cat sat on the mat." 
Both analyses are equally accurate and 
can lead to good NL surface expressions, 
although they refer to different semantic 
facts. Indeed, to define an object relationship 
between "sitting" and "cat" is to say that the 
cat was already sat before the beginning of 
the event (e,g., The cat sat on the mat ate the 
fish.). In this case, the animal does not 
actually perform the action, but is 
conditioned to it, the main performer position 
being empty, thus yielding the referred noun 
phrase. In Figure 1, instead, the cat on its 
own has taken the sitting position, therefore 
introducing an agent relationship. These two 
different semantic facts may correspond, in 
English, to a single surface structure. Indeed, 
(1) is orthographically identical to (1'). 
However, other languages (e.g., Portuguese) 
do behave differently. 
Although it is also possible to have, in 
Portuguese, the same surface structure 
corresponding to both UNL representations 
("sentado no tapete"), it is more feasible to 
have, for each case, completely different 
constructions. In the case depicted by Figure 
1, the UW "sit" would be associated to the 
verb "sentar" (corresponding to "to sit"). 
Thus, the generation result should be 
something like "O gato sentou no tapete" or 
"O gato sentado no tapete". On the other 
hand, for Figure 1', the same UW 'sit' would 
be generated in a completely different way, 
corresponding to the passive form of the 
Portuguese xpression "colocar sentado" (to 
be put in a sitted position), for which there is 
no adequate English surface xpression. 
Distinguishing such situations to cope 
with syntactic-semantic troublesome 
mappings, though interesting, is a highly 
31 
context-sensitive task, often surpassing 
sentence boundaries. UNL descriptions do 
not address such fine-grained level of 
meaning representation, being limited to 
meanings derived from context-free source 
sentences, even when context-freeness 
implies insufficient information. When this is 
not possible, UNL offers a default analysis 
for semantically ambiguous sentences, in 
which case we can say that the UNL 
representation is probabilistic, rather than 
deterministic. 
The _way we believe some of UNL 
limitations can-be  overcome and/or 
minimized is by designing a fully-fledged 
testing procedure to assess outputs of both 
decoder and encoder for the various 
languages. Since the same encoding and 
decoding procedures have been delivered to 
the UNL teams, it is possible that part of the 
set of rules or translation strategies of a given 
team may be interchangeable with another 
one from a different language. In this way, 
sharing procedures may become a warranty 
for common ground assessment of the varied 
models, in which case it may be possible to 
make eligible concurrent strategies equally 
available for the languages involved. 
Concerning the UNL means to 
disambiguate or proceed to reference 
resolution or other discourse figures, most of 
the troublesome occurrences are enclosed in 
the treatment issued by specialists and, thus, 
they are constrained to, and handled by, at the 
level of native speakers use. This measure 
can be somewhat fruitful, provided that each 
signatory of the Project finds a way to trace a 
UNL text back onto its own NL text or vice- 
versa, making a proper use of the UNL 
syntax or symbols. This, in fact, can be a 
good method to evaluate (de)coding: once a 
UNL code has been produced from any NL 
text, this code can be the source to decoding 
into the same NL, in order to compare the 
original NL text with the automatically 
generated one. Evaluation, in this case, can 
be carried out by the same research group 
responsible for both processes. 
Compared to other interlingua pproaches 
(e.g., Mikrokosmos, Gazelle, or Kant), the 
UNL Project is in a much earlier stage - most 
of those are over 10 years old, while the UNL 
one is about 3 years old - but it is much more 
ambitious than most of the current systems 
under construction. For UNL is actually a 
front-end to a many-to-many communication 
system, with no constraints that are normally 
inherent in MT systems. Since knowledge is 
specified by native speakers for each NL 
module, grammar, semantics and world 
knowledge can be well founded. Its 
limitations, from a conceptual viewpoint, are 
shared by most of its counterparts, as in 
treating text at the sentence level only. In 
addition, by no means is the UNL system 
committed to event replication as it is the 
case of human translation. Automatic 
strategies have no psychological motivation 
whatsoever and are solely based upon 
computer efficiency principles, namely time 
and space. 
Acknowledgments 
The development of resources for 
Brazilian Portuguese in the UNL Project has 
been sponsored by the Institute of Advanced 
Studies of the United Nations University. The 
authors are also grateful to CNPq and Finep 
(Brazil) for the financial support and to Mr. 
Tadao Takahashi, the coordinator of the 
Brazilian branch in the UNL Project. 
References 
Chomsky, N. (1957). Syntactic Structures. 
The Hague, Mouton. 
Chomsky, N. (1965). Aspects of the Theory of 
Syntax. MIT Press, Cambridge, MA. 
Fillmore, C. (1968). The case for case. In 
Bach, E. and Harms, R.T. (orgs.), 
Universals in linguistic theory, pp. 1-88. 
Rinehard and Winston, New York. 
32 
Frege, G. (1892). On Sinn and Bedeutung. In 
Beaney, M. (ed.), The Frege Reader. 
Blackwell Publishers, Malden, MA, 1997. 
Martins, R.T., Rino, L.H.M., Nunes, M.G.V. 
(1998a). As Regras Gramaticais para a 
Decodtfica~ao UNL-Portugu~s no Projeto 
UNL. Relat6rio T6cnico 67. Instituto de 
CiSncias Matem~iticas e da Computa~ao. 
Universidade de S~o Paulo, Sao Carlos. 
Martins, R.T.; Rino, L.H.M.; Nunes, 
M.G.V.; Oliveira Jr., O.N. (1998b). Can 
the syntactic realization be detached from 
the syntactic analysis during generation of 
natural ldnguage sentences? III Encontro 
para o processamento c mputacional da 
lingua portuguesa escrita e falada 
(PROPOR'98). Porto Alegre - RS. 
Novembro. 
Uchida, H. (1996). UNL: Universal 
Networking Language - An Electronic 
Language for Communication, 
Understanding, and Collaboration. 
UNU/IAS/UNL Center. Tokyo, Japan. 
33 
