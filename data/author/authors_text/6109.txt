Improving Chronological Sentence Ordering
by Precedence Relation
Naoaki OKAZAKI
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656,
Japan
okazaki@miv.t.u-tokyo.ac.jp
Yutaka MATSUO
AIST
2-41-6 Aomi, Koto-ku,
Tokyo 135-0064,
Japan
y.matsuo@carc.aist.go.jp
Mitsuru ISHIZUKA
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656,
Japan
ishizuka@miv.t.u-tokyo.ac.jp
Abstract
It is necessary to find a proper arrangement of sen-
tences in order to generate a well-organized sum-
mary from multiple documents. In this paper we de-
scribe an approach to coherent sentence ordering for
summarizing newspaper articles. Since there is no
guarantee that chronological ordering of extracted
sentences, which is widely used by conventional sum-
marization system, arranges each sentence behind
presupposed information of the sentence, we improve
chronological ordering by resolving antecedent sen-
tences of arranged sentences. Combining the re-
finement algorithm with topical segmentation and
chronological ordering, we address our experiment to
test the effectiveness of the proposed method. The
results reveal that the proposed method improves
chronological sentence ordering.
1 Introduction
The growth of computerized documents enables
us to find relevant information easily owing to
technological advances in Information Retrieval.
Although it is convenient that we can obtain a
great number of documents with a search en-
gine, this situation also presents the information
pollution problem: ?Who is willing to take the
tedious burden of reading all those text docu-
ments?? Automatic text summarization (Mani,
2001), is one solution to the problem, providing
users with a condensed version of the original
text.
Most existing summarization systems make
use of sentence or paragraph extraction, which
finds significant textual segments in source doc-
uments, and compile them in a summary. After
we select significant sentences as a material for
a summary, we must find a proper arrangement
of the sentences and edit each sentence by delet-
ing unnecessary parts or inserting necessary ex-
pressions. Although there has been a great deal
of research on extraction since the early stage
of natural language processing (Luhn, 1958),
research on post-processing of automatic sum-
marization is relatively small in number. It is
essential to pay attention to sentence ordering
in case of multi-document summarization. Sen-
tence position in the original document, which
yields a good clue to sentence arrangement for
single-document summarization, is not enough
for multi-document summarization because we
must consider inter-document order at the same
time.
In this paper we propose an approach to co-
herent text structuring for summarizing news-
paper articles. We improve chronological order-
ing, which is widely used by conventional sum-
marization system, complementing presupposed
information of each sentence. The rest of this
paper is organized as follows. We first review
the sentence ordering problem and present our
approach to generate an acceptable ordering in
the light of coherence relation. The subsequent
section (Section 3) addresses evaluation metrics
and experiment results. In Section 4 we discuss
future work and conclude this paper.
2 Sentence Ordering
2.1 Sentence ordering problem
Our goal is to determine the most probable per-
mutation of given sentences and to generate a
well-structured text. When a human is asked
to make an arrangement of sentences, he or she
may perform this task without difficulty just
as we write out thoughts in a text. However,
we must consider what accomplishes this task
since computers are unaware of order of things
by nature. Discourse coherence as typified by
rhetorical relation (Mann and Thompson, 1988)
and coherence relation (Hobbs, 1990) is of help
to this question. Hume (Hume, 1748) claimed
that qualities from which association arises and
by which the mind is conveyed from one idea
to another are three: resemblance; contiguity
in time or place; and cause and effect. That
is to say we should organize a text from frag-
c) Dolly gave birth to two children in her life.
b) The father is of a different kind and Dolly 
    had been pregnant for about five months.
a) Dolly the clone sheep was born in 1996.
Sentences Preferred ordering
[a-c-b]
Refinement
Chronological order
Figure 1: A chronological ordering is not enough.
mented information on the basis of topical rele-
vancy, chronological sequence, and cause-effect
relation. It is especially true in sentence order-
ing of newspaper articles because we must ar-
range a large number of time-series events con-
cerning several topics.
Barzilay et al (Barzilay et al, 2002) address
the problem of sentence ordering in the context
of multi-document summarization and the im-
pact of sentence ordering on readability of a
summary. They proposed two naive sentence-
ordering techniques such as majority ordering
(examines most frequent orders in the original
documents) and chronological ordering (orders
sentence by the publication date). Showing that
using naive ordering algorithms does not pro-
duce satisfactory orderings, Barzilay et al also
investigates through experiments with humans,
how to identify patterns of orderings that can
improve the algorithm. Based on the exper-
iments, they propose another algorithm that
utilizes chronological ordering with topical seg-
mentation to separate sentences referring to a
topic from ones referring to another.
Lapata (Lapata, 2003) proposes another ap-
proach to information ordering based on a prob-
abilistic model that assumes the probability of
any given sentence is determined by its adja-
cent sentence and learns constraints on sen-
tence order from a corpus of domain specific
texts. Lapata estimates transitional probabil-
ity between sentences by some attributes such
as verbs (precedence relationships of verbs in
the corpus), nouns (entity-based coherence by
keeping track of the nouns) and dependencies
(structure of sentences).
2.2 Improving chronological ordering
Against the background of these studies, we
propose the use of antecedence sentences to ar-
range sentences. Let us consider an example
shown in Figure 1. There are three sentences a,
b, and c from which we get an order [a-b-c]
by chronological ordering. When we read these
sentences in this order, we find sentence b to
be incorrectly positioned. This is because sen-
tence b is written on the presupposition that
the reader may know that Dolly had a child. In
other words, it is more fitting to assume sen-
tence b to be an elaboration of sentence c. As
one may easily imagine, there are some prece-
dent sentences prior to sentence b in the origi-
nal document. Lack of presupposition obscures
what a sentence is saying and confuses the read-
ers. Hence, we should refine the chronological
order and revise the order to [a-c-b], putting
sentence c before sentence b.
We show a block diagram of our ordering al-
gorithm shown in Figure 2. Given nine sen-
tences denoted by [a b ... i], for example,
the algorithm eventually produces an order-
ing, [a-b-f-c-i-g-d-h-e]. We consider top-
ical segmentation and chronological ordering to
be fundamental to sentence ordering as well as
conventional ordering techniques (Barzilay et
al., 2002) and make an attempt to refine the
ordering. We firstly recognize topics in source
documents to separate sentences referring to a
topic from ones referring to another. In Fig-
ure 2 example we obtain two topical segments
(clusters) as an output from the topical cluster-
ing. In the second phase we order sentences of
each segment by the chronological order. If two
sentences have the same chronological order, we
elaborate the order on the basis of sentence po-
sition and resemblance relation. Finally, we re-
fine each ordering by resolving antecedent sen-
tences and output the final ordering. In the rest
of this section we give a detailed description of
each phase.
2.3 Topical clustering
The first task is to categorize sentences by their
topics. We assume a newspaper article to be
written about one topic. Hence, to classify top-
ics in sentences, we have only to classify articles
a ab bc c
d
d
e
e
f
f
g
g
h hi
i
abc
he
ig
d
f
abf
dh
ci
e
g
Topical clustering by documents
Chronological ordering 
with resemblance relation
Ordering refinement
by precedence relation
Cluster #1
Unorderedsentences Orderedsentences
Cluster #2
Figure 2: The outline of the ordering algorithm.
by their topics. Given l articles and we found
m kinds of terms in the articles. Let D be a
document-term matrix (l ?m), whose element
Dij represents frequency of a term #j in doc-
ument #i, We use Di to denote a term vector
(i-component row vector) of document #i. Af-
ter measuring distance or dissimilarity between
two articles #x and #y:
distance(Dx, Dy) = 1? Dx ?Dy|Dx||Dy| , (1)
we apply the nearest neighbor method (Cover
and Hart, 1967) to merge a pair of clusters
when their minimum distance is lower than a
given parameter ? = 0.3 (determined empiri-
cally). At last we classify sentences according
to topical clusters, assuming that a sentence in
a document belonging to a cluster also belongs
to the same cluster.
2.4 Chronological ordering
It is difficult for computers to find a resemblance
or cause-effect relation between two phenom-
ena while we do not have conclusive evidence
whether a pair of sentences gathered arbitrarily
from multiple documents has some relation. A
newspaper usually deals with novel events that
have occurred since the last publication. Hence,
publication date (time) of each article turns out
to be a good estimator of resemblance relation
(i.e., we observe a trend or series of relevant
events in a time period), contiguity in time, and
cause-effect relation (i.e., an event occurs as a
result of previous events). Although resolving
temporal expressions in sentences (e.g., yester-
day, the next year, etc.) (Mani and Wilson,
2000; Mani et al, 2003) may give a more pre-
cise estimation of these relations, it is not an
easy task. For this reason we order sentences
of each segment (cluster) by the chronological
a
.
.
c'
.
b
c
.
Article #1 Article #2 Article #3
chronological order
Figure 3: Background idea of ordering refine-
ment by precedence relation.
order, assigning a time stamp for each sentence
by its publication date (i.e., the date when the
article was written).
When there are sentences having the same
time stamp, we elaborate the order on the ba-
sis of sentence position and sentence connectiv-
ity. We restore an original ordering if two sen-
tences have the same time stamp and belong
to the same article. If sentences have the same
time stamp and are not from the same article,
we arrange a sentence which is more similar to
previously ordered sentences to assure sentence
connectivity.
2.5 Ordering refinement by precedence
relation
After we obtain an ordering of a topical seg-
ment by chronological ordering, we improve it
as shown in Figure 1 based on antecedence sen-
tences. Figure 3 shows the background idea
of ordering refinement by precedence relation.
Just as in the example in Figure 1, we have
three sentences a, b, and c in chronological or-
der. At first we get sentence a out of the sen-
tences and check its antecedent sentences. See-
ing that there are no sentences prior to sentence
a in article #1, we accept to put sentence a
here. Then we get sentence b out of remaining
sentences and check its antecedent sentences.
We find several sentences before sentence b in
article #2 this time. Grasping what the an-
tecedent sentences are saying, we confirm first
of all whether what they are saying is mentioned
by previously arranged sentences (i.e., sentence
a). If it is mentioned, we put sentence b here
and extend the ordering to [a-b]. Otherwise,
we search a substitution for what the precedence
sentences are saying from the remaining sen-
tences (i.e., sentence c in this example). In the
Figure 3 example, we find out that sentence a is
not referring to what sentence c? is saying but
sentence c is approximately referring to that.
Start
End
a
b
ef
e f
c
d
f
f
d
df
d f
e
de
d e
f
No precedent sentences before 
sentence a. Choose a.
Choose the rest, sentence f.
The refined ordering is
a-b-e-c-d-f.
No precedent sentences before 
sentence b. Choose b.
There are precedent sentences 
before sentence c.
Search a shortest path from c 
to b and a. We found sentence 
e to be the closest to the 
precedent sentences of c.
Search a shortest path from e 
to b and a. No precedent 
sentences before e. Choose e.
We find a path from c to b and 
a via e is the shortest.
There are precedent sentences 
before sentence d.
Search a shortest path from d 
to c, e, b and a. We find the 
direct path from d to c is the 
shortest.
0
0
.2
.7
.6 1
0
0
.4.8
(1)(1)
(2)
(3)
(3)
(4)
(5)
(6)
(7)
(8)(8)
(6)(7)
(5)
(4)
(2)
Figure 4: Ordering refinement by precedence relation as a shortest path problem.
Putting sentence c before b, we finally get the
refined ordering [a-c-b].
Supposing that sentence c mentions similar
information as c? but expresses more than c?,
it is nothing unusual that an extraction method
does not choose sentence c? but sentence c.
Because a method for multi-document summa-
rization (e.g., MMR (Carbonell and Goldstein,
1998)) makes effort to acquire information cov-
erage and refuse redundant information at the
same time, it is quite natural that the method
does not choose both sentence c? and c in terms
of redundancy and prefers sentence c as c? in
terms of information coverage.
Figure 4 illustrates how the algorithm refines
a given chronological ordering [a-b-c-d-e-f].
We define distance as a dissimilarity value of
precedent information of a sentence. When
a sentence has antecedent sentences and their
content is not mentioned by previously arranged
sentences, this distance will be high. When a
sentence has no precedent sentences, we define
the distance to be 0. In the example shown
in Figure 4 example we do not change posi-
tion of sentences a and b because they do not
have precedent sentences (i.e., they are lead sen-
tences). On the other hand, sentence c has
some precedent sentences in its original docu-
ment. Preparing a term vector of the precedent
sentences, we calculate how much the precedent
content is covered by other sentences using dis-
tance defined above. In Figure 4 example the
distance from sentence a and b to c is high
(distance = 0.7). We search a shortest path
from sentence c to sentences a and b by best-
first search in order to find suitable sentences
before sentence c. Given that sentence e in Fig-
ure 4 describes similar content as the precedent
sentences of sentence c and is a lead sentence,
we trace the shortest path from sentence c to
sentences a and b via sentence e. We extend
the resultant ordering to [a-b-e-c], inserting
sentence e before sentence c. Then we consider
sentence d, which is not a lead sentence again
(distance = 0.4). Preparing a term vector of the
precedent sentences of sentence d, we search a
shortest path from sentence d to sentences a,
b, c, and e. The search result shows that we
should leave sentence d this time because the
precedent content seems to be described in sen-
tences a, b, c, and e better than f. In this way
we get the final ordering, [a-b-e-c-d-f].
3 Evaluation
In this section we describe our experiment to
test the effectiveness of the proposed method.
3.1 Experiment and evaluation metrics
We conducted an experiment of sentence order-
ing through multi-document summarization to
test the effectiveness of the proposed method.
We utilized the TSC-3 (Hirao et al, to appear in
2004) test collection, which consists of 30 sets of
multi-document summarization tasks. For more
information about TSC-3 task, see the work-
shop proceedings. Performing an important
sentence extraction (Okazaki et al, to appear
in 2004) up to the specified number of sentences
(approximately 10% of summarization rate), we
made a material for a summary (i.e., extracted
sentences) for each task. We order the sentences
by six methods: human-made ordering (HO) as
the highest anchor; random ordering (RO) as
the lowest anchor; chronological ordering (CO)
(i.e., phase 2 only); chronological ordering with
topical segmentation (COT) (i.e., phases 1 and
2); proposed method without topical segmenta-
tion (PO) (i.e., phases 2 and 3); and proposed
method with topical segmentation (POT)). We
asked human judges to evaluate sentence order-
ing of these summaries.
The first evaluation task is a subjective grad-
ing where a human judge marks an ordering of
summary sentences on a scale of 4: 4 (perfect), 3
(acceptable), 2 (poor), and 1 (unacceptable). We
give a clear criterion of scoring to the judges as
follows. A perfect summary is a text that we
cannot improve any further by re-ordering. An
acceptable summary is a one that makes sense
and is unnecessary to be revised even though
there may be some room for improvement in
terms of readability. A poor summary is a one
that loses a thread of the story at some places
and requires minor amendment to bring it up to
the acceptable level. An unacceptable summary
is a one that leaves much to be improved and
requires overall restructuring rather than par-
tial revision. Additionally, we inform the judges
that summaries were made of the same set of
extracted sentences and only sentence ordering
made differences between the summaries in or-
der to avoid any disturbance in rating.
In addition to the rating, it is useful that we
examine how close an ordering is to an accept-
able one when the ordering is regarded as poor.
Considering that several sentence-ordering pat-
terns are acceptable for a given summary, we
An ordering to evaluate: 
The corrected ordering:
s5, s6, s7, s8, s1, s2, s9, s3, s4
s5, s6, s7, s9, s2, s8, s1, s3, s4
( )
)(
Correction by move operation
A judge is supposed to show how to improve an ordering.
The judge's reading is interupted before the points marked with black circles.
Figure 5: Correction of an ordering.
think that it is valuable to measure the degree of
correction because this metric virtually requires
a human corrector to prepare a correct answer
for each ordering in his or her mind. Therefore,
a human judge is supposed to illustrate how to
improve an ordering of a summary when he or
she marks the summary with poor in the rat-
ing task. We restrict applicable operations of
correction to move operation so as to keep the
minimum correction of the ordering. We define
a move operation here as removing a sentence
and inserting the sentence into an appropriate
place (see Figure 5).
Supposing a sentence ordering to be a rank,
we can calculate rank correlation coefficient of a
permutation of an ordering pi and a permutation
of the reference ordering ?. Let {s1, ..., sn} be a
set of summary sentences identified with index
numbers from 1 to n. We define a permutation
pi ? Sn to denote an ordering of sentences where
pi(i) represents an order of sentence si. Simi-
larly, we define a permutation ? ? Sn to denote
the corrected ordering. For example, the pi and
? in Figure 5 will be:
pi =
( 1 2 3 4 5 6 7 8 9
5 6 8 9 1 2 3 4 7
)
, (2)
? =
( 1 2 3 4 5 6 7 8 9
7 5 8 9 1 2 3 6 4
)
. (3)
Spearman?s rank correlation ?s(pi, ?) and
Kendall?s rank correlation ?k(pi, ?) are known
as famous rank correlation metrics.
?s(pi, ?) = 1? 6n(n+ 1)(n? 1)
n?
i=1
(pi(i)? ?(i))2
(4)
?k(pi, ?) = 1n(n? 1)/2 ?
n?1?
i=1
n?
j=i+1
sgn(pi(j)? pi(i)) ? sgn(?(j)? ?(i)), (5)
4 3 2 1
RO 0.0 0.0 6.0 94.0
CO 13.1 22.6 63.1 1.2
COT 10.7 22.6 61.9 4.8
PO 16.7 38.1 45.2 0.0
POT 15.5 36.9 44.0 3.6
HO 52.4 21.4 26.2 0.0
Table 1: Distribution of rating score of order-
ings in percent figures.
where sgn(x) = 1 for x > 0 and ?1 otherwise.
These metrics range from ?1 (an inverse rank)
to 1 (an identical rank) via 0 (a non-correlated
rank). In the example shown in Equations 2 and
3 we obtain ?s(pi, ?) = 0.85 and ?k(pi, ?) = 0.72.
We propose another metric to assess the de-
gree of sentence continuity in reading, ?c(pi, ?):
?c(pi, ?) = 1n
n?
i=1
eq
(
pi??1(i), pi??1(i? 1) + 1
)
,
(6)
where: pi(0) = ?(0) = 0; eq(x, y) = 1 when x
equals y and 0 otherwise. This metric ranges
from 0 (no continuity) to 1 (identical). The
summary in Figure 5 may interrupt judge?s
reading after sentence S7, S1, S2 and S9 as he
or she searches a next sentence to read. Hence,
we observe four discontinuities in the order-
ing and calculate sentence continuity ?c(pi, ?) =
(9? 4)/9 = 0.56.
3.2 Results
Table 1 shows distribution of rating score of
each method in percent figures. Judges marked
about 75% of human-made ordering (HO) as ei-
ther perfect or acceptable while they rejected as
many as 95% of random ordering (RO). Chrono-
logical ordering (CO) did not yield satisfactory
result losing a thread of 63% summaries al-
though CO performed much better than RO.
Topical segmentation could not contribute to
ordering improvement of CO as well: COT is
slightly worse than CO. After taking an in-
depth look at the failure orderings, we found
the topical clustering did not perform well dur-
ing this test. We suppose the topical clustering
could not prove the merits with this test collec-
tion because the collection consists of relevant
articles retrieved by some query and polished
well by a human so as not to include unrelated
articles to a topic.
On the other hand, the proposed method
(PO) improved chronological ordering much
better than topical segmentation. Note that the
sum of perfect and acceptable ratio jumped up
from 36% (CO) to 55% (PO). This shows the
ordering refinement by precedence relation im-
proves chronological ordering by pushing poor
ordering to an acceptable level.
Table 2 reports closeness of orderings to the
corrected ones with average scores (AVG) and
the standard deviations (SD) of the three met-
rics ?s, ?k and ?c. It appears that average figures
shows similar tendency to the rating task with
three measures: HO is the best; PO is better
than CO; and RO is definitely the worst. We
applied one-way analysis of variance (ANOVA)
to test the effect of four different methods (RO,
CO, PO and HO). ANOVA proved the effect of
the different methods (p < 0.01) for three met-
rics. We also applied Tukey test to compare the
difference between these methods. Tukey test
revealed that RO was definitely the worst with
all metrics. However, Spearman?s rank correla-
tion ?S and Kendall?s rank correlation ?k failed
to prove the significant difference between CO,
PO and HO. Only sentence continuity ?c proved
PO is better than CO; and HO is better than
CO (? = 0.05). The Tukey test proved that
sentence continuity has better conformity to the
rating results and higher discrimination to make
a comparison.
Table 3 shows closeness of orderings to ones
made by human (all results of HO should be 1
by necessity). Although we found RO is clearly
the worst as well as other results, we cannot find
the significant difference between CO, PO, and
HO with all metrics. This result presents to the
difficulty of automatic evaluation by preparing
one correct ordering.
4 Conclusions
In this paper we described our approach to co-
herent sentence ordering for summarizing news-
paper articles. We conducted an experiment
of sentence ordering through multi-document
summarization. The proposed method which
utilizes precedence relation of sentence archived
good results, raising poor chronological order-
ings to an acceptable level by 20%. We also pro-
posed an evaluation metric that measures sen-
tence continuity and a amendment-based eval-
uation task. The amendment-based evalua-
tion outperformed the evaluation that compares
an ordering with an answer made by a hu-
man. The sentence continuity metric applied to
the amendment-based task showed more agree-
Spearman Kendall Continuity
Method AVG SD AVG SD AVG SD
RO 0.041 0.170 0.035 0.152 0.018 0.091
CO 0.838 0.185 0.870 0.270 0.775 0.210
COT 0.847 0.164 0.791 0.440 0.741 0.252
PO 0.843 0.180 0.921 0.144 0.856 0.180
POT 0.851 0.158 0.842 0.387 0.820 0.240
HO 0.949 0.157 0.947 0.138 0.922 0.138
Table 2: Comparison with corrected ordering.
Spearman Kendall Continuity
Method AVG SD AVG SD AVG SD
RO -0.117 0.265 -0.073 0.202 0.054 0.064
CO 0.838 0.185 0.778 0.198 0.578 0.218
COT 0.847 0.164 0.782 0.186 0.571 0.229
PO 0.843 0.180 0.792 0.184 0.606 0.225
POT 0.851 0.158 0.797 0.171 0.599 0.237
HO 1.000 0.000 1.000 0.000 1.000 0.000
Table 3: Comparison with human-made ordering.
ments with the rating result.
We plan to do further study on the sentence
ordering problem in future work, exploring how
to apply our algorithm to documents other than
newspaper or integrate ordering problem with
extraction problem to improve each other. We
also recognize the necessity to establish an auto-
matic evaluation method of sentence ordering.
Acknowledgments
We made use of Mainichi Newspaper and Yomi-
uri Newspaper articles and summarization test
collection of TSC-3.
References
R. Barzilay, E. Elhadad, and K. McKeown.
2002. Inferring strategies for sentence order-
ing in multidocument summarization. Jour-
nal of Artifical Intelligence Research (JAIR),
17:35?55.
J. Carbonell and J. Goldstein. 1998. The use of
MMR, diversity-based reranking for reorder-
ing documents and producing summaries.
In Proceedings of the 21st Annual Interna-
tional ACM-SIGIR Conference on Research
and Development in Information Retrieval,
pages 335?336.
T. M. Cover and P. E. Hart. 1967. Nearest
neighbor pattern classification. IEEE Trans-
actions on Information Theory, IT-13:21?27.
T. Hirao, T. Fukusima, M. Okumura, and
H. Nanba. to appear in 2004. Text summa-
rization challenge 3: text summarization eval-
uation at ntcir workshop4. In Working note
of the 4th NTCIR Workshop Meeting.
J. Hobbs. 1990. Literature and Cognition, CSLI
Lecture Notes 21. CSLI.
D. Hume. 1748. Philosophical Essays concern-
ing Human Understanding.
M. Lapata. 2003. Probabilistic text structur-
ing: experiments with sentence ordering. In
Proceedings of the 41st Meeting of the Asso-
ciation of Computational Linguistics, pages
545?552.
H. P. Luhn. 1958. The automatic creation of
literature abstracts. IBM Journal of Research
and Development, 2(2):159?165.
I. Mani and G. Wilson. 2000. Robust temporal
processing of news. In Proceedings of the 38th
Annual Meeting of ACL?2000, pages 69?76.
I. Mani, B. Schiffman, and J. Zhang. 2003. In-
ferring temporal ordering of events in news.
Proceedings of the Human Language Technol-
ogy Conference (HLT-NAACL) ?03.
I. Mani. 2001. Audomatic Summarization.
John Benjamins.
W. Mann and S. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory
of text organization. Text, 8:243?281.
N. Okazaki, Y. Matsuo, and M. Ishizuka. to
appear in 2004. TISS: An integrated summa-
rization system for TSC-3. In Working note
of the 4th NTCIR Workshop Meeting.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 657?664
Manchester, August 2008
A Discriminative Alignment Model for Abbreviation Recognition
Naoaki Okazaki
?
okazaki@is.s.u-tokyo.ac.jp
Sophia Ananiadou
?
sophia.ananiadou@manchester.ac.uk
Jun?ichi Tsujii
??
tsujii@is.s.u-tokyo.ac.jp
?
Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
?
School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
Abstract
This paper presents a discriminative align-
ment model for extracting abbreviations
and their full forms appearing in actual
text. The task of abbreviation recognition
is formalized as a sequential alignment
problem, which finds the optimal align-
ment (origins of abbreviation letters) be-
tween two strings (abbreviation and full
form). We design a large amount of fine-
grained features that directly express the
events where letters produce or do not pro-
duce abbreviations. We obtain the optimal
combination of features on an aligned ab-
breviation corpus by using the maximum
entropy framework. The experimental re-
sults show the usefulness of the alignment
model and corpus for improving abbrevia-
tion recognition.
1 Introduction
Abbreviations present two major challenges in nat-
ural language processing: term variation and am-
biguity. Abbreviations substitute for expanded
terms (e.g., dynamic programming) through the
use of shortened term-forms (e.g., DP). At the
same time, the abbreviation DP appearing alone in
text is ambiguous, in that it may refer to different
concepts, e.g., data processing, dirichlet process,
differential probability. Associating abbreviations
and their full forms is useful for various applica-
tions including named entity recognition, informa-
tion retrieval, and question answering.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The task of abbreviation recognition, in which
abbreviations and their expanded forms appearing
in actual text are extracted, addresses the term vari-
ation problem caused by the increase in the num-
ber of abbreviations (Chang and Sch?utze, 2006).
Furthermore, abbreviation recognition is also cru-
cial for disambiguating abbreviations (Pakhomov,
2002; Gaudan et al, 2005; Yu et al, 2006), pro-
viding sense inventories (lists of abbreviation def-
initions), training corpora (context information of
full forms), and local definitions of abbreviations.
Hence, abbreviation recognition plays a key role in
abbreviation management.
Numerous researchers have proposed a variety
of heuristics for recognizing abbreviation defini-
tions, e.g., the use of initials, capitalizations, syl-
lable boundaries, stop words, lengths of abbrevia-
tions, and co-occurrence statistics (Park and Byrd,
2001; Wren and Garner, 2002; Liu and Fried-
man, 2003; Okazaki and Ananiadou, 2006; Zhou
et al, 2006; Jain et al, 2007). Schwartz and
Hearst (2003) implemented a simple algorithm that
finds the shortest expression containing all alpha-
numerical letters of an abbreviation. Adar (2004)
presented four scoring rules to choose the most
likely expanded form in multiple candidates. Ao
and Takagi (2005) designed more detailed condi-
tions for accepting or discarding candidates of ab-
breviation definitions.
However, these studies have limitations in dis-
covering an optimal combination of heuristic rules
from manual observations of a corpus. For exam-
ple, when expressions transcrip
:
tion factor 1 and
thyroid transcription factor 1 are full-form can-
didates for the abbreviation TTF-1
1
, an algorithm
should choose the latter expression over the shorter
1
In this paper, we use straight and
::::
wavy underlines to rep-
resent correct and incorrect origins of abbreviation letters.
657
expression (former). Previous studies hardly han-
dle abbreviation definitions where full forms (e.g.,
water activity) shuffle their abbreviation letters
(e.g., AW). It is also difficult to reject ?negative?
definitions in a text; for example, an algorithm
should not extract an abbreviation definition from
the text, ?the replicon encodes a large
:::
replic
:
ation
protein (RepA),? since RepA provides a descrip-
tion of the protein rather than an abbreviation.
In order to acquire the optimal rules from
the corpora, several researchers applied machine
learning methods. Chang and Sch?utze (2006) ap-
plied logistic regression to combine nine features.
Nadeau and Turney (2005) also designed seven-
teen features to classify candidates of abbrevia-
tion definitions into positive or negative instances
by using the Support Vector Machine (SVM).
Notwithstanding, contrary to our expectations, the
machine-learning approach could not report better
results than those with hand-crafted rules.
We identify the major problem in the previ-
ous machine-learning approach: these studies did
not model associations between abbreviation let-
ters and their origins, but focused only on indirect
features such as the number of abbreviation letters
that appear at the head of a full form. This was
probably because the training corpus did not in-
clude annotations on the exact origins of abbrevia-
tion letters but only pairs of abbreviations and full
forms. It was thus difficult to design effective fea-
tures for abbreviation recognition and to reuse the
knowledge obtained from the training processes.
In this paper, we formalize the task of abbrevi-
ation recognition as a sequential alignment prob-
lem, which finds the optimal alignment (origins of
abbreviation letters) between two strings (abbrevi-
ation and full form). We design a large amount
of features that directly express the events where
letters produce or do not produce abbreviations.
Preparing an aligned abbreviation corpus, we ob-
tain the optimal combination of the features by us-
ing the maximum entropy framework (Berger et
al., 1996). We report the remarkable improve-
ments and conclude this paper.
2 Proposed method
2.1 Abbreviation alignment model
We express a sentence x as a sequence of letters
(x
1
, ..., x
L
), and an abbreviation candidate y in the
sentence as a sequence of letters (y
1
, ..., y
M
). We
define a letter mapping a = (i, j) to indicate that
the abbreviation letter y
j
is produced by the letter
in the full form x
i
. A null mapping a = (i, 0) indi-
cates that the letter in the sentence x
i
is unused to
form the abbreviation. Similarly, a null mapping
a = (0, j) indicates that the abbreviation letter y
j
does not originate from any letter in x. We de-
fine a
(x)
and a
(y)
in order to represent the first and
second elements of the letter mapping a. In other
words, a
(x)
and a
(y)
are equal to i and j respec-
tively, when a = (i, j). Finally, an abbreviation
alignment a is defined as a sequence of letter map-
pings, a = (a
1
, ..., a
T
), where T represents the
number of mappings in the alignment.
Let us consider the following example sentence:
We investigate the effect of thyroid tran-
scription factor 1 (TTF-1).
This sentence contains an abbreviation candidate
TTF-1 in parentheses
2
. Figure 1 illustrates the
correct alignment a (bottom line) and its two-
dimensional representation for the example sen-
tence
3
; the abbreviation letters ?t,? ?t,? ?f,? ?-,? and
?1? originate from x
30
, x
38
, x
52
, nowhere (null
mapping), and x
59
respectively.
We directly model the conditional probability of
the alignment a, given x and y, using the maxi-
mum entropy framework (Berger et al, 1996),
P (a|x,y) =
exp {? ? F (a,x,y)}
?
a?C(x,y)
exp {? ? F (a,x,y)}
.
(1)
In Formula 1, F = {f
1
, ..., f
K
} is a global feature
vector whose elements present K feature func-
tions, ? = {?
1
, ..., ?
K
} denotes a weight vector
for the feature functions, and C(x,y) yields a set
of possible alignments for the given x and y. We
obtain the following decision rule to choose the
most probable alignment a? for given x and y,
a? = argmax
a?C(x,y)
P (a|x,y). (2)
Note that a set of possible alignments C(x,y)
always includes a negative alignment whose ele-
ments are filled with null-mappings (refer to Sec-
tion 2.3 for further detail). This allows the formula
to withdraw the abbreviation candidate y when any
expression in x is unlikely to be a definition.
2
Refer to Section 3.1 for text makers for abbreviations.
3
We ignore non-alphabetical letters in abbreviations.
658
    We investigate the effect of thyroid transcription factor 1 (TTF-1) ...x:
a
            ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~ ~ 
8 words 4 lettersmin(|y|+5, 2|y|)
<NIL> T  T  F  -   1  <SF>
y:
0 0 0 0 0 0 0 0 0
1
2
3
5
91 13 16 2122 25 28 30 38 47 52 55 59 61
1
0
1
2
3
4
5
6
t =
i
 j
a =
a 
2 3 4 5 6 7 8 9 10 11 12 13 14
Null outsideOther positions Abbreviation Null inside Associate inside
a = ((9,0), (13,0), (16,0), (21,0), (22, 0), (25,0), (28,0), (30,1), (38,2), (47,0), (52,3), (55,0), (59,5), (61,6))
Figure 1: The correct alignment for the example sentence and its two dimensional representation.
2.2 Features
The main advantage of the discriminative align-
ment model is its ability to incorporate a wide
range of non-independent features. Inspired
by feature engineering for Conditional Random
Fields (CRFs) (Lafferty et al, 2001), we design
two kinds of features: unigram (state) features de-
fined on each letter mapping, and bigram (tran-
sition) features defined on each pair of adjacent
letter mappings. Given a triplet, a, x, and y, a
global feature function f
k
(a,x,y) ? F sums up
the boolean values (0 or 1) of the corresponding
local feature g
k
(a,x,y, t) at t ? {1, ..., T},
f
k
(a,x,y) =
T
?
t=1
g
k
(a,x,y, t). (3)
In other words, f
k
(a,x,y) counts the number of
times the local feature is fired in the alignment a.
A unigram feature corresponds to the observa-
tion at x
i
and y
j
associated by a mapping a
t
=
(i, j). A unigram feature encodes the condition
where the letter in the full form x
i
is chosen or
unchosen for producing the abbreviation letter y
j
.
For example, we may infer from the letter mapping
at a
8
= (30, 1) in Figure 1, that x
30
is mapped to
y
1
because: x
30
is at the head of the word, y
1
is a
capital letter, and both x
30
and y
1
are at the head
of the word and abbreviation.
Bigram features, combining two observations at
a
s
and a
t
(1 ? s < t ? T ), are useful in capturing
the common characteristics shared by an abbrevi-
ation definition. For instance, we may presume in
Figure 1 that the head letters in the full form might
be selectively used for producing the abbreviation,
based on the observations at a
8
= (30, 1) and
a
9
= (38, 2). In order to focus on the conditions
for consecutive non-null mappings, we choose the
previous position s for the given t.
s =
?
?
?
t? 1
(
a
t(y)
= 0 ? ?u : a
u(y)
= 0
)
max
1?u<t
{
u | a
u(y)
6= 0
}
(otherwise)
(4)
Formula 4 prefers the non-null mapping that is the
most adjacent to t over the previous mapping (t ?
1). In Figure 1, transitions a
9
?a
11
and a
11
?a
13
exist for this reason.
In this study, we express unigram and bi-
gram features with atomic functions (Table 1)
that encode observation events of x
a
t(x)
, y
a
t(y)
,
a
t
, x
a
s(x)
?x
a
t(x)
, and y
a
s(y)
?y
a
t(y)
. Atomic
functions x ctype, y ctype, x position, and
y position present common heuristics used by
previous studies. The function x word examines
the existence of stop words (e.g., the, of, in) to
prevent them from producing abbreviation letters.
We also include x pos (part-of-speech of the word)
since a number of full forms are noun phrases.
Functions x diff , x diff wd, and y diff are de-
signed specifically for bigram features, receiving
two positions s and t in their arguments. The
function x diff mainly deals with abbreviation def-
initions that include consecutive letters of their
full forms, e.g., amplifier (AMP). The function
659
Function Return value
x ctype
?
(a,x, t) x
a
t(x)
+?
is {U (uppercase), L (lowercase), D (digit), W (whitespace), S (symbol) } letter
x position
?
(a,x, t) x
a
t(x)
+?
is at the {H (head), T (tail), S (syllable head), I (inner), W (whitespace) } of the word
x char
?
(a,x, t) The lower-cased letter of x
a
t(x)
+?
x word
?
(a,x, t) The lower-cased word (offset position ?) containing the letter x
a
t(x)
x pos
?
(a,x, t) The part-of-speech code of the word (offset position ?) containing the letter x
a
t(x)
y ctype(a,y, t) y
a
t(y)
is {N (NIL) U (uppercase), L (lowercase), D (digit), S (symbol) } letter
y position(a,y, t) y
a
t(y)
is at the {N (NIL) H (head), T (tail), I (inner)} of the word
y char(a,y, t) The lower-cased letter of y
a
t(y)
a state(a,y, t) {SKIP (a
t(y)
= 0),MATCH (1 ? a
t(y)
? |y|),ABBR (a
t(y)
= |y|+ 1)}
x diff(a,x, s, t) (a
t(x)
? a
s(x)
) if letters x
a
t(x)
and x
a
s(x)
are in the same word, NONE otherwise
x diff wd(a,x, s, t) The number of words between x
a
t(x)
and x
a
s(x)
y diff(a,y, s, t) (a
t(y)
? a
s(y)
)
Table 1: Atomic functions to encode observation events in x and y
Combination Rules
unigram(t) xy unigram(t)? {a state(t)}
xy unigram(t) x unigram(t)? y unigram(t)?
(
x unigram(t)? y unigram(t)
)
x unigram(t) x state
0
(t)? x state
?1
(t)? x state
1
(t)
?
(
x state
?1
(t)? x state
0
(t)
)
?
(
x state
0
(t)? x state
1
(t)
)
y unigram(t)
{
y ctype(t), y position(t), y ctype(t)y position(t)
}
x state
?
(t)
{
x ctype
?
(t), x position
?
(t), x char
?
(t), x word
?
(t), x pos
?
(t), x ctype
?
(t)x position
?
(t),
x position
?
(t)x pos
?
(t), x pos
?
(t)x ctype
?
(t), x ctype
?
(t)x position
?
(t)x pos
?
(t)
}
bigram(s, t) xy bigram(s, t)? {a state(s)a state(t)}
xy bigram(s, t)
(
x state
0
(s)? x state
0
(t)? trans(s, t)
)
?
(
y unigram(s)? y unigram(t)? trans(s, t)
)
?
(
x state
0
(s)? y unigram(s)? x state
0
(t)? y unigram(t)? trans(s, t)
)
trans(s, t)
{
x diff(s, t), x diff wd(s, t), y diff(s, t)
}
Table 2: Generation rules for unigram and bigram features.
x diff wd measures the distance of two words.
The function y diff models the ordering of abbre-
viation letters; this function always returns non-
negative values if the abbreviation contains letters
in the same order as in its full form.
We express unigram and bigram features with
the atomic functions. For example, Formula 5 de-
fines a unigram feature for the event where the cap-
ital letter in a full-form word x
a
t(x)
produces the
identical abbreviation letter y
a
t(y)
.
g
k
(a,x,y, t) =
?
?
?
?
?
?
?
?
?
?
?
1 x ctype
0
(a,x, t) = U
? y ctype(a,y, t) = U
? a state(a,y, t) = MATCH
0 (otherwise)
(5)
For notation simplicity, we rewrite this boolean
function as (arguments a, x, and y are omitted),
1
{x ctype
0
(t)y ctype(t)a state(t)=U;U;MATCH}
. (6)
In this formula, 1
{v=v?}
is an indicator function that
equals 1 when v = v? and 0 otherwise. The term
v presents a generation rule for a feature, i.e., a
combination rule of atomic functions.
Table 2 displays the complete list of gener-
ation rules for unigram and bigram features
4
,
unigram(t) and bigram(s, t). For each genera-
tion rule in unigram(t) and bigram(s, t), we de-
fine boolean functions that test the possible values
yielded by the corresponding atomic function(s).
2.3 Alignment candidates
Formula 1 requires a sum over the possible align-
ments, which amounts to 2
LM
for a sentence (L
letters) with an abbreviation (M letters). It is
unrealistic to compute the partition factor of the
formula directly; therefore, the factor has been
computed by dynamic programing (McCallum et
al., 2005; Blunsom and Cohn, 2006; Shimbo and
Hara, 2007) or approximated by the n-best list of
highly probable alignments (Och and Ney, 2002;
Liu et al, 2005). Fortunately, we can prune align-
ments that are unlikely to present full forms, by in-
troducing the natural assumptions for abbreviation
definitions:
4
In Table 2, a set of curly brackets {} denotes a list (array)
rather than a mathematical set. Operators ? and ? present
concatenation and Cartesian product of lists. For instance,
when A = {a, b} and B = {c, d}, A?B = {a, b, c, d} and
A?B = {ac, ad, bc, bd}.
660
  investigate the effect of thyroid transcription factor 1
0   0  0    00  0  0 0       0        0    0  0   00   0  0    00  0  0 0       1        2    3  0   50   0  0    00  0  0 1       2        0    3  0   50   0  0    00  0  0 1       0        2    3  0   50   0  0    00  0  0 2       1        0    3  0   50   0  0    00  0  0 2       0        1    3  0   50   0  0    00  0  3 0       0        1    0  2   50   0  0    00  0  3 0       1        2    0  0   50   0  0    00  0  3 0       1        0    0  2   5
.   .  .    ..  .  . .       .        .    .  .   ..   .  .    ..  .  . .       .        .    .  .   ..   .  .    ..  .  . .       .        .    .  .   .
x: ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~
min(|y|+5, 2|y|) = 8 words, (|y| = 4; y = "TTF-1")
94 13 16 2122 25 28 30 38 47 52 55 59ia =
ShffleShffleShffleShffleShffle
#0
#1
#2
#3
#4
#5
#6
#7
#8
Figure 2: A part of the possible alignments for the
abbreviation TTF-1 in the example sentence.
1. A full form may appear min(m + 5, 2m)
words before its abbreviation in the same sen-
tence, where m is the number of alphanu-
meric letters in the abbreviation (Park and
Byrd, 2001).
2. Every alphanumeric letter in an abbreviation
must be associated with the identical (case-
insensitive) letter in its full form.
3. An abbreviation letter must not originate from
multiple letters in its full form; a full-form let-
ter must not produce multiple letters.
4. Words in a full form may be shuffled at most
d times, so that all alphanumeric letters in the
corresponding abbreviation appear in the re-
arranged full form in the same order. We de-
fine a shuffle operation as removing a series
of word(s) from a full form, and inserting the
removed word(s) to another position.
5. A full form does not necessarily exist in the
text span defined by assumption 1.
Due to the space limitation, we do not describe
the algorithm for obtaining possible alignments
that are compatible with these assumptions. Al-
ternatively, Figure 2 illustrates a part of possible
alignments C(x,y) for the example sentence. The
alignment #2 represents the correct definition for
the abbreviation TTF-1. We always include the
negative alignment (e.g., #0) where no abbrevia-
tion letters are associated with any letters in x.
The alignments #4?8 interpret the generation
process of the abbreviation by shuffling the words
in x. For example, the alignment #6 moves the
word ?of? to the position between ?factor? and
?1?. Shuffled alignments cover abbreviation defini-
tions such as receptor of estrogen (ER) and water
activity (AW). We call the parameter d, distortion
parameter, which controls the acceptable level of
reordering (distortion) for the abbreviation letters.
2.4 Parameter estimation
Parameter estimation for the abbreviation
alignment model is essentially the same as
for general maximum entropy models. Given
a training set that consists of N instances,
(
(a
(1)
,x
(1)
,y
(1)
), ..., (a
(N)
,x
(N)
,y
(N)
)
)
, we
maximize the log-likelihood of the conditional
probability distribution by using the maximum
a posterior (MAP) estimation. In order to avoid
overfitting, we regularize the log-likelihood with
either the L
1
or L
2
norm of the weight vector ?,
L
1
=
N
?
n=1
logP (a
(n)
|x
(n)
,y
(n)
)?
||?||
1
?
1
, (7)
L
2
=
N
?
n=1
logP (a
(n)
|x
(n)
,y
(n)
)?
||?||
2
2
2?
2
2
. (8)
In these formulas, ?
1
and ?
2
are regularization pa-
rameters for the L
1
and L
2
norms. Formulas 7
and 8 are maximized by the Orthant-Wise Limited-
memory Quasi-Newton (OW-LQN) method (An-
drew and Gao, 2007) and the Limited-memory
BFGS (L-BFGS) method (Nocedal, 1980)
5
.
3 Experiments
3.1 Aligned abbreviation corpus
The Medstract Gold Standard Corpus (Pustejovsky
et al, 2002) was widely used for evaluating abbre-
viation recognition methods (Schwartz and Hearst,
2003; Adar, 2004). However, we cannot use
this corpus for training the abbreviation alignment
model, since it lacks annotations on the origins of
abbreviation letters. In addition, the size of the
corpus is insufficient for a supervised machine-
learning method.
Therefore, we built our training corpus with
1,000 scientific abstracts that were randomly cho-
sen from the MEDLINE database. Although the
alignment model is independent of linguistic pat-
terns for abbreviation definitions, in the corpus we
found only three abbreviation definitions that were
described without parentheses. Hence, we em-
ployed parenthetical expressions, full-form ?(? ab-
breviation ?)?, to locate possible abbreviation def-
initions (Wren and Garner, 2002). In order to ex-
clude parentheses inserting clauses into passages,
5
We used Classias for parameter estimation:
http://www.chokkan.org/software/classias/
661
we consider the inner expression of parentheses as
an abbreviation candidate, only if the expression
consists of two words at most, the length of the ex-
pression is between two to ten characters, the ex-
pression contains at least an alphabetic letter, and
the first character is alphanumeric.
We asked a human annotator to assign refer-
ence abbreviation alignments for 1,420 parentheti-
cal expressions (instances) in the corpus. If a par-
enthetical expression did not introduce an abbre-
viation, e.g., ?... received treatment at 24 months
(RRMS),? the corresponding instance would have
a negative alignment (as #0 in Figure 2). Eventu-
ally, our aligned corpus consisted of 864 (60.8%)
abbreviation definitions (with positive alignments)
and 556 (39.2%) other usages of parentheses (with
negative alignments). Note that the log-likelihood
in Formula 7 or 8 increases only if the probabilistic
model predicts the reference alignments, regard-
less of whether they are positive or negative.
3.2 Baseline systems
We prepared five state-of-the-art systems of ab-
breviation recognition as baselines: Schwartz
and Hearst?s method (SH) (Schwartz and Hearst,
2003), SaRAD (Adar, 2004), ALICE (Ao and
Takagi, 2005), Chang and Sch?utze?s method
(CS) (Chang and Sch?utze, 2006), and Nadeau and
Turney?s method (NT) (Nadeau and Turney, 2005).
We utilized the implementations available on the
Web for SH
6
, CS
78
, and ALICE
9
, and we repro-
duced SaRAD and NT, based on their papers.
Our implementation of NT consists of a classi-
fier that discriminates between positive (true) and
negative (false) full forms, using all of the feature
functions presented in the original paper. Although
the original paper presented heuristics for gener-
ating full-form candidates, we replaced the candi-
date generator with the function C(x,y), so that
the classifier and our alignment model can receive
the same set of full-form candidates. The classi-
fier of the NT system was modeled by the LIB-
SVM implementation
10
with Radial Basis Func-
6
Abbreviation Definition Recognition Software:
http://biotext.berkeley.edu/software.html
7
Biomedical Abbreviation Server:
http://abbreviation.stanford.edu/
8
We applied a score cutoff of 0.14.
9
Abbreviation LIfter using Corpus-based Extraction:
http://uvdb3.hgc.jp/ALICE/ALICE index.html
10
LIBSVM ? A Library for Support Vector Machines:
http://www.csie.ntu.edu.tw/
?
cjlin/libsvm/
System P R F1
Schwartz & Hearst (SH) .978 .940 .959
SaRAD .891 .919 .905
ALICE .961 .920 .940
Chang & Sch?utze (CS) .942 .900 .921
Nadeau & Turney (NT) .954 .871 .910
Proposed (d = 0; L
1
) .973 .969 .971
Proposed (d = 0; L
2
) .964 .968 .966
Proposed (d = 1; L
1
) .960 .981 .971
Proposed (d = 1; L
2
) .957 .976 .967
Table 3: Performance on our corpus.
tion (RBF) kernel
11
. If multiple full-form can-
didates for an abbreviation are classified as posi-
tives, we choose the candidate that yields the high-
est probability estimate.
3.3 Results
We trained and evaluated the proposed method on
our corpus by performing 10-fold cross valida-
tion
12
. Our corpus includes 13 out of 864 (1.5%)
abbreviation definitions in which the abbreviation
letters are shuffled. Thus, we have examined two
different distortion parameters, d = 0, 1 in this
experiment. The average numbers of candidates
produced by the candidate generator C(x,y) per
instance were 8.46 (d = 0) and 69.1 (d = 1), re-
spectively. The alignment model was trained in a
reasonable execution time
13
, ca. 5 minutes (d = 0)
and 1.5 hours (d = 1).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) on the basis of the number of cor-
rect abbreviation definitions recognized by each
system. The proposed method achieved the best
F1 score (0.971) of all systems. The inclusion of
distorted abbreviations (d = 1) gained the high-
est recall (0.981 with L
1
regularization). Base-
line systems with refined heuristics (SaRAD and
ALICE) could not outperform the simplest sys-
tem (SH). The previous approaches with machine
learning (CS and NT) were roughly comparable to
rule-based methods.
We also evaluated the alignment model on the
Medstract Gold Standard development corpus to
examine the adaptability of the alignment model
trained with our corpus (Table 4). Since the origi-
11
We tuned kernel parameters C = 128 and ? = 2.0 by
using the grid-search tool in the LIBSVM distribution.
12
We determined the regularization parameters as ?
1
= 3
and ?
2
= 3 after testing {0.1, 0.2, 0.3, 0.5, 1, 2, 3, 5, 10} for
the regularization parameters. The difference between the
highest and lowest F1 scores was 1.8%.
13
On Intel Dual-Core Xeon 5160/3GHz CPU, excluding
time for feature generation and data input/output.
662
System P R F1
Schwartz & Hearst (SH) .942 .891 .916
SaRAD .909 .859 .884
ALICE .960 .945 .953
Chang & Sch?utze (CS) .858 .852 .855
Nadeau & Turney (NT) .889 .875 .882
Proposed (d = 1; L
1
) .976 .945 .960
Table 4: Performance on Medstract corpus.
# Atomic function(s) F1
(1) x position + x ctype .905
(2) (1) + x char + y char .885
(3) (1) + x word + x pos .941
(4) (1) + x diff + x diff wd + y diff .959
(5) (1) + y position + y ctype .964
(6) All atomic functions .966
Table 5: Effect of atomic functions (d = 0; L
2
).
nal version of the Medstract corpus includes anno-
tation errors, we used the version revised by Ao
and Takagi (2005). For this reason, the perfor-
mance of ALICE might be over-estimated in this
evaluation; ALICE delivered much better results
than Schwartz & Hearst?s method on this corpus.
The abbreviation alignment model trained with
our corpus (d = 1; L
1
) outperformed the baseline
systems for all evaluation metrics. It is notable that
the model could recognize abbreviation definitions
with shuffled letters, e.g., transfer of single embryo
(SET) and inorganic phosphate (PI), without any
manual tuning for this corpus. In some false cases,
the alignment model yielded incorrect probability
estimates. For example, the probabilities of the
alignments prepubertal bipolarity, bi
:
polarity, and
non-definition (negative) for the abbreviation BP
were computed as 3.4%, 89.6%, and 6.7%, respec-
tively; but the first expression prepubertal bipolar-
ity is the correct definition for the abbreviation.
Table 5 shows F1 scores of the proposed method
trained with different sets of atomic functions. The
baseline setting (1), which built features only with
x position and x ctype functions, gained a 0.905
F1 score; further, adding more atomic functions
generally improves the score. However, the x char
and y char functions decreased the performance
since the alignment model was prone to overfit to
the training data, relying on the existence of spe-
cific letters in the training instances. Interestingly,
the model was flexible enough to achieve a high
performance with four atomic functions (5).
Table 6 demonstrates the ability for our ap-
proach to obtain effective features; the table shows
the top 10 (out of 850,009) features with high
# Feature ?
1 U: x position
0
=H;y ctype
0
=U;y position
0
=H/M 1.7370
2 B: y position
0
=I/y position
0
=I/x diff=1/M-M 1.3470
3 U: x ctype
?1
=L;x ctype
0
=L/S 0.96342
4 B: x ctype
0
=L/x ctype
0
=L/x diff wd=0/M-M 0.94009
5 U: x position
0
=I;x char
1
=?t?/S 0.91645
6 U: x position
0
=H;x pos
0
=NN;y ctype
0
=U/M 0.86786
7 U: x ctype
?1
=S;x
c
type
0
=L;M 0.86474
8 B: x char
0
=?o?/x ctype
0
=L/y diff=0/M-S 0.71262
9 U: x char
?1
=?o?;x ctype
0
=L/M 0.69764
10 B: x position
0
=H/x ctype
0
=U/y diff=1/M-M 0.66418
Table 6: Top ten features with high weights.
weights assigned by the MAP estimation with L
1
regularization. A unigram and bigram features
have prefixes ?U:? and ?B:? respectively; a feature
expresses conditions at s (bigram features only),
conditions at t, and mapping status (match or skip)
separated by ?/? symbols. For example, the #1 fea-
ture associates a letter at the head of a full-form
word with the uppercase letter at the head of its
abbreviation. The #4 feature is difficult to obtain
from manual observations, i.e., the bigram feature
suggests the production of two abbreviation letters
from two lowercase letters in the same word.
4 Conclusion
We have presented a novel approach for recogniz-
ing abbreviation definitions. The task of abbrevi-
ation recognition was successfully formalized as
a sequential alignment problem. We developed
an aligned abbreviation corpus, and obtained fine-
grained features that express the events wherein
a full forum produces an abbreviation letter. The
experimental results showed remarkable improve-
ments and usefulness of the alignment approach
for abbreviation recognition. We expect the use-
fullness of the discriminative model for building
an comprehensible abbreviation dictionary.
Future work would be to model cases in which
a full form yields non-identical letters (e.g., ?one?
? ?1? and ?deficient? ? ?-?), and to demonstrate
this approach with more generic linguistic patterns
(e.g., aka, abbreviated as, etc.). We also plan to
explore a method for training a model with an un-
aligned abbreviation corpus, estimating the align-
ments simultaneously from the corpus.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
663
References
Adar, Eytan. 2004. SaRAD: A simple and robust
abbreviation dictionary. Bioinformatics, 20(4):527?
533.
Andrew, Galen and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Ma-
chine Learning (ICML 2007), pages 33?40.
Ao, Hiroko and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
Berger, Adam L., Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Blunsom, Phil and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics (Coling-ACL 2006), pages 65?72.
Chang, Jeffrey T. and Hinrich Sch?utze. 2006. Abbre-
viations in biomedical text. In Ananiadou, Sophia
and John McNaught, editors, Text Mining for Biol-
ogy and Biomedicine, pages 99?119. Artech House,
Inc.
Gaudan, Sylvain, Harald Kirsch, and Dietrich Rebholz-
Schuhmann. 2005. Resolving abbreviations to their
senses in Medline. Bioinformatics, 21(18):3658?
3664.
Jain, Alpa, Silviu Cucerzan, and Saliha Azzam. 2007.
Acronym-expansion recognition and ranking on the
web. In Proceedings of the IEEE International Con-
ference on Information Reuse and Integration (IRI
2007), pages 209?214.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 18th International Con-
ference on Machine Learning (ICML 2001), pages
282?289.
Liu, Hongfang and Carol Friedman. 2003. Mining ter-
minological knowledge in large biomedical corpora.
In the 8th Pacific Symposium on Biocomputing (PSB
2003), pages 415?426.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL 2005), pages 459?466.
McCallum, Andrew, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proceedings of the 21st Conference on Un-
certainty in Artificial Intelligence (UAI 2005), pages
388?395.
Nadeau, David and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
In the 8th Canadian Conference on Artificial Intel-
ligence (AI?2005) (LNAI 3501), page 10 pages.
Nocedal, Jorge. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Och, Franz Josef and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics (ACL 2002), pages 295?302.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Build-
ing an abbreviation dictionary using a term recogni-
tion approach. Bioinformatics, 22(24):3089?3095.
Pakhomov, Serguei. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics (ACL 2002), pages 160?167.
Park, Youngja and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of the 2001 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2001), pages 126?133.
Pustejovsky, James, Jos?e Casta?no, Roser Saur??, Anna
Rumshinsky, Jason Zhang, and Wei Luo. 2002.
Medstract: creating large-scale information servers
for biomedical libraries. In Proceedings of the ACL-
02 workshop on Natural language processing in the
biomedical domain, pages 85?92.
Schwartz, Ariel S. and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In the 8th Pacific Symposium on
Biocomputing (PSB 2003), pages 451?462.
Shimbo, Masashi and Kazuo Hara. 2007. A dis-
criminative learning model for coordinate conjunc-
tions. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 610?619.
Wren, Jonathan D. and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated construc-
tion of comprehensive acronym-definition dictionar-
ies. Methods of Information in Medicine, 41(5):426?
434.
Yu, Hong, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based
approach for automatically disambiguating biomedi-
cal abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380?404.
Zhou, Wei, Vetle I. Torvik, and Neil R. Smalheiser.
2006. ADAM: another database of abbreviations in
MEDLINE. Bioinformatics, 22(22):2813?2818.
664
Coling 2008: Companion volume ? Posters and Demonstrations, pages 127?130
Manchester, August 2008
Building a Bilingual Lexicon Using Phrase-based
Statistical Machine Translation via a Pivot Language
Takashi Tsunakawa? Naoaki Okazaki? Jun?ichi Tsujii??
?Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo 7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-0033 Japan
?School of Computer Science, University of Manchester / National Centre for Text Mining
131 Princess Street, Manchester, M1 7DN, UK
{tuna, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper proposes a novel method for
building a bilingual lexicon through a pivot
language by using phrase-based statisti-
cal machine translation (SMT). Given two
bilingual lexicons between language pairs
L
f
?L
p
and L
p
?L
e
, we assume these lexi-
cons as parallel corpora. Then, we merge
the extracted two phrase tables into one
phrase table between L
f
and L
e
. Fi-
nally, we construct a phrase-based SMT
system for translating the terms in the lex-
icon L
f
?L
p
into terms of L
e
and, ob-
tain a new lexicon L
f
?L
e
. In our experi-
ments with Chinese-English and Japanese-
English lexicons, our system could cover
72.8% of Chinese terms and drastically im-
prove the utilization ratio.
1 Introduction
The bilingual lexicon is a crucial resource for mul-
tilingual applications in natural language process-
ing including machine translation (Brown et al,
1990) and cross-lingual information retrieval (Nie
et al, 1999). A number of bilingual lexicons have
been constructed manually, despite their expensive
compilation costs. However, it is unrealistic to
build a bilingual lexicon for every language pair;
thus, comprehensible bilingual lexicons are avail-
able only for a limited number of language pairs.
One of the solutions is to build a bilingual lex-
icon of the source language L
f
and the target L
e
through a pivot language L
p
, when large bilingual
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
lexicons L
f
?L
p
and L
p
?L
e
are available. Numer-
ous researchers have explored the use of pivot lan-
guages (Tanaka and Umemura, 1994; Schafer and
Yarowsky, 2002; Zhang et al, 2005). This ap-
proach is advantageous because we can obtain a
bilingual lexicon between L
e
and L
f
, even if no
bilingual lexicon exists between these languages.
Pivot-based methods for dictionary construction
may produce incorrect translations when the word
w
e
is translated from a word w
f
by a polysemous
pivot word w
p
1
. Previous work addressed the poly-
semy problem in pivot-based methods (Tanaka and
Umemura, 1994; Schafer and Yarowsky, 2002).
Pivot-based methods also suffer from a mismatch
problem, in which a pivot word w
p
from a source
word w
f
does not exist in the bilingual lexicon L
p
?
L
e
2
. Moreover, a bilingual lexicon for technical
terms is prone to include a number of pivot terms
that are not included in another lexicon.
This paper proposes a method for building a
bilingual lexicon through a pivot language by us-
ing phrase-based statistical machine translation
(SMT) (Koehn et al, 2003). We build a transla-
tion model between L
f
and L
e
by assuming two
lexicons L
f
?L
p
and L
p
?L
e
as parallel corpora, in
order to increase the obtained lexicon size by han-
dling multi-word expressions appropriately. The
main advantage of this method is its ability to in-
corporate various translation models that associate
languages L
f
?L
e
; for example, we can further im-
prove the translation model by integrating a small
bilingual lexicon L
f
?L
e
.
1A Japanese term ????: dote, embankment, may be as-
sociated with a Chinese term ???,? y??ngha?ng: banking in-
stitution, using the pivot word bank in English.
2It is impossible to associate two translation pairs (???
??? (chikyu?-ondanka),? global warming), and (global heat-
ing, ????? (qua?nqiu?-bia`nnua?n)?) because of the differ-
ence in English (pivot) terms.
127
Lf-Lp lexicon Le-Lp lexicon
Lf-Lp translation 
phrase table
Le-Lp translation 
phrase table
Lf-Le translation 
phrase table
Phrase-based
SMT system
Word alignment &
grow-diag-final method
Merging phrase tables
Additional
features
INPUT OUTPUT
Le: translations of 
Lf-Lp lexicon
Figure 1: Framework of our approach
2 Merging two bilingual lexicons
We introduce phrase-based SMT for merging the
lexicons, in order to improve both the merged
lexicon size and its accuracy. Recently, several
researchers proposed the use of the pivot lan-
guage for phrase-based SMT (Utiyama and Isa-
hara, 2007; Wu and Wang, 2007). We employ a
similar approach for obtaining phrase translations
with the translation probabilities by assuming the
bilingual lexicons as parallel corpora. Figure 1 il-
lustrates the framework of our approach.
Let us suppose that we have two bilingual lex-
icons L
f
?L
p
and L
p
?L
e
. We obtain word align-
ments of these lexicons by applying GIZA++ (Och
and Ney, 2003), and grow-diag-final heuristics
(Koehn et al, 2007). Let w?
x
be a phrase that
represents a sequence of words in the language
L
x
. For phrase pairs (w?
p
, w?
f
) and (w?
e
, w?
p
), the
translation probabilities p(w?
p
|w?
f
) and p(w?
e
|w?
p
)
are computed using the maximum likelihood esti-
mation from the co-occurrence frequencies, con-
sistent with the word alignment in the bilingual
lexicons. We calculate the direct translation prob-
abilities between source and target phrases,
p(w?
e
|w?
f
) =
?
w?
p
p(w?
e
|w?
p
)p(w?
p
|w?
f
)
?
w?
?
e
?
w?
p
p(w?
?
e
|w?
p
)p(w?
p
|w?
f
)
. (1)
We employ the log-linear model of phrase-based
SMT (Och and Ney, 2002) for translating the
source term w?
f
in the lexicon L
f
?L
p
into the tar-
get language by finding a term ?w?
e
that maximizes
the translation probability,
?
w?
e
= argmax
w?
e
Pr(w?
e
|w?
f
)
= argmax
w?
e
M
?
m=1
?
m
h
m
(w?
e
, w?
f
), (2)
where we have M feature functions h
m
(w?
e
, w?
f
)
and model parameters ?
m
.
In addition to the typical features for the SMT
framework, we introduce two features: character-
based similarity, and additional bilingual lexicon.
We define a character-based similarity feature,
h
char sim
(w?
e
, w?
f
) = 1 ?
ED(w?
e
, w?
f
)
max(w?
e
, w?
f
)
, (3)
where ED(x, y) represents a Levenshtein distance
of characters between the two terms x and y3. We
also define an additional bilingual lexicon feature,
h
add lex
(w?
e
, w?
f
) =
?
i
log p
?
(w?
(i)
e
|w?
(i)
f
), (4)
where w?(i)
e
and w?(i)
f
represent an i-th translated
phrase pair on the term pair (w?
e
, w?
f
) during the
decoding, and p?(w?(i)
e
|w?
(i)
f
) represents the phrase
translation probabilities derived from the addi-
tional lexicon. The probability p?(w?(i)
e
|w?
(i)
f
) is cal-
culated using the maximum likelihood estimation.
3 Experiment
3.1 Data
For building a Chinese-to-Japanese lexicon, we
used the Japanese-English lexicon released by
JST4 (527,206 term pairs), and the Chinese-
English lexicon compiled by Wanfang Data5
(525,259 term pairs). Both cover a wide range
of named entities and technical terms that may
not be included in an ordinary dictionary. As an
additional lexicon, we used the Japanese-English-
Chinese trilingual lexicon6 (596,967 term pairs)
generated from EDR7 Japanese-English lexicon.
We lower-cased and tokenized all terms by the
following analyzers: JUMAN8 for Japanese, the
MEMM-based POS tagger9 for English, and cjma
(Nakagawa and Uchimoto, 2007) for Chinese.
3.2 The sizes and coverage of merged lexicons
Table 1 shows the distinct numbers of terms in
the original and merged lexicons, and the uti-
3We regard the different shapes of Han characters between
Chinese and Japanese as identical in our experiments.
4Japan Science and Technology Agency (JST)
http://pr.jst.go.jp/others/tape.html
5http://www.wanfangdata.com/
6This data was manually compiled by NICT, Japan.
7http://www2.nict.go.jp/r/r312/EDR/index.html
8http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
9http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/
128
Lexicon L
C
size L
E
size L
J
size
L
C
?L
E
375,990 429,807 -
L
E
?L
J
- 418,044 465,563
L
E
(distinct) - 783,414 -
Additional lex. 94,928 - 90,605
Exact matching 98,537 68,996 103,437
(26.2%) (22.2%)
Unique matching 4,875 4,875 4,875
(1.3%) (1.0%)
Table 1: The statistics of lexicons
lization ratio10 in the parentheses. For compari-
son, we prepared two baseline systems for build-
ing Chinese-Japanese lexicons. Exact matching
connects source and target terms that share at
least one common translation term in the pivot
language. Unique matching is an extreme ap-
proach for avoiding negative effects of polysemous
pivot terms: it connects source and target terms if
source, pivot, and target terms appear only once in
the corresponding lexicons.
Exact matching achieved 26.2% of the utiliza-
tion ratio in Japanese-to-Chinese translation, and
22.2% in Chinese-to-Japanese translation. These
figures imply that about 75% of the terms remained
unused in building the Japanese-Chinese lexicon.
With unique matching, as little as 1% of Japanese
and Chinese terms could be used. In contrast, our
method could cover 72.8% of Chinese terms by
generating Japanese terms, which was a drastic im-
provement in the utilization ratio.
3.3 Generating Japanese translations of the
Chinese-English lexicon
For evaluating the correctness of the merged lex-
icons, we assumed the lexicon generated by the
unique matching as a development/test set. Devel-
opment and test sets consist of about 2,400 term
pairs, respectively. Next, we input Chinese terms
in the development/test set into our system based
on Moses (Koehn et al, 2007), and obtained the
Japanese translations. We evaluated the perfor-
mance by using BLEU, NIST, and accuracy mea-
sures. Table 2 shows the evaluation results on the
test set. Our system could output correct trans-
lations for 68.5% of 500 input terms. The table
also reports that additional features were effective
in improving the performance.
We also conducted another experiment to gen-
erate Japanese translations for Chinese terms in-
cluded in an external resource. We randomly ex-
10The number of terms in the original lexicon used for
building the merged lexicon.
Features BLEU NIST Acc.
Typical features 0.4519 7.4060 0.676
w/ character similarity 0.4670 7.4963 0.682
w/ additional lexicon 0.4800 7.5907 0.674
All 0.4952 7.7046 0.685
Table 2: Translation performance on the test set
Features/Models Prec1 Prec10 MRR
Typical features 0.142 0.232 0.1719
w/ character similarity 0.136 0.224 0.1654
w/ additional lexicon 0.140 0.230 0.1704
All 0.140 0.230 0.1714
E-to-J translation 0.090 0.206 0.1256
Table 3: Evaluation results for the Eijiro dictionary
tracted 500 Chinese-English term pairs from the
Wanfang Data lexicon, for which the English term
cannot be mapped by the JST lexicon, but can be
mapped by another lexicon Eijiro11. Table 3 shows
the results for these 500 terms. Prec1 or Prec10 are
the precisions that the 1- or 10-best translations in-
clude the correct one, respectively. MRR (mean
reciprocal rank) is (1/500)?
i
(1/r
i
), where r
i
is
the highest rank of the correct translations for the
i-th term.
Since the input lexicons are Chinese-English
term pairs, their Japanese translations can be gen-
erated directly from the English terms by applying
an English-Chinese translation system. We com-
pared our system to an English-Japanese phrase-
based SMT system (E-to-J translation), con-
structed from the JST Japanese-English lexicon.
Table 3 shows that our system outperformed the
English-to-Japanese direct translation system.
Table 4 displays translation examples. The first
example shows that our system could output a cor-
rect translation (denoted by [T]); and the E-to-J
system failed to translate the source term ([F]),
because it could not reorder the source English
words and translate the word pubis correctly. In
the second example, our system could reproduce
Chinese characters ??? (fluid)?, but the E-to-J
system output a semantically acceptable but awk-
ward Japanese term. In the last example, the word
segmentation of the source Chinese term was in-
correct (???? (lumber)?? (lymph)?? is cor-
rect). Thus, our system received an invalid word ?
??? and could not find a translation for the word.
11http://www.eijiro.jp/
129
English Chinese Japanese (Eijiro) Japanese (C-to-J) Japanese (E-to-J)
symphysis pubis ???? ???? ???? [T] ??? (symphysis shame) [F]
ideal fluid dy-
namics
?? ??
???
?????? ?????? [T] ??? (fluid)?? [F]
intermediate
lumbar lymph
nodes
?? ??
??
??????? ?? ? ?? (inter-
mediate node [lumbar-
lymph]
INVALID
) [F]
??????? [T]
Table 4: Translation examples on Eijiro dictionary
4 Conclusion
This paper proposed a novel method for building a
bilingual lexicon by using a pivot language. Given
two bilingual lexicons L
f
?L
p
and L
p
?L
e
, we con-
structed a phrase-based SMT system from L
f
?L
e
by merging the lexicons into a phrase translation
table L
f
?L
e
. The experimental results demon-
strated that our method improves the utilization ra-
tio of given lexicons drastically. We also showed
that the pivot approach was more effective than the
SMT system that translates from L
p
to L
e
directly.
The future direction would be to introduce other
resources such as the parallel corpora and other
pivot languages into the SMT system for improv-
ing the precision and the coverage of the obtained
lexicon. We are also planning on evaluating a ma-
chine translation system that integrates our model.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan).
References
Brown, Peter F., John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology, pages 48?54.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics, demonstration ses-
sion, pages 177?180.
Nakagawa, Tetsuji and Kiyotaka Uchimoto. 2007. Hy-
brid approach to word segmentation and POS tag-
ging. In Companion Volume to the Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 217?220.
Nie, Jian-Yun, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language informa-
tion retrieval based on parallel texts and automatic
mining of parallel texts from the Web. In Proc. of
the 22nd Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 74?81.
Och, Franz Josef and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 295?302.
Och, Franz Josef and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Schafer, Charles and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In Proc. of the 6th Conference
on Natural Language Learning, volume 20, pages 1?
7.
Tanaka, Kumiko and Kyoji Umemura. 1994. Construc-
tion of a bilingual dictionary intermediated by a third
language. In Proc. of the 15th International Confer-
ence on Computational Linguistics, pages 297?303.
Utiyama, Masao and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Proc. of Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 484?491.
Wu, Hua and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine trans-
lation. In Proc. of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, pages 856?
863.
Zhang, Yujie, Qing Ma, and Hitoshi Isahara. 2005.
Construction of a Japanese-Chinese bilingual dictio-
nary using English as an intermediary. International
Journal of Computer Processing of Oriental Lan-
guages, 18(1):23?39.
130
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 447?456,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Discriminative Candidate Generator for String Transformations
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Yoshimasa Tsuruoka?
yoshimasa.tsuruoka@manchester.ac.uk
Sophia Ananiadou?
sophia.ananiadou@manchester.ac.uk
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
Abstract
String transformation, which maps a source
string s into its desirable form t?, is related
to various applications including stemming,
lemmatization, and spelling correction. The
essential and important step for string trans-
formation is to generate candidates to which
the given string s is likely to be transformed.
This paper presents a discriminative approach
for generating candidate strings. We use sub-
string substitution rules as features and score
them using an L1-regularized logistic regres-
sion model. We also propose a procedure to
generate negative instances that affect the de-
cision boundary of the model. The advantage
of this approach is that candidate strings can
be enumerated by an efficient algorithm be-
cause the processes of string transformation
are tractable in the model. We demonstrate
the remarkable performance of the proposed
method in normalizing inflected words and
spelling variations.
1 Introduction
String transformation maps a source string s into its
destination string t?. In the broad sense, string trans-
formation can include labeling tasks such as part-
of-speech tagging and shallow parsing (Brill, 1995).
However, this study addresses string transformation
in its narrow sense, in which a part of a source string
is rewritten with a substring. Typical applications of
this task include stemming, lemmatization, spelling
correction (Brill and Moore, 2000; Wilbur et al,
2006; Carlson and Fette, 2007), OCR error correc-
tion (Kolak and Resnik, 2002), approximate string
matching (Navarro, 2001), and duplicate record de-
tection (Bilenko and Mooney, 2003).
Recent studies have formalized the task in the dis-
criminative framework (Ahmad and Kondrak, 2005;
Li et al, 2006; Chen et al, 2007),
t? = argmax
t?gen(s)
P (t|s). (1)
Here, the candidate generator gen(s) enumerates
candidates of destination (correct) strings, and the
scorer P (t|s) denotes the conditional probability of
the string t for the given s. The scorer was modeled
by a noisy-channel model (Shannon, 1948; Brill and
Moore, 2000; Ahmad and Kondrak, 2005) and max-
imum entropy framework (Berger et al, 1996; Li et
al., 2006; Chen et al, 2007).
The candidate generator gen(s) also affects the
accuracy of the string transformation. Previous stud-
ies of spelling correction mostly defined gen(s),
gen(s) = {t | dist(s, t) < ?}. (2)
Here, the function dist(s, t) denotes the weighted
Levenshtein distance (Levenshtein, 1966) between
strings s and t. Furthermore, the threshold ? requires
the distance between the source string s and a can-
didate string t to be less than ?.
The choice of dist(s, t) and ? involves a tradeoff
between the precision, recall, and training/tagging
speed of the scorer. A less restrictive design of these
factors broadens the search space, but it also in-
creases the number of confusing candidates, amount
of feature space, and computational cost for the
scorer. Moreover, the choice is highly dependent on
the target task. It might be sufficient for a spelling
447
correction program to gather candidates from known
words, but a stemmer must handle unseen words ap-
propriately. The number of candidates can be huge
when we consider transformations from and to un-
seen strings.
This paper addresses these challenges by explor-
ing the discriminative training of candidate genera-
tors. More specifically, we build a binary classifier
that, when given a source string s, decides whether
a candidate t should be included in the candidate set
or not. This approach appears straightforward, but it
must resolve two practical issues. First, the task of
the classifier is not only to make a binary decision
for the two strings s and t, but also to enumerate a
set of positive strings for the string s,
gen(s) = {t | predict(s, t) = 1}. (3)
In other words, an efficient algorithm is necessary
to find a set of strings with which the classifier
predict(s, t) yields positive labels for the string s.
Another issue arises when we prepare a training
set. A discriminative model requires a training set
in which each instance (pair of strings) is annotated
with a positive or negative label. Even though some
existing resources (e.g., inflection table and query
log) are available for positive instances, such re-
sources rarely contain negative instances. Therefore,
we must generate negative instances that are effec-
tive for discriminative training.
To address the first issue, we design features that
express transformations from a source string s to its
destination string t. Feature selection and weight-
ing are performed using an L1-regularized logistic
regression model, which can find a sparse solution
to the classification model. We also present an al-
gorithm that utilizes the feature weights to enumer-
ate candidates of destination strings efficiently. We
deal with the second issue by generating negative
instances from unlabeled instances. We describe a
procedure to choose negative instances that affect
the decision boundary of the classifier.
This paper is organized as follows. Section 2 for-
malizes the task of the candidate generator as a bi-
nary classification modeled by logistic regression.
Features for the classifier are designed using the
rules of substring substitution. Therefore, we can
obtain, efficiently, candidates of destination strings
and negative instances for training. Section 3 re-
ports the remarkable performance of the proposed
method in various applications including lemmati-
zation, spelling normalization, and noun derivation.
We briefly review previous work in Section 4, and
conclude this paper in Section 5.
2 Candidate generator
2.1 Candidate classification model
In this section, we first introduce a binary classifier
that yields a label y ? {0, 1} indicating whether a
candidate t should be included in the candidate set
(1) or not (0), given a source string s. We express
the conditional probability P (y|s, t) using a logistic
regression model,
P (1|s, t) =
1
1 + exp (??TF (s, t))
, (4)
P (0|s, t) = 1? P (1|s, t). (5)
In these equations, F = {f1, ..., fK} denotes a vec-
tor of the Boolean feature functions; K is the num-
ber of feature functions; and ? = {?1, ..., ?K}
presents a weight vector of the feature functions.
We obtain the following decision rule to choose
the most probable label y? for a given pair ?s, t?,
y? = argmax
y?{0,1}
P (y|s, t) =
{
1
(
?TF (s, t) > 0
)
0 (otherwise)
.
(6)
Finally, given a source string s, the generator func-
tion gen(s) is defined to collect all strings to which
the classifier assigns positive labels:
gen(s) = {t | P (1|s, t) > P (0|s, t)}
= {t | ?TF (s, t) > 0}. (7)
2.2 Substitution rules as features
The binary classifier can include any arbitrary fea-
ture. This is exemplified by the Levenshtein dis-
tance and distributional similarity (Lee, 1999) be-
tween two strings s and t. These features can im-
prove the classification accuracy, but it is unrealistic
to compute these features for every possible string,
as in equation 7. For that reason, we specifically
examine substitution rules, with which the process
448
^oestrogen$
^estrogen$
^anaemia$
^anemia$
^studies$
^study$
('o', ''), ('^o', '^'), ('oe', 'e'),
('^oe', '^e'), ('^oes', '^es'), ...
('a', ''), ('na', 'n'), ('ae', 'e'),
('ana', 'an'), ('nae', 'ne'), ('aem', 'em'),
...
('ies', 'y'), ('dies', 'dy'), ('ies$', 'y$'),
('udies', 'udy'), ('dies$', 'dy$'), ...
S:
t:
S:
t:
S:
t:
(1)
(2)
(3)
Figure 1: Generating substitution rules.
of transforming a source string s into its destination
form t is tractable.
In this study, we assume that every string has a
prefix ??? and postfix ?$?, which indicate the head
and tail of a string. A substitution rule r = (?, ?)
replaces every occurrence of the substring ? in a
source string into the substring ?. Assuming that a
string s can be transformed into another string twith
a single substitution operation, substitution rules ex-
press the different portion between strings s and t.
Equation 8 defines a binary feature function with
a substitution rule between two strings s and t,
fk(s, t) =
{
1 (rule rk can convert s into t)
0 (otherwise)
.
(8)
We allow multiple substitution rules for a given pair
of strings. For instance, substitution rules (?a?,
??), (?na?, ?n?), (?ae?, ?e?), (?nae?, ?ne?), etc.
form feature functions that yield 1 for strings s =
??anaemia$? and t = ??anemia$?. Equation
6 produces a decision based on the sum of feature
weights, or scores of substitution rules, representing
the different portions between s and t.
Substitution rules for the given two strings s and
t are obtained as follows. Let l denote the longest
common prefix between strings s and t, and r the
longest common postfix. We define cs as the sub-
string in s that is not covered by the longest common
prefix l and postfix r, and define ct for t analogously.
In other words, strings s and t are divided into three
regions, lcsr and lctr, respectively. For strings s =
??anaemia$? and t = ??anemia$? in Figure 1
(2), we obtain cs = ?a? and ct = ?? because l =
??an? and r = ?emia$?.
Because substrings cs and ct express different
portions between strings s and t, we obtain the mini-
mum substitution rule (cs, ct), which can convert the
string s into t by replacing substrings cs in s with
ct; the minimum substitution rule for the same ex-
ample is (?a?, ??). However, replacing letters ?a?
in ??anaemia$? into empty letters does not pro-
duce the correct string ??anemia$? but ??nemi$?.
Furthermore, the rule might be inappropriate for ex-
pressing string transformation because it always re-
moves the letter ?a? from every string.
Therefore, we also obtain expanded substitution
rules, which insert postfixes of l to the head of min-
imum substitution rules, and/or append prefixes of
r to the rules. For example, we find an expanded
substitution rule (?na?, ?n?), by inserting a postfix
of l = ??an? to the head of the minimum substitu-
tion rule (?a?, ??); similarly, we obtain an expanded
substitution rule (?ae?, ?e?), by appending a prefix
of r = ?emia$? to the tail of the rule (?a?, ??).
Figure 1 displays examples of substitution rules
(the right side) for three pairs of strings (the left
side). Letters in blue, green, and red respectively
represent the longest common prefixes, longest com-
mon postfixes, and different portions. In this study,
we expand substitution rules such that the number of
letters in rules is does not pass a threshold ?1.
2.3 Parameter estimation
Given a training set that consists of N instances,
D =
(
(s(1), t(1), y(1)), ..., (s(N), t(N), y(N))
)
, we
optimize the feature weights in the logistic regres-
sion model by maximizing the log-likelihood of the
conditional probability distribution,
L? =
N?
i=1
logP (y(i)|s(i), t(i)). (9)
The partial derivative of the log-likelihood with re-
spect to a feature weight ?k is given as equation 10,
?L?
??k
=
N?
i=1
{
y(i) ? P (1|s(i), t(i))
}
fk(s
(i), t(i)).
(10)
The maximum likelihood estimation (MLE) is
known to suffer from overfitting the training set. The
1The number of letters for a substitution rule r = (?, ?) is
defined as the sum of the quantities of letters in ? and ?, i.e.,
|?|+ |?|. We determined the threshold ? = 12 experimentally.
449
common approach for addressing this issue is to use
the maximum a posteriori (MAP) estimation, intro-
ducing a regularization term of the feature weights
?, i.e., a penalty on large feature weights. In addi-
tion, the generation algorithm of substitution rules
might produce inappropriate rules that transform a
string incorrectly, or overly specific rules that are
used scarcely. Removing unnecessary substitution
rules not only speeds up the classifier but also the
algorithm for candidate generation, as presented in
Section 2.4.
In recent years, L1 regularization has received in-
creasing attention because it produces a sparse so-
lution of feature weights in which numerous fea-
ture weights are zero (Tibshirani, 1996; Ng, 2004).
Therefore, we regularize the log-likelihood with the
L1 norm of the weight vector ? and define the final
form the objective function to be minimized as
E? = ?L? +
|?|
?
. (11)
Here, ? is a parameter to control the effect of L1
regularization; the smaller the value we set to ?,
the more features the MAP estimation assigns zero
weights to: it removes a number of features from the
model. Equation 11 is minimized using the Orthant-
Wise Limited-memory Quasi-Newton (OW-LQN)
method (Andrew and Gao, 2007) because the second
term of equation 11 is not differentiable at ?k = 0.
2.4 Candidate generation
The advantage of our feature design is that we can
enumerate strings to which the classifier is likely to
assign positive labels. We start by observing the nec-
essary condition for t in equation 7,
?TF (s, t) > 0? ?k : fk(s, t) = 1 ? ?k > 0.
(12)
The classifier might assign a positive label to strings
s and t when at least one feature function whose
weight is positive can transform s to t.
Let R+ be a set of substitution rules to which
MAP estimation has assigned positive feature
weights. Because each feature corresponds to a sub-
stitution rule, we can obtain gen(s) for a given string
s by application of every substitution rule r ? R+,
gen(s) = {r(s) | r ? R+ ??TF (s, r(s)) > 0}.
(13)
Input: s = (s1, ..., sl): an input string s (series of letters)
Input: D: a trie dictionary containing positive features
Output: T : gen(s)
T = {};1
U = {};2
foreach i ? (1, ..., |s|) do3
F ? D.prefix search(s, i);4
foreach f ? F do5
if f /? U then6
t? f .apply(s);7
if classify(s, t) = 1 then8
add t to T ;9
end10
add f to U ;11
end12
end13
end14
return T ;15
Algorithm 1: A pseudo-code for gen(s).
Here, r(s) presents the string to which the substitu-
tion rule r transforms the source string s. We can
compute gen(s) with a small computational cost if
the MAP estimation with L1 regularization reduces
the number of active features.
Algorithm 1 represents a pseudo-code for obtain-
ing gen(s). To search for positive substitution rules
efficiently, the code stores a set of rules in a trie
structure. In line 4, the code obtains a set of positive
substitution rules F that can rewrite substrings start-
ing at offset #i in the source string s. For each rule
f ? F , we obtain a candidate string t by application
of the substitution rule f to the source string s (line
7). The candidate string t is qualified to be included
in gen(s) when the classifier assigns a positive label
to strings s and t (lines 8 and 9). Lines 6 and 11 pre-
vent the algorithm from repeating evaluation of the
same substitution rule.
2.5 Generating negative instances
The parameter estimation requires a training set D
in which each instance (pair of strings) is annotated
with a positive or negative label. Negative instances
(counter examples) are essential for penalizing in-
appropriate substitution rules, e.g. (?a?, ??). Even
though some existing resources (e.g. verb inflection
table) are available for positive instances, such re-
sources rarely contain negative instances.
A common approach for handling this situation
is to assume that every pair of strings in a resource
450
Input: D+ = [(s1, t1), ..., (sl, tl)]: positive instances
Input: V : a suffix array of all strings (vocabulary)
Output: D?: negative instances
Output: R: substitution rules (features)
D? = [];1
R = {};2
foreach d ? D+ do3
foreach r ? features(d) do4
add r to R;5
end6
end7
foreach r ? R do8
S ? V .search(r.src);9
foreach s ? S do10
t? r.apply(s);11
if (s, t) /? D+ then12
if t ? V then13
append (s, t) to D?;14
end15
end16
end17
end18
return D?, R;19
Algorithm 2: Generating negative instances.
is a negative instance; however, negative instances
amount to ca. V (V ? 1)/2, where V represents the
total number of strings. Moreover, substitution rules
expressing negative instances are innumerable and
sparse because the different portions are peculiar to
individual negative instances. For instance, the min-
imum substitution rule for unrelated words anaemia
and around is (?naemia?, ?round?), but the rule
cannot be too specific to generalize the conditions
for other negative instances.
In this study, we generate negative instances so
that they can penalize inappropriate rules and settle
the decision boundary of the classifier. This strat-
egy is summarized as follows. We consider every
pair of strings as candidates for negative instances.
We obtain substitution rules for the pair using the
same algorithm as that described in Section 2.2 if a
string pair is not included in the dictionary (i.e., not
in positive instances). The pair is used as a nega-
tive instance only when any substitution rule gener-
ated from the pair also exists in the substitution rules
generated from positive instances.
Algorithm 2 presents the pseudo-code that imple-
ments the strategy for generating negative instances
efficiently. First, we presume that we have positive
instances D+ = [(s1, t1), ..., (sl, tl)] and unlabeled
Table Description # Entries
LRSPL Spelling variants 90,323
LRNOM Nominalizations (derivations) 14,029
LRAGR Agreement and inflection 910,854
LRWD Word index (vocabulary) 850,236
Table 1: Excerpt of tables in the SPECIALIST Lexicon.
Data set # + # - # Rules
Orthography 15,830 33,296 11,098
Derivation 12,988 85,928 5,688
Inflection 113,215 124,747 32,278
Table 2: Characteristics of datasets.
strings V . For example, positive instance D+ repre-
sent orthographic variants, and unlabeled strings V
include all possible words (vocabulary). We insert
the vocabulary into a suffix array, which is used to
locate every occurrence of substrings in V .
The algorithm first generates substitution rules R
only from positive instances D+ (lines 3 to 7). For
each substitution rule r ? R, we enumerate known
strings S that contain the source substring r.src (line
9). We apply the substitution rule to each string s ?
S and obtain its destination string t (line 11). If the
pair of strings ?s, t? is not included in D+ (line 12),
and if the destination string t is known (line 13), the
substitution rule r might associate incorrect strings
s and t, which do not exist in D+. Therefore, we
insert the pair to the negative set D? (line 14).
3 Evaluation
3.1 Experiments
We evaluated the candidate generator using three
different tasks: normalization of orthographic vari-
ants, noun derivation, and lemmatization. The
datasets for these tasks were obtained from the
UMLS SPECIALIST Lexicon2, a large lexicon that
includes both commonly occurring English words
and biomedical vocabulary. Table 1 displays the list
of tables in the SPECIALIST Lexicon that were used
in our experiments. We prepared three datasets, Or-
thography, Derivation, and Inflection.
The Orthography dataset includes spelling vari-
ants (e.g., color and colour) in the LRSPL table. We
2UMLS SPECIALIST Lexicon:
http://specialist.nlm.nih.gov/
451
chose entries as positive instances in which spelling
variants are caused by (case-insensitive) alphanu-
meric changes3. The Derivation dataset was built di-
rectly from the LRNOM table, which includes noun
derivations such as abandon ? abandonment. The
LRAGR table includes base forms and their inflec-
tional variants of nouns (singular and plural forms),
verbs (infinitive, third singular, past, past participle
forms, etc), and adjectives/adverbs (positive, com-
parative, and superlative forms). For the Inflection
dataset, we extracted the entries in which inflec-
tional forms differ from their base forms4, e.g., study
? studies.
For each dataset, we applied the algorithm de-
scribed in Section 2.5 to generate substitution rules
and negative instances. Table 2 shows the number of
positive instances (# +), negative instances (# -), and
substitution rules (# Rules). We evaluated the per-
formance of the proposed method in two different
goals of the tasks: classification (Section 3.2) and
normalization (Section 3.3).
3.2 Experiment 1: Candidate classification
In this experiment, we measured the performance
of the classification task in which pairs of strings
were assigned with positive or negative labels.
We trained and evaluated the proposed method
by performing ten-fold cross validation on each
dataset5. Eight baseline systems were prepared
for comparison: Levenshtein distance (LD), nor-
malized Levenshtein distance (NLD), Dice coef-
ficient on letter bigrams (DICE) (Adamson and
Boreham, 1974), Longest Common Substring Ra-
tio (LCSR) (Melamed, 1999), Longest Common
Prefix Ratio (PREFIX) (Kondrak, 2005), Porter?s
stemmer (Porter, 1980), Morpha (Minnen et al,
2001), and CST?s lemmatiser (Dalianis and Jonge-
3LRSPL table includes trivial spelling variants that can be
handled using simple character/string operations. For example,
the table contains spelling variants related to case sensitivity
(e.g., deg and Deg) and symbols (e.g., Feb and Feb.).
4LRAGR table also provides agreement information even
when word forms do not change. For example, the table con-
tains an entry indicating that the first-singular present form of
the verb study is study, which might be readily apparent to En-
glish speakers.
5We determined the regularization parameter ? = 5 experi-
mentally. Refer to Figure 2 for the performance change.
jan, 2006)6.
The five systems LD, NLD, DICE, LCSR, and
PREFIX employ corresponding metrics of string
distance or similarity. Each system assigns a posi-
tive label to a given pair of strings ?s, t? if the dis-
tance/similarity of strings s and t is smaller/larger
than the threshold ? (refer to equation 2 for distance
metrics). The threshold of each system was chosen
so that the system achieves the best F1 score.
The remaining three systems assign a positive la-
bel only if the system transforms the strings s and
t into the identical string. For example, a pair of
two words studies and study is classified as positive
by Porter?s stemmer, which yields the identical stem
studi for these words. We trained CST?s lemmatiser
for each dataset to obtain flex patterns that are used
for normalizing word inflections.
To examine the performance of the L1-
regularized logistic regression as a discriminative
model, we also built two classifiers based on the
Support Vector Machine (SVM). These SVM
classifiers were implemented by the SVMperf 7 on
a linear kernel8. An SVM classifier employs the
same feature set (substitution rules) as the proposed
method so that we can directly compare the L1-
regularized logistic regression and the linear-kernel
SVM. Another SVM classifier incorporates the five
string metrics; this system can be considered as our
reproduction of the discriminative string similarity
proposed by Bergsma and Kondrak (2007).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) based on the number of correct de-
cisions for positive instances. The proposed method
outperformed the baseline systems, achieving 0.919,
0.888, and 0.984 of F1 scores, respectively. Porter?s
stemmer worked on the Inflection set, but not on
the Orthography set, which is beyond the scope of
the stemming algorithms. CST?s lemmatizer suf-
fered from low recall on the Inflection set because
it removed suffixes of base forms, e.g., (cloning,
clone) ? (clone, clo). Morpha and CST?s lemma-
6We used CST?s lemmatiser version 2.13:
http://www.cst.dk/online/lemmatiser/uk/
index.html
7SVM for Multivariate Performance Measures (SVMperf ):
http://svmlight.joachims.org/svm_perf.html
8We determined the parameter C = 500 experimentally; it
controls the tradeoff between training error and margin.
452
System Orthography Derivation Inflection
P R F1 P R F1 P R F1
Levenshtein distance (? = 1) .319 .871 .467 .004 .006 .005 .484 .679 .565
Levenshtein distance .323 .999 .488 .131 1.00 .232 .479 .988 .646
Normalized Levenshtein distance .441 .847 .580 .133 .990 .235 .598 .770 .673
Dice coefficient (letter bigram) .401 .918 .558 .137 .984 .240 .476 1.00 .645
LCSR .322 1.00 .487 .156 .841 .263 .476 1.00 .645
PREFIX .418 .927 .576 .140 .943 .244 .476 1.00 .645
Porter stemmer (Porter, 1980) .084 .074 .079 .197 .846 .320 .926 .839 .881
Morpha (Minnen et al, 2001) .009 .007 .008 .012 .022 .016 .979 .836 .902
CST?s lemmatiser (Dalianis et al 2006) .119 .008 .016 .383 .682 .491 .821 .176 .290
Proposed method .941 .898 .919 .896 .880 .888 .985 .986 .984
Substitution rules trained with SVM .943 .890 .916 .894 .886 .890 .980 .987 .983
+ LD, NLD, DICE, LCSR, PREFIX .946 .906 .926 .894 .886 .890 .980 .987 .983
Table 3: Performance of candidate classification
Rank Src Dst Weight Examples
1 uss us 9.81 focussing
2 aev ev 9.56 mediaeval
3 aen en 9.53 ozaena
4 iae$ ae$ 9.44 gadoviae
5 nni ni 9.16 prorennin
6 nne ne 8.84 connexus
7 our or 8.54 colour
8 aea ea 8.31 paean
9 aeu eu 8.22 stomodaeum
10 ooll ool 7.79 woollen
Table 4: Feature weights for the Orthography set
tizer were not designed for orthographic variants and
noun derivations.
Levenshtein distance (? = 1) did not work for
the Derivation set because noun derivations often
append two or more letters (e.g., happy ? happi-
ness). No string similarity/distance metrics yielded
satisfactory results. Some metrics obtained the best
F1 scores with extreme thresholds only to classify
every instance as positive. These results imply the
difficulty of the string metrics for the tasks.
The L1-regularized logistic regression was com-
parable to the SVM with linear kernel in this exper-
iment. However, the presented model presents the
advantage that it can reduce the number of active
features (features with non-zero weights assigned);
the L1 regularization can remove 74%, 48%, and
82% of substitution rules in each dataset. The
performance improvements by incorporating string
metrics as features were very subtle (less than 0.7%).
What is worse, the distance/similarity metrics do not
specifically derive destination strings to which the
classifier is likely to assign positive labels. There-
fore, we can no longer use the efficient algorithm
as a candidate generator (in Section 2.4) with these
features.
Table 4 demonstrates the ability of our approach
to obtain effective features; the table shows the top
10 features with high weights assigned for the Or-
thography data. An interesting aspect of the pro-
posed method is that the process of the orthographic
variants is interpretable through the feature weights.
Figure 2 shows plots of the F1 scores (y-axis) for
the Inflection data when we change the number of
active features (x-axis) by controlling the regular-
ization parameter ? from 0.001 to 100. The larger
the value we set for ?, the better the classifier per-
forms, generally, with more active features. In ex-
treme cases, the number of active features drops to
97 with ? = 0.01; nonetheless, the classifier still
achieves 0.961 of the F1 score. The result suggests
that a small set of substitution rules can accommo-
date most cases of inflectional variations.
3.3 Experiment 2: String transformation
The second experiment examined the performance
of the string normalization tasks formalized in equa-
tion 1. In this task, a system was given a string s and
was required to yield either its transformed form t?
(s 6= t?) or the string s itself when the transforma-
tion is unnecessary for s. The conditional probabil-
ity distribution (scorer) in equation 1 was modeled
453
System Orthography Derivation Inflection XTAG morph 1.5
P R F1 P R F1 P R F1 P R F1
Morpha .078 .012 .021 .233 .016 .029 .435 .682 .531 .830 .587 .688
CST?s lemmatiser .135 .160 .146 .378 .732 .499 .367 .762 .495 .584 .589 .587
Proposed method .859 .823 .841 .979 .981 .980 .973 .979 .976 .837 .816 .827
Table 5: Performance of string transformation
0.96
0.965
0.97
0.975
0.98
0.985
0.99
0 1000 2000 3000 4000 5000 6000 7000
F1 s
core
Number of active features (with non-zero weights)
Spelling variation
Figure 2: Number of active features and performance.
by the maximum entropy framework. Features for
the maximum entropy model consist of: substitution
rules between strings s and t, letter bigrams and tri-
grams in s, and letter bigrams and trigrams in t.
We prepared four datasets, Orthography, Deriva-
tion, Inflection, and XTAG morphology. Each
dataset is a list of string pairs ?s, t? that indicate
the transformation of the string s into t. A source
string s is identical to its destination string t when
string s should not be changed. These instances
correspond to the case where string s has already
been lemmatized. For each string pair (s, t) in LR-
SPL9, LRNOM, and LRAGR tables, we generated
two instances ?s, t? and ?t, t?. Consequently, a sys-
tem is expected to leave the string t unchanged. We
also used XTAG morphology10 to perform a cross-
domain evaluation of the lemmatizer trained on the
Inflection dataset11. The entries in XTAG morphol-
9We define that s precedes t in dictionary order.
10XTAG morphology database 1.5:
ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.
5/morph-1.5.tar.gz
11We found that XTAG morphology contains numerous in-
ogy that also appear in the Inflection dataset were
39,130 out of 317,322 (12.3 %). We evaluated
the proposed method and CST?s lemmatizer by per-
forming ten-fold cross validation.
Table 5 reports the performance based on the
number of correct transformations. The proposed
method again outperformed the baseline systems
with a wide margin. It is noteworthy that the pro-
posed method can accommodate morphological in-
flections in the XTAG morphology corpus with no
manual tuning or adaptation.
Although we introduced no assumptions about
target tasks (e.g. a known vocabulary), the aver-
age number of positive substitution rules relevant
to source strings was as small as 23.9 (in XTAG
morphology data). Therefore, the candidate gen-
erator performed 23.9 substitution operations for a
given string. It applied the decision rules (equa-
tion 7) 21.3 times, and generated 1.67 candidate
strings per source string. The experimental results
described herein demonstrated that the candidate
generator was modeled successfully by the discrim-
inative framework.
4 Related work
The task of string transformation has a long history
in natural language processing and information re-
trieval. As described in Section 1, this task is re-
lated closely to various applications. Therefore, we
specifically examine several prior studies that are
relevant to this paper in terms of technical aspects.
Some researchers have reported the effectiveness
of the discriminative framework of string similarity.
MaCallum et al (2005) proposed a method to train
the costs of edit operations using Conditional Ran-
dom Fields (CRFs). Bergsma and Kondrak (2007)
correct comparative and superlative adjectives, e.g., unpopular
? unpopularer ? unpopularest and refundable ? refundabler
? refundablest. Therefore, we removed inflection entries for
comparative and superlative adjectives from the dataset.
454
presented an alignment-based discriminative string
similarity. They extracted features from substring
pairs that are consistent to a character-based align-
ment of two strings. Aramaki et al (2008) also used
features that express the different segments of the
two strings. However, these studies are not suited for
a candidate generator because the processes of string
transformations are intractable in their discrimina-
tive models.
Dalianis and Jongejan (2006) presented a lem-
matiser based on suffix rules. Although they pro-
posed a method to obtain suffix rules from a training
data, the method did not use counter-examples (neg-
atives) for reducing incorrect string transformations.
Tsuruoka et al (2008) proposed a scoring method
for discovering a list of normalization rules for dic-
tionary look-ups. However, their objective was to
transform given strings, so that strings (e.g., studies
and study) referring to the same concept in the dic-
tionary are mapped into the same string (e.g., stud);
in contrast, this study maps strings into their destina-
tion strings that were specified by the training data.
5 Conclusion
We have presented a discriminative approach for
generating candidates for string transformation.
Unlike conventional spelling-correction tasks, this
study did not assume a fixed set of destination
strings (e.g. correct words), but could even generate
unseen candidate strings. We used anL1-regularized
logistic regression model with substring-substitution
features so that candidate strings for a given string
can be enumerated using the efficient algorithm. The
results of experiments described herein showed re-
markable improvements and usefulness of the pro-
posed approach in three tasks: normalization of or-
thographic variants, noun derivation, and lemmati-
zation.
The method presented in this paper allows only
one region of change in string transformation. A
natural extension of this study is to handle mul-
tiple regions of changes for morphologically rich
languages (e.g. German) and to handle changes
at the phrase/term level (e.g., ?estrogen receptor?
and ?receptor of oestrogen?). Another direction
would be to incorporate the methodologies for semi-
supervised machine learning to accommodate situa-
tions in which positive instances and/or unlabeled
strings are insufficient.
Acknowledgments
This work was partially supported by Grants-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and for Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
References
George W. Adamson and Jillian Boreham. 1974. The
use of an association measure based on character struc-
ture to identify semantically related pairs of words and
document titles. Information Storage and Retrieval,
10(7-8):253?260.
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 955?962.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Machine
Learning (ICML 2007), pages 33?40.
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko
Ohe. 2008. Orthographic disambiguation incorporat-
ing transliterated probability. In Proceedings of the
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP 2008), pages 48?55.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
656?663.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adap-
tive duplicate detection using learnable string simi-
larity measures. In Proceedings of the ninth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining (KDD 2003), pages 39?48.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on the As-
sociation for Computational Linguistics (ACL 2000),
pages 286?293.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
455
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
Andrew Carlson and Ian Fette. 2007. Memory-based
context-sensitive spelling correction at web scale. In
Proceedings of the Sixth International Conference on
Machine Learning and Applications (ICMLA 2007),
pages 166?171.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improv-
ing query spelling correction using web search results.
In Proceedings of the Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning (EMNLP-CoNLL
2007), pages 181?189.
Hercules Dalianis and Bart Jongejan. 2006. Hand-
crafted versus machine-learned inflectional rules: The
euroling-siteseeker stemmer and cst?s lemmatiser. In
In Proceedings of the 6th International Conference
on Language Resources and Evaluation (LREC 2006),
pages 663?666.
Okan Kolak and Philip Resnik. 2002. OCR error correc-
tion using a noisy channel model. In Proceedings of
the second international conference on Human Lan-
guage Technology Research (HLT 2002), pages 257?
262.
Grzegorz Kondrak. 2005. Cognates and word alignment
in bitexts. In Proceedings of the Tenth Machine Trans-
lation Summit (MT Summit X), pages 305?312.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL 1999),
pages 25?32.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association for
Computational Linguistics (Coling-ACL 2006), pages
1025?1032.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Proceedings
of the 21st Conference on Uncertainty in Artificial In-
telligence (UAI 2005), pages 388?395.
I. Dan Melamed. 1999. Bitext maps and alignment
via pattern recognition. Computational Linguistics,
25(1):107?130.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(3):207?223.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys (CSUR),
33(1):31?88.
Andrew Y. Ng. 2004. Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning (ICML 2004), pages 78?85.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27(3):379?423.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), 58(1):267?288.
Yoshimasa Tsuruoka, John McNaught, and Sophia Ana-
niadou. 2008. Normalizing biomedical terms by min-
imizing ambiguity and variability. BMC Bioinformat-
ics, Suppl 3(9):S2.
W. John Wilbur, Won Kim, and Natalie Xie. 2006.
Spelling correction in the PubMed search engine. In-
formation Retrieval, 9(5):543?564.
456
A Machine Learning Approach to Sentence
Ordering for Multidocument Summarization
and Its Evaluation
Danushka Bollegala, Naoaki Okazaki, and Mitsuru Ishizuka
University of Tokyo, Japan
Abstract. Ordering information is a difficult but a important task for
natural language generation applications. A wrong order of information
not only makes it difficult to understand, but also conveys an entirely
different idea to the reader. This paper proposes an algorithm that learns
orderings from a set of human ordered texts. Our model consists of a set
of ordering experts. Each expert gives its precedence preference between
two sentences. We combine these preferences and order sentences. We
also propose two new metrics for the evaluation of sentence orderings.
Our experimental results show that the proposed algorithm outperforms
the existing methods in all evaluation metrics.
1 Introduction
The task of ordering sentences arises in many fields. Multidocument Summa-
rization (MDS) [5], Question and Answer (QA) systems and concept to text
generation systems are some of them. These systems extract information from
different sources and combine them to produce a coherent text. Proper ordering
of sentences improves readability of a summary [1]. In most cases it is a trivial
task for a human to read a set of sentences and order them coherently. Hu-
mans use their wide background knowledge and experience to decide the order
among sentences. However, it is not an easy task for computers. This paper pro-
poses a sentence ordering algorithm and evaluate its performance with regard
to MDS.
MDS is the task of generating a human readable summary from a given set of
documents. With the increasing amount of texts available in electronic format,
automatic text summarization has become necessary. It can be considered as a
two-stage process. In the first stage the source documments are analyzed and a
set of sentences are extracted. However, the document set may contain repeating
information as well as contradictory information and these challenges should
be considered when extracting sentences for the summary. Researchers have
already investigated this problem and various algorithms exist. The second stage
of MDS creates a coherent summary from this extract. When summarizing a
single document, a naive strategy that arranges extracted sentences according
to the appearance order may yield a coherent summary. However, in MDS the
extracted sentences belong to different source documents. The source documents
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 624?635, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
A Machine Learning Approach to Sentence Ordering 625
may have been written by various authors and on various dates. Therefore we
cannot simply order the sentences according to the position of the sentences in
the original document to get a comprehensible summary.
This second stage of MDS has received lesser attention compared to the
first stage. Chronological ordering; ordering sentences according to the pub-
lished date of the documents they belong to [6], is one solution to this problem.
However, showing that this approach is insufficient, Barzilay [1] proposed an
refined algorithm which integrates chronology ordering with topical relatedness
of documents. Okazaki [7] proposes a improved chronological ordering algorithm
using precedence relations among sentences. His algorithm searches for an order
which satisfies the precedence relations among sentences. In addition to these
studies which make use of chronological ordering, Lapata [3] proposes a prob-
abilistic model of text structuring and its application to the sentence ordering.
Her system calculates the conditional probabilities between sentences from a
corpus and uses a greedy ordering algorithm to arrange sentences according to
the conditional probabilities.
Even though these previous studies proposed different strategies to decide the
sentence ordering, the appropriate way to combine these different methods to
obtain more robust and coherent text remains unknown. In addition to these ex-
isting sentence ordering heuristics, we propose a new method which we shall call
succession in this paper. We then learn the optimum linear combination of these
heuristics that maximises readability of a summary using a set of human-made
orderings. We then propose two new metrics for evaluating sentence orderings;
Weighted Kendall Coefficient and Average Continuity. Comparing with an in-
trinsic evaluation made by human subjects, we perform a quantitative evaluation
using a number of metrics and discuss the possiblity of the automatic evaluation
of sentence orderings.
2 Method
For sentences taken from the same document we keep the order in that docu-
ment as done in single document summarization. However, we have to be careful
when ordering sentences which belong to different documents. To decide the or-
der among such sentences, we implement five ranking experts: Chronological,
Probabilistic, Topical relevance, Precedent and Succedent. These experts return
precedence preference between two sentences. Cohen [2] proposes an elegant
learning model that works with preference functions and we adopt this learn-
ing model to our task. Each expert e generates a pair-wise preference function
defined as following:
PREFe(u, v, Q) ? [0, 1]. (1)
Where, u, v are two sentences that we want to order; Q is the set of sentences
which has been already ordered. The expert returns its preference of u to v. If
the expert prefers u to v then it returns a value greater than 0.5. In the extreme
case where the expert is absolutely sure of preferring u to v it will return 1.0.
On the other hand, if the expert prefers v to u it will return a value lesser than
626 D. Bollegala, N. Okazaki, and M. Ishizuka
0.5. In the extreme case where the expert is absolutely sure of preferring v to u
it will return 0. When the expert is undecided of its preference between u and v
it will return 0.5.
The linear weighted sum of these individual preference functions is taken as
the total preference by the set of experts as follows:
PREFtotal(u, v, Q) =
?
e?E
wePREFe(u, v, Q). (2)
Therein: E is the set of experts and we is the weight associated to expert e ? E.
These weights are normalized so that the sum of them is 1. We use the Hedge
learning algorithm to learn the weights associated with each expert?s preference
function. Then we use the greedy algorithm proposed by Cohen [2] to get an
ordering that approximates the total preference.
2.1 Chronological Expert
Chronological expert emulates conventional chronological ordering [4,6] which
arranges sentences according to the dates on which the documents were published
and preserves the appearance order for sentences in the same document. We
define a preference function for the expert as follows:
PREFchro(u, v, Q) =
?
?
?
?
?
1 T (u) < T (v)
1 [D(u) = D(v)] ? [N(u) < N(v)]
0.5 [T (u) = T (v)] ? [D(u) = D(v)]
0 otherwise
. (3)
Therein: T (u) is the publication date of sentence u; D(u) presents the unique
identifier of the document to which sentence u belongs; N(u) denotes the line
number of sentence u in the original document. Chronological expert gives 1
(preference) to the newly published sentence over the old and to the prior over
the posterior in the same article. Chronological expert returns 0.5 (undecided)
when comparing two sentences which are not in the same article but have the
same publication date.
2.2 Probabilistic Expert
Lapata [3] proposes a probabilistic model to predict sentence order. Her model
assumes that the position of a sentence in the summary depends only upon the
sentences preceding it. For example let us consider a summary T which has
sentences S1, . . . , Sn in that order. The probability P (T ) of getting this order is
given by:
P (T ) =
n
?
i=1
P (Sn|S1, . . . , Sn?i). (4)
She further reduces this probability using bi-gram approximation as follows.
P (T ) =
n
?
i=1
P (Si|Si?1) (5)
A Machine Learning Approach to Sentence Ordering 627
She breaks each sentence into features and takes the vector product of features
as follows:
P (Si|Si?1) =
?
(a<i,j>,a<i?1,k>)?Si?Si?1
P (a<i,j>, a<i?1,k>). (6)
Feature conditional probabilities can be calculated using frequency counts of
features as follows:
P (a<i,j>|a<i?1,k>) =
f(a<i,j>, a<i?1,k>)
?
a<i,j>
f(a<i,j>, a<i?1,k>)
. (7)
Lapata [3] uses nouns,verbs and dependency structures as features. Where as
in our expert we implemented only nouns and verbs as features. We performed
back-off smoothing on the frequency counts in equation 7 as these values were
sparse. Once these conditional probabilities are calculated, for two sentences u,v
we can define the preference function for the probabilistic expert as follows:
PREFprob(u, v, Q) =
{
1+P (u|r)?P (v|r)
2 Q = 
1+P (u)?P (v)
2 Q = 
. (8)
Where, Q is the set of sentences ordered so far and r ? Q is the lastly ordered
sentence in Q. Initially, Q is null and we prefer the sentence with higher absolute
probability. When Q is not null and u is preferred to v, i.e. P (u|r) > P (v|r),
according to definition 8 a preference value greater than 0.5 is returned. If v is
preferred to u, i.e. P (u|r) < P (v|r), we have a preference value smaller than 0.5.
When P (u|r) = P (v|r), the expert is undecided and it gives the value 0.5.
2.3 Topical Relevance Expert
In MDS, the source documents could contain multiple topics. Therefore, the
extracted sentences could be covering different topics. Grouping the extracted
sentences which belong to the same topic, improves readability of the summary.
Motivated by this fact, we designed an expert which groups the sentences which
belong to the same topic. This expert prefers sentences which are more similar
to the ones that have been already ordered. For each sentence l in the extract
we define its topical relevance, topic(l) as follows:
topic(l) = max
q?Q
sim(l, q). (9)
We use cosine similarity to calculate sim(l, q). The preference function of this
expert is defined as follows:
PREFtopic(u, v, Q) =
{
0.5 [Q = ] ? [topic(u) = topic(v)]
1 [Q = ] ? [topic(u) > topic(v)]
0 otherwise
. (10)
Where,  represents the null set, u,v are the two sentences under considera-
tion and Q is the block of sentences that has been already ordered so far in
the summary.
628 D. Bollegala, N. Okazaki, and M. Ishizuka
3
7PQTFGTGF
5GPVGPEGU

.5WOOCT[
Fig. 1. Topical relevance expert
2.4 Precedent Expert
When placing a sentence in the summary it is important to check whether the
preceding sentences convey the necessary background information for this sen-
tence to be clearly understood. Placing a sentence without its context being
stated in advanced, makes an unintelligible summary. As shown in figure 2, for
each extracted sentence l, we can compare the block of text that appears before
it in its source document (P ) with the block of sentences which we have ordered
so far in the summary (Q). If P and Q matches well, then we can safely as-
sume that Q contains the necessary background information required by l. We
can then place l after Q. Such relations among sentences are called precedence
relations. Okazaki [7] proposes precedence relations as a method to improve the
chronological ordering of sentences. He considers the information stated in the
documents preceding the extracted sentences to judge the order. Based on this
idea, we define precedence pre(l) of the extracted sentence l as follows:
pre(l) = max
p?P,q?Q
sim(p, q). (11)
l
2
&QEWOGPV
&5WOOCT[
3
Fig. 2. Precedent expert
Here, P is the set of sentences preceding the extract sentence l in the original
document. We calculate sim(p, q) using cosine similarity. The preference function
for this expert can be written as follows:
PREFpre(u, v, Q) =
{
0.5 [Q = ] ? [pre(u) = pre(v)]
1 [Q = ] ? [pre(u) > pre(v)]
0 otherwise
. (12)
A Machine Learning Approach to Sentence Ordering 629
&QEWOGPV

&
5WOOCT[
3
r
-
7PQTFGTGF
5GPVGPEGU

.
l
Fig. 3. Succedent expert
2.5 Succedent Expert
When extracting sentences from source documents, sentences which are similar to
the ones that are already extracted, are usually ignored to prevent repetition of
information. However, this information is valuable when ordering sentences. For
example, a sentence that was ignored by the sentence extraction algorithm might
turn out to be more suitable when ordering the extracted sentences. However, we
assume that the sentence ordering algorithm is independent from the sentence ex-
traction algorithmand therefore does not possess this knowledge regarding the left
out candidates. This assumption improves the compatibility of our algorithm as it
can be used to order sentences extracted by any sentence extraction algorithm. We
design an expert which uses this information to order sentences.
Let us consider the siuation depicted in Figure 3 where a block Q of text is
orderd in the summary so far. The lastly ordered setence r belongs to document
D in which a block K of sentences follows r. The author of this document assumes
that K is a natural consequence of r. However, the sentence selection algorithm
might not have selected any sentences from K because it already selected some
sentences with this information from some other document. Therefore, we search
the extract L for a sentence that best matches with a sentence in K. We define
succession as a measure of this agreement(13) as follows:
succ(l) = max
k?K
sim(l, k). (13)
Here, we calculate sim(l, k) using cosine similarity. Sentences with higher succes-
sion values are preferred by the expert. The preference function for this expert
can be written as follows:
PREFsucc(u, v, Q) =
{
0.5 [Q = ] ? [succ(u) = succ(v)]
1 [Q = ] ? [succ(u) > succ(v)]
0 otherwise
. (14)
2.6 Ordering Algorithm
Using the five preference functions described in the previous sections, we compute
the total preference function of the set of experts as defined by equation 2. Sec-
tion 2.7 explains the method that we use to calculate the weights assigned to each
expert?s preference. In this section we will consider the problem of finding an order
that satisfies the total preference function. Finding the optimal order for a given
630 D. Bollegala, N. Okazaki, and M. Ishizuka
total preference function is NP-complete [2]. However, Cohen [2] proposes a greedy
algorithm that approximates the optimal ordering. Once the unordered extract X
and total preference (equation 2) are given, this greedy algorithm can be used to
generate an approximately optimal ordering function ??.
let V = X
for each v ? V do
?(v) =
?
u?V
PREF(v, u, Q) ?
?
u?V
PREF(u, v, Q)
while V is non-empty do
let t = arg maxu?V ?(u)
let ??(t) = |V |
V = V ? {t}
for each v ? V do
?(v) = ?(v) + PREF(t, u) ? PREF(v, t)
endwhile
2.7 Learning Algorithm
Cohen [2] proposes a weight allocation algorithm that learns the weights associ-
ated with each expert in equation 2. We shall explain this algorithm in regard
to our model of five experts.
Rate of learning ? ? [0, 1], initial weight vector w1 ? [0, 1]5, s.t.
?
e?E w
1
e = 1.
Do for t = 1, 2, . . . , T where T is the number of training examples.
1. Get Xt; the set of sentences to be ordered.
2. Compute a total order ??t which approximates,
PREFttotal(u, v, Q) =
?
e?E
PREFte(u, v, Q).
We used the greedy ordering algorithm described in section 2.6 to get ??t.
3. Order Xt using ??t.
4. Get the human ordered set F t of Xt. Calculate the loss for each expert.
Loss(PREFte, F
t) = 1 ? 1|F |
?
(u,v)?F
PREFte(u, v, Q) (15)
5. Set the new weight vector,
wt+1e =
wte?
Loss(PREFte,F
t)
Zt
(16)
where, Zt is a normalization constant, chosen so that,
?
e?E w
t+1
e = 1.
A Machine Learning Approach to Sentence Ordering 631
In our experiments we set ? = 0.5 and w1i = 0.2. To explain equation 15 let us
assume that sentence u comes before sentence v in the human ordered summary.
Then the expert must return the value 1 for PREF(u,v,Q). However,if the expert
returns any value less than 1, then the difference is taken as the loss. We do this
for all such sentence pairs in F . For a summary of length N we have N(N ?1)/2
such pairs. Since this loss is taken to the power of ?, a value smaller than 1, the
new weight of the expert gets changed according to the loss as in equation 16.
3 Evaluation
In addition to Kendall?s ? coefficient and Spearman?s rank correlation coefficient
which are widely used for comparing two ranks, we use sentence continuity [7]
as well as two metrics we propose; Weighted Kendall and Average Continuity.
3.1 Weighted Kendall Coefficient
The Kendall?s ? coefficient is defined as following:
? = 1 ? 2Q
nC2
. (17)
Where, Q is the number of discordant pairs and nC2 is the number of combi-
nations that can be generated from a set of n distinct elements by taking two
elements at a time with replacement. However, one major drawback of this met-
ric when evaluating sentence orderings is that, it does not take into consideration
the relative distance d between the discordant pairs. However, when reading a
text a human reader is likely to be more sensitive to a closer discordant pair than
a discordant pair far apart. Therefore, a closer discordant pair is more likely to
harm the readability of the summary compared to a far apart discordant pair. In
order to reflect these differences in our metric, we use an exponentially decreasing
weight function as follows:
h(d) =
{
exp(1 ? d) d ? 1
0 else
. (18)
Here, d is the number of sentences that lie between the two sentences of the
discordant pair. Going by the traditional Kendall?s ? coefficient we defined our
weighted Kendall coefficient as following, so that it becomes a metric in [1, ?1]
range.
?w = 1 ?
2
?
d h(d)
?n
i=1 h(i)
(19)
3.2 Average Continuity
Both Kendall?s ? coefficient and the Weighted Kendall coefficient measure dis-
cordants between ranks. However, in the case of summaries, we need a metric
which expresses the continuity of the sentences. A summary which can be read
632 D. Bollegala, N. Okazaki, and M. Ishizuka
continuously is better compared to a one that cannot. If the ordered extract
contains most of the sentence blocks of the reference summary then we can
safely assume that it is far more readable and coherent to a one that is not.
Sentence n-gram counts of continuous sentences give a rough idea of this kind
of continuity.
For a summary of length N there are N ? n + 1 possible sentence n-grams
of length n. Therefore, we can define a precision Pn of continuity length n as:
Pn =
number of matched n-grams
N ? n + 1 . (20)
Due to sparseness of higher order n-grams Pn decreases in an exponential-like
curve with n. Therefore, we define Average Continuity as the logrithmic average
of Pn as follows:
Average Continuity = exp(
1
3
4
?
n=2
log(Pn)) (21)
We add a small quantity ? to numerator and denominator of Pn in equation
20 so that the logarithm will not diverge when n-grams count is zero. We used
? = 0.01 in our evaluations. Experimental results showed that taking n-grams up
to four gave contrasting results because the n-grams tend to be sparse for larger
n values. BLEU(BiLingual Evaluation Understudy) proposed by Papineni [8]
for the task of evaluating machine translations has an analogical form to our
average continuity. In BLEU, a machine translation is compared against multiple
reference translations and precision values are calculated using word n-grams.
BLEU is then defined as the logarithmic average of these precision values.
4 Results
We used the 3rd Text Summarization Challenge (TSC) corpus for our exper-
iments. TSC1 corpus contains news articles taken from two leading Japanese
newspapers; Mainichi and Yomiuri. TSC-3 corpus contains human selected ex-
tracts for 30 different topics. However, in the TSC corpus the extracted sentences
are not ordered to make a readable summary. Therefore, we first prepared 30
summaries by ordering the extraction data of TSC-3 corpus by hand. We then
compared the orderings by the proposed algorithm against these human ordered
summaries. We used 10-fold cross validation to learn the weights assigned to
each expert in our proposed algorithm. These weights are shown in table 1.
According to table 1, succedent, chronology and precedent experts have the
highest weights among the five experts and therefore almost entirely control the
process of ordering. Whereas probabilistic and topical relevance experts have
almost no influence on their decisions. However, we cannot directly compare La-
pata?s [3] approach with our probabilistic expert as we do not use dependency
1 http://lr-www.pi.titech.ac.jp/tsc/index-en.html
A Machine Learning Approach to Sentence Ordering 633
Table 1. Weights learned
Expert Chronological Probabilistic Topical Relevance Precedent Succedent
Weights 0.327947 0.000039 0.016287 0.196562 0.444102
Table 2. Comparison with Human Ordering
Spearman Kendall Continuity Weighted Kendall Average Continuity
RO -0.267 -0.160 -0.118 -0.003 0.024
PO 0.062 0.040 0.187 0.013 0.029
CO 0.774 0.735 0.629 0.688 0.511
LO 0.783 0.746 0.706 0.717 0.546
HO 1.000 1.000 1.000 1.000 1.000








Identifying Sections in Scientific Abstracts using Conditional Random Fields
Kenji Hirohata?
hirohata@nii.ac.jp
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Sophia Ananiadou?
sophia.ananiadou@manchester.ac.uk
?Graduate School of Information
Science and Technology,
University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656, Japan
Mitsuru Ishizuka?
ishizuka@i.u-tokyo.ac.jp
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre,
131 Princess Street, Manchester M1 7DN, UK
Abstract
OBJECTIVE: The prior knowledge about
the rhetorical structure of scientific abstracts
is useful for various text-mining tasks such
as information extraction, information re-
trieval, and automatic summarization. This
paper presents a novel approach to cate-
gorize sentences in scientific abstracts into
four sections, objective, methods, results,
and conclusions. METHOD: Formalizing
the categorization task as a sequential label-
ing problem, we employ Conditional Ran-
dom Fields (CRFs) to annotate section la-
bels into abstract sentences. The train-
ing corpus is acquired automatically from
Medline abstracts. RESULTS: The pro-
posed method outperformed the previous
approaches, achieving 95.5% per-sentence
accuracy and 68.8% per-abstract accuracy.
CONCLUSION: The experimental results
showed that CRFs could model the rhetor-
ical structure of abstracts more suitably.
1 Introduction
Scientific abstracts are prone to share a similar
rhetorical structure. For example, an abstract usu-
ally begins with the description of background in-
formation, and is followed by the target problem,
solution to the problem, evaluation of the solution,
and conclusion of the paper. Previous studies ob-
served the typical move of rhetorical roles in sci-
entific abstracts: problem, solution, evaluation, and
conclusion (Graetz, 1985; Salanger-Meyer, 1990;
Swales, 1990; Ora?san, 2001). The American Na-
tional Standard Institute (ANSI) recommends au-
thors and editors of abstracts to state the purpose,
methods, results, and conclusions presented in the
documents (ANSI, 1979).
The prior knowledge about the rhetorical structure
of abstracts is useful to improve the performance of
various text-mining tasks. Marcu (1999) proposed
an extraction method for summarization that cap-
tured the flow of text, based on Rhetorical Struc-
ture Theory (RST). Some extraction methods make
use of cue phrases (e.g., ?in conclusion?, ?our in-
vestigation has shown that ...?), which suggest that
the rhetorical role of sentences is to identify im-
portant sentences (Edmundson, 1969; Paice, 1981).
We can survey the problems, purposes, motivations,
and previous approaches of a research field by read-
ing texts in background sections of scientific papers.
Tbahriti (2006) improved the performance of their
information retrieval engine, giving more weight to
sentences referring to purpose and conclusion.
In this paper, we present a supervised machine-
learning approach that categorizes sentences in sci-
entific abstracts into four sections, objective, meth-
ods, results, and conclusions. Figure 1 illustrates
the task of this study. Given an unstructured ab-
stract without section labels indicated by boldface
type, the proposed method annotates section labels
of each sentence. Assuming that this task is well
formalized as a sequential labeling problem, we use
Conditional Random Fields (CRFs) (Lafferty et al,
2001) to identify rhetorical roles in scientific ab-
stracts.The proposed method outperforms previous
approaches to this problem, achieving 95.5% per-
381
OBJECTIVE: This study assessed the role of adrenergic signal transmission in the control of renal erythropoietin (EPO) pro-
duction in humans. METHODS: Forty-six healthy male volunteers underwent a hemorrhage of 750 ml. After phlebotomy, they
received (intravenously for 6 hours in a parallel, randomized, placebo-controlled and single-blind design) either placebo (0.9%
sodium chloride), or the beta 2-adrenergic receptor agonist fenoterol (1.5 microgram/min), or the beta 1-adrenergic receptor ago-
nist dobutamine (5 micrograms/kg/min), or the nonselective beta-adrenergic receptor antagonist propranolol (loading dose of 0.14
mg/kg over 20 minutes, followed by 0.63 micrograms/kg/min). RESULTS: The AUCEPO(0-48 hr)fenoterol was 37% higher (p ?
0.03) than AUCEPO(0-48 hr)placebo, whereas AUCEPO(0-48 hr)dobutamine and AUCEPO(0-48 hr)propranolol were comparable
with placebo. Creatinine clearance was significantly increased during dobutamine treatment. Urinary cyclic adenosine monophos-
phate excretion was increased only by fenoterol treatment, whereas serum potassium levels were decreased. Plasma renin activity
was significantly increased during dobutamine and fenoterol infusion. CONCLUSIONS: This study shows in a model of con-
trolled, physiologic stimulation of renal erythropoietin production that the beta 2-adrenergic receptor agonist fenoterol but not the
beta 1-adrenergic receptor agonist dobutamine is able to increase erythropoietin levels in humans. The result can be interpreted as
a hint that signals for the control of erythropoietin production may be mediated by beta 2-adrenergic receptors rather than by beta
1-adrenergic receptors. It appears to be unlikely that an increase of renin concentrations or glomerular filtration rate is causally
linked to the control of erythropoietin production in this experimental setting.
Figure 1: An abstract with section labels indicated by boldface type (Gleiter et al, 1997).
sentence accuracy and 68.8% per-abstract accuracy.
This paper is organized as follows. Section 2
describes previous approaches to this task. For-
malizing the task as a sequential-labeling problem,
Section 3 designs a sentence classifier using CRFs.
Training corpora for the classifier are acquired au-
tomatically from the Medline abstracts. Section 4
reports considerable improvements in the proposed
method over the baseline method using Support Vec-
tor Machine (SVM) (Cortes and Vapnik, 1995). We
conclude this paper in Section 5.
2 Related Work
The previous studies regarded the task of identify-
ing section names as a text-classification problem
that determines a label (section name) for each sen-
tence. Various classifiers for text categorization,
Na??ve Bayesian Model (NBM) (Teufel and Moens,
2002; Ruch et al, 2007), Hidden Markov Model
(HMM) (Wu et al, 2006; Lin et al, 2006), and Sup-
port Vector Machines (SVM) (McKnight and Arini-
vasan, 2003; Shimbo et al, 2003; Ito et al, 2004;
Yamamoto and Takagi, 2005) were applied.
Table 1 summarizes these approaches and perfor-
mances. All studies target scientific abstracts except
for Teufel and Moens (2002) who target scientific
full papers. Field classes show the set of section
names that each study assumes: background (B),
objective/aim/purpose (O), method (M), result (R),
conclusion (C), and introduction (I) that combines
the background and objective. Although we should
not compare directly the performances of these stud-
ies, which use a different set of classification labels
and evaluation corpora, SVM classifiers appear to
yield better results for this task. The rest of this sec-
tion elaborates on the previous studies with SVMs.
Shimbo et al (2003) presented an advanced text
retrieval system for Medline that can focus on a
specific section in abstracts specified by a user.
The system classifies sentences in each Medline ab-
stract into four sections, objective, method, results,
and conclusion. Each sentence is represented by
words, word bigrams, and contextual information of
the sentence (e.g., class of the previous sentence,
relative location of the current sentence). They
reported 91.9% accuracy (per-sentence basis) and
51.2% accuracy (per-abstract basis1) for the clas-
sification with the best feature set for quadratic
SVM. Ito et al (2004) extended the work with a
semi-supervised learning technique using transduc-
tive SVM (TSVM).
Yamamoto and Takagi (2005) developed a sys-
tem to classify abstract sentences into five sections,
background, purpose, method, result, and conclu-
sion. They trained a linear-SVM classifier with fea-
tures such as unigram, subject-verb, verb tense, rel-
ative sentence location, and sentence score (average
TF*IDF score of constituent words). Their method
achieved 68.9%, 63.0%, 83.6%, 87.2%, 89.8% F-
scores for classifying background, purpose, method,
result, and conclusion sentences respectively. They
also reported the classification performance of intro-
duction sentences, which combines background and
purpose sentences, with 91.3% F-score.
1An abstract is considered correct if all constituent sentences
are correctly labeled.
382
Methods Model Classes Performance (reported in papers)
Teufel and Moens (2002) NBM (7 classes) 44% precision and 65% recall for aim sentences
Ruch et al (2007) NBM O M R C 85% F-score for conclusion sentences
Wu et al (2006) HMM B O M R C 80.54% precision
Lin et al (2006) HMM I M R C 88.5%, 84.3%, 89.8%, 89.7% F-scores
McKnight and Srinivasan (2003) SVM I M R C 89.2%, 82.0%, 82.1%, 89.5% F-scores
Shimbo et al (2003) SVM B O M R C 91.9% accuracy
Ito et al (2004) TSVM B O M R C 66.0%, 51.0%, 49.3%, 72.9%, 67.7% F-scores
Yamamoto and Takagi (2005) SVM I (B O) M R C 91.3% (68.9%, 63.0%), 83.6%, 87.2%, 89.8% F-scores
Table 1: Approaches and performances of previous studies on section identification
3 Proposed method
3.1 Section identification as a sequence labeling
problem
The previous work saw the task of labeling as a text
categorization that determines the class label yi for
each sentence xi. Even though some work includes
features of the surrounding sentences for xi, e.g.
?class label of xi?1 sentence,? ?class label of xi+1
sentence,? and ?unigram in xi?1 sentence,? the clas-
sifier determines the class label yi for each sentence
xi independently. It has been an assumption for text
classification tasks to decide a class label indepen-
dently of other class labels.
However, as described in Section 1, scientific ab-
stracts have typical moves of rhetorical roles: it
would be very peculiar if result sentences appear-
ing before method sentences were described in an
abstract. Moreover, we would like to model the
structure of abstract sentences rather than model-
ing just the section label for each sentence. Thus,
the task is more suitably formalized as a sequence
labeling problem: given an abstract with sentences
x = (x1, ..., xn), determine the optimal sequence of
section names y = (y1, ..., yn) of all possible se-
quences.
Conditional Random Fields (CRFs) have been
successfully applied to various NLP tasks includ-
ing part-of-speech tagging (Lafferty et al, 2001) and
shallow parsing (Sha and Pereira, 2003). CRFs de-
fine a conditional probability distribution p(y|x) for
output and input sequences, y and x,
p(y|x) =
1
Z?(x)
exp {? ? F (y,x)} . (1)
Therein: function F (y,x) denotes a global feature
vector for input sequence x and output sequence y,
F (y,x) =
?
i
f(y,x, i), (2)
i ranges over the input sequence, function f(y,x, i)
is a feature vector for input sequence x and output
sequence y at position i (based on state features and
transition features), ? is a vector where an element
?k represents the weight of feature Fk(y,x), and
Z?(x) is a normalization factor,
Z?(x) =
?
y
exp {? ? F (y,x)} . (3)
The optimal output sequence y? for an input se-
quence x,
y? = argmax
y
p(y|x), (4)
is obtained efficiently by the Viterbi algorithm. The
optimal set of parameters ? is determined efficiently
by the Generalized Iterative Scaling (GIS) (Darroch
and Ratcliff, 1972) or Limited-memory Broyden-
Fletcher-Goldfarb-Shanno (L-BFGS) (Nocedal and
Wright, 1999) method.
3.2 Features
We design three kinds of features to represent each
abstract sentence for CRFs. The contributions of
these features will be evaluated later in Section 4.
Content (n-gram) This feature examines the exis-
tence of expressions that characterize a specific sec-
tion, e.g. ?to determine ...,? and ?aim at ...? for stat-
ing the objective of a study. We use features for sen-
tence contents represented by: i) words, ii) word bi-
grams, and iii) mixture of words and word bigrams.
Words are normalized into their base forms by the
GENIA tagger (Tsuruoka and Tsujii, 2005), which
is a part-of-speech tagger trained for the biomedical
383
Rank OBJECTIVE METHOD RESULTS CONCLUSIONS
1 # to be measure % ) suggest that
2 be to be perform ( p may be
3 to determine n = p < # these
4 study be be compare ) . should be
5 this study be determine % . these result
Table 2: Bigram features with high ?2 values (?#? stands for a beginning of a sentence).
domain. We measure the co-occurrence strength (?2
value) between each feature and section label. If a
feature appears selectively in a specific section, the
?2 value is expected to be high. Thus, we extract the
top 200,000 features2 that have high ?2 values to re-
duce the total number of features. Table 3.2 shows
examples of the top five bigrams that have high ?2
values.
Relative sentence location An abstract is likely to
state objective of the study at the beginning and its
conclusion at the end. The position of a sentence
may be a good clue for determining its section la-
bel. Thus, we design five binary features to indicate
relative position of sentences in five scales.
Features from previous/next w sentences This
reproduces features from previous and following w
sentences to the current sentence (w = {0, 1, 2}),
so that a classifier can make use of the content of
the surrounding sentences. Duplicated features have
prefixes (e.g. PREV_ and NEXT_) to distinguish
their origins.
3.3 Section labels
It would require much effort and time to prepare a
large amount of abstracts annotated with section la-
bels. Fortunately, some Medline abstracts have sec-
tion labels stated explicitly by its authors. We ex-
amined section labels in 7,811,582 abstracts in the
whole Medline3, using the regular-expression pat-
tern:
?[A-Z]+([ ][A-Z]+){0,3}:[ ]
A sentence is qualified to have a section name if it
begins with up to 4 uppercase token(s) followed by
2We chose the number of features based on exploratory ex-
periments.
3The Medline database was up-to-date on March 2006.
a colon ?:?. This pattern identified 683,207 (ca. 9%)
abstracts with structured sections.
Table 3 shows typical moves of sections in Med-
line abstracts. The majority of sequences in this
table consists of four sections compatible with the
ANSI standard, purpose, methods, results, and con-
clusions. Moreover, the most frequent sequence
is ?OBJECTIVE ? METHOD(S) ? RESULTS
? CONCLUSION(S),? supposing that AIM and
PURPOSE are equivalent to OBJECTIVE. Hence,
this study assumes four sections, OBJECTIVE,
METHOD, RESULTS, and CONCLUSIONS.
Meanwhile, it is common for NP chunking tasks
to represent a chunk (e.g., NP) with two labels,
the begin (e.g., B-NP) and inside (e.g., I-NP) of
a chunk (Ramshaw and Marcus, 1995). Although
none of the previous studies employed this repre-
sentation, attaching B- and I- prefixes to section la-
bels may improve a classifier by associating clue
phrases (e.g., ?to determine?) with the starts of sec-
tions (e.g., B-OBJECTIVE). We will compare clas-
sification performances on two sets of label repre-
sentations: namely, we will compare four section
labels and eight labels with BI prefixes attached to
section names.
4 Evaluation
4.1 Experiment
We constructed two sets of corpora (?pure? and ?ex-
panded?), each of which contains 51,000 abstracts
sampled from the abstracts with structured sections.
The ?pure? corpus consists of abstracts that have the
exact four section labels. In other words, this cor-
pus does not include AIM or PURPOSE sentences
even though they are equivalent to OBJECTIVE sen-
tences. The ?pure? corpus is useful to compare the
performance of this study with the previous work.
384
Rank # abstracts (%) Section sequence
1 111,617 (17.6) OBJECTIVE?METHOD(S)? RESULT(S)? CONCLUSION(S)
2 107,124 (16.9) BACKGROUND(S)?METHOD(S)? RESULT(S)? CONCLUSION(S)
3 40,083 (6.3) PURPOSE?METHOD(S)? RESULT(S)? CONCLUSION(S)
4 20,519 (3.2) PURPOSE?MATERIAL AND METHOD(S)? RESULT(S)? CONCLUSION(S)
5 16,705 (2.6) AIM(S)?METHOD(S)? RESULT(S)? CONCLUSION(S)
6 16,400 (2.6) BACKGROUND? OBJECTIVE?METHOD(S)? RESULT(S)? CONCLUSION(S)
7 12,227 (1.9) OBJECTIVE? STUDY DESIGN? RESULT(S)? CONCLUSION(S)
8 11,483 (1.8) BACKGROUND?METHOD(S) AND RESULT(S)? CONCLUSION(S)
9 8,866 (1.4) OBJECTIVE?MATERIAL AND METHOD(S)? RESULT(S)? CONCLUSION(S)
10 8,537 (1.3) PURPOSE? PATIENT AND METHOD(S)? RESULT(S)? CONCLUSION(S)
.. ... ... ...
Total 683,207 (100.0)
Table 3: Typical sequences of sections in Medline abstracts
Representative Equivalent section labels
OBJECTIVE AIM, AIM OF THE STUDY, AIMS, BACKGROUND/AIMS, BACKGROUND/PURPOSE, BACK-
GROUND, BACKGROUND AND AIMS, BACKGROUND AND OBJECTIVE, BACKGROUND AND
OBJECTIVES, BACKGROUND AND PURPOSE, CONTEXT, INTRODUCTION, OBJECT, OBJEC-
TIVE, OBJECTIVES, PROBLEM, PURPOSE, STUDY OBJECTIVE, STUDY OBJECTIVES, SUM-
MARY OF BACKGROUND DATA
METHOD ANIMALS, DESIGN, DESIGN AND METHODS, DESIGN AND SETTING, EXPERIMENTAL DE-
SIGN,INTERVENTION, INTERVENTION(S), INTERVENTIONS, MATERIAL AND METHODS, MA-
TERIALS AND METHODS, MEASUREMENTS, METHOD, METHODOLOGY, METHODS, METH-
ODS AND MATERIALS, PARTICIPANTS, PATIENT(S), PATIENTS, PATIENTS AND METHODS,
PROCEDURE, RESEARCH DESIGN AND METHODS, SETTING, STUDY DESIGN, STUDY DESIGN
AND METHODS, SUBJECTS, SUBJECTS AND METHODS
RESULTS FINDINGS, MAIN RESULTS, RESULT, RESULT(S), RESULTS
CONCLUSIONS CONCLUSION, CONCLUSION(S), CONCLUSIONS, CONCLUSIONS AND CLINICAL RELE-
VANCE, DISCUSSION, IMPLICATIONS, INTERPRETATION, INTERPRETATION AND CONCLU-
SIONS
Table 4: Representative section names and their expanded sections
In contrast, the ?expanded? corpus includes sen-
tences in equivalent sections: AIM and PURPOSE
sentences are mapped to the OBJECTIVE. Table 4
shows the sets of equivalent sections for representa-
tive sections. We created this mapping table man-
ually by analyzing the top 100 frequent section la-
bels found in the Medline. The ?expanded? corpus
is close to the real situation in which the proposed
method annotates unstructured abstracts.
We utilized FlexCRFs4 implementation to build
a classifier with linear-chain CRFs. As a baseline
method, we also prepared an SVM classifier5 with
the same features.
4Flexible Conditional Random Field Toolkit (FlexCRFs):
http://flexcrfs.sourceforge.net/
5We used SVMlight implementation with the linear kernel,
which achieved the best accuracy through this experiment:
http://svmlight.joachims.org/
 20
 30
 40
 50
 60
 70
 80
 1000  10000  100000
Pe
r-a
bs
tra
ct
 a
cc
ur
ac
y 
(%
)
Number of abstracts for training
CRF
SVM
Figure 2: Training curve
4.2 Results
Given the number of abstracts for training n, we ran-
domly sampled n abstracts from a corpus for train-
ing and 1,000 abtracts for testing. Content (n-gram)
features were generated for each trainig set. We
385
Section labels With B- and I- prefixes Without B- and I- prefixes
Features CRF SVM CRF SVM
n-gram 88.7 (42.4) 81.5 (19.1) 85.7 (33.0) 83.3 (23.4)
n-gram + position 93.4 (59.7) 88.2 (35.5) 92.4 (55.4) 89.6 (39.4)
n-gram + surrounding (w = 1) 93.3 (60.4) 89.9 (42.2) 92.1 (52.8) 90.0 (42.0)
n-gram + surrounding (w = 2) 93.7 (61.1) 91.8 (49.4) 92.8 (54.3) 91.8 (47.0)
Full 94.3 (62.9) 93.3 (55.5) 93.3 (56.1) 92.9 (52.2)
Table 5: Classification performance (accuracy) on ?pure? corpus (n = 10, 000)
Section labels With B- and I- prefixes Without B- and I- prefixes
Features CRF SVM CRF SVM
n-gram 87.7 (35.6) 78.5 (14.5) 81.9 (21.0) 80.0 (16.2)
n-gram + position 92.6 (54.3) 87.1 (31.2) 91.4 (48.7) 88.1 (31.2)
n-gram + surrounding (w = 1) 92.3 (52.0) 88.5 (37.6) 89.9 (44.0) 88.4 (37.1)
n-gram + surrounding (w = 2) 92.4 (52.5) 90.1 (41.1) 91.2 (46.6) 90.4 (41.6)
Full 93.0 (55.0) 92.0 (47.3) 92.5 (50.9) 91.7 (44.0)
Table 6: Classification performance (accuracy) on ?expanded? corpus (n = 10, 000)
measured the classification accuracy of sentences
(per-sentence accuracy) and abstracts (per-abstract
accuracy). In per-abstract accuracy, an abstract is
considered correct if all constituent sentences are
correctly labeled.
Trained with n = 50, 000 abstracts from ?pure?
corpus, the proposed method achieved 95.5% per-
sentence accuracy and 68.8% per-abstract accuracy.
The F-score for each section label was 98.7% (O),
95.8% (M), 95.0% (R), and 94.2% (C). The pro-
posed method performed this task better than the
previous studies by a great margin. Figure 2 shows
the training curve for the ?pure? corpus with all fea-
tures presented in this paper. CRF and SVM meth-
ods performed better with more abstracts used for
training. This training curve demonstrated that, with
less than half the number of training corpus, the pro-
posed method could achieve the same accuracy as
the baseline method.
Tables 5 and 6 report the performance of the
proposed and baseline methods on ?pure? and ?ex-
panded? corpora respectively (n = 10, 000). These
tables show per-sentence accuracy followed by per-
abstract accuracy in parentheses with different con-
figurations of features (row) and label representa-
tions (column). For example, the proposed method
obtained 94.3% per-sentence accuracy and 62.9%
per-abstract accuracy with 10,000 training abstracts
from ?pure? corpus, all features, and BI prefixes for
class labels.
The proposed method outperformed the baseline
method in all experimental configurations. This
suggests that CRFs are more suitable for modeling
moves of rhetorical roles in scientific abstracts. It
is noteworthy that the CRF classifier gained higher
per-abstract accuracy than the SVM. For example,
both the CRF classifier with features from surround-
ing sentences (w = 1), and SVM classifier with full
features, obtained 93.3% per-sentence accuracy in
Table 5. Nevertheless, the per-abstract accuracies of
the former and latter were 60.4% and 55.5% respec-
tively: the CRF classifier had roughly 5% advantage
on per-abstract accuracy over SVM. This analysis
reflects the capability of CRFs to determine the op-
timal sequence of section names.
Additional features such as sentence position and
surrounding sentences improved the performance by
ca. 5?10%. The proposed method achieved the best
results with all features. Another interesting discus-
sion arises with regard to the representations of sec-
tion labels. The BI representation always boosted
the per-abstract accuracy of CRF classifiers by ca.
4?14%. In contrast, the SVM classifier could not
leverage the BI representation, and in some configu-
rations, even degraded the accuracy.
386
5 Conclusion
This paper presented a novel approach to identifying
rhetorical roles in scientific abstracts using CRFs.
The proposed method achieved more successful re-
sults than any other previous reports. The CRF clas-
sifier had roughly 5% advantage on per-abstract ac-
curacy over SVM. The BI representation of section
names also boosted the classification accuracy by
5%. In total, the proposed method gained more than
10% improvement on per-abstract accuracy.
We have evaluated the proposed method only on
medical literatures. In addition to improving the
classification performance, a future direction for this
study would be to examine the adaptability of the
proposed method to include other types of texts. We
are planning to construct a summarization system
using the proposed method.
References
ANSI. 1979. American national standard for writing
abstracts. Z39.14-1979, American National Standards
Institute (ANSI).
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
John N. Darroch and Douglas Ratcliff. 1972. General-
ized iterative scaling for log-linear models. The An-
nals of Mathematical Statistics, 43(5):1470?1480.
Harold P. Edmundson. 1969. New methods in automatic
extracting. Journal of the Association for Computing
Machinery, 16(2):264?285.
Christoph H. Gleiter, Tilmann Becker, Katharina H.
Schreeb, Stefan Freudenthaler, and Ursula Gundert-
Remy. 1997. Fenoterol but not dobutamine increases
erythropoietin production in humans. Clinical Phar-
macology & Therapeutics, 61(6):669?676.
Naomi Graetz. 1985. Teaching EFL students to extract
structural information from abstracts. In Jan M. Ulijn
and Anthony K. Pugh, editors, Reading for Profes-
sional Purposed: Methods and Materials in Teaching
Languages, pages 123?135. Acco, Leuven, Belgium.
Takahiko Ito, Masashi Simbo, Takahiro Yamasaki, and
Yuji Matsumoto. 2004. Semi-supervised sentence
classification for medline documents. In IPSJ SIG
Technical Report, volume 2004-ICS-138, pages 141?
146.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning (ICML-2001), pages 282?289.
Jimmy Lin, Damianos Karakos, Dina Demner-Fushman,
and Sanjeev Khudanpur. 2006. Generative con-
tent models for structural analysis of medical ab-
stracts. In Proceedings of the HLT/NAACL 2006
Workshop on Biomedical Natural Language Process-
ing (BioNLP?06), pages 65?72, New York City, USA.
Daniel Marcu. 1999. Discourse trees are good indicators
of importance in text. In Inderjeet Mani and Mark T.
Maybury, editors, Advances in Automatic Text Summa-
rization. MIT Press.
Larry McKnight and Padmini Arinivasan. 2003. Cate-
gorization of sentence types in medical abstracts. In
AMIA 2003 Symposium Proceedings, pages 440?444.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
Optimization. Springer-Verlag, New York, USA.
Constantin Ora?san. 2001. Patterns in scientific abstracts.
In Proceedings of Corpus Linguistics 2001 Confer-
ence, pages 433 ? 443, Lancaster University, Lan-
caster, UK.
Chris D. Paice. 1981. The automatic generation of litera-
ture abstracts: an approach based on the identification
of self-indicating phrases. In SIGIR ?80: Proceedings
of the 3rd annual ACM conference on Research and
development in information retrieval, pages 172?191,
Kent, UK. Butterworth & Co.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Pro-
ceedings of the ACL 3rd Workshop on Very Large Cor-
pora, pages 82?94.
Patrick Ruch, Celia Boyer, Christine Chichester, Imad
Tbahriti, Antoine Geissbu?hler, Paul Fabry, Julien Gob-
eill, Violaine Pillet, Dietrich Rebholz-Schuhmann,
Christian Lovis, and Anne-Lise Veuthey. 2007. Using
argumentation to extract key sentences from biomedi-
cal abstracts. International Journal of Medical Infor-
matics, 76(2?3):195?200.
Franc?oise Salanger-Meyer. 1990. Discoursal flaws
in medical english abstracts: A genre analysis per
research- and text-type. Text, 10(4):365?384.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL ?03: Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 134?
141, Edmonton, Canada.
387
Masashi Shimbo, Takahiro Yamasaki, and Yuji Mat-
sumoto. 2003. Using sectioning information for text
retrieval: a case study with the medline abstracts. In
Proceedings of Second International Workshop on Ac-
tive Mining (AM?03), pages 32?41.
John M. Swales, 1990. Genre Analysis: English in aca-
demic and research settings, chapter 6. Cambridge
University Press, UK.
Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,
and Patrick Ruch. 2006. Using argumentation to re-
trieve articles with similar citations: An inquiry into
improving related articles search in the medline digital
library. International Journal OF Medical Informat-
ics, 75(6):488?495.
Simone Teufel and Marc Moens. 2002. Summa-
rizing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy for
tagging sequence data. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 467?474, Vancouver, British Columbia, Canada.
Jien-Chen Wu, Yu-Chia Chang, Hsien-Chin Liou, and
Jason S. Chang. 2006. Computational analysis of
move structures in academic abstracts. In Proceed-
ings of the COLING/ACL on Interactive presentation
sessions, pages 41?44, Sydney, Australia.
Yasunori Yamamoto and Toshihisa Takagi. 2005. A sen-
tence classification system for multi-document sum-
marization in the biomedical domain. In Proceedings
of the International Workshop on Biomedical Data En-
gineering (BMDE2005), pages 90?95.
388
A Discriminative Approach to Japanese Abbreviation Extraction
Naoaki Okazaki?
okazaki@is.s.u-tokyo.ac.jp
Mitsuru Ishizuka?
ishizuka@i.u-tokyo.ac.jp
Jun?ichi Tsujii??
tsujii@is.s.u-tokyo.ac.jp
?Graduate School of Information
Science and Technology,
University of Tokyo
7-3-1 Hongo, Bunkyo-ku,
Tokyo 113-8656, Japan
?School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre,
131 Princess Street, Manchester M1 7DN, UK
Abstract
This paper addresses the difficulties in rec-
ognizing Japanese abbreviations through the
use of previous approaches, examining ac-
tual usages of parenthetical expressions in
newspaper articles. In order to bridge the
gap between Japanese abbreviations and
their full forms, we present a discrimina-
tive approach to abbreviation recognition.
More specifically, we formalize the abbrevi-
ation recognition task as a binary classifica-
tion problem in which a classifier determines
a positive (abbreviation) or negative (non-
abbreviation) class, given a candidate of ab-
breviation definition. The proposed method
achieved 95.7% accuracy, 90.0% precision,
and 87.6% recall on the evaluation corpus
containing 7,887 (1,430 abbreviations and
6,457 non-abbreviation) instances of paren-
thetical expressions.
1 Introduction
Human languages are rich enough to be able to
express the same meaning through different dic-
tion; we may produce different sentences to convey
the same information by choosing alternative words
or syntactic structures. Lexical resources such as
WordNet (Miller et al, 1990) enhance various NLP
applications by recognizing a set of expressions re-
ferring to the same entity/concept. For example, text
retrieval systems can associate a query with alterna-
tive words to find documents where the query is not
obviously stated.
Abbreviations are among a highly productive type
of term variants, which substitutes fully expanded
terms with shortened term-forms. Most previous
studies aimed at establishing associations between
abbreviations and their full forms in English (Park
and Byrd, 2001; Pakhomov, 2002; Schwartz and
Hearst, 2003; Adar, 2004; Nadeau and Turney,
2005; Chang and Schu?tze, 2006; Okazaki and Ana-
niadou, 2006). Although researchers have proposed
various approaches to solving abbreviation recog-
nition through methods such as deterministic algo-
rithm, scoring function, and machine learning, these
studies rely on the phenomenon specific to English
abbreviations: all letters in an abbreviation appear in
its full form.
However, abbreviation phenomena are heavily de-
pendent on languages. For example, the term one-
segment broadcasting is usually abbreviated as one-
seg in Japanese; English speakers may find this pe-
culiar as the term is likely to be abbreviated as 1SB
or OSB in English. We show that letters do not pro-
vide useful clues for recognizing Japanese abbrevia-
tions in Section 2. Elaborating on the complexity of
the generative processes for Japanese abbreviations,
Section 3 presents a supervised learning approach to
Japanese abbreviations. We then evaluate the pro-
posed method on a test corpus from newspaper arti-
cles in Section 4 and conclude this paper.
2 Japanese Abbreviation Survey
Researchers have proposed several approaches to
abbreviation recognition for non-alphabetical lan-
guages. Hisamitsu and Niwa (2001) compared dif-
ferent statistical measures (e.g., ?2 test, log like-
889
Table 1: Parenthetical expressions used in Japanese newspaper articles
lihood ratio) to assess the co-occurrence strength
between the inner and outer phrases of parenthet-
ical expressions X (Y). Yamamoto (2002) utilized
the similarity of local contexts to measure the para-
phrase likelihood of two expressions based on the
distributional hypothesis (Harris, 1954). Chang and
Teng (2006) formalized the generative processes of
Chinese abbreviations with a noisy channel model.
Sasano et al (2007) designed rules about letter types
and occurrence frequency to collect lexical para-
phrases used for coreference resolution.
How are these approaches effective in recogniz-
ing Japanese abbreviation definitions? As a prelimi-
nary study, we examined abbreviations described in
parenthetical expressions in Japanese newspaper ar-
ticles. We used the 7,887 parenthetical expressions
that occurred more than eight times in Japanese ar-
ticles published by the Mainichi Newspapers and
Yomiuri Shimbun in 1998?1999. Table 1 summa-
rizes the usages of parenthetical expressions in four
groups. The field ?para? indicates whether the inner
and outer elements of parenthetical expressions are
interchangeable.
The first group acronym (I) reduces a full form to
a shorter form by removing letters. In general, the
process of acronym generation is easily interpreted:
the left example in Table 1 consists of two Kanji let-
ters taken from the heads of the two words, while
the right example consists of the letters at the end of
the 1st, 2nd, and 4th words in the full form. Since
all letters in an acronym appear in its full form, pre-
vious approaches to English abbreviations are also
applicable to Japanese acronyms. Unfortunately, in
this survey the number of such ?authentic? acronyms
amount to as few as 90 (1.2%).
The second group acronym with translation (II) is
characteristic of non-English languages. Full forms
are imported from foreign terms (usually in En-
glish), but inherit the foreign abbreviations. The
third group alias (III) presents generic paraphrases
that cannot be interpreted as abbreviations. For ex-
ample, Democratic People?s Republic of Korea is
known as its alias North Korea. Even though the
formal name does not refer to the ?northern? part, the
alias consists of Korea, and the locational modifier
North. Although the second and third groups retain
their interchangeability, computers cannot recognize
abbreviations with their full forms based on letters.
The last group (IV) does not introduce inter-
changeable expressions, but presents additional in-
formation for outer phrases. For example, a location
usage of a parenthetical expression X (Y) describes
an entity X, followed by its location Y. Inner and
outer elements of parenthetical expressions are not
interchangeable. We regret to find that as many as
81.9% of parenthetical expressions were described
for this usage. Thus, this study regards acronyms
(with and without translation) and alias as Japanese
890
Table 2: Top 10 frequent parenthetical expressions
used in Japanese newspapers from 1998?1999
abbreviations in a broad sense, based on their in-
terchangeabilities. In other words, the goal of this
study is to classify parenthetical expressions X (Y)
into true abbreviations (groups I, II, III) and other
usages of parentheses (group IV).
How much potential do statistical approaches
have to identify Japanese abbreviations? Table 2
shows the top 10 most frequently appearing paren-
thetical expressions in this survey. The ?class? field
represents the category1: T: acronym with transla-
tion, A: alias, and O: non-abbreviation. The most
frequently occurring parenthetical expression was
Democratic People?s Republic of Korea (North Ko-
rea) (4,160 occurrences). 7 instances in the table
were acronyms with translation (#2?5, #7?8), and
an alias (#1), but 3 non-abbreviation instances (#6,
#9, and #10) expressed nationalities of information
sources. Even if we designed a simple method
to choose the top 10 parenthetical expressions, the
recognition performance would be no greater than
70% precision.
3 A discriminative approach to
abbreviation recognition
In order to bridge the gap between Japanese abbre-
viations and their full forms, we present a discrim-
inative approach to abbreviation recognition. More
specifically, we formalize the abbreviation recogni-
tion task as a binary classification problem in which
1No acronym was included in the top 10 list.
Figure 1: Paraphrase occurrence with parentheses
a classifier determines a positive (abbreviation) or
negative (non-abbreviation) class, given a parenthet-
ical expression X (Y). We model the classifier by
using Support Vector Machines (SVMs) (Vapnik,
1998). The classifier combines features that char-
acterize various aspects of abbreviation definitions.
Table 3 shows the features and their values for the
abbreviation EU, and its full form: O-shu Rengo
(European Union). A string feature is converted into
a set of boolean features, each of which indicates
?true? or ?false? of the value. Due to the space limita-
tion, the rest of this section elaborates on paraphrase
ratio and SKEW features.
Paraphrase ratio Let us consider the situation in
which an author describes an abbreviation definition
X (Y) to state a paraphrase X ? Y in a document.
The effect of the statement is to define the meaning
of the abbreviation Y as X in case the reader may
be unaware/uncertain of the abbreviation Y. For ex-
ample, if an author wrote a parenthetical expression,
Multi-Document Summarization (MDS), in a docu-
ment, readers would recognize the meaning of the
expression MDS. Even if they were aware of the def-
inition, MDS alone would be ambiguous; it could
stand for Multi Dimensional Scaling, Missile De-
fense System, etc. Therefore, an author rarely uses
the expression Y before describing its definition.
At the same time, the author would use the expres-
sion Y more than X after describing the definition, if
it were to declare the abbreviation Y for X. Figure 1
illustrates this situation with two documents. Doc-
ument (a) introduces the abbreviation EU for Euro-
pean Union because the expression EU occurs more
frequently than European Union after the parentheti-
cal expression. In contrast, the parenthetical expres-
891
Feature Type Description Example
PR(X,Y ) numeric Paraphrase ratio 0.426
SKEW(X,Y ) numeric Similarity of local contexts measured by the skew divergence 1.35
freq(X) numeric Frequency of occurrence of X 2,638
freq(Y ) numeric Frequency of occurrence of Y 8,326
freq(X,Y ) numeric Frequency of co-occurrence of X and Y 3,121
?2(X,Y ) numeric Co-occurrence strength measured by the ?2 test 2,484,521
LLR(X,Y ) numeric Co-occurrence strength measured by the log-likelihood ratio 6.8
match(X,Y ) boolean Predicate to test whether X contains all letters in Y 0
Letter types string Pair of letter types of X and Y Kanji/Alpha
First letter string The first letter in the abbreviation Y E
Last letter string The last letter in the abbreviation Y U
POS tags string Pair of POS tags for X and Y NNP/NNP
POS categories string Pair of POS categories for X and Y NN/NN
NE tags string Pair of NE tags for X and Y ORG/ORG
Table 3: Features for the SVM classifier and their values for the abbreviation EU.
sion in document (b) describes the property (nation-
ality) of a person Beckham.
Suppose that we have a document that has a par-
enthetical expression with expressionsX and Y . We
regard a document introducing an abbreviation Y for
X if the document satisfies both of these conditions:
1. The expression Y appears more frequently than
the expression X does after the definition pat-
tern.
2. The expression Y does not appear before the
definition pattern.
Formula 1 assesses the paraphrase ratio of the ex-
pressions X and Y,
PR(X,Y ) =
dpara(X,Y )
d(X,Y )
. (1)
In this formula, dpara(X,Y ) denotes the number
of documents satisfying the above conditions, and
d(X,Y ) presents the number of documents having
the parenthetical expression X(Y ). The function
PR(X, Y) ranges from 0 (no abbreviation instance)
to 1 (all parenthetical expressions introduce the ab-
breviation).
Similarity of local contexts We regard words that
have dependency relations from/to the target expres-
sion as the local contexts of the expression, apply-
ing all sentences to a dependency parser (Kudo and
Matsumoto, 2002). Collecting the local context of
the target expressions, we compute the skew diver-
gence (Lee, 2001), which is a weighted version of
Kullback-Leibler (KL) divergence, to measure the
resemblance of probability distributions P and Q:
SKEW?(P ||Q) = KL(P ||?Q+ (1? ?)P ), (2)
KL(P ||Q) =
?
i
P (i) log
P (i)
Q(i)
. (3)
In these formulas, P is the probability distribution
function of the words in the local context for the ex-
pression X , Q is for Y , and ? is a skew parameter
set to 0.99. The function SKEW?(P ||Q) becomes
close to zero if the probability distributions of local
contexts for the expressions X and Y are similar.
Other features In addition, we designed twelve
features for abbreviation recognition: five fea-
tures, freq(X), freq(Y ), freq(X,Y ), ?2(X,Y ), and
LLR(X,Y ) to measure the co-occurrence strength
of the expressions X and Y (Hisamitsu and Niwa,
2001), match(X,Y ) feature to test whether or not
all letters in an abbreviation appear in its full form,
three features letter type, first letter, and last let-
ter corresponding to rules about letter types in ab-
breviation definitions, and three features POS tags,
POS categories, and NE tags to utilize information
from a morphological analyzer and named-entity
tagger (Kudo and Matsumoto, 2002).
4 Evaluation
4.1 Results
We built a system for Japanese abbreviation recogni-
tion by using the LIBSVM implementation2 with a
2http://www.csie.ntu.edu.tw/?cjlin/
libsvm
892
Group Recall
Acronym 94.4%
Acronym with translation 97.4%
Alias 81.4%
Total 87.6%
Table 4: Recall for each role of parentheses
linear kernel, which obtained the best result through
experiments. The performance was measured under
a ten-fold cross-validation on the corpus built in the
survey, which contains 1,430 abbreviation instances
and 6,457 non-abbreviation instances.
The proposed method achieved 95.7% accuracy,
90.0% precision, and 87.6% recall for recognizing
Japanese abbreviations. We cannot compare this
performance directly with the previous work be-
cause of the differences in the task design and cor-
pus. For reference, Yamamoto (2002) reported 66%
precision (he did not provide the recall value) for
a similar task: the acquisition of lexical paraphrase
from Japanese newspaper articles.
Table 4 reports the recall value for each group
of abbreviations. This analysis shows the distribu-
tion of abbreviations unrecognized by the proposed
method. Japanese acronyms, acronyms with transla-
tion, and aliases were recognized at 94.4%, 97.4%,
and 81.4% recall respectively. It is interesting to see
that the proposed method could extract acronyms
with translation and aliases even though we did not
use any bilingual dictionaries.
4.2 Analyses for individual features
The numerical and boolean features are monotone
increasing functions (decreasing for the SKEW fea-
ture) as two expressions X and Y are more likely
to present an abbreviation definition. For example,
the more authors introduce a paraphrase X ? Y,
the higher the value that PR(X,Y ) feature yields.
Thus, we emulate a simple classifier for each feature
that labels a candidate of abbreviation definition as a
positive instance only if the feature value is higher
than a given threshold ?, e.g., PR(X,Y ) > 0.9.
Figure 2 shows the precision?recall curve for each
feature with variable thresholds.
The paraphrase ratio (PR) feature outperformed
other features with a wide margin: the precision and
recall values for the best F1 score were 66.2% and
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
isi
on
Recall
Co-occurrence frequency
Log likelihood ratio
Skew divergence
Letter match
Paraphrase rate
Chi square
Figure 2: Precision?recall curve of each feature
Feature Accuracy Reduction
All 95.7% ?
- PR(X,Y ) 95.2% 0.5%
- SKEW(X,Y ) 95.4% 0.3%
- freq(X,Y ) 95.6% 0.1%
- ?2(X,Y ) 95.6% 0.1%
- LLR(X,Y ) 95.3% 0.4%
- match(X,Y ) 95.5% 0.2%
- Letter type 94.5% 1.2%
- POS tags 95.6% 0.1%
- NE tags 95.7% 0.0%
Table 5: Contribution of the features
48.1% respectively. Although the performance of
this feature alone was far inferior to the proposed
method, to some extent Formula 1 estimated actual
occurrences of abbreviation definitions.
The performance of the match (letter inclusion)
feature was as low as 58.2% precision and 6.9% re-
call3. It is not surprising that the match feature had
quite a low recall, because of the ratio of ?authentic?
acronyms (about 6%) in the corpus. However, the
match feature did not gain a good precision either.
Examining false cases, we found that this feature
could not discriminate cases where an outer element
contains its inner element accidentally; e.g., Tokyo
Daigaku (Tokyo), which describes a university name
followed by its location (prefecture) name.
Finally, we examined the contribution of each fea-
ture by eliminating a feature one by one. If a feature
was important for recognizing abbreviations, the ab-
sence of the feature would drop the accuracy. Each
row in Table 5 presents an eliminated feature, the
accuracy without the feature, and the reduction of
3This feature drew the precision?recall locus in a stepping
shape because of its discrete values (0 or 1).
893
the accuracy. Unfortunately, the accuracy reductions
were so few that we could not discuss contributions
of features with statistical significance. The letter
type feature had the largest influence (1.2%) on the
recognition task, followed by the paraphrase ratio
(0.5%) and log likelihood ratio (0.4%).
5 Conclusion
In this paper we addressed the difficulties in rec-
ognizing Japanese abbreviations by examining ac-
tual usages of parenthetical expressions in news-
paper articles. We also presented the discrimina-
tive approach to Japanese abbreviation recognition,
which achieved 95.7% accuracy, 90.0% precision,
and 87.6% recall on the evaluation corpus. A future
direction of this study would be to apply the pro-
posed method to other non-alphabetical languages,
which may have similar difficulties in modeling the
generative process of abbreviations. We also plan to
extend this approach to the Web documents.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and Solution-Oriented Research for Science
and Technology (JST, Japan). We used Mainichi
Shinbun and Yomiuri Shinbun newspaper articles for
the evaluation corpus.
References
Eytan Adar. 2004. SaRAD: A simple and robust abbre-
viation dictionary. Bioinformatics, 20(4):527?533.
Jeffrey T. Chang and Hinrich Schu?tze. 2006. Abbre-
viations in biomedical text. In S. Ananiadou and
J. McNaught, editors, Text Mining for Biology and
Biomedicine, pages 99?119. Artech House, Inc.
Jing-Shin Chang and Wei-Lun Teng. 2006. Mining
atomic chinese abbreviation pairs: A probabilistic
model for single character word recovery. In Proceed-
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 17?24, Sydney, Australia,
July. Association for Computational Linguistics.
Zellig S. Harris. 1954. Distributional structure. Word,
10:146?162.
Toru Hisamitsu and Yoshiki Niwa. 2001. Extracting
useful terms from parenthetical expression by combin-
ing simple rules and statistical measures: A compara-
tive evaluation of bigram statistics. In Didier Bouri-
gault, Christian Jacquemin, and Marie-C L?Homme,
editors, Recent Advances in Computational Terminol-
ogy, pages 209?224. John Benjamins.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the CoNLL 2002 (COLING 2002 Post-
Conference Workshops), pages 63?69.
Lillian Lee. 2001. On the effectiveness of the skew di-
vergence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller. 1990.
Introduction to wordnet: An on-line lexical database.
Journal of Lexicography, 3(4):235?244.
David Nadeau and Peter D. Turney. 2005. A su-
pervised learning approach to acronym identification.
In 8th Canadian Conference on Artificial Intelligence
(AI?2005) (LNAI 3501), pages 319?329.
Naoaki Okazaki and Sophia Ananiadou. 2006. A term
recognition approach to acronym recognition. In Pro-
ceedings of the COLING-ACL 2006 Main Conference
Poster Sessions, pages 643?650, Sydney, Australia.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of 40th
annual meeting of ACL, pages 160?167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of the EMNLP 2001, pages 126?133.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2007. Improving coreference resolution us-
ing bridging reference resolution and automatically
acquired synonyms. In Anaphora: Analysis, Alo-
gorithms and Applications, 6th Discourse Anaphora
and Anaphor Resolution Colloquium, DAARC2007,
pages 125?136.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Pacific Symposium on Biocom-
puting (PSB 2003), number 8, pages 451?462.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Kazuhide Yamamoto. 2002. Acquisition of lexical para-
phrases from texts. In 2nd International Workshop
on Computational Terminology (Computerm 2002, in
conjunction with COLING 2002), pages 1?7, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
894
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 424?432,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semi-Supervised Lexicon Mining from Parenthetical Expressions
in Monolingual Web Pages
Xianchao Wu? Naoaki Okazaki? Jun?ichi Tsujii??
?Computer Science, Graduate School of Information Science and Technology, University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
?School of Computer Science, University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
{wxc, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a semi-supervised learn-
ing framework for mining Chinese-English
lexicons from large amount of Chinese Web
pages. The issue is motivated by the ob-
servation that many Chinese neologisms are
accompanied by their English translations in
the form of parenthesis. We classify par-
enthetical translations into bilingual abbrevi-
ations, transliterations, and translations. A
frequency-based term recognition approach is
applied for extracting bilingual abbreviations.
A self-training algorithm is proposed for min-
ing transliteration and translation lexicons. In
which, we employ available lexicons in terms
of morpheme levels, i.e., phoneme correspon-
dences in transliteration and grapheme (e.g.,
suffix, stem, and prefix) correspondences in
translation. The experimental results verified
the effectiveness of our approaches.
1 Introduction
Bilingual lexicons, as lexical or phrasal parallel
corpora, are widely used in applications of multi-
lingual language processing, such as statistical ma-
chine translation (SMT) and cross-lingual informa-
tion retrieval. However, it is a time-consuming task
for constructing large-scale bilingual lexicons by
hand. There are many facts cumber the manual de-
velopment of bilingual lexicons, such as the contin-
uous emergence of neologisms (e.g., new technical
terms, personal names, abbreviations, etc.), the dif-
ficulty of keeping up with the neologisms for lexi-
cographers, etc. In order to turn the facts to a better
way, one of the simplest strategies is to automati-
cally mine large-scale lexicons from corpora such as
the daily updated Web.
Generally, there are two kinds of corpora used
for automatic lexicon mining. One is the purely
monolingual corpora, wherein frequency-based
expectation-maximization (EM, refer to (Dempster
et al, 1977)) algorithms and cognate clues play a
central role (Koehn and Knight, 2002). Haghighi
et al (2008) presented a generative model based
on canonical correlation analysis, in which monolin-
gual features such as the context and orthographic
substrings of words were taken into account. The
other is multilingual parallel and comparable cor-
pora (e.g., Wikipedia1), wherein features such as co-
occurrence frequency and context are popularly em-
ployed (Cheng et al, 2004; Shao and Ng, 2004; Cao
et al, 2007; Lin et al, 2008).
In this paper, we focus on a special type of com-
parable corpus, parenthetical translations. The issue
is motivated by the observation that Web pages and
technical papers written in Asian languages (e.g.,
Chinese, Japanese) sometimes annotate named enti-
ties or technical terms with their translations in En-
glish inside a pair of parentheses. This is considered
to be a traditional way to annotate new terms, per-
sonal names or other named entities with their En-
glish translations expressed in brackets. Formally,
a parenthetical translation can be expressed by the
following pattern,
f1 f2 ... fJ (e1 e2 ... eI). (1)
Here, f1 f2 ... fJ (fJ1 ), the pre-parenthesis text, de-
notes the word sequence of some language other
than English; and e1 e2 ... eI (eI1), the in-parenthesis
text, denotes the word sequence of English. We sep-
arate parenthetical translations into three categories:
1http://en.wikipedia.org/wiki/Main Page
424
Type Examples with translations in italic
?? ?? ?? ?? ?? (GCOS)to Global Climate Observing System (GCOS)
?? ? ?? ???- ???(Shipton-Tilman)brand will be among Shipton-Tilman (Shipton-Tilman)
?????? ???(Cancelbots)time bomb, Cancelbots (Cancelbots)
? ?? ?? ? ??? ???? ??(Bradford University)
the English Bradford University (Bradford University)
that holds lessons in Hongkong
Abbreviation
Transliteration
Translation
Mixture
Table 1: Parenthetical translation categories and exam-
ples extracted from Chinese Web pages. Mixture stands
for the mixture of translation (University) and translitera-
tion (Bradford). ??? denotes the left boundary of fJ1 .
bilingual abbreviation, transliteration, and transla-
tion. Table 1 illustrates examples of these categories.
We address several characteristics of parenthetical
translations that differ from traditional comparable
corpora. The first is that they only appear in mono-
lingual Web pages or documents, and the context
information of eI1 is unknown. Second, frequency
and word number of eI1 are frequently small. This
is because parenthetical translations are only used
when the authors thought that fJ1 contained some
neologism(s) which deserved further explanation in
another popular language (e.g., English). Thus, tra-
ditional context based approaches are not applicable
and frequency based approaches may yield low re-
call while with high precision. Furthermore, cog-
nate clues such as orthographic features are not ap-
plicable between language pairs such as English and
Chinese.
Parenthetical translation mining faces the follow-
ing issues. First, we need to distinguish paren-
thetical translations from parenthetical expressions,
since parenthesis has many functions (e.g., defining
abbreviations, elaborations, ellipsis, citations, anno-
tations, etc.) other than translation. Second, the
left boundary (denoted as ? in Table 1) of the pre-
parenthesis text need to be determined to get rid of
the unrelated words. Third, we need further distin-
guish different translation types, such as bilingual
abbreviation, the mixture of translation and translit-
eration, as shown in Table 1.
In order to deal with these problems, supervised
(Cao et al, 2007) and unsupervised (Li et al, 2008)
methods have been proposed. However, supervised
approaches are restricted by the quality and quantity
of manually constructed training data, and unsuper-
vised approaches are totally frequency-based with-
out using any semantic clues. In contrast, we pro-
pose a semi-supervised framework for mining par-
enthetical translations. We apply a monolingual ab-
breviation extraction approach to bilingual abbrevia-
tion extraction. We construct an English-syllable to
Chinese-pinyin transliteration model which is self-
trained using phonemic similarity measurements.
We further employ our cascaded translation model
(Wu et al, 2008) which is self-trained based on
morpheme-level translation similarity.
This paper is organized as follows. We briefly
review the related work in the next section. Our
system framework and self-training algorithm is de-
scribed in Section 3. Bilingual abbreviation ex-
traction, self-trained transliteration models and cas-
caded translation models are described in Section 4,
5, and 6, respectively. In Section 7, we evaluate our
mined lexicons by Wikipedia. We conclude in Sec-
tion 8 finally.
2 Related Work
Numerous researchers have proposed a variety of
automatic approaches to mine lexicons from the
Web pages or other large-scale corpora. Shao and
Ng (2004) presented a method to mine new transla-
tions from Chinese and English news documents of
the same period from different news agencies, com-
bining both transliteration and context information.
Kuo et al (2006) used active learning and unsu-
pervised learning for mining transliteration lexicon
from the Web pages, in which an EM process was
used for estimating the phonetic similarities between
English syllables and Chinese characters.
Cao et al (2007) split parenthetical translation
mining task into two parts, transliteration detection
and translation detection. They employed a translit-
eration lexicon for constructing a grapheme-based
transliteration model and annotated boundaries man-
ually to train a classifier. Lin et al (2008) applied
a frequency-based word alignment approach, Com-
petitive Link (Melanmed, 2000), to determine the
outer boundary (Section 7).
On the other hand, there have been many semi-
supervised approaches in numerous applications
425
Parenthetical expression extraction{C(E)} 
Chinese word segmentation{c?(e?)} S-MSRSeg 
Heuristic filtering{c?(e?)} 
Chinese Web pages 
Bilingual abbreviation mining 
Section 4 
Transliteration lexicon mining 
Section 5 
Translation lexicon mining 
Section 6 
(Lin et al, 2008) 
Figure 1: The system framework of mining lexicons from
Chinese Web pages.
(Zhu, 2007), such as self-training in word sense
disambiguation (Yarowsky, 2005) and parsing (Mc-
Closky et al, 2008). In this paper, we apply self-
training to a new topic, lexicon mining.
3 System Framework and Self-Training
Algorithm
Figure 1 illustrates our system framework for min-
ing lexicons from Chinese Web pages. First, par-
enthetical expressions matching Pattern 1 are ex-
tracted. Then, pre-parenthetical Chinese sequences
are segmented into word sequences by S-MSRSeg2
(Gao et al, 2006). The initial parenthetical transla-
tion corpus is constructed by applying the heuristic
rules defined in (Lin et al, 2008)3. Based on this
corpus, we mine three lexicons step by step, a bilin-
gual abbreviation lexicon, a transliteration lexicon,
and a translation lexicon. The abbreviation candi-
dates are extracted firstly by using a heuristic rule
(Section 4.1). Then, the transliteration candidates
are selected by employing a transliteration model
(Section 5.1). Specially, fJ1 (eI1) is taken as a translit-
eration candidate only if a word ei in eI1 can be
transliterated. In addition, a transliteration candidate
will also be considered as a translation candidate if
not all ei can be transliterated (refer to the mixture
example in Table1). Finally, after abbreviation filter-
ing and transliteration filtering, the remaining candi-
2http://research.microsoft.com/research/downloads/details/
7a2bb7ee-35e6-40d7-a3f1-0b743a56b424/details.aspx
3e.g., fJ1 is predominantly in Chinese and eI1 is predomi-
nantly in English
Algorithm 1 self-training algorithm
Require: L, U = {fJ1 (eI1)}, T , M ?L, (labeled) train-
ing set; U , (unlabeled) candidate set; T , test set; M, the
transliteration or translation model.
1: Lexicon = {} ? new mined lexicon
2: repeat
3: N = {} ? new mined lexicon during one iteration
4: train M on L
5: evaluate M on T
6: for fJ1 (eI1) ? U do
7: topN = {C?|decode eI1 by M}
8: N = N ? {(c, eI1)|c ? fJ1 ?
?C? ? topN s.t. similarity{c, C?} ? ?}
9: end for
10: U = U ?N
11: L = unified(L ?N)
12: Lexicon = unified(Lexicon ?N)
13: until |N | ? ?
14: return Lexicon ? the output
dates are used for translation lexicon mining.
Algorithm 1 addresses the self-training algorithm
for lexicon mining. The main part is a loop from
Line 2 to Line 13. A given seed lexicon is taken
as labeled data and is split into training and testing
sets (L and T ). U={fJ1 (eI1)}, stands for the (unla-
beled) parenthetical expression set. Initially, a trans-
lation/transliteration model (M) is trained on L and
evaluated on T (Line 4 and 5). Then, the English
phrase eI1 of each unlabeled entry is decoded by M,
and the top-N outputs are stored in set topN (Line
7?8). A similarity function on c (a word substring
of fJ1 ) and a top-N output C ? is employed to make
the decision of classification: the pair (c, eI1) will be
selected as a new entry if the similarity between c
and C ? is no smaller than a threshold value ? (Line
8). After processing each entry in U , the new mined
lexicon N is deleted from U and unified with the
current training set L as the new training set (Line
10 and 11). Also, N is added to the final lexicon
(Line 12). When |N | is lower than a threshold, the
loop stops. Finally, the algorithm returns the mined
lexicon.
One of the open problems in Algorithm 1 is how
to append new mined entries into the existing seed
lexicon, considering they have different distribu-
tions. One way is to design and estimate a weight
function on the frequency of new mined entries. For
simplicity, we use a deficient strategy that takes the
weights of all new mined entries to be one.
426
4 Bilingual Abbreviation Extraction
4.1 Methodology
The method that we use for extracting a bilingual
abbreviation lexicon from parenthetical expressions
is inspired by (Okzaki and Ananiadou, 2006). They
used a term recognition approach to build a monolin-
gual abbreviation dictionary from the Medical Liter-
ature Analysis and Retrieval System Online (MED-
LINE) abstracts, wherein acronym definitions (e.g.,
ADM is short for adriamycin, adrenomedullin, etc.)
are abundant. They reported 99% precision and 82-
95% recall. Through locating a textual fragment
with an acronym and its expanded form in pattern
long form (short form), (2)
they defined a heuristic formula to compute the long-
form likelihood LH(c) for a candidate c:
LH(c) = freq(c)? ?
t?Tc
freq(t)? freq(t)?
t?Tc freq(t)
.
(3)
Here, c is a long-form candidate; freq(c) denotes the
frequency of co-occurrence of c with a short-form;
and Tc is a set of nested long-form candidates, each
of which consists of a preceding word followed by
the candidate c. Obviously, for t ? Tc, Equation 3
can be explained as:
LH(c) = freq(c)? E[freq(t)]. (4)
In this paper, we apply their method on the task
of bilingual abbreviation lexicon extraction. Now,
the long-form is a Chinese word sequence and the
short-form is an English acronym. We filter the par-
enthetical expressions in the Web pages with several
heuristic rules to meet the form of pattern 2 and to
save the computing time:
? the short-form (eI1) should contain only one En-
glish word (I = 1), and all letters in which
should be capital;
? similar with (Lin et al, 2008), the pre-
parenthesis text is trimmed with: |c| ? 10 ?
|eI1|+ 6 when |eI1| ? 6, and |c| ? 2? |eI1|+ 6,
otherwise. |c| and |eI1| are measured in bytes.
We further trim the remaining pre-parenthesis
text by punctuations other than hyphens and
dots, i.e., the right most punctuation and its left
subsequence are discarded.
o. Chinese long-form candidates LH T/F
1 ?? ?? ?? 172.5 T
Tumor-Associated Antigen
2 ? ? ?? ? 79.9 T
thioacetamide
3 ? 33.8 F
amine
4 ?? 24.5 F
antigen
5 ?? ?? 21.2 F
associated antigen
6 ? ?? ?? ?? 16.5 F
's Tumor-Associated Antigen
7 ? ??? 16.2 T
total amino acid
Table 2: Top-7 Chinese long-form candidates for the En-
glish acronym TAA, according to the LH score.
4.2 Experiment
We used SogouT Internet Corpus Version 2.04,
which contains about 13 billion original Web pages
(mainly Chinese) in the form of 252 gigabyte .txt
files. In addition, we used 55 gigabyte (.txt for-
mat) Peking University Chinese Paper Corpus. We
constructed a partially parallel corpus in the form
of Pattern 1 from the union of the two corpora us-
ing the heuristic rules defined in (Lin et al, 2008).
We gained a partially parallel corpus which contains
12,444,264 entries.
We extracted 107,856 distinct English acronyms.
Limiting LH score ? 1.0 in Equation 3, we gained
2,020,012 Chinese long-form candidates for the
107,856 English acronyms. Table 2 illustrates the
top-7 Chinese long-form candidates of the English
acronym TAA. Three candidates are correct (T) long-
forms while the other 4 are wrong (F). Wrong can-
didates from No. 3 to 5 are all subsequences of the
correct candidate No. 1. No. 6 includes No. 1 while
with a Chinese functional word de in the left most
side. These error types can be easily tackled with
some filtering patterns, such as ?remove the left most
functional word in the long-form candidates?, ?only
keep the relatively longer candidates with larger LH
score?, etc.
Since there does not yet exists a common eval-
uation data set for the bilingual abbreviation lexi-
con, we manually evaluated a small sample of it.
4http://www.sogou.com/labs/dl/t.html
427
Of the 107,856 English acronyms, we randomly se-
lected 200 English acronyms and their top-1 Chi-
nese long-form candidates for manually evaluating.
We found, 92 candidates were correct including 3
transliteration examples. Of the 108 wrong candi-
dates, 96 candidates included the correct long-form
with some redundant words on the left side (i.e., c =
(word)+ correct long-form), the other 12 candidates
missed some words of the correct long-form or had
some redundant words right before the left paren-
thesis (i.e., c = (word)? correct long-form (word)+
or c = (word)? subsequence of correct long-form
word)?). We classified the redundant word right be-
fore the correct long-form of each of the 96 candi-
dates, de occupied 32, noun occupied 7, verb occu-
pied 18, prepositions and conjunctions occupied the
remaining ones.
In total, the abbreviation translation accuracy is
44.5%. We improved the accuracy to 60.5% with
an additional de filtering pattern. According to for-
mer mentioned error analysis, the accuracy may fur-
ther be improved if a Chinese part-of-speech tagger
is employed and the non-nominal words in the long-
form are removed beforehand.
5 Self-Training for Transliteration Models
In this section, we first describe and compare three
transliteration models. Then, we select and train the
best model following Algorithm 1 for lexicon min-
ing. We investigate two things, the scalability of the
self-trained model given different amount of initial
training data, and the performance of several strate-
gies for selecting new training samples.
5.1 Model description
We construct and compare three forward translit-
eration models, a phoneme-based model (English
phonemes to Chinese pinyins), a grapheme-based
model (English syllables to Chinese characters)
and a hybrid model (English syllables to Chinese
pinyins). Similar models have been compared in
(Oh et al, 2006) for English-to-Korean and English-
to-Japanese transliteration. All the three models are
phrase-based, i.e., adjacent phonemes or graphemes
are allowable to form phrase-level transliteration
units. Building the correspondences on phrase
level can effectively tackle the missing or redundant
phoneme/grapheme problem during transliteration.
For example, when Aamodt is transliterated into a
mo? te`5, a and d are missing. The problem can be
easily solved when taking Aa and dt as single units
for transliterating.
Making use of Moses (Koehn et al, 2007), a
phrase-based SMT system, Matthews (2007) has
shown that the performance was comparable to re-
cent state-of-the-art work (Jiang et al, 2007) in
English-to-Chinese personal name transliteration.
Matthews (2007) took transliteration as translation
at the surface level. Inspired by his idea, we also
implemented our transliteration models employing
Moses. The main difference is that, while Matthews
(2007) tokenized the English names into individual
letters before training in Moses, we split them into
syllables using the heuristic rules described in (Jiang
et al, 2007), such that one syllable only contains one
vowel letter or a combination of a consonant and a
vowel letter.
English syllable sequences are used in the
grapheme-based and hybrid models. In the
phoneme-based model, we transfer English names
into phonemes and Chinese characters into Pinyins
in virtue of the CMU pronunciation dictionary6 and
the LDC Chinese character-to-pinyin list7.
In the mass, the grapheme-based model is the
most robust model, since no additional resources are
needed. However, it suffers from the Chinese homo-
phonic character problem. For instance, pinyin ai
corresponds to numerous Chinese characters which
are applicable to personal names. The phoneme-
based model is the most suitable model that reflects
the essence of transliteration, while restricted by ad-
ditional grapheme to phoneme dictionaries. In or-
der to eliminate the confusion of Chinese homo-
phonic characters and alleviate the dependency on
additional resources, we implement a hybrid model
that accepts English syllables and Chinese pinyins
as formats of the training data. This model is called
hybrid, since English syllables are graphemes and
Chinese pinyins are phonemes.
5The tones of Chinese pinyins are ignored in our translitera-
tion models for simplicity.
6http://www.speech.cs.cmu.edu/cgi-bin/cmudict
7http://projects.ldc.upenn.edu/Chinese/docs/char2pinyin.txt
428
 grapheme-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
 phoneme-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
 Comparison on EMatch
0.0
0.1
0.2
0.3
0.4
0.5
1 2 3 4 5 6 7 8max_phrase_length
grapheme phoneme hybrid
 hybrid-based
0.0
0.2
0.4
0.6
0.8
1.0
1 2 3 4 5 6 7 8max_phrase_length
BLEU WER PER EMatch
Figure 2: The performances of the transliteration models
and their comparison on EMatch.
5.2 Experimental model selection
Similar to (Jiang et al, 2007), the transliteration
models were trained and tested on the LDC Chinese-
English Named Entity Lists Version 1.08. The origi-
nal list contains 572,213 English people names with
Chinese transliterations. We extracted 74,725 en-
tries in which the English names also appeared in
the CMU pronunciation dictionary. We randomly
selected 3,736 entries as an open testing set and the
remaining entries as a training set9. The results were
evaluated using the character/pinyin-based 4-gram
BLEU score (Papineni et al, 2002), word error rate
(WER), position independent word error rate (PER),
and exact match (EMatch).
Figure 2 reports the performances of the three
models and the comparison based on EMatch. From
the results, we can easily draw the conclusion that
the hybrid model performs the best under the maxi-
mal phrase length (mpl, the maximal phrase length
allowed in Moses) from 1 to 8. The performances
of the models converge at or right after mpl =
4. The pinyin-based WER of the hybrid model is
39.13%, comparable to the pinyin error rate 39.6%,
reported in (Jiang et al, 2007)10. Thus, our further
8Linguistic Data Consortium catalog number:
LDC2005T34 (former catalog number: LDC2003E01)
9Jiang et al (2007) selected 25,718 personal name pairs
from LDC2003E01 as the experiment data: 200 as development
set, 200 as test set, and the remaining entries as training set.
10It should be notified that we achieved this result by using
larger training set (70,989 vs. 25,718) and larger test set (3,736
vs. 200) comparing with (Jiang et al, 2007), and we did not use
% 0t 1t 2t 3t 4t 5t Strategy
5 .3879 .3937 .3971 .3958 .3972 .3971 top1 em
.3911 .3979 .3954 .3974 .3965 top1 am
.4062 .4182 .4208 .4218 .4201 top5 em
.3987 .4177 .4190 .4192 .4189 top5 am
10 .4092 .4282 .4258 .4202 .4203 .4205 top1 em
.4121 .4190 .4180 .4174 .4200 top1 am
.4305 .4386 .4399 .4438 .4403 top5 em
.4289 .4263 .4292 .4291 .4288 top5 am
20 .4561 .4538 .4562 .4550 .4543 .4551 top1 em
.4532 .4578 .4544 .4545 .4541 top1 am
.4624 .4762 .4754 .4748 .4746 top5 em
.4605 .4677 .4677 .4674 .4679 top5 am
40 .4779 .4791 .4793 .4799 .4794 .4808 top1 em
.4774 .4794 .4779 .4789 .4784 top1 am
.4808 .4811 .4791 .4795 .4790 top5 em
.4775 .4778 .4781 .4785 .4779 top5 am
60 .5032 .4939 .5004 .5012 .5012 .5016 top1 em
.4919 .4988 .4990 .4994 .4990 top1 am
.5013 .5063 .5059 .5066 .5065 top5 em
.4919 .4960 .4970 .4977 .4962 top5 am
80 .5038 .4984 .4984 .5004 .5006 .4995 top1 em
.4916 .4916 .4914 .4915 .4916 top1 am
.5039 .5037 .5053 .5054 .5042 top5 em
.4950 .5028 .5027 .5032 .5032 top5 am
100 .5045 .5077 .5053 .5067 .5063 .5066 top1 em
.5045 .5054 .5046 .5050 .5055 top1 am
.5108 .5102 .5111 .5108 .5115 top5 em
.5105 .5106 .5100 .5094 .5109 top5 am
Table 3: The BLEU score of self-trained h4 translitera-
tion models under four selection strategies. nt (n=1..5)
stands for the n-th iteration.
self-training experiments are pursued on the hybrid
model taking mpl to be 4 (short for h4, hereafter).
5.3 Experiments on the self-trained hybrid
model
As former mentioned, we investigate the scalability
of the self-trained h4 model by respectively using 5,
10, 20, 40, 60, 80, and 100 percent of initial training
data, and the performances of using exact matching
(em) or approximate matching (am, line 8 in Algo-
rithm 1) on the top-1 and top-5 outputs (line 7 in Al-
gorithm 1) for selecting new training samples. We
used edit distance (ed) to measure the em and am
similarities:
ed(c, C ?) = 0 or < syllable number(C ?)/2. (5)
When applying Algorithm 1 for transliteration lexi-
con mining, we decode each word in eI1 respectively.
The algorithm terminated in five iterations when we
set the terminal threshold ? (Line 13 in Algorithm 1)
to be 100.
For simplicity, Table 3 only illustrates the BLEU
score of h4 models under four selection strategies.
From this table, we can draw the following conclu-
sions. First, with fewer initial training data, the im-
provement is better. The best relative improvements
additional Web resources as Jiang et al (2007) did.
429
are 8.74%, 8.46%, 4.41%, 0.67%, 0.68%, 0.32%,
and 1.39%, respectively. Second, using top-5 and
em for new training data selection performs the best
among the four strategies. Compared under each it-
eration, using top-5 is better than using top-1; em
is better than am; and top-5 with am is a little bet-
ter than top-1 with em. We mined 39,424, 42,466,
46,116, 47,057, 49,551, 49,622, and 50,313 distinct
entries under the six types of initial data with top-5
plus em strategy. The 50,313 entries are taken as the
final transliteration lexicon for further comparison.
6 Self-Training for a Cascaded Translation
Model
We classify the parenthetical translation candidates
by employing a translation model. In contrast to
(Lin et al, 2008), wherein the lengthes of prefixes
and suffixes of English words were assumed to be
three bytes, we segment words into morphemes (se-
quences of prefixes, stems, and suffixes) by Morfes-
sor 0.9.211, an unsupervised language-independent
morphological analyzer (Creutz and Lagus, 2007).
We use the morpheme-level translation similarity
explicitly in our cascaded translation model (Wu et
al., 2008), which makes use of morpheme, word,
and phrase level translation units. We train Moses
to gain a phrase-level translation table. To gain a
morpheme-level translation table, we run GIZA++
(Och and Ney, 2003) on both directions between En-
glish morphemes and Chinese characters, and take
the intersection of Viterbi alignments. The English-
to-Chinese translation probabilities computed by
GIZA++ are attached to each morpheme-character
element in the intersection set.
6.1 Experiment
The Wanfang Chinese-English technical term dictio-
nary12, which contains 525,259 entries in total, was
used for training and testing. 10,000 entries were
randomly selected as the test set and the remaining
as the training set. Again, we investigated the scala-
bility of the self-trained cascaded translation model
by respectively using 20, 40, 60, 80, and 100 per-
cent of initial training data. An aggressive similar-
11http://www.cis.hut.fi/projects/morpho/
12http://www.wanfangdata.com.cn/Search/ResourceBrowse
.aspx
% 0t 1t 2t 3t 4t 5t
20 .1406 .1196 .1243 .1239 .1176 .1179
40 .1091 .1224 .1386 .1345 .1479 .1466
60 .1630 .1624 .1429 .1714 .1309 .1398
80 .1944 .1783 .1886 .1870 .1884 .1873
100 .1810 .1814 .1539 .1981 .1542 .1944
Table 4: The BLEU score of self-trained cascaded trans-
lation model under five initial training sets.
ity measurement was used for selecting new training
samples:
first char(c) = first char(C ?) ? min{ed(c, C ?)}.
(6)
Here, we judge if the first characters of c and C ?
are similar or not. c was gained by deleting zero
or more characters from the left side of fJ1 . When
more than one c satisfied this condition, the c that
had the smallest edit distance with C ? was selected.
When applying Algorithm 1 for translation lexicon
mining, we took eI1 as one input for decoding instead
of decoding each word respectively. Only the top-1
output (C ?) was used for comparing. The algorithm
stopped in five iterations when we set the terminal
threshold ? to be 2000.
For simplicity, Table 4 only illustrates the BLEU
score of the cascaded translation model under five
initial training sets. For the reason that there are fi-
nite phonemes in English and Chinese while the se-
mantic correspondences between the two languages
tend to be infinite, Table 4 is harder to be analyzed
than Table 3. When initially using 40%, 60%, and
100% training data for self-training, the results tend
to be better at some iterations. We gain 35.6%,
5.2%, and 9.4% relative improvements, respectively.
However, the results tend to be worse when 20% and
80% training data were used initially, with 11.6%
and 3.0% minimal relative loss. The best BLEU
scores tend to be better when more initial training
data are available. We mined 1,038,617, 1,025,606,
1,048,761, 1,056,311, and 1,060,936 distinct entries
under the five types of initial training data. The
1,060,936 entries are taken as the final translation
lexicon for further comparison.
7 Wikipedia Evaluation
We have mined three kinds of lexicons till now,
an abbreviation lexicon containing 107,856 dis-
430
En. to Ch. Ch. to En.
Cov EMatch Cov EMatch
Our Lexicon 22.8% 5.2% 23.2% 5.5%
Unsupervised 23.5% 5.4% 24.0% 5.4%
Table 5: The results of our lexicon and an unsupervised-
mined lexicon (Lin et al, 2008) evaluated under
Wikipedia title dictionary. Cov is short for coverage.
similar English acronyms with 2,020,012 Chinese
long-form candidates; a transliteration lexicon with
50,313 distinct entries; and a translation lexicon
with 1,060,936 distinct entries. The three lexicons
are combined together as our final lexicon.
Similar with (Lin et al, 2008), we compare our
final mined lexicon with a dictionary extracted from
Wikipedia, the biggest multilingual free-content en-
cyclopedia on the Web. We extracted the titles of
Chinese and English Wikipedia articles13 that are
linked to each other. Since most titles contain less
than five words, we take a linked title pair as a trans-
lation entry without considering the word alignment
relation between the words inside the titles. The re-
sult lexicon contains 105,320 translation pairs be-
tween 103,823 Chinese titles and 103,227 English
titles. Obviously, only a small percentage of titles
have more than one translation. Whenever there is
more than one translation, we take the candidate en-
try as correct if and only if it matches one of the
translations.
Moreover, we compare our semi-supervised ap-
proach with an unsupervised approach (Lin et al,
2008). Lin et al (2008) took ?2(fj , ei) score
14(Gale and Church, 1991) with threshold 0.001 as
the word alignment probability in a word alignment
algorithm, Competitive Link. Competitive Link tries
to align an unlinked ei with an unlinked fj by the
condition that ?2(fj , ei) is the biggest. Lin et al
(2008) relaxed the unlinked constraints to allow con-
secutive sequence of words on one side to be linked
to the same word on the other side15. The left
13English and Chinese Wikipedia pages due to 2008.09.23
are used here.
14?2(fj , ei) = (ad?bc)
2
(a+b)(a+c)(b+d)(c+d) , where a is the number
of fJ1 (eI1) containing both ei and fj ; (a + b) is the number of
fJ1 (eI1) containing ei; (a+ c) is the number of fJ1 (eI1) contain-
ing fj ; and d is the number of fJ1 (eI1) containing neither ei nor
fj .
15Instead of requiring both ei and fj to have no previous link-
boundary inside fJ1 is determined when each ei in
eI1 is aligned. After applying the modified Compet-
itive Link on the partially parallel corpus which in-
cludes 12,444,264 entries (Section 4.2), we obtained
2,628,366 distinct pairs.
Table 5 shows the results of the two lexicons eval-
uated under Wikipedia title dictionary. The coverage
is measured by the percentage of titles which ap-
pears in the mined lexicon. We then check whether
the translation in the mined lexicon is an exact match
of one of the translations in the Wikipedia lexicon.
Through comparing the results, our mined lexicon is
comparable with the lexicon mined in an unsuper-
vised way. Since the selection is based on phone-
mic and semantic clues instead of frequency, a par-
enthetical translation candidate will not be selected
if the in-parenthetical English text is failed to be
transliterated or translated. This is one reason that
explains why we earned a little lower coverage. An-
other reason comes from the low coverage rate of
seed lexicons used for self-training, only 8.65% En-
glish words in the partially parallel corpus are cov-
ered by the Wanfang dictionary.
8 Conclusion
We have proposed a semi-supervised learning
framework for mining bilingual lexicons from par-
enthetical expressions in monolingual Web pages.
We classified the parenthesis expressions into three
categories: abbreviation, transliteration, and transla-
tion. A set of heuristic rules, a self-trained hybrid
transliteration model, and a self-trained cascaded
translation model were proposed for each category,
respectively.
We investigated the scalability of the self-trained
transliteration and translation models by training
them with different amount of data. The results shew
the stability (transliteration) and feasibility (transla-
tion) of our proposals. Through employing the par-
allel Wikipedia article titles as a gold standard lex-
icon, we gained the comparable results comparing
our semi-supervised framework with our implemen-
tation of Lin et al (2008)?s unsupervised mining
approach.
ages, they only require that at least one of them be unlinked and
that (suppose ei is unlinked and fj is linked to ek) none of the
words between ei and ek be linked to any word other than fj .
431
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Japanese/Chinese Machine Translation Project
in Special Coordination Funds for Promoting Sci-
ence and Technology (MEXT, Japan). We thank
the anonymous reviewers for their constructive com-
ments.
References
Cao, Guihong, Jianfeng Gao, and Jian-Yun Nie. 2007.
A system to Mine Large-Scale Bilingual Dictionar-
ies from Monolingual Web Pages. In MT Summit XI.
pages 57?64, Copenhagen, Denmark.
Cheng, Pu-Jen, Yi-Cheng Pan, Wen-Hsiang Lu, and Lee-
Feng Chien. 2004. Creating Multilingual Translation
Lexicons with Regional Variations Using Web Cor-
pora. In ACL 2004, pages 534?541, Barcelona,
Spain.
Creutz, Mathias and Krista Lagus. 2007. Unsupervised
Models for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):Article 3.
Dempster, A. P., N. M. Laird and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical Soci-
ety, 39:1?38.
Gale, W. and K. Church. 1991. Identifying word corre-
spondence in parallel text. In DARPA NLP Workshop.
Gao, Jianfeng, Mu Li, Andi Wu, and Chang-Ning Huang.
2006. Chinese Word Segmentation and Named Entity
Recognition: A Pragmatic Approach. Computational
Linguistics, 31(4):531?574.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein 2008. Learning Bilingual Lexicons
from Monolingual Corpora. In ACL-08:HLT. pages
771?779, Columbus, Ohio.
Jiang, Long, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named Entity Translation with Web Min-
ing and Transliteration. In IJCAI 2007. pages 1629?
1634, Hyderabad, India.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007 Poster Session, pages 177?180.
Koehn, Philipp and Kevin Knight. 2002. Learning
a translation lexicon from monolingual corpora. In
SIGLEX 2002, pages 9?16.
Kuo, Jin-Shea, Haizhou Li, and Ying-Kuei Yang. 2006.
Learning Transliteration Lexicons from the Web. In
COLING-ACL 2006. pages 1129?1136.
Lin, Dekang, Shaojun Zhao, Benjamin Van Durme, and
Marius Pas?ca. 2008. Mining Parenthetical Transla-
tions from the Web by Word Alignment. In ACL-
08:HLT, pages 994?1002, Columbus, Ohio.
Matthews, David. 2007. Machine Transliteration of
Proper Names. A Thesis of Master. University of Ed-
inburgh.
McClosky, David, Eugene Charniak, and Mark Johnson
2008. When is Self-Training Effective for Parsing? In
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 561?
568, manchester, UK.
Melamed, I. Dan. 2000. Models of Translational Equiv-
alence among Words. Computational Linguistics,
26(2):221?249.
Och, Franz Josef and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Oh, Jong-Hoon, Key-Sun Choi, and Hitoshi Isahara.
2006. A Comparison of Different Machine Translit-
eration Models. Journal of Artifical Intelligence Re-
search, 27:119?151.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Building
an Abbreviation Dictionary Using a Term Recognition
Approach. Bioinformatics, 22(22):3089?3095.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL). pages 311?318, Philadel-
phia.
Shao, Li and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING), pages 618?624,
Geneva, Switzerland.
Wu, Xianchao, Naoaki Okazaki, Takashi Tsunakawa, and
Jun?ichi Tsujii. 2008. Improving English-to-Chinese
Translation for Technical Terms Using Morphological
Information. In Proceedings of the 8th Conference of
the Association for Machine Translation in the Ameri-
cas (AMTA), pages 202?211, Waikiki, Hawai?i.
Yarowsky, David. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd annual meeting on Association
for Computational Linguistics, pages 189?196, Cam-
bridge, Massachusetts.
Zhu, Xiaojin. 2007. Semi-Supervised Learning Litera-
ture Survery. University of Wisconsin - Madison.
432
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 385?392,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Bottom-up Approach to Sentence Ordering
for Multi-document Summarization
Danushka Bollegala Naoaki Okazaki ?
Graduate School of Information Science and Technology
The University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
{danushka,okazaki}@mi.ci.i.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Mitsuru Ishizuka
Abstract
Ordering information is a difficult but
important task for applications generat-
ing natural-language text. We present
a bottom-up approach to arranging sen-
tences extracted for multi-document sum-
marization. To capture the association and
order of two textual segments (eg, sen-
tences), we define four criteria, chronol-
ogy, topical-closeness, precedence, and
succession. These criteria are integrated
into a criterion by a supervised learning
approach. We repeatedly concatenate two
textual segments into one segment based
on the criterion until we obtain the overall
segment with all sentences arranged. Our
experimental results show a significant im-
provement over existing sentence ordering
strategies.
1 Introduction
Multi-document summarization (MDS) (Radev
and McKeown, 1999) tackles the information
overload problem by providing a condensed ver-
sion of a set of documents. Among a number
of sub-tasks involved in MDS, eg, sentence ex-
traction, topic detection, sentence ordering, infor-
mation extraction, sentence generation, etc., most
MDS systems have been based on an extraction
method, which identifies important textual seg-
ments (eg, sentences or paragraphs) in source doc-
uments. It is important for such MDS systems
to determine a coherent arrangement of the tex-
tual segments extracted from multi-documents in
order to reconstruct the text structure for summa-
rization. Ordering information is also essential for
?Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
other text-generation applications such as Ques-
tion Answering.
A summary with improperly ordered sen-
tences confuses the reader and degrades the qual-
ity/reliability of the summary itself. Barzi-
lay (2002) has provided empirical evidence that
proper order of extracted sentences improves their
readability significantly. However, ordering a
set of sentences into a coherent text is a non-
trivial task. For example, identifying rhetorical
relations (Mann and Thompson, 1988) in an or-
dered text has been a difficult task for computers,
whereas our task is even more complicated: to
reconstruct such relations from unordered sets of
sentences. Source documents for a summary may
have been written by different authors, by different
writing styles, on different dates, and based on dif-
ferent background knowledge. We cannot expect
that a set of extracted sentences from such diverse
documents will be coherent on their own.
Several strategies to determine sentence order-
ing have been proposed as described in section 2.
However, the appropriate way to combine these
strategies to achieve more coherent summaries re-
mains unsolved. In this paper, we propose four
criteria to capture the association of sentences in
the context of multi-document summarization for
newspaper articles. These criteria are integrated
into one criterion by a supervised learning ap-
proach. We also propose a bottom-up approach
in arranging sentences, which repeatedly concate-
nates textual segments until the overall segment
with all sentences arranged, is achieved.
2 Related Work
Existing methods for sentence ordering are di-
vided into two approaches: making use of chrono-
logical information (McKeown et al, 1999; Lin
385
and Hovy, 2001; Barzilay et al, 2002; Okazaki
et al, 2004); and learning the natural order of sen-
tences from large corpora not necessarily based on
chronological information (Lapata, 2003; Barzi-
lay and Lee, 2004). A newspaper usually dissem-
inates descriptions of novel events that have oc-
curred since the last publication. For this reason,
ordering sentences according to their publication
date is an effective heuristic for multidocument
summarization (Lin and Hovy, 2001; McKeown
et al, 1999). Barzilay et al (2002) have proposed
an improved version of chronological ordering by
first grouping sentences into sub-topics discussed
in the source documents and then arranging the
sentences in each group chronologically.
Okazaki et al (2004) have proposed an algo-
rithm to improve chronological ordering by re-
solving the presuppositional information of ex-
tracted sentences. They assume that each sen-
tence in newspaper articles is written on the basis
that presuppositional information should be trans-
ferred to the reader before the sentence is inter-
preted. The proposed algorithm first arranges sen-
tences in a chronological order and then estimates
the presuppositional information for each sentence
by using the content of the sentences placed before
each sentence in its original article. The evaluation
results show that the proposed algorithm improves
the chronological ordering significantly.
Lapata (2003) has suggested a probabilistic
model of text structuring and its application to the
sentence ordering. Her method calculates the tran-
sition probability from one sentence to the next
from a corpus based on the Cartesian product be-
tween two sentences defined using the following
features: verbs (precedent relationships of verbs
in the corpus); nouns (entity-based coherence by
keeping track of the nouns); and dependencies
(structure of sentences). Although she has not
compared her method with chronological order-
ing, it could be applied to generic domains, not re-
lying on the chronological clue provided by news-
paper articles.
Barzilay and Lee (2004) have proposed con-
tent models to deal with topic transition in do-
main specific text. The content models are formal-
ized by Hidden Markov Models (HMMs) in which
the hidden state corresponds to a topic in the do-
main of interest (eg, earthquake magnitude or pre-
vious earthquake occurrences), and the state tran-
sitions capture possible information-presentation
orderings. The evaluation results showed that
their method outperformed Lapata?s approach by a
wide margin. They did not compare their method
with chronological ordering as an application of
multi-document summarization.
As described above, several good strate-
gies/heuristics to deal with the sentence ordering
problem have been proposed. In order to integrate
multiple strategies/heuristics, we have formalized
them in a machine learning framework and have
considered an algorithm to arrange sentences us-
ing the integrated strategy.
3 Method
We define notation a ? b to represent that sen-
tence a precedes sentence b. We use the term seg-
ment to describe a sequence of ordered sentences.
When segment A consists of sentences a1, a2, ...,
am in this order, we denote as:
A = (a1 ? a2 ? ... ? am). (1)
The two segments A and B can be ordered either
B after A or A after B. We define the notation
A ? B to show that segment A precedes segment
B.
Let us consider a bottom-up approach in arrang-
ing sentences. Starting with a set of segments ini-
tialized with a sentence for each, we concatenate
two segments, with the strongest association (dis-
cussed later) of all possible segment pairs, into
one segment. Repeating the concatenating will
eventually yield a segment with all sentences ar-
ranged. The algorithm is considered as a variation
of agglomerative hierarchical clustering with the
ordering information retained at each concatenat-
ing process.
The underlying idea of the algorithm, a bottom-
up approach to text planning, was proposed by
Marcu (1997). Assuming that the semantic units
(sentences) and their rhetorical relations (eg, sen-
tence a is an elaboration of sentence d) are given,
he transcribed a text structuring task into the prob-
lem of finding the best discourse tree that satisfied
the set of rhetorical relations. He stated that global
coherence could be achieved by satisfying local
coherence constraints in ordering and clustering,
thereby ensuring that the resultant discourse tree
was well-formed.
Unfortunately, identifying the rhetorical rela-
tion between two sentences has been a difficult
386
a
A B C D
b c d
E = (b a)
G = (b a c d)
F = (c d)
Segments
Sentences
f (as
soci
ation
 stre
ngth
)
Figure 1: Arranging four sentences A, B, C, and
D with a bottom-up approach.
task for computers. However, the bottom-up algo-
rithm for arranging sentences can still be applied
only if the direction and strength of the associa-
tion of the two segments (sentences) are defined.
Hence, we introduce a function f(A ? B) to rep-
resent the direction and strength of the association
of two segments A and B,
f(A ? B) =
{ p (if A precedes B)
0 (if B precedes A) , (2)
where p (0 ? p ? 1) denotes the association
strength of the segments A and B. The associa-
tion strengths of the two segments with different
directions, eg, f(A ? B) and f(B ? A), are not
always identical in our definition,
f(A ? B) 6= f(B ? A). (3)
Figure 1 shows the process of arranging four
sentences a, b, c, and d. Firstly, we initialize four
segments with a sentence for each,
A = (a), B = (b), C = (c), D = (d). (4)
Suppose that f(B ? A) has the highest value of
all possible pairs, eg, f(A ? B), f(C ? D), etc,
we concatenate B and A to obtain a new segment,
E = (b ? a). (5)
Then we search for the segment pair with the
strongest association. Supposing that f(C ? D)
has the highest value, we concatenate C and D to
obtain a new segment,
F = (c ? d). (6)
Finally, comparing f(E ? F ) and f(F ? E), we
obtain the global sentence ordering,
G = (b ? a ? c ? d). (7)
In the above description, we have not defined
the association of the two segments. The previ-
ous work described in Section 2 has addressed the
association of textual segments (sentences) to ob-
tain coherent orderings. We define four criteria to
capture the association of two segments: chronol-
ogy; topical-closeness; precedence; and succes-
sion. These criteria are integrated into a function
f(A ? B) by using a machine learning approach.
The rest of this section explains the four criteria
and an integration method with a Support Vector
Machine (SVM) (Vapnik, 1998) classifier.
3.1 Chronology criterion
Chronology criterion reflects the chronological or-
dering (Lin and Hovy, 2001; McKeown et al,
1999), which arranges sentences in a chronologi-
cal order of the publication date. We define the as-
sociation strength of arranging segments B after A
measured by a chronology criterion fchro(A ? B)
in the following formula,
fchro(A ? B)
=
?
???
???
1 T(am) < T(b1)
1 [D(am) = D(b1)] ? [N(am) < N(b1)]
0.5 [T(am) = T(b1)] ? [D(am) 6= D(b1)]
0 otherwise
.
(8)
Here, am represents the last sentence in segment
A; b1 represents the first sentence in segment B;
T (s) is the publication date of the sentence s;
D(s) is the unique identifier of the document to
which sentence s belongs: and N(s) denotes the
line number of sentence s in the original docu-
ment. The chronological order of arranging seg-
ment B after A is determined by the comparison
between the last sentence in the segment A and the
first sentence in the segment B.
The chronology criterion assesses the appropri-
ateness of arranging segment B after A if: sen-
tence am is published earlier than b1; or sentence
am appears before b1 in the same article. If sen-
tence am and b1 are published on the same day but
appear in different articles, the criterion assumes
the order to be undefined. If none of the above
conditions are satisfied, the criterion estimates that
segment B will precede A.
3.2 Topical-closeness criterion
The topical-closeness criterion deals with the as-
sociation, based on the topical similarity, of two
387
a1a2
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....a3a4
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
b1
b2
b3
b3
b2
b1 Pb1 Pb2 Pb3
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
Segment A
?
Segment B
Original articlefor sentence b Original articlefor sentence b2 Original articlefor sentence b31
max
average
maxmax
Figure 2: Precedence criterion
segments. The criterion reflects the ordering strat-
egy proposed by Barzilay et al(2002), which
groups sentences referring to the same topic. To
measure the topical closeness of two sentences, we
represent each sentence with a vector whose ele-
ments correspond to the occurrence1 of the nouns
and verbs in the sentence. We define the topical
closeness of two segments A and B as follows,
ftopic(A ? B) = 1|B|
?
b?B
max
a?A
sim(a, b). (9)
Here, sim(a, b) denotes the similarity of sentences
a and b, which is calculated by the cosine similar-
ity of two vectors corresponding to the sentences.
For sentence b ? B, maxa?A sim(a, b) chooses
the sentence a ? A most similar to sentence b and
yields the similarity. The topical-closeness crite-
rion ftopic(A ? B) assigns a higher value when
the topic referred by segment B is the same as seg-
ment A.
3.3 Precedence criterion
Let us think of the case where we arrange seg-
ment A before B. Each sentence in segment B
has the presuppositional information that should
be conveyed to a reader in advance. Given sen-
tence b ? B, such presuppositional information
may be presented by the sentences appearing be-
fore the sentence b in the original article. How-
ever, we cannot guarantee whether a sentence-
extraction method for multi-document summa-
rization chooses any sentences before b for a sum-
mary because the extraction method usually deter-
1The vector values are represented by boolean values, i.e.,
1 if the sentence contains a word, otherwise 0.
a1a2
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ........ .. .. .... .. ....... ....... ... ...... .. .., .... ... ....a3 .... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
b
b2
b3
a3
a2
a1 S a1 S a2 S a3
. ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
.... .. .. .... .. ....... ....... ... ...... .. .., .... ... ....
Segment A
?
Segment B
Original articlefor sentence a1 Original articlefor sentence a2 Original articlefor sentence a3
max
average
maxmax
.... .. .. .... .. ....... ......1
Figure 3: Succession criterion
mines a set of sentences, within the constraint of
summary length, that maximizes information cov-
erage and excludes redundant information. Prece-
dence criterion measures the substitutability of the
presuppositional information of segment B (eg,
the sentences appearing before sentence b) as seg-
ment A. This criterion is a formalization of the
sentence-ordering algorithm proposed by Okazaki
et al (2004).
We define the precedence criterion in the fol-
lowing formula,
fpre(A ? B) = 1|B|
?
b?B
max
a?A,p?Pb
sim(a, p).
(10)
Here, Pb is a set of sentences appearing before sen-
tence b in the original article; and sim(a, b) de-
notes the cosine similarity of sentences a and b
(defined as in the topical-closeness criterion). Fig-
ure 2 shows an example of calculating the prece-
dence criterion for arranging segment B after A.
We approximate the presuppositional information
for sentence b by sentences Pb, ie, sentences ap-
pearing before the sentence b in the original arti-
cle. Calculating the similarity among sentences in
Pb and A by the maximum similarity of the pos-
sible sentence combinations, Formula 10 is inter-
preted as the average similarity of the precedent
sentences ?Pb(b ? B) to the segment A.
3.4 Succession criterion
The idea of succession criterion is the exact op-
posite of the precedence criterion. The succession
criterion assesses the coverage of the succedent in-
formation for segment A by arranging segment B
388
ab
c
d
Partitioning point
 segment before the
 partitioning point
segment after the
partitioning point
Partitioning 
window
Figure 4: Partitioning a human-ordered extract
into pairs of segments
after A:
fsucc(A ? B) = 1|A|
?
a?A
max
s?Sa,b?B
sim(s, b).
(11)
Here, Sa is a set of sentences appearing after sen-
tence a in the original article; and sim(a, b) de-
notes the cosine similarity of sentences a and b
(defined as in the topical-closeness criterion). Fig-
ure 3 shows an example of calculating the succes-
sion criterion to arrange segments B after A. The
succession criterion measures the substitutability
of the succedent information (eg, the sentences ap-
pearing after the sentence a ? A) as segment B.
3.5 SVM classifier to assess the integrated
criterion
We integrate the four criteria described above
to define the function f(A ? B) to represent
the association direction and strength of the two
segments A and B (Formula 2). More specifi-
cally, given the two segments A and B, function
f(A ? B) is defined to yield the integrated asso-
ciation strength from four values, fchro(A ? B),
ftopic(A ? B), fpre(A ? B), and fsucc(A ? B).
We formalize the integration task as a binary clas-
sification problem and employ a Support Vector
Machine (SVM) as the classifier. We conducted a
supervised learning as follows.
We partition a human-ordered extract into pairs
each of which consists of two non-overlapping
segments. Let us explain the partitioning process
taking four human-ordered sentences, a ? b ?
c ? d shown in Figure 4. Firstly, we place the
partitioning point just after the first sentence a.
Focusing on sentence a arranged just before the
partition point and sentence b arranged just after
we identify the pair {(a), (b)} of two segments
(a) and (b). Enumerating all possible pairs of two
segments facing just before/after the partitioning
point, we obtain the following pairs, {(a), (b ?
c)} and {(a), (b ? c ? d)}. Similarly, segment
+1 : [fchro(A ? B), ftopic(A ? B), fpre(A ? B), fsucc(A ? B)]
?1 : [fchro(B ? A), ftopic(B ? A), fpre(B ? A), fsucc(B ? A)]
Figure 5: Two vectors in a training data generated
from two ordered segments A ? B
pairs, {(b), (c)}, {(a ? b), (c)}, {(b), (c ? d)},
{(a ? b), (c ? d)}, are obtained from the parti-
tioning point between sentence b and c. Collect-
ing the segment pairs from the partitioning point
between sentences c and d (i.e., {(c), (d)}, {(b ?
c), (d)} and {(a ? b ? c), (d)}), we identify ten
pairs in total form the four ordered sentences. In
general, this process yields n(n2?1)/6 pairs from
ordered n sentences. From each pair of segments,
we generate one positive and one negative training
instance as follows.
Given a pair of two segments A and B arranged
in an order A ? B, we calculate four values,
fchro(A ? B), ftopic(A ? B), fpre(A ? B),
and fsucc(A ? B) to obtain the instance with
the four-dimensional vector (Figure 5). We label
the instance (corresponding to A ? B) as a posi-
tive class (ie, +1). Simultaneously, we obtain an-
other instance with a four-dimensional vector cor-
responding to B ? A. We label it as a negative
class (ie, ?1). Accumulating these instances as
training data, we obtain a binary classifier by using
a Support Vector Machine with a quadratic kernel.
The SVM classifier yields the association direc-
tion of two segments (eg, A ? B or B ? A) with
the class information (ie, +1 or ?1). We assign
the association strength of two segments by using
the class probability estimate that the instance be-
longs to a positive (+1) class. When an instance
is classified into a negative (?1) class, we set the
association strength as zero (see the definition of
Formula 2).
4 Evaluation
We evaluated the proposed method by using the
3rd Text Summarization Challenge (TSC-3) cor-
pus2. The TSC-3 corpus contains 30 sets of ex-
tracts, each of which consists of unordered sen-
tences3 extracted from Japanese newspaper arti-
cles relevant to a topic (query). We arrange the
extracts by using different algorithms and evaluate
2http://lr-www.pi.titech.ac.jp/tsc/tsc3-en.html
3Each extract consists of ca. 15 sentences on average.
389
Table 1: Correlation between two sets of human-
ordered extracts
Metric Mean Std. Dev Min Max
Spearman 0.739 0.304 -0.2 1
Kendall 0.694 0.290 0 1
Average Continuity 0.401 0.404 0.001 1
the readability of the ordered extracts by a subjec-
tive grading and several metrics.
In order to construct training data applica-
ble to the proposed method, we asked two hu-
man subjects to arrange the extracts and obtained
30(topics) ? 2(humans) = 60 sets of ordered
extracts. Table 1 shows the agreement of the or-
dered extracts between the two subjects. The cor-
relation is measured by three metrics, Spearman?s
rank correlation, Kendall?s rank correlation, and
average continuity (described later). The mean
correlation values (0.74 for Spearman?s rank cor-
relation and 0.69 for Kendall?s rank correlation)
indicate a certain level of agreement in sentence
orderings made by the two subjects. 8 out of 30
extracts were actually identical.
We applied the leave-one-out method to the pro-
posed method to produce a set of sentence or-
derings. In this experiment, the leave-out-out
method arranges an extract by using an SVM
model trained from the rest of the 29 extracts. Re-
peating this process 30 times with a different topic
for each iteration, we generated a set of 30 ex-
tracts for evaluation. In addition to the proposed
method, we prepared six sets of sentence orderings
produced by different algorithms for comparison.
We describe briefly the seven algorithms (includ-
ing the proposed method):
Agglomerative ordering (AGL) is an ordering
arranged by the proposed method;
Random ordering (RND) is the lowest anchor,
in which sentences are arranged randomly;
Human-made ordering (HUM) is the highest
anchor, in which sentences are arranged by
a human subject;
Chronological ordering (CHR) arranges sen-
tences with the chronology criterion defined
in Formula 8. Sentences are arranged in
chronological order of their publication date;
Topical-closeness ordering (TOP) arranges sen-
tences with the topical-closeness criterion de-
fined in Formula 9;
0 20 40 60 80 100
UnacceptablePoorAcceptablePerfect
HUM
AGL
CHR
RND
%
Figure 6: Subjective grading
Precedence ordering (PRE) arranges sentences
with the precedence criterion defined in For-
mula 10;
Suceedence ordering (SUC) arranges sentences
with the succession criterion defined in For-
mula 11.
The last four algorithms (CHR, TOP, PRE, and
SUC) arrange sentences by the corresponding cri-
terion alone, each of which uses the association
strength directly to arrange sentences without the
integration of other criteria. These orderings are
expected to show the performance of each expert
independently and their contribution to solving the
sentence ordering problem.
4.1 Subjective grading
Evaluating a sentence ordering is a challenging
task. Intrinsic evaluation that involves human
judges to rank a set of sentence orderings is a nec-
essary approach to this task (Barzilay et al, 2002;
Okazaki et al, 2004). We asked two human judges
to rate sentence orderings according to the follow-
ing criteria. A perfect summary is a text that we
cannot improve any further by re-ordering. An ac-
ceptable summary is one that makes sense and is
unnecessary to revise even though there is some
room for improvement in terms of readability. A
poor summary is one that loses a thread of the
story at some places and requires minor amend-
ment to bring it up to an acceptable level. An un-
acceptable summary is one that leaves much to be
improved and requires overall restructuring rather
than partial revision. To avoid any disturbance in
rating, we inform the judges that the summaries
were made from a same set of extracted sentences
and only the ordering of sentences is different.
Figure 6 shows the distribution of the subjective
grading made by two judges to four sets of order-
ings, RND, CHR, AGL and HUM. Each set of or-
390
Teval = (e ? a ? b ? c ? d)
Tref = (a ? b ? c ? d ? e)
Figure 7: An example of an ordering under evalu-
ation Teval and its reference Tref .
derings has 30(topics) ? 2(judges) = 60 ratings.
Most RND orderings are rated as unacceptable.
Although CHR and AGL orderings have roughly
the same number of perfect orderings (ca. 25%),
the AGL algorithm gained more acceptable order-
ings (47%) than the CHR alghrotihm (30%). This
fact shows that integration of CHR experts with
other experts worked well by pushing poor order-
ing to an acceptable level. However, a huge gap
between AGL and HUM orderings was also found.
The judges rated 28% AGL orderings as perfect
while the figure rose as high as 82% for HUM
orderings. Kendall?s coefficient of concordance
(Kendall?s W ), which asses the inter-judge agree-
ment of overall ratings, reported a higher agree-
ment between the two judges (W = 0.939).
4.2 Metrics for semi-automatic evaluation
We also evaluated sentence orderings by reusing
two sets of gold-standard orderings made for the
training data. In general, subjective grading con-
sumes much time and effort, even though we
cannot reproduce the evaluation afterwards. The
previous studies (Barzilay et al, 2002; Lapata,
2003) employ rank correlation coefficients such
as Spearman?s rank correlation and Kendall?s rank
correlation, assuming a sentence ordering to be
a rank. Okazaki et al (2004) propose a metric
that assess continuity of pairwise sentences com-
pared with the gold standard. In addition to Spear-
man?s and Kendall?s rank correlation coefficients,
we propose an average continuity metric, which
extends the idea of the continuity metric to contin-
uous k sentences.
A text with sentences arranged in proper order
does not interrupt a human?s reading while moving
from one sentence to the next. Hence, the qual-
ity of a sentence ordering can be estimated by the
number of continuous sentences that are also re-
produced in the reference sentence ordering. This
is equivalent to measuring a precision of continu-
ous sentences in an ordering against the reference
ordering. We define Pn to measure the precision of
Table 2: Comparison with human-made ordering
Method Spearman Kendall Average
coefficient coefficient Continuity
RND -0.127 -0.069 0.011
TOP 0.414 0.400 0.197
PRE 0.415 0.428 0.293
SUC 0.473 0.476 0.291
CHR 0.583 0.587 0.356
AGL 0.603 0.612 0.459
n continuous sentences in an ordering to be evalu-
ated as,
Pn = mN ? n+ 1 . (12)
Here, N is the number of sentences in the refer-
ence ordering; n is the length of continuous sen-
tences on which we are evaluating; m is the num-
ber of continuous sentences that appear in both the
evaluation and reference orderings. In Figure 7,
the precision of 3 continuous sentences P3 is cal-
culated as:
P3 = 25? 3 + 1 = 0.67. (13)
The Average Continuity (AC) is defined as the
logarithmic average of Pn over 2 to k:
AC = exp
(
1
k ? 1
k?
n=2
log(Pn + ?)
)
. (14)
Here, k is a parameter to control the range of the
logarithmic average; and ? is a small value in case
if Pn is zero. We set k = 4 (ie, more than five
continuous sentences are not included for evalua-
tion) and ? = 0.01. Average Continuity becomes
0 when evaluation and reference orderings share
no continuous sentences and 1 when the two or-
derings are identical. In Figure 7, Average Conti-
nuity is calculated as 0.63. The underlying idea of
Formula 14 was proposed by Papineni et al (2002)
as the BLEU metric for the semi-automatic evalu-
ation of machine-translation systems. The origi-
nal definition of the BLEU metric is to compare a
machine-translated text with its reference transla-
tion by using the word n-grams.
4.3 Results of semi-automatic evaluation
Table 2 reports the resemblance of orderings pro-
duced by six algorithms to the human-made ones
with three metrics, Spearman?s rank correlation,
Kendall?s rank correlation, and Average Continu-
ity. The proposed method (AGL) outperforms the
391
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
AGLCHRSUCPRETOPRND
8765432
P
re
ci s i o
n P
n
Length n
Figure 8: Precision vs unit of measuring continu-
ity.
rest in all evaluation metrics, although the chrono-
logical ordering (CHR) appeared to play the major
role. The one-way analysis of variance (ANOVA)
verified the effects of different algorithms for sen-
tence orderings with all metrics (p < 0.01). We
performed Tukey Honest Significant Differences
(HSD) test to compare differences among these al-
gorithms. The Tukey test revealed that AGL was
significantly better than the rest. Even though we
could not compare our experiment with the prob-
abilistic approach (Lapata, 2003) directly due to
the difference of the text corpora, the Kendall co-
efficient reported higher agreement than Lapata?s
experiment (Kendall=0.48 with lemmatized nouns
and Kendall=0.56 with verb-noun dependencies).
Figure 8 shows precision Pn with different
length values of continuous sentence n for the six
methods compared in Table 2. The number of
continuous sentences becomes sparse for a higher
value of length n. Therefore, the precision values
decrease as the length n increases. Although RND
ordering reported some continuous sentences for
lower n values, no continuous sentences could be
observed for the higher n values. Four criteria de-
scribed in Section 3 (ie, CHR, TOP, PRE, SUC)
produce segments of continuous sentences at all
values of n.
5 Conclusion
We present a bottom-up approach to arrange sen-
tences extracted for multi-document summariza-
tion. Our experimental results showed a signif-
icant improvement over existing sentence order-
ing strategies. However, the results also implied
that chronological ordering played the major role
in arranging sentences. A future direction of this
study would be to explore the application of the
proposed framework to more generic texts, such
as documents without chronological information.
Acknowledgment
We used Mainichi Shinbun and Yomiuri Shinbun
newspaper articles, and the TSC-3 test collection.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen McK-
eown. 2002. Inferring strategies for sentence order-
ing in multidocument news summarization. Journal
of Artificial Intelligence Research, 17:35?55.
Mirella Lapata. 2003. Probabilistic text structuring:
Experiments with sentence ordering. Proceedings of
the annual meeting of ACL, 2003., pages 545?552.
C.Y. Lin and E. Hovy. 2001. Neats:a multidocument
summarizer. Proceedings of the Document Under-
standing Workshop(DUC).
W. Mann and S. Thompson. 1988. Rhetorical structure
theory: Toward a functional theory of text organiza-
tion. Text, 8:243?281.
Daniel Marcu. 1997. From local to global coherence:
A bottom-up approach to text planning. In Proceed-
ings of the 14th National Conference on Artificial
Intelligence, pages 629?635, Providence, Rhode Is-
land.
Kathleen McKeown, Judith Klavans, Vasileios Hatzi-
vassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by
reformulation: Progress and prospects. AAAI/IAAI,
pages 453?460.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru
Ishizuka. 2004. Improving chronological sentence
ordering by precedence relation. In Proceedings
of 20th International Conference on Computational
Linguistics (COLING 04), pages 750?756.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu:a method for automatic eval-
uation of machine translation. Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311?318.
Dragomir R. Radev and Kathy McKeown. 1999.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24:469?500.
V. Vapnik. 1998. Statistical Learning Theory. Wiley,
Chichester, GB.
392
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 643?650,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Term Recognition Approach to Acronym Recognition
Naoaki Okazaki ?
Graduate School of Information
Science and Technology
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
113-8656 Japan
okazaki@mi.ci.i.u-tokyo.ac.jp
Sophia Ananiadou
National Centre for Text Mining
School of Informatics
Manchester University
PO Box 88, Sackville Street, Manchester
M60 1QD United Kingdom
Sophia.Ananiadou@manchester.ac.uk
Abstract
We present a term recognition approach
to extract acronyms and their definitions
from a large text collection. Parentheti-
cal expressions appearing in a text collec-
tion are identified as potential acronyms.
Assuming terms appearing frequently in
the proximity of an acronym to be
the expanded forms (definitions) of the
acronyms, we apply a term recognition
method to enumerate such candidates and
to measure the likelihood scores of the
expanded forms. Based on the list of
the expanded forms and their likelihood
scores, the proposed algorithm determines
the final acronym-definition pairs. The
proposed method combined with a letter
matching algorithm achieved 78% preci-
sion and 85% recall on an evaluation cor-
pus with 4,212 acronym-definition pairs.
1 Introduction
In the biomedical literature the amount of terms
(names of genes, proteins, chemical compounds,
drugs, organisms, etc) is increasing at an astound-
ing rate. Existing terminological resources and
scientific databases (such as Swiss-Prot1, SGD2,
FlyBase3, and UniProt4) cannot keep up-to-date
with the growth of neologisms (Pustejovsky et al,
2001). Although curation teams maintain termino-
logical resources, integrating neologisms is very
difficult if not based on systematic extraction and
?Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
1http://www.ebi.ac.uk/swissprot/
2http://www.yeastgenome.org/
3http://www.flybase.org/
4http://www.ebi.ac.uk/GOA/
collection of terminology from literature. Term
identification in literature is one of the major bot-
tlenecks in processing information in biology as it
faces many challenges (Ananiadou and Nenadic,
2006; Friedman et al, 2001; Bodenreider, 2004).
The major challenges are due to term variation,
e.g. spelling, morphological, syntactic, semantic
variations (one term having different termforms),
term synonymy and homonymy, which are all cen-
tral concerns of any term management system.
Acronyms are among the most productive type
of term variation. Acronyms (e.g. RARA)
are compressed forms of terms, and are used
as substitutes of the fully expanded termforms
(e.g., retinoic acid receptor alpha). Chang and
Schu?tze (2006) reported that, in MEDLINE ab-
stracts, 64,242 new acronyms were introduced in
2004 with the estimated number being 800,000.
Wren et al (2005) reported that 5,477 documents
could be retrieved by using the acronym JNK
while only 3,773 documents could be retrieved by
using its full term, c-jun N-terminal kinase.
In practice, there are no rules or exact patterns
for the creation of acronyms. Moreover, acronyms
are ambiguous, i.e., the same acronym may re-
fer to different concepts (GR abbreviates both glu-
cocorticoid receptor and glutathione reductase).
Acronyms also have variant forms (e.g. NF kappa
B, NF kB, NF-KB, NF-kappaB, NFKB factor for
nuclear factor-kappa B). Ambiguity and variation
present a challenge for any text mining system,
since acronyms have not only to be recognised, but
their variants have to be linked to the same canon-
ical form and be disambiguated.
Thus, discovering acronyms and relating them
to their expanded forms is important for terminol-
ogy management. In this paper, we present a term
recognition approach to construct an acronym dic-
643
tionary from a large text collection. The proposed
method focuses on terms appearing frequently in
the proximity of an acronym and measures the
likelihood scores of such terms to be the expanded
forms of the acronyms. We also describe an algo-
rithm to combine the proposed method with a con-
ventional letter-based method for acronym recog-
nition.
2 Related Work
The goal of acronym identification is to extract
pairs of short forms (acronyms) and long forms
(their expanded forms or definitions) occurring in
text5. Currently, most methods are based on let-
ter matching of the acronym-definition pair, e.g.,
hidden markov model (HMM), to identify short/-
long form candidates. Existing methods of short-
/long form recognition are divided into pattern
matching approaches, e.g., exploring an efficient
set of heuristics/rules (Adar, 2004; Ao and Takagi,
2005; Schwartz and Hearst, 2003; Wren and Gar-
ner, 2002; Yu et al, 2002), and pattern mining ap-
proaches, e.g., Longest Common Substring (LCS)
formalization (Chang and Schu?tze, 2006; Taghva
and Gilbreth, 1999).
Schwartz and Hearst (2003) implemented an al-
gorithm for identifying acronyms by using paren-
thetical expressions as a marker of a short form.
A character matching technique was used, i.e. all
letters and digits in a short form had to appear in
the corresponding long form in the same order, to
determine its long form. Even though the core al-
gorithm was very simple, the authors report 99%
precision and 84% recall on the Medstract gold
standard6.
However, the letter-matching approach is af-
fected by the expressions in the source text and
sometimes finds incorrect long forms such as
acquired syndrome and a patient with human
immunodeficiency syndrome7 instead of the cor-
rect one, acquired immune deficiency syndrome
for the acronym AIDS. This approach also en-
counters difficulties finding a long form whose
short form is arranged in a different word order,
e.g., beta 2 adrenergic receptor (ADRB2). To
5This paper uses the terms ?short form? and ?long form?
hereafter. ?Long form? is what others call ?definition?,
?meaning?, ?expansion?, and ?expanded form? of acronym.
6http://www.medstract.org/
7These examples are obtained from the actual MED-
LINE abstracts submitted to Schwartz and Hearst?s algorithm
(2003). An author does not always write a proper definition
with a parenthetic expression.
improve the accuracy of long/short form recogni-
tion, some methods measure the appropriateness
of these candidates based on a set of rules (Ao and
Takagi, 2005), scoring functions (Adar, 2004), sta-
tistical analysis (Hisamitsu and Niwa, 2001; Liu
and Friedman, 2003) and machine learning ap-
proaches (Chang and Schu?tze, 2006; Pakhomov,
2002; Nadeau and Turney, 2005).
Chang and Schu?tze (2006) present an algorithm
for matching short/long forms with a statistical
learning method. They discover a list of abbrevia-
tion candidates based on parentheses and enumer-
ate possible short/long form candidates by a dy-
namic programming algorithm. The likelihood of
the recognized candidates is estimated as the prob-
ability calculated from a logistic regression with
nine features such as the percentage of long-form
letters aligned at the beginning of a word. Their
method achieved 80% precision and 83% recall on
the Medstract corpus.
Hisamitsu and Niwa (2001) propose a method
for extracting useful parenthetical expressions
from Japanese newspaper articles. Their method
measures the co-occurrence strength between the
inner and outer phrases of a parenthetical expres-
sion by using statistical measures such as mutual
information, ?2 test with Yate?s correction, Dice
coefficient, log-likelihood ratio, etc. Their method
deals with generic parenthetical expressions (e.g.,
abbreviation, non abbreviation paraphrase, supple-
mentary comments), not focusing exclusively on
acronym recognition.
Liu and Friedman (2003) proposed a method
based on mining collocations occurring before the
parenthetical expressions. Their method creates a
list of potential long forms from collocations ap-
pearing more than once in a text collection and
eliminates unlikely candidates with three rules,
e.g., ?remove a set of candidates Tw formed by
adding a prefix word to a candidate w if the num-
ber of such candidates Tw is greater than 3?. Their
approach cannot recognise expanded forms occur-
ring only once in the corpus. They reported a pre-
cision of 96.3% and a recall of 88.5% for abbrevi-
ations recognition on their test corpus.
3 Methodology
3.1 Term-based long-form identification
We propose a method for identifying the long
forms of an acronym based on a term extrac-
tion technique. We focus on terms appearing fre-
644
factor 1 (TTF-1)transcription
transciptiontransription
thyroid
thyroidtissue specific nkx2
thyroidthyroid
expression of
co-expression ofregulation of thecontainingexpressedstained foridentification of
encodinggene
examinedexploreincreasedstudiedits...... ............
............
............................................................
............
............
............ ......
216 218213209
11
33
1
1
1
1
1
1
1
1
1
1
1
1
factor5one1protein11
4 231 factor2
1
nuclearthyroid...... 1
* These candidates are spelling mistakes    found in the MEDLINE abstracts.
Figure 1: Long-form candidates for TTF-1.
quently in the proximity of an acronym in a text
collection. More specifically, if a word sequence
co-occurs frequently with a specific acronym and
not with other surrounding words, we assume that
there is a relationship8 between the acronym and
the word sequence.
Figure 1 illustrates our hypothesis taking the
acronym TTF-1 as an example. The tree consists
of expressions collected from all sentences with
the acronym in parentheses and appearing before
the acronym. A node represents a word, and a path
from any node to TTF-1 represents a long-form
candidate9. The figure above each node shows
the co-occurrence frequency of the corresponding
long-form candidate. For example, long-form can-
didates 1, factor 1, transcription factor 1, and thy-
roid transcription factor 1 co-occur 218, 216, 213,
and 209 times respectively with the acronym TTF-
1 in the text collection.
Even though long-form candidates 1, factor
1 and transcription factor 1 co-occur frequently
with the acronym TTF-1, we note that they
also co-occur frequently with the word thyroid.
Meanwhile, the candidate thyroid transcription
factor 1 is used in a number of contexts (e.g.,
expression of thyroid transcription factor 1,
expressed thyroid transcription factor 1, gene
encoding thyroid transcription factor 1, etc.).
Therefore, we observe this to be the strongest
relationship between acronym TTF-1 and its
8A sequence of words that co-occurs with an acronym
does not always imply the acronym-definition relation. For
example, the acronym 5-HT co-occurs frequently with the
term serotonin, but their relation is interpreted as a synony-
mous relation.
9The words with function words (e.g., expression of, reg-
ulation of the, etc.) are combined into a node. This is due
to the requirement for a long-form candidate discussed later
(Section 3.3).
A large collection of text
Contextual sentencesfor acronyms
Acronym dictionary
Short-formmining
Long-formminingLong-formvalidation
Raw text
Sentences witha specific acronym
All sentences withany acronyms
Acronyms andexpanded forms
Figure 2: System diagram of acronym recognition
long-form candidate thyroid transcription factor 1
in the tree. We apply a number of validation rules
(described later) to the candidate pair to make
sure that it has an acronym-definition relation. In
this example, the candidate pair is likely to be
an acronym-definition relation because the long
form thyroid transcription factor 1 contains all
alphanumeric letters in the short form TTF-1.
Figure 1 also shows another notable character-
istic of long-form recognition. Assuming that the
term thyroid transcription factor 1 has an acronym
TTF-1, we can disregard candidates such as tran-
scription factor 1, factor 1, and 1 since they lack
the necessary elements (e.g., thyroid for all can-
didates; thyroid transcription for candidates fac-
tor 1 and 1; etc.) to produce the acronym TTF-
1. Similarly, we can disregard candidates such
as expression of thyroid transcription factor 1 and
encoding thyroid transcription factor 1 since they
contain unnecessary elements (i.e., expression of
and encoding) attached to the long-form. Hence,
once thyroid transcription factor 1 is chosen as
the most likely long form of the acronym TTF-
1, we prune the unlikely candidates: nested can-
didates (e.g., transcription factor 1); expansions
(e.g., expression of thyroid transcription factor 1);
and insertions (e.g., thyroid specific transcription
factor 1).
3.2 Extracting acronyms and their contexts
Before describing in detail the formalization of
long-form identification, we explain the whole
process of acronym recognition. We divide the
acronym extraction task into three steps (Figure
2):
1. Short-form mining: identifying and extract-
ing short forms (i.e., acronyms) in a collec-
tion of documents
2. Long-form mining: generating a list of
ranked long-form candidates for each short
645
Acronym Contextual sentence
... .... .... .. . .... ..
HML Hard metal lung diseases (HML) are rare, and complex
to diagnose.
HMM Heavy meromyosin (HMM) from conditioned hearts
had a higher Ca++-ATPase activity than from controls.
HMM Heavy meromyosin (HMM) and myosin subfragment 1
(S1) were prepared from myosin by using low concen-
trations of alpha-chymotrypsin.
HMM Hidden Markov model (HMM) techniques are used to
model families of biological sequences.
HMM Hexamethylmelamine (HMM) is a cytotoxic agent
demonstrated to have broad antitumor activity.
HMN Hereditary metabolic neuropathies (HMN) are marked
by inherited enzyme or other metabolic defects.
... ... .. ..... .. ....... . .......
Table 1: An example of extracted acronyms and
their contextual sentences.
form by using a term extraction technique
3. Long-form validation: extracting short/long
form pairs recognized as having an acronym-
definition relation and eliminating unneces-
sary candidates.
The first step, short-form mining, enumerates all
short forms in a target text which are likely to be
acronyms. Most studies make use of the follow-
ing pattern to find candidate acronyms (Wren and
Garner, 2002; Schwartz and Hearst, 2003):
long form ?(? short form ?)?
Just as the heuristic rules described in Schwartz
and Hearst (Schwartz and Hearst, 2003), we con-
sider short forms to be valid only if they consist of
at most two words; their length is between two to
ten characters; they contain at least an alphabetic
letter; and the first character is alphanumeric. All
sentences containing a short form in parenthesis
are inserted into a database, which returns all con-
textual sentences for a short form to be processed
in the next step. Table 1 shows an example of the
database content.
3.3 Formalizing long-form mining as a term
extraction problem
The second step, long-form mining, generates a
list of long-form candidates and their likelihood
scores for each short form. As mentioned previ-
ously, we focus on words or word sequences that
co-occur frequently with a specific acronym and
not with any other surrounding words. We deal
with the problem of extracting long-form candi-
dates from contextual sentences for an acronym
in a similar manner as the term recognition task
which extracts terms from the given text. For that
purpose, we used a modified version of the C-
value method (Frantzi and Ananiadou, 1999).
C-value is a domain-independent method for
automatic term recognition (ATR) which com-
bines linguistic and statistical information, empha-
sis being placed on the statistical part. The lin-
guistic analysis enumerates all candidate terms in
a given text by applying part-of-speech tagging,
candidate extraction (e.g., extracting sequences
of adjectives/nouns based on part-of-speech tags),
and a stop-list. The statistical analysis assigns
a termhood (likelihood to be a term) to a candi-
date term by using the following features: the fre-
quency of occurrence of the candidate term; the
frequency of the candidate term as part of other
longer candidate terms; the number of these longer
candidate terms; and the length of the candidate
term.
The C-value approach is characterized by the
extraction of nested terms which gives preference
to terms appearing frequently in a given text but
not as a part of specific longer terms. This is a de-
sirable feature for acronym recognition to identify
long-form candidates in contextual sentences. The
rest of this subsection describes the method to ex-
tract long-form candidates and to assign scores to
the candidates based on the C-value approach.
Given a contextual sentence as shown in Ta-
ble 1, we tokenize a contextual sentence by
non-alphanumeric characters (e.g., space, hyphen,
colon, etc.) and apply Porter?s stemming algo-
rithm (Porter, 1980) to obtain a sequence of nor-
malized words. We use the following pattern to
extract long-form candidates from the sequence:
[:WORD:].*$ (1)
Therein: [:WORD:] matches a non-function
word; .* matches an empty string or any word(s)
of any length; and $ matches a short form of the
target acronym. The extraction pattern accepts a
word or word sequence if the word or word se-
quence begins with any non-function word, and
ends with any word just before the corresponding
short form in the contextual sentence. We have
defined 113 function words such as a, the, of, we,
and be in an external dictionary so that long-form
candidates cannot begin with these words.
Let us take the example of a contextual sen-
tence, ?we studied the expression of thyroid tran-
scription factor-1 (TTF-1)?. We extract the fol-
lowing substrings as long form candidates (words
are stemmed): 1; factor 1; transcript factor 1; thy-
roid transcript factor 1; expression of thyroid tran-
script factor 1; and studi the expression of thyroid
646
Candidate Length Freq Score Valid
adriamycin 1 727 721.4 o
adrenomedullin 1 247 241.7 o
abductor digiti minimi 3 78 74.9 o
doxorubicin 1 56 54.6 L
effect of adriamycin 3 25 23.6 E
adrenodemedullated 1 19 17.7 o
acellular dermal matrix 3 17 15.9 o
peptide adrenomedullin 2 17 15.1 E
effects of adrenomedullin 3 15 13.2 E
resistance to adriamycin 3 15 13.2 E
amyopathic dermatomyositis 2 14 12.8 o
vincristine (vcr) and adriamycin 4 11 10.0 E
drug adriamycin 2 14 10.0 E
brevis and abductor digiti minimi 5 11 9.8 E
minimi 1 83 5.8 N
digiti minimi 2 80 3.9 N
right abductor digiti minimi 4 4 2.5 E
automated digital microscopy 3 1 0.0 m
adrenomedullin concentration 2 1 0.0 N
Valid = { o: valid, m: letter match, L: lacks necessary letters, E: expansion,
N: nested, B: below the threshold }
Table 2: Long-form candidates for ADM.
transcript factor 1. Substrings such as of thyroid
transcript factor 1 (which begins with a function
word) and thyroid transcript (which ends prema-
turely before the short form) are not selected as
long-form candidates.
We define the likelihood LF(w) for candidate w
to be the long form of an acronym:
LF(w) = freq(w)?
?
t?Tw
freq(t)? freq(t)freq(Tw) . (2)
Therein: w is a long-form candidate; freq(x) de-
notes the frequency of occurrence of a candidate
x in the contextual sentences (i.e., co-occurrence
frequency with a short form); Tw is a set of nested
candidates, long-form candidates each of which
consists of a preceding word followed by the can-
didate w; and freq(Tw) represents the total fre-
quency of such candidates Tw.
The first term is equivalent to the co-occurrence
frequency of a long-form candidate with a short
form. The second term discounts the co-
occurrence frequency based on the frequency dis-
tribution of nested candidates. Given a long-form
candidate t ? Tw, freq(t)freq(Tw) presents the occurrence
probability of candidate t in the nested candidate
set Tw. Therefore, the second term of the formula
calculates the expectation of the frequency of oc-
currence of a nested candidate accounting for the
frequency of candidate w.
Table 2 shows a list of long-form candidates for
acronym ADM extracted from 7,306,153 MED-
LINE abstracts10. The long-form mining step
10 52GB XML files (from medline05n0001.xml to
medline05n0500.xml)
extracted 10,216 unique long-form candidates
from 1,319 contextual sentences containing the
acronym ADM in parentheses. Table 2 arranges
long-form candidates with their scores in de-
sending order. Long-form candidates adriamycin
and adrenomedullin co-occur frequently with the
acronym ADM.
Note the huge difference in scores between
the candidates abductor digiti minimi and minimi.
Even though the candidate minimi co-occurs more
frequently (83 times) than abductor digiti minimi
(78 times), the co-occurrence frequency is mostly
derived from the longer candidate, i.e., digiti min-
imi. In this case, the second term of Formula
2, the occurrence-frequency expectation of expan-
sions for minimi (e.g., digiti minimi), will have a
high value and will therefore lower the score of
candidate minimi. This is also true for the can-
didate digiti minimi, i.e., the score of candidate
digiti minimi is lowered by the longer candidate
abductor digiti minimi. In contrast, the candidate
abductor digiti minimi preserves its co-occurrence
frequency since the second term of the formula is
low, which means that each expansion (e.g, brevis
and abductor digiti minimi, right abductor digiti
minimi, ...) is expected to have a low frequency of
occurrence.
3.4 Validation rules for long-form candidates
The final step of Figure 2 validates the extracted
long-form candidates to generate a final set of
short/long form pairs. According to the score
in Table 2, adriamycin is the most likely long-
form for acronym ADM. Since the long-form
candidate adriamycin contains all letters in the
acronym ADM, it is considered as an authentic
long-form (marked as ?o? in the Valid field). This
is also true for the second and third candidate
(adrenomedullin and abductor digiti minimi).
The fourth candidate doxorubicin looks inter-
esting, i.e., the proposed method assigns a high
score to the candidate even though it lacks the let-
ters a and m, which are necessary to form the cor-
responding short form. This is because doxoru-
bicin is a synonymous term for adriamycin and de-
scribed directly with its acronym ADM. In this pa-
per, we deal with the acronym-definition relation
although the proposed method would be applica-
ble to mining other types of relations marked by
parenthetical expressions. Hence, we introduce a
constraint that a long form must cover all alphanu-
647
# [ V a r i a b l e s ]
# s f : t h e t a r g e t s h o r t?form .
# c a n d i d a t e s : long?form c a n d i d a t e s .
# r e s u l t : t h e l i s t o f d e c i s i v e long?f o rms .
# t h r e s h o l d : t h e t h r e s h o l d o f cu t?o f f .
# S o r t long?form c a n d i d a t e s i n d e s c e n d i n g o r d e r
c a n d i d a t e s . s o r t ( # o f s c o r e s .
key=lambda l f : l f . s c o r e , r e v e r s e =True )
# I n i t i a l i z e r e s u l t l i s t as empty .
r e s u l t = [ ]
# Pick up a lo ng form one by one from c a n d i d a t e s .
f o r l f in c a n d i d a t e s :
# Apply a cu t?o f f based on termhood s c o r e .
# Al low c a n d i d a t e s w i t h l e t t e r match ing . . . . . ( a )
i f l f . s c o r e < t h r e s h o l d and not l f . match :
c o n t in u e
# A long?form must c o n t a i n a l l l e t t e r s . . . . . . ( b )
i f l e t t e r r e c a l l ( s f , l f ) < 1 :
c o n t in u e
# Apply p r u n i n g o f r e d u n d a n t l ong form . . . . . . ( c )
i f r e d u n d a n t ( r e s u l t , l f ) :
c o n t in u e
# I n s e r t t h i s l ong form t o t h e r e s u l t l i s t .
r e s u l t . append ( l f )
# Outpu t t h e d e c i s i v e long?f o rms .
p r i n t r e s u l t
Figure 3: Pseudo-code for long-form validation.
meric letters in the short form.
The fifth candidate effect of adriamycin is an
expansion of a long form adriamycin, which has
a higher score than effect of adriamycin. As we
discussed previously, the candidate effect of adri-
amycin is skipped since it contains unnecessary
word(s) to form an acronym. Similarly, we prune
the candidate minimi because it forms a part of an-
other long form abductor digiti minimi, which has
a higher score than the candidate minimi. The like-
lihood score LF (w) determines the most appro-
priate long-form among similar candidates sharing
the same words or lacking some words.
We do not include candidates with scores be-
low a given threshold. Therefore, the proposed
method cannot extract candidates appearing rarely
in the text collection. It depends on the applica-
tion and considerations of the trade-off between
precision and recall, whether or not an acronym
recognition system should extract such rare long
forms. When integrating the proposed method
with e.g., Schwartz and Hearst?s algorithm, we
treat candidates recognized by the external method
as if they pass the score cut-off. In Table 2, for
example, candidate automated digital microscopy
is inserted into the result set whereas candidate
adrenomedullin concentration is skipped since it
is nested by candidate adrenomedullin.
Figure 3 is a pseudo-code for the long-form val-
idation algorithm described above. A long-form
Rank Parenthetic phrase # contextual # unique
sentence long-forms
1 CT 30,982 171
2 PCR 25,387 39
3 HIV 19,566 13
4 LPS 18,071 51
5 MRI 16,966 18
6 ELISA 16,527 25
7 SD 15,760 165
8 BP 14,860 145
9 DA 14,518 129
10 CSF 14,035 34
11 CNS 13,573 47
12 IL 13,423 60
13 PKC 13,414 11
14 TNF-ALPHA 12,228 14
15 HPLC 12,211 16
16 ER 12,155 140
17 RT-PCR 12,153 21
18 TNF 12,145 13
19 LDL 11,960 24
20 5-HT 11,836 20
.. .... ... ..
? (overall 50 acronyms) 600,375 4,212
Table 3: Statistics on our evaluation corpus.
candidate is considered valid if the following con-
ditions are met: (a) it has a score greater than
a threshold or is nominated by a letter-matching
algorithm; (b) it contains all letters in the corre-
sponding short form; and (c) it is not nested, ex-
pansion, or insertion of the previously chosen long
forms.
4 Evaluation
Several evaluation corpora for acronym recogni-
tion are available. The Medstract Gold Standard
Evaluation Corpus, which consists of 166 alias
pairs annotated to 201 MEDLINE abstracts, is
widely used for evaluation (Chang and Schu?tze,
2006; Schwartz and Hearst, 2003). However, the
amount of the text in the corpus is insufficient for
the proposed method, which makes use of statisti-
cal features in a text collection. Therefore, we pre-
pared an evaluation corpus with a large text collec-
tion and examined how the proposed algorithm ex-
tracts short/long forms precisely and comprehen-
sively.
We applied the short-form mining described
in Section 3 to 7,306,153 MEDLINE abstracts10.
Out of 921,349 unique short-forms recognized by
the short-form mining, top 50 acronyms11 appear-
ing frequently in the abstracts were chosen for our
11We have excluded several parenthetical expressions such
as II (99,378 occurrences), OH (37,452 occurrences), and
P<0.05 (23,678 occurrences). Even though they are enclosed
within parentheses, they do not introduce acronyms. We have
also excluded a few acronyms such as RA (18,655 occur-
rences) and AD (15,540 occurrences) because they have many
variations of their expanded forms to prepare the evaluation
corpus manually.
648
evaluation corpus. We asked an expert in bio-
informatics to extract long forms from 600,375
contextual sentences with the following criteria:
a long form with minimum necessary elements
(words) to produce its acronym is accepted; a long
form with unnecessary elements, e.g., magnetic
resonance imaging unit (MRI) or computed x-ray
tomography (CT), is not accepted; a misspelled
long-form, e.g., hidden markvov model (HMM),
is accepted (to separate the acronym-recognition
task from a spelling-correction task). Table 3
shows the top 20 acronyms in our evaluation cor-
pus, the number of their contextual sentences, and
the number of unique long-forms extracted.
Using this evaluation corpus as a gold standard,
we examined precision, recall, and f-measure12 of
long forms recognized by the proposed algorithm
and baseline systems. We compared five sys-
tems: the proposed algorithm with Schwartz and
Hearst?s algorithm integrated (PM+SH); the pro-
posed algorithm without any letter-matching algo-
rithm integrated (PM); the proposed algorithm but
using the original C-value measure for long-form
likelihood scores (CV+SH); the proposed algo-
rithm but using co-occurrence frequency for long-
form likelihood scores (FQ+SH); and Schwartz
and Hearst?s algorithm (SH). The threshold for the
proposed algorithm was set to four.
Table 4 shows the evaluation result. The best-
performing configuration of algorithms (PM+SH)
achieved 78% precision and 85% recall. The
Schwartz and Hearst?s (SH) algorithm obtained a
good recall (93%) but misrecognized a number
of long-forms (56% precision), e.g., the kinetics
of serum tumour necrosis alpha (TNF-ALPHA)
and infected mice lacking the gamma interferon
(IFN-GAMMA). The SH algorithm cannot gather
variations of long forms for an acronym, e.g.,
ACE as angiotensin-converting enzyme level, an-
giotensin i-converting enzyme gene, angiotensin-
1-converting enzyme, angiotensin-converting, an-
giotensin converting activity, etc. The proposed
method combined with the Schwartz and Hearst?s
algorithm remedied these misrecognitions based
on the likelihood scores and the long-form vali-
dation algorithm. The PM+SH also outperformed
other likelihood measures, CV+SH and FQ+SH.
12We count the number of unique long forms, i.e., count
once even if short/long form pair ?HMM, hidden markov
model? occurs more than once in the text collection. The
Porter?s stemming algorithm was applied to long forms be-
fore comparing them with the gold standard.
Method Precision Recall F-measure
PM+SH 0.783 0.849 0.809
CV+SH 0.722 0.838 0.765
FQ+SH 0.716 0.800 0.747
SH 0.555 0.933 0.681
PM 0.815 0.140 0.216
Table 4: Evaluation result of long-form recogni-
tion.
The proposed algorithm without Schwartz and
Hearst?s algorithm (PM) identified long forms the
most precisely (81% precision) but misses a num-
ber of long forms in the text collection (14% re-
call). The result suggested that the proposed likeli-
hood measure performed well to extract frequently
used long-forms in a large text collection, but
could not extract rare acronym-definition pairs.
We also found the case where PM missed a set of
long forms for acronym ER which end with rate,
e.g., eating rate, elimination rate, embolic rate,
etc. This was because the word rate was used with
a variety of expansions (i.e., the likelihood score
for rate was not reduced much) while it can be
also interpreted as the long form of the acronym.
Even though the Medstract corpus is insuffi-
cient for evaluating the proposed method, we ex-
amined the number of long/short pairs extracted
from 7,306,153 MEDLINE abstracts and also ap-
pearing in the Medstract corpus. We can neither
calculate the precision from this experiment nor
compare the recall directly with other acronym
recognition methods since the size of the source
texts is different. Out of 166 pairs in Medstract
corpus, 123 (74%) pairs were exactly covered by
the proposed method, and 15 (83% in total) pairs
were partially covered13. The algorithm missed 28
pairs because: 17 (10%) pairs in the corpus were
not acronyms but more generic aliases, e.g., alpha
tocopherol (Vitamin E); 4 (2%) pairs in the cor-
pus were incorrectly annotated (e.g, long form in
the corpus embryo fibroblasts lacks word mouse to
form acronym MEFS); and 7 (4%) long forms are
missed by the algorithm, e.g., the algorithm recog-
nized pair protein kinase (PKR) while the correct
pair in the corpus is RNA-activated protein kinase
(PKR).
13Medstract corpus leaves unnecessary elements attached
to some long-forms such as general transcription factor iib
(TFIIB), whereas the proposed algorithm may drop the un-
necessary elements (i.e. general) based on the frequency. We
regard such cases as partly correct.
649
5 Conclusion
In this paper we described a term recognition ap-
proach to extract acronyms and their definitions
from a large text collection. The main contribution
of this study has been to show the usefulness of
statistical information for recognizing acronyms in
large text collections. The proposed method com-
bined with a letter matching algorithm achieved
78% precision and 85% recall on the evaluation
corpus with 4,212 acronym-definition pairs.
A future direction of this study would be to
incorporate other types of relations expressed
with parenthesis such as synonym, paraphrase,
etc. Although this study dealt with the acronym-
definition relation only, modelling these relations
will also contribute to the accuracy of the acronym
recognition, establishing a methodology to distin-
guish the acronym-definition relation from other
types of relations.
References
Eytan Adar. 2004. SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics, 20(4):527?
533.
Sophia Ananiadou and Goran Nenadic. 2006. Auto-
matic terminology management in biomedicine. In
Sophia Ananiadou and John McNaught, editors, Text
Mining for Biology and Biomedicine, pages 67?97.
Artech House, Inc.
Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): Integrating biomedical ter-
minology. Nucleic Acids Research, 32:267?270.
Jeffrey T. Chang and Hinrich Schu?tze. 2006. Abbre-
viations in biomedical text. In S. Ananiadou and
J. McNaught, editors, Text Mining for Biology and
Biomedicine, pages 99?119. Artech House, Inc.
Katerina T. Frantzi and Sophia Ananiadou. 1999. The
C-value / NC-value domain independent method for
multi-word term extraction. Journal of Natural Lan-
guage Processing, 6(3):145?179.
Carol Friedman, Hongfang Liu, Lyuda Shagina,
Stephen Johnson, and George Hripcsak. 2001.
Evaluating the UMLS as a source of lexical knowl-
edge for medical language processing. In AMIA
Symposium, pages 189?193.
Toru Hisamitsu and Yoshiki Niwa. 2001. Extract-
ing useful terms from parenthetical expression by
combining simple rules and statistical measures: A
comparative evaluation of bigram statistics. In Di-
dier Bourigault, Christian Jacquemin, and Marie-
C L?Homme, editors, Recent Advances in Compu-
tational Terminology, pages 209?224. John Ben-
jamins.
Hongfang Liu and Carol Friedman. 2003. Mining
terminological knowledge in large biomedical cor-
pora. In 8th Pacific Symposium on Biocomputing
(PSB 2003), pages 415?426.
David Nadeau and Peter D. Turney. 2005. A su-
pervised learning approach to acronym identifica-
tion. In 8th Canadian Conference on Artificial In-
telligence (AI?2005) (LNAI 3501), page 10 pages.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbrevia-
tion normalization in medical texts. In 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 160?167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 126?133.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130?137.
James Pustejovsky, Jose? Castan?o, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym meaning pairs from
MEDLINE databases. MEDINFO 2001, pages 371?
375.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In Pacific Symposium on Biocom-
puting (PSB 2003), number 8, pages 451?462.
Kazem Taghva and Jeff Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition (IJ-
DAR), 1(4):191?198.
Jonathan D. Wren and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated con-
struction of comprehensive acronym-definition dic-
tionaries. Methods of Information in Medicine,
41(5):426?434.
Jonathan D. Wren, Jeffrey T. Chang, James Puste-
jovsky, Eytan Adar, Harold R. Garner, and Russ B.
Altman. 2005. Biomedical term mapping
databases. Database Issue, 33:D289?D293.
Hong Yu, George Hripcsak, and Carol Friedman. 2002.
Mapping abbreviations to full forms in biomedical
articles. Journal of the American Medical Informat-
ics Association, 9(3):262?272.
650
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 19?27,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Comparative Study on Generalization of Semantic Roles in FrameNet
Yuichiroh Matsubayashi? Naoaki Okazaki? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Centre for Text Mining, UK
{y-matsu,okazaki,tsujii}@is.s.u-tokyo.ac.jp
Abstract
A number of studies have presented
machine-learning approaches to semantic
role labeling with availability of corpora
such as FrameNet and PropBank. These
corpora define the semantic roles of predi-
cates for each frame independently. Thus,
it is crucial for the machine-learning ap-
proach to generalize semantic roles across
different frames, and to increase the size
of training instances. This paper ex-
plores several criteria for generalizing se-
mantic roles in FrameNet: role hierar-
chy, human-understandable descriptors of
roles, semantic types of filler phrases, and
mappings from FrameNet roles to the-
matic roles of VerbNet. We also pro-
pose feature functions that naturally com-
bine and weight these criteria, based on
the training data. The experimental result
of the role classification shows 19.16%
and 7.42% improvements in error reduc-
tion rate and macro-averaged F1 score, re-
spectively. We also provide in-depth anal-
yses of the proposed criteria.
1 Introduction
Semantic Role Labeling (SRL) is a task of analyz-
ing predicate-argument structures in texts. More
specifically, SRL identifies predicates and their
arguments with appropriate semantic roles. Re-
solving surface divergence of texts (e.g., voice
of verbs and nominalizations) into unified seman-
tic representations, SRL has attracted much at-
tention from researchers into various NLP appli-
cations including question answering (Narayanan
and Harabagiu, 2004; Shen and Lapata, 2007;
buy.v PropBank FrameNet
Frame buy.01 Commerce buy
Roles ARG0: buyer Buyer
ARG1: thing bought Goods
ARG2: seller Seller
ARG3: paid Money
ARG4: benefactive Recipient
... ...
Figure 1: A comparison of frames for buy.v de-
fined in PropBank and FrameNet
Moschitti et al, 2007), and information extrac-
tion (Surdeanu et al, 2003).
In recent years, with the wide availability of cor-
pora such as PropBank (Palmer et al, 2005) and
FrameNet (Baker et al, 1998), a number of stud-
ies have presented statistical approaches to SRL
(Ma`rquez et al, 2008). Figure 1 shows an exam-
ple of the frame definitions for a verb buy in Prop-
Bank and FrameNet. These corpora define a large
number of frames and define the semantic roles for
each frame independently. This fact is problem-
atic in terms of the performance of the machine-
learning approach, because these definitions pro-
duce many roles that have few training instances.
PropBank defines a frame for each sense of
predicates (e.g., buy.01), and semantic roles are
defined in a frame-specific manner (e.g., buyer and
seller for buy.01). In addition, these roles are asso-
ciated with tags such as ARG0-5 and AM-*, which
are commonly used in different frames. Most
SRL studies on PropBank have used these tags
in order to gather a sufficient amount of training
data, and to generalize semantic-role classifiers
across different frames. However, Yi et al (2007)
reported that tags ARG2?ARG5 were inconsis-
tent and not that suitable as training instances.
Some recent studies have addressed alternative ap-
proaches to generalizing semantic roles across dif-
ferent frames (Gordon and Swanson, 2007; Zapi-
19
Transfer::RecipientGiving::Recipient Commerce_buy::BuyerCommerce_sell::Buyer Commerce_buy::SellerCommerce_sell::SellerGiving::Donor
Transfer::Donor
Buyer SellerAgent role-to-role relationhierarchical classthematic rolerole descriptor
Recipient Donor
Figure 2: An example of role groupings using different criteria.
rain et al, 2008).
FrameNet designs semantic roles as frame spe-
cific, but also defines hierarchical relations of se-
mantic roles among frames. Figure 2 illustrates
an excerpt of the role hierarchy in FrameNet; this
figure indicates that the Buyer role for the Com-
merce buy frame (Commerce buy::Buyer here-
after) and the Commerce sell::Buyer role are in-
herited from the Transfer::Recipient role. Al-
though the role hierarchy was expected to gener-
alize semantic roles, no positive results for role
classification have been reported (Baldewein et al,
2004). Therefore, the generalization of semantic
roles across different frames has been brought up
as a critical issue for FrameNet (Gildea and Juraf-
sky, 2002; Shi and Mihalcea, 2005; Giuglea and
Moschitti, 2006)
In this paper, we explore several criteria for gen-
eralizing semantic roles in FrameNet. In addi-
tion to the FrameNet hierarchy, we use various
pieces of information: human-understandable de-
scriptors of roles, semantic types of filler phrases,
and mappings from FrameNet roles to the thematic
roles of VerbNet. We also propose feature func-
tions that naturally combines these criteria in a
machine-learning framework. Using the proposed
method, the experimental result of the role classi-
fication shows 19.16% and 7.42% improvements
in error reduction rate and macro-averaged F1, re-
spectively. We provide in-depth analyses with re-
spect to these criteria, and state our conclusions.
2 Related Work
Moschitti et al (2005) first classified roles by us-
ing four coarse-grained classes (Core Roles, Ad-
juncts, Continuation Arguments and Co-referring
Arguments), and built a classifier for each coarse-
grained class to tag PropBank ARG tags. Even
though the initial classifiers could perform rough
estimations of semantic roles, this step was not
able to solve the ambiguity problem in PropBank
ARG2-5. When training a classifier for a seman-
tic role, Baldewein et al (2004) re-used the train-
ing instances of other roles that were similar to the
target role. As similarity measures, they used the
FrameNet hierarchy, peripheral roles of FrameNet,
and clusters constructed by a EM-based method.
Gordon and Swanson (2007) proposed a general-
ization method for the PropBank roles based on
syntactic similarity in frames.
Many previous studies assumed that thematic
roles bridged semantic roles in different frames.
Gildea and Jurafsky (2002) showed that classifica-
tion accuracy was improved by manually replac-
ing FrameNet roles into 18 thematic roles. Shi
and Mihalcea (2005) and Giuglea and Moschitti
(2006) employed VerbNet thematic roles as the
target of mappings from the roles defined by the
different semantic corpora. Using the thematic
roles as alternatives of ARG tags, Loper et al
(2007) and Yi et al (2007) demonstrated that the
classification accuracy of PropBank roles was im-
proved for ARG2 roles, but that it was diminished
for ARG1. Yi et al (2007) also described that
ARG2?5 were mapped to a variety of thematic
roles. Zapirain et al (2008) evaluated PropBank
ARG tags and VerbNet thematic roles in a state-of-
the-art SRL system, and concluded that PropBank
ARG tags achieved a more robust generalization of
the roles than did VerbNet thematic roles.
3 Role Classification
SRL is a complex task wherein several problems
are intertwined: frame-evoking word identifica-
tion, frame disambiguation (selecting a correct
frame from candidates for the evoking word), role-
phrase identification (identifying phrases that fill
semantic roles), and role classification (assigning
correct roles to the phrases). In this paper, we fo-
cus on role classification, in which the role gen-
eralization is particularly critical to the machine
learning approach.
In the role classification task, we are given a
sentence, a frame evoking word, a frame, and
20
member roles 
Commerce_pay::Buyer
Intentionall_act::Agent
Giving::Donor
Getting::Recipient
Giving::Recipient
Sending::Recipient
Giving::Time
Placing::Time
Event::Time
Commerce_pay::Buyer
Commerce_buy::Buyer
Commerce_sell::Buyer
Buyer
Recipient Time
C_pay::Buyer
GIVING::Donor
Intentionally_ACT::Agent
Avoiding::Agent
Evading::Evader
Evading::Evader
Avoiding::Agent
Getting::Recipient
Evading::Evader
St::Sentient St::Physical_Obj
Giving::Theme
Placing::Theme
St::State_of_affairs
Giving::Reason   Evading::Reason
Giving::Means    Evading::Purpose
Theme::Agent
Theme::Theme
Commerce_buy::Goods
Getting::Theme
Evading:: Pursuer
Commerce_buy::Buyer
Commerce_sell::Seller
Evading::Evader
Role-descriptor groupsHierarchical-relation groups Semantic-type groupsThematic-role groups
Group name
legend
Figure 4: Examples for each type of role group.
INPUT:frame = Commerce_sell
candidate   roles ={Seller, Buyer, Goods, Reason, Time, ... , Place}
sentence = Can't [you] [sell Commerce_sell] [the factory] [to some other company]? 
OUTPUT:  
sentence = Can't [you Seller] [sell Commerce_sell] [the factory Goods][to some other company Buyer] ?
Figure 3: An example of input and output of role
classification.
phrases that take semantic roles. We are inter-
ested in choosing the correct role from the can-
didate roles for each phrase in the frame. Figure 3
shows a concrete example of input and output; the
semantic roles for the phrases are chosen from the
candidate roles: Seller, Buyer, Goods, Reason,
... , and Place.
4 Design of Role Groups
We formalize the generalization of semantic roles
as the act of grouping several roles into a
class. We define a role group as a set of
role labels grouped by a criterion. Figure 4
shows examples of role groups; a group Giv-
ing::Donor (in the hierarchical-relation groups)
contains the roles Giving::Donor and Com-
merce pay::Buyer. The remainder of this section
describes the grouping criteria in detail.
4.1 Hierarchical relations among roles
FrameNet defines hierarchical relations among
frames (frame-to-frame relations). Each relation
is assigned one of the seven types of directional
relationships (Inheritance, Using, Perspective on,
Causative of, Inchoative of, Subframe, and Pre-
cedes). Some roles in two related frames are also
connected with role-to-role relations. We assume
that this hierarchy is a promising resource for gen-
eralizing the semantic roles; the idea is that the
role at a node in the hierarchy inherits the char-
acteristics of the roles of its ancestor nodes. For
example, Commerce sell::Seller in Figure 2 in-
herits the property of Giving::Donor.
For Inheritance, Using, Perspective on, and
Subframe relations, we assume that descendant
roles in these relations have the same or special-
ized properties of their ancestors. Hence, for each
role yi, we define the following two role groups,
Hchildyi = {y|y = yi ? y is a child of yi},
Hdescyi = {y|y = yi ? y is a descendant of yi}.
The hierarchical-relation groups in Figure 4 are
the illustrations of Hdescyi .
For the relation types Inchoative of and
Causative of, we define role groups in the oppo-
site direction of the hierarchy,
Hparentyi = {y|y = yi ? y is a parent of yi},
Hanceyi = {y|y = yi ? y is an ancestor of yi}.
This is because lower roles of Inchoative of
and Causative of relations represent more neu-
tral stances or consequential states; for example,
Killing::Victim is a parent of Death::Protagonist
in the Causative of relation.
Finally, the Precedes relation describes the se-
quence of states and events, but does not spec-
ify the direction of semantic inclusion relations.
Therefore, we simply try Hchildyi , H
desc
yi , H
parent
yi ,
and Hanceyi for this relation type.
4.2 Human-understandable role descriptor
FrameNet defines each role as frame-specific; in
other words, the same identifier does not appear
in different frames. However, in FrameNet,
human experts assign a human-understandable
name to each role in a rather systematic man-
ner. Some names are shared by the roles in
different frames, whose identifiers are dif-
ferent. Therefore, we examine the semantic
21
commonality of these names; we construct an
equivalence class of the roles sharing the same
name. We call these human-understandable
names role descriptors. In Figure 4, the role-
descriptor group Buyer collects the roles Com-
merce pay::Buyer, Commerce buy::Buyer,
and Commerce sell::Buyer.
This criterion may be effective in collecting
similar roles since the descriptors have been anno-
tated by intuition of human experts. As illustrated
in Figure 2, the role descriptors group the seman-
tic roles which are similar to the roles that the
FrameNet hierarchy connects as sister or parent-
child relations. However, role-descriptor groups
cannot express the relations between the roles
as inclusions since they are equivalence classes.
For example, the roles Commerce sell::Buyer
and Commerce buy::Buyer are included in the
role descriptor group Buyer in Figure 2; how-
ever, it is difficult to merge Giving::Recipient
and Commerce sell::Buyer because the Com-
merce sell::Buyer has the extra property that one
gives something of value in exchange and a hu-
man assigns different descriptors to them. We ex-
pect that the most effective weighting of these two
criteria will be determined from the training data.
4.3 Semantic type of phrases
We consider that the selectional restriction is help-
ful in detecting the semantic roles. FrameNet pro-
vides information concerning the semantic types
of role phrases (fillers); phrases that play spe-
cific roles in a sentence should fulfill the se-
mantic constraint from this information. For
instance, FrameNet specifies the constraint that
Self motion::Area should be filled by phrases
whose semantic type is Location. Since these
types suggest a coarse-grained categorization of
semantic roles, we construct role groups that con-
tain roles whose semantic types are identical.
4.4 Thematic roles of VerbNet
VerbNet thematic roles are 23 frame-independent
semantic categories for arguments of verbs,
such as Agent, Patient, Theme and Source.
These categories have been used as consis-
tent labels across verbs. We use a partial
mapping between FrameNet roles and Verb-
Net thematic roles provided by SemLink. 1
Each group is constructed as a set Tti =
1http://verbs.colorado.edu/semlink/
{y|SemLink maps y into the thematic role ti}.
SemLink currently maps 1,726 FrameNet roles
into VerbNet thematic roles, which are 37.61% of
roles appearing at least once in the FrameNet cor-
pus. This may diminish the effect of thematic-role
groups than its potential.
5 Role classification method
5.1 Traditional approach
We are given a frame-evoking word e, a frame f
and a role phrase x detected by a human or some
automatic process in a sentence s. Let Yf be the
set of semantic roles that FrameNet defines as be-
ing possible role assignments for the frame f , and
let x = {x1, . . . , xn} be observed features for x
from s, e and f . The task of semantic role classifi-
cation can be formalized as the problem of choos-
ing the most suitable role y? from Yf . Suppose we
have a model P (y|f,x) which yields the condi-
tional probability of the semantic role y for given
f and x. Then we can choose y? as follows:
y? = argmax
y?Yf
P (y|f,x). (1)
A traditional way to incorporate role groups
into this formalization is to overwrite each role
y in the training and test data with its role
group m(y) according to the memberships of
the group. For example, semantic roles Com-
merce sell::Seller and Giving::Donor can be re-
placed by their thematic-role group Theme::Agent
in this approach. We determine the most suitable
role group c? as follows:
c? = argmax
c?{m(y)|y?Yf}
Pm(c|f,x). (2)
Here, Pm(c|f,x) presents the probability of the
role group c for f and x. The role y? is determined
uniquely iff a single role y ? Yf is associated
with c?. Some previous studies have employed this
idea to remedy the data sparseness problem in the
training data (Gildea and Jurafsky, 2002). How-
ever, we cannot apply this approach when multi-
ple roles in Yf are contained in the same class. For
example, we can construct a semantic-type group
St::State of affairs in which Giving::Reason and
Giving::Means are included, as illustrated in Fig-
ure 4. If c? = St::State of affairs, we cannot dis-
ambiguate which original role is correct. In ad-
dition, it may be more effective to use various
22
groupings of roles together in the model. For in-
stance, the model could predict the correct role
Commerce sell::Seller for the phrase ?you? in
Figure 3 more confidently, if it could infer its
thematic-role group as Theme::Agent and its par-
ent group Giving::Donor correctly. Although the
ensemble of various groupings seems promising,
we need an additional procedure to prioritize the
groupings for the case where the models for mul-
tiple role groupings disagree; for example, it is un-
satisfactory if two models assign the groups Giv-
ing::Theme and Theme::Agent to the same phrase.
5.2 Role groups as feature functions
We thus propose another approach that incorpo-
rates group information as feature functions. We
model the conditional probability P (y|f,x) by us-
ing the maximum entropy framework,
p(y|f,x) = exp(
?
i ?igi(x, y))
?
y?Yf exp(
?
i ?igi(x, y))
. (3)
Here, G = {gi} denotes a set of n feature func-
tions, and ? = {?i} denotes a weight vector for
the feature functions.
In general, feature functions for the maximum
entropy model are designed as indicator functions
for possible pairs of xj and y. For example, the
event where the head word of x is ?you? (x1 = 1)
and x plays the role Commerce sell::Seller in a
sentence is expressed by the indicator function,
grole1 (x, y) =
?
?
?
?
?
1 (x1 = 1 ?
y = Commerce sell::Seller)
0 (otherwise)
.
(4)
We call this kind of feature function an x-role.
In order to incorporate role groups into the
model, we also include all feature functions for
possible pairs of xj and role groups. Equation 5
is an example of a feature function for instances
where the head word of x is ?you? and y is in the
role group Theme::Agent,
gtheme2 (x, y) =
?
?
?
?
?
1 (x1 = 1 ?
y ? Theme::Agent)
0 (otherwise)
. (5)
Thus, this feature function fires for the roles wher-
ever the head word ?you? plays Agent (e.g., Com-
merce sell::Seller, Commerce buy::Buyer and
Giving::Donor). We call this kind of feature func-
tion an x-group function.
In this way, we obtain x-group functions for
all grouping methods, e.g., gthemek , g
hierarchy
k .
The role-group features will receive more training
instances by collecting instances for fine-grained
roles. Thus, semantic roles with few training in-
stances are expected to receive additional clues
from other training instances via role-group fea-
tures. Another advantage of this approach is that
the usefulness of the different role groups is de-
termined by the training processes in terms of
weights of feature functions. Thus, we do not need
to assume that we have found the best criterion for
grouping roles; we can allow a training process to
choose the criterion. We will discuss the contribu-
tions of different groupings in the experiments.
5.3 Comparison with related work
Baldewein et al (2004) suggested an approach
that uses role descriptors and hierarchical rela-
tions as criteria for generalizing semantic roles
in FrameNet. They created a classifier for each
frame, additionally using training instances for the
role A to train the classifier for the role B, if the
roles A and B were judged as similar by a crite-
rion. This approach performs similarly to the over-
writing approach, and it may obscure the differ-
ences among roles. Therefore, they only re-used
the descriptors as a similarity measure for the roles
whose coreness was peripheral. 2
In contrast, we use all kinds of role descriptors
to construct groups. Since we use the feature func-
tions for both the original roles and their groups,
appropriate units for classification are determined
automatically in the training process.
6 Experiment and Discussion
We used the training set of the Semeval-2007
Shared task (Baker et al, 2007) in order to ascer-
tain the contributions of role groups. This dataset
consists of the corpus of FrameNet release 1.3
(containing roughly 150,000 annotations), and an
additional full-text annotation dataset. We ran-
domly extracted 10% of the dataset for testing, and
used the remainder (90%) for training.
Performance was measured by micro- and
macro-averaged F1 (Chang and Zheng, 2008) with
respect to a variety of roles. The micro average bi-
ases each F1 score by the frequencies of the roles,
2In FrameNet, each role is assigned one of four different
types of coreness (core, core-unexpressed, peripheral, extra-
thematic) It represents the conceptual necessity of the roles
in the frame to which it belongs.
23
and the average is equal to the classification accu-
racy when we calculate it with all of the roles in
the test set. In contrast, the macro average does
not bias the scores, thus the roles having a small
number of instances affect the average more than
the micro average.
6.1 Experimental settings
We constructed a baseline classifier that uses
only the x-role features. The feature de-
sign is similar to that of the previous stud-
ies (Ma`rquez et al, 2008). The characteristics
of x are: frame, frame evoking word, head
word, content word (Surdeanu et al, 2003),
first/last word, head word of left/right sister,
phrase type, position, voice, syntactic path (di-
rected/undirected/partial), governing category
(Gildea and Jurafsky, 2002), WordNet super-
sense in the phrase, combination features of
frame evoking word & headword, combination
features of frame evoking word & phrase type,
and combination features of voice & phrase type.
We also used PoS tags and stem forms as extra
features of any word-features.
We employed Charniak and Johnson?s rerank-
ing parser (Charniak and Johnson, 2005) to an-
alyze syntactic trees. As an alternative for the
traditional named-entity features, we used Word-
Net supersenses: 41 coarse-grained semantic cate-
gories of words such as person, plant, state, event,
time, location. We used Ciaramita and Altun?s Su-
per Sense Tagger (Ciaramita and Altun, 2006) to
tag the supersenses. The baseline system achieved
89.00% with respect to the micro-averaged F1.
The x-group features were instantiated similarly
to the x-role features; the x-group features com-
bined the characteristics of x with the role groups
presented in this paper. The total number of fea-
tures generated for all x-roles and x-groups was
74,873,602. The optimal weights ? of the fea-
tures were obtained by the maximum a poste-
rior (MAP) estimation. We maximized an L2-
regularized log-likelihood of the training set us-
ing the Limited-memory BFGS (L-BFGS) method
(Nocedal, 1980).
6.2 Effect of role groups
Table 1 shows the micro and macro averages of F1
scores. Each role group type improved the micro
average by 0.5 to 1.7 points. The best result was
obtained by using all types of groups together. The
result indicates that different kinds of group com-
Feature Micro Macro ?Err.
Baseline 89.00 68.50 0.00
role descriptor 90.78 76.58 16.17
role descriptor (replace) 90.23 76.19 11.23
hierarchical relation 90.25 72.41 11.40
semantic type 90.36 74.51 12.38
VN thematic role 89.50 69.21 4.52
All 91.10 75.92 19.16
Table 1: The accuracy and error reduction rate of
role classification for each type of role group.
Feature #instances Pre. Rec. Micro
baseline ? 10 63.89 38.00 47.66
? 20 69.01 51.26 58.83
? 50 75.84 65.85 70.50
+ all groups ? 10 72.57 55.85 63.12
? 20 76.30 65.41 70.43
? 50 80.86 74.59 77.60
Table 2: The effect of role groups on the roles with
few instances.
plement each other with respect to semantic role
generalization. Baldewein et al (2004) reported
that hierarchical relations did not perform well for
their method and experimental setting; however,
we found that significant improvements could also
be achieved with hierarchical relations. We also
tried a traditional label-replacing approach with
role descriptors (in the third row of Table 1). The
comparison between the second and third rows in-
dicates that mixing the original fine-grained roles
and the role groups does result in a more accurate
classification.
By using all types of groups together, the
model reduced 19.16 % of the classification errors
from the baseline. Moreover, the macro-averaged
F1 scores clearly showed improvements resulting
from using role groups. In order to determine
the reason for the improvements, we measured
the precision, recall, and F1-scores with respect
to roles for which the number of training instances
was at most 10, 20, and 50. In Table 2, we show
that the micro-averaged F1 score for roles hav-
ing 10 instances or less was improved (by 15.46
points) when all role groups were used. This result
suggests the reason for the effect of role groups; by
bridging similar semantic roles, they supply roles
having a small number of instances with the infor-
mation from other roles.
6.3 Analyses of role descriptors
In Table 1, the largest improvement was obtained
by the use of role descriptors. We analyze the ef-
fect of role descriptors in detail in Tables 3 and 4.
Table 3 shows the micro-averaged F1 scores of all
24
Coreness #roles #instances/#role #groups #instances/#group #roles/#group
Core 1902 122.06 655 354.4 2.9
Peripheral 1924 25.24 250 194.3 7.7
Extra-thematic 763 13.90 171 62.02 4.5
Table 4: The analysis of the numbers of roles, instances, and role-descriptor groups, for each type of
coreness.
Coreness Micro
Baseline 89.00
Core 89.51
Peripheral 90.12
Extra-thematic 89.09
All 90.77
Table 3: The effect of employing role-descriptor
groups of each type of coreness.
semantic roles when we use role-descriptor groups
constructed from each type of coreness (core3, pe-
ripheral, and extra-thematic) individually. The pe-
ripheral type generated the largest improvements.
Table 4 shows the number of roles associated
with each type of coreness (#roles), the number of
instances for the original roles (#instances/#role),
the number of groups for each type of coreness
(#groups), the number of instances for each group
(#instances/#group), and the number of roles per
each group (#roles/#group). In the peripheral
type, the role descriptors subdivided 1,924 distinct
roles into 250 groups, each of which contained 7.7
roles on average. The peripheral type included
semantic roles such as place, time, reason, dura-
tion. These semantic roles appear in many frames,
because they have general meanings that can be
shared by different frames. Moreover, the seman-
tic roles of peripheral type originally occurred in
only a small number (25.24) of training instances
on average. Thus, we infer that the peripheral
type generated the largest improvement because
semantic roles in this type acquired the greatest
benefit from the generalization.
6.4 Hierarchical relations and relation types
We analyzed the contributions of the FrameNet hi-
erarchy for each type of role-to-role relations and
for different depths of grouping. Table 5 shows
the micro-averaged F1 scores obtained from var-
ious relation types and depths. The Inheritance
and Using relations resulted in a slightly better ac-
curacy than the other types. We did not observe
any real differences among the remaining five re-
lation types, possibly because there were few se-
3We include Core-unexpressed in core, because it has a
property of core inside one frame.
No. Relation Type Micro
- baseline 89.00
1 + Inheritance (children) 89.52
2 + Inheritance (descendants) 89.70
3 + Using (children) 89.35
4 + Using (descendants) 89.37
5 + Perspective on (children) 89.01
6 + Perspective on (descendants) 89.01
7 + Subframe (children) 89.04
8 + Subframe (descendants) 89.05
9 + Causative of (parents) 89.03
10 + Causative of (ancestors) 89.03
11 + Inchoative of (parents) 89.02
12 + Inchoative of (ancestors) 89.02
13 + Precedes (children) 89.01
14 + Precedes (descendants) 89.03
15 + Precedes (parents) 89.00
16 + Precedes (ancestors) 89.00
18 + all relations (2,4,6,8,10,12,14) 90.25
Table 5: Comparison of the accuracy with differ-
ent types of hierarchical relations.
mantic roles associated with these types. We ob-
tained better results by using not only groups for
parent roles, but also groups for all ancestors. The
best result was obtained by using all relations in
the hierarchy.
6.5 Analyses of different grouping criteria
Table 6 reports the precision, recall, and micro-
averaged F1 scores of semantic roles with respect
to each coreness type.4 In general, semantic roles
of the core coreness were easily identified by all
of the grouping criteria; even the baseline system
obtained an F1 score of 91.93. For identifying se-
mantic roles of the peripheral and extra-thematic
types of coreness, the simplest solution, the de-
scriptor criterion, outperformed other criteria.
In Table 7, we categorize feature functions
whose weights are in the top 1000 in terms of
greatest absolute value. The behaviors of the role
groups can be distinguished by the following two
characteristics. Groups of role descriptors and se-
mantic types have large weight values for the first
word and supersense features, which capture the
characteristics of adjunctive phrases. The original
roles and hierarchical-relation groups have strong
4The figures of role descriptors in Tables 4 and 6 differ.
In Table 4, we measured the performance when we used one
or all types of coreness for training. In contrast, in Table 6,
we used all types of coreness for training, but computed the
performance of semantic roles for each coreness separately.
25
Feature Type Pre. Rec. Micro
baseline c 91.07 92.83 91.93
p 81.05 76.03 78.46
e 78.17 66.51 71.87
+ descriptor group c 92.50 93.41 92.95
p 84.32 82.72 83.51
e 80.91 69.59 74.82
+ hierarchical c 92.10 93.28 92.68
relation p 82.23 79.84 81.01
class e 77.94 65.58 71.23
+ semantic c 92.23 93.31 92.77
type group p 83.66 81.76 82.70
e 80.29 67.26 73.20
+ VN thematic c 91.57 93.06 92.31
role group p 80.66 76.95 78.76
e 78.12 66.60 71.90
+ all group c 92.66 93.61 93.13
p 84.13 82.51 83.31
e 80.77 68.56 74.17
Table 6: The precision and recall of each type of
coreness with role groups. Type represents the
type of coreness; c denotes core, p denotes periph-
eral, and e denotes extra-thematic.
associations with lexical and structural character-
istics such as the syntactic path, content word, and
head word. Table 7 suggests that role-descriptor
groups and semantic-type groups are effective for
peripheral or adjunctive roles, and hierarchical re-
lation groups are effective for core roles.
7 Conclusion
We have described different criteria for general-
izing semantic roles in FrameNet. They were:
role hierarchy, human-understandable descriptors
of roles, semantic types of filler phrases, and
mappings from FrameNet roles to thematic roles
of VerbNet. We also proposed a feature design
that combines and weights these criteria using the
training data. The experimental result of the role
classification task showed a 19.16% of the error
reduction and a 7.42% improvement in the macro-
averaged F1 score. In particular, the method we
have presented was able to classify roles having
few instances. We confirmed that modeling the
role generalization at feature level was better than
the conventional approach that replaces semantic
role labels.
Each criterion presented in this paper improved
the accuracy of classification. The most success-
ful criterion was the use of human-understandable
role descriptors. Unfortunately, the FrameNet hi-
erarchy did not outperform the role descriptors,
contrary to our expectations. A future direction
of this study would be to analyze the weakness of
the FrameNet hierarchy in order to discuss possi-
ble improvement of the usage and annotations of
features of x class type
or hr rl st vn
frame 0 4 0 1 0
evoking word 3 4 7 3 0
ew & hw stem 9 34 20 8 0
ew & phrase type 11 7 11 3 1
head word 13 19 8 3 1
hw stem 11 17 8 8 1
content word 7 19 12 3 0
cw stem 11 26 13 5 0
cw PoS 4 5 14 15 2
directed path 19 27 24 6 7
undirected path 21 35 17 2 6
partial path 15 18 16 13 5
last word 15 18 12 3 2
first word 11 23 53 26 10
supersense 7 7 35 25 4
position 4 6 30 9 5
others 27 29 33 19 6
total 188 298 313 152 50
Table 7: The analysis of the top 1000 feature func-
tions. Each number denotes the number of feature
functions categorized in the corresponding cell.
Notations for the columns are as follows. ?or?:
original role, ?hr?: hierarchical relation, ?rd?: role
descriptor, ?st?: semantic type, and ?vn?: VerbNet
thematic role.
the hierarchy.
Since we used the latest release of FrameNet
in order to use a greater number of hierarchical
role-to-role relations, we could not make a direct
comparison of performance with that of existing
systems; however we may say that the 89.00% F1
micro-average of our baseline system is roughly
comparable to the 88.93% value of Bejan and
Hathaway (2007) for SemEval-2007 (Baker et al,
2007). 5 In addition, the methodology presented in
this paper applies generally to any SRL resources;
we are planning to determine several grouping cri-
teria from existing linguistic resources and to ap-
ply the methodology to the PropBank corpus.
Acknowledgments
The authors thank Sebastian Riedel for his useful
comments on our work. This work was partially
supported by Grant-in-Aid for Specially Promoted
Research (MEXT, Japan).
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of Coling-ACL 1998, pages 86?90.
Collin Baker, Michael Ellsworth, and Katrin Erk.
2007. Semeval-2007 task 19: Frame semantic struc-
5There were two participants that performed whole SRL
in SemEval-2007. Bejan and Hathaway (2007) evaluated role
classification accuracy separately for the training data.
26
ture extraction. In Proceedings of SemEval-2007,
pages 99?104.
Ulrike Baldewein, Katrin Erk, Sebastian Pado?, and
Detlef Prescher. 2004. Semantic role labeling
with similarity based generalization using EM-based
clustering. In Proceedings of Senseval-3, pages 64?
68.
Cosmin Adrian Bejan and Chris Hathaway. 2007.
UTD-SRL: A Pipeline Architecture for Extract-
ing Frame Semantic Structures. In Proceedings
of SemEval-2007, pages 460?463. Association for
Computational Linguistics.
X. Chang and Q. Zheng. 2008. Knowledge Ele-
ment Extraction for Knowledge-Based Learning Re-
sources Organization. Lecture Notes in Computer
Science, 4823:102?113.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 173?180.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proceedings of EMNLP-2006, pages 594?602.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Semantic role labeling via FrameNet, VerbNet and
PropBank. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the ACL, pages 929?936.
Andrew Gordon and Reid Swanson. 2007. General-
izing semantic role annotations across syntactically
similar verbs. In Proceedings of ACL-2007, pages
192?199.
Edward Loper, Szu-ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between
propbank and verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Semantics,
pages 118?128.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: an introduction to the special
issue. Computational linguistics, 34(2):145?159.
Alessandro Moschitti, Ana-Maria Giuglea, Bonaven-
tura Coppola, and Roberto Basili. 2005. Hierar-
chical semantic role labeling. In Proceedings of
CoNLL-2005, pages 201?204.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of ACL-07,
pages 776?783.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In Pro-
ceedings of Coling-2004, pages 693?701.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of EMNLP-CoNLL 2007, pages 12?21.
Lei Shi and Rada Mihalcea. 2005. Putting Pieces To-
gether: Combining FrameNet, VerbNet and Word-
Net for Robust Semantic Parsing. In Proceedings of
CICLing-2005, pages 100?111.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of ACL-2003, pages 8?15.
Szu-ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In
Proceedings of HLT-NAACL 2007, pages 548?555.
Ben?at Zapirain, Eneko Agirre, and Llu??s Ma`rquez.
2008. Robustness and generalization of role sets:
PropBank vs. VerbNet. In Proceedings of ACL-08:
HLT, pages 550?558.
27
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 905?913,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Robust Approach to Abbreviating Terms:
A Discriminative Latent Variable Model with Global Information
Xu Sun?, Naoaki Okazaki?, Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo,
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan
?School of Computer Science, University of Manchester, UK
?National Centre for Text Mining, UK
{sunxu, okazaki, tsujii}@is.s.u-tokyo.ac.jp
Abstract
The present paper describes a robust ap-
proach for abbreviating terms. First, in
order to incorporate non-local informa-
tion into abbreviation generation tasks, we
present both implicit and explicit solu-
tions: the latent variable model, or alter-
natively, the label encoding approach with
global information. Although the two ap-
proaches compete with one another, we
demonstrate that these approaches are also
complementary. By combining these two
approaches, experiments revealed that the
proposed abbreviation generator achieved
the best results for both the Chinese and
English languages. Moreover, we directly
apply our generator to perform a very dif-
ferent task from tradition, the abbreviation
recognition. Experiments revealed that the
proposed model worked robustly, and out-
performed five out of six state-of-the-art
abbreviation recognizers.
1 Introduction
Abbreviations represent fully expanded forms
(e.g., hidden markov model) through the use of
shortened forms (e.g., HMM). At the same time,
abbreviations increase the ambiguity in a text.
For example, in computational linguistics, the
acronym HMM stands for hidden markov model,
whereas, in the field of biochemistry, HMM is gen-
erally an abbreviation for heavy meromyosin. As-
sociating abbreviations with their fully expanded
forms is of great importance in various NLP ap-
plications (Pakhomov, 2002; Yu et al, 2006;
HaCohen-Kerner et al, 2008).
The core technology for abbreviation disam-
biguation is to recognize the abbreviation defini-
tions in the actual text. Chang and Schu?tze (2006)
reported that 64,242 new abbreviations were intro-
duced into the biomedical literatures in 2004. As
such, it is important to maintain sense inventories
(lists of abbreviation definitions) that are updated
with the neologisms. In addition, based on the
one-sense-per-discourse assumption, the recogni-
tion of abbreviation definitions assumes senses of
abbreviations that are locally defined in a docu-
ment. Therefore, a number of studies have at-
tempted to model the generation processes of ab-
breviations: e.g., inferring the abbreviating mech-
anism of the hidden markov model into HMM.
An obvious approach is to manually design
rules for abbreviations. Early studies attempted
to determine the generic rules that humans use
to intuitively abbreviate given words (Barrett and
Grems, 1960; Bourne and Ford, 1961). Since
the late 1990s, researchers have presented var-
ious methods by which to extract abbreviation
definitions that appear in actual texts (Taghva
and Gilbreth, 1999; Park and Byrd, 2001; Wren
and Garner, 2002; Schwartz and Hearst, 2003;
Adar, 2004; Ao and Takagi, 2005). For example,
Schwartz and Hearst (2003) implemented a simple
algorithm that mapped all alpha-numerical letters
in an abbreviation to its expanded form, starting
from the end of both the abbreviation and its ex-
panded forms, and moving from right to left.
These studies performed highly, especially for
English abbreviations. However, a more extensive
investigation of abbreviations is needed in order to
further improve definition extraction. In addition,
we cannot simply transfer the knowledge of the
hand-crafted rules from one language to another.
For instance, in English, abbreviation characters
are preferably chosen from the initial and/or cap-
ital characters in their full forms, whereas some
905
p o l y g l y c o l i c a c i dP S S S P S S S S S S S S P S S S [PGA]
??? ? ???S P P S S S P [???]Institute of History and Philology at Academia Sinica(b): Chinese Abbreviation Generation
(a): English Abbreviation Generation    
Figure 1: English (a) and Chinese (b) abbreviation
generation as a sequential labeling problem.
other languages, including Chinese and Japanese,
do not have word boundaries or case sensitivity.
A number of recent studies have investigated
the use of machine learning techniques. Tsuruoka
et al (2005) formalized the processes of abbrevia-
tion generation as a sequence labeling problem. In
the present study, each character in the expanded
form is tagged with a label, y ? {P,S}1, where
the label P produces the current character and
the label S skips the current character. In Fig-
ure 1 (a), the abbreviation PGA is generated from
the full form polyglycolic acid because the under-
lined characters are tagged with P labels. In Fig-
ure 1 (b), the abbreviation is generated using the
2nd and 3rd characters, skipping the subsequent
three characters, and then using the 7th character.
In order to formalize this task as a sequential
labeling problem, we have assumed that the la-
bel of a character is determined by the local in-
formation of the character and its previous label.
However, this assumption is not ideal for model-
ing abbreviations. For example, the model can-
not make use of the number of words in a full
form to determine and generate a suitable num-
ber of letters for the abbreviation. In addition, the
model would be able to recognize the abbreviat-
ing process in Figure 1 (a) more reasonably if it
were able to segment the word polyglycolic into
smaller regions, e.g., poly-glycolic. Even though
humans may use global or non-local information
to abbreviate words, previous studies have not in-
corporated this information into a sequential label-
ing model.
In the present paper, we propose implicit and
explicit solutions for incorporating non-local in-
formation. The implicit solution is based on the
1Although the original paper of Tsuruoka et al (2005) at-
tached case sensitivity information to the P label, for simplic-
ity, we herein omit this information.
y1 y2 ym
xmx2x1
h1 h2 hm
xmx2x1
ymy2y1
CRF DPLVM
Figure 2: CRF vs. DPLVM. Variables x, y, and h
represent observation, label, and latent variables,
respectively.
discriminative probabilistic latent variable model
(DPLVM) in which non-local information is mod-
eled by latent variables. We manually encode non-
local information into the labels in order to provide
an explicit solution. We evaluate the models on the
task of abbreviation generation, in which a model
produces an abbreviation for a given full form. Ex-
perimental results indicate that the proposed mod-
els significantly outperform previous abbreviation
generation studies. In addition, we apply the pro-
posed models to the task of abbreviation recogni-
tion, in which a model extracts the abbreviation
definitions in a given text. To the extent of our
knowledge, this is the first model that can per-
form both abbreviation generation and recognition
at the state-of-the-art level, across different lan-
guages and with a simple feature set.
2 Abbreviator with Non-local
Information
2.1 A Latent Variable Abbreviator
To implicitly incorporate non-local information,
we propose discriminative probabilistic latent
variable models (DPLVMs) (Morency et al, 2007;
Petrov and Klein, 2008) for abbreviating terms.
The DPLVM is a natural extension of the CRF
model (see Figure 2), which is a special case of the
DPLVM, with only one latent variable assigned for
each label. The DPLVM uses latent variables to
capture additional information that may not be ex-
pressed by the observable labels. For example, us-
ing the DPLVM, a possible feature could be ?the
current character xi = X, the label yi = P, and
the latent variable hi = LV.? The non-local infor-
mation can be effectively modeled in the DPLVM,
and the additional information at the previous po-
sition or many of the other positions in the past
could be transferred via the latent variables (see
Figure 2).
906
Using the label set Y = {P,S}, abbreviation
generation is formalized as the task of assigning
a sequence of labels y = y1, y2, . . . , ym for a
given sequence of characters x = x1, x2, . . . , xm
in an expanded form. Each label, yj , is a mem-
ber of the possible labels Y . For each sequence,
we also assume a sequence of latent variables
h = h1, h2, . . . , hm, which are unobservable in
training examples.
We model the conditional probability of the la-
bel sequence P (y|x) using the DPLVM,
P (y|x,?) =
?
h
P (y|h,x,?)P (h|x,?). (1)
Here, ? represents the parameters of the model.
To ensure that the training and inference are ef-
ficient, the model is often restricted to have dis-
jointed sets of latent variables associated with each
label (Morency et al, 2007). Each hj is a member
in a set Hyj of possible latent variables for the la-
bel yj . Here, H is defined as the set of all possi-
ble latent variables, i.e., H is the union of all Hyj
sets. Since the sequences having hj /? Hyj will,
by definition, yield P (y|x,?) = 0, the model is
rewritten as follows (Morency et al, 2007; Petrov
and Klein, 2008):
P (y|x,?) =
?
h?Hy1?...?Hym
P (h|x,?). (2)
Here, P (h|x,?) is defined by the usual formula-
tion of the conditional random field,
P (h|x,?) = exp??f(h,x)?
?h exp??f(h,x)
, (3)
where f(h,x) represents a feature vector.
Given a training set consisting of n instances,
(xi,yi) (for i = 1 . . . n), we estimate the pa-
rameters ? by maximizing the regularized log-
likelihood,
L(?) =
n?
i=1
logP (yi|xi,?)?R(?). (4)
The first term expresses the conditional log-
likelihood of the training data, and the second term
represents a regularizer that reduces the overfitting
problem in parameter estimation.
2.2 Label Encoding with Global Information
Alternatively, we can design the labels such that
they explicitly incorporate non-local information.
? ? ? ? ? ? ? ? ? ? ? ? ? ?S S P S S S S S S P S P S SS0 S0 P1 S1 S1 S1 S1 S1 S1 P2 S2 P3 S3 S3
Management office of the imports and exports of endangered speciesOrig.GI
Figure 3: Comparison of the proposed label en-
coding method with global information (GI) and
the conventional label encoding method.
In this approach, the label yi at position i at-
taches the information of the abbreviation length
generated by its previous labels, y1, y2, . . . , yi?1.
Figure 3 shows an example of a Chinese abbre-
viation. In this encoding, a label not only con-
tains the produce or skip information, but also the
abbreviation-length information, i.e., the label in-
cludes the number of all P labels preceding the
current position. We refer to this method as label
encoding with global information (hereinafter GI).
The concept of using label encoding to incorporate
non-local information was originally proposed by
Peshkin and Pfeffer (2003).
Note that the model-complexity is increased
only by the increase in the number of labels. Since
the length of the abbreviations is usually quite
short (less than five for Chinese abbreviations and
less than 10 for English abbreviations), the model
is still tractable even when using the GI encoding.
The implicit (DPLVM) and explicit (GI) solu-
tions address the same issue concerning the in-
corporation of non-local information, and there
are advantages to combining these two solutions.
Therefore, we will combine the implicit and ex-
plicit solutions by employing the GI encoding in
the DPLVM (DPLVM+GI). The effects of this
combination will be demonstrated through experi-
ments.
2.3 Feature Design
Next, we design two types of features: language-
independent features and language-specific fea-
tures. Language-independent features can be used
for abbreviating terms in English and Chinese. We
use the features from #1 to #3 listed in Table 1.
Feature templates #4 to #7 in Table 1 are used
for Chinese abbreviations. Templates #4 and #5
express the Pinyin reading of the characters, which
represents a Romanization of the sound. Tem-
plates #6 and #7 are designed to detect character
duplication, because identical characters will nor-
mally be skipped in the abbreviation process. On
907
#1 The input char. xi?1 and xi
#2 Whether xj is a numeral, for j = (i? 3) . . . i
#3 The char. bigrams starting at (i? 2) . . . i
#4 The Pinyin of char. xi?1 and xi
#5 The Pinyin bigrams starting at (i? 2) . . . i
#6 Whether xj = xj+1, for j = (i? 2) . . . i
#7 Whether xj = xj+2, for j = (i? 3) . . . i
#8 Whether xj is uppercase, for j = (i? 3) . . . i
#9 Whether xj is lowercase, for j = (i? 3) . . . i
#10 The char. 3-grams starting at (i? 3) . . . i
#11 The char. 4-grams starting at (i? 4) . . . i
Table 1: Language-independent features (#1 to
#3), Chinese-specific features (#4 through #7), and
English-specific features (#8 through #11).
the other hand, such duplication detection features
are not so useful for English abbreviations.
Feature templates #8?#11 are designed for En-
glish abbreviations. Features #8 and #9 encode the
orthographic information of expanded forms. Fea-
tures #10 and #11 represent a contextual n-gram
with a large window size. Since the number of
letters in Chinese (more than 10K characters) is
much larger than the number of letters in English
(26 letters), in order to avoid a possible overfitting
problem, we did not apply these feature templates
to Chinese abbreviations.
Feature templates are instantiated with values
that occur in positive training examples. We used
all of the instantiated features because we found
that the low-frequency features also improved the
performance.
3 Experiments
For Chinese abbreviation generation, we used the
corpus of Sun et al (2008), which contains 2,914
abbreviation definitions for training, and 729 pairs
for testing. This corpus consists primarily of noun
phrases (38%), organization names (32%), and
verb phrases (21%). For English abbreviation gen-
eration, we evaluated the corpus of Tsuruoka et
al. (2005). This corpus contains 1,200 aligned
pairs extracted from MEDLINE biomedical ab-
stracts (published in 2001). For both tasks, we
converted the aligned pairs of the corpora into la-
beled full forms and used the labeled full forms as
the training/evaluation data.
The evaluation metrics used in the abbreviation
generation are exact-match accuracy (hereinafter
accuracy), including top-1 accuracy, top-2 accu-
racy, and top-3 accuracy. The top-N accuracy rep-
resents the percentage of correct abbreviations that
are covered, if we take the top N candidates from
the ranked labelings of an abbreviation generator.
We implemented the DPLVM in C++ and op-
timized the system to cope with large-scale prob-
lems. We employ the feature templates defined in
Section 2.3, taking into account these 81,827 fea-
tures for the Chinese abbreviation generation task,
and the 50,149 features for the English abbrevia-
tion generation task.
For numerical optimization, we performed a
gradient descent with the Limited-Memory BFGS
(L-BFGS) optimization technique (Nocedal and
Wright, 1999). L-BFGS is a second-order
Quasi-Newton method that numerically estimates
the curvature from previous gradients and up-
dates. With no requirement on specialized Hes-
sian approximation, L-BFGS can handle large-
scale problems efficiently. Since the objective
function of the DPLVM model is non-convex,
different parameter initializations normally bring
different optimization results. Therefore, to ap-
proach closer to the global optimal point, it is
recommended to perform multiple experiments on
DPLVMs with random initialization and then se-
lect a good start point. To reduce overfitting,
we employed a L2 Gaussian weight prior (Chen
and Rosenfeld, 1999), with the objective function:
L(?) = ?ni=1 logP (yi|xi,?)?||?||2/?2. Dur-
ing training and validation, we set ? = 1 for the
DPLVM generators. We also set four latent vari-
ables for each label, in order to make a compro-
mise between accuracy and efficiency.
Note that, for the label encoding with
global information, many label transitions (e.g.,
P2S3) are actually impossible: the label tran-
sitions are strictly constrained, i.e., yiyi+1 ?
{PjSj,PjPj+1,SjPj+1,SjSj}. These con-
straints on the model topology (forward-backward
lattice) are enforced by giving appropriate features
a weight of ??, thereby forcing all forbidden la-
belings to have zero probability. Sha and Pereira
(2003) originally proposed this concept of imple-
menting transition restrictions.
4 Results and Discussion
4.1 Chinese Abbreviation Generation
First, we present the results of the Chinese abbre-
viation generation task, as listed in Table 2. To
evaluate the impact of using latent variables, we
chose the baseline system as the DPLVM, in which
each label has only one latent variable. Since this
908
Model T1A T2A T3A Time
Heu (S08) 41.6 N/A N/A N/A
HMM (S08) 46.1 N/A N/A N/A
SVM (S08) 62.7 80.4 87.7 1.3 h
CRF 64.5 81.1 88.7 0.2 h
CRF+GI 66.8 82.5 90.0 0.5 h
DPLVM 67.6 83.8 91.3 0.4 h
DPLVM+GI (*) 72.3 87.6 94.9 1.1 h
Table 2: Results of Chinese abbreviation gener-
ation. T1A, T2A, and T3A represent top-1, top-
2, and top-3 accuracy, respectively. The system
marked with the * symbol is the recommended
system.
special case of the DPLVM is exactly the CRF
(see Section 2.1), this case is hereinafter denoted
as the CRF. We compared the performance of the
DPLVM with the CRFs and other baseline sys-
tems, including the heuristic system (Heu), the
HMM model, and the SVM model described in
S08, i.e., Sun et al (2008). The heuristic method
is a simple rule that produces the initial character
of each word to generate the corresponding abbre-
viation. The SVM method described by Sun et al
(2008) is formalized as a regression problem, in
which the abbreviation candidates are scored and
ranked.
The results revealed that the latent variable
model significantly improved the performance
over the CRF model. All of its top-1, top-2,
and top-3 accuracies were consistently better than
those of the CRF model. Therefore, this demon-
strated the effectiveness of using the latent vari-
ables in Chinese abbreviation generation.
As the case for the two alternative approaches
for incorporating non-local information, the la-
tent variable method and the label encoding
method competed with one another (see DPLVM
vs. CRF+GI). The results showed that the la-
tent variable method outperformed the GI encod-
ing method by +0.8% on the top-1 accuracy. The
reason for this could be that the label encoding ap-
proach is a solution without the adaptivity on dif-
ferent instances. We will present a detailed discus-
sion comparing DPLVM and CRF+GI for the En-
glish abbreviation generation task in the next sub-
section, where the difference is more significant.
In contrast, to a larger extent, the results demon-
strate that these two alternative approaches are
complementary. Using the GI encoding further
improved the performance of the DPLVM (with
+4.7% on top-1 accuracy). We found that major
? ? ? ? ? ? ?P S P S P S PP1 S1 P2 S2 S2 S2 P3
State Tobacco Monopoly Administration DPLVM DPLVM+GI ???? [Wrong]??? [Correct]
Figure 4: An example of the results.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  1  2  3  4  5  6
P
er
ce
nt
ag
e 
(%
)
Length of Produced Abbr.
Gold Train
Gold Test
DPLVM
DPLVM+GI
Figure 5: Percentage distribution of Chinese
abbreviations/Viterbi-labelings grouped by length.
improvements were achieved through the more ex-
act control of the output length. An example is
shown in Figure 4. The DPLVM made correct de-
cisions at three positions, but failed to control the
abbreviation length.2 The DPLVM+GI succeeded
on this example. To perform a detailed analysis,
we collected the statistics of the length distribution
(see Figure 5) and determined that the GI encod-
ing improved the abbreviation length distribution
of the DPLVM.
In general, the results indicate that all of the se-
quential labeling models outperformed the SVM
regression model with less training time.3 In the
SVM regression approach, a large number of neg-
ative examples are explicitly generated for the
training, which slowed the process.
The proposed method, the latent variable model
with GI encoding, is 9.6% better with respect to
the top-1 accuracy compared to the best system on
this corpus, namely, the SVM regression method.
Furthermore, the top-3 accuracy of the latent vari-
able model with GI encoding is as high as 94.9%,
which is quite encouraging for practical usage.
4.2 English Abbreviation Generation
In the English abbreviation generation task, we
randomly selected 1,481 instances from the gen-
2The Chinese abbreviation with length = 4 should have
a very low probability, e.g., only 0.6% of abbreviations with
length = 4 in this corpus.
3On Intel Dual-Core Xeon 5160/3 GHz CPU, excluding
the time for feature generation and data input/output.
909
Model T1A T2A T3A Time
CRF 55.8 65.1 70.8 0.3 h
CRF+GI 52.7 63.2 68.7 1.3 h
CRF+GIB 56.8 66.1 71.7 1.3 h
DPLVM 57.6 67.4 73.4 0.6 h
DPLVM+GI 53.6 63.2 69.2 2.5 h
DPLVM+GIB (*) 58.3 N/A N/A 3.0 h
Table 3: Results of English abbreviation genera-
tion.
somatosensory evoked potentials
(a) P1P2 P3 P4 P5 SMEPS
(b) P P P P SEPS
(a): CRF+GI with p=0.001 [Wrong]
(b): DPLVM with p=0.191 [Correct]
Figure 6: A result of ?CRF+GI vs. DPLVM?. For
simplicity, the S labels are masked.
eration corpus for training, and 370 instances for
testing. Table 3 shows the experimental results.
We compared the performance of the DPLVM
with the performance of the CRFs. Whereas the
use of the latent variables still significantly im-
proves the generation performance, using the GI
encoding undermined the performance in this task.
In comparing the implicit and explicit solutions
for incorporating non-local information, we can
see that the implicit approach (the DPLVM) per-
forms much better than the explicit approach (the
GI encoding). An example is shown in Figure 6.
The CRF+GI produced a Viterbi labeling with a
low probability, which is an incorrect abbrevia-
tion. The DPLVM produced the correct labeling.
To perform a systematic analysis of the
superior-performance of DPLVM compare to
CRF+GI, we collected the probability distribu-
tions (see Figure 7) of the Viterbi labelings from
these models (?DPLVM vs. CRF+GI? is high-
lighted). The curves suggest that the data sparse-
ness problem could be the reason for the differ-
ences in performance. A large percentage (37.9%)
of the Viterbi labelings from the CRF+GI (ENG)
have very small probability values (p < 0.1).
For the DPLVM (ENG), there were only a few
(0.5%) Viterbi labelings with small probabilities.
Since English abbreviations are often longer than
Chinese abbreviations (length < 10 in English,
whereas length < 5 in Chinese4), using the GI
encoding resulted in a larger label set in English.
4See the curve DPLVM+GI (CHN) in Figure 7, which
could explain the good results of GI encoding for the Chi-
nese task.
 0
 10
 20
 30
 40
 50
 0  0.2  0.4  0.6  0.8  1
P
er
ce
nt
ag
e 
(%
)
Probability of Viterbi labeling
CRF (ENG)
CRF+GI (ENG)
DPLVM (ENG)
DPLVM+GI (ENG)
DPLVM+GI (CHN)
Figure 7: For various models, the probability dis-
tributions of the produced abbreviations on the test
data of the English abbreviation generation task.
mitomycin C
DPLVM P P MC [Wrong]
DPLVM+GI P1 P2 P3 MMC [Correct]
Figure 8: Example of abbreviations composed
of non-initials generated by the DPLVM and the
DPLVM+GI.
Hence, the features become more sparse than in
the Chinese case.5 Therefore, a significant number
of features could have been inadequately trained,
resulting in Viterbi labelings with low probabili-
ties. For the latent variable approach, its curve
demonstrates that it did not cause a severe data
sparseness problem.
The aforementioned analysis also explains the
poor performance of the DPLVM+GI. However,
the DPLVM+GI can actually produce correct ab-
breviations with ?believable? probabilities (high
probabilities) in some ?difficult? instances. In
Figure 8, the DPLVM produced an incorrect la-
beling for the difficult long form, whereas the
DPLVM+GI produced the correct labeling con-
taining non-initials.
Hence, we present a simple voting method to
better combine the latent variable approach with
the GI encoding method. We refer to this new
combination as GI encoding with ?back-off? (here-
inafter GIB): when the abbreviation generated by
the DPLVM+GI has a ?believable? probability
(p > 0.3 in the present case), the DPLVM+GI
then outputs it. Otherwise, the system ?backs-off?
5In addition, the training data of the English task is much
smaller than for the Chinese task, which could make the mod-
els more sensitive to data sparseness.
910
Model T1A Time
CRF+GIB 67.2 0.6 h
DPLVM+GIB (*) 72.5 1.4 h
Table 4: Re-evaluating Chinese abbreviation gen-
eration with GIB.
Model T1A
Heu (T05) 47.3
MEMM (T05) 55.2
DPLVM (*) 57.5
Table 5: Results of English abbreviation genera-
tion with five-fold cross validation.
to the parameters trained without the GI encoding
(i.e., the DPLVM).
The results in Table 3 demonstrate that the
DPVLM+GIB model significantly outperformed
the other models because the DPLVM+GI model
improved the performance in some ?difficult? in-
stances. The DPVLM+GIB model was robust
even when the data sparseness problem was se-
vere.
By re-evaluating the DPLVM+GIB model for
the previous Chinese abbreviation generation task,
we demonstrate that the back-off method also im-
proved the performance of the Chinese abbrevia-
tion generators (+0.2% from DPLVM+GI; see Ta-
ble 4).
Furthermore, for interests, like Tsuruoka et al
(2005), we performed a five-fold cross-validation
on the corpus. Concerning the training time in
the cross validation, we simply chose the DPLVM
for comparison. Table 5 shows the results of the
DPLVM, the heuristic system (Heu), and the max-
imum entropy Markov model (MEMM) described
by Tsuruoka et al (2005).
5 Recognition as a Generation Task
We directly migrate this model to the abbrevia-
tion recognition task. We simplify the abbrevia-
tion recognition to a restricted generation problem
(see Figure 9). When a context expression (CE)
with a parenthetical expression (PE) is met, the
recognizer generates the Viterbi labeling for the
CE, which leads to the PE or NULL. Then, if the
Viterbi labeling leads to the PE, we can, at the
same time, use the labeling to decide the full form
within the CE. Otherwise, NULL indicates that the
PE is not an abbreviation.
For example, in Figure 9, the recognition is re-
stricted to a generation task with five possible la-
... cannulate for arterial pressure (AP)...
(1) P P AP
(2) P P AP
(3) P P AP
(4) P P AP
(5) SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS NULL
Figure 9: Abbreviation recognition as a restricted
generation problem. In some labelings, the S la-
bels are masked for simplicity.
Model P R F
Schwartz & Hearst (SH) 97.8 94.0 95.9
SaRAD 89.1 91.9 90.5
ALICE 96.1 92.0 94.0
Chang & Schu?tze (CS) 94.2 90.0 92.1
Nadeau & Turney (NT) 95.4 87.1 91.0
Okazaki et al (OZ) 97.3 96.9 97.1
CRF 89.8 94.8 92.1
CRF+GI 93.9 97.8 95.9
DPLVM 92.5 97.7 95.1
DPLVM+GI (*) 94.2 98.1 96.1
Table 6: Results of English abbreviation recogni-
tion.
belings. Other labelings are impossible, because
they will generate an abbreviation that is not AP.
If the first or second labeling is generated, AP is
selected as an abbreviation of arterial pressure. If
the third or fourth labeling is generated, then AP
is selected as an abbreviation of cannulate for ar-
terial pressure. Finally, the fifth labeling (NULL)
indicates that AP is not an abbreviation.
To evaluate the recognizer, we use the corpus6
of Okazaki et al (2008), which contains 864 ab-
breviation definitions collected from 1,000 MED-
LINE scientific abstracts. In implementing the
recognizer, we simply use the model from the ab-
breviation generator, with the same feature tem-
plates (31,868 features) and training method; the
major difference is in the restriction (according to
the PE) of the decoding stage and penalizing the
probability values of the NULL labelings7.
For the evaluation metrics, following Okazaki
et al (2008), we use precision (P = k/m), re-
call (R = k/n), and the F-score defined by
6The previous abbreviation generation corpus is improper
for evaluating recognizers, and there is no related research on
this corpus. In addition, there has been no report of Chinese
abbreviation recognition because there is no data available.
The previous generation corpus (Sun et al, 2008) is improper
because it lacks local contexts.
7Due to the data imbalance of the training corpus, we
found the probability values of the NULL labelings are ab-
normally high. To deal with this imbalance problem, we sim-
ply penalize all NULL labelings by using p = p? 0.7.
911
Model P R F
CRF+GIB 94.0 98.9 96.4
DPLVM+GIB 94.5 99.1 96.7
Table 7: English abbreviation recognition with
back-off.
2PR/(P + R), where k represents #instances in
which the system extracts correct full forms, m
represents #instances in which the system extracts
the full forms regardless of correctness, and n rep-
resents #instances that have annotated full forms.
Following Okazaki et al (2008), we perform 10-
fold cross validation.
We prepared six state-of-the-art abbreviation
recognizers as baselines: Schwartz and Hearst?s
method (SH) (2003), SaRAD (Adar, 2004), AL-
ICE (Ao and Takagi, 2005), Chang and Schu?tze?s
method (CS) (Chang and Schu?tze, 2006), Nadeau
and Turney?s method (NT) (Nadeau and Turney,
2005), and Okazaki et al?s method (OZ) (Okazaki
et al, 2008). Some methods use implementations
on the web, including SH8, CS9, and ALICE10.
The results of other methods, such as SaRAD, NT,
and OZ, are reproduced for this corpus based on
their papers (Okazaki et al, 2008).
As can be seen in Table 6, using the latent vari-
ables significantly improved the performance (see
DPLVM vs. CRF), and using the GI encoding
improved the performance of both the DPLVM
and the CRF. With the F-score of 96.1%, the
DPLVM+GI model outperformed five of six state-
of-the-art abbreviation recognizers. Note that all
of the six systems were specifically designed and
optimized for this recognition task, whereas the
proposed model is directly transported from the
generation task. Compared with the generation
task, we find that the F-measure of the abbrevia-
tion recognition task is much higher. The major
reason for this is that there are far fewer classifi-
cation candidates of the abbreviation recognition
problem, as compared to the generation problem.
For interests, we also tested the effect of the
GIB approach. Table 7 shows that the back-off
method further improved the performance of both
the DPLVM and the CRF model.
8http://biotext.berkeley.edu/software.html
9http://abbreviation.stanford.edu/
10http://uvdb3.hgc.jp/ALICE/ALICE index.html
6 Conclusions and Future Research
We have presented the DPLVM and GI encod-
ing by which to incorporate non-local information
in abbreviating terms. They were competing and
generally the performance of the DPLVM was su-
perior. On the other hand, we showed that the two
approaches were complementary. By combining
these approaches, we were able to achieve state-
of-the-art performance in abbreviation generation
and recognition in the same model, across differ-
ent languages, and with a simple feature set. As
discussed earlier herein, the training data is rela-
tively small. Since there are numerous unlabeled
full forms on the web, it is possible to use a semi-
supervised approach in order to make use of such
raw data. This is an area for future research.
Acknowledgments
We thank Yoshimasa Tsuruoka for providing the
English abbreviation generation corpus. We also
thank the anonymous reviewers who gave help-
ful comments. This work was partially supported
by Grant-in-Aid for Specially Promoted Research
(MEXT, Japan).
References
Eytan Adar. 2004. SaRAD: A simple and robust ab-
breviation dictionary. Bioinformatics, 20(4):527?
533.
Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
June A. Barrett and Mandalay Grems. 1960. Abbrevi-
ating words systematically. Communications of the
ACM, 3(5):323?324.
Charles P. Bourne and Donald F. Ford. 1961. A study
of methods for systematically abbreviating english
words and names. Journal of the ACM, 8(4):538?
552.
Jeffrey T. Chang and Hinrich Schu?tze. 2006. Abbre-
viations in biomedical text. In Sophia Ananiadou
and John McNaught, editors, Text Mining for Biol-
ogy and Biomedicine, pages 99?119. Artech House,
Inc.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, CMU.
Yaakov HaCohen-Kerner, Ariel Kass, and Ariel Peretz.
2008. Combined one sense disambiguation of ab-
breviations. In Proceedings of ACL?08: HLT, Short
Papers, pages 61?64, June.
912
Louis-Philippe Morency, Ariadna Quattoni, and Trevor
Darrell. 2007. Latent-dynamic discriminative mod-
els for continuous gesture recognition. Proceedings
of CVPR?07, pages 1?8.
David Nadeau and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
In the 8th Canadian Conference on Artificial Intelli-
gence (AI?2005) (LNAI 3501), page 10 pages.
Jorge Nocedal and Stephen J. Wright. 1999. Numeri-
cal optimization. Springer.
Naoaki Okazaki, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2008. A discriminative alignment model for
abbreviation recognition. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING?08), pages 657?664, Manch-
ester, UK.
Serguei Pakhomov. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
ACL?02, pages 160?167.
Youngja Park and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of EMNLP?01, pages 126?133.
Leonid Peshkin and Avi Pfeffer. 2003. Bayesian in-
formation extraction network. In Proceedings of IJ-
CAI?03, pages 421?426.
Slav Petrov and Dan Klein. 2008. Discriminative log-
linear grammars with latent variables. Proceedings
of NIPS?08.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In the 8th Pacific Symposium on
Biocomputing (PSB?03), pages 451?462.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. Proceedings of
HLT/NAACL?03.
Xu Sun, Houfeng Wang, and Bo Wang. 2008. Pre-
dicting chinese abbreviations from definitions: An
empirical learning approach using support vector re-
gression. Journal of Computer Science and Tech-
nology, 23(4):602?611.
Kazem Taghva and Jeff Gilbreth. 1999. Recogniz-
ing acronyms and their definitions. International
Journal on Document Analysis and Recognition (IJ-
DAR), 1(4):191?198.
Yoshimasa Tsuruoka, Sophia Ananiadou, and Jun?ichi
Tsujii. 2005. A machine learning approach to
acronym generation. In Proceedings of the ACL-
ISMB Workshop, pages 25?31.
Jonathan D. Wren and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated con-
struction of comprehensive acronym-definition dic-
tionaries. Methods of Information in Medicine,
41(5):426?434.
Hong Yu, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based ap-
proach for automatically disambiguating biomedical
abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380?404.
913
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1021?1029,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Relation Extraction by Mining Wikipedia Texts Using
Information from the Web
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang and Mitsuru Ishizuka
The University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
yulan@mi.ci.i.u-tokyo.ac.jp
okazaki@is.s.u-tokyo.ac.jp
matsuo@biz-model.t.utokyo.ac.jp
yangzl@tkl.iis.u-tokyo.ac.jp
ishizuka@i.u-tokyo.ac.jp
Abstract
This paper presents an unsupervised rela-
tion extraction method for discovering and
enhancing relations in which a specified
concept in Wikipedia participates. Using
respective characteristics of Wikipedia ar-
ticles and Web corpus, we develop a clus-
tering approach based on combinations of
patterns: dependency patterns from depen-
dency analysis of texts in Wikipedia, and
surface patterns generated from highly re-
dundant information related to the Web.
Evaluations of the proposed approach on
two different domains demonstrate the su-
periority of the pattern combination over
existing approaches. Fundamentally, our
method demonstrates how deep linguistic
patterns contribute complementarily with
Web surface patterns to the generation of
various relations.
1 Introduction
Machine learning approaches for relation extrac-
tion tasks require substantial human effort, partic-
ularly when applied to the broad range of docu-
ments, entities, and relations existing on the Web.
Even with semi-supervised approaches, which use
a large unlabeled corpus, manual construction of a
small set of seeds known as true instances of the
target entity or relation is susceptible to arbitrary
human decisions. Consequently, a need exists for
development of semantic information-retrieval al-
gorithms that can operate in a manner that is as
unsupervised as possible.
Currently, the leading methods in unsupervised
information extraction collect redundancy infor-
mation from a local corpus or use the Web as a
corpus (Pantel and Pennacchiotti, 2006); (Banko
et al, 2007); (Bollegala et al, 2007): (Fan et
al., 2008); (Davidov and Rappoport, 2008). The
standard process is to scan or search the cor-
pus to collect co-occurrences of word pairs with
strings between them, and then to calculate term
co-occurrence or generate surface patterns. The
method is used widely. However, even when pat-
terns are generated from well-written texts, fre-
quent pattern mining is non-trivial because the
number of unique patterns is loose, but many pat-
terns are non-discriminative and correlated. A
salient challenge and research interest for frequent
pattern mining is abstraction away from different
surface realizations of semantic relations to dis-
cover discriminative patterns efficiently.
Linguistic analysis is another effective tech-
nology for semantic relation extraction, as de-
scribed in many reports such as (Kambhatla,
2004); (Bunescu and Mooney, 2005); (Harabagiu
et al, 2005); (Nguyen et al, 2007). Currently, lin-
guistic approaches for semantic relation extraction
are mostly supervised, relying on pre-specification
of the desired relation or initial seed words or pat-
terns from hand-coding. The common process is
to generate linguistic features based on analyses of
the syntactic features, dependency, or shallow se-
mantic structure of text. Then the system is trained
to identify entity pairs that assume a relation and
to classify them into pre-defined relations. The ad-
vantage of these methods is that they use linguistic
technologies to learn semantic information from
different surface expressions.
As described herein, we consider integrating
linguistic analysis with Web frequency informa-
tion to improve the performance of unsupervised
relation extraction. As (Banko et al, 2007)
reported, ?deep? linguistic technology presents
problems when applied to heterogeneous text on
the Web. Therefore, we do not parse informa-
tion from the Web corpus, but from well written
texts. Particularly, we specifically examine unsu-
pervised relation extraction from existing texts of
Wikipedia articles. Wikipedia resources of a fun-
1021
damental type are of concepts (e.g., represented
by Wikipedia articles as a special case) and their
mutual relations. We propose our method, which
groups concept pairs into several clusters based on
the similarity of their contexts. Contexts are col-
lected as patterns of two kinds: dependency pat-
terns from dependency analysis of sentences in
Wikipedia, and surface patterns generated from
highly redundant information from the Web.
The main contributions of this paper are as fol-
lows:
? Using characteristics of Wikipedia articles
and the Web corpus respectively, our study
yields an example of bridging the gap sep-
arating ?deep? linguistic technology and re-
dundant Web information for Information
Extraction tasks.
? Our experimental results reveal that relations
are extractable with good precision using
linguistic patterns, whereas surface patterns
from Web frequency information contribute
greatly to the coverage of relation extraction.
? The combination of these patterns produces
a clustering method to achieve high pre-
cision for different Information Extraction
applications, especially for bootstrapping a
high-recall semi-supervised relation extrac-
tion system.
2 Related Work
(Hasegawa et al, 2004) introduced a method for
discovering a relation by clustering pairs of co-
occurring entities represented as vectors of con-
text features. They used a simple representation
of contexts; the features were words in sentences
between the entities of the candidate pairs.
(Turney, 2006) presented an unsupervised algo-
rithm for mining the Web for patterns expressing
implicit semantic relations. Given a word pair, the
output list of lexicon-syntactic patterns was ranked
by pertinence, which showed how well each pat-
tern expresses the relations between word pairs.
(Davidov et al, 2007) proposed a method for
unsupervised discovery of concept specific rela-
tions, requiring initial word seeds. That method
used pattern clusters to define general relations,
specific to a given concept. (Davidov and Rap-
poport, 2008) presented an approach to discover
and represent general relations present in an arbi-
trary corpus. That approach incorporated a fully
unsupervised algorithm for pattern cluster discov-
ery, which searches, clusters, and merges high-
frequency patterns around randomly selected con-
cepts.
The field of Unsupervised Relation Identifica-
tion (URI)?the task of automatically discover-
ing interesting relations between entities in large
text corpora?was introduced by (Hasegawa et
al., 2004). Relations are discovered by cluster-
ing pairs of co-occurring entities represented as
vectors of context features. (Rosenfeld and Feld-
man, 2006) showed that the clusters discovered by
URI are useful for seeding a semi-supervised rela-
tion extraction system. To compare different clus-
tering algorithms, feature extraction and selection
method, (Rosenfeld and Feldman, 2007) presented
a URI system that used surface patterns of two
kinds: patterns that test two entities together and
patterns that test either of two entities.
In this paper, we propose an unsupervised rela-
tion extraction method that combines patterns of
two types: surface patterns and dependency pat-
terns. Surface patterns are generated from the Web
corpus to provide redundancy information for re-
lation extraction. In addition, to obtain seman-
tic information for concept pairs, we generate de-
pendency patterns to abstract away from different
surface realizations of semantic relations. Depen-
dency patterns are expected to be more accurate
and less spam-prone than surface patterns from the
Web corpus. Surface patterns from redundancy
Web information are expected to address the data
sparseness problem. Wikipedia is currently widely
used information extraction as a local corpus; the
Web is used as a global corpus.
3 Characteristics of Wikipedia articles
Wikipedia, unlike the whole Web corpus, has
several characteristics that markedly facilitate in-
formation extraction. First, as an earlier report
(Giles, 2005) explained, Wikipedia articles are
much cleaner than typical Web pages. Because
the quality is not so different from standard writ-
ten English, we can use ?deep? linguistic tech-
nologies, such as syntactic or dependency parsing.
Secondly, Wikipedia articles are heavily cross-
linked, in a manner resembling cross-linking of
the Web pages. (Gabrilovich and Markovitch,
2006) assumed that these links encode numerous
interesting relations among concepts, and that they
provide an important source of information in ad-
1022
dition to the article texts.
To establish the background for this paper, we
start by defining the problem under consideration:
relation extraction from Wikipedia. We use the en-
cyclopedic nature of the corpus by specifically ex-
amining the relation extraction between the enti-
tled concept (ec) and a related concept (rc), which
are described in anchor text in this article. A com-
mon assumption is that, when investigating the se-
mantics in articles such as those in Wikipedia (e.g.
semantic Wikipedia (Volkel et al, 2006)), key in-
formation related to a concept described on a page
p lies within the set of links l(p) on that page; par-
ticularly, it is likely that a salient semantic relation
r exists between p and a related page p? ? l(p).
Given the scenario we described along with
earlier related works, the challenges we face are
these: 1) enumerating all potential relation types
of interest for extraction is highly problematic for
corpora as large and varied as Wikipedia; 2) train-
ing data or seed data are difficult to label. Consid-
ering (Davidov and Rappoport, 2008), which de-
scribes work to get the target word and relation
cluster given a single (?hook?) word, their method
depends mainly on frequency information from
the Web to obtain a target and clusters. Attempt-
ing to improve the performance, our solution for
these challenges is to combine frequency informa-
tion from the Web and the ?high quality? charac-
teristic of Wikipedia text.
4 Pattern Combination Method for
Relation Extraction
With the scene and challenges stated, we propose a
solution in the following way. The intuitive idea is
that we integrate linguistic technologies on high-
quality text in Wikipedia and Web mining tech-
nologies on a large-scale Web corpus. In this sec-
tion, we first provide an overview of our method
along with the function of the main modules. Sub-
sequently, we explain each module in the method
in detail.
4.1 Overview of the Method
Given a set of Wikipedia articles as input, our
method outputs a list of concept pairs for each ar-
ticle with a relation label assigned to each concept
pair. Briefly, the proposed approach has four main
modules, as depicted in Fig. 1.
? Text Preprocessor and Concept Pair Col-
lector preprocesses Wikipedia articles to
Wikipedia articles
Preprocessor
Concept pair collection
Sentence filtering
Web context collector
Web Context
T
i
= t1, t2?tn
P
i
=  p1,p2?pn
Dependency 
pattern Extractor
n1i,?n1j
?
ni2i, ..n2j
ni,?nj
?
surface clustering
depend clustering
Relation list
Output: 
relations for each article
input:
Eric Emerson Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
Eric Emers  Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
Eric Emers  Schmidt
CEO
a-member-of
Born
Google
Board of Directors
Washington, D.C.
Is-a chairman
Novell
...
...
?
?
?
?
...
...
?
?
?
?
...
...
?
?
?
?
Tyco becoming
joined
comp:
CEO
obj: cc:
joined
obj:subj:
joined
obj: cc:
Clustering approach
Figure 1: Framework of the proposed approach
split text and filter sentences. It outputs con-
cept pairs, each of which has an accompany-
ing sentence.
? Web Context Collector collects context in-
formation from the Web and generates ranked
relational terms and surface patterns for each
concept pair.
? Dependency Pattern Extractor generates
dependency patterns for each concept pair
from corresponding sentences in Wikipedia
articles.
? Clustering Algorithm clusters concept pairs
based on their context. It consists of the two
sub-modules described below.
? Depend Clustering, which merges con-
cept pairs using dependency patterns
alone, aiming at obtaining clusters of
concept pairs with good precision;
? Surface Clustering, which clusters
concept pairs using surface patterns
based on the resultant clusters of depend
clustering. The aim is to merge more
concept pairs into existing clusters with
surface patterns to improve the coverage
of clusters.
1023
4.2 Text Preprocessor and Concept Pair
Collector
This module pre-processes Wikipedia article texts
to collect concept pairs and corresponding sen-
tences. Given a concept described in a Wikipedia
article, our idea of preprocessing executes initial
consideration of all anchor-text concepts linking
to other Wikipedia articles in the article as related
concepts that might share a semantic relation with
the entitled concept. The link structure, more par-
ticularly, the structure of outgoing links, provides
a simple mechanism for identifying relevant arti-
cles. We split text into sentences and select sen-
tences containing one reference of an entitled con-
cept and one of the linked texts for the dependency
pattern extractor module.
4.3 Web Context Collector
Querying a concept pair using a search engine
(Google), we characterize the semantic relation
between the pair by leveraging the vast size of the
Web. Our hypothesis is that there exist some key
terms and patterns that provide clues to the rela-
tions between pairs. From the snippets retrieved
by the search engine, we extract relational infor-
mation of two kinds: ranked relational terms as
keywords and surface patterns. Here surface pat-
terns are generated with support of ranked rela-
tional terms.
4.3.1 Relational Term Ranking
To collect relational terms as indicators for each
concept pair, we look for verbs and nouns from
qualified sentences in the snippets instead of sim-
ply finding verbs. Using only verbs as relational
terms might engender the loss of various important
relations, e.g. noun relations ?CEO?, ?founder?
between a person and a company. Therefore, for
each concept pair, a list of relational terms is col-
lected. Then all the collected terms of all concept
pairs are combined and ranked using an entropy-
based algorithm which is described in (Chen et al,
2005). With their algorithm, the importance of
terms can be assessed using the entropy criterion,
which is based on the assumption that a term is ir-
relevant if its presence obscures the separability of
the dataset. After the ranking, we obtain a global
ranked list of relational terms Tall for the whole
dataset (all the concept pairs). For each concept
pair, a local list of relational terms Tcp is sorted ac-
cording to the terms? order in Tall. Then from the
relational term list Tcp, a keyword tcp is selected
Table 1: Surface patterns for a concept pair
Pattern Pattern
ec ceo rc rc found ec
ceo rc found ec rc succeed as ceo of ec
rc be ceo of ec ec ceo of rc
ec assign rc as ceo ec found by ceo rc
ceo of ec rc ec found in by rc
for each concept pair cp as the first term appearing
in the term list Tcp. Keyword tcp will be used to
initialize the clustering algorithm in Section 4.5.1.
4.3.2 Surface Pattern Generation
Because simply taking the entire string between
two concept words captures an excess of extra-
neous and incoherent information, we use Tcp of
each concept pair as a key for surface pattern gen-
eration. We classified words into Content Words
(CWs) and Functional Words (FWs). From each
snippet sentence, the entitled concept, related con-
cept, or the keyword kcp is considered to be a Con-
tent Word (CW). Our idea of obtaining FWs is to
look for verbs, nouns, prepositions, and coordinat-
ing conjunctions that can help make explicit the
hidden relations between the target nouns.
Surface patterns have the following general
form.
[CW1] Infix1 [CW2] Infix2 [CW3] (1)
Therein, Infix1 and Infix2 respectively con-
tain only and any number of FWs. A pattern ex-
ample is ?ec assign rc as ceo (keyword)?. All gen-
erated patterns are sorted by their frequency, and
all occurrences of the entitled concept and related
concept are replaced with ?ec? and ?rc?, respec-
tively for pattern matching of different concept
pairs.
Table 1 presents examples of surface patterns
for a sample concept pair. Pattern windows are
bounded by CWs to obtain patterns more precisely
because 1) if we use only the string between two
concepts, it may not contain some important re-
lational information, such as ?ceo ec resign rc?
in Table 1; 2) if we generate patterns by setting
a windows surrounding two concepts, the number
of unique patterns is often exponential.
4.4 Dependency Pattern Extractor
In this section, we describe how to obtain depen-
dency patterns for relation clustering. After pre-
processing, selected sentences that contain at least
1024
one mention of an entitled concept or related con-
cept are parsed into dependency structures. We de-
fine dependency patterns as sub-paths of the short-
est dependency path between a concept pair for
two reasons. One is that the shortest path de-
pendency kernels outperform dependency tree ker-
nels by offering a highly condensed representation
of the information needed to assess their relation
(Bunescu and Mooney, 2005). The other reason
is that embedded structures of the linguistic repre-
sentation are important for obtaining good cover-
age of the pattern acquisition, as explained in (Cu-
lotta and Sorensen, 2005); (Zhang et al, 2006).
The process of inducing dependency patterns has
two steps.
1. Shortest dependency path inducement. From
the original dependency tree structure by parsing
the selected sentence for each concept pair, we
first induce the shortest dependency path with the
entitled concept and related concept.
2. Dependency pattern generation. We use
a frequent tree-mining algorithm (Zaki, 2002) to
generate sub-paths as dependency patterns from
the shortest dependency path for relation cluster-
ing.
4.5 Clustering Algorithm for Relation
Extraction
In this subsection, we present a clustering algo-
rithm that merges concept pairs based on depen-
dency patterns and surface patterns. The algorithm
is based on k-means clustering for relation cluster-
ing.
The dependency pattern has the properties of
being more accurate, but the Web context has the
advantage of containing much more redundant in-
formation than Wikipedia. Our idea of concept
pair clustering is a two-step clustering process:
first it clusters concept pairs into clusters with
good precision using dependency patterns; then it
improves the coverage of the clusters using surface
patterns.
4.5.1 Initial Centroid Selection and Distance
Function Definition
The standard k-means algorithm is affected by
the choice of seeds and the number of clusters
k. However, as we claimed in the Introduc-
tion section, because we aim to extract relations
from Wikipedia articles in an unsupervised man-
ner, cluster number k is unknown and no good
centroids can be predicted. As described in this
paper, we select centroids based on the keyword
tcp of each concept pair.
First of all, all concept pairs are grouped by
their keywords tcp. Let G = {G1, G2, ...Gn}
be the resultant groups, where each Gi =
{cpi1, cpi2, ...} identify a group of concept pairs
sharing the same keyword tcp (such as ?CEO?).
We rank all the groups by their number of concept
pairs and then choose the top k groups. Then a
centroid ci is selected for each group Gi by Eq. 2.
ci = argmaxcp?Gi |{cpij |(dis1(cpij , cp)+
? ? dis2(cpij , cp)) <= Dz, 1 ? j ? |Gi|}| (2)
We assume a centroid for each group to be the
concept pair which has the most other concept
pairs in the same group that have distance less
than Dz with it. Also, Dz is a threshold to avoid
noisy concept pairs: we assign it 1/3. To balance
the contribution between dependency patterns and
surface patterns, ? is used. The distance function
to calculate the distance between dependency pat-
tern sets DPi, DPj of two concept pairs cpi and
cpj is dis1. The distance is decided by the number
of overlapped dependency patterns with Eq. 3.
dis1(cpi, cpj) = 1? |DPi ?DPj |?(|DPi| ? |DPj |)
(3)
Actually, dis2 is the distance function to calcu-
late distance between two surface pattern sets of
two concept pairs. To compute the distance over
surface patterns, we implement the distance func-
tion dis2(cpi, cpj) in Fig. 2.
Algorithm 1: distance function dis2(cpi, cpj)
Input: SP1 = {sp11, ..., sp1m}(surface patterns of
cpi)
SP2 = {sp21, ..., sp2n} (surface patterns of cpj)
Output: dis (distance between SP1 and SP2)
define a m? n distance matrix A:
{Aij = LD(sp1i,sp2j)Max(|sp1i|,|sp2j |) , 1?i?m; 1?j?n};
dis ? 0
for min(m,n) times do
(x, y) ? argmin0<i<m;0<j<nAij ;
dis ? dis + Axy/min(m,n);
Ax? ? 1; A?y ? 1;
return dis
Figure 2: Distance function over surface patterns
As shown in Fig. 2, the distance algorithm per-
forms as: firstly it defines a m?n distance matrix
A, then repeatedly selects two nearest sequences
and sums up their distances. While computing
1025
dis2, we use the Levenshtein distance LD to mea-
sure the difference of two surface patterns. The
Levenshtein distance is a metric for measuring the
amount of difference between two sequences (i.e.,
the so-called edit distance). Each generated sur-
face pattern is a sequence of words. The distance
of two surface patterns is defined as the fraction of
the LD value to the length of the longer sequence.
For estimating the number of clusters k, we ap-
ply the stability-based criteria from (Chen et al,
2005) to decide the number of optimal clusters k
automatically.
4.5.2 Concept Pair Clustering with
Dependency Patterns
Given the initial seed concept pairs and cluster
number k, this stage merges concept pairs over de-
pendency patterns into k clusters. Each concept
pair cpi has a set of dependency patterns DPi. We
calculate distances between two pairs cpi and cpj
using above the function dis1(cpi, cpj). The clus-
tering algorithm is portrayed in Fig. 3. The pro-
cess of depend clustering is to assign each concept
pair to the cluster with the closest centroid and
then recomputing each centroid based on the cur-
rent members of its cluster. As shown in Figure 3,
this is done iteratively by repeating both two steps
until a stopping criterion is met. We apply the ter-
mination condition as: centroids do not change be-
tween iterations.
Algorithm 2: Depend Clustering
Input: I = {cp1, ..., cpn}(all concept pairs)
C = {c1, ..., ck} (k initial centroids)
Output: Md : I ? C (cluster membership)
Ir (rest of concept pairs not clustered)
Cd = {c1, ..., ck} (recomputed centroids)
while stopping criterion has not been met do
for each cpi ? I do
if mins?1..k dis1(cpi, cs) <= Dl then
Md(cpi) ? argmins?1..k dis1(cpi, cs)else
Md(cpi) ? 0
for each j ? {1..k} do
recompute cj as the centroid of
{cpi|mloc(cpi) = j}
Ir ? C0
return C and Cd
Figure 3: Clustering with dependency patterns
Because many concept pairs are scattered and
do not belong to any of the top k clusters, we
filter concept pairs with distance larger than Dl
with the seed concept pairs. Such concept pairs
ST1
ST3 ST4
ST2
Text3: RC was hired as EC?s CEO Text4: EC assign RC as CEO
Text1: the CEO of EC is RC Text2: RC is the CEO of EC
Figure 4: Example showing why surface cluster-
ing is needed
are stored in C0. We named the cluster of concept
pairs Ir which are left to be clustered in the next
step of clustering. After this step, concept pairs
with similar dependency patterns are merged into
same clusters, see Fig. 4 (ST1, ST2).
4.5.3 Concept Pair Clustering with Surface
Patterns
A salient difficulty posed by dependency pattern
clustering is that concept pairs of the same se-
mantic relation cannot be merged if they are ex-
pressed in different dependency structures. Fig-
ure 4 presents an example demonstrating why we
perform surface pattern clustering. As depicted
in Fig. 4, ST1, ST2, ST3, and ST4 are depen-
dency structures for four concept pairs that should
be classified as the same relation ?CEO?. However
ST3 and ST4 can not be merged with ST1 and
ST2 using the dependency patterns because their
dependency structures are too diverse to share suf-
ficient dependency patterns.
In this step, we use surface patterns to merge
more concept pairs for each cluster to improve the
coverage. Figure 5 portrays the algorithm. We
assume that each concept pair has a set of sur-
face patterns from the Web context collector mod-
ule. As shown in Figure 5, surface clustering is
done iteratively by repeating two steps until a stop-
ping criterion is met: using the distance function
dis2 explained in the preceding section, assign
each concept pair to the cluster with the closest
centroid and recomputing each centroid based on
the current members of its cluster. We apply the
same termination condition as depend clustering.
1026
Additionally, we filter concept pairs with distance
greater than Dg with the centroid concept pairs.
Algorithm 3: Surface Clustering
Input: Ir (rest of concept pairs)
Cd = {c1, ..., ck} (initial centroids)
Output: Ms : Ir ? C (cluster membership)
Cs = {c1, ..., ck} (final centroids)
while stopping criterion has not been met do
for each cpi ? Ir do
if mins?1..k dis2(cpi, cs) <= Dg then
Ms(cpi) ? argmins?1..k dis2(cpi, cs)else
Ms(cpi) ? 0
for each j ? 1..k do
recompute cj as the centroid of cluster
{cpi|Md(cpi) = j ?Ms(cpi) = j}
return clusters C
Figure 5: Clustering with surface patterns
Finally we have k clusters of concept pairs, each
of which has a centroid concept pair. To attach
a single relation label to each cluster, we use the
centroid concept pair.
5 Experiments
We apply our algorithm to two categories in
Wikipedia: ?American chief executives? and
?Companies?. Both categories are well defined
and closed. We conduct experiments for extract-
ing various relations and for measuring the quality
of these relations in terms of precision and cover-
age. We use coverage as an evaluation instead of
using recall as a measure. The coverage is used to
evaluate all correctly extracted concept pairs. It is
defined as the fraction of all the correctly extracted
concept pairs to the whole set of concept pairs. To
balance between precision and coverage of clus-
tering, we integrate two parameters: Dl, Dg.
We downloaded the Wikipedia dump as of De-
cember 3, 2008. The performance of the pro-
posed method is evaluated using different pattern
types: dependency patterns, surface patterns, and
their combination. We compare our method with
(Rosenfeld and Feldman, 2007)?s URI method.
Their algorithm outperformed that presented in the
earlier work using surface features of two kinds for
unsupervised relation extraction: features that test
two entities together and features that test only one
entity each. For comparison, we use a k-means
clustering algorithm using the same cluster num-
ber k.
Table 2: Results for the category: ?American chief
executives?
method Existing method Proposed method
(Rosenfeld et al) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
chairman 434 63.52 547 68.37
(x be chairman of y)
ceo 396 73.74 423 77.54
(x be ceo of y)
bear 138 83.33 276 86.96
(x be bear in y)
attend 225 67.11 313 70.28
(x attend y)
member 14 85.71 175 91.43
(x be member of y)
receive 97 67.97 117 73.53
(x receive y)
graduate 18 83.33 92 88.04
(x graduate from y)
degree 5 80.00 78 82.05
(x obtain y degree)
marry 55 41.67 74 61.25
(x marry y)
earn 23 86.96 51 88.24
(x earn y)
award 23 43.47 46 84.78
(x won y award)
hold 5 80.00 37 72.97
(x hold y degree)
become 35 74.29 37 81.08
(x become y)
director 24 67.35 29 79.31
(x be director of y)
die 18 77.78 19 84.21
(x die in y)
all 1510 68.27 2314 75.63
5.1 Wikipedia Category: ?American chief
executives?
We choose appropriate Dl(concept pair filter in
depend clustering) and Dg(concept pair filter in
surface clustering) in a development set. To bal-
ance precision and coverage, we set 1/3 for both
Dl and Dg.
The 526 articles in this category are used for
evaluation. We obtain 7310 concept pairs from
the articles as our dataset. The top 18 groups are
chosen to obtain the centroid concept pairs. Of
these, 15 binary relations are the clearly identifi-
able relations shown in Table 2, where # Ins. rep-
resents the number of concept pairs clustered us-
ing each method, and pre denotes the precision of
each cluster.
The proposed approach shows higher precision
and better coverage than URI in Table 2. This
result demonstrates that adding dependency pat-
terns from linguistic analysis contributes more to
the precision and coverage of the clustering task
than the sole use of surface patterns.
1027
Table 3: Performance of different pattern types
Pattern type #Instance Precision Coverage
dependency 1127 84.29 13.00%
surface 1510 68.27 14.10%
Combined 2314 75.63 23.94%
Table 4: Results for the category: ?Companies?
Method Existing method Proposed method
(Rosenfeld et al) (Our method)
Relation # Ins. pre # Ins. pre
(sample)
found 82 75.61 163 84.05
(found x in y)
base 82 76.83 122 82.79
(x be base in y)
headquarter 23 86.97 120 89.34
(x be headquarter in y)
service 37 51.35 108 69.44
(x offer y service)
store 113 77.88 88 72.72
(x open store in y)
acquire 59 62.71 70 64.28
(x acquire y)
list 51 64.71 67 70.15
(x list on y)
product 25 76.00 57 77.19
(x produce y)
CEO 37 64.86 39 66.67
(ceo x found y)
buy 53 62.26 37 56.76
(x buy y)
establish 35 82.86 26 80.77
(x be establish in y)
locate 14 50.00 24 75.00
(x be locate in y)
all 685 71.03 1039 76.87
To examine the contribution of dependency pat-
terns, we compare results obtained with patterns
of different kinds. Table 3 shows the precision and
coverage scores. The best precision is achieved by
dependency patterns. The precision is markedly
better than that of surface patterns. However, the
coverage is worse than that by surface patterns. As
we reported, many concept pairs are scattered and
do not belong to any of the top k clusters, the cov-
erage is low.
5.2 Wikipedia Category: ?Companies?
We also evaluate the performance for the ?Com-
panies? category. Instead of using all the arti-
cles, we randomly select 434 articles for evalua-
tion and 4073 concept pairs from the articles form
our dataset for this category. We also set Dl and
Dg to 1/3. Then 28 groups are chosen. For each
group, a centroid concept pair is obtained. Finally,
of 28 clusters, 25 binary relations are clearly iden-
tifiable relations. Table 4 presents some relations.
Table 5: Performance of different pattern types
Pattern type #Instance Precision Coverage
dependency 551 82.58 11.17%
surface 685 71.03 11.95%
Combined 1039 76.87 19.61%
Our clustering algorithms use two filters Dl and
Dg to filter scattering concept pairs. In Table 4, we
present that concept pairs are clustered with good
precision. As in the first experiments, the combi-
nation of dependency patterns and surface patterns
contribute greatly to the precision and coverage.
Table 5 shows that, using dependency patterns,
the precision is the highest (82.58%), although the
coverage is the lowest.
All experimental results support our idea
mainly in two aspects: 1) Dependency analysis
can abstract away from different surface realiza-
tions of text. In addition, embedded structures of
the dependency representation are important for
obtaining a good coverage of the pattern acqui-
sition. Furthermore, the precision is better than
that of the string surface patterns from Web pages
of various kinds. 2) Surface patterns are used to
merge concept pairs with relations represented in
different dependency structures with redundancy
information from the vast size of Web pages. Us-
ing surface patterns, more concept pairs are clus-
tered, and the coverage is improved.
6 Conclusions
To discover a range of semantic relations from
a large corpus, we present an unsupervised rela-
tion extraction method using deep linguistic in-
formation to alleviate surface and noisy surface
patterns generated from a large corpus, and use
Web frequency information to ease the sparse-
ness of linguistic information. We specifically ex-
amine texts from Wikipedia articles. Relations
are gathered in an unsupervised way over pat-
terns of two types: dependency patterns by parsing
sentences in Wikipedia articles using a linguistic
parser, and surface patterns from redundancy in-
formation from the Web corpus using a search en-
gine. We report our experimental results in com-
parison to those of previous works. The results
show that the best performance arises from a com-
bination of dependency patterns and surface pat-
terns.
1028
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead and Oren Etzioni. 2007.
Open information extraction from the Web. In Pro-
ceedings of IJCAI-2007.
Danushka Bollegala, Yutaka Matsuo and Mitsuru
Ishizuka. 2007. Measuring Semantic Similarity be-
tween Words Using Web Search Engines. In Pro-
ceedings of WWW-2007.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of HLT/EMLNP-2005.
Jinxiu Chen, Donghong Ji, Chew Lim Tan and
Zhengyu Niu. 2005. Unsupervised Feature Se-
lection for Relation Extraction. In Proceedings of
IJCNLP-2005.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the ACL-2004.
Dmitry Davidov, Ari Rappoport and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by Web mining. In Proceed-
ings of ACL-2007.
Dmitry Davidov and Ari Rappoport. 2008. Classifi-
cation of Semantic Relationships between Nominals
Using Pattern Clusters. In Proceedings of ACL-
2008.
Wei Fan, Kun Zhang, Hong Cheng, Jing Gao, Xifeng
Yan, Jiawei Han, Philip S. Yu and Olivier Ver-
scheure. 2008. Direct Mining of Discriminative and
Essential Frequent Patterns via Model-based Search
Tree. In Proceedings of KDD-2008.
Evgeniy Gabrilovich and Shaul Markovitch. 2006.
Overcoming the brittleness bottleneck using
wikipedia: Enhancing text categorization with
encyclopedic knowledge. In Proceedings of
AAAI-2006.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature 438:900C901.
Sanda Harabagiu, Cosmin Adrian Bejan and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In Proceedings of IJCAI-2005.
Takaaki Hasegawa, Satoshi Sekine and Ralph Grish-
man. 2004. Discovering Relations among Named
Entities from Large Corpora. In Proceedings of
ACL-2004.
Nanda Kambhatla. 2004. Combining lexical, syntactic
and semantic features with maximum entropy mod-
els. In Proceedings of ACL-2004.
Dat P.T. Nguyen, Yutaka Matsuo and Mitsuru Ishizuka.
2007. Relation extraction from Wikipedia using sub-
tree mining. In Proceedings of AAAI-2007.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceedings
of ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2006.
URES: an Unsupervised Web Relation Extraction
System. In Proceedings of COLING/ACL-2006.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for Unsupervised Relation Identification. In
Proceedings of CIKM-2007.
Peter D. Turney. 2006. Expressing implicit seman-
tic relations without supervision. In Proceedings of
ACL-2006.
Max Volkel, Markus Krotzsch, Denny Vrandecic,
Heiko Haller and Rudi Studer. 2006. Semantic
wikipedia. In Proceedings of WWW-2006.
Mohammed J. Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of SIGKDD-2002.
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of ACL-2006.
1029
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 851?859,
Beijing, August 2010
Simple and Efficient Algorithm
for Approximate Dictionary Matching
Naoaki Okazaki
University of Tokyo
okazaki@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
University of Tokyo
University of Manchester
National Centre for Text Mining
tsujii@is.s.u-tokyo.ac.jp
Abstract
This paper presents a simple and effi-
cient algorithm for approximate dictio-
nary matching designed for similarity
measures such as cosine, Dice, Jaccard,
and overlap coefficients. We propose this
algorithm, called CPMerge, for the ? -
overlap join of inverted lists. First we
show that this task is solvable exactly by
a ? -overlap join. Given inverted lists re-
trieved for a query, the algorithm collects
fewer candidate strings and prunes un-
likely candidates to efficiently find strings
that satisfy the constraint of the ? -overlap
join. We conducted experiments of ap-
proximate dictionary matching on three
large-scale datasets that include person
names, biomedical names, and general
English words. The algorithm exhib-
ited scalable performance on the datasets.
For example, it retrieved strings in 1.1
ms from the string collection of Google
Web1T unigrams (with cosine similarity
and threshold 0.7).
1 Introduction
Languages are sufficiently flexible to be able to
express the same meaning through different dic-
tion. At the same time, inconsistency of surface
expressions has persisted as a serious problem in
natural language processing. For example, in the
biomedical domain, cardiovascular disorder can
be described using various expressions: cardio-
vascular diseases, cardiovascular system disor-
der, and disorder of the cardiovascular system. It
is a nontrivial task to find the entry from these sur-
face expressions appearing in text.
This paper addresses approximate dictionary
matching, which consists of finding all strings in
a string collection V such that they have similar-
ity that is no smaller than a threshold ? with a
query string x. This task has a broad range of ap-
plications, including spelling correction, flexible
dictionary look-up, record linkage, and duplicate
detection (Henzinger, 2006; Manku et al, 2007).
Formally, the task obtains a subset Yx,? ? V ,
Yx,? = {y ? V
?? sim(x, y) ? ?}, (1)
where sim(x, y) presents the similarity between x
and y. A na??ve solution to this task is to com-
pute similarity values |V | times, i.e., between x
and every string y ? V . However, this solution
is impractical when the number of strings |V | is
huge (e.g., more than one million).
In this paper, we present a simple and effi-
cient algorithm for approximate dictionary match-
ing designed for similarity measures such as co-
sine, Dice, Jaccard, and overlap coefficients. Our
main contributions are twofold.
1. We show that the problem of approximate
dictionary matching is solved exactly by a
? -overlap join (Sarawagi and Kirpal, 2004)
of inverted lists. Then we present CPMerge,
which is a simple and efficient algorithm for
the ? -overlap join. In addition, the algorithm
is easily implemented.
2. We demonstrate the efficiency of the al-
gorithm on three large-scale datasets with
person names, biomedical concept names,
851
and general English words. We com-
pare the algorithm with state-of-the-art al-
gorithms, including Locality Sensitive Hash-
ing (Ravichandran et al, 2005; Andoni and
Indyk, 2008) and DivideSkip (Li et al,
2008). The proposed algorithm retrieves
strings the most rapidly, e.g., in 1.1 ms from
Google Web1T unigrams (with cosine simi-
larity and threshold 0.7).
2 Proposed Method
2.1 Necessary and sufficient conditions
In this paper, we assume that the features of a
string are represented arbitrarily by a set. Al-
though it is important to design a string represen-
tation for an accurate similarity measure, we do
not address this problem: our emphasis is not on
designing a better representation for string simi-
larity but on establishing an efficient algorithm.
The most popular representation is given by n-
grams: all substrings of size n in a string. We
use trigrams throughout this paper as an example
of string representation. For example, the string
?methyl sulphone? is expressed by 17 elements
of letter trigrams1, {?$$m?, ?$me?, ?met?,
?eth?, ?thy?, ?hyl?, ?yl ?, ?l s?, ? su?,
?sul?, ?ulp?, ?lph?, ?pho?, ?hon?,
?one?, ?ne$?, ?e$$?}. We insert two $s be-
fore and after the string to denote the start or end
of the string. In general, a string x consisting of
|X| letters yields (|x| + n ? 1) elements of n-
grams. We call |x| and |X| the length and size,
respectively, of the string x.
Let X and Y denote the feature sets of the
strings x and y, respectively. The cosine similarity
between the two strings x and y is,
cosine(X,Y ) = |X ? Y |?
|X||Y |
. (2)
By integrating this definition with Equation 1, we
obtain the necessary and sufficient condition for
1In practice, we attach ordinal numbers to n-grams to rep-
resent multiple occurrences of n-grams in a string (Chaud-
huri et al, 2006). For example, the string ?prepress?, which
contains two occurrences of the trigram ?pre?, yields
the set {?$$p?#1, ?$pr?#1, ?pre?#1, ?rep?#1,
?epr?#1, ?pre?#2, ?res?#1, ?ess?#1, ?ss$?#1,
?s$$?#1}.
Table 1: Conditions for each similarity measure
Measure min |Y | max |Y | ?(= min |X ? Y |)
Dice ?2?? |X| 2??? |X| 12?(|X|+ |Y |)
Jaccard ?|X| |X|/? ?(|X|+|Y |)1+?
Cosine ?2|X| |X|/?2 ??|X||Y |
Overlap ? ? ?min{|X|, |Y |}
approximate dictionary matching,
?
?
?
|X||Y |
?
? |X ? Y | ? min{|X|, |Y |}.
(3)
This inequality states that two strings x and y must
have at least ? =
?
?
?
|X||Y |
?
features in com-
mon. When ignoring |X?Y | in the inequality, we
have an inequality about |X| and |Y |,
?
?2|X|
?
? |Y | ?
? |X|
?2
?
(4)
This inequality presents the search range for re-
trieving similar strings; that is, we can ignore
strings whose feature size is out of this range.
Other derivations are also applicable to similar-
ity measures, including Dice, Jaccard, and overlap
coefficients. Table 1 summarizes the conditions
for these similarity measures.
We explain one usage of these conditions. Let
query string x = ?methyl sulphone? and thresh-
old for approximate dictionary matching ? = 0.7
with cosine similarity. Representing the strings
with letter trigrams, we have the size of x, |X| =
17. The inequality 4 gives the search range of |Y |
of the retrieved strings, 9 ? |Y | ? 34. Presum-
ing that we are searching for strings of |Y | = 16,
we obtain the necessary and sufficient condition
for the approximate dictionary matching from the
inequality 3, ? = 12 ? |X ? Y |. Thus, we need
to search for strings that have at least 12 letter tri-
grams that overlap with X . When considering a
string y = ?methyl sulfone?, which is a spelling
variant of y (ph ? f), we confirm that the string
is a solution for approximate dictionary matching
because |X?Y | = 13 (? ? ). Here, the actual sim-
ilarity is cosine(X,Y ) = 13/?17? 16 = 0.788
(? ?).
2.2 Data structure and algorithm
Algorithm 1 presents the pseudocode of the ap-
proximate dictionary matching based on Table 1.
852
Input: V : collection of strings
Input: x: query string
Input: ?: threshold for the similarity
Output: Y: list of strings similar to the query
X ? string to feature(x);1
Y ?[];2
for l? min y(|X|, ?) to max y(|X|, ?) do3
? ? min overlap(|X|, l, ?);4
R? overlapjoin(X , ? , V , l);5
foreach r ? R do append r to Y;6
end7
return Y;8
Algorithm 1: Approximate dictionary
matching.
Given a query string x, a collection of strings V ,
and a similarity threshold ?, the algorithm com-
putes the size range (line 3) given by Table 1.
For each size l in the range, the algorithm com-
putes the minimum number of overlaps ? (line 4).
The function overlapjoin (line 5) finds sim-
ilar strings by solving the following problem (? -
overlap join): given a list of features of the query
string X and the minimum number of overlaps ? ,
enumerate strings of size l in the collection V such
that they have at least ? feature overlaps with X .
To solve this problem efficiently, we build an
inverted index that stores a mapping from the fea-
tures to their originating strings. Then, we can
perform the ? -overlap join by finding strings that
appear at least ? times in the inverted lists re-
trieved for the query features X .
Algorithm 2 portrays a na??ve solution for the
? -overlap join (AllScan algorithm). In this algo-
rithm, function get(V , l, q) returns the inverted
list of strings (of size l) for the feature q. In
short, this algorithm scans strings in the inverted
lists retrieved for the query features X , counts the
frequency of occurrences of every string in the
inverted lists, and returns the strings whose fre-
quency of occurrences is no smaller than ? .
This algorithm is inefficient in that it scans
all strings in the inverted lists. The number of
scanned strings is large, especially when some
query features appear frequently in the strings,
e.g., ?s$$? (words ending with ?s?) and ?pre?
(words with substring ?pre?). To make matters
worse, such features are too common for charac-
terizing string similarity. The AllScan algorithm
Input: X: array of features of the query string
Input: ? : minimum number of overlaps
Input: V : collection of strings
Input: l: size of target strings
Output: R: list of strings similar to the query
M ? {};1
R? [];2
foreach q ? X do3
foreach i ? get(V , l, q) do4
M [i]?M [i] + 1;5
if ? ?M [i] then6
append i to R;7
end8
end9
end10
return R;11
Algorithm 2: AllScan algorithm.
is able to maintain numerous candidate strings in
M , but most candidates are not likely to qualified
because they have few overlaps with X .
To reduce the number of the candidate strings,
we refer to signature-based algorithms (Arasu et
al., 2006; Chaudhuri et al, 2006):
Property 1 Let there be a set (of size h) X and a
set (of any size) Y . Consider any subset Z ? X of
size (h? ? +1). If |X ?Y | ? ? , then Z ?Y 6= ?.
We explain one usage of this property. Let query
string x = ?methyl sulphone? and its trigram set
X be features (therefore, |X| = h = 17). Pre-
suming that we seek strings whose trigrams are
size 16 and have 12 overlaps withX , then string y
must have at least one overlap with any subset of
size 6 (= 17 ? 12 + 1) of X . We call the subset
signatures. The property leads to an algorithmic
design by which we obtain a small set of candi-
date strings from the inverted lists for signatures,
(|X| ? ? + 1) features in X , and verify whether
each candidate string satisfies the ? overlap with
the remaining (? ? 1) n-grams.
Algorithm 3 presents the pseudocode employ-
ing this idea. In line 1, we arrange the features in
X in ascending order of the number of strings in
their inverted lists. We denote the k-th element in
the ordered features as Xk (k ? {0, ..., |X| ? 1}),
where the index number begins with 0. Based on
this notation,X0 andX|X|?1 are the most uncom-
mon and the most common features in X , respec-
tively.
In lines 2?7, we use (|X| ? ? + 1) features
853
Input: X: array of features of the query string
Input: ? : minimum number of overlaps
Input: V : collection of strings
Input: l: size of target strings
Output: R: list of strings similar to the query
sort elements in X by order of |get(V , l, Xk)|;1
M ? {};2
for k ? 0 to (|X| ? ?) do3
foreach s ? get(V , l, Xk) do4
M [s]?M [s] + 1;5
end6
end7
R? [];8
for k ? (|X| ? ? + 1) to (|X| ? 1) do9
foreach s ?M do10
if bsearch(get(V , l, Xk), s) then11
M [s]?M [s] + 1;12
end13
if ? ?M [s] then14
append s to R;15
remove s from M ;16
else if M [s] + (|X| ? k ? 1) < ? then17
remove s from M ;18
end19
end20
end21
return R;22
Algorithm 3: CPMerge algorithm.
X0, ..., X|X|?? to generate a compact set of can-
didate strings. The algorithm stores the occur-
rence count of each string s in M [s]. In lines 9?
21, we increment the occurrence counts if each
of X|X|??+1, ..., X|X|?1 inverted lists contain the
candidate strings. For each string s in the candi-
dates (line 10), we perform a binary search on the
inverted list (line 11), and increment the overlap
count if the string s exists (line 12). If the overlap
counter of the string reaches ? (line 14), then we
append the string s to the result list R and remove
s from the candidate list (lines 15?16). We prune
a candidate string (lines 17?18) if the candidate is
found to be unreachable for ? overlaps even if it
appears in all of the unexamined inverted lists.
3 Experiments
We report the experimental results of approximate
dictionary matching on large-scale datasets with
person names, biomedical names, and general En-
glish words. We implemented various systems of
approximate dictionary matching.
? Proposed: CPMerge algorithm.
? Naive: Na??ve algorithm that computes the
cosine similarity |V | times for every query.
? AllScan: AllScan algorithm.
? Signature: CPMerge algorithm without
pruning; this is equivalent to Algorithm 3
without lines 17?18.
? DivideSkip: our implementation of the algo-
rithm (Li et al, 2008)2.
? Locality Sensitive Hashing (LSH) (Andoni
and Indyk, 2008): This baseline system fol-
lows the design of previous work (Ravichan-
dran et al, 2005). This system approxi-
mately solves Equation 1 by finding dictio-
nary entries whose LSH values are within
the (bit-wise) hamming distance of ? from
the LSH value of a query string. To adapt
the method to approximate dictionary match-
ing, we used a 64-bit LSH function com-
puted with letter trigrams. By design, this
method does not find an exact solution to
Equation 1; in other words, the method can
miss dictionary entries that are actually sim-
ilar to the query strings. This system has
three parameters, ?, q (number of bit permu-
tations), and B (search width), to control the
tradeoff between retrieval speed and recall3.
Generally speaking, increasing these param-
eters improves the recall, but slows down the
speed. We determined ? = 24 and q = 24
experimentally4, and measured the perfor-
mance when B ? {16, 32, 64}.
The systems, excluding LSH, share the same
implementation of Algorithm 1 so that we can
specifically examine the differences of the algo-
rithms for ? -overlap join. The C++ source code of
the system used for this experiment is available5.
We ran all experiments on an application server
running Debian GNU/Linux 4.0 with Intel Xeon
5140 CPU (2.33 GHz) and 8 GB main memory.
2We tuned parameter values ? ? {0.01, 0.02, 0.04, 0.1,
0.2, 0.4, 1, 2, 4, 10, 20, 40, 100} for each dataset. We se-
lected the parameter with the fastest response.
3We followed the notation of the original pa-
per (Ravichandran et al, 2005) here. Refer to the original
paper for definitions of the parameters ?, q, and B.
4q was set to 24 so that the arrays of shuffled hash values
are stored in memory. We chose ? = 24 from {8, 16, 24} be-
cause it showed a good balance between accuracy and speed.
5http://www.chokkan.org/software/simstring/
854
3.1 Datasets
We used three large datasets with person names
(IMDB actors), general English words (Google
Web1T), and biomedical names (UMLS).
? IMDB actors: This dataset comprises actor
names extracted from the IMDB database6.
We used all actor names (1,098,022 strings;
18 MB) from the file actors.list.gz.
The average number of letter trigrams in the
strings is 17.2. The total number of trigrams
is 42,180. The system generated index files
of 83 MB in 56.6 s.
? Google Web1T unigrams: This dataset con-
sists of English word unigrams included in
the Google Web1T corpus (LDC2006T13).
We used all word unigrams (13,588,391
strings; 121 MB) in the corpus after remov-
ing the frequency information. The aver-
age number of letter trigrams in the strings
is 10.3. The total number of trigrams is
301,459. The system generated index files
of 601 MB in 551.7 s.
? UMLS: This dataset consists of English
names and descriptions of biomedical con-
cepts included in the Unified Medical Lan-
guage System (UMLS). We extracted all
English concept names (5,216,323 strings;
212 MB) from MRCONSO.RRF.aa.gz and
MRCONSO.RRF.ab.gz in UMLS Release
2009AA. The average number of letter tri-
grams in the strings is 43.6. The total number
of trigrams is 171,596. The system generated
index files of 1.1 GB in 1216.8 s.
For each dataset, we prepared 1,000 query
strings by sampling strings randomly from the
dataset. To simulate the situation where query
strings are not only identical but also similar to
dictionary entries, we introduced random noise
to the strings. In this experiment, one-third of
the query strings are unchanged from the original
(sampled) strings, one-third of the query strings
have one letter changed, and one-third of the
query strings have two letters changed. When
changing a letter, we randomly chose a letter po-
sition from a uniform distribution, and replaced
6ftp://ftp.fu-berlin.de/misc/movies/database/
the letter at the position with an ASCII letter ran-
domly chosen from a uniform distribution.
3.2 Results
To examine the scalability of each system, we
controlled the number of strings to be indexed
from 10%?100%, and issued 1,000 queries. Fig-
ure 1 portrays the average response time for re-
trieving strings whose cosine similarity values are
no smaller than 0.7. Although LSH (B=16) seems
to be the fastest in the graph, this system missed
many true positives7; the recall scores of approx-
imate dictionary matching were 15.4% (IMDB),
13.7% (Web1T), and 1.5% (UMLS). Increasing
the parameterB improves the recall at the expense
of the response time. LSH (B=64)8. It not only
ran slower than the proposed method, but also
suffered from low recall scores, 25.8% (IMDB),
18.7% (Web1T), and 7.1% (UMLS). LSH was
useful only when we required a quick response
much more than recall.
The other systems were guaranteed to find
the exact solution (100% recall). The proposed
algorithm was the fastest of all exact systems
on all datasets: the response times per query
(100% index size) were 1.07 ms (IMDB), 1.10 ms
(Web1T), and 20.37 ms (UMLS). The response
times of the Na??ve algorithm were too slow, 32.8 s
(IMDB), 236.5 s (Web1T), and 416.3 s (UMLS).
The proposed algorithm achieved substantial
improvements over the AllScan algorithm: the
proposed method was 65.3 times (IMDB), 227.5
times (Web1T), and 13.7 times (UMLS) faster
than the Na??ve algorithm. We observed that the
Signature algorithm, which is Algorithm 3 with-
out lines 17?18, did not perform well: The Sig-
nature algorithm was 1.8 times slower (IMDB),
2.1 times faster (Web1T), and 135.0 times slower
(UMLS) than the AllScan algorithm. These re-
sults indicate that it is imperative to minimize the
number of candidates to reduce the number of
binary-search operations. The proposed algorithm
was 11.1?13.4 times faster than DivideSkip.
Figure 2 presents the average response time
7Solving Equation 1, all systems are expected to retrieve
the exact set of strings retrieved by the Na??ve algorithm.
8The response time of LSH (B=64) on the IMDB dataset
was 29.72 ms (100% index size).
855
05
10
15
20
25
0 20 40 60 80 100
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Number of indexed strings (%)
Proposed
AllScan
Signature
DivideSkip
LSH (B=16)
LSH (B=32)
0
10
20
30
40
50
0 20 40 60 80 100
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Number of indexed strings (%)
Proposed
AllScan
Signature
DivideSkip
LSH (B=16)
LSH (B=32)
LSH (B=64)
0
10
20
30
40
50
60
0 20 40 60 80 100
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Number of indexed strings (%)
(a) IMDB actors (b) Google Web1T unigrams (c) UMLS
Proposed
AllScan
Signature
DivideSkip
LSH (B=16)
LSH (B=32)
LSH (B=64)
Figure 1: Average response time for processing a query (cosine similarity; ? = 0.7).
0
5
10
15
20
25
30
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Similarity threshold
Dice
Jaccard
Cosine
Overlap
0
10
20
30
40
50
60
70
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Similarity threshold
Dice
Jaccard
Cosine
Overlap
0
50
100
150
200
250
300
350
400
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
v
e
ra
g
e
 r
e
s
p
o
n
s
e
 p
e
r 
q
u
e
ry
 [
m
s
]
Similarity threshold
Dice
Jaccard
Cosine
Overlap
(a) IMDB actors (b) Google Web1T unigram (c) UMLS
Figure 2: Average response time for processing a query.
of the proposed algorithm for different similarity
measures and threshold values. When the similar-
ity threshold is lowered, the algorithm runs slower
because the number of retrieved strings |Y| in-
creases exponentially. The Dice coefficient and
cosine similarity produced similar curves.
Table 2 summarizes the run-time statistics of
the proposed method for each dataset (with co-
sine similarity and threshold 0.7). Using the
IMDB dataset, the proposed method searched for
strings whose size was between 8.74 and 34.06;
it retrieved 4.63 strings per query string. The
proposed algorithm scanned 279.7 strings in 4.6
inverted lists to obtain 232.5 candidate strings.
The algorithm performed a binary search on 4.3
inverted lists containing 7,561.8 strings in all.
In contrast, the AllScan algorithm had to scan
16,155.1 strings in 17.7 inverted lists and con-
sidered 9,788.7 candidate strings, and found only
4.63 similar strings.
This table clearly demonstrates three key con-
tributions of the proposed algorithm for efficient
approximate dictionary matching. First, the pro-
posed algorithm scanned far fewer strings than did
the AllScan algorithm. For example, to obtain
candidate strings in the IMDB dataset, the pro-
posed algorithm scanned 279.7 strings, whereas
the AllScan algorithm scanned 16,155.1 strings.
Therefore, the algorithm examined only 1.1%?
3.5% of the strings in the entire inverted lists in
the three datasets. Second, the proposed algo-
rithm considered far fewer candidates than did
the AllScan algorithm: the number of candidate
strings considered by the algorithm was 1.2%?
6.6% of those considered by the AllScan algo-
rithm. Finally, the proposed algorithm read fewer
inverted lists than did the AllScan algorithm. The
proposed algorithm actually read 8.9 (IMDB), 6.0
(Web1T), and 31.7 (UMLS) inverted lists during
the experiments9. These values indicate that the
proposed algorithm can solve ? -overlap join prob-
lems by checking only 50.3% (IMDB), 53.6%
(Web1T), and 51.9% of the total inverted lists re-
9These values are 4.6 + 4.3, 3.1 + 2.9, and 14.3 + 17.4.
856
Table 2: Run-time statistics of the proposed algorithm for each dataset
Averaged item IMDB Web1T UMLS Description
min |y| 8.74 5.35 21.87 minimum size of trigrams of target strings
max |y| 34.06 20.46 88.48 maximum size of trigrams of target strings
? 14.13 9.09 47.77 minimum number of overlaps required/sufficient per query
|Y| 4.63 3.22 111.79 number of retrieved strings per query
Total ? averaged for each query and target size:
# inverted lists 17.7 11.2 61.1 number of inverted lists retrieved for a query
# strings 16 155.1 52 557.6 49 561.4 number of strings in the inverted list
# unique strings 9 788.7 44 834.6 17 457.5 number of unique strings in the inverted list
Candidate stage ? averaged for each query and target size:
# inverted lists 4.6 3.1 14.3 number of inverted lists scanned for generating candidates
# strings 279.7 552.7 1 756.3 number of strings scanned for generating candidates
# candidates 232.5 523.7 1 149.7 number of candidates generated for a query
Validation stage ? averaged for each query and target size:
# inverted lists 4.3 2.9 17.4 number of inverted lists examined by binary search for a query
# strings 7 561.8 19 843.6 20 443.7 number of strings targeted by binary search
trieved for queries.
4 Related Work
Numerous studies have addressed approximate
dictionary matching. The most popular configu-
ration uses n-grams as a string representation and
the edit distance as a similarity measure. Gra-
vano et al (1998; 2001) presented various filter-
ing strategies, e.g., count filtering, position fil-
tering, and length filtering, to reduce the num-
ber of candidates. Kim et al (2005) proposed
two-level n-gram inverted indices (n-Gram/2L) to
eliminate the redundancy of position information
in n-gram indices. Li et al (2007) explored the
use of variable-length grams (VGRAMs) for im-
proving the query performance. Lee et al (2007)
extended n-grams to include wild cards and de-
veloped algorithms based on a replacement semi-
lattice. Xiao et al (2008) proposed the Ed-Join
algorithm, which utilizes mismatching n-grams.
Several studies addressed different paradigms
for approximate dictionary matching. Bocek et
al. (2007) presented the Fast Similarity Search
(FastSS), an enhancement of the neighborhood
generation algorithms, in which multiple variants
of each string record are stored in a database.
Wang et al (2009) further improved the technique
of neighborhood generation by introducing parti-
tioning and prefix pruning. Huynh et al (2006)
developed a solution to the k-mismatch problem
in compressed suffix arrays. Liu et al (2008)
stored string records in a trie, and proposed a
framework called TITAN. These studies are spe-
cialized for the edit distance measure.
A few studies addressed approximate dictio-
nary matching for similarity measures such as
cosine and Jaccard similarities. Chaudhuri et
al. (2006) proposed the SSJoin operator for sim-
ilarity joins with several measures including the
edit distance and Jaccard similarity. This algo-
rithm first generates signatures for strings, finds
all pairs of strings whose signatures overlap,
and finally outputs the subset of these candi-
date pairs that satisfy the similarity predicate.
Arasu et al (2006) addressed signature schemes,
i.e., methodologies for obtaining signatures from
strings. They also presented an implementation of
the SSJoin operator in SQL. Although we did not
implement this algorithm in SQL, it is equivalent
to the Signature algorithm in Section 3.
Sarawagi and Kirpal (2004) proposed the Mer-
geOpt algorithm for the ? -overlap join to approx-
imate string matching with overlap, Jaccard, and
cosine measures. This algorithm splits inverted
lists for a given query A into two groups, S and
L, maintains a heap to collect candidate strings on
S, and performs a binary search on L to verify the
condition of the ? -overlap join for each candidate
string. Their subsequent work includes an effi-
cient algorithm for the top-k search of the overlap
join (Chandel et al, 2006).
Li et al (2008) extended this algorithm to the
SkipMerge and DivideSkip algorithms. The Skip-
Merge algorithm uses a heap to compute the ? -
overlap join on entire inverted lists A, but has
an additional mechanism to increment the fron-
857
tier pointers of inverted lists efficiently based on
the strings popped most recently from the heap.
Consequently, SkipMerge can reduce the number
of strings that are pushed to the heap. Similarly
to the MergeOpt algorithm, DivideSkip splits in-
verted lists A into two groups S and L, but it ap-
plies SkipMerge to S. In Section 3, we reported
the performance of DivideSkip.
Charikar (2002) presented the Locality Sen-
sitive Hash (LSH) function (Andoni and Indyk,
2008), which preserves the property of cosine
similarity. The essence of this function is to map
strings into N -bit hash values where the bitwise
hamming distance between the hash values of two
strings approximately corresponds to the angle of
the two strings. Ravichandran et al (2005) ap-
plied LSH to the task of noun clustering. Adapting
this algorithm to approximate dictionary match-
ing, we discussed its performance in Section 3.
Several researchers have presented refined sim-
ilarity measures for strings (Winkler, 1999; Cohen
et al, 2003; Bergsma and Kondrak, 2007; Davis et
al., 2007). Although these studies are sometimes
regarded as a research topic of approximate dic-
tionary matching, they assume that two strings for
the target of similarity computation are given; in
other words, it is out of their scope to find strings
in a large collection that are similar to a given
string. Thus, it is a reasonable approach for an ap-
proximate dictionary matching to quickly collect
candidate strings with a loose similarity threshold,
and for a refined similarity measure to scrutinize
each candidate string for the target application.
5 Conclusions
We present a simple and efficient algorithm for
approximate dictionary matching with the co-
sine, Dice, Jaccard, and overlap measures. We
conducted experiments of approximate dictio-
nary matching on large-scale datasets with person
names, biomedical names, and general English
words. Even though the algorithm is very sim-
ple, our experimental results showed that the pro-
posed algorithm executed very quickly. We also
confirmed that the proposed method drastically re-
duced the number of candidate strings considered
during approximate dictionary matching. We be-
lieve that this study will advance practical NLP
applications for which the execution time of ap-
proximate dictionary matching is critical.
An advantage of the proposed algorithm over
existing algorithms (e.g., MergeSkip) is that it
does not need to read all the inverted lists retrieved
by query n-grams. We observed that the proposed
algorithm solved ? -overlap joins by checking ap-
proximately half of the inverted lists (with cosine
similarity and threshold ? = 0.7). This charac-
teristic is well suited to processing compressed
inverted lists because the algorithm needs to de-
compress only half of the inverted lists. It is nat-
ural to extend this study to compressing and de-
compressing inverted lists for reducing disk space
and for improving query performance (Behm et
al., 2009).
Acknowledgments
This work was partially supported by Grants-
in-Aid for Scientific Research on Priority Areas
(MEXT, Japan) and for Solution-Oriented Re-
search for Science and Technology (JST, Japan).
References
Andoni, Alexandr and Piotr Indyk. 2008. Near-
optimal hashing algorithms for approximate nearest
neighbor in high dimensions. Communications of
the ACM, 51(1):117?122.
Arasu, Arvind, Venkatesh Ganti, and Raghav Kaushik.
2006. Efficient exact set-similarity joins. In VLDB
?06: Proceedings of the 32nd International Confer-
ence on Very Large Data Bases, pages 918?929.
Behm, Alexander, Shengyue Ji, Chen Li, and Jiaheng
Lu. 2009. Space-constrained gram-based indexing
for efficient approximate string search. In ICDE
?09: Proceedings of the 2009 IEEE International
Conference on Data Engineering, pages 604?615.
Bergsma, Shane and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
ACL ?07: Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 656?663.
Bocek, Thomas, Ela Hunt, and Burkhard Stiller. 2007.
Fast similarity search in large dictionaries. Tech-
nical Report ifi-2007.02, Department of Informatics
(IFI), University of Zurich.
858
Chandel, Amit, P. C. Nagesh, and Sunita Sarawagi.
2006. Efficient batch top-k search for dictionary-
based entity recognition. In ICDE ?06: Proceed-
ings of the 22nd International Conference on Data
Engineering.
Charikar, Moses S. 2002. Similarity estimation tech-
niques from rounding algorithms. In STOC ?02:
Proceedings of the thiry-fourth annual ACM sym-
posium on Theory of computing, pages 380?388.
Chaudhuri, Surajit, Venkatesh Ganti, and Raghav
Kaushik. 2006. A primitive operator for similar-
ity joins in data cleaning. In ICDE ?06: Proceed-
ings of the 22nd International Conference on Data
Engineering.
Cohen, William W., Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks.
In Proceedings of the IJCAI-2003 Workshop on
Information Integration on the Web (IIWeb-03),
pages 73?78.
Davis, Jason V., Brian Kulis, Prateek Jain, Suvrit Sra,
and Inderjit S. Dhillon. 2007. Information-theoretic
metric learning. In ICML ?07: Proceedings of the
24th International Conference on Machine Learn-
ing, pages 209?216.
Gravano, Luis, Panagiotis G. Ipeirotis, H. V. Jagadish,
Nick Koudas, S. Muthukrishnan, and Divesh Srivas-
tava. 2001. Approximate string joins in a database
(almost) for free. In VLDB ?01: Proceedings of the
27th International Conference on Very Large Data
Bases, pages 491?500.
Henzinger, Monika. 2006. Finding near-duplicate
web pages: a large-scale evaluation of algorithms.
In SIGIR ?06: Proceedings of the 29th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 284?
291.
Huynh, Trinh N. D., Wing-Kai Hon, Tak-Wah Lam,
and Wing-Kin Sung. 2006. Approximate string
matching using compressed suffix arrays. Theoreti-
cal Computer Science, 352(1-3):240?249.
Kim, Min-Soo, Kyu-Young Whang, Jae-Gil Lee, and
Min-Jae Lee. 2005. n-Gram/2L: a space and time
efficient two-level n-gram inverted index structure.
In VLDB ?05: Proceedings of the 31st International
Conference on Very Large Data Bases, pages 325?
336.
Lee, Hongrae, Raymond T. Ng, and Kyuseok Shim.
2007. Extending q-grams to estimate selectivity of
string matching with low edit distance. In VLDB
?07: Proceedings of the 33rd International Confer-
ence on Very Large Data Bases, pages 195?206.
Li, Chen, Bin Wang, and Xiaochun Yang. 2007.
Vgram: improving performance of approximate
queries on string collections using variable-length
grams. In VLDB ?07: Proceedings of the 33rd In-
ternational Conference on Very Large Data Bases,
pages 303?314.
Li, Chen, Jiaheng Lu, and Yiming Lu. 2008. Effi-
cient merging and filtering algorithms for approx-
imate string searches. In ICDE ?08: Proceedings
of the 2008 IEEE 24th International Conference on
Data Engineering, pages 257?266.
Liu, Xuhui, Guoliang Li, Jianhua Feng, and Lizhu
Zhou. 2008. Effective indices for efficient approxi-
mate string search and similarity join. In WAIM ?08:
Proceedings of the 2008 The Ninth International
Conference on Web-Age Information Management,
pages 127?134.
Manku, Gurmeet Singh, Arvind Jain, and Anish
Das Sarma. 2007. Detecting near-duplicates for
web crawling. In WWW ?07: Proceedings of the
16th International Conference on World Wide Web,
pages 141?150.
Navarro, Gonzalo and Ricardo Baeza-Yates. 1998. A
practical q-gram index for text retrieval allowing er-
rors. CLEI Electronic Journal, 1(2).
Ravichandran, Deepak, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: us-
ing locality sensitive hash function for high speed
noun clustering. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 622?629.
Sarawagi, Sunita and Alok Kirpal. 2004. Efficient
set joins on similarity predicates. In SIGMOD ?04:
Proceedings of the 2004 ACM SIGMOD interna-
tional conference on Management of data, pages
743?754.
Wang, Wei, Chuan Xiao, Xuemin Lin, and Chengqi
Zhang. 2009. Efficient approximate entity extrac-
tion with edit distance constraints. In SIGMOD
?09: Proceedings of the 35th SIGMOD Interna-
tional Conference on Management of Data, pages
759?770.
Winkler, William E. 1999. The state of record link-
age and current research problems. Technical Re-
port R99/04, Statistics of Income Division, Internal
Revenue Service Publication.
Xiao, Chuan, Wei Wang, and Xuemin Lin. 2008. Ed-
Join: an efficient algorithm for similarity joins with
edit distance constraints. In VLDB ?08: Proceed-
ings of the 34th International Conference on Very
Large Data Bases, pages 933?944.
859
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382?391,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Is a 204 cm Man Tall or Small ?
Acquisition of Numerical Common Sense from the Web
Katsuma Narisawa1 Yotaro Watanabe1 Junta Mizuno2
Naoaki Okazaki1,3 Kentaro Inui1
1Graduate School of Information Sciences, Tohoku University
2National Institute of Information and Communications Technology (NICT)
3Japan Science and Technology Agency (JST)
{katsuma,yotaro-w,junta-m,okazaki,inui}@ecei.tohoku.ac.jp
Abstract
This paper presents novel methods for
modeling numerical common sense: the
ability to infer whether a given number
(e.g., three billion) is large, small, or nor-
mal for a given context (e.g., number of
people facing a water shortage). We first
discuss the necessity of numerical com-
mon sense in solving textual entailment
problems. We explore two approaches for
acquiring numerical common sense. Both
approaches start with extracting numeri-
cal expressions and their context from the
Web. One approach estimates the distribu-
tion of numbers co-occurring within a con-
text and examines whether a given value is
large, small, or normal, based on the distri-
bution. Another approach utilizes textual
patterns with which speakers explicitly ex-
presses their judgment about the value of
a numerical expression. Experimental re-
sults demonstrate the effectiveness of both
approaches.
1 Introduction
Textual entailment recognition (RTE) involves a
wide range of semantic inferences to determine
whether the meaning of a hypothesis sentence (h)
can be inferred from another text (t) (Dagan et
al., 2006). Although several evaluation campaigns
(e.g., PASCAL/TAC RTE challenges) have made
significant progress, the RTE community recog-
nizes the necessity of a deeper understanding of
the core phenomena involved in textual inference.
Such recognition comes from the ideas that cru-
cial progress may derive from decomposing the
complex RTE task into basic phenomena and from
solving each basic phenomenon separately (Ben-
tivogli et al, 2010; Sammons et al, 2010; Cabrio
and Magnini, 2011; Toledo et al, 2012).
Given this background, we focus on solving one
of the basic phenomena in RTE: semantic infer-
ence related to numerical expressions. The spe-
cific problem we address is acquisition of numeri-
cal common sense. For example,
(1) t : Before long, 3b people will face a water
shortage in the world.
h : Before long, a serious water shortage
will occur in the world.
Although recognizing the entailment relation be-
tween t and h is frustratingly difficult, we assume
this inference is decomposable into three phases:
3b people face a water shortage.
? 3,000,000,000 people face a water shortage.
|= many people face a water shortage.
|= a serious water shortage.
In the first phase, it is necessary to recognize 3b
as a numerical expression and to resolve the ex-
pression 3b into the exact amount 3,000,000,000.
The second phase is much more difficult because
we need subjective but common-sense knowledge
that 3,000,000,000 people is a large number.
In this paper, we address the first and sec-
ond phases of inference as an initial step towards
semantic processing with numerical expressions.
The contributions of this paper are four-fold.
1. We examine instances in existing RTE cor-
pora, categorize them into groups in terms of
the necessary semantic inferences, and dis-
cuss the impact of this study for solving RTE
problems with numerical expressions.
2. We describe a method of normalizing numer-
ical expressions referring to the same amount
in text into a unified semantic representation.
3. We present approaches for aggregating nu-
merical common sense from examples of nu-
merical expressions and for judging whether
a given amount is large, small, or normal.
382
4. We demonstrate the effectiveness of this ap-
proach, reporting experimental results and
analyses in detail. Although it would be ideal
to evaluate the impact of this study on the
overall RTE task, we evaluate each phase sep-
arately. We do this because the existing RTE
data sets tend to exhibit very diverse linguis-
tic phenomena, and it is difficult to employ
such data for evaluating the real impact of
this study.
2 Related work
Surprisingly, NLP research has paid little atten-
tion to semantic processing of numerical expres-
sions. This is evident when we compare with tem-
poral expressions, for which corpora (e.g., ACE-
20051, TimeBank2) were developed with annota-
tion schemes (e.g., TIMEX3, TimeML4).
Several studies deal with numerical expressions
in the context of information extraction (Bakalov
et al, 2011), information retrieval (Fontoura et al,
2006; Yoshida et al, 2010), and question answer-
ing (Moriceau, 2006). Numbers such as prod-
uct prices and weights have been common targets
of information extraction. Fontoura et al (2006)
and Yoshida et al (2010) presented algorithms and
data structures that allow number-range queries
for searching documents. However, these studies
do not interpret the quantity (e.g., 3,000,000,000)
of a numerical expression (e.g., 3b people), but
rather treat numerical expressions as strings.
Banerjee et al (2009) focused on quantity con-
sensus queries, in which there is uncertainty about
the quantity (e.g., weight airbus A380 pounds).
Given a query, their approach retrieves documents
relevant to the query and identifies the quantities
of numerical expressions in the retrieved docu-
ments. They also proposed methods for enumer-
ating and ranking the candidates for the consen-
sus quantity intervals. Even though our study
shares a similar spirit (modeling of consensus for
quantities) with Banerjee et al (2009), their goal
is different: to determine ground-truth values for
queries.
In question answering, to help ?sanity check?
answers with numerical values that were
1http://www.itl.nist.gov/iad/mig/
tests/ace/ace05/
2http://www.timeml.org/site/timebank/
timebank.html
3http://timex2.mitre.org/
4http://timeml.org/site/index.html
way out of common-sense ranges, IBM?s PI-
QUANT (Prager et al, 2003; Chu-Carroll et al,
2003) used information in Cyc (Lenat, 1995).
For example, their question-answering system
rejects 200 miles as a candidate answer for the
height of Mt. Everest, since Cyc knows mountains
are between 1,000 and 30,000 ft. high. They
also consider the problem of variations in the
precision of numbers (e.g., 5 million, 5.1 million,
5,200,390) and unit conversions (e.g., square
kilometers and acres).
Some recent studies delve deeper into the se-
mantic interpretation of numerical expressions.
Aramaki et al (2007) focused on the physical size
of an entity to predict the semantic relation be-
tween entities. For example, knowing that a book
has a physical size of 20 cm ? 25 cm and that a li-
brary has a size of 10 m ? 10 m, we can estimate
that a library contains a book (content-container
relation). Their method acquires knowledge about
entity size from the Web (by issuing queries like
?book (*cm x *cm)?), and integrates the knowl-
edge as features for the classification of relations.
Davidov and Rappoport (2010) presented a
method for the extraction from the Web and ap-
proximation of numerical object attributes such as
height and weight. Given an object-attribute pair,
the study expands the object into a set of compa-
rable objects and then approximates the numerical
values even when no exact value can be found in a
text. Aramaki et al (2007) and Davidov and Rap-
poport (2010) rely on hand-crafted patterns (e.g.,
?Object is * [unit] tall?), focusing on a specific set
of numerical attributes (e.g., height, weight, size).
In contrast, this study can handle any kind of target
and situation that is quantified by numbers, e.g.,
number of people facing a water shortage.
Recently, the RTE community has started to
pay some attention to the appropriate processing
of numerical expressions. Iftene (2010) presented
an approach for matching numerical ranges ex-
pressed by a set of phrases (e.g., more than and at
least). Tsuboi et al (2011) designed hand-crafted
rules for matching intervals expressed by temporal
expressions. However, these studies do not nec-
essarily focus on semantic processing of numeri-
cal expressions; thus, these studies do not normal-
ize units of numerical expressions nor make infer-
ences with numerical common sense.
Sammons et al (2010) reported that most sys-
tems submitted to RTE-5 failed on examples
383
where numeric reasoning was necessary. They ar-
gued the importance of aligning numerical quanti-
ties and performing numerical reasoning in RTE.
LoBue and Yates (2011) identified 20 categories
of common-sense knowledge that are prevalent in
RTE. One of the categories comprises arithmetic
knowledge (including computations, comparisons,
and rounding). They concluded that many kinds
of the common-sense knowledge have received
scarce attention from researchers even though the
knowledge is essential to RTE. These studies pro-
vided a closer look at the phenomena involved in
RTE, but they did not propose a solution for han-
dling numerical expressions.
3 Investigation of textual-entailment
pairs with numerical expressions
In this section, we investigate textual entailment
(TE) pairs in existing corpora in order to study
the core phenomena that establish an entailment
relation. We used two Japanese TE corpora:
RITE (Shima et al, 2011) and Odani et al (2008).
RITE is an evaluation workshop of textual entail-
ment organized by NTCIR-9, and it targets the
English, Japanese, and Chinese languages. We
used the Japanese portions of the development
and training data. Odani et al (2008) is another
Japanese corpus that was manually created. The
total numbers of text-hypothesis (T -H) pairs are
1,880 (RITE) and 2,471 (Odani).
We manually selected sentence pairs in which
one or both of the sentences contained a numerical
expression. Here, we define the term numerical
expression as an expression containing a number
or quantity represented by a numeral and a unit.
For example, 3 kilometers is a numerical expres-
sion with the numeral 3 and the unit kilometer.
Note that intensity of 4 is not a numerical expres-
sion because intensity is not a unit.
We obtained 371 pairs from the 4,351 T -H
pairs. We determined the inferences needed to
prove ENTAILMENT or CONTRADICTION of the
hypotheses, and classified the 371 pairs into 11
categories. Note that we ignored T -H pairs in
which numerical expressions were unnecessary
to prove the entailment relation (e.g., Socrates
was sentenced to death by 500 jury members and
Socrates was sentenced to death). Out of 371
pairs, we identified 114 pairs in which numerical
expressions played a central role in the entailment
relation.
Table 1 summarizes the categories of TE phe-
nomena we found in the data set. The largest cate-
gory is numerical matching (32 pairs). We can in-
fer an entailment relation in this category by align-
ing two numerical expressions, e.g., 2.2 million
|= over 800 thousand. This is the most funda-
mental task in numerical reasoning, interpreting
the amount (number, unit, and range) in a numer-
ical expression. We address this task in Section
4.1. The second largest category requires com-
mon sense about numerical amounts. In order to
recognize textual entailment of pairs in this cat-
egory, we need common-sense knowledge about
humans? subjective judgment of numbers. We
consider this problem in Section 5.
To summarize, this study covers 37.9% of the
instances in Table 1, focusing on the first and sec-
ond categories. Due to space limitations, we omit
the explanations for the other phenomena, which
require such things as lexical knowledge, arith-
metic operations, and counting. The coverage of
this study might seem small, but it is difficult to
handle varied phenomena with a unified approach.
We believe that this study forms the basis for in-
vestigating other phenomena of numerical expres-
sions in the future.
4 Collecting numerical expressions from
the Web
In this paper, we explore two approaches to acquir-
ing numerical common sense. Both approaches
start with extracting numerical expressions and
their context from the Web. We define a context
as the verb and its arguments that appear around a
numerical expression.
For instance, the context of 3b people in the sen-
tence 3b people face a water shortage is ?face?
and ?water shortage.? In order to extract and
aggregate numerical expressions in various doc-
uments, we converted the numerical expressions
into semantic representations (to be described in
Section 4.1), and extracted their context (to be de-
scribed in Section 4.2).
The first approach for acquiring numerical com-
mon sense estimates the distribution of numbers
that co-occur within a context, and examines
whether a given value is large, small, or normal
based on that distribution (to be described in Sec-
tion 5.1). The second approach utilizes textual
patterns with which speakers explicitly expresses
their judgment about the value of a numerical ex-
384
Category Definition Example #
Numerical matching
Aligning numerical expres-
sions in T and H, considering
differences in unit, range, etc.
t: It is said that there are about 2.2 million alcoholics in the whole country.
h: It is estimated that there are over 800 thousand people who are alcoholics. 32
Numerical common sense
Inferring by interpreting the
numerical amount (large or
small).
t: In the middle of the 21st century, 7 billion people, corresponding to 70% of the
global population, will face a water shortage.
h: It is concerning that a serious water shortage will spread around the world in the
near future.
12
Lexical knowledge Inferring by using numericalaspects of word meanings.
t: Mr. and Ms. Sato celebrated their 25th wedding anniversary.
h: Mr. and Ms. Sato celebrated their silver wedding anniversary. 12
Arithmetic Arithmetic operations includ-ing addition and subtraction.
t: The number of 2,000-yen bills in circulation has increased to 450 million, in
contrast with 440 million 5,000-yen bills.
h: The number of 2,000-yen bills in circulation exceeds the number of 5,000-yen
bills by 10 million bills.
11
Numeric-range expression
of verbs
Numerical ranges expressed by
verbs (e.g., exceed).
t: It is recorded that the maximum wave height reached 13.8 meters during the Sea
of Japan Earthquake Tsunami in May 1983.
h: During the Sea of Japan Earthquake, the height of the tsunami exceeded 10meters.
9
Simple Rewrite Rule This includes various simplerules for rewriting.
t: The strength of Taro?s grip is No. 1 in his class.
h: Taro?s grip is the strongest in his class. 7
State change Expressing the change of avalue by a multiplier or ratio.
t: Consumption of pickled plums is 1.5 times the rate of 20 years ago.
h: Consumption of pickled plums has increased. 6
Ordinal numbers Inference by interpreting ordi-nal numbers.
t: Many precious lives were sacrificed in the Third World War.
h: So far, there have been at least three World Wars. 6
Temporal expression
Inference by interpreting tem-
poral expressions such as an-
niversary, age, and ordinal
numbers.
t: Mr. and Ms. Sato celebrate their 25th wedding anniversary.
h: Mr. and Ms. Sato got married 25 years ago. 3
Count Counting up the number of var-ious entities.
t: In Japan, there are the Asian Triopsidae, the American Triopsidae, and the Euro-
pean Triopsidae.
h: In Japan, there are 3 types of Triopsidae.
3
Others 15
All 116
Table 1: Frequency and simple definitions for each category of the entailment phenomena in the survey.
Numerical Semantic representation
Expression Value Unit Mod.
about seven grams 7 g about
roughly 7 kg 7000 g about
as heavy as 7 tons 7? 106 g large
as cheap as $1 1 $ small
30?40 people [30, 40] nin (people)
more than 30 cars 30 dai (cars) over
7 km per hour 7000 m/h
Table 2: Normalized representation examples
pression (to be explained in Section 5.2).
In this study, we acquired numerical common
sense from a collection of 8 billion sentences in
100 million Japanese Web pages (Shinzato et al,
2012). For this reason, we originally designed
text patterns specialized for Japanese dependency
trees. For the sake of the readers? understand-
ing, this paper uses examples with English trans-
lations for explaining language-independent con-
cepts, and both Japanese and English translations
for explaining language-dependent concepts.
4.1 Extracting and normalizing numerical
expressions
The first step for collecting numerical expres-
sions is to recognize when a numerical expression
is mentioned and then to normalize it into a seman-
tic representation. This is the most fundamental
String Operation
gram(s) set-unit: ?g?
kilogram(s) set-unit: ?g?; multiply-value: 1,000
kg set-unit: ?g?; multiply-value: 1,000
ton(s) set-unit: ?g?; multiply-value: 1,000,000
nin (people) set-unit: ?nin? (person)
about set-modifier: ?about?
as many as set-modifier: ?large?
as little as set-modifier: ?small?
Table 3: An example of unit/modifier dictionary
step in numerical reasoning and has a number of
applications. For example, this step handles cases
of numerical matching, as in Table 1.
The semantic representation of a numerical ex-
pression consists of three fields: the value or range
of the real number(s)5, the unit (a string), and the
optional modifiers. Table 2 shows some exam-
ples of numerical expressions and their semantic
representations. During normalization, we identi-
fied spelling variants (e.g., kilometer and km) and
transformed auxiliary units into their correspond-
ing canonical units (e.g., 2 tons and 2,000 kg to
2,000,000 grams). When a numerical expression
is accompanied by a modifier such as over, about,
or more than, we updated the value and modifier
fields appropriately.
5Internally, all values are represented by ranges (e.g., 75
is represented by the range [75, 75]).
385
We developed an extractor and a normalizer for
Japanese numerical expressions6. We will outline
the algorithm used in the normalizer with an exam-
ple sentence: ?Roughly three thousand kilograms
of meats have been provided every day.?
1. Find numbers in the text by using regular ex-
pressions and convert the non-Arabic num-
bers into their corresponding Arabic num-
bers. For example, we find three thousand7
and represent it as 3, 000.
2. Check whether the words that precede or fol-
low the number are units that are registered in
the dictionary. Transform any auxiliary units.
In the example, we find that kilograms8 is a
unit. We multiply the value 3, 000 by 1, 000,
and obtain the value 3, 000, 000 with the unit
g.
3. Check whether the words that precede or fol-
low the number have a modifier that is regis-
tered in the dictionary. Update the value and
modifier fields if necessary. In the example,
we find roughly and set about in the modifier
field.
We used a dictionary9 to perform procedures 2
and 3 (Table 3). If the words that precede or fol-
low an extracted number match an entry in the dic-
tionary, we change the semantic representation as
described in the operation.
The modifiers ?large? and ?small? require elab-
oration because the method in Section 5.2 relies
heavily on these modifiers. We activated the mod-
ifier ?large? when a numerical expression occurred
with the Japanese word mo, which roughly cor-
responds to as many as, as large as, or as heavy
as in English10. Similarly, we activated the modi-
fier ?small? when a numerical expression occurred
with the word shika, which roughly corresponds
to as little as, as small as, or as light as11. These
modifiers are important for this study, reflecting
the writer?s judgment about the amount.
6The software is available at http://www.cl.
ecei.tohoku.ac.jp/?katsuma/software/
normalizeNumexp/
7In Japanese 3, 000 is denoted by the Chinese symbols ?
???.
8We write kilograms as ??????? in Japanese.
9The dictionary is bundled with the tool. See Footnote 6.
10In Japanese, we can use the word mo with a numerical
expression to state that the amount is ?large? regardless of
how large it is (e.g., large, big, many, heavy).
11Similarly, we can use the word shika with any adjective.
?? ??? ??? ????? ????
He gave to a friend$300 at the bank.
Japanese:
English:
nsubj dobj prep_to
prep_at
Number: {value: 300; unit: ?$? }
Context: {verb: ?give? ; nsubj: ?he? ; 
 prep_to: ?friend? ; prep_at: ?bank? }
Figure 1: Example of context extraction
4.2 Extraction of context
The next step in acquiring numerical common
sense is to capture the context of numerical ex-
pressions. Later, we will aggregate numbers that
share the same context (see Section 5). The con-
text of a numerical expression should provide suf-
ficient information to determine what it measures.
For example, given the sentence, ?He gave $300 to
a friend at the bank,? it would be better if we could
generalize the context to someone gives money to
a friend for the numerical expression $300. How-
ever, it is a nontrivial task to design an appropriate
representation of varying contexts. For this rea-
son, we employ a simple rule to capture the con-
text of numerical expressions: we represent the
context with the verb that governs the numerical
expression and its typed arguments.
Figure 1 illustrates the procedure for extracting
the context of a numerical expression12. The com-
ponent in Section 4.1 recognizes $300 as a numer-
ical expression, then normalizes it into a semantic
representation. Because the numerical expression
is a dependent of the verb gave, we extract the verb
and its arguments (except for the numerical ex-
pression itself) as the context. After removing in-
flections and function words from the arguments,
we obtain the context representation of Figure 1.
5 Acquiring numerical common sense
In this section, we present two approaches for ac-
quiring numerical common sense from a collec-
tion of numerical expressions and their contexts.
Both approaches start with collecting the numbers
(in semantic representation) and contexts of nu-
merical expressions from a large number of sen-
tences (Shinzato et al, 2012), and storing them
12The English dependency tree might look peculiar be-
cause it is translated from the Japanese dependency tree.
386
in a database. When a context and a value are
given for a prediction (hereinafter called the query
context and query value, respectively), these ap-
proaches judge whether the query value is large,
small, or normal.
5.1 Distribution-based approach
Given a query context and query value, this
approach retrieves numbers associated with the
query context and draws a distribution of normal-
ized numbers. This approach considers the dis-
tribution estimated for the query context and de-
termines if the value is within the top 5 percent
(large), within the bottom 5 percent (small), or is
located in between these regions (normal).
The underlying assumption of this approach is
that the real distribution of a query (e.g., money
given to a friend) can be approximated by the dis-
tribution of numbers co-occurring with the context
(e.g., give and friend) on the Web. However, the
context space generated in Section 4.2 may be too
sparse to find numbers in the database, especially
when a query context is fine-grained. Therefore,
when no item is retrieved for the query context,
we employ a backoff strategy to drop some of the
uninformative elements in the query context: ele-
ments are dropped from the context based on the
type of argument, in this order: he (prep to), kara
(prep from), ha (nsubj), yori (prep from), made
(prep to), nite (prep at), de (prep at, prep by), ni
(prep at), wo (dobj), ga (nsubj), and verb.
5.2 Clue-based approach
This approach utilizes textual clues with which a
speaker explicitly expresses his or her judgment
about the amount of a numerical expression. We
utilize large and small modifiers (described in Sec-
tion 4.1), which correspond to textual clues mo
(as many as, as large as) and shika (only, as
few as), respectively, for detecting humans? judg-
ments. For example, we can guess that $300 is
large if we find an evidential sentence13, He gave
as much as $100 to a friend.
Similarly to the distribution-based approach,
this approach retrieves numbers associated with
the query context. This approach computes the
13Although the sentence states a judgment about $100, we
can infer that $300 is also large because $300 > $100.
largeness L(x) of a value x:
L(x) = pl(x)ps(x) + pl(x)
, (1)
pl(x) =
??{r|rv < x ? rm 3 large}
??
??{r|rm 3 large}
?? , (2)
ps(x) =
??{r|rv > x ? rm 3 small}
??
??{r|rm 3 small}
?? . (3)
In these equations, r denotes a retrieved item for
the query context, and rv and rm represent the nor-
malized value and modifier flags, respectively, of
the item r. The numerator of Equation 2 counts
the number of numerical expressions that support
the judgment that x is large14, and its denominator
counts the total number of numerical expressions
with large as a modifier. Therefore, pl(x) com-
putes the ratio of times there is textual evidence
that says that x is large, to the total number of
times there is evidences with large as a modifier.
In an analogous way, ps(x) is defined to be the ra-
tio for evidence that says x is small. Hence, L(x)
approaches 1 if everyone on the Web claims that
x is large, and approaches 0 if everyone claims
that x is small. This approach predicts large if
L(x) > 0.95, small if L(x) < 0.05, and normal
otherwise.
6 Experiments
6.1 Normalizing numerical expressions
We evaluated the method that we described in Sec-
tion 4.1 for extracting and normalizing numerical
expressions. In order to prepare a gold-standard
data set, we obtained 1,041 sentences by randomly
sampling about 1% of the sentences containing
numbers (Arabic digits and/or Chinese numerical
characters) in a Japanese Web corpus (100 million
pages) (Shinzato et al, 2012). For every numer-
ical expression in these sentences, we manually
determined a tuple of the normalized value, unit,
and modifier. Here, non-numerical expressions
such as temporal expressions, telephone numbers,
and postal addresses, which were very common,
were beyond the scope of the project15. We ob-
tained 329 numerical expressions from the 1,041
sentences.
We evaluated the correctness of the extraction
and normalization by measuring the precision and
14This corresponds to the events where we find an evidence
expression ?as many as rv?, where rv < x.
15If a tuple was extracted from a non-numerical expres-
sion, we regarded this as a false positive
387
recall using the gold-standard data set16. Our
method performed with a precision of 0.78 and a
recall of 0.92. Most of the false negatives were
caused by the incompleteness of the unit dictio-
nary. For example, the proposed method could not
identify 1Ghz as a numerical expression because
the unit dictionary did not register Ghz but GHz.
It is trivial to improve the recall of the method by
enriching the unit dictionary.
The major cause of false positives was the se-
mantic ambiguity of expressions. For example, the
proposed method identified Seven Hills as a nu-
merical expression although it denotes a location
name. In order to reduce false positives, it may
be necessary to utilize broader contexts when lo-
cating numerical expressions; this could be done
by using, for example, a named entity recognizer.
This is the next step to pursue in future work.
However, these errors do not have a large effect
on the estimation of the distribution of the numer-
ical values that occur with specific named entities
and idiomatic phrases. Moreover, as explained in
Section 5, we draw distributions for fine-grained
contexts of numerical expressions. For these rea-
sons, we think that the current performance is suf-
ficient for acquiring numerical common sense.
6.2 Acquisition of numerical common sense
6.2.1 Preparing an evaluation set
We built a gold-standard data set for numerical
common sense. We applied the method in Sec-
tion 4.1 to sentences sampled at random from the
Japanese Web corpus (Shinzato et al, 2012), and
we extracted 2,000 numerical expressions. We
asked three human judges to annotate every nu-
merical expression with one of six labels, small,
relatively small, normal, relatively large, large,
and unsure. The label relatively small could be
applied to a numerical expression when the judge
felt that the amount was rather small (below the
normal) but hesitated to label it small. The la-
bel relatively large was defined analogously. We
gave the following criteria for labeling an item as
unsure: when the judgment was highly dependent
on the context; when the sentence was incompre-
hensible; and when it was a non-numerical expres-
sions (false positives of the method are discussed
in Section 4.1).
Table 4 reports the inter-annotator agreement.
16All fields (value, unit, modifier) of the extracted tuple
must match the gold-standard data set.
Agreement # expressions
3 annotators 735 (36.7%)
2 annotators 963 (48.2%)
no agreement 302 (15.1%)
Total 2000 (100.0%)
Table 4: Inter-annotator agreement
0"
100"
200"
300"
400"
500"
0"
100"
200"
130" 140" 150" 160" 170" 180" 190" 200" 210"[cm]
distribu7on:based"clue:based(large)"clue:based(small)"
[#"extrac7on]"(distribu7on:based)[#"extrac7on]"(clue:based)
Figure 2: Distributions of numbers with large and
small modifiers for the context human?s height.
For the evaluation of numerical expressions in the
data set, we used those for which at least two anno-
tators assigned the same label. After removing the
unsure instances, we obtained 640 numerical ex-
pressions (20 small, 35 relatively small, 152 nor-
mal, 263 relatively large, and 170 large) as the
evaluation set.
6.2.2 Results
The proposed method extracted about 23 million
pairs of numerical expressions and their context
from the corpus (with 100 million Web pages).
About 15% of the extracted pairs were accom-
panied by either a large or small modifier. Fig-
ure 2 depicts the distributions of the context hu-
man?s height produced by the distribution-based
and clue-based approaches. These distributions
are quite reasonable as common-sense knowledge:
we can interpret that numbers under 150 cm are
perceived as small and those above 180 cm as
large.
We measured the correctness of the proposed
methods on the gold-standard data. For this
evaluation, we employed two criteria for correct-
ness: strict and lenient. With the strict crite-
rion, the method must predict a label identical to
that in the gold-standard. With the lenient crite-
rion, the method was also allowed to predict either
large/small or normal when the gold-standard la-
bel was relatively large/small.
Table 5 reports the precision (P), recall (R), F1
(F1), and accuracy (Acc) of the proposed methods.
388
No. System Gold Sentence Remark
1 small small
I think that three men can
create such a great thing in
the world.
Correct
2 normal normal I have two cats. Correct
3 large large It?s above 32 centigrade. Correct
4 large large I earned 10 million yen fromhorse racing. Correct
5 small normal There are 2 reasons. Difficulty in judging small. Since a few people say, ?There areonly 2 reasons,? our approach predicted a small label.
6 small large
Ten or more people came,
and my eight-mat room was
packed.
Difficulty in modeling the context because this sentence omits
the locational argument for the verb came. We should extract
the context as the number of people who came to my eight-mat
room instead of the number of people who came.
7 small normal
I have two friends who
have broken up with their
boyfriends recently.
Difficulty in modeling the context. We should extract context as
the number of friends who have broken up with their boyfriends
recently instead of the number of friends.
8 small large
Lack of knowledge. We extract the context as the number of
heads of a turtle, but no corresponding information was found
on the Web.
Table 6: Output example and error analysis. We present translations of the sentences, which were origi-
nally in Japanese.
Approach Label P R F1 Acc
large+ 0.892 0.498 0.695
Distribution normal+ 0.753 0.935 0.844 0.760
small+ 0.273 0.250 0.262
large 0.861 0.365 0.613
Distribution normal 0.529 0.908 0.719 0.590
small 0.222 0.100 0.161
large+ 0.923 0.778 0.851
Clue normal+ 0.814 0.765 0.790 0.770
small+ 0.228 0.700 0.464
large 0.896 0.659 0.778
Clue normal 0.593 0.586 0.590 0.620
small 0.164 0.550 0.357
Table 5: Precision (P), recall (R), F1 score (F1),
and accuracy (Acc) of the acquisition of numerical
common sense.
Labels with the suffix ?+? correspond to the lenient
criterion. The clue-based approach achieved 0.851
F1 (for large), 0.790 F1 (for normal), and 0.464
(for small) with the lenient criterion. The perfor-
mance is surprisingly good, considering the sub-
jective nature of this task.
The clue-based approach was slightly better
than the distribution-based approach. In particu-
lar, the clue-based approach is good at predicting
large and small labels, whereas the distribution-
based approach is good at predicting normal la-
bels. We found some targets for which the distri-
bution on the Web is skewed from the ?real? dis-
tribution. For example, let us consider the distri-
bution of the context ?the amount of money that a
person wins in a lottery?. We can find a number
of sentences like if you won the 10-million-dollar
lottery, .... In other words, people talk about a
large amount of money even if they did not win
any money at all. In order to remedy this problem,
we may need to enrich the context representation
by introducing, for example, the factuality of an
event.
6.2.3 Discussion
Table 6 shows some examples of predictions from
the clue-based approach. Because of space limita-
tions, we mention only the false instances of this
approach.
The clue-based approach tends to predict small
even if the gold-standard label is normal. About
half of the errors of the clue-based approach were
of this type; this is why the precision for small and
the recall for normal are low. The cause of this er-
ror is exemplified by the sentence, ?there are two
reasons.? Human judges label normal to the nu-
merical expression two reasons, but the method
predicts small. This is because a few people say
there are only two reasons, but no one says there
are as many as two reasons. In order to handle
these cases, we may need to incorporate the distri-
bution information with the clue-based approach.
We found a number of examples for which
modeling the context is difficult. Our approach
represents the context of a numerical expression
with the verb that governs the numerical expres-
sion and its typed arguments. However, this ap-
proach sometimes misses important information,
especially when an argument of the verb is omit-
ted (Example 6). The approach also suffers from
the relative clause in Example 7, which conveys an
essential context of the number. These are similar
to the scope-ambiguity problem such as encoun-
389
tered with negation and quantification; it is diffi-
cult to model the scope when a numerical expres-
sion refers to a situation.
Furthermore, we encountered some false exam-
ples even when we were able to precisely model
the context. In Example 8, the proposed method
was unable to predict the label correctly because
no corresponding information was found on the
Web. The proposed method might more easily pre-
dict a label if we could generalize the word turtle
as animal. It may be worth considering using lan-
guage resources (e.g., WordNet) to generalize the
context.
7 Conclusions
We proposed novel approaches for acquiring nu-
merical common sense from a collection of texts.
The approaches collect numerical expressions and
their contexts from the Web, and acquire numeri-
cal common sense by considering the distributions
of normalized numbers and textual clues such as
mo (as many as) and shika (only, as few as). The
experimental results showed that our approaches
can successfully judge whether a given amount
is large, small, or normal. The implementations
and data sets used in this study are available on
the Web17. We believe that acquisition of numer-
ical common sense is an important step towards a
deeper understanding of inferences with numbers.
There are three important future directions for
this research. One is to explore a more sophis-
ticated approach for precisely modeling the con-
texts of numbers. Because we confirmed in this
paper that these two approaches have different
characteristics, it would be interesting to incorpo-
rate textual clues into the distribution-based ap-
proach by using, for example, machine learning
techniques. Finally, we are planning to address the
?third phase? of the example explained in Section
1: associating many people face a water shortage
with a serious water shortage.
Acknowledgments
This research was partly supported by JST,
PRESTO. This research was partly supported by
JSPS KAKENHI Grant Numbers 23240018 and
23700159.
17http://www.cl.ecei.tohoku.ac.jp/
?katsuma/resource/numerical common sense/
References
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and
Kazuhiko Ohe. 2007. Uth: Svm-based semantic
relation classification using physical sizes. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 464?467.
Anton Bakalov, Ariel Fuxman, Partha Pratim Talukdar,
and Soumen Chakrabarti. 2011. SCAD: collective
discovery of attribute values. In Proceedings of the
20th international conference on World wide web,
WWW ?11, pages 447?456.
Somnath Banerjee, Soumen Chakrabarti, and Ganesh
Ramakrishnan. 2009. Learning to rank for quantity
consensus queries. In Proceedings of the 32nd inter-
national ACM SIGIR conference on Research and
development in information retrieval, SIGIR ?09,
pages 243?250.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo
Giampiccolo, Medea Lo Leggio, and Bernardo
Magnini. 2010. Building textual entailment special-
ized data sets: a methodology for isolating linguis-
tic phenomena relevant to inference. Proceedings of
the Seventh International Conference on Language
Resources and Evaluation, pages 3542?3549.
Elena Cabrio and Bernardo Magnini. 2011. Towards
component-based textual entailment. In Proceed-
ings of the Ninth International Conference on Com-
putational Semantics, IWCS ?11, pages 320?324.
Jennifer Chu-Carroll, David A. Ferrucci, John M.
Prager, and Christopher A. Welty. 2003. Hybridiza-
tion in question answering systems. In New Direc-
tions in Question Answering?03, pages 116?121.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine Learning Challenges. Eval-
uating Predictive Uncertainty, Visual Object Classi-
fication, and Recognising Tectual Entailment, pages
177?190.
Dmitry Davidov and Ari Rappoport. 2010. Extrac-
tion and approximation of numerical attributes from
the web. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1308?1317.
Marcus Fontoura, Ronny Lempel, Runping Qi, and Ja-
son Zien. 2006. Inverted index support for numeric
search. Internet Mathematics, 3(2):153?185.
Adrian Iftene and Mihai-Alex Moruz. 2010. UAIC
participation at RTE-6. In Proceedings of the Third
Text Analysis Conference (TAC 2010) November.
Douglas B Lenat. 1995. Cyc: A large-scale investment
in knowledge infrastructure. Communications of the
ACM, 38(11):33?38.
Peter LoBue and Alexander. Yates. 2011. Types of
common-sense knowledge needed for recognizing
390
textual entailment. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers-Volume 2, pages 329?334.
Ve?ronique Moriceau. 2006. Generating intelligent
numerical answers in a question-answering system.
In Proceedings of the Fourth International Natural
Language Generation Conference, INLG ?06, pages
103?110.
Michitaka Odani, Tomohide Shibata, Sadao Kurohashi,
and Takayuki Nakata. 2008. Building data of
japanese text entailment and recognition of infer-
encing relation based on automatic achieved similar
expression. In Proceeding of 14th Annual Meeting
of the Association for ?atural Language Processing,
pages 1140?1143.
John M. Prager, Jennifer Chu-Carroll, Krzysztof
Czuba, Christopher A. Welty, Abraham Ittycheriah,
and Ruchi Mahindru. 2003. IBM?s PIQUANT in
TREC2003. In TREC, pages 283?292.
Mark Sammons, Vinod V.G. Vydiswaran, and Dan
Roth. 2010. Ask not what textual entailment can do
for you... In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1199?1208.
Hideki Shima, Hiroshi Kanayama, Cheng-Wei Lee,
Chuan-Jie Lin, Teruko Mitamura, Yusuke Miyao,
Shuming Shi, and Koichi Takeda. 2011. Overview
of ntcir-9 rite: Recognizing inference in text. In Pro-
ceeding of NTCIR-9 Workshop Meeting, pages 291?
301.
Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,
and Sadao Kurohashi. 2012. Tsubaki: An open
search engine infrastructure for developing informa-
tion access methodology. Journal of Information
Processing, 20(1):216?227.
Assaf Toledo, Sophia Katrenko, Stavroula Alexan-
dropoulou, Heidi Klockmann, Asher Stern, Ido Da-
gan, and Yoad Winter. 2012. Semantic annotation
for textual entailment recognition. In Proceedings of
the 11th Mexican International Conference on Arti-
ficial Intelligence, MICAI ?12.
Yuta Tsuboi, Hiroshi Kanayama, Masaki Ohno, and
Yuya Unno. 2011. Syntactic difference based ap-
proach for ntcir-9 rite task. In Proceedings of the
9th NTCIR Workshop, pages 404?411.
Minoru Yoshida, Issei Sato, Hiroshi Nakagawa, and
Akira Terada. 2010. Mining numbers in text using
suffix arrays and clustering based on dirichlet pro-
cess mixture models. Advances in Knowledge Dis-
covery and Data Mining, pages 230?237.
391
Proceedings of the ACL Student Research Workshop, pages 110?116,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Detecting Chronic Critics Based on
Sentiment Polarity and User?s Behavior in Social Media
Sho Takase? Akiko Murakami? Miki Enoki? Naoaki Okazaki? Kentaro Inui?
Tohoku University? IBM Research - Tokyo?
{takase, okazaki, inui}@ecei.tohoku.ac.jp
{akikom, enomiki}@jp.ibm.com
Abstract
There are some chronic critics who al-
ways complain about the entity in social
media. We are working to automatically
detect these chronic critics to prevent the
spread of bad rumors about the reputation
of the entity. In social media, most com-
ments are informal, and, there are sarcas-
tic and incomplete contexts. This means
that it is difficult for current NLP technol-
ogy such as opinion mining to recognize
the complaints. As an alternative approach
for social media, we can assume that users
who share the same opinions will link to
each other. Thus, we propose a method
that combines opinion mining with graph
analysis for the connections between users
to identify the chronic critics. Our ex-
perimental results show that the proposed
method outperforms analysis based only
on opinion mining techniques.
1 Introduction
On a social media website, there may be millions
of users and large numbers of comments. The
comments in social media are related to the real
world in such fields as marketing and politics. An-
alyzing comments in social media has been shown
to be effective in predicting the behaviors of stock
markets and of voters in elections (Bollen et al,
2011; Tumasjan et al, 2010; O?Connor et al,
2010). Because of their effects on the real world,
some complaints may harm the reputation of a cor-
poration or an individual and cause serious dam-
age. Consider a comment such as ?Working for
Company A is really awful? as an example. The
complaint gives viewers a negative impression of
Company A and can increase the number of people
who think the company is bad.
Some complaints are expressed by a specific
user who is always criticizing a specific target en-
tity (in this example, Company A). We call this
user a chronic critic of that entity, a person who
is deliberately trying to harm the reputation of the
entity. That is, a chronic critic is trying to run a
negative campaign against the entity. If the entity
is aware of its own chronic critics, then it is able
to take prompt action to stop the malicious com-
plaints. When the complaints are false, the entity
can use that defense. In contrast, if the chronic
critics are justified, then the entity should address
the concerns to limit the damage. Hence, to han-
dle malicious rumors, it is important to detect the
chronic critics.
However, it is generally quite difficult for a
computer to detect a chronic critic?s comments,
since especially the comments in social media are
often quite informal. In addition, there are com-
plexities such as sarcasm and incomplete contexts.
For example, if Company A has been involved in a
widely recognized fiasco, then some chronic crit-
ics might sarcastically write ?good job? or ?won-
derful? about Company A. They are using posi-
tive words, but in the context they are effectively
criticizing Company A. Some chronic critics bash
a target entity solely with sarcasm, so they dam-
age the target with positive words. It is exceed-
ingly difficult to directly detect these chronic crit-
ics based on their comments. In an example of
an incomplete context, if one author starts an ex-
change with a comment such as ?The new prod-
uct from Company A is difficult to use? and an-
other user responds with something like ?Fool?,
we cannot easily recognize the meaning of this
comment as related to ?Company A being foolish
because the product really is difficult to use? or
whether ?the user is the fool because the product
is easy for other people to use?. To find chronic
critics for a given entity, we need to identify the
actual target of the complaints. Take the comment
?Company B is much worse than Company A? for
110
example. This comment is probably complaining
about Company B but not Company A. In contrast,
most of the previous work on sentiment analysis
in social media does not consider these kinds of
problems (Barbosa and Feng, 2010; Davidov et
al., 2010; Speriosu et al, 2011).
Switching to the behavior of each user, in so-
cial media we often see that users who have sim-
ilar ideas will tend to cooperate with each other.
In fact, previous work suggests that users who
have the same opinions tend to create links to each
other (Conover et al, 2011b; Yang et al, 2012).
Because chronic critics share the purpose of at-
tacking some target?s reputation, they may also
decide to cooperate. For this reason, to detect
chronic critics, we believe that information about
the connections among users will be effective.
In this paper, we present a method that com-
bines opinion mining based on NLP and graph
analysis of the connections among users to rec-
ognize the chronic critics. In the experiments, we
demonstrate the difficulty in detecting chronic crit-
ics by analyzing only the individual comments. In
addition, we investigate the effectiveness of using
the connections between users, i.e., using the pro-
posed method. For our experiments, we used Twit-
ter, a popular social media service. In particular,
we focus on Japanese comments on Twitter.
This paper is organized as follows. Section 2
reviews related work. Section 3 presents the pro-
posed method which applies the opinion mining
and graph analysis. Section 4 demonstrates the ef-
fectiveness of the proposed method and discusses
the experimental results. Section 5 concludes this
paper.
2 Related Work
In recent years, an interest in opinion mining in
online communities has emerged (Conover et al,
2011a; O?Connor et al, 2010; Speriosu et al,
2011; Murakami and Raymond, 2010; Barbosa
and Feng, 2010; Davidov et al, 2010). O?Connor
et al (2010), Barbosa and Feng (2010), Davidov
et al (2010), and Speriosu et al (2011) proposed
methods to predict a sentiment polarity (i.e., pos-
itive or negative) of a comment in social media.
O?Connor et al (2010) studied a subjectivity lexi-
con. Barbosa and Feng (2010) and Davidov et al
(2010) used machine learning approaches. Spe-
riosu et al (2011) introduced connections between
words, emoticons, tags, n-grams, comments and
users. These studies did not identify the target of
the polarized sentiment of each comment.
Conover et al (2011a) proposed a method that
predicts the political polarity of a social media
user based on the connections between users and
tags. They demonstrated that label propagation
on the graph representing the connections between
users is effective. However, this method is not
guaranteed to obtain the optimal solution. In con-
trast, our research uses graph analysis that con-
verges on the optimal solution.
Murakami and Raymond (2010) proposed a
method that uses the connections between users
to predict each user?s opinion, i.e., support or op-
pose a topic in online debates. They analyzed
the content of the discussions to infer the connec-
tions. However, in social media, it is difficult to in-
fer connections based on content because of such
complexities as incomplete contexts. To address
these problem, we analyzed the behavior of the
users to predict the connections between users.
Our task is similar to spammer detection (Wang,
2010; Yang et al, 2012). Wang (2010) pro-
posed a method using a classifier to detect spam-
mers. They used the content in the comments
and the number of linked users as features. Yang
et al (2012) analyzed spammer communities and
demonstrated that spammers closely link to each
other in social media. They also proposed a
method that extracts spammers using the connec-
tions between users. WhileWang (2010) and Yang
et al (2012) required manually annotated data for
training or as seeds, we extract the seeds for the
graph analysis automatically through opinion min-
ing.
3 Proposed Method
Figure 1 presents an overview of the proposed
method. The proposed method has two phases,
opinion mining and graph analysis. First, we ex-
tract a few chronic critics by analyzing the opin-
ions of many users referencing the target entity.
For the opinion mining, we are initially looking
for users who strongly criticize the target entity. In
Figure 1, given Company A as a target entity, we
find users ?b? and ?e? since they said ?Working
for Company A is really awful? and ?This product
from Company A is useless?. However, we may
miss the other chronic critics since they used sar-
casm and incomplete contexts.
Next, we find the users who are linked to the
111
????
Opinion mining
Making graph Company A is very very nice.I use only products of Company A.
I like products of Company A.
The attitude of Company A is perfect.
Company A always makes shitty produces.
Working for Company A is really awful.
Comments in social media
Company B is much worse than Company A.
A product of Company A is wonderful.
Fiasco of Company A occurs almost every day.
Company A, good job!
The product from Company A is difficult to use.
This product from Company A is useless.
Company A is a bad company.
Why do people praise Company A?a
d
c
b
g
f
e
An account of a target entity x
Working for Company A is really awful.Company A always makes shitty produces.
A product of Company A is useless.A new product of Company A is difficult to use.
Chronic critics extracted by the opinion mining
b
e
The graph based on a relationship of agreements  Results of the graph analysis
Graph analysis 
Chronic critics
a
d
c
b
g
f
e
a
d
c
b
g
f
e
x x
?
Figure 1: Overview of proposed method
chronic critics that were detected through opinion
mining. We built a graph in which the users are
represented by nodes and the links between the
users are represented by edges.We recognize ad-
ditional chronic critics based on the graph anal-
ysis. In the example of Figure 1, we find more
chronic critics not recognized by the opinion min-
ing, such as ?a? and ?c?, because they are linked
to the chronic critics ?b? and ?e?. In this section,
we explain the opinion mining and graph analysis.
Since a comment in Twitter is called a tweet, we
use the term tweet below.
3.1 Opinion Mining
As defined in Section 1, we defined a user who
frequently criticizes a target entity as a chronic
critic. Therefore, we classify the tweets of each
user into critical or non-critical and label any users
who complain about the target entity many times
as chronic critics. Because we want to investi-
gate the opinions of each user in public, we an-
alyze public tweets, excluding the private conver-
sations between users. In Twitter, this means we
ignore a reply that is a response to a specific user
named username (written in the format ?@user-
name response?) and QT that is a mention in a
quoted tweet from username (written in the format
?mention RT @username: quoted tweet?).
We assume a phrase representing negative po-
larity or profanity to be critical phrases. The pro-
posed method determines whether a tweet com-
plains about the target entity by investigating a
critical phrase and the target of the phrase.
Note that a negative polarity is represented by
declinable words or substantives. We used the
sentiment analyzer created by Kanayama and Na-
sukawa (2012) to detect a phrase representing neg-
   A??????????????
Working for   Company A is    really    awful.
Figure 2: Example of critic tweet
ative polarity by using declinable words. We used
the lexicon collected by Higashiyama et al (2008)
to find negative polarity in substantives. For de-
tecting profanity, we use a profane lexicon col-
lected by Ogino et al (2012).
The sentiment analyzer can find not only senti-
ment phrases but the targets of the phrases based
on syntactic parsing and the case frames1. How-
ever, because there are many informal tweets and
because most users omit the grammatical case in
tweets, the sentiment analyzer often fails to cap-
ture any target. To address this problem, in ad-
dition to a target extracted by the sentiment ana-
lyzer, we obtain a target based on the dependency
tree. We extract nouns in parent and child phrases
within distance 2 from a critical phrase in the de-
pendency tree.
Figure 2 shows an example of a Japanese tweet
criticizing Company A and its English translation.
The Japanese tweet is split into phrase-like units
(bunsetsu). Each English phrase is linked to the
corresponding bunsetsu by a dotted line. The de-
pendency relationships among the bunsetsu are ex-
pressed by the arrows. In the tweet, the black-
edged phrase ?awful? is a critical phrase. We ex-
tract the nouns in ?Working for? and ?Company
A is? as targets of the critical phrase since these
1A case frame is a list which represents grammatical cases
of a predicate.
112
phrases are parents within distance 2 of the criti-
cal phrase. Therefore, we decide that the tweet is
criticizing Company A.
Since a chronic critic frequently complains
about the target entity, we can predict that most
of the tweets written by a chronic critic of the tar-
get entity will be critical tweets. Therefore, we
can calculate a ratio of critical tweets for all of the
tweets about the target entity. We score the user ui
with equation (1).
scorei =
ni
Ni
(1)
Ni is the number of all tweets about the target en-
tity and ni is the number of critical tweets about
the entity by that user 2. We extract the top M
users based on scorei as chronic critics.
3.2 Graph Analysis
In social media, it is often very difficult to deter-
mine whether a tweet is critical since many tweets
include sarcasm or incomplete contexts. The opin-
ion mining may miss numerous complaints with
sarcasm or incomplete contexts. To resolve this
problem, we apply user behaviors. In social me-
dia, we assume that users having the same opinion
interact with each other in order to demonstrate the
correctness of their opinion. In particular, since
the purpose of chronic critics is to spread the bad
reputation, we assume that they want to assist each
other. We supplement the opinion mining by a
graph analysis using this assumption. Thus, we
make a graph representing connections among the
users and use label propagation on the graph based
on the results of the opinion mining as the seeds.
In addition, we believe that a user will try to
spread user matching opinions. This implies that a
user who spreads the opinion of another of agrees
with the author of that opinion. In Twitter, a user
can spread an opinion as an RT, which is a repost-
ing of a tweet by a username (written in the format
?RT @username: tweet?). Conover et al (2011b)
demonstrated that they can make a graph repre-
senting the connections among users who support
each others opinions by using RTs. Hence, an RT
expresses a relationship of endorsement. We also
created a graph based on this feature.
Our graph has m users (U = {u1, ..., um}) as
nodes, where ui connects with uj via an edge that
2The formula (1) assigns a high score to a user if the user
only produces one or two tweets about the target entity and
those tweets are negative. To prevent this, we disregard the
users whose the number of tweets are fewer than 5.
has weightwij (0 ? wij ? 1) andwij corresponds
to the degree to which ui supports uj . We calcu-
late wij by using Equation (2).
wij =
1
2
(
rij
Ri
+ rjiRj
)
(2)
rij is the total RT tweets of uj by ui and Ri is the
number of RTs by ui. Therefore, the more ui and
uj RT each other, the more weight wij is close to
1. In contrast, if ui and uj rarely RT each other, the
value of wij will approach 0. In addition, this wij
definition is symmetric means (i.e., wij = wji).
We find more new chronic critics by label prop-
agation on the graph. We use the chronic critics
obtained by the opinion mining as seeds. It is as-
sumed that a user who supports the target entity is
not a chronic critic. Using this knowledge, we use
the account of the target entity as a seed.
The label propagation assigns a confidence
score c = (c1, ..., cm) to each node U =
u1, ..., um, where the score is a real number be-
tween ?1 and 1. A score close to 1 indicates
that we are very confident that the node (user) is
a chronic critic. A score close to ?1 indicates that
we are sure that the node is not a chronic critic. In
addition, the scores of seeds are fixed and cannot
be changed. The scores of chronic critics obtained
by the opinion mining are 1 and the score of the
target entity is set to ?1. To formulate the label
propagation as an optimization problem, we used
the loss function proposed by Zhu et al (2003),
because wij ? 0 for all i, j.
E(c) = 12
?
i,j
wij(ci ? cj)2 (3)
To minimize E(c), ci is close to cj when wij
is greater than 0. That is, if the users support
each other, the scores of the users are close to
each other. Thus, by minimizing E(c), we as-
sign the confidence scores considering the results
of the opinion mining and agreement relationships
among the users. We find the users that have
scores greater than the threshold.
We believe that if the distance between users on
the graph is large, then users slightly support each
other. However, we can assign a score of 1 to each
node in any subgraph that has chronic critics ex-
tracted by the opinion mining to minimize E(c)
if the subgraph does not include the account of
the target entity, no matter how far away a node
113
Table 1: Properties of the experimental datasets
Target entity Tweets Critics Kappa
Company A 35,807 112 0.81
Politician A 45,378 254 1.0
is from the seeds. To avoid this problem, Yin and
Tan (2011) introduced a neutral fact, which de-
creases each confidence score by considering the
distance from the seeds. The neutral fact has a
fixed confidence score 0 and connects with all of
the nodes except the seeds. Suppose u1 is the neu-
tral fact, Ul = {u2, ..., ul} is the set of seeds and
Ut = {ul+1, ..., um} is the set of all nodes except
seeds. To assign the weight of the edge between
u1 and other nodes considering the degrees of the
nodes, we calculate the weight by as:
w1i =
{
0 i = 1, ..., l
??j>1 |wij | i = l + 1, ...,m
(4)
where ? is a small constant. Thus, the weight is
proportional to the total weight of the edges from
each node.
4 Experiments
4.1 Experimental Setting
For our experiment, we gathered tweets by using
the Twitter search API. The twitter search API re-
turns the tweets that contain an input query. We
used the name of a target entity, words related to
the entity3, and the account name of the entity as
queries. In this research, there were two target
entities, Company A and Politician A. We found
many critical tweets about these target entities.
The entities have their own accounts in Twitter.
We collected the Japanese tweets for one month.
We want to extract the users who frequently ex-
press a public opinion related to a target entity.
For this reason, we eliminated users whose the
number of tweets except conversation (i.e., reply,
QT, RT) are fewer than 5. In addition, to elimi-
nate bots that automatically post specific tweets,
we eliminated users whose conversational tweets
were fewer than 2. We selected some of the re-
maining users for the experiment. To satisfy our
definition, a chronic critic must tweet about the
target entity many times. Therefore, we focused
3We manually prepared the words that have a correlation
with the entity. In this paper, we only used the name of the
political party of Politician A as the related word.
on the top 300 users based on the number of tweets
as our experimental users. Table 1 shows the total
numbers of tweets by the top 300 users, excluding
the account of the target entity.
We created an evaluation set by manually di-
viding the experimental users into chronic critics
and regular users. A chronic critic actively com-
plained and tried to harm the reputation of the
target entity. We also regarded a user who fre-
quently reposted a critic?s tweets and unfavorable
news about the target entity as a chronic critic. For
the experimental users tweeting aboutCompany A,
we asked two human annotators to judge whether
a user was a chronic critic based on one month of
tweets. The Cohen?s kappa value was 0.81 which
inter-annotator agreement was good. We selected
the arbitrarily annotating by one of the annotators
as our evaluation set. Table 1 expresses the num-
ber of chronic critics for each target entity in the
evaluation set. For the experimental users tweet-
ing about Politician A, we randomly extracted 50
users randomly to calculate Cohen?s kappa, which
is displayed in Table 1.
We evaluated the effects of combining the opin-
ion mining with the graph analysis. We compared
opinion mining (OM), graph analysis (GA), and
the combination of opinion mining and graph anal-
ysis (our proposed method). GA randomly se-
lected M users from experimental users as seeds
and takes the average of the results obtained by
performing label propagation three times. The
number of chronic critics extracted by the opinion
mining (i.e., the valuable M ) was set to 30. The
parameter ?, that we use to calculate the weight of
the edges connected to neutral fact, was set to 0.1.
4.2 Results
Figure 3 represents the precision and recall of each
method for each target entity. In OM, we varied
the threshold from 0 to 0.2 in increments of 0.02
and accepted a user with a score over the threshold
as a chronic critic. In GA, we varied the threshold
from 0.35 to 0.8 in increments of 0.05.
In Figure 3, the results for Company A and
Politician A are quite different, though there are
some similar characteristics. Figure 3 shows that
OM achieved high precision but it was difficult to
improve the recall. In contrast, GA easily achieved
high recall. The proposed method achieved high
precision similar to OM and high recall. In
other words, the proposed method found many
114
!"#$%!"&%
!"&$%!"$%
!"$$%!"'%
!"'$%!"(%
!"($%!")%
!")$%
!% !"*% !"+% !"#% !"&% !"$% !"'% !"(% !")% !",% *%
Prec
ision

Recall
OM GA The proposed method 
(a) Company A
!"#$
!"#%$
!"&$
!"&%$
'$
!$ !"'$ !"($ !")$ !"*$ !"%$ !"+$ !",$ !"#$ !"&$ '$
Prec
ision

Recall
OM GA The proposed method 
(b) Politician A
Figure 3: Precision and recall of each method for each target entity
Table 2: Users connected with the target entity
Target entity Users Non-critics
Company A 45 39
Politician A 74 35
chronic critics while retaining high precision of
OM. Therefore, the combination of the opinion
mining and the graph analysis improved the per-
formance of recognizing the chronic critics.
Figure 3 shows that the recall of OM was low,
which means that OM missed some of the critical
tweets. In this paper, we used domain-independent
lexicons to detect the critical phrases. Therefore,
OM failed to find domain-dependent critic phrases
such as slang words. In addition, some chronic
critics do not express criticism clearly in their own
tweets. To spread the bad reputation, they refer-
ence only a title and link to a webpage that criti-
cizes the target entity such as:
This shows the reality of Company A.
Why do you buy products from this
company? http://xxx
We believe that is often done because each tweet is
limited to 140 characters. It is difficult to classify
the tweet as a complaint based only on its content.
However, the proposed method recognized most
chronic critics that complain with these methods
based on the GA.
It cannot reasonably be assumed that a user
who supports the account of the target entity is a
chronic critic. For this reason, in the graph analy-
sis, we used the entity?s account to recognize non-
critics. We believe that using the account corrects
for mistakes in selecting the seed chronic critics.
Table 2 shows the number of users connected with
the account. Table 2 also shows the number of
non-critics among the users. As seen in Table 2,
many non-critics were connected with the account.
Especially for Politician A, most of the non-critics
in the evaluation set were connected with the ac-
count. Therefore, incorporating the account into
the graph analysis can correct for errors in the
seeding of chronic critics. However, some chronic
critics were connected with the target?s account
and reposted tweets from the account. We noticed
that they mentioned their negative opinions about
the content of such a tweet immediately after re-
posting that tweet. Hence, we need to analyze the
contexts before and after each RT.
For Politician A, Table 1 shows that most of the
users in the evaluation set criticized the politician.
We were able to find most of the chronic critics
by extracting the users linked to each other. How-
ever, for Company A, the precision of GA was low.
This means we need high accuracy in selecting the
seeds to correctly capture chronic critics. Because
we used the users extracted by the opinion mining
as the seeds, the proposed method outperformed
OM and GA.
5 Conclusion
In this paper, we proposed a method that uses not
only opinion mining but graph analysis of the con-
nections between users to detect chronic critics.
In our experiments, we found that the proposed
method outperformed each technique.
In our study, we used two entities. To im-
prove reliability, we should study more entities.
We used a relationship between users that support
each other. However, we suspect that the rela-
tionship includes adversaries. We hope to address
these topics in the future.
115
Acknowledgments
This research was partly supported by JSPS KAK-
ENHI Grant Numbers 23240018. The authors
would like to acknowledge Hiroshi Kanayama and
Shiho Ogino in IBM Research-Tokyo for provid-
ing their tools for our experiments.
References
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 36?44.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1?8.
Michael D. Conover, Bruno Gonc?alves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011a. Predicting the Political Alignment
of Twitter Users. In Proceedings of the 3rd IEEE
Conference on Social Computing, pages 192?199.
Michael D. Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Alessandro Flammini, and
Filippo Menczer. 2011b. Political Polarization on
Twitter. In Proceeding of the 5th International AAAI
Conference on Weblogs and Social Media, pages
89?96.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced Sentiment Learning Using Twitter Hash-
tags and Smileys. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 241?249.
Masahiko Higashiyama, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Learning Polarity of Nouns by Se-
lectional Preferences of Predicates (in Japanese). In
Proceedings of the 14th Annual Meeting of The As-
sociation for Natural Language Processing, pages
584?587.
Hiroshi Kanayama and Tetsuya Nasukawa. 2012. Un-
supervised Lexicon Induction for Clause-Level De-
tection of Evaluations. Natural Language Engineer-
ing, 18(1):83?107.
Akiko Murakami and Rudy Raymond. 2010. Support
or Oppose? Classifying Positions in Online Debates
from Reply Activities and Opinion Expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 869?875, Beijing,
China.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
4h International AAAI Conference on Weblogs and
Social Media, pages 122?129.
Shiho Ogino, Tetsuya Nasukawa, Hiroshi Kanayama,
and Miki Enoki. 2012. Knowledge Discovery Us-
ing Swearwords (in Japanese). In Proceedings of the
8th Annual Meeting of The Association for Natural
Language Processing, pages 58?61.
Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-
son Baldridge. 2011. Twitter Polarity Classification
with Label Propagation over Lexical Links and the
Follower Graph. In Proceedings of the 1st workshop
on Unsupervised Learning in NLP, pages 53?63.
Andranik Tumasjan, Timm O. Sprenger, Philipp G.
Sandner, and Isabell M. Welpe. 2010. Predicting
Elections with Twitter: What 140 Characters Reveal
about Political Sentiment. In Proceedings of the 4th
International AAAI Conference on Weblogs and So-
cial Media, pages 178?185.
Alex Hai Wang. 2010. Don?t Follow Me - Spam De-
tection in Twitter. In Proceedings of the 5th Inter-
national Conference on Security and Cryptography,
pages 142?151.
Chao Yang, Robert Harkreader, Jialong Zhang, Seung-
won Shin, and Guofei Gu. 2012. Analyzing Spam-
mers? Social Networks for Fun and Profit: A Case
Study of Cyber Criminal Ecosystem on Twitter. In
Proceedings of the 21st international conference on
World Wide Web, pages 71?80.
Xiaoxin Yin and Wenzhao Tan. 2011. Semi-
Supervised Truth Discovery. In Proceedings of the
20th international conference on World Wide Web,
pages 217?226.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-Supervised Learning Using Gaussian
Fields and Harmonic Functions. In Proceedings
of the 20th International Conference on Machine
Learning, pages 912?919.
116
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 65?73,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Automatic Acquisition of Huge Training Data
for Bio-Medical Named Entity Recognition
Yu Usami? ? Han-Cheol Cho? Naoaki Okazaki? and Jun?ichi Tsujii?
?Aizawa Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Tsujii Laboratory, Department of Computer Science, The University of Tokyo, Tokyo, Japan
? Inui Laboratory, Department of System Information Sciences, Tohoku University, Sendai, Japan
? Microsoft Research Asia, Beijing, China
{yusmi, hccho}@is.s.u-tokyo.ac.jp
okazaki@ecei.tohoku.ac.jp
jtsujii@microsoft.com
Abstract
Named Entity Recognition (NER) is an im-
portant first step for BioNLP tasks, e.g., gene
normalization and event extraction. Employ-
ing supervised machine learning techniques
for achieving high performance recent NER
systems require a manually annotated corpus
in which every mention of the desired seman-
tic types in a text is annotated. However, great
amounts of human effort is necessary to build
and maintain an annotated corpus. This study
explores a method to build a high-performance
NER without a manually annotated corpus,
but using a comprehensible lexical database
that stores numerous expressions of seman-
tic types and with huge amount of unanno-
tated texts. We underscore the effectiveness of
our approach by comparing the performance
of NERs trained on an automatically acquired
training data and on a manually annotated cor-
pus.
1 Introduction
Named Entity Recognition (NER) is the task widely
used to detect various semantic classes such as
genes (Yeh et al, 2005), proteins (Tanabe and
Wilbur, 2002), and diseases in the biomedical field.
A na??ve approach to NER handles the task as a
dictionary-matching problem: Prepare a dictionary
(gazetteer) containing textual expressions of named
entities of specific semantic types. Scan an input
text, and recognize a text span as a named entity if
the dictionary includes the expression of the span.
Although this approach seemingly works well, it
presents some critical issues. First, the dictionary
must be comprehensive so that every NE mention
can be found in the dictionary. This requirement
for dictionaries is stringent because new terminol-
ogy is being produced continuously, especially in
the biomedical field. Second, this approach might
suffer from an ambiguity problem in which a dic-
tionary includes an expression as entries for multi-
ple semantic types. For this reason, we must use
the context information of an expression to make
sure that the expression stands for the target seman-
tic type.
Nadeau and Sekine (2007) reported that a strong
trend exists recently in applying machine learning
(ML) techniques such as Support Vector Machine
(SVM) (Kazama et al, 2002; Isozaki and Kazawa,
2002) and Conditional Random Field (CRF) (Set-
tles, 2004) to NER, which can address these issues.
In this approach, NER is formalized as a classifi-
cation problem in which a given expression is clas-
sified into a semantic class or other (non-NE) ex-
pressions. Because the classification problem is usu-
ally modeled using supervised learning methods, we
need a manually annotated corpus for training NER
classifier. However, preparing manually annotated
corpus for a target domain of text and semantic types
is cost-intensive and time-consuming because hu-
man experts are needed to reliably annotate NEs in
text. For this reason, manually annotated corpora
for NER are often limited to a specific domain and
covers a small amount of text.
In this paper we propose a novel method for au-
tomatically acquiring training data for NER from a
comprehensible lexical database and huge amounts
of unlabeled text. This paper presents four contribu-
65
Gene or Protein name
Official name
Aliases
References
Figure 1: Example of an Entrez Gene record.
tions:
1. We show the ineffectiveness of a na??ve
dictionary-matching for acquiring a training
data automatically and the significance of the
quality of training data for supervised NERs
2. We explore the use of reference information
that bridges the lexical database and unlabeled
text for acquiring high-precision and low-recall
training data
3. We develop two strategies for expanding NE
annotations, which improves the recall of the
training data
4. The proposed method acquires a large amount
of high-quality training data rapidly, decreasing
the necessity of human efforts
2 Proposed method
The proposed method requires two resources to ac-
quire training data automatically: a comprehen-
sive lexical database and unlabeled texts for a tar-
get domain. We chose Entrez Gene (National Li-
brary of Medicine, 2005) as the lexical database be-
cause it provides rich information for lexical entries
and because genes and proteins constitute an im-
portant semantic classes for Bio NLP. Entrez Gene
consists of more than six million gene or protein
records, each of which has various information such
as the official gene (protein) name, synonyms, or-
ganism, description, and human created references.
Figure 1 presents an example of an Entrez Gene
record. We created a dictionary by collecting offi-
cial gene (protein) names and their synonyms from
the Entrez Gene records. For unlabeled text, we use
the all 2009 release MEDLINE (National Library
of Medicine, 2009) data. MEDLINE consists of
about ten million abstracts covering various fields of
biomedicine and health. In our study, we focused on
recognizing gene and protein names within biomed-
ical text.
Our process to construct a NER classifier is as fol-
lows: We apply the GENIA tagger (Tsuruoka et al,
2005) to split the training data into tokens and to at-
tach part of speech (POS) tags and chunk tags. In
this work, tokenization is performed by an external
program that separates tokens by a space, hyphen,
comma, period, semicolon, or colon character. Part
of speech tags present grammatical roles of tokens,
e.g. verbs, nouns, and prepositions. Chunk tags
compose tokens into syntactically correlated seg-
ments, e.g. verb phrases, noun phrases, and preposi-
tional phrases. We use the IOBES notation (Ratinov
and Roth, 2009) to represent NE mentions with label
sequences, thereby NER is formalized as a multi-
class classification problem in which a given token
is classified into IOBES labels. To classify labels of
tokens, we use a linear kernel SVM which applies
the one-vs.-the-rest method (Weston and Watkins,
1999) to extend binary classification to multi-class
classification. Given the t-th token xt in a sentence,
we predict the label yt,
yt = argmax
y
s(y|xt, yt?1).
In this equation, s(y|xt, yt?1) presents the score
(sum of feature weights) when the token xt is la-
beled y. We use yt?1 (the label of the previous to-
ken) to predict yt, expecting that this feature behaves
as a label bigram feature (also called translation fea-
ture) in CRF. If the sentence consists of x1 to xT , we
repeat prediction of labels sequentially from the be-
ginning (y1) to the end (yT ) of a sentence. We used
LIBLINEAR (Fan et al, 2008) as an SVM imple-
mentation.
Table 1 lists the features used in the classifier
modeled by SVM. For each token (?Human? in the
example of Table 1), we created several features in-
cluding: token itself (w), lowercase token (wl), part
of speech (pos), chunk tag (chk), character pattern of
66
Name Description Example Value
w token Human
wl token in small letters human
pos part of speech NNP
chk chunk tag B-NP
shape entity pattern ULLLL
shaped entity pattern 2 UL
type token type InitCap
pn(n = 1...4) prefix n characters (H,Hu,Hum,Huma)
sn(n = 1...4) suffix n characters (n,an,man,uman)
Table 1: Example of features used in machine learning
process.
token (shape), character pattern designated (shaped),
token type (type), prefixes of length n (pn), and suf-
fixes of length n (sn). More precisely, the character
pattern of token (shape) replaces each character in
the token with either an uppercase letter (U), a low-
ercase letter (L), or a digit (D). The character pat-
tern designated (shaped) is similar to a shape feature,
but the consecutive character types are reduced to
one symbol, for example, ?ULLLL? (shape) is rep-
resented with ?UL? (shaped) in the example of Ta-
ble 1). The token type (type) represents whether the
token satisfies some conditions such as ?begins with
a capital letter?, ?written in all capitals?, ?written
only with digits?, or ?contains symbols?. We created
unigram features and bigram features (excluding wl,
pn, sn) from the prior 2 to the subsequent 2 tokens
of the current position.
2.1 Preliminary Experiment
As a preliminary experiment, we acquired training
data using a na??ve dictionary-matching approach.
We obtained the training data from all 2009 MED-
LINE abstracts with an all gene and protein dictio-
nary in Entrez Gene. The training data consisted of
nine hundred million tokens. We constructed a NER
classifier using only four million tokens of the train-
ing data because of memory limitations. For evalua-
tion, we used the Epigenetics and Post-translational
Modification (EPI) corpus BioNLP 2011 Shared
Task (SIGBioMed, 2011). Only development data
and training data are released as the EPI corpus at
present, we used both of the data sets for evalua-
tion in this experiment. Named entities in the corpus
are annotated exhaustively and belong to a single se-
mantic class, Gene or Gene Product (GGP) (Ohta
et al, 2009). We evaluated the performance of the
Method A P R F1
dictionary matching 92.09 39.03 42.69 40.78
trained on acquired data 85.76 10.18 23.83 14.27
Table 2: Results of the preliminary experiment.
(a) It is clear that in culture media of AM,
cystatin C and cathepsin B are present as
proteinase?antiproteinase complexes.
(b) Temperature in the puerperium is higher
in AM, and lower in PM.
Figure 2: Dictionary-based gene name annotating exam-
ple (annotated words are shown in italic typeface).
NER on four measures: Accuracy (a), Precision (P),
Recall (R), and F1-measure (F1). We used the strict
matching criterion that a predicted named entity is
correct if and only if the left and the right bound-
aries are both correct.
Table 2 presents the evaluation results of this ex-
periment. The first model ?dictionary matching?
performs exact dictionary-matching on the test cor-
pus. It achieves a 40.78 F1-score. The second model
?trained on acquired data? uses the training data
acquired automatically for constructing NER clas-
sifier. It scores very low-performance (14.27 F1-
score), even compared with the simple dictionary-
matching NER. Exploring the annotated training
data, we investigate why this machine learning ap-
proach shows extremely low performance.
Figure 2 presents an example of the acquired
training data. The word ?AM? in the example (a)
is correct because it is gene name, although ?AM?
in the example (b) is incorrect because ?AM? in (b)
is the abbreviation of ante meridiem, which means
before noon. This is a very common problem, espe-
cially with abbreviations and acronyms. If we use
this noisy training data for learning, then the result
of NER might be low because of such ambiguity. It
is very difficult to resolve errors in the training data
even with the help of machine learning methods.
2.2 Using Reference Information
To obtain high-precision data, we used reference in-
formation included with each record in Entrez Gene.
Figure 3 portrays a simple example of reference in-
formation. It shows the reference information of the
67
 PMID 1984484: 
 It is clear that in culture media of AM, 
cystatin C and cathepsin B are present as 
proteinase-antiproteinase complexes.
Gene: AM
Entrez Gene Records
MEDLINE Abstracts
 PMID 23456:
 Temperature in puerperium is higher in AM, 
lower in PM.
Reference
Figure 3: Reference to MEDLINE abstract example.
Entrez Gene record which describes that the gene
?AM?. The reference information indicates PMIDs
in which the gene or protein is described.
We applied the rule whereby we annotated a
dictionary-matching in each MEDLINE abstract
only if they were referred by the Entrez Gene
records. Figure 3 shows that the gene ?AM? has
reference to the MEDLINE abstract #1984484 only.
Using this reference information between the En-
trez Gene record ?AM? and the MEDLINE abstract
#1984484, we can annotate the expansion ?AM? in
MEDLINE abstract #1984484 only. In this way, we
can avoid incorrect annotation such as example b in
Figure 2.
We acquired training data automatically using ref-
erence information, as follows:
1. Construct a gene and protein dictionary includ-
ing official names, synonyms and reference in-
formation in Entrez Gene
2. Apply a dictionary-matching on the all MED-
LINE abstracts with the dictionary
3. Annotate the MEDLINE abstract only if it was
referred by the Entrez Gene records which de-
scribe the matched expressions
We obtained about 48,000,000 tokens of training
data automatically by using this process using all the
2009 MEDLINE data. This training data includes
about 3,000,000 gene mentions.
? ... in the following order: tna, gltC, gltS,
pyrE; gltR is located near ...
? The three genes concerned (designated
entA, entB and entC) ...
? Within the hypoglossal nucleus large
amounts of acetylcholinesterase (AChE)
activity are ...
Figure 4: False negative examples.
2.3 Training Data Expansion
In the previous section, we were able to obtain train-
ing data with high-precision by exploiting reference
information in the Entrez Gene. However, the result-
ing data include many false negatives (low-recall),
meaning that correct gene names in the data are
unannotated. Figure 4 presents an example of miss-
ing annotation. In this figure, all gene mentions
are shown in italic typeface. The underlined en-
tities were annotated by using the method in Sec-
tion 2.2, because they were in the Entrez Gene dic-
tionary and this MEDLINE abstract was referred by
these entities. However, the entities in italic type-
face with no underline were not annotated, because
these gene names in Entrez Gene have no link to
this MEDLINE abstract. Those expressions became
false negatives and became noise for learning. This
low-recall problem occurred because no guarantee
exists of exhaustiveness in Entrez Gene reference in-
formation.
To improve the low-recall while maintaining
high-precision, we focused on coordination struc-
tures. We assumed that coordinated noun phrases
belong to the same semantic class. Figure 5 portrays
the algorithm for the annotation expansion based
on coordination analysis. We expanded training
data annotation using this coordination analysis al-
gorithm to improve annotation recall. This algo-
rithm analyzes whether the words are reachable or
not through coordinate tokens such as ?,?, ?.?, or
?and? from initially annotated entities. If the words
are reachable and their entities are in the Entrez
Gene records (ignoring reference information), then
they are annotated.
68
Input: Sequence of sentence tokens S, Set of
symbols and conjunctions C, Dictionary with-
out reference D, Set of annotated tokens A
Output: Set of Annotated tokens A
begin
for i = 1 to |S| do
if S[i] ? A then
j ? i? 2
while 1 ? j ? |S| ? S[j] ? D ? S[j] /?
A ? S[j + 1] ? C do
A? A ? {S[j]}
j ? j ? 2
end while
j ? i + 2
while 1 ? j ? |S| ? S[j] ? D ? S[j] /?
A ? S[j ? 1] ? C do
A? A ? {S[j]}
j ? j + 2
end while
end if
end for
Output A
end
Figure 5: Coordination analysis algorithm.
2.4 Self-training
The method described in Section 2.3 reduces false
negatives based on coordination structures. How-
ever, the training data contain numerous false neg-
atives that cannot be solved through coordination
analysis. Therefore, we used a self-training algo-
rithm to automatically correct the training data. In
general, a self-training algorithm obtains training
data with a small amount of annotated data (seed)
and a vast amount of unlabeled text, iterating this
process (Zadeh Kaljahi, 2010):
1. Construct a classification model from a seed,
then apply the model on the unlabeled text.
2. Annotate recognized expressions as NEs.
3. Add the sentences which contain newly anno-
tated expressions to the seed.
In this way, a self-training algorithm obtains a huge
amount of training data.
Input: Labeled training data D, Machine
learning algorithm A, Iteration times n,
Threshold ?
Output: Training data Tn
begin
T0 ? A seed data from D
i? 0
D ? D\T0
while i 6= n do
Mi ? Construct model with Ti
U ? Sample some amount of data from D
L? Annotate U with model Mi
Unew ?Merge U with L if their confidence
values are larger than ?
Ti+1 ? Ti ? Unew
D ? D\U
i? i + 1
end while
Output Tn
end
Figure 6: Self-training algorithm.
In contrast, our case is that we have a large
amount of training data with numerous false neg-
atives. Therefore, we adapt a self-training algo-
rithm to revise the training data obtained using the
method described in Section 2.3. Figure 6 shows
the algorithm. We split the data set (D) obtained in
Section 2.3 into a seed set (T0) and remaining set
(D\T0). Then, we iterate the cycle (0 ? i ? n):
1. Construct a classification model (Mi) trained
on the training data (Ti).
2. Sample some amount of data (U ) from the re-
maining set (D).
3. Apply the model (Mi) on the sampled data (U ).
4. Annotate entities (L) recognized by this model.
5. Merge newly annotated expressions (L) with
expressions annotated in Section 2.3 (U ) if
their confidence values are larger than a thresh-
old (?).
6. Add the merged data (Unew) to the training data
(Ti).
69
In this study, we prepared seed data of 683,000 to-
kens (T0 in Figure 6). In each step, 227,000 tokens
were sampled from the remaining set (U ).
Because the remaining set U has high precision
and low recall, we need not revise NEs that were
annotated in Section 2.3. It might lower the qual-
ity of the training data to merge annotated entities,
thus we used confidence values (Huang and Riloff,
2010) to revise annotations. Therefore, we retain the
NE annotations of the remaining setU and overwrite
a span of a non-NE annotation only if the current
model predicts the span as an NE with high confi-
dence. We compute the confidence of the prediction
(f(x)) which a token x is predicted as label y as,
f(x) = s(x, y)?max(?z 6=ys(x, z)).
Here, s(x, y) denotes the score (the sum of feature
weights) computed using the SVM model described
in the beginning of Section 2. A confidence score
presents the difference of scores between the pre-
dicted (the best) label and the second-best label. The
confidence value is computed for each token label
prediction. If the confidence value is greater than
a threshold (?) and predicted as an NE of length 1
token (label S in IOBES notation), then we revise
the NE annotation. When a new NE with multiple
tokens (label B, I, or E in IOBES notation) is pre-
dicted, we revise the NE annotation if the average
of confidence values is larger than a threshold (?).
If a prediction suggests a new entity with multiple
tokens xi, ..., xj , then we calculate the average of
confidence values as
f(xi, ..., xj) =
1
j ? i + 1
j
?
k=i
f(xk).
The feature set presented in the beginning of Sec-
tion 2 uses information of the tokens themselves.
These features might overfit the noisy seed set, even
if we use regularization in training. Therefore, when
we use the algorithm of Figure 6, we do not gen-
erate token (w) features from tokens themselves but
only from tokens surrounding the current token. In
other words, we hide information from the tokens of
an entity, and learn models using information from
surrounding words.
Method A P R F1
dictionary matching 92.09 39.03 42.69 40.78
svm 85.76 10.18 23.83 14.27
+ reference 93.74 69.25 39.12 50.00
+ coordination 93.97 66.79 47.44 55.47
+ self-training 93.98 63.72 51.18 56.77
Table 3: Evaluation results.
3 Experiment
The training data automatically generated using the
proposed method have about 48,000,000 tokens and
3,000,000 gene mentions. However, we used only
about 10% of this data because of the computational
cost. For evaluation, we chose to use the BioNLP
2011 Shared Task EPI corpus and evaluation mea-
sures described in Section 2.1.
3.1 Evaluation of Proposed Methods
In the previous section, we proposed three methods
for automatic training data acquisition. We first in-
vestigate the effect of these methods on the perfor-
mance of NER. Table 3 presents evaluation results.
The first method ?dictionary matching? simply
performs exact string matching with the Entrez Gene
dictionary on the evaluation corpus. It achieves a
40.78 F1-measure; this F1-measure will be used as
the baseline performance. The second method, as
described in Section 2.1, ?svm? uses training data
generated automatically from the Entrez Gene and
unlabeled texts without reference information of the
Entrez Gene. The third method, ?+ reference? ex-
ploits the reference information of the Entrez Gene.
This method drastically improves the performance.
As shown in Table 3, this model achieves the highest
precision (69.25%) with comparable recall (39.12%)
to the baseline model with a 50.00 F1-measure. The
fourth method, ?+ coordination?, uses coordination
analysis results to expand the initial automatic an-
notation. Compared to the ?+ reference? model, the
annotation expansion based on coordination analy-
sis greatly improves the recall (+8.32%) with only
a slight decrease of the precision (-2.46%). The
last method ?+ self-training? applies a self-training
technique to improve the performance further. This
model achieves the highest recall (51.18%) among
all models with a reasonable cost in the precision.
70
Figure 7: Results of self-training.
To analyze the effect of self-training, we evalu-
ated the performance of this model for each itera-
tion. Figure 7 shows the F1-measure of the model
as iterations increase. The performance improved
gradually. It did not converge even for the last iter-
ation. The size of the training data at the 17th itera-
tion was used in Table 3 experiment. It is the same
to the size of the training data for other methods.
3.2 Comparison with a Manually Annotated
Corpus
NER systems achieving state-of-the-art performance
are based mostly on supervised machine learn-
ing trained on manually annotated corpus. In
this section, we present a comparison of our best-
performing NER model with a NER model trained
on manually annotated corpus. In addition to the
performance comparison, we investigate how much
manually annotated data is necessary to outperform
our best-performing system. In this experiment, we
used only the development data for evaluation be-
cause the training data are used for training the NER
model.
We split the training data of EPI corpus randomly
into 20 pieces and evaluated the performance of
the conventional NER system as the size of manu-
ally annotated corpus increases. Figure 8 presents
the evaluation results. The performance of our our
best-performing NER is a 62.66 F1-measure; this
is shown as horizontal line in Figure 8. The NER
model trained on the all training data of EPI cor-
Figure 8: Manual annotation vs. our method.
pus achieves a 67.89 F1-measure. The result shows
that our best-performing models achieve compara-
ble performance to that of the NER model when us-
ing about 40% (60,000 tokens, 2,000 sentences) of
the manually annotated corpus.
3.3 Discussion
Although the proposed methods help us to obtain
training data automatically with reasonably high
quality, we found some shortcomings in these meth-
ods. For example, the annotation expansion method
based on coordination analysis might find new enti-
ties in the training data precisely. However, it was
insufficient in the following case.
tna loci, in the following order: tna, gltC,
gltS, pyrE; gltR is located near ...
In this example, all gene mentions are shown in
italic typeface. The words with underline were ini-
tial annotation with reference information. The sur-
rounding words represented in italic typeface are an-
notated by annotation expansion with coordination
analysis. Here, the first word ?tna? shown in italic
typeface in this example is not annotated, although
its second mention is annotated at the annotation ex-
pansion step. We might apply the one sense per dis-
course (Gale et al, 1992) heuristic to label this case.
Second, the improvement of self-training tech-
niques elicited less than a 1.0 F1-measure. To as-
certain the reason for this small improvement, we
analyzed the distribution of entity length both origi-
71
Original
Added
0% 25% 50% 75% 100%
Length 1 Length 2 Length 3 More than 4
Figure 9: Distribution of entity length.
nally included entities and newly added entities dur-
ing self-training, as shown in Figure 9. They repre-
sent the ratio of entity length to the number of total
entities. Figure 9 shows the added distribution of
entity length (Added) differs from the original one
(Original). Results of this analysis show that self-
training mainly annotates entities of the length one
and barely recognizes entities of the length two or
more. It might be necessary to devise a means to fol-
low the corpus statistics of the ratio among the num-
ber of entities of different length as the self-training
iteration proceeds.
4 Related Work
Our study focuses mainly on achieving high per-
formance NER without manual annotation. Several
previous studies aimed at reducing the cost of man-
ual annotations.
Vlachos and Gasperin (2006) obtained noisy
training data from FlyBase1 with few manually an-
notated abstracts from FlyBase. This study sug-
gested the possibility of acquiring high-quality train-
ing data from noisy training data. It used a boot-
strapping method and a highly context-based classi-
fiers to increase the number of NE mentions in the
training data. Even though the method achieved a
high-performance NER in the biomedical domain, it
requires curated seed data.
Whitelaw et al (2008) attempted to create ex-
tremely huge training data from the Web using a
seed set of entities and relations. In generating train-
ing data automatically, this study used context-based
tagging. They reported that quite a few good re-
sources (e.g., Wikipedia2) listed entities for obtain-
ing training data automatically.
1http://flybase.org/
2http://www.wikipedia.org/
Muramoto et al (2010) attempted to create train-
ing data from Wikipedia as a lexical database and
blogs as unlabeled text. It collected about one mil-
lion entities from these sources, but they did not re-
port the performance of the NER in their paper.
5 Conclusions
This paper described an approach to the acquisi-
tion of huge amounts of training data for high-
performance Bio NER automatically from a lexical
database and unlabeled text. The results demon-
strated that the proposed method outperformed
dictionary-based NER. Utilization of reference in-
formation greatly improved its precision. Using co-
ordination analysis to expand annotation increased
recall with slightly decreased precision. Moreover,
self-training techniques raised recall. All strategies
presented in the paper contributed greatly to the
NER performance.
We showed that the self-training algorithm
skewed the length distribution of NEs. We plan
to improve the criteria for adding NEs during self-
training. Although we obtained a huge amount of
training data by using the proposed method, we
could not utilize all of acquired training data be-
cause they did not fit into the main memory. A fu-
ture direction for avoiding this limitation is to em-
ploy an online learning algorithm (Tong and Koller,
2002; Langford et al, 2009), where updates of fea-
ture weights are done for each training instance. The
necessity of coordination handling and self-training
originates from the insufficiency of reference infor-
mation in the lexical database, which was not de-
signed to be comprehensive. Therefore, establish-
ing missing reference information from a lexical
database to unlabeled texts may provide another so-
lution for improving the recall of the training data.
References
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 233?237.
72
Ruihong Huang and Ellen Riloff. 2010. Inducing
domain-specific semantic class taggers from (almost)
nothing. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
275?285.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient sup-
port vector classifiers for named entity recognition. In
Proceedings of the 19th international conference on
Computational linguistics - Volume 1, pages 1?7.
Jun?ichi Kazama, Takaki Makino, Yoshihiro Ohta, and
Jun?ichi Tsujii. 2002. Tuning support vector ma-
chines for biomedical named entity recognition. In
Proceedings of the ACL-02 workshop on Natural lan-
guage processing in the biomedical domain - Volume
3, pages 1?8.
John Langford, Lihong Li, and Tong Zhang. 2009.
Sparse online learning via truncated gradient. J. Mach.
Learn. Res., 10:777?801.
Hideki Muramoto, Nobuhiro Kaji, Naoki Suenaga, and
Masaru Kitsuregawa. 2010. Learning semantic cat-
egory tagger from unlabeled data. In The Fifth NLP
Symposium for Yung Researchers. (in Japanese).
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Lingvisti-
cae Investigationes, 30(1):3?26.
National Library of Medicine. 2005. Entrez Gene. avail-
able at http://www.ncbi.nlm.nih.gov/gene.
National Library of Medicine. 2009. MEDLINE. avail-
able at http://www.ncbi.nlm.nih.gov/.
Tomoko Ohta, Jin-Dong Kim, Sampo Pyysalo, Yue
Wang, and Jun?ichi Tsujii. 2009. Incorporating
genetag-style annotation to genia corpus. In Proceed-
ings of the Workshop on Current Trends in Biomedical
Natural Language Processing, pages 106?107.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, pages 147?155.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of the International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications, pages 104?107.
SIGBioMed. 2011. BioNLP 2011 Shared Task.
http://sites.google.com/site/bionlpst/.
Lorraine K. Tanabe and W. John Wilbur. 2002. Tagging
gene and protein names in biomedical text. Bioin-
formatics/computer Applications in The Biosciences,
18:1124?1132.
Simon Tong and Daphne Koller. 2002. Support vector
machine active learning with applications to text clas-
sification. J. Mach. Learn. Res., 2:45?66.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun ?ichi Tsujii. 2005. Developing a robust part-
of-speech tagger for biomedical text. In Advances in
Informatics, volume 3746, pages 382?392.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and evaluating named entity recognition in
the biomedical domain. In Proceedings of the HLT-
NAACL BioNLP Workshop on Linking Natural Lan-
guage and Biology, pages 138?145.
Jason Weston and Chris Watkins. 1999. Support vec-
tor machines for multi-class pattern recognition. In
ESANN?99, pages 219?224.
Casey Whitelaw, Alex Kehlenbeck, Nemanja Petrovic,
and Lyle Ungar. 2008. Web-scale named entity recog-
nition. In Proceeding of the 17th ACM conference on
Information and knowledge management, pages 123?
132.
Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. Biocreative task 1a: gene
mention finding evaluation. BMC Bioinformatics,
6(1):S2.
Rasoul Samad Zadeh Kaljahi. 2010. Adapting self-
training for semantic role labeling. In Proceedings of
the ACL 2010 Student Research Workshop, pages 91?
96.
73
LAW VIII - The 8th Linguistic Annotation Workshop, pages 70?74,
Dublin, Ireland, August 23-24 2014.
A Corpus Study for Identifying Evidence on Microblogs
Paul Reisert
1
Junta Mizuno
2
Miwa Kanno
1
Naoaki Okazaki
1,3
Kentaro Inui
1
1
Gradute School of Information Sciences, Tohoku University / Miyagi, Japan
2
Resilient ICT Research Center, NICT / Miyagi, Japan
3
Japan Science and Technology Agency (JST) / Tokyo, Japan
preisert@ecei.tohoku.ac.jp junta-m@nict.go.jp {meihe, okazaki, inui}@ecei.tohoku.ac.jp
Abstract
Microblogs are a popular way for users to communicate and have recently caught the attention
of researchers in the natural language processing (NLP) field. However, regardless of their rising
popularity, little attention has been given towards determining the properties of discourse rela-
tions for the rapid, large-scale microblog data. Therefore, given their importance for various NLP
tasks, we begin a study of discourse relations on microblogs by focusing on evidence relations.
As no annotated corpora for evidence relations on microblogs exist, we conduct a corpus study
to identify such relations on Twitter, a popular microblogging service. We create annotation
guidelines, conduct a large-scale annotation phase, and develop a corpus of annotated evidence
relations. Finally, we report our observations, annotation difficulties, and data statistics.
1 Introduction
Microblogs have become a popular method for users to express their ideas and communicate with other
users. Twitter
1
, a popular microblogging service, has recently been the attraction of many natural lan-
guage processing (NLP) tasks ranging from flu epidemic detection (Aramaki et al., 2011) to gender
inference for its users (Ciot et al., 2013). While various tasks are available, despite its daily, rapid large-
scale data, evidence relation studies have yet to be explored using Twitter data. Previous research exists
for determining the credibility of information on Twitter (Castillo et al., 2011); however, the focus of this
work is to determine and annotate evidence relations on microblogs.
Our primary motivation behind focusing on evidence relations includes the possibility of discovering
support for a claim which can support the debunking of false information. During the March 2011
Great East Japan Earthquake and Tsunami disaster, victims turned to the Internet in order to obtain
information on current conditions, such as family member whereabouts, refuge center information, and
general information (Sakaki et al., 2011). However, false information, such as the popular Cosmo Oil
explosion causing toxic rain, interfered with those looking to find correct information on the status of
the disaster areas (Okazaki et al., 2013). This is a scenario in which identification of potentially false
information is necessary in order to provide accurate information to victims and others relying on and
trusting in the Internet. Therefore, as a start to find support for counterclaims for false information such
as the Cosmo Oil explosion, we focus on dialogue between two individuals: a topic starter, or a post
with no parent; and a respondent who provides either an agreeing or disagreeing claim and support for
their claim. An example is provided in Figure 1.
We note that our task can appear similar to the field of Why-QA (Verberne, 2006; Oh et al., 2013;
Mrozinski et al., 2008), which attempts to discover the answer for Why questions. Given our task of
discovering agreeing or conflicting claims, and finding specific reasoning to support the claim, we end
up with a Why question similar to Why is it true/not true that X, where X is the contents of the claim
found in the parent post. However, we consider source mentions or hyperlinks, which can either stand
alone or be contained in a statement, question, or request, as a way to answer the above question.
To the best of our knowledge, no corpora for evidence relations on microblogs currently exists. In
terms of argumentation corpora, the Araucaria Argumentation Corpus
2
exists which utilizes various
argumentation schemes (Walton, 1996; Katzav and Reed, 2004; Pollock, 1995). In this work, we
1
https://twitter.com
2
http://araucaria.computing.dundee.ac.uk/doku.php
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
70
# 