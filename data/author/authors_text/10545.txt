BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 106?107,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Temporal Annotation of Clinical Text  Danielle L. Mowery MS, Henk Harkema PhD, Wendy W. Chapman PhD Department of Biomedical Informatics University of Pittsburgh, Pittsburgh, PA 15260, USA dlm31@pitt.edu, heh23@pitt.edu, wec6@pitt.edu    Abstract 
We developed a temporal annotation schema that provides a structured method to capture contextual and temporal features of clinical conditions found in clinical reports. In this poster we describe the elements of the annota-tion schema and provide results of an initial annotation study on a document set compris-ing six different types of clinical reports.  
1 Introduction Distinguishing between historical and recent con-ditions is important for most tasks involving re-trieval of patients or extraction of information from textual clinical records. Various approaches can be used to determine whether a condition is historical or recent. Chapman et al (2007) developed an al-gorithm called ConText that uses trigger terms like ?history? to predict whether a condition is histori-cal. Studies of ConText show that this approach is inadequate for determining whether a condition is historical, achieving recall of 67% and precision 74% on emergency department reports. Temporal modeling methods commonly reason about the temporality of an event with respect to absolute time and other temporally related events (Zhou et al, 2006; Chambers et al, 2007). Knowing the relative or absolute time the condition occurred can be useful in determining whether the condition is historical. However, we hypothesize that many clinical conditions in clinical reports are not modi-fied by explicit temporal references. To test this hypothesis and explore other types of information that may be useful in automatically distinguishing historical from recent clinical condi-tions in dictated clinical records, we developed a temporal annotation schema that accounts for ex-plicit temporal expressions, temporal trigger terms, 
and clinical reporting acts described in reports. Three annotators applied the schema to six types of reports. We measured inter-annotator agreement scores and obtained prevalence and distribution figures for the three annotation types. 
2 Methods 2.1 Dataset Our dataset is comprised of 24 clinical reports of six types dictated at the University of Pittsburgh Medical Center during 2007: discharge summaries, surgical pathology, radiology, echocardiograms, operative gastrointestinal, and emergency depart-ment reports. A physician pre-annotated the 518 clinical conditions in the reports and marked each one as recent or historical. We developed our annotation schema using one of each report type (six reports). Annotators (authors HH, DM and WC) annotated the remain-ing 18 reports as described below.  2.2 Annotation Schema  For our temporal annotation study, each pre-annotated clinical condition was annotated with three types of information: temporal expression, trigger term, and clinical reporting act. The set of temporal expressions (TEs) is taken from Zhou et al (2006) and includes categories such as DATE AND TIME for explicit TEs and KEY EVENTS for TEs relative to significant clinical events. A given clinical condition is annotated with the category of the TE it is modified by. For exam-ple, in the sentence ?The stroke occurred on 1/5/2000?, the condition ?stroke? is annotated with category DATE AND TIME. There is also a category NO TEMPORAL EXPRESSION for annotating condi-tions that are not linked to a TE. Trigger terms (TTs) are explicit signals (words and phrases) in text other than TEs that indicate 
106
whether a condition is recent or historical (Chap-man et al, 2007). If a condition co-occurs with a TT, it is annotated with TRIGGER: YES. For exam-ple, ?pneumonia? in the sentence ?Films indicate pneumonia, which is new for this patient? is anno-tated as TRIGGER: YES because ?new? is a TT.  Error analyses of our previous studies indicate that the context in which a condition is mentioned in a report is potentially useful for prediction of a condition as recent or historical. Clinical reports consist of statements that group into segments ac-cording to the clinical reporting act (CRA) they describe, such as noting a past history and consid-ering a diagnosis. CRAs are tightly correlated with report sections; however, sections are not consis-tent, and different CRAs can occur within a single section. We distinguish 16 CRAs. Each clinical condition is annotated with one CRA. For exam-ple, the condition ?smoker? in the sentence ?She was a smoker? is annotated SOCIAL HISTORY.  2.3 Analysis  To establish the level of inter-annotator agreement, we iteratively annotated groups of six reports (one of each type). After each iteration, we refined our annotation schema and guidelines. We analyzed annotations, overall and by report type, in the fol-lowing way: 1) calculate inter-annotator kappa score, 2) measure prevalence of TT and TE catego-ries, and 3) observe distribution of CRAs. 3 Results and Discussion As shown in figure 1, average inter-annotator scores as measured by Cohen's kappa for TE, TT, and CRA (.68, .82 and .72 respectively) reached acceptable levels after three iterations and are ex-pected to rise further with increased annotation experience and understanding of the guidelines. Table 1 shows the prevalence of TEs and TTs across six report types, where prevalence is defined as the frequency of TE or TT found in a given re-port. Use of TEs across report types ranged from 0% to 52% whereas TTs were found less often at 0% to 34% by report genre. Table 2 plots the cor-relation between the CRA assigned to a clinical condition and the condition's classification as re-cent or historical. We found that there is a strong correlation for the most commonly occurring clini-cal reporting acts (PH, PR, and PO). We are there-fore optimistic that CRAs can serve as an 
informative feature for a statistical recent/historical classifier. 
kappa 
0
1
1 2 3
i t e r a t i o n
TE
TT
CRA
 Figure 1. Average Cohen?s kappa agreement for 3 iterations.   DS E ED GI RAD SP O TE 48(52) 0(0) 51(20) 2(10) 1(5) 8(36) 110(21) TT 32(34) 0(0) 54(21) 1(5) 0(0) 6(27) 93(17)  Table 1. Prevalence, count (%), of TE and TT across report types, overall. DS: discharge summary, E: echocardiogram, ED: emergency department, GI: operative gastrointestinal, RAD: radiology, SP: surgical pathology and O: overall.   
0%
100%
 
P
H
P
R
H
P
I
P
O
A
l
l
 
C
C
S
H
P
F
P
M
x
D
x
P
T
x
M
d
x
R
P
R
M
D
C
D
x
C R A
Recent
Historical
 Table 2. Historical/recent distribution of CRAs. PH: Past his-tory, PR, Patient reporting, HPI: History of present illness, PO: Physician observing, All: Allergies, CC: Chief complaint, SH: Social history, FH: Family history, PF: Past Finding, PMx, Past medication, Dx: Diagnosis, PTx: Plan treatment, Mdx: Prescribing medication, RP: Referring problem, RMD: Refer to MD, CDx: Considering diagnosis.  The finding that many conditions are associated with neither a TE nor a TT and study of ConText?s limitations with such categories at the scope of the sentence suggests that additional features are nec-essary to discern a condition as recent or historical. Whereas temporality in discourse may follow a sequential chronology as narrative unfolds, refer-ences to past instances within clinical text are not easily resolved. We are optimistic that CRAs may help this issue and will focus our study to evaluate whether these three features are sufficient together. References  L. Zhou, G. B. Melton, S. Parsons, G. Hripcsak. 2006. A temporal constraint structure for extracting tempo-ral information from clinical narrative. Journal of Biomedical Informatics, 39(4):424-439 N. Chambers, S. Wang, D. Jurafsky. 2007. Classifying Temporal Relations Between Events. In: ACL-07.  W. Chapman, D. Chu, J. N. Dowling. 2007. ConText: An Algorithm for Identifying Contextual Features from Clinical Text. In: ACL-07. 
107
Proceedings of the Workshop on BioNLP, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Distinguishing Historical from Current Problems in Clinical      
Reports?Which Textual Features Help?  
 
Danielle L. Mowery MS, Henk Harkema PhD, John N. Dowling MS MD,  
Jonathan L. Lustgarten PhD, Wendy W. Chapman PhD 
Department of Biomedical Informatics 
University of Pittsburgh, Pittsburgh, Pa 15260, USA 
dlm31@pitt.edu, heh23@pitt.edu, dowling@pitt.edu, jll47@pitt.edu, wec6@pitt.edu 
 
 
Abstract 
Determining whether a condition is historical 
or recent is important for accurate results in 
biomedicine. In this paper, we investigate four 
types of information found in clinical text that 
might be used to make this distinction. We 
conducted a descriptive, exploratory study us-
ing annotation on clinical reports to determine 
whether this temporal information is useful 
for classifying conditions as historical or re-
cent. Our initial results suggest that few of 
these feature values can be used to predict 
temporal classification. 
1 Introduction 
Clinical applications for decision support, biosur-
veillance and quality of care assessment depend on 
patient data described in unstructured, free-text 
reports.  For instance, patient data in emergency 
department reports contain valuable indicators for 
biosurveillance applications that may provide early 
signs and symptoms suggestive of an outbreak. 
Quality assurance departments can use free-text 
medical record data to assess adherence to quality 
care guidelines, such as determining whether an 
MI patient was given an aspirin within twenty-four 
hours of arrival. In either application, one must 
consider how to address the question of time, but 
each of the applications requires a different level of 
temporal granularity: the biosurveillance system 
needs a coarse-grained temporal model that dis-
cerns whether the signs and symptoms are histori-
cal or recent. In contrast, the quality assurance 
system needs a fine-grained temporal model to 
identify the admission event, when (or if) aspirin 
was given, and the order and duration of time be-
tween these events. One important problem in nat-
ural language processing is extracting the appro-
priate temporal granularity for a given task. 
Many solutions exist for extracting temporal in-
formation, and each is designed to address ques-
tions of various degrees of temporal granularity, 
including determining whether a condition is his-
torical or recent, identifying explicit temporal ex-
pressions, and identifying temporal relations 
among events in text. (Chapman et al, 2007; Zhou 
et al, 2008; Irvine et al, 2008;  Verhagen and Pus-
tejovsky, 2008; Bramsen et al, 2006). We pre-
viously extended the NegEx algorithm in ConText, 
a simple algorithm that relies on lexical cues to 
determine whether a condition is historical or re-
cent (Chapman et al, 2007). However, ConText 
performs with moderate recall (76%) and precision 
(75%) across different report types implying that 
trigger terms and simple temporal expressions are 
not sufficient for the task of identifying historical 
conditions.  
In order to extend work in identifying historical 
conditions, we conducted a detailed annotation 
study of potentially useful temporal classification 
features for conditions found in six genres of clini-
cal text. Our three main objectives were: (1) cha-
racterize the temporal similarity and differences 
found in different genres of clinical text; (2) de-
termine which features successfully predict wheth-
er a condition is historical, and (3) compare 
ConText to machine learning classifiers that ac-
count for this broader set of temporal features. 
2 Temporality in Clinical Text 
For several decades, researchers have been study-
ing temporality in clinical records (Zhou and 
Hripcsak, 2007). Readers use a variety of clues to 
distinguish temporality from the clinical narrative, 
and we wanted to identify features from other tem-
10
poral models that may be useful for determining 
whether a condition is historical or recent.  
There are a number of automated systems for 
extracting, representing, and reasoning time in a 
variety of text. One system that emerged from the 
AQUAINT workshops for temporal modeling of 
newspaper articles is TARSQI. TARSQI processes 
events annotated in text by anchoring and ordering 
them with respect to nearby temporal expressions 
(Verhagen and Pustejovsky, 2008). A few recent 
applications, such as TimeText and TN-TIES 
(Zhou et al, 2008; Irvine et al, 2008), identify 
medically relevant events from clinical texts and 
use temporal expressions to order the events. One 
method attempts to order temporal segments of 
clinical narratives (Bramsen et al, 2006). One key 
difference between these previous efforts and our 
work is that these systems identify all temporal 
expressions from the text and attempt to order all 
events. In contrast, our goal is to determine wheth-
er a clinical condition is historical or recent, so we 
focus only on temporal information related to the 
signs, symptoms, and diseases described in the 
text. Therefore, we ignore explicit temporal ex-
pressions that do not modify clinical conditions. If 
a condition does not have explicit temporal mod-
ifiers, we still attempt to determine the historical 
status for that condition (e.g., ?Denies cough?). In 
order to improve the ability to determine whether a 
condition is historical, we carried out this annota-
tion study to identify any useful temporal informa-
tion related to the clinical conditions in six clinical 
genres. Building on work in this area, we explored 
temporal features used in other temporal annota-
tion studies. 
TimeML is a well-known standard for complex, 
temporal annotation. TimeML supports the annota-
tion of events defined as ?situations that happen or 
occur? and temporal expressions such as dates and 
durations in order to answer temporal questions 
about these events and other entities in news text 
(Saur??, et al, 2006). One notable feature of the 
TimeML schema is its ability to capture verb tense 
such as past or present and verb aspect such as 
perfective or progressing. We annotated verb tense 
and aspect in medical text according to the Time-
ML standard. 
Within the medical domain, Zhou et al (2006) 
developed an annotation schema used to identify 
temporal expressions and clinical events. They 
measured the prevalence of explicit temporal ex-
pressions and key medical events like admission or 
transfer found in discharge summaries. We used 
the Zhou categorization scheme to explore tempor-
al expressions and clinical events across genres of 
reports. 
A few NLP systems rely on lexical cues to ad-
dress time. MediClass is a knowledge-based sys-
tem that classifies the content of an encounter 
using both free-text and encoded information from 
electronic medical records (Hazelhurst et al, 
2005). For example, MediClass classifies smoking 
cessation care delivery events by identifying the 
status of a smoker as continued, former or history 
using words like continues. ConText, an extension 
of the NegEx algorithm, temporally classifies con-
ditions as historical, recent, or hypothetical using 
lexical cues such as history, new, and if, respec-
tively (Chapman et al, 2007). Drawing from these 
applications, we used state and temporal trigger 
terms like active, unchanged, and history to cap-
ture coarse, temporal information about a condi-
tion.  
Temporal information may also be implied in 
the document structure, particularly with regards to 
the section in which the condition appears. SecTag 
marks explicit and implicit sections found 
throughout patient H&P notes (Denny et al, 2008). 
We adopted some section headers from the SecTag 
terminology to annotate sections found in reports.  
Our long-term goal is to build a robust temporal 
classifier for information found in clinical text 
where the output is classification of whether a con-
dition is historical or recent (historical categoriza-
tion). An important first step in classifying 
temporality in clinical text is to identify and cha-
racterize temporal features found in clinical re-
ports. Specifically, we aim to determine which 
expressions or features are predictive of historical 
categorization of clinical conditions in dictated 
reports. 
3 Historical Assignment and Temporal 
Features 
We conducted a descriptive, exploratory study of 
temporal features found across six genres of clini-
cal reports. We had three goals related to our task 
of determining whether a clinical condition was 
historical or recent. First, to develop a temporal 
classifier that is generalizable across report types, 
we compared temporality among different genres 
11
of clinical text. Second, to determine which fea-
tures predict whether a condition is historical or 
recent, we observed common rules generated by 
three different rule learners based on manually an-
notated temporal features we describe in the fol-
lowing section. Finally, we compared the 
performance of ConText and automated rule learn-
ers and assessed which features may improve the 
ConText algorithm.  
Next, we describe the temporal features we as-
sessed for identification of historical signs, symp-
toms, or diseases, including temporal expressions, 
lexical cues, verb tense and aspect, and sections.  
(1) Temporal Expressions: Temporal expres-
sions are time operators like dates (May 5th 2005) 
and durations (for past two days), as well as clini-
cal processes related to the encounter (discharge, 
transfer). For each clinical condition, we annotated 
whether a temporal expression modified it and, if 
so, the category of temporal expression. We used 
six major categories from Zhou et al (2006) in-
cluding: Date and Time, Relative Date and Time, 
Durations, Key Events, Fuzzy Time, and No Tem-
poral Expression. These categories also have 
types. For instance, Relative Date and Time has a 
type Yesterday, Today or Tomorrow.  For the con-
dition in the sentence ?The patient had a stroke in 
May 2006?, the temporal expression category is 
Date and Time with type Date. Statements without 
a temporal expression were annotated No Tempor-
al Expression with type N/A. 
(2) Tense and Aspect: Tense and aspect define 
how a verb is situated and related to a particular 
time. We used TimeML Specification 1.2.1 for 
standardization of tense and aspect where exam-
ples of tense include Past or Present and aspect 
may be Perfective, Progressive, Both or None as 
found in Saur??, et al (2006). We annotated the 
verb that scoped a condition and annotated its tense 
and aspect. The primary verb may be a predicate 
adjective integral to interpretation of the condition 
(Left ventricle is enlarged), a verb preceding the 
condition (has hypertension), or a verb following a 
condition (Chest pain has resolved). In ?her chest 
pain has resolved,? we would mark ?has resolved? 
with tense Present and aspect Perfective. State-
ments without verbs (e.g., No murmurs) would be 
annotated Null for both.  
(3) Trigger Terms: We annotated lexical cues 
that provide temporal information about a condi-
tion. For example, in the statement, ?Patient has 
past history of diabetes,? we would annotate ?his-
tory? as Trigger Term: Yes and would note the ex-
act trigger term. 
     (4) Sections: Sections are ?clinically meaning-
ful segments which act independently of the 
unique narrative? for a patient (Denny et al 2008). 
Examples of report sections include Review of Sys-
tems (Emergency Department), Findings (Opera-
tive Gastrointestinal and Radiology) and 
Discharge Diagnosis (Emergency Department and 
Discharge Summary).  
We extended Denny?s section schema with ex-
plicit, report-specific section headers not included 
in the original terminology. Similar to Denny, we 
assigned implied sections in which there was an 
obvious change of topic and paragraph marker. For 
instance, if the sentence ?the patient is allergic to 
penicillin? followed the Social History section, we 
annotated the section as Allergies, even if there 
was not a section heading for allergies. 
4 Methods 
4.1 Dataset Generation 
We randomly selected seven reports from each of 
six genres of clinical reports dictated at the Univer-
sity of Pittsburgh Medical Center during 2007 
These included Discharge Summaries, Surgical 
Pathology, Radiology, Echocardiograms, Opera-
tive Gastrointestinal, and Emergency Department 
reports. The dataset ultimately contained 42 clini-
cal reports and 854 conditions. Figure 1 show our 
annotation process, which was completed in 
GATE, an open-source framework for building 
NLP systems (http://gate.ac.uk/). A physician 
board-certified in internal medicine and infectious 
diseases annotated all clinical conditions in the set 
and annotated each condition as either historical or 
recent. He used a general guideline for annotating 
a condition as historical if the condition began 
more than 14 days before the current encounter and 
as recent if it began or occurred within 14 days or 
during the current visit. However, the physician 
was not bound to this definition and ultimately 
used his own judgment to determine whether a 
condition was historical. 
Provided with pre-annotated clinical conditions 
and blinded to the historical category, three of the 
authors annotated the features iteratively in groups 
of six (one of each report type) using guidelines we 
12
developed for the first two types of temporal fea-
tures (temporal expressions and trigger terms.) 
Between iterations, we resolved disagreements 
through discussion and updated our guidelines. 
Cohen?s kappa for temporal expressions and trig-
ger terms by the final iteration was at 0.66 and 0.69 
respectively. Finally, one author annotated sec-
tions, verb tense, and aspect.  Cases in which as-
signing the appropriate feature value was unclear 
were resolved after consultation with one other 
author-annotator.  
4.2 Data Analysis 
 
We represented each condition as a vector with  
temporal features and their manually-assigned val-
ues as input features for predicting the binary out-
come value of historical or recent. We trained three 
rule learning algorithms to classify each condition 
as historical or recent: J48 Decision Tree, Ripper, 
and Rule Learner (RL) (Witten and Frank, 2005; 
Clearwater and Provost, 1990). Rule learners per-
form well at classification tasks and provide expli-
cit rules that can be viewed, understood, and 
potentially implemented in existing rule-based ap-
plications. We used Weka 3.5.8, an openly-
available machine learning application for predic-
tion modeling, to implement the Decision Tree 
(J48) and Ripper (JRip) algorithms, and we applied 
an in house version of RL retrieved from 
www.dbmi.pitt.edu\probe. For all rule learners, we 
used the default settings and ran ten-fold cross-
validation. The J48 algorithm produces mutually 
exclusive rules for predicting the outcome value. 
Thus, two rules cannot cover or apply to any one 
case. In contrast, both JRip and RL generate non-
mutually-exclusive rules for predicting the out-
come value. Although J48 and JRip are sensitive to 
bias in outcome values, RL accounts for skewed 
distribution of the data.  
We also applied ConText to the test cases to 
classify them as historical or recent. ConText looks 
for trigger terms and a limited set of temporal ex-
pressions within a sentence. Clinical conditions 
within the scope of the trigger terms are assigned 
the value indicated by the trigger terms (e.g., his-
torical for the term history). Scope extends from 
the trigger term to the end of the sentence or until 
the presence of a termination term, such as pre-
senting. For instance, in the sentence ?History of 
CHF, presenting with chest pain,? CHF would be 
annotated as historical.  
5 Evaluation 
To characterize the different reports types, we es-
tablished the overall prevalence and proportion of 
conditions annotated as historical for each clinical 
report genre.  We assessed the prevalence of each 
feature (temporal expressions, trigger terms, tense 
and aspect, and sections) by report genre to deter-
mine the level of similarity or difference between 
genres. To determine which features values are 
predictive of whether a condition is historical or 
recent, we observed common rules found by more 
than one rule learning algorithm. Amongst com-
mon rules, we identified new rules that could im-
prove the ConText algorithm.  
We also measured predictive performance with 
95% confidence intervals of the rule learners and 
ConText by calculating overall accuracy, as well as 
recall and precision for historical classifications 
and recall and precision for recent classifications.  
Table 1 describes equations for the evaluation me-
trics. 
 
Table 1. Description of evaluation metrics. RLP = rule 
learner prediction. RS = Reference Standard 
 
 
Figure 1. Annotation process for dataset and objectives 
for evaluation. 
13
Recall:                 number of TP              
(number of TP + number of FN) 
 
Precision:           number of TP              
(number of TP + number of FP) 
 
Accuracy:   number of instances correctly classified 
                      total number of possible instances  
6 Results 
Overall, we found 854 conditions of interest across 
all six report genre. Table 2 illustrates the preva-
lence of conditions across report genres. Emergen-
cy Department reports contained the highest 
concentration of conditions. Across report genres, 
87% of conditions were recent (741 conditions). 
All conditions were recent in Echocardiograms, in 
contrast to Surgical Pathology reports in which 
68% were recent.  
 
Table 2. Prevalence and count of conditions by temporal 
category and report genre. DS = Discharge Summary, 
Echo = Echocardiogram, ED = Emergency Department, 
GI = Operative Gastrointestinal, RAD = Radiology and 
SP = Surgical Pathology. (%) = percent; Ct = count.  
 
6.1 Prevalence of Temporal Features 
Table 3 shows that most conditions were not mod-
ified by a temporal expression or a trigger term. 
Conditions were modified by a temporal expres-
sion in Discharge Summaries more often than in 
other report genres. Similarly, Surgical Pathology 
had the highest prevalence of conditions modified 
by a trigger term. Operative Gastrointestinal and 
Radiology reports showed the lowest prevalence of 
both temporal expressions and trigger terms. Nei-
ther temporal expressions nor trigger terms oc-
curred in Echocardiograms. Overall, the 
prevalence of conditions scoped by a verb varied 
across report types ranging from 46% (Surgical 
Pathology) to 81% (Echocardiogram). 
Table 3. Prevalence of conditions modified by temporal 
features. All conditions were assigned a section and are 
thereby excluded. TE = temporal expression; TT = trig-
ger term; V = scoped by verb.  
 
6.2 Common Rules 
Rule learners generated a variety of rules. The J48 
Decision Tree algorithm learned 27 rules, six for 
predicting conditions as historical and the remain-
ing for classifying the condition as recent. The 
rules predominantly incorporated the trigger term 
and verb tense and aspect feature values. JRip 
learned nine rules, eight for classifying the histori-
cal temporal category and one ?otherwise? rule for 
the majority class. The JRip rules most heavily 
incorporated the section feature. The RL algorithm 
found 79 rules, 18 of which predict the historical 
category. Figure 2 illustrates historical rules 
learned by each rule learner. JRip and RL pre-
dicted the following sections alone can be used to 
predict a condition as historical: Past Medical His-
tory, Allergies and Social History. Both J48 and 
RL learned that trigger terms like previous, known 
and history predict historical. There was only one 
common, simple rule for the historical category 
found amongst all three learners: the trigger term 
no change predicts the historical category. All al-
gorithms learned a number of rules that include 
two features values; however, none of the com-
pound rules were common amongst all three algo-
rithms.    
 
Figure 2. Historical rules learned by each rule learner 
algorithm. Black dots represent simple rules whereas 
triangles represent compound rules. Common rules 
shared by each algorithm occur in the overlapping areas 
of each circle. 
14
6.3 Predictive Performance 
Table 4 shows predictive performance for each 
rule learner and for ConText. The RL algorithm 
outperformed all other algorithms in almost all 
evaluation measures. The RL scores were com-
puted based on classifying the 42 cases (eight his-
torical) for which the algorithm did not make a 
prediction as recent. ConText and J48, which ex-
clusively relied on trigger terms, had lower recall 
for the historical category.  
All of the rule learners out-performed ConText. 
JRip and RL showed substantially higher recall for 
assigning the historical category, which is the most 
important measure in a comparison with ConText, 
because ConText assigns the default value of re-
cent unless there is textual evidence to indicate a 
historical classification. Although the majority 
class baseline shows high accuracy due to high 
prevalence of the recent category, all other classifi-
ers show even higher accuracy, achieving fairly 
high recall and precision for the historical cases 
while maintaining high performance on the recent 
category. 
 
Table 4. Performance results with 95% confidence in-
tervals for three rule learners trained on manually anno-
tated features and ConText, which uses automatically 
generated features. Bolded values do not have overlap-
ping confidence intervals with ConText. MCB = Ma-
jority Class Baseline (recent class)   
 
7 Discussion 
Our study provides a descriptive investigation of 
temporal features found in clinical text. Our first 
objective was to characterize the temporal similari-
ties and differences amongst report types. We 
found that the majority of conditions in all report 
genres were recent conditions, indicating that a 
majority class classifier would produce an accura-
cy of about 87% over our data set.  According to 
the distributions of temporal category by report 
genre (Table 2), Echocardiograms exclusively de-
scribe recent conditions. Operative Gastrointestinal 
and Radiology reports contain similar proportions 
of historical conditions (9% and 6%). Echocardio-
grams appear to be most similar to Radiology re-
ports and Operative Gastrointestinal reports, which 
may be supported by the fact that these reports are 
used to document findings from tests conducted 
during the current visit. Emergency Department 
reports and Discharge Summaries contain similar 
proportions of historical conditions (17% and 19% 
respectively), which might be explained by the fact 
that both reports describe a patient?s temporal pro-
gression throughout the stay in the Emergency De-
partment or the hospital.  
Surgical Pathology reports may be the most 
temporally distinct report in our study, showing the 
highest proportion of historical conditions. This 
may seem counter-intuitive given that Surgical 
Pathology reports also facilitate the reporting of 
findings described from a recent physical speci-
men. However, we had a small sample size (28 
conditions in seven reports), and most of the his-
torical conditions were described in a single ad-
dendum report. Removing this report decreased the 
prevalence of historical conditions to 23% (3/13).  
Discharge Summaries and Emergency Depart-
ment reports displayed more variety in the ob-
served types of temporal expressions (9 to 14 
subtypes) and trigger terms (10 to 12 terms) than 
other report genres. This is not surprising consider-
ing the range of events described in these reports. 
Other reports tend to have between zero and three 
subtypes of temporal expressions and zero and 
seven different trigger terms. In all report types, 
temporal expressions were mainly subtype past, 
and the most frequent trigger term was history. 
Our second objective was to identify which fea-
tures predict whether a condition is historical or 
recent. Due to high prevalence of the recent cate-
gory, we were especially interested in discovering 
temporal features that predict whether a condition 
is historical. With one exception (date greater than 
four weeks prior to the current visit), temporal ex-
pression features always occurred in compound 
rules in which the temporal expression value had to 
co-occur with another feature value. For instance, 
any temporal expression in the category key event 
had to also occur in the secondary diagnosis sec-
tion to classify the condition as historical. For ex-
15
ample, in ?SECONDARY DIAGNOSIS: Status 
post Coronary artery bypass graft with complica-
tion of mediastinitis? the key event is the coronary 
artery bypass graft, the section is secondary diag-
nosis, and the correct classification is historical.  
Similarly, verb tense and aspect were only use-
ful in conjunction with other feature values. One 
rule predicted a condition as historical if the condi-
tion was modified by the trigger term history and 
fell within the scope of a present tense verb with 
no aspect. An example of this is ?The patient is a 
50 year old male with history of hypertension.? 
Intuitively, one would think that a past tense verb 
would always predict historical; however, we 
found the presence of a past tense verb with no 
aspect was a feature only when the condition was 
in the Patient History section.  Sometimes the ab-
sence of a verb in conjunction with another feature 
value predicted a condition as historical. For ex-
ample, in the sentences ?PAST MEDICAL 
HISTORY: History of COPD. Also diabetes?? 
also functioned as a trigger term that extended the 
scope of a previous trigger term, history, in the 
antecedent sentence.  
A few historical trigger terms were discovered 
as simple rules by the rule learners: no change, 
previous, known, status post, and history. A few 
rules incorporated both a trigger term and a partic-
ular section header value. One rule predicted his-
torical if the trigger term was status post and the 
condition occurred in the History of Present Illness 
section. This rule would classify the condition 
CABG as historical in ?HISTORY OF PRESENT 
ILLNESS: The patient is...status post CABG.? 
One important detail to note is that a number of the 
temporal expressions categorized as Fuzzy Time 
also act as trigger terms, such as history and status 
post?both of which were learned by J48. A histor-
ical trigger term did not always predict the catego-
ry historical. In the sentence ?No focal sensory or 
motor deficits on history,? history may suggest that 
the condition was not previously documented, but 
was interpreted as not presently identified during 
the current physical exam.   
Finally, sections appeared in the majority of 
JRip and RL historical rules: 4/8 simple rules and 
13/18 compound rules. A few sections were con-
sistently classified as historical: Past Medical His-
tory, Allergies, and Social History.  One important 
point to address is that these sections were manual-
ly annotated.  
Our results revealed a few unexpected observa-
tions. We found at least two trigger terms indicated 
in the J48 rules, also and status post, which did not 
have the same predictive ability across report ge-
nres.  For instance, in the statement ?TRANSFER 
DIAGNOSIS: status post coiling for left posterior 
internal carotid artery aneurysm,? status post indi-
cates the reason for the transfer as an inpatient 
from the Emergency Department and the condition 
is recent. In contrast, status post in a Surgical Pa-
thology report was interpreted to mean historical 
(e.g., PATIENT HISTORY: Status post double 
lung transplant for COPD.) In these instances, 
document knowledge of the meaning of the section 
may be useful to resolve these cases.  
One other unexpected finding was that the trig-
ger term chronic was predictive of recent rather 
than historical. This may seem counterintuitive; 
however, in the statement ?We are treating this as 
chronic musculoskeletal pain with oxycodone?, the 
condition is being referenced in the context of the 
reason for the current visit. Contextual information 
surrounding the condition, in this case treating or 
administering medication for the condition, may 
help discriminate several of these cases.  
Our third objective was to assess ConText in re-
lation to the rules learned from manually annotated 
temporal features. J48 and ConText emphasized 
the use of trigger terms as predictors of whether a 
condition was historical or recent and performed 
with roughly the same overall accuracy. JRip and 
RL learned rules that incorporated other feature 
values including sections and temporal expres-
sions, resulting in a 12% increase in historical re-
call over ConText and a 31% increase in historical 
recall over J48. 
Many of the rules we learned can be easily ex-
tracted and incorporated into ConText (e.g., trigger 
terms previous and no change). The ConText algo-
rithm largely relies on the use of trigger terms like 
history and one section header, Past Medical His-
tory. By incorporating additional section headers 
that may strongly predict historical, ConText could 
potentially predict a condition as historical when a 
trigger term is absent and the header title is the 
only predictor as in the case of ?ALLERGIES: 
peanut allergy?. Although these sections header 
may only be applied to Emergency Department 
and Discharge Summaries, trigger terms and tem-
poral expressions may be generalizable across ge-
nre of reports.  Some rules do not lend themselves 
16
to ConText?s trigger-term-based approach, particu-
larly those that require sophisticated representation 
and reasoning. For example, ConText only reasons 
some simple durations like several day history. 
ConText cannot compute dates from the current 
visit to reason that a condition occurred in the past 
(e.g., stroke in March 2000).  The algorithm per-
formance would gain from such a function; how-
ever, such a task would greatly add to its 
complexity.   
8 Limitations 
The small sample size of reports and few condi-
tions found in three report genres (Operative Ga-
strointestinal, Radiology, and Surgical Pathology) 
is a limitation in this study. Also, annotation of 
conditions, temporal category, sections, verb tense 
and aspect were conducted by a single author, 
which may have introduced bias to the study. Most 
studies on temporality in text focus on the temporal 
features themselves. For instance, the prevalence 
of temporal expressions reported by Zhou et al 
(2006) include all temporal expressions found 
throughout a discharge summary, whereas we an-
notated only those expressions that modified the 
condition. This difference makes comparing our 
results to other published literature challenging.  
9 Future Work  
Although our results are preliminary, we be-
lieve our study has provided a few new insights 
that may help improve the state of the art for his-
torical categorization of a condition. The next step 
to building on this work includes automatically 
extracting the predictive features identified by the 
rule learners. Some features may be easier to ex-
tract than others. Since sections appear to be strong 
indicators for historical categorization we may start 
by implementing the SecTag tagger. Often a sec-
tion header does not exist between text describing 
the past medical history and a description of the 
current problem, so relying merely on the section 
heading is not sufficient. The SecTag tagger identi-
fies both implicit and explicit sections and may 
prove useful for this task. To our knowledge, Sec-
Tag was only tested on Emergency Department 
reports, so adapting it to other report genres will be 
necessary. Both JRip and RL produced high per-
formance, suggesting a broader set of features may 
improve historical classification; however, because 
these features do not result in perfect performance, 
there are surely other features necessary for im-
proving historical classification. For instance, hu-
mans use medical knowledge about conditions that 
are inherently chronic or usually experienced over 
the course of a patient?s life (i.e., HIV, social ha-
bits like smoking, allergies etc). Moreover, physi-
cians are able to integrate knowledge about chronic 
conditions with understanding of the patient?s rea-
son for visit to determine whether a chronic condi-
tion is also a recent problem. An application that 
imitated experts would need to integrate this type 
of information. We also need to explore adding 
features captured at the discourse level, such as 
nominal and temporal coreference. We have begun 
work in these areas and are optimistic that they 
will improve historical categorization.  
10 Conclusion 
Although most conditions in six clinical report ge-
nres are recent problems, identifying those that are 
historical is important in understanding a patient?s 
clinical state. A simple algorithm that relies on lex-
ical cues and simple temporal expressions can 
classify the majority of historical conditions, but 
our results indicate that the ability to reason with 
temporal expressions, to recognize tense and as-
pect, and to place conditions in the context of their 
report sections will improve historical classifica-
tion. We will continue to explore other features to 
predict historical categorization. 
 
Acknowledgments 
 
This work was funded by NLM grant 1 
R01LM009427-01, ?NLP Foundational Studies 
and Ontologies for Syndromic Surveillance from 
ED Reports?.  
References 
 
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee, 
and Regina Barzilay. 2006. Finding Temporal Order 
in Discharge Summaries. AMIA Annu Symp Proc. 
2006; 81?85 
Wendy W Chapman, David Chu, and John N. Dowling. 
2007. ConText: An Algorithm for Identifying Contex-
tual Features from Clinical Text. Association for 
Computational Linguistics, Prague, Czech Republic 
17
Scott H. Clearwater and Foster J. Provost. 1990. RL4: A 
Tool for Knowledge-Based Induction. Tools for Ar-
tificial Intelligence, 1990. Proc of the 2nd Intern 
IEEE Conf: 24-30. 
Joshua C. Denny, Randolph A. Miller, Kevin B. John-
son, and Anderson Spickard III. 2008. Development 
and Evaluation of a Clinical Note Section Header 
Terminology. SNOMED. AMIA 2008 Symp. Pro-
ceedings: 156-160. 
Brian Hazlehurst, H. Robert Frost, Dean F. Sittig, and 
Victor J. Stevens. 2005. MediClass: A system for de-
tecting and classifying encounter-based clinical 
events in any electronic medical record. J Am Med 
Inform Assoc 12(5): 517-29 
Ann K. Irvine, Stephanie W. Haas, and Tessa Sullivan. 
2008. TN-TIES: A System for Extracting Temporal 
Information from Emergency Department Triage 
Notes. AMIA 2008 Symp Proc: 328-332. 
Roser Saur??, Jessica Littman, Bob Knippen, Robert 
Gaizauskas, Andrea Setzer, and James Pustejovsky. 
2006. TimeML Annotation Guidelines Version 1.2.1. 
at: 
http://www.timeml.org/site/publications/timeMLdocs
/annguide_1.2.1.pdf 
Marc Verhagen and James Pustejovsky. 2008. Temporal 
Processing with TARSQI Toolkit. Coling 2008: Com-
panion volume ? Posters and Demonstrations, Man-
chester, 189?192 
Ian H. Witten and Eibe Frank. 2005. Data Mining: 
Practical machine learning tools and techniques, 2nd 
Edition, Morgan Kaufmann, San Francisco, 2005. 
Li Zhou, Genevieve B. Melton, Simon Parsons and 
George Hripcsak. 2006. A temporal constraint struc-
ture for extracting temporal information from clini-
cal narrative. J Biomed Inform 39(4): 424-439. 
Li Zhou and George Hripcsak. 2007. Temporal reason-
ing with medical data--a review with emphasis on 
medical natural language processing. J Biomed In-
form Apr; 40(2):183-202. 
Li Zhou, Simon Parson, and George Hripcsak. 2008. 
The Evaluation of a Temporal Reasoning System in 
Processing Discharge Summaries. J Am Med Inform 
Assoc 15(1): 99?106.  
 
18
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 56?64,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Medical diagnosis lost in translation ? Analysis of uncertainty and negation
expressions in English and Swedish clinical texts
Danielle L Mowery
University of Pittsburgh
200 Meyran Ave
Pittsburgh, PA 15260
dlm31@pitt.edu
Sumithra Velupillai
Stockholm University
164 40 Kista
Stockholm, Sweden
sumithra@dsv.su.se
Wendy W Chapman
University of California San Diego
10100 Hopkins Dr
La Jolla, CA 92093
wwchapman@ucsd.edu
Abstract
In the English clinical and biomedical text do-
mains, negation and certainty usage are two
well-studied phenomena. However, few stud-
ies have made an in-depth characterization
of uncertainties expressed in a clinical set-
ting, and compared this between different an-
notation efforts. This preliminary, qualita-
tive study attempts to 1) create a clinical un-
certainty and negation taxonomy, 2) develop
a translation map to convert annotation la-
bels from an English schema into a Swedish
schema, and 3) characterize and compare two
data sets using this taxonomy. We define
a clinical uncertainty and negation taxonomy
and a translation map for converting annota-
tion labels between two schemas and report
observed similarities and differences between
the two data sets.
1 Introduction and Background
Medical natural language processing techniques are
potentially useful for extracting information con-
tained in clinical texts, such as emergency depart-
ment reports (Meystre et al, 2008). One impor-
tant aspect to take into account when developing ac-
curate information extraction tools is the ability to
distinguish negated, affirmed, and uncertain infor-
mation (Chu et al, 2006). Several research stud-
ies have targeted this problem and created anno-
tation schemas and manually annotated reference
standards for uncertainty and negation occurrence
in news documents (Saur?? and Pustejovsky (2009),
Wiebe et al (2001), Rubin et al (2006)), biomedical
research articles (Wilbur et al (2006), Vincze et al
(2008)), and clinical narratives (Uzuner et al (2011)
and Uzuner et al (2009)). There are encoding tools
developed for automatic identification of uncertainty
and negation in English, such as ConText (Harkema
et al, 2009), which relies on heuristics and keyword
lists, and MITRE?s CARAFE (Clark et al, 2011),
which combines heuristic and statistical techniques.
However, most relevant annotation schemas, ref-
erence standards, and encoding tools are built for
English documents. For smaller languages, such as
Swedish, resources are scarce.
We present a pilot, qualitative study to compare
two different annotation schemas and subsequent
annotated corpora for uncertainty modeling of dis-
order mentions, e.g., signs, symptoms, and diseases,
in clinical texts, for two different languages: English
and Swedish. We compare these annotation schemas
and their instantiation in the two languages in an at-
tempt to gain a deeper understanding of how uncer-
tainty and negation are expressed in different clini-
cal texts with an emphasis on creating a portable un-
certainty and negation application that generalizes
among clinical texts of different languages.
This pilot study is motivated for at least two
reasons. First, little attention has been given to
mapping, characterizing, or comparing annotation
schemas built for different languages or to character-
izing different types of uncertainty expressions and
the intention underlying those expressions. Such
knowledge is needed for building information ex-
traction tools that can accurately identify or track
differential diagnoses over time, particularly when
medical reasoning can be laden with uncertainty
about a disorder?s existence or change over time.
56
Second, building new resources for small lan-
guages is time consuming. Utilizing existing tools
and techniques already developed for one language,
such as English, could be an efficient way of devel-
oping new useful tools for other less exploited lan-
guages, such as Swedish.
Our overall goal is to move towards improving au-
tomatic information extraction from clinical texts by
leveraging language differences and similarities. In
order to address this issue, our aims in this study
are to 1) create a taxonomy for deepened charac-
terization of how uncertainty and negation is ex-
pressed in clinical texts, 2) compare two existing un-
certainty and negation annotation schemas from this
perspective, and 3) compare differences and similar-
ities in expressions of uncertainty and negation be-
tween two languages: English and Swedish.
2 Methods
In this pilot, qualitative comparison study, we used
grounded theory (Strauss and Corbin, 1990) to in-
ductively identify themes that characterize clini-
cal uncertainty and negation expressed in both En-
glish (University of Pittsburgh Medical Center) and
Swedish (Karolinska University Hospital) research
data sets derived from emergency department re-
ports.
2.1 Uncertainty/negation annotation schemas
Two independently developed annotation schemas
were used to annotate disorder mentions in the
clinical texts: a schema developed for English re-
ports (Mowery et al (2012)) and one for Swedish
(Velupillai et al (2011)). Each disorder mention
was pre-annotated and constituted the input to a sep-
arate set of annotators, who assigned values to a set
of attributes defined in the schema. For instance, in
the sentence ?Patient with possible pneumonia.?, an-
notators for the English data set assigned values to
four attributes for the instance of pneumonia:
? Existence(yes, no): whether the disorder was ever present
? AspectualPhase(initiation, continuation, culmination, un-
marked): the stage of the disorder in its progression
? Certainty(low, moderate, high, unmarked): amount of certainty
expressed about whether the disorder exists
? MentalState(yes, no): whether an outward thought or feeling
about the disorder?s existence is mentioned
In the Swedish schema, annotators assigned val-
ues to two attributes:
? Polarity(positive, negative): whether a disorder mention is in the
positive or negative polarity, i.e., affirmed (positive) or negated
(negative)
? Certainty(possibly, probably, certainly): gradation of certainty
for a disorder mention, to be assigned with a polarity value.
2.2 Data Sets
The English data set included 30 de-identified, full-
length emergency department reports annotated with
283 disorders related to influenza-like illnesses by
a board-certified infectious disease physician. Each
disorder was annotated with four attributes ? exis-
tence, aspectual phase, certainty and mental state ?
by two independent annotators (including DM) who
came to consensus after reviewing disagreements.
The Swedish data set included 1,297 assessment
sections from emergency department reports anno-
tated with approx. 2,000 disorders, automatically
marked from a manually created list of approx-
imately 300 unique disorders by two physicians.
The two physicians annotated each disorder mention
with attributes of polarity and certainty. A random
subset of approx. 200 annotated disorder mentions
from the data set were used for this qualitative study.
2.3 Study Process
In order to better understand how physicians de-
scribe uncertainty of the presence or absence of a
disorder, we evaluated the annotations from the two
data sets as follows: 1) created a clinical uncertainty
and negation taxonomy, 2) developed a translation
map for mapping attributes and values from the En-
glish schema into the Swedish schema, and 3) char-
acterized and compared both data sets and languages
using the taxonomy.
To create the uncertainty and negation taxonomy,
we conducted a literature review of recent annota-
tion schemas (e.g. Vincze et al (2008)), assignment
applications (e.g. Uzuner et al (2011), Harkema
et al (2009), Clark et al (2011), Chapman et al
(2011)), and observational studies (Lingard et al,
2003) about uncertainty or negation in the clinical
domain. From our review, we created a clinical tax-
onomy describing notable characteristics of uncer-
tainty and negation, which were added to and re-
fined using grounded theory, by inspecting the dis-
order annotations in our data sets and documenting
57
emerging themes consistent with issues found from
the literature review. For instance, one characteristic
of negation annotations found in the literature and in
our data sets is the existence of a lexical cue indicat-
ing that a disorder is negated, and the lexical cue can
occur before, within, or after the disorder mention.
The characteristics included in the taxonomy repre-
sent features describing the attributes of uncertainty
and negation in the data sets (see Section 3.1).
To develop the translation map between certainty
and negation values from each annotation schema,
authors DM and SV jointly reviewed each annotated
disorder mention from the English data set and as-
signed a Swedish polarity and certainty label, then
devised a map from the English schema into the
Swedish schema.
To characterize and compare manifestations of
uncertainty and negation using annotations from the
two data sets, DM and SV annotated each disorder
mention in both data sets with the features in the
clinical uncertainty and negation taxonomy. In the
English data set, each disorder was annotated by DM
and adjudicated by SV. In the Swedish data set, each
disorder was annotated by SV then translated into
English for adjudication by DM.
3 Results
3.1 Clinical Uncertainty and Negation
Taxonomy
We developed a clinical uncertainty and negation
taxonomy to characterize the linguistic manifesta-
tions of uncertainty and negation in clinical text
(Figure 1). We found three high-level features in
the literature and in our data sets: position of lexical
cue (i.e., position of the lexical expression indicat-
ing uncertainty or negation with respect to the dis-
order), opinion source (i.e. person believing there
is absence, presence, or uncertainty), and evidence
evaluation (i.e., reason for the uncertainty or nega-
tion belief).
Position of lexical cue demonstrated itself in the
data sets in three non-mutually exclusive ways:
? pre-disorder (lexical cue precedes the disorder) ?Patient denies
chest pain.?
? intra-disorder (lexical cue occurs within the name of the disor-
der) ?x-ray...possibly be indicative of pneumonia.?
? post-disorder (lexical cue occurs after the disorder)
?abdominal cramping..is unlikely.?
Opinion source exhibited the following values:
? dictating physician (dictating physician alone expressed pres-
ence, absence, or uncertainty regarding the disorder) ?I suspect
bacterial pneumonia.?
? dictating physician with consultation (dictating physician explic-
itly includes other clinical professional in statement) ?Discussing
with Dr. **NAME**, pneumonia can not be excluded.?
? other clinical care providers (other clinical team members ex-
plicitly stated as expressing presence, absence or uncertainty re-
garding the disorder) ?per patient?s primary doctor, pneumonia
is suspected.?
? patient (patient expressed presence, absence, or uncertainty re-
garding the disorder) ?Pt doesn?t think she has pneumonia.?
? unknown (ambiguous who is expressing presence, absence, or
uncertainty regarding the disorder) ?there was a short episode of
coughing.?
Evidence evaluation includes a modified subset
of values found in the model of uncertainty pro-
posed by Lingard et al (2003) to connote perceived
reasons for the provider uncertainty (and negation)
about the disorder mention as used in our data sets.
? limits of evidence (data limitations for hypothesis testing), one
diagnosis
? evidence contradicts (data contradicts expected hypothe-
sis), ?Blood test normal, but we still think Lyme disease.?
? evidence needed (evidence unavailable to test hypoth-
esis) ?Waiting for x-ray results to determine if it?s a
femur fracture.?
? evidence not convincing, but diagnosis asserted (data
doesn?t fully support proposed hypothesis), ?Slightly el-
evated levels of WBCs suggests infection.?
? limits of evidence, more than one diagnosis
? differential diagnoses enumerated (competing diagnoses
reasoned), ?bacterial infection vs. viral infection.?
? limits in source of evidence (untrusted evidence)
? non-clinical source (from non-provider source), ?Pt can?t
remember if she was diagnosed with COPD.?
? clinical source (from provider source), ?I do not agree
with Dr. X?s diagnosis of meningitis.?
? test source (from test e.g., poor quality), ?We cannot de-
termine from the x-ray if the mass is fluid or a tumor.?
? limitless possibilities (large number of likely diagnoses so diag-
nosis defaulted to most likely), ?This is probably an infection of
some sort.?
? other (no evidence limitation)
? asserting a diagnosis or disorder as affirmed (positive
case), ?Confirms nausea.?
? asserting a diagnosis or disorder as negated (negative
case), ?No vomiting.?
58
Figure 1: Uncertainty and negation taxonomy with features ? Position of lexical cue, Opinion source and Evidence evaluation ?
with corresponding values (nested lines and sub-lines).
3.2 Translation Map
In order to compare annotations between the data
sets, we developed a mapping procedure for convert-
ing the four annotated attribute values from the En-
glish schema into the two annotated attribute values
from the Swedish schema. This mapping procedure
uses two normalization steps, negation and certainty
(see Figure 2).
Using Figure 2, we explain the mapping proce-
dure to convert English annotations into Swedish
annotations. Our steps and rules are applied with
precedence, top down and left to right. For ?I have
no suspicion for bacterial infection for this patient?,
English annotations are Existence(no) AND Aspec-
tualPhase(null) AND Certainty(high) AND Men-
talState(yes), and Swedish annotations are Polar-
ity(negative) AND Certainty(probably). The map-
ping procedure applies two normalization steps,
negation and uncertainty, with the following rules.
The first step is negation normalization to convert
Existence and Aspectual Phase into Polarity anno-
tations. In this example, Existence(no) ? Polar-
ity(negative).
The second step is certainty normalization with
up to two sub steps. For Certainty mapping, in sum-
mary, map English NOT Certainty(unmarked) to
Swedish Certainty level, e.g., Certainty(high)
? Certainty(probably). For MentalState
mapping, if English Certainty(unmarked) AND
MentalState(yes), map to either Swedish Cer-
tainty(probably) OR Certainty(possibly) using
your best judgment; otherwise, map to Cer-
tainty(certainly). For our example sentence,
Certainty mapping was sufficient to map from the
English to the Swedish Certainty levels.
We found that these two schemas were mappable.
Despite the binary mapping splits from English Cer-
tainty(Moderate) ? Swedish Certainty(possibly)
OR Certainty(probably) and judgment calls neces-
sary for MentalState mapping, few annotations were
not easily mapped.
3.3 Characterization of English and Swedish
Data sets
In this study, we characterized our data sets accord-
ing to a clinical uncertainty and negation taxonomy
comprised of three concepts ? position of lexical
cue, opinion source, and evidence evaluation.
3.3.1 Position of lexical cue
In Table 1, we show examples of phrases from each
data set representing the Polarity and Certainty lev-
els in the taxonomy. In our data set, we did not
explicitly annotate markers for the highest certainty
levels in the positive polarity, such as ?definitely
has?. We did not encounter any of these cases in the
59
Figure 2: Map between values for attributes in Swedish and English schemas. Bolded rules indicate the rules used to assign values
to the example sentence (English sentence on top).
data set. We observed that most uncertainty expres-
sions precede a disorder mention. Few expressions
both precede and follow the disorder mention, or
within the disorder mention itself. We observed that
most expressions of uncertainty are conveyed using
positive polarity gradations such as ?probably? and
?possibly?, for example ?likely?, ?appears to have?,
?signs of?. Lexical cues of low levels of certainty in
the negative polarity were rare.
3.3.2 Opinion source
In Table 2, we report examples of the various in-
dividuals ? dictating physician, dictating physician
with consultation, other clinical care providers, pa-
tient, unknown ? that are the source of the belief
state for uncertainty about a disorder. We observed
explicit judgments or mental postulations e.g., ?I
judge? or implied speculations in which the physi-
cian was not the subject and passive expressions
were used e.g., ?patient appears to have?. In cases
of dictating physician with consultation, the physi-
cian speculated about the disorder using references
to other providers consulted to strengthen the as-
sessment e.g., ?Discussing with Dr...?. In cases of
other clinical care providers, there was no owner-
ship on the part of the dictating physician, but of
other members of the clinical care team e.g., ?Con-
sulting Attending (Infection) thinks...?. In cases for
patient, the patient is conveying statements of con-
fusion with respect to self-diagnosing e.g., ?Pat. re-
ports that she finds it difficult to discern...?. We ob-
served no expressions of uncertainty owned by the
patient in the English data set or by a relative in the
Swedish data set. In the unknown case, it is unclear
from the context of the report whether the specu-
lation is on the part of the physician to believe the
symptom reported or the relative unsure about re-
porting the symptoms e.g., ?there was apparently?.
3.3.3 Evidence evaluation
Below we list examples of the different rea-
sons for uncertainties that were identified. Not all
types were observed in both corpora (Not observed).
limits of evidence, one diagnosis
- evidence contradicts ? English: ?Likely upper GI bleed
with elevated bun, but normal h and h.?; Swedish: ?Kon-
sulterar infektionsjour som anser viros vara osannolikt
med tanke pa? normalt leverstatus. (Consulting Attend-
ing (infection) who thinks that virosis is improbable given
normal liver status.)?
- evidence needed ? English: ?chest x-ray was ordered
to rule out TB.?; Swedish: ?Diskuterar med RAH-jour;
vi bo?rjar utredning med CT-skalle med kontrast pa? mis-
stanke om metastaser och na?gon form av epileptiskt anfall
(Discussion with Attendant [CLINIC]; we start inves-
60
Table 1: Common lexical cues and their relative position to the disorder mention: Pre-disorder: uncertainty marker before disor-
der, Intra-disorder: uncertainty marker inside disorder, Post-disorder: uncertainty marker after disorder, }= schema compatibil-
ity/neutral case.
Table 2: Opinion source of uncertainty or negation types with English and Swedish examples.
tigation with CT-brain with contrast on suspicion for
metastasis and some form of epileptic seizure.)?
- evidence not convincing, but diagnosis asserted ? En-
glish: Not observed; Swedish: ?Fo?rmodligen en viros
eftersom man kan se en viss lymfocytopeni i diff (Proba-
bly a virosis since there is some lymphocyte in blood cell
count.)?
limits of evidence, more than one diagnosis
- differential diagnoses enumerated ? English: ?ques-
tionable right-sided increased density on the right side
of the chest x-ray that could possibly be indicative of
a pneumonia versus increased pulmonary vasculature?;
Swedish: ?Fo?refaller neurologiskt, blo?dning? Infarkt?
(Appears neurological, bleeding? Infarction?)?
limits in source of evidence
- non-clinical source ? English: ?I am not convinced that
he is perfectly clear on his situation..?; Swedish: ?Pat
uppger att hon har sva?rt att skilja pa? panika?ngest och an-
dra symtom. (Pat. reports that she finds it difficult to
discern panick disorder from other symptoms...)?
- clinical source ? English: ?there was no definite diagno-
sis and they thought it was a viral syndrome of unknown
type..?; Swedish: Not observed
- test source ? English: ?..confusion was possible related
a TIA without much facial droop appreciated on my
physical exam?; Swedish: ?Ter sig mest sannolikt som
reumatoid artrit ba?de klinisk och lab-ma?ssigt (Seems like
it most probably is rheumatoid arthritis both clinically
and lab-wise.)?
limitless possibilities ? English: ?I think this is probably a
viral problem.?; Swedish: ?Pat bedo?mes ha en fo?rkylning,
troligen virusinfektion. (Patient is evaluated as having a cold,
probably a virus infection.)?
other
61
- asserting dx or disorder as affirmed ? English: ?I sus-
pect that colon cancer is both the cause of the patient?s
bleeding..?; Swedish: Not observed
- asserting dx or disorder as negated ? English: ?...her
fever has abated.?; Swedish: Not observed
In many cases, the local context was sufficient for
understanding the evidential origins for uncertainty.
When a single disorder was mentioned, uncertainty
was due to data insufficient to make a definitive di-
agnosis because it contradicted a hypothesis, was
unavailable, or was not convincing. For instance,
data was to be ordered and the opportunity to inter-
pret it had not presented itself, such as ?..was or-
dered to rule out TB? or ?..start investigation with
CT-brain with contrast..?. In few cases, more than
one diagnosis was being enumerated due to a lim-
itation in the evidence or data gathered e.g., ?Ap-
pears neurological, bleeding? Infarction??. We ob-
served cases in which the source of the evidence pro-
duced uncertainty including both non-clinical and
clinical sources (care providers consulted and tests
produced). In cases of limitless possibilities, the
physician resorted to a common, default diagnosis
e.g., ?probably a virus infection?. Limitations of ev-
idence from a clinical source were not found in the
Swedish data set and few were found in the English
data set. We expect that more examples of this cat-
egory would be found in e.g. radiology reports in
which the quality of the image is a critical factor in
making an interpretation.
4 Discussion and Conclusion
From the resulting clinical taxonomy and charac-
terization, we observe some general differences and
similarities between the two data sets and languages.
The Swedish assessment entries are more verbose
compared to the English medical records in terms
of a more detailed account of the uncertainty and
what is being done by whom to derive a diagnosis
from a disorder mention. This might reflect cultural
differences in how documentation is both produced
and used. Differential diagnoses are often listed with
question marks (???) in the Swedish set, e.g., ?Dis-
order 1? Disorder 2? Disorder 3??, whereas in the
English data set enumerations are either listed or
competing, e.g., ?disorder 1 vs. disorder 2?. De-
spite these differences, there are many similarities
between the two data sets.
Mapping observations from the English schema
into the Swedish schema was not complicated
despite the difference in the modeled attributes.
In most cases, we determined that designating
attribute-value rules for negation and certainty nor-
malization steps was sufficient to accurately map ob-
servations between the language schemas without
changing an observation?s semantics. This finding
suggests that simple heuristics can be used to trans-
late annotations made from English trained tools
into the Swedish schema values.
The majority of the lexical markers are pre-
positioned in both languages, and the majority of
these markers are similar across the two languages,
e.g., ?likely?, ?possible?, ?suspicion for?. How-
ever, inflections and variants are more common in
Swedish, and the language allows for free word or-
der, this relation needs to be studied further. The
default case, i.e. affirmed, or certainly positive, was
rarely expressed through lexical markers.
When it comes to the opinion source of an un-
certainty or negation, we observed a pattern in the
use of passive voice, e.g. ?it was felt?, indicating
avoidance to commitment in a statement. Accurate
extraction of the opinion source of an expression
has important implications for a system that, for in-
stance, tracks the reasoning about a patient case over
time by source. This has been recognized and incor-
porated in other annotation efforts, for example for
news documents (Saur?? and Pustejovsky, 2009). In
the English data set, no cases of self-diagnosing are
found, i.e. a patient owning the expressed uncer-
tainty. In both data sets, an implicit dictating physi-
cian source is most common, i.e. there is no explicit
use of pronouns indicating the opinion holder. In
most cases it is clear that it is the writer?s (i.e. the
dictating physician?s) opinion that is expressed, but
in some cases, a larger context is needed for this
knowledge to be resolved.
Reviewing the evidential origins or reason for ex-
pressed uncertainty, for both the Swedish and En-
glish data sets, the category ?limits of evidence? is
most common. This reflects a clinical reality, where
many disorders require test results, radiology find-
ings and other similar procedures before ascertain-
ing a diagnosis. Although most cases of uncertainty
are manifested and strengthened through a lexical
62
marker, there are also instances where the uncer-
tainty is evident without such explicit markers, e.g.
the ordering of a test may in itself indicate uncer-
tainty.
4.1 Limitations
There are several limitations of this study. The
Swedish data set only contains parts of the medi-
cal record and the English data set is very small.
In the creation of the taxonomy and characteristics,
we have not focused on studying uncertainty lev-
els, i.e. distinctions between ?possibly? and ?prob-
ably?. The values of our taxonomy are preliminary
and may change as we develop the size of our data
set. Additionally, we only studied emergency de-
partment reports. We need to study other report
types to evaluate the generalizability of the taxon-
omy.
The two compared languages both origin from the
same language family (Germanic), which limits gen-
eralizability for other languages. Furthermore, the
definitions of disorders in the two sets differ to some
extent, i.e., English disorders are related to specific
influenza-like illnesses and Swedish to more general
disorders found in emergency departments.
4.2 Comparison to related work
Annotation schemas and reference standards for un-
certainty and negation have been created from dif-
ferent perspectives, for different levels and pur-
poses. The BioScope Corpus, for instance, contains
sentence-level uncertainty annotations with token-
level annotations for speculation and negation cues,
along with their linguistic scope (Vincze et al,
2008). In Wilbur et al (2006), five qualitative di-
mensions for characterizing biomedical articles are
defined, including levels of certainty. In the 2010
i2b2/VA Challenge on concepts, assertions and re-
lations in clinical texts, medical problem concepts
were annotated. The assertion task included six an-
notation classes (present, absent, possible, hypothet-
ical, conditional, not associated with the patient),
to be assigned to each medical problem concept
(Uzuner et al, 2011). Vincze et al (2011) present
a quantitative comparison of the intersection of two
English corpora annotated for negation and specula-
tion (BioScope and Genia Event) from two different
perspectives (linguistic and event-oriented).
We extend these schemas by characterizing the
underlying meaning and distinctions evident by the
linguistic expressions used to indicate uncertainty
and negation in the clinical domain and by exploring
the relationship between uncertainty and negation,
through an analysis and comparison of two differ-
ent annotation schemas. However, this study is not a
proposal for mapping to these schemas or others.
From an application perspective, uncertainty and
negation handling have been included in rule-based
systems such as NegEx and ConText, applied on dis-
order mentions. In Chapman et al (2011), a gener-
alized version of ConText is presented, with uncer-
tainty values (probably, definitely) linked to either a
positive or negative assertion, with an added indeter-
minate value. A previous study has shown promis-
ing results for adapting NegEx to Swedish (Skepp-
stedt, 2011), indicating that further extensions and
adaptations between the two languages for e.g. un-
certainty modeling should be viable. Machine-
learning based approaches outperform rule-based
for assertion classification according to results pre-
sented in Uzuner et al (2009). A machine-learning
approach was also used in the top performing sys-
tem in the 2010 i2b2/VA Challenge assertion task
(de Bruijn et al, 2011).
4.3 Implications and future work
With uncertainty lexicons for both Swedish and En-
glish, we hypothesize that we will be able to ex-
tend ConText to handle uncertainties in English as
well as in Swedish. This enables both improve-
ments over the existing system and the possibilities
of further comparing system performances between
languages. We will also experiment with machine-
learning approaches to detect and annotate uncer-
tainty and negation. We plan to extend both data
sets, the English data set using semi-automatically
translated disorders marked in the Swedish data set
to encode new disorder mentions, and the Swedish
data set by extracting the full medical records, thus
creating a larger set for comparison. We will extend
the taxonomy as needed e.g., syntactic and semantic
patterns, and investigate how to integrate the clini-
cal taxonomy to inform ConText by providing more
granular descriptions of the motivation behind the
uncertainty, thus bringing us closer to natural lan-
guage understanding.
63
Acknowledgments
For the English and Swedish data sets, we obtained
approval from the University of Pittsburgh IRB and
the Regional Ethical Review Board in Stockholm
(Etikpro?vningsna?mnden i Stockholm). The study is
part of the Interlock project, funded by the Stock-
holm University Academic Initiative and partially
funded by NLM Fellowship 5T15LM007059. Lex-
icons and probabilities will be made available and
updated on the iDASH NLP ecosystem under Re-
sources: http://idash.ucsd.edu/nlp/natural-language-
processing-nlp-ecosystem.
References
B. E. Chapman, S. Lee, H. Peter Kang, and W. W. Chap-
man. 2011. Document-level Classification of CT Pul-
monary Angiography Reports Based on an Extension
of the ConText Algorithm. Journal of Biomedical In-
formatics, 44:728?737.
D. Chu, J.N. Dowling, and WW Chapman. 2006. Eval-
uating the Effectiveness of Four Contextual Features
in Classifying Annotated Clinical Conditions in Emer-
gency Department Reports. In AMIA Annu Symp Proc,
pages 141?145.
C. Clark, J. Aberdeen, M. Coarr, D. Tresner-Kirsh,
B. Wellner, A. Yeh, and L. Hirschman. 2011. MITRE
system for Clinical Assertion Status Classification. J
Am Med Inform Assoc, 11(18):563?567.
B. de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, and
X. Zhu. 2011. Machine-learned Solutions for Three
Stages of Clinical Information Extraction: The State of
the Art at i2b2 2010. Journal of the American Medical
Informatics Association, 18:557?562.
H. Harkema, J. N. Dowling, T. Thornblade, and W. W.
Chapman. 2009. ConText: An Algorithm for De-
termining Negation, Experiencer, and Temporal Status
from Clinical Reports. Journal of Biomedical Infor-
matics, 42:839?851.
L. Lingard, K. Garwood, C. F. Schryer, and M. M. Spaf-
ford. 2003. A Certain Art of Uncertainty: Case Pre-
sentation and the Development of Professional Iden-
tity. Social science medicine, 56(3):603?616.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler, and
John E. Hurdle. 2008. Extracting Information from
Textual Documents in the Electronic Health Record: A
Review of Recent Research. IMIA Yearbook of Medi-
cal Informatics 2008. 47 Suppl 1:138-154.
D. Mowery, P. Jordan, J.M. Wiebe, H. Harkema, and
W.W. Chapman. 2012. Semantic Annotation of Clini-
cal Text: A Pilot Study. Unpublished.
V. L. Rubin, E. D. Liddy, and N. Kando. 2006. Cer-
tainty Identification in Texts: Categorization Model
and Manual Tagging Results. In Computing Affect and
Attitutde in Text: Theory and Applications. Springer.
R. Saur?? and J. Pustejovsky. 2009. FactBank: A Corpus
Annotated with Event Factuality. Language Resources
and Evaluation, 43(3):227?268?268, September.
M. Skeppstedt. 2011. Negation Detection in Swedish
Clinical Text: An Adaptation of NegEx to Swedish.
Journal of Biomedical Semantics, 2(Suppl. 3):S3.
A. L. Strauss and J. Corbin. 1990. Basics of Qual-
itative Research: Grounded Theory Procedures and
Techniques. Sage.
O?. Uzuner, X. Zhang, and T. Sibanda. 2009. Ma-
chine Learning and Rule-based Approaches to Asser-
tion Classification. Journal of the American Medical
Informatics Association, 16(1):109?115.
O?. Uzuner, B. R. South, S. Shen, and S. L. DuVall. 2011.
2010 i2b2/VA Challenge on Concepts, Assertions, and
Relations in Clinical Text. JAMIA, 18(5):552?556.
S. Velupillai, H. Dalianis, and M. Kvist. 2011. Factual-
ity Levels of Diagnoses in Swedish Clinical Text. In
A. Moen, S. K. Andersen, J. Aarts, and P. Hurlen, ed-
itors, Proc. XXIII International Conference of the Eu-
ropean Federation for Medical Informatics (User Cen-
tred Networked Health Care), pages 559 ? 563, Oslo,
August. IOS Press.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The BioScope Corpus: Biomedical Texts An-
notated for Uncertainty, Negation and Their Scopes.
BMC Bioinformatics, 9(S-11).
V. Vincze, G. Szarvas, G. M?ora, T. Ohta, and R. Farkas.
2011. Linguistic Scope-based and Biological Event-
based Speculation and Negation Annotations in the
BioScope and Genia Event Corpora. Journal of
Biomedical Semantics, 2(Suppl. 5):S8.
J. Wiebe, R. Bruce, M. Bell, M. Martin, and T. Wilson.
2001. A Corpus Study of Evaluative and Specula-
tive Language. In Proceedings of the Second SIG-
dial Workshop on Discourse and Dialogue - Volume
16, SIGDIAL ?01, pages 1?10, Stroudsburg, PA, USA.
Association for Computational Linguistics.
J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006.
New Directions in Biomedical Text Annotation: Def-
initions, Guidelines and Corpus Construction. BMC
Bioinformatics, 7:356+, July.
64
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54?58,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Generating Patient Problem Lists from the ShARe Corpus using
SNOMED CT/SNOMED CT CORE Problem List
Danielle Mowery
Janyce Wiebe
University of Pittsburgh
Pittsburgh, PA
dlm31@pitt.edu
wiebe@cs.pitt.edu
Mindy Ross
University of California
San Diego
La Jolla, CA
mkross@ucsd.edu
Sumithra Velupillai
Stockholm University
Stockholm, SE
sumithra@dsv.su.se
Stephane Meystre
Wendy W Chapman
University of Utah
Salt Lake City, UT
stephane.meystre,
wendy.chapman@utah.edu
Abstract
An up-to-date problem list is useful for
assessing a patient?s current clinical sta-
tus. Natural language processing can help
maintain an accurate problem list. For in-
stance, a patient problem list from a clin-
ical document can be derived from indi-
vidual problem mentions within the clin-
ical document once these mentions are
mapped to a standard vocabulary. In
order to develop and evaluate accurate
document-level inference engines for this
task, a patient problem list could be gen-
erated using a standard vocabulary. Ad-
equate coverage by standard vocabularies
is important for supporting a clear rep-
resentation of the patient problem con-
cepts described in the texts and for interop-
erability between clinical systems within
and outside the care facilities. In this
pilot study, we report the reliability of
domain expert generation of a patient
problem list from a variety of clinical
texts and evaluate the coverage of anno-
tated patient problems against SNOMED
CT and SNOMED Clinical Observation
Recording and Encoding (CORE) Prob-
lem List. Across report types, we learned
that patient problems can be annotated
with agreement ranging from 77.1% to
89.6% F1-score and mapped to the CORE
with moderate coverage ranging from
45%-67% of patient problems.
1 Introduction
In the late 1960?s, Lawrence Weed published
about the importance of problem-oriented medi-
cal records and the utilization of a problem list
to facilitate care provider?s clinical reasoning by
reducing the cognitive burden of tracking cur-
rent, active problems from past, inactive problems
from the patient health record (Weed, 1970). Al-
though electronic health records (EHR) can help
achieve better documentation of problem-specific
information, in most cases, the problem list is
manually created and updated by care providers.
Thus, the problem list can be out-of-date con-
taining resolved problems or missing new prob-
lems. Providing care providers with problem list
update suggestions generated from clinical docu-
ments can improve the completeness and timeli-
ness of the problem list (Meystre and Haug, 2008).
In recent years, national incentive and standard
programs have endorsed the use of problem lists
in the EHR for tracking patient diagnoses over
time. For example, as part of the Electronic Health
Record Incentive Program, the Center for Medi-
care and Medicaid Services defined demonstra-
tion of Meaningful Use of adopted health infor-
mation technology in the Core Measure 3 objec-
tive as ?maintaining an up-to-date problem list of
current and active diagnoses in addition to histor-
ical diagnoses relevant to the patients care? (Cen-
ter for Medicare and Medicaid Services, 2013).
More recently, the Systematized Nomenclature of
Medicine Clinical Terms (SNOMED CT) has be-
come the standard vocabulary for representing and
documenting patient problems within the clinical
record. Since 2008, this list is iteratively refined
four times each year to produce a subset of gen-
eralizable clinical problems called the SNOMED
CT CORE Problem List. This CORE list repre-
sents the most frequent problem terms and con-
cepts across eight major healthcare institutions in
the United States and is designed to support in-
teroperability between regional healthcare institu-
tions (National Library of Medicine, 2009).
In practice, there are several methodologies ap-
plied to generate a patient problem list from clin-
ical text. Problem lists can be generated from
coded diagnoses such as the International Statis-
tical Classification of Disease (ICD-9 codes) or
54
concept labels such as Unified Medical Language
System concept unique identifiers (UMLS CUIs).
For example, Meystre and Haug (2005) defined 80
of the most frequent problem concepts from coded
diagnoses for cardiac patients. This list was gen-
erated by a physician and later validated by two
physicians independently. Coverage of coded pa-
tient problems were evaluated against the ICD-9-
CM vocabulary. Solti et al. (2008) extended the
work of Meystre and Haug (2005) by not limit-
ing the types of patient problems from any list
or vocabulary to generate the patient problem list.
They observed 154 unique problem concepts in
their reference standard. Although both studies
demonstrate valid methods for developing a pa-
tient problem list reference standard, neither study
leverages a standard vocabulary designed specifi-
cally for generating problem lists.
The goals of this study are 1) determine how
reliably two domain experts can generate a pa-
tient problem list leveraging SNOMED CT from
a variety of clinical texts and 2) assess the cover-
age of annotated patient problems from this corpus
against the CORE Problem List.
2 Methods
In this IRB-approved study, we obtained the
Shared Annotated Resource (ShARe) corpus
originally generated from the Beth Israel Dea-
coness Medical Center (Elhadad et al., un-
der review) and stored in the Multiparameter
Intelligent Monitoring in Intensive Care, ver-
sion 2.5 (MIMIC II) database (Saeed et al.,
2002). This corpus consists of discharge sum-
maries (DS), radiology (RAD), electrocardiogram
(ECG), and echocardiogram (ECHO) reports from
the Intensive Care Unit (ICU). The ShARe cor-
pus was selected because it 1) contains a variety of
clinical text sources, 2) links to additional patient
structured data that can be leveraged for further
system development and evaluation, and 3) has en-
coded individual problem mentions with semantic
annotations within each clinical document that can
be leveraged to develop and test document-level
inference engines. We elected to study ICU pa-
tients because they represent a sensitive cohort that
requires up-to-date summaries of their clinical sta-
tus for providing timely and effective care.
2.1 Annotation Study
For this annotation study, two annotators - a physi-
cian and nurse - were provided independent train-
ing to annotate clinically relevant problems e.g.,
signs, symptoms, diseases, and disorders, at the
document-level for 20 reports. The annotators
were given feedback based on errors over two it-
erations. For each patient problem in the remain-
ing set, the physician was instructed to review the
full text, span the a problem mention, and map the
problem to a CUI from SNOMED-CT using the
extensible Human Oracle Suite of Tools (eHOST)
annotation tool (South et al., 2012). If a CUI did
not exist in the vocabulary for the problem, the
physician was instructed to assign a ?CUI-less? la-
bel. Finally, the physician then assigned one of
five possible status labels - Active, Inactive, Re-
solved, Proposed, and Other - based on our pre-
vious study (Mowery et al., 2013) to the men-
tion representing its last status change at the con-
clusion of the care encounter. Patient problems
were not annotated as Negated since patient prob-
lem concepts are assumed absent at a document-
level (Meystre and Haug, 2005). If the patient
was healthy, the physician assigned ?Healthy - no
problems? to the text. To reduce the cognitive bur-
den of annotation and create a more robust refer-
ence standard, these annotations were then pro-
vided to a nurse for review. The nurse was in-
structed to add missing, modify existing, or delete
spurious patient problems based on the guidelines.
We assessed how reliably annotators agreed
with each other?s patient problem lists using inter-
annotator agreement (IAA) at the document-level.
We evaluated IAA in two ways: 1) by problem
CUI and 2) by problem CUI and status. Since
the number of problems not annotated (i.e., true
negatives (TN)) are very large, we calculated F1-
score as a surrogate for kappa (Hripcsak and Roth-
schild, 2005). F1-score is the harmonic mean of
recall and precision, calculated from true posi-
tive, false positive, and false negative annotations,
which were defined as follows:
true positive (TP) = the physician and nurse prob-
lem annotation was assigned the same CUI
(and status)
false positive (FP) = the physician problem anno-
tation (and status) did not exist among the
nurse problem annotations
55
false negative (FN) = the nurse problem anno-
tation (and status) did not exist among the
physician problem annotations
Recall =
TP
(TP + FN)
(1)
Precision =
TP
(TP + FP )
(2)
F1-score =
2
(Recall ? Precision)
(Recall + Precision)
(3)
We sampled 50% of the corpus and determined
the most common errors. These errors with
examples were programmatically adjudicated
with the following solutions:
Spurious problems: procedures
solution: exclude non-problems via guidelines
Problem specificity: CUI specificity differences
solution: select most general CUIs
Conflicting status: negated vs. resolved
solution: select second reviewer?s status
CUI/CUI-less: C0031039 vs. CUI-less
solution: select CUI since clinically useful
We split the dataset into about two-thirds train-
ing and one-third test for each report type. The re-
maining data analysis was performed on the train-
ing set.
2.2 Coverage Study
We characterized the composition of the reference
standard patient problem lists against two stan-
dard vocabularies SNOMED-CT and SNOMED-
CT CORE Problem List. We evaluated the cover-
age of patient problems against the SNOMED CT
CORE Problem List since the list was developed
to support encoding clinical observations such as
findings, diseases, and disorders for generating pa-
tient summaries like problem lists. We evaluated
the coverage of patient problems from the corpus
against the SNOMED-CT January 2012 Release
which leverages the UMLS version 2011AB. We
assessed recall (Eq 1), defining a TP as a patient
problem CUI occurring in the vocabulary and a
FN as a patient problem CUI not occurring in the
vocabulary.
3 Results
We report the results of our annotation study on
the full set and vocabulary coverage study on the
training set.
3.1 Annotation Study
The full dataset is comprised of 298 clinical doc-
uments - 136 (45.6%) DS, 54 (18.1%) ECHO,
54 (18.1%) RAD, and 54 (18.1%) ECG. Seventy-
four percent (221) of the corpus was annotated by
both annotators. Table 1 shows agreement overall
and by report, matching problem CUI and prob-
lem CUI with status. Inter-annotator agreement
for problem with status was slightly lower for all
report types with the largest agreement drop for
DS at 15% (11.6 points).
Report Type CUI CUI + Status
DS 77.1 65.5
ECHO 83.9 82.8
RAD 84.7 82.8
ECG 89.6 84.8
Table 1: Document-level IAA by report type for problem
(CUI) and problem with status (CUI + status)
We report the most common errors by frequency
in Table 2. By report type, the most common er-
rors for ECHO, RAD, and ECG were CUI/CUI-
less, and DS was Spurious Concepts.
Errors DS ECHO RAD ECG
SP 423 (42%) 26 (23%) 30 (35%) 8 (18%)
PS 139 (14%) 31 (27%) 8 (9%) 0 (0%)
CS 318 (32%) 9 (8%) 8 (9%) 14 (32%)
CC 110 (11%) 34 (30%) 37 (44%) 22 (50%)
Other 6 (>1%) 14 (13%) 2 (2%) 0 (0%)
Table 2: Error types by frequency - Spurious Problems (SP),
Problem Specificity (PS), Conflicting status (CS), CUI/CUI-
less (CC)
3.2 Coverage Study
In the training set, there were 203 clinical docu-
ments - 93 DS, 37 ECHO, 38 RAD, and 35 ECG.
The average number of problems were 22?10 DS,
10?4 ECHO, 6?2 RAD, and 4?1 ECG. There
are 5843 total current problems in SNOMED-CT
CORE Problem List. We observed a range of
unique SNOMED-CT problem concept frequen-
cies: 776 DS, 63 ECHO, 113 RAD, and 36 ECG
56
by report type. The prevalence of covered prob-
lem concepts by CORE is 461 (59%) DS, 36
(57%) ECHO, 71 (63%) RAD, and 16 (44%)
ECG. In Table 3, we report coverage of patient
problems for each vocabulary. No reports were
annotated as ?Healthy - no problems?. All reports
have SNOMED CT coverage of problem mentions
above 80%. After mapping problem mentions to
CORE, we observed coverage drops for all report
types, 24 to 36 points.
Report Patient Annotated with Mapped to
Type Problems SNOMED CT CORE
DS 2000 1813 (91%) 1335 (67%)
ECHO 349 300 (86%) 173 (50%)
RAD 190 156 (82%) 110 (58%)
ECG 95 77(81%) 43 (45%)
Table 3: Patient problem coverage by SNOMED-CT and
SNOMED-CT CORE
4 Discussion
In this feasibility study, we evaluated how reliably
two domain experts can generate a patient problem
list and assessed the coverage of annotated patient
problems against two standard clinical vocabular-
ies.
4.1 Annotation Study
Overall, we demonstrated that problems can be re-
liably annotated with moderate to high agreement
between domain experts (Table 1). For DS, agree-
ment scores were lowest and dropped most when
considering the problem status in the match crite-
ria. The most prevalent disagreement for DS was
Spurious problems (Table 2). Spurious problems
included additional events (e.g., C2939181: Mo-
tor vehicle accident), procedures (e.g., C0199470:
Mechanical ventilation), and modes of administra-
tion (e.g., C0041281: Tube feeding of patient) that
were outside our patient problem list inclusion cri-
teria. Some pertinent findings were also missed.
These findings are not surprising given on average
more problems occur in DS and the length of DS
documents are much longer than other document
types. Indeed, annotators are more likely to miss
a problem as the number of patient problems in-
crease.
Also, status differences can be attributed to mul-
tiple status change descriptions using expressions
of time e.g., ?cough improved then? and modal-
ity ?rule out pneumonia?, which are harder to
track and interpret over a longer document. The
most prevalent disagreements for all other doc-
ument types were CUI/CUI-less in which iden-
tifying a CUI representative of a clinical obser-
vation proved more difficult. An example of
Other disagreement was a sidedness mismatch
or redundant patient problem annotation. For
example, C0344911: Left ventricular dilatation
vs. C0344893: Right ventricular dilatation or
C0032285: Pneumonia was recorded twice.
4.2 Coverage Study
We observed that DS and RAD reports have higher
counts and coverage of unique patient problem
concepts. We suspect this might be because other
document types like ECG reports are more likely
to have laboratory observations, which may be
less prevalent findings in CORE. Across document
types, coverage of patient problems in the corpus
by SNOMED CT were high ranging from 81%
to 91% (Table 3). However, coverage of patient
problems by CORE dropped to moderate cover-
ages ranging from 45% to 67%. This suggests that
the CORE Problem List is more restrictive and
may not be as useful for capturing patient prob-
lems from these document types. A similar report
of moderate problem coverage with a more restric-
tive concept list was also reported by Meystre and
Haug (2005).
5 Limitations
Our study has limitations. We did not apply a tra-
ditional adjudication review between domain ex-
perts. In addition, we selected the ShARe corpus
from an ICU database in which vocabulary cover-
age of patient problems could be very different for
other domains and specialties.
6 Conclusion
Based on this feasibility study, we conclude that
we can generate a reliable patient problem list
reference standard for the ShARe corpus and
SNOMED CT provides better coverage of patient
problems than the CORE Problem List. In fu-
ture work, we plan to evaluate from each ShARe
report type, how well these patient problem lists
can be derived and visualized from the individ-
ual disease/disorder problem mentions leveraging
temporality and modality attributes using natu-
ral language processing and machine learning ap-
proaches.
57
Acknowledgments
This work was partially funded by NLM
(5T15LM007059 and 1R01LM010964), ShARe
(R01GM090187), Swedish Research Council
(350-2012-6658), and Swedish Fulbright Com-
mission.
References
Center for Medicare and Medicaid Services. 2013.
EHR Incentive Programs-Maintain Problem
List. http://www.cms.gov/Regulations-and-
Guidance/Legislation/EHRIncentivePrograms/
downloads/3 Maintain Problem ListEP.pdf.
Noemie Elhadad, Wendy Chapman, Tim OGorman,
Martha Palmer, and Guergana. Under Review
Savova. under review. The ShARe Schema for
the Syntactic and Semantic Annotation of Clinical
Texts.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-measure, and Reliability in In-
formation Retrieval. J Am Med Inform Assoc,
12(3):296?298.
Stephane Meystre and Peter Haug. 2005. Automation
of a Problem List using Natural Language Process-
ing. BMC Medical Informatics and Decision Mak-
ing, 5(30).
Stephane M. Meystre and Peter J. Haug. 2008. Ran-
domized Controlled Trial of an Automated Problem
List with Improved Sensitivity. International Jour-
nal of Medical Informatics, 77:602?12.
Danielle L. Mowery, Pamela W. Jordan, Janyce M.
Wiebe, Henk Harkema, John Dowling, and
Wendy W. Chapman. 2013. Semantic Annotation
of Clinical Events for Generating a Problem List. In
AMIA Annu Symp Proc, pages 1032?1041.
National Library of Medicine. 2009. The
CORE Problem List Subset of SNOMED-
CT. Unified Medical Language System 2011.
http://www.nlm.nih.gov/research/umls/SNOMED-
CT/core subset.html.
Mohammed Saeed, C. Lieu, G. Raber, and Roger G.
Mark. 2002. MIMIC II: a massive temporal ICU
patient database to support research in intelligent pa-
tient monitoring. Comput Cardiol, 29.
Imre Solti, Barry Aaronson, Grant Fletcher, Magdolna
Solti, John H. Gennari, Melissa Cooper, and Thomas
Payne. 2008. Building an Automated Problem List
based on Natural Language Processing: Lessons
Learned in the Early Phase of Development. pages
687?691.
Brett R. South, Shuying Shen, Jianwei Leng, Tyler B.
Forbush, Scott L. DuVall, and Wendy W. Chapman.
2012. A prototype tool set to support machine-
assisted annotation. In Proceedings of the 2012
Workshop on Biomedical Natural Language Pro-
cessing, BioNLP ?12, pages 130?139. Association
for Computational Linguistics.
Lawrence Weed. 1970. Medical Records, Med-
ical Education and Patient Care: The Problem-
Oriented Record as a Basic Tool. Medical Pub-
lishers: Press of Case Western Reserve University,
Cleveland: Year Book.
58
