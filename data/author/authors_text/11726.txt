Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 95?99,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
MATREX: The DCU MT System for WMT 2009
Jinhua Du, Yifan He, Sergio Penkale, Andy Way
Centre for Next Generation Localisation
Dublin City University
Dublin 9, Ireland
{jdu,yhe,spenkale,away}@computing.dcu.ie
Abstract
In this paper, we describe the machine
translation system in the evaluation cam-
paign of the Fourth Workshop on Statisti-
cal Machine Translation at EACL 2009.
We describe the modular design of our
multi-engine MT system with particular
focus on the components used in this par-
ticipation.
We participated in the translation task
for the following translation directions:
French?English and English?French, in
which we employed our multi-engine ar-
chitecture to translate. We also partic-
ipated in the system combination task
which was carried out by the MBR de-
coder and Confusion Network decoder.
We report results on the provided devel-
opment and test sets.
1 Introduction
In this paper, we present a multi-engine MT
system developed at DCU, MATREX (Machine
Translation using Examples). This system exploits
EBMT, SMT and system combination techniques
to build a cascaded translation framework.
We participated in both the French?English and
English-French News tasks. In these two tasks,
we employ three individual MT system which are
1) Baseline: phrase-based system (PB); 2) EBMT:
Monolingually chunking both source and target
sides of the dataset using a marker-based chun-
ker (Gough and Way, 2004). 3) HPB: a typical
hierarchical phrase-based system (Chiang, 2005).
Meanwhile, we also use a word-level combina-
tion framework (Rosti et al, 2007) to combine the
multiple translation hypotheses and employ a new
rescoring model to generate the final result.
For the system combination task, we first use
the minimum Bayes-risk (MBR) (Kumar and
Byrne, 2004) decoder to select the best hypothe-
sis as the alignment reference for the Confusion
Network (CN) (Mangu et al, 2000). We then build
the CN using the TER metric (Snover et al, 2006),
and finally search and generate the translation.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the multi-engine strate-
gies used for the shared task. In Section 3, we
outline the complete system setup for the shared
task and provide results on the development and
test sets. Section 4 is our conclusion.
2 The MATREX System
2.1 System Architecture
The MATREX system is a combination-based
multi-engine architecture, which exploits aspects
of both the EBMT and SMT paradigms.
This architecture includes three individual sys-
tems which are phrase-based, example-based and
hierarchical phrase-based.
The combination structure is the MBR decoder
and CN decoder, which is based on the word-level
combination strategy.
In the final stage, we use a new rescoring mod-
ule to process the N -best list generated by the
combination module. See Figure 1 as a detailed
illustration.
2.2 Example-Based Machine Translation
EBMT obtains resources using the Marker Hy-
pothesis (Green, 1979), a psycholinguistic con-
straint which posits that all languages are marked
for surface syntax by a specific closed set of lex-
emes or morphemes which signify context. Given
a set of closed-class words we segment each sen-
tence into chunks, creating a chunk at each new
occurrence of a marker word, with the restriction
that each segment must contain at least one non-
marker word (Gough and Way, 2004).
95
Mutiple 1-best
MBR Decoder
CN/MERT
System 
Combination
HPB Baseline EBMT
Dev/MERT
Decoding
Rescore/MERT
Rescore/MERT
TestSet
Recaser
Rescore
Mutiple 1-best
MBR Decoder
CN Decoder
Rescore
Recaser
Figure 1: System Framework
We then align these segments using an edit-
distance-style algorithm, in which the insertion
and deletion probabilities depend on word-to-
word translation probabilities and word-to-word
cognates (Stroppa and Way, 2006).
We extracted phrases of at most 7 words on
each side. We then merged these phrases with the
phrases extracted by the baseline system adding
word alignment information, and used this system
seeded with this additional information.
2.3 Hierarchical Machine Translation
HPB translation system is a re-implementation of
the hierarchical phrase translation model which is
based on PSCFG (Chiang, 2005). We generate re-
cursively PSCFG rules from the initial rules as
N ? f1 . . . fm/e1 . . . en
where N is a rule which is initial or includes non-
terminals.
M ? fi . . . fj/eu . . . ev
where 1 ? i ? j ? m and 1 ? u ? v ? n, at
which point a new rule can be obtained, named,
N ? f i?11 Xkfmj+1/eu?11 Xkenv+1
where k is an index for the nonterminal X . The
number of nonterminals permitted in a rule is no
more than two.
When extracting hierarchical rules,we set some
limitations that initial rules are of no more than
7 words in length and other rules should have
no more than 5 terminals and nonterminals, and
we disallow rules with adjacent source-side and
target-side nonterminals.
The decoder is an enhanced CYK-style chart
parser that maximizes the derivation probability
and spans up to 12 source words. A 4-gram lan-
guage model generated by SRI Language Model-
ing toolkit (SRILM) (Stolcke, 2002) is used in the
cube-pruning process. The search space is pruned
with a chart cell size limit of 50.
2.4 System Combination
For multiple system combination, we implement
an MBR-CN framework as shown in Figure 1. In-
stead of using a single system output as the skele-
ton, we employ a minimum Bayes-risk decoder
to select the best single system output from the
merged N -best list by minimizing the BLEU (Pa-
pineni et al, 2002) loss.
The confusion network is built by the output of
MBR as the backbone which determines the word
order of the combination. The other hypotheses
are aligned against the backbone based on the TER
metric. NULL words are allowed in the alignment.
Each arc in the CN represents an alternative word
at that position in the sentence and the number of
votes for each word is counted when constructing
the network. The features we used are as follows:
? word posterior probability (Fiscus, 1997);
? 3, 4-gram target language model;
? word length penalty;
? Null word length penalty;
Also, we use MERT (Och, 2003) to tune the
weights of confusion network.
2.5 Rescore
Rescore is a very important part in post-processing
which can select a better hypothesis from the N -
best list. We add some new global features in
rescore model. The features we used are as fol-
lows:
? Direct and inverse IBM model;
? 3, 4-gram target language model;
? 3, 4, 5-gram POS language model (Ratna-
parkhi, 1996; Schmid, 1994);
96
? Sentence length posterior probability (Zens
and Ney, 2006);
? N -gram posterior probabilities within the N -
Best list (Zens and Ney, 2006);
? Minimum Bayes Risk probability;
? Length ratio between source and target sen-
tence;
The weights are optimized via MERT algorithm.
3 Experimental Setup
The following section describes the system and
experimental setup for the French-English and
English-French translation tasks.
3.1 Statistics of Data
Parallel Corpus
We used Europarl and Giga data for this evalua-
tion. The statistics of parallel data are shown in
Table 1.
Corpra Sen Token-En Token-Fr Len
Europarl 1.46M 39,240,672 42,252,067 80
Giga 2M 48,648,104 57,869,002 65
Table 1: Statistics of Parallel Data
In this table, Sen indicates the number of sentence
pairs; Len denotes the maximum sentence length
of each corpus. This year the translation task is
only evaluated on News Domain. Experimental re-
sults showed that giga data is more correlated than
Europarl and the BLEU score is significantly im-
proved(See Table 4).
Monolingual Corpus
In this evaluation, we trained a small 4-gram lan-
guage model using data in Table 1 and a large 4-
gram language model using data in Table 2. We
configured these two LMs for Baseline and EBMT
systems while HPB only used the large one.
Language Sen Token Source
English 9,966,838 240,849,221 E/N/NC
French 9,966,838 260,520,313 E/N/NC
Table 2: Statistics of Monolingual Data
In the above table, E/N/NC refers to Eu-
roparl/News/New Commentary corpus.
3.2 Pre-Processing
We preprocessed both Europarl and Giga Release
1 corpus. For the Europarl corpus, we removed
the reserved characters in GIZA++ and tokenized
and lowercased the corpus with tools provided by
WMT09. The Giga corpus was too large for our
resource, so we performed sentence selection be-
fore cleaning, in the following steps.
? We split the Giga corpus into even segments,
each segment consisting of 20 lines.
? We trained an SVM classifier on English side
with positive examples from the monolin-
gual news data and negative examples from
noisy sentences (numbers, meaningless word
combinations, and random segments) from
the Giga corpus. We used ?-ly? and ?-ing?
to approximate adverbs and present partici-
ples and did not use other POS-induced fea-
tures, as in (Ferizis and Bailey, 2006). We
added these features to remove noise: aver-
age length of sentences, frequency of capital-
ized characters, frequency of numerical char-
acters and short word penalty (equals to 1
when average length of words < 4, and 0
otherwise). We used the classifier to remove
20% segments of lowest scores.
? We selected 1, 600 words having the highest
mutual information scores with monolingual
training data against the Giga corpus.
? We selected 100, 000 segments where these
words occurred most frequently. However
the sentence was dropped if the length ratio
between English and French was larger than
1.5 or less than 0.67.
3.3 System Configuration
The two language models were done using the
SRILM employing linear interpolation and modi-
fied K-N discounting (Chen and Goodman, 1996).
The configuration for the three systems is listed
in Table 3.
System P-Table Length LM Features
Baseline-E 55.9M 7 2 15
Baseline-G 58.4M 7 2 15
EBMT 59.4M 7 2 15
HPB 122M 5 1 8
Table 3: Statistics of MT Systems
In this table, E indicates the Europarl corpus
97
which is used for all three systems, and G stands
for the Giga corpus which is only used for the
Baseline system. We can see from Table 3 that
the size of the HPB phrase-table is more than 2
times as large as the other phrase tables. How to
filter and process such a huge hierarchical table is
a challenging problem.
We tuned our systems on the development set
devset2009-a and devset2009-b, and performed
the crossover experiment by these two devsets.
3.4 Experimental Results
The system output is evaluated with respect to
BLEU score. In Table 4, we used devset2009-b
to tune the various parameters in our three single
systems and devset2009-a for testing. In terms of
the Europarl data, we can see that the three sys-
tems we used achieved similar performance on the
test set for both translation directions, with the
Baseline-E system yielding slightly better results
than the other two.
System Fr-En En-Fr
Baseline-E 22.24 22.68
Baseline-G 24.90 ?1
EBMT 22.04 22.12
HPB 21.69 21.12
MBR 25.11 22.68
CN 25.24 22.76
Rescore 25.40 22.97
Table 4: Experimental Results on Devset2009-a
We then used the translations of the devset2009-
a produced by each system to tune the parame-
ters of our system combination module. From Ta-
ble 4, we can see that using MBR and confusion
network decoding leads to a slight improvement
over the strongest single system, i.e. the baseline
Phrase-Based SMT system. Rescoring the N -best
lists yielded an increase of 0.5 (2.0 relative) ab-
solute BLEU points over the baseline for French?
English Translation and 0.29 (1.28 relative) abso-
lute BLEU points for English?French Translation.
Table 5 is the results on 2009 Test Data. The
scores with a slash in the last two rows are low-
ercased and cased respectively. From the table we
1Not much time to do the experiments on English-French
direction. EBMT and HPB just used the Europarl corpus.
2The official automatic result is scored on 2525 sentences
out of the whole 3007 sentences in test set. The other 502
sentences are used as the development set for combination
evaluation task.
System Fr-En En-Fr
Baseline-E 25.64 24.47
Baseline-G 26.75 ?
EBMT 25.67 24.43
HPB 25.20 24.19
Combination 27.20/25.14 25.26/22.28
Official-Auto2 26.86/24.93 23.78/22.14
Table 5: Summary of Results on 2009 Test Data
can see that combination yielded 0.45 and 0.79 ab-
solute BLEU points over the best single system for
Fr-En and En-Fr direction respectively. However,
1.93 (7.2 relative) and 1.64 (6.58 relative) BLEU
points are dropped between cased and lowercased
results of both directions. Accordingly, training an
effective recasing model is very important for our
future work.
4 Conclusion
This paper presents our machine translation sys-
tem in WMT2009 shared task campaign. We de-
veloped a multi-engine framework which com-
bined the output results of the three MT sys-
tems and generated a new N -best list after CN
decoding. Then by using some global features
the rescoring model generated the final translation
output. The experimental result proved that the
combination module and rescoring module are ef-
fective in our framework.
We also applied simple yet effective methods
of genre and topical classification to remove noise
and out-of-domain sentences in the Giga corpus,
from which we built better translation models than
from Europarl.
In future work, we will refine our system frame-
work to investigate its effect on the tasks pre-
sented here, and we will develop more powerful
post-processing tools such as recaser to reduce the
BLEU loss.
Acknowledgments
This work is supported by Science Foundation Ireland (Grant
No. 07/CE/I1142). Thanks also to the reviewers for their
insightful comments and suggestions.
References
Chen, S. F. and Goodman, J. (1996). An Empirical Study of
Smoothing Techniques for Language Modeling. In Pro-
ceedings of the Thirty-Fourth Annual Meeting of the As-
sociation for Computational Linguistics, pages 310?318,
San Francisco, CA.
Chiang, D. (2005). A Hierarchical Phrase-Based Model for
Statistical Machine Translation. In Proceedings of the
98
43rd Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 263?270, Ann Arbor,
MI.
Ferizis, G. and Bailey, P. (2006). Towards practical genre
classification of web documents. In Proceedings of the
15th international conference on World Wide Web (WWW
?06), pages 1013?1014, New York, USA.
Fiscus, J. G. (1997). A post-processing system to yield re-
duced word error rates: Recognizer output voting error
reduction (ROVER). In Proceedings 1997 IEEE Work-
shop on Automatic Speech Recognition and Understand-
ing (ASRU), pages 347?352, Santa Barbara, CA.
Gough, N. and Way, A. (2004). Robust Large-Scale EBMT
with Marker-Based Segmentation. In Proceedings of
the 10th International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI-04),
pages 95?104, Baltimore, MD.
Green, T. (1979). The Necessity of Syntax Markers. Two
experiments with artificial languages. Journal of Verbal
Learning and Behavior, 18:481?496.
Kumar, S. and Byrne, W. (2004). Minimum Bayes-Risk De-
coding for Statistical Machine Translation. In Proceed-
ings of the Joint Meeting of the Human Language Tech-
nology Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL
2004), pages 169?176, Boston, MA.
Mangu, L., Brill, E., and Stolcke, A. (2000). Finding con-
sensus in speech recognition: Word error minimization
and other applications of confusion networks. Computer
Speech and Language, 14(4):373?400.
Och, F. (2003). Minimum error rate training in statistical
machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002).
BLEU: a Method for Automatic Evaluation of Machine
Translation. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-02),
pages 311?318, Philadelphia, PA.
Ratnaparkhi, A. (1996). A Maximum Entropy Model for
Part-Of-Speech Tagging. In Proceedings of the Empiri-
cal Methods in Natural Language Processing Conference
(EMNLP), pages 133?142, Philadelphia, PA.
Rosti, A.-V. I., Xiang, B., Matsoukas, S., Schwartz, R., Ayan,
N. F., and Dorr, B. J. (2007). Combining outputs from
multiple machine translation systems. In Proceedings
of the Joint Meeting of the Human Language Technol-
ogy Conference and the North American Chapter of the
Association for Computational Linguistics (HLT-NAACL
2007), pages 228?235, Rochester, NY.
Schmid, H. (1994). Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In Proceedings of International Con-
ference on New Methods in Language Processing, pages
44?49, Manchester, UK.
Snover, M., Dorr, B., Schwartz, R., Micciula, L., and
Makhoul, J. (2006). A study of translation edit rate with
targeted human annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Translation in the
Americas (AMTA 2006), pages 223?231, Cambridge, MA.
Stolcke, A. (2002). SRILM - An Extensible Language Mod-
eling Toolkit. In Proceedings of the International Confer-
ence Spoken Language Processing, pages 901?904, Den-
ver, CO.
Stroppa, N. and Way, A. (2006). MaTrEx: the DCU machine
translation system for IWSLT 2006. In Proceedings of the
International Workshop on Spoken Language Translation,
pages 31?36, Kyoto, Japan.
Zens, R. and Ney, H. (2006). N-gram Posterior Probabilities
for Statistical Machine Translation. In Proceedings of the
Joint Meeting of the Human Language Technology Con-
ference and the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL 2006),
pages 72?77, New York, USA.
99
Coling 2010: Poster Volume, pages 374?382,
Beijing, August 2010
Integrating N-best SMT Outputs into a TM System
Yifan He Yanjun Ma Andy Way Josef van Genabith
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yhe,yma,away,josef}@computing.dcu.ie
Abstract
In this paper, we propose a novel frame-
work to enrich Translation Memory (TM)
systems with Statistical Machine Trans-
lation (SMT) outputs using ranking. In
order to offer the human translators mul-
tiple choices, instead of only using the
top SMT output and top TM hit, we
merge the N-best output from the SMT
system and the k-best hits with highest
fuzzy match scores from the TM sys-
tem. The merged list is then ranked ac-
cording to the prospective post-editing ef-
fort and provided to the translators to aid
their work. Experiments show that our
ranked output achieve 0.8747 precision at
top 1 and 0.8134 precision at top 5. Our
framework facilitates a tight integration
between SMT and TM, where full advan-
tage is taken of TM while high quality
SMT output is availed of to improve the
productivity of human translators.
1 Introduction
Translation Memories (TM) are databases that
store translated segments. They are often used to
assist translators and post-editors in a Computer
Assisted Translation (CAT) environment by re-
turning the most similar translated segments. Pro-
fessional post-editors and translators have long
been relying on TMs to avoid duplication of work
in translation.
With the rapid development in statistical ma-
chine translation (SMT), MT systems are begin-
ning to generate acceptable translations, espe-
cially in domains where abundant parallel corpora
exist. It is thus natural to ask if these translations
can be utilized in some way to enhance TMs.
However advances in MT are being adopted
only slowly and sometimes somewhat reluctantly
in professional localization and post-editing envi-
ronments because of 1) the usefulness of the TM,
2) the investment and effort the company has put
into TMs, and 3) the lack of robust SMT confi-
dence estimation measures which are as reliable
as fuzzy match scores (cf. Section 4.1.2) used in
TMs. Currently the localization industry relies on
TM fuzzy match scores to obtain both a good ap-
proximation of post-editing effort and an estima-
tion of the overall translation cost.
In a forthcoming paper, we propose a trans-
lation recommendation model to better integrate
MT outputs into a TM system. Using a binary
classifier, we only recommend an MT output to
the TM-user when the classifier is highly confi-
dent that it is better than the TM output. In this
framework, post-editors continue to work with the
TM while benefiting from (better) SMT outputs;
the assets in TMs are not wasted and TM fuzzy
match scores can still be used to estimate (the up-
per bound of) post-editing labor.
In the previous work, the binary predictor
works on the 1-best output of the MT and TM sys-
tems, presenting either the one or the other to the
post-editor. In this paper, we develop the idea fur-
ther by moving from binary prediction to ranking.
We use a ranking model to merge the k-best lists
of the two systems, and produce a ranked merged
374
list for post-editing. As the list is an enriched ver-
sion of the TM?s k-best list, the TM related assets
are better preserved and the cost estimation is still
valid as an upper bound.
More specifically, we recast SMT-TM integra-
tion as a ranking problem, where we apply the
Ranking SVM technique to produce a ranked list
of translations combining the k-best lists of both
the MT and the TM systems. We use features in-
dependent of the MT and TM systems for rank-
ing, so that outputs from MT and TM can have
the same set of features. Ideally the transla-
tions should be ranked by their associated post-
editing efforts, but given the very limited amounts
of human annotated data, we use an automatic
MT evaluation metric, TER (Snover et al, 2006),
which is specifically designed to simulate post-
editing effort to train and test our ranking model.
The rest of the paper is organized as follows:
we first briefly introduce related research in Sec-
tion 2, and review Ranking SVMs in Section 3.
The formulation of the problem and experiments
with the ranking models are presented in Sections
4 and 5. We analyze the post-editing effort ap-
proximated by the TER metric in Section 6. Sec-
tion 7 concludes and points out avenues for future
research.
2 Related Work
There has been some work to help TM users to
apply MT outputs more smoothly. One strand is
to improve the MT confidence measures to bet-
ter predict post-editing effort in order to obtain a
quality estimation that has the potential to replace
the fuzzy match score in the TM. To the best of
our knowledge, the first paper in this area is (Spe-
cia et al, 2009a), which uses regression on both
the automatic scores and scores assigned by post-
editors. The method is improved in (Specia et
al., 2009b), which applies Inductive Confidence
Machines and a larger set of features to model
post-editors? judgment of the translation quality
between ?good? and ?bad?, or among three levels
of post-editing effort.
Another strand is to integrate high confidence
MT outputs into the TM, so that the ?good? TM
entries will remain untouched. In our forthcoming
paper, we recommend SMT outputs to a TM user
when a binary classifier predicts that SMT outputs
are more suitable for post-editing for a particular
sentence.
The research presented here continues the line
of research in the second strand. The difference
is that we do not limit ourselves to the 1-best out-
put but try to produce a k-best output in a rank-
ing model. The ranking scheme also enables us
to show all TM hits to the user, and thus further
protects the TM assets.
There has also been work to improve SMT us-
ing the knowledge from the TM. In (Simard and
Isabelle, 2009), the SMT system can produce a
better translation when there is an exact or close
match in the corresponding TM. They use regres-
sion Support Vector Machines to model the qual-
ity of the TM segments. This is also related to
our work in spirit, but our work is in the opposite
direction, i.e. using SMT to enrich TM.
Moreover, our ranking model is related to
reranking (Shen et al, 2004) in SMT as well.
However, our method does not focus on produc-
ing better 1-best translation output for an SMT
system, but on improving the overall quality of the
k-best list that TM systems present to post-editors.
Some features in our work are also different in na-
ture to those used in MT reranking. For instance
we cannot use N-best posterior scores as they do
not make sense for the TM outputs.
3 The Support Vector Machines
3.1 The SVM Classifier
Classical SVMs (Cortes and Vapnik, 1995) are
binary classifiers that classify an input instance
based on decision rules which minimize the reg-
ularized error function in (Eq. 1):
min
w,b,?
1
2w
Tw + C
l?
i=1
?i
subject to: yi(wT xi + b) > 1 ? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {1,?1} are l training in-
stances. w is the weight vector, ? is the relaxation
variable and C > 0 is the penalty parameter.
3.2 Ranking SVM for SMT-TM Integration
The SVM classification algorithm is extended to
the ranking case in (Joachims, 2002). For a cer-
375
tain group of instances, the Ranking SVM aims
at producing a ranking r that has the maximum
Kendall?s ? coefficient with the the gold standard
ranking r?.
Kendall?s ? measures the relevance of two rank-
ings: ?(ra, rb) = P?QP+Q , where P and Q arethe amount of concordant and discordant pairs in
ra and rb. In practice, this is done by building
constraints to minimize the discordant pairs Q.
Following the basic idea, we show how Ranking
SVM can be applied to MT-TM integration as fol-
lows.
Assume that for each source sentence s, we
have a set of outputs from MT, M and a set of
outputs from TM, T. If we have a ranking r(s)
over translation outputs M?T where for each
translation output d ? M?T, (di, dj) ? r(s) iff
di <r(s) dj , we can rewrite the ranking constraints
as optimization constraints in an SVM, as in Eq.
(2).
min
w,b,?
1
2w
Tw + C? ?
subject to:
?(di, dj) ? r(s1) : w(?(s1, di)? ?(s1, dj)) > 1 ? ?i,j,1
...
?(di, dj) ? r(sn) : w(?(sn, di)? ?(sn, dj)) > 1? ?i,j,n
?i,j,k > 0
(2)
where ?(sn, di) is a feature vector of translation
output di given source sentence sn. The Ranking
SVM minimizes the discordant number of rank-
ings with the gold standard according to Kendall?s
? .
When the instances are not linearly separable,
we use a mapping function ? to map the features
xi (?(sn, di) in the case of ranking) to high di-
mensional space, and solve the SVMwith a kernel
function K in where K(xi, xj) = ?(xi)T?(xj).
We perform our experiments with the Radial
Basis Function (RBF) kernel, as in Eq. (3).
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (3)
4 The Ranking-based Integration Model
In this section we present the Ranking-based
SMT-TM integration model in detail. We first in-
troduce the k-best lists in MT (called N-best list)
and TM systems (called m-best list in this section)
and then move on to the problem formulation and
the feature set.
4.1 K-Best Lists in SMT and TM
4.1.1 The SMT N-best List
The N-best list of the SMT system is generated
during decoding according to the internal feature
scores. The features include language and transla-
tion model probabilities, reordering model scores
and a word penalty.
4.1.2 The TM M-Best List and the Fuzzy
Match Score
The m-best list of the TM system is gener-
ated in descending fuzzy match score. The fuzzy
match score (Sikes, 2007) uses the similarity of
the source sentences to predict a level to which a
translation is reusable or editable.
The calculation of fuzzy match scores is one of
the core technologies in TM systems and varies
among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Lev-
enshtein, 1966) between the source and TM en-
try, normalized by the length of the source as in
Eq. (4), as most of the current implementations
are based on edit distance while allowing some
additional flexible matching.
FuzzyMatch(t) = min
e
EditDistance(s, e)
Len(s) (4)
where s is the source side of the TM hit t, and e
is the source side of an entry in the TM.
4.2 Problem Formulation
Ranking lists is a well-researched problem in
the information retrieval community, and Ranking
SVMs (Joachims, 2002), which optimizes on the
ranking correlation ? have already been applied
successfully in machine translation evaluation (Ye
et al, 2007). We apply the same method here to
rerank a merged list of MT and TM outputs.
Formally given an MT-produced N-best list
M = {m1,m2, ...,mn}, a TM-produced m-best
list T = {t1, t2, ..., tm} for a input sentence s,
we define the gold standard using the TER met-
ric (Snover et al, 2006): for each d ? M?T,
(di, dj) ? r(s) iff TER(di) < TER(dj). We
train and test a Ranking SVM using cross vali-
dation on a data set created according to this cri-
terion. Ideally the gold standard would be cre-
ated by human annotators. We choose to use TER
376
as large-scale annotation is not yet available for
this task. Furthermore, TER has a high correla-
tion with the HTER score (Snover et al, 2006),
which is the TER score using the post-edited MT
output as a reference, and is used as an estimation
of post-editing effort.
4.3 The Feature Set
When building features for the Ranking SVM, we
are limited to features that are independent of the
MT and TM system. We experiment with system-
independent fluency and fidelity features below,
which capture translation fluency and adequacy,
respectively.
4.3.1 Fluency Features
Source-side Language Model Scores. We
compute the LM probability and perplexity of the
input source sentence on a language model trained
on the source-side training data of the SMT sys-
tem, which is also the TM database. The inputs
that have lower perplexity on this language model
are more similar to the data set on which the SMT
system is built.
Target-side LanguageModel Scores. We com-
pute the LM probability and perplexity as a mea-
sure of the fluency of the translation.
4.3.2 Fidelity Features
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM performs well
enough, these two sentences should be the same
or very similar. Therefore the fuzzy match score
here gives an estimation of the confidence level of
the output.
The IBMModel 1 Score. We compute the IBM
Model 1 score in both directions to measure the
correspondence between the source and target, as
it serves as a rough estimation of how good a
translation it is on the word level.
5 Experiments
5.1 Experimental Settings
5.1.1 Data
Our raw data set is an English?French trans-
lation memory with technical translation from a
multi-national IT security company, consisting of
51K sentence pairs. We randomly select 43K to
train an SMT system and translate the English side
of the remaining 8K sentence pairs, which is used
to run cross validation. Note that the 8K sentence
pairs are from the same TM, so that we are able to
create a gold standard by ranking the TER scores
of the MT and TM outputs.
Duplicated sentences are removed from the
data set, as those will lead to an exact match in
the TM system and will not be translated by trans-
lators. The average sentence length of the training
set is 13.5 words and the size of the training set
is comparable to the (larger) translation memories
used in the industry.
5.1.2 SMT and TM systems
We use a standard log-linear PB-SMT
model (Och and Ney, 2002): GIZA++ imple-
mentation of IBM word alignment model 4, the
phrase-extraction heuristics described in (Koehn
et al, 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing trained with SRILM (Stolcke, 2002)
on the English side of the training data, and
Moses (Koehn et al, 2007) to decode. We train a
system in the opposite direction using the same
data to produce the pseudo-source sentences.
We merge distinct 5-best lists from MT and TM
systems to produce a new ranking. To create the
distinct list for the SMT system, we search over
a 100-best list and keep the top-5 distinct out-
puts. Our data set consists of mainly short sen-
tences, leading to many duplications in the N-best
output of the SMT decoder. In such cases, top-
5 distinct outputs are good representations of the
SMT?s output.
5.2 Training, Tuning and Testing the
Ranking SVM
We run training and prediction of the Ranking
SVM in 4-fold cross validation. We use the
377
SVMlight1 toolkit to perform training and testing.
When using the Ranking SVM with the RBF
kernel, we have two free parameters to tune on:
the cost parameter C in Eq. (1) and the radius
parameter ? in Eq. (3). We optimize C and
? using a brute-force grid search before running
cross-validation and maximize precision at top-5,
with an inner 3-fold cross validation on the (outer)
Fold-1 training set. We search within the range
[2?6, 29], the step size is 2 on the exponent.
5.3 The Gold Standard
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     



























  
  
  0%
  20%
  40%
  60%
  80%
  100%
Top1 Top3 Top5
Go
ld S
tan
dar
d %
TM
MT
Figure 1: MT and TM?s percentage in gold stan-
dard
Figure 1 shows the composition of translations
in the gold standard. Each source sentence is asso-
ciated with a list of translations from two sources,
i.e. MT output and TM matches. This list of
translations is ranked from best to worst accord-
ing TER scores. The figure shows that over 80%
of the translations are from the MT system if we
only consider the top-1 translation. As the num-
ber of top translations we consider increases, more
TM matches can be seen. On the one hand, this
does show a large gap in quality between MT out-
put and TM matches; on the other hand, however,
it also reveals that we will have to ensure two ob-
jectives in ranking: the first is to rank the 80%
MT translations higher and the second is to keep
the 20% ?good? TM hits in the Top-5. We design
our evaluation metrics accordingly.
5.4 Evaluation Metrics
The aim of this research is to provide post-editors
with translations that in many cases are easier to
1http://svmlight.joachims.org/
edit than the original TM output. As we formulate
this as a ranking problem, it is natural to measure
the quality of the ranking output by the number
of better translations that are ranked high. Some-
times the top TM output is the easiest to edit; in
such a case we need to ensure that this translation
has a high rank, otherwise the system performance
will degrade.
Based on this observation, we introduce the
idea of relevant translations, and our evaluation
metrics: PREC@k and HIT@k.
Relevant Translations. We borrow the idea
of relevence from the IR community to define
the idea of translations worth ranking high. For
a source sentence s which has a top TM hit t,
we define an MT/TM output m as relevant, if
TER(m) ? TER(t). According to the defini-
tion, relevant translations should need no more
post-edits than the original top hit from the TM
system. Clearly the top TM hit is always relevant.
PREC@k. We calculate the precision
(PREC@k) of the ranking for evaluation. As-
suming that there are n relevant translations in
the top k list for a source sentence s, we have
PREC@k= n/k for s. We test PREC@k, for
k = 1...10, in order to evaluate the overall quality
of the ranking.
HIT@k. We also estimate the probability of
having one of the relevant translations in the top
k, denoted as HIT@k. For a source sentence s,
HIT@k equals to 1 if there is at least one relevant
translation in top k, and 0 otherwise. This mea-
sures the quality of the best translation in top k,
which is the translation the post-editor will find
and work on if she reads till the kth place in the
list. HIT@k equals to 1.0 at the end of the list.
We report the mean PREC@k and HIT@k for
all s with the 0.95 confidence interval.
5.5 Experimental Results
In Table 1 we report PREC@k and HIT@k
for k = 1..10. The ranking receives 0.8747
PREC@1, which means that most of the top
ranked translations have at least the same quality
as the top TM output. We notice that precision re-
mains above 0.8 till k = 5, leading us to conclude
that most of the relevant translations are ranked in
the top-5 positions in the list.
378
Table 1: PREC@k and HIT@k of Ranking
PREC % HIT %
k=1 87.47?1.60 87.47?1.60
k=2 85.42?1.07 93.36?0.53
k=3 84.13?0.94 95.74?0.61
k=4 82.79?0.57 97.08?0.26
k=5 81.34?0.51 98.04?0.23
k=6 79.26?0.59 99.41?0.25
k=7 74.99?0.53 99.66?0.29
k=8 70.87?0.59 99.84?0.10
k=9 67.23?0.48 99.94?0.08
k=10 64.00?0.46 100.0?0.00
Using the HIT@k scores we can further con-
firm this argument. The HIT@k score grows
steadily from 0.8747 to 0.9941 for k = 1...6, so
most often there will be at least one relevant trans-
lation in top-6 for the post-editor to work with.
After that room for improvement becomes very
small.
In sum, both of the PREC@k scores and the
HIT@k scores show that the ranking model effec-
tively integrates the two translation sources (MT
and TM) into one merged k-best list, and ranks
the relevant translations higher.
Table 2: PREC@k - MT and TM Systems
MT % TM %
k=1 85.87?1.32 100.0?0.00
k=2 82.52?1.60 73.58?1.04
k=3 80.05?1.11 62.45?1.14
k=4 77.92?0.95 56.11?1.11
k=5 76.22?0.87 51.78?0.78
To measure whether the ranking model is ef-
fective compared to pure MT or TM outputs, we
report the PREC@k of those outputs in Table 2.
The k-best output used in this table is ranked by
the MT or TM system, without being ranked by
our model. We see the ranked outputs consistently
outperform the MT outputs for all k = 1...5 w.r.t.
precision at a significant level, indicating that our
system preserves some high quality hits from the
TM.
The TM outputs alone are generally of much
lower quality than the MT and Ranked outputs, as
is shown by the precision scores for k = 2...5. But
TM translations obtain 1.0 PREC@1 according to
the definition of the PREC calculation. Note that
it does not mean that those outputs will need less
post-editing (cf. Section 6.1), but rather indicates
that each one of these outputs meet the lowest ac-
ceptable criterion to be relevant.
6 Analysis of Post-Editing Effort
A natural question follows the PREC and HIT
numbers: after reading the ranked k-best list, will
the post-editors edit less than they would have to if
they did not have access to the list? This question
would be best answered by human post-editors in
a large-scale experimental setting. As we have not
yet conducted a manual post-editing experiment,
we try to measure the post-editing effort implied
by our model with the edit statistics captured by
the TER metric, sorted into four types: Insertion,
Substitution, Deletion and Shift. We report the av-
erage number of edits incurred along with the 0.95
confidence interval.
6.1 Top-1 Edit Statistics
We report the results on the 1-best output of TM,
MT and our ranking system in Table 3.
In the single best results, it is easy to see that
the 1-best output from the MT system requires
the least post-editing effort. This is not surpris-
ing given the distribution of the gold standard in
Section 5.3, where most MT outputs are of better
quality than the TM hits.
Moreover, since TM translations are generally
of much lower quality as is indicated by the num-
bers in Table 3 (e.g. 2x as many substitutions
and 3x as many deletions compared to MT), un-
justly including very few of them in the ranking
output will increase loss in the edit statistics. This
explains why the ranking model has better rank-
ing precision in Tables 1 and 2, but seems to in-
cur more edit efforts. However, in practice post-
editors can neglect an obvious ?bad? translation
very quickly.
6.2 Top-k Edit Statistics
We report edit statistics of the Top-3 and Top-5
outputs in Tables 4 and 5, respectively. For each
system we report two sets of statistics: the Best-
statistics calculated on the best output (according
379
Table 3: Edit Statistics on Ranked MT and TM Outputs - Single Best
Insertion Substitution Deletion Shift
TM-Top1 0.7554 ? 0.0376 4.2461 ? 0.0960 2.9173 ? 0.1027 1.1275 ? 0.0509
MT-Top1 0.9959 ? 0.0385 2.2793 ? 0.0628 0.8940 ? 0.0353 1.2821 ? 0.0575
Rank-Top1 1.0674 ? 0.0414 2.6990 ? 0.0699 1.1246 ? 0.0412 1.2800 ? 0.0570
to TER score) in the list, and the Mean- statistics
calculated on the whole Top-k list.
The Mean- numbers allow us to have a general
overview of the ranking quality, but it is strongly
influenced by the poor TM hits that can easily be
neglected in practice. To control the impact of
those TM hits, we rely on the Best- numbers to es-
timate the edits performed on the translations that
are more likely to be used by post-editors.
In Table 4, the ranking output?s edit statistics
is closer to the MT output than the Top-1 case
in Table 3. Table 5 continues this tendency, in
which the Best-in-Top5 Ranking output requires
marginally less Substitution and Deletion opera-
tions and significantly less Insertion and Shift op-
erations (starred) than its MT counterpart. This
shows that when more of the list is explored, the
advantage of the ranking model ? utilizing mul-
tiple translation sources ? begins to compensate
for the possible large number of edits required by
poor TM hits and finally leads to reduced post-
editing effort.
There are several explanations to why the rel-
ative performance of the ranking model improves
when k increases, as compared to other models.
The most obvious explanation is that a single poor
translation is less likely to hurt edit statistics on
a k-best list with large k, if most of the transla-
tions in the k-best list are of good quality. We see
from Tables 1 and 2 that the ranking output is of
better quality than the MT and TM outputs w.r.t.
precision. For a larger k, the small number of in-
correctly ranked translations are less likely to be
chosen as the Best- translation and hold back the
Best- numbers.
A further reason is related to our ranking model
which optimizes on Kendall?s ? score. Accord-
ingly the output might not be optimal when we
evaluate the Top-1 output, but will behave better
when we evaluate on the list. This is also in ac-
cordance with our aim, which is to enrich the TM
with MT outputs and help the post-editor, instead
of choosing the translation for the post-editor.
6.3 Comparing the MT, TM and Ranking
Outputs
One of the interesting findings from Tables 3 and
4 is that according to the TER edit statistics, the
MT outputs generally need a smaller number of
edits than the TM and Ranking outputs. This cer-
tainly confirms the necessity to integrate MT into
today?s TM systems.
However, this fact should not lead to the con-
clusion that TMs should be replaced by MT com-
pletely. First of all, all of our experiments exclude
exact TM matches, as those translations will sim-
ply be reused and not translated. While this is a
realistic setting in the translation industry, it re-
moves all sentences for which the TM works best
from our evaluations.
Furthermore, Table 5 shows that the Best-in-
Top5 Ranking output performs better than the MT
outputs, hence there are TM outputs that lead to
smaller number of edits. As k increases, the rank-
ing model is able to better utilize these outputs.
Finally, in this task we concentrate on rank-
ing useful translations higher, but we are not in-
terested in how useless translations are ranked.
Ranking SVM optimizes on the ranking of the
whole list, which is slightly different from what
we actually require. One option is to use other
optimization techniques that can make use of this
property to get better Top-k edit statistics for a
smaller k. Another option is obviously to perform
regression directly on the number of edits instead
of modeling on the ranking. We plan to explore
these ideas in future work.
7 Conclusions and Future Work
In this paper we present a novel ranking-based
model to integrate SMT into a TM system, in or-
der to facilitate the work of post-editors. In such
380
Table 4: Edit Statistics on Ranked MT and TM Outputs - Top 3
Insertion Substitution Deletion Shift
TM-Best-in-Top3 0.4241 ? 0.0250 3.7395 ? 0.0887 2.9561 ? 0.0966 0.9738 ? 0.0505
TM-Mean-Top3 0.6718 ? 0.0200 5.1428 ? 0.0559 3.6192 ? 0.0649 1.3233 ? 0.0310
MT-Best?in-Top3 0.7696 ? 0.0351 1.9210 ? 0.0610 0.7706 ? 0.0332 1.0842 ? 0.0545
MT-Mean-Top3 1.1296 ? 0.0229 2.4405 ? 0.0368 0.9341 ? 0.0209 1.3797 ? 0.0344
Rank-Best-in-Top3 0.8170 ? 0.0355 2.0744 ? 0.0608 0.8410 ? 0.0338 1.0399 ? 0.0529
Rank-Mean-Top3 1.0942 ? 0.0234 2.7437 ? 0.0392 1.0786 ? 0.0231 1.3309 ? 0.0334
Table 5: Edit Statistics on Ranked MT and TM Outputs
Insertion Substitution Deletion Shift
TM-Best-in-Top5 0.4239 ? 0.0250 3.7319 ? 0.0885 2.9552 ? 0.0967 0.9673 ? 0.0504
TM-Mean-Top5 0.6143 ? 0.0147 5.5092 ? 0.0473 3.9451 ? 0.0521 1.3737 ? 0.0240
MT-Best-in-Top5 0.7690 ? 0.0351 1.9163 ? 0.0610 0.7685 ? 0.0332 1.0811 ? 0.0544
MT-Mean-Top5 1.1912 ? 0.0182 2.5326 ? 0.0291 0.9487 ? 0.0165 1.4305 ? 0.0272
Rank-Best-in-Top5 0.7246 ? 0.0338* 1.8887 ? 0.0598 0.7562 ? 0.0327 0.9705 ? 0.0515*
Rank-Mean-Top5 1.1173 ? 0.0181 2.8777 ? 0.0312 1.1585 ? 0.0200 1.3675 ? 0.0260
a model, the user of the TM will be presented
with an augmented k-best list, consisting of trans-
lations from both the TM and theMT systems, and
ranked according to ascending prospective post-
editing effort.
From the post-editors? point of view, the TM
remains intact. And unlike in the binary transla-
tion recommendation, where only one translation
recommendation is provided, the ranking model
offers k-best post-editing candidates, enabling the
user to use more resources when translating. As
we do not actually throw away any translation pro-
duced from the TM, the assets represented by the
TM are preserved and the related estimation of the
upper bound cost is still valid.
We extract system independent features from
theMT and TM outputs and use Ranking SVMs to
train the ranking model, which outperforms both
the TM?s and MT?s k-best list w.r.t. precision at k,
for all ks.
We also analyze the edit statistics of the inte-
grated k-best output using the TER edit statistics.
Our ranking model results in slightly increased
number of edits compared to the MT output (ap-
parently held back by a small number of poor TM
outputs that are ranked high) for a smaller k, but
requires less edits than both the MT and the TM
output for a larger k.
This work can be extended in a number of ways.
Most importantly, We plan to conduct a user study
to validate the effectiveness of the method and
to gather HTER scores to train a better ranking
model. Furthermore, we will try to experiment
with learning models that can further reduce the
number of edit operations on the top ranked trans-
lations. We also plan to improve the adaptability
of this method and apply it beyond a specific do-
main and language pair.
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 07/CE/I1142) as part of
the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We
thank Symantec for providing the TM database
and the anonymous reviewers for their insightful
comments.
References
Cortes, Corinna and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Joachims, Thorsten. 2002. Optimizing search engines
using clickthrough data. In KDD ?02: Proceed-
ings of the eighth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 133?142, New York, NY, USA.
381
Koehn, Philipp., Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
(NAACL/HLT-2003), pages 48 ? 54, Edmonton, Al-
berta, Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions (ACL-2007), pages 177?
180, Prague, Czech Republic.
Levenshtein, Vladimir Iosifovich. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. Soviet Physics Doklady, 10(8):707?710.
Och, Franz Josef and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), pages 295?302,
Philadelphia, PA, USA.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics (ACL-2003), pages 160?167,
Morristown, NJ, USA.
Shen, Libin, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative reranking for machine trans-
lation. In HLT-NAACL 2004: Main Proceedings,
pages 177?184, Boston, Massachusetts, USA. As-
sociation for Computational Linguistics.
Sikes, Richard. 2007. Fuzzy matching in theory and
practice. Multilingual, 18(6):39 ? 43.
Simard, Michel and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted
translation environment. In Proceedings of the
Twelfth Machine Translation Summit (MT Summit
XII), pages 120 ? 127, Ottawa, Ontario, Canada.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas (AMTA-2006), pages 223?231,
Cambridge, MA, USA.
Specia, Lucia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009a. Esti-
mating the sentence-level quality of machine trans-
lation systems. In Proceedings of the 13th An-
nual Conference of the European Association for
Machine Translation (EAMT-2009), pages 28 ? 35,
Barcelona, Spain.
Specia, Lucia, Craig Saunders, Marco Turchi, Zhuo-
ran Wang, and John Shawe-Taylor. 2009b. Improv-
ing the confidence of machine translation quality
estimates. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 136 ?
143, Ottawa, Ontario, Canada.
Stolcke, Andreas. 2002. SRILM-an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, volume 2, pages 901?904, Denver, CO,
USA.
Ye, Yang, Ming Zhou, and Chin-Yew Lin. 2007.
Sentence level machine translation evaluation as a
ranking. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 240?247,
Prague, Czech Republic.
382
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622?630,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bridging SMT and TM with Translation Recommendation
Yifan He Yanjun Ma Josef van Genabith Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
{yhe,yma,josef,away}@computing.dcu.ie
Abstract
We propose a translation recommendation
framework to integrate Statistical Machine
Translation (SMT) output with Transla-
tion Memory (TM) systems. The frame-
work recommends SMT outputs to a TM
user when it predicts that SMT outputs are
more suitable for post-editing than the hits
provided by the TM. We describe an im-
plementation of this framework using an
SVM binary classifier. We exploit meth-
ods to fine-tune the classifier and inves-
tigate a variety of features of different
types. We rely on automatic MT evalua-
tion metrics to approximate human judge-
ments in our experiments. Experimental
results show that our system can achieve
0.85 precision at 0.89 recall, excluding ex-
act matches. Furthermore, it is possible for
the end-user to achieve a desired balance
between precision and recall by adjusting
confidence levels.
1 Introduction
Recent years have witnessed rapid developments
in statistical machine translation (SMT), with con-
siderable improvements in translation quality. For
certain language pairs and applications, automated
translations are now beginning to be considered
acceptable, especially in domains where abundant
parallel corpora exist.
However, these advances are being adopted
only slowly and somewhat reluctantly in profes-
sional localization and post-editing environments.
Post-editors have long relied on translation memo-
ries (TMs) as the main technology assisting trans-
lation, and are understandably reluctant to give
them up. There are several simple reasons for
this: 1) TMs are useful; 2) TMs represent con-
siderable effort and investment by a company or
(even more so) an individual translator; 3) the
fuzzy match score used in TMs offers a good ap-
proximation of post-editing effort, which is useful
both for translators and translation cost estimation
and, 4) current SMT translation confidence esti-
mation measures are not as robust as TM fuzzy
match scores and professional translators are thus
not ready to replace fuzzy match scores with SMT
internal quality measures.
There has been some research to address this is-
sue, see e.g. (Specia et al, 2009a) and (Specia et
al., 2009b). However, to date most of the research
has focused on better confidence measures for MT,
e.g. based on training regression models to per-
form confidence estimation on scores assigned by
post-editors (cf. Section 2).
In this paper, we try to address the problem
from a different perspective. Given that most post-
editing work is (still) based on TM output, we pro-
pose to recommend MT outputs which are better
than TM hits to post-editors. In this framework,
post-editors still work with the TM while benefit-
ing from (better) SMT outputs; the assets in TMs
are not wasted and TM fuzzy match scores can
still be used to estimate (the upper bound of) post-
editing labor.
There are three specific goals we need to
achieve within this framework. Firstly, the rec-
ommendation should have high precision, other-
wise it would be confusing for post-editors and
may negatively affect the lower bound of the post-
editing effort. Secondly, although we have full
access to the SMT system used in this paper,
our method should be able to generalize to cases
where SMT is treated as a black-box, which is of-
622
ten the case in the translation industry. Finally,
post-editors should be able to easily adjust the rec-
ommendation threshold to particular requirements
without having to retrain the model.
In our framework, we recast translation recom-
mendation as a binary classification (rather than
regression) problem using SVMs, perform RBF
kernel parameter optimization, employ posterior
probability-based confidence estimation to sup-
port user-based tuning for precision and recall, ex-
periment with feature sets involvingMT-, TM- and
system-independent features, and use automatic
MT evaluation metrics to simulate post-editing ef-
fort.
The rest of the paper is organized as follows: we
first briefly introduce related research in Section 2,
and review the classification SVMs in Section 3.
We formulate the classification model in Section 4
and present experiments in Section 5. In Section
6, we analyze the post-editing effort approximated
by the TER metric (Snover et al, 2006). Section
7 concludes the paper and points out avenues for
future research.
2 Related Work
Previous research relating to this work mainly fo-
cuses on predicting the MT quality.
The first strand is confidence estimation for MT,
initiated by (Ueffing et al, 2003), in which pos-
terior probabilities on the word graph or N-best
list are used to estimate the quality of MT out-
puts. The idea is explored more comprehensively
in (Blatz et al, 2004). These estimations are often
used to rerank the MT output and to optimize it
directly. Extensions of this strand are presented
in (Quirk, 2004) and (Ueffing and Ney, 2005).
The former experimented with confidence esti-
mation with several different learning algorithms;
the latter uses word-level confidence measures to
determine whether a particular translation choice
should be accepted or rejected in an interactive
translation system.
The second strand of research focuses on com-
bining TM information with an SMT system, so
that the SMT system can produce better target lan-
guage output when there is an exact or close match
in the TM (Simard and Isabelle, 2009). This line
of research is shown to help the performance of
MT, but is less relevant to our task in this paper.
A third strand of research tries to incorporate
confidence measures into a post-editing environ-
ment. To the best of our knowledge, the first paper
in this area is (Specia et al, 2009a). Instead of
modeling on translation quality (often measured
by automatic evaluation scores), this research uses
regression on both the automatic scores and scores
assigned by post-editors. The method is improved
in (Specia et al, 2009b), which applies Inductive
Confidence Machines and a larger set of features
to model post-editors? judgement of the translation
quality between ?good? and ?bad?, or among three
levels of post-editing effort.
Our research is more similar in spirit to the third
strand. However, we use outputs and features from
the TM explicitly; therefore instead of having to
solve a regression problem, we only have to solve
a much easier binary prediction problem which
can be integrated into TMs in a straightforward
manner. Because of this, the precision and recall
scores reported in this paper are not directly com-
parable to those in (Specia et al, 2009b) as the lat-
ter are computed on a pure SMT system without a
TM in the background.
3 Support Vector Machines for
Translation Quality Estimation
SVMs (Cortes and Vapnik, 1995) are binary clas-
sifiers that classify an input instance based on de-
cision rules which minimize the regularized error
function in (1):
min
w,b,?
1
2w
Tw + C
l
?
i=1
?i
s. t. yi(wT?(xi) + b) > 1? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {+1,?1} are l training
instances that are mapped by the function ? to a
higher dimensional space. w is the weight vec-
tor, ? is the relaxation variable and C > 0 is the
penalty parameter.
Solving SVMs is viable using the ?kernel
trick?: finding a kernel function K in (1) with
K(xi, xj) = ?(xi)T?(xj). We perform our ex-
periments with the Radial Basis Function (RBF)
kernel, as in (2):
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (2)
When using SVMs with the RBF kernel, we
have two free parameters to tune on: the cost pa-
rameter C in (1) and the radius parameter ? in (2).
In each of our experimental settings, the param-
eters C and ? are optimized by a brute-force grid
623
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
4 Translation Recommendation as
Binary Classification
We use an SVM binary classifier to predict the rel-
ative quality of the SMT output to make a recom-
mendation. The SVM classifier uses features from
the SMT system, the TM and additional linguis-
tic features to estimate whether the SMT output is
better than the hit from the TM.
4.1 Problem Formulation
As we treat translation recommendation as a bi-
nary classification problem, we have a pair of out-
puts from TM and MT for each sentence. Ideally
the classifier will recommend the output that needs
less post-editing effort. As large-scale annotated
data is not yet available for this task, we use auto-
matic TER scores (Snover et al, 2006) as the mea-
sure for the required post-editing effort. In the fu-
ture, we hope to train our system on HTER (TER
with human targeted references) scores (Snover et
al., 2006) once the necessary human annotations
are in place. In the meantime we use TER, as TER
is shown to have high correlation with HTER.
We label the training examples as in (3):
y =
{
+1 if TER(MT) < TER(TM)
?1 if TER(MT) ? TER(TM) (3)
Each instance is associated with a set of features
from both the MT and TM outputs, which are dis-
cussed in more detail in Section 4.3.
4.2 Recommendation Confidence Estimation
In classical settings involving SVMs, confidence
levels are represented as margins of binary predic-
tions. However, these margins provide little in-
sight for our application because the numbers are
only meaningful when compared to each other.
What is more preferable is a probabilistic confi-
dence score (e.g. 90% confidence) which is better
understood by post-editors and translators.
We use the techniques proposed by (Platt, 1999)
and improved by (Lin et al, 2007) to obtain the
posterior probability of a classification, which is
used as the confidence score in our system.
Platt?s method estimates the posterior probabil-
ity with a sigmod function, as in (4):
Pr(y = 1|x) ? PA,B(f) ?
1
1 + exp(Af + B) (4)
where f = f(x) is the decision function of the
estimated SVM. A and B are parameters that min-
imize the cross-entropy error function F on the
training data, as in Eq. (5):
min
z=(A,B)
F (z) = ?
l
?
i=1
(tilog(pi) + (1? ti)log(1? pi)),
where pi = PA,B(fi), and ti =
{N++1
N++2
if yi = +1
1
N?+2
if yi = ?1
(5)
where z = (A,B) is a parameter setting, and
N+ and N? are the numbers of observed positive
and negative examples, respectively, for the label
yi. These numbers are obtained using an internal
cross-validation on the training set.
4.3 The Feature Set
We use three types of features in classification: the
MT system features, the TM feature and system-
independent features.
4.3.1 The MT System Features
These features include those typically used in
SMT, namely the phrase-translation model scores,
the language model probability, the distance-based
reordering score, the lexicalized reordering model
scores, and the word penalty.
4.3.2 The TM Feature
The TM feature is the fuzzy match (Sikes, 2007)
cost of the TM hit. The calculation of fuzzy match
score itself is one of the core technologies in TM
systems and varies among different vendors. We
compute fuzzy match cost as the minimum Edit
Distance (Levenshtein, 1966) between the source
and TM entry, normalized by the length of the
source as in (6), as most of the current implemen-
tations are based on edit distance while allowing
some additional flexible matching.
hfm(t) = min
e
EditDistance(s, e)
Len(s) (6)
where s is the source side of t, the sentence to
translate, and e is the source side of an entry in the
TM. For fuzzy match scores F , this fuzzy match
cost hfm roughly corresponds to 1?F . The differ-
ence in calculation does not influence classifica-
tion, and allows direct comparison between a pure
TM system and a translation recommendation sys-
tem in Section 5.4.2.
624
4.3.3 System-Independent Features
We use several features that are independent of
the translation system, which are useful when a
third-party translation service is used or the MT
system is simply treated as a black-box. These
features are source and target side LM scores,
pseudo source fuzzy match scores and IBM model
1 scores.
Source-Side Language Model Score and Per-
plexity. We compute the language model (LM)
score and perplexity of the input source sentence
on a LM trained on the source-side training data of
the SMT system. The inputs that have lower per-
plexity or higher LM score are more similar to the
dataset on which the SMT system is built.
Target-Side Language Model Perplexity. We
compute the LM probability and perplexity of the
target side as a measure of fluency. Language
model perplexity of the MT outputs are calculated,
and LM probability is already part of the MT sys-
tems scores. LM scores on TM outputs are also
computed, though they are not as informative as
scores on the MT side, since TM outputs should
be grammatically perfect.
The Pseudo-Source Fuzzy Match Score. We
translate the output back to obtain a pseudo source
sentence. We compute the fuzzy match score
between the original source sentence and this
pseudo-source. If the MT/TM system performs
well enough, these two sentences should be the
same or very similar. Therefore, the fuzzy match
score here gives an estimation of the confidence
level of the output. We compute this score for both
the MT output and the TM hit.
The IBM Model 1 Score. The fuzzy match
score does not measure whether the hit could be
a correct translation, i.e. it does not take into ac-
count the correspondence between the source and
target, but rather only the source-side information.
For the TM hit, the IBM Model 1 score (Brown
et al, 1993) serves as a rough estimation of how
good a translation it is on the word level; for the
MT output, on the other hand, it is a black-box
feature to estimate translation quality when the in-
formation from the translation model is not avail-
able. We compute bidirectional (source-to-target
and target-to-source) model 1 scores on both TM
and MT outputs.
5 Experiments
5.1 Experimental Settings
Our raw data set is an English?French translation
memory with technical translation from Syman-
tec, consisting of 51K sentence pairs. We ran-
domly selected 43K to train an SMT system and
translated the English side of the remaining 8K
sentence pairs. The average sentence length of
the training set is 13.5 words and the size of the
training set is comparable to the (larger) TMs used
in the industry. Note that we remove the exact
matches in the TM from our dataset, because ex-
act matches will be reused and not presented to the
post-editor in a typical TM setting.
As for the SMT system, we use a stan-
dard log-linear PB-SMT model (Och and Ney,
2002): GIZA++ implementation of IBM word
alignment model 4,1 the refinement and phrase-
extraction heuristics described in (Koehn et
al., 2003), minimum-error-rate training (Och,
2003), a 5-gram language model with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained with
SRILM (Stolcke, 2002) on the English side of the
training data, and Moses (Koehn et al, 2007) to
decode. We train a system in the opposite direc-
tion using the same data to produce the pseudo-
source sentences.
We train the SVM classifier using the lib-
SVM (Chang and Lin, 2001) toolkit. The SVM-
training and testing is performed on the remaining
8K sentences with 4-fold cross validation. We also
report 95% confidence intervals.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 4-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameterC in (1) we search in the range
[2?5, 215], and for parameter ? (2) we search in the
range [2?15, 23]. The step size is 2 on the expo-
nent.
5.2 The Evaluation Metrics
We measure the quality of the classification by
precision and recall. Let A be the set of recom-
mended MT outputs, and B be the set of MT out-
puts that have lower TER than TM hits. We stan-
dardly define precision P , recall R and F-value as
in (7):
1More specifically, we performed 5 iterations of Model 1,
5 iterations of HMM, 3 iterations of Model 3, and 3 iterations
of Model 4.
625
P = |A
?
B|
|A| , R =
|A
?
B|
|B| and F =
2PR
P + R (7)
5.3 Recommendation Results
In Table 1, we report recommendation perfor-
mance using MT and TM system features (SYS),
system features plus system-independent features
(ALL:SYS+SI), and system-independent features
only (SI).
Table 1: Recommendation Results
Precision Recall F-Score
SYS 82.53?1.17 96.44?0.68 88.95?.56
SI 82.56?1.46 95.83?0.52 88.70?.65
ALL 83.45?1.33 95.56?1.33 89.09?.24
From Table 1, we observe that MT and TM
system-internal features are very useful for pro-
ducing a stable (as indicated by the smaller con-
fidence interval) recommendation system (SYS).
Interestingly, only using some simple system-
external features as described in Section 4.3.3 can
also yield a system with reasonably good per-
formance (SI). We expect that the performance
can be further boosted by adding more syntactic
and semantic features. Combining all the system-
internal and -external features leads to limited
gains in Precision and F-score compared to using
only system-internal features (SYS) only. This in-
dicates that at the default confidence level, current
system-external (resp. system-internal) features
can only play a limited role in informing the sys-
tem when current system-internal (resp. system-
external) features are available. We show in Sec-
tion 5.4.2 that combing both system-internal and -
external features can yield higher, more stable pre-
cision when adjusting the confidence levels of the
classifier. Additionally, the performance of system
SI is promising given the fact that we are using
only a limited number of simple features, which
demonstrates a good prospect of applying our rec-
ommendation system to MT systems where we do
not have access to their internal features.
5.4 Further Improving Recommendation
Precision
Table 1 shows that classification recall is very
high, which suggests that precision can still be im-
proved, even though the F-score is not low. Con-
sidering that TM is the dominant technology used
by post-editors, a recommendation to replace the
hit from the TM would require more confidence,
i.e. higher precision. Ideally our aim is to obtain
a level of 0.9 precision at the cost of some recall,
if necessary. We propose two methods to achieve
this goal.
5.4.1 Classifier Margins
We experiment with different margins on the train-
ing data to tune precision and recall in order to
obtain a desired balance. In the basic case, the
training example would be marked as in (3). If we
label both the training and test sets with this rule,
the accuracy of the prediction will be maximized.
We try to achieve higher precision by enforc-
ing a larger bias towards negative examples in the
training set so that some borderline positive in-
stances would actually be labeled as negative, and
the classifier would have higher precision in the
prediction stage as in (8).
y =
{
+1 if TER(SMT) + b < TER(TM)
?1 if TER(SMT) + b > TER(TM)
(8)
We experiment with b in [0, 0.25] usingMT sys-
tem features and TM features. Results are reported
in Table 2.
Table 2: Classifier margins
Precision Recall
TER+0 83.45?1.33 95.56?1.33
TER+0.05 82.41?1.23 94.41?1.01
TER+0.10 84.53?0.98 88.81?0.89
TER+0.15 85.24?0.91 87.08?2.38
TER+0.20 87.59?0.57 75.86?2.70
TER+0.25 89.29?0.93 66.67?2.53
The highest accuracy and F-value is achieved
by TER + 0, as all other settings are trained
on biased margins. Except for a small drop in
TER+0.05, other configurations all obtain higher
precision than TER+ 0. We note that we can ob-
tain 0.85 precision without a big sacrifice in recall
with b=0.15, but for larger improvements on pre-
cision, recall will drop more rapidly.
When we use b beyond 0.25, the margin be-
comes less reliable, as the number of positive
examples becomes too small. In particular, this
causes the SVM parameters we tune on in the first
fold to become less applicable to the other folds.
This is one limitation of using biased margins to
626
obtain high precision. The method presented in
Section 5.4.2 is less influenced by this limitation.
5.4.2 Adjusting Confidence Levels
An alternative to using a biased margin is to output
a confidence score during prediction and to thresh-
old on the confidence score. It is also possible to
add this method to the SVM model trained with a
biased margin.
We use the SVM confidence estimation tech-
niques in Section 4.2 to obtain the confidence
level of the recommendation, and change the con-
fidence threshold for recommendation when nec-
essary. This also allows us to compare directly
against a simple baseline inspired by TM users. In
a TM environment, some users simply ignore TM
hits below a certain fuzzy match score F (usually
from 0.7 to 0.8). This fuzzy match score reflects
the confidence of recommending the TM hits. To
obtain the confidence of recommending an SMT
output, our baseline (FM) uses fuzzy match costs
hFM ? 1?F (cf. Section 4.3.2) for the TM hits as
the level of confidence. In other words, the higher
the fuzzy match cost of the TM hit is (lower fuzzy
match score), the higher the confidence of recom-
mending the SMT output. We compare this base-
line with the three settings in Section 5.
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
Pre
cis
ion
Confidence
SISysAllFM
Figure 1: Precision Changes with Confidence
Level
Figure 1 shows that the precision curve of FM
is low and flat when the fuzzy match costs are
low (from 0 to 0.6), indicating that it is unwise to
recommend an SMT output when the TM hit has
a low fuzzy match cost (corresponding to higher
fuzzy match score, from 0.4 to 1). We also observe
that the precision of the recommendation receives
a boost when the fuzzy match costs for the TM
hits are above 0.7 (fuzzy match score lower than
0.3), indicating that SMT output should be recom-
mended when the TM hit has a high fuzzy match
cost (low fuzzy match score). With this boost, the
precision of the baseline system can reach 0.85,
demonstrating that a proper thresholding of fuzzy
match scores can be used effectively to discrimi-
nate the recommendation of the TM hit from the
recommendation of the SMT output.
However, using the TM information only does
not always find the easiest-to-edit translation. For
example, an excellent SMT output should be rec-
ommended even if there exists a good TM hit (e.g.
fuzzy match score is 0.7 or more). On the other
hand, a misleading SMT output should not be rec-
ommended if there exists a poor but useful TM
match (e.g. fuzzy match score is 0.2).
Our system is able to tackle these complica-
tions as it incorporates features from the MT and
the TM systems simultaneously. Figure 1 shows
that both the SYS and the ALL setting consistently
outperform FM, indicating that our classification
scheme can better integrate the MT output into the
TM system than this naive baseline.
The SI feature set does not perform well when
the confidence level is set above 0.85 (cf. the de-
scending tail of the SI curve in Figure 1). This
might indicate that this feature set is not reliable
enough to extract the best translations. How-
ever, when the requirement on precision is not that
high, and the MT-internal features are not avail-
able, it would still be desirable to obtain transla-
tion recommendations with these black-box fea-
tures. The difference between SYS and ALL is
generally small, but ALL performs steadily better
in [0.5, 0,8].
Table 3: Recall at Fixed Precision
Recall
SYS @85PREC 88.12?1.32
SYS @90PREC 52.73?2.31
SI @85PREC 87.33?1.53
ALL @85PREC 88.57?1.95
ALL @90PREC 51.92?4.28
5.5 Precision Constraints
In Table 3 we also present the recall scores at 0.85
and 0.9 precision for SYS, SI and ALL models to
demonstrate our system?s performance when there
is a hard constraint on precision. Note that our
system will return the TM entry when there is an
exact match, so the overall precision of the system
627
is above the precision score we set here in a ma-
ture TM environment, as a significant portion of
the material to be translated will have a complete
match in the TM system.
In Table 3 for MODEL@K, the recall scores are
achieved when the prediction precision is better
than K with 0.95 confidence. For each model, pre-
cision at 0.85 can be obtained without a very big
loss on recall. However, if we want to demand
further recommendation precision (more conser-
vative in recommending SMT output), the recall
level will begin to drop more quickly. If we use
only system-independent features (SI), we cannot
achieve as high precision as with other models
even if we sacrifice more recall.
Based on these results, the users of the TM sys-
tem can choose between precision and recall ac-
cording to their own needs. As the threshold does
not involve training of the SMT system or the
SVM classifier, the user is able to determine this
trade-off at runtime.
Table 4: Contribution of Features
Precision Recall F Score
SYS 82.53?1.17 96.44?0.68 88.95?.56
+M1 82.87?1.26 96.23?0.53 89.05?.52
+LM 82.82?1.16 96.20?1.14 89.01?.23
+PS 83.21?1.33 96.61?0.44 89.41?.84
5.6 Contribution of Features
In Section 4.3.3 we suggested three sets of
system-independent features: features based on
the source- and target-side language model (LM),
the IBMModel 1 (M1) and the fuzzy match scores
on pseudo-source (PS). We compare the contribu-
tion of these features in Table 4.
In sum, all the three sets of system-independent
features improve the precision and F-scores of the
MT and TM system features. The improvement
is not significant, but improvement on every set of
system-independent features gives some credit to
the capability of SI features, as does the fact that
SI features perform close to SYS features in Table
1.
6 Analysis of Post-Editing Effort
A natural question on the integration models is
whether the classification reduces the effort of the
translators and post-editors: after reading these
recommendations, will they translate/edit less than
they would otherwise have to? Ideally this ques-
tion would be answered by human post-editors in
a large-scale experimental setting. As we have
not yet conducted a manual post-editing experi-
ment, we conduct two sets of analyses, trying to
show which type of edits will be required for dif-
ferent recommendation confidence levels. We also
present possible methods for human evaluation at
the end of this section.
6.1 Edit Statistics
We provide the statistics of the number of edits
for each sentence with 0.95 confidence intervals,
sorted by TER edit types. Statistics of positive in-
stances in classification (i.e. the instances in which
MT output is recommended over the TM hit) are
given in Table 5.
When an MT output is recommended, its TM
counterpart will require a larger average number
of total edits than the MT output, as we expect. If
we drill down, however, we also observe that many
of the saved edits come from the Substitution cat-
egory, which is the most costly operation from the
post-editing perspective. In this case, the recom-
mended MT output actually saves more effort for
the editors than what is shown by the TER score.
It reflects the fact that TM outputs are not actual
translations, and might need heavier editing.
Table 6 shows the statistics of negative instances
in classification (i.e. the instances in which MT
output is not recommended over the TM hit). In
this case, the MT output requires considerably
more edits than the TM hits in terms of all four
TER edit types, i.e. insertion, substitution, dele-
tion and shift. This reflects the fact that some high
quality TM matches can be very useful as a trans-
lation.
6.2 Edit Statistics on Recommendations of
Higher Confidence
We present the edit statistics of recommendations
with higher confidence in Table 7. Comparing Ta-
bles 5 and 7, we see that if recommended with
higher confidence, the MT output will need sub-
stantially less edits than the TM output: e.g. 3.28
fewer substitutions on average.
From the characteristics of the high confidence
recommendations, we suspect that these mainly
comprise harder to translate (i.e. different from
the SMT training set/TM database) sentences, as
indicated by the slightly increased edit operations
628
Table 5: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.5
Insertion Substitution Deletion Shift
MT 0.9849 ? 0.0408 2.2881 ? 0.0672 0.8686 ? 0.0370 1.2500 ? 0.0598
TM 0.7762 ? 0.0408 4.5841 ? 0.1036 3.1567 ? 0.1120 1.2096 ? 0.0554
Table 6: Edit Statistics when NOT Recommending MT Outputs in Classification, confidence=0.5
Insertion Substitution Deletion Shift
MT 1.0830 ? 0.1167 2.2885 ? 0.1376 1.0964 ? 0.1137 1.5381 ? 0.1962
TM 0.7554 ? 0.0376 1.5527 ? 0.1584 1.0090 ? 0.1850 0.4731 ? 0.1083
Table 7: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.85
Insertion Substitution Deletion Shift
MT 1.1665 ? 0.0615 2.7334 ? 0.0969 1.0277 ? 0.0544 1.5549 ? 0.0899
TM 0.8894 ? 0.0594 6.0085 ? 0.1501 4.1770 ? 0.1719 1.6727 ? 0.0846
on the MT side. TM produces much worse edit-
candidates for such sentences, as indicated by
the numbers in Table 7, since TM does not have
the ability to automatically reconstruct an output
through the combination of several segments.
6.3 Plan for Human Evaluation
Evaluation with human post-editors is crucial to
validate and improve translation recommendation.
There are two possible avenues to pursue:
? Test our system on professional post-editors.
By providing them with the TM output, the
MT output and the one recommended to edit,
we can measure the true accuracy of our
recommendation, as well as the post-editing
time we save for the post-editors;
? Apply the presented method on open do-
main data and evaluate it using crowd-
sourcing. It has been shown that crowd-
sourcing tools, such as the Amazon Me-
chanical Turk (Callison-Burch, 2009), can
help developers to obtain good human judge-
ments on MT output quality both cheaply and
quickly. Given that our problem is related to
MT quality estimation in nature, it can poten-
tially benefit from such tools as well.
7 Conclusions and Future Work
In this paper we present a classification model to
integrate SMT into a TM system, in order to facili-
tate the work of post-editors. Insodoing we handle
the problem of MT quality estimation as binary
prediction instead of regression. From the post-
editors? perspective, they can continue to work in
their familiar TM environment, use the same cost-
estimation methods, and at the same time bene-
fit from the power of state-of-the-art MT. We use
SVMs to make these predictions, and use grid
search to find better RBF kernel parameters.
We explore features from inside the MT sys-
tem, from the TM, as well as features that make
no assumption on the translation model for the bi-
nary classification. With these features we make
glass-box and black-box predictions. Experiments
show that the models can achieve 0.85 precision at
a level of 0.89 recall, and even higher precision if
we sacrifice more recall. With this guarantee on
precision, our method can be used in a TM envi-
ronment without changing the upper-bound of the
related cost estimation.
Finally, we analyze the characteristics of the in-
tegrated outputs. We present results to show that,
if measured by number, type and content of ed-
its in TER, the recommended sentences produced
by the classification model would bring about less
post-editing effort than the TM outputs.
This work can be extended in the following
ways. Most importantly, it is useful to test the
model in user studies, as proposed in Section 6.3.
A user study can serve two purposes: 1) it can
validate the effectiveness of the method by mea-
suring the amount of edit effort it saves; and 2)
the byproduct of the user study ? post-edited sen-
tences ? can be used to generate HTER scores
to train a better recommendation model. Further-
more, we want to experiment and improve on the
adaptability of this method, as the current experi-
ment is on a specific domain and language pair.
629
Acknowledgements
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank Symantec for providing the TM database and the
anonymous reviewers for their insightful comments.
References
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and
Nicola Ueffing. 2004. Confidence estimation for ma-
chine translation. In The 20th International Conference
on Computational Linguistics (Coling-2004), pages 315 ?
321, Geneva, Switzerland.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263 ? 311.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Mechani-
cal Turk. In The 2009 Conference on Empirical Methods
in Natural Language Processing (EMNLP-2009), pages
286 ? 295, Singapore.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
?cjlin/libsvm.
Corinna Cortes and Vladimir Vapnik. 1995. Support-vector
networks. Machine learning, 20(3):273 ? 297.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In The 1995 International
Conference on Acoustics, Speech, and Signal Processing
(ICASSP-95), pages 181 ? 184, Detroit, MI.
Philipp. Koehn, Franz Josef Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In The 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technol-
ogy (NAACL/HLT-2003), pages 48 ? 54, Edmonton, Al-
berta, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In The 45th Annual Meet-
ing of the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster Ses-
sions (ACL-2007), pages 177 ? 180, Prague, Czech Re-
public.
Vladimir Iosifovich Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10(8):707 ? 710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vector
machines. Machine Learning, 68(3):267 ? 276.
Franz Josef Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical ma-
chine translation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics (ACL-
2002), pages 295 ? 302, Philadelphia, PA.
Franz Josef Och. 2003. Minimum error rate training in sta-
tistical machine translation. In The 41st Annual Meet-
ing on Association for Computational Linguistics (ACL-
2003), pages 160 ? 167.
John C. Platt. 1999. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood meth-
ods. Advances in Large Margin Classifiers, pages 61 ? 74.
Christopher B. Quirk. 2004. Training a sentence-level ma-
chine translation confidence measure. In The Fourth In-
ternational Conference on Language Resources and Eval-
uation (LREC-2004), pages 825 ? 828, Lisbon, Portugal.
Richard Sikes. 2007. Fuzzy matching in theory and practice.
Multilingual, 18(6):39 ? 43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation en-
vironment. In The Twelfth Machine Translation Sum-
mit (MT Summit XII), pages 120 ? 127, Ottawa, Ontario,
Canada.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of transla-
tion edit rate with targeted human annotation. In The 2006
conference of the Association for Machine Translation in
the Americas (AMTA-2006), pages 223 ? 231, Cambridge,
MA.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco
Turchi, and Nello Cristianini. 2009a. Estimating the
sentence-level quality of machine translation systems. In
The 13th Annual Conference of the European Association
for Machine Translation (EAMT-2009), pages 28 ? 35,
Barcelona, Spain.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang,
and John Shawe-Taylor. 2009b. Improving the confidence
of machine translation quality estimates. In The Twelfth
Machine Translation Summit (MT Summit XII), pages 136
? 143, Ottawa, Ontario, Canada.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In The Seventh International Confer-
ence on Spoken Language Processing, volume 2, pages
901 ? 904, Denver, CO.
Nicola Ueffing and Hermann Ney. 2005. Application
of word-level confidence measures in interactive statisti-
cal machine translation. In The Ninth Annual Confer-
ence of the European Association for Machine Translation
(EAMT-2005), pages 262 ? 270, Budapest, Hungary.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation.
In The Ninth Machine Translation Summit (MT Summit
IX), pages 394 ? 401, New Orleans, LA.
630
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1239?1248,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Consistent Translation using Discriminative Learning:
A Translation Memory-inspired Approach?
Yanjun Ma? Yifan He? Andy Way? Josef van Genabith?
? Baidu Inc., Beijing, China
yma@baidu.com
?Centre for Next Generation Localisation
School of Computing, Dublin City University
{yhe,away,josef}@computing.dcu.ie
Abstract
We present a discriminative learning method
to improve the consistency of translations in
phrase-based Statistical Machine Translation
(SMT) systems. Our method is inspired by
Translation Memory (TM) systems which are
widely used by human translators in industrial
settings. We constrain the translation of an in-
put sentence using the most similar ?transla-
tion example? retrieved from the TM. Differ-
ently from previous research which used sim-
ple fuzzy match thresholds, these constraints
are imposed using discriminative learning to
optimise the translation performance. We ob-
serve that using this method can benefit the
SMT system by not only producing consis-
tent translations, but also improved translation
outputs. We report a 0.9 point improvement
in terms of BLEU score on English?Chinese
technical documents.
1 Introduction
Translation consistency is an important factor
for large-scale translation, especially for domain-
specific translations in an industrial environment.
For example, in the translation of technical docu-
ments, lexical as well as structural consistency is es-
sential to produce a fluent target-language sentence.
Moreover, even in the case of translation errors, con-
sistency in the errors (e.g. repetitive error patterns)
are easier to diagnose and subsequently correct by
translators.
?This work was done while the first author was in the Cen-
tre for Next Generation Localisation at Dublin City University.
In phrase-based SMT, translation models and lan-
guage models are automatically learned and/or gen-
eralised from the training data, and a translation is
produced by maximising a weighted combination of
these models. Given that global contextual informa-
tion is not normally incorporated, and that training
data is usually noisy in nature, there is no guaran-
tee that an SMT system can produce translations in
a consistent manner.
On the other hand, TM systems ? widely used by
translators in industrial environments for enterprise
localisation by translators ? can shed some light on
mitigating this limitation. TM systems can assist
translators by retrieving and displaying previously
translated similar ?example? sentences (displayed as
source-target pairs, widely called ?fuzzy matches? in
the localisation industry (Sikes, 2007)). In TM sys-
tems, fuzzy matches are retrieved by calculating the
similarity or the so-called ?fuzzy match score? (rang-
ing from 0 to 1 with 0 indicating no matches and 1
indicating a full match) between the input sentence
and sentences in the source side of the translation
memory.
When presented with fuzzy matches, translators
can then avail of useful chunks in previous transla-
tions while composing the translation of a new sen-
tence. Most translators only consider a few sen-
tences that are most similar to the current input sen-
tence; this process can inherently improve the con-
sistency of translation, given that the new transla-
tions produced by translators are likely to be similar
to the target side of the fuzzy match they have con-
sulted.
Previous research as discussed in detail in Sec-
1239
tion 2 has focused on using fuzzy match score as
a threshold when using the target side of the fuzzy
matches to constrain the translation of the input
sentence. In our approach, we use a more fine-
grained discriminative learning method to determine
whether the target side of the fuzzy matches should
be used as a constraint in translating the input sen-
tence. We demonstrate that our method can consis-
tently improve translation quality.
The rest of the paper is organized as follows:
we begin by briefly introducing related research in
Section 2. We present our discriminative learning
method for consistent translation in Section 3 and
our feature design in Section 4. We report the exper-
imental results in Section 5 and conclude the paper
and point out avenues for future research in Section
6.
2 Related Research
Despite the fact that TM and MT integration has
long existed as a major challenge in the localisation
industry, it has only recently received attention in
main-stream MT research. One can loosely combine
TM and MT at sentence (called segments in TMs)
level by choosing one of them (or both) to recom-
mend to the translators using automatic classifiers
(He et al, 2010), or simply using fuzzy match score
or MT confidence measures (Specia et al, 2009).
One can also tightly integrate TM with MT at the
sub-sentence level. The basic idea is as follows:
given a source sentence to translate, we firstly use
a TM system to retrieve the most similar ?example?
source sentences together with their translations. If
matched chunks between input sentence and fuzzy
matches can be detected, we can directly re-use the
corresponding parts of the translation in the fuzzy
matches, and use an MT system to translate the re-
maining chunks.
As a matter of fact, implementing this idea is
pretty straightforward: a TM system can easily de-
tect the word alignment between the input sentence
and the source side of the fuzzy match by retracing
the paths used in calculating the fuzzy match score.
To obtain the translation for the matched chunks, we
just require the word alignment between source and
target TM matches, which can be addressed using
state-of-the-art word alignment techniques. More
importantly, albeit not explicitly spelled out in pre-
vious work, this method can potentially increase the
consistency of translation, as the translation of new
input sentences is closely informed and guided (or
constrained) by previously translated sentences.
There are several different ways of using the
translation information derived from fuzzy matches,
with the following two being the most widely
adopted: 1) to add these translations into a phrase
table as in (Bic?ici and Dymetman, 2008; Simard and
Isabelle, 2009), or 2) to mark up the input sentence
using the relevant chunk translations in the fuzzy
match, and to use an MT system to translate the parts
that are not marked up, as in (Smith and Clark, 2009;
Koehn and Senellart, 2010; Zhechev and van Gen-
abith, 2010). It is worth mentioning that translation
consistency was not explicitly regarded as their pri-
mary motivation in this previous work. Our research
follows the direction of the second strand given that
consistency can no longer be guaranteed by con-
structing another phrase table.
However, to categorically reuse the translations
of matched chunks without any differentiation could
generate inferior translations given the fact that the
context of these matched chunks in the input sen-
tence could be completely different from the source
side of the fuzzy match. To address this problem,
both (Koehn and Senellart, 2010) and (Zhechev and
van Genabith, 2010) used fuzzy match score as a
threshold to determine whether to reuse the transla-
tions of the matched chunks. For example, (Koehn
and Senellart, 2010) showed that reusing these trans-
lations as large rules in a hierarchical system (Chi-
ang, 2005) can be beneficial when the fuzzy match
score is above 70%, while (Zhechev and van Gen-
abith, 2010) reported that it is only beneficial to a
phrase-based system when the fuzzy match score is
above 90%.
Despite being an informative measure, using
fuzzy match score as a threshold has a number of
limitations. Given the fact that fuzzy match score
is normally calculated based on Edit Distance (Lev-
enshtein, 1966), a low score does not necessarily
imply that the fuzzy match is harmful when used
to constrain an input sentence. For example, in
longer sentences where fuzzy match scores tend to
be low, some chunks and the corresponding trans-
lations within the sentences can still be useful. On
1240
the other hand, a high score cannot fully guarantee
the usefulness of a particular translation. We address
this problem using discriminative learning.
3 Constrained Translation with
Discriminative Learning
3.1 Formulation of the Problem
Given a sentence e to translate, we retrieve the most
similar sentence e? from the translation memory as-
sociated with target translation f ?. The m com-
mon ?phrases? e?m1 between e and e? can be iden-
tified. Given the word alignment information be-
tween e? and f ?, one can easily obtain the corre-
sponding translations f? ?m1 for each of the phrases in
e?m1 . This process can derive a number of ?phrase
pairs? < e?m, f? ?m >, which can be used to specify
the translations of the matched phrases in the input
sentence. The remaining words without specified
translations will be translated by an MT system.
For example, given an input sentence e1e2 ? ? ?
eiei+1 ? ? ? eI , and a phrase pair < e?, f? ? >, e? =
eiei+1, f? ? = f ?jf
?
j+1 derived from the fuzzy match,
we can mark up the input sentence as:
e1e2 ? ? ? <tm=?f ?jf ?j+1?> eiei+1 < /tm> ? ? ? eI .
Our method to constrain the translations using
TM fuzzy matches is similar to (Koehn and Senel-
lart, 2010), except that the word alignment between
e? and f ? is the intersection of bidirectional GIZA++
(Och and Ney, 2003) posterior alignments. We use
the intersected word alignment to minimise the noise
introduced by word alignment of only one direction
in marking up the input sentence.
3.2 Discriminative Learning
Whether the translation information from the fuzzy
matches should be used or not (i.e. whether the input
sentence should be marked up) is determined using
a discriminative learning procedure. The translation
information refers to the ?phrase pairs? derived us-
ing the method described in Section 3.1. We cast
this problem as a binary classification problem.
3.2.1 Support Vector Machines
SVMs (Cortes and Vapnik, 1995) are binary classi-
fiers that classify an input instance based on decision
rules which minimise the regularised error function
in (1):
min
w,b,?
1
2
wT w + C
l
?
i=1
?i
s. t. yi(wT?(xi) + b) > 1? ?i
?i > 0
(1)
where (xi, yi) ? Rn ? {+1,?1} are l training in-
stances that are mapped by the function ? to a higher
dimensional space. w is the weight vector, ? is the
relaxation variable and C > 0 is the penalty param-
eter.
Solving SVMs is viable using a kernel function
K in (1) with K(xi, xj) = ?(xi)T?(xj). We per-
form our experiments with the Radial Basis Func-
tion (RBF) kernel, as in (2):
K(xi, xj) = exp(??||xi ? xj ||2), ? > 0 (2)
When using SVMs with the RBF kernel, we have
two free parameters to tune on: the cost parameter
C in (1) and the radius parameter ? in (2).
In each of our experimental settings, the param-
eters C and ? are optimised by a brute-force grid
search. The classification result of each set of pa-
rameters is evaluated by cross validation on the
training set.
The SVM classifier will thus be able to predict
the usefulness of the TM fuzzy match, and deter-
mine whether the input sentence should be marked
up using relevant phrase pairs derived from the fuzzy
match before sending it to the SMT system for trans-
lation. The classifier uses features such as the fuzzy
match score, the phrase and lexical translation prob-
abilities of these relevant phrase pairs, and addi-
tional syntactic dependency features. Ideally the
classifier will decide to mark up the input sentence
if the translations of the marked phrases are accurate
when taken contextual information into account. As
large-scale manually annotated data is not available
for this task, we use automatic TER scores (Snover
et al, 2006) as the measure for training data annota-
tion.
We label the training examples as in (3):
y =
{
+1 if TER(w. markup) < TER(w/o markup)
?1 if TER(w/o markup) ? TER(w. markup)
(3)
Each instance is associated with a set of features
which are discussed in more detail in Section 4.
1241
3.2.2 Classification Confidence Estimation
We use the techniques proposed by (Platt, 1999) and
improved by (Lin et al, 2007) to convert classifica-
tion margin to posterior probability, so that we can
easily threshold our classifier (cf. Section 5.4.2).
Platt?s method estimates the posterior probability
with a sigmoid function, as in (4):
Pr(y = 1|x) ? PA,B(f) ?
1
1 + exp(Af + B)
(4)
where f = f(x) is the decision function of the esti-
mated SVM. A and B are parameters that minimise
the cross-entropy error function F on the training
data, as in (5):
min
z=(A,B)
F (z) = ?
l
?
i=1
(tilog(pi) + (1 ? ti)log(1? pi)),
where pi = PA,B(fi), and ti =
{
N++1
N++2 if yi = +1
1
N?+2 if yi = ?1
(5)
where z = (A,B) is a parameter setting, and
N+ and N? are the numbers of observed positive
and negative examples, respectively, for the label yi.
These numbers are obtained using an internal cross-
validation on the training set.
4 Feature Set
The features used to train the discriminative classi-
fier, all on the sentence level, are described in the
following sections.
4.1 The TM Feature
The TM feature is the fuzzy match score, which in-
dicates the overall similarity between the input sen-
tence and the source side of the TM output. If the
input sentence is similar to the source side of the
matching segment, it is more likely that the match-
ing segment can be used to mark up the input sen-
tence.
The calculation of the fuzzy match score itself is
one of the core technologies in TM systems, and
varies among different vendors. We compute fuzzy
match cost as the minimum Edit Distance (Leven-
shtein, 1966) between the source and TM entry, nor-
malised by the length of the source as in (6), as
most of the current implementations are based on
edit distance while allowing some additional flexi-
ble matching.
hfm(e) = min
s
EditDistance(e, s)
Len(e)
(6)
where e is the sentence to translate, and s is the
source side of an entry in the TM. For fuzzy match
scores F , hfm roughly corresponds to 1? F .
4.2 Translation Features
We use four features related to translation probabil-
ities, i.e. the phrase translation and lexical probabil-
ities for the phrase pairs < e?m, f? ?m > derived us-
ing the method in Section 3.1. Specifically, we use
the phrase translation probabilities p(f? ?m|e?m) and
p(e?m|f? ?m), as well as the lexical translation prob-
abilities plex(f? ?m|e?m) and plex(e?m|f? ?m) as calcu-
lated in (Koehn et al, 2003). In cases where mul-
tiple phrase pairs are used to mark up one single
input sentence e, we use a unified score for each
of the four features, which is an average over the
corresponding feature in each phrase pair. The intu-
ition behind these features is as follows: phrase pairs
< e?m, f? ?m > derived from the fuzzy match should
also be reliable with respect to statistically produced
models.
We also have a count feature, i.e. the number of
phrases used to mark up the input sentence, and a
binary feature, i.e. whether the phrase table contains
at least one phrase pair < e?m, f? ?m > that is used to
mark up the input sentence.
4.3 Dependency Features
Given the phrase pairs < e?m, f? ?m > derived from
the fuzzy match, and used to translate the corre-
sponding chunks of the input sentence (cf. Sec-
tion 3.1), these translations are more likely to be co-
herent in the context of the particular input sentence
if the matched parts on the input side are syntacti-
cally and semantically related.
For matched phrases e?m between the input sen-
tence and the source side of the fuzzy match, we de-
fine the contextual information of the input side us-
ing dependency relations between words em in e?m
and the remaining words ej in the input sentence e.
We use the Stanford parser to obtain the depen-
dency structure of the input sentence. We add
a pseudo-label SYS PUNCT to punctuation marks,
whose governor and dependent are both the punc-
tuation mark. The dependency features designed to
capture the context of the matched input phrases e?m
are as follows:
1242
Coverage features measure the coverage of de-
pendency labels on the input sentence in order to
obtain a bigger picture of the matched parts in the
input. For each dependency label L, we consider its
head or modifier as covered if the corresponding in-
put word em is covered by a matched phrase e?m.
Our coverage features are the frequencies of gov-
ernor and dependent coverage calculated separately
for each dependency label.
Position features identify whether the head and
the tail of a sentence are matched, as these are the
cases in which the matched translation is not af-
fected by the preceding words (when it is the head)
or following words (when it is the tail), and is there-
fore more reliable. The feature is set to 1 if this hap-
pens, and to 0 otherwise. We distinguish among the
possible dependency labels, the head or the tail of
the sentence, and whether the aligned word is the
governor or the dependent. As a result, each per-
mutation of these possibilities constitutes a distinct
binary feature.
The consistency feature is a single feature which
determines whether matched phrases e?m belong to
a consistent dependency structure, instead of being
distributed discontinuously around in the input sen-
tence. We assume that a consistent structure is less
influenced by its surrounding context. We set this
feature to 1 if every word in e?m is dependent on an-
other word in e?m, and to 0 otherwise.
5 Experiments
5.1 Experimental Setup
Our data set is an English?Chinese translation mem-
ory with technical translation from Symantec, con-
sisting of 87K sentence pairs. The average sentence
length of the English training set is 13.3 words and
the size of the training set is comparable to the larger
TMs used in the industry. Detailed corpus statistics
about the training, development and test sets for the
SMT system are shown in Table 1.
The composition of test subsets based on fuzzy
match scores is shown in Table 2. We can see that
sentences in the test sets are longer than those in the
training data, implying a relatively difficult trans-
lation task. We train the SVM classifier using the
libSVM (Chang and Lin, 2001) toolkit. The SVM-
Train Develop Test
SENTENCES 86,602 762 943
ENG. TOKENS 1,148,126 13,955 20,786
ENG. VOC. 13,074 3,212 3,115
CHI. TOKENS 1,171,322 10,791 16,375
CHI. VOC. 12,823 3,212 1,431
Table 1: Corpus Statistics
Scores Sentences Words W/S
(0.9, 1.0) 80 1526 19.0750
(0.8, 0.9] 96 1430 14.8958
(0.7, 0.8] 110 1596 14.5091
(0.6, 0.7] 74 1031 13.9324
(0.5, 0.6] 104 1811 17.4135
(0, 0.5] 479 8972 18.7307
Table 2: Composition of test subsets based on fuzzy
match scores
training and validation is on the same training sen-
tences1 as the SMT system with 5-fold cross valida-
tion.
The SVM hyper-parameters are tuned using the
training data of the first fold in the 5-fold cross val-
idation via a brute force grid search. More specifi-
cally, for parameter C in (1), we search in the range
[2?5, 215], while for parameter ? (2) we search in the
range [2?15, 23]. The step size is 2 on the exponent.
We conducted experiments using a standard log-
linear PB-SMT model: GIZA++ implementation of
IBM word alignment model 4 (Och and Ney, 2003),
the refinement and phrase-extraction heuristics de-
scribed in (Koehn et al, 2003), minimum-error-
rate training (Och, 2003), a 5-gram language model
with Kneser-Ney smoothing (Kneser and Ney, 1995)
trained with SRILM (Stolcke, 2002) on the Chinese
side of the training data, and Moses (Koehn et al,
2007) which is capable of handling user-specified
translations for some portions of the input during de-
coding. The maximum phrase length is set to 7.
5.2 Evaluation
The performance of the phrase-based SMT system
is measured by BLEU score (Papineni et al, 2002)
and TER (Snover et al, 2006). Significance test-
1We have around 87K sentence pairs in our training data.
However, for 67.5% of the input sentences, our MT system pro-
duces the same translation irrespective of whether the input sen-
tence is marked up or not.
1243
ing is carried out using approximate randomisation
(Noreen, 1989) with a 95% confidence level.
We also measure the quality of the classification
by precision and recall. Let A be the set of pre-
dicted markup input sentences, and B be the set
of input sentences where the markup version has a
lower TER score than the plain version. We stan-
dardly define precision P and recall R as in (7):
P =
|A?B|
|A| , R =
|A?B|
|B| (7)
5.3 Cross-fold translation
In order to obtain training samples for the classifier,
we need to label each sentence in the SMT training
data as to whether marking up the sentence can pro-
duce better translations. To achieve this, we translate
both the marked-up versions and plain versions of
the sentence and compare the two translations using
the sentence-level evaluation metric TER.
We do not make use of additional training data to
translate the sentences for SMT training, but instead
use cross-fold translation. We create a new training
corpus T by keeping 95% of the sentences in the
original training corpus, and creating a new test cor-
pus H by using the remaining 5% of the sentences.
Using this scheme we make 20 different pairs of cor-
pora (Ti,Hi) in such a way that each sentence from
the original training corpus is in exactly one Hi for
some 1 ? i ? 20. We train 20 different systems
using each Ti, and use each system to translate the
corresponding Hi as well as the marked-up version
of Hi using the procedure described in Section 3.1.
The development set is kept the same for all systems.
5.4 Experimental Results
5.4.1 Translation Results
Table 3 contains the translation results of the SMT
system when we use discriminative learning to mark
up the input sentence (MARKUP-DL). The first row
(BASELINE) is the result of translating plain test
sets without any markup, while the second row is
the result when all the test sentences are marked
up. We also report the oracle scores, i.e. the up-
perbound of using our discriminative learning ap-
proach. As we can see from this table, we obtain sig-
nificantly inferior results compared to the the Base-
line system if we categorically mark up all the in-
TER BLEU
BASELINE 39.82 45.80
MARKUP 41.62 44.41
MARKUP-DL 39.61 46.46
ORACLE 37.27 48.32
Table 3: Performance of Discriminative Learning (%)
put sentences using phrase pairs derived from fuzzy
matches. This is reflected by an absolute 1.4 point
drop in BLEU score and a 1.8 point increase in TER.
On the other hand, both the oracle BLEU and TER
scores represent as much as a 2.5 point improve-
ment over the baseline. Our discriminative learning
method (MARKUP-DL), which automatically clas-
sifies whether an input sentence should be marked
up, leads to an increase of 0.7 absolute BLEU points
over the BASELINE, which is statistically signifi-
cant. We also observe a slight decrease in TER com-
pared to the BASELINE. Despite there being much
room for further improvement when compared to the
Oracle score, the discriminative learning method ap-
pears to be effective not only in maintaining transla-
tion consistency, but also a statistically significant
improvement in translation quality.
5.4.2 Classification Confidence Thresholding
To further analyse our discriminative learning ap-
proach, we report the classification results on the test
set using the SVM classifier. We also investigate the
use of classification confidence, as described in Sec-
tion 3.2.2, as a threshold to boost classification pre-
cision if required. Table 4 shows the classification
and translation results when we use different con-
fidence thresholds. The default classification con-
fidence is 0.50, and the corresponding translation
results were described in Section 5.4.1. We inves-
tigate the impact of increasing classification confi-
dence on the performance of the classifier and the
translation results. As can be seen from Table 4,
increasing the classification confidence up to 0.70
leads to a steady increase in classification precision
with a corresponding sacrifice in recall. The fluc-
tuation in classification performance has an impact
on the translation results as measured by BLEU and
TER. We can see that the best BLEU as well as TER
scores are achieved when we set the classification
confidence to 0.60, representing a modest improve-
1244
Classification Confidence
0.50 0.55 0.60 0.65 0.70 0.75 0.80
BLEU 46.46 46.65 46.69 46.59 46.34 46.06 46.00
TER 39.61 39.46 39.32 39.36 39.52 39.71 39.71
P 60.00 68.67 70.31 74.47 72.97 64.28 88.89
R 32.14 29.08 22.96 17.86 13.78 9.18 4.08
Table 4: The impact of classification confidence thresholding
ment over the default setting (0.50). Despite the
higher precision when the confidence is set to 0.7,
the dramatic decrease in recall cannot be compen-
sated for by the increase in precision.
We can also observe from Table 4 that the recall
is quite low across the board, and the classification
results become unstable when we further increase
the level of confidence to above 0.70. This indicates
the degree of difficulty of this classification task, and
suggests some directions for future research as dis-
cussed at the end of this paper.
5.4.3 Comparison with Previous Work
As discussed in Section 2, both (Koehn and Senel-
lart, 2010) and (Zhechev and van Genabith, 2010)
used fuzzy match score to determine whether the in-
put sentences should be marked up. The input sen-
tences are only marked up when the fuzzy match
score is above a certain threshold. We present the
results using this method in Table 5. From this ta-
Fuzzy Match Scores
0.50 0.60 0.70 0.80 0.90
BLEU 45.13 45.55 45.58 45.84 45.82
TER 40.99 40.62 40.56 40.29 40.07
Table 5: Performance using fuzzy match score for classi-
fication
ble, we can see an inferior performance compared to
the BASELINE results (cf. Table 3) when the fuzzy
match score is below 0.70. A modest gain can only
be achieved when the fuzzy match score is above
0.8. This is slightly different from the conclusions
drawn in (Koehn and Senellart, 2010), where gains
are observed when the fuzzy match score is above
0.7, and in (Zhechev and van Genabith, 2010) where
gains are only observed when the score is above 0.9.
Comparing Table 5 with Table 4, we can see that
our classification method is more effective. This
confirms our argument in the last paragraph of Sec-
tion 2, namely that fuzzy match score is not informa-
tive enough to determine the usefulness of the sub-
sentences in a fuzzy match, and that a more compre-
hensive set of features, as we have explored in this
paper, is essential for the discriminative learning-
based method to work.
FM Scores w. markup w/o markup
[0,0.5] 37.75 62.24
(0.5,0.6] 40.64 59.36
(0.6,0.7] 40.94 59.06
(0.7,0.8] 46.67 53.33
(0.8,0.9] 54.28 45.72
(0.9,1.0] 44.14 55.86
Table 6: Percentage of training sentences with markup
vs without markup grouped by fuzzy match (FM) score
ranges
To further validate our assumption, we analyse
the training sentences by grouping them accord-
ing to their fuzzy match score ranges. For each
group of sentences, we calculate the percentage of
sentences where markup (and respectively without
markup) can produce better translations. The statis-
tics are shown in Table 6. We can see that for sen-
tences with fuzzy match scores lower than 0.8, more
sentences can be better translated without markup.
For sentences where fuzzy match scores are within
the range (0.8, 0.9], more sentences can be better
translated with markup. However, within the range
(0.9, 1.0], surprisingly, actually more sentences re-
ceive better translation without markup. This indi-
cates that fuzzy match score is not a good measure to
predict whether fuzzy matches are beneficial when
used to constrain the translation of an input sentence.
5.5 Contribution of Features
We also investigated the contribution of our differ-
ent feature sets. We are especially interested in
the contribution of dependency features, as they re-
1245
Example 1
w/o markup after policy name , type the name of the policy ( it shows new host integrity
policy by default ) .
Translation ????????????????? (????? ???????
??????
w. markup after policy name <tm translation=????????????? ??
?? ?????????>, type the name of the policy ( it shows new host
integrity policy by default ) .< /tm>
Translation ????????????????????? ???? ????????
Reference ????????????????????? ???? ????????
Example 2
w/o markup changes apply only to the specific scan that you select .
Translation ??????????????
w. markup changes apply only to the specific scan that you select <tm translation=???>.< /tm>
Translation ???????????????
Reference ???????????????
flect whether translation consistency can be captured
using syntactic knowledge. The classification and
TER BLEU P R
TM+TRANS 40.57 45.51 52.48 27.04
+DEP 39.61 46.46 60.00 32.14
Table 7: Contribution of Features (%)
translation results using different features are re-
ported in Table 7. We observe a significant improve-
ment in both classification precision and recall by
adding dependency (DEP) features on top of TM
and translation features. As a result, the translation
quality also significantly improves. This indicates
that dependency features which can capture struc-
tural and semantic similarities are effective in gaug-
ing the usefulness of the phrase pairs derived from
the fuzzy matches. Note also that without including
the dependency features, our discriminative learning
method cannot outperform the BASELINE (cf. Ta-
ble 3) in terms of translation quality.
5.6 Improved Translations
In order to pinpoint the sources of improvements by
marking up the input sentence, we performed some
manual analysis of the output. We observe that the
improvements can broadly be attributed to two rea-
sons: 1) the use of long phrase pairs which are miss-
ing in the phrase table, and 2) deterministically using
highly reliable phrase pairs.
Phrase-based SMT systems normally impose a
limit on the length of phrase pairs for storage and
speed considerations. Our method can overcome
this limitation by retrieving and reusing long phrase
pairs on the fly. A similar idea, albeit from a dif-
ferent perspective, was explored by (Lopez, 2008),
where he proposed to construct a phrase table on the
fly for each sentence to be translated. Differently
from his approach, our method directly translates
part of the input sentence using fuzzy matches re-
trieved on the fly, with the rest of the sentence trans-
lated by the pre-trained MT system. We offer some
more insights into the advantages of our method by
means of a few examples.
Example 1 shows translation improvements by
using long phrase pairs. Compared to the refer-
ence translation, we can see that for the underlined
phrase, the translation without markup contains (i)
word ordering errors and (ii) a missing right quota-
tion mark. In Example 2, by specifying the transla-
tion of the final punctuation mark, the system cor-
rectly translates the relative clause ?that you select?.
The translation of this relative clause is missing
when translating the input without markup. This
improvement can be partly attributed to the reduc-
tion in search errors by specifying the highly reliable
translations for phrases in an input sentence.
6 Conclusions and Future Work
In this paper, we introduced a discriminative learn-
ing method to tightly integrate fuzzy matches re-
trieved using translation memory technologies with
phrase-based SMT systems to improve translation
consistency. We used an SVM classifier to predict
whether phrase pairs derived from fuzzy matches
could be used to constrain the translation of an in-
1246
put sentence. A number of feature functions includ-
ing a series of novel dependency features were used
to train the classifier. Experiments demonstrated
that discriminative learning is effective in improving
translation quality and is more informative than the
fuzzy match score used in previous research. We re-
port a statistically significant 0.9 absolute improve-
ment in BLEU score using a procedure to promote
translation consistency.
As mentioned in Section 2, the potential improve-
ment in sentence-level translation consistency us-
ing our method can be attributed to the fact that
the translation of new input sentences is closely in-
formed and guided (or constrained) by previously
translated sentences using global features such as
dependencies. However, it is worth noting that
the level of gains in translation consistency is also
dependent on the nature of the TM itself; a self-
contained coherent TM would facilitate consistent
translations. In the future, we plan to investigate
the impact of TM quality on translation consistency
when using our approach. Furthermore, we will ex-
plore methods to promote translation consistency at
document level.
Moreover, we also plan to experiment with
phrase-by-phrase classification instead of sentence-
by-sentence classification presented in this paper,
in order to obtain more stable classification results.
We also plan to label the training examples using
other sentence-level evaluation metrics such as Me-
teor (Banerjee and Lavie, 2005), and to incorporate
features that can measure syntactic similarities in
training the classifier, in the spirit of (Owczarzak et
al., 2007). Currently, only a standard phrase-based
SMT system is used, so we plan to test our method
on a hierarchical system (Chiang, 2005) to facilitate
direct comparison with (Koehn and Senellart, 2010).
We will also carry out experiments on other data sets
and for more language pairs.
Acknowledgments
This work is supported by Science Foundation Ire-
land (Grant No 07/CE/I1142) and part funded under
FP7 of the EC within the EuroMatrix+ project (grant
No 231720). The authors would like to thank the
reviewers for their insightful comments and sugges-
tions.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65?72, Ann Arbor, MI.
Ergun Bic?ici and Marc Dymetman. 2008. Dynamic
translation memory: Using statistical machine trans-
lation to improve translation memory. In Proceedings
of the 9th Internation Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 454?465, Haifa, Israel.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/
?
cjlin/libsvm.
David Chiang. 2005. A hierarchical Phrase-Based model
for Statistical Machine Translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, MI.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Yifan He, Yanjun Ma, Josef van Genabith, and Andy
Way. 2010. Bridging SMT and TM with translation
recommendation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 622?630, Uppsala, Sweden.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume 1,
pages 181?184, Detroit, MI.
Philipp Koehn and Jean Senellart. 2010. Convergence of
translation memory and statistical machine translation.
In Proceedings of AMTA Workshop on MT Research
and the Translation Industry, pages 21?31, Denver,
CO.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of the 2003 Human Language Technology Conference
and the North American Chapter of the Association
for Computational Linguistics, pages 48?54, Edmon-
ton, AB, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for Statistical Machine Translation. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics Companion Vol-
1247
ume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic.
Vladimir Iosifovich Levenshtein. 1966. Binary codes ca-
pable of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707?710.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vec-
tor machines. Machine Learning, 68(3):267?276.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in Sta-
tistical Machine Translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 160?167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation
evaluation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 104?111,
Prague, Czech Republic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized likeli-
hood methods. Advances in Large Margin Classifiers,
pages 61?74.
Richard Sikes. 2007. Fuzzy matching in theory and prac-
tice. Multilingual, 18(6):39?43.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted translation
environment. In Proceedings of the Twelfth Machine
Translation Summit (MT Summit XII), pages 120 ?
127, Ottawa, Ontario, Canada.
James Smith and Stephen Clark. 2009. EBMT for SMT:
A new EBMT-SMT hybrid. In Proceedings of the 3rd
International Workshop on Example-Based Machine
Translation, pages 3?10, Dublin, Ireland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas (AMTA-2006), pages 223?231, Cam-
bridge, MA, USA.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
confidence of machine translation quality estimates.
In Proceedings of the Twelfth Machine Translation
Summit (MT Summit XII), pages 136 ? 143, Ottawa,
Ontario, Canada.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901?904, Denver, CO.
Ventsislav Zhechev and Josef van Genabith. 2010.
Seeding statistical machine translation with translation
memory output through tree-based structural align-
ment. In Proceedings of the Fourth Workshop on Syn-
tax and Structure in Statistical Translation, pages 43?
51, Beijing, China.
1248
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338?343,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Identifying High-Impact Sub-Structures for Convolution Kernels in
Document-level Sentiment Classification
Zhaopeng Tu? Yifan He?? Jennifer Foster? Josef van Genabith? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Computer Science Department ?School of Computing
Institute of Computing Technology, CAS New York University Dublin City University
?{tuzhaopeng,liuqun,sxlin}@ict.ac.cn,
?yhe@cs.nyu.edu, ?{jfoster,josef}@computing.dcu.ie
Abstract
Convolution kernels support the modeling of
complex syntactic information in machine-
learning tasks. However, such models are
highly sensitive to the type and size of syntac-
tic structure used. It is therefore an importan-
t challenge to automatically identify high im-
pact sub-structures relevant to a given task. In
this paper we present a systematic study inves-
tigating (combinations of) sequence and con-
volution kernels using different types of sub-
structures in document-level sentiment classi-
fication. We show that minimal sub-structures
extracted from constituency and dependency
trees guided by a polarity lexicon show 1.45
point absolute improvement in accuracy over a
bag-of-words classifier on a widely used sen-
timent corpus.
1 Introduction
An important subtask in sentiment analysis is sen-
timent classification. Sentiment classification in-
volves the identification of positive and negative
opinions from a text segment at various levels of
granularity including document-level, paragraph-
level, sentence-level and phrase-level. This paper
focuses on document-level sentiment classification.
There has been a substantial amount of work
on document-level sentiment classification. In ear-
ly pioneering work, Pang and Lee (2004) use a
flat feature vector (e.g., a bag-of-words) to rep-
resent the documents. A bag-of-words approach,
however, cannot capture important information ob-
tained from structural linguistic analysis of the doc-
uments. More recently, there have been several ap-
proaches which employ features based on deep lin-
guistic analysis with encouraging results including
Joshi and Penstein-Rose (2009) and Liu and Senef-
f (2009). However, as they select features manually,
these methods would require additional labor when
ported to other languages and domains.
In this paper, we study and evaluate diverse lin-
guistic structures encoded as convolution kernels for
the document-level sentiment classification prob-
lem, in order to utilize syntactic structures without
defining explicit linguistic rules. While the applica-
tion of kernel methods could seem intuitive for many
tasks, it is non-trivial to apply convolution kernels
to document-level sentiment classification: previous
work has already shown that categorically using the
entire syntactic structure of a single sentence would
produce too many features for a convolution ker-
nel (Zhang et al, 2006; Moschitti et al, 2008). We
expect the situation to be worse for our task as we
work with documents that tend to comprise dozens
of sentences.
It is therefore necessary to choose appropriate
substructures of a sentence as opposed to using the
whole structure in order to effectively use convolu-
tion kernels in our task. It has been observed that
not every part of a document is equally informa-
tive for identifying the polarity of the whole doc-
ument (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Koppel and Schler, 2005; Ferguson et
al., 2009): a film review often uses lengthy objective
paragraphs to simply describe the plot. Such objec-
tive portions do not contain the author?s opinion and
are irrelevant with respect to the sentiment classifi-
338
cation task. Indeed, separating objective sentences
from subjective sentences in a document produces
encouraging results (Yu and Hatzivassiloglou, 2003;
Pang and Lee, 2004; Koppel and Schler, 2005; Fer-
guson et al, 2009). Our research is inspired by these
observations. Unlike in the previous work, however,
we focus on syntactic substructures (rather than en-
tire paragraphs or sentences) that contain subjective
words.
More specifically, we use the terms in the lexi-
con constructed from (Wilson et al, 2005) as the
indicators to identify the substructures for the con-
volution kernels, and extract different sub-structures
according to these indicators for various types of
parse trees (Section 3). An empirical evaluation on
a widely used sentiment corpus shows an improve-
ment of 1.45 point in accuracy over the baseline
resulting from a combination of bag-of-words and
high-impact parse features (Section 4).
2 Related Work
Our research builds on previous work in the field
of sentiment classification and convolution kernel-
s. For sentiment classification, the design of lexi-
cal and syntactic features is an important first step.
Several approaches propose feature-based learning
algorithms for this problem. Pang and Lee (2004)
and Dave et al (2003) represent a document as a
bag-of-words; Matsumoto et al, (2005) extract fre-
quently occurring connected subtrees from depen-
dency parsing; Joshi and Penstein-Rose (2009) use
a transformation of dependency relation triples; Liu
and Seneff (2009) extract adverb-adjective-noun re-
lations from dependency parser output.
Previous research has convincingly demonstrat-
ed a kernel?s ability to generate large feature set-
s, which is useful to quickly model new and not
well understood linguistic phenomena in machine
learning, and has led to improvements in various
NLP tasks, including relation extraction (Bunescu
and Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al, 2006; Nguyen et al, 2009), question
answering (Moschitti and Quarteroni, 2008), seman-
tic role labeling (Moschitti et al, 2008).
Convolution kernels have been used before in sen-
timent analysis: Wiegand and Klakow (2010) use
convolution kernels for opinion holder extraction,
Johansson and Moschitti (2010) for opinion expres-
sion detection and Agarwal et al (2011) for sen-
timent analysis of Twitter data. Wiegand and K-
lakow (2010) use e.g. noun phrases as possible can-
didate opinion holders, in our work we extract any
minimal syntactic context containing a subjective
word. Johansson and Moschitti (2010) and Agarwal
et al (2011) process sentences and tweets respec-
tively. However, as these are considerably shorter
than documents, their feature space is less complex,
and pruning is not as pertinent.
3 Kernels for Sentiment Classification
3.1 Linguistic Representations
We explore both sequence and convolution kernels
to exploit information on surface and syntactic lev-
els. For sequence kernels, we make use of lexical
words with some syntactic information in the form
of part-of-speech (POS) tags. More specifically, we
define three types of sequences:
? SW, a sequence of lexical words, e.g.: A tragic
waste of talent and incredible visual effects.
? SP, a sequence of POS tags, e.g.: DT JJ NN IN
NN CC JJ JJ NNS.
? SWP, a sequence of words and POS tags,
e.g.: A/DT tragic/JJ waste/NN of/IN talent/NN
and/CC incredible/JJ visual/JJ effects/NNS.
In addition, we experiment with constituency tree
kernels (CON), and dependency tree kernels (D),
which capture hierarchical constituency structure
and labeled dependency relations between words,
respectively. For dependency kernels, we test with
word (DW), POS (DP), and combined word-and-
POS settings (DWP), and similarly for simple se-
quence kernels (SW, SP and SWP). We also use a
vector kernel (VK) in a bag-of-words baseline. Fig-
ure 1 shows the constituent and dependency struc-
ture for the above sentence.
3.2 Settings
As kernel-based algorithms inherently explore the
whole feature space to weight the features, it is im-
portant to choose appropriate substructures to re-
move unnecessary features as much as possible.
339
NP
PP
NP
DT JJ NN
A tragic waste
NP
IN
of
NP NP
NN
talent
CC
and
JJ JJ NNS
incredible visual effect
(a)
waste
det amod prep of
A tragic talent
conj and
effects
amod amod
incredible visual
(b)
waste
det amod prep of
DT JJ NN
conj and
NNS
amod amod
JJ JJ
(c)
waste
det amod prep of
DT
A
JJ
tragic
NN
talent
conj and
NNS
effects
amod amod
JJ
incredible
visual
visual
(d)
Figure 1: Illustration of the different tree structures employed for convolution kernels. (a) Constituent parse tree
(CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b)
with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP).
NP
DT JJ NN
A tragic waste
(a)
waste
amod
JJ
tragic
(b)
Figure 2: Illustration of the different settings on con-
stituency (CON) and dependency (DWP) parse trees with
tragic as the indicator word.
Unfortunately, in our task there exist several cues
indicating the polarity of the document, which are
distributed in different sentences. To solve this prob-
lem, we define the indicators in this task as subjec-
tive words in a polarity lexicon (Wilson et al, 2005).
For each polarity indicator, we define the ?scope?
(the minimal syntactic structure containing at least
one subjective word) of each indicator for different
representations as follows:
For a constituent tree, a node and its children
correspond to a grammatical production. There-
fore, considering the terminal node tragic in the con-
stituent structure tree in Figure 1(a), we extract the
subtree rooted at the grandparent of the terminal, see
Figure 2(a). We also use the corresponding sequence
Scopes Trees Size
Document 32 24
Subjective Sentences 22 27
Constituent Substructures 30 10
Dependency Substructures 40 3
Table 1: The detail of the corpus. Here Trees denotes the
average number of trees, and Size denotes the averaged
number of words in each tree.
of words in the subtree for the sequential kernel.
For a dependency tree, we only consider the sub-
tree containing the lexical items that are directly
connected to the subjective word. For instance, giv-
en the node tragic in Figure 1(d), we will extract its
direct parent waste integrated with dependency rela-
tions and (possibly) POS, as in Figure 2(b).
We further add two background scopes, one be-
ing subjective sentences (the sentences that contain
subjective words), and the entire document.
4 Experiments
4.1 Setup
We carried out experiments on the movie review
dataset (Pang and Lee, 2004), which consists of
340
1000 positive reviews and 1000 negative reviews.
To obtain constituency trees, we parsed the docu-
ment using the Stanford Parser (Klein and Man-
ning, 2003). To obtain dependency trees, we passed
the Stanford constituency trees through the Stanford
constituency-to-dependency converter (de Marneffe
and Manning, 2008).
We exploited Subset Tree (SST) (Collins and
Duffy, 2001) and Partial Tree (PT) kernels (Mos-
chitti, 2006) for constituent and dependency parse
trees1, respectively. A sequential kernel is applied
for lexical sequences. Kernels were combined using
plain (unweighted) summation. Corpus statistics are
provided in Table 1.
We use a manually constructed polarity lexicon
(Wilson et al, 2005), in which each entry is annotat-
ed with its degree of subjectivity (strong, weak), as
well as its sentiment polarity (positive, negative and
neutral). We only take into account the subjective
terms with the degree of strong subjectivity.
We consider two baselines:
? VK: bag-of-words features using a vector ker-
nel (Pang and Lee, 2004; Ng et al, 2006)
? Rand: a number of randomly selected sub-
structures similar to the number of extracted
substructures defined in Section 3.2
All experiments were carried out using the SVM-
Light-TK toolkit2 with default parameter settings.
All results reported are based on 10-fold cross vali-
dation.
4.2 Results and Discussions
Table 2 lists the results of the different kernel type
combinations. The best performance is obtained by
combining VK and DW kernels, gaining a signifi-
cant improvement of 1.45 point in accuracy. As far
as PT kernels are concerned, we find dependency
trees with simple words (DW) outperform both de-
pendency trees with POS (DP) and those with both
words and POS (DWP). We conjecture that in this
case, as syntactic information is already captured by
1A SubSet Tree is a structure that satisfies the constraint that
grammatical rules cannot be broken, while a Partial Tree is a
more general form of substructures obtained by the application
of partial production rules of the grammar.
2available at http://disi.unitn.it/moschitti/
Kernels Doc Sent Rand Sub
VK 87.05
VK + SW 87.25 86.95 87.25 87.40
VK + SP 87.35 86.95 87.45 87.35
VK + SWP 87.30 87.45 87.30 88.15*
VK + CON 87.45 87.65 87.45 88.30**
VK + DW 87.35 87.50 87.30 88.50**
VK + DP 87.75* 87.20 87.35 87.75
VK + DWP 87.70* 87.30 87.65 87.80*
Table 2: Results of kernels. Here Doc denotes the whole
document of the text, Sent denotes the sentences that con-
tains subjective terms in the lexicon, Rand denotes ran-
domly selected substructures, and Sub denotes the sub-
structures defined in Section 3.2. We use ?*? and ?**? to
denote a result is better than baseline VK significantly at
p < 0.05 and p < 0.01 (sign test), respectively.
the dependency representation, POS tags can intro-
duce little new information, and will add unneces-
sary complexity. For example, given the substruc-
ture (waste (amod (JJ (tragic)))), the PT kernel will
use both (waste (amod (JJ))) and (waste (amod (JJ
(tragic)))). We can see that the former is adding no
value to the model, as the JJ tag could indicate ei-
ther positive words (e.g. good) or negative words
(e.g. tragic). In contrast, words are good indicators
for sentiment polarity.
The results in Table 2 confirm two of our hy-
potheses. Firstly, it clearly demonstrates the val-
ue of incorporating syntactic information into the
document-level sentiment classifier, as the tree k-
ernels (CON and D*) generally outperforms vector
and sequence kernels (VK and S*). More impor-
tantly, it also shows the necessity of extracting ap-
propriate substructures when using convolution ker-
nels in our task: when using the dependency kernel
(VK+DW), the result on lexicon guided substruc-
tures (Sub) outperforms the results on document,
sentence, or randomly selected substructures, with
statistical significance (p<0.05).
5 Conclusion and Future Work
We studied the impact of syntactic information on
document-level sentiment classification using con-
volution kernels, and reduced the complexity of the
kernels by extracting minimal high-impact substruc-
tures, guided by a polarity lexicon. Experiments
341
show that our method outperformed a bag-of-words
baseline with a statistically significant gain of 1.45
absolute point in accuracy.
Our research focuses on identifying and using
high-impact substructures for convolution kernels in
document-level sentiment classification. We expect
our method to be complementary with sophisticated
methods used in state-of-the-art sentiment classifica-
tion systems, which is to be explored in future work.
Acknowledgement
The authors were supported by 863 State Key
Project No. 2006AA010108, the EuroMatrixPlus F-
P7 EU project (grant No 231720) and Science Foun-
dation Ireland (Grant No. 07/CE/I1142). Part of the
research was done while Zhaopeng Tu was visiting,
and Yifan He was at the Centre for Next Generation
Localisation (www.cngl.ie), School of Computing,
Dublin City University. We thank the anonymous
reviewers for their insightful comments. We are al-
so grateful to Junhui Li for his helpful feedback.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, pages 30?38. Association
for Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005a. A
Shortest Path Dependency Kernel for Relation Extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Methods
in Natural Language Processing, pages 724?731, Van-
couver, British Columbia, Canada, oct. Association for
Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005b. Sub-
sequence Kernels for Relation Extraction. In Y Weis-
s, B Sch o lkopf, and J Platt, editors, Proceedings of
the 19th Conference on Neural Information Processing
Systems, pages 171?178, Cambridge, MA. MIT Press.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems, pages 625?632.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation, Manchester, August.
Paul Ferguson, Neil O?Hare, Michael Davy, Adam
Bermingham, Paraic Sheridan, Cathal Gurrin, and
Alan F. Smeaton. 2009. Exploring the use of
paragraph-level annotations for sentiment analysis of
financial blogs. In Proceedings of the Workshop on
Opinion Mining and Sentiment Analysis.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing Dependency Features for Opinion Mining.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 313?316, Suntec, Singapore, jul.
Suntec, Singapore.
Dan Klein and Christopher D Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, jul. As-
sociation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2005. Using neutral
examples for learning polarity. In Proceedings of In-
ternational Joint Conferences on Artificial Intelligence
(IJCAI) 2005, pages 1616?1616.
Steve Lawrence Kushal Dave and David Pennock. 2003.
Mining the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proceed-
ings of the 12th International Conference on World
Wide Web, pages 519?528, ACM. ACM.
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Paradigm.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 161?
169, Singapore, aug. Singapore.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. Proceed-
ings of PAKDD?05, the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
3518/2005:21?32.
Alessandro Moschitti and Silvia Quarteroni. 2008. K-
ernels on Linguistic Structures for Answer Extraction.
In Proceedings of ACL-08: HLT, Short Papers, pages
113?116, Columbus, Ohio, jun. Association for Com-
putational Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning, pages 318?329, Berlin, Germany,
342
sep. Machine Learning: ECML 2006, 17th European
Conference on Machine Learning, Proceedings.
Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin. 2006.
Examining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 611?618,
Sydney, Australia, jul. Sydney, Australia.
Truc-Vien T Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1378?1387.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 271?278, Barcelona, S-
pain, jun. Barcelona, Spain.
Michael Wiegand and Dietrich Klakow. 2010. Convolu-
tion Kernels for Opinion Holder Extraction. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 795?803, Los An-
geles, California, jun. Los Angeles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354, Vancouver, British Columbia, Cana-
da, oct. Association for Computational Linguistics.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Toward-
s answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
pages 129?136, Association for Computational Lin-
guistics. Association for Computational Linguistics.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Features.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 825?832, Sydney, Australia, jul. Association for
Computational Linguistics.
343
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 349?353,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The DCU Dependency-Based Metric in WMT-MetricsMATR 2010
Yifan He Jinhua Du Andy Way Josef van Genabith
Centre for Next Generation Localisation
School of Computing
Dublin City University
Dublin 9, Ireland
{yhe,jdu,away,josef}@computing.dcu.ie
Abstract
We describe DCU?s LFG dependency-
based metric submitted to the shared eval-
uation task of WMT-MetricsMATR 2010.
The metric is built on the LFG F-structure-
based approach presented in (Owczarzak
et al, 2007). We explore the following
improvements on the original metric: 1)
we replace the in-house LFG parser with
an open source dependency parser that
directly parses strings into LFG depen-
dencies; 2) we add a stemming module
and unigram paraphrases to strengthen the
aligner; 3) we introduce a chunk penalty
following the practice of METEOR to re-
ward continuous matches; and 4) we intro-
duce and tune parameters to maximize the
correlation with human judgement. Exper-
iments show that these enhancements im-
prove the dependency-based metric?s cor-
relation with human judgement.
1 Introduction
String-based automatic evaluation metrics such as
BLEU (Papineni et al, 2002) have led directly
to quality improvements in machine translation
(MT). These metrics provide an alternative to ex-
pensive human evaluations, and enable tuning of
MT systems based on automatic evaluation results.
However, there is widespread recognition in
the MT community that string-based metrics are
not discriminative enough to reflect the translation
quality of today?s MT systems, many of which
have gone beyond pure string-based approaches
(cf. (Callison-Burch et al, 2006)).
With that in mind, a number of researchers have
come up with metrics which incorporate more so-
phisticated and linguistically motivated resources.
Examples include METEOR (Banerjee and Lavie,
2005; Lavie and Denkowski, 2009) and TERP
(Snover et al, 2010), both of which now uti-
lize stemming, WordNet and paraphrase informa-
tion. Experimental and evaluation campaign re-
sults have shown that these metrics can obtain bet-
ter correlation with human judgements than met-
rics that only use surface-level information.
Given that many of today?s MT systems incor-
porate some kind of syntactic information, it was
perhaps natural to use syntax in automatic MT
evaluation as well. This direction was first ex-
plored by (Liu and Gildea, 2005), who used syn-
tactic structure and dependency information to go
beyond the surface level matching.
Owczarzak et al (2007) extended this line of
research with the use of a term-based encoding of
Lexical Functional Grammar (LFG:(Kaplan and
Bresnan, 1982)) labelled dependency graphs into
unordered sets of dependency triples, and calculat-
ing precision, recall, and F-score on the triple sets
corresponding to the translation and reference sen-
tences. With the addition of partial matching and
n-best parses, Owczarzak et al (2007)?s method
considerably outperforms Liu and Gildea?s (2005)
w.r.t. correlation with human judgement.
The EDPM metric (Kahn et al, 2010) im-
proves this line of research by using arc labels
derived from a Probabilistic Context-Free Gram-
mar (PCFG) parse to replace the LFG labels,
showing that a PCFG parser is sufficient for pre-
processing, compared to a dependency parser in
(Liu and Gildea, 2005) and (Owczarzak et al,
2007). EDPM also incorporates more information
sources: e.g. the parser confidence, the Porter
stemmer, WordNet synonyms and paraphrases.
Besides the metrics that rely solely on the de-
pendency structures, information from the depen-
dency parser is a component of some other metrics
that use more diverse resources, such as the textual
entailment-based metric of (Pado et al, 2009).
In this paper we extend the work of (Owczarzak
et al, 2007) in a different manner: we use an
349
adapted version of the Malt parser (Nivre et al,
2006) to produce 1-best LFG dependencies and
allow triple matches where the dependency la-
bels are different. We incorporate stemming, syn-
onym and paraphrase information as in (Kahn et
al., 2010), and at the same time introduce a chunk
penalty in the spirit of METEOR to penalize dis-
continuous matches. We sort the matches accord-
ing to the match level and the dependency type,
and weight the matches to maximize correlation
with human judgement.
The remainder of the paper is organized as fol-
lows. Section 2 reviews the dependency-based
metric. Sections 3, 4, 5 and 6 introduce our im-
provements on this metric. We report experimen-
tal results in Section 7 and conclude in Section 8.
2 The Dependency-Based Metric
In this section, we briefly review the metric pre-
sented in (Owczarzak et al, 2007).
2.1 C-Structure and F-Structure in LFG
In Lexical Functional Grammar (Kaplan and Bres-
nan, 1982), a sentence is represented as both a hi-
erarchical c-(onstituent) structure which captures
the phrasal organization of a sentence, and a f-
(unctional) structure which captures the functional
relations between different parts of the sentence.
Our metric currently only relies on the f-structure,
which is encoded as labeled dependencies in our
metric.
2.2 MT Evaluation as Dependency Triple
Matching
The basic method of (Owczarzak et al, 2007) can
be illustrated by the example in Table 1.
The metric in (Owczarzak et al, 2007) performs
triple matching over the Hyp- and Ref-Triples and
calculates the metric score using the F-score of
matching precision and recall. Let m be the num-
ber of matches, h be the number of triples in the
hypothesis and e be the number of triples in the
reference. Then we have the matching precision
P = m/h and recall R = m/e. The score of the
hypothesis in (Owczarzak et al, 2007) is the F-
score based on the precision and recall of match-
ing as in (1):
Fscore = 2PRP +R (1)
Table 1: Sample Hypothesis and Reference
Hypothesis
rice will be held talks in egypt next week
Hyp-Triples
adjunct(will, rice)
xcomp(will, be)
adjunct(talks, held)
xcomp(be, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
adjunct(talks, week)
Reference
rice to hold talks in egypt next week
Ref-Triples
obl(rice, to)
obj(hold, to)
adjunct(week, talks)
adjunct(talks, in)
obj(in, egypt)
adjunct(week, next)
obj(hold, week)
2.3 Details of the Matching Strategy
(Owczarzak et al, 2007) uses several techniques
to facilitate triple matching. First of all, consider-
ing that the MT-generated hypotheses have vari-
able quality and are sometimes ungrammatical,
the metric will search the 50-best parses of both
the hypothesis and reference and use the pair that
has the highest F-score to compensate for parser
noise.
Secondly, the metric performs complete or par-
tial matching according to the dependency labels,
so the metric will find more matches on depen-
dency structures that are presumably more infor-
mative.
More specifically, for all except the LFG
Predicate-Only labeled triples of the form
dep(head, modifier), the method does not
allow a match if the dependency labels (deps)
are different, thus enforcing a complete match.
For the Predicate-Only dependencies, par-
tial matching is allowed: i.e. two triples are con-
sidered identical even if only the head or the
modifier are the same.
Finally, the metric also uses linguistic resources
for better coverage. Besides using WordNet syn-
onyms, the method also uses the lemmatized out-
put of the LFG parser, which is equivalent to using
350
an English lemmatizer.
If we do not consider these additional lin-
guistic resources, the metric would find the fol-
lowing matches in the example in Table 1:
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next), as these three
triples appear both in the reference and in the hy-
pothesis.
2.4 Points for Improvement
We see several points for improvement from Table
1 and the analysis above.
? More linguistic resources: we can use more
linguistic resources than WordNet in pursuit
of better coverage.
? Using the 1-best parse instead of 50-best
parses: the parsing model we currently use
does not produce k-best parses and using only
the 1-best parse significantly improves the
speed of triple matching. We allow ?soft?
triple matches to capture the triple matches
which we might otherwise miss using the 1-
best parse.
? Rewarding continuous matches: it
would be more desirable to reflect
the fact that the 3 matching triples
adjunct(talks, in), obj(in,
egypt) and adjunct(week, next)
are continuous in Table 1.
We introduce our improvements to the metric
in response to these observations in the following
sections.
3 Producing and Matching LFG
Dependency Triples
3.1 The LFG Parser
The metric described in (Owczarzak et al, 2007)
uses the DCU LFG parser (Cahill et al, 2004)
to produce LFG dependency triples. The parser
uses a Penn treebank-trained parser to produce
c-structures (constituency trees) and an LFG f-
structure annotation algorithm on the c-structure
to obtain f-structures. In (Owczarzak et al, 2007),
triple matching on f-structures produced by this
paradigm correlates well with human judgement,
but this paradigm is not adequate for the WMT-
MetricsMatr evaluation in two respects: 1) the in-
house LFG annotation algorithm is not publicly
available and 2) the speed of this paradigm is not
satisfactory.
We instead use the Malt Parser1 (Nivre et al,
2006) with a parsing model trained on LFG de-
pendencies to produce the f-structure triples. Our
collaborators2 first apply the LFG annotation algo-
rithm to the Penn Treebank training data to obtain
f-structures, and then the f-structures are converted
into dependency trees in CoNLL format to train
the parsing model. We use the liblinear (Fan et
al., 2008) classification module to for fast parsing
speed.
3.2 Hard and Soft Dependency Matching
Currently our parser produces only the 1-best
outputs. Compared to the 50-best parses in
(Owczarzak et al, 2007), the 1-best parse limits
the number of triple matches that can be found. To
compensate for this, we allow triple matches that
have the same Head and Modifier to consti-
tute a match, even if their dependency labels are
different. Therefore for triples Dep1(Head1,
Mod1) and Dep2(Head2, Mod2), we allow
three types of match: a complete match if
the two triples are identical, a partial match if
Dep1=Dep2 and Head1=Head2, and a soft
match if Head1=Head2 and Mod1=Mod2.
4 Capturing Variations in Language
In (Owczarzak et al, 2007), lexical variations at
the word-level are captured by WordNet. We
use a Porter stemmer and a unigram paraphrase
database to allow more lexical variations.
With these two resources combined, there are
four stages of word level matching in our sys-
tem: exact match, stem match, WordNet match and
unigram paraphrase match. The stemming mod-
ule uses Porter?s stemmer implementation3 and the
WordNet module uses the JAWS WordNet inter-
face.4 Our metric only considers unigram para-
phrases, which are extracted from the paraphrase
database in TERP5 using the script in the ME-
TEOR6 metric.
1http://maltparser.org/index.html
2O?zlem C?etinog?lu and Jennifer Foster at the National
Centre for Language Technology, Dublin City University
3http://tartarus.org/?martin/
PorterStemmer/
4http://lyle.smu.edu/?tspell/jaws/
index.html
5http://www.umiacs.umd.edu/?snover/
terp/
6http://www.cs.cmu.edu/?alavie/METEOR/
351
5 Adding Chunk Penalty to the
Dependency-Based Metric
The metric described in (Owczarzak et al, 2007)
does not explicitly consider word order and flu-
ency. METEOR, on the other hand, utilizes this in-
formation through a chunk penalty. We introduce
a chunk penalty to our dependency-based metric
following METEOR?s string-based approach.
Given a reference r = wr1...wrn, we denote
wri as ?covered? if it is the head or modifier of
a matched triple. We only consider the wris that
appear as head or modifier in the reference
triples. After this notation, we follow METEOR?s
approach by counting the number of chunks in
the reference string, where a chunk wrj ...wrk is
a sequence of adjacent covered words in the refer-
ence. Using the hypothesis and reference in Ta-
ble 1 as an example, the three matched triples
adjunct(talks, in), obj(in, egypt)
and adjunct(week, next) will cover a con-
tinuous word sequence in the reference (under-
lined), constituting one single chunk:
rice to hold talks (in) egypt next week
Based on this observation, we introduce a simi-
lar chunk penalty Pen as in METEOR in our met-
ric, as in 2:
Pen = ? ? ( #chunks#matches )
? (2)
where ? and ? are free parameters, which we tune
in Section 6.2. We add this penalty to the depen-
dency based metric (cf. Eq. (1)), as in Eq. (3).
score = (1? Pen) ? Fscore (3)
6 Parameter Tuning
6.1 Parameters of the Metric
In our metric, dependency triple matches can be
categorized according to many criteria. We as-
sume that some matches are more critical than
others and encode the importance of matches by
weighting them differently. The final match will
be the sum of weighted matches, as in (4):
m =
?
?tmt (4)
where ?t and mt are the weight and number of
match category t. We categorize a triple match ac-
cording to three perspectives: 1) the level of match
L={complete, partial}; 2) the linguistic resource
used in matching R={exact, stem, WordNet, para-
phrase}; and 3) the type of dependency D. To
avoid too large a number of parameters, we only
allow a set of frequent dependency types, along
with the type other, which represents all the other
types and the type soft for soft matches. We have
D={app, subj, obj, poss, adjunct, topicrel, other,
soft}.
Therefore for each triple match m, we can have
the type of the match t ? L?R?D.
6.2 Tuning
In sum, we have the following parameters to tune
in our metric: precision weight ?, chunk penalty
parameters ?, ?, and the match type weights
?1...?n. We perform Powell?s line search (Press et
al., 2007) on the sufficient statistics of our metric
to find the set of parameters that maximizes Pear-
son?s ? on the segment level. We perform the op-
timization on the MT06 portion of the NIST Met-
ricsMATR 2010 development set with 2-fold cross
validation.
7 Experiments
We experiment with four settings of the metric:
HARD, SOFT, SOFTALL and WEIGHTED in or-
der to validate our enhancements. The first two
settings compare the effect of allowing/not al-
lowing soft matches, but only uses WordNet as
in (Owczarzak et al, 2007). The third setting ap-
plies our additional linguistic features and the final
setting tunes parameter weights for higher correla-
tion with human judgement.
We report Pearson?s r, Spearman?s ? and
Kendall?s ? on segment and system levels on the
NIST MetricsMATR 2010 development set using
Snover?s scoring tool.7
Table 2: Correlation on the Segment Level
r ? ?
HARD 0.557 0.586 0.176
SOFT 0.600 0.634 0.213
SOFTALL 0.633 0.662 0.235
WEIGHTED 0.673 0.709 0.277
Table 2 shows that allowing soft triple matches
and using more linguistic features all lead
to higher correlation with human judgement.
Though the parameters might somehow overfit on
7http://www.umiacs.umd.edu/?snover/
terp/scoring/
352
the data set even if we apply cross validation, this
certainly confirms the necessity of weighing de-
pendency matches according to their types.
Table 3: Correlation on the System Level
r ? ?
HARD 0.948 0.905 0.786
SOFT 0.964 0.905 0.786
SOFTALL 0.975 0.976 0.929
WEIGHTED 0.989 1.000 1.000
When considering the system-level correlation
in Table 3, the trend is very similar to that of the
segment level. The improvements we introduce all
lead to improvements in correlation with human
judgement.
8 Conclusions and Future Work
In this paper we describe DCU?s dependency-
based MT evaluation metric submitted to WMT-
MetricsMATR 2010. Building upon the LFG-
based metric described in (Owczarzak et al,
2007), we use a publicly available parser instead
of an in-house parser to produce dependency la-
bels, so that the metric can run on a third party
machine. We improve the metric by allowing more
lexical variations and weighting dependency triple
matches depending on their importance according
to correlation with human judgement.
For future work, we hope to apply this method
to languages other than English, and performmore
refinement on dependency type labels and linguis-
tic resources.
Acknowledgements
This research is supported by the Science Foundation Ireland
(Grant 07/CE/I1142) as part of the Centre for Next Gener-
ation Localisation (www.cngl.ie) at Dublin City University.
We thank O?zlem C?etinog?lu and Jennifer Foster for providing
us with the LFG parsing model for the Malt Parser, as well as
the anonymous reviewers for their insightful comments.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An
automatic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the ACL
Workshop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization, pages
65?72, Ann Arbor, MI.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-distance depen-
dency resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings of the
42nd Meeting of the Association for Computational Lin-
guistics (ACL-2004), pages 319?326, Barcelona, Spain.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluation the role of bleu in machine trans-
lation research. In Proceedings of 11th Conference of the
European Chapter of the Association for Computational
Linguistics, pages 249?256, Trento, Italy.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library for
large linear classification. Journal of Machine Learning
Research, 9:1871?1874.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2010. Expected dependency pair match: predicting trans-
lation quality with expected syntactic structure. Machine
Translation.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammatical
representation. The mental representation of grammatical
relations, pages 173?281.
Alon Lavie andMichael J. Denkowski. 2009. he meteor met-
ric for automatic evaluation of machine translation. Ma-
chine Translation, 23(2-3).
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summarization,
pages 25?32, Ann Arbor, MI.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for dependency
parsing. In In The fifth international conference on Lan-
guage Resources and Evaluation (LREC-2006), pages
2216?2219, Genoa, Italy.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007. Labelled dependencies in machine translation eval-
uation. In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 104?111, Prague, Czech
Republic.
Sebastian Pado, Michel Galley, Dan Jurafsky, and Christo-
pher D. Manning. 2009. Robust machine translation
evaluation with entailment features. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 297?305,
Suntec, Singapore.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of 40th Annual
Meeting of the Association for Computational Linguistics
(ACL-2002), pages 311?318, Philadelphia, PA.
William H. Press, Saul A. Teukolsky, William T. Vetterling,
and Brian P. Flannery. 2007. Numerical Recipes 3rd Edi-
tion: The Art of Scientific Computing. Cambridge Univer-
sity Press, New York, NY.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2010. Ter-plus: paraphrase, semantic, and
alignment enhancements to translation edit rate. Machine
Translation.
353
Proceedings of SADAATL 2014, pages 11?20,
Dublin, Ireland, August 24, 2014.
Jargon-Term Extraction by Chunking
Adam Meyers
?
, Zachary Glass
?
, Angus Grieve-Smith
?
, Yifan He
?
,
Shasha Liao
?
and Ralph Grishman
?
New York University
?
, Google
?
meyers/angus/yhe/grishman@cs.nyu.edu, zglass@alumni.princeton.edu
Abstract
NLP definitions of Terminology are usually application-dependent. IR terms are noun sequences
that characterize topics. Terms can also be arguments for relations like abbreviation, definition or
IS-A. In contrast, this paper explores techniques for extracting terms fitting a broader definition:
noun sequences specific to topics and not well-known to naive adults. We describe a chunking-
based approach, an evaluation, and applications to non-topic-specific relation extraction.
1 Introduction
Webster?s II New College Dictionary (Houghton Mifflin Company, 2001, p.1138) defines terminology
as: The vocabulary of technical terms and usages appropriate to a particular field, subject, science,
or art. Systems for automatically extracting instances of terminology (terms) usually assume narrow
operational definitions that are compatible with particular tasks. Terminology, in the context of Infor-
mation Retrieval (IR) (Jacquemin and Bourigault, 2003) refers to keyword search terms (microarray,
potato, genetic algorithm), single or multi-word (mostly nominal) expressions collectively representing
topics of documents that contain them. These same terms are also used for creating domain-specific
thesauri and ontologies (Velardi et al., 2001). We will refer to these types of terms as topic-terms and
this type of terminology topic-terminology. In other work, types of terminology (genes, chemical names,
biological processes, etc.) are defined relative to a specific field like Chemistry or Biology (Kim et al.,
2003; Corbett et al., 2007; Bada et al., 2010). These classes are used for narrow tasks, e.g., Information
Extraction (IE) slot filling tasks within a particular genre of interest (Giuliano et al., 2006; Bundschus
et al., 2008; BioCreAtIvE, 2006). Other projects are limited to Information Extraction tasks that may
not be terminology-specific, but have terms as arguments, e.g., (Schwartz and Hearst, 2003; Jin et al.,
2013) detect abbreviation and definition relations respectively and the arguments are terms. In contrast
to this previous work, we have built a system that extracts a larger set of terminology, which we call
jargon-terminology. Jargon-terms may include ultracentrifuge, which is unlikely to be a topic-term of a
current biology article, but will not include potato, a non-technical word that could be a valid topic-term.
We aim to find all the jargon-terms found in a text, not just the ones that fill slots for specific relations.
As we show, jargon-terminology closely matches the notional (e.g., Webster?s) definition of terminol-
ogy. Furthermore, the important nominals in technical documents tend to be jargon-terms, making them
likely arguments of a wide variety of possible IE relations (concepts or objects that are invented, two
nominals that are in contrast, one object that is ?better than? another, etc.). Specifically, the identification
of jargon-terms lays the ground for IE tasks that are not genre or task dependent. Our approach which
finds all instances of terms (tokens) in text is conducive to these tasks. In contrast, topic-term detection
techniques find smaller sets of terms (types), each term occurring multiple times and the set of terms
collectively represents a topic, in a similar way that a set of documents can represent a topic.
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
11
This paper describes a system for extracting jargon-terms in technical documents (patents and journal
articles); the evaluation of this system using manually annotated documents; and a set of information
extraction (IE) relations which take jargon-terms as arguments. We incorporate previous work in termi-
nology extraction, assuming that terminology is restricted to noun groups (minus some left modifiers)
(Justeson and Katz, 1995);
1
and we use both topic-term extraction techniques (Navigli and Velardi,
2004) and relation-based extraction techniques (Jin et al., 2013) in components of our system. Rather
than looking at the distribution of noun groups as a whole for determining term-hood, we refine the
classes used by the noun group chunker itself, placing limitations on the candidate noun groups pro-
posed and then filtering the output by setting thresholds on the number and quality of the ?jargon-like?
components of the phrase. The resulting system admits not only topic-terms, but also other non-topic
instances of terminology. Using the more inclusive set of jargon-terms (rather than just topic-terms) as
arguments of the IE relations in section 6, we are able to detect a larger and more informative set of rela-
tion. Furthermore, these relations are salient for a wide variety of genres (unlike those in (BioCreAtIvE,
2006)) ? a genre-neutral definition of terminology makes this possible. For example, the CONTRAST
relation between the two bold face terms in necrotrophic effector system
A1
that is an exciting contrast
to the biotrophic effector models
A2
. would be applicable in most academic genres. Our jargon-terms
also contrast with the tactic of filling terminology slots in relations with any noun-group (Justeson and
Katz, 1995), as such a strategy overgenerates, lowering precision.
2 Topic-term Extraction
Topic-term extractors (Velardi et al., 2001; Tomokiyo and Hurst, 2003) collect candidate terms (N-grams,
noun groups, words) that are more representative of a foreground corpus (documents about a specific
topic) than they are of a background corpus (documents about a wide range of topics), using statistical
measures such as
Term Frequency
Inverse Document Frequency
(TFIDF), or a variation thereof. Due to the metrics used
and cutoffs assumed, the list of terms selected is usually no more than a few hundred distinct terms,
even for a large set of foreground documents and tend to be especially salient to that topic. The terms
can be phrases that lay people would not know (e.g., microarray, genetic algorithm) or common topics
for that document set (e.g., potato, computer). Such systems rank all candidate terms, using cutoffs
(minimum scores or percentages of the list) to separate out the highest-ranked terms as output. Thus
sets of topic-terms, derived this way, are dependent on the foreground and background assumed, and the
publication dates. So a precise definition would include such information, e.g., topic-terms(biomedical-
patents, random-patents, 1990?1999) would refer to those topic-terms that differentiate a foreground of
biomedical patents from the 1990s from a background of diverse patents from the same epoch. Narrower
topics are possible (e.g., comparing DNA-microarray patents to the same background); or broader ones
(e.g., if a diverse corpus including news articles, fiction and travel writing are the background set instead
of patents, then patent terms such as national stage application may be highly ranked in the output).
Thus topic-terms generated by these methods model a relationally based definition and are relative to the
chosen foregrounds, backgrounds and dates.
Topic-terms can include words/phrases like potato, wheat, rat, monkey, which may be common sub-
jects of some set of biomedical documents, but are not specific to a technical field. In contrast, jargon-
terms would include words (like ultracentrifuge, theorem, graduated cylinder) that are specific to tech-
nical language, but don?t tend to be topics of any current document of interest. Jargon-terms, like topic-
terms, can be defined relative to a particular foreground (which can also be represented as a set of
documents), but there is the implicit assumption that they all share the same background set: non-genre-
specific language (or simply a very diverse set of documents). It is also possible to refer to terminology in
general as the union of jargon-terms with respect to the set of specialized knowledge areas as foregrounds
and all sharing the same background of non-genre-specific language. Jargon-terms, like topic-terms, are
also time dependent, since some terms will eventually be absorbed into the common lexicon, e.g., com-
puter. However, we can make the simplifying assumption that we are talking about jargon in the present
1
We restrict our scope to nominal terminology, but acknowledge the importance of non-nominal terminology, e.g., event
verb terms (calcify, coactivate) which are crucial to IE.
12
time. Furthermore, jargon-term status is somewhat less time sensitive than topic-term status because ter-
minology is absorbed very sparingly (and very slowly) into the popular lexicon, whereas topics go in and
out of fashion quickly within a literature that is meant for an expert audience. Ignoring the potato type
cases, topic-terms are a proper subset of jargon-terms and, thus, the set of jargon-terms is larger than the
set of topic-terms. Finally, topic terms are ranked with respect to how well they can serve as keywords,
i.e., how specific they are to a particular document set, whereas +/-jargon-term is a binary distinction.
We built a topic term extractor that combines several metrics together in an ensemble including:
TFIDF, KL Divergence (Cover and Thomas, 1991; Hisamitsu et al., 1999) and a combination of Do-
main Relevance and Document Consensus (DRDC) based on (Navigli and Velardi, 2004). Furthermore,
we filtered the output by requiring that each term would be recognized as a term by the jargon-term chun-
ker described below in section 3. We manually scored the top 100 terms generated for two classes of
biology patents (US patent classes 435 and 436) and achieved accuracies of 85% and 76% respectively.
We also manually evaluated the top 100 terms taken from biology articles, yielding an accuracy of about
88%. As discussed, we use the output of this system for our jargon-term extraction system.
3 Jargon-term Extraction by Chunking
(Justeson and Katz, 1995) uses manual rules to detect noun groups (sequences of nouns and adjectives
ending in a noun) with the goal of detecting instances of topic-terms. They filter out those noun groups
that occur only once in the document on the theory that the multiply used noun groups are more likely to
be topics. They manually score their output from two computer science articles and one biotechnology
article, with 146, 350 and 834 instances of terms and achieve accuracies of 96%, 86% and 77%. (Frantzi
et al., 2000) uses linguistic rules similar to noun chunking to detect candidate terms; filters the results
using a stop list and other linguistic constraints; uses statistical filters to determine whether substrings
are likely to be terms as well; and uses statistical filters based on neighboring words (context). (Frantzi et
al., 2000) ranks their terms by scores and achieve about 75% accuracy for the top 40 terms ? their system
is tested on medical records (quite a different corpus form ours). Our system identifies all instances
of terminology (not just topic terms) and identifies many more instances per document (919, 1131 and
2166) than (Justeson and Katz, 1995) or (Frank, 2000). As we aim to find all instances of jargon-terms,
we evaluate for both precision and recall rather than just accuracy (section 5). Two of the documents
that we test on are patents, which have a very different word distribution than articles. In fact, due to
both the amount of repetition in patents and the presence of multiple types of terminology (legal terms
as well as topic-related terms), it is hard to imagine that eliminating terms occurring below a frequency
threshold (as with (Justeson and Katz, 1995)) would be an effective method of filtering. Furthermore,
(Frank, 2000) used a very different corpus than we did and they focused on a slightly different problem
(e.g., we did not attempt to find the highest-ranked terms and we did not attempt to find both long terms
and substrings which were terms). Thus while it is appropriate to compare our methodology, it is difficult
to compare our results.
We have implemented a hand-crafted term extractor, which we will call a jargon-term chunker because
it functions in much the same way as a noun group chunker. It uses a deterministic finite state machine,
based on parts of speech (POS) and a fine-tuned set of lexical categories. We observed that jargon-terms
are typically noun groups, minus some left modifiers, and normally include words that are not in standard
vocabulary or belong to certain other classes of words (e.g., nominalizations). While topic-term tech-
niques factor the distribution of whole term sequences into the choice of topic-terms, our method focuses
on the distribution of words within topic-term sequences. The primary function of POS classification is
to cluster words distributionally in a language. A POS tag reflects the syntactic distribution of the word
in the sense that words with the same POS should be able to replace each other in sentences. Morpholog-
ically, POSs are subject to the same morphological variation (prefixes, suffixes, tense, gender, number,
etc.). For example, the English word duck belongs to the POS noun because it tends to occur: after a
determiner, after an adjective, and ending a unit that can be the subject of a verb: nouns are substitutable
for each other. Furthermore, it has a plural form resulting from an -s or -es suffix, etc. Similarly, we
hold that the presence of particular classes of words within a noun group affects its potential to function
13
as a jargon-term. As will become evident, we can use topic-term-like metrics to identify some of these
word classes. Furthermore, given our previous assertion that topic-terms are a subset of jargon-terms,
we assume that the most saliently ranked topic-terms are also jargon-terms and words that are commonly
parts of topic-terms tend to be parts of jargon-terms. There are also ?morphological properties? that are
indicative of subsets of jargon-terms: allCap acronyms, chemical formulas, etc.
Our system classifies each word using POS tags, manually created dictionaries and the output of our
own topic-term system. These classifications are achieved in four stages. In the first stage we divide
the text into smaller segments using coordinate conjunctions (and, or, as well as, . . .) and punctuation
(periods, left/right parentheses and brackets, quotation marks, commas, colons, semi-colons). These
segments are typically smaller than the level of the sentence, but larger than most noun groups. These
segments are good units to process because they are larger than jargon-terms (substrings of noun groups)
and smaller than sentences (and thus provide a smaller search space). In the second stage, potential
jargon-term (PJs) are generated by processing tokens from left to right and classifying them using a
finite state machine (FSM). The third stage filters the PJs generated with a set of manually constructed
constraints, yielding a set of jargon-terms. A final filter (stage 4) identifies named entities and separates
them out from the true jargon-terms: it turns out that many named entities have similar phrase-internal
properties as jargon-terms.
The FSM (that generates PJs) in the second stage includes the following states (Ramshaw and Marcus,
1995): START (S) (marking the beginning of a segment), Begin Term (B-T), Inside Term (I-T), End
Term (E-T), and Other (O). A PJ is a sequence consisting of: (a) a single E-T; or (b) exactly one B-T,
followed by zero or more instances of I-T, followed by zero or one instances of E-T. Each transition to
a new state is conditioned on: (a) the (extended) POS tag of the current word; (b) the extended POS
tag of the previous word; and (c) the previous state. The extended POSs are derived from the output of
a Penn-Treebank-based POS tagger and refinements based on machine readable dictionaries, including
COMLEX Syntax (Macleod et al., 1997), NOMLEX (Macleod et al., 1998), and some manually encoded
dictionaries created for this project. Table 1 describes the transitions in the FSM (unspecified entries
mean no restriction). ELSE indicates that in all cases other than those listed, the FST goes to state O.
Extended POS tags are classified as follows.
Adjectives, words with POS tags JJ, JJR or JJS, are subdivided into:
STAT-ADJ: Words in this class are marked adjective in our POS dictionaries and found as the first word
in one of the top ranked topic-terms (for the topic associated with the input document).
TECH-ADJ: If an adjective ends in a suffix indicating (-ic, -cous, -xous, and several others) it is a
technical word, but it is not found in our list of exceptions, it is marked TECH-ADJ.
NAT-ADJ: An adjective, usually capitalized, that is the adjectival form of a country, state, city or conti-
nent, e.g., European, Indian, Peruvian, . . .
CAP-ADJ: An adjective such that the first letter is capitalized (but is not marked NAT-ADJ).
ADJ: Other adjectives
Nouns are marked NN or NNS by the POS tagger and are the default POS for out of vocabulary (OOV)
words. POS tags like NNP, NNPS and FW (proper nouns and foreign nouns) are not reliable for our POS
tagger (trained on news) when applied to patents and technical articles. So NOUN is also assumed for
these. Subclasses include:
O-NOUN: (Singular or plural) nouns not found in any of our dictionaries (COMLEX plus some person
names) or nouns found in lists of specialized vocabulary which currently include chemical names.
PER-NOUN: Nouns beginning with a capital that are in our dictionary of first and last names.
PLUR-NOUN: Nouns with POS NNS nouns that are not marked O-NOUN or PER-NOUN.
C-NOUN: Nouns with POS NN that are not marked O-NOUN or PER-NOUN.
Verbs Only ING-VERBs (VBG) and ED-VERBs (VBN and VBD) are needed for this task (other
verbs trigger state O). Finally, we use the following additional POS tags:
POSS: POS for ?s, split off from a possessive noun.
PREP: All prepositions (POS IN and TO)
ROM-NUM: Roman numerals (I, II, . . ., MMM)
14
Previous Current Previous New
POS POS State State
DET, PREP, POSS, VERB O
O-NOUN, C-NOUN, PLUR-NOUN ROM-NUM B-T, I-T E-T
PLUR-NOUN B-T,I-T I-T
ADJ, CAP-ADJ I-T I-T
C-NOUN, PER-NOUN, O-NOUN B-T, I-T I-T
O-NOUN CAP-ADJ, TECH-ADJ, B-T, I-T I-T
STAT-ADJ, NAT-ADJ
CAP-ADJ, TECH-ADJ, NAT-ADJ, E-T, O, S B-T
ING-VERB, ED-VERB, STAT-ADJ
C-NOUN, O-NOUN, PER-NOUN
TECH-ADJ, NAT-ADJ TECH-ADJ, NAT-ADJ B-T, I-T I-T
ADJ, CAP-ADJ ADJ, CAP-ADJ
ELSE O
Table 1: Transition Table
A potential jargon-term (PJ) is an actual jargon-term unless it is filtered out as follows. First, a jargon
term J must meet all of these conditions:
1. J must contain at least one noun.
2. J must be more than one character long, not counting a final period.
3. J must contain at least one word consisting completely of alphabetic characters.
4. J must not end in a common abbreviation from a list (e.g., cf., etc.)
5. J must not contain a word that violates a morphological filter, designed to rule out numeric identi-
fiers (patent numbers), mathematical formulas and other non-words. This rules out tokens beginning
with numbers that include letters; tokens including plus signs, ampersands, subscripts, superscripts;
tokens containing no alphanumeric characters at all, etc.
6. J must not contain a word that is a member of a list of common patent section headings.
Secondly, a jargon-term J must satisfy at least one of the following additional conditions:
1. J = highly ranked topic-term or a substring of J is a highly ranked topic-term.
2. J contains at least one O-NOUN.
3. J consists of at least 4 words, at least 3 of which are either nominalizations (C-NOUNs found in
NOMLEX-PLUS (Meyers et al., 2004; Meyers, 2007)) or TECH-ADJs.
4. J = nominalization at least 11 characters long.
5. J = multi-word ending in a common noun and containing a nominalization.
A final stage aims to distinguish named entities from jargon-terms. It turns out that named entities, like
jargon terms, include many out of vocabulary words. Thus we look for NEs among those PJs that remain
after stage 3 and contain capitalized words (a single capital letter followed by lowercase letters). These
NE filters are based on manually collected lists of named entities and nationality adjectives, as well as
common NE endings. Dictionary lookup is used to assign GPE (ACE?s Geopolitical Entity) to New York
or American; LOC(ation) to Aegean Sea and Ural Mountains; and FAC(ility) to Panama Canal and Suez
Canal. Plurals of nationality words, e.g., Americans are filtered out as non-terms. PJs are filtered by
endings typically associated with non-terms, e.g., et al signals PJs as citations to articles and honorifics
(Esq, PhD, Jr, Snr) signal PER(son) named entities. Finally, if at least one of the words in a multi-word
term is a first or last person name, we can further filter them by endings, where ORGanization endings
15
include Agency, Association, College and more than 65 others; GPE endings include Heights, Township,
Park; LOC(ation) endings include Street, Avenue and Boulevard. It turns out that 2 word capitalized
structures including at least one person name are usually either ORG or GPE in our patent corpus, and
we maintain this ambiguity, but mark them as non-terms.
We have described a first implementation of a jargon-term chunker based on a combination of prin-
ciples previously implemented in noun group chunking and topic-term extraction systems. The chunker
can use essentially the same algorithms as previous noun group chunkers, though in this case we used
a manual-rule based FSM. The extended POSs are defined according to conventional POS (represent-
ing substitutability, morphology, etc.), statistical topic-term extraction, OOV status (absence from our
dictionary) or presence in specialized dictionaries (NOMLEX, dictionary of chemicals, etc.). We use
topic-term extraction to identify both particular noun sequences (high-ranked topic-terms) and some of
their components (STAT-ADJ), and could extend this strategy to other components, e.g., common head
nouns. We approximated the concept of ?rare word? by noting which words were not found in our
standard dictionary (O-NOUN). As is well-known, ?noun? and ?adjective? are the first and second most
frequent POS for OOV words and both POSs are typically found as part of noun groups. Furthermore,
rare instances of O-NOUN (and OOV adjectives) are typically parts of jargon-terms. This approximation
is fine-tuned by the addition of word lists (e.g., chemicals). In future work, we can use more distribu-
tional information to fine-tune these categories, e.g., we can use topic-term techniques to identify single
topic words (nouns and adjectives) and experiment with these additional POS (instead of or in addition
to the current POS classes).
4 The Annotator Definition of Jargon-Term
For purposes of annotation, we defined jargon-term as a word or multi-word nominal expression that is
specific to some technical sublanguage. It need not be a proper noun, but it should be conventionalized
in one of the following two ways:
1. The term is defined early (possibly by being abbreviated) in the document and used repeatedly
(possibly only in its abbreviated form).
2. The term is special to a particular field or subfield (not necessarily the field of the document being
annotated). It is not enough if the document contains a useful description of an object of interest
? there must be some conventional, definable term that can be used and reused. Thus multi-word
expressions that are defined as jargon terms must be somewhat word-like ? mere descriptions that
are never reused verbatim are not jargon terms. (Justeson and Katz, 1995) goes further than we do:
they require that terms be reused within the document being annotated, whereas we only require
that they be reused (e.g., frequent hits in a web search).
Criterion 2 leaves open the question of how specific to a genre an expression must be to be considered a
jargon-term. At an intuitive level, we would like to exclude words like patient, which occur frequently
in medical texts, but are also commonly found in non-expert, everyday language. By contrast, we would
like to include words like tumor and chromosome, which are more intrinsic to technical language insofar
as they have specialized definitions and subtypes within medical language. To clarify, we posited that a
jargon-term must be sufficiently specialized so that a typical naive adult should not be expected to know
the meaning of the term. We developed 2 alternative models of a naive adult:
1. Homer Simpson, an animated TV character who caricatures the typical naive adult?the annotators
invoke the question: Would Homer Simpson know what this means?
2. The Juvenile Fiction sub-corpus of the COCA: The annotators go to http://corpus.byu.
edu/coca/ and search under FIC:Juvenile ? a single occurrence of an expression in this corpus
suggests that it is probably not a jargon-term.
In addition, several rules limited the span of terms to include the head and left modifiers that collocate
with the heads. Decisions about which modifiers to include in a term were difficult. However, as this
16
Strict Sloppy
Doc Terms Matches Pre Rec F Matches Pre Rec F
Annot 1
SRP 1131 798 70.8% 70.6% 70.7% 1041 92.5% 92.0% 92.2%
SUP 2166 1809 87.5% 83.5% 85.5% 1992 96.3% 92.0% 94.1%
VVA 919 713 90.9% 77.6% 83.7% 762 97.2% 82.9% 89.5%
Annot 2
SRP 1131 960 98.4% 84.9% 91.1% 968 99.2% 85.6% 91.9%
SUP 2166 1999 95.5% 92.3% 93.8% 2062 98.5% 95.2% 96.8%
VVA 919 838 97.4% 91.2% 94.2% 855 99.4% 93.0% 96.1%
Base 1
SRP 1131 602 24.3% 53.2% 33.4% 968 44.2% 96.8% 60.7%
SUP 2166 1367 36.5% 63.1% 46.2% 1897 50.6% 87.6% 64.2%
VVA 919 576 28.5% 62.7% 39.2% 887 44.0% 96.5% 60.4%
Base 2:
SRP 1131 66 24.9% 5.8% 9.5% 151 57.0% 13.4% 21.6%
SUP 2166 771 52.3% 35.6% 42.4% 1007 68.4% 46.5% 55.3%
VVA 919 270 45.8% 29.4% 35.8% 392 66.5% 42.6% 51.9%
System SRP 1131 932 39.0% 82.4% 53.0% 1121 46.9% 99.1% 63.7%
Without SUP 2166 1475 39.7% 68.1% 50.2% 1962 52.8% 90.6% 66.7%
Filter VVA 919 629 27.8% 68.4% 39.5% 900 39.8% 97.9% 56.6%
System
SRP 1131 669 69.0% 59.2% 63.7% 802 82.8% 70.9% 76.4%
SUP 2166 1193 64.7% 55.1% 59.5% 1526 82.8% 70.5% 76.1%
VVA 919 581 62.1% 63.2% 62.7% 722 77.2% 78.6% 77.9%
Table 2: Evaluation of Annotation, Baseline and Complete System Against Adjudicated Data
evaluation task came on the heels of the relation extraction task described in section 6, we based our
extent rules on the definitions and the set of problematic examples that were discussed and cataloged
during that project. This essentially formed the annotation equivalent of case-law for extents. We will
make our annotation specifications available on-line, along with discussions of these cases.
5 Evaluation
For evaluation purposes, we annotated all the instances of jargon-terms in a speech recognition patent
(SRP), a sunscreen patent (SUP) and an article about a virus vaccine (VVA). Each document was an-
notated by 2 people and then adjudicated by Annotator 2 after discussing controversial cases. Table 2
scores the system, annotator 1 and annotator 2, by comparing each against the answer key providing:
number of terms in the answer key, number of matches, precision, recall and F-measure. The ?strict?
scores are based on exact matches between system terms and answer key terms, whereas the ?sloppy?
scores count as correct instances where part of a system term matches part of an answer key term (span
errors). As the SRP document was annotated first, some of specification agreement process took place
after annotation and the scores for annotators are somewhat lower than for the other documents. How-
ever, Annotator 1?s scores for SUP and VVA are good approximations of how well a human being should
be expected to perform and the system?s scores should be compared to Annotator 1 (i.e., accounting for
the adjudicator?s bias).
There are 4 system results: two baseline systems and two stages of the system described in section 3.
Baseline 1 assumes terms derived by removing determiners from noun groups ? we used an MEMM
chunker using features from the GENIA corpus (Kim et al., 2003). That system has relatively high recall,
but overgenerates, yielding a lower precision and F-measure than our full system ? it is also inaccurate
at determining the extent of terms. Baseline 2 restricts the noun groups from this same chunker to those
with O-NOUN heads. This improves the precision at a high cost to recall. Similarly, we first ran our
system without filtering the potential jargon-terms, and then we ran the full system. Clearly our more
complex strategy performs better than these baselines and the linguistic filters increase precision more
than they reduce recall, resulting in higher F-measures (though low-precision high-recall output may be
better for some applications).
17
6 Relations with Jargon-Terms
(Meyers et al., 2014) describes the annotation of 200 PubMed articles from and 26 patents with several
relations, as well as a system for automatically extracting relations. It turned out that the automatic
system depended on the creation of a jargon-term extraction system and thus that work was the major
motivating factor for the research described here. Choosing topic-terms as potential arguments would
have resulted in low recall. In contrast, allowing any noun-group to be an argument would have lowered
precision, e.g., diagram, large number, accordance and first step are unlikely to be valid arguments of
relations. In the example: The resequencing pathogen microarray
A2
in the diagram is a promising new
technology., we can detect that the authors of the articles view pathogen microarray as significant, and
not the NG diagram. By selecting jargon-terms as potential arguments we are selecting the most probable
noun group arguments for our relations. For the current system (which does not use a parser), the system
performs best if non-jargon-terms are not considered as potential relation arguments at all. However, one
could imagine a wider coverage (and slower) system incorporating a preference for jargon-terms (like a
selection restriction) with dependency-based constraints.
We will only describe a few of these relations due to space considerations. Our relations include:
(1) ABBREVIATE, a relation between two terms that are equivalent. In the normal case, one term
is clearly a shorthand version of the other, e.g., ?The D. melanogaster gene Muscle LIM protein at
84B
A1
(abbreviated as Mlp84B
A2
)?. However, in the special case (ABBREVIATE:ALIAS) neither
term is a shorthand for the other. For example in ?Silver behenate
A1
, also known as CH3-(CH2)20-
COOAg
A2
?, the chemical name establishes that this substance is a salt, whereas the formula provides
the proportions of all its constituent elements; (2) ORIGINATE, the relation between an ARG1 (person,
organization or document) and an ARG2 (a term), such that the ARG1 is an inventor, discoverer, manu-
facturer, or distributor of the ARG2 and some of these roles are differentiated as subtypes of the relation.
Examples include the following: ?Eagle
A1
?s minimum essential media
A2
and DOPG
A2
was obtained
from Avanti Polar Lipids
A1
?. (3) EXEMPLIFY, an IS-A relation (Hearst, 1992) between terms so
that ARG1 is an instance of ARG2, e.g., ?Cytokines
A2
, for instance interferon
A1
?; and ?proteins
A2
such as insulin
A1
?; (4) CONTRAST relations, e.g., ?necrotrophic effector system
A1
that is an ex-
citing contrast to the biotrophic effector models
A2
?; (5) BETTER THAN relations, e.g., ?Bayesian
networks
A1
hold a considerable advantage over pairwise association tests
A2
?; and (6) SIGNIFICANT
relations, e.g., ?Anaerobic SBs
A2
are an emerging area of research and development? (ARG1, the author
of the article, is implicit). These relations are applicable to most technical genres.
7 Concluding Remarks
We have described a method for extracting instances of jargon-terms with an F-measure of between
62% and 77% (strict vs sloppy), about 73% to 84% of human performance. We expect this work to
facilitate the extraction of a wide reange of relations from technical documents. Previous work has
focused on generating topic-terminology or term types, extracted over sets of documents. In contrast, we
describe an effective method of extracting term tokens, which represent a larger percent of the instances
of terminology in documents and constitute arguments of many more potential relations. Our work on
relation extraction yielded very low recalls until we adopted this methodology. Consequently, we have
obtained recall of over 50% for many relations (with precision ranging from 70% for OPINION relations
like Significant to 96% for Originate.).
Acknowledgments
Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Inte-
rior National Business Center contract number D11PC20154. The U.S. Government is authorized to
reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should
not be interpreted as necessarily representing the official policies or endorsements, either expressed or
implied, of IARPA, DoI/NBC, or the U.S. Government.
18
References
M. Bada, L. E. Hunter, M. Eckert, and M. Palmer. 2010. An overview of the craft concept annotation guidelines.
In The Linguistic Annotation Workshop, ACL 2010, pages 207?211.
BioCreAtIvE. 2006. Biocreative ii.
M. Bundschus, M. Dejori, M. Stetter, V Tresp, and H. Kriegel. 2008. Extraction of semantic biomedical relations
from text using conditional random fields. BMC Bioinformatics, 9.
P. Corbett, C. Batchelor, and S. Teufel. 2007. Annotation of chemical named entities. In BioNLP 2007, pages
57?64.
Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. Wiley-Interscience, New York.
A. Frank. 2000. Automatic F-Structure Annotation of Treebank Trees. In Proceedings of The LFG00 Conference,
Berkeley.
K. Frantzi, S. Ananiadou, and H. Mima. 2000. Automatic recognition of multi-word terms:. the C-value/NC-value
method. International Journal on Digital Libraries, 3(2):115?130.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploiting shallow linguistic information for relation extraction
from biomedical literature. In EACL 2006, pages 401?408, Trento.
M. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. In ACL 1992, pages 539?545.
T. Hisamitsu, Y. Niwa, S. Nishioka, H. Sakurai, O. Imaichi, M. Iwayama, and A. Takano. 1999. Term extraction
using a new measure of term representativeness. In Proceedings of the First NTCIR Workshop on Research in
Japanese Text Retrieval and Term Recognition.
Houghton Mifflin Company. 2001. Webster?s II New College Dictionary. Houghton Mifflin Company.
C. Jacquemin and D. Bourigault. 2003. Term Extraction and Automatic Indexing. In R. Mitkov, editor, Handbook
of Computational Linguistics. Oxford University Press, Oxford.
Y. Jin, M. Kan, J. Ng, and X. He. 2013. Mining scientific terms and their definitions: A study of the acl anthology.
In EMNLP-2013.
J. S. Justeson and S. M. Katz. 1995. Technical terminology: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering, 1(1):9?27.
J. D. Kim, T. Ohta, Y. Tateisi, and J. I. Tsujii. 2003. Genia corpus?a semantically annotated corpus for bio-
textmining. Bioinformatics, 19 (suppl 1):i180?i182.
C. Macleod, R. Grishman, and A. Meyers. 1997. COMLEX Syntax. Computers and the Humanities, 31:459?481.
C. Macleod, R. Grishman, A. Meyers, L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of nominalizations. In
Proceedings of Euralex98.
A. Meyers, R. Reeves, C. Macleod, R. Szekeley, V. Zielinska, and B. Young. 2004. The Cross-Breeding of
Dictionaries. In Proceedings of LREC-2004, Lisbon, Portugal.
A. Meyers, G. Lee, A. Grieve-Smith, Y. He, and H. Taber. 2014. Annotating Relations in Scientific Articles. In
LREC-2014.
A. Meyers. 2007. Those Other NomBank Dictionaries ? Manual for Dictionaries that Come with NomBank.
http:nlp.cs.nyu.edu/meyers/nombank/nomdicts.pdf.
R. Navigli and P. Velardi. 2004. Learning Domain Ontologies from Document Warehouses and Dedicated Web
Sites. Computational Linguistics, 30.
L. A. Ramshaw and M. P. Marcus. 1995. Text Chunking using Transformation-Based Learning. In ACL Third
Workshop on Very Large Corpora, pages 82?94.
A. Schwartz and M. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text.
In Pacific Composium on Biocomputing.
T. Tomokiyo and M. Hurst. 2003. A language model approach to keyphrase extraction. In ACL 2003 Workshop
on Multiword Expressions: Analysis, Acquisition and Treatment.
19
P. Velardi, M. Missikoff, and R. Basili. 2001. Identification of relevant terms to support the construction of domain
ontologies. In Workshop on Human Language Technology and Knowledge Management - Volume 2001, pages
5:1?5:8.
20
