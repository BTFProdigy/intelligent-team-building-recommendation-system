Domain Portability in Speech-to-Speech Translation
Alon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin Han
Alicia Tribble, Donna Gates, Dorcas Wallace and Kay Peterson
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
1. INTRODUCTION
Speech-to-speech translation has made significant advances over
the past decade, with several high-visibility projects (C-STAR, Verb-
mobil, the Spoken Language Translator, and others) significantly
advancing the state-of-the-art. While speech recognition can cur-
rently effectively deal with very large vocabularies and is fairly
speaker independent, speech translation is currently still effective
only in limited, albeit large, domains. The issue of domain porta-
bility is thus of significant importance, with several current research
efforts designed to develop speech-translation systems that can be
ported to new domains with significantly less time and effort than
is currently possible.
This paper reports on three experiments on portability of a speech-
to-speech translation system between semantic domains.1 The ex-
periments were conducted with the JANUS system [5, 8, 12], ini-
tially developed for a narrow travel planning domain, and ported
to the doctor-patient domain and an extended tourism domain. The
experiments cover both rule-based and statistical methods, and hand-
written as well as automatically learned rules. For rule-based sys-
tems, we have investigated the re-usability of rules and other knowl-
edge sources from other domains. For statistical methods, we have
investigated how much additional training data is needed for each
new domain. We are also experimenting with combinations of
hand-written and automatically learned components. For speech
recognition, we have conducted studies of what parameters change
when a recognizer is ported from one domain to another, and how
these changes affect recognition performance.
2. DESCRIPTION OF THE INTERLINGUA
The first two experiments concern the analysis component of our
interlingua-based MT system. The analysis component takes a sen-
tence as input and produces an interlingua representation as output.
We use a task-oriented interlingua [4, 3] based on domain actions.
Examples of domain actions are giving information about the on-
set of a symptom (e.g., I have a headache) or asking a patient
1We have also worked on the issue of portability across languages
via our interlingua approach to translation [3] and on portability of
speech recognition across languages [10].
.
to perform some action (e.g., wiggle your fingers). The interlin-
gua, shown in the example below, has five main components: (1) a
speaker tag such as a: for doctor (agent) and c: for a patient (cus-
tomer), (2) a speech act, in this case, give-information (3)
some concepts (+body-state and+existence), and (4) some
arguments (body-state-spec= andbody-location=), and
(5) some sub-arguments (identifiability=no and
inside=head).
I have a pain in my head.
c:give-information+existence+body-state
(body-state-spec=(pain,identifiability=no),
body-location=(inside=head))
3. EXPERIMENT 1:
EXTENSION OF SEMANTIC GRAMMAR
RULES BY HAND AND BY AUTOMATIC
LEARNING
Experiment 1 concerns extension of the coverage of semantic
grammars in the medical domain. Semantic grammars are based
on semantic constituents such as request information phrases (e.g.,
I was wondering : : : ) and location phrases (e.g., in my right arm)
rather than syntactic constituents such as noun phrases and verb
phrases. In other papers [12, 5], we have described how our mod-
ular grammar design enhances portability across domains. The
portable grammar modules are the cross-domain module, contain-
ing rules for things like greetings, and the shared module, contain-
ing rules for things like times, dates, and locations. Figure 1 shows
a parse tree for the sentence How long have you had this pain? XDM
indicates nodes that were produced by cross-domain rules. MED in-
dicates nodes that were produced by rules from the new medical
domain grammar.
The preliminary doctor-patient grammar focuses on three med-
ical situations: give-information+existence ? giving
information about the existence of a symptom (I have been get-
ting headaches); give-information+onset ? giving infor-
mation about the onset of a symptom (The headaches started three
months ago); and give-information+occurrence ? giv-
ing information about the onset of an instance of the symptoms
(The headaches start behind my ears). Symptoms are expressed
as body-state (e.g., pain), body-object (e.g., rash), and
body-event (e.g., bleeding).
Our experiment on extendibility was based on a hand written
seed grammar that was extended by hand and by automatic learn-
ing. The seed grammar covered the domain actions mentioned
above, but did not cover very many ways to phrase each domain
action. For example, it might have covered The headaches started
[request-information+existence+body-state]::MED
( WH-PHRASES::XDM
( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )
HAVE-GET-FEEL::MED ( GET ( have ) ) you
HAVE-GET-FEEL::MED ( HAS ( had ) )
[super_body-state-spec=]::MED
( [body-state-spec=]::MED
( ID-WHOSE::MED
( [identifiability=]
( [id:non-distant] ( this ) ) )
BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )
Figure 1: Parser output with nodes produced by medical and cross-domain grammars.
Seed Extended Learned
IF 37.2 37.2 31.3
Domain Action 37.2 37.2 31.3
Speech Act
Recall 43.3 48.2 49.3
Precision 71.0 75.0 45.8
Concept List
Recall 2.2 10.1 32.5
Precision 12.5 42.2 25.1
Top-Level Arguments
Recall 0.0 7.2 29.6
Precision 0.0 42.2 34.4
Top-Level Values
Recall 0.0 8.3 29.8
Precision 0.0 50.0 39.2
Sub-Level Arguments
Recall 0.0 28.3 14.1
Precision 0.0 48.2 12.6
Sub-level Values
Recall 1.2 28.3 14.1
Precision 6.2 48.2 12.9
Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data
three months ago but not I started getting the headaches three months
ago. The seed grammar was extended by hand and by automatic
learning to cover a development set of 133 utterances. The re-
sult was two new grammars, a human-extended grammar and a
machine-learned grammar, referred to as the extended and learned
grammars in Table 1. The two new grammars were then tested on
132 unseen sentences in order to compare generality of the rules.
Results are reported only for 83 of the 132 sentences which were
covered by the current interlingua design. The remaining 49 sen-
tences were not covered by the current interlingua design and were
not scored. Results are shown in Table 1.
The parsed test sentences were scored in comparison to a hand-
coded correct interlingua representation. Table 1 separates results
for six components of the interlingua: speech act, concepts, top-
level arguments, top-level values, sub-level arguments, and sub-
level values, in addition to the total interlingua, and the domain
action (speech act and concepts combined). The components of the
interlingua were described in Section 2.
The scores for the total interlingua and domain action are re-
ported as percent correct. The scores for the six components of the
interlingua are reported as average percent precision and recall. For
example, if the correct interlingua for a sentence has two concepts,
and the parser produces three, two of which are correct and one of
which is incorrect, the precision is 66% and the recall is 100%.
Several trends are reflected in the results. Both the human-ex-
tended grammar and the machine-learned grammar show improved
performance over the seed grammar. However, the human extended
grammar tended to outperform the automatically learned grammar
in precision, whereas the automatically learned grammar tended to
outperform the human extended grammar in recall. This result is to
be expected: humans are capable of formulating correct rules, but
may not have time to analyze the amount of data that a machine can
analyze. (The time spent on the human extended grammar after the
seed grammar was complete was only five days.)
Grammar Induction: Our work on automatic grammar induc-
tion for Experiment 1 is still in preliminary stages. At this point,
we have experimented with completely automatic induction (no in-
teraction with a user)2 of new grammar rules starting from a core
grammar and using a development set of sentences that are not
parsable according to the core grammar. The development sen-
tences are tagged with the correct interlingua, and they do not stray
from the concepts covered by the core grammar ? they only cor-
respond to alternative (previously unseen) ways of expressing the
same set of covered concepts. The automatic induction is based
on performing tree matching between a skeletal tree representation
obtained from the interlingua, and a collection of parse fragments
2Previous work on our project [2] investigated learning of grammar
rules with user interaction.
[give-information+onset+symptom]
[manner=]
[sudden]
suddenly
[symptom-location=]
DETP
DET
POSS
my
BODYLOCATION
BODYFLUID
[urine]
urine
became [adj:symptom-name=]
ADJ-SYMPTOM
FUNCTION-ADJ-VALS [attribute=]
[color_attribute]
colored
[abnormal]
dis
Parse chunk #1 Parse chunk #2 Parse chunk #3
Original interlingua:
give-information+onset+symptom
(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,
manner=sudden)
Learned Grammar Rule:
s[give-information+onset+symptom]
( [manner=] [symptom-location=] *+became [adj:symptom-name=] )
Figure 2: A reconstructed parse tree from the Interlingua
that is derived from parsing the new sentence with the core gram-
mar. Extensions to the existing rules are hypothesized in a way that
would produce the correct interlingua representation for the input
utterance.
Figure 2 shows a tree corresponding to an automatically learned
rule. The input to the learning algorithm is the interlingua (shown
in bold boxes in the figure) and three parse chunks (circled in the
figure). The dashed edges are augmented by the learning algorithm.
4. EXPERIMENT 2:
PORTING TO A NEW DOMAIN
USING A HYBRID RULE-BASED AND
STATISTICAL ANALYSIS APPROACH
We are in the process of developing a new alternative analysis
approach for our interlingua-based speech-translation systems that
combines rule-based and statistical methods and we believe inher-
ently supports faster porting into new domains. The main aspects
of the approach are the following. Rather than developing com-
plete semantic grammars for analyzing utterances into our interlin-
gua (either completely manually, or using grammar induction tech-
niques), we separate the task into two main levels. We continue to
develop and maintain rule-based grammars for phrases that corre-
spond to argument-level concepts of our interlingua representation
(e.g., time expressions, locations, symptom-names, etc.). However,
instead of developing grammar rules for assembling the argument-
level phrases into appropriate domain actions, we apply machine
learning and classification techniques [1] to learn these mappings
from a corpus of interlingua tagged utterances. (Earlier work on
this task is reported in [6].)
We believe this approach should prove to be more suitable for
fast porting into new domains for the following reasons. Many of
the required argument-level phrase grammars for a new domain are
likely to be covered by already existing grammar modules, as can
be seen by examining the XDM (cross-domain) nodes in Figure 1.
The remaining new phrase grammars are fairly fast and straightfor-
ward to develop. The central questions, however, are whether the
statistical methods used for classifying strings of arguments into
domain actions are accurate enough, and what amounts of tagged
data are required to obtain reasonable levels of performance. To
assess this last question, we tested the performance of the current
speech-act and concept classifiers for the expanded travel-domain
when trained with increasing amounts of training data. The results
of these experiments are shown in Figure 3. We also report the
performance of the domain-action classification derived from the
combined speech-act and concepts. As can be seen, performance
reaches a relative plateau at around 4000-5000 utterances. We see
these results as indicative that this approach should indeed prove to
be significantly easier to port to new domains. Creating a tagged
database of this order of magnitude can be done in a few weeks,
rather than the months required for complete manual grammar de-
velopment time.
5. EXPERIMENT 3:
PORTING THE SPEECH RECOGNIZER
TO NEW DOMAINS
When the speech recognition components (acoustic models, pro-
nunciation dictionary, vocabulary, and language model) are ported
across domains and languages mainly three types of mismatches
Speech Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Concept Sequence Classification Accuracy for 16-
fold Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Dialog Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Figure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training Data
Baseline Systems WER on Different Tasks [%]
BN (Broadcast News) h4e98 1, all F-conditions 18.5
ESST (scheduling and travel planning domain) 24.3
BN+ESST 18.4
C-STAR (travel planning domain) 20.2
Adaptation!Meeting Recognition
ESST on meeting data 54.1
BN on meeting data 44.2
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (16 meetings) 38.7
BN+ESST on meeting data 42.2
+ language model interpolation (16 meetings) 39.0
Adaptation! Doctor-Patient Domain
C-STAR on doctor-patient data 34.1
+ language model interpolation ( 34 dialogs) 25.1
Table 2: Recognition Results
occur: (1) mismatches in recording condition; (2) speaking style
mismatches; as well as (3) vocabulary and language model mis-
matches. In the past these problems have mostly been solved by
collecting large amounts of acoustic data for training the acoustic
models and development of the pronunciation dictionary, as well
as large text data for vocabulary coverage and language model cal-
culation. However, especially for highly specialized domains and
conversational speaking styles, large databases cannot always be
provided. Therefore, our research has focused on the problem of
how to build LVCSR systems for new tasks and languages [7, 9]
using only a limited amount of data. In this third experiment we
investigate the results of porting the speech recognition component
of our MT system to different new domains. The experiments and
improvements were conducted with the Janus Speech Recognition
Toolkit JRTk [13].
Table 2 shows the results of porting four baseline speech recog-
nition systems to the doctor-patient domain, and to the meeting do-
main. The four baseline systems are trained on Broadcast News
(BN), English SpontaneousScheduling Task (ESST), combined BN
and ESST, and the travel planning domain of the C-STAR consor-
tium (http://www.c-star.org). The given tasks illustrate
a variety of domain size, speaking styles and recording conditions
ranging from clean spontaneous speech in a very limited domain
(ESST, C-STAR) to highly conversational multi-party speech in an
extremely broad domain (Meeting). As a consequence the error
rates on the meeting data are quite high but using MAP (Maximum
A Posteriori) acoustic model adaptation and language model adap-
tation the error rate can be reduced by about 10.2% relative over the
BN baseline system. With the doctor-patient data the drop in error
rate was less severe which can be explained by the similar speaking
style and recording conditions for C-STAR and doctor-patient data.
Details about the applied recognition engine can be found in [10]
for ESST and [11] for the BN system.
6. ACKNOWLEDGMENTS
The research work reported here was funded in part by the DARPA
TIDES Program and supported in part by the National Science
Foundation under Grant number 9982227. Any opinions, findings
and conclusions or recomendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the
National Science Foundation (NSF) or DARPA.
7. REFERENCES
[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. TiMBL: Tilburg Memory Based Learner, version 3.0
Reference Guide. Technical Report Technical Report 00-01,
ILK, 2000. Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`. Epiphenomenal Grammar Acquisition with
GSG. In Proceedings of the Workshop on Conversational
Systems of the 6th Conference on Applied Natural Language
Processing and the 1st Conference of the North American
Chapter of the Association for Computational Linguistics
(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,
D. Koll, and A. Waibel. The Janus-III Translation System.
Machine Translation. To appear.
[6] M. Munk. Shallow statistical parsing for machine translation.
Master?s thesis, University of Karlsruhe, Karlsruhe,
Germany, 1999. http://www.is.cs.cmu.edu/papers/
speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel. Polyphone Decision Tree
Specialization for Language Adaptation. In Proceedings of
the ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel. Interactive Translation of Conversational Speech.
Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,
and M. Woszczyna. Multilinguality in Speech and Spoken
Language Systems. Proceedings of the IEEE, Special Issue
on Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.
Multilingual Speech Recognition, chapter From Speech Input
to Augmented Word Lattices, pages 33?45. Springer Verlag,
Berlin, Heidelberg, New York, artificial Intelligence edition,
2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,
F. Metze, and M. Bett. Advances in Meeting Recognition.
Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,
A. Lavie, L. Levin, and A. Waibel. A Modular Approach to
Spoken Language Translation for Large Domains. In
Proceedings of Conference of the Association for Machine
Translation in the Americas (AMTA?98), Langhorn, PA,
October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.
Recognition of Conversational Telephone Speech using the
Janus Speech Engine. In Proceedings of the ICASSP?97,
Mu?nchen, Germany, 1997.
SPEECHALATOR: TWO-WAY SPEECH-TO-SPEECH TRANSLATION IN YOUR HAND
Alex Waibel
 
, Ahmed Badran
 
, Alan W Black
 
, Robert Frederking
 
, Donna Gates
 
Alon Lavie
 
, Lori Levin
 
, Kevin Lenzo

, Laura Mayfield Tomokiyo
Juergen Reichert

, Tanja Schultz   , Dorcas Wallace   , Monika Woszczyna , Jing Zhang
 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA

Cepstral, LLC,

Multimodal Technologies Inc,

Mobile Technologies Inc.
speechalator@speechinfo.org
ABSTRACT
This demonstration involves two-way automatic speech-
to-speech translation on a consumer off-the-shelf PDA. This
work was done as part of the DARPA-funded Babylon project,
investigating better speech-to-speech translation systems for
communication in the field. The development of the Speecha-
lator software-based translation system required addressing
a number of hard issues, including a new language for the
team (Egyptian Arabic), close integration on a small device,
computational efficiency on a limited platform, and scalable
coverage for the domain.
1. BACKGROUND
The Speechalator was developed in part as the next genera-
tion of automatic voice translation systems. The Phrasalator
is a one-way device that can recognize a set of pre-defined
phrases and play a recorded translation, [1]. This device
can be ported easily to new languages, requiring only a
hand translation of the phrases and a set of recorded sen-
tences. However, such a system severely limits communica-
tion as the translation is one way, thus reducing one party?s
responses to simple pointing and perhaps yes and no.
The Babylon project addresses the issues of two-way
communication where either party can use the device for
conversation. A number of different groups throughout the
US were asked to address specific aspects of the task, such
as different languages, translation techniques and platform
specifications. The Pittsburgh group was presented with
three challenges. First, we were to work with Arabic, a lan-
guage with which the group had little experience, to test our
capabilities in moving to new languages quickly. Second,
we were instructed to use an interlingua approach to trans-
lation, where the source language is translated into an in-
termediate form that is shared between all languages. This
step streamlines expansion to new languages, and CMU has
a long history in working with interlingua based translation
systems. Third, we were constrained to one portable PDA-
class device to host the entire two-way system: two recog-
nizers, two translation engines, and two synthesizers.
2. RECOGNITION
We used an HMM-based recognizer, developed by Multi-
modal Technologies Inc, which has been specifically tuned
for PDAs. The recognizer allows a grammar to be tightly
coupled with the recognizer, which offers important effi-
ciencies considering the limited computational power of the
device. With only minor modification we were able to gen-
erate our interlingua interchange format (IF) representation
directly as output from the recognizer, removing one mod-
ule from the process.
MTI?s recognizer requires under 1M of memory with
acoustic models of around 3M per language. Special op-
timizations deal with the slow processor and ensure low
use of memory during decoding. The Arabic models were
bootstrapped from the GlobalPhone [2] Arabic collections
as well as data collected as part of this project.
3. TRANSLATION
As part of this work we investigated two different tech-
niques for translation, both interlingua based. The first was
purely knowledge-based, following our previous work [3].
The engine developed for this was too large to run on the
device, although we were able to run the generation part off-
line seamlessly connected by a wireless link from the hand-
held device. The second technique we investigated used
a statistical training method to build a model to translate
structured interlingua IF to text in the target language. Be-
cause this approach was developed with the handheld in
mind, it is efficient enough to run directly on the device,
and is used in this demo.
4. SYNTHESIS
The synthesis engine is Cepstral?s Theta system. As the
Speechalator runs on very small hardware devices (at least
small compared to standard desktops), it was important that
the synthesis footprint remained as small as possible.
The speechalator is to be used for people with little ex-
posure to synthetic speech, and the output quality must be
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 29-30
                                                         Proceedings of HLT-NAACL 2003
very high. Cepstral?s unit selection voices, tailored to the
domain, meet the requirements for both quality and size.
Normal unit selection voices may take hundreds of megabytes,
but the 11KHz voices developed by Cepstral were around 9
megabytes each.
5. ARABIC
The Arabic language poses a number of challenges for any
speech translation system. The first problem is the wide
range of dialects of the language. Just as Jamaican and
Glaswegian speakers may find it difficult to understand each
other?s dialect of English, Arabic speakers of different di-
alects may find it impossible to communicate.
Modern Standard Arabic (MSA) is well-defined and widely
understood by educated speakers across the Arab world.
MSA is principally a written language and not a spoken lan-
guage, however. Our interest was in dealing with a normal
spoken dialect, and we chose Egyptian Arabic; speakers of
that dialect were readily accessible to us, and media influ-
ences have made it perhaps the most broadly understood of
the regional dialects.
Another feature of Arabic is that the written form, ex-
cept in specific rare cases, does not include vowels. For
speech recognition and synthesis, this makes pronunciations
hard. Solutions have been tested for recognition where the
vowels are not explicitly modeled, but implicitly modeled
by context. This would not work well for synthesis; we have
defined an internal romanization, based on the CallHome
[4] romanization, from which full phonetic forms can easily
be derived. This romanization is suitable for both recog-
nizer and synthesis systems, and can easily be transformed
into the Arabic script for display.
6. SYSTEM
The end-to-end system runs on a standard Pocket PC de-
vice. We have tested it on a number of different machines,
including various HP (Compaq) iPaq machines (38xx 39xx)
and Dell Axims. It can run on 32M machines, but runs best
on a 64M machine with about 40M made available for pro-
gram space. Time from the end of spoken input to start of
translated speech is around 2-4 seconds depending on the
length of the sentence and the actual processor. We have
found StrongARM 206MHz processors, found on the older
Pocket PCs, slightly faster than XScale 400MHz, though no
optimization for the newer processors has been attempted.
Upon startup, the user is presented with the screen as
shown in Figure 1. A push-to-talk button is used and the
speaker speaks in his language. The recognized utterance
is first displayed, with the translation following, and the ut-
terance is then spoken in the target language. Buttons are
provided for replaying the output and for switching the in-
put to the other language.
7. DISCUSSION
The current demonstration is designed for the medical inter-
view domain, with the doctor speaking English and the pa-
tient speaking Arabic. At this point in the project no formal
evaluation has taken place. However, informally, in office-
like acoustic environments, accuracy within domain is well
over 80%.
Arabic input Screen
Speechalator snapshot
8. REFERENCES
[1] Sarich, A., ?Phraselator, one-way speech translation
system,? http://www.sarich.com/translator/, 2001.
[2] T. Schultz and A. Waibel, ?The globalphone project:
Multilingual lvcsr with janus-3,? in Multilingual Infor-
mation Retrieval Dialogs: 2nd SQEL Workshop, Plzen,
Czech Republic, 1997, pp. 20?27.
[3] A. Lavie, et al ?A multi-perspective evaluation of
the NESPOLE! speech-to-speech translation system,?
in Proceedings of ACL 2002 workshop on Speech-to-
speech Translation: Algorithms and Systems, Philadel-
phia, PA., 2002.
[4] Linguistic Data Consortium, ?Callhome egyptian ara-
bic speech,? 1997.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 136?143,
New York, June 2006. c?2006 Association for Computational Linguistics
Understanding Temporal Expressions in Emails
Benjamin Han, Donna Gates and Lori Levin
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Ave, Pittsburgh PA 15213
{benhdj|dmg|lsl}@cs.cmu.edu
Abstract
Recent years have seen increasing re-
search on extracting and using temporal
information in natural language applica-
tions. However most of the works found
in the literature have focused on identi-
fying and understanding temporal expres-
sions in newswire texts. In this paper
we report our work on anchoring tempo-
ral expressions in a novel genre, emails.
The highly under-specified nature of these
expressions fits well with our constraint-
based representation of time, Time Cal-
culus for Natural Language (TCNL). We
have developed and evaluated a Tempo-
ral Expression Anchoror (TEA), and the
result shows that it performs significantly
better than the baseline, and compares fa-
vorably with some of the closely related
work.
1 Introduction
With increasing demand from ever more sophisti-
cated NLP applications, interest in extracting and
understanding temporal information from texts has
seen much growth in recent years. Several works
have addressed the problems of representing tem-
poral information in natural language (Setzer, 2001;
Hobbs and Pan, 2004; Saur?? et al, 2006), extracting
and/or anchoring (normalizing) temporal and event
related expressions (Wiebe et al, 1998; Mani and
Wilson, 2000; Schilder and Habel, 2001; Vazov,
2001; Filatova and Hovy, 2001), and discovering the
ordering of events (Mani et al, 2003). Most of these
works have focused on capturing temporal informa-
tion contained in newswire texts, and whenever both
recognition and normalization tasks of temporal ex-
pressions were attempted, the latter almost always
fell far behind from the former in terms of perfor-
mance.
In this paper we will focus on a different combi-
nation of the problems: anchoring temporal expres-
sions in scheduling-related emails. In our project
work of building personal agents capable of schedul-
ing meetings among different users1, understand-
ing temporal expressions is a crucial step. We have
therefore developed and evaluated our system Tem-
poral Expression Anchorer (TEA) that is capable of
normalizing such expressions in texts. As input TEA
takes English text with temporal expressions al-
ready identified, and transduces the expressions into
their representations using Time Calculus for Nat-
ural Language (TCNL) (Han and Kohlhase, 2003).
These representations, or TCNL formulae, are then
evaluated by incorporating the contextual informa-
tion to give the final normalized output. TCNL has
the following characteristics: (1) a human calendar
(e.g., the Gregorian calendar) is explicitly modeled
as a constraint system to deal with the highly under-
specified nature of many temporal expressions, and
it allows easy extension to include new temporal
primitives; (2) a set of NL-motivated operators with
a granularity-enriched type system facilitates the
representation of the intensional meaning of a tem-
1Project RADAR,
http://www.radar.cs.cmu.edu/external.asp
136
poral expression in a compositional way; and (3) the
use of temporal references such as ?focus? in the
representation cleanly separates the core meaning of
an expression from its contextual dependency.
The rest of this paper is organized as follows.
Sec. 2 first surveys the characteristics of temporal
expressions in emails compared to those in newswire
texts, and motivates the design of our representation.
Sec 3 then introduces the formalism TCNL. The sys-
tem TEA and the anchoring process is detailed in
Sec. 4, and the evaluation of the system is reported
in Sec. 5. Finally Sec. 6 concludes this paper and
outlines the future work.
2 Temporal Expressions in Emails
The extent of temporal expressions considered in
this paper includes most of the expressions using
temporal terms such as 2005, summer, evening,
1:30pm, tomorrow, etc. These expressions can be
classified into the following categories:
? Explicit: These expressions can be immedi-
ately anchored, i.e., positioned on a timeline.
E.g., June 2005, 1998 Summer, etc.
? Deictic: These expressions form a specific re-
lation with the speech time (timestamp of an
email). E.g., tomorrow, last year, two weeks
from today.
? Relative: These include the other expressions
that form a specific relation with a temporal fo-
cus, i.e., the implicit time central to the discus-
sion. E.g., from 5 to 7, on Wednesday, etc. Dif-
ferent from the speech time, a temporal focus
can shift freely during the discourse.
? Durational: These are the expressions that de-
scribe certain length in time. E.g., for about
an hour, less than 20 minutes. This is differ-
ent from an interval expression where both the
starting point and the ending point are given
(e.g., from 5 to 7). Most durational expres-
sions are used to build more complex expres-
sions, e.g., for the next 20-30 minutes.
It is worth emphasizing the crucial difference be-
tween deictic expressions and relative expressions:
anchoring the former only relies on the fixed speech
time while normalizing the latter requires the usually
hidden focus. As illustrated below the latter task can
be much more challenging:
?I?m free next week. Let?s meet on
Wednesday.?
?Are you free on Wednesday??
In the first example the ?Wednesday? denotes a dif-
ferent date since the first sentence sets up a different
focus. To make things even more interesting, ver-
bal tense can also play a role, e.g., ?He finished the
report on Wednesday.?
There are other types of temporal expressions
such as recurrence (?every Tuesday?) and rate ex-
pressions (?twice on Wednesday?) that are not sup-
ported in our system, although they are planned in
our future work (Sec. 6).
To appreciate the different nature of emails as a
genre, an interesting observation can be made by
comparing the distributions of temporal expressions
in emails and in newswire texts. The email cor-
pora we used for development and testing were col-
lected from MBA students of Carnegie Mellon Uni-
versity over the year 1997 and 1998. The 277 stu-
dents, organized in approximately 50 teams of 4 to
6 members, were participating in a 14-week course
and running simulated companies in a variety of
market scenarios (Kraut et al, 2004). The original
dataset, the CSpace email corpus, contains approx-
imately 15,000 emails. We manually picked 1,196
emails that are related to scheduling - these include
scheduling meetings, presentations, or general plan-
ning for the groups. The emails are then randomly
divided into five sets (email1 to email5), and only
four of them are used in this work: email1 was used
to establish our baseline, email2 and email5 were
used for development, and part of email4 was used
for testing. Table 1 shows some basic statistics of
these three datasets2, and an edited sample email is
shown in Fig. 1 (names altered). The most appar-
ent difference comparing these emails to newswire
texts is in the percentage of explicit expressions oc-
curring in the two different genres. In (Mani et al,
2003) it was reported that the proportion of such ex-
pressions is about 25% in the newswire corpus they
2The percentages in some rows do not add up to 100% be-
cause some expressions like coordination can be classified into
more than one type.
137
Date: Thu, 11 Sep 1997 00:14:36 -0500
I have put an outline out in the n10f1 OpReview directory...
(omitted)
We have very little time for this. Please call me Thursday
night to get clarification. I will need graphs and prose in
files by Saturday Noon.
? Mary
ps. Mark and John , I waited until AFTER midnight to
send this .
Figure 1: A sample email (edited)
used3. In contrast, explicit expressions on average
only account for around 9.5% in the three email
datasets. This is not surprising given that people
tend to use under-specified expressions in emails for
economic reasons. Another thing to note is that there
are roughly the same number of relative expressions
and non-relative expressions. Since non-relative ex-
pressions (including deictic expressions) can be an-
chored without tracking the temporal focus over a
discourse and therefore can be dealt with in a fairly
straightforward way, we may assign 50% as a some-
what generous baseline performance of any anchor-
ing system4.
Another difference between emails and newswire
texts is that the former is a medium for communi-
cation: an email can be used as a reply, or can be
attached within another email, or even be used to
address to multiple recipients. All of this compli-
cates a great deal of our task. Other notable dif-
ferences are that in emails hour ambiguity tend to
appear more often (?I?ll be home at 2.?), and peo-
ple tend to be more creative when they compose
short messages such as using tables (e.g., an entire
column of numbers to denote the number of min-
utes alloted for each presenter), bullet lists, abbrevi-
ations, and different month/day formats (?1/9? can
mean January 9 or September 1), etc. Emails also
contain more ?human errors? such as misspellings
(?Thusday? to mean Thursday) and confusion about
dates (e.g., using ?tomorrow? when sending emails
3Using the North American News Corpus.
4This is a bit generous since solving simple calendric arith-
metics such as anchoring last summer still requires a non-trivial
modeling of human calendars; see Sec. 3.
around midnight), etc. Overall it is very difficult to
recover from this type of errors.
3 Representing Times in Natural
Language
This section provides a concise overview of TCNL;
readers are referred to (Han and Kohlhase, 2003;
Han et al, 2006) for more detail.
TCNL has two major components: a constraint-
based model for human calendars and a represen-
tational language built on top of the model. Dif-
ferent from the other representations such as Zeit-
Gram (Stede and Haas, 1998), TOP (Androut-
sopoulos, 1999), and TimeML/Timex3 (Saur?? et al,
2006), the language component of TCNL is essen-
tially ?calendar-agnostic? - any temporal unit can be
plugged in a formula once it is defined in the cal-
endar model, i.e., the calendar model serves as the
lexicon for the TCNL language.
Fig. 2 shows a partial model for the Gregorian cal-
endar used in TEA. The entire calendar model is ba-
sically a constraint graph with partial ordering. The
nodes labeled with ?year? etc. represent temporal
units (or variables when viewed as a constraint sat-
isfaction problem (CSP) (Ruttkay, 1998)), and each
unit can take on a set of possible values. The undi-
rected edges represent constraints among the units,
e.g., the constraint between month and day man-
dates that February cannot have more than 29 days.
A temporal expression in NL is then viewed as if
it assigns values to some of the units, e.g., ?Friday
the 13th? assigns values to only units dow (day-
of-week) and day. An interval-based AC-3 algo-
rithm with a chronological backtracking mechanism
is used to derive at the consistent assignments to the
other units, therefore allowing us to iterate to any
one of the possible Friday the 13th.
The ordering among the units is designated by two
relations: measurement and periodicity (arrows in
Fig. 2). These relations are essential for supporting
various operations provided by the TCNL language
such as determining temporal ordering of two time
points, performing arithmetic, and changing tempo-
ral granularity, etc. For example, to interpret the ex-
pression ?early July?, we identify that July is a value
of unit month, and month is measured by day. We
then obtain the size of July in terms of day (31) and
138
Table 1: Basic statistics of the email corpora
# of
emails
# of
tempex
explicit deictic relative durational
email1 253 300 3 (1%) 139 (46.33%) 158 (52.67%) N/A
email2 253 344 19 (5.5%) 112 (32.6%) 187 (54.4%) 27 (7.8%)
email4 (part.) 149 279 71 (25.4%) 77 (27.6%) 108 (38.7%) 22 (7.9%)
email5 126 213 14 (6.6%) 105 (49.3%) 92 (43.2%) 3 (1.4%)
Year
Month Day
Hour
Minute
Second
Week
Day-of-week
Time-of-day
Time-of-week
Year component Week component
?
X component
unit constraints
alignment constraints
is-measured-by relation
is-periodic-in relation
*
*
*
*
*
*
*
(* marks a representative)
*
temporal unit
Figure 2: A partial model of the Gregorian calendar
designate the first 10 days (31/3) as the ?early? part
of July.
Internally the calendar model is further parti-
tioned into several components, and different com-
ponents are aligned using non-binary constraints
(e.g., in Fig. 2 the year component and the week
component are aligned at the day and dow units).
This is necessary because the top units in these com-
ponent are not periodic within one another. All of
the operations are then extended to deal with multi-
ple calendar components.
Built on top of the calendar model is the typed
TCNL language. The three major types are coor-
dinates (time points; e.g., {sep,6day} for Septem-
ber 6), quantities (durations; e.g., |1hour| for one
hour) and enumerations (sets of points, including
intervals; e.g., [{wed},{fri}] for Wednesday and
Friday). More complex expressions can be rep-
resented by using various operators, relations and
temporal references; e.g., {now?|1day|} for yes-
terday, {|1mon|@{>= }} for the coming Monday
(or the first coming Monday in the future; the
? ? represents the temporal focus), | < |1hour|| for
less than one hour, [{wed}:{fri}] for Wednes-
day to Friday, [f {sat, noon}] for by Saturday
noon5, and [[{15hour}:{17hour}]&{wed}] for 3-5pm
on Wednesday. The TCNL language is designed
in such a way that syntactically different formu-
lae can be evaluated to denote the same date;
e.g., {tue, now+|1week|} (?Tuesday next week?) and
{now+|1tue|} (?next Tuesday?) can denote the same
date.
Associated with the operators are type and granu-
larity requirements. For example, when a focus is
specified down to second granularity, the formula
{now+|1day|} will return a coordinate at the day
granularity - essentially stripping away information
finer than day. This is because the operator ?+?
(called fuzzy forward shifting) requires the left-hand
side operand to have the same granularity as that of
the right-hand side operand. Type coercion can also
happen automatically if it is required by an operator.
For example, the operator ?@? (ordinal selection) re-
quires that the right-hand side operand to be of type
enumeration. When presenting a coordinate such as
{>= } (some point in the future), it will be coerced
5The f denotes the relation ?finishes? (Allen, 1984); the for-
mula denotes a set of coordinates no later than a Saturday noon.
139
Table 2: Summary of operators in TCNL; LHS/RHS is the left/right operand, g(e) returns the granularity of
e and min(s) returns the set of minimal units among s.
operator Type requirement Granularity requirement Semantics Example
+ and ? C ? Q ? C g(LHS) ? g(RHS) fuzzy forward/backward
shifting
{now+|1day|}
(?tomorrow?)
++ and ?? C ? Q ? C g(LHS) ?
min(g(LHS)?g(RHS))
exact forward/backward
shifting
{now++|2hour|}
(?2 hours from now?)
@ Q ? E ? C g(RHS) ? g(LHS) ordinal {|2{sun}|@{may}}
(?the 2nd Sunday in May?)
& C ? C ? C
C ? E ? E
E ? C ? E
E ? E ? E
g(LHS) ?
min(g(LHS)?g(RHS))
distribution {now &{now+|1year|}}
(?this time next year?)
[{15hour}&[{wed}:{fri}]]
(?3pm from Wednesday to
Friday?)
into an enumeration so that the ordinal operator can
select a requested element out of it. These designs
make granularity change and re-interpretation part
of a transparent process. Table 2 lists the operators
in the TCNL language.
Most of under-specified temporal expressions still
lack necessary information in themselves in order to
be anchored. For example, it is not clear what to
make out of ?on Wednesday? with no context. In
TCNL more information can be supplied by using
one of the coordinate prefixes: the ?+?/??? prefix
signifies the relation of a coordinate with the fo-
cus (after/before the focus), and the ?f?/?p? indicates
the relation of a coordinate with the speech time
(future/past). For example, the Wednesday in ?the
company will announce on Wednesday? is repre-
sented as +f{wed}, while ?the company announced
on Wednesday? is represented as ?p{wed}. When
evaluating these formulae, TEA will rewrite the for-
mer into {|1wed|@{>= , >= now}} and the latter
into {?|1wed|@{<= , <= now}} if necessary, es-
sentially trying to find the nearest Wednesday ei-
ther in the future or in the past. Since TCNL for-
mulae can be embedded, prefixed coordinates can
also appear inside a more complex formula; e.g.,
{{|2{sun}|@f{may}}+|2day|} represents ?2 days af-
ter a future Mother?s day?6.
Note that TCNL itself does not provide a mecha-
nism to instantiate the temporal focus (? ?). The re-
sponsibility of shifting a focus whenever necessary
(focus tracking) is up to TEA, which is described in
the next section.
6This denotes a possible range of dates, but it is still different
from an enumeration.
4 TEA: Temporal Expression Anchorer
The input to our system TEA is English texts with
temporal expression markups, and the output is a
time string for each temporal expression. The format
of a time string is similar to the ISO 8601 scheme:
for a time point the format is YYYYMMDDTHHMMSS
(T is a separator), for an interval it is a pair of points
separated by ?/? (slash). Also whenever there are
slots that lack information, we use ??? (question
mark) in its place. If a points can reside at any place
between two bounds, we use (lower..upper)
to represent it. Table. 3 shows the TEA output over
the example email given in Fig. 1 (min and max are
the minimal and the maximal time points TEA can
reason with).
TEA uses the following procedure to anchor each
temporal expression:
1. The speech time (variable now) and the focus
(? ?) is first assigned to a timestamp (e.g., the
received date of an email).
2. For each temporal expression, its nearest verb
chunk is identified using the part-of-speech
tags of the sentence. Expressions associated
with a verb of past tense or present imperfective
will be given prefix ??p? to its TCNL formula,
otherwise it is given ?+f?7.
3. A finite-state parser is then used to transduce an
expression into its TCNL formula. At the pars-
ing stage the tense and granularity information
is available to the parser.
7This is of course a simplification; future work needs to be
done to explore other possibilities.
140
Table 3: Anchoring example for the email in Fig. 1
Expression TCNL formula Temporal focus (f ) Anchored time string
(timestamp) 19970911T001436
Thursday night +f{thu,night} 19970911T001436 (19970911T18????..
19970911T23????)
by Saturday Noon [f +f{sat,noon}] (19970911T18????..
19970911T23????)
min/19970913T12????
until AFTER mid-
night
[f{>= ?p{midnight}}] 19970911T001436 min/(19970911..max)
4. The produced TCNL formula (or formulae
when ambiguity arises) is then evaluated with
the speech time and the current focus. In case
of ambiguity, one formula will be chosen based
on certain heuristics (below). The result of the
evaluation is the final output for the expression.
5. Recency-based focus tracking: we use the fol-
lowing procedure to determine if the result ob-
tained above can replace the current focus (be-
low). In cases where the result is an ambigu-
ous coordinate (i.e., it denotes a possible range
of points), if one of the bounds is min or max,
we use the other to be the new focus; if it is
not possible, we choose to keep the focus un-
changed. On the other hand, if the result is
an enumeration, we go through a similar pro-
cedure to avoid using an enumeration with a
min/max bound as the new focus. Finally no
quantity can become a focus.
Note that in Step 3 the decision to make partial
semantics of a temporal expression available to our
parser is based on the following observation: con-
sider the two expressions below
?Tuesday before Christmas?
= {tue, < {|25day|@{dec}}}
?Tuesday before 6pm?
= {< {tue,18hour}, de {tue}}
Both expressions share the same ?X before Y ? pat-
tern, but their interpretations are different8. The key
to discriminate the two is to compare the granulari-
ties of X and Y : if Y if at a coarser granularity then
the first interpretation should be adopted.
In Step 4 we use the following procedure to dis-
ambiguate the result:
8de denotes a relation ?during or equal? (Allen, 1984).
1. Remove any candidate that resulted in an in-
consistency when solving for a solution in the
calendar CSP.
2. If the result is meant to be a coordinate, pick
the one that is closest to the focus.
3. If the result is supposed to be an enumeration,
pick the one whose starting point is closest to
the focus, and whose length is the shortest one.
4. Otherwise pick the first one as the result.
For example, if the current time is 2:00 pm, for ex-
pression ?at 3? with a present/future tense, the best
answer is 15:00. For expression ?from 3 to 5?, the
best answer is from 3 pm to 5 pm.
When deciding whether a temporal expression
can become the next focus, we use simple heuris-
tics to rule out any expression that behaves like a
noun modifier. This is motivated by the following
example (timestamp: 19970919T103315):
IT basically analyses the breakdown on
labor costs and compares our 1998 labor
costs with their demands for 1999-2000.
...
I will check mail on Sunday and see any
feedback.
Without blocking the expression 1999-2000 from
becoming the focus, the last expression will be in-
correctly anchored in year 2000. The key obser-
vation here is that a noun-modifying temporal ex-
pression usually serves as a temporal co-reference
instead of representing a new temporal entity in the
discourse. These references tend to have a more con-
fined effect in anchoring the subsequent expressions.
141
Table 4: Development and testing results
Accuracy Parsing errors Human errors Anchoring errors
email2 (dev) 78.2% 10.47% 1.7% 9.63%
email5 (dev) 85.45% 5.16% 1% 8.39%
email4 (test-
ing)
76.34% 17.92% < 1% 5.74%
5 Evaluation
The temporal expressions in all of the datasets were
initially tagged using rules developed for Minor-
Third9, and subsequently corrected manually by two
of the authors. We then developed a prototype sys-
tem and established our baseline over email1 (50%).
The system at that time did not have any focus track-
ing mechanism (i.e., it always used the timestamp
as the focus), and it did not use any tense infor-
mation. The result confirms our estimate given in
Sec. 2. We then gradually developed TEA to its cur-
rent form using email1, email2 and email5. Dur-
ing the four-month development we added the focus
tracking mechanism, incorporating the tense infor-
mation into each TCNL formula via the coordinate
prefixes, and introduced several representational im-
provements. Finally we tested the system on the un-
seen dataset email4, and obtained the results shown
in Table 4. Note that the percentages reported in
the table are accuracies, i.e., the number of cor-
rectly anchored expressions over the total number
of temporal expressions over a dataset, since we are
assuming correct tagging of all of the expressions.
Our best result was achieved in the dev set email5
(85.45%), and the accuracy over the test set email4
was 76.34%.
Table 4 also lists the types of the errors made by
our system. The parsing errors are mistakes made
at transducing temporal expressions using the finite-
state parser into their TCNL formulae, the human
errors are described in Sec. 2, and the rest are the
anchoring errors. The accuracy numbers are all
compared favorably to the baseline (50%). To put
this performance in perspective, in (Wiebe et al,
1998) a similar task was performed over transcribed
scheduling-related phone conversations. They re-
ported an average accuracy 80.9% over the CMU
9http://minorthird.sourceforge.net/
test set and 68.9% over the NMSU test set. Although
strictly speaking the two results cannot be compared
due to differences in the nature of the corpora (tran-
scription vs. typing), we nevertheless believe it rep-
resents a closer match compared to the other works
done on newswire genre.
It should also be noted that we adopted a simi-
lar recency-based focus model as in (Wiebe et al,
1998). Although simple to implement, this naive
approach proved to be one major contributor to the
anchoring errors in our experiments. An example is
given below (the anchored times are shown in sub-
script):
This research can not proceed until the
trade-offs are known on Monday19970818 .
...
Mary will perform this by
Friday(min..19970822) using the data
from Monday19970825 .
The last expression received an incorrect date: it
should be the same date the expression ?on Mon-
day? refers to. Our system made this error because
it blindly used the most recently mentioned time
((min..19970822)) as the focus to anchor the
formula +f{mon}. This error later also propagated
to the anchoring of the subsequent expressions.
6 Conclusion and Future Work
In this paper we have adopted a constraint-based
representation of time, Time Calculus for Natural
Language (TCNL), to tackle the task of anchoring
temporal expressions in a novel genre, emails. We
believe that the genre is sufficiently different from
newswire texts, and its highly under-specified nature
fits well with a constraint-based modeling of human
calendars. TCNL also allows for an explicit repre-
sentation of temporal focus, and many of our intu-
itions about granularity change and temporal arithe-
142
matics are encapsulated in its type system and oper-
ators. The performance of our anchoring system is
significantly better than baseline, and compares fa-
vorably with some of the closely related work.
In the future we will re-examine our focus track-
ing mechanism (being the most significant source of
errors), and possibly treat it as a classification prob-
lem (similar to (Mani et al, 2003)). We also need to
investigate the disambiguation procedure and pos-
sibly migrate the functionality into a separate dis-
course module. In addition, the co-referencing ten-
dency of noun-modifying expressions could lead to
a better way to anchoring this particular type of tem-
poral expressions. Finally we would like to ex-
pand our coverage of temporal expressions to in-
clude other types of expressions such as recurrence
expressions10.
Acknowledgments
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. NBCHD030010.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA), or the Department of Interior-National
Business Center (DOI-NBC).
References
J. F. Allen. 1984. Towards a General Theory of Action
and Time. Artificial Intelligence, 23:123?154.
I. Androutsopoulos. 1999. Temporal Meaning Rep-
resentations in a Natural Language Front-end. In
M. Gergatsoulis and P. Rondogiannis, editors, Inten-
sional Programming II (Proceedings of the 12th In-
ternational Symposium on Languages for Intensional
Programming, Athens, Greece.
E. Filatova and E. Hovy. 2001. Assigning Time-
Stamps To Event-Clauses. In Proceedings of ACL-
2001: Workshop on Temporal and Spatial Information
Processing, Toulouse, France, 7.
10The current design of TCNL allows for a more restricted
type of recurrence: e.g., ?3pm from Wednesday to Friday? is
represented as [{15hour}&[{wed}:{fri}]]. However this is in-
sufficient to represent expressions such as ?every 4 years?.
Benjamin Han and Michael Kohlhase. 2003. A Time
Calculus for Natural Language. In The 4th Work-
shop on Inference in Computational Semantics, Nancy,
France, September.
B. Han, D. Gates, and L. Levin. 2006. From Language to
Time: A Temporal Expression Anchorer. In Proceed-
ings of the 13th International Symposium on Tempo-
ral Representation and Reasoning (TIME 2006), Bu-
dapest, Hungary.
J. R. Hobbs and Feng. Pan. 2004. An ontology of time
for the semantic web. TALIP Special Issue on Spa-
tial and Temporal Information Processing, 3(1):66?
85, March.
R. E. Kraut, S. R. Fussell, F. J. Lerch, and A Espinosa.
2004. Coordination in teams: Evidence from a sim-
ulated management game. Journal of Organizational
Behavior, to appear.
I. Mani and G. Wilson. 2000. Robust Temporal Process-
ing of News. In Proceedings of ACL-2000.
I. Mani, B. Schiffman, and J. Zhang. 2003. Inferring
Temporal Ordering of Events in News. In Proceedings
of the Human Language Technology Conference (HLT-
NAACL?03).
Zso?fia Ruttkay. 1998. Constraint Satisfaction - a Survey.
Technical Report 11(2-3), CWI.
Roser Saur??, Jessica Littman, Bob Knippen, Robert
Gaizauskas, Andrea Setzer, and James Pustejovsky,
2006. TimeML Annotation Guidelines, Version 1.2.1,
January 31.
F. Schilder and C. Habel. 2001. From Temporal Expres-
sions To Temporal Information: Semantic Tagging Of
News Messages. In Proceedings of ACL-2001: Work-
shop on Temporal and Spatial Information Processing,
Toulouse, France, 7.
Andrea Setzer. 2001. Temporal Information in Newswire
Articles: an Annotation Scheme and Corpus Study.
Ph.D. thesis, University of Sheffield.
M. Stede and S. Haas. 1998. Understanding and track-
ing temporal descriptions in dialogue. In B. Schro?der,
W. Lenders, W. Hess, and T. Portele, editors, Proceed-
ings of the 4th Conference on Natural Language Pro-
cessing - KONVENS ?98.
N. Vazov. 2001. A System for Extraction of Tempo-
ral Expressions from French Texts Based on Syntac-
tic and Semantic Constraints. In Proceedings of ACL-
2001: Workshop on Temporal and Spatial Information
Processing, Toulouse, France, 7.
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren, and
K. J. McKeever. 1998. An Empirical Approach to
Temporal Reference Resolution. Journal of Artificial
Intelligence Research, 9:247?293.
143
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 5?8,
New York, June 2006. c?2006 Association for Computational Linguistics
The MILE Corpus for Less Commonly Taught Languages 
 
Alison Alvarez, Lori Levin, Robert 
Frederking, Simon Fung, Donna 
Gates 
Language Technologies Institute 
5000 Forbes Avenue 
Pittsburgh, PA 15213 
[nosila, lsl, ref+, 
sfung, dmg] 
@cs.cmu.edu  
Jeff Good 
Max Planck Institute for Evolutionary 
Anthropology 
Deutscher Platz 6 
04103 Leipzig 
good@eva.mpg.de 
 
 
Abstract 
This paper describes a small, struc-
tured English corpus that is 
designed for translation into Less 
Commonly Taught Languages 
(LCTLs), and a set of re-usable 
tools for creation of similar cor-
pora. 1  The corpus systematically 
explores meanings that are known to 
affect morphology or syntax in the 
world?s languages.  Each sentence 
is associated with a feature structure 
showing the elements of meaning 
that are represented in the sentence.   
The corpus is highly structured so 
that it can support machine learning 
with only a small amount of data.   
As part of the REFLEX program, 
the corpus will be translated into 
multiple LCTLs, resulting in paral-
lel corpora can be used for training 
of MT and other language technolo-
gies. Only the untranslated English 
corpus is described in this paper.  
 
1   Introduction 
 
Of the 6,000 living languages in the world 
only a handful have the necessary monolin-
gual or bilingual resources to build a 
working statistical or example-based ma-
chine translation system.  Currently, there 
                                                 
1 AVENUE/MILE is supported by the US Na-
tional Science Foundation NSF grant number 
IIS-0121-631 and the US Government?s 
REFLEX Program. 
are efforts to build language packs for Less 
Commonly Taught Languages (LCTLs).  
Each language pack includes parallel cor-
pora consisting of naturally occurring text 
translated from English into the LCTL or 
vice versa.  
This paper describes a small corpus 
that supplements naturally occurring text 
with highly systematic enumeration of 
meanings that are known to affect morphol-
ogy and syntax in the world?s languages.   
The supplemental corpus will enable the 
exploration of constructions that are sparse 
or obscured in natural data.  The corpus 
consists of 12,875 English sentences, total-
ing 76,202 word tokens.    
This paper describes the construc-
tion of the corpus, including tools and 
resources that can be used for the construc-
tion of similar corpora.   
 
2 Structure of the corpus 
 
| 247: John said "The woman is a teacher." 
| 248: John said the woman is not a teacher. 
| 249: John said "The woman is not a teacher." 
| 250: John asked if the woman is a teacher. 
| 251: John asked "Is the woman a teacher?" 
| 252: John asked if the woman is not a teacher. 
| ?
| 1488: Men are not baking cookies. 
| 1489: The women are baking cookies.
| ?
| 1537: The ladies' waiter brought appetizers. 
| 1538: The ladies' waiter will bring appetizers. 
Figure 1: A sampling of sentences from 
the complete elicitation corpus 
5
srcsent: Mary was not a doctor. 
context: Translate this as though it were spoken to a peer co-worker; 
 
((actor ((np-function fn-actor)(np-animacy anim-human)(np-biological-gender bio-gender-female) 
(np-general-type  proper-noun-type)(np-identifiability identifiable) 
 (np-specificity specific)?))     
(pred ((np-function fn-predicate-nominal)(np-animacy anim-human)(np-biological-gender bio-
gender-female) (np-general-type common-noun-type)(np-specificity specificity-neutral)?)) 
(c-v-lexical-aspect state)(c-copula-type copula-role)(c-secondary-type secondary-copula)(c-
solidarity solidarity-neutral) (c-power-relationship power-peer) (c-v-grammatical-aspect gram-
aspect-neutral)(c-v-absolute-tense past) (c-v-phase-aspect phase-aspect-neutral) (c-general-
type declarative-clause)(c-polarity polarity-negative)(c-my-causer-intentionality intentionality-
n/a)(c-comparison-type comparison-n/a)(c-relative-tense relative-n/a)(c-our-boundary boundary-
n/a)?) 
Figure 2:  An abridged feature structure, sentence and context field 
The MILE (Minor Language Elicitation) 
corpus is a highly structured set of English 
sentences.  Each sentence represents a 
meaning or combination of meanings that 
we want to elicit from a speaker of an 
LCTL.  For example, the corpus excerpts 
in Figure 1 explore quoted and non quoted 
sentential complements, embedded ques-
tions, negation, definiteness, biological 
gender, and possessive noun phrases.   
Underlying each sentence is a feature 
structure that serves to codify its meaning.  
Additionally, sentences are accompanied by 
a context field that provides information that 
may be present in the feature structure, but 
not inherent in the English sentence.  For 
example, in Figure 2, the feature structure 
specifies solidarity with the hearer and 
power relationship of the speaker and hearer, 
as evidenced by the features-value pairs (c-
solidarity solidarity-neutral) and (c-power-
relationship power-peer).  Because this is 
not an inherent part of English grammar, this 
aspect of meaning is conveyed in the context 
field.   
 
3 Building the Corpus 
 
Figure 3 shows the steps in creating the 
corpus.  Corpus creation is driven by a Fea-
ture Specification.  The Feature 
Specification defines features such as tense, 
person, and number, and values for each 
feature such past, present, future, remote 
past, recent past, for tense.  Additionally, 
the feature specification defines illegal com-
binations of features, such as the use of a 
singular number with an inclusive or exclu-
sive pronoun (We = you and me vs we = me 
and other people).  The inventory of fea-
tures and values is informed by typological 
studies of which elements of meaning are 
known to affect syntax and morphology in 
some of the world?s languages. The feature 
specification currently contains 42 features 
and 340 values and covers. In order to select 
the most relevant features we drew guidance 
from Comrie and Smith (1977) and Bouqui-
aux and Thomas (1992).  We also used the 
World Atlas of Language Structures 
(Haspelmath et al 2005) as a catalog of ex-
isting language features and their prevalence.  
In the process of corpus creation, feature 
structures are created before their corre-
sponding English sentences.   There are 
three reasons for this.  First, as mentioned 
above, the feature structure may contain 
elements of meaning that are not explicitly 
represented in the English sentence.  Sec-
ond, multiple elicitation languages can be 
generated from the same set of feature struc-
tures.  For example, when we elicit South 
American languages we use Spanish instead 
of English sentences.  Third, what we want 
to know about each LCTL is not how it 
translates the structural elements of English 
such as determiners and auxiliary verbs, but 
how it renders certain meanings such as 
6
List of semantic 
features and 
values 
The Corpus
Feature Maps:  which 
combinations of 
features and values 
are of interest 
Clause-
Level 
Noun-
Phrase
Tense &
Aspect Modality 
Feature Structure Sets
Feature 
Specification 
Reverse Annotated Feature Structure
Sets: add English sentences
Smaller   CorpusSampling
?
Figure 3: An overview of the elicitation corpus production process 
definiteness, tense, and modality, which are 
not in one-to-one correspondence with Eng-
lish words.    
Creation of feature structures takes place 
in two steps.  First, we define which com-
binations of features and values are of 
interest.  Then the feature structures are 
automatically created from the feature speci-
fication.    
Combinations of features are specified 
in Feature Maps (Figure 3).  These maps 
identify features that are known to interact 
syntactically or morphologically in some 
languages.  For example, tense in English 
is partially expressed using the auxiliary 
verb system.  An unrelated aspect of mean-
ing, whether a sentence is declarative or 
interrogative, interacts with the tense system 
in that it affects the word order of auxiliary 
verbs (He was running, Was he running), 
Thus there is an interaction of tense with 
interrogativity.   We use studies of lan-
guage typology to identify combinations of 
features that are known to interact.   
Feature Maps are written in a concise 
formalism that is automatically expanded 
into a set of feature structures.  For exam-
ple, we can formally specify that we want 
three values of tense combined with three 
values of person, and nine feature structures 
will be produced.  These are shown as Fea-
ture Structure Sets in Figure 3.   
 
 
4 Sentence Writing 
 
 As stated previously, our corpus 
consists of feature structures that have been 
human annotated with a sentence and con-
text field.  Our feature structures contain 
functional-typological information, but do 
not contain specific lexical items.  This 
means that our set of feature structures can 
be interpreted into any language using ap-
propriate word choices and used for 
elicitation.  Additionally, this leaves the 
human annotator with some freedom when 
selecting vocabulary items.  Due to feed-
back from previous elicitation subjects we 
chose basic vocabulary words while steering 
clear of overly primitive subject matter that 
may be seen as insulting.  Moreover, we 
did our best to avoid lexical gaps; for exam-
ple, many languages do not have a single 
word that means winner.   
7
 Translator accuracy was also an im-
portant objective and we took pains to 
construct natural sounding, unambiguous 
sentences.  The context field is used to 
clarify the sentence meaning and spell out 
features that may not manifest themselves in 
English. 
 
5 Tools 
 
 In conjunction with this project we 
created several tools that can be reused to 
make new corpora with other purposes. 
? An XML schema and XSLT can be used 
to make different feature specifications 
? A feature structure generator that can be 
used as a guide to specify and design 
feature maps 
? A feature structure browser can be used 
to make complicated feature structures 
easier to read and annotate 
 
6 Conclusion 
 
The basic steps for creating a func-
tional-typological corpus are: 
  
1. Combinations of features are selected 
2. Sets of feature structures representing all 
feature combinations are generated 
3. Humans write sentences with basic vo-
cabulary that represent the meaning in 
the feature structure 
4. If the corpus is too large, some or all of 
the corpus can be sampled 
 
We used sampling and assessments of 
the most crucial features in order to compile 
our corpus and restrict it to a size small 
enough to be translatable by humans.  As a 
result it is possible that this corpus will miss 
important feature combinations in some lan-
guages.  However, a corpus containing all 
possible combinations of features would 
produce hundreds of billions of feature 
structures.   
Our future research includes building a 
Corpus Navigation System to dynamically 
explore the full feature space.  Using ma-
chine learning we will use information de-
tected from translated sentences in order to 
decide what parts of the feature space are 
redundant and what parts must be explored 
and translated next. A further description of 
this process can be read in Levin et al 
(2006). 
Additionally, we will change from using 
humans to write sentences and context fields 
to having them generated by using a natural 
language generation system (Alvarez et al 
2005).   
We also ran small scale experiments to 
measure translator accuracy and consistency 
and encountered positive results. Hebrew 
and Japanese translators provided consistent, 
accurate translations.  Large scale experi-
ments will be conducted in the near future to 
see if the success of the smaller experiments 
will carry over to a larger scale. 
 
7 References 
 
Alvarez, Alison, and Lori Levin, Robert  
  Frederking, Jeff Good, Erik Peterson  
 September 2005, Semi-Automated Elicitation 
 Corpus Generation. In Proceedings of MT 
 Summit X, Phuket: Thailand. 
 
Bouquiaux, Luc and J.M.C. Thomas. 1992.  
Studying and Describing Unwritten Lan-
guages. Dallas, TX: The Summer Institute of 
Linguistcs. 
 
Comrie, Bernard and N. Smith. 1977.  
  Lingua descriptive series: Questionnaire. In:      
  Lingua, 42:1-72. 
 
Haspelmath, Martin and Matthew S. Dryer,     
  David Gil, Bernard Comrie, editors. 2005    
  World Atlas of Language Strucutures. Oxford  
  University Press. 
 
Lori Levin, Alison Alvarez, Jeff Good, and     
  Robert Frederking. 2006 "Automatic Learning    
  of Grammatical Encoding." To appear in Jane 
  Grimshaw, Joan Maling, Chris Manning, Joan    
  Simpson and Annie Zaenen (eds)  
  Architectures, Rules and Preferences: A  
  Festschrift for Joan Bresnan , CSLI Publica  
  tions.  In Press. 
 
8
Evaluation of a Practical Interlingua 
for Task-Oriented Dialogue 
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi, 
Dorcas Wallace, Taro Watanabe, Monika Woszczyna 
Language Technologies Inst i tute,  Carnegie Mellon Univers i ty  and 
IRST  ITC,  Trento, I taly 
Internet:  l s l?cs ,  cmu. edu 
Abstract 
IF (Interchange Format), the interlingua used by 
the C-STAR consortium, is a speech-act based in- 
terlingua for task-oriented ialogue. IF was de- 
signed as a practical interlingua that could strike 
a balance between expressivity and simplicity. If 
it is too simple, components of meaning will be 
lost and coverage of unseen data will be low. On 
the other hand, if it is too complex, it cannot be 
used with a high degree of consistency by collab- 
orators on different continents. In this paper, we 
suggest methods for evaluating the coverage of IF 
and the consistency with which it was used in the 
C-STAR consortium. 
Introduction 
IF (Interchange Format) is an interlingua used by 
the C-STAR consortium 1 for task-oriented ia- 
logues. Because it is used in five different coun- 
tries for six different languages, it had to achieve 
a careful balance between being expressive hough 
and being simple enough to be used consistently. 
If it was not expressive nough, components of 
meaning would be lost and coverage of unseen data 
would be low. On the other hand, if was not sim- 
ple enough, different system developers would use 
it inconsistently and the wrong meanings would be 
translated. IF is described in our previous papers 
(\[PT98, LGLW98, LLW+\]). 
For this paper, we have proposed methods for 
evaluating the coverage of IF and the degree to 
which it can be used consistently across C-STAR 
sites. Coverage was measured by having human IF 
specialists annotate unseen data. Consistency was 
measured by two means. The first was inter-coder 
agreement among IF specialists at Carnegie Mel- 
lonUniversity and ITC-irst (Centre per la ricerca 
lhttp://www.c-star.org 
18 
scientifica e tecnologica). The second, less direct 
method, was a cross-site nd-to-end evaluation of 
English-to-Italian translation where the English- 
to-IF analysis grammars were written at CMU and 
IF-to-Italian generation was developed at IRST. If 
the English and Italian grammar writers did not 
agree on the meaning of the IF, wrong transla- 
tions will be produced. In this way, the cross-site 
evaluation can be an indirect indicator of whether 
the CMU and IRST IF specialists agreed on the 
meaning of IF representations. For comparison, 
we also present within-site nd-to-end evaluations 
of English-to-German, English-to-Japanese, and 
English-to-IF-to-English, where all of the analysis 
and generation grammars were written at CMU. 
The  In terchange Format  
Because we are working with task-oriented dia- 
logues, adequate rendering of the speech act in the 
target language often overshadows the need for lit- 
eral translation of the words. IF is therefore based 
on domain actions (DAs), which consist of on 
speech acts plus domain-specific concepts. An ex- 
ample of a DA is give-information+price+room 
(giving information about the price of a room). 
DAs are composed from 45 general speech acts 
(e.g., acknowledge, give- information, accept) 
and about 96 domain-specific oncepts (e.g, 
pr ice,  temporal, room, f l ight ,  ava i lab i l i ty ) .  
In addition to the DA, IF representations can con- 
tain arguments such as room-type, dest inat ion,  
and price. There are about 119 argument types. 
In the following example, the DA consists 
of a speaker tag (a: for agent), the speech- 
act give- information,  and two main concepts, 
+price and +room. The DA is followed by a list 
of arguments: room-type= and price=. The ar- 
guments have values that represent-information 
for the type of room double and the cost repre- 
Percent 
Cumulatlve Percent Count 
Coverage 
15.7 15,7 652 
19.8 4.1 172 
28.3 3.4 143 
26.0 2.7 113 
28.0 2.0 85 
30.1 2.0 85 
31,9 1.9 78 
33.7 1.8 75 
35.5 1.8 73 
37.2 1.7 70 
38.8 1.6 66 
40.3 l .S 64 
41.7 1,4 60 
43.2 1.4 60 
44.5 1.3 56 
45.8 1.3 52 
46.9 1.2 48 
48.0 1.1 46 
49.1 1.1 44 
50.1 1.0 42 
NA* ;:; 244 
DA 
acknowledge 
aff i rm 
thank 
introduce-self 
give-lnformation+prtce 
greeting 
give-lnfor marion+tern poral 
give-lnformatlon+numeral 
give-in formation+ pr ice+room 
request-in for matio n+ payment 
give-information + payment 
g ive- inform+features+room 
give-in form -t- availabil ity + room 
accept 
give-information+personal-data 
req-act +reserv+ feat ures+room 
req- verif-give-inforra +numera l  
offer+help 
apologize 
request-inform+personal-data 
no-tag 
Figure 1: Coverage of Top20 DAs and No-tag in 
development data 
sented with the complex argument price= which 
has its own arguments quantity=, currency= and 
per-unit=. This IF representation is neutral be- 
tween sentences that have different verbs, sub- 
jects, and objects uch as A double room costs 150 
dollars a night, The price of  a double room is 150 
dollars a night, and A double room is 150 dollars 
a night. ~ 
AGENT: ''a double room costs $150 a night.'' 
a:give-information+price+room 
( room-type=doub le ,  
price=(quantity=lSO, 
currency=dollar, 
per-unit=night) 
Coverage and D is t r ibut ion  of  
Dia logue  Acts  
In this section, we address the coverage of IF for 
task-oriented dialogues about ravel planning. We 
want to know whether a very simple interlingua 
like IF can have good coverage. We are using a 
rather subjective measure of coverage: IF experts 
hand-tagged unseen data with IF representations 
and counted the percentage ofutterances towhich 
no IF could be assigned. (When they tagged the 
unseen data, they were not told that the IF was 
being tested for coverage. The tagging was done 
for system development purposes.) Our end-to- 
end evaluation described in the following sections 
can be taken as a less subjective measure of cov- 
2When we add anaphora resolution, we will need 
to know whether a verb (cost) or a noun (price) was 
used. This will be an issue our new project, NESPOLEI 
(http://nespole. itc. it/). 
Percent 
Cumulative Percent Count Speech Act 
Coverage 
30.1 80.1 1250 glve-lnformation 
45,8 15.7 655 acknowledge 
57,7 11.9 498 request- lnformation 
62,7 5,0 209 request-verif ication-give-inform 
87.6 4.9 203 request-actlon 
71.7 4.1 172 affirm 
75,1 3.4 143 thank 
77,9 2.7 113 introduce-self 
80.2 2.4 98 offer 
82,4 2.1 89 accept 
84.4 2.0 85 greeting 
85.7 1.3 55 suggest 
66.8 I . I  44 apologize 
87.8 1.0 41 closing 
88.5 0.8 32 negate.give-information 
89.2 0.6 27 delay-action 
89,8 0.6 25 introduce-topic 
90,2 0.5 19 please-wait 
90.6 0.4 15 reject 
91.0 0.4 15 request-suggestlon 
Figure 2: 
data 
Coverage of speech-acts in development 
erage. However, the score of an end-to-end evalu- 
ation encompasses grammar coverage problems as 
well as IF coverage problems. 
The development portion of the coverage x- 
periment proceeded as follows. Over a period of 
two years, a database of travel planning dialogues 
was collected by C-STAR partners in the U.S., 
Italy, and Korea. The dialogues were role-playing 
dialogues between a person pretending to be a 
traveller and a person pretending to be a travel 
agent. For the English and Italian dialogues, the 
traveller and agent were talking face-to-face in the 
same language - -  both speaking English or both 
speaking Italian. The Korean dialogues were also 
role playing dialogues, but one participant was 
speaking Korean and the other was speaking En- 
glish. From these dialogues, only the Korean ut- 
terances are included in the database. Each utter- 
ance in the database is annotated with an English 
translation and an IF representation. Table 1 sum- 
marizes the amount of data in each language. The 
English, Italian, and Korean data was used for IF 
development. 
The development database contains over 4000 
dialogue act units, which are covered by a total of 
about 542 distinct DAs (346 agent DAs and 278 
client DAs). Figures 1 and 2 show the cumulative 
coverage of the top twenty DA's and speech acts 
in the development data. Figure 1 also shows the 
percentage ofno-tag utterances (the ones we de- 
cided not to cover) in the development data. The 
first column shows the percent of the development 
data that is covered cumulatively by the DA's or 
speech acts from the top of the table to the cur- 
rent line. For example, acknowledg e and aff irm 
together account for 19.8 percent of the data. The 
19 
Language(s) Type  of Dialogue Number  of DA Units  
D'evelopment Data: 
English 
Italian 
Korean-English 
Test Data: 
Japanese-English 
monolingual 
monolingual 
biiingual (only 'Korean 
utterances are included) 
bilingual (Japanese and 
English utterances are 
included) 
Table 1: The IF Database 
2698 
1142 
6069 
Percent 
' Cumulat ive Percent Count DA 
Cover~,--= - " 4.6 263 no-tag 
15.6 15.6 ? - 885 acknowledge 
20.2 4,6 260 thank 
23.7 3.5 200 introduce-self 
27.0 3.4 191 affirm 
29.7 2.7 153 apologize 
32.3 2.6 147 greeting 
34.6 2.3 128 closing 
36.3 1.7 98 give- information+personal-data 
38.0 1.7 95 glve-inform ation +t  em poraI 
39.5 1.6 89 give-in formation +price 
41.1 1.5 88 please-wait 
42.5 1.4 82 give-inform+telephone-number 
43.8 1.3 75 g ive- informat ion+features+room 
45.0 I . I  65 request- inform+personal-data 
46.0 1.0 59 give-in for m ?temp oral-.{- arrival 
47.0 1.0 55 accept 
48.0 l.O 55 give-infor m +avai labi l i ty + room 
48.9 1.0 55 give-information+price-broom 
49.8 0.9 50 verify 
50.7 0.9 49 request-in form +tempora l+arr iva l  
Figure 3: Coverage of Top 20 DAs and No-tag in 
test data 
Percent 
Cumulat ive 
Coverage 
25.6 
Percent Count DA 
25.6 1454 give-information 
41.7 16.1 916 acknowledge 
53.6 11.9 677 request- information 
58.2 4.6 260 thank 
62,0 3.7 213 request-verification-give-inform 
65.5 3.5 200 introduce-self 
68.8 3.4 191 a f f i rm 
72.0 3.2 181 request -act ion  
74.8 2.8 159 accept 
77.5 2.7 153 apologize 
80.1 2.6 147 greet ing 
82.4 2.3 130 closing 
84.4 2.1 117 suggest 
86.3 1.8 104 verlfy-give-information 
87.9 1.7 94 offer 
89.5 1.5 88 please-wait 
90.6 I . I  65 negate-glve-lnformation 
91.5 0.9 50 verify 
92.0 0.5 30 negate 
92.5 0.5 . 26 request-aff irmatlon 
Figure 4: Coverage of Top 20 SAs in test data 
second column shows the percent of the develop- 
ment data covered by each DA or speech act. The 
third column shows the number of times each DA 
or speech act occurs in the development data. 
The evaluation portion of the coverage x- 
periment was carried out on 124 dialogues (6069 
dialogue act units) that were collected at ATR, 
Japan. One participant in each dialogue was 
speaking Japanese and the other was speaking En- 
glish. Both Japanese and English utterances are 
included in the data. The 124 Japanese-English 
dialogues were not examined closely by system de- 
velopers during IF development. After the IF de- 
sign was finalized and frozen in Summer 1999, the 
Japanese-English data was tagged with IFs. No 
further IF development took place at this point 
except hat values for arguments were added. For 
example, Miyako could be added as a hotel name, 
but no new speech acts, concepts, or argument 
types could be added. Sentences were tagged as 
no-tag if the IF did not cover them. 
Figures 3 and 4 show the cumulative cover- 
age of the top twenty DAs and speech acts in the 
Japanese-English data, including the percent of 
no-tag sentences. 
Notice that the percentage of no-tag was 
lower in our test data than in our development 
data. This is because the role playing instructions 
for the test data were more restrictive than the 
role playing instructions for the development data. 
Figures 1 and 3 show that slightly more of the test 
data is covered by slightly fewer DAs. 
Cross-Site Reliability of IF 
Representations 
In this section we attempt o measure how reliably 
IF is used by researchers at different sites. Recall 
that one of the design criteria of IF was consis- 
tency of use by researchers who are separated by 
oceans. This criterion limits the complexity of IF. 
Two measures of consistency are used - inter-coder 
agreement and a cross-site nd-to-end evaluation. 
Inter -Coder  Agreement:  Inter-coder agree- 
ment is a direct measure of consistency among 
20 
Percent Agreement 
Speech-act 82.14 
Dialog-act 65.48 
Concept lists 88.00 
Argument lists I 85.79 
Table 2: Inter-coder Agreement between CMU 
and IRST 
C-STAR partners. We used 84 DA units from 
the Japanese-English data described above. The 
84 DA units consisted of some coherent dialogue 
fragments and and some isolated sentences. The 
data was coded at CMU and at IRST. We counted 
agreement on ~he components ofthe IF separately. 
Table 2 shows agreement on speech acts, dialogue 
acts (speech act plus concepts), concepts, and ar- 
guments. The results are reported in Table 2 in 
terms of percent agreement. Further work might 
include some other calculation of agreement such 
as Kappa or precision and recall of the coders 
against each other. Figure 5 shows a fragment of 
a dialogue coded by CMU and IRST. The coders 
disagreed on the IF middle sentence, I'd like a twin 
room please. One coded it as an acceptance of a 
twin room, the other coded it as a preference for 
a twin room. 
Cross-Site Evaluation: As an approximate and 
indirect measure of consistency, we have compared 
intra-site end-to-end evaluation with cross-site 
end-to-end evaluation. An end-to-end evaluation 
includes an analyzer, which maps the source lan- 
guage input into IF and a generator, which maps 
IF into target language sentences. The intra-site 
evaluation was carried out on English-German, 
English-Japanese, and English-IF-English trans- 
lation. The English analyzer and the German, 
Japanese, and English generators were all writ- 
ten at CMU by IF experts who worked closely 
with each other. The cross-site valuation was car- 
ried out on English-Italian translation, involving 
an English analyzer written at CMU and an Ital- 
ian generator written at IRST. The IF experts at 
CMU and IRST were in occasional contact with 
each other by email, and met in person two or 
three times between 1997 and 1999. 
A number of factors contribute to the success 
of an inter-site valuation, just one of which is that 
the sites used IF consistently with each other. An- 
other factor is that the two sites used similar de- 
velopment data and have approximately the same 
coverage. If the inter-site valuation results are 
about as good as the intra-site results, we can con- 
clude that all factors are handled acceptably, in- 
cluding consistency of IF usage. If the inter-site 
results are worse than the intra-site results, con- 
sistency of IF use or some other factor may be 
to blame. Before conducting this evaluation, we 
already knew that there was some degree of cross- 
site consistency in IF usage because we conducted 
successful inter-continental demos with speech 
translation and video conferencing in Summer 
1999. (The demos and some of the press coverage 
are reported on the C-STAR web site.) The de- 
mos included ialogues in English-Italian, English- 
German, English-Japanese, English-Korean, and 
English-French. At a later date, an Italian-Korean 
demo was produced with no additional work, thus 
illustrating the well-cited advantage of an inter- 
lingual approach in a multi-lingual situation. The 
end-to-end evaluation reported here goes beyond 
the demo situation to include data that was un- 
seen by system developers. 
Evaluation Data: The Summer 1999 intra-site 
evaluation was conducted on about 130 utterances 
from a CMU user study. The traveller was played 
by a second time user - -  someone who had partici- 
pated in one previous user study, but had no other 
experience with our MT system. The travel agent 
was played by a system developer. Both people 
were speaking English, but they were in different 
rooms, and their utterances were paraphrased us- 
ing IF. The end-to-end procedure was that (1) an 
English utterance was spoken and decoded by the 
JANUS speech recognizer, (2) the output of the rec- 
ognizer was parsed into an IF representation, and 
(3) a different English utterance (supposedly with 
the same meaning) was generated from the IF rep- 
resentation. The speakers had no other means of 
communication with each other. 
In order to evaluate English-German and 
English-Japanese translation, the IFs of the 130 
test sentences were fed into German and Japanese 
generation components atCMU. The data used in 
the evaluation was unseen by system developers 
at the time of the evaluation. For English-Italian 
translation, the IF representations produced by 
the English analysis component were sent to IRST 
to be generated in Italian. 
Evaluation Scoring: In order to score the eval- 
uation, input and output sentences were compared 
by bilingual people, or monolingual people in the 
case of English-IF-English evaluation. A score of 
ok is assigned if the target language utterance is
comprehensible and no components ofmeaning are 
deleted, added, or" changed by the translation. A
21 
We have singles, and t,ins and also Japanese rooms available on the eleventh. 
CMU a:give-information+availability+room 
(room-type=(single ~ twin ~ japanese_style), time=mdll) 
IRST a:give-in2ormation+availability+room 
(room-type=(single ~ twin & japanese_style), time=mdll) 
I'd like a twin room, please. 
CMU c:accept+features+room (room-typeffitwin) 
IBST c:give-information+preference+features+room (room-type=twin) 
A twin room is fourteen thousand yen. 
CMU a:give-information+price+room 
(room-type=twin, price=(currency=yen, quantity=f4000)) 
IRST a:give-in.formation+price+room 
(room-type=twin, price=(currency=yen, quantity=f4000)) 
Figure 5: Examples of IF coding from CMU and IRST 
.o  
Method 
1 Recosnition only 
2 Transcription 
3 Recosnition 
4 Transcription 
5 Recognition 
6 Transcription 
7 Recognition 
8 Transcription 
9 Recognition 
10 Transcription 
11 Recognition 
I OutPut Language II OK+Perfect Perfect Grader I No. of Graders 
En$1ish 
English. 
En$1ish 
Japanese 
Japanese 
German 
German 
German 
German 
Italian 
Italian 
78 % 62 % CMU 3 
74 % 54 % CMU 3 
59 ~ 42 % CMU 3 
777 % 59 % CMU 2 
62 % 4,5 % CMU 2 
70 %- ..... s9 % CMU " 
58 % 34 % CMU 2 
67 ~ 43 % IRST 2 
59 % 36 % IRST 2 
73 % 51% IRST .... . .  6 
61% 42 % IRST 6 
Figure 6: Translation Grades for English to English, Japanese, German, and Italian 
score of perfect is assigned if, in addition to the 
previous criteria, the translation is fluent in the 
target language. A score of bad is assigned if the 
target language sentence is incomprehensible or 
some element of meaning has been added, deleted, 
or changed. The evaluation procedure isdescribed 
in detail in \[GLL+96\]. In Figure 6, acceptable is
the sum of per fec t  and ok scores, s 
Figure 6 shows the results of the intra-site 
and inter-site evaluations. The first row grades 
the speech recognition output against a human- 
produced transcript of what was said. This gives 
us a ceiling for how well we could do if trans- 
lation were perfect, given speech recognition er- 
rors. Rows 2 through 7 show the results of the 
intra-site evaluation. All analyzers and genera- 
tors were written at CMU, and the results were 
graded by CMU researchers. (The German re- 
sults are a lower than the English and Japanese 
results because a shorter time was spent on gram- 
mar development.) Rows 8 and 9 report on CMU's 
intra~site valuation of English-German transla~ 
Sin another paper (\[LBL+00\]), we describe a task- 
based evaluation which focuses on success of commu- 
nicative goals and how long it takes to achieve them. 
tion (the same system as in Rows 6 and 7), but 
the results were graded by researchers at IRST. 
Comparing Rows 6 and 7 with Rows 8 and 9, we 
can check that CMU and IRST graders were us- 
ing roughly the same grading criteria: a difference 
of up to ten percent among graders is normal in 
our experience. Rows 10 and 11 show the results 
of the inter-site English-Italian evaluation. The 
CMU English analyzer produced IF representa- 
tions which were sent to IRST and were fed into 
IRST's Italian generator. The results were graded 
by IRST researchers. 
Conclusions drawn from the inter-site valuation: 
Since the inter-site evaluation results are compa- 
rable to the intra-site results, we conclude that re- 
searchers at IRST and CMU are using IF at least 
as consistently as researchers within CMU. 
Future Plans 
In the next phase of C-STAR, we will cover de- 
scriptive sentences (e.g., The castle was built in 
the thirteenth century and someone was impris- 
oned in the tower) as well as task-oriented sen- 
tences. Descriptive sentences will be represented 
22 
in a more traditional frame-based interlingua fo- 
cusing on lexical meaning and grammatical fea- 
tures of the sentences. We are working on disam- 
biguating literal from task-oriented meanings in 
context. For example That's great could be an ac- 
ceptance (like I'll take it) (task oriented) or could 
just express appreciation. Sentences may also con- 
tain a combination of task oriented (e.g., Can you 
tell me) and descriptive (how long the castle has 
been standing) components. 
\[GLL+96\] 
\[LBL+O0\] 
\[LGLW98\] 
Re ferences  
Donna Gates, A. Lavie, L. Levin, 
A. Waibel, M. Gavald~, L. Mayfield, 
M:-Woszczyna, and P. Zhan. End-to- 
End Evaluation in JANUS: A Speech- 
to-Speech Translation System. In Pro- 
ceedings of ECAI-96, Budapest, Hun- 
gary, 1996. 
Lori Levin, Boris Bartlog, Ari- 
adna Font Llitjos, Donna Gates, Alon 
Lavie, Dorcas Wallace, Taro Watan- 
abe, and Monika Woszczyna. Lessons 
Learned from a Task-Based Evaluation 
of Speech-to-Speech MT. In Proceed- 
ings of LREC 2000, Athens, Greece, 
June to appear, 2000. 
Lori Levin, D. Gates, A. Lavie, and 
A. Waibel. An Interlingua Based on 
Domain Actions for Machine Transla- 
tion of Task-Oriented Dialogues. In 
Proceedings of the International Con- 
ference on Spoken Language Process- 
ing (ICSLP'98), Sydney, Australia, 
1998. 
\[LLW +\] 
\[PT98\] 
Lori Levin, A. Lavie, M. Woszczyna, 
D. Gates, M. Gavald~, D. Koll, and 
A. Waibel. The Janus-III Translation 
System. Machine Translation. To ap- 
pear. 
Fabio Pianesi and Lucia Tovena. Us- 
ing the Interchange Format for Encod- 
ing Spoken Dialogue. In Proceedings of
SIG-IL Workshop, 1998. 
23 
Spoken Language Parsing Using Phrase-Level Grammars and Trainable 
Classifiers 
Chad Langley, Alon Lavie, Lori Levin, Dorcas Wallace, Donna Gates, and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, USA 
{clangley|alavie|lsl|dorcas|dmg|kay}@cs.cmu.edu 
 
Abstract 
In this paper, we describe a novel 
approach to spoken language analysis 
for translation, which uses a combination 
of grammar-based phrase-level parsing 
and automatic classification. The job of 
the analyzer is to produce a shallow 
semantic interlingua representation for 
spoken task-oriented utterances. The 
goal of our hybrid approach is to provide 
accurate real-time analyses while 
improving robustness and portability to 
new domains and languages. 
1 Introduction 
Interlingua-based approaches to Machine 
Translation (MT) are highly attractive in systems 
that support a large number of languages. For each 
source language, an analyzer that converts the 
source language into the interlingua is required. 
For each target language, a generator that converts 
the interlingua into the target language is needed. 
Given analyzers and generators for all supported 
languages, the system simply connects the source 
language analyzer with the target language 
generator to perform translation. 
Robust and accurate analysis is critical in 
interlingua-based translation systems. In speech-to-
speech translation systems, the analyzer must be 
robust to speech recognition errors, spontaneous 
speech, and ungrammatical inputs as described by 
Lavie (1996). Furthermore, the analyzer should run 
in (near) real time. 
In addition to accuracy, speed, and robustness, 
the portability of the analyzer with respect to new 
domains and new languages is an important 
consideration. Despite continuing improvements in 
speech recognition and translation technologies, 
restricted domains of coverage are still necessary 
in order to achieve reasonably accurate machine 
translation. Porting translation systems to new 
domains or even expanding the coverage in an 
existing domain can be very difficult and time-
consuming.  This creates significant challenges in 
situations where translation is needed for a new 
domain within relatively short notice. Likewise, 
demand can be high for translation systems that 
can be rapidly expanded to include new languages 
that were not previously considered important. 
Thus, it is important that the analysis approach 
used in a translation system be portable to new 
domains and languages. 
One approach to analysis in restricted domains 
is to use semantic grammars, which focus on 
parsing semantic concepts rather than syntactic 
structure. Semantic grammars can be especially 
useful for parsing spoken language because they 
are less susceptible to syntactic deviations caused 
by spontaneous speech effects. However, the focus 
on meaning rather than syntactic structure 
generally makes porting to a new domain quite 
difficult. Since semantic grammars do not exploit 
syntactic similarities across domains, completely 
new grammars must usually be developed. 
While grammar-based parsing can provide very 
accurate analyses on development data, it is 
difficult for a grammar to completely cover a 
domain, a problem that is exacerbated by spoken 
input. Furthermore, it generally takes a great deal 
of effort by human experts to develop a high-
coverage grammar. On the other hand, machine 
learning approaches can generalize beyond training 
data and tend to degrade gracefully in the face of 
noisy input. Machine learning methods may, 
however, be less accurate on clearly in-domain 
input than grammars and may require a large 
amount of training data. 
We describe a prototype version of an analyzer 
that combines phrase-level parsing and machine 
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 15-22.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
learning techniques to take advantage of the 
benefits of each. Phrase-level semantic grammars 
and a robust parser are used to extract low-level 
interlingua arguments from an utterance. Then, 
automatic classifiers assign high-level domain 
actions to semantic segments in the utterance. 
2 MT System Overview 
The analyzer we describe is used for English and 
German in several multilingual human-to-human 
speech-to-speech translation systems, including the 
NESPOLE! system (Lavie et al, 2002). The goal 
of NESPOLE! is to provide translation for 
common users within real-world e-commerce 
applications. The system currently provides 
translation in the travel and tourism domain 
between English, French, German and Italian.  
NESPOLE! employs an interlingua-based 
translation approach that uses four basic steps to 
perform translation. First, an automatic speech 
recognizer processes spoken input. The best-
ranked hypothesis from speech recognition is then 
passed through the analyzer to produce interlingua. 
Target language text is then generated from the 
interlingua. Finally, the target language text is 
synthesized into speech. 
This interlingua-based translation approach 
allows for distributed development of the 
components for each language. The components 
for each language are assembled into a translation 
server that accepts speech, text, or interlingua as 
input and produces interlingua, text, and 
synthesized speech. In addition to the analyzer 
described here, the English translation server uses 
the JANUS Recognition Toolkit for speech 
recognition, the GenKit system (Tomita & Nyberg, 
1988) for generation, and the Festival system 
(Black et al, 1999) for synthesis. 
NESPOLE! uses a client-server architecture 
(Lavie et al, 2001) to enable users who are 
browsing the web pages of a service provider (e.g. 
a tourism bureau) to seamlessly connect to a 
human agent who speaks a different language. 
Using commercially available software such as 
Microsoft NetMeeting?, a user is connected to the 
NESPOLE! Mediator, which establishes 
connections with the agent and with translation 
servers for the appropriate languages. During a 
dialogue, the Mediator transmits spoken input from 
the users to the translation servers and synthesized 
translations from the servers to the users. 
3 The Interlingua 
The interlingua used in the NESPOLE! system is 
called Interchange Format (IF) (Levin et al, 1998; 
Levin et al, 2000). The IF defines a shallow 
semantic representation for task-oriented 
utterances that abstracts away from language-
specific syntax and idiosyncrasies while capturing 
the meaning of the input. Each utterance is divided 
into semantic segments called semantic dialog 
units (SDUs), and an IF is assigned to each SDU. 
An IF representation consists of four parts: a 
speaker tag, a speech act, an optional sequence of 
concepts, and an optional set of arguments. The 
representation takes the following form: 
 
speaker : speech act +concept* (argument*) 
 
The speaker tag indicates the role of the speaker 
in the dialogue. The speech act captures the 
speaker?s intention. The concept sequence, which 
may contain zero or more concepts, captures the 
focus of an SDU. The speech act and concept 
sequence are collectively referred to as the domain 
action (DA). The arguments use a feature-value 
representation to encode specific information from 
the utterance. Argument values can be atomic or 
complex. The IF specification defines all of the 
components and describes how they can be legally 
combined. Several examples of utterances with 
corresponding IFs are shown below. 
 
Thank you very much. 
a:thank 
Hello. 
c:greeting (greeting=hello) 
How far in advance do I need to book a room for the Al-
Cervo Hotel? 
c:request-suggestion+reservation+room ( 
   suggest-strength=strong, 
   time=(time-relation=before, 
     time-distance=question), 
   who=i, 
   room-spec=(room, identifiability=no, 
     location=(object-name=cervo_hotel))) 
4 The Hybrid Analysis Approach 
Our hybrid analysis approach uses a combination 
of grammar-based parsing and machine learning 
techniques to transform spoken utterances into the 
IF representation described above. The speaker tag 
is assumed to be given. Thus, the goal of the 
analyzer is to identify the DA and arguments.  
The hybrid analyzer operates in three stages. 
First, semantic grammars are used to parse an 
utterance into a sequence of arguments. Next, the 
utterance is segmented into SDUs. Finally, the DA 
is identified using automatic classifiers. 
4.1 Argument Parsing 
The first stage in analysis is parsing an utterance 
for arguments. During this stage, utterances are 
parsed with phrase-level semantic grammars using 
the robust SOUP parser (Gavald?, 2000). 
4.1.1 The Parser 
The SOUP parser is a stochastic, chart-based, top-
down parser that is designed to provide real-time 
analysis of spoken language using context-free 
semantic grammars. One important feature 
provided by SOUP is word skipping. The amount 
of skipping allowed is configurable and a list of 
unskippable words can be defined. Another feature 
that is critical for phrase-level argument parsing is 
the ability to produce analyses consisting of 
multiple parse trees. SOUP also supports modular 
grammar development (Woszczyna et al, 1998). 
Subgrammars designed for different domains or 
purposes can be developed independently and 
applied in parallel during parsing. Parse tree nodes 
are then marked with a subgrammar label. When 
an input can be parsed in multiple ways, SOUP can 
provide a ranked list of interpretations. 
In the prototype analyzer, word skipping is only 
allowed between parse trees. Only the best-ranked 
argument parse is used for further processing. 
4.1.2 The Grammars 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument 
grammar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains phrase-
level rules for parsing arguments defined in the IF. 
Top-level argument grammar nonterminals 
correspond to top-level arguments in the IF. 
The pseudo-argument grammar contains top-
level nonterminals that do not correspond to 
interlingua concepts. These rules are used for 
parsing common phrases that can be grouped into 
classes to capture more useful information for the 
classifiers. For example, all booked up, full, and 
sold out might be grouped into a class of phrases 
that indicate unavailability. In addition, rules in the 
pseudo-argument grammar can be used for 
contextual anchoring of ambiguous arguments. For 
example, the arguments [who=] and [to-whom=] 
have the same values. To parse these arguments 
properly in a sentence like ?Can you send me the 
brochure??, we use a pseudo-argument grammar 
rule, which refers to the arguments [who=] and [to-
whom=] within the appropriate context.  
The cross-domain grammar contains rules for 
parsing whole DAs that are domain-independent. 
For example, this grammar contains rules for 
greetings (Hello, Good bye, Nice to meet you, etc.). 
Cross-domain grammar rules do not cover all 
possible domain-independent DAs. Instead, the 
rules focus on DAs with simple or no argument 
lists. Domain-independent DAs with complex 
argument lists are left to the classifiers. Cross-
domain rules play an important role in the 
prediction of SDU boundaries. 
Finally, the shared grammar contains common 
grammar rules that can be used by all other 
subgrammars. These include definitions for most 
of the arguments, since many can also appear as 
sub-arguments. RHSs in the argument grammar 
contain mostly references to rules in the shared 
grammar. This method eliminates redundant rules 
in the argument and shared grammars and allows 
for more accurate grammar maintenance. 
4.2 Segmentation 
The second stage of processing in the hybrid 
analysis approach is segmentation of the input into 
SDUs. The IF representation assigns DAs at the 
SDU level. However, since dialogue utterances 
often consist of multiple SDUs, utterances must be 
segmented into SDUs before DAs can be assigned. 
Figure 1 shows an example utterance containing 
four arguments segmented into two SDUs. 
 
SDU1  SDU2  
greeting= disposition= visit-spec= location= 
hello i would like to take a vacation in val di fiemme 
Figure 1. Segmentation of an utterance into SDUs. 
The argument parse may contain trees for cross-
domain DAs, which by definition cover a complete 
SDU. Thus, there must be an SDU boundary on 
both sides of a cross-domain tree. Additionally, no 
SDU boundaries are allowed within parse trees. 
The prototype analyzer drops words skipped 
between parse trees, leaving only a sequence of 
trees. The parse trees on each side of a potential 
boundary are examined, and if either tree was 
constructed by the cross-domain grammar, an SDU 
boundary is inserted. Otherwise, a simple statistical 
model similar to the one described by Lavie et al 
(1997) estimates the likelihood of a boundary. 
The statistical model is based only on the root 
labels of the parse trees immediately preceding and 
following the potential boundary position. Suppose 
the position under consideration looks like 
[A1?A2], where there may be a boundary between 
arguments A1 and A2. The likelihood of an SDU 
boundary is estimated using the following formula: 
 
])C([A  ])C([A
])AC([  ])C([A])AF([A
21
21
21
+
?+?
??  
 
The counts C([A1?]), C([?A2]), C([A1]), C([A2]) 
are computed from the training data. An evaluation 
of this baseline model is presented in section 6.  
4.3 DA Classification 
The third stage of analysis is the identification of 
the DA for each SDU using automatic classifiers. 
After segmentation, a cross-domain parse tree may 
cover an SDU. In this case, analysis is complete 
since the parse tree contains the DA. Otherwise, 
automatic classifiers are used to assign the DA. In 
the prototype analyzer, the DA classification task 
is split into separate subtasks of classifying the 
speech act and concept sequence. This reduces the 
complexity of each subtask and allows for the 
application of specialized techniques to identify 
each component. 
One classifier is used to identify the speech act, 
and a second classifier identifies the concept 
sequence. Both classifiers are implemented using 
TiMBL (Daelemans et al, 2000), a memory-based 
learner. Speech act classification is performed first. 
Input to the speech act classifier is a set of binary 
features that indicate whether each of the possible 
argument and pseudo-argument labels is present in 
the argument parse for the SDU. No other features 
are currently used. Concept sequence classification 
is performed after speech act classification. The 
concept sequence classifier uses the same feature 
set as the speech act classifier with one additional 
feature: the speech act assigned by the speech act 
classifier. We present an evaluation of this baseline 
DA classification scheme in section 6. 
4.4 Using the IF Specification 
The IF specification imposes constraints on how 
elements of the IF representation can legally 
combine. DA classification can be augmented with 
knowledge of constraints from the IF specification, 
providing two advantages over otherwise na?ve 
classification. First, the analyzer must produce 
valid IF representations in order to be useful in a 
translation system. Second, using knowledge from 
the IF specification can improve the quality of the 
IF produced, and thus the translation. 
Two elements of the IF specification are 
especially relevant to DA classification. First, the 
specification defines constraints on the 
composition of DAs. There are constraints on how 
concepts are allowed to pair with speech acts as 
well as ordering constraints on how concepts are 
allowed to combine to form a valid concept 
sequence. These constraints can be used to 
eliminate illegal DAs during classification. The 
second important element of the IF specification is 
the definition of how arguments are licensed by 
speech acts and concepts. In order for an IF to be 
valid, at least one speech act or concept in the DA 
must license each argument. 
The prototype analyzer uses the IF specification 
to aid classification and guarantee that a valid IF 
representation is produced. The speech act and 
concept sequence classifiers each provide a ranked 
list of possible classifications. When the best 
speech act and concept sequence combine to form 
an illegal DA or form a legal DA that does not 
license all of the arguments, the analyzer attempts 
to find the next best legal DA that licenses the 
most arguments. Each of the alternative concept 
sequences (in ranked order) is combined with each 
of the alternative speech acts (in ranked order). For 
each possible legal DA, the analyzer checks if all 
of the arguments found during parsing are licensed. 
If a legal DA is found that licenses all of the 
arguments, then the process stops. If not, one 
additional fallback strategy is used. The analyzer 
then tries to combine the best classified speech act 
with each of the concept sequences that occurred in 
the training data, sorted by their frequency of 
occurrence. Again, the analyzer checks if each 
legal DA licenses all of the arguments and stops if 
such a DA is found. If this step fails to produce a 
legal DA that licenses all of the arguments, the 
best-ranked DA that licenses the most arguments is 
returned. In this case, any arguments that are not 
licensed by the selected DA are removed. This 
approach is used because it is generally better to 
select an alternative DA and retain more arguments 
than to keep the best DA and lose the information 
represented by the arguments. An evaluation of 
this strategy is presented in the section 6. 
5 Grammar Development and 
Classifier Training 
During grammar development, it is generally 
useful to see how changes to the grammar affect 
the IF representations produced by the analyzer. In 
a purely grammar-based analysis approach, full 
interlingua representations are produced as the 
result of parsing, so testing new grammars simply 
requires loading them into the parser. Because the 
grammars used in our hybrid approach parse at the 
argument level, testing grammar modifications at 
the complete IF level requires retraining the 
segmentation model and the DA classifiers. 
 When new grammars are ready for testing, 
utterance-IF pairs for the appropriate language are 
extracted from the training database. Each 
utterance-IF pair in the training data consists of a 
single SDU with a manually annotated IF. Using 
the new grammars, the argument parser is applied 
to each utterance to produce an argument parse. 
The counts used by the segmentation model are 
then recomputed based on the new argument 
parses. Since each utterance contains a single 
SDU, the counts C([?A2]) and C([A1?]) can be 
computed directly from the first and last arguments 
in the parse respectively. 
Next, the training examples for the DA 
classifiers are constructed. Each training example 
for the speech act classifier consists of the speech 
act from the annotated IF and a vector of binary 
features with a positive value set for each argument 
or pseudo-argument label that occurs in the 
argument parse. The training examples for the 
concept sequence classifiers are similar with the 
addition of the annotated speech act to the feature 
vector. After the training examples are constructed, 
new classifiers are trained. 
Two tools are available to support easy testing 
during grammar development. First, the entire 
training process can be run using a single script. 
Retraining for a new grammar simply requires 
running the script with pointers to the new 
grammars. Then, a special development mode of 
the translation servers allows the grammar writers 
to load development grammars and their 
corresponding segmentation model and DA 
classifiers. The translation server supports input in 
the form of individual utterances or files and 
allows the grammar developers to look at the 
results of each stage of the analysis process. 
6 Evaluation 
We present the results from recent experiments to 
measure the performance of the analyzer 
components and of end-to-end translation using the 
analyzer. We also report the results of an ablation 
experiment that used earlier versions of the 
analyzer and IF specification. 
6.1 Translation Experiment 
 
Acceptable Perfect 
SR Hypotheses 66% 56% 
Translation from 
Transcribed Text 58% 43% 
Translation from 
SR Hypotheses 45% 32% 
Table 1. English-to-English end-to-end translation 
 
Acceptable Perfect 
Translation from 
Transcribed Text 55% 38% 
Translation from 
SR Hypotheses 43% 27% 
Table 2. English-to-Italian end-to-end translation 
Tables 1 and 2 show end-to-end translation 
results of the NESPOLE! system. In this 
experiment, the input was a set of English 
utterances. The utterances were paraphrased back 
into English via the interlingua (Table 1) and 
translated into Italian (Table 2). The data used to 
train the DA classifiers consisted of 3350 SDUs 
annotated with IF representations. The test set 
contained 151 utterances consisting of 332 SDUs 
from 4 unseen dialogues. Translations were 
compared to human transcriptions and graded as 
described in (Levin et al, 2000). A grade of 
perfect, ok, or bad was assigned to each 
translation by human graders. A grade of perfect 
or ok is considered acceptable. The table shows the 
average of grades assigned by three graders. 
The row in Table 1 labeled SR Hypotheses 
shows the grades when the speech recognizer 
output is compared directly to human transcripts. 
As these grades show, recognition errors can be a 
major source of unacceptable translations. These 
grades provide a rough bound on the translation 
performance that can be expected when using input 
from the speech recognizer since meaning lost due 
to recognition errors cannot be recovered. The 
rows labeled Translation from Transcribed Text 
show the results when human transcripts are used 
as input. These grades reflect the combined 
performance of the analyzer and generator. The 
rows labeled Translation from SR Hypotheses 
show the results when the speech recognizer 
produces the input utterances. As expected, 
translation performance was worse with the 
introduction of recognition errors. 
 
Precision Recall 
70% 54% 
Table 3. SDU boundary detection performance 
Table 3 shows the performance of the 
segmentation model on the test set. The SDU 
boundary positions assigned automatically were 
compared with manually annotated positions. 
 
 
Classifier Accuracy 
Speech Act 65% 
Concept Sequence 54% 
Domain Action 43% 
Table 4. Classifier accuracy on transcription 
 
Frequency 
Speech Act 33% 
Concept Sequence 40% 
Domain Action 14% 
Table 5. Frequency of most common DA elements 
Table 4 shows the performance of the DA 
classifiers, and Table 5 shows the frequency of the 
most common DA, speech act, and concept 
sequence in the test set. Transcribed utterances 
were used as input and were segmented into SDUs 
before analysis. This experiment is based on only 
293 SDUs. For the remaining SDUs in the test set, 
it was not possible to assign a valid representation 
based on the current IF specification. 
These results demonstrate that it is not always 
necessary to find the canonical DA to produce an 
acceptable translation. This can be seen by 
comparing the Domain Action accuracy from Table 
4 with the Transcribed grades from Table 1. 
Although the DA classifiers produced the 
canonical DA only 43% of the time, 58% of the 
translations were graded as acceptable. 
 
 
Changed 
Speech Act 5% 
Concept Sequence 26% 
Domain Action 29% 
Table 6. DA elements changed by IF specification 
In order to examine the effects of using IF 
specification constraints, we looked at the 182 
SDUs which were not parsed by the cross-domain 
grammar and thus required DA classification. 
Table 6 shows how many DAs, speech acts, and 
concept sequences were changed as a result of 
using the constraints. DAs were changed either 
because the DA was illegal or because the DA did 
not license some of the arguments. Without the IF 
specification, 4% of the SDUs would have been 
assigned an illegal DA, and 29% of the SDUs 
(those with a changed DA) would have been 
assigned an illegal IF. Furthermore, without the IF 
specification, 0.38 arguments per SDU would have 
to be dropped while only 0.07 arguments per SDU 
were dropped when using the fallback strategy. 
The mean number of arguments per SDU was 1.47. 
6.2 Ablation Experiment 
Classification Accuracy (16-fold Cross 
Validation)
0
0.2
0.4
0.6
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
e
a
n
 
A
cc
ur
a
c
y
Speech Act
Concept
Sequence
Domain Action
 
Figure 2: DA classifier accuracy with varying 
amounts of data 
Figure 2 shows the results of an ablation 
experiment that examined the effect of varying the 
training set size on DA classification accuracy. 
Each point represents the average accuracy using a 
16-fold cross validation setup. 
The training data contained 6409 SDU-
interlingua pairs. The data were randomly divided 
into 16 test sets containing 400 examples each. In 
each fold, the remaining data were used to create 
training sets containing 500, 1000, 2000, 3000, 
4000, 5000, and 6009 examples. 
The performance of the classifiers appears to 
begin leveling off around 4000 training examples. 
These results seem promising with regard to the 
portability of the DA classifiers since a data set of 
this size could be constructed in a few weeks. 
7 Related Work 
Lavie et al (1997) developed a method for 
identifying SDU boundaries in a speech-to-speech 
translation system. Identifying SDU boundaries is 
also similar to sentence boundary detection. 
Stevenson and Gaizauskas (2000) use TiMBL 
(Daelemans et al, 2000) to identify sentence 
boundaries in speech recognizer output, and Gotoh 
and Renals (2000) use a statistical approach to 
identify sentence boundaries in automatic speech 
recognition transcripts of broadcast speech. 
Munk (1999) attempted to combine grammars 
and machine learning for DA classification. In 
Munk?s SALT system, a two-layer HMM was used 
to segment and label arguments and speech acts. A 
neural network identified the concept sequences. 
Finally, semantic grammars were used to parse 
each argument segment. One problem with SALT 
was that the segmentation was often inaccurate and 
resulted in bad parses. Also, SALT did not use a 
cross-domain grammar or interlingua specification. 
Cattoni et al (2001) apply statistical language 
models to DA classification. A word bigram model 
is trained for each DA in the training data. To label 
an utterance, the most likely DA is assigned. 
Arguments are identified using recursive transition 
networks. IF specification constraints are used to 
find the most likely valid DA and arguments. 
8 Discussion and Future Work 
One of the primary motivations for developing the 
hybrid analysis approach described here is to 
improve the portability of the analyzer to new 
domains and languages. We expect that moving 
from a purely grammar-based parsing approach to 
this hybrid approach will help attain this goal. 
The SOUP parser supports portability to new 
domains by allowing separate grammar modules 
for each domain and a grammar of rules shared 
across domains (Woszczyna et al, 1998). This 
modular grammar design provides an effective 
method for adding new domains to existing 
grammars. Nevertheless, developing a full 
semantic grammar for a new domain requires 
significant effort by expert grammar writers. 
The hybrid approach reduces the manual labor 
required to port to new domains by incorporating 
machine learning. The most labor-intensive part of 
developing full semantic grammars for producing 
IF is writing DA-level rules. This is exactly the 
work eliminated by using automatic DA classifiers. 
Furthermore, the phrase-level argument grammars 
used in the analyzer contain fewer rules than a full 
semantic grammar. The argument-level grammars 
are also less domain-dependent than the full 
grammars and thus more reusable. The DA 
classifiers should also be more tolerant than full 
grammars of deviations from the domain. 
We analyzed the grammars from a previous 
version of the translation system, which produced 
complete IFs using strictly grammar-based parsing, 
to estimate what portion of the grammar was 
devoted to the identification of domain actions. 
Approximately 2200 rules were used to cover 400 
DAs. Nonlexical rules made up about half of the 
grammar, and the DA rules accounted for about 
20% of the nonlexical rules. Using these figures, 
we can project the number of DA rules that would 
have to be added to the current system, which uses 
our hybrid analysis approach. The database for the 
new system contains approximately 600 DAs. 
Assuming the average number of rules per DA is 
the same as before, roughly 3300 DA-level rules 
would have to be added to the current grammar, 
which has about 17500 nonlexical rules, to cover 
the DAs in the database. 
Our hybrid approach should also improve the 
portability of the analyzer to new languages. Since 
grammars are language specific, adding a new 
language still requires writing new argument 
grammars. Then the DA classifiers simply need to 
be retrained on data for the new language. If 
training data for the new language were not 
available, DA classifiers using only language-
independent features, from the IF for example, 
could be trained on data for existing languages and 
used for the new language. Such classifiers could 
be used as a starting point until training data was 
available in the new language. 
The experimental results indicate the promise 
of the analysis approach we have described. The 
level of performance reported here was achieved 
using a simple segmentation model and simple DA 
classifiers with limited feature sets. We expect that 
performance will substantially improve with a 
more informed design of the segmentation model 
and DA classifiers. We plan to examine various 
design options, including richer feature sets and 
alternative classification techniques. We are also 
planning experiments to evaluate robustness and 
portability when the coverage of the NESPOLE! 
system is expanded to the medical domain later 
this year. In these experiments, we will measure 
the effort needed to write new argument grammars, 
the extent to which existing argument grammars 
are reusable, and the effort required to expand the 
argument grammar to include DA-level rules. 
9 Acknowledgements 
The research work reported here was supported by 
the National Science Foundation under Grant 
number 9982227. Special thanks to Alex Waibel 
and everyone in the NESPOLE! group for their 
support on this work. 
References 
Black, A., P. Taylor, and R. Caley. 1999. The 
Festival Speech Synthesis System: System 
Documentation. Human Computer Research 
Centre, University of Edinburgh, Scotland. 
http://www.cstr.ed.ac.uk/projects/festival/ma
nual 
Cattoni, R., M. Federico, and A. Lavie. 2001. 
Robust Analysis of Spoken Input Combining 
Statistical and Knowledge-Based Information 
Sources. In Proceedings of the IEEE Automatic 
Speech Recognition and Understanding 
Workshop, Trento, Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2000. TiMBL: Tilburg Memory 
Based Learner, version 3.0, Reference Guide. 
ILK Technical Report 00-01. 
http://ilk.kub.nl/~ilk/papers/ilk0001.ps.gz 
Gavald?, M. 2000. SOUP: A Parser for Real-
World Spontaneous Speech. In Proceedings of 
the IWPT-2000, Trento, Italy. 
Gotoh, Y. and S. Renals. Sentence Boundary 
Detection in Broadcast Speech Transcripts. 2000. 
In Proceedings on the International Speech 
Communication Association Workshop: 
Automatic Speech Recognition: Challenges for 
the New Millennium, Paris. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. 
Enhancing the Usability and Performance of 
NESPOLE! ? a Real-World Speech-to-Speech 
Translation System. In Proceedings of HLT-
2002, San Diego, CA. 
Lavie, A., C. Langley, A. Waibel, et al 2001. 
Architecture and Design Considerations in 
NESPOLE!: a Speech Translation System for E-
commerce Applications. In Proceedings of HLT-
2001, San Diego, CA. 
Lavie, A., D. Gates, N. Coccaro, and L. Levin. 
1997. Input Segmentation of Spontaneous Speech 
in JANUS: a Speech-to-speech Translation 
System. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-
96 Workshop, E. Maier, M. Mast, and S. 
Luperfoy (eds.), LNCS series, Springer Verlag. 
Lavie, A. 1996. GLR*: A Robust Grammar-
Focused Parser for Spontaneously Spoken 
Language. PhD dissertation, Technical Report 
CMU-CS-96-126, Carnegie Mellon University, 
Pittsburgh, PA. 
Levin, L., D. Gates, A. Lavie, et al 2000. 
Evaluation of a Practical Interlingua for Task-
Oriented Dialogue. In Workshop on Applied 
Interlinguas: Practical Applications of 
Interlingual Approaches to NLP, Seattle. 
Levin, L., D. Gates, A. Lavie, and A. Waibel. 
1998. An Interlingua Based on Domain Actions 
for Machine Translation of Task-Oriented 
Dialogues. In Proceedings of ICSLP-98, Vol. 4, 
pp. 1155-1158, Sydney, Australia. 
Munk, M. 1999. Shallow Statistical Parsing for 
Machine Translation. Diploma Thesis, Karlsruhe 
University. 
Stevenson, M. and R. Gaizauskas. Experiments on 
Sentence Boundary Detection. 2000. In 
Proceedings of ANLP and NAACL-2000, Seattle. 
Tomita, M. and E. H. Nyberg. 1988. Generation 
Kit and Transformation Kit, Version 3.2: User?s 
Manual. Technical Report CMU-CMT-88-
MEMO, Carnegie Mellon University, Pittsburgh, 
PA. 
Woszczyna, M., M. Broadhead, D. Gates, et al 
1998. A Modular Approach to Spoken Language 
Translation for Large Domains. In Proceedings 
of AMTA-98, Langhorne, PA. 
Balancing Expressiveness and Simplicity
in an Interlingua for Task Based Dialogue
Lori Levin, Donna Gates, Dorcas Wallace,
Kay Peterson, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
email: lsl@cs.cmu.edu
Fabio Pianesi, Emanuele Pianta,
Roldano Cattoni, Nadia Mana
IRST-itc, Italy
Abstract
In this paper we compare two interlin-
gua representations for speech transla-
tion. The basis of this paper is a distri-
butional analysis of the C-star II and
Nespole databases tagged with inter-
lingua representations. The C-star II
database has been partially re-tagged
with the Nespole interlingua, which
enables us to make comparisons on the
same data with two types of interlin-
guas and on two types of data (C-
star II and Nespole) with the same
interlingua. The distributional infor-
mation presented in this paper show
that the Nespole interlingua main-
tains the language-independence and
simplicity of the C-star II speech-act-
based approach, while increasing se-
mantic expressiveness and scalability.
1 Introduction
Several speech translation projects have chosen
interlingua-based approaches because of its con-
venience (especially in adding new languages)
in multi-lingual projects. However, interlingua
design is notoriously dicult and inexact. The
main challenge is deciding on the grain size of
meaning to represent and what facets of mean-
ing to include. This may depend on the do-
main and the contexts in which the translation
system is used. For projects that take place at
multiple research sites, another factor becomes
important in interlingua design: if the interlin-
gua is too complex, it cannot be used reliably by
researchers at remote sites. Furthermore, the in-
terlingua should not be biased toward one fam-
ily of languages. Finally, an interlingua should
clearly distinguish general and domain specic
components for easy scalability and portability
between domains.
Sections 2 and 3 describe how we balanced
the factors of grain-size, language independence,
and simplicity in two interlinguas for speech
translation projects | the C-star II Inter-
change Format (Levin et al, 1998) and the Ne-
spole Interchange Format. Both interlinguas
are based in the framework of domain actions
as described in (Levin et al, 1998). We will
show that the Nespole interlingua has a ner
grain-size of meaning, but is still simple enough
for collaboration across multiple research sites,
and still maintains language-independence.
Section 4 will address the issue of scalabil-
ity of interlinguas based on domain actions to
larger domains. The basis of Section 4 is a dis-
tributional analysis of the C-star II and Ne-
spole databases tagged with interlingua repre-
sentations. The C-star II database has been
partially re-tagged with the Nespole interlin-
gua, which enables us to make comparisons on
the same data with two types of interlinguas and
on two types of data (C-star II and Nespole)
with the same type of interlingua.
2 The C-star II Domain, Database,
and Interlingua
The C-star II interlingua (Levin et al, 1998)
was developed between 1997 and 1999 for use
in the C-star II 1999 demo (www.c-star.org).
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 53-60.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
c: can I have some flight times
that would leave some time around June sixth
a: the there are several flights leaving D C
there?d be one at one twenty four
there?s a three fifty nine flight
that arrives at four fifty eight
...
what time would you like to go
c: I would take the last one that you mentioned
...
a: what credit card number would you like
to reserve this with
c: I have a visa card
and the number is double oh five three
three one one six
ninety nine eighty seven
a okay
c: the expiration date is eleven ninety seven
...
a okay they should be ready tomorrow
c: okay thank you very much
Figure 1: Excerpt from a C-star II dialogue
with six participating research sites. The seman-
tic domain was travel, including reservations
and payments for hotels, tours, and transporta-
tion. Figure 1 shows a sample dialogue from
the C-star II database. (C is the client and a
is the travel agent.) The C-star II database
contains 2278 English sentences and 7148 non-
English (Japanese, Italian, Korean) sentences
tagged with interlingua representations. Most
of the database consists of transcripts of role-
playing conversations.
The driving concept behind the C-star II
interlingua is that there are a limited num-
ber of actions in the domain | requesting the
price of a room, telling the price of a room,
requesting the time of a flight, giving a credit
card number, etc. | and that each utter-
ance can be classied as an instance of one
of these domain actions . Figure 2 illustrates
the components of the C-star II interlingua:
(1) the speaker tag, in this case c for client,
(2) a speech act (request-action), (3) a list
of concepts (reservation, temporal, hotel),
(4) arguments (e.g., time), and (5) values of ar-
guments. The C-star II interlingua specica-
tion document contains denitions for 44 speech
acts, 93 concepts, and 117 argument names.
The domain action is the part of the interlin-
gua consisting of the speech act and concepts, in
this case request-action+reservation+tem-
poral+hotel. The domain action does not in-
clude the list of argument-value pairs.
First it is important to point out that do-
main actions are created compositionally. A do-
main action consists of a speech act followed by
zero or more concepts. (Recall that argument-
value pairs are not part of the domain action.)
The Nespole interlingua includes 65 speech
acts and 110 concepts. An interlingua speci-
cation document denes the legal combinations
of speech acts and arguments.
The linguistic justication for an interlingua
based on domain-actions is that many travel do-
main utterances contain xed, formulaic phrases
(e.g., can you tell me; I was wondering; how
about; would you mind, etc.) that signal domain
actions, but either do not translate literally into
other languages or have a meaning that is su-
ciently indirect that the literal meaning is irrele-
vant for translation. To take two examples, how
about as a signal of a suggestion does not trans-
late into other languages with the words corre-
sponding to how and about . Also, would you
mind might translate literally into some Euro-
pean languages as a way of signaling a request,
but the literal meaning of minding is not rel-
evant to the translation, only the fact that it
signals politeness.
The measure of success for the domain-action
based interlingua (as described in (Levin et al,
2000a)) is that (1) it covers the data in the C-
star II database with less than 8% no-tag rate,
(2) inter-coder agreement across research sites
is reasonably high: 82% for speech acts, 88%
for concepts, and 65% for domain actions, and
(3) end-to-end translation results using an an-
alyzer and generator written at dierent sites
were about the same as end-to-end translation
results using an analyzer and generator written
at the same site.
3 The Nespole Domain, Database,
and Interlingua
The Nespole interlingua has been under devel-
opment for the last two years as part of the Ne-
spole project (http://nespole.itc.it). Fig-
I would like to make a hotel reservation for the fourth through
the seventh of july
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Figure 2: Example of a C-star II interlingua representation
ure 3 shows a Nespole dialogue. The Ne-
spole domain does not include reservations and
payments, but includes more detailed inquiries
about hotels and facilities for ski vacations and
summer vacations in Val di Fiemme, Italy. (The
tourism board of the Trentino area is a partner
of the Nespole project.) Most of the database
consists of transcripts of dialogues between an
Italian-speaking travel agent and an English or
German speaker playing the role of a traveller.
There are fewer xed, formulaic phrases in the
Nespole domain, prompting us to move toward
domain actions that are more general, and also
requiring more detailed interlingua representa-
tions. Changes from the C-star II interlingua
fall into several categories:
1. Extending semantic expressivity and
syntactic coverage: Increased coverage of
modality, tense, aspect, articles, fragments,
coordinate structures, number, and rhetor-
ical relations. In addition, we have added
more explicit representation of grammati-
cal relations and improved capabilities for
representing modication and embedding.
2. Additional Domain-Specic Con-
cepts: New concepts include giving
directions, describing sizes and dimensions
of objects, traveling routes, equipment and
gear, airports, tourist services, facilities,
vehicles, information objects (brochures,
web pages, rules and regulations), hours
of operation of businesses and attractions,
etc.
3. Utterances that accompany multi-
modal gestures: The Nespole system
includes capabilities to share web pages
and draw marks such as circles and arrows
on web pages. The interlingua was ex-
tended to cover colord, descriptions of two-
dimensional objects, and actions of show-
ing.
4. General concept names from Word-
Net: The Nespole interlingua includes
conventions for making new concept names
based on WordNet synsets.
5. More general domain actions replac-
ing specic ones: For example, replacing
hotel with accommodation.
Interlinguas based on domain actions con-
trast with interlinguas based on lexical seman-
tics (Dorr, 1993; Lee et al, 2001; Goodman and
Nirenburg, 1991). A lexical-semantic interlingua
includes a representation of predicates and their
arguments. For example, the sentence I want to
take a vacation has a predicate want with two
arguments I and to take a vacation, which in
turn has a predicate take and two arguments, I
and a vacation. Of course, predicates like take
may be represented as word senses that are less
language-dependent like participate-in. The
strength and weakness of the lexical-semantic
approach is that it is less domain dependent
than the domain-action approach.
In order to cover the less formulaic utterances
of the Nespole domain, we have taken a step
closer to the lexical-semantic approach. How-
ever, we have maintained the overall framework
of the domain-action approach because there are
still many formulaic utterances that are better
represented in a non-literal way. Also, in or-
der to abstract away from English syntax, con-
cepts such as disposition, eventuality, and obli-
gation are not represented in the interlingua as
argument-taking main verbs in order to accom-
modate languages in which these meanings are
c: and I have some questions about coming about a trip I?m gonna be taking to Trento
a: okay what are your questions
c: I currently have a hotel booking at the
Panorama-Hotel in Panchia but at the moment I have no idea how to get to my hotel from Trento
and I wanted to ask what would be the best way for me to get there
a: okay I?m gonna show you a map that and then describe the directions to you
okay so right so you will arrive in the train station in Trento
the that is shown in the middle of the map stazione FFSS
and just below that here is a bus stop labeled number forty
so okay on the map that I?m showing you here
the hotel is the orange building off on the right hand side
...
c: I also wanted to ask about skiing in the area once I?m in Panchia
a: all right just a moment and I?ll show you another map
c: okay
a: okay so on the map you see now Panchia is right in the center of the map
c: I see it
Figure 3: Excerpt from a Nespole dialogue
represented as adverbs or suxes on verbs. Fig-
ure 4 shows the Nespole interlingua represen-
tation corresponding to the C-star II interlin-
gua in Figure 2. The specication document for
the Nespole interlingua denes 65 speech acts,
110 concepts, 292 arguments, and 7827 values
grouped into 222 value classes. As in the C-
star II interlingua, domain actions are dened
compositionally from speech acts and arguments
in combinations that are allowed by the interlin-
gua specication.
3.1 Comparison of Nespole and
C-star II Interlinguas
It is useful to compare the Nespole and C-
star II Interlinguas in expressivity, language in-
dependence, and simplicity.
Expressivity of the Nespole interlingua,
Argument 1: The metric we use for expres-
sivity is the no-tag rate in the databases. The
no-tag rate is the percentage of sentences that
cannot be assigned an interlingua representation
by a human expert. The C-star II database
tagged with C-star II interlingua had a no-
tag rate of 7.3% (Levin et al, 2000a). The
C-star II database tagged with Nespole in-
terlingua has a no-tag rate of 2.4%. More than
300 English sentences in the C-star II database
that were not covered by the C-star II interlin-
gua are now covered by the Nespole interlin-
gua. (See Table 2.) We conclude from this that
the Nespole interlingua is more expressive in
that it covers more data.
Language-independence of the Nespole
interlingua: We do not have a numerical
measure of language-independence, but we note
that interlinguas based on domain actions are
particularly suitable for avoiding translation
mismatches (Dorr, 1994), particularly head-
switching mismatches (e.g., I just arrived and
Je vient d?arriver where the meaning of recent
past is expressed by an adverb just or a syn-
tactic verb vient (venir).) Interlinguas based
on domain actions resolve head-switching mis-
matches by identifying the types of meanings
that are often involved in mismatches | modal-
ity, evidentiality, disposition, and so on | and
assigning them a representation that abstracts
away from predicate argument structure. In-
terlinguas based on domain actions also neu-
tralize the dierent ways of expressing indirect
speech acts within and across languages (for ex-
ample, Would you mind..., I was wondering if
you could...., and Please.... as ways of request-
ing an action). Although Nespole domain ac-
tions are more general than C-star II domain
actions, they maintain language independence
by abstracting away from predicate-argument
structure.
Simplicity and cross-site reliability of the
Nespole interlingua: Simplicity of an inter-
lingua is measured by cross-site reliability in
I would like to make a hotel reservation for the fourth through
the seventh of july
C-star II Interlingua:
c:request-action+reservation+temporal+hotel
(time=(start-time=md4, end-time=(md7, july)))
Nespole Interlingua:
c:give-information+disposition+reservation+accommodation
(disposition=(who=i, desire),
reservation-spec=(reservation, identifiability=no),
accommodation-spec=hotel,
object-time=(start-time=(md=4), end-time=(md=7, month=7, incl-excl=inclusive)))}
Figure 4: Example of Nespole interlingua representation
inter-coder agreement and end-to-end transla-
tion performance. At the time of writing this pa-
per we have not conducted cross-site inter-coder
agreement experiments using the Nespole in-
terlingua. We have, however, conducted cross-
site evaluations (Lavie et al, 2002), in which the
analyzer and generator were written at dier-
ent sites. Experiments at the end of C-star II
showed that cross-site evaluations were compa-
rable to intra-site evaluations (analyzer and gen-
erator written at the same site) (Levin et al,
2000b). Nespole evaluations so far show a loss
of cross-site reliability: intra-site evaluations are
noticeably better than cross-site evaluations, as
reported in (Lavie et al, 2002). This seems to
indicate that developers at dierent sites have
a lower level of agreement on the Nespole in-
terlingua. However there are other possible ex-
planations for the discrepancy | for example
developers at dierent sites may have focused
their development on dierent sub-domains |
that are currently under investigation.
4 Scalability of the Nespole
Interlingua
The rest of this paper addresses the scalability
of the Nespole interlingua. A possible criti-
cism of domain actions is that they are domain
dependent and that the number of domain ac-
tions might increase too quickly with the size
of the domain. In this section, we will examine
the rate of increase in the number of domain ac-
tions as a function of the amount of data and
the diversity of the data.
Dierences in the C-star and Nespole Do-
mains: We will rst show that the C-star
and Nespole domains are signicantly dierent
even though they both pertain to travel. The
combination of the two domains is therefore sig-
nicantly larger than either domain alone.
In order to demonstrate the dierences be-
tween the C-star travel domain and the Ne-
spole travel domain, we measured the overlap
in vocabulary. The numbers in Table 4 are based
on the rst 7900 word tokens in the C-star En-
glish database and the rst 7900 word tokens
in the Nespole English database. The table
shows the number of unique word types in each
database, the number of word types that occur
in both databases, and the number of word types
that occur in one of the databases, but not in the
other. In each database, about half of the word
types overlap with the other database. The non-
overlapping vocabulary (402 C-star word types
and 344 Nespole word types) indicates that the
two databases cover quite dierent aspects of the
travel domain.
Scalability: Argument 1: We will now be-
gin to address the issue of scalability of the
domain action approach to interlingua design.
Our rst argument concerns the number of
Number of unique word types
CSTAR English 745
Nespole English 687
Word types in both CSTAR and Nespole 343
Words types in CSTAR not in Nespole 402
Words types n Nespole not in CSTAR 344
Table 1: Number of overlapping word types in the C-star English and Nespole English
databases
SA Con. Snts. Domain Ac-
tions
Old C-star English 44 93 2278 358
New C-star English 65 110 2564 452
Nespole English 65 110 1446 337
Nespole German 65 110 3298 427
Nespole Italian 65 110 1063 206
Table 2: Number of unique domain actions in interlingua databases
speech acts and concepts in the combined C-
star/Nespole domain. The C-star II in-
terlingua, designed for coverage of the C-star
travel domain, included 44 speech acts and 93
concepts. The Nespole interlingua, designed
for coverage of the combined C-star and Ne-
spole domains, has 65 speech acts and 110 con-
cepts. Thus a relatively small increase in the
number of speech acts and concepts is required
to cover a signicantly larger domain.
The increased size of the C-star/Nepsole
domain is reflected in the number of arguments
and values. The C-star II interlingua contained
denitions for 117 arguments, whereas the Ne-
spole interlingua contains denitions for 292 ar-
guments. The number of values for arguments
also has increased signicantly in the Nespole
domain. There are 7827 values grouped into 222
classes (airport names, days of the week, etc.).
Distributional Data: number of domain
actions in each database: Next we will
present distributional data concerning the num-
ber of domain actions as a function of database
size. We will compare several databases: Old
C-star English (around 2278 sentences tagged
with C-star II interlingua), New C-star En-
glish (2564 sentences tagged with Nespole in-
terlingua, including the 2278 sentences from Old
C-star English), Nespole English, Nespole
German, and Nespole Italian. Table 2 shows
the number of sentences and the number of do-
main actions in each database. The number of
domain actions refers to the number of types,
not tokens, of domain actions.
Distributional data: Coverage of the top
50 domain actions: Table 3 shows the per-
centage of each database that is covered by the
5, 10, 20, and 50 most frequent domain actions
in that database. For each database, the do-
main actions were ordered by frequency. The
percentage of sentences covered by the top-n
domain actions was then calculated. For this
experiment, we separated sentences spoken by
the traveller (client) and sentences spoken by
the travel agent (agent). C-star data in Ta-
ble 3 refers to 2564 English sentences from the
C-star database that were tagged with Ne-
spole interlingua. Nespole data refers to the
English portion of the Nespole database (1446
sentences). Combined data refers to the combi-
nation of the two (4014 sentences).
Two points are worth noting about Table 3.
First, the Nespole agent data has a higher cov-
erage rate than the Nespole client data. That
is, more data is covered by the top-n domain
actions. This may be because there was was
Domain Actions Top 5 Top 10 Top 20 Top 50
Client
C-star data 33.6 42.7 53.1 66.7
Nespole data 31.7 43.5 53.9 66.5
Combined data 31.6 40.0 50.3 62.9
Agent
C-star data 33.8 42.8 54.1 67.3
Nespole data 39.0 47.8 56.1 71.4
Combined data 33.6 41.5 51.7 64.0
Table 3: DA Coverage using Nespole interlingua on English data for both C-star and
Nespole
only a small amount of English agent data and
it was spoken by non-native speakers. Second,
the combined data has a slightly lower cover-
age rate than either the C-star or Nespole
databases alone. This is expected because, as
shown above, the combined domain is signi-
cantly more diverse than either domain by itself.
Scalability: Argument 2: Table 3 provides
additional evidence for the scalability of the Ne-
spole interlingua to larger domains. In the
combined C-star and Nespole domain, the
top 50 domain actions cover only slightly less
data than the top 50 domain actions in either
domain separately. There is not, in fact, an ex-
plosion of domain actions when the two C-star
and Nespole domains are combined.
Distributional Data: domain actions as a
function of database size: Table 3 shows
that in each of our databases, the 50 most fre-
quent domain actions cover approximately 65%
of the sentences. The next issue we address is
the nature of the \tail" of less frequent domain
actions covering the remainder of the data.
Figure 5 shows the number of domain actions
as a function of data set size. Sampling was done
for intervals of 25 sentences starting at 100 sen-
tences. For each sample size s there was ten-fold
cross-validation. Ten random samples of size s
were chosen, and the number of dierent domain
actions in each sample was counted. The aver-
age of the number of domain actions in each of
the ten samples of size s are plotted in Figure 5.
The four databases represented in Figure 5 are
IF Coverage of Four Datasets
0
100
200
300
400
500
600
700
10
0
70
0
13
00
19
00
25
00
31
00
number of SDUs in sample
av
er
ag
e 
nu
m
be
r o
f u
ni
qu
e 
DA
s 
o
ve
r 
10
 ra
nd
om
 s
am
pl
es
Old CSTAR
New CSTAR
NESPOLE
Combined
Figure 5: Number of domain actions as a function of
database size
the C-star English database tagged with C-
star II interlingua, the C-star II database
tagged with Nespole interlingua, the Nespole
English database, and the combined C-star
and Nespole English databases.
Expressivity, Argument 2: Figure 5 pro-
vides evidence for the increased expressivity of
the Nespole interlingua. In contrast to Ta-
ble 3, which deals with samples containing the
most frequent domain actions, the samples plot-
ted in Figure 5 contain random mixtures of fre-
quent and non-frequent domain actions. The
curve representing the C-star data with C-
star II interlingua is the slowest growing of the
four curves. This is because the grain-size of
meaning represented in the C-star II interlin-
gua was larger than in the Nespole interlin-
gua. Also many infrequent domain actions were
not covered by the C-star II interlingua. The
faster growth of the curve representing the C-
star data with Nespole interlingua indicates
improved expressivity of the Nespole interlin-
gua | it covers more of the infrequent domain
actions. The highest curve in Figure 5 repre-
sents the combined C-star and Nespole do-
mains. This curve is higher than the others be-
cause, as shown above, the two travel domains
are signicantly dierent from each other.
Expressivity and Simplicity, the right bal-
ance: Comparing Table 3 and Figure 5, we ar-
gue that the Nespole interlingua strikes a good
balance between expressivity and simplicity. Ta-
ble 3 shows evidence for the simplicity of the Ne-
spole interlingua: Only 50 domain actions are
needed to cover 60-70% of the sentences in the
database. Figure 5 shows evidence for expressiv-
ity: because domain actions are compositionally
formed from speech acts and concepts, it is pos-
sible to form a large number of low-frequency
domain actions in order to cover the domain.
Over 600 domain actions are used in the com-
bined C-star and Nespole domains.
5 Conclusions
We have presented a comparison of a purely
domain-action-based interlingua (the C-star II
interlingua) and a more expressive, but still
domain-action-based interlingua (the Nespole
interlingua). The data that we have presented
show that the more expressive interlingua has
better coverage of the domain (a decrease from
7.3% to 2.4% uncovered data in the C-star II
domain) and can also scale up to larger domains
without an explosion of domain actions. Thus
we have a reasonable compromise between sim-
plicity and expressiveness of the interlingua.
Acknowledgments
We would like to acknowledge Hans-Ulrich Block
for rst proposing the domain-action-based in-
terlingua to the C-star consortium. We would
also like to thank all of the C-star and Ne-
spole partners who have participated in the de-
sign of the interlingua. This work was supported
by NSF Grant 9982227 and EU Grant IST 1999-
11562 as part of the joint EU/NSF MLIAM re-
search initiative.
References
Bonnie J. Dorr. 1993. Machine Translation: A View
from the Lexicon. The MIT Press, Cambridge,
Massachusetts.
Bonnie J. Dorr. 1994. Machine Translation Diver-
gences: A Formal Description and Proposed Solu-
tion. Computational Linguistics, 20(4):597{633.
Kenneth Goodman and Sergei Nirenburg. 1991.
The KBMT Project: A Case Study in Knowledge-
Based Machine Translation. Morgan Kaufmann,
San Mateo, CA.
Alon Lavie, Florian Metze, Roldano Cattoni, and Er-
ica Constantini. 2002. A Multi-Perspective Eval-
uation of the NESPOLE! Speech-to-Speech Trans-
lation System. In Proceedings of Speech-to-Speech
Translation: Algorithms and Systems.
Young-Suk Lee, W. Yi, Cliord Weinstein, and
Stephanie Sene. 2001. Interlingua-based broad-
coverage korean-to-english translation. In Pro-
ceedings of HLT, San Diego.
Lori Levin, Donna Gates, Alon Lavie, and Alex
Waibel. 1998. An Interlingua Based on Domain
Actions for Machine Translation of Task-Oriented
Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (IC-
SLP?98), pages Vol. 4, 1155{1158, Sydney, Aus-
tralia.
Lori Levin, Donna Gates, Alon Lavie, Fabio Pianesi,
Dorcas Wallace, Taro Watanabe, and Monika
Woszczyna. 2000a. Evaluation of a Practical In-
terlingua for Task-Oriented Dialogue. In Work-
shop on Applied Interlinguas: Practical Applica-
tions of Interlingual Approaches to NLP, Seattle.
Lori Levin, Alon Lavie, Monika Woszczyna, Donna
Gates, Marsal Gavalda, Detlef Koll, and Alex
Waibel. 2000b. The Janus-III Translation Sys-
tem. Machine Translation.
Domain Specific Speech Acts for Spoken Language Translation 
Lori Levin, Chad Langley, Alon Lavie,  
Donna Gates, Dorcas Wallace and Kay Peterson 
Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA, United States 
{lsl,clangley,alavie,dmg,dorcas,kay+}@cs.cmu.edu 
Abstract 
We describe a coding scheme for ma-
chine translation of spoken task-
oriented dialogue. The coding scheme 
covers two levels of speaker intention ? 
domain independent speech acts and 
domain dependent domain actions. Our 
database contains over 14,000 tagged 
sentences in English, Italian, and Ger-
man. We argue that domain actions, and 
not speech acts, are the relevant dis-
course unit for improving translation 
quality. We also show that, although 
domain actions are domain specific, the 
approach scales up to large domains 
without an explosion of domain actions 
and can be coded with high inter-coder 
reliability across research sites. Fur-
thermore, although the number of do-
main actions is on the order of ten times 
the number of speech acts, sparseness is 
not a problem for the training of classi-
fiers for identifying the domain action. 
We describe our work on developing 
high accuracy speech act and domain 
action classifiers, which is the core of 
the source language analysis module of 
our NESPOLE machine translation sys-
tem. 
1 Introduction 
The NESPOLE and C-STAR machine translation 
projects use an interlingua representation based 
on speaker intention rather than literal meaning. 
The speaker's intention is represented as a 
domain independent speech act followed by do-
main dependent concepts. We use the term 
domain action to refer to the combination of a 
speech act with domain specific concepts. Exam-
ples of domain actions and speech acts are shown 
in Figure 1. 
 
c:give-information+party  
?I will be traveling with my husband and 
our two children ages two and eleven? 
 
c:request-information+existence+facility  
?Do they have parking available?" 
?Is there someplace to go ice skating?" 
 
c:give-information+view+information-
object  
?I see the bus icon?  
 
Figure 1: Examples of Speech Acts and Domain 
Actions. 
 
Domain actions are constructed compositionally 
from an inventory of speech acts and an inven-
tory of concepts. The allowable combinations of 
speech acts and concepts are formalized in a hu-
man- and machine-readable specification docu-
ment. The specification document is supported 
by a database of over 14,000 tagged sentences in 
English, German, and Italian. 
The discourse community has long recog-
nized the potential for improving NLP systems 
by identifying speaker intention. It has been hy-
pothesized that predicting speaker intention of 
the next utterance would improve speech recog-
nition (Reithinger et al, Stolcke et al), or reduce 
ambiguity for machine translation (Qu et al, 
1996, Qu et al, 1997). Identifying speaker inten-
tion is also critical for sentence generation. 
We argue in this paper that the explicit repre-
sentation of speaker intention using domain ac-
tions can serve as the basis for an effective 
language-independent representation of meaning 
for speech-to-speech translation and that the 
relevant units of speaker intention are the domain 
specific domain action as well as the domain in-
dependent speech act. After a brief description of 
our database, we present linguistic motivation for 
domain actions. We go on to show that although 
domain actions are domain specific, there is not 
an explosion or exponential growth of domain 
actions when we scale up to a larger domain or 
port to a new domain. Finally we will show that, 
although the number of domain actions is on the 
order of ten times the number of speech acts, 
data sparseness is not a problem in training a 
domain action classifier. We present extensive 
work on developing a high-accuracy classifier 
for domain actions using a variety of classifica-
tion approaches and conclusions on the adequacy 
of these approaches to the task of domain action 
classification.  
2 Data Collection Scenario and Data-
base 
Our study is based on data that was collected for 
the NESPOLE and C-STAR speech-to-speech 
translation projects. Three domains are included. 
The NESPOLE travel domain covers inquiries 
about vacation packages. The C-STAR travel 
domain consists largely of reservation and pay-
ment dialogues and overlaps only about 50% in 
vocabulary with the NESPOLE travel domain. 
The medical assistance domain includes dia-
logues about chest pain and flu-like symptoms. 
There were two data collection protocols for 
the NESPOLE travel domain ? monolingual and 
bilingual. In the monolingual protocol, an Eng-
lish speaker in the United States had a conversa-
tion with an Italian travel agent speaking (non-
native) English in Italy. Monolingual data was 
also collected for German, French and Italian. 
Bilingual data was collected during user studies 
with, for example, an English speaker in the 
United States talking to an Italian-speaking travel 
agent in Italy, with the NESPOLE system pro-
viding the translation between the two parties. 
The C-STAR data consists of only monolingual 
role-playing dialogues with both speakers at the 
same site. The medical dialogues are monolin-
gual with doctors playing the parts of both doctor 
and patient. 
The dialogues were transcribed and multi-
sentence utterances were broken down into mul-
tiple Semantic Dialogue Units (SDUs) that each 
correspond to one domain action. Some SDUs 
have been translated into other NESPOLE or C-
STAR languages. Over 14,000 SDUs have been 
tagged with interlingua representations including 
domain actions as well as argument-value pairs. 
Table 1 summarizes the number of tagged SDUs 
in complete dialogues in the interlingua database. 
There are some additional tagged dialogue frag-
ments that are not counted. Figure 2 shows an 
excerpt from the database. 
 
 
 
English NESPOLE Travel 4691 
English C-STAR Travel 2025 
German NESPOLE Travel 1538 
Italian NESPOLE Travel 2248 
English Medical Assistance 2001 
German Medical Assistance 1152 
Italian Medical Assistance 935 
Table 1: Tagged SDUs in the Interlingua Data-
base. 
 
e709wa.19.0  comments: DATA from 
e709_1_0018_ITAGOR_00 
 
e709wa.19.1  olang ITA  lang ITA Prv CMU   
?hai in mente una localita specifica?" 
e709wa.19.1  olang ITA  lang GER  Prv CMU   
?haben Sie einen bestimmten Ort im Sinn?" 
e709wa.19.1  olang ITA  lang FRE  Prv 
CLIPS ?" 
e709wa.19.1  olang ITA  lang ENG  Prv CMU   
?do you have a specific place in mind" 
e709wa.19.1                   IF  Prv CMU   
a:request-information+disposition+object  
(object-spec=(place, modifier=specific, 
identifiability=no), disposi-
tion=(intention, who=you)) 
e709wa.19.1  comments: Tagged by dmg 
 
Figure 2: Excerpt from the Interlingua Database. 
3 Linguistic Argument for Domain Ac-
tions 
Proponents of Construction Grammar (Fillmore 
et. al. 1988, Goldberg 1995) have argued that 
human languages consist of constructional units 
that include a syntactic structure along with its 
associated semantics and pragmatics. Some con-
structions follow the typical syntactic rules of the 
language but have a semantic or pragmatic focus 
that is not compositionally predictable from the 
parts. Other constructions do not even follow the 
typical syntax of the language (e.g., Why not go? 
with no tensed verb). 
Our work with multilingual machine transla-
tion of spoken language shows that fixed expres-
sions cannot be translated literally. For example, 
Why not go to the meeting? can be translated 
into Japanese as Kaigi ni itte mitara doo? (meet-
ing to going see/try-if how), which differs from 
the English in several ways. It does not have a 
word corresponding to not; it has a word that 
means see/try that does not appear in the English 
sentence; and so on. In order to produce an ac-
ceptable translation, we must find a common 
ground between the English fixed expression 
Why not V-inf? and the Japanese fixed expression 
-te mittara doo?. The common ground is the 
speaker's intention (in this case, to make a sug-
gestion) rather than the syntax or literal meaning. 
Speaker intention is partially captured with a 
direct or indirect speech act. However, whereas 
speech acts are generally domain independent, 
task-oriented language abounds with fixed ex-
pressions that have domain specific functions. 
For example, the phrases We have? or There 
are? in the hotel reservation domain express 
availability of rooms in addition to their more 
literal meanings of possession and existence. In 
the past six years, we have been successful in 
using domain specific domain actions as the ba-
sis for translation of limited-domain task-
oriented spoken language (Levin et al, 1998, 
Levin et al 2002; Langley and Lavie, 2003) 
4 Scalability and Portability of Domain 
Actions 
Domain actions, like speech acts, convey speaker 
intention. However, domain actions also repre-
sent components of meaning and are therefore 
more numerous than domain independent speech 
acts. 1168 unique domain actions are used in our 
NESPOLE database, in contrast to only 72 
speech acts. We show in this section that domain 
actions yield good coverage of task-oriented do-
mains, that domain actions can be coded effec-
tively by humans, and that scaling up to larger 
domains or porting to new domains is feasible 
without an explosion of domain actions.  
 
Coverage of Task-Oriented Domains: Our 
NESPOLE domain action database contains dia-
logues from two task-oriented domains: medical 
assistance and travel. Table 2 shows the number 
of speech acts and concepts that are used in the 
travel and medical domains.  The 1168 unique 
domain actions that appear in our database are 
composed of the 72 speech acts and 125 con-
cepts. 
 
 Travel Medical Combined 
DAs 880 459 1168 
SAs 67 44 72 
Concepts 91 74 125 
Table 2: DA component counts in NESPOLE 
data. 
 
Our domain action based interlingua has quite 
high coverage of the travel and medical dia-
logues we have collected. To measure how well 
the interlingua covers a domain, we define the 
no-tag rate as the percent of sentences that are 
not covered by the interlingua, according to a 
human expert. The no-tag rate for the English 
NESPOLE travel dialogues is 4.3% for dialogues 
that have been used for system development.  
We have also estimated the domain action no-
tag rate for unseen data using the NESPOLE 
travel database (English, German, and Italian 
combined). We randomly selected 100 SDUs as 
seen data and extracted their domain actions. We 
then randomly selected 100 additional SDUs 
from the remaining data and estimated the no-tag 
rate by counting the number of SDUs not cov-
ered by the domain actions in the seen data. We 
then added the unseen data to the seen data set 
and randomly selected 100 new SDUs. We re-
peated this process until the entire database had 
been seen, and we repeated the entire sampling 
process 10 times. Although the number of do-
main actions increases steadily with the database 
size (Figure 4), the no-tag rate for unseen data 
stabilizes at less than 10%.  
We also randomly selected half of the SDUs 
(4200) from the database as seen data and ex-
tracted the domain actions. Holding the seen data 
set fixed, we then estimated the no-tag rates in 
increasing amounts of unseen data from the re-
maining half of the database. We repeated this 
process 10 times. With a fixed amount of seen 
data, the no-tag rate remains stable for increasing 
amounts of unseen data. We observed similar no-
tag rate results for the medical assistance domain 
and for the combination of travel and medical 
domains. 
It is also important to note that although there 
is a large set of uncommon domain actions, the 
top 105 domain actions cover 80% of the sen-
tences in the travel domain database. Thus do-
main actions are practical for covering task-
oriented domains. 
 
Intercoder Agreement: Intercoder agreement is 
another indicator of manageability of the domain 
action based interlingua. We calculate intercoder 
agreement as percent agreement. Three interlin-
gua experts at one NESPOLE site achieved 94% 
agreement (average pairwise agreement) on 
speech acts and 88% agreement on domain ac-
tions. Across sites, expert agreement on speech 
acts is still quite high (89%), although agreement 
on domain actions is lower (62%). Since many 
domain actions are similar in meaning, some dis-
agreement can be tolerated without affecting 
translation quality. 
 
Figure 3: DAs to cover data (English). 
Figure 4: DAs to cover data (All languages). 
 
Scalability and Portability: The graphs in Figure 
3 and Figure 4 illustrate growth in the number of 
domain actions as the database size increases and 
as new domains are added. The x-axis represents 
the sample size randomly selected from the data-
base. The y-axis shows the number of unique 
domain actions (types) averaged over 10 samples 
of each size. Figure 3 shows the growth in do-
main actions for three English databases 
(NESPOLE travel, C-STAR travel, and medical 
assistance) as well as the growth in domain ac-
tions for a database consisting of equal amounts 
of data from each domain. Figure 4 shows the 
growth in domain actions for combined English, 
German, and Italian data in the NESPOLE travel 
and medical domains.  
Figure 3 and Figure 4 show that the number 
of domain actions increases steadily as the data-
base grows. However, closer examination reveals 
that scalability to larger domains and portability 
to new domains are in fact feasible.  The curves 
representing combined domains (travel plus 
medical in Figure 4 and NESPOLE travel, C-
STAR travel, and medical in Figure 3) show only 
a small increase in the number of domain actions 
when two domains are combined. In fact, there is 
a large overlap between domains.  In Table 3 the 
Overlap columns show the number of DA types 
and tokens that are shared between the travel and 
medical domains. We can see around 70% of DA 
tokens are covered by DA types that occur in 
both domains. 
 
 
DA 
Types 
Type 
Overlap 
DA 
Tokens 
Token 
Overlap 
NESPOLE 
Travel 880 171 8477 
6004 
(70.8%) 
NESPOLE 
Medical 459 171 4088 
2743 
(67.1%) 
Table 3: DA Overlap (All languages). 
5 A Hybrid Analysis Approach for Pars-
ing Domain Actions 
Langley et al (2002; Langley and Lavie, 2003) 
describe the hybrid analysis approach that is used 
in the NESPOLE! system (Lavie et al, 2002). 
The hybrid analysis approach combines gram-
mar-based phrasal parsing and machine learning 
techniques to transform utterances into our inter-
lingua representation. Our analyzer operates in 
three stages to identify the domain action and 
arguments. 
First, an input utterance is parsed into a se-
quence of arguments using phrase-level semantic 
grammars and the SOUP parser (Gavald?, 2000). 
Four grammars are defined for argument parsing: 
an argument grammar, a pseudo-argument gram-
mar, a cross-domain grammar, and a shared 
grammar. The argument grammar contains 
phrase-level rules for parsing arguments defined 
in the interlingua. The pseudo-argument gram-
mar contains rules for parsing common phrases 
that are not covered by interlingua arguments. 
For example, all booked up, full, and sold out 
might be grouped into a class of phrases that in-
dicate unavailability. The cross-domain grammar 
contains rules for parsing complete DAs that are 
domain independent. For example, this grammar 
contains rules for greetings (Hello, Good bye, 
Nice to meet you, etc.). Finally, the shared 
grammar contains low-level rules that can be 
used by all other subgrammars. 
After argument parsing, the utterance is seg-
mented into SDUs using memory-based learning 
(k-nearest neighbor) techniques. Spoken utter-
ances often consist of several SDUs. Since DAs 
are assigned at the SDU level, it is necessary to 
segment utterances before assigning DAs. 
0
100
200
300
400
500
600
700
800
900
0 1000 2000 3000 4000 5000 6000 7000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical C-STAR
Nespole Travel+Medical C-STAR + Nespole Travel+Medical
0
100
200
300
400
500
600
700
800
900
1000
0 1000 2000 3000 4000 5000 6000 7000 8000 9000
SDUs per Sample
M
ea
n
 
Un
iq
u
e 
DA
s 
o
v
er
 
10
 
Ra
n
do
m
 
Sa
m
pl
es
Nespole Travel Nespole Medical Nespole Travel+Medical
The final stage in the hybrid analysis ap-
proach is domain action classification.  
6 Domain Action Classification 
Identifying the domain action is a critical step in 
the analysis process for our interlingua-based 
translation systems. One possible approach 
would be to manually develop grammars de-
signed to parse input utterances all the way to the 
domain action level. However, while grammar-
based parsing may provide very accurate analy-
ses, it is generally not feasible to develop a 
grammar that completely covers a domain. This 
problem is exacerbated with spoken input, where 
disfluencies and deviations from the grammar are 
very common. Furthermore, a great deal of effort 
by human experts is generally required to de-
velop a wide-coverage grammar. 
An alternative to writing full domain action 
grammars is to train classifiers to identify the 
DA. Machine learning approaches allow the ana-
lyzer to generalize beyond training data and tend 
to degrade gracefully in the face of noisy input. 
Machine learning methods may, however, be less 
accurate than grammars, especially on common 
in-domain input, and may require a large amount 
of training data in order to achieve adequate lev-
els of performance. In the hybrid analyzer de-
scribed above, classifiers are used to identify the 
DA for domain specific portions of utterances 
that are not covered by the cross-domain gram-
mar. 
We tested classifiers trained to classify com-
plete DAs. We also split the DA classification 
task into two subtasks: speech act classification 
and concept sequence classification. This simpli-
fies the task of each classifier, allows for the use 
of different approaches and/or feature sets for 
each task, and reduces data sparseness. Our hy-
brid analyzer uses the output of each classifier 
along with the interlingua specification to iden-
tify the DA (Langley et al, 2002; Langley and 
Lavie, 2003). 
7 Experimental Setup 
We conducted experiments to assess the per-
formance of several machine-learning ap-
proaches on the DA classification tasks. We 
evaluated all of the classifiers on English and 
German input in the NESPOLE travel domain.  
7.1 Corpus 
The corpus used in all of the experiments was the 
NESPOLE! travel and tourism database. Since 
our goal was to evaluate the SA and concept se-
quence classifiers and not segmentation, we cre-
ated training examples for each SDU in the 
database rather than for each utterance. Table 4 
contains statistics regarding the contents of the 
corpus for our classification tasks. Table 5 shows 
the frequency of the most common domain ac-
tion, speech act, and concept sequence in the 
corpus. These frequencies provide a baseline that 
would be achieved by a simple classifier that al-
ways returned the most common class. 
 
 English German 
SDUs 8289 8719 
Domain Actions 972 1001 
Speech Acts 70 70 
Concept Sequences 615 638 
Vocabulary Size 1946 2815 
Table 4: Corpus Statistics. 
 
 English German 
DA (acknowledge) 19.2% 19.7% 
SA (give-information) 41.4% 40.7% 
Concept Sequence 
(No concepts) 
38.9% 40.3% 
Table 5: Most frequent DAs, SAs, and CSs. 
 
All of the results presented in this paper were 
produced using a 20-fold cross validation setup. 
The corpus was randomly divided into 20 sets of 
equal size. Each of the sets was held out as the 
test set for one fold with the remaining 19 sets 
used as training data. Within each language, the 
same random split was used for all of the classi-
fication experiments. Because the same split of 
the data was used for different classifiers, the 
results of two classifiers on the same test set are 
directly comparable. Thus, we tested for signifi-
cance using two-tailed matched pair t-tests. 
7.2 Machine Learning Approaches 
We evaluated the performance of four different 
machine-learning approaches on the DA classifi-
cation tasks: memory-based learning (k-Nearest-
Neighbor), decision trees, neural networks, and 
na?ve Bayes n-gram classifiers. We selected 
these approaches because they vary substantially 
in the their representations of the training data 
and their methods for selecting the best class. 
Our purpose was not to implement each ap-
proach from scratch but to test the approach for 
our particular task. Thus, we chose to use exist-
ing software for each approach ?off the shelf.? 
The ease of acquiring and setting up the software 
influenced our choice. Furthermore, the ease of 
incorporating the software into our online trans-
lation system was also a factor. 
Our memory-based classifiers were imple-
mented using TiMBL (Daelemans et al, 2002). 
We used C4.5 (Quinlan, 1993) for our decision 
tree classifiers. Our neural network classifiers 
were implemented using SNNS (Zell et al, 
1998). We used Rainbow (McCallum, 1996) for 
our na?ve Bayes n-gram classifiers. 
8 Experiments 
In our first experiment, we compared the per-
formance of the four machine learning ap-
proaches. Each SDU was parsed using the 
argument and pseudo-argument grammars de-
scribed above. The feature set for the DA and SA 
classifiers consisted of binary features indicating 
the presence or absence of labels from the 
grammars in the parse forest for the SDU. The 
feature set included 212 features for English and 
259 features for German. The concept sequence 
classifiers used the same feature set with the ad-
dition of the speech act. 
In the SA classification experiment, the 
TiMBL classifier used the IB1 (k-NN) algorithm 
with 1 neighbor and gain ratio feature weighting. 
The C4.5 classifier required at least one instance 
per branch and used node post-pruning. Both the 
TiMBL and C4.5 classifiers used the binary fea-
tures described above and produced the single 
best class as output. The SNNS classifier used a 
simple feed-forward network with 1 input unit 
for each binary feature, 1 hidden layer containing 
15 units, and 1 output unit for each speech act. 
The network was trained using backpropagation. 
The order of presentation of the training exam-
ples was randomized in each epoch, and the 
weights were updated after each training exam-
ple presentation. In order to simulate the binary 
features used by the other classifiers as closely as 
possible, the Rainbow classifier used a simple 
unigram model whose vocabulary was the set of 
labels included in the binary feature set. The 
setup for the DA classification experiment was 
identical except that the neural network had 50 
hidden units. 
The setup of the classifiers for the concept se-
quence classification experiment was very simi-
lar. The TiMBL and C4.5 classifiers were set up 
exactly as in the DA and SA experiments with 
one extra feature whose value was the speech act. 
The SNNS concept sequence classifier used a 
similar network with 50 hidden units. The SA 
feature was represented as a set of binary input 
units. The Rainbow classifier was set up exactly 
as in the DA and SA experiments. The SA fea-
ture was not included. 
As mentioned above, both experiments used a 
20-fold cross-validation setup. In each fold, the 
TiMBL, C4.5, and Rainbow classifiers were sim-
ply trained on 19 subsets of the data and tested 
on the remaining set. The SNNS classifiers re-
quired a more complex setup to determine the 
number of epochs to train the neural network for 
each test set. Within each fold, a cross-validation 
setup was used to determine the number of train-
ing epochs. Each of the 19 training subsets for a 
fold was used as a validation set. The network 
was trained on the remaining 18 subsets until the 
accuracy on the validation set did not improve 
for 50 consecutive epochs. The network was then 
trained on all 19 training subsets for the average 
number of epochs from the validation sets. This 
process was used for all 20-folds in the SA clas-
sification experiment. For the DA and concept 
sequence experiments, this process ran for ap-
proximately 1.5 days for each fold. Thus, this 
process was run for the first two folds, and the 
average number of epochs from those folds was 
used for training. 
 
 English German 
TiMBL 49.69% 46.51% 
C4.5 48.90% 46.58% 
SNNS 49.39% 46.21% 
Rainbow 39.74% 38.32% 
Table 6: Domain Action classifier accuracy. 
 
 English German 
TiMBL 69.82% 67.57% 
C4.5 70.41% 67.90% 
SNNS 71.52% 67.61% 
Rainbow 51.39% 46.00% 
Table 7: Speech Act classifier accuracy. 
 
 English German 
TiMBL 69.59% 67.08% 
C4.5 68.47% 66.45% 
SNNS 71.35% 68.67% 
Rainbow 51.64% 51.50% 
Table 8: Concept Sequence classifier accuracy. 
 Table 6, Table 7, and Table 8 show the aver-
age accuracy of each learning approach on the 
20-fold cross validation experiments for domain 
action, speech act, and concept classification re-
spectively. For DA classification, there were no 
significant differences between the TiMBL, 
C4.5, and SNNS classifiers for English or Ger-
man. In the SA experiment, the difference be-
tween the TiMBL and C4.5 classifiers for 
English was not significant. The SNNS classifier 
was significantly better than both TiMBL and 
C4.5 (at least p=0.0001). For German SA classi-
fication, there were no significant differences 
between the TiMBL, C4.5, and SNNS classifiers. 
For concept sequence classification, SNNS was 
significantly better than TiMBL and C4.5 (at 
least p=0.0001) for both English and German. 
For English only, TiMBL was significantly better 
than C4.5 (p=0.005). 
For both languages, the Rainbow classifier 
performed much worse than the other classifiers. 
However, the unigram model over arguments did 
not exploit the strengths of the n-gram classifica-
tion approach. Thus, we ran another experiment 
in which the Rainbow classifier was trained on 
simple word bigrams. No stemming or stop 
words were used in building the bigram models. 
 
 English German 
Domain Action 48.59% 48.09% 
Speech Act 79.00% 77.46% 
Concept Sequence 56.87% 57.77% 
Table 9: Rainbow accuracy with word bigrams. 
 
Table 9 shows the average accuracy of the 
Rainbow word bigram classifiers using the same 
20-fold cross-validation setup as in the previous 
experiments. As we expected, using word bi-
grams rather than parse label unigrams improved 
the performance of the Rainbow classifiers. For 
German DA classification, the word bigram clas-
sifier was significantly better than all of the pre-
vious German DA classifiers (at least p=0.005). 
Furthermore, the Rainbow word bigram SA clas-
sifiers for both languages outperformed all of the 
SA classifiers that used only the parse labels. 
Although the argument parse labels provide 
an abstraction of the words present in an SDU, 
the words themselves also clearly provided use-
ful information for classification, at least for the 
SA task. Thus, we conducted additional experi-
ments to examine whether combining parse and 
word information could further improve per-
formance. 
We chose to incorporate word information 
into the TiMBL classifiers used in the first ex-
periment. Although the SNNS SA classifier per-
formed significantly better than the TiMBL SA 
classifier for English, there was no significant 
difference for SA classification in German. Fur-
thermore, because of the complexity and time 
required for training with SNNS, we preferred 
working with TiMBL. 
We tested two approaches to adding word in-
formation to the TiMBL classifier. In both ap-
proaches, the word-based information for each 
fold was computed only based on the data in the 
training set. In our first approach, we added bi-
nary features for the 250 words that had the 
highest mutual information with the class. Each 
feature indicated the presence or absence of the 
word in the SDU. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 3 neighbors, and unweighted voting. The 
second approach we tested combined the Rain-
bow word bigram classifier with the TiMBL 
classifier. We added one input feature for each 
possible speech act to the TiMBL classifier. The 
value of each SA feature was the probability of 
the speech act computed by the Rainbow word 
bigram classifier. In this condition, we used the 
TiMBL classifier with gain ratio feature weight-
ing, 11 neighbors, and inverse linear distance 
weighted voting. 
 
 English German 
TiMBL + words 78.59% 75.98% 
TiMBL + Rainbow 81.25% 78.93% 
Table 10: Word+Parse SA classifier accuracy. 
 
Table 10 shows the average accuracy of the 
SA classifiers that combined parse and word in-
formation using the same 20-fold cross-
validation setup as the previous experiments. 
Although adding binary features for individual 
words improved performance over the classifiers 
with no word information, it did not allow the 
combined classifiers to outperform the Rainbow 
word bigram classifiers. However, for both 
languages, adding the probabilities computed by 
the Rainbow bigram model resulted in a SA clas-
sifier that outperformed all previous classifiers. 
The improvement in accuracy was highly signifi-
cant for both languages. 
We conducted a similar experiment for com-
bining parse and word information in the concept 
sequence classifiers. The first condition was 
analogous to the first condition in the combined 
SA classification experiment. The second condi-
tion was slightly different. A concept sequence 
can be broken down into a set of individual con-
cepts. The set of individual concepts is much 
smaller than the set of concept sequences (110 
for English and 111 for German). Thus, we used 
a Rainbow word bigram classifier to compute the 
probability of each individual concept rather than 
the complete concept sequence. The probabilities 
for the individual concepts were added to the 
parse label features for the combined classifier. 
In both conditions, the performance of the com-
bined classifiers was roughly the same as the 
classifiers that used only parse labels as features. 
 
 English German 
TiMBL + words 56.48% 54.98% 
Table 11: Word+Parse DA classifier accuracy. 
 
Table 11 shows the average accuracy of DA 
classifiers for English and German using a setup 
similar to the first approach in the combined SA 
experiment. In this experiment, we added binary 
features for the 250 words that the highest mu-
tual information with the class. We used a 
TiMBL classifier with gain ratio feature weight-
ing and one neighbor. The improvement in accu-
racy for both languages was highly significant. 
 
 English German 
TiMBL SA 
+ TiMBL CS 49.63% 46.50% 
TiMBL+Rainbow SA 
+ TiMBL CS 57.74% 53.93% 
Table 12: DA accuracy of SA+CS classifiers. 
 
Finally, Table 12 shows the results from two 
tests to compare the performance of combining 
the best output of the SA and concept sequence 
classifiers with the performance of the complete 
DA classifiers. In the first test, we combined the 
output from the TiMBL SA and CS classifiers 
shown in Table 7 and Table 8. The performance 
of the combined SA+CS classifiers was almost 
identical to that of the TiMBL DA classifiers 
shown in Table 6. In the second test, we com-
bined our best SA classifier (TiMBL+Rainbow, 
shown in Table 10) with the TiMBL CS classi-
fier. In this case, we had mixed results. The per-
formance of the combined classifiers was better 
than our best DA classifier for English and worse 
for German. 
9 Discussion 
One of our main goals was to determine the fea-
sibility of automatically classifying domain ac-
tions. As the data in Table 4 show, DA 
classification is a challenging problem with ap-
proximately 1000 classes. Even when the task is 
divided into subproblems of identifying the SA 
and concept sequence, the subtasks remain diffi-
cult. The difficulty is compounded by relatively 
sparse training data with unevenly distributed 
classes. Although the most common classes in 
our training corpus had over 1000 training exam-
ples, many of the classes had only 1 or 2 exam-
ples. 
Despite these difficulties, our results indicate 
that domain action classification is feasible. For 
SA classification in particular we were able to 
achieve very strong performance. Although per-
formance on concept sequence and DA classifi-
cation is not as high, it is still quite strong, 
especially given that there are an order of magni-
tude more classes than in SA classification. 
Based on our experiments, it appears that all of 
the learning approaches we tested were able to 
cope with data sparseness at the level found in 
our data, with the possible exception of the na?ve 
Bayes n-gram approach (Rainbow) for the con-
cept sequence task. 
One additional point worth noting is that there 
is evidence that domain action classification 
could be performed reasonably well using only 
word-based information. Although our best-
performing classifiers combined word and argu-
ment parse information, the na?ve Bayes word 
bigram classifier (Rainbow) performed very well 
on the SA classification task. With additional 
data, the performance of the concept sequence 
and DA word bigram classifiers could be ex-
pected to improve. Cattoni et al (2001) also ap-
ply statistical language models to DA 
classification. A word bigram model is trained 
for each DA, and the DA with the highest likeli-
hood is assigned to each SDU. Arguments are 
identified using recursive transition networks, 
and interlingua specification constraints are used 
to find the most likely valid interlingua represen-
tation. Although it is clear that argument infor-
mation is useful for the task, it appears that 
words alone can be used to achieve reasonable 
performance. 
Another goal of our experiments was to help 
in the selection of a machine learning approach 
to be used in our hybrid analyzer. Certainly one 
of the most important considerations is how well 
the learning approach performs the task. For SA 
classification, the combination of parse features 
and word bigram probabilities clearly gave the 
best performance. For concept sequence classifi-
cation, no learning approach clearly outper-
formed any other (with the exception that the 
na?ve Bayes n-gram approach performed worse 
than other approaches). However, the perform-
ance of the classifiers is not the only considera-
tion to be made in selecting the classifier for our 
hybrid analyzer. 
Several additional factors are also important 
in selecting the particular machine learning ap-
proach to be used. One important attribute of the 
learning approach is the speed of both classifica-
tion and training. Since the classifiers are part of 
a translation system designed for use between 
two humans to facilitate (near) real-time com-
munication, the DA classifiers must classify in-
dividual utterances online very quickly. 
Furthermore, since humans must write and test 
the argument grammars, training and batch 
classification should be fast so that the grammar 
writers can update the grammars, retrain the clas-
sifiers, and test efficiently. 
The machine learning approach should also 
be able to easily accommodate both continuous 
and discrete features from a variety of sources. 
Possible sources for features include words 
and/or phrases in an utterance, the argument 
parse, the interlingua representation of the argu-
ments, and properties of the dialogue (e.g. 
speaker tag). The classifier should be able to eas-
ily combine features from any or all of these 
sources. 
Another desirable attribute for the machine 
learning approach is the ability to produce a 
ranked list of possible classes. Our interlingua 
specification defines how speech acts and con-
cepts are allowed to combine as well as how ar-
guments are licensed by the domain action. 
These constraints can be used to select an alter-
native DA if the best DA violates the specifica-
tion. 
Based on all of these considerations, the 
TiMBL+Rainbow classifier, which combines 
parse label features with word bigram probabili-
ties, seems like an excellent choice for speech act 
classification. It was the most accurate classifier 
that we tested. Furthermore, the main TiMBL 
classifier meets all of the requirements discussed 
above except the ability to produce a complete 
ranked list of the classes for each instance. How-
ever, such a list could be produced as a backup 
from the Rainbow probability features. Adding 
new features to the combined classifier would 
also be very easy because TiMBL was the pri-
mary classifier in the combination. Finally, since 
both TiMBL and Rainbow provide an online 
server mode for classifying single instances, in-
corporating the combined classifier into an 
online translation system would not be difficult. 
Since there were no significant differences in the 
performance of most of the concept sequence 
classifiers, this combined approach is probably 
also a good option for that task. 
10 Conclusion 
We have described a representation of 
speaker intention that includes domain independ-
ent speech acts as well as domain dependent do-
main actions. We have shown that domain 
actions are a useful level of abstraction for ma-
chine translation of task-oriented dialogue, and 
that, in spite of their domain specificity, they are 
scalable to larger domains and portable to new 
domains.  
We have also presented classifiers for domain 
actions that have been comparatively tested and 
used successfully in the NESPOLE speech-to-
speech translation system. We experimentally 
compared the effectiveness of several machine-
learning approaches for classification of domain 
actions, speech acts, and concept sequences on 
two input languages. Despite the difficulty of the 
classification tasks due to a large number of 
classes and relatively sparse data, the classifiers 
exhibited strong performance on all tasks. We 
also demonstrated how the combination of two 
learning approaches could be used to improve 
performance and overcome the weaknesses of 
the individual approaches. 
Acknowledgements: NESPOLE was funded 
by NSF (Grant number 9982227) and the EU. 
The NESPOLE partners are ITC-irst, Universite 
Joseph Fourrier, Universitat Karlsruhe, APT 
Trentino travel board, and AETHRA telecom-
munications. We would like to acknowledge the 
contribution of the following people in particu-
lar: Fabio Pianesi, Emanuele Pianta, Nadia 
Mana, and Herve Blanchon. 
References 
Cattoni, R., M. Federico, and A. Lavie. 2001. Robust 
Analysis of Spoken Input Combining Statistical 
and Knowledge-Based Information Sources. In 
Proceedings of the IEEE ASRU Workshop, Trento, 
Italy. 
Daelemans, W., J. Zavrel, K. van der Sloot, and A. 
van den Bosch. 2002. TiMBL: Tilburg Memory 
Based Learner, version 4.3, Reference Guide. ILK 
Technical Report 02-10. Available from 
http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.
gz. 
Fillmore, C.J., Kay, P. and O'Connor, M.C. 1988. 
Regularity and Idiomaticity in Grammatical Con-
structions. Language, 64(3), 501-538. 
Gavald?, M. 2000. SOUP: A Parser for Real-World 
Spontaneous Speech. In Proceedings of IWPT-
2000, Trento, Italy. 
Goldberg, Adele E. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. 
Chicago University Press. 
Langley, C. and A. Lavie. 2003. Parsing Domain Ac-
tions with Phrase-Level Grammars and Memory-
Based Learners. To appear in Proceedings of 
IWPT-2003. Nancy, France. 
Langley, C., A. Lavie, L. Levin, D. Wallace, D. 
Gates, and K. Peterson. 2002. Spoken Language 
Parsing Using Phrase-Level Grammars and Train-
able Classifiers. In Workshop on Algorithms for 
Speech-to-Speech Machine Translation at ACL-02. 
Philadelphia, PA. 
Lavie, A., F. Metze, F. Pianesi, et al 2002. Enhancing 
the Usability and Performance of NESPOLE! ? a 
Real-World Speech-to-Speech Translation System. 
In Proceedings of HLT-2002. San Diego, CA. 
Levin, L., D. Gates, A. Lavie, A. Waibel. 1998. An 
Interlingua Based on Domain Actions for Machine 
Translation of Task-Oriented Dialogues. In Pro-
ceedings of ICSLP 98, Vol. 4, pages 1155-1158, 
Sydney, Australia. 
Levin, L., D. Gates, D. Wallace, K. Peterson, A. La-
vie F. Pianesi, E. Pianta, R. Cattoni, N. Mana. 
2002. Balancing Expressiveness and Simplicity in 
an Interlingua for Task Based Dialogue. In Pro-
ceedings of Workshop on Spoken Language Trans-
lation. ACL-02, Philadelphia. 
McCallum, A. K. 1996. Bow: A toolkit for statistical 
language modeling, text retrieval, classification and 
clustering. 
http://www.cs.cmu.edu/~mccallum/bow. 
Qu, Y., B. DiEugenio, A. Lavie, L. Levin and C.P. 
Rose. 1997. Minimizing Cumulative Error in Dis-
course Context. In Dialogue Processing in Spoken 
Language Systems: Revised Papers from ECAI-96 
Workshop, E. Maier, M. Mast and S. LuperFoy 
(eds.), LNCS series, Springer Verlag. 
Qu, Y., C. P. Rose, and B. DiEugenio. 1996. Using 
Discourse Predictions for Ambiguity Resolution. In 
Proceedings of COLING-1996. 
Quinlan, J. R. 1993. C4.5: Programs for Machine 
Learning. San Mateo: Morgan Kaufmann. 
Reithinger, N., R. Engel, M. Kipp, M. Klesen. 1996. 
Predicting Dialogue Acts for a Speech-To-Speech 
Translation System. DFKI GmbH Saarbruecken. 
Verbmobil-Report 151. 
http://verbmobil.dfki.de/cgi-
bin/verbmobil/htbin/doc-access.cgi 
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. 
Bates, D. Jurafsky, P. Taylor, R. Martin, M. 
Meteer, and C. Van Ess-Dykema. 2000. Dialogue 
Act Modeling for Automatic Tagging and Recogni-
tion of Conversational Speech. Computational Lin-
guistics 26:3, 339-371.  
Zell, A., G. Mamier, M. Vogt, et al 1998. SNNS: 
Stuttgart Neural Network Simulator User Manual, 
Version 4.2. 
