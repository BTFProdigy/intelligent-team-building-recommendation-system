Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 50?57,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Deeper spoken language understanding for man-machine dialogue on
broader application domains: a logical alternative to concept spotting
Jeanne Villaneau
UEB (Universite? Europe?enne de Bretagne)
VALORIA
France
villanea@univ-ubs.fr
Jean-Yves Antoine
Universite? Franc?ois Rabelais - Tours
LI
France
Jean-Yves.Antoine@univ-tours.fr
Abstract
LOGUS is a French-speaking spoken lan-
guage understanding (SLU) system which
carries out a deeper analysis than those
achieved by standard concept spotters. It
is designed for multi-domain conversa-
tional systems or for systems that are
working on complex application domains.
Based on a logical approach, the sys-
tem adapts the ideas of incremental ro-
bust parsing to the issue of SLU. The pa-
per provides a detailed description of the
system as well as results from two evalu-
ation campaigns that concerned all of cur-
rent French-speaking SLU systems. The
observed error rates suggest that our log-
ical approach can stand comparison with
concept spotters on restricted application
domains, but also that its behaviour is
promising for larger domains. The ques-
tion of the generality of the approach is
precisely addressed by our current inves-
tigations on a new task: SLU for an emo-
tional robot companion for young hospital
patents.
1 Introduction
Despite the indisputable advances of automatic
speech recognition (ASR), highly spontaneous
speech remains an important barrier to the wide
spreading of speech based applications. The goal
of spontaneous speech understanding remains fea-
sible, provided the interaction between the user
and the system is restricted to a task-oriented di-
alogue (restricted vocabulary). Present research is
investigating mixed or user initiated dialogue for
less restricted tasks. It is the purpose of this paper,
which focuses on spontaneous speech understand-
ing in such complex applications.
Generally speaking, information speech dia-
logue systems are based on the same architecture.
At first, a speech recognizer processes the speech
signal and provides a string (or a lattice) of words
that should correspond to the spoken sentence.
Then, this string is parsed by a spoken language
understanding module (SLU) in order to build a
semantic representation that represents its propo-
sitional meaning. Finally, this semantic structure
is sent to a dialogue manager which controls the
interaction with the user (database interrogation,
dialogue management, answer generation). The
answers to the user can be displayed on screen
and/or through a message generated by a text-to-
speech synthesis. This paper focuses on the SLU
module of such a dialogue system. On the whole,
SLU has to cope with two main difficulties:
? speech recognition errors: highly sponta-
neous speech remains hard to recognize for
current ASR systems (Zue et al, 2000).
Therefore, the SLU module has to work on
a strongly corrupted string of words.
? spoken disfluencies: filled pauses, repetitions
and repairs make the parsing of conversa-
tional spoken language significantly harder to
achieve (Heeman, Allen, 2001).
In order to overcome those difficulties, most SLU
systems follow a selective strategy which comes
down to a simple concept spotting: they restrict
the semantic analysis to a mapping of the sentence
with the main expectations of the user in relation
with the task (Minker W. et al, 1999; Bangalore S.
et al, 2006). Consider, for instance, an air trans-
port information system and the following spoken
utterance:
(1) Cou- could you list me the flights uh the
scheduled flights for Tenerife Tenerife Tenerife
North please
Satisfying the speaker?s goals only requires de-
tecting the nature of their requests (list flights) and
the required destination (Tenerife North). Those
50
two concepts (list, Tenerife North) will fill a shal-
low semantic frame which is supposed to repre-
sent the useful meaning of the sentence. Such
task-driven approaches meet, to a great extent, the
needs of SLU in terms of robustness, since they
only involve a partial analysis of the sentence.
Whether the processing is based on a statistical or
a knowledge-based approach, several evaluation
campaigns proved that concept spotting is suitable
for spoken language understanding, provided the
application task is sufficiently restricted. How-
ever, concept spotters suffer from noticeable limi-
tations:
? Although they resist gracefully speech recog-
nition errors, they are not able to detect their
eventual presence, since they do not consider
the global structure of the sentence. This lim-
itation can be particularly penalizing when
the error is related to a key element, for ex-
ample when the error prevents the system to
determine the type (dialogue act) of the ut-
terance. Indeed, concept spotters often base
SLU on the initial characterization of the
question type. When analyzing the errors
of his statistical concept spotter, Minker has
shown that the correct identification of the
question type is a key issue in terms of final
robustness (Minker W. et al, 1999).
? Since they are based on the identification of
rather flat semantic frames, these approaches
hardly succeed in representing complex syn-
tactic relations such as overlapping coordi-
nate phrases or negations.
? Although it is well known that generality is
an important issue for SLU, this question
is generally approached in term of technical
portability from one (narrow) task to another.
Now, one should wonder whether concept
spotting is still suitable on larger application
domains. It seems that the robustness of the
spotting process depends strongly on the de-
gree of lexical ambiguity of the considered
task. For instance, Bousquet has shown that
the concept error rate of her stochastic spot-
ter is two times higher on ambiguous words
than on non ambiguous ones (Bousquet et al,
2003).
Such considerations tend to show that to apply
concept spotting to more complex tasks could be
difficult. Such observations are well known (Zech-
ner K., 1998; Van Noord et al, 1999), and no-
ticeable attempts have already been done to reach
a deeper semantic analysis. However, statistical
or knowledge-based concept spotting remains the
prevailing paradigm in SLU, mainly because of
engineering motivations (quick and easy build-
ing). On the contrary, we have decided to de-
velop a SLU system (LOGUS1) which carries out
a complete analysis of the utterance while keep-
ing the robustness of standard concept spotting ap-
proaches. The system, which is based on a logi-
cal approach, adapts the ideas of incremental ro-
bust parsing (A??t-Mokhtar S., 2002; Basili, 2003)
to the issue of speech conversational systems. In
section 2, we will describe the system into de-
tail. Then, section 3 will present results from dif-
ferent evaluation campaigns in which we partici-
pated. These experiments concerned standard re-
stricted tasks (hotel reservation for instance) for
which concept spotting is well adapted. As a re-
sult, this section does not aim to prove a supe-
riority of our approach, but simply to show that
this deeper processing is able to keep a satisfac-
tory robustness, by comparison with prevailing ap-
proaches. Finally, we give in section 4 a brief de-
scription on our present work concerning the inte-
gration of LOGUS in a conversational robot which
is dedicated to general interaction with children
who are in hospital for a long-stay. This exam-
ple will illustrate the portability abilities of our ap-
proach on complex application tasks, in addition
with our previous works on general tourism infor-
mation.
2 Description of the LOGUS system
The task of a SLU is to turn a sequence or a graph
of words into a semantic representation; so a SLU
system has to perform a translation from natural
language to a formal target language. This section
begins with the description of the formal language
chosen for the LOGUS system. We then explain
the basic principles of parsing and its main steps.
2.1 Semantic representation
When it comes to the choice of a target language
for the system, the following points must be taken
into account.
? We want to implement automatic understand-
ing in application domains where predefined
1LOGical Understanding System.
51
semantic frames are not sufficient to repre-
sent all the possible queries (Van Noord et al,
1999). Furthermore, any SLU aims at pro-
viding results usable by a dialogue manager:
the target language must reconcile simplicity
with precision.
? This semantic representation must obviously
extend to a pragmatic one. That means that it
should involve the characterization of the di-
alogue acts related to the speech turn (Austin
J.-L., 1962).
We have chosen a formalism compatible with
these constraints and inspired by the illocutionary
logic of D. Vanderveken (Vanderveken D., 1981).
In this formalism, the form of an elementary illo-
cutionary act is F(P) where F is the illocutionary
force, and P its propositional content.
The LOGUS system thus provides a logical for-
mula as the semantic representation of an utter-
ance. A language act contains clues about the in-
tentions of the speaker: it is labelled illocutionary
force, while the propositional content is a structure
built with the domain objects and their properties
which is called an object string.
The following example shows a single speech
turn uttered for a tourism information system:
(2) j?ai re?serve? une chambre dans un deux
e?toiles l?ho?tel euh l?ho?tel Rex pour y aller d?ici
comment est-ce que je peux faire (I booked a room
in a two-star hotel in the hotel hum in the Rex hotel
from here how can I go at there)
This turn expresses two different language acts,
which is quite usual in conversational speech:
a piece of information (I booked a room...) is
followed by the user question (... how can I go....
Such complex speech turns are difficult to analyze
for concept spotters, since they usually base
the parsing on one language act detection. The
logical formula LOGUS provides is split into two
language acts: (information act) and (question
how). The second act is interpreted by the system
in the context of the first one:
((information act)
(of (reservation [])
(hotel [(ident. (name ?Rex?)),(star (int 2))])))
((question how)
(to go [(to (contextual location [])),
(from (hotel [(ident. (name ?Rex?))]))]))
In the formula, reservation, hotel and to go are
object labels; (ident. (name ?Rex?)), (star (int 2))
are properties. The two objects of labels reserva-
tion and hotel are linked with the generic relation
of, which indicates a subordination relation. It is
the main relation, (in addition with logical coordi-
nations and, or and not) which is used for building
complex object strings.
2.2 General system architecture
Incremental parsing methodology is used for text
parsing in order to combine efficiency with robust-
ness (A??t-Mokhtar S., 2002). With LOGUS, we
tried to show that such methods can be extended
to spoken language parsing.
The system has to parse out-of-grammar con-
structions but spoken language studies have shown
that minimal syntactic structures are generally pre-
served in repairs and false-starts (Mc Kelvie D.,
1998). We have thus chosen to carry out an
incremental bottom-up parsing, where words are
gradually combined. At the beginning, the parser
groups words according to mainly syntactic rules
in order to form minimal chunks that correspond
to basic concepts of the application domain. Then,
as word group size increases, their meaning be-
comes more precise, enough to relax syntactic cri-
teria and thereby overcome the problem of out-of-
grammar sentences.
The general architecture of the system is shown
in Figure 1. The parsing is essentially split into
three stages. The first stage is chunking (Ab-
ney S., 1991) where grammatical words are linked
to the lexical words to which they are referred.
The following stage gradually builds links be-
tween the chunks in order to detect semantic re-
lations between the corresponding concepts, and
the last one achieves a contextual interpretation
(anaphoric resolution for instance). The process
of building links between chunks and contextual
understanding uses a domain ontology.
Only one formalism is used during these pars-
ing stages. It is designed to distinguish syntax and
semantics and to preserve genericity of the pars-
ing rules. Each component is specified by a list
of what we can call definitions; each of them is a
triplet < C, R, T > where
C: is a syntactic label, called syntactic category:
for example adjective, (verb 1 present).
R: points out the semantic function of the compo-
nent. It is called semantic role: for example
object, (prop price) where prop is for prop-
erty.
52
? level 1
? level 2
? level 3
? semantic kernel
dependenciescontextual understanding
chunk dependencies
chunking
word sequence
domain
ontology
lexicon
logical formula
Figure 1: General architecture of the LOGUS sys-
tem
T: is the semantic translation. It is an element
of the logical formula built by the system. It
belongs thus to the target language.
The first two triplet elements, C and R, are
widely domain independent. A basic principle is
to define parsing rules from these elements in or-
der to preserve the genericity of the system. Each
parsing rule combines two or three triplets in order
to build a new result triplet.
2.3 Chunking
Our experiments with LOGUS have clearly shown
that chunking is effective for spoken language,
provided the chunks are very short: more pre-
cisely, errors made at the speech recognition level
make it dangerous to link objects or properties ac-
cording to pure syntactic criteria, without check-
ing these links with semantic criteria. Therefore
the chunks built by LOGUS include only one con-
tent word: we call them minimal chunks. Chunk-
ing is based on the principle of linking function
words to the near content word.
The formalism used in this step is inspired by
Categorial Grammars of the AB type2, whose
rules are generalized from the first two elements of
the constituent triplets. Function words have def-
initions in which syntactic category and semantic
role are fractional. In such definitions, the seman-
tic translation is a ?-abstraction (in the ?-calculus
meaning)3. The semantic translation of the re-
sult triplet is achieved by applying this abstrac-
tion to the semantic translation of the un-fractional
triplet. Formally, the following two rules are ap-
plied, where F is an abstraction:
< CA/CB, RA/RB, F >, < CB, RB, SB >
? < CA, RA, (F SB) >
< CB, RB, SB >,< CB\CA, RB\RA, F >
? < CA, RA, (F SB) >
2The formalism can be expressed in terms of pregroup
formalism too (Lambek J., 1999).
3LOGUS is implemented in ?Prolog, a logic programming
language whose terms are ?-terms with simple types.
In the following example only one definition is
shown for each component (gn is for nominal
group).
trois (three) e?toiles (stars)
C adj num adj num\gn
R (prop nb) (prop nb)\(prop nb star)
S (int 3) ?x.(star x)
By applying the second rule, we obtain the fol-
lowing chunk:
?trois e?toiles? (three stars)
<gn, (prop nb star), (star (int 3))>.
The semantic translation of the result triplet
is obtained by ?-reduction of the ?-term
(?x. (star x) (int 3)). For example, the utterance
(3) ? `A l?ho?tel Caumartin quels sont le les tar-
ifs pour pour une chambre double? (In Caumartin
hotel what are the the prices for for a double room)
is segmented into six chunks during the chunking
stage. Their semantic translations are:
[1] (hotel []),
[2] (identity (name ?Caumartin?))]),
[3] (what (interrogation)), [4] (price []),
[5] (room []), [6] (size double).
At the end of the chunking process, the deter-
miner le and the first occurrence of the preposition
pour are deleted because they are fragments with-
out semantic content. Deletions such as these are
a first way of dealing with repairs.
2.4 Domain ontology
The limited scope of the application domain
makes it possible to describe exhaustively the
pragmatic and semantic domain knowledge. A do-
main ontology specifies how objects and proper-
ties can be compounded. The handled processings
are expected to be generic while using a domain
dependent ontology: to achieve that, the ontology
is defined by generic predicates whose domain ob-
jects and domain properties are the arguments.
For example, the possibility of building the con-
ceptual relation of between two objects (cf. 2.1)
is defined by the predicate is sub object whose
arguments are two object labels: so the relation
is sub object(room, hotel) expresses a part-whole
relation possibility between such two objects.
2.5 Chunk dependencies
Chunk dependencies are built by an incremental
process which is compound of several successive
stages. Each stage is based on rewriting rules
53
which are specified from the first two components
of the constituent triplets and from the generic on-
tology predicates. They are thus not specific to the
domain of application, what assures, to a certain
extent, the genericity of the process.
Consider for instance the following rule, which
leads to the binding of two consecutive chunks
which share a meronomic (part of) relation:
< C1, object, O1 >, < C2, object, O2 >
- O1 simple object of label Et1
- O2 object string of label Et2
- is sub object(Et1, Et2)
< C, object, (of O1 O2) >
where C is obtained by composing C1 and C2.
As an illustration, this rule will form a com-
plex object (of (price []) (room [(size double)]))
from the initial two chunks (price []) and (room
(size double)). This rule is completely generic and
should apply on any task. The knowledge spe-
cific to the task intervenes only on the definition of
the predicate is sub object. As a result, one could
speak of procedural genericity to qualify our sys-
tem.
As long as possible, the first processing stages
try to respect syntactic criteria. However, in pres-
ence of spoken disfluencies or speech recogni-
tion errors, it is likely that the utterance is out-
of-grammar. Therefore, since the detected links
between chunks make the meaning of the linked
chunks more specific, the next stage tries to detect
chunk dependencies more on more on semantic or
pragmatic features only. Subsequently, studying
dependencies between the components makes it
possible to eliminate some components, especially
in the case of word recognition errors.
As an illustration, Figure 2 shows how links are
gradually built during the parsing stage of utter-
ance (3) (cf. section 2.3). The chunks are in rect-
angular boxes in dotted lines.
The first step of chunk binding links the first two
chunks into the object:
(hotel [(ident. (name ?Caumartin?))]).
The second step links the object (room []) with
the property (size (double)) to obtain the object
(room [(size double)]). Then, the two objects price
and room are linked with the conceptual relation of
to obtain (of (price []) (room [(size double)])) and
this object string is connected to the language act:
(question what). The position of the prepositional
phrase a` l?hotel Caumartin is not usual in French
syntagmatic ordering. It is indeed an example of
extraposition which is not accepted by the syn-
tactic constraints considered by the system. As a
result, the conceptual relation of, which links the
object of label room with the object (hotel [ident.
(name ?Caumartin?)]) is built later, when these
constraints are relaxed.
2.6 Contextual understanding
Many sentences are elliptical and incomplete in a
dialogue. Therefore, it is necessary to use the cur-
rent context of the task and the dialogue history
in order to complete their understanding. The ob-
jectives of the contextual understanding in LOGUS
are thus close to the objectives of the authors of the
OntoSem system (McShane M., 2005): the com-
pletion of semantic fragments. Reference resolu-
tion is thereby extended to a more general comple-
tion of the semantic representation.
While syntactic anaphora criteria are generally
respected in texts, anaphora gender and number
are frequently broken in spoken language. More-
over, gender and number morphological marks are
hardly perceptible in spoken French. They are
therefore very often corrupted by speech recogni-
tion errors. So, in the LOGUS system, anaphora
resolution is based on the same principles as the
rest of the parsing: combining syntactic and se-
mantic criteria. Both nominal and pronominal
anaphora (with definite expressions) are consid-
ered during this contextual interpretation stage.
Completion is based on the concept of object
string. A property or an object may be completed
by an ?over-object? of the context, if the ontology
makes it possible to do so. For example, the ob-
ject price of the sentence ?quel est le tarif? (what
is the price) is automatically completed in
(of (price []) (of (room []) (hotel [(name ?Rex?)]))
if the object string (of (room []) (hotel [(name
?Rex?)])) is an object string which is part of the
previous utterance.
3 Evaluations and results
LOGUS is a French-speaking system. It took part
in the two evaluation campaigns that were carried
out in the last year designed for French spoken
language understanding: the GDR-I3 challenge-
based campaign and the MEDIA project.
3.1 The GDR-I3 campaign
LOGUS took part in the challenge-based cam-
paign, held by the GDR-I3 consortium of the
54
question
what
[a l hotel] [Caumartin] [quels sont]
identity
[les tarifs]
price
[pour une
 chambre]
room
[double]
size
double
of
of
:level 3
: level2
name
"Caumartin"
pourand le and elimination of 
: level 1
hotel
processAfter chunking   
conceptchunk conceptual relation
Figure 2: Characterization of chunk dependencies : example on the utterance ? a` l?hotel Caumartin quels
sont le les tarifs pour pour une chambre double? (in Caumartin hotel what are the the prices for for a
double room.
French CNRS research agency (Antoine et al,
2002). We won?t describe here in detail the re-
sults of this campaign, since it concerned a for-
mer version of LOGUS. It seems however in-
teresting to analyse the distribution of the errors
made by LOGUS to have an idea of the benefits
of our approach. The evaluation corpus was di-
vided among several tests which were respectively
related to a specific difficulty: speech recognition
errors, speech repairs and other disfluences, and fi-
nally messages of a structural complexity (embed-
ded coordination or subordination, for instance)
significantly higher than those usually met in stan-
dard ATIS-like application domains.
The distribution of the concept error rates of the
LOGUS SLU system is the following:
Speech recognition: 9.5%
Complex structures: 9.8%
Repairs: 15%
It should be noted here that the robustness of
LOGUS decreases rather gracefully on complex
messages, while SLU systems based on concept
spotting meet real difficulties on such utterances.
For instance, Cacao (Bousquet-Vernhettes et al,
1999; Bousquet-Vernhettes et al, 2003) is a con-
cept spotter which participated to the GDR-I3
campaign. It has been shown that most of its er-
rors resulted from its difficulties to resolve lexical
ambiguities in complex sentences. This observa-
tion suggests that our logical deep parsing should
fulfill better than concept spotting the needs of
complex application domains such as general pur-
pose tourist information or collaborative plan-
ning (Allen J. et al, 2002), or even multi-domain
applications (Dzikovska M. et al, 2005). Unfortu-
natedly, French evaluation campaigns have never
investigated such difficult tasks.
3.2 The MEDIA project
MEDIA-EVALDA was an evaluation campaign
hold by the French Ministry of Research. It con-
cerned all the French laboratory working on SLU.
Once again, this evaluation investigated a rather
restricted application domain: hotel reservation.
It is well known that concept spotters fit succes-
fully such simple tasks. Nevertheless, we decided
to take part in this evaluation in order to see to
which extent LOGUS should be compared to stan-
dard concept spotters in such disavantageous con-
ditions.
Participants defined reservation scenarios which
were used to build a corpus made up of 1250
recorded dialogues. Recording used a WOZ sys-
tem simulating vocal tourist phone server (Dev-
illers et al, 2004). The MEDIA corpus, which
is made up of real-life French spontaneous dia-
logues, is surely to become a benchmark reference
for French contextual SLU.
The evaluation paradigm forced every partici-
pant to convert his own semantic representation
into a common reference, which relies-on an at-
tribute/value frame: each utterance is divided into
semantic segments, aligned on the sentence, and
each segment is represented by a triplet: (mode,
attribute, value). Relations between attributes are
represented by their order in the representation and
the composed attribute names.
Nine systems participated to this first campaign.
An error was count for any difference with one
of the elements of the reference (mode, attribute
55
System 1 2 3 4 (LOGUS) 5
Approach concept
spotting
concept
spotting
syntactic
deep parsing
logical
deep parsing
concept
spotting
Error rate 29.0% 30.3% 36.3% 37.8% 41.3%
Table 1: MEDIA results.
or value). Table 1 summarises the results of the
best five systems. At first glance, one should find
the reported error rates rather deceptive. How-
ever, one must realize that the test corpus involved
highly spontaneous conversational speech, with
very frequent speech disfluences. As a result,
these results should be compared, for instance,
to ASR errors rates observed on the SWITCH-
BOARD corpus (Greenberg S. et al, 2000).
LOGUS was ranked fourth and its robustness
was rather close to the best participants. Now,
if you consider that the systems ranked 1st, 2nd
and 5th were using a concept spotter, these re-
sults shows that our approach can bear compar-
ison with standard approaches even on this task.
These encouraging performances suggest that it is
possible to achieve a deep understanding of con-
versational speech while respecting at the same
time some robustness requirements: our approach
seems indeed competitive even in a domain where
concept spotters are known to be very efficient. To
our mind, the interest of our approach is that this
robustness should remain on larger application do-
mains. We are precisely trying to test this gener-
icity by adapting LOGUS to a wider application
domain in the framework of the Emotirob project.
4 Genericity and portability experiment
We are currently testing the portability of our
approach by adapting LOGUS to a really differ-
ent task, which corresponds to an unrestricted
application domain, general purpose understand-
ing of child language, with additional emotional
state detection. The whole project, supported by
ANR (National French Research Agency), aims
at achieving a robot companion which can inter-
act with sick or disabled young children with the
help of facial expressions. Although the robot
does not have to react to every speech act of the
child, we have to deal with spoken understanding
in an unrestricted domain. Fortunately, the age of
the children involved (3-5) implies a restricted vo-
cabulary. This work is still in progress. Our first
investigations suggest however that LOGUS is a
suitable understanding system for the pursued pur-
pose: since there will never be significant corpora
related to this kind of task, we can?t use statisti-
cal methods. Moreover, because of the generic-
ity of LOGUS, the main part of the analysis can
be reused without important changes. Thus, three-
month work was enough to build a first prototype
of the system and the problem is restricted to the
main problem of this project: building an ontology
which models the cognitive and emotional world
of young children.
The generality of the used formalism makes it
possible to include an emotional component by
turning the triplet structure into a quadruplet struc-
ture. Of course, composition rules have to in-
clude this new component. We are currently work-
ing on the computation of the emotional states
from both prosodic and lexical cues. Whereas
many works have investigated a prosodic-based
detection (Devillers et al, 2005), word-based ap-
proaches remain quite original. Our hypothesis is
that emotion is compositional, e.g. that is pos-
sible to compute the global emotion carried by a
sentence from the emotion of every content word.
This calculation depends obviously of the seman-
tic structure of the utterance: our system will
precisely benefit from the characterization of the
chunk dependencies carried on by LOGUS. For the
moment being, we are working on the definition of
a complete lexical norm of emotional values from
children of 3, 5 and 7 years. This norm will be
established in collaboration with psycholinguists
from Montpellier University, France.
5 Conclusion
When we started implementing the LOGUS sys-
tem, one of our objectives was to achieve robust
parsing of spontaneous spoken language while
making the application domain much wider than
is currently done. Logical formalisms are not usu-
ally viewed as efficient tools for pragmatic appli-
cations. The promising results of LOGUS show
that they can be brought into interesting new ap-
proaches.
56
Another objective was to have a rather generic
system, despite the use of a domain-based seman-
tic knowledge. We have fulfilled this constraint
through the definition of generic predicates as
well as generic rules working on semantic triplets
or quadruplets which makes it possible to have
generic chunk linking rules. The performances of
LOGUS show that a deeper understanding can bear
comparison with concept spotting approaches.
References
Abney S. 1991. Parsing by Chunks. Principle Based
Parsing. R. Berwick, S. Abney and C. Tenny Eds.
Kluwer Academix Publishers.
A??t-Mokhtar S., Chanod J.-P. and Roux C. 2002.
Robustness beyond Shallowness: Incremental Deep
Parsing. Natural Language Engineering, 8 (2-3):
p. 121?144.
Allen J. and Ferguson G. 2002. Human-Machine Col-
laborative Planning. Proc. of the 3rd International
NASA Workshop on Planning and Scheduling for
Space, Houston, TX.
Antoine J.-Y. et al 2002. Predictive and Ob-
jective Evaluation of Speech Understanding: the
?challenge? evaluation campaign of the I3 speech
workgroup of the french CNRS. Proceedings of
the LREC 2002, 3rd International Conference on
Language Resources and Evaluation, Las Palmas,
Spain.
Austin J.-L. 1962. How to do things with words. Ox-
ford.
Bangalore S., Hakkani-Tu?r D. and Tu?r G. 2006. Spe-
cial issue on Spoken Language Understanding in
Conversational Systems. Speech Communication.
48.
Basili R. and Zanzotto F.M. 2003. Parsing engineering
and empirical robustness. Natural Language Engi-
neering. 8 (2-3).
Bousquet-Vernhettes C., Bouraoui J.-L. and
Vigouroux N. 2003. Language Model Study for
Speech Understanding. Proc. Internationnal Work-
shop on Speech and Computer (SPECOM?2003) ,
Moscow, Russia, p. 205?208.
Bousquet-Vernhettes C., Privat R. and Vigouroux N.
2003. Error handling in spoken dialogue systems:
toward corrective dialogue. ISCA workshop on Er-
ror Handling in Spoken Dialogue Systems, Chteau-
d?Oex-Vaud, Suisse, p. 41?45.
Bousquet-Vernhettes C., Vigouroux N. and Pe?rennou
G. 1999. Stochastic Conceptual Model for Spo-
ken Language Understanding. Proc. Internationnal
Workshop on Speech and Computer (SPECOM?99) ,
Moscow, Russia, p. 71?74.
Devillers L. et al 2004. The French Evalda-Media
project: the evaluation of the understanding ca-
pabilities of Spoken Language Dialogue Systems.
Proceedings of the LREC 2004, 4rd International
Conference on Language Resources and Evaluation,
Lisboa, Portugal.
Devillers L., Vidrascu, L. and Lamel, L. 2005. Chal-
lenges in real-life emotion annotation and machine
learning based detection. Neural Networks, 18, p.
407-422.
Dzikovska M., Swift M. and Allen J. and de Beau-
mont W. 2005. Generic parsing for multi-domain
semantic interpretation. Proc. 9th International
Workshop on Parsing Technologies (IWPT05)), Van-
couver BC.
Greenberg S. and Chang, S. 2000. Linguistic dissec-
tion of switchboard-corpus automatic speech recog-
nition systems. Proc. ISCA Workshop on Automatic
Speech Recognition: Challenges for the New Mil-
lennium, Paris, France.
Heeman P. and Allen J. 2001. Improving robustness
by modeling spontaneous events. Robustness in lan-
guage and speech technology, Kluwer Academics.
Dordrecht, NL. p. 123?152.
Lambek J. 1999. Type grammars revisited. Logical
Aspects of Computational Linguistics, A. Lecomte,
F. Lamarche and G. Perrier (eds), LNAI 1582,
Springer, Berlin, p. 1?27.
Mc Kelvie D. 1998. The syntax of disfluency in spon-
taneous spoken language. HCRC Research Paper,
HCRC/RP-95.
McShane M. 2005. Semantics-based resolution of
fragments and underspecified structures. Traitement
Automatique des Langues, 46(1): p. 163?184.
Minker W., Waibel A. and Mariani J.. 1999. Stochas-
tically based semantic analysis. Kluwer Ac., Ams-
terdam, The Netherlands.
Vanderveken D. 2001. Universal Grammar and
Speech act Theory. Essays in Speech Act The-
ory. Eds J. Benjamin, D. Vanderveken and S. Kubo,
p. 25?62.
van Noord G., Bouma G. and Koeling R. and Nederhof
M. 1999. Robust grammatical analysis for spoken
dialogue systems. Natural Language Engineering.
5(1): p. 45?93.
Zechner K. 1998. Automatic construction of frame
representations for spontaneous speech in unre-
stricted domains. COLING-ACL?1998. Montreal,
Canada. p. 1448?1452.
Zue V., Seneff S., Glass J., Polifrini J., Pao C., Hazen
T.J. and Hetherington L. 2000. Jupiter: a telephone-
based conversational interface for weather informa-
tion. IEEE Transactions on speech and audio pro-
cessing. 8(1).
57
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 550?559,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Weighted Krippendorff's alpha is a more reliable metrics for multi-
coders ordinal annotations: experimental studies on emotion, opinion 
and coreference annotation  
Jean-Yves Antoine 
Universit? Fran?ois Rabelais de 
Tours, LI (EA 6300) 
Blois, France 
Jean-Yves.Antoine@univ-tours.fr 
Jeanne Villaneau 
Universit? Europ?enne de  
Bretagne, IRISA 
Lorient, France 
Jeanne.Villaneau@univ-ubs.fr 
Ana?s Lefeuvre 
Universit? Fran?ois Rabelais 
de Tours, LI (EA 6300) 
Blois, France 
anais.lefeuvre@univ-tours.fr 
 
 
Abstract 
The question of data reliability is of first im-
portance to assess the quality of manually an-
notated corpora. Although Cohen?s ? is the 
prevailing reliability measure used in NLP, al-
ternative statistics have been proposed. This 
paper presents an experimental study with four 
measures (Cohen?s ?, Scott?s pi, binary and 
weighted Krippendorff ? s ?) on three tasks: 
emotion, opinion and coreference annotation. 
The reported studies investigate the factors of 
influence (annotator bias, category prevalence, 
number of coders, number of categories) that 
should affect reliability estimation. Results 
show that the use of a weighted measure re-
stricts this influence on ordinal annotations. 
They suggest that weighted ? is the most reli-
able metrics for such an annotation scheme. 
1 Introduction 
The newly intensive use of machine learning 
techniques as well as the need of evaluation data 
has led Natural Language Processing (NLP) to 
develop large annotated corpora. The interest for 
such enriched language resources has reached 
domains (semantics, pragmatics, affective com-
puting) where the annotation process is highly 
affected by the coders subjectivity. The reliabil-
ity of the resulting annotations must be trusted by 
measures that assess the inter-coders agreement. 
While medecine, psychology, and more gener-
ally content analysis, have considered for years 
the issue of data reliability, NLP has only inves-
tigated this question from the mid 1990s. The 
influential work of Carletta (1996) has led the ? 
statistic (Cohen, 1960) to become the prevailing 
standard for measuring the reliability of corpus 
annotation. Many studies have however ques-
tioned the limitations of the ? statistic and have 
proposed alternative measures of reliability. 
Krippendorff claims that ?popularity of ? not-
withstanding, Cohen?s ? is simply unsuitable as 
a measure of the reliability of data? in a paper 
presenting his ? coefficient (Krippendorff, 
2008).  
Except for some rare but noticeable studies 
(Arstein and Poesio, 2005), most of these critical 
works restrict to theoretical issues about chance 
agreement estimation or limitations due to vari-
ous statistical biases (Arstein and Poesio, 2008). 
On the opposite, this paper investigates experi-
mentally these questions on three different tasks: 
emotion, opinion and coreference annotation. 
Four measures of reliability will be considered: 
Cohen?s ? (Cohen, 1960), Scott?s pi (Scott, 1955) 
and two measures of Krippendorff?s ? (Krippen-
dorff, 2004) with different distance.  
 Section 2 gives a comprehensive presentation 
of these metrics. Section 3 details the potential 
methodological biases that should affect the reli-
ability estimation. In section 4, we explain the 
methodology we followed for this study. Lastly, 
experimental results are presented in section 5. 
2 Reliability measures 
Any reliability measure considers the most perti-
nent criterion to estimate data reliability to be 
reproducibility. Reproducibility can be estimated 
by observing the agreement among independent 
annotators (Krippendorff, 2004): the more the 
coders agree on the data they have produced, the 
more their annotations are likely to be repro-
duced by any other set of coders.  
Pure observed agreement is not considered as 
a good estimator since it does not give any ac-
count to the amount of chance that yields to this 
agreement. For instance, a restricted number of 
coding categories should favor chance agree-
ment. What must be estimated is the proportion 
of observed agreement beyond the one that is 
expected by chance: 
(1)  Measure = 
e
eo
A
AA
?
?
1
 
550
where Ao is the observed agreement between 
coders and Ae is an estimation of the possible 
chance agreement. Reliability metrics differ by 
the way they estimate this chance agreement. 
Cohen?s ? (Cohen, 1960) defines chance as 
the statistical independence of the use of coding 
categories by the annotators. It postulates that 
chance annotation is governed by prior distribu-
tions that are specific to each coder (annotator 
bias). ? was originally developed for two coders 
and nominal data. (Davies and Fleiss, 1982) has 
proposed a generalization to any number of cod-
ers, while (Cohen, 1968) has defined a weighted 
version of the ? measure that fulfils better the 
need of reliability estimation for ordinal annota-
tions: the disagreement between two ordinal an-
notations is no more binary, but depends on a 
Euclidian distance. This weighted generalization 
restricts however to a two coders scheme (Art-
stein and Poesio, 2008): a weighted version of 
the multi-coders ? statistics is still missing. 
Unlike Cohen?s ?, Scott?s pi (Scott, 1955) 
does not aim at modelling annotator bias. It de-
fines chance as the statistical independence of 
the data and the set of coding categories, inde-
pendently from the coders. It considers therefore 
the annotation process and not the behaviour of 
the annotators. Scott?s original proposal con-
cerned only two coders. (Fleiss 1971) gave a 
generalisation of the statistics to any number of 
coders through a measure of pairwise agreement.  
Krippendorff?s ? (Krippendorff, 2004) con-
siders chance independently from coders like 
Scott?s pi, but data reliability is estimated de-
pending on disagreement instead of agreement: 
(2)  Alpha  = 
e
oe
D
DD ?
 
where Do is the observed disagreement be-
tween coders and De is an estimation of the pos-
sible chance disagreement. Another original as-
pect of this metrics is to allow disagreement es-
timation between two categories through any 
distance measure. This implies that ? handles 
directly any number of coders and any kind of 
annotation (nominal or ordinal coding scheme). 
In this paper, we will consider the ? statistics 
with a binary as well as a Euclidian distance, in 
order to assess separately the influence of the 
distance measure and the metrics by itself. 
3 Quality criteria for reliability metrics 
There is an abundant literature about the criteria 
of quality a reliability measure should satisfy 
(Hayes, 2007). These works emphasize on two 
important points: 
? A trustworthy measure should provide sta-
ble results: measures must be reasonably 
independent of any factor of influence. 
? The magnitude of the measure must be in-
terpreted in terms of absolute level of reli-
ability: the statistics must come up with 
trustworthy reliability thresholds. 
These questions have mainly been investigated 
from a theoretical point of view. This section 
summarizes the main conclusions that should be 
drawn from these critical studies.  
3.1 Annotator bias and number of coders 
Annotator bias refers to the influence of the idio-
syncratic behavior of the coders. It can be esti-
mated by a bias index which measures the extent 
to which the distribution of categories differs 
from one coder?s annotation to another (Sim and 
Wright, 2005). Annotator bias has an influence 
on the magnitude of the reliability measures 
(Feinstein and Cicchetti,1990). Besides, it con-
cerns the invariance of the measures to the per-
mutation or selection of annotators but also to the 
number of coders. A review of the literature 
shows that theoretical studies on annotator bias 
are not convergent. In particular, opposite argu-
ments have been proposed concerning Cohen?s ? 
(Di Eugenio and Glass 2004, Arstein and Poesio 
2008, Hayes, 2007). This is why we have carried 
on experiments that investigate: 
?  to what extent measures depend on the se-
lection of a specific set of coders (? 5.3), 
?  to what extent the stability of the measures 
depends on the number of coders (? 5.4). 
Arstein and Poesio (2005) have shown 
that the greater the number of coders is, 
the lower the annotator bias decreases. 
Our aim is to go further this conclusion: 
we will study whether one measure needs 
fewer coders than another one to converge 
towards an acceptable annotator bias. 
3.2 Category prevalence 
Prevalence refers to the influence on reliability 
estimation of a coding category under which a 
disproportionate amount of annotated data falls. 
It can be estimated by a prevalence index which 
measures the frequency differences of categories 
on cases where the coders agree (Sim and 
Wright, 2005). When the prevalence index is 
551
high, chance-corrected measures are spuriously 
reduced since chance agreement is higher in this 
situation (Brennan and Sliman, 1992; Di Eugenio 
and Glass, 2004). This yields some authors to 
propose corrected coefficients like the PABAK 
measure (Byrt and al., 1993), which is a preva-
lence adjusted and annotator bias adjusted ver-
sion of Cohen?s ?. The influence of prevalence 
will not be investigated here, since no category is 
significantly prevalent in our data. 
3.3 Number of coding categories 
The number of coding categories has an influ-
ence on the reliability measures magnitude: the 
larger the number of categories is, the less the 
coders have a chance to agree. Even if this de-
crease should concern chance agreement too, 
lower reliability estimations are observed with 
high numbers of categories (Brenner and 
Kliebsch, 1996). This paper investigates this in-
fluence by comparing reliability values obtained 
with a 3-categories and a 5-categories coding 
scheme applied on the same data (see ? 5.1). 
3.4  Interpreting the magnitude of meas-
ures in terms of effective reliability 
One last question concerns the interpretation of 
the reliability measures magnitude. It has been 
particularly investigated with Cohen?s ?. Carletta 
(1996) advocates 0.8 to be a threshold of good 
reliability, while a value between 0.67 and 0.8 is 
considered sufficient to allow tentative conclu-
sion to be drawn. On the opposite, Krippendorff 
(2004b) claims that this 0.67 cutoff is a pretty 
low standard while Neuendorf (2002) supports 
an even more restrictive interpretation.  
Thus, the definition of relevant levels of reli-
ability remains an open problem. We will see 
how our experiments should draw a methodo-
logical framework to answer this crucial issue. 
4 Experiments: methodology  
4.1 Introduction 
We have conducted experiments on three dif-
ferent annotation tasks in order to guarantee an 
appreciable generality of our findings. The first 
two experiments correspond to an ordinal anno-
tation. They concern the affective dimension of 
language (emotion and opinion annotation). They 
have been conducted with na?ve coders to pre-
serve the spontaneity of judgment which is 
searched for in affective computing. 
The third experiment concerns coreference 
annotation. It is a nominal annotation that has 
been designed to be used as a comparison with 
the previous ordinal annotations tasks. 
The corresponding annotated corpora are 
available (TestAccord database) on the french 
Parole_Publique1 corpus repository under a CC-
BY-SA Creative Commons licence. 
4.2 Emotion corpus 
Emotion annotation consists in adding emo-
tional information to written messages or speech 
transcripts. There is no real consensus about how 
an emotion has to be described in an annotation 
scheme. Two main approaches can be found in 
the literature. On the one hand, emotions are 
coded by affective modalities (Scherer, 2005), 
among which sadness, disgust, enjoyment, fear, 
surprise and anger are the most usual (Ekman, 
1999; Cowie and Cornelius, 2003). On the other 
hand, an ordinal classification in a multidimen-
sional space is considered. Several dimensions 
have been proposed among which three are pre-
vailing (Russell, 1980): valence, intensity and 
activation. Activation distinguishes passive from 
active emotional states. Valence describes 
whether the emotional state conveyed by the text 
is positive, negative or neutral. Lastly, intensity 
describes the level of emotion conveyed.  
Whatever the approach, low to moderate inter-
annotator agreements are observed, what ex-
plains that reference annotation must be achieved 
through a majority vote with a significant num-
ber of coders (Schuller and al. 2009). Inter-coder 
agreement is particularly low when emotions are 
coded into modalities (Devillers and al., 2005; 
Callejas and Lopez-Cozar, 2008). This is why 
this study focuses on an ordinal annotation. 
Our works on emotion detection (Le Tallec 
and al., 2011) deal with a specific context: affec-
tive robotics. We consider an affective multimo-
dal interaction between hospitalized children and 
a companion robot. Consequently, this experi-
ment will concern a child-dedicated corpus. Al-
though many works already focused on child 
language (MacWhinney, 2000), no emotional 
child corpus is currently available in French, our 
studied language. We have decided to create a 
little corpus (230 sentences) of fairy tales, which 
are regularly used in works related to child affect 
analysis (Alm and al., 2005; Volkova and al., 
2010). The selected texts come from modern 
fairy tales (Vassallo, 2004; Vanderheyden, 1995) 
which present the interest of being quite confi-
dential. This guarantees that the coders discover 
                                                 
1
 www.info.univ-tours.fr/~antoine/parole_publique 
552
the text during the annotation. We asked 25 sub-
jects to characterize the emotional value con-
veyed by every sentence through a 5-items scale 
of values, ranging from very negative to very 
positive. 
As shown on Table 1, this affective scale en-
compasses valence and intensity dimensions. It 
enables to compare without methodological bias 
an annotation with 3 coding categories (valence: 
negative, positive, neutral) and the original 5-
categories (valence+intensity) annotation. 
A preliminary experiment showed us that 
children meet difficulties to handle a 5-values 
emotional scale. This is why the annotation was 
conducted on the fairy tales corpus with adults 
(11 men/14 women; average age: 31.6 years). All 
the coders have a superior level of education (at 
least, high-school diploma), they did not know 
each other and worked separately during the an-
notation task. Only four of them had a prior ex-
perience in corpus annotation. 
 
Value Meaning Valence / 
Polarity 
Intensity / 
Strength 
-2 very negative negative strong 
-1 moderately 
negative 
negative moderate 
0 no emotion neutral none 
1 moderately 
positive 
positive moderate 
2 very positive positive strong 
 
Table 1. emotion or opinion annotation schemes 
 
The coders were not trained but were given 
precise annotation guidelines providing some 
explanations and examples on the emotional val-
ues they had to use. They achieved the annota-
tion once, without any restriction on time. They 
had to rely on their own judgment, without con-
sidering any additional information. Sentences 
were given in a random order to investigate an 
out-of-context perception of emotion. We con-
ducted a second experiment where the order of 
the sentences followed the original fairy tale, in 
order to study the influence of the discourse con-
text. The criterion of data significance ? at least 
five chance agreements per category ? proposed 
by (Krippendorff, 2004) is greatly satisfied for 
the valence annotation (3 categories). It is ap-
proached on the complete annotation where we 
can assure 4 chance agreements per category. 
4.3 Opinion corpus 
The second experiment concerns opinion an-
notation. Emotion detection can be related to a 
certain extent, with opinion mining (or sentiment 
analysis), whose aim is to detect the attitude of 
people in the texts they produce. A basic task in 
opinion mining consists in classifying the polar-
ity of a given text, which should be either a sen-
tence (Wilson and al., 2005), a speech turn or a 
complete document (Turney, 2002). Polarity 
plays the same role as valence does for affect 
analysis: it describes whether the expressed 
judgment is positive, negative, or neutral. One 
should also characterize the sentiment strength 
(Thelwall and al., 2010). This feature can be re-
lated to the notion of intensity used in emotional 
annotation. Both polarity and sentiment strength 
are considered in our annotation task. 
This experiment has been carried out on a cor-
pus of film reviews. The reviews were relatively 
short texts written by ordinary people on dedi-
cated French websites (www.senscritique.com 
and www.allocine.fr). They concerned the same 
French movie. The corpus contains 183 sen-
tences. Its annotation was conducted by the 25 
previous subjects. The methodology is identical 
to the emotion annotation task. The subjects were 
asked to qualify the opinion that was conveyed 
by every sentence of the reviews by means of  
the same scale of values (Table 1). This scale 
encompasses this time the polarity and sentiment 
strength dimensions. Once again, the sentences 
were given in a random order and contextual or-
der respectively. The criterion of data signifi-
cance is satisfied here too. 
On both annotations, experiments with the 
random or the contextual order give similar re-
sults. Results from the contextual annotation will 
be given only when necessary. 
4.4 Coreference corpus 
The last experiment concerns coreference an-
notation. We have developed an annotated cor-
pus (ANCOR) which clusters various types of 
spontaneous and conversational speech. With a 
total of 488,000 lexical units, it is one of the 
largest coreference corpora dedicated to spoken 
language (Muzerelle and al. 2014). Its annotation 
was split into three successive phases: 
? Entity mentions marking, 
? Referential relations marking, 
? Referential relations characterization 
The experiment described in this paper con-
cerns the characterization of the referential rela-
tions. This nominal annotation consists in classi-
fying relations among five different types: 
553
? Direct coreference (DIR) ? Coreferent 
mentions are NPs with same lexical heads. 
? Indirect coreference (IND) ? These men-
tions are NPs with distinct lexical heads. 
? Pronominal anaphora (PRO) ? The subse-
quent coreferent mention is a pronoun. 
? Bridging anaphora (BRI) ? The subse-
quent mention does not refer to its antece-
dent but depends on it for its referential in-
terpretation (example: meronymy). 
? Bridging pronominal anaphora (BPA) ? 
Bridging anaphora where the subsequent 
mention is a pronoun. This type empha-
sizes metonymies (example: Avoid Cen-
tral Hostel? they are unpleasant) 
The subjects (3 men / 6 women) were adult 
people (average age: 41.2 years) with a high pro-
ficiency in linguistics (researchers in NLP or cor-
pus linguistics). They know each other but 
worked separately during the annotation, without 
any restriction on time. They are considered as 
experts since they participated to the definition 
of the annotation guide. The study was con-
ducted on an extract of 10 dialogues, represent-
ing 384 relations. Krippendorff?s (2004) criterion 
of significance is therefore satisfied here too. 
4.5 Reliability measures 
The experiments have been conducted with four 
chance-balanced reliability measures2 : 
? Multi-? : multiple coders/binary distance 
Cohen?s ?  (Davies and Fleiss, 1982),  
? Multi-pi : multiple coders/binary distance 
Scott?s pi  (Fleiss, 1971),  
? ?b : Krippendorff?s ? with binary distance, 
? ? : standard Krippendorff?s ? with a 1-
dimension Euclidian distance. 
The use of Euclidian distance is unfounded on 
coreference which handles a nominal annotation. 
Thus, ? will not be computed on this last corpus. 
                                                 
2
 Experiments were also conducted with Cronbach??c 
?(Cronbach, 1951). This metrics is based on a correlation 
measure. Krippendorff (2009) considers soundly that corre-
lation coefficients are inappropriate to estimate reliability. 
Our results show that ?c is systematically outperformed by 
the other metrics. In particular, it is highly dependent to 
coder bias. For instance we observed a relative standard 
deviation of ?c measures higher than 22% when measuring 
the influence of coders set permuation (? 5.3, table 5). This 
observation discards Cronbach??c ?as a trustworthy measure. 
5 Results   
5.1 Influence of the number of categories 
Our affective coding scheme enables a direct 
comparison between a 3-classes (valence or po-
larity) and a 5-classes annotation. The 3-classes 
scheme clusters the coding categories with the 
same valence or polarity. For instance {-2,-1} 
negative values are clustered in the same cate-
gory which receive the index 1. For the computa-
tion of the weighted ?, the distance between 
negative (-1) and positive (1) classes will be 
equal to 2. Table 2 presents the reliability meas-
ures observed on all of the corpora. 
 
Corpus Emotion (fairy tales) 
Metric M-? M-pi ?b ? 
3-classes 0.41 0.41 0.41 0.57   
5-classes 0.29 0.29 0.29 0.57 
Abs. diff. 0.12 0.12 0.12 0.0 
Corpus Opinion (film reviews) 
Metric M-? M-pi ?b ? 
3-classes 0.58 0.58 0.58 0.75 
5-classes 0.45 0.45 0.45 0.80 
Abs. diff. 0.13 0.13 0.13 0.05 
Corpus Coreference (spoken dialogues) 
Metric M-? M-pi ?b ? 
5-classes 0.69 0.69 0.69 n.s. 
 
Table 2. Reliability measures: emotion and opinion 
random annotation as well as coreference annotation 
 
Several general conclusions can be drawn 
from these figures. At first, low inter-coder 
agreements are observed on affective annotation, 
which is coherent with many other studies (Dev-
illers and al., 2005; Callejas and Lopez-Cozar, 
2008). Non-weighted metrics (multi-?, multi-pi, 
?b) range from 0.29 to 0.58, depending on the 
annotation scheme. This confirms that these an-
notation tasks are prone to high subjectivity. 
Higher levels of agreement may have been ob-
tained if the annotators were trained with super-
vision. As said before, this would have reduced 
the spontaneity of judgment. Furthermore, a 
comprehensive meta-analysis (Bayerl and Paul, 
2011) has shown that no difference may be found 
on data reliability between experts and novices. 
The reliability measures given by the weighted 
version of Krippendorff?s ? on the two affective 
tasks are significantly higher: ? values range 
from 0.57 to 0.80, which suggests a rather suffi-
cient reliability. These results are not an artifact. 
They come from better disagreement estimation. 
For instance, the difference between a positive 
554
and a negative annotation is more serious than 
between the positive and the neutral emotion, 
what a weighted metrics accounts for.  
Satisfactory measures are found on the con-
trary on the coreference task (0.69 with every 
metric). This result was expected, since a large 
part of the annotation decisions are based on ob-
jective (syntactic or semantic) considerations.  
Whatever the experiment you consider, multi-
?, multi-pi and ?b coefficients present very close 
values (identical until the 3rd decimal). A similar 
observation was made by (Arstein and Poesio, 
2005) with 18 coders. This validates the theoreti-
cal hypothesis on the convergence of individual-
distribution and single-distribution measures 
when the number of coders increases. Our ex-
periments show that annotator bias is moderate 
with 25 coders when inter-coders agreement is 
rather low (affective tasks), while 9 coders are 
enough to guarantee a low annotator bias when 
data reliability is higher (coreference task). 
Lastly, the comparison between the two anno-
tation schemes (3 or 5 classes) in affective tasks 
provides some indications on the influence of the 
number of coding categories on reliability esti-
mation3. As expected (see ? 3.3), multi-?, multi-pi 
and ?b values increase significantly when the 
number of classes decreases.  
On the contrary, weighted ? is significantly 
less affected by the increase of the number of 
categories. The ? value remains unchanged on 
the emotional corpus and its variation restricts to 
0.05 on the opinion task. It seems that the use of 
a Euclidian distance counterbalances the higher 
risk of disagreement when the number of catego-
ries grows. Such an independence of the number 
of coding categories is an interesting property for 
a reliability measure, which has never been re-
ported as far as we know. 
 
Metric M-? M-pi ?b ? 
3-classes 0.61 0.61 0.61 0.78 
5-classes 0.49 0.49 0.49 0.83 
Abs. diff. 0.12 0.12 0.12 0.05 
 
Table 3. Reliability measures with 3 and 5 annotation 
classes: opinion contextual annotation (film reviews). 
 
Finally, Table 3 presents as an illustration the 
reliabilities measures we obtained with the con-
textual annotation of the opinion corpus. These 
                                                 
3
 The 3-classes coding scheme is a semantic reduction of the 
5-classes one. One should wonder whether the same results 
can be observed with unrelated categories. (Chu-Ren and 
al., 2002) shows indeed that expanding PoS tags with sub-
categories does not increase categorical ambiguity. 
results are fully coherent with the previous ones. 
One should note in addition that reliability meas-
ures are significantly higher on these contextual 
annotations: the context of discourse helps the 
coders to qualify opinions more objectively. 
5.2 Influence of prevalence 
Table 4 presents the distribution of the annota-
tions on the three corpora. (Devillers and al., 
2005; Callejas and Lopez-Cozar, 2008) reported 
that more than 80% of the speech turns are clas-
sified as neutral in their emotional corpora. This 
prevalence was not found on our affective cor-
pora. Positive annotations are nearly as frequent 
as the neutral ones on the emotion task. This ob-
servation is due to the deliberate emotional na-
ture of fairy tales. Likewise, the neutral opinion 
is minority among the film reviews, which aim 
frequently at expressing pronounced judgments. 
Positive opinions are slightly majority on the 
opinion corpus but this prevalence is limited: it 
represents an increase of only 50% of frequency, 
by comparison with a uniform distribution.  
 
Corpus Emotion (fairy tales) 
5-classes 
?2 ?1 0 1 2 
Distribution 8% 17% 38% 23%   14% 
3-classes Negative neutral Positive 
Distribution 25% 38% 37% 
Corpus Opinion (film reviews) 
5-classes -2 -1 0 1 2 
Distribution 15% 21% 14% 26% 25% 
3-classes negative neutral positive 
Distribution 36% 14% 51% 
Corpus Coreference (spoken dialogues) 
5-classes DIR IND PRO BRI BPA 
Distribution 40% 7% 42% 10% 1% 
 
Table 4. Distribution of the coding categories  
 
In the coreference corpus, two classes are 
highly dominant, but they are not prevalent 
alone. There is no indication in the literature that 
the prevalence of two balanced categories has a 
bias on data reliability measure. For all these rea-
sons, we didn't investigate the influence of preva-
lence. Besides, relevant works are questioning 
the importance of the influence of prevalence on 
inter-coders agreement measures (Vach, 2005). 
5.3 Influence of coders set permutation 
?a coefficient for assessing the reliability of data 
must treat coders as interchangeable (Krippen-
dorff, 2004b). We have studied the stability of 
reliability measures computed on any combina-
tion of 10 coders (among 25) on the affective 
corpora, and 4 coders (among 9) on the corefer-
555
ence corpus. The influence of permutation is 
quantified by a measure of relative standard de-
viation (e.g. related to the average value) among 
the sets of coders (Table 5).   
 
Corpus Emotion (fairy tales) 
Metric M-? M-pi ?b ? 
3-classes 7.4% 7.7% 7.6% 6.2%   
5-classes 9.0% 9.1% 9.1% 6.1%   
Corpus Opinion (film reviews) 
3-classes 3.4% 3.3% 3.3% 2.6% 
5-classes 4.0% 4.0% 4.1% 1.7% 
Corpus Coreference (spoken dialogues) 
5-classes 4.6% 4.6% 4.6% n.c. 
 
Table 5. Relative standard deviation of measures on 
any independent sets of coders 
Binary metrics do not differ on this criterion: 
multi-?, multi-pi and ?b present very similar re-
sults. On the opposite, the benefit of a Euclidian 
distance of agreement is clear: ? is significantly 
less influenced by coders set permutation. 
5.4 Influence of the number of coders 
A good way to limit annotator bias is to enroll an 
important number of annotators. This need is 
unfortunately contradictory with a restriction of 
annotation costs. The estimation of data reliabil-
ity must thereby remain trustworthy with a 
minimal number of coders. As far as we know, 
there is no clear indication in the literature about 
the definition of such a minimal size. 
We have conducted an experiment which in-
vestigates the influence of the number of coders 
on the relevancy of reliability estimation. Con-
sidering N annotations (N=25 for affective anno-
tation and N=9 for coreference annotation), we 
compute all the possible reliability values with 
any subsets of S coders, S varying from 2 to N. 
As an estimation of the trustworthiness of the 
coefficients, the relative standard deviation of the 
reliability values is computed for every size S 
(Figures 1 to 3). The influence of the number of 
coders is obvious: detrimental standard devia-
tions are found with small coders set sizes. This 
finding concerns above all multi-?, multi-pi and 
?b, which present very close behaviors on all 
annotations. One the opposite, the weighted 
? coefficient converges significantly faster to a 
trustworthy reliability measure The comparison 
between ?b and ?  is enlightening. It shows again 
that the main benefit of Krippendorff?s proposal 
results from its accounting for a weighted dis-
tance in a multi-coders ordinal annotation. 
 
 
0%
10%
20%
30%
40%
2 4 6 8 10 12 14 16 18 20 22 24
Number of coders
R
e
la
tiv
e
 
s
td
de
v
 
(%
)
multi-pi
alpha
multi-k
alpha binary
 
0%
5%
10%
15%
20%
2 4 6 8 10 12 14 16 18 20 22 24
Number of coders
R
el
at
iv
e 
st
dd
ev
 
(%
)
multi-pi
alpha
multi-k
binary alpha
 
 
Figure 1. Relative standard deviation on any set of 
coders of a given size. 5-classes coding scheme. Emo-
tion (top) and opinion (bottom) random annotation. 
 
0%
10%
20%
30%
2 4 6 8 10 12 14 16 18 20 22 24
Number of coders
R
e
la
tiv
e
 
s
td
de
v
 
(%
)
multi-pi
alpha
multi-k
binary alpha
 
0%
5%
10%
15%
2 4 6 8 10 12 14 16 18 20 22 24
Number of coders
Re
la
tiv
e 
st
dd
ev
 
(%
)
multi-pi
alpha
multi-k
alpha binary
 
 
Figure 2. Relative standard deviation on any set of 
coders of a given size. 3-classes coding scheme. Emo-
tion (top) and opinion (bottom) random annotation. 
556
0%
5%
10%
2 3 4 5 6 7 8
Number of coders
Re
la
tiv
e 
s
td
de
v
 
(%
) multi-pi
multi-k
binary alpha
 
Figure 3. Relative std deviation of measures on any 
sets of coders for a given coders set size: coreference 
6 Conclusion and perspectives 
Our experiments were conducted on various an-
notation tasks which assure a certain representa-
tiveness of our conclusions: 
 
? Cohen?s ?, Krippendorff?s ? ?and Scott?s pi? 
provide close values when they use the 
same measure of disagreement. 
? A convergence of these measures has been 
noticed in the literature when the number 
of coders is high. We observed it even on 
very restricted sets of annotators. 
? The use of a weighted measure (Euclidian 
distance) has several benefits on ordinal 
data. It restricts the influence on reliability 
measure of both the number of categories 
and the number of coders. Unfortunately, 
Cohen?s ? ??statistics cannot consider a 
weighted distance in a multi-coders 
framework contrary to Krippendorff?s ?.  
? There is no benefit of using Krippendorff?s 
? on nominal data, since a binary distance 
is mandatory on this situation. 
To conclude, the main interest of Krippen-
dorff?s ? is thus its ability to integrate any kind 
of distance. In light of our results, the weighted 
version of this coefficient must be preferred 
every time an ordinal annotation with multiple 
coders is considered. 
Our experiments leave open an essential ques-
tion: the objective definition of trustworthy 
thresholds of reliability. We propose to investi-
gate this question in terms of expected modifica-
tions of the reference annotation. A majority vote 
is generally used as a gold standard to create this 
reference with multiple coders. As a preliminary 
experiment, we have compared our reference 
affective annotations (25 coders) with those ob-
tained on any other included set of coders.  
0%
10%
20%
30%
40%
50%
1 3 5 7 9 11 13 15 17 19 21 23
number of coders
%
 o
f m
od
ifi
ca
tio
ns 3 classes
5 classes
0,0%
10,0%
20,0%
30,0%
40,0%
50,0%
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
number of coders
%
 
o
f m
o
di
fic
a
tio
n
s HC 3 classes
HC 5 classes
 
Figure 4. Average modifications of the reference ac-
cording to the number of coders. Emotion annotation 
(top) and opinion annotation (bottom) 
 
Figure 4 presents the average percentage of 
modifications of the reference according to the 
number of coders. We wonder to what extent 
these curves can be related to reliability meas-
ures. It seems indeed that the higher the meas-
ures are, the lower the modifications are too. For 
instance, almost all of the coefficients present 
higher or equal reliability values with 3 coding 
categories (Tables 2 & 3), which corresponds to 
lower levels of modifications on Figure 3. Like-
wise, reliability measures are higher on the opin-
ion annotation, where we observe lower modifi-
cations of the reference.  
As a result, we expect results like those pre-
sented on figure 4 to enable a direct interpreta-
tion of reliability measures. For instance, with a 
multi-? values of 0.41, or a ?b value of 0.57 (Ta-
ble 2, 3-classes emotion annotation), one should 
expect around 8% of errors on our reference an-
notation if 10 coders are considered. We plan to 
extend these experiments with simultated syn-
thetic data to characterize precisely the relations 
between absolute reliability measures and ex-
pected confidence in the reference annotation. 
We expect to obtain with simulated annotation a 
sufficient variety of agreement to establish sound 
recommendations on data reliability thresholds. 
We intend to modify randomly human annota-
tions to conduct this simulation.  
557
References  
Cecilia Alm, Dan Roth, Richard Sproat. 2005. Emo-
tions from Text: Machine Learning for Text-based 
Emotion Prediction, In Proc. HLT&EMNLP?2005. 
Vancouver, Canada. 579-586 
Ron Arstein and Masimo Poesio. 2008. Inter-Coder 
Agreement for Computational Linguistics. Compu-
tational Linguistics. 34(4):555-596. 
Ron Artstein and Massimo Poesio. 2005. Bias de-
creases in proportion to the number of annotators. 
In Proceedings FG-MoL?2005, 141:150, Edin-
burgh, UK. 
Petra Saskia Bayerl and Karsten Ingmar Paul, 2011. 
What Determines Inter-Coder Agreement in Man-
ual Annotations? A Meta-Analytic Investigation  . 
Computational Linguistics. 37(4), 699:725. 
Paul Brennan and Alan Silman. 1992. Statistical 
methods for assessing observer variability in clini-
cal measures. BMJ, 304:1491-1494. 
Ted Byrt, Janet Bishop, John Carlin. 1993. Bias, 
prevalence and kappa. Journal of Clinical Epide-
miology, 46:423-429. 
Hermann Brenner and Ulrike Kliebsch. 1996. Depen-
dance of weighted kappa coefficients on the num-
ber of categories. Epidemiology. 7:199-202. 
Zoraida Callejas and Ramon Lopez-Cozar. 2008. In-
fluence of contextual information in emotion anno-
tation for spoken dialogue systems, Speech Com-
munication, 50:416-433 
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the Kappa statistic. Computational 
Linguistics, 22(2):249-254 
Jacob Cohen. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20:37-46. 
Jacob Cohen. 1968. Weighted kappa: nominal scale 
agreement with provision for scaled disagreement 
or partial credit. Psychol. Bulletin, 70(4):213?220 
Roddy Cowie and Randolph Cornelius. 2003. De-
scribing the emotional states that are expressed in 
speech. Speech Communication. 40 :5-32. 
Lee J. Cronbach. 1951. Coefficient alpha and the in-
ternal structure of tests. Psychometrica. 16:297-334 
Laurence Devillers, Laurence Vidrascu, Lori Lamel. 
2005. Emotion detection in real-life spoken dialogs 
recorded in call center. Journal of Neural Net-
works, 18(4):407-422. 
Paul Ekman. 1999. Patterns of emotions: New Analy-
sis of Anxiety and Emotion. Plenum Press, New-
York, NY. 
Barbara Di Eugenio and Michael Glass. 2004. The 
kappa statistic: A second look. Computational Lin-
guistics, 30(1):95?101 
Mark Davies and Joseph Fleiss. 1982. Measuring 
agreement for multinomial data. Biometrics, 
38(4):1047-1051. 
Alvan Feinstein and Domenic Cicchetti. 1990. High 
agreement but low Kappa : the problem of two 
paradoxes. J. of Clinical Epidemiology, 43:543-549 
Joseph L. Fleiss. 1971 Measuring nominal scale 
agreement among many raters. Psychological Bul-
letin, 76(5): 378?382 
Andrew Hayes. 2007. Answering the call for a stan-
dard reliability measure for coding data. Communi-
cation Methods and Measures 1, 1:77-89. 
Klaus Krippendorff. 2004. Content Analysis: an In-
troduction to its Methodology. Chapter 11. Sage: 
Thousand Oaks, CA. 
Klaus Krippendorff. 2004b. Reliability in Content 
Analysis: Some Common Misconceptions and 
Recommendations. Human Communication Re-
search, 30(3): 411-433, 2004 
Klaus Krippendorff. 2008. Testing the reliability of 
content analysis data: what is involved and why. In 
Klaus Krippendorff, Mark Angela Bloch (Eds) The 
content analysis reader. Sage Publications. Thou-
sand Oaks, CA. 
Klaus Krippendorff. 2009. Testing the reliability of 
content analysis data: what is involved and why. In 
Klaus Krippendorff , Mary Angela Bock. The Con-
tent Analysis Reader. Sage: Thousand Oaks, CA 
Marc Le Tallec, Jeanne Villaneau, Jean-Yves An-
toine, Dominique Duhaut. 2011 Affective Interac-
tion with a Companion Robot for vulnerable Chil-
dren: a Linguistically based Model for Emotion 
Detection. In Proc. Language Technology Confer-
ence 2011, Poznan, Poland, 445-450. 
Brian MacWhinney. 2000. The CHILDES project : 
Tools for analyzing talk. 3rd edition. Lawrence Erl-
baum associates Mahwah, NJ. 
Judith Muzerelle, Ana?s Lefeuvre, Emmanuel Schang, 
Jean-Yves Antoine, Aurore Pelletier, Denis Mau-
rel, Iris Eshkol, Jeanne Villaneau. 2014. AN-
COR_Centre, a large free spoken French corefer-
ence corpus: description of the resource and reli-
ability measures. In Proc. LREC?2014 (submitted). 
Kimberly Neuendorf. 2002. The Content Analysis 
Guidebook. Sage Publications, Thousand Oaks, CA 
James Russell. 1980. A Circumplex Model of Affect, 
J. Personality and Social Psy., 39(6): 1161-1178. 
Klaus Scherer. 2005. What are emotions? and how 
can they be measured? Social Science Information, 
44 (4):694?729. 
558
Bj?rn Schuller, Stefan Steidl, Anto Batliner. 2009. 
The Interspeech'2009 emotion challenge. In Pro-
ceedings Interspeech'2009, Brighton, UK. 312:315. 
William Scott. 1955. Reliability of content analysis: 
the case of nominal scale coding. Public Opinions 
Quaterly, 19:321-325. 
Julius Sim and Chris Wright. 2005. The Kappa Statis-
tic in Reliability Studies: Use, Interpretation, and 
Sample Size Requirements. Physical Therapy, 
85(3):257:268. 
Mike Thelwall, Kevan Buckley, Georgios Paltoglou, 
Di Cai, Arvid Kappas. 2010. Sentiment strength 
detection in short informal text. Journal of the 
American Society for Information Science and 
Technology, 61 (12): 2544?2558. 
Peter Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews, In Proceedings ACL?02, 
Philadelphia, Pennsylvania, 417-424. 
Werner Vach, 2005. The dependence of Cohen?s 
kappa on the prevalence does not matter, Journal 
of Clinical Epidemiology, 58, 655-661).  
Rose-Marie Vassallo. 2004. Comment le Grand Nord 
d?couvrit l??t?. Flammarion, Paris, France. 
Kees Vanderheyden. 1995. Le Noel des animaux de la 
montagne. Fairy tale available at the URL : 
http://www.momes.net/histoiresillustrees/contesde
montagne/noelanimaux.html 
Ekaterina Volkova, Betty Mohler, Detmar Meurers, 
Dale Gerdemann and Heinrich B?lthoff. 2010. 
Emotional perception of fairy tales: achieving 
agreement in emotion annotation of   text, In Pro-
ceedings NAACL HLT 2010. Los Angeles, CA. 
Theresa Wilson, Janyce Wiebe, Paul Hoffmann. 2005. 
Recognizing contextual polarity in phrase-level 
sentiment analysis. In Proc. of HLT-EMNLP?2005. 
347-354. 
 
 
 
 
559
