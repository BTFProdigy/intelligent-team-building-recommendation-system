Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 136?143,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Machine-Learning to Assign Function Labels to Parser
Output for Spanish
Grzegorz Chrupa?a1 and Josef van Genabith1,2
1National Center for Language Technology
Dublin City University
Glasnevin, Dublin 9, Ireland
2IBM Dublin Center for Advanced Studies
grzegorz.chrupala@computing.dcu.ie
josef@computing.dcu.ie
Abstract
Data-driven grammatical function tag as-
signment has been studied for English us-
ing the Penn-II Treebank data. In this pa-
per we address the question of whether
such methods can be applied success-
fully to other languages and treebank re-
sources. In addition to tag assignment ac-
curacy and f-scores we also present re-
sults of a task-based evaluation. We use
three machine-learning methods to assign
Cast3LB function tags to sentences parsed
with Bikel?s parser trained on the Cast3LB
treebank. The best performing method,
SVM, achieves an f-score of 86.87% on
gold-standard trees and 66.67% on parser
output - a statistically significant improve-
ment of 6.74% over the baseline. In a
task-based evaluation we generate LFG
functional-structures from the function-
tag-enriched trees. On this task we achive
an f-score of 75.67%, a statistically signif-
icant 3.4% improvement over the baseline.
1 Introduction
The research presented in this paper forms
part of an ongoing effort to develop methods
to induce wide-coverage multilingual Lexical-
Functional Grammar (LFG) (Bresnan, 2001) re-
sources from treebanks by means of automatically
associating LFG f-structure information with con-
stituency trees produced by probabilistic parsers
(Cahill et al, 2004). Inducing deep syntactic anal-
yses from treebank data avoids the cost and time
involved in manually creating wide-coverage re-
sources.
Lexical Functional Grammar f-structures pro-
vide a level of syntactic representation based on
the notion of grammatical functions (e.g. Sub-
ject, Object, Oblique, Adjunct etc.). This level
is more abstract and cross-linguistically more uni-
form than constituency trees. F-structures also in-
clude explicit encodings of phenomena such as
control and raising, pro-drop or long distance de-
pendencies. Those characteristics make this level
a suitable representation for many NLP applica-
tions such as transfer-based Machine Translation
or Question Answering.
The f-structure annotation algorithm used for
inducing LFG resources from the Penn-II treebank
for English (Cahill et al, 2004) uses configura-
tional, categorial, function tag and trace informa-
tion. In contrast to English, in many other lan-
guages configurational information is not a good
predictor for LFG grammatical function assign-
ment. For such languages the function tags in-
cluded in many treebanks are a much more impor-
tant source of information for the LFG annotation
algorithm than Penn-II tags are for English.
Cast3LB (Civit and Mart??, 2004), the Spanish
treebank used in the current research, contains
comprehensive grammatical function annotation.
In the present paper we use a machine-learning ap-
proach in order to add Cast3LB function tags to
nodes of basic constituent trees output by a prob-
abilistic parser trained on Cast3LB. To our knowl-
edge, this paper is the first to describe applying
a data-driven approach to function-tag assignment
to a language other than English.
Our method statistically significantly outper-
forms the previously used approach which relied
exclusively on the parser to produce trees with
Cast3LB tags (O?Donovan et al, 2005). Addi-
tionally, we perform a task-driven evaluation of
our Cast3LB tag assignment method by using the
tag-enriched trees as input to the Spanish LFG f-
structure annotation algorithm and evaluating the
quality of the resulting f-structures.
Section 2 describes the Spanish Cast3LB tree-
bank. In Section 3 we describe previous research
in LFG induction for English and Spanish as well
136
as research on data-driven function tag assign-
ment to parsed text in English. Section 4 provides
the details of our approach to the Cast3LB func-
tion tag assignment task. In Sections 5 and 6 we
present evaluation results for our method. In Sec-
tion 7 we present the error analysis of the results.
Finally, in Section 8 we conclude and discuss ideas
for further research.
2 The Spanish Treebank
As input to our LFG annotation algorithm we use
the output of Bikel?s parser (Bikel, 2002) trained
on the Cast3LB treebank (Civit and Mart??, 2004).
Cast3LB contains around 3,500 constituency trees
(100,000 words) taken from different genres of
European and Latin American Spanish. The POS
tags used in Cast3LB encode morphological infor-
mation in addition to Part-of-Speech information.
Due to the relatively flexible order of main sen-
tence constituents in Spanish, Cast3LB uses a flat,
multiply-branching structure for the S node. There
is no VP node, but rather all complements and ad-
juncts depending on a verb are sisters to the gv
(Verb Group) node containing this verb. An exam-
ple sentence (with the corresponding f-structure)
is shown in Figure 1.
Tree nodes are additionally labelled with gram-
matical function tags. Table 1 provides a list of
function tags with short explanations. Civit (2004)
provides Cast3LB function tag guidelines.
Functional tags carry some of the information
that would be encoded in terms of tree configura-
tions in languages with stricter constituent order
constraints than Spanish.
3 Previous Work
3.1 LFG Annotation
A methodology for automatically obtaining LFG
f-structures from trees output by probabilistic
parsers trained on the Penn-II treebank has been
described by Cahill et al (2004). It has been
shown that the methods can be ported to other lan-
guages and treebanks (Burke et al, 2004; Cahill et
al., 2003), including Cast3LB (O?Donovan et al,
2005).
Some properties of Spanish and the encoding
of syntactic information in the Cast3LB treebank
make it non-trivial to apply the method of auto-
matically mapping c-structures to f-structures used
by Cahill et al (2004), which assigns grammatical
Tag Meaning
ATR Attribute of copular verb
CAG Agent of passive verb
CC Compl. of circumstance
CD Direct object
CD.Q Direct object of quantity
CI Indirect object
CPRED Predicative complement
CPRED.CD Predicative of Direct Object
CPRED.SUJ Predicative of Subject
CREG Prepositional object
ET Textual element
IMPERS Impersonal marker
MOD Verbal modifier
NEG Negation
PASS Passive marker
SUJ Subject
VOC Vocative
Table 1: List of function tags in Cast3LB.
functions to tree nodes based on their phrasal cat-
egory, the category of the mother node and their
position relative to the local head.
In Spanish, the order of sentence constituents
is flexible and their position relative to the head
is an imperfect predictor of grammatical function.
Also, much of the information that the Penn-II
Treebank encodes in terms of tree configurations
is encoded in Cast3LB in the form of function
tags. As Cast3LB trees lack a VP node, the con-
figurational information normally used in English
to distinguish Subjects (NP which is left sister to
VP) from Direct Objects (NP which is right sister
to V) is not available in Cast3LB-style trees. This
means that assigning correct LFG functional an-
notations to nodes in Cast3LB trees is rather dif-
ficult without use of Cast3LB function tags, and
those tags are typically absent in output generated
by probabilistic parsers.
In order to solve this difficulty, O?Donovan et
al. (2005) train Bikel?s parser to output complex
category-function labels. A complex label such as
sn-SUJ (an NP node tagged with the Subject gram-
matical function) is treated as an atomic category
in the training data, and is output in the trees pro-
duced by the parser. This baseline process is rep-
resented in Figure 2.
This approach can be problematic for two main
reasons. Firstly, by treating complex labels as
atomic categories the number of unique labels in-
creases and parse quality can deteriorate due to
sparse data problems. Secondly, this approach, by
relying on the parser to assign function tags, offers
137
Sneg-NEG
no
not
gv
espere
expect
sn-SUJ
el lector
the reader
sn-CD
una definicio?n
a definition
?
?????????????
PRED ?esperar?SUBJ,OBJ??
NEG +
TENSE PRES
MOOD SUBJUNCTIVE
SUBJ
[
SPEC
[
SPEC-FORM EL]
PRED ?lector?
]
OBJ
[
SPEC
[
SPEC-FORM UNO]
PRED ?definicio?n?
]
?
?????????????
Figure 1: On the left flat structure of S. Cast3LB function tags are shown in bold. On the right the
corresponding (simplified) LFG f-structure. Translation: Let the reader not expect a definition.
Figure 2: Processing architecture for the baseline.
limited control over, or room for improvement in,
this task.
3.2 Adding Function Tags to Parser Output
The solution we adopt instead is to add Cast3LB
functional tags to simple constituent trees output
by the parser, as a postprocessing step. For En-
glish, such approaches have been shown to give
good results for the output of parsers trained on
the Penn-II Treebank.
Blaheta and Charniak (2000) use a probabilis-
tic model with feature dependencies encoded by
means of feature trees to add Penn-II Treebank
function tags to Charniak?s parser output. They re-
port an f-score 88.472% on original treebank trees
and 87.277% on the correctly parsed subset of tree
nodes.
Jijkoun and de Rijke (2004) describe a method
of enriching output of a parser with information
that is included in the original Penn-II trees, such
as function tags, empty nodes and coindexations.
They first transform Penn trees to a dependency
format and then use memory-based learning to
perform various graph transformations. One of the
transformations is node relabelling, which adds
function tags to parser output. They report an f-
score of 88.5% for the task of function tagging on
correctly parsed constituents.
4 Assigning Cast3LB Function Tags to
Parsed Spanish Text
The complete processing architecture of our ap-
proach is depicted in Figure 3. We describe it in
detail in this and the following sections.
We divided the Spanish treebank into a training
set of 80%, a development set of 10%, and a test
set of 10% of all trees. We randomly assigned tree-
bank files to these sets to ensure that different tex-
tual genres are about equally represented among
the training, development and test trees.
4.1 Constituency Parsing
For constituency parsing we use Bikel?s (2002)
parser for which we developed a Spanish language
package adapted to the Cast3LB data. Prior to
parsing, we perform one of the tree transforma-
tions described by Cowan and Collins (2005), i.e.
we add a CP and SBAR nodes to subordinate and
relative clauses. This is undone in parser output.
The category labels in the Spanish treebank are
rather fine grained and often contain redundant in-
formation.1 We preprocess the treebank and re-
1For example there are several labels for Nominal Group,
138
Figure 3: Processing architecture for the machine-
learning-based method.
duce the number of category labels, only retaining
distinctions that we deem useful for our purposes.2
For constituency parsing we also reduce the
number of POS tags by including only selected
morphological features. Table 2 provides the
list of features included for the different parts of
speech. In our experiments we use gold standard
POS tagged development and test-set sentences as
input rather than tagging text automatically.
The results of the evaluation of parsing perfor-
mance on the test set are shown in Table 3. La-
belled bracketing f-score for all sentences is just
below 84% for all sentences, and 84.58% for sen-
tences of length ? 70. In comparison, Cowan
and Collins (2005) report an f-score of 85.1%
(? 70) using a version of Collins? parser adapted
for Cast3LB, and using reranking to boost perfor-
such as grup.nom.ms (masculine singular), grup.nom.fs (fem-
inine singular), grup.nom.mp (masculine plural) etc. This
number and gender information is already encoded in the
POS tags of nouns heading these constituents.
2The labels we retain are the following: INC, S, S.NF,
S.NF.R, S.NF, S.R, conj.subord, coord, data, espec, gerundi,
grup.nom, gv, infinitiu, interjeccio, morf, neg, numero, prep,
relatiu, s.a, sa, sadv, sn, sp, and versions of those suffixed
with .co to indicate coordination).
Part of Speech Features included
Determiner type, number
Noun type, number
Adjective type, number
Pronoun type, number, person
Verb type, number, mood
Adverb type
Conjunction type
Table 2: Features included in POS tags. Type
refers to subcategories of parts of speech such as
e.g. common and proper for nouns, or main, aux-
iliary and semiauxiliary for verbs. For details see
(Civit, 2000).
LB Precision LB Recall F-score
All 84.18 83.74 83.96
? 70 84.82 84.35 84.58
Table 3: Parser performance.
mance. They use a different, more reduced cat-
egory label set as well as a different training-test
split. Both Cowan and Collins and the present pa-
per report scores which ignore punctuation.
4.2 Cast3LB Function Tagging
For the task of Cast3LB function tag assign-
ment we experimented with three generic machine
learning algorithms: a memory-based learner
(Daelemans and van den Bosch, 2005), a maxi-
mum entropy classifier (Berger et al, 1996) and a
Support Vector Machine classifier (Vapnik, 1998).
For each algorithm we use the same set of features
to represent nodes that are to be assigned one of
the Cast3LB function tags. We use a special null
tag for nodes where no Cast3LB tag is present.
In Cast3LB only nodes in certain contexts are
eligible for function tags. For this reason we only
consider a subset of all nodes as candidates for
function tag assignment, namely those which are
sisters of nodes with the category labels gv (Verb
Group), infinitiu (Infinitive) and gerundi (Gerund).
For these candidates we extract the following three
types of features encoding configurational, mor-
phological and lexical information for the target
node and neighboring context nodes:
? Node features: position relative to head, head
lemma, alternative head lemma (i.e. the head
of NP in PP), head POS, category, definite-
ness, agreement with head verb, yield, hu-
man/nonhuman
139
? Local features: head verb, verb person, verb
number, parent category
? Context features: node features (except posi-
tion) of the two previous and two following
sister nodes (if present).
We used cross-validation for refining the set
of features and for tuning the parameters of the
machine-learning algorithms. We did not use any
additional automated feature-selection procedure.
We made use of the following implementations:
TiMBL (Daelemans et al, 2004) for Memory-
Based Learning, the MaxEnt Toolkit (Le, 2004)
for Maximum Entropy and LIBSVM (Chang and
Lin, 2001) for Support Vector Machines. For
TiMBL we used k nearest neighbors = 7 and the
gain ratio metric for feature weighting. For Max-
Ent, we used the L-BFGS parameter estimation
and 110 iterations, and we regularize the model
using a Gaussian prior with ?2 = 1. For SVM we
used the RBF kernel with ? = 2?7 and the cost
parameter C = 32.
5 Cast3LB Tag Assignment Evaluation
We present evaluation results on the original gold-
standard trees of the test set as well as on the
test-set sentences parsed by Bikel?s parser. For
the evaluation of Cast3LB function tagging per-
formance on gold trees the most straightforward
metric is the accuracy, or the proportion of all can-
didate nodes that were assigned the correct label.
However we cannot use this metric for evalu-
ating results on the parser output. The trees out-
put by the parser are not identical to gold standard
trees due to parsing errors, and the set of candi-
date nodes extracted from parsed trees will not be
the same as for gold trees. For this reason we use
an alternative metric which is independent of tree
configuration and uses only the Cast3LB function
labels and positional indices of tokens in a sen-
tence. For each function-tagged tree we first re-
move the punctuation tokens. Then we extract a
set of tuples of the form ?GF, i, j?, where GF is
the Cast3LB function tag and i ? j is the range
of tokens spanned by the node annotated with this
function. We use the standard measures of preci-
sion, recall and f-score to evaluate the results.
Results for the three algorithms are shown in
Table 4. MBL and MaxEnt show a very sim-
ilar performance, while SVM outperforms both,
t
t
t
t
t
7.0 7.5 8.0 8.5 9.0 9.5
0.7
6
0.8
0
0.8
4
0.8
8
log(n)
Ac
cu
rac
y
s
s
s
s
s
m
m
m
m
m
Figure 4: Learning curves for TiMBL (t), MaxEnt
(m) and SVM (s).
Acc. Prec. Recall F-score
MBL 87.55 87.00 82.98 84.94
MaxEnt 88.06 87.66 86.87 85.52
SVM 89.34 88.93 84.90 86.87
Table 4: Cast3LB function tagging performance
for gold-standard trees
scoring 89.34% on accuracy and 86.87% on f-
score. The learning curves for the three algo-
rithms, shown in Figure 4, are also informative,
with SVM outperforming the other two methods
for all training set sizes. In particular, the last sec-
tion of the plot shows SVM performing almost as
well as MBL with half as much learning material.
Neither of the three curves shows signs of hav-
ing reached a maximum, which indicates that in-
Precision Recall F-score
all corr. all corr. all corr.
Baseline 59.26 72.63 60.61 75.35 59.93 73.96
MBL 64.74 78.09 64.18 78.75 64.46 78.42
MaxEnt 65.48 78.90 64.55 79.44 65.01 79.17
SVM 66.96 80.58 66.38 81.27 66.67 80.92
Table 5: Cast3LB function tagging performance
for parser output, for all constituents, and for cor-
rectly parsed constituents only
140
Methods p-value
Baseline vs SVM 1.169? 10?9
Baseline vs MBL 2.117? 10?6
MBL vs MaxEnt 0.0799
MaxEnt vs SVM 0.0005
Table 6: Statistical significance testing results on
for the Cast3LB tag assignment on parser output.
Precision Recall F-score
Baseline 73.95 70.67 72.27
SVM 76.90 74.48 75.67
Table 7: LFG F-structure evaluation results for
parser output
creasing the size of the training data should result
in further improvements in performance.
Table 5 shows the performance of the three
methods on parser output. The baseline con-
tains the results achieved by treating compound
category-function labels as atomic during parser
training so that they are included in parser output.
For this task we present two sets of results: (i) for
all constituents, and (ii) for correctly parsed con-
stituents only. Again the best algorithm turns out
to be SVM. It outperforms the baseline by a large
margin (6.74% for all constituents).
The difference in performance for gold stan-
dard trees, and the correctly parsed constituents
in parser output is rather larger than what Blaheta
and Charniak report. Further analysis is needed
to identify the source of this difference but we
suspect that one contributing factor is the use of
greater number of context features combined with
a higher parse error rate in comparison to their ex-
periments on the Penn II Treebank. Since any mis-
analysis of constituency structure in the vicinity of
target node can have negative impact, greater re-
liance on context means greater susceptibility to
parse errors. Another factor to consider is the fact
that we trained and adjusted parameters on gold-
standard trees, and the model learned may rely on
features of those trees that the parser is unable to
reproduce.
For the experiments on parser output (all con-
stituents) we performed a series of sign tests in
order to determine to what extent the differences
in performance between the different methods are
statistically significant. For each pair of methods
we calculate the f-score for each sentence in the
test set. For those sentences on which the scores
differ (i.e. the number of trials) we calculate in
how many cases the second method is better than
the first (i.e. the number of successes). We then
perform the test with the null hypothesis that the
probability of success is chance (= 0.5) and the
alternative hypothesis that the probability of suc-
cess is greater than chance (> 0.5). The results
are summarized in Table 6. Given that we perform
4 pairwise comparisons, we apply the Bonferroni
correction and adjust our target ?? = ?4 . For the
confidence level 95% (?? = 0.0125) all pairs give
statistically significant results, except for MBL vs
MaxEnt.
6 Task-Based LFG Annotation
Evaluation
Finally, we also evaluated the actual f-structures
obtained by running the LFG-annotation algo-
rithm on trees produced by the parser and enriched
with Cast3LB function tags assigned using SVM.
For this task-based evaluation we produced a gold
standard consisting of f-structures corresponding
to all sentences in the test set. The LFG-annotation
algorithm was run on the test set trees (which con-
tained original Cast3LB treebank function tags),
and the resulting f-structures were manually cor-
rected.
Following Crouch et al (2002), we convert
the f-structures to triples of the form ?GF,Pi, Pj?,
where Pi is the value of the PRED attribute of the
f-structure, GF is an LFG grammatical function
attribute, and Pj is the value of the PRED attribute
of the f-structure which is the value of the GF
attribute. This is done recursively for each level
of embedding in the f-structure. Attributes with
atomic values are ignored for the purposes of this
evaluation. The results obtained are shown in Ta-
ble 7. We also performed a statistical significance
test for these results, using the same method as for
the Cast3LB tag assigment task. The p-value given
by the sign test was 2.118?10?5, comfortably be-
low ? = 1%.
The higher scores achieved in the LFG f-
structure evaluation in comparison with the pre-
ceding Cast3LB tag assignment evaluation (Table
5) can be attributed to two main factors. Firstly,
the mapping from Cast3LB tags to LFG grammat-
ical functions is not one-to-one. For example three
Cast3LB tags (CC, MOD and ET) are all mapped
to LFG ADJUNCT. Thus mistagging a MOD as
141
ATR CC CD CI CREG MOD SUJ
ATR 136 2 0 0 0 0 5
CC 6 552 12 4 25 18 6
CD 1 19 418 5 3 0 26
CI 0 6 1 50 1 0 0
CREG 0 6 0 2 43 0 0
MOD 0 0 0 0 0 19 0
SUJ 0 8 24 2 0 0 465
Table 8: Simplified confusion matrix for SVM
on test-set gold-standard trees. The gold-standard
Cast3LB function tags are shown in the first row,
the predicted tags in the first column. So e.g. SUJ
was mistagged as CD in 26 cases. Low frequency
function tags as well as those rarely mispredicted
have been omitted for clarity.
CC does not affect the f-structure score. On the
other hand the Cast3LB CD tag can be mapped
to OBJ, COMP, or XCOMP, and it can be easily
decided which one is appropriate depending on
the category label of the target node. Addition-
ally many nodes which receive no function tag in
Cast3LB, such as noun modifiers, are straightfor-
wardly mapped to LFG ADJUNCT. Similarly, ob-
jects of prepositions receive the LFG OBJ function.
Secondly, the f-structure evaluation metric is
less sensitive to small constituency misconfigura-
tions: it is not necessary to correctly identify the
token range spanned by a target node as long as the
head (which provides the PRED attribute) is cor-
rect.
7 Error Analysis
In order to understand sources of error and de-
termine how much room for further improvement
there is, we examined the most common cases of
Cast3LB function mistagging. A simplified confu-
sion matrix with the most common Cast3LB tags
is shown in Table 8. The most common mistakes
occur between SUJ and CD, in both directions, and
many also CREGs are erroneously tagged as CC.
7.1 Subject vs Direct Object
We noticed that in over 50% of cases when a
Direct Object (CD) was misidentified as Subject
(SUJ), the target node?s mother was a relative
clause. It turns out that in Spanish relative clauses
genuine syntactic ambiguity is not uncommon.
Consider the following Spanish phrase:
(1) Sistemas
Systems
que
which
usan
use
el
DET
95%
95%
de
of
los
DET
ordenadores.
computers
Its translation into English is either Systems that
use 95% of computers or alternatively Systems that
95% of computers use. In Spanish, unlike in En-
glish, preverbal / postverbal position of a con-
stituent is not a good guide to its grammatical
function in this and similar contexts. Human an-
notators can use their world knowledge to decide
on the correct semantic role of a target constituent
and use it in assigning a correct grammatical func-
tion, but such information is obviously not used
in our machine learning methods. Thus such mis-
takes seem likely to remain unresolvable in our
current approach.
7.2 Prepositional Object vs Adjunct
The frequent misidentification of Prepositional
Objects (CREG) as Adjuncts (CC) seen in Table 8
can be accounted for by several factors. Firstly,
Prepositional Objects are strongly dependent on
specific verbs and the comparatively small size of
our training data means that there is limited oppor-
tunity for a machine-learning algorithm to learn
low-frequency lexical dependencies. Here the ob-
vious solution is to use a more adequate amount of
training material when it becomes available.
A further problem with the Prepositional Object
- Adjunct distinction is its inherent fuzziness. Be-
cause of this, treebank designers may fail to pro-
vide easy-to-follow, clearcut guidelines and hu-
man annotators necessarily exercise a certain de-
gree of arbitrariness in assigning one or the other
function.
8 Conclusions and Future Research
Our research has shown that machine-learning-
based Cast3LB tag assignment as a post-
processing step to raw tree parser output statisti-
cally significantly outperforms a baseline where
the parser itself is trained to learn category
/ Cast3LB-function pairs. In contrast to the
parser-based method, the machine-learning-based
method avoids some sparse data problems and al-
lows for more control over Cast3LB tag assign-
ment. We have found that the SVM algorithm out-
performs the other two machine learning methods
used.
142
In addition, we evaluated Cast3LB tag assign-
ment in a task-based setting in the context of au-
tomatically acquiring LFG resources for Spanish
from Cast3LB. Machine-learning-based Cast3LB
tag assignment yields statistically-significantly
improved LFG f-structures compared to parser-
based assignment.
One limitation of our method is the fact that it
treats the classification task separately for each tar-
get node. It thus fails to observe constraints on the
possible sequences of grammatical function tags
in the same local context. Some functions are
unique, such as the Subject, whereas others (Di-
rect and Indirect Object) can only be realized by a
full NP once, although they can be doubled by a
clitic pronoun. Capturing such global constraints
will need further work.
Acknowledgements
We gratefully acknowledge support from Science
Foundation Ireland grant 04/IN/I527 for the re-
search reported in this paper.
References
A. L. Berger, V. J. Della Pietra, and S. A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71, March.
D. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In
Human Language Technology Conference (HLT),
San Diego, CA, USA. Software available
at http://www.cis.upenn.edu/?dbikel/
software.html#stat-parser.
D. Blaheta and E. Charniak. 2000. Assigning function
tags to parsed text. In Proceedings of the 1st Con-
ference of the North American Chapter of the ACL,
pages 234?240, Rochester, NY, USA.
J. Bresnan. 2001. Lexical-Functional Syntax. Black-
well Publishers, Oxford.
M. Burke, O. Lam, A. Cahill, R. Chan, R. O?Donovan,
A. Bodomo, J. van Genabith, and A. Way. 2004.
Treebank-based acquisition of a Chinese Lexical-
Functional Grammar. In Proceedings of the 18th
Pacific Asia Conference on Language, Information
and Computation (PACLIC-18).
A. Cahill, M. Forst, M. McCarthy, R. O?Donovan,
and C. Roher. 2003. Treebank-based multilingual
unification-grammar development. In Proceedings
of the 15th Workshop on Ideas and Strategies for
Multilingual Grammar Development, ESSLLI 15,
Vienna, Austria.
A. Cahill, M. Burke, R. O?Donovan, J. van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceed-
ings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 319?326,
Barcelona, Spain.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/?cjlin/libsvm.
M. Civit and M. A. Mart??. 2004. Building Cast3LB: A
Spanish treebank. Research on Language and Com-
putation, 2(4):549?574, December.
M. Civit. 2000. Gu??a para la anotacio?n mor-
fosinta?ctica del corpus CLiC-TALP, X-TRACT
Working Paper. Technical report. Avail-
able at http://clic.fil.ub.es/personal/
civit/PUBLICA/guia morfol.ps.
M. Civit. 2004. Gu??a para la anotacio?n de las funciones
sinta?cticas de Cast3LB. Technical report. Avail-
able at http://clic.fil.ub.es/personal/
civit/PUBLICA/funcions.pdf.
B. Cowan and M. Collins. 2005. Morphology and
reranking for the statistical parsing of Spanish. In
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada.
R. Crouch, R. M. Kaplan, T. H. King, and S. Riezler.
2002. A comparison of evaluation metrics for a
broad-coverage stochastic parser. In Conference on
Language Resources and Evaluation (LREC 02).
W. Daelemans and A. van den Bosch. 2005. Memory-
Based Language Processing. Cambridge University
Press, September.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg Memory
Based Learner, version 5.1, Reference Guide. Tech-
nical report. Available from http://ilk.uvt.
nl/downloads/pub/papers/ilk0402.pdf.
V. Jijkoun and M. de Rijke. 2004. Enriching the output
of a parser using memory-based learning. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Barcelona, Spain.
Zh. Le, 2004. Maximum Entropy Modeling
Toolkit for Python and C++. Available
at http://homepages.inf.ed.ac.uk/
s0450736/software/maxent/manual.pdf.
R. O?Donovan, A. Cahill, J. van Genabith, and A. Way.
2005. Automatic acquisition of Spanish LFG re-
sources from the CAST3LB treebank. In Proceed-
ings of the Tenth International Conference on LFG,
Bergen, Norway.
V. N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience, September.
143
Hierarchical Recognition of Propositional Arguments with Perceptrons
Xavier Carreras and Llu??s Ma`rquez
TALP Research Centre
Technical University of Catalonia (UPC)
{carreras,lluism}@lsi.upc.es
Grzegorz Chrupa?a
GRIAL Research Group
University of Barcelona (UB)
grzegorz@pithekos.net
1 Introduction
We describe a system for the CoNLL-2004 Shared Task
on Semantic Role Labeling (Carreras and Ma`rquez,
2004a). The system implements a two-layer learning ar-
chitecture to recognize arguments in a sentence and pre-
dict the role they play in the propositions. The explo-
ration strategy visits possible arguments bottom-up, navi-
gating through the clause hierarchy. The learning compo-
nents in the architecture are implemented as Perceptrons,
and are trained simultaneously online, adapting their be-
havior to the global target of the system. The learn-
ing algorithm follows the global strategy introduced in
(Collins, 2002) and adapted in (Carreras and Ma`rquez,
2004b) for partial parsing tasks.
2 Semantic Role Labeling Strategy
The strategy for recognizing propositional arguments in
sentences is based on two main observations about argu-
ment structure in the data. The first observation is the
relation of the arguments of a proposition with the chunk
and clause hierarchy: a proposition places its arguments
in the clause directly containing the verb (local clause),
or in one of the ancestor clauses. Given a clause, we de-
fine the sequence of top-most syntactic elements as the
words, chunks or clauses which are directly rooted at the
clause. Then, arguments are formed as subsequences of
top-most elements of a clause. Finally, for local clauses
arguments are found strictly to the left or to the right of
the target verb, whereas for ancestor clauses arguments
are usually to the left of the verb. This observation holds
for most of the arguments in the data. A general excep-
tion are arguments of type V, which are found only in the
local clause, starting at the position of the target verb.
The second observation is that the arguments of all
propositions of a sentence do not cross their boundaries,
and that arguments of a particular proposition are usually
found strictly within an argument of a higher level propo-
sition. Thus, the problem can be thought of as finding a
hierarchy of arguments in which arguments are embed-
ded inside others, and each argument is related to a num-
ber of propositions of a sentence in a particular role. If an
argument is related to a certain verb, no other argument
linking to the same verb can be found within it.
The system presented in this paper translates these ob-
servations into constraints which are enforced to hold in
a solution, and guide the recognition strategy. A limita-
tion of the system is that it makes no attempt to recognize
arguments which are split in many phrases.
In what follows, x is a sentence, and xi is the i-th word
of the sentence. We assume a mechanism to access the
input information of x (PoS tags, chunks and clauses),
as well as the set of target verbs V , represented by their
position. A solution y ? Y for a sentence x is a set of
arguments of the form (s, e)kv , where (s, e) represents an
argument spanning from word xs to word xe, playing a
semantic role k ? K with a verb v ? V . Finally, [S,E]
denotes a clause spanning from word xS to word sE .
The SRL(x) function, predicting semantic roles of a
sentence x, implements the following strategy:
1. Initialize set of arguments, A, to empty.
2. Define the level of each clause as its distance to the
root clause.
3. Explore clauses bottom-up, i.e. from deeper levels
to the root clause. For a clause [S,E]:
A := A ? arg search(x, [S,E])
4. Return A
2.1 Building Argument Hierarchies
Here we describe the function arg search, which builds a
set of arguments organized hierarchically, within a clause
[S,E] of a sentence x. The function makes use of two
learning-based components, defined here and described
below. First, a filtering function F, which, given a can-
didate argument, determines its plausible categories, or
rejects it when no evidence for it being an argument
is found. Second, a set of k-score functions, for each
k ? K, which, given an argument, predict a score of plau-
sibility for it being of role type k of a certain proposition.
The function arg search searches for the argument hi-
erarchy which optimizes a global score on the hierarchy.
As in earlier works, we define the global score (?) as the
summation of scores of each argument in the hierarchy.
The function explores all possible arguments in the clause
formed by contiguous top-most elements, and selects the
subset which optimizes the global score function, forcing
a hierarchy in which the arguments linked to the same
verb do not embed.
Using dynamic programming, the function can be
computed in cubic time. It considers fragments of top-
most elements, which are visited bottom-up, incremen-
tally in length, until the whole clause is explored. While
exploring, it maintains a two-dimensional matrix A of
partial solutions: each position [s, e] contains the optimal
argument hierarchy for the fragment from s to e. Finally,
the solution is found at A[S,E]. For a fragment from s to
e the algorithm is as follows:
1. A := A[s, r] ? A[r+1, e] where
r := arg maxs?r<e ?
(
A[s, r]
)
+ ?
(
A[r+1, e]
)
2. For each prop v ? V :
(a) K := F((s, e), v)
(b) Compute k? such that
k? := arg maxk?K k-score((s, e), v, x)
Set ? to the score of category k?.
(c) Set Av as the arguments in A linked to v.
(d) If (?(Av) < ?
)
then A := A\Av ?{(s, e)k
?
v }
3. A[s, e] := A
Note that an argument is visited once, and that its score
can be stored to efficiently compute the ? global score.
2.2 Start-End Filtering
The function F determines which categories in K are
plausible for an argument (s, e) to relate to a verb v.
This is done via start-end filters (FkS and FkE), one for
each type in K1. They operate on words, independently
of verbs, deciding whether a word is likely to start or end
some argument of role type k.
The selection of categories is conditional to the relative
level of the verb and the clause, and to the relative posi-
tion of the verb and the argument. The conditions are:
? v is local to the clause, and (v=s) and FVE(xe):
K := {V}
? v is local, and (e<v ? v<s):
K := {k ? K | FkS(xs) ? FkE(xe)}
1Actually, we share start-end filters for A0-A5 arguments.
? v is at deeper level, and (e<v):
K := {k ? K | k 6?K(v) ? FkS(xs) ? FkE(xe)}
where K(v) is the set of categories already assigned
to the verb in deeper clauses.
? Otherwise, K is set to empty.
Note that setting K to empty has the effect of filter-
ing out the argument for the proposition. Note also that
Start-End classifications do not depend on the verb, thus
they can be performed once per candidate word, before
entering the exploration of clauses. Then, when visiting
a clause, the Start-End filtering can be performed with
stored predictions.
3 Learning with Perceptrons
In this section we describe the learning components of
the system, namely start, end and score functions, and the
Perceptron-based algorithm to train them together online.
Each function is implemented using a linear separator,
hw : Rn ? R, operating in a feature space defined by
a feature extraction function, ? : X ? Rn, for some
instance space X . The start-end functions (FkS and FkE)
are formed by a prediction vector for each type, noted as
wkS or w
k
E, and a shared representation function ?w which
maps a word in context to a feature vector. A prediction
is computed as FkS(x) = wkS ??w(x), and similarly for the
FkE, and the sign is taken as the binary classification.
The score functions compute real-valued scores for
arguments (s, e)v . We implement these functions with
a prediction vector wk for each type k ? K, and
a shared representation function ?a which maps an
argument-verb pair to a feature vector. The score pre-
diction for a type k is then given by the expression:
k-score((s, e), v, x) = wk ? ?a((s, e), v, x).
3.1 Perceptron Learning Algorithm
We describe a mistake-driven online algorithm to train
prediction vectors together. The algorithm is essentially
the same as the one introduced in (Collins, 2002). Let W
be the set of prediction vectors:
? Initialize: ?w?W w := 0
? For each epoch t := 1 . . . T ,
for each sentence-solution pair (x, y) in training:
1. y? = SRLW (x)
2. learning feedback(W,x, y, y?)
? Return W
3.2 Learning Feedback for Filtering-Ranking
We now describe the learning feedback rule, introduced
in earlier works (Carreras and Ma`rquez, 2004b). We dif-
ferentiate two kinds of global errors in order to give feed-
back to the functions being learned: missed arguments
and over-predicted arguments. In each case, we identify
the prediction vectors responsible for producing the in-
correct argument and update them additively: vectors are
moved towards instances predicted too low, and moved
away from instances predicted too high.
Let y? be the gold set of arguments for a sentence
x, and y? those predicted by the SRL function. Let
goldS(xi, k) and goldE(xi, k) be, respectively, the per-
fect indicator functions for start and end boundaries of
arguments of type k. That is, they return 1 if word xi
starts/ends some k-argument in y? and -1 otherwise. The
feedback is as follows:
? Missed arguments: ?(s, e)kv ? y?\y?:
1. Update misclassified boundary words:
if (wkS ? ?w(xs) ? 0) then wkS = wkS + ?w(xs)
if (wkE ??w(xe) ? 0) then wkE = wkE +?w(xe)
2. Update score function, if applied:
if (k ? F ((s, e), v) then
wk = wk + ?a((s, e), v, x)
? Over-predicted arguments: ?(s, e)kp ? y?\y?:
1. Update score function:
wk = wk ? ?a((s, e), v, x)
2. Update words misclassified as S or E:
if (goldS(xs, k)=?1) then wkS = wkS??w(xs)
if (goldE(xe, k)=?1) then wkE =wkE??w(xe)
3.3 Kernel Perceptrons with Averaged Predictions
Our final architecture makes use of Voted Perceptrons
(Freund and Schapire, 1999), which compute a predic-
tion as an average of all vectors generated during train-
ing. Roughly, each vector contributes to the average pro-
portionally to the number of correct positive training pre-
dictions the vector has made. Furthermore, a prediction
vector can be expressed in dual form as a combination of
training instances, which allows the use of kernel func-
tions. We use standard polynomial kernels of degree 2.
4 Features
The features of the system are extracted from three types
of elements: words, target verbs, and arguments. They
are formed making use of PoS tags, chunks and clauses
of the sentence. The functions ?w and ?a are defined
in terms of a collection of feature extraction patterns,
which are binarized in the functions: each extracted pat-
tern forms a binary dimension indicating the existence of
the pattern in a learning instance.
Extraction on Words. The list of features extracted
from a word xi is the following:
? PoS tag.
? Form, if the PoS tag does not match with the Perl
regexp /?(CD|FW|J|LS|N|POS|SYM|V)/.
? Chunk type, of the chunk containing the word.
? Binary-valued flags: (a) Its chunk is one-word or
multi-word; (b) Starts and/or ends, or is strictly
within a chunk (3 flags); (c) Starts and/or ends
clauses (2 flags); (d) Aligned with a target verb; and
(e) First and/or last word of the sentence (2 flags).
Given a word xi, the ?w function implements a ?3
window, that is, it returns the features of the words xi+r,
with ?3?r?+3, each with its relative position r.
Extraction on Target Verbs. Given a target verb v, we
extract the following features from the word xv:
? Form, PoS tag, and target verb infinitive form.
? Voice : passive, if xv has PoS tag VBN, and either its
chunk is not VP or xv is preceded by a form of ?to
be? or ?to get? within its chunk; otherwise active.
? Chunk type.
? Binary-valued flags: (a) Its chunk is multi-word or
not; and (b) Starts and/or ends clauses (2 flags).
Extraction on Arguments. The ?a function performs
the following feature extraction for an argument (s, e)
linked to a verb v:
? Target verb features, of verb v.
? Word features, of words s?1, s, e, and e+1, each
anchored with its relative position.
? Distance of v to s and to e: for both pairs, a flag
indicating if distance is {0, 1,?1, >1, <1}.
? PoS Sequence, of PoS tags from s to e: (a) n-grams
of size 2, 3 and 4; and (b) the complete PoS pattern,
if it is less than 5 tags long.
? TOP sequence: tags of the top-most elements found
strictly from s to e. The tag of a word is its PoS. The
tag of a chunk is its type. The tag of a clause is its
type (S) enriched as follows: if the PoS tag of the
first word matches /?(IN|W|TO)/ the tag is en-
riched with the form of that word (e.g. S-to); if
that word is a verb, the tag is enriched with its PoS
(e.g. S-VBG); otherwise, it is just S. The follow-
ing features are extracted: (a) n-grams of sizes 2, 3
and 4; (b) The complete pattern, if it is less than 5
tags long; and (c) Anchored tags of the first, second,
penultimate and last elements.
? PATH sequence: tags of elements found between
the argument and the verb. It is formed by a con-
catenation of horizontal tags and vertical tags. The
horizontal tags correspond to the TOP sequence of
elements at the same level of the argument, from it to
the phrase containing the verb, both excluded. The
vertical part is the list of tags of the phrases which
contain the verb, from the phrase at the level of the
argument to the verb. The tags of the PATH se-
quence are extracted as in the TOP sequence, with
an additional mark indicating whether an element is
horizontal to the left or to the right of the argument,
or vertical. The following features are extracted: (a)
n-grams of sizes 4 and 5; and (b) The complete pat-
tern, if it is less than 5 tags long.
? Bag of Words: we consider the top-most elements
of the argument which are not clauses, and extract
all nouns, adjectives and adverbs. We then form a
separate bag for each category.
? Lexicalization: we extract the form of the head of
the first top-most element of the argument, via com-
mon head word rules; if the first element is a PP
chunk, we also extract the head of the first NP found.
5 Experiments and Results
We have build a system which implements the presented
architecture for recognizing arguments and their semantic
roles. The configuration of learning functions, related to
the roles in the CoNLL-2004 data, is set as follows :
? Five score functions for the A0?A4 types, and two
shared filtering functions FANS and FANE .
? For each of the 13 adjunct types (AM-*), a score
function and a pair of filtering functions.
? Three score functions for the R0?R2 types, and two
filtering functions FRS and FRE shared among them.
? For verbs, a score function and an end filter.
We ran the learning algorithm on the training set (with
predicted input syntax) with a polynomial kernel of de-
gree 2, for up to 8 epochs. Table 1 presents the ob-
tained results on the development set, either artificial or
real. The second and third rows provide, respectively, the
loss suffered because of errors in the filtering and scor-
ing layer. The filtering layer performs reasonably well,
since 89.44% recall can be achieved on the top of it.
However, the scoring functions clearly moderate the per-
formance, since working with perfect start-end functions
only achieve an F1 at 75.60. Finally, table 2 presents final
detailed results on the test set.
Precision Recall F?=1
g?FS, g?FE, g-score 99.92% 94.73% 97.26
FS, FE, g?score 99.90% 89.44% 94.38
g?FS, g?FE, score 85.12% 67.99% 75.60
FS, FE, score 73.40% 63.70% 68.21
Table 1: Overall results on the development set. Func-
tions with prefix g are gold functions, providing bounds
of our performance. The top row is the upper bound per-
formance of our architecture. The bottom row is the real
performance.
Precision Recall F?=1
Overall 71.81% 61.11% 66.03
A0 81.83% 76.46% 79.05
A1 68.73% 65.27% 66.96
A2 59.41% 34.03% 43.28
A3 58.18% 21.33% 31.22
A4 72.97% 54.00% 62.07
A5 0.00% 0.00% 0.00
AM-ADV 54.50% 35.50% 43.00
AM-CAU 58.33% 28.57% 38.36
AM-DIR 64.71% 22.00% 32.84
AM-DIS 64.06% 57.75% 60.74
AM-EXT 100.00% 50.00% 66.67
AM-LOC 35.62% 22.81% 27.81
AM-MNR 50.89% 22.35% 31.06
AM-MOD 97.57% 95.25% 96.40
AM-NEG 90.23% 94.49% 92.31
AM-PNC 36.11% 15.29% 21.49
AM-PRD 0.00% 0.00% 0.00
AM-TMP 61.86% 48.86% 54.60
R-A0 78.85% 77.36% 78.10
R-A1 64.29% 51.43% 57.14
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 0.00% 0.00% 0.00
V 98.32% 98.24% 98.28
Table 2: Results on the test set
Acknowledgements
This research is supported by the European Commission
(Meaning, IST-2001-34460) and the Spanish Research Depart-
ment (Aliado, TIC2002-04447-C02). Xavier Carreras is sup-
ported by a grant from the Catalan Research Department.
References
Xavier Carreras and Llu?is Ma`rquez. 2004a. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic Role
Labeling. In Proceedings of CoNLL-2004.
Xavier Carreras and Llu?is Ma`rquez. 2004b. Online
learning via global feedback for phrase recognition.
In Advances in Neural Information Processing Systems
16. MIT Press.
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proceedings of the
EMNLP?02.
Y. Freund and R. E. Schapire. 1999. Large Margin Clas-
sification Using the Perceptron Algorithm. Machine
Learning, 37(3):277?296.
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422?1426,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Elephant: Sequence Labeling for Word and Sentence Segmentation
Kilian Evang*, Valerio Basile*, Grzegorz Chrupa?a? and Johan Bos*
*University of Groningen, Oude Kijk in ?t Jatstraat 26, 9712 EK Groningen, The Netherlands
?Tilburg University, PO Box 90153, 5000 LE Tilburg, The Netherlands
*{k.evang, v.basile, johan.bos}@rug.nl ?g.chrupala@uvt.nl
Abstract
Tokenization is widely regarded as a solved
problem due to the high accuracy that rule-
based tokenizers achieve. But rule-based
tokenizers are hard to maintain and their
rules language specific. We show that high-
accuracy word and sentence segmentation can
be achieved by using supervised sequence la-
beling on the character level combined with
unsupervised feature learning. We evalu-
ated our method on three languages and ob-
tained error rates of 0.27 ? (English), 0.35 ?
(Dutch) and 0.76 ? (Italian) for our best mod-
els.
1 An Elephant in the Room
Tokenization, the task of segmenting a text into
words and sentences, is often regarded as a solved
problem in natural language processing (Dridan and
Oepen, 2012), probably because many corpora are
already in tokenized format. But like an elephant in
the living room, it is a problem that is impossible to
overlook whenever new raw datasets need to be pro-
cessed or when tokenization conventions are recon-
sidered. It is moreover an important problem, be-
cause any errors occurring early in the NLP pipeline
affect further analysis negatively. And even though
current tokenizers reach high performance, there are
three issues that we feel haven?t been addressed sat-
isfactorily so far:
? Most tokenizers are rule-based and therefore
hard to maintain and hard to adapt to new do-
mains and new languages (Silla Jr. and Kaest-
ner, 2004);
? Word and sentence segmentation are often seen
as separate tasks, but they obviously inform
each other and it could be advantageous to view
them as a combined task;
? Most tokenization methods provide no align-
ment between raw and tokenized text, which
makes mapping the tokenized version back
onto the actual source hard or impossible.
In short, we believe that regarding tokenization,
there is still room for improvement, in particular on
the methodological side of the task. We are partic-
ularly interested in the following questions: Can we
use supervised learning to avoid hand-crafting rules?
Can we use unsupervised feature learning to reduce
feature engineering effort and boost performance?
Can we use the same method across languages? Can
we combine word and sentence boundary detection
into one task?
2 Related Work
Usually the text segmentation task is split into word
tokenization and sentence boundary detection. Rule-
based systems for finding word and sentence bound-
aries often are variations on matching hand-coded
regular expressions (Grefenstette, 1999; Silla Jr. and
Kaestner, 2004; Jurafsky and Martin, 2008; Dridan
and Oepen, 2012).
Several unsupervised systems have been proposed
for sentence boundary detection. Kiss and Strunk
(2006) present a language-independent, unsuper-
vised approach and note that abbreviations form a
major source of ambiguity in sentence boundary
detection and use collocation detection to build a
high-accuracy abbreviation detector. The resulting
system reaches high accuracy, rivalling handcrafted
rule-based and supervised systems. A similar sys-
tem was proposed earlier by Mikheev (2002).
Existing supervised learning approaches for sen-
tence boundary detection use as features tokens pre-
ceding and following potential sentence boundary,
part of speech, capitalization information and lists
of abbreviations. Learning methods employed in
1422
these approaches include maximum entropy models
(Reynar and Ratnaparkhi, 1997) decision trees (Ri-
ley, 1989), and neural networks (Palmer and Hearst,
1997).
Closest to our work are approaches that present
token and sentence splitters using conditional ran-
dom fields (Tomanek et al, 2007; Fares et al, 2013).
However, these previous approaches consider tokens
(i.e. character sequences) as basic units for labeling,
whereas we consider single characters. As a con-
sequence, labeling is more resource-intensive, but it
also gives us more expressive power. In fact, our ap-
proach kills two birds with one stone, as it allows us
to integrate token and sentence boundaries detection
into one task.
3 Method
3.1 IOB Tokenization
IOB tagging is widely used in tasks identifying
chunks of tokens. We use it to identify chunks of
characters. Characters outside of tokens are labeled
O, inside of tokens I. For characters at the beginning
of tokens, we use S at sentence boundaries, other-
wise T (for token). This scheme offers some nice
features, like allowing for discontinuous tokens (e.g.
hyphenated words at line breaks) and starting a new
token in the middle of a typographic word if the to-
kenization scheme requires it, as e.g. in did|n?t. An
example is given in Figure 1.
It didn?t matter if the faces were male,
SIOTIITIIOTIIIIIOTIOTIIOTIIIIOTIIIOTIIITO
female or those of children. Eighty-
TIIIIIOTIOTIIIIOTIOTIIIIIIITOSIIIIIIO
three percent of people in the 30-to-34
IIIIIOTIIIIIIOTIOTIIIIIOTIOTIIOTIIIIIIIO
year old age range gave correct responses.
TIIIOTIIOTIIOTIIIIOTIIIOTIIIIIIOTIIIIIIIIT
Figure 1: Example of IOB-labeled characters
3.2 Datasets
In our experiments we use three datasets to compare
our method for different languages and for different
domains: manually checked English newswire texts
taken from the Groningen Meaning Bank, GMB
(Basile et al, 2012), Dutch newswire texts, com-
prising two days from January 2000 extracted from
the Twente News Corpus, TwNC (Ordelman et al,
2007), and a random sample of Italian texts from the
PAISA` corpus (Borghetti et al, 2011).
Table 1: Datasets characteristics.
Name Language Domain Sentences Tokens
GMB English Newswire 2,886 64,443
TNC Dutch Newswire 49,537 860,637
PAI Italian Web/various 42,674 869,095
The data was converted into IOB format by infer-
ring an alignment between the raw text and the seg-
mented text.
3.3 Sequence labeling
We apply the Wapiti implementation (Lavergne et
al., 2010) of Conditional Random Fields (Lafferty
et al, 2001), using as features the output label of
each character, combined with 1) the character it-
self, 2) the output label on the previous character, 3)
characters and/or their Unicode categories from con-
text windows of varying sizes. For example, with a
context size of 3, in Figure 1, features for the E in
Eighty-three with the output label S would be E/S,
O/S, /S, i/S, Space/S, Lowercase/S. The intuition
is that the 31 existing Unicode categories can gen-
eralize across similar characters whereas character
features can identify specific contexts such as abbre-
viations or contractions (e.g. didn?t). The context
window sizes we use are 0, 1, 3, 5, 7, 9, 11 and 13,
centered around the focus character.
3.4 Deep learning of features
Automatically learned word embeddings have been
successfully used in NLP to reduce reliance on man-
ual feature engineering and boost performance. We
adapt this approach to the character level, and thus,
in addition to hand-crafted features we use text
representations induced in an unsupervised fashion
from character strings. A complete discussion of
our approach to learning text embeddings can be
found in (Chrupa?a, 2013). Here we provide a brief
overview.
Our representations correspond to the activation
of the hidden layer in a simple recurrent neural
(SRN) network (Elman, 1990; Elman, 1991), imple-
mented in a customized version of Mikolov (2010)?s
RNNLM toolkit. The network is sequentially pre-
sented with a large amount of raw text and learns to
1423
predict the next character in the sequence. It uses the
units in the hidden layer to store a generalized rep-
resentation of the recent history. After training the
network on large amounts on unlabeled text, we run
it on the training and test data, and record the activa-
tion of the hidden layer at each position in the string
as it tries to predict the next character. The vector of
activations of the hidden layer provides additional
features used to train and run the CRF. For each of
the K = 10 most active units out of total J = 400
hidden units, we create features (f(1) . . . f(K)) de-
fined as f(k) = 1 if sj(k) > 0.5 and f(k) = 0 oth-
erwise, where sj(k) returns the activation of the kth
most active unit. For training the SRN only raw text
is necessary. We trained on the entire GMB 2.0.0
(2.5M characters), the portion of TwNC correspond-
ing to January 2000 (43M characters) and a sample
of the PAISA` corpus (39M characters).
4 Results and Evaluation
In order to evaluate the quality of the tokenization
produced by our models we conducted several ex-
periments with different combinations of features
and context sizes. For these tests, the models are
trained on an 80% portion of the data sets and tested
on a 10% development set. Final results are obtained
on a 10% test set. We report both absolute number
of errors and error rates per thousand (?).
4.1 Feature sets
We experiment with two kinds of features at the
character level, namely Unicode categories (31 dif-
ferent ones), Unicode character codes, and a combi-
nation of them. Unicode categories are less sparse
than the character codes (there are 88, 134, and 502
unique characters for English, Dutch and Italian, re-
spectively), so the combination provide some gener-
alization over just character codes.
Table 2: Error rates obtained with different feature sets.
Cat stands for Unicode category, Code for Unicode char-
acter code, and Cat-Code for a union of these features.
Error rates per thousand (?)
Feature set English Dutch Italian
Cat-9 45 (1.40) 1,403 (2.87) 1,548 (2.67)
Code-9 6 (0.19) 782 (1.60) 692 (1.20)
Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)
From these results we see that categories alone
perform worse than only codes. For English there is
no gain from the combination over using only char-
acter codes. For Dutch and Italian there is an im-
provement, although it is only significant for Ital-
ian (p = 0.480 and p = 0.005 respectively, bino-
mial exact test). We use this feature combination in
the experiments that follow. Note that these models
are trained using a symmetrical context of 9 charac-
ters (four left and four right of the current character).
In the next section we show performance of models
with different window sizes.
4.2 Context window
We run an experiment to evaluate how the size of the
context in the training phase impacts the classifica-
tion. In Table 4.2 we show the results for symmetri-
cal windows ranging in size from 1 to 13.
Table 3: Using different context window sizes.
Error rates per thousand (?)
Feature set English Dutch Italian
Cat-Code-1 273 (8.51) 4,924 (10.06) 9,108 (15.86)
Cat-Code-3 118 (3.68) 3,525 (7.20) 2,013 (3.51)
Cat-Code-5 20 (0.62) 930 (1.90) 788 (1.37)
Cat-Code-7 10 (0.31) 778 (1.60) 667 (1.16)
Cat-Code-9 8 (0.25) 774 (1.58) 657 (1.14)
Cat-Code-11 9 (0.28) 761 (1.56) 692 (1.21)
Cat-Code-13 8 (0.25) 751 (1.54) 670 (1.17)
4.3 SRN features
We also tested the automatically learned features de-
rived from the activation of the hidden layer of an
SRN language model, as explained in Section 3.
We combined these features with character code and
Unicode category features in windows of different
sizes. The results of this test are shown in Table 4.
The first row shows the performance of SRN fea-
tures on their own. The following rows show the
combination of SRN features with the basic feature
sets of varying window size. It can be seen that aug-
menting the feature sets with SRN features results
in large reductions of error rates. The Cat-Code-1-
SRN setting has error rates comparable to Cat-Code-
9.
The addition of SRN features to the two best
previous models, Cat-Code-9 and Cat-Code-13, re-
duces the error rate by 83% resp. 81% for Dutch,
1424
and by 24% resp. 26% for Italian. All these dif-
ferences are statistically significant according to the
binomial test (p < 0.001). For English, there are too
few errors to detect a statistically significant effect
for Cat-Code-9 (p = 0.07), but for Cat-Code-13 we
find p = 0.016.
Table 4: Results obtained using different context window
sizes and addition of SRN features.
Error rates per thousand (?)
Feature set English Dutch Italian
SRN 24 (0.75) 276 (0.56) 738 (1.28)
Cat-Code-1-SRN 7 (0.21) 212 (0.43) 549 (0.96)
Cat-Code-3-SRN 4 (0.13) 165 (0.34) 507 (0.88)
Cat-Code-5-SRN 3 (0.10) 136 (0.28) 476 (0.83)
Cat-Code-7-SRN 1 (0.03) 111 (0.23) 497 (0.86)
Cat-Code-9-SRN 2 (0.06) 135 (0.28) 497 (0.86)
Cat-Code-11-SRN 2 (0.06) 132 (0.27) 468 (0.81)
Cat-Code-13-SRN 1 (0.03) 142 (0.29) 496 (0.86)
In a final step, we selected the best models based
on the development sets (Cat-Code-7-SRN for En-
glish and Dutch, Cat-Code-11-SRN for Italian), and
checked their performance on the final test set. This
resulted in 10 errors (0.27 ?) for English (GMB
corpus), 199 errors (0.35 ?) for Dutch (TwNC cor-
pus), and 454 errors (0.76 ?) for Italian (PAISA`
corpus).
5 Discussion
It is interesting to examine what kind of errors the
SRN features help avoid. In the English and Dutch
datasets many errors are caused by failure to rec-
ognize personal titles and initials or misparsing of
numbers. In the Italian data, a large fraction of er-
rors is due to verbs with clitics, which are written as
a single word, but treated as separate tokens. Table 5
shows examples of errors made by a simpler model
that are fixed by adding SRN features. Table 6 shows
the confusion matrices for the Cat-Code-7 and Cat-
Code-7-SRN sets on the Dutch data. The mistake
most improved by SRN features is T/I with 89% er-
ror reduction (see also Table 5). The is also the most
common remaining mistake.
A comparison with other approaches is hard be-
cause of the difference in datasets and task defini-
tion (combined word/sentence segmentation). Here
we just compare our results for sentence segmenta-
tion (sentence F1 score) with Punkt, a state-of-the-
Table 5: Positive impact of SRN features.
Ms. Hughes will joi
Cat-Code-7 SIIOSIIIIIOTIIIOTII
Cat-Code-7-SRN SIIOTIIIIIOTIIIOTII
$ 3.9 trillion by t
Cat-Code-7 TOTTIOTIIIIIIIOTIOT
Cat-Code-7-SRN TOTIIOTIIIIIIIOTIOT
bleek 0,4 procent
Cat-Code-11 OTIIIIOTTIOTIIIIIIO
Cat-Code-11-SRN OTIIIIOTIIOTIIIIIIO
toebedeeld: 6,2. In
Cat-Code-11 TIIIIIIIIITOTTITOSI
Cat-Code-11-SRN TIIIIIIIIITOTIITOSI
prof. Teulings het
Cat-Code-11 TIIITOSIIIIIIIOTIIO
Cat-Code-11-SRN TIIIIOTIIIIIIIOTIIO
per costringerlo al
Cat-Code-11 TIIOTIIIIIIIIIIIOTI
Cat-Code-11-SRN TIIOTIIIIIIIIITIOTI
Table 6: Confusion matrix for Dutch development set.
Predicted, Cat-Code-7 Predicted, Cat-Code-7-SRN
Gold I O S T I O S T
I 328128 0 2 469 328546 0 0 53
O 0 75234 0 0 0 75234 0 0
S 4 0 4323 18 1 0 4332 12
T 252 0 33 80828 35 0 10 81068
art sentence boundary detection system (Kiss and
Strunk, 2006). With its standard distributed mod-
els, Punkt achieves 98.51% on our English test set,
98.87% on Dutch and 98.34% on Italian, compared
with 100%, 99.54% and 99.51% for our system. Our
system benefits here from its ability to adapt to a new
domain with relatively little (but annotated) training
data.
6 What Elephant?
Word and sentence segmentation can be recast as a
combined tagging task. This way, tokenization is
cast as a supervised learning task, causing a shift of
labor from writing rules to manually correcting la-
bels. Learning this task with CRF achieves high ac-
curacy.1 Furthermore, our tagging method does not
lose the connection between original text and tokens.
In future work, we plan to broaden the scope of
this work to other steps in document preparation,
1All software needed to replicate our experiments is
available at http://gmb.let.rug.nl/elephant/
experiments.php
1425
such as normalization of punctuation, and their in-
teraction with segmentation. We further plan to test
our method on a wider range of datasets, allowing a
more direct comparison with other approaches. Fi-
nally, we plan to explore the possibility of a statis-
tical universal segmentation model for mutliple lan-
guages and domains.
In a famous scene with a live elephant on stage,
the comedian Jimmy Durante was asked about it by
a policeman and surprisedly answered: ?What ele-
phant?? We feel we can say the same now as far as
tokenization is concerned.
References
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istanbul,
Turkey.
Claudia Borghetti, Sara Castagnoli, and Marco Brunello.
2011. I testi del web: una proposta di classificazione
sulla base del corpus PAISA`. In M. Cerruti, E. Corino,
and C. Onesti, editors, Formale e informale. La vari-
azione di registro nella comunicazione elettronica,
pages 147?170. Carocci, Roma.
Grzegorz Chrupa?a. 2013. Text segmentation with
character-level text embeddings. In ICML Workshop
on Deep Learning for Audio, Speech and Language
Processing, Atlanta, USA.
Rebecca Dridan and Stephan Oepen. 2012. Tokeniza-
tion: Returning to a long solved problem ? a survey,
contrastive experiment, recommendations, and toolkit
?. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 2:
Short Papers), pages 378?382, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time. Cog-
nitive science, 14(2):179?211.
Jeffrey L. Elman. 1991. Distributed representations,
simple recurrent networks, and grammatical structure.
Machine learning, 7(2):195?225.
Murhaf Fares, Stephan Oepen, and Zhang Yi. 2013. Ma-
chine learning for high-quality tokenization - replicat-
ing variable tokenization schemes. In A. Gelbukh, ed-
itor, CICLING 2013, volume 7816 of Lecture Notes in
Computer Science, pages 231?244, Berlin Heidelberg.
Springer-Verlag.
Gregory Grefenstette. 1999. Tokenization. In Hans van
Halteren, editor, Syntactic Wordclass Tagging, pages
117?133. Kluwer Academic Publishers, Dordrecht.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing. An Introduction to Natural
Language Processing, Computational Linguistics, and
Speech Recognition. Prentice Hall, 2nd edition.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multi-
lingual sentence boundary detection. Computational
Linguistics, 32(4):485?525.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML-01, pages 282?289.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 504?513, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Andrei Mikheev. 2002. Periods, capitalized words, etc.
Computational Linguistics, 28(3):289?318.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Interspeech.
Roeland Ordelman, Franciska de Jong, Arjan van Hessen,
and Hendri Hondorp. 2007. TwNC: a multifaceted
Dutch news corpus. ELRA Newsleter, 12(3/4):4?7.
David D. Palmer and Marti A. Hearst. 1997. Adap-
tive multilingual sentence boundary disambiguation.
Computational Linguistics, 23(2):241?267.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the Fifth Conference
on Applied Natural Language Processing, pages 16?
19, Washington, DC, USA. Association for Computa-
tional Linguistics.
Michael D. Riley. 1989. Some applications of tree-based
modelling to speech and language. In Proceedings of
the workshop on Speech and Natural Language, HLT
?89, pages 339?352, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Carlos N. Silla Jr. and Celso A. A. Kaestner. 2004. An
analysis of sentence boundary detection systems for
English and Portuguese documents. In Fifth Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, volume 2945 of Lecture
Notes in Computer Science, pages 135?141. Springer.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. Sentence and token splitting based on condi-
tional random fields. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 49?57, Melbourne, Australia.
1426
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 85?93,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Lemmatization and Lexicalized Statistical Parsing of Morphologically Rich
Languages: the Case of French
Djam? Seddah
Alpage Inria & Univ. Paris-Sorbonne
Paris, France
Grzegorz Chrupa?a
Spoken Language System, Saarland Univ.
Saarbr?cken, Germany
?zlem ?etinog?lu and Josef van Genabith
NCLT & CNGL, Dublin City Univ.
Dublin, Ireland
Marie Candito
Alpage Inria & Univ. Paris 7
Paris, France
Abstract
This paper shows that training a lexicalized
parser on a lemmatized morphologically-rich
treebank such as the French Treebank slightly
improves parsing results. We also show that
lemmatizing a similar in size subset of the En-
glish Penn Treebank has almost no effect on
parsing performance with gold lemmas and
leads to a small drop of performance when au-
tomatically assigned lemmas and POS tags are
used. This highlights two facts: (i) lemmati-
zation helps to reduce lexicon data-sparseness
issues for French, (ii) it also makes the pars-
ing process sensitive to correct assignment of
POS tags to unknown words.
1 Introduction
Large parse-annotated corpora have led to an explo-
sion of interest in statistical parsing methods, includ-
ing the development of highly successful models for
parsing English using the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1994)). Over the
last 10 years, parsing performance on the PTB has
hit a performance plateau of 90-92% f-score using
the PARSEVAL evaluation metric. When adapted to
other language/treebank pairs (such as German, He-
brew, Arabic, Italian or French), to date these mod-
els have performed much worse.
A number of arguments have been advanced
to explain this performance gap, including limited
amounts of training data, differences in treebank an-
notation schemes, inadequacies of evaluation met-
rics, linguistic factors such as the degree of word or-
der variation, the amount of morphological informa-
tion available to the parser as well as the effects of
syncretism prevalent in many morphologically rich
languages.
Even though none of these arguments in isola-
tion can account for the systematic performance gap,
a pattern is beginning to emerge: morphologically
rich languages tend to be susceptible to parsing per-
formance degradation.
Except for a residual clitic case system, French
does not have explicit case marking, yet its mor-
phology is considerably richer than that of English,
and French is therefore a candidate to serve as an
instance of a morphologically rich language (MRL)
that requires specific treatment to achieve reasonable
parsing performance.
Interestingly, French also exhibits a limited
amount of word order variation occurring at dif-
ferent syntactic levels including (i) the word level
(e.g. pre or post nominal adjective, pre or post ver-
bal adverbs); (ii) phrase level (e.g. possible alterna-
tions between post verbal NPs and PPs). In order
to avoid discontinuous constituents as well as traces
and coindexations, treebanks for this language, such
as the French Treebank (FTB, (Abeill? et al, 2003))
or the Modified French Treebank (MFT, (Schluter
and van Genabith, 2007)), propose a flat annota-
tion scheme with a non-configurational distinction
between adjunct and arguments.
Finally, the extraction of treebank grammars from
the French treebanks, which contain less than a third
of the annotated data as compared to PTB, is subject
to many data sparseness issues that contribute to a
performance ceiling, preventing the statistical pars-
ing of French to reach the same level of performance
as for PTB-trained parsers (Candito et al, 2009).
This data sparseness bottleneck can be summa-
rized as a problem of optimizing a parsing model
along two axes: the grammar and the lexicon. In
both cases, the goal is either to get a more compact
grammar at the rule level or to obtain a consider-
85
ably less sparse lexicon. So far, both approaches
have been tested for French using different means
and with different degrees of success.
To obtain better grammars, Schluter and van Gen-
abith (2007) extracted a subset of an early release
of the FTB and carried out extensive restructuring,
extensions and corrections (referred to as the Modi-
fied French Treebank MFT) to support grammar ac-
quisition for PCFG-based LFG Parsing (Cahill et
al., 2004) while Crabb? and Candito (2008) slightly
modified the original FTB POS tagset to optimize
the grammar with latent annotations extracted by the
Berkeley parser (BKY, (Petrov et al, 2006)).
Moreover, research oriented towards adapting
more complex parsing models to French showed
that lexicalized models such as Collins? model 2
(Collins, 1999) can be tuned to cope effectively with
the flatness of the annotation scheme in the FTB,
with the Charniak model (Charniak, 2000) perform-
ing particularly well, but outperformed by the BKY
parser on French data (Seddah et al, 2009).
Focusing on the lexicon, experiments have been
carried out to study the impact of different forms of
word clustering on the BKY parser trained on the
FTB. Candito et al (2009) showed that using gold
lemmatization provides a significant increase in per-
formance. Obviously, less sparse lexical data which
retains critical pieces of information can only help a
model to perform better. This was shown in (Candito
and Crabb?, 2009) where distributional word clus-
ters were acquired from a 125 million words corpus
and combined with inflectional suffixes extracted
from the training data. Training the BKY parser
with 1000 clusters boosts its performance to the cur-
rent state-of-the-art with a PARSEVAL F1 score of
88.28% (baseline was 86.29 %).
We performed the same experiment using the
CHARNIAK parser and recorded only a small im-
provement (from 84.96% to 85.51%). Given the
fact that lexical information is crucial for lexicalized
parsers in the form of bilexical dependencies, this
result raises the question whether this kind of clus-
tering is in fact too drastic for lexicalized parsers as
it may give rise to head-to-head dependencies which
are too coarse. To answer this question, in this paper
we explore the impact of lemmatization, as a (rather
limited) constrained form of clustering, on a state-
of-the-art lexicalized parser (CHARNIAK). In order
to evaluate the influence of lemmatization on this
parser (which is known to be highly tuned for En-
glish) we carry out experiments on both the FTB and
on a lemmatized version of the PTB. We used gold
lemmatization when available and an automatic sta-
tistical morphological analyzer (Chrupa?a, 2010) to
provide more realistic parsing results.
The idea is to verify whether lemmatization will help
to reduce data sparseness issues due to the French
rich morphology and to see if this process, when
applied to English will harm the performance of a
parser optimized for the limited morphology of En-
glish.
Our results show that the key issue is the way un-
seen tokens (lemmas or words) are handled by the
CHARNIAK parser. Indeed, using pure lemma is
equally suboptimal for both languages. On the other
hand, feeding the parser with both lemma and part-
of-speech slightly enhances parsing performance for
French.
We first describe our data sets in Section 2, intro-
duce our data driven morphology process in Section
3, then present experiments in Section 4. We dis-
cuss our results in Section 5 and compare them with
related research in Section 6 before concluding and
outlining further research.
2 Corpus
THE FRENCH TREEBANK is the first annotated and
manually corrected treebank for French. The data is
annotated with labeled constituent trees augmented
with morphological annotations and functional an-
notations of verbal dependents. Its key properties,
compared with the PTB, are the following :
Size: The FTB consists of 350,931 tokens and
12,351 sentences, that is less than a third of the size
of PTB. The average length of a sentence is 28.41
tokens. By contrast, the average sentence length in
the Wall Street Journal section of the PTB is 25.4
tokens.
A Flat Annotation Scheme: Both the FTB and the
PTB are annotated with constituent trees. However,
the annotation scheme is flatter in the FTB. For in-
stance, there are no VPs for finite verbs and only one
sentential level for clauses or sentences whether or
not they are introduced by a complementizer. Only
the verbal nucleus (VN) is annotated and comprises
86
the verb, its clitics, auxiliaries, adverbs and nega-
tion.
Inflection: French morphology is richer than En-
glish and leads to increased data sparseness for sta-
tistical parsing. There are 24,098 lexical types in
the FTB, with an average of 16 tokens occurring for
each type.
Compounds: Compounds are explicitly annotated
and very frequent in the treebank: 14.52% of to-
kens are part of a compound. Following Candito
and Crabb? (2009), we use a variation of the tree-
bank where compounds with regular syntactic pat-
terns have been expanded. We refer to this instance
as FTB-UC.
Lemmatization: Lemmas are included in the tree-
bank?s morphological annotations and denote an ab-
straction over a group of inflected forms. As there
is no distinction between semantically ambiguous
lexemes at the word form level, polysemic homo-
graphs with common inflections are associated with
the same lemma (Abeill? et al, 2003). Thus, except
for some very rare cases, a pair consisting of a word
form and its part-of-speech unambiguously maps to
the same lemma.
2.1 Lemmatizing the Penn Treebank
Unlike the FTB, the PTB does not have gold lem-
mas provided within the treebank. We use the finite
state morphological analyzer which comes within
the English ParGram Grammar (Butt et al, 1999) for
lemmatization. For open class words (nouns, verbs,
adjectives, adverbs) the word form is sent to the mor-
phological analyzer. The English ParGram morpho-
logical analyzer outputs all possible analyses of the
word form. The associated gold POS from the PTB
is used to disambiguate the result. The same process
is applied to closed class words where the word form
is different from the lemma (e.g. ?ll for will). For the
remaining parts of speech the word form is assigned
to the lemma.
Since gold lemmas are not available for the PTB,
a large-scale automatic evaluation of the lemmatizer
is not possible. Instead, we conducted two manual
evaluations. First, we randomly extracted 5 sam-
ples of 200 <POS,word> pairs from Section 23 of
the PTB. Each data set is fed into the lemmatiza-
tion script, and the output is manually checked. For
the 5x200 <POS,word> sets the number of incorrect
lemmas is 1, 3, 2, 0, and 2. The variance is small
indicating that the results are fairly stable. For the
second evaluation, we extracted each unseen word
from Section 23 and manually checked the accuracy
of the lemmatization. Of the total of 1802 unseen
words, 394 words are associated with an incorrect
lemma (331 unique) and only 8 with an incorrect
<POS,lemma> pair (5 unique). For an overall un-
seen word percentage of 3.22%, the lemma accu-
racy is 77.70%. If we assume that all seen words
are correctly lemmatized, overall accuracy would be
99.28%.
2.2 Treebank properties
In order to evaluate the influence of lemmatization
on comparable corpora, we extracted a random sub-
set of the PTB with properties comparable to the
FTB-UC (mainly with respect to CFG size and num-
ber of tokens). We call this PTB subset S.PTB. Ta-
ble 1 presents a summary of some relevant features
of those treebanks.
FTBUC S.PTB PTB
# of tokens 350,931 350,992 1,152,305
# of sentences 12,351 13,811 45,293
average length 28,41 25.41 25.44
CFG size 607,162 638,955 2,097,757
# unique CFG rules 43,413 46,783 91,027
# unique word forms 27,130 26,536 47,678
# unique lemmas 17,570 20,226 36,316
ratio words/lemma 1.544 1.311 1.312
Table 1: French and Penn Treebanks properties
Table 1 shows that the average number of word
forms associated with a lemma (i.e. the lemma ratio)
is higher in the FTB-UC (1.54 words/lemma) than in
the PTB (1.31). Even though the PTB ratio is lower,
it is still large enough to suggest that even the limited
English morphology should be taken into account
when aiming at reducing lexicon sparseness.
Trying to learn French and English morphology
in a data driven fashion in order to predict lemma
from word forms is the subject of the next section.
3 Morphology learning
In order to assign morphological tags and lemmas
to words we use the MORFETTE model (Chrupa?a,
2010), which is a variation of the approach described
in (Chrupa?a et al, 2008).
87
MORFETTE is a sequence labeling model which
combines the predictions of two classification mod-
els (one for morphological tagging and one for
lemmatization) at decoding time, using beam search.
3.1 Overview of the Morfette model
The morphological classes correspond simply to the
(fine-grained) POS tags. Lemma classes are edit
scripts computed from training data: they specify
which string manipulations (such as character dele-
tions and insertions) need to be performed in order
to transform the input string (word form) into the
corresponding output string (lemma).
The best sequence of lemmas and morphological
tags for input sentence x is defined as:
(?l, m?) = arg max
(l,m)
P (l,m|x)
The joint probability is decomposed as follows:
P (l0...li,m0...mi|x) =PL(li|mi,x)PM (mi|x)
? P (m0...mi?1, l0...li?1|x)
where PL(li|mi,x) is the probability of lemma class
l at position i according to the lemma classifier,
PM (mi|x) is the probability of the tag m at posi-
tion i according to the morphological tag classifier,
and x is the sequence of words to label.
While Chrupa?a et al (2008) use Maximum En-
tropy training to learn PM and PL, here we learn
them using Averaged Perceptron algorithm due to
Freund and Schapire (1999). It is a much simpler
algorithm which in many scenarios (including ours)
performs as well as or better than MaxEnt.
We also use the general Edit Tree instantiation of
the edit script as developed in (Chrupa?a, 2008). We
find the longest common substring (LCS) between
the form w and the lemma w?. The portions of the
string in the word form before (prefix) and after (suf-
fix) the LCS need to be modified in some way, while
the LCS (stem) stays the same. If there is no LCS,
then we simply record that we need to replace w
with w? . As for the modifications to the prefix and
the suffix, we apply the same procedure recursively:
we try to find the LCS between the prefix of w and
the prefix of w?. If we find one, we recurse; if we do
not, we record the replacement; we do the same for
the suffix.
3.2 Data Set
We trained MORFETTE on the standard splits of the
FTB with the first 10% as test set, the next 10% for
the development set and the remaining for training
(i.e. 1235/1235/9881 sentences). Lemmas and part-
of-speech tags are given by the treebank annotation
scheme.
As pointed out in section 2.1, PTB?s lemmas have
been automatically generated by a deterministic pro-
cess, and only a random subset of them have been
manually checked. For the remainder of this paper,
we treat them as gold, regardless of the errors in-
duced by our PTB lemmatizer.
The S.PTB follows the same split as the FTB-UC,
first 10% for test, next 10% for dev and the last 80%
for training (i.e. 1380/1381/11050 sentences).
MORFETTE can optionally use a morphological
lexicon to extract features. For French, we used the
extended version of Lefff (Sagot et al, 2006) and for
English, the lexicon used in the Penn XTAG project
(Doran et al, 1994). We reduced the granularity of
the XTAG tag set, keeping only the bare categories.
Both lexicons contain around 225 thousands word
form entries.
3.3 Performance on French and English
Table 2 presents results of MORFETTE applied to the
development and test sets of our treebanks. Part-of-
speech tagging performance for French is state-of-
the-art on the FTB-UC, with an accuracy of 97.68%,
on the FTB-UC test set, only 0.02 points (absolute)
below the MaxEnt POS tagger of Denis and Sagot
(2009). Comparing MORFETTE?s tagging perfor-
mance for English is a bit more challenging as we
only trained on one third of the full PTB and evalu-
ated on approximately one section, whereas results
reported in the literature are usually based on train-
ing on sections 02-18 and evaluating on either sec-
tions 19-21 or 22-24. For this setting, state-of-the-
art POS accuracy for PTB tagging is around 97.33%.
On our PTB sample, MORFETTE achieves 96.36%
for all words and 89.64 for unseen words.
Comparing the lemmatization performance for both
languages on the same kind of data is even more dif-
ficult as we are not aware of any data driven lem-
matizer on the same data. However, with an overall
accuracy above 98% for the FTB-UC (91.5% for un-
88
seen words) and above 99% for the S.PTB (95% for
unseen words), lemmatization performs well enough
to properly evaluate parsing on lemmatized data.
FTBUC S.PTB
DEV All Unk. (4.8) All Unk. (4.67)
POS acc 97.38 91.95 96.36 88.90
Lemma acc 98.20 92.52 99.11 95.51
Joint acc 96.35 87.16 96.26 87.05
TEST All Unk. (4.62) All Unk. (5.04)
POS acc 97.68 90.52 96.53 89.64
Lemma acc 98.36 91.54 99.13 95.72
Joint acc 96.74 85.28 96.45 88.49
Table 2: POS tagging and lemmatization performance on
the FTB and on the S.PTB
4 Parsing Experiments
In this section, we present the results of two sets
of experiments to evaluate the impact of lemmatiza-
tion on the lexicalized statistical parsing of two lan-
guages, one morphologically rich (French), but with
none of its morphological features exploited by the
CHARNIAK parser, the other (English) being quite
the opposite, with the parser developed mainly for
this language and PTB annotated data. We show that
lemmatization results in increased performance for
French, while doing the same for English penalizes
parser performance.
4.1 Experimental Protocol
Data The data sets described in section 3.2 are used
throughout. The version of the CHARNIAK parser
(Charniak, 2000) was released in August 2005 and
recently adapted to French (Seddah et al, 2009).
Metrics We report results on sentences of length
less than 40 words, with three evaluation met-
rics: the classical PARSEVAL Labeled brackets F1
score, POS tagging accuracy (excluding punctua-
tion tags) and the Leaf Ancestor metric (Sampson
and Babarczy, 2003) which is believed to be some-
what more neutral with respect to the treebank an-
notation scheme than PARSEVAL (Rehbein and van
Genabith, 2007).
Treebank tag sets Our experiments involve the in-
clusion of POS tags directly in tokens. We briefly
describe our treebank tag sets below.
? FTB-UC TAG SET: ?CC? This is the tag set de-
veloped by (Crabb? and Candito, 2008) (Table
4), known to provide the best parsing perfor-
mance for French (Seddah et al, 2009). Like in
the FTB, preterminals are the main categories,
but they are also augmented with a WH flag
for A, ADV, PRO and with the mood for verbs
(there are 6 moods). No information is propa-
gated to non-terminal symbols.
ADJ ADJWH ADV ADVWH CC CLO CLR CLS CS DET
DETWH ET I NC NPP P P+D P+PRO PONCT PREF PRO
PROREL PROWH V VIMP VINF VPP VPR VS
Table 4: CC tag set
? THE PTB TAG SET This tag set is described
at length in (Marcus et al, 1994) and contains
supplementary morphological information (e.g.
number) over and above what is represented in
the CC tag set for French. Note that some infor-
mation is marked at the morphological level in
English (superlative, ?the greatest (JJS)?) and
not in French (? le plus (ADV) grand (ADJ)?).
CC CD DT EX FW IN JJ JJR JJS LS MD NN NNP NNPS
NNS PDT POS PRP PRP$ RB RBR RBS RP SYM TO UH
VB VBD VBG VBN VBP VBZ WDT WP WP$ WRB
Table 5: PTB tag set
4.2 Cross token variation and parsing impact
From the source treebanks, we produce 5 versions
of tokens: tokens are generated as either simple
POS tag, gold lemma, gold lemma+gold POS, word
form, and word form+gold POS. The token versions
successively add more morphological information.
Parsing results are presented in Table 3.
Varying the token form The results show that
having no lexical information at all (POS-only) re-
sults in a small drop of PARSEVAL performance for
French compared to parsing lemmas, while the cor-
responding Leaf Ancestor score is actually higher.
For English having no lexical information at all
leads to a drop of 2 points in PARSEVAL. The so-
called impoverished morphology of English appears
to bring enough morphological information to raise
tagging performance to 95.92% (from POS-only to
word-only).
For French the corresponding gain is only 2 points
of POS tagging accuracy. Moreover, between these
89
Tokens
POS-only
lemma-only
word-only
(1)lemma-POS
(1)word-POS
French Treebank UC
F1 score Pos acc. leaf-Anc.
84.48 100 93.97
84.77 94.23 93.76
84.96 96.26 94.08
86.83(1) 98.79 94.65
86.13(2) 98.4 94.46
Sampled Penn Treebank
F1 score Pos acc. leaf-Anc.
85.62 100 94.02
87.69 89.22 94.92
88.64 95.92 95.10
89.59(3) 99.97 95.41
89.53(4) 99.96 95.38
Table 3: Parsing performance on the FTB-UC and the S.PTB with tokens variations using gold lemmas and gold POS.
( p-value (1) & (2) = 0.007; p-value (3) & (4) = 0.146. All other configurations are statistically significant.)
two tokens variations, POS-only and word-only,
parsing results gain only half a point in PARSEVAL
and almost nothing in leaf Ancestor.
Thus, it seems that encoding more morphology
(i.e. including word forms) in the tokens does not
lead to much improvement for parsing French as op-
posed to English. The reduction in data sparseness
due to the use of lemmas alone is thus not sufficient
to counterbalance the lack of morphological infor-
mation.
However, the large gap between POS tagging
accuracy seen between lemma-only and word-only
for English indicates that the parser makes use of
this information to provide at least reasonable POS
guesses.
For French, only 0.2 points are gained for PAR-
SEVAL results between lemma-only to word-only,
while POS accuracy benefits a bit more from includ-
ing richer morphological information.
This raises the question whether the FTB-UC pro-
vides enough data to make its richer morphology in-
formative enough for a parsing model.
Suffixing tokens with POS tags It is only when
gold POS are added to the lemmas that one can see
the advantage of a reduced lexicon for French. In-
deed, performance peaks for this setting (lemma-
POS). The situation is not as clear for English, where
performance is almost identical when gold POS are
added to lemmas or words. POS Tagging is nearly
perfect, thus a performance ceiling is reached. The
very small differences between those two configura-
tions (most noticeable with the Leaf Ancestor score
of 95.41 vs. 95.38) indicates that the reduced lemma
lexicon is actually of some limited use but its impact
is negligible compared to perfect tagging.
While the lemma+POS setting clearly boosts per-
formance for parsing the FTB, the situation is less
clear for English. Indeed, the lemma+POS and the
word+POS gold variations give almost the same re-
sults. The fact that the POS tagging accuracy is close
to 100% in this mode shows that the key parameter
for optimum parsing performance in this experiment
is the ability to guess POS for unknown words well.
In fact, the CHARNIAK parser uses a two letter
suffix context for its tagging model, and when gold
POS are suffixed to any type of token (being lemma
or word form), the PTB POS tagset is used as a sub-
stitute for lack of morphology.
It should also be noted that the FTB-UC tag set
does include some discriminative features (such as
PART, INF and so on) but those are expressed by
more than two letters, and therefore a two letter
suffix tag cannot really be useful to discriminate
a richer morphology. For example, in the PTB,
the suffix BZ, as in VBZ, always refers to a verb,
whereas the FTB pos tag suffix PP, as in NPP
(Proper Noun) is also found in POS labels such as
VPP (past participle verb).
4.3 Realistic Setup: Using Morfette to help
parsing
Having shown that parsing French benefits from a
reduced lexicon is not enough as results imply that a
key factor is POS tag guessing. We therefore test our
hypothesis in a more realistic set up. We use MOR-
FETTE to lemmatize and tag raw words (instead of
the ?gold? lemma-based approach described above),
and the resulting corpus is then parsed using the cor-
responding training set.
In order to be consistent with PARSEVAL POS eval-
uation, which does not take punctuation POS into
account, we provide a summary of MORFETTE?s
performance for such a configuration in (Table 6).
Results shown in Table 7 confirm our initial hy-
90
POS acc Lemma acc Joint acc
FTB-UC 97.34 98.12 96.26
S.PTB 96.15 99.04 96.07
Table 6: PARSEVAL Pos tagging accuracy of treebanks
test set
pothesis for French. Indeed, parsing performance
peaks with a setup involving automatically gener-
ated lemma and POS pairs, even though the differ-
ence with raw words+auto POS is not statistically
significant for the PARSEVAL F1 metric1. Note that
parser POS accuracy does not follow this pattern. It
is unclear exactly why this is the case. We specu-
late that the parser is helped by the reduced lexicon
but that performance suffers when a <lemma,POS>
pair has been incorrectly assigned by MORFETTE,
leading to an increase in unseen tokens. This is con-
firmed by parsing the same lemma but with gold
POS. In that case, parsing performance does not suf-
fer too much from CHARNIAK?s POS guessing on
unseen data.
For the S.PTB, results clearly show that both the
automatic <lemma,POS> and <word,POS> config-
urations lead to very similar results (yet statistically
significant with a F1 p-value = 0.027); having the
same POS accuracy indicates that most of the work
is done at the level of POS guessing for unseen
tokens, and in this respect the CHARNIAK parser
clearly takes advantage of the information included
in the PTB tag set.
F1 score Pos acc. leaf-Anc.
S.PTB
auto lemma only 87.11 89.82 94.71
auto lemma+auto pos (a) 88.15 96.21 94.85
word +auto pos (b) 88.28 96.21 94.88
F1 p-value: (a) and (b) 0.027
auto lemma+gold pos 89.51 99.96 95,36
FTB-UC
auto lemma only 83.92 92.98 93.53
auto lemma+auto pos (c) 85.06 96.04 94.14
word +auto pos (d) 84.99 96.47 94.09
F1 p-value: (c) and (d) 0.247
auto lemma+gold pos 86.39 97.35 94.68
Table 7: Realistic evaluation of parsing performance
1Statistical significance is computed using Dan Bikel?s
stratified shuffling implementation: www.cis.upenn.edu/
~dbikel/software.html.
5 Discussion
When we started this work, we wanted to explore
the benefit of lemmatization as a means to reduce
data sparseness issues underlying statistical lexical-
ized parsing of small treebanks for morphologically
rich languages, such as the FTB. We showed that
the expected benefit of lemmatization, a less sparse
lexicon, was in fact hidden by the absence of inflec-
tional information, as required by e.g. the CHAR-
NIAK parser to provide good POS guesses for un-
seen words. Even the inclusion of POS tags gen-
erated by a state-of-the-art tagger (MORFETTE) did
not lead to much improvement compared to a parser
run in a regular bare word set up.
An unexpected effect is that the POS accuracy
of the parser trained on the French data does not
reach the same level of performance as our tag-
ger (96.47% for <word, auto POS> vs. 97.34% for
MORFETTE). Of course, extending the CHARNIAK
tagging model to cope with lemmatized input should
be enough, because its POS guessing model builds
on features such as capitalization, hyphenation and
a two-letter suffix (Charniak, 2000). Those features
are not present in our current lemmatized input and
thus cannot be properly estimated.
CHARNIAK also uses the probability that a given
POS is realized by a previously unobserved word.
If any part of a <lemma,POS> pair is incorrect, the
number of unseen words in the test set would be
higher than the one estimated from the training set,
which only contained correct lemmas and POS tags
in our setting. This would lead to unsatisfying POS
accuracy. This inadequate behavior of the unknown
word tagging model may be responsible for the POS
accuracy result for <auto lemma> (cf. Table 7, lines
<auto lemma only> for both treebanks).
We believe that this performance degradation (or
in this case the somewhat less than expected im-
provement in parsing results) calls for the inclusion
of all available lexical information in the parsing
model. For example, nothing prevents a parsing
model to condition the generation of a head upon
a lemma, while the probability to generate a POS
would depend on both morphological features and
(potentially) the supplied POS.
91
6 Related Work
A fair amount of recent research in parsing morpho-
logically rich languages has focused on coping with
unknowns words and more generally with the small
and limited lexicons acquired from treebanks. For
instance, Goldberg et al (2009) augment the lex-
icon for a generative parsing model by including
lexical probabilities coming from an external lexi-
con. These are estimated using an HMM tagger with
Baum-Welch training. This method leads to a sig-
nificant increase of parsing performance over pre-
viously reported results for Modern Hebrew. Our
method is more stratified: external lexical resources
are included as features for MORFETTE and there-
fore are not directly seen by the parser besides gen-
erated lemma and POS.
For parsing German, Versley and Rehbein (2009)
cluster words according to linear context features.
The clusters are then integrated as features to boost a
discriminative parsing model to cope with unknown
words. Interestingly, they also include all possible
information: valence information, extracted from a
lexicon, is added to verbs and preterminal nodes are
annotated with case/number. This leads their dis-
criminative model to state-of-the-art results for pars-
ing German.
Concerning French, Candito and Crabb? (2009)
present the results of different clustering methods
applied to the parsing of FTB with the BKY parser.
They applied an unsupervised clustering algorithm
on the 125 millions words ?Est Republicain? corpus
to get a reduced lexicon of 1000 clusters which they
then augmented with various features such as capi-
talization and suffixes. Their method is the best cur-
rent approach for the probabilistic parsing of French
with a F1 score (<=40) of 88.29% on the standard
test set. We run the CHARNIAK parser on their clus-
terized corpus. Table 8 summarizes the current state-
of-the-art for lexicalized parsing on the FTB-UC.2
Clearly, the approach consisting in extending clus-
ters with features and suffixes seems to improve
CHARNIAK?s performance more than our method.
2For this comparison, we also trained the CHARNIAK parser
on a disinflected variation of the FTB-UC. Disinflection is a de-
terministic, lexicon based process, standing between stemming
and lemmatization, which preserves POS assignment ambigui-
ties (Candito and Crabb?, 2009).
In that case, the lexicon is drastically reduced, as
well as the amount of out of vocabulary words
(OOVs). Nevertheless, the relatively low POS ac-
curacy, with only 36 OOVs, for this configuration
confirms that POS guessing is the current bottleneck
if a process of reducing the lexicon increases POS
assignment ambiguities.
tokens F1 Pos acc % of OOVs
raw word (a) 84.96 96.26 4.89
auto <lemma,pos> (b) 85.06 96.04 6.47
disinflected (c) 85.45 96.51 3.59
cluster+caps+suffixes (d) 85.51 96.89 0.10
Table 8: CHARNIAK parser performance summary on the
FTB-UC test set (36340 tokens). Compared to (a), all F1 re-
sults, but (b), are statistically significant (p-values < 0.05), dif-
ferences between (c) & (d), (b) & (c) and (b) & (d) are not
(p-values are resp. 0.12, 0.41 and 0.11). Note that the (b) &
(d) p-value for all sentences is of 0.034, correlating thus the
observed gap in parsing performance between these two con-
figuration.
7 Conclusion
We showed that while lemmatization can be of
some benefit to reduce lexicon size and remedy data
sparseness for a MRL such as French, the key factor
that drives parsing performance for the CHARNIAK
parser is the amount of unseen words resulting from
the generation of <lemma,POS> pairs for the FTB-
UC. For a sample of the English PTB, morphologi-
cal analysis did not produce any significant improve-
ment.
Finally, even if this architecture has the potential to
help out-of-domain parsing, adding morphological
analysis on top of an existing highly tuned statisti-
cal parsing system can result in suboptimal perfor-
mance. Thus, in future we will investigate tighter
integration of the morphological features with the
parsing model.
Acknowledgments
D. Seddah and M. Candito were supported by the ANR
Sequoia (ANR-08-EMER-013); ?. ?etinog?lu and J.
van Genabith by the Science Foundation Ireland (Grant
07/CE/I1142) as part of the Centre for Next Generation
Localisation at Dublin City University; G. Chrupa?a by
BMBF project NL-Search (contract 01IS08020B).
92
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer, Dor-
drecht.
Miriam Butt, Mar?a-Eugenia Ni?o, and Fr?d?rique
Segond. 1999. A Grammar Writer?s Cookbook. CSLI
Publications, Stanford, CA.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 320?
327, Barcelona, Spain.
Marie Candito and Beno?t Crabb?. 2009. Im-
proving generative statistical parsing with semi-
supervised word clustering. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 138?141, Paris, France, Octo-
ber. Association for Computational Linguistics.
Marie Candito, Benoit Crabb?, and Djam? Seddah. 2009.
On statistical parsing of french with supervised and
semi-supervised strategies. In EACL 2009 Workshop
Grammatical inference for Computational Linguistics,
Athens, Greece.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics (NAACL 2000), pages 132?
139, Seattle, WA.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with morfette. In
In Proceedings of LREC 2008, Marrakech, Morocco.
ELDA/ELRA.
Grzegorz Chrupa?a. 2008. Towards a machine-learning
architecture for lexical functional grammar parsing.
Ph.D. thesis, Dublin City University.
Grzegorz Chrupa?a. 2010. Morfette: A tool for su-
pervised learning of morphology. http://sites.
google.com/site/morfetteweb/. Version
0.3.1.
Michael Collins. 1999. Head Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Pascal Denis and Beno?t Sagot. 2009. Coupling an anno-
tated corpus and a morphosyntactic lexicon for state-
of-the-art pos tagging with less human effort. In Proc.
of PACLIC, Hong Kong, China.
Christy Doran, Dania Egedi, Beth Ann Hockey, B. Srini-
vas, and Martin Zaidel. 1994. Xtag system: A wide
coverage grammar for english. In Proceedings of the
15th conference on Computational linguistics, pages
922?928, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine learning, 37(3):277?296.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In Proc. of EACL-09, pages 327?335, Athens, Greece.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for german.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague.
Benoit Sagot, Lionel Cl?ment, Eric V. de La Clergerie,
and Pierre Boullier. 2006. The lefff 2 syntactic lexi-
con for french: Architecture, acquisition, use. Proc. of
LREC 06, Genoa, Italy.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djam? Seddah, Marie Candito, and Benoit Crabb?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
93
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 27?32,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Relatedness Curves for Acquiring Paraphrases
Georgiana Dinu
Saarland University
Saarbruecken, Germany
dinu@coli.uni-sb.de
Grzegorz Chrupa?a
Saarland University
Saarbruecken, Germany
gchrupala@lsv.uni-saarland.de
Abstract
In this paper we investigate methods
for computing similarity of two phrases
based on their relatedness scores across
all ranks k in a SVD approximation of
a phrase/term co-occurrence matrix. We
confirm the major observations made in
previous work and our preliminary experi-
ments indicate that these methods can lead
to reliable similarity scores which in turn
can be used for the task of paraphrasing.
1 Introduction
Distributional methods for word similarity use
large amounts of text to acquire similarity judg-
ments based solely on co-occurrence statistics.
Typically each word is assigned a representation
as a point in a high dimensional space, where the
dimensions represent contextual features; follow-
ing this, vector similarity measures are used to
judge the meaning relatedness of words. One way
to make these computations more reliable is to use
Singular Value Decomposition (SVD) in order to
obtain a lower rank approximation of an original
co-occurrence matrix.
SVD is a matrix factorization method which
has applications in a large number of fields such
as signal processing or statistics. In natural lan-
guage processing methods such as Latent Seman-
tic Analysis (LSA) (Deerwester et al, 1990)
use SVD to obtain a factorization of a (typically)
word/document co-occurrence matrix. The under-
lying idea in these models is that the dimension-
ality reduction will produce meaningful dimen-
sions which represent concepts rather than just
terms, rendering similarity measures on these vec-
tors more accurate. Over the years, it has been
shown that these methods can closely match hu-
man similarity judgments and that they can be
used in various applications such as information
retrieval, document classification, essay grading
etc. However it has been noted that the success
of these methods is drastically determined by the
choice of dimension k to which the original space
is reduced.
(Bast and Majumdar, 2005) investigates exactly
this aspect and proves that no fixed choice of di-
mension is appropriate. The authors show that two
terms can be reliably compared only by investigat-
ing the curve of their relatedness scores over all
dimensions k. The authors use a term/document
matrix and analyze relatedness curves for inducing
a hard related/not-related decision and show that
their algorithms significantly improve over previ-
ous methods for information retrieval.
In this paper we investigate: 1) how the findings
of (Bast and Majumdar, 2005) carry over to ac-
quiring paraphrases using SVD on a phrase/term
co-occurrence matrix and 2) if reliable similarity
scores can be obtained from the analysis of relat-
edness curves.
2 Background
2.1 Singular Value Decomposition
Models such as LSA use Singular Value Decom-
position, in order to obtain term representations
over a space of concepts.
Given a co-occurrence matrix X of size (t, d),
we can compute the singular value decomposition:
U?V T of rank r. Matrices U and V T of sizes
(t, r) and (r, d) are the left and right singular vec-
tors; ? is the (r, r) diagonal matrix of singular
values (ordered in descending order)1. Similarity
between terms i and j is computed as the scalar
product between the two vectors associated to the
words in the U matrix:
sim(ui, uj) = ?
k
l=1uilujl
1Any approximation of rank k < r can simply be ob-
tained from an approximation or rank r by deleting rows and
columns.
27
2.2 Relatedness curves
Finding the optimal dimensionality k has proven
to be an extremely important and not trivial step.
(Bast and Majumdar, 2005) show that no single cut
dimension is appropriate to compute the similarity
of two terms but this should be deduced from the
curve of similarity scores over all dimensions k.
The curve of relatedness for two terms ui and uj is
given by their scalar product across all dimensions
k, k smaller than a rank r:
k ? ?kl=1uilujl, for k = 1, ..., r
They show that a smooth curve indicates closely
related terms, while a curve exhibiting many direc-
tion changes indicates unrelated terms; the actual
values of the similarity scores are often mislead-
ing, which explains why a good cut dimension k
is so difficult to find.
2.3 Vector space representation of phrases
We choose to apply this to acquiring paraphrases
(or inference rules, i.e. entailments which hold in
just one direction) in the sense of DIRT (Lin and
Pantel, 2001).
In the DIRT algorithm a phrase is a noun-
ending path in a dependency graph and the goal
is to acquire inference rules such as (X solve Y,
X find solution to Y). We will call dependency
paths patterns. The input data consists of large
amounts of parsed text, from which patterns to-
gether with X-filler and Y-filler frequency counts
are extracted.
In this setting, a pattern receives two vector rep-
resentation, one in a X-filler space and one in the
Y-filler space. In order to compute the similarity
between two patterns, these are compared in the
X space and in the Y space, and the two result-
ing scores are multiplied. (The DIRT algorithm
uses Lin measure for computing similarity, which
is given in Section 4). Obtaining these vectors
from the frequency counts is straightforward and
it is exemplified in Table 1 which shows a frag-
ment of a Y-filler DIRT-like vector space.
.. case problem ..
(X solve Y, Y) .. 6.1 4.4 ..
(X settle Y, Y) .. 5.2 5.9 ..
Table 1: DIRT-like vector representation in the Y-filler
space. The values represent mutual information.
3 Relatedness curves for acquiring
paraphrases
3.1 Setup
We parsed the XIE fragment of GigaWord (ap-
prox. 100 mil. tokens) with Stanford dependency
parser. From this we built a pattern/word matrix of
size (85000, 3000) containing co-occurrence data
of the most frequent patterns with the most fre-
quent words2. We perform SVD factorization on
this matrix of rank k = 800. For each pair of pat-
terns, we can associate two relatedness curves: a
X curve and Y curve given by the scalar products
of their vectors in the U matrix, across dimensions
k : 1, ..., 800.
3.2 Evaluating smoothness of the relatedness
curves
In Figure 1 we plotted the X and Y curves of com-
paring the pattern X subj???? win dobj???? Y with itself.
Figure 1: X-filler and Y-filler relatedness curves
for the identity pair (X subj???? win dobj???? Y,X subj????
win
dobj
???? Y )
Figure 2: X-filler and Y-filler relatedness curves
for (X subj???? leader prp??? of pobj???? Y,X pobj???? by prp???
lead
subj
???? Y )
Normally, the X and Y curves for the identical
pair are monotonically increasing. However what
can be noticed is that the actual values of these
functions differ by one order of magnitude in the
X and in the Y curves of identical patterns, show-
ing that in themselves they are not a good indica-
2Even if conceptually we have two semantic spaces (given
by X-fillers and Y-fillers), in reality we can work with a sin-
gle matrix, containing for each pattern also its reverse, both
represented solely in a X-filler space
28
Figure 3: X-filler and Y-filler relatedness curves
for (X subj???? win dobj???? Y,X subj???? murder dobj???? Y )
tor of similarity. In Figure 2 we investigate a pair
of closely related patterns: (X subj???? leader prp???
of
pobj
???? Y,X
pobj
???? by
prp
??? lead
subj
???? Y ). It can be
noticed that while still not comparable to those of
the identical pair, these curves are much smoother
than the ones associated to the pair of unrelated
patterns in Figure 33.
However, unlike in the information retrieval
scenario in (Bast and Majumdar, 2005), for which
a hard related/not-related assignment works best,
for acquiring paraphrases we need to quantify the
smoothness of the curves. We describe two func-
tions for evaluating curve smoothness which we
will use to compute scores in X-filler and Y-filler
semantic spaces.
Smooth function 1 This function simply com-
putes the number of changes in the direction of the
curve, as the percentage of times the scalar prod-
uct increases or remains equal from step l to step
l + 1:
CurveS1(ui, uj) =
?uilujl?01
k
, l = 1, ..., k
An increasing curve will be assigned the maximal
value 1, while for a curve that is monotonically
decreasing the score will be 0.
Smooth function 2 (Bast and Majumdar, 2005)
The second smooth function is given by:
CurveS2(ui, uj) =
max?min
?kl=1abs(uilujl)
where max and min are the largest and smallest
values in the curves. A curve which is always in-
creasing or always decreasing will get a score of 1.
Unlike the previous method this function is sensi-
tive to the absolute values in the drops of a curve.
3The drop out dimension discussed in (Bast and Majum-
dar, 2005) Section 3, does not seem to exist for our data. This
is to be expected since this result stems from a definition of
perfectly related terms which is adapted to the particularities
of term/document matrices, and not of term/term matrices.
A curve with large drops, irrelevant of their cardi-
nality, will be penalized by being assigned a low
score.
4 Experimental results
In order to compute the similarity score between
two phrases, we follow (Lin and Pantel, 2001)
and compute two similarity scores, corresponding
to the X-fillers and Y-fillers, and multiply them.
Given a similarity function, any pattern encoun-
tered in the corpus can be paraphrased by return-
ing its most similar patterns.
We implement five similarity functions on the
data we have described in the previous section.
The first one is the DIRT algorithm and it is the
only method using the original co-occurrence ma-
trix in which raw counts are replaced by point-
wise mutual information scores.
DIRT method The similarity function for two
vectors pi and pj is:
simLin(pi, pj) =
?
l?I(pi)?I(pj)
(pil + pjl)
?
l?I(pi)
pil +
?
l?I(pj)
pjl
where values in pi and pj are point-wise mu-
tual information, and I(?) gives the indices of non-
negative values in a vector.
Methods on SVD factorization All these meth-
ods perform computations the (85000, 800) U ma-
trix in the SVD factorization. On this we imple-
ment two methods which do an arbitrary dimen-
sion cut of k = 600: 1) SP-600 (scalar product)
and 2) COS-600 (cosine similarity). The other
two algorithms: CurveS1 and CurveS2 use the
two curve smoothness functions in Section 3.2; the
curves plot the scalar product corresponding to the
two patterns, from dimension 1 to 800.
Data In these preliminary experiments we limit
ourselves to paraphrasing a set of patterns ex-
tracted from a subset of the TREC02-TREC06
question answering tracks. From these questions
we extracted and paraphrased the most frequently
occurring 20 patterns. Since judging the cor-
rectness of these paraphrases ?out-of-context? is
rather difficult we limit ourselves to giving exam-
ples and analyzing errors made on this data; im-
portant observations can be clearly made this way,
however in future work we plan to build a proper
evaluation setting (e.g. task-based or instance-
based in the sense of (Szpektor et al, 2007)) for
29
a more detailed analysis of the performance on the
methods discussed.
4.1 Results
We list the paraphrases obtained with the different
methods for the pattern X subj???? show dobj???? Y . This
pattern has been chosen out of the total set due
to its medium difficulty in terms of paraphrasing;
some of the patterns in our list are relatively ac-
curately paraphrased by all methods, such as win,
while others such as marry are almost impossible
to paraphrase, for all methods. In Table 2 we list
the top 10 expansions returned by the four meth-
ods using the SVD factorization. In bold we mark
correct patterns, which we consider to be patterns
for which there is a context in which the entail-
ment holds in at least one direction.
As it is clearly reflected in this example the SP-
600 is much worse than any of the curve analy-
sis methods; however using cosine as similarity
measure at the same arbitrarily chosen dimension
(COS-600) brings major improvements.
The two curve smoothness methods exhibit a
systematic difference between them. In this ex-
ample, and also across all 20 instances we have
considered, CurveS1 ranks as most similar, a large
variety of patterns with the same lexical root (in
which, of course, syntax is often incorrect). Only
following this we can find patterns expressing lex-
ical variations; these again will be present in many
syntactic variations. This sets CurveS1 apart from
both CurveS2 and from COS-600 methods. These
latter two methods, although conceptually differ-
ent seem to exhibit surprisingly similar behavior.
The behavior of CurveS1 smoothing method is
difficult to judge without a proper evaluation; it
can be the case that the errors (mostly in syntac-
tic relations) are indeed errors of the algorithm or
that the parser introduces them already in our input
data.
Table 3 shows the top 10 paraphrases returned
by the DIRT algorithm. The DIRT paraphrases are
rather accurate, however it is interesting to observe
that DIRT and SVD methods can extract differ-
ent paraphrases. Table 4 gives examples of correct
paraphrases which are identified by DIRT but not
CurveS2 and the other way around. This seems to
indicate that these algorithms do capture different
aspects of the data and can be combined for bet-
ter results. An important aspect here is the fact
that obtaining highly accurate paraphrases at the
DIRT
subj
???? reflect
dobj
????
subj
???? indicate
dobj
????
subj
???? demonstrate
dobj
????
pobj
???? in
prp
??? show
dobj
????
pobj
???? to
prp
??? show
dobj
????
subj
???? represent
dobj
????
subj
???? show
prp
??? in
pobj
????
subj
???? display
dobj
????
subj
???? bring
dobj
????
pobj
???? with
prp
??? show
dobj
????
Table 3: Top 10 paraphrases for X subj???? show dobj????
Y
cost of losing coverage is not particularly difficult4
however not very useful. Previous work such as
(Dinu and Wang, 2009) has shown that for these
resources, the coverage is a rather important as-
pect, since they have to capture the great variety
of ways in which a meaning can be expressed in
different contexts.
CurveS2 DIRT
subj
???? show
dobj
????
pobj
???? in
prp
??? indicate
dobj
????
subj
???? display
dobj
????
pobj
???? in
prp
??? reflect
dobj
????
subj
???? confirm
dobj
????
dobj
???? interpret
prp
??? as
pobj
????
subj
???? point
prp
??? to
pobj
????
subj
???? win
dobj
????
subj
???? vie
prp
??? for
pobj
????
pos
??? victory
prp
??? in
pobj
????
subj
???? compete
prp
??? for
pobj
????
subj
???? win
dobj
???? title
nn
???
subj
???? secure
dobj
????
appos
????? winner
nn
???
subj
???? enter
dobj
????
subj
???? march
prp
??? into
pobj
????
subj
???? start
prp
??? in
pobj
????
subj
???? advance
prp
??? into
pobj
????
subj
???? play
prp
??? in
pobj
????
pos
??? entry
prp
??? to
pobj
????
subj
???? join
prp
??? in
pobj
????
Table 4: Example of paraphrases (i.e. ranked in
the top 30) identified by one method and not the
other
4.2 Discussion
In this section we attempt to get more insight into
the way the relatedness curves relate to the intu-
itive notion of similarity, by examining curves of
incorrect paraphrases extracted by our methods.
The first error we consider, is the pattern X pos???
confidence
pobj
???? of
prp
??? Y which is judged as be-
ing very similar to show by SP-600, COS-600 as
well as CurveS2. Figure 4 shows the relatedness
curves. As it can be noticed, both the X and Y
similarities grow dramatically around dimension
4High precision can be very easily achieved simply by in-
tersecting the sets of paraphrases returned by two or more of
the methods implemented
30
SP-600 COS-600 CurveS1 CurveS2
pos
??? confidence
pobj
???? of
prp
???
subj
???? indicate
dobj
????
subj
???? show
prp
??? in
pobj
????
subj
???? indicate
dobj
????
subj
???? boost
dobj
???? rate
nn
???
subj
???? show
prp
??? of
pobj
????
subj
???? indicate
dobj
????
subj
???? reflect
dobj
????
subj
???? show
prp
??? of
pobj
????
subj
???? represent
dobj
????
subj
???? show
prp
??? with
pobj
????
subj
???? represent
dobj
????
prp
??? to
pobj
???? percent
nn
???
pobj
???? by
prp
??? show
partmod
???????
pobj
???? with
prp
??? show
dobj
????
subj
???? bring
dobj
???? rate
nn
???
subj
???? total
dobj
???? yuan
appos
?????
pobj
???? in
prp
??? reflect
dobj
????
subj
???? show
tmod
?????
subj
???? show
prp
??? of
pobj
????
subj
???? hit
dobj
???? dollar
appos
?????
pos
??? confidence
pobj
???? of
prp
???
subj
???? show
prp
??? despite
pobj
????
dobj
???? interpret
prp
??? as
pobj
????
subj
???? reach
dobj
???? dollar
appos
?????
pobj
???? by
prp
??? reflect
dobj
????
pobj
???? during
prp
??? show
dobj
????
pos
??? confidence
pobj
???? of
prp
???
subj
???? slash
dobj
???? rate
nn
???
pobj
???? in
prp
??? indicate
dobj
????
pobj
???? in
prp
??? show
dobj
????
subj
???? show
dobj
???? rate
nn
???
nn
??? confidence
pobj
???? of
prp
???
subj
???? reflect
dobj
????
pobj
???? by
prp
??? show
partmod
???????
subj
???? put
dobj
???? rate
nn
???
subj
???? raise
dobj
???? rate
nn
???
subj
???? interpret
prp
??? as
pobj
????
pobj
???? on
prp
??? show
dobj
????
pobj
???? by
prp
??? show
partmod
???????
Table 2: Top 10 paraphrases for X subj???? show dobj???? Y
500. Therefore the scalar product will be very high
at cut point 600, leading to methods? SP-600 and
COS-600 error. However the two curve methods
are sensitive to the shape of the relatedness curves.
Since CurveS2 is sensitive to actual drop values in
these curves, this pair will still be ranked very sim-
ilar. The curves do decrease by small amounts in
many points which is why method CurveS1 does
score these two patterns as very similar.
An interesting point to be made here is that, this
pair is ranked similar by three methods out of four
because of the dramatic increase in relatedness at
around dimension 500. However, intuitively, such
an increase should be more relevant at earlier di-
mensions, which correspond to the larger eigen-
values, and therefore to the most relevant con-
cepts. Indeed, in the data we have analyzed, highly
similar patterns exhibit large increases at earlier
(first 100-200) dimensions, similarly to the exam-
ples given in Figure 1 and Figure 2. This leads
us to a particular aspect that we would like to in-
vestigate in future work, which is to analyze the
behavior of a relatedness curve in relation to rel-
evance weights obtained from the eigenvalues of
the matrix factorization.
In Figure 5 we plot a second error, the relat-
edness curves of show with X subj???? boost dobj????
rate
nn
??? Y which is as error made only by the SP-
600 method. The similarity reflected in curve Y
is relatively high (given by the large overlap of Y-
filler interest), however we obtain a very high X
similarity only due to the peak of the scalar prod-
uct exactly around the cut dimension 600.
5 Conclusion
In this paper we have investigated the relevance of
judging similarity of two phrases across all ranks
k in a SVD approximation of a phrase/term co-
Figure 4: X-filler and Y-filler relatedness curves
for (X subj???? show dobj???? Y,X pos??? confidence pobj????
of
prp
??? Y )
Figure 5: X-filler and Y-filler relatedness curves
for (X subj???? show dobj???? Y,X subj???? boost dobj????
rate
nn
??? Y )
occurrence matrix. We confirm the major observa-
tions made in previous work and our preliminary
experiments indicate that reliable similarity scores
for paraphrasing can be obtained from the analysis
of relatedness scores across all dimensions.
In the future we plan to 1) use the observations
we have made in Section 4.2 to focus on iden-
tifying good curve-smoothness functions and 2)
build an appropriate evaluation setting in order to
be able to accurately judge the performance of the
methods we propose.
Finally, in this paper we have investigated these
aspects for the task of paraphrasing in a particular
setting, however our findings can be applied to any
vector space method for semantic similarity.
31
References
Scott C. Deerwester and Susan T. Dumais and Thomas
K. Landauer and George W. Furnas and Richard A.
Harshman 1990. Indexing by Latent Semantic Anal-
ysis In JASIS.
Bast, Holger and Majumdar, Debapriyo. 2005. Why
spectral retrieval works. SIGIR ?05: Proceedings of
the 28th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval.
Dekang Lin and Patrick Pantel. 2001. DIRT - Discov-
ery of Inference Rules from Text. In Proceedings of
the ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009).
Idan Szpektor and Eyal Shnarch and Ido Dagan 2007.
Instance-based Evaluation of Entailment Rule Ac-
quisition. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics.
32
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 182?191,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Online Entropy-based Model of Lexical Category Acquisition
Grzegorz Chrupa?a
Saarland University
gchrupala@lsv.uni-saarland.de
Afra Alishahi
Saarland University
afra@coli.uni-saarland.de
Abstract
Children learn a robust representation of
lexical categories at a young age. We pro-
pose an incremental model of this process
which efficiently groups words into lexi-
cal categories based on their local context
using an information-theoretic criterion.
We train our model on a corpus of child-
directed speech from CHILDES and show
that the model learns a fine-grained set
of intuitive word categories. Furthermore,
we propose a novel evaluation approach
by comparing the efficiency of our induced
categories against other category sets (in-
cluding traditional part of speech tags) in
a variety of language tasks. We show the
categories induced by our model typically
outperform the other category sets.
1 The Acquisition of Lexical Categories
Psycholinguistic studies suggest that early on chil-
dren acquire robust knowledge of the abstract lex-
ical categories such as nouns, verbs and deter-
miners (e.g., Gelman & Taylor, 1984; Kemp et
al., 2005). Children?s grouping of words into
categories might be based on various cues, in-
cluding phonological and morphological proper-
ties of a word, the distributional information about
its surrounding context, and its semantic features.
Among these, the distributional properties of the
local context of a word have been thoroughly stud-
ied. It has been shown that child-directed speech
provides informative co-occurrence cues, which
can be reliably used to form lexical categories
(Redington et al, 1998; Mintz, 2003).
The process of learning lexical categories by
children is necessarily incremental. Human lan-
guage acquisition is bounded by memory and pro-
cessing limitations, and it is implausible that hu-
mans process large volumes of text at once and
induce an optimum set of categories. Efficient on-
line computational models are needed to investi-
gate whether distributional information is equally
useful in an online process of word categoriza-
tion. However, the few incremental models of
category acquisition which have been proposed
so far are generally inefficient and over-sensitive
to the properties of the input data (Cartwright &
Brent, 1997; Parisien et al, 2008). Moreover, the
unsupervised nature of these models makes their
assessment a challenge, and the evaluation tech-
niques proposed in the literature are limited.
The main contributions of our research are
twofold. First, we propose an incremental en-
tropy model for efficiently clustering words into
categories given their local context. We train our
model on a corpus of child-directed speech from
CHILDES (MacWhinney, 2000) and show that the
model learns a fine-grained set of intuitive word
categories. Second, we propose a novel evalua-
tion approach by comparing the efficiency of our
induced categories against other category sets, in-
cluding the traditional part of speech tags, in a va-
riety of language tasks. We evaluate our model on
word prediction (where a missing word is guessed
based on its sentential context), semantic inference
(where the semantic properties of a novel word are
predicted based on the context), and grammatical-
ity judgment (where the syntactic well-formedness
of a sentence is assessed based on the category la-
bels assigned to its words). The results show that
the categories induced by our model can be suc-
cessfully used in a variety of tasks and typically
perform better than other category sets.
1.1 Unsupervised Models of Category
Induction
Several computational models have used distri-
butional information for categorizing words (e.g.
Brown et al, 1992; Redington et al, 1998; Clark,
2000; Mintz, 2002). The majority of these mod-
182
els partition the vocabulary into a set of optimum
clusters (e.g., Brown et al, 1992; Clark, 2000).
The generated clusters are intuitive, and can be
used in different tasks such as word prediction
and parsing. Moreover, these models confirm the
learnability of abstract word categories, and show
that distributional cues are a useful source of in-
formation for this purpose. However, (i) they cat-
egorize word types rather than word tokens, and
as such provide no account of words belonging to
more than one category, and (ii) the batch algo-
rithms used by these systems make them implau-
sible for modeling human category induction. Un-
supervised models of PoS tagging such as Gold-
water & Griffiths (2007) do assign labels to word-
tokens, but they still typically use batch process-
ing, and what is even more problematic, they hard-
wire important aspects of the model, such as the
final number of categories.
Only few previously proposed models process
data incrementally, categorize word-tokens and do
not pre-specify a fixed category set. The model
of Cartwright & Brent (1997) uses an algorithm
which incrementally merges word clusters so that
a Minimum Description Length criterion for a
template grammar is optimized. The model treats
whole sentences as contextual units, which sacri-
fices a degree of incrementality, as well as making
it less robust to noise in the input.
Parisien et al (2008) propose a Bayesian clus-
tering model which copes with ambiguity and ex-
hibits the developmental trends observed in chil-
dren (e.g. the order of acquisition of different cat-
egories). However, their model is overly sen-
sitive to context variability, which results in the
creation of sparse categories. To remedy this is-
sue they introduce a ?bootstrapping? component
where the categories assigned to context words are
use to determine the category of the current target
word. They also perform periodical cluster reorga-
nization. These mechanisms improve the overall
performance of the model when trained on large
amounts of training data, but they complicate the
model with ad-hoc extensions and add to the (al-
ready considerable) computational load.
What is lacking is an incremental model of lex-
ical category which can efficiently process natu-
ralistic input data and gradually build robust cate-
gories with little training data.
1.2 Evaluation of the Induced Categories
There is no standard and straightforward method
for evaluating the unsupervised models of cate-
gory learning (see Clark, 2003, for discussion).
Many unsupervised models of lexical category ac-
quisition treat the traditional part of speech (PoS)
tags as the gold standard, and measure the accu-
racy and completeness of their induced categories
based on how closely they resemble the PoS cate-
gories (e.g. Redington et al, 1998; Mintz, 2003;
Parisien et al, 2008). However, it is not at all
clear whether humans form the same types of cate-
gories. In fact, many language tasks might benefit
from finer-grained categories than the traditional
PoS tags used for corpus annotation.
Frank et al (2009) propose a different, automat-
ically generated set of gold standard categories for
evaluating an unsupervised categorization model.
The gold-standard categories are formed accord-
ing to ?substitutability?: if one word can be re-
placed by another and the resulting sentence is still
grammatical, then there is a good chance that the
two words belong to the same category. They ex-
tract 3-word frames from the training data, and
form the gold standard categories based on the
words that appear in the same frame. They em-
phasize that in order to provide some degree of
generalization, different data sets must be used for
forming the gold-standard categories and perform-
ing the evaluation. However, the resulting cate-
gories are bound to be incomplete, and using them
as gold standard inevitably favors categorization
models which use a similar frame-based principle.
All in all, using any set of gold standard cate-
gories for evaluating an unsupervised categoriza-
tion model has the disadvantage of favoring one
set of principles and intuitions over another; that
is, assuming that there is a correct set of cate-
gories which the model should converge to. Al-
ternatively, automatically induced categories can
be evaluated based on how useful they are in per-
forming different tasks. This approach is taken by
Clark (2000), where the perplexity of a finite-state
model is used to compare different category sets.
We build on this idea and propose a more gen-
eral usage-based approach to evaluating the auto-
matically induced categories from a data set, em-
phasizing that the ultimate goal of a category in-
duction model is to form categories that can be ef-
ficiently used in a variety of language tasks. We
argue that for such tasks, a finer-grained set of cat-
183
egories might be more appropriate than the coarse-
grained PoS categories. Therefore, we propose a
number of tasks for which we compare the perfor-
mance based on various category sets, including
those induced by our model.
2 An Incremental Entropy-based Model
of Category Induction
A model of human category acquisition should
possess two key features:
? It should process input as it arrives, and incre-
mentally update the current set of clusters.
? The set of clusters should not be fixed in ad-
vance, but rather determined by the charac-
teristics of the input data.
We propose a simple algorithm which fulfills those
two conditions.
Our goal is to categorize word usages based on
the similarity of their form (the content) and their
surrounding words (the context). While grouping
word usages into categories, we attempt to trade
off two conflicting criteria. First, the categories
should be informative about the properties of their
members. Second, the number and distribution of
the categories should be parsimonious. An appro-
priate tool for formalizing both informativeness
and parsimony is information-theoretic entropy.
The parsimony criterion can be formalized as
the entropy of the random variable (Y ) represent-
ing the cluster assignments:
H(Y ) = ?
NX
i=1
P (Y = yi) log2(P (Y = yi)) (1)
where N is the number of clusters and P (Y = yi)
stands for the relative size of the ith cluster.
The informativeness criterion can be formalized
as the conditional entropy of training examples
(X) given the cluster assignments:
H(X|Y ) =
NX
i=1
P (Y = yi)H(X|Y = yi) (2)
and H(X|Y = yi) is calculated as
H(X|Y = yi) = ?
TX
j=1
[P (X = xj |Y = yi)
? log2(P (X = xj |Y = yi)] (3)
where T is the number of word usages in the train-
ing set.
The two criteria presented by Equations 1 and
2 can be combined together as the joint entropy of
the two random variables X and Y :
H(X,Y ) = H(X|Y ) +H(Y ) (4)
For a random variableX corresponding to a sin-
gle feature, minimizing the joint entropyH(X,Y )
will trade off our two desired criteria.
The joint entropy will be minimal if each dis-
tinct value of variable X is assigned the same cat-
egory (i.e. same value of Y ). There are many
assignments which satisfy this condition. They
range from putting all values of X in a single cat-
egory, to having a unique category for each unique
value of X . We favor the latter solution algorith-
mically by creating a new category in case of ties.
Finally, since our training examples contain a
bundle of categorical features, we minimize the
joint entropy simultaneously for all the features.
We consider our training examples to be vectors
of random variables (Xj)Mj=1, where each random
variable corresponds to one feature. For an incom-
ing example we will choose the cluster assignment
which leads to the least increase in the joint en-
tropy H(Xj , Y ), summed over all the features j:
MX
j=1
H(Xj , Y ) =
MX
j=1
?
H(Xj |Y ) +H(Y )
?
(5)
=
MX
j=1
?
H(Xj |Y )
?
+M ?H(Y )
In the next section, we present an incremental
algorithm which uses this criterion for inducing
categories from a sequence of input data.
The Incremental Algorithm. For each word us-
age that the model processes at time t, we need to
find the best category among the ones that have
been formed so far, as well as a potential new cat-
egory. The decision is made based on the change
in the function
?M
j=1H(Xj , Y ) (Equation 5) from
point t ? 1 to point t, as a result of assigning the
current input xt to a category y:
?Hty =
MX
j=1
?
Hty(Xj , Y )?H
t?1(Xj , Y )
?
(6)
whereHty(X,Y ) is the joint entropy of the assign-
ment Y for the input X = {x1, . . . , xt}, after the
last input item xt is assigned to the category y.
The winning category y? is the one that leads to the
smallest increase. Ties are broken by preferring a
new category.
y? =
(
argminy?{y}Ni=1 ?H
t
y if ?yn[?H
t
yn < ?H
t
yN+1 ]
yN+1 otherwise
(7)
184
where N is the number of categories created up to
point t, and yN+1 represents a new category.
Efficiency. We maintain the relative size P t(y)
and the entropy H(Xj |Y = y) for each category
y over time. When performing an assignment of xt
to a category yi, we only need to update the condi-
tional entropies H(Xj |Y = yi) for all features Xj
for this particular category, since other categories
have not changed. For a feature Xj at point t, the
change in the conditional entropy for the selected
category yi is given by:
?Htyi(Xj |Y ) = H
t
yi(Xj |Y )?H
t?1(Xj |Y )
=
X
yk 6=yi
?
P (Y = yk)H
t?1(Xj |Y = yi)
?
? P t?1(Y = yi)H
t?1(X|Y = yi)
? P t(Y = yi)H
t(Xj |Y = yi)
where only the last term depends on the current
time index t. Therefore, the entropy H(Xj |Y ) at
each step can be efficiently updated by calculating
this term for the modified category at that step.
A number of previous studies have considered
entropy-based criteria for clustering (e.g. Barbara?
et al, 2002; Li et al, 2004). The main contri-
bution of our proposed model is the emphasis on
rarely explored combination of the two character-
istics we consider crucial for modeling human cat-
egory acquisition, incrementality and an open set
of clusters.
3 Experimental Setup
We evaluate the categories formed by our model
through three different tasks. The first task is word
prediction, where a target word is predicted based
on the sentential context it appears in. The second
task is to infer the semantic properties of a novel
word based on its context. The third task is to as-
sess the grammaticality of a sentence tagged with
category labels. We run our model on a corpus of
child-directed speech, and use the categories that it
induces from that corpus in the above-mentioned
tasks. For each task, we compare the performance
using our induced categories against the perfor-
mance using other category sets. In the follow-
ing sections, we describe the properties of the data
sets used for training and testing the model, and
the formation of other category sets against which
we compare our model.
Data Set Sessions #Sentences #Words
Training 26?28 22, 491 125, 339
Development 29?30 15, 193 85, 361
Test 32?33 14, 940 84, 130
Table 1: Experimental data
3.1 Input Data
We use the Manchester corpus (Theakston et al,
2001) from CHILDES database (MacWhinney,
2000) as experimental data. The Manchester cor-
pus consists of conversations with 12 children be-
tween the ages of eighteen months to three years
old. The corpus is manually tagged using 60 PoS
labels. We use the mother?s speech from tran-
scripts of 6 children, remove punctuation, and con-
catenate the corresponding sessions.
We used data from three sessions as the training
set, two sessions as the development set, and two
sessions as the test set. We discarded all one-word
sentences from the data sets, as they do not pro-
vide any context for our evaluation tasks. Table 1
summarizes the properties of each data set.
3.2 Category Sets
We define each word usage in the training or test
data set as a vector of three categorical features:
the content feature (i.e., the focus word in a us-
age), and two context features (i.e. the preceding
and following bigrams). We ran our clustering al-
gorithm on the training set, which resulted in a
set of 944 categories (of which 442 have only one
member). Table 3 shows two sample categories
from the training set, and Figure 1 shows the size
distribution of the categories.
For each evaluation task, we use the following
category sets to label the test set:
?H. The categories induced by our entropy-
based model from the training set, as de-
scribed above.
PoS. The part-of-speech tags the Manchester cor-
pus is annotated with.
Words. The set of all the word types in the data
set (i.e. assuming that all the usages of the
same word form are grouped together).
Parisien. The induced categories by the model of
Parisien et al (2008) from the training set.
185
Gold PoS Words Parisien ?H
VI (0.000) 5.294 5.983 4.806
ARI (1.000) 0.139 0.099 0.168
Table 2: Comparison against gold PoS tags using
Variation of Information (VI) and Adjusted Rand
Index (ARI).
Sample Cluster 1
going (928)
doing (190)
back (150)
coming (80)
looking (76)
making (64)
playing (55)
taking (45)
. . .
Sample Cluster 2
than (45)
more (20)
silly (10)
bigger (9)
frightened (5)
dark (4)
harder (4)
funny (3)
. . .
Table 3: Sample categories induced from the train-
ing data. The frequency of each word in the cate-
gory is shown in parentheses.
For the first two tasks (word prediction and se-
mantic inference), we do not use the content fea-
ture in labeling the test set, since the assumption
underlying both tasks is that we do not have ac-
cess to the form of the target word. Therefore,
we do not measure the performance of these tasks
on the Words category set. However, we do use
the content feature in labeling the test examples in
grammaticality judgment.
For completeness, in Table 2 we report the
results of evaluation against Gold PoS tags us-
ing two metrics, Variation of Information (Meila,
2003) and Adjusted Rand Index (Hubert & Arabie,
1985).
4 Word Prediction
Humans can predict a word based on the context it
is used in with remarkable accuracy (e.g. Lesher et
al., 2002). Different versions of this task such as
Cloze Test (Taylor, 1953) are used for the assess-
ment of native and second language learning.
We simulate this task, where a missing word is
predicted based on its context. We use each of the
category sets introduced in Section 3.2 to label a
word usage in the test set, without using the word
form itself as a feature. That is, we assume that
the target word is unknown, and find the best cat-
egory for it based only on its surrounding context.
5 50 500 50001
2
5
10
20
501
00
Category size frequencies
Size
Frequ
ency
Figure 1: The distribution of the induced cate-
gories based on their size
We then output a ranked list of the content feature
values of the selected category as the prediction
of the model for the target word. To evaluate this
prediction, we use the reciprocal rank of the target
word in the predicted list.
The third row of Table 4 shows the Mean Re-
ciprocal Rank (MRR) over all the word usages in
the test data across different category sets. The re-
sults show that the category labels predicted by our
model (?H) perform much better than those of
Parisien, but still not as good as the gold-standard
part of speech categories. The fact that PoS tags
are better here does not necessarily mean that the
PoS category set is better for word prediction as
such, since they are manually assigned and thus
noise-free, unlike the automatic category labels
predicted by the two models. In the second set
of experiments described below we try to factor in
the uncertainty about category assignment inher-
ent in automatic labeling.
Using only the best category output by the
model to produce word predictions is simple and
neutral; however, it discards part of the informa-
tion learned by the model. We can predict words
more accurately by combining information from
the whole ranked list of category labels.
We use the ?H model to rank the values of the
content feature in the following fashion: for the
current test usage, we rank each cluster assign-
ment y by the change in the ?Htyi function that
it causes. For each of the assignments, we com-
pute the relative frequencies P (w|yi) of each pos-
sible focus word. The final rank of the word w in
context h is determined by the sum of the cluster-
186
Gold PoS Words Parisien ?H
Word Prediction (MRR) 0.354 - 0.212 0.309
Semantic Inference (MAP) 0.351 - 0.213 0.366
Grammaticality Judgment (Accuracy) 0.728 0.685 0.683 0.715
Table 4: The performance in each of the three tasks using different category sets.
dependent relative frequencies weighted by the
normalized reciprocal ranks of the clusters:
P (w|h) =
NX
i=1
P (w|yi)
R(yi|h)?1
PN
i=1 R(yi|h)
?1
(8)
where R(yi|h)?1 is the reciprocal rank of cluster
yi for context h according to the model.
We compare the performance of the ?H model
with this word-prediction method to that of an
n-gram language model, which is an established
technique for assigning probabilities to words
based on their context. For the language model
we use several n-gram orders (n = 1 . . . 5), and
smooth the n-gram counts using absolute dis-
counting (Zhai & Lafferty, 2004). The probability
of the word w given the context h is given by the
following model of order n:
Pn(w|h) = max
`
0,
c(h,w)? d
c(h)
?
+ ?(h)Pn?1(w|h) (9)
where d is the discount parameter, c(?) is the fre-
quency count function, Pn?1 is the lower-order
back-off distribution, and ? is the normalization
factor:
?(h) =
(
1 if r(h) = 0
dr(h) 1c(h) otherwise
(10)
and r(h) is the number of distinct words that fol-
low context h in the training corpus.
In addition to the ?H model and the n-gram
language models, we also report how well words
can be predicted from their manually assigned PoS
tags from CHILDES: for each token we predict the
most likely word given the token?s true PoS tag
based on frequencies in the training data.
Table 4 summarizes the evaluation results. The
?H model can predict missing words better than
any of the n-gram language models, and even
slightly better than the true POS tags. Given the
simplicity of our clustering model, this is a very
encouraging result. Simple n-gram language mod-
els are known for providing quite a strong base-
line for word prediction; for example, Brown et
al. (1992)?s class-based language model failed to
Model MRR
LM n = 1 0.1253
LM n = 2 0.2884
LM n = 3 0.3278
LM n = 4 0.3305
LM n = 5 0.3297
?H 0.3591
Gold POS 0.3540
Table 5: Mean reciprocal rank on the word predic-
tion task on the test set
improve test-set perplexity over a word-based tri-
gram model.
5 Semantic Inference
Several experimental studies have shown that chil-
dren and adults can infer (some aspects of) the se-
mantic properties of a novel word based on the
context it appears in (e.g. Landau & Gleitman,
1985; Gleitman, 1990; Naigles & Hoff-Ginsberg,
1995). For example, in an experimental study by
Fisher et al (2006), two-year-olds watched as a
hand placed a duck on a box, and pointed to it as a
new word was uttered. Half of the children heard
the word presented as a noun (This is a corp!),
while half heard it as a preposition (This is acorp
my box!). After training, children heard a test sen-
tence (What else is acorp (my box)?) while watch-
ing two test events: one showed another duck be-
side the box, and the other showed a different ob-
ject on the box. Looking-preferences revealed ef-
fects of sentence context: subjects in the preposi-
tion condition interpreted the novel word as a lo-
cation, whereas those in the noun condition inter-
preted it as an object.
To study a similar effect in our model, we as-
sociate each word with a set of semantic features.
For nouns, we extract the semantic features from
WordNet 3.0 (Fellbaum, 1998) as follows: We
take all the hypernyms of the first sense of the
word, and the first word in the synset of each
hypernym to the set of the semantic features of
187
ball
? GAME EQUIPMENT#1
? EQUIPMENT#1
? INSTRUMENTALITY#3, INSTRUMENTATION#1
? ARTIFACT#1, ARTEFACT#1
? WHOLE#2, UNIT#6
? OBJECT#1, PHYSICAL OBJECT#1
? PHYSICAL ENTITY#1
? ENTITY#1
ball: { GAME EQUIPMENT#1,EQUIPMENT#1,
INSTRUMENTALITY#3,ARTIFACT#1, ... }
Figure 2: Semantic features of ball, as extracted
from WordNet.
the target word (see Figure 2 for an example).
For verbs, we additionally extract features from
a verb-specific resource, VerbNet 2.3 (Schuler,
2005). Due to lack of proper resources for other
lexical categories, we limit our evaluation to nouns
and verbs.
The semantic features of words are not used in
the formation of lexical categories. However, at
each point of time in learning, we can associate
a semantic profile to a category as the aggregated
set of the semantic features of its members: each
feature in the set is assigned a count that indicates
the number of the category members which have
that semantic property. This is done for each of
the category sets described in Section 3.2.
As in the word-prediction task, we use differ-
ent category sets to label each word usage in a test
set based only on the context features of the word.
When the model encounters a novel word, it can
use the semantic profile of the word?s labeled cat-
egory as a prediction of the semantic properties of
that word. We can evaluate the quality of this pre-
diction by comparing the true meaning represen-
tation of the target word (i.e., its set of semantic
features according to the lexicon) against the se-
mantic profile of the selected category. We use the
Mean Average Precision (MAP) (Manning et al,
2008) for comparing the ranked list of semantic
features predicted by the model with the flat set
of semantic features extracted from WordNet and
VerbNet. Average Precision for a ranked list F
with respect to a set R of correct features is:
APR(F ) =
1
|R|
|F |X
r=1
P (r)? 1R(Fr) (11)
where P (r) is precision at rank r and 1R is the
indicator function of set R.
The middle row of Table 4 shows the MAP
scores over all the noun or verb usages in the
test set, based on four different category sets. As
can be seen, the categories induced by our model
(?H) outperform all the other category sets. The
word-type categories are particularly unsuitable
for this task, since they provide the least degree
of generalization over the semantic properties of
a group of words. The categories of Parisien
et al (2008) result in a better performance than
word types, but they are still too sparse for this
task. However, the average score gained by part of
speech tags is also lower than the one by our cat-
egories. This suggests that too broad categories
are also unsuitable for this task, since they can
only provide predictions about the most general
semantic properties, such as ENTITY for nouns,
and ACTION for verbs. These findings again con-
firm our hypothesis that a finer-grained set of cat-
egories that are extracted directly from the input
data provide the highest predictive power in a nat-
uralistic language task such as semantic inference.
6 Grammaticality Judgment
Speakers of a natural language have a general
agreement on the grammaticality of different sen-
tences. Grammaticality judgment has been viewed
as one of the main criteria for measuring how
well a language is learned by a human learner.
Experimental studies have shown that children as
young as five years old can judge the grammati-
cality of the sentences that they hear, and that both
children?s and adults? grammaticality judgments
are influenced by the distributional properties of
words and their context (e.g., Theakston, 2004).
Several methods have been proposed for auto-
matically distinguishing between grammatical and
ungrammatical usages (e.g., Wagner et al, 2007).
The ?shallow? methods are mainly based on n-
gram frequencies of words or categories in a cor-
pus, whereas the ?deep? methods treat a parsing
failure as an indication of a grammatical error.
Since our focus is on evaluating our category set,
we use trigram probabilities as a measure of gram-
maticality, using Equation 9 with n = 3.
As before, we label each test sentence using dif-
ferent category sets, and calculate the probability
for each trigram in that sentence. We define the
overall grammaticality score of a sentence as the
minimum of the probabilities of all the trigrams in
that sentence. Note that, unlike the previous tasks,
here we do use the content word as a feature in
188
labeling a test word usage. The actual word form
affects the grammaticality of its usage, and this in-
formation is available to the human subjects who
evaluate the grammaticality of a sentence.
Since we know of no publicly available corpus
of ungrammatical sentences, we artificially con-
struct one: for each sentence in our test data set,
we randomly move one word to another position.1
We define the accuracy of this task as the propor-
tion of the test usages for which the model calcu-
lates a higher grammaticality score for the original
sentence than for its ungrammatical version.
The last row of Table 4 shows the accuracy of
the grammaticality judgment task across different
category sets. As can be seen, the highest accu-
racy in choosing the grammatical sentence over
the ungrammatical one is achieved by using the
PoS categories (0.728), followed by the categories
induced by our model (0.715). These levels of ac-
curacy are rather good considering that some of
the automatically generated errors are also gram-
matical (e.g., there you are vs. you are there, or
can you reach it vs. you can reach it). The results
by the other two category sets are lower and very
close to each other.
These results suggest that, unlike the semantic
inference task, the grammaticality judgment task
might require a coarser-grained set of categories
which provide a higher level of abstraction. How-
ever, taking into account that the PoS categories
are manually assigned to the test usages, the dif-
ference in their performance might be due to lack
of noise in the labeling procedure. We plan to in-
vestigate this matter in future by improving our
categorization model (as discussed in Section 7).
Also, we intend to implement more accurate ways
of estimating grammaticality, using an approach
similar to that described for word prediction task
in Section 4.
7 Discussion
We have proposed an incremental model of lexi-
cal category acquisition based on the distributional
properties of words. Our model uses an informa-
tion theoretic clustering algorithm which attempts
to optimize the category assignments of the in-
coming word usages at each point in time. The
model can efficiently process the training data, and
induce an intuitive set of categories from child-
directed speech. However, due to the incremen-
1We used the software of Foster & Andersen (2009).
tal nature of the clustering algorithm, it does not
revise its previous decisions according to the data
that it later receives. A potential remedy would be
to consider merging the clusters that have recently
been updated, in order to allow for recovery from
early mistakes the model has made.
We used the categories induced by our model
in word prediction, inferring the semantic prop-
erties of novel words, and grammaticality judg-
ment. Our experimental results show that the per-
formance in these tasks using our categories is
comparable or better than the performance based
on the manually assigned part of speech tags in
our experimental data. Furthermore, in all these
tasks the performance using our categories im-
proves over a previous incremental categorization
model (Parisien et al, 2008). However, the model
of Parisien employs a number of cluster reorgani-
zation techniques which improve the overall qual-
ity of the clusters after processing a substantial
amount of input data. In future we plan to increase
the size of our training data, and perform a more
extensive comparison with the model of Parisien
et al (2008).
The promising results of our experiments sug-
gest that an information-theoretic approach is a
plausible one for modeling the induction of lexi-
cal categories from distributional data. Our results
imply that in many language tasks, a fine-grained
set of categories which are formed in response to
the properties of the input are more appropriate
than the coarser-grained part of speech categories.
Therefore, the ubiquitous approach of using PoS
categories as the gold standard in evaluating un-
supervised category induction models needs to be
reevaluated. To further investigate this claim, in
future we plan to collect experimental data from
human subjects performing our suggested tasks,
and measure the correlation between their perfor-
mance and that of our model.
Acknowledgments
We would like to thank Nicolas Stroppa for
insightful comments on our paper, and Chris
Parisien for sharing the implementation of his
model. Grzegorz Chrupa?a was funded by the
BMBF project NL-Search under contract number
01IS08020B. Afra Alishahi was funded by IRTG
715 ?Language Technology and Cognitive Sys-
tems? provided by the German Research Founda-
tion (DFG).
189
References
Barbara?, D., Li, Y., & Couto, J. (2002). COOL-
CAT: an entropy-based algorithm for categori-
cal clustering. In Proceedings of the Eleventh
International Conference on Information and
Knowledge Management (pp. 582?589).
Brown, P., Mercer, R., Della Pietra, V., & Lai,
J. (1992). Class-based n-gram models of natu-
ral language. Computational linguistics, 18(4),
467?479.
Cartwright, T., & Brent, M. (1997). Syntac-
tic categorization in early language acquisition:
Formalizing the role of distributional analysis.
Cognition, 63(2), 121?170.
Clark, A. (2000). Inducing syntactic categories by
context distribution clustering. In Proceedings
of the 2nd workshop on Learning Language in
Logic and the 4th conference on Computational
Natural Language Learning (pp. 91?94).
Clark, A. (2003). Combining distributional and
morphological information for part of speech
induction. In Proceedings of the 10th Confer-
ence of the European Chapter of the Association
for Computational Linguistics (pp. 59?66).
Fellbaum, C. (Ed.). (1998). WordNet, an elec-
tronic lexical database. MIT Press.
Fisher, C., Klingler, S., & Song, H. (2006). What
does syntax say about space? 2-year-olds use
sentence structure to learn new prepositions.
Cognition, 101(1), 19?29.
Foster, J., & Andersen, ?. (2009). GenERRate:
generating errors for use in grammatical error
detection. In Proceedings of the fourth work-
shop on innovative use of nlp for building edu-
cational applications (pp. 82?90).
Frank, S., Goldwater, S., & Keller, F.(2009). Eval-
uating models of syntactic category acquisition
without using a gold standard. In Proceedings
of the 31st Annual Meeting of the Cognitive Sci-
ence Society.
Gelman, S., & Taylor, M. (1984). How two-
year-old children interpret proper and common
names for unfamiliar objects. Child Develop-
ment, 1535?1540.
Gleitman, L.(1990). The structural sources of verb
meanings. Language acquisition, 1(1), 3?55.
Goldwater, S., & Griffiths, T. (2007). A fully
Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th An-
nual Meeting of the Association for Computa-
tional Linguistics (Vol. 45, p. 744).
Hubert, L., & Arabie, P. (1985). Comparing parti-
tions. Journal of classification, 2(1), 193?218.
Kemp, N., Lieven, E., & Tomasello, M. (2005).
Young Children?s Knowledge of the? Deter-
miner? and? Adjective? Categories. Journal
of Speech, Language and Hearing Research,
48(3), 592?609.
Landau, B., & Gleitman, L.(1985). Language and
experience: Evidence from the blind child. Har-
vard University Press Cambridge, Mass.
Lesher, G., Moulton, B., Higginbotham, D., & Al-
sofrom, B. (2002). Limits of human word pre-
diction performance. Proceedings of the CSUN
2002.
Li, T., Ma, S., & Ogihara, M. (2004). Entropy-
based criterion in categorical clustering. In Pro-
ceedings of the 21st International Conference
on Machine Learning (p. 68).
MacWhinney, B. (2000). The CHILDES project:
Tools for analyzing talk. Lawrence Erlbaum As-
sociates Inc, US.
Manning, C., Raghavan, P., & Schtze, H. (2008).
Introduction to Information Retrieval. Cam-
bridge University Press New York, NY, USA.
Meila, M. (2003). Comparing Clusterings by the
Variation of Information. In Learning theory
and kernel machines (pp. 173?187). Springer.
Mintz, T. (2002). Category induction from distri-
butional cues in an artificial language. Memory
and Cognition, 30(5), 678?686.
Mintz, T. (2003). Frequent frames as a cue for
grammatical categories in child directed speech.
Cognition, 90(1), 91?117.
Naigles, L., & Hoff-Ginsberg, E. (1995). Input to
Verb Learning: Evidence for the Plausibility of
Syntactic Bootstrapping. Developmental Psy-
chology, 31(5), 827?37.
Parisien, C., Fazly, A., & Stevenson, S. (2008).
An incremental bayesian model for learning
syntactic categories. In Proceedings of the
Twelfth Conference on Computational Natural
Language Learning.
Redington, M., Crater, N., & Finch, S.(1998). Dis-
tributional information: A powerful cue for ac-
190
quiring syntactic categories. Cognitive Science:
A Multidisciplinary Journal, 22(4), 425?469.
Schuler, K. (2005). VerbNet: A broad-coverage,
comprehensive verb lexicon. Unpublished doc-
toral dissertation, University of Pennsylvania.
Taylor, W. (1953). Cloze procedure: A new tool
for measuring readability. Journalism Quar-
terly, 30(4), 415?433.
Theakston, A.(2004). The role of entrenchment in
childrens and adults performance on grammati-
cality judgment tasks. Cognitive Development,
19(1), 15?34.
Theakston, A., Lieven, E., Pine, J., & Rowland, C.
(2001). The role of performance limitations in
the acquisition of verb-argument structure: An
alternative account. Journal of Child Language,
28(01), 127?152.
Wagner, J., Foster, J., & van Genabith, J.(2007). A
comparative evaluation of deep and shallow ap-
proaches to the automatic detection of common
grammatical errors. Proceedings of EMNLP-
CoNLL-2007.
Zhai, C., & Lafferty, J.(2004). A study of smooth-
ing methods for language models applied to in-
formation retrieval. ACM Transactions on In-
formation Systems (TOIS), 22(2), 214.
191
