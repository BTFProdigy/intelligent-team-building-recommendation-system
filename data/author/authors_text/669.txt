Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 57?60,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The SAMMIE System: Multimodal In-Car Dialogue
Tilman Becker, Peter Poller,
Jan Schehl
DFKI
First.Last@dfki.de
Nate Blaylock, Ciprian Gerstenberger,
Ivana Kruijff-Korbayova?
Saarland University
talk-mit@coli.uni-sb.de
Abstract
The SAMMIE1 system is an in-car multi-
modal dialogue system for an MP3 ap-
plication. It is used as a testing environ-
ment for our research in natural, intuitive
mixed-initiative interaction, with particu-
lar emphasis on multimodal output plan-
ning and realization aimed to produce out-
put adapted to the context, including the
driver?s attention state w.r.t. the primary
driving task.
1 Introduction
The SAMMIE system, developed in the TALK
project in cooperation between several academic
and industrial partners, employs the Information
State Update paradigm, extended to model collab-
orative problem solving, multimodal context and
the driver?s attention state. We performed exten-
sive user studies in a WOZ setup to guide the sys-
tem design. A formal usability evaluation of the
system?s baseline version in a laboratory environ-
ment has been carried out with overall positive re-
sults. An enhanced version of the system will be
integrated and evaluated in a research car.
In the following sections, we describe the func-
tionality and architecture of the system, point out
its special features in comparison to existing work,
and give more details on the modules that are in
the focus of our research interests. Finally, we
summarize our experiments and evaluation results.
2 Functionality
The SAMMIE system provides a multi-modal inter-
face to an in-car MP3 player (see Fig. 1) through
speech and haptic input with a BMW iDrive input
device, a button which can be turned, pushed down
and sideways in four directions (see Fig. 2 left).
System output is provided by speech and a graphi-
cal display integrated into the car?s dashboard. An
example of the system display is shown in Fig. 2.
1SAMMIE stands for Saarbru?cken Multimodal MP3 Player
Interaction Experiment.
Figure 1: User environment in laboratory setup.
The MP3 player application offers a wide range
of functions: The user can control the currently
playing song, search and browse an MP3 database
by looking for any of the fields (song, artist, al-
bum, year, etc.), search and select playlists and
even construct and edit playlists.
The user of SAMMIE has complete freedom in
interacting with the system. Input can be through
any modality and is not restricted to answers to
system queries. On the contrary, the user can give
new tasks as well as any information relevant to
the current task at any time. This is achieved by
modeling the interaction as a collaborative prob-
lem solving process, and multi-modal interpreta-
tion that fits user input into the context of the
current task. The user is also free in their use
of multimodality: SAMMIE handles deictic refer-
ences (e.g., Play this title while pushing the iDrive
button) and also cross-modal references, e.g., Play
the third song (on the list). Table 1 shows a typ-
ical interaction with the SAMMIE system; the dis-
played song list is in Fig. 2. SAMMIE supports in-
teraction in German and English.
3 Architecture
Our system architecture follows the classical ap-
proach (Bunt et al, 2005) of a pipelined architec-
ture with multimodal interpretation (fusion) and
57
U: Show me the Beatles albums.
S: I have these four Beatles albums.
[shows a list of album names]
U: Which songs are on this one?
[selects the Red Album]
S: The Red Album contains these songs
[shows a list of the songs]
U: Play the third one.
S: [music plays]
Table 1: A typical interaction with SAMMIE.
fission modules encapsulating the dialogue man-
ager. Fig. 2 shows the modules and their inter-
action: Modality-specific recognizers and analyz-
ers provide semantically interpreted input to the
multimodal fusion module that interprets them in
the context of the other modalities and the cur-
rent dialogue context. The dialogue manager de-
cides on the next system move, based on its model
of the tasks as collaborative problem solving, the
current context and also the results from calls to
the MP3 database. The turn planning module then
determines an appropriate message to the user by
planning the content, distributing it over the avail-
able output modalities and finally co-ordinating
and synchronizing the output. Modality-specific
output modules generate spoken output and graph-
ical display update. All modules interact with the
extended information state which stores all context
information.
Figure 2: SAMMIE system architecture.
Many tasks in the SAMMIE system are mod-
eled by a plan-based approach. Discourse mod-
eling, interpretation management, dialogue man-
agement and linguistic planning, and turn plan-
ning are all based on the production rule system
PATE2 (Pfleger, 2004). It is based on some con-
cepts of the ACT-R 4.0 system, in particular the
goal-oriented application of production rules, the
2Short for (P)roduction rule system based on (A)ctivation
and (T)yped feature structure (E)lements.
activation of working memory elements, and the
weighting of production rules. In processing typed
feature structures, PATE provides two operations
that both integrate data and also are suitable for
condition matching in production rule systems,
namely a slightly extended version of the general
unification, but also the discourse-oriented opera-
tion overlay (Alexandersson and Becker, 2001).
4 Related Work and Novel Aspects
Many dialogue systems deployed today follow a
state-based approach that explicitly models the
full (finite) set of dialogue states and all possible
transitions between them. The VoiceXML3 stan-
dard is a prominent example of this approach. This
has two drawbacks: on the one hand, this approach
is not very flexible and typically allows only so-
called system controlled dialogues where the user
is restricted to choosing their input from provided
menu-like lists and answering specific questions.
The user never is in control of the dialogue. For
restricted tasks with a clear structure, such an ap-
proach is often sufficient and has been applied suc-
cessfully. On the other hand, building such appli-
cations requires a fully specified model of all pos-
sible states and transitions, making larger applica-
tions expensive to build and difficult to test.
In SAMMIE we adopt an approach that mod-
els the interaction on an abstract level as collab-
orative problem solving and adds application spe-
cific knowledge on the possible tasks, available re-
sources and known recipes for achieving the goals.
In addition, all relevant context information is
administered in an Extended Information State.
This is an extension of the Information State Up-
date approach (Traum and Larsson, 2003) to the
multi-modal setting.
Novel aspects in turn planning and realization
include the comprehensive modeling in a sin-
gle, OWL-based ontology and an extended range
of context-sensitive variation, including system
alignment to the user on multiple levels.
5 Flexible Multi-modal Interaction
5.1 Extended Information State
The information state of a multimodal system
needs to contain a representation of contextual in-
formation about discourse, but also a represen-
tation of modality-specific information and user-
specific information which can be used to plan
system output suited to a given context. The over-
3http://www.w3.org/TR/voicexml20
58
all information state (IS) of the SAMMIE system is
shown in Fig. 3.
The contextual information partition of the IS
represents the multimodal discourse context. It
contains a record of the latest user utterance and
preceding discourse history representing in a uni-
form way the salient discourse entities introduced
in the different modalities. We adopt the three-
tiered multimodal context representation used in
the SmartKom system (Pfleger et al, 2003). The
contents of the task partition are explained in the
next section.
5.2 Collaborative Problem Solving
Our dialogue manager is based on an
agent-based model which views dialogue
as collaborative problem-solving (CPS)
(Blaylock and Allen, 2005). The basic building
blocks of the formal CPS model are problem-
solving (PS) objects, which we represent as
typed feature structures. PS object types form a
single-inheritance hierarchy. In our CPS model,
we define types for the upper level of an ontology
of PS objects, which we term abstract PS objects.
There are six abstract PS objects in our model
from which all other domain-specific PS objects
inherit: objective, recipe, constraint, evaluation,
situation, and resource. These are used to model
problem-solving at a domain-independent level
and are taken as arguments by all update opera-
tors of the dialogue manager which implement
conversation acts (Blaylock and Allen, 2005).
The model is then specialized to a domain by
inheriting and instantiating domain-specific types
and instances of the PS objects.
5.3 Adaptive Turn Planning
The fission component comprises detailed con-
tent planning, media allocation and coordination
and synchronization. Turn planning takes a set
of CPS-specific conversational acts generated by
the dialogue manager and maps them to modality-
specific communicative acts.
Information on how content should be dis-
tributed over the available modalities (speech or
graphics) is obtained from Pastis, a module which
stores discourse-specific information. Pastis pro-
vides information about (i) the modality on which
the user is currently focused, derived by the cur-
rent discourse context; (ii) the user?s current cog-
nitive load when system interaction becomes a
secondary task (e.g., system interaction while
driving); (iii) the user?s expertise, which is rep-
resented as a state variable. Pastis also contains
information about factors that influence the prepa-
ration of output rendering for a modality, like the
currently used language (German or English) or
the display capabilities (e.g., maximum number of
displayable objects within a table). Together with
the dialogue manager?s embedded part of the in-
formation state, the information stored by Pastis
forms the Extended Information State of the SAM-
MIE system (Fig. 3).
Planning is then executed through a set of pro-
duction rules that determine which kind of infor-
mation should be presented through which of the
available modalities. The rule set is divided in two
subsets, domain-specific and domain-independent
rules which together form the system?s multi-
modal plan library.
contextual-info:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
last-user-utterance:
:
[
interp : set(grounding-acts)
modality-requested : modality
modalities-used : set(msInput)
]
discourse-history:
: list(discourse-objects)
modality-info:
:
[
speech : speechInfo
graphic : graphicInfo
]
user-info:
:
[
cognitive-load : cogLoadInfo
user-expertise : expertiseInfo
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
task-info:
[
cps-state : c-situation (see below for details)
pending-sys-utt : list(grounding-acts)
]
Figure 3: SAMMIE Information State structure.
5.4 Spoken Natural Language Output
Generation
Our goal is to produce output that varies in the sur-
face realization form and is adapted to the con-
text. A template-based module has been devel-
oped and is sufficient for classes of system output
that do not need fine-tuned context-driven varia-
tion. Our template-based generator can also de-
liver alternative realizations, e.g., alternative syn-
tactic constructions, referring expressions, or lexi-
cal items. It is implemented by a set of straightfor-
ward sentence planning rules in the PATE system
to build the templates, and a set of XSLT trans-
formations to yield the output strings. Output in
German and English is produced by accessing dif-
ferent dictionaries in a uniform way.
In order to facilitate incremental development
of the whole system, our template-based mod-
ule has a full coverage wrt. the classes of sys-
59
tem output that are needed. In parallel, we are
experimenting with a linguistically more power-
ful grammar-based generator using OpenCCG4,
an open-source natural language processing en-
vironment (Baldridge and Kruijff, 2003). This al-
lows for more fine-grained and controlled choices
between linguistic expressions in order to achieve
contextually appropriate output.
5.5 Modeling with an Ontology
We use a full model in OWL as the knowledge rep-
resentation format in the dialogue manager, turn
planner and sentence planner. This model in-
cludes the entities, properties and relations of the
MP3 domain?including the player, data base and
playlists. Also, all possible tasks that the user may
perform are modeled explicitly. This task model
is user centered and not simply a model of the
application?s API.The OWL-based model is trans-
formed automatically to the internal format used
in the PATE rule-interpreter.
We use multiple inheritance to model different
views of concepts and the corresponding presen-
tation possibilities; e.g., a song is a browsable-
object as well as a media-object and thus allows
for very different presentations, depending on con-
text. Thereby PATE provides an efficient and ele-
gant way to create more generic presentation plan-
ning rules.
6 Experiments and Evaluation
So far we conducted two WOZ data collection
experiments and one evaluation experiment with
a baseline version of the SAMMIE system. The
SAMMIE-1 WOZ experiment involved only spo-
ken interaction, SAMMIE-2 was multimodal, with
speech and haptic input, and the subjects had
to perform a primary driving task using a Lane
Change simulator (Mattes, 2003) in a half of their
experiment session. The wizard was simulating
an MP3 player application with access to a large
database of information (but not actual music) of
more than 150,000 music albums (almost 1 mil-
lion songs). In order to collect data with a variety
of interaction strategies, we used multiple wizards
and gave them freedom to decide about their re-
sponse and its realization. In the multimodal setup
in SAMMIE-2, the wizards could also freely de-
cide between mono-modal and multimodal output.
(See (Kruijff-Korbayova? et al, 2005) for details.)
We have just completed a user evaluation to
explore the user-acceptance, usability, and per-
formance of the baseline implementation of the
4http://openccg.sourceforge.net
SAMMIE multimodal dialogue system. The users
were asked to perform tasks which tested the sys-
tem functionality. The evaluation analyzed the
user?s interaction with the baseline system and
combined objective measurements like task com-
pletion (89%) and subjective ratings from the test
subjects (80% positive).
Acknowledgments This work has been carried
out in the TALK project, funded by the EU 6th
Framework Program, project No. IST-507802.
References
[Alexandersson and Becker2001] J. Alexandersson and
T. Becker. 2001. Overlay as the basic operation for
discourse processing in a multimodal dialogue system. In
Proceedings of the 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Seattle,
Washington, August.
[Baldridge and Kruijff2003] J.M. Baldridge and G.J.M. Krui-
jff. 2003. Multi-Modal Combinatory Categorial Gram-
mar. In Proceedings of the 10th Annual Meeting of the
European Chapter of the Association for Computational
Linguistics (EACL?03), Budapest, Hungary, April.
[Blaylock and Allen2005] N. Blaylock and J. Allen. 2005. A
collaborative problem-solving model of dialogue. In Laila
Dybkj?r and Wolfgang Minker, editors, Proceedings of
the 6th SIGdial Workshop on Discourse and Dialogue,
pages 200?211, Lisbon, September 2?3.
[Bunt et al2005] H. Bunt, M. Kipp, M. Maybury, and
W. Wahlster. 2005. Fusion and coordination for multi-
modal interactive information presentation: Roadmap, ar-
chitecture, tools, semantics. In O. Stock and M. Zanca-
naro, editors, Multimodal Intelligent Information Presen-
tation, volume 27 of Text, Speech and Language Technol-
ogy, pages 325?340. Kluwer Academic.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG, pages
191?196.
[Mattes2003] S. Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. In Proc. of IGfA.
[Pfleger et al2003] N. Pfleger, J. Alexandersson, and
T. Becker. 2003. A robust and generic discourse model
for multimodal dialogue. In Proceedings of the 3rd
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Acapulco.
[Pfleger2004] N. Pfleger. 2004. Context based multimodal
fusion. In ICMI ?04: Proceedings of the 6th interna-
tional conference on Multimodal interfaces, pages 265?
272, New York, NY, USA. ACM Press.
[Traum and Larsson2003] David R. Traum and Staffan Lars-
son. 2003. The information state approach to dialog man-
agement. In Current and New Directions in Discourse and
Dialog. Kluwer.
60
An Experiment Setup for Collecting Data for Adaptive Output Planning
in a Multimodal Dialogue System
Ivana Kruijff-Korbayova?, Nate Blaylock,
Ciprian Gerstenberger, Verena Rieser
Saarland University, Saarbru?cken, Germany
korbay@coli.uni-sb.de
Tilman Becker, Michael Kai?er,
Peter Poller, Jan Schehl
DFKI, Saarbru?cken, Germany
tilman.becker@dfki.de
Abstract
We describe a Wizard-of-Oz experiment setup for
the collection of multimodal interaction data for a
Music Player application. This setup was devel-
oped and used to collect experimental data as part
of a project aimed at building a flexible multimodal
dialogue system which provides an interface to an
MP3 player, combining speech and screen input
and output. Besides the usual goal of WOZ data
collection to get realistic examples of the behav-
ior and expectations of the users, an equally im-
portant goal for us was to observe natural behavior
of multiple wizards in order to guide our system
development. The wizards? responses were there-
fore not constrained by a script. One of the chal-
lenges we had to address was to allow the wizards
to produce varied screen output a in real time. Our
setup includes a preliminary screen output planning
module, which prepares several versions of possi-
ble screen output. The wizards were free to speak,
and/or to select a screen output.
1 Introduction
In the larger context of the TALK project1 we are develop-
ing a multimodal dialogue system for a Music Player appli-
cation for in-car and in-home use, which should support nat-
ural, flexible interaction and collaborative behavior. The sys-
tem functionalities include playback control, manipulation of
playlists, and searching a large MP3 database. We believe
that in order to achieve this goal, the system needs to provide
advanced adaptive multimodal output.
We are conducting Wizard-of-Oz experiments
[Bernsen et al, 1998] in order to guide the development
of our system. On the one hand, the experiments should
give us data on how the potential users interact with such
an application. But we also need data on the multimodal
interaction strategies that the system should employ to
achieve the desired naturalness, flexibility and collaboration.
We therefore need a setup where the wizard has freedom of
1TALK (Talk and Look: Tools for Ambient Linguistic Knowl-
edge; www.talk-project.org) is funded by the EU as project
No. IST-507802 within the 6th Framework program.
choice w.r.t. their response and its realization through single
or multiple modalities. This makes it different from previous
multimodal experiments, e.g., in the SmartKom project
[Tu?rk, 2001], where the wizard(s) followed a strict script.
But what we need is also different in several aspects from
taking recordings of straight human-human interactions: the
wizard does not hear the user?s input directly, but only gets a
transcription, parts of which are sometimes randomly deleted
(in order to approximate imperfect speech recognition);
the user does not hear the wizard?s spoken output directly
either, as the latter is transcribed and re-synthesized (to
produce system-like sounding output). The interactions
should thus more realistically approximate an interaction
with a system, and thereby contain similar phenomena (cf.
[Duran et al, 2001]).
The wizard should be able to present different screen out-
puts in different context, depending on the search results and
other aspects. However, the wizard cannot design screens on
the fly, because that would take too long. Therefore, we de-
veloped a setup which includes modules that support the wiz-
ard by providing automatically calculated screen output op-
tions the wizard can select from if s/he want to present some
screen output.
Outline In this paper we describe our experiment setup and
the first experiences with it. In Section 2 we overview the
research goals that our setup was designed to address. The
actual setup is presented in detail in Section 3. In Section 4
we describe the collected data, and we summarize the lessons
we learnt on the basis of interviewing the experiment partici-
pants. We briefly discuss possible improvements of the setup
and our future plans with the data in Section 5.
2 Goals of the Multimodal Experiment
Our aim was to gather interactions where the wizard can com-
bine spoken and visual feedback, namely, displaying (com-
plete or partial) results of a database search, and the user can
speak or select on the screen.
Multimodal Presentation Strategies The main aim was to
identify strategies for the screen output, and for the multi-
modal output presentation. In particular, we want to learn
Figure 1: Multimodal Wizard-of-Oz data collection setup for
an in-car music player application, using the Lane Change
driving simulator. Top right: User, Top left: Wizard, Bottom:
transcribers.
when and what content is presented (i) verbally, (ii) graphi-
cally or (iii) by some combination of both modes. We expect
that when both modalities are used, they do not convey the
same content or use the same level of granularity. These are
important questions for multimodal fission and for turn plan-
ning in each modality.
We also plan to investigate how the presentation strategies
influence the responses of the user, in particular w.r.t. what
further criteria the user specifies, and how she conveys them.
Multimodal Clarification Strategies The experiments
should also serve to identify potential strategies for multi-
modal clarification behavior and investigate individual strat-
egy performance. The wizards? behavior will give us an ini-
tial model how to react when faced with several sources of
interpretation uncertainty. In particular we are interested in
what medium the wizard chooses for the clarification request,
what kind of grounding level he addresses, and what ?sever-
ity? he indicates. 2 In order to invoke clarification behavior
we introduced uncertainties on several levels, for example,
multiple matches in the database, lexical ambiguities (e.g., ti-
tles that can be interpreted denoting a song or an album), and
errors on the acoustic level. To simulate non-understanding
on the acoustic level we corrupted some of the user utterances
by randomly deleting parts of them.
3 Experiment Setup
We describe here some of the details of the experiment. The
experimental setup is shown schematically in Figure 1. There
are five people involved in each session of the experiment: an
experiment leader, two transcribers, a user and a wizard.
The wizards play the role of an MP3 player application
and are given access to a database of information (but not
actual music) of more than 150,000 music albums (almost 1
2Severity describes the number of hypotheses indicated by the
wizard: having no interpretation, an uncertain interpretation, or sev-
eral ambiguous interpretations.
Figure 2: Screenshot from the FreeDB-based database appli-
cation, as seen by the wizard. First-level of choice what to
display.
million songs), extracted from the FreeDB database.3 Fig-
ure 2 shows an example screen shot of the music database
as it is presented to the wizard. Subjects are given a set of
predefined tasks and are told to accomplish them by using
an MP3 player with a multimodal interface. Tasks include
playing songs/albums and building playlists, where the sub-
ject is given varying amounts of information to help them
find/decide on which song to play or add to the playlist. In
a part of the session the users also get a primary driving task,
using a Lane Change driving simulator [Mattes, 2003]. This
enabled us to test the viability of combining primary and sec-
ondary task in our experiment setup. We also aimed to gain
initial insight regarding the difference in interaction flow un-
der such conditions, particularly with regard to multimodal-
ity.
The wizards can speak freely and display the search result
or the playlist on the screen. The users can also speak as well
as make selections on the screen.
The user?s utterances are immediately transcribed by a typ-
ist and also recorded. The transcription is then presented to
the wizard.4 We did this for two reasons: (1) To deprive
the wizards of information encoded in the intonation of utter-
ances, because our system will not have access to it either. (2)
To be able to corrupt the user input in a controlled way, sim-
ulating understanding problems at the acoustic level. Unlike
[Stuttle et al, 2004], who simulate automatic speech recogni-
tion errors using phone-confusion models, we used a tool that
?deletes? parts of the transcribed utterances, replacing them
by three dots. Word deletion was triggered by the experiment
leader. The word deletion rate varied: 20% of the utterances
got weakly and 20% strongly corrupted. In 60% of the cases
the wizard saw the transcribed speech uncorrupted.
The wizard?s utterances are also transcribed (and recorded)
3Freely available at http://www.freedb.org
4We were not able to use a real speech recognition system, be-
cause we do not have one trained for this domain. This is one of the
purposes the collected data will be used for.
Figure 3: Screenshot from the display presentation tool offer-
ing options for screen output to the wizard for second-level
of choice what to display an how.
and presented to the user via a speech synthesizer. There are
two reasons for doing this: One is to maintain the illusion for
the subjects that they are actually interacting with a system,
since it is known that there are differences between human-
human and human-computer dialogue [Duran et al, 2001],
and we want to elicit behavior in the latter condition; the
other has to do with the fact that synthesized speech is imper-
fect and sometimes difficult to understand, and we wanted to
reproduce this condition.
The transcription is also supported by a typing and spelling
correction module to minimize speech synthesis errors and
thus help maintain the illusion of a working system.
Since it would be impossible for the wizard to construct
layouts for screen output on the fly, he gets support for his
task from the WOZ system: When the wizard performs a
database query, a graphical interface presents him a first level
of output alternatives, as shown in Figure 2. The choices are
found (i) albums, (ii) songs, or (iii) artists. For a second level
of choice, the system automatically computes four possible
screens, as shown in Figure 3. The wizard can chose one of
the offered options to display to the user, or decide to clear
the user?s screen. Otherwise, the user?s screen remains un-
changed. It is therefore up to the wizard to decide whether
to use speech only, display only, or to combine speech and
display.
The types of screen output are (i) a simple text-message
conveying how many results were found, (ii) output of a list
of just the names (of albums, songs or artists) with the cor-
responding number of matches (for songs) or length (for al-
bums), (iii) a table of the complete search results, and (iv) a
table of the complete search results, but only displaying a sub-
set of columns. For each screen output type, the system uses
heuristics based on the search to decide, e.g., which columns
should be displayed. These four screens are presented to the
wizard in different quadrants on a monitor (cf. Figure 3),
allowing for selection with a simple mouse click. The heuris-
tics for the decision what to display implement preliminary
strategies we designed for our system. We are aware that due
to the use of these heuristics, the wizard?s output realization
may not be always ideal. We have collected feedback from
both the wizards and the users in order to evaluate whether
the output options were satisfactory (cf. Section 4 for more
details).
Technical Setup To keep our experimental system modu-
lar and flexible we implemented it on the basis of the Open
Agent Architecture (OAA) [Martin et al, 1999], which is a
framework for integrating a community of software agents in
a distributed environment. Each system module is encapsu-
lated by an OAA wrapper to form an OAA agent, which is
able to communicate with the OAA community. The exper-
imental system consists of 12 agents, all of them written in
Java. We made use of an OAA monitor agent which comes
with the current OAA distribution to trace all communication
events within the system for logging purposes.
The setup ran distributed over six PCs running different
versions of Windows and Linux.5
4 Collected Data and Experience
The SAMMIE-26 corpus collected in this experiment contains
data from 24 different subjects, who each participated in one
session with one of our six wizards. Each subject worked on
four tasks, first two without driving and then two with driving.
The duration was restricted to twice 15 minutes. Tasks were
of two types: searching for a title either in the database or in
an existing playlist, building a playlist satisfying a number of
constraints. Each of the two sets for each subject contained
one task of each type. The tasks again differed in how specific
information was provided. We aimed to keep the difficulty
level constant across users. The interactions were carried out
in German.7
The data for each session consists of a video and audio
recording and a logfile. Besides the transcriptions of the spo-
ken utterances, a number of other features have been anno-
tated automatically in the log files of the experiment, e.g.,
the wizard?s database query and the number of found results,
the type and form of the presentation screen chosen by the
wizard, etc. The gathered logging information for a single
experiment session consists of the communication events in
chronological order, each marked by a timestamp. Based on
this information, we can recapitulate the number of turns and
the specific times that were necessary to accomplish a user
task. We expect to use this data to analyze correlations be-
5We would like to thank our colleagues from CLT Sprachtech-
nologie http://www.clt-st.de/ for helping us to set up the
laboratory.
6SAMMIE stands for Saarbru?cken Multimodal MP3 Player In-
teraction Experiment. We have so far conducted two series of data-
collection experiments: SAMMIE-1 involved only spoken interaction
(cf. [Kruijff-Korbayova? et al, 2005] for more details), SAMMIE-2 is
the multimodal experiment described in this paper.
7However, most of the titles and artist names in the music
database are in English.
tween queries, numbers of results, and spoken and graphical
presentation strategies.
Whenever the wizard made a clarification request, the
experiment leader invoked a questionnaire window on the
screen, where the wizard had to classify his clarification re-
quest according to the primary source of the understanding
problem. At the end of each task, users were asked to what
extent they believed they accomplished their tasks and how
satisfied they were with the results. Similar to methods used
by [Skantze, 2003] and [Williams and Young, 2004], we plan
to include subjective measures of task completion and cor-
rectness of results in our evaluation matrix, as task descrip-
tions can be interpreted differently by different users.
Each subject was interviewed immediately after the ses-
sion. The wizards were interviewed once the whole experi-
ment was over. The interviews were carried out verbally, fol-
lowing a prepared list of questions. We present below some
of the points gathered through these interviews.
Wizard Interviews All 6 wizards rated the overall under-
standing as good, i.e., that communication completed suc-
cessfully. However, they reported difficulties due to delays in
utterance transmission in both directions, which caused un-
necessary repetitions due to unintended turn overlap.
There were differences in how different wizards rated and
used the different screen output options: The table containing
most of the information about the queried song(s) or album(s)
was rated best and shown most often by some wizards, while
others thought it contained too much information and would
not be clear at first glance for the users and hence they used
it less or never. The screen option containing the least infor-
mation in tabular form, namely only a list of songs/albums
with their length, received complementary judgments: some
of the wizards found it useless because it contained too little
information, and they thus did not use it, and others found it
very useful because it would not confuse the user by present-
ing too much information, and they thus used it frequently.
Finally, the screen containing a text message conveying only
the number of matches, if any, has been hardly used by the
wizards. The differences in the wizards? opinions about what
the users would find useful or not clearly indicate the need
for evaluation of the usefulness of the different screen output
options in particular contexts from the users? view point.
When showing screen output, the most common pattern
used by the wizards was to tell the user what was shown (e.g.,
I?ll show you the songs by Prince), and to display the screen.
Some wizards adapted to the user?s requests: if asked to show
something (e.g., Show me the songs by Prince), they would
show it without verbal comments; but if asked a question
(e.g., What songs by Prince are there? or What did you find?),
they would show the screen output and answer in speech.
Concerning the adaptation of multimodal presentation
strategies w.r.t. whether the user was driving or not, four
of the six wizards reported that they consciously used speech
instead of screen output if possible when the user was driving.
The remaining two wizards did not adapt their strategy.
On the whole, interviewing the wizards brought valuable
information on presentation strategies and the use of modal-
ities, but we expect to gain even more insight after the an-
notation and evaluation of the collected data. Besides ob-
servations about the interaction with the users, the wizards
also gave us various suggestions concerning the software used
in the experiment, e.g., the database interface (e.g., the pos-
sibility to decide between strict search and search for par-
tial matches, and fuzzy search looking for items with similar
spelling when no hits are found), the screen options presenter
(e.g., ordering of columns w.r.t. their order in the database in-
terface, the possibility to highlight some of the listed items),
and the speech synthesis system.
Subject Interviews In order to use the wizards? behavior as
a model for interaction design, we need to evaluate the wiz-
ards? strategies. We used user satisfaction, task experience,
and multi-modal feedback behavior as evaluation metrics.
The 24 experimental subjects were all native speakers of
German with good English skills. They were all students
(equally spread across subject areas), half of them male and
half female, and most of them were between 20 to 30 years
old.
In order to calculate user satisfaction, users were inter-
viewed to evaluate the system?s performance with a user sat-
isfaction survey. The survey probed different aspects of the
users? perception of their interaction with the system. We
asked the users to evaluate a set of five core metrics on a
5-point Likert scale. We followed [Walker et al, 2002] def-
inition of the overall user satisfaction as the sum of text-to-
speech synthesis performance, task ease, user expertise, over-
all difficulty and future use. The mean for user satisfaction
across all dialogues was 15.0 (with a standard derivation of
2.9). 8 A one-way ANOVA for user satisfaction between wiz-
ards (df=5, F=1.52 p=0.05) shows no significant difference
across wizards, meaning that the system performance was
judged to be about equally good for all wizards.
To measure task experience we elicited data on perceived
task success and satisfaction on a 5-point Likert scale after
each task was completed. For all the subjects the final per-
ceived task success was 4.4 and task satisfaction 3.9 across
the 4 tasks each subject had to complete. For task success
as well as for task satisfaction no significant variance across
wizards was detected.
Furthermore the subjects were asked about the employed
multi-modal presentation and clarification strategies.
The clarification strategies employed by the wizards
seemed to be successful: From the subjects? point of view,
mutual understanding was very good and the few misunder-
standings could be easily resolved. Nevertheless, in the case
of disambiguation requests and when grounding an utterance,
subjects ask for more display feedback. It is interesting to
note that subjects judged understanding difficulties on higher
levels of interpretation (especially reference resolution prob-
lems and problems with interpreting the intention) to be more
costly than problems on lower levels of understanding (like
the acoustic understanding). For the clarification strategy this
8[Walker et al, 2002] reported an average user satisfaction of
16.2 for 9 Communicator systems.
implies that the system should engage in clarification at the
lowest level a error was detected.9
Multi-modal presentation strategies were perceived to be
helpful in general, having a mean of 3.1 on a 5-point Lik-
ert scale. However, the subjects reported that too much in-
formation was being displayed especially for the tasks with
driving. 85.7% of the subjects reported that the screen out-
put was sometimes distracting them. 76.2% of the sub-
jects would prefer to more verbal feedback, especially while
driving. On a 3-point Likert scale subjects evaluated the
amount of the information presented verbally to be about
right (mean of 1.8), whereas they found the information pre-
sented on the screen to be too much (mean of 2.3). Stud-
ies by [Bernsen and Dybkjaer, 2001] on the appropriateness
of using verbal vs. graphical feedback for in-car dialogues
indicate that the need for text output is very limited. Some
subjects in that study, as well subjects in our study report that
they would prefer to not have to use the display at all while
driving. On the other hand subjects in our study perceived the
screen output to be very helpful in less stressful driving situa-
tions and when not driving (e.g. for memory assistance, clari-
fications etc.). Especially when they want to verify whether a
complex task was finally completed (e.g. building a playlist),
they ask for a displayed proof. For modality selection in in-
car dialogues the driver?s mental workload on primary and
secondary task has to be carefully evaluated with respect to a
situation model.
With respect to multi-modality subjects also asked for
more personalized data presentation. We therefore need to
develop intelligent ways to reduce the amount of data being
displayed. This could build on prior work on the generation
of ?tailored? responses in spoken dialogue according to a user
model [Moore et al, 2004].
The results for multi-modal feedback behavior showed no
significant variations across wizards except for the general
helpfulness of multi-modal strategies. An ANOVA Planned
Comparison of the wizard with the lowest mean against the
other wizards showed that his behavior was significantly
worse. It is interesting to note, that this wizard was using
the display less than the others. We might consider not to in-
clude the 4 sessions with this wizard in our output generation
model.
We also tried to analyze in more detail how the wizards?
presentation strategies influenced the results. The option
which was chosen most of the time was to present a table
with the search results (78.6%); to present a list was only cho-
sen in 17.5% of the cases and text only 0.04%. The wizards?
choices varied significantly only for presenting the table op-
tion. The wizard who was rated lowest for multimodality was
using the table option less, indicating that this option should
be used more often. This is also supported by the fact that the
show table option is the only presentation strategy which is
positively correlated to how the user evaluated multimodality
(Spearman?s r = 0.436*). We also could find a 2-tailed corre-
9Note that engaging at the lowest level just helps to save dialogue
?costs?. Other studies have shown that user satisfaction is higher
for strategies that would ?hide? the understanding error by asking
questions on higher levels [Skantze, 2003], [Raux et al, 2005]
lation between user satisfaction and multimodality judgment
(Spearman?s r = 0.658**). This indicates the importance of
good multimodal presentation strategies for user satisfaction.
Finally, the subjects were asked for own comments. They
liked to be able to provide vague information, e.g., ask for ?an
oldie?, and were expecting collaborative suggestions. They
also appreciated collaborative proposals based on inferences
made from previous conversations.
In sum, as the measures for user satisfaction, task experi-
ence, and multi-modal feedback strategies, the subjects? judg-
ments show a positive trend. The dialogue strategies em-
ployed by most of the wizards seem to be a good starting
point for building a baseline system. Furthermore, the results
indicate that intelligent multi-modal generation needs to be
adaptive to user and situation models.
5 Conclusions and Future Steps
We have presented an experiment setup that enables us to
gather multimodal interaction data aimed at studying not only
the behavior of the users of the simulated system, but also
that of the wizards. In order to simulate a dialogue system in-
teraction, the wizards were only shown transcriptions of the
user utterances, sometimes corrupted, to simulate automatic
speech recognition problems. The wizard?s utterances were
also transcribed and presented to the user through a speech
synthesizer. In order to make it possible for the wizards to
produce contextually varied screen output in real time, we
have included a screen output planning module which auto-
matically calculated several screen output versions every time
the wizard ran a database query. The wizards were free to
speak and/or display screen output. The users were free to
speak or select on the screen. In a part of each session, the
user was occupied by a primary driving task.
The main challenge for an experiment setup as described
here is the considerable delay between user input and wizard
response. This is due partly to the transcription and spelling
correction step and partly due to the time it takes the wizard to
decide on and enter a query to the database, then select a pre-
sentation and in parallel speak to the user. We have yet to ana-
lyze the exact distribution of time needed for these tasks. Sev-
eral ways can be chosen to speed up the process. Transcrip-
tion can be eliminated either by using speech recognition and
dealing with its errors, or instead applying signal processing
software, e.g., to filter out prosodic information from the user
utterance and/or to transform the wizard?s utterance into syn-
thetically sounding speech (e.g., using a vocoder). Database
search can be sped up in a number of ways too, ranging from
allowing selection directly from the transcribed text to auto-
matically preparing default searches by analyzing the user?s
utterance. Note, however, that the latter will most likely prej-
udice the wizard to stick to the proposed search.
We plan to annotate the corpus, most importantly w.r.t.
wizard presentation strategies and context features relevant
for the choice between them. We also plan to compare the
presentation strategies to the strategies in speech-only mode,
for which we collected data in an earlier experiment (cf.
[Kruijff-Korbayova? et al, 2005]).
For clarification strategies previous studies already showed
that the decision process needs to be highly dynamic by tak-
ing into account various features such as interpretation uncer-
tainties and local utility [Paek and Horvitz, 2000]. We plan
to use the wizard data to learn an initial multi-modal clarifi-
cation policy and later on apply reinforcement learning meth-
ods to the problem in order to account for long-term dialogue
goals, such as task success and user satisfaction.
The screen output options used in the experiment will also
be employed in the baseline system we are currently imple-
menting. The challenges involved there are to decide (i) when
to produce screen output, (ii) what (and how) to display and
(iii) what the corresponding speech output should be. We will
analyze the corpus in order to determine what the suitable
strategies are.
References
[Bernsen and Dybkjaer, 2001] Niels Ole Bernsen and Laila
Dybkjaer. Exploring natural interaction in the car. In
CLASS Workshop on Natural Interactivity and Intelligent
Interactive Information Representation, 2001.
[Bernsen et al, 1998] N. O. Bernsen, H. Dybkj?r, and
L. Dybkj?r. Designing Interactive Speech Systems ?
From First Ideas to User Testing. Springer, 1998.
[Duran et al, 2001] Christine Duran, John Aberdeen, Laurie
Damianos, and Lynette Hirschman. Comparing several as-
pects of human-computer and human-human dialogues. In
Proceedings of the 2nd SIGDIAL Workshop on Discourse
and Dialogue, Aalborg, 1-2 September 2001, pages 48?57,
2001.
[Kruijff-Korbayova? et al, 2005] Ivana Kruijff-Korbayova?,
Tilman Becker, Nate Blaylock, Ciprian Gerstenberger,
Michael Kai?er, Peter Poler, Jan Schehl, and Verena
Rieser. Presentation strategies for flexible multimodal
interaction with a music player. In Proceedings of
DIALOR?05 (The 9th workshop on the semantics and
pragmatics of dialogue (SEMDIAL), 2005.
[Martin et al, 1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. The open agent architecture: A framework for
building distributed software systems. Applied Artificial
Intelligence: An International Journal, 13(1?2):91?128,
Jan?Mar 1999.
[Mattes, 2003] Stefan Mattes. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of IGfA,
2003.
[Moore et al, 2004] Johanna D. Moore, Mary Ellen Foster,
Oliver Lemon, and Michael White. Generating tailored,
comparative descriptions in spoken dialogue. In Proceed-
ings of the Seventeenth International Florida Artificial In-
telligence Research Sociey Conference, AAAI Press, 2004.
[Paek and Horvitz, 2000] Tim Paek and Eric Horvitz. Con-
versation as action under uncertainty. In Proceedings of
the Sixteenth Conference on Uncertainty in Artificial In-
telligence, 2000.
[Raux et al, 2005] Antoine Raux, Brian Langner, Dan Bo-
hus, Allan W. Black, and Maxine Eskenazi. Let?s go pub-
lic! taking a spoken dialog system to the real world. 2005.
[Skantze, 2003] Gabriel Skantze. Exploring human error
handling strategies: Implications for spoken dialogue sys-
tems. In Proceedings of the ISCA Tutorial and Research
Workshop on Error Handling in Spoken Dialogue Systems,
2003.
[Stuttle et al, 2004] Matthew Stuttle, Jason Williams, and
Steve Young. A framework for dialogue data collection
with a simulated asr channel. In Proceedings of the IC-
SLP, 2004.
[Tu?rk, 2001] Ulrich Tu?rk. The technical processing in
smartkom data collection: a case study. In Proceedings
of Eurospeech2001, Aalborg, Denmark, 2001.
[Walker et al, 2002] Marylin Walker, R. Passonneau, J. Ab-
erdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom,
J. Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky,
G. Sandersa, S. Seneff, D. Stallard, and S. Whittaker.
Cross-site evaluation in darpa communicator: The june
2000 data collection. 2002.
[Williams and Young, 2004] Jason D. Williams and Steve
Young. Characterizing task-oriented dialog using a sim-
ulated asr channel. In Proceedings of the ICSLP, 2004.
187
188
189
190
Synchronization in an Asynchronous Agent-based Architecture
for Dialogue Systems
Nate Blaylock and James Allen and George Ferguson
Department of Computer Science
University of Rochester
Rochester, New York 14627
USA
{blaylock,james,ferguson}@cs.rochester.edu
Abstract
Most dialogue architectures are ei-
ther pipelined or, if agent-based,
are restricted to a pipelined flow-
of-information. The TRIPS di-
alogue architecture is agent-based
and asynchronous, with several lay-
ers of information flow. We present
this architecture and the synchro-
nization issues we encountered in
building a truly distributed, agent-
based dialogue architecture.
1 Introduction
More and more people are building dia-
logue systems. Architecturally, these sys-
tems tend to fall into two camps: those with
pipelined architectures (e.g., (Lamel et al,
1998; Nakano et al, 1999)), and those with
agent-based architectures (e.g., (Seneff et al,
1999; Stent et al, 1999; Rudnicky et al,
1999)). Agent-based architectures are advan-
tageous because they free up system com-
ponents to potentially act in a more asyn-
chronous manner. However, in practice, most
dialogue systems built on an agent-based ar-
chitecture pass messages such that they are
basically functioning in terms of a pipelined
flow-of-information.
Our original implementation of the TRIPS
spoken dialogue system (Ferguson and Allen,
1998) was such an agent-based, pipelined
flow-of-information system. Recently, how-
ever, we made changes to the system (Allen
et al, 2001a) which allow it to take advan-
tage of the distributed nature of an agent-
based system. Instead of system components
passing information in a pipelined manner
(interpretation ? discourse management ?
generation), we allow the subsystems of in-
terpretation, behavior (reasoning and acting)
and generation to work asynchronously. This
makes the TRIPS system truly distributed
and agent-based.
The driving forces behind these changes are
to provide a framework for incremental and
asynchronous language processing, and to al-
low for a mixed-initiative system at the task
level. We describe these motivations briefly
here.
Incremental Language Processing In a
pipelined (or pipelined flow-of-information)
system, generation does not occur until af-
ter both the interpretation and reasoning pro-
cesses have completed. This constraint is
not present in human-human dialogue as ev-
idenced by the presence of grounding, ut-
terance acknowledgment, and interruptions.
Making interpretation, behavior, and gener-
ation asynchronous allows, for example, the
system to acknowledge a question while it is
still working on finding the answer.
Mixed-initiative Interaction Although
pipelined systems allow the system to take
discourse-level initiative (cf. (Chu-Caroll and
Brown, 1997)), it is difficult to see how they
could allow the system to take task-level ini-
tiative in a principled way. In most systems,
reasoning and action are driven mostly by in-
terpreted input (i.e., they are reactive to the
user?s utterances). In a mixed-initiative sys-
tem, the system?s response should be deter-
mined not only by user input, but also system
goals and obligations, as well as exogenous
        Philadelphia, July 2002, pp. 1-10.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
events. For example, a system with an asyn-
chronous behavior subsystem can inform the
user of a new, important event, regardless of
whether it is tied to the user?s last utterance.
On the other hand, in the extreme version of
pipelined flow-of-control, behavior cannot do
anything until the user says something, which
is the only way to get the pipeline flowing.
The reasons for our changes are described
in further detail in (Allen et al, 2001a). In
this paper, we focus on the issues we encoun-
tered in developing an asynchronous agent-
based dialogue system and their respective so-
lutions, which turn out to be highly related to
the process of grounding.
We first describe the general TRIPS archi-
tecture and information flow and then discuss
the various points of synchronization within
the system. We then discuss what these is-
sues mean in general for the implementation
of an asynchronous agent-based system.
2 TRIPS Architecture
As mentioned above, the TRIPS system1
(Allen et al, 2000; Allen et al, 2001a; Allen
et al, 2001b) is built on an agent-based ar-
chitecture. Unlike many systems, however,
the flow of information within TRIPS is not
pipelined. The architecture and information
flow between components is shown in Fig-
ure 1. In TRIPS, information flows between
the three general areas of interpretation, be-
havior, and generation.
Each TRIPS component is implemented as
a separate process. Information is shared by
passing KQML (Finin et al, 1997) messages
through a central hub, the Facilitator, which
supports message logging and syntax checking
as well as broadcast and selective broadcast
between components.
We first discuss the individual system com-
ponents and their functions. We then de-
scribe the flow of information through the sys-
tem and illustrate it with an example.
1Further details of the TRIPS dia-
logue system can be found at our website:
http://www.cs.rochester.edu/research/cisd/
Behavioral
Agent
Interpretation
Manager
Generation
Manager
Parser
Speech
Planner Scheduler Monitors Events
Task- and Domain-specific
Knowledge Sources Exogenous Event Sources
Response
Planner
GraphicsSpeech
Task
Manager
Reference
Discourse
Context
Interpretation
Generation
Behavior
Task
Interpretation
Requests
Problem-Solving
Acts recognized
from user
Problem-Solving
Acts
to perform
Task
Execution
Requests
Figure 1: The TRIPS Architecture (Allen et
al., 2001a)
2.1 System Components
Figure 1 shows the various components in
the TRIPS system. Components are divided
among three main categories: Interpretation,
Behavior, and Generation. As shown in the
figure, some components straddle categories,
meaning they represent state and provide
services necessary for both sorts of process-
ing. The Interpretation Manager (IM) in-
terprets user input coming from the various
modality processors as it arises. It inter-
acts with Reference to resolve referring ex-
pressions and with the Task Manager (TM)
to perform plan and intention recognition, as
part of the interpretation process. It broad-
casts recognized speech acts and their inter-
pretation as collaborative problem solving ac-
tions (see below), and incrementally updates
the Discourse Context (DC). The Behavioral
Agent (BA) is in some sense the autonomous
?heart? of the agent. It plans system be-
havior based on its own goals and obliga-
tions, the user?s utterances and actions, and
changes in the world state. Actions that re-
quire task- and domain-dependent processing
are performed by the Task Manager. Ac-
tions that involve communication and collab-
oration with the user are sent to the Gener-
ation Manager (GM) in the form of commu-
nicative acts. The GM coordinates planning
the specific content of utterances and display
updates and producing the results. Its behav-
ior is driven by discourse obligations (from the
DC), and the directives it receives from the
BA.
2.1.1 Collaborative Problem Solving
Model
The three main components (IM, BA, GM)
communicate using messages based on a col-
laborative problem solving model of dia-
logue (Allen et al, 2002; Blaylock, 2002).
We model dialogue as collaboration between
agents which are planning and acting. To-
gether, collaborating agents (i.e., dialogue
partners) build and execute plans, deciding on
such things as objectives, recipes, resources,
situations (facts about the world), and so
forth. These are called collaborative problem
solving objects, and are operated on by col-
laborative problem solving acts such as iden-
tity (present as a possibility), evaluate, adopt,
and others. Thus, together, two agents may
decide to adopt a certain objective, or iden-
tify a recipe to use for an objective. The
agreed-upon beliefs, objectives, recipes, and
so forth constitute the collaborative problem
solving state.
Of course, because the agents are au-
tonomous, no agent can single-handedly
change the collaborative problem solving
(CPS) state. Interaction acts are actions that
a single agent performs to attempt to change
the CPS state. The interaction acts are ini-
tiate, continue, complete, and reject. Initi-
ate proposes a new change to the CPS state.
Continue adds new information to the pro-
posal, and complete simply accepts the pro-
posal (bringing about the change), without
adding additional information. Of course,
proposals can be rejected at any time, causing
them to fail.
As an example, the utterance ?Let?s
save the heart-attack victim in Pitts-
ford? in an emergency planning domain
would be interpreted as two interaction
acts: (initiate (identify objective
(rescue person1))) and (initiate
(adopt objective (rescue person1))).
Here the user is proposing that they consider
rescuing person1 as a possible objective to
pursue. He is also proposing that they adopt
it as an objective to plan for.2
Interaction acts are recognized (via inten-
tion recognition) from speech acts. Inter-
action acts and speech acts differ in several
ways. First, speech acts describe a linguistic
level of interaction (ask, tell, etc.), whereas
interaction acts deal with a problem solving
level (adopting objectives, evaluating recipes
and so forth). Also, as shown above, a single
speech act may correspond to many interac-
tion acts.
2.2 Information Flow in the System
There are several paths along which informa-
tion asynchronously flows through the sys-
tem. We discuss information flow at the levels
of problem solving, discourse, and grounding.
The section that follows then gives an exam-
ple of how this proceeds.
2.2.1 Problem Solving Level
The problem solving level describes the ac-
tual underlying task or purposes of the di-
alogue and is based on interaction acts. We
first describe the problem solving information
flow when the user makes an utterance. We
then discuss the case where the system takes
initiative and how this results in an utterance
by the system.
User Utterance Following the diagram in
Figure 1, when a user makes an utterance,
it goes through the Speech Recognizer to the
Parser, which then outputs a list of speech
acts (which cover the input) to the Interpre-
tation Manager (IM). The IM then sends the
speech acts to Reference for resolution.
2Here two interaction acts are posited because of
the ability of the system to react to each separately,
for example completing the first, but rejecting the sec-
ond. Consider the possible response ?No, not right
now.? (accept this as a possible objective, but re-
ject adopting it right now), versus ?The 911 center
in Pittsford is handling that, we don?t have to worry
about it.? (reject this as even a possible objective and
reject adopting it). The scope of this paper precludes
us from giving more detail about multiple interaction
acts.
The IM then sends these speech act hy-
potheses to the Task Manager (TM), which
computes the corresponding interaction acts
for each as well as a confidence score that each
hypothesis is the correct interpretation.
Based on this, the IM then chooses the
best interpretation and broadcasts3 the cho-
sen CPS act(s) in a ?system understood? mes-
sage. The TM receives this message and
updates to the new collaborative problem
solving state which this interpretation en-
tails. The Behavioral Agent (BA) receives the
broadcast and decides if it wants to form any
intentions for action based on the interaction
act.
Assuming the BA decides to act on the
user?s utterance, it sends execution and rea-
soning requests to the TM, which passes them
on to the appropriate back-end components
and returns the result to the BA.
The BA then forms an interaction act based
on this result and sends it to the GM to be
communicated to the user. The GM then gen-
erates text and/or graphical updates based on
the interaction act and utters/presents them
to the user.
In most pipelined and pipelined flow-of-
information systems, the only flow of infor-
mation is at this problem solving level. In
TRIPS, however, there are other paths of in-
formation flow.
System Initiative TRIPS is also capable
of taking initiative. As we stated above, this
initiative originates in the BA and can come
from one of three areas: user utterances, pri-
vate system objectives, or exogenous events.
If the system, say because of an exogenous
event, decides to take initiative and commu-
nicate with the user, it sends an interaction
act to the GM. The GM then, following the
same path as above, outputs content to the
user.
3This is a selective broadcasts to the components
which have registered for such messages.
2.2.2 Discourse Level
The discourse level4 describes information
which is not directly related to the task at
hand, but rather is linguistic in nature. This
information is represented as salience infor-
mation (for Reference) and discourse obliga-
tions (Traum and Allen, 1994).
When the user makes an utterance, the in-
put passes (as detailed above) through the
Speech Recognizer, to the Parser, and then to
the IM, which calls Reference to do resolution.
Based on this reference resolved form, the IM
computes any discourse obligations which the
utterance entails (e.g., if the utterance was
a question, to address or answer it, also, to
acknowledge that it heard the question).
At this point, the IM broadcasts an ?sys-
tem heard? message, which includes incurred
discourse obligations and changes in salience.
Upon receipt of this message, Discourse Con-
text updates its discourse obligations and Ref-
erence updates its salience information.
The GM learns of new discourse obligations
from the Discourse Context and begins to try
to fulfill them, regardless of whether or not
it has heard from the BA about the prob-
lem solving side of things. However, there
are some obligations it will be unable to ful-
fill without knowledge of what is happening
at the problem solving level ? answering or
addressing the question, for example. How-
ever, other obligations can be fulfilled without
problem solving knowledge ? an acknowledg-
ment, for example ? in which case, the GM
produces content to fulfill the discourse obli-
gation.
If the GM receives interaction acts and
discourse obligations simultaneously, it must
produce content which fulfills both problem
solving and discourse needs. Usually, these
interaction acts and discourse obligations are
towards the same objective ? an obligation
to address or answer a question, and an inter-
action act of identifying a situation (commu-
4Although it works in a conceptually similar way,
the current system does not handle discourse level in-
formation flow quite so cleanly as is presented here.
We intend to clean things up and move to this exact
model in the near future.
nicating the answer to the user), for example.
However, because the system has the ability
to take initiative, these interaction acts and
discourse obligations may be disparate ? an
obligation to address or answer a question and
an interaction act to identify and adopt a new
pressing objective, for example. In this case,
the GM must plan content to fulfill the acts
and obligations the best it can ? apologize
for not answering the question and then in-
forming the user, for example. Through this
method, the GM maintains dialogue coher-
ence even though the BA is autonomous.
2.2.3 Grounding Level
The last level of information flow is at the
level that we loosely call grounding (Clark
and Schaefer, 1989; Traum, 1994).5 In
TRIPS, acts and obligations are not accom-
plished and contexts are not updated unless
the user has heard and/or understood the sys-
tem?s utterance.6
Upon receiving a new utterance, the IM
first determines if it contains evidence of the
user having heard and understood the utter-
ance.7 If the user heard and understood, the
IM broadcasts a ?user heard? message which
contains both salience information from the
previous system utterance as well as what dis-
course obligations the system utterance ful-
filled. This message can be used by Reference
to update salience information and by Dis-
course Context to discharge fulfilled discourse
obligations.
It is important that these contexts not be
updated until the system know that the user
heard its last utterance. If the user for ex-
ample, walks away as the system speaks, the
system?s discourse obligations will still not
fulfilled, and salience information will not
5TRIPS only uses a small subset of Traum?s
grounding model. In practice, however, this has not
presented problems thus far.
6The acceptance or rejection of the actual content
of an utterance is handled by our collaborative prob-
lem solving model (Allen et al, 2002; Blaylock, 2002)
and is not further discussed here.
7Hearing and understanding are not currently rec-
ognized separately in the system. For future work, we
would like to extend the system to handle them sepa-
rately (e.g., the case of the user having heard but not
understood).
change.
The GM receives the ?user heard? mes-
sage and also knows which interaction act(s)
the system utterance was presenting. It
then broadcasts a ?user understood? message,
which causes the TM to update the collabo-
rative problem solving state, and the BA to
release any goals and intentions fulfilled by
the interaction act(s).
Again, it is important that these context
updates do not occur until the system has ev-
idence that the user understood its last utter-
ance (for reasons similar to those discussed
above).
This handling of grounding frees the sys-
tem from the assumptions that the user al-
ways hears and understands each utterance.
2.3 An Example
We use here an example from our TRIPS
Medication Advisor domain ((Ferguson et al,
2002)). The Medication Advisor is a project
carried out in conjunction with the Cen-
ter for Future Health at the University of
Rochester.8 The system is designed to help
people (especially the elderly) understand and
manage their prescription medications.
With the huge growth in the number of
pharmaceutical therapies, patients tend to
end up taking a combination of several differ-
ent drugs, each of which has its own charac-
teristics and requirements. For example, each
drug needs to be taken at a certain rate: once
a day, every four hours, as needed, and so on.
Some drugs need to be taken on an empty
stomach, others with milk, others before or
after meals, and so on. Overwhelmed with
this large set of complex interactions many
patients simply do not (or cannot) comply
with their prescribed drug regimen (Claxton
et al, 2001).
The TRIPS Medication Advisor is designed
to help alleviate this problem by giving pa-
tients easy and accessible prescription infor-
mation an management in their own home.
For our example, we assume that a dialogue
between the system and user is in progress,
8http://www.centerforfuturehealth.org
and a number of other topics have been ad-
dressed. At this certain point in the conver-
sation, the system has just uttered ?Thanks,
I?ll try that? and now the user utters the fol-
lowing:
User: ?Can I take an aspirin??
We trace information flow first at the
grounding level, then at the discourse level,
and finally at the problem solving level. This
information flow is illustrated in Figure 2.
Grounding Level The utterance goes
through the Speech Recognizer and Parser to
the IM. As illustrated in Figure 2a, based on
the utterance, the IM recognizes that the user
heard and understood the system?s last ut-
terance, so it sends a ?user heard? message,
which causes the Discourse Context to update
discourse obligations and Reference to update
salience based on the system?s last utterance.
The GM receives the ?user heard? mes-
sage and sends the corresponding ?user un-
derstood? message, containing the interaction
act(s) motivating the system?s last utterance.
Upon receiving this message, the TM updates
the collaborative problem solving state, and
the BA updates its intentions and goals.
Meanwhile ... things have been happening
at the discourse level.
Discourse Level After the IM sends the
?user heard? message, as shown in Figure 2b,
it sends Reference a request to resolve refer-
ences within the user?s utterance. It then rec-
ognizes that the user has asked a question,
which gives the system the discourse obliga-
tions of answering (or addressing) the ques-
tion, as well as acknowledging the question.
The IM then sends a ?system heard?
message, which causes Reference to update
salience and Discourse Context to store the
newly-incurred discourse obligations.
The GM receives the new discourse obliga-
tions, but has not yet received anything from
the BA about problem solving (see below).
Without knowledge of what is happening in
problem solving, the GM is unable to ful-
fill the discourse obligation to answer (or ad-
dress) the question. However, it is able to ful-
fill the obligation of acknowledging the ques-
tion, so, after a certain delay of no response
from the BA, the GM plans content to pro-
duce an acknowledgment, which causes the
avatar9 to graphically show that it is think-
ing, and also causes the system to utter the
following:
System: ?Hang on.?
Meanwhile ... things have been happening
at the problem solving level as well.
Problem Solving Level After it sends the
?system heard? message, as shown in Fig-
ure 2c, the IM computes possible speech acts
for the input. In this case, there are two: a
yes-no question about the ability to take as-
pirin and a request to evaluate the action of
taking aspirin.
These are sent to the TM for intention
recognition. The first case (the yes-no ques-
tion) does not seem to fit the task model well
and receives a low score. (The system prefers
interpretations in which the user wants infor-
mation for a reason and not just for the sake
of knowing something.) The second speech
act is recognized as an initiate of an evalua-
tion of the action of taking aspirin (i.e., the
user wants to evaluate this action with the
system). This hypothesis receives a higher
score.
The IM chooses the second interpretation
and broadcasts a ?system understood? mes-
sage that announces this interpretation. The
TM receives this message and updates its
collaborative problem solving state to reflect
that the user did this interaction act. The
BA receives the message and, as shown in
Figure 2d, decides to adopt the intention of
doing the evaluation and reporting it to the
user. It sends an evaluation request for the ac-
tion of the user taking an aspirin to the TM,
which queries the back-end components (user
knowledge-base and medication knowledge-
base) about what prescriptions the user has
and if any of them interact with aspirin.
9The TRIPSMedication Advisor avatar is a talking
capsule whose top half rotates when it is thinking.
IM Ref
TM BA
GM
User understood (0)
User heard (0)
IM Ref
TM BA
GM
IM Ref
TM BA
GMResolveReply
System heard (1);Obligation to AckObligation to Answer
?Hang on? (2)
Address obligto Ack
System understood (1);CPS Act: evaluate-action
Interpret
Reply
IM Ref
TM BA
GM
?No, you are taking? (3)
Address obligto Answer
Perform PS Act
Result
Inform userof result
S: Thanks, I?ll try that.        (0)U: Can I take an aspirin?     (1)
(a) (b)
(c) (d)
Figure 2: Flow of Information for the Utterance ?Can I take an aspirin?? (a) Grounding Level,
(b) Discourse Level, (c) and (d) Problem-Solving Level
The back-end components report that the
user has a prescription for Celebrex, and that
Celebrex interacts with aspirin. The TM then
reports to the BA that the action is a bad
idea.
The BA then formulates an interaction act
reflecting these facts and sends it to the GM.
The GM then produces the following utter-
ance, which performs the interaction act as
well as fulfills the discourse obligation of re-
sponding to the question.
System: ?No, you are taking Celebrex
and Celebrex interacts with
aspirin.?
3 Synchronization
The architecture above is somewhat idealized
in that we have not yet given the details of
how the components know which context to
interpret messages in and how to ensure that
messages get to components in the right or-
der.
We first illustrate these problems by giving
a few examples. We then discuss the solution
we have implemented.
3.1 Examples of Synchronization
Problems
One of the problems that faces most dis-
tributed systems is that there is no shared
state between the agents. The first problem
with the architecture described in Section 2 is
the lack of context in which to interpret mes-
sages. This is well illustrated by the interpret
request from the IM to the TM.
As discussed above, the IM sends its candi-
date speech acts to the TM, which performs
intention recognition and assigns a score. The
problem is, in which context should the TM
interpret utterances? It cannot simply change
its collaborative problem solving state each
time it performs intention recognition, since it
may get multiple requests from the IM, only
one of which gets chosen to be the official ?in-
terpretation? of the system.
We have stated that the TM updates its
context each time it receives a ?system under-
stood? or ?user understood? message. This
brings up, however, the second problem of
our distributed system. Because all compo-
nents are operating asynchronously (includ-
ing the user, we may add), it is impossible
to guarantee that messages will arrive at a
component in the desired order. This is be-
cause ?desired order? is a purely pragmatic
assessment. Even with a centralized Facili-
tator through which all messages must pass,
the only guarantee is that messages from a
particular component to a particular compo-
nent will arrive in order; i.e., if component A
sends component B three messages, they will
get there in the order that component A sent
them. However, if components A and C each
send component B a message, we cannot say
which will arrive at component B first.
What this means is that the ?current? con-
text of the IM may be very different from that
of the TM. Consider the case where the sys-
tem has just made an utterance and the user
is responding. As we describe above, the first
thing the IM does is check for hearing and un-
derstanding and sends off a ?user heard? mes-
sage. The GM, when it receives this message,
sends the corresponding ?user understood?
message, which causes the TM to update to a
context containing the system?s utterance.
In the meantime, the IM is assuming the
context of the systems last utterance, as it
does interpretation. It then sends off inter-
pret requests to the TM. Now, if the TM re-
ceives an interpret request from the IM be-
fore it receives the ?user understood? message
from the GM, it will try to interpret the in-
put in the context of the user?s last utterance
(as if the user had made two utterance in a
row, without the system saying anything in
between). This situation will give erroneous
results and must be avoided.
3.2 Synchronization Solution
The solution to these problems is, of course,
synchronization: causing components to wait
at certain stages to make sure they are in
the same context. It is interesting to note
that these synchronization points are highly
related to a theory of grounding and common
ground.
To solve the first problem listed above (lack
of context), we have components append con-
text assumptions to the end of each message.
Thus, instead of the IM sending the TM a
request to interpret B, it sends the TM a re-
quest to interpret B in the context of hav-
ing understood A. Likewise, instead of the
IM requesting that Reference resolve D, it re-
quests that Reference resolve D having heard
C. Having messages explicitly contain con-
text assumptions allows components to inter-
pret messages in the correct context.
With this model, context now becomes dis-
crete, incrementing with every ?chunk? of
common ground.10 These common ground
updates correspond exactly to the ?heard?
and ?understood? messages we described
above. Thus, in order to perform a certain
task (reference resolution, intention recogni-
tion, etc.), a component must know in which
common ground context it must be done.
The solution to the second problem (mes-
sage ordering) follows from explicitly listing
context assumptions. If a component receives
a message that is appended with a context
about which the component hasn?t received
an update notice (the ?heard? or ?under-
stood? message), the component simply de-
fers processing of the message until it has re-
ceived the corresponding update message and
can update its context. This ensures that, al-
though messages may not be guaranteed to
arrive in the right order, they will be pro-
cessed in the right context. This provides
the necessary synchronization and allows the
asynchronous system components to work to-
gether in a coherent manner.
4 Discussion
We believe that, in general, this has sev-
eral ramifications for any agent-based, non-
pipelined flow-of-information architecture.
1. Agents which are queried about more
than one hypothesis must keep state for
10For now we treat each utterance as a single
?chunk?. We are interested, however, in moving to
more fine-grained models of dialogue. We believe that
our current architecture will still be useful as we move
to a finer-grained model.
all hypotheses until one is chosen.
2. Agents cannot assume shared context.
Because both the system components
and user are acting asynchronously, it
is impossible in general for any agent to
know what context another agent is cur-
rently in.
3. Agents must be able to defer working on
input. This feature allows them to wait
for synchronization if they receive a mes-
sage to be interpreted in a context they
have not yet reached.
Asynchronous agent-based architectures al-
low dialogue systems to interact with users in
a much richer and more natural way. Unfor-
tunately, the cost of moving to a truly dis-
tributed system is the need to deal with syn-
chronization. Fortunately, for dialogue sys-
tems, models of grounding provide a suitable
and intuitive basis for system synchroniza-
tion.
5 Conclusion and Future Work
In this paper we presented the TRIPS dia-
logue system architecture: an asynchronous,
agent-based architecture, with multiple lay-
ers of flow-of-information. We also discussed
the problems with building this distributed
system. As it turns out, models of ground-
ing provide a foundation for necessary system
synchronization.
For future work we plan to ?clean up? the
model in the ways we have discussed above.
We are also interested in moving to a more in-
cremental model of grounding, where ground-
ing can take place and context can change
within sentence boundaries. Also, we are in-
terested in extending the model to handle
asynchronous issues at the turn-taking level.
For example, what happens to context when
a user barges in while the system is talking, or
if the user and system speak simultaneous for
a time. We believe we will be able to lever-
age our asynchronous model to handle these
cases.
6 Acknowledgments
We would like to thank Amanda Stent, who
was involved with the original formulation of
this architecture. We also wish to thank the
anonymous reviewers for their helpful com-
ments.
This material is based upon work supported
by Department of Education (GAANN) grant
no. P200A000306; ONR research grant no.
N00014-01-1-1015; DARPA research grant
no. F30602-98-2-0133; NSF grant no. EIA-
0080124; and a grant from the W. M. Keck
Foundation.
Any opinions, findings, and conclusions or
recommendations expressed in this material
are those of the authors and do not necessar-
ily reflect the views of the above-mentioned
organizations.
References
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An archi-
tecture for a generic dialogue shell. Journal
of Natural Language Engineering special issue
on Best Practices in Spoken Language Dialogue
Systems Engineering, 6(3):1?16, December.
James Allen, George Ferguson, and Amanda
Stent. 2001a. An architecture for more real-
istic conversational systems. In Proceedings of
Intelligent User Interfaces 2001 (IUI-01), pages
1?8, Santa Fe, NM, January.
James F. Allen, Donna K. Byron, Myroslava
Dzikovska, George Ferguson, Lucian Galescu,
and Amanda Stent. 2001b. Towards conversa-
tional human-computer interaction. AI Maga-
zine, 22(4):27?37.
James Allen, Nate Blaylock, and George Fergu-
son. 2002. A problem solving model for col-
laborative agents. In First International Joint
Conference on Autonomous Agents and Multi-
agent Systems, Bologna, Italy, July 15-19. To
appear.
Nate Blaylock. 2002. Managing communica-
tive intentions in dialogue using a collaborative
problem solving model. Technical Report 774,
University of Rochester, Department of Com-
puter Science, April.
Jennifer Chu-Caroll and Michael K. Brown. 1997.
Initiative in collaborative interactions ? its
cues and effects. In S. Haller and S. McRoy,
editors, Working Notes of AAAI Spring 1997
Symposium on Computational Models of Mixed
Initiative Interaction, pages 16?22, Stanford,
CA.
Herbert H. Clark and Edward F. Schaefer. 1989.
Contributing to discourse. Cognitive Science,
13:259?294.
A. J. Claxton, J. Cramer, and C. Pierce. 2001.
A systematic review of the associations be-
tween dose regimens and medication compli-
ance. Clinincal Therapeutics, 23(8):1296?1310,
August.
George Ferguson and James F. Allen. 1998.
TRIPS: An intelligent integrated intelligent
problem-solving assistant. In Proceedings of the
Fifteenth National Conference on Artificial In-
telligence (AAAI-98), pages 567?573, Madison,
WI, July.
George Ferguson, James Allen, Nate Blaylock,
Donna Byron, Nate Chambers, Myroslava
Dzikovska, Lucian Galescu, Xipeng Shen,
Robert Swier, and Mary Swift. 2002. The Med-
ication Advisor project: Preliminary report.
Technical Report 776, University of Rochester,
Department of Computer Science, May.
Tim Finin, Yannis Labrou, and James Mayfield.
1997. KQML as an agent communication lan-
guage. In J. M. Bradshaw, editor, Software
Agents. AAAI Press, Menlo Park, CA.
L. Lamel, S. Rosset, J. L. Gauvain, S. Bennacef,
M. Garnier-Rizet, and B. Prouts. 1998. The
LIMSI ARISE system. In Proceedings of the 4th
IEEE Workshop on Interactive Voice Technol-
ogy for Telecommunications Applications, pages
209?214, Torino, Italy, September.
Mikio Nakano, Noboru Miyazaki, Jun ichi Hira-
sawa, Kohji Dohsaka, and Takeshi Kawabata.
1999. Understanding unsegmented user ut-
terances in real-time spoken dialogue systems.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics
(ACL-99), pages 200?207.
A. I. Rudnicky, E. Thayer, P. Constantinides,
C. Tchou, R. Shern, K. Lenzo, W. Xu, and
A. Oh. 1999. Creating natural dialogs
in the carnegie mellon communicator system.
In Proceedings of the 6th European Confer-
ence on Speech Communication and Technology
(Eurospeech-99), pages 1531?1534, Budapest,
Hungary, September.
Stephanie Seneff, Raymond Lau, and Joseph Po-
lifroni. 1999. Organization, communication,
and control in the Galaxy-II conversational sys-
tem. In Proceedings of the 6th European Con-
ference on Speech Communication and Tech-
nology (Eurospeech-99), Budapest, Hungary,
September.
Amanda Stent, John Dowding, Jean Mark
Gawron, Elizabeth Owen Bratt, and Robert
Moore. 1999. The CommandTalk spoken dia-
logue system. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-99).
David R. Traum and James F. Allen. 1994.
Discourse obligations in dialogue processing.
In Proceedings of the 32nd Annual Meeting of
the Association for Computational linguistics
(ACL-94), pages 1?8, Las Cruces, New Mexico.
David R. Traum. 1994. A computational theory
of grounding in natural language conversation.
Technical Report 545, University of Rochester,
Department of Computer Science, December.
PhD Thesis.
Proceedings of NAACL HLT 2009: Short Papers, pages 45?48,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
TESLA: A Tool for Annotating Geospatial Language Corpora
Nate Blaylock and Bradley Swain and James Allen
Institute for Human and Machine Cognition (IHMC)
Pensacola, Florida, USA
{blaylock,bswain,jallen}@ihmc.us
Abstract
In this paper, we present The gEoSpatial
Language Annotator (TESLA)?a tool which
supports human annotation of geospatial lan-
guage corpora. TESLA interfaces with a GIS
database for annotating grounded geospatial
entities and uses Google Earth for visualiza-
tion of both entity search results and evolving
object and speaker position from GPS tracks.
We also discuss a current annotation effort us-
ing TESLA to annotate location descriptions
in a geospatial language corpus.
1 Introduction
We are interested in geospatial language under-
standing? the understanding of natural language
(NL) descriptions of spatial locations, orientation,
movement and paths that are grounded in the real
world. Such algorithms would enable a number of
applications, including automated geotagging of text
and speech, robots that can follow human route in-
structions, and NL-description based localization.
To aide development of training and testing cor-
pora for this area, we have built The gEoSpa-
tial Language Annotator (TESLA)?a tool which
supports the visualization and hand-annotation of
both text and speech-based geospatial language cor-
pora. TESLA can be used to create a gold-standard
for training and testing geospatial language under-
standing algorithms by allowing the user to anno-
tate geospatial references with object (e.g., streets,
businesses, and parks) and latitude and longitude
(lat/lon) coordinates. An integrated search capa-
bility to a GIS database with results presented in
Google Earth allow the human annotator to eas-
ily annotate geospatial references with ground truth.
Figure 1: A session in the PURSUIT Corpus
Furthermore, TESLA supports the playback of GPS
tracks of multiple objects for corpora associated
with synchronized speaker or object movement, al-
lowing the annotator to take this positional context
into account. TESLA is currently being used to an-
notate a corpus of first-person, spoken path descrip-
tions of car routes.
In this paper, we first briefly describe the corpus
that we are annotating, which provides a grounded
example of using TESLA. We then discuss the
TESLA annotation tool and its use in annotating that
corpus. Finally, we describe related work and our
plans for future work.
2 The PURSUIT Corpus
The PURSUIT Corpus (Blaylock and Allen, 2008)
is a collection of speech data in which subjects de-
scribe their path in real time (i.e., while they are trav-
eling it) and a GPS receiver simultaneously records
the actual paths taken. (These GPS tracks of the
actual path can aide the annotator in determining
what geospatial entities and events were meant by
the speaker?s description.)
Figure 1 shows an example of the experimental
setup for the corpus collection. Each session con-
sisted of a lead car and a follow car. The driver of the
45
Figure 2: The TESLA annotation and visualization windows
lead car was instructed to drive wherever he wanted
for an approximate amount of time (around 15 min-
utes). The driver of the follow car was instructed to
follow the lead car. One person in the lead car (usu-
ally a passenger) and one person in the follow car
(usually the driver) were given close-speaking head-
set microphones and instructed to describe, during
the ride, where the lead car was going, as if they
were speaking to someone in a remote location who
was trying to follow the car on a map. The speak-
ers were also instructed to try to be verbose, and
that they did not need to restrict themselves to street
names?they could use businesses, landmarks, or
whatever was natural. Both speakers? speech was
recorded during the session. In addition, a GPS re-
ceiver was placed in each car and the GPS track was
recorded at a high sampling rate. The corpus con-
sists of 13 audio recordings1 of seven paths along
with the corresponding GPS tracks. The average
session length was 19 minutes.
3 TESLA
TESLA is an extensible tool for geospatial language
annotation and visualization. It is built on the NXT
Toolkit (Carletta et al, 2003) and data model (Car-
letta et al, 2005) and uses Google Earth for visu-
alization. It supports geospatial entity search using
the TerraFly GIS database (Rishe et al, 2005). Cur-
rently, TESLA supports annotation of geospatial lo-
cation referring expressions, but is designed to be
easily extended to other annotation tasks for geospa-
1In one session, there was no speaker in the lead car.
tial language corpora. (Our plans for extensions are
described in Section 6.)
Figure 2 shows a screenshot of the main view
in the TESLA annotator, showing a session of the
PURSUIT Corpus. In the top-left corner is a wid-
get with playback controls for the session. This pro-
vides synchronized playback of the speech and GPS
tracks. When the session is playing, audio from a
single speaker (lead or follow) is played back, and
the blue car icon in the Google Earth window on the
right moves in synchronized fashion. Although this
Google Earth playback is somewhat analogous to a
video of the movement, Google Earth remains us-
able and the user can move the display or zoom in
and out as desired. If location annotations have pre-
viously been made, these pop up at the given lat/lon
as they are mentioned in the audio, allowing the an-
notator to verify that the location has been correctly
annotated. In the center, on the left-hand side is a
display of the audio transcription, which also moves
in sync with the audio and Google Earth visualiza-
tion. The user creates an annotation by highlighting
a group of words, and choosing the appropriate type
of annotation. The currently selected annotation ap-
pears to the right where the corresponding geospatial
entity information (e.g., name, address, lat/lon) can
be entered by hand, or by searching for the entity in
a GIS database.
3.1 GIS Search and Visualization
In addition to allowing information on annotated
geospatial entities to be entered by hand, TESLA
also supports search with a GIS database. Cur-
46
Figure 3: Search results display in TESLA
rently, TESLA supports search queries to the Ter-
raFly database (Rishe et al, 2005), although other
databases could be easily added. TerraFly contains
a large aggregation of GIS data from major distrib-
utors including NavTeq and Tiger streets and roads,
12 million U.S. Businesses through Yellow Pages,
and other various freely available geospatial data.
It supports keyword searches on database fields as
well as radius-bounded searches from a given point.
TESLA, by default, uses the position of the GPS
track of the car at the time of the utterance as the
center for search queries, although any point can be
chosen.
Search results are shown to the user in Google
Earth as illustrated in Figure 3. This figure shows
the result of searching for intersections with the key-
word ?Romana?. The annotator can then select one
of the search results, which will automatically pop-
ulate the geospatial entity information for that an-
notation. Such visualization is important in geospa-
tial language annotation, as it allows the annotator
to verify that the correct entity is chosen.
4 Annotation of the PURSUIT Corpus
To illustrate the use of TESLA, we briefly describe
our current annotation efforts on the PURSUIT Cor-
pus. We are currently involved in annotating refer-
ring expressions to locations in the corpus, although
later work will involve annotating movement and
orientation descriptions as well.
Location references can occur in a number of syn-
tactic forms, including proper nouns (Waffle House),
definite (the street) and indefinite (a park) refer-
ences, and often, complex noun phrases (one of the
historic churches of Pensacola). Regardless of its
syntactic form, we annotate all references to loca-
tions in the corpus that correspond to types found
in our GIS database. References to such things as
fields, parking lots, and fire hydrants are not anno-
tated, as our database does not contain these types
of entities. (Although, with access to certain local
government resources or advanced computer vision
systems, these references could be resolved as well.)
In PURSUIT, we markup the entire noun phrase (as
opposed to e.g., the head word) and annotate that
grouping.
Rather than annotate a location reference with just
latitude and longitude coordinates, we annotate it
with the geospatial entity being referred to, such
as a street or a business. The reasons for this are
twofold: first, lat/lon coordinates are real numbers,
and it would be difficult to guarantee that each ref-
erence to the same entity was marked with the same
coordinates (e.g., to identify coreference). Secondly,
targeting the entity allows us to include more infor-
mation about that entity (as detailed below).
In the corpus, we have found four types of en-
tities that are references, which are also in our
database: streets, intersections, addresses (e.g., 127
Main Street), and other points (a catch-all category
containing other point-like entities such as busi-
nesses, parks, bridges, etc.)
An annotation example is shown in Figure 4,
in which the utterance contains references to two
47
Figure 4: Sample annotations of referring expressions to
geospatial locations
streets and an intersection. Here the intersection re-
ferring expression spans two referring expressions to
streets, and each is annotated with a canonical name
as well as lat/lon coordinates. Note also that our
annotation schema allows us to annotate embedded
references (here the streets within the intersection).
5 Related Work
The SpatialML module for the Callisto annotator
(Mani et al, 2008) was designed for human anno-
tation of geospatial locations with ground truth by
looking up targets in a gazetteer. It does not, how-
ever, have a geographic visualization components
such as Google Earth and does not support GPS
track playback.
The TAME annotator (Leidner, 2004) is a simi-
lar tool, supporting hand annotation of toponym ref-
erences by gazetteer lookup. It too does not, as
far as we are aware, have a visualization compo-
nent nor GPS track information, likely because the
level of geospatial entities being looked at were at
the city/state/country level. The PURSUIT Corpus
mostly contains references to geospatial entities at
a sub-city level, which may introduce more uncer-
tainty as to the intended referent.
6 Conclusion and Future Work
In this paper, we have presented TESLA?a gen-
eral human annotation tool for geospatial language.
TESLA uses a GIS database, GPS tracks, and
Google Earth to allow a user to annotate refer-
ences to geospatial entities. We also discussed how
TESLA is being used to annotate a corpus of spoken
path descriptions.
Though currently we are only annotating PUR-
SUIT with location references, future plans in-
clude extending TESLA to support the annotation
of movement, orientation, and path descriptions. We
also plan to use this corpus as test and training data
for algorithms to automatically annotate such infor-
mation.
Finally, the path descriptions in the PURSUIT
Corpus were all done from a first-person, ground-
level perspective. As TESLA allows us to replay the
actual routes from GPS tracks within Google Earth,
we believe we could use this tool to gather more spo-
ken descriptions of the paths from an aerial perspec-
tive from different subjects. This would give us sev-
eral more versions of descriptions of the same path
and allow the comparison of descriptions from the
two different perspectives.
References
Nate Blaylock and James Allen. 2008. Real-time path
descriptions grounded with gps tracks: a preliminary
report. In LREC Workshop on Methodologies and Re-
sources for Processing Spatial Language, pages 25?
27, Marrakech, Morocco, May 31.
Jean Carletta, Stefan Evert, Ulrich Heid, Jonathan Kil-
gour, Judy Robertson, and Holger Voormann. 2003.
The NITE XML toolkit: flexible annotation for multi-
modal language data. Behavior Research Methods, In-
struments, and Computers, 35(3):353?363.
Jean Carletta, Stefan Evert, Ulrich Heid, and Jonathan
Kilgour. 2005. The NITE XML toolkit: data model
and query language. Language Resources and Evalu-
ation Journal, 39(4):313?334.
Jochen L. Leidner. 2004. Towards a reference corpus
for automatic toponym resolution evaluation. In Work-
shop on Geographic Information Retrieval, Sheffield,
UK.
Inderjeet Mani, Janet Hitzeman, Justin Richer, Dave Har-
ris, Rob Quimby, and Ben Wellner. 2008. SpatialML:
Annotation scheme, corpora, and tools. In 6th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2008), Marrakech, Morocco, May.
N. Rishe, M. Gutierrez, A. Selivonenko, and S. Graham.
2005. TerraFly: A tool for visualizing and dispensing
geospatial data. Imaging Notes, 20(2):22?23.
48
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 146?154,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Building Timelines from Narrative Clinical Records: Initial Results 
Based-on Deep Natural Language Understanding
Hyuckchul Jung, James Allen, Nate Blaylock, Will de Beaumont, 
Lucian Galescu, Mary Swift
Florida Institute for Human and Machine Cognition
40 South Alcaniz Street, Pensacola, Florida, USA
{hjung,blaylock,jallen,wbeaumont,lgalescu,mswift}@ihmc.us
Abstract
We present an end-to-end system that proc-
esses narrative clinical records, constructs 
timelines for the medical histories of pa-
tients, and visualizes the results. This work 
is motivated by real clinical records and 
our general approach is based on deep se-
mantic natural language understanding.
1 Introduction
It is critical for physicians and other healthcare 
providers to have complete and accurate knowl-
edge of the medical history of patients that  in-
cludes disease/symptom progression over time and 
related tests/treatments in chronological order. 
While various types of clinical records (e.g., dis-
charge summaries, consultation notes, etc.) contain 
comprehensive medical history information, it  can 
be often challenging and time-consuming to com-
prehend the medical history of patients when the 
information is stored in multiple documents in dif-
ferent  formats and the relations among various 
pieces of information is not explicit.
For decades, researchers have investigated tem-
poral information extraction and reasoning in the 
medical domain (Zhou and Hripcsak, 2007). How-
ever, information extraction in the medical domain 
typically relies on shallow NLP techniques (e.g., 
pattern matching, chunking, templates, etc.),  and 
most temporal reasoning techniques are based on 
structured data with temporal tags (Augusto, 2005; 
Stacey and McGregor, 2007).
In this paper, we present our work on develop-
ing an end-to-end system that  (i) extracts interest-
ing medical concepts (e.g., medical conditions/
tests/treatments), related events and temporal ex-
pressions from raw clinical text records, (ii) con-
structs timelines of the extracted information; and 
(iii) visualizes the timelines, all using deep seman-
tic natural language understanding (NLU). 
Our deep NLU system extracts rich semantic 
information from narrative text records and builds 
logical forms that  contain ontology types as well as 
linguistic features. Ontology- and pattern-based 
extraction rules are used on the logical forms to 
retrieve time points/intervals, medical concepts/
events and their temporal/causal relations that are 
pieced together by our system?s temporal reasoning 
component to create comprehensive timelines.
Our system is an extension to a well-proven 
general-purpose NLP system (Allen et  al., 2000) 
rather than a system specialized to the clinical do-
main, and the temporal reasoning in our system is 
tightly integrated into the NLP system?s deep se-
mantic analysis. We believe this approach will al-
low us to process a broader variety of documents 
and complex forms of temporal expressions.
In the coming sections, we first present a moti-
vating example, a real clinical record of a cancer 
patient. Next, we give an overview of our NLU 
system including how medical ontology is inte-
grated into our system. The overview section is 
followed by detailed description of our information 
extraction and temporal reasoning approach. Then, 
we discuss our results and conclude.
2 Motivating Example
Our work is carried out as a collaboration with the 
Moffitt  Cancer Center (part of the NCI Compre-
hensive Cancer Centers), who have provided us 
with access to clinical records for over 1500 pa-
tients. Figure 1 shows a (de-identified) ?History of 
Present Illness? (HPI) section of a Thoracic Con-
sultation Note from this data set. 
146
The text of this section provides a very detailed 
description of what  problems/tests/treatments an 
anonymous cancer patient went  through over a pe-
riod. Such narrative text is common in clinical 
notes and, because such notes are carefully created 
by physicians, they tend to have only relevant in-
formation about patient medical history. 
Nonetheless, there are lots of challenges in con-
structing complete and accurate medical history 
because of complex temporal expressions/
relations, medical language specific grammar/
jargons, implicit  information and domain-specific 
medical knowledge (Zhou and Hripcsak, 2007).
In this paper, as an initial step towards con-
structing complete timelines from narrative text, 
we focus on sentences with explicit  temporal ex-
pressions listed below (tagged as Line 1 ~ 11) plus 
a sentence in the present tense (Line 12):
1
?
Line 1: She had a left radical nephrectomy in  09/
2007; pathological stage at that time  was a T3 
NX MX. 
?
Line 2: Prior to her surgery CT scan in 08/2007 
showed lung nodules. 
?
Line 3: She was placed on Nexavar in 11/2007. 
?
Line 4: She was started on Afinitor on 03/05/08. 
?
Line 5: She states that prior to starting the Afini-
tor she had no shortness of breath or dyspnea on 
exertion and she was quite active. 
?
Line 6: Unfortunately 4 weeks after starting the 
Afinitor she developed a dry cough and progres-
sive shortness of breath with dyspnea on exer-
tion. 
?
Line 7: She received a 5 day dose pack of 
prednisone and was treated with Augmentin in 
05/2008. 
?
Line 8: She subsequently had a CT scan of the 
chest  done on 05/14/08 that  showed interval de-
velopment of bilateral lower lobe infiltrates that 
were not present on the 02/19/08 scan. 
?
Line 9: Because of her respiratory symptoms, the 
Afinitor was stopped on 05/18/2008. 
?
Line 10: Prior to  the Afinitor she was able to 
walk, do gardening, and swim without any 
shortness of breath.  
?
Line 11: She has had a 140 pound weight  since 
10/2007.
?
Line 12: She denies fevers, chills, hemoptysis or 
chest pain. 
In these 12 sentences, there are instances of 10 
treatments (e.g., procedures such as ?nephrectomy? 
and drugs such as ?Nexavar?), 3 tests (e.g., CT-
scan), 13 problems/symptoms (e.g., lung nodules) 
and 2 other types of clinical findings (e.g., the can-
cer stage level ?T3 NX MX?). There are also 23 
events of various types represented with verbs such 
as  ?had?, ?was?, ?showed?, and ?was started?.
While there are simple expressions such as ?on 
03/05/08? in Line 3, there are also temporal ex-
pressions in more complex forms with time rela-
tions (e.g., ?prior to?), time references (e.g., ?at 
that time?) or event references (e.g., ?4 weeks after 
starting Afinitor?). Throughout  this paper, we will 
use Line 1 ~ 12 as a concrete example based on 
which we develop general techniques to construct 
timelines.
1
 For privacy, identities of patients/physicians were concealed and the dates/time-spans in the original sources were 
altered while maintaining their chronological order. Some measurements and geographic names were also modified.
Figure 1: A sample medical record -- Thoracic 
Consultation Note
1
PAST MEDICAL HISTORY: 
1. History of melanoma of the left arm.  She had excision of 3 sentinel lymph nodes in the left axilla 
that were negative.  This was in 07/2007.
2. Status post right hip replacement.
3. Status post cholecystectomy.
4. Status post renal stone removal.
5. Fracture of the right hip and left wrist in a motor vehicle accident.
6. Diabetes.
7. Elevated cholesterol.
8. Hypertension.
9. Spinal stenosis. 
ALLERGIES:
She has no known drug allergies.  She is allergic to IVP dye which causes shortness of breath.  She 
tolerates IV dye when she is pre treated.  
SOCIAL HISTORY: 
She is born and raised in California and she lived in Florida for 30 years.  She has worked as a 
medical billing analyst.  She has never smoked. She does not use alcoholic beverages.
FAMILY HISTORY:
Her father died at age 69 of prostate cancer.  Her mother died at age 72 of emphysema.  She had 1 
sister who died from melanoma.  
REVIEW OF SYSTEMS: 
A complete review of systems was performed.  See the questionnaire.  She has hypothyroidism.  
She has some back pain related to her spinal stenosis.  She suffers from mild depression.  
CURRENT MEDICATIONS:
1. Carvedilol 6.25 mg p.o. daily.
2. Darvocet N100, 1 tablet as needed.
3. Fish oil, 1000 mg three times a day.
4. Glimepiride 4 mg daily in the morning and 2 mg at bedtime.
5. Lipitor 20 mg daily.
6. Metformin 1000 mg twice daily.
7. Paroxetine 20 mg daily.
8. Synthroid 0.112 mg daily.
9. Tylenol as needed.
10. Vitamin B12, 2500 mcg p.o. twice daily.
XXX X XX-XX-XX 
CONSULTATION DATE: 07/06/2008
RE: XXX BIRTH DATE: XX/XX/XXXX
UR#: XX-XX-XX AGE: 75
THORACIC CONSULTATION NOTE
REQUESTING PHYSICIAN:
XXXXXXXXXX, MD.
REASON FOR CONSULTATION:
Shortness of breath and abnormal chest x ray.
HISTORY OF PRESENT ILLNESS:
Ms. XXX is a 75 year old woman who has a history of metastatic renal cancer.  She had a left radical 
nephrectomy in 09/2007; pathological stage at that time was a T3 NX MX.  Prior to her surgery CT 
scan in 08/2007 showed lung nodules.  These nodules have progressed with time.  She was placed 
on Nexavar in 11/2007.  She subsequently was found to have a new mass in her left nephrectomy 
bed.  She was continued on the Nexavar, however, she showed radiographic progression and the 
Nexavar was discontinued.  She was started on Afinitor on 03/05/08.  She states that prior to starting 
the Afinitor she had no shortness of breath or dyspnea on exertion and she was quite active.  
Unfortunately 4 weeks after starting the Afinitor she developed a dry cough and progressive 
shortness of breath with dyspnea on exertion.  She received a 5 day dose pack of prednisone and 
was treated with Augmentin in 05/2008.  This had no impact on her cough or shortness of breath.  
She subsequently had a CT scan of the chest done on 05/14/08 that showed interval development 
of bilateral lower lobe infiltrates that were not present on the 02/19/08 scan.  She had mediastinal 
and right hilar adenopathy that had increased.  She had multiple lung nodules and there was 
recurrent tumor noted in the left renal bed which was thought to be larger.  Because of her 
respiratory symptoms, the Afinitor was stopped on 05/18/08.  She still has a dry cough.  She is short 
of breath after walking 15 to 20 feet.  She has no shortness of breath at rest.  She denies PND or 
orthopnea.  Prior to the Afinitor she was able to walk, do gardening, and swim without any shortness 
of breath.  She has had a 140 pound weight since 10/2007.  She notices anorexia.  She has no 
travel history.
She denies fevers, chills, hemoptysis or chest pain.  She has never smoked.  She denies 
pneumonia, asthma, wheezing, or myocardial infarction, congestion heart failure or heart murmur.  
She has dogs and cats at home and has had them for a long time and this never caused her 
respiratory problems. 
PHYSICAL EXAMINATION: 
VITAL SIGNS:   Blood pressure 131/74, pulse 106, respiratory rate 20, temperature 97.3, weight 
64.0 kg.
HEENT:   Pupils equal, round, reactive to light.  Extraocular muscles were intact.  Nose and mouth 
were clear. 
NECK:  Trachea midline.  Carotids were 2 plus.  No masses, thyromegaly or adenopathy.  
LUNGS:   Respirations were unlabored.  There is no dullness to percussion or tenderness to 
palpation.  She has some bibasilar dry rales.
HEART:   Regular rate and rhythm without murmur.
ABDOMEN:  Soft, positive bowel sounds, nontender. 
EXTREMITIES:   No clubbing or cyanosis.  She had some mild pedal edema. 
DATABASE:
Chest x ray from 06/01/08 was reviewed.  She had bilateral lower lobe patchy densities.  She had 
some nodular densities bilaterally as well.  There is widening of the mediastinum on the right.  CT 
scan of the chest from 05/14/08 also was reviewed.  She had bilateral lower lobe infiltrates that were 
new.  She had mediastinal and right hilar adenopathy.  She had multiple lung nodules.  There is 
recurrent tumor in the left renal bed that was thought to be larger. 
IMPRESSION:
1. Metastatic renal cancer with multiple lung nodules with mediastinal and hilar adenopathy.  
2. Bilateral lower lobe infiltrates.  These infiltrates had developed after starting the Afinitor, as did 
her shortness of breath and dyspnea on exertion.  She recently started on oxygen by her primary 
care physician when she was found to have exercise O2 saturations of 86%.  She is currently taking 
2 liters of oxygen.  I would be concerned that the infiltrates may be related to pneumonitis from the 
Afinitor.  I also think her shortness of breath, cough and hypoxemia are related to the infiltrates as 
well.  
RECOMMENDATIONS:
1. I reviewed my impressions with the patient.
2. I am going to schedule her for a bronchoscopy and bronchoalveolar lavage.  I am going to get 
baseline pulmonary function tests on her. 
3. She will be seen by Dr. XXX on 08/12/08.  I will call and discuss the case with him pending the 
above results.  The options are likely going to be observation off Afinitor or may consider placing her 
on prednisone, if the bronchoalveolar lavage is unremarkable.  
4. Further recommendations will be made after the above.
Do not type or edit below this line. This will cause format damage.
  
Dictated by XXXX, MD
Electronically Signed
FXXXXXXX, MD 07/10/2008 10:15
________________________
XXXXX, MD
DD: 07/10/2008  9:24 A
DT: 07/13/2008 11:46 A
ID: XXXXXXX.LML
CS: XXXXXX
cc: 
??
HISTORY OF PRESENT ILLNESS:
Ms. XXX is a 75 year ld woman who has a history of metastatic renal 
cancer. She had a left adi al nephrectomy in 09/2007; pathological stage 
at that time was a T3 NX MX. Prior to her surgery CT scan in 08/2007 
showed lung nodules. These nodules have progressed with time. She was 
placed on Nexavar in 11/2007. She subsequently was found to have a 
new mass in her left nephrectomy bed. She was continued on the 
N xavar, owever, she showed radiographic progression and the Nexavar 
was disc ntinued. She was started on Afinitor on 03/05/08. She states 
that p i r to starting the Afinitor she had no shortness of breath or 
dys n a on exe tion and she was quite active. Unfortunately 4 weeks 
after starting the Afinitor she developed a dry cough and progressive 
tness f br th with dyspnea on exertion. She received a 5 day dose 
pack of prednisone and was treated with Augmentin in 05/2008. This had 
no impact on her cough or shortness of breath. She subsequently had a 
CT scan of the chest done on 05/14/08 that showed interval development 
of bilateral lower lobe infiltrates that were not present on the 02/19/08 
scan. She had mediastinal and right hilar adenopathy that had increased. 
She had multiple lung nodules and there was recurrent tumor noted in the 
left renal bed which was thought to be larger. Because of her respiratory 
symptoms, the Afinitor was stopped on 05/18/2008. She still has a dry 
cough. She is short of breath after walking 15 to 20 feet. She has no 
shortness of breath at rest. She denies PND or orthopnea. Prior to the 
Afinitor she was able to walk, do gardening, and swim without any 
shortness of breath. She has had a 140 pound weight since 10/2007. She 
notices anorexia. She has no travel history. She denies fevers, chills, 
hemoptysis or chest pain. She has never smoked. She denies pneumonia, 
asthma, wheezing, or myocardial infarction, congestion heart failure or 
heart murmur. She has dogs and cats at home and has had them for a long 
time and this never caused her respiratory problems.
147
3 Natural Language Understanding 
(NLU) System
Our system is an extension to an existing NLU sys-
tem that is the result of a decade-long research ef-
fort  in developing generic natural language tech-
nology. The system uses a ?deep? understanding 
approach, attempting to find a linked, overall 
meaning for all the words in a paragraph. An archi-
tectural view of the system is shown in Figure 2.
3.1 Core NLU Components
At the core of the system is a packed-forest  chart 
parser which builds constituents bottom-up using a 
best-first search strategy. The core grammar is a 
hand-built, lexicalized context-free grammar, aug-
mented with feature structures and feature unifica-
tion. The parser draws on a general purpose seman-
tic lexicon and ontology which define a range of 
word senses and lexical semantic relations. The 
core semantic lexicon was constructed by hand and 
contains more than 7000 lemmas. It  can be also 
dynamically augmented for unknown words by 
consulting WordNet (Miller, 1995). 
To support  more robust processing as well as 
domain configurability, the core system is in-
formed by a variety of statistical and symbolic pre-
processors. These include several off-the-shelf sta-
tisical NLP tools such as the Stanford POS tagger 
(Toutanova and Manning, 2000), the Stanford 
named-entity recognizer (NER) (Finkel et al, 
2005) and the Stanford Parser (Klein and Manning, 
2003). The output of these and other specialized 
preprocessors (such as a street address recognizer) 
are sent  to the parser as advice. The parser then can 
include or not include this advice (e.g., that a cer-
tain phrase is a named entity) as it searches for the 
optimal parse of the sentence.
The result  of parsing is a frame-like semantic 
representation that we call the Logical Form (LF). 
The LF representation includes semantic types, 
semantic roles for predicate arguments, and de-
pendency relations. Figure 3 shows an LF example 
for the sentence ?She had a left radical nephrec-
tomy in 09/2007?. In the representation, elements 
that start  with colons (e.g., :THEME) are semantic 
roles of ontological concepts, and role values can 
be a variable to refer to another LF term.
3.2 UMLS Integration
By far the most  critical aspect  of porting our ge-
neric NLU components to the task of understand-
ing clinical text  is the need for domain-specific 
lexical and ontologic information. One widely used 
comprehensive resource that  can provide both is 
the National Library of Medicine?s Unified Medi-
cal Language System (UMLS) (Bodenreider, 
2004). UMLS was integrated into or system via 
MetaMap (Aronson and Lang, 2010), a tool also 
developed by NLM, that  can identify and rank 
UMLS concepts in text.
Specifically, we added MetaMap as a special 
kind of named entity recognizer feeding advice 
into the Parser?s input chart (see Figure 2). We run 
MetaMap twice on the input  text  to obtain UMLS 
information both for the maximal constituents, and 
for individual words in those constituents (e.g., 
?lung cancer?, as well as ?lung? and ?cancer?).
The lexicon constructs representations for the 
new words and phrases on the fly. Our general ap-
proach for dealing with how the corresponding 
concepts fit  in our system ontology uses an ontol-
Core Lexicon
& Semantic Ontology
Grammar
Parser
Wordnet
Unknown Word 
Processing
New Lexical Entries
Output
Chart
Input
Chart
Statistical Parser
Bracketing Preferences
Input
Named Entity 
Recognizer
Name 
Hypotheses
POS 
Tagging
POS
Hypotheses
Word Hypotheses
MetaMap UMLS
UMLS
POS/sense 
Hypotheses
LF Semantic 
Representation 
for reasoners
Figure 2: Front-end language processing components with MetaMap and UMLS
148
ogy specialization mechanism which we call on-
tology grafting, whereby new branches are created 
from third party ontological sources, and attached 
to appropriate leaf nodes in our ontology.
The UMLS Semantic Network and certain vo-
cabularies included in the UMLS Metathesaurus 
define concept hierarchies along multiple axes. 
First, we established links between the 15 UMLS 
semantic groups and corresponding concepts in our 
ontology. Second, we selected a list of nodes from 
the SNOMED-CT and NCI hierarchies (27 and 11 
nodes, respectively) and formed ontological 
branches rooted in these nodes that  we grafted onto 
our ontology. 
Based on these processes, UMLS information 
gets integrated into our LF representation. In Fig-
ure 3, the 3rd term has a role called :domain-info 
and, in fact, its value is (UMLS :CUI C2222800 
:CONCEPT "left nephrectomy" :PREFERRED 
"nephrectomy of left kidney (treatment)" 
:SEMANTIC-TYPES (TOPP) :SEMANTIC-
GROUPS (PROC) :SOURCES (MEDCIN MTH)) 
that provides detailed UMLS concept information. 
Here, the semantic type ?TOPP? is a UMLS abbre-
viation for ?Therapeutic or Preventive Procedure?. 
More details about complex issues surrounding 
UMLS integration into our system can be found in 
(Swift et al, 2010).
4 Information Extraction (IE) from Clinical 
Text Records
In this section, we describe how to extract basic 
elements that will be used as a foundation to con-
struct timelines. We first describe our general ap-
proach to extracting information from LF graphs. 
Then we give details specific to the various types 
of information we extract in our system: various 
clinical concepts, temporal concepts (points as well 
as intervals), events and temporal relations.
4.1 LF Pattern-based Extraction
Given LF outputs from the NLU system described 
in Section 3, we use LF pattern-based rules for in-
formation extraction. The basic structure of an ex-
traction rule is a list  of LF patterns followed by a 
unique rule ID and the output specification.
Each LF-pattern specifies a pattern against  an 
LF. Variables can appear anywhere except as role 
names in different formats:
?
?x - (unconstrained) match anything 
?
?!x - match any non-null value
?
(? x V1 V2 ...) - (constrained) match one of the 
specified values V1,  V2, ...
As an example, the extraction rule in Figure 4 
will match LFs that mean a person had a treatment 
or a medical-diagnostic with explicit  UMLS in-
formation (i.e., part of LFs in Figure 3 matches). 
The output specification records critical informa-
tion from the extraction to be used by other rea-
soners. 
The extraction rules have all been developed by 
hand. Nevertheless, they are quite general, since a) 
LF patterns abstract away from lexical and syntac-
tic variability in the broad class of expressions of 
interest (however, lexical and syntactic features 
may be used if needed); and b) LF patterns make 
heavy use of ontological categories, which pro-
vides abstraction at the semantic level.
4.2 Clinical Concept Extraction
Among various types of concepts included in clini-
cal records, we focus on concepts related to 
problems/tests/treatments to build a medical his-
(F V1 (:* ONT::HAVE W::HAVE) :AFFECTED V2 :THEME V3 :MOD V4 :TENSE W::PAST) 
(PRO V2 (:* ONT::PERSON W::SHE) :PROFORM ONT::SHE :CO-REFERENCE V0) 
(A V3 (:* ONT::TREATMENT W::LEFT-RADICAL-NEPHRECTOMY) :DOMAIN-INFO (UMLS .....)
(F V4 (:* ONT::TIME-SPAN-REL W::IN) :OF V1 :VAL V5)
(THE V5 ONT::TIME-LOC :YEAR 2007 :MONTH 9)
Figure 3: LF semantic representation for ?She had a left radical nephrectomy in 09/2007?
(?x1 ?y2 (? type1 ONT::HAVE) :AFFECTED ?y2 :THEME ?y3 :MOD ?y4)
(?x2 ?y2 (? type2 ONT::PERSON)))
(?x3 ?y3 (? type3 ONT::TREATMENT ONT::MEDICAL-DIAGNOSTIC) :DOMAIN-INFO ?!info)
List of LF patterns
-extract-person-has-treatment-or-medical-diagnostic>
(EVENT :type ?type1 :class occurrence :subject ?y2 :object ?y3)
Unique rule ID
Output Specification
Figure 4: An example extraction rule
149
tory and extract  them using extraction rules as de-
scribed above. Figure 5 shows a rule to extract 
substances by matching any LF with a substance 
concept (as mentioned already, subclasses such as 
pharmacologic substances, would also match).
The rule in Figure 5 checks the :quantifier role 
and its value (e.g., none) is used to infer the pres-
ence or the absence of concepts. Using similar 
rules, we extract  additional concepts such as 
medical-disorders-and-conditions, physical-
symptom, treatment, medical-diagnostic, medical-
action and clinical-finding. Here, medical-action 
and clinical-finding are to extract concepts in a 
broader sense.
2
 To cover additional concepts, we 
can straightforwardly update extraction rules.
4.3 Temporal Expression Extraction
Temporal expressions are also extracted in the 
same way but using different  LF patterns. We have 
14 rules to extract  dates and time-spans of varying 
levels of complexity; for the example in Figure 1 
six of these rules were applied. Figure 6 shows LF 
patterns for a rule to extract temporal expressions 
of the form ?until X days/months/years ago?; for 
example, here is what the rule extracts for ?until 3 
days ago?:
(extraction :type time-span :context-rel (:* 
ont::event-time-rel w::until) :reference (time-position 
:context-rel (:* ont::event-time-rel w::ago) :amount 3 
:unit (:* ont::time-unit ont::day))) 
From this type of output, other reasoners can 
easily access necessary information about given 
temporal expressions without investigating the 
whole LF representation on their own.
4.4 Event Extraction
To construct timelines, the concepts of interest 
(Section 4.2) and the temporal expressions (Sec-
tion 4.3) should be pieced together. For that pur-
pose, it  is critical to extract events because they not 
only describe situations that  happen or occur but 
also represent states or circumstances where some-
thing holds. Furthermore, event features provide 
useful cues to reason about  situations surrounding 
extracted clinical concepts.
Here, we do not formally define events, but  refer 
to (Sauri et  al., 2006) for detailed discussion about 
events. While events can be expressed by multiple 
means (e.g., verbs, nominalizations, and adjec-
tives), our extraction rules for events focus on 
verbs and their features such as class, tense, aspect, 
and polarity. Figure 7 shows a rule to extract  an 
event  with the verb ?start? like the one in Line 4, 
?She was started on Afinitor on 03/05/08?. The 
output specification from this rule for Line 4 will 
have the :class, :tense, and :passive roles as (aspec-
tual initiation), past, and true respectively.
These event  features play a critical role in con-
structing timelines (Section 5). For instance, the 
event  class (aspectual initiation) from applying the 
rule in Figure 7 to Line 4 implies that  the concept 
?Afinitor? (a pharmacologic-substance) is not  just 
something tried on the given date, 03/05/08,  but 
something that continued from that date.
4.5 Relation Information Extraction
The relations among extracted concepts (namely, 
conjoined relations between events and set rela-
tions between clinical concepts) also play a key 
role in our approach. When events or clinical con-
cepts are closely linked with such relations, heuris-
tically, they tend to share similar properties that are 
exploited in constructing timelines as described in 
Section 5.
5 Building Timelines from Extracted Results
Extracted clinical concepts, temporal expressions, 
events, and relations (Section 4) are used as a 
2
 While concept classification into certain categories is a very important task in the medical domain, sophisticated 
concept categorization like the one specified in the 2010 i2b2/VA Challenge (https://www.i2b2.org/NLP/Relations/) 
is not the primary goal of this paper. We rather focus on how to associate extracted concepts with other events and 
temporal expressions to build timelines.
(?x1 ?y1 (:* ont::event-time-rel w::until) :val ?val) 
(?x2 ?val (? type2 ont::time-loc) :mod ?mod) 
(?x3 ?mod (? type3 ont::event-time-rel) :displacement 
?displacement) 
(?x4 ?displacement (? type4 ont::quantity) :unit ?unit 
:amount ?amount) 
(?x5 ?amount ont::number :value ?num)
Figure 6: LF patterns to extract a time-span
((?x1 ?y1 (? type1 ONT::SUBSTANCE) :domain-info 
?info :quantifier ?quan)
-extract-substance>
(extraction :type substance :concept ?type1 :umlsinfo 
?info :ont-term ?y1 :quantifier ?quan))
Figure 5: A rule to extract substances
150
foundation to construct timelines that  represent 
patients? medical history. In this section, we pre-
sent  timeline construction processes (as shown in 
Figure 8), using example sentences from Section 2.
Step 1: We first  make connections between events 
and clinical concepts. In the current system, events 
and clinical concepts are extracted in separate rules 
and their relations are not always explicit  in the 
output specification of the rules applied. For in-
stance, Figure 9 shows LFs for the sentence in Line 
7 in a graph format, using simplified LF terms for 
illustration. The clinical concept ?prednisone? and 
the event  ?received? get  extracted by different 
rules and the relation between them is not explicit 
in their output specifications.
To address such a case, for a pair of an event 
and a clinical concept, we traverse LF graphs and 
decide that a relation between them exists if there 
is a path that  goes through certain pre-defined con-
cepts that  do not separate them semantically and 
syntactically (e.g., concepts of measure-units, 
evidence/history, development, and some proposi-
tions).
Step 2: Second, we find temporal expressions as-
sociated with events. This step is relatively 
straightforward. While temporal expressions and 
events get  extracted separately, by investigating 
their LFs, we can decide if a given temporal ex-
pression is a modifier of an event. In Figure 9, the 
time-span-relation (i.e., ?in?) in the dotted-line box 
is a direct modifier of the event ?was treated?.
Step 3: Next, we propagate the association be-
tween events and temporal expressions. That is, 
when the relation between an event and a temporal 
expression is found, we check if the temporal ex-
pression can be associated with additional events 
related to the event  (esp. when the related events 
do not have any associated temporal expression). 
In Figure 9, the event  ?received? does not have a 
temporal expression as a modifier. However, it  is 
conjoined with the event  ?was treated? in the same 
past  tense under the same speech act. Thus, we let 
the event  ?received? share the same temporal ex-
pression with its conjoined event. Here, the con-
joined relation was extracted with relation rules 
described in Section 4.5, which allows us to focus 
on only related events.
Step 4: When temporal expressions do not have 
concrete time values within the expressions, we 
need to designate times for them by looking into 
information in their LFs:
?
Event references: The system needs to find the 
referred event  and gets its time value. For in-
stance, in ?4 weeks after starting Afinitor? (Line 
6),  ?starting Afinitor? refers to a previous event 
in Line 4. The system investigates all events with 
a verb with the same- or sub-type of ont::start 
and Afinitor as its object  (active verbs) or its 
subject (passive verbs). After resolving event 
references, additional time reference or relation 
computation may be required (e.g., computation 
for ?4 weeks after?).
?
Time references: Concrete times for expressions 
like the above example ?N weeks after 
<reference-time>? can be easily computed by 
checking the time displacement  information in 
LFs with the reference time. However, expres-
sions such as ?N days ago?  are based on the 
context of clinical records (e.g., record creation 
(?x1 ?ev (? type1 ont::start) :affected ?affected :tense ?tense :passive ?passive :progressive ?progresive 
  :perfective ?perfective :negation ?negation)
-extract-start-event>
(EVENT :type ?type1 :class (aspectual initiation) :subject ?affected :object null :tense ?tense :passive
   ?passive :progressive ?progresive :perfective ?perfective :negation ?negation :ont-term ?ev)
Figure 7: An event extraction rule example
Inputs: Clinical concepts, Temporal 
Expressions, Events, Relations, LFs
Outputs: Clinical concepts with associated dates 
or timespans.
Steps: 
1. Build links between events and clinical 
concepts
2. Find associated temporal expressions for 
events
3. Propagate temporal expressions through 
relations between events when applicable
4. Compute concrete time values for temporal 
expressions, taking into account the context of 
clinical records
5. Compute time values for clinical concepts 
based on their associated events
Figure 8: Pseudocode for Timeline Construction
151
time). Document creation time is usually repre-
sented as metadata attached to the document  it-
self, or it could be retrieved from a database 
where clinical records are stored. In addition, 
previously mentioned dates or time-spans can be 
referred to using pronouns (e.g., ?at  that/this 
time?). For such expressions, we heuristically 
decide that it refers to the most  recent  temporal 
expression.
?
Time relation: Some temporal expressions have 
directional time relations (e.g., ?until?, ?prior 
to?, and ?after?) specifying intervals with open 
ends. When the ending time of a time span is not 
specified (e.g., ?since 10/2007? in Line 10). We 
heuristically set it from the context of the clinical 
record such as the document creation time.
Step 5: Finally, we designate or compute times on 
or during which the presence or the absence of 
each clinical concept is asserted. Since temporal 
expressions are associated with events, to find time 
values for clinical concepts, we first check the rela-
tions between events and clinical concepts. When 
an event with a concrete time is found for a clinical 
concept, the event?s class is examined. For classes 
such as state and occurrence, the concrete time 
value of the event  is used. In contrast, for an aspec-
tual event, we check its feature (e.g., initiation or 
termination) and look for other aspectual events 
related to the clinical concept and compute a time 
span. For instance, regarding ?Afinitor?, Line 4 
and Line 9 have events with classes (aspectual ini-
tiation) and (aspectual termination) respectively, 
which leads to a time span between the two dates 
in Line 4 and Line 9. Currently, we do not resolve 
conflicting hypotheses. 
Assertion of Presence  or Absence  of Clinical 
Concepts: To check if a certain concept is present 
or not, we take into account quantifier information 
(e.g., none), the negation role values of events, and 
the verb types of events (e.g., ?deny? indicates the 
absence assertion). In addition to such information 
readily available in the output  specifications of the 
clinical concept- and event-extraction rules, we 
also check the path (as in Step 1) that relates the 
clinical concepts and the events, and the quantifiers 
of the concepts in the path are used to compute 
negation values. For instance, given ?The scan 
shows no evidence of lung nodules?, the quantifier 
of the concept  ?evidence? indicates the absence of 
the clinical finding ?lung nodules?.
6 Timeline Results and Discussion 
For the example in Section 2 (Line 1 ~ 12), we ex-
tract all the instances of the clinical concepts and 
the temporal expressions. Out of 23 events, 17 
were extracted. While we missed events such as 
state/was (Line 5), done (Line 8), and walk/do/
swim  (Line 10), our event  extraction rules can be 
extended to cover them if need be.
Figure 10 visualizes the extraction results of the 
example. We use a web widget tool called Simile 
Timeline (www.simile-widgets.org/timeline/). 
Some property values (that  were also extracted by 
rules) are shown alongside some concepts (e.g., 
weight  measurement). Note that not  all extracted 
clinical findings are displayed in Figure 10 because 
we visualize clinical concepts only when they are 
associated with temporal expressions in our LFs. 
For instance, the CT-scan on 05/14/08 in Line 8 is 
not shown because the date was not  associated 
with it due to fragmented LFs from the Parser. 
Figure 9: Graph format LFs of the sentence in Line 7 -- ?She received a 5 day dose pack of 
                  prednisone and was treated with Augmentin in 05/2008.?
152
However, we were still able to extract ?no infil-
trates? and ?scan? from a meaningful fragment.
In addition to the fragmented LF issue, we plan 
to work on temporal reasoning for concepts in the 
sentences without explicit temporal expressions, 
and the current limited event reference resolution 
will be improved. We are also working on evalua-
tion with 48 clinical records from 10 patients. An-
notated results will be created as a gold-standard 
and precision/recall will be measured.
7 Related Work
Temporal information is of crucial importance in 
clinical applications, which is why it  has attracted 
a lot  interest over the last  two decades or more 
(Augusto, 2005). Since so much clinical informa-
tion is still residing in unstructured form, in par-
ticular as text  in the patient?s health record, the last 
decade has seen a number of serious efforts in 
medical NLP  in general (Meystre et  al., 2008) and 
in extracting temporal information from clinical 
text in particular. 
Some of this surge in interest  has been spurred 
by dedicated competitions on extraction of con-
cepts and events from clinical text (such as the 
i2b2 NLP challenges). At the same time, the evolu-
tion of temporal markup languages such as Ti-
meML (Sauri et al, 2006), and temporal 
extraction/inference competitions (such as the two 
TempEval challenges,  Verhagen et  al., 2009) in the 
general area of NLP have led to the development 
of tools such as TARSQI (Verhagen et  al., 2005) 
that could be adapted to the clinical domain.
Although the prevailing paradigm in this area is 
to use superficial methods for extracting and clas-
sifying temporal expressions, it has long been rec-
ognized that higher level semantic processing, in-
cluding discourse-level analysis, would have to be 
performed to get  past the limits of the current  ap-
proaches (cf. Zhou and Hripcsak, 2007). 
Recent attempts to use deeper linguistic features 
include the work of  Bethard et  al. (2007), who 
used syntactic structure in addition to lexical and 
some minor semantic features to classify temporal 
relations of the type we discussed in Section 4.3. 
Savova and her team have also expressed interest 
in testing off-the-shelf deep parsers and semantic 
role labelers for aiding in temporal relation identi-
fication and classification (Savova et al, 2009); 
although we are not  aware of any temporal extrac-
tion results yet, we appreciate their effort in ex-
panding the TimeML annotation schema for the 
clinical domain, as well as their efforts in develop-
ing corpora of clinical text  annotated with temporal 
information.
The work of Mulkar-Mehta et  al. (2009) also 
deserves a mention, even though they apply their 
techniques to biomedical text rather than clinical 
text. They obtain a shallow logical form that repre-
sents predicate-argument relations implicit  in the 
syntax by post-processing the results of a statistical 
parser. Temporal relations are obtained from the 
shallow LF based on a set  of hand-built rules by an 
abductive inference engine.
To our knowledge, however, our system is the 
first  general-purpose NLU system that produces a 
full, deep syntactic and semantic analysis of the 
text as a prerequisite to the extraction and analysis 
of relevant clinical and temporal information.
8 Conclusion
In this paper, we presented a prototype deep natu-
ral language understanding system to construct 
timelines for the medical histories of patients. Our 
approach is generic and extensible to cover a vari-
ety of narrative clinical text  records. The results 
from our system are promising and they can be 
used to support medical decision making.
9 Acknowledgement
This work was supported by the National Cancer 
Institute and the H. Lee Moffitt  Cancer Center and 
Research Institute (Award # RC2CA1488332).
Figure 10: Visualization of timeline results
153
References 
James Allen, Donna Byron, Myroslava Dzikovska, 
George Ferguson, Lucian  Galescu, and Amanda 
Stent.  2000. An architecture for a generic dialogue 
shell. Journal of Natural Language Engineering 
6(3):1?16.
Mary Swift, Nate Blaylock, James Allen, Will de 
Beaumont, Lucian Galescu, and Hyuckchul Jung. 
2010. Augmenting a Deep Natural Language Proc-
essing System with UMLS. Proceedings of the 
Fourth International Symposium on Semantic Mining 
in Biomedicine (poster abstract)
Alan R. Aronson and Fran?ois-Michel Lang.  2010. An 
overview of MetaMap: historical perspective and 
recent advances.  Journal of the American Medical 
Informatics Association. 17:229-236.
Juan C. Augusto. 2005.  Temporal reasoning for decision 
support in medicine. Artificial Intelligence in Medi-
cine, 33(1): 1-24.
Steven Bethard, James H.  Martin, and Sara Klingen-
stein. 2007. Timelines from Text: Identification of 
Syntactic Temporal Relations. In Proceedings of the 
International Conference on Semantic Computing 
(ICSC '07), 11-18.
Olivier Bodenreider. 2004. The Unified Medical Lan-
guage System (UMLS): integrating biomedical ter-
minology. Nucleic Acids Research, Vol. 32.
Jenny Rose Finkel, Trond Grenager, and Christopher 
Manning. 2005. Incorporating Non-local Information 
into Information Extraction Systems by Gibbs Sam-
pling.  Proceedings of the Annual Meeting of the As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning.  2003. Fast Ex-
act Inference with a Factored Model for Natural Lan-
guage Parsing. In Advances in Neural Information 
Processing Systems 15 (NIPS 2002), Cambridge, 
MA: MIT Press.
S.  M. Meystre, G. K. Savova, K. C. Kipper-Schuler, J. 
F. Hurdle. 2008. Extracting information from textual 
documents in the electronic health record: a review 
of recent research. IMIA Yearbook of Medical Infor-
matics.
George A. Miller. 1995. WordNet: A lexical database for 
English. Communications of the ACM, 38(5).
R. Mulkar-Mehta, J.R. Hobbs, C.-C. Liu, and X.J. Zhou. 
2009. Discovering causal and temporal relations in 
biomedical texts. In AAAI Spring Symposium, 74-
80.
Roser Sauri, Jessica Littman, Bob Knippen, Robert Gai-
zauskas, Andrea Setzer, and James Pustejovsky. 
2006. TimeML annotation guidelines. (available at 
http://www.timeml.org/site/publications/time 
MLdocs/annguide_1.2.1.pdf)
G. Savova, S. Bethard, W. Styler, J. Martin, M. Palmer, 
J. Masanz, and W. Ward. 2009. Towards temporal 
relation discovery from the clinical narrative. Pro-
ceedings of the Annual AMIA Symposium, 568-572.
Michael Stacey and Carolyn McGregor. 2007.  Temporal 
abstraction in intelligent clinical data analysis: A sur-
vey. Artificial Intelligence in Medicine, 39.
Kristina Toutanova and Christopher D. Manning. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings 
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large 
Corpora (EMNLP/VLC-2000).
M. Verhagen, I. Mani, R. Sauri, R. Knippen, S.B. Jang, 
J. Littman, A. Rumshisky, J. Phillips, and 
J. Pustejovsky. 2005. Automating temporal annota-
tion with TARSQI. In Proceedings of the ACL 2005 
on Interactive poster and demonstration sessions 
(ACLdemo '05), 81-84. 
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, 
J. Moszkowicz, and J. Pustejovsky. 2009. The Tem-
pEval challenge: identifying temporal relations in 
text . Language Resources and Evaluation 
 43(2):161-179.
Li Zhou, Carol Friedman, Simon Parsons and George 
Hripcsak. 2005. System Architecture for Temporal 
Information Extraction,  Representation and Reason-
ing in Clinical Narrative Reports. Proceedings of the 
Annual AMIA Symposium.
Li Zhou and George Hripcsak. 2007. Temporal reason-
ing with medical data - A review with emphasis on 
medical natural language processing. Journal of 
Biomedical Informatics, 40.
154
