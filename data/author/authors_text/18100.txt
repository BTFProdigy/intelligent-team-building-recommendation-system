Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 912?920,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Ranking-based Approach to Word Reordering
for Statistical Machine Translation?
Nan Yang?, Mu Li?, Dongdong Zhang?, and Nenghai Yu?
?MOE-MS Key Lab of MCC
University of Science and Technology of China
v-nayang@microsoft.com, ynh@ustc.edu.cn
?Microsoft Research Asia
{muli,dozhang}@microsoft.com
Abstract
Long distance word reordering is a major
challenge in statistical machine translation re-
search. Previous work has shown using source
syntactic trees is an effective way to tackle
this problem between two languages with sub-
stantial word order difference. In this work,
we further extend this line of exploration and
propose a novel but simple approach, which
utilizes a ranking model based on word or-
der precedence in the target language to repo-
sition nodes in the syntactic parse tree of a
source sentence. The ranking model is auto-
matically derived from word aligned parallel
data with a syntactic parser for source lan-
guage based on both lexical and syntactical
features. We evaluated our approach on large-
scale Japanese-English and English-Japanese
machine translation tasks, and show that it can
significantly outperform the baseline phrase-
based SMT system.
1 Introduction
Modeling word reordering between source and tar-
get sentences has been a research focus since the
emerging of statistical machine translation. In
phrase-based models (Och, 2002; Koehn et al,
2003), phrase is introduced to serve as the funda-
mental translation element and deal with local re-
ordering, while a distance based distortion model is
used to coarsely depict the exponentially decayed
word movement probabilities in language transla-
tion. Further work in this direction employed lexi-
?This work has been done while the first author was visiting
Microsoft Research Asia.
calized distortion models, including both generative
(Koehn et al, 2005) and discriminative (Zens and
Ney, 2006; Xiong et al, 2006) variants, to achieve
finer-grained estimations, while other work took into
account the hierarchical language structures in trans-
lation (Chiang, 2005; Galley and Manning, 2008).
Long-distance word reordering between language
pairs with substantial word order difference, such as
Japanese with Subject-Object-Verb (SOV) structure
and English with Subject-Verb-Object (SVO) struc-
ture, is generally viewed beyond the scope of the
phrase-based systems discussed above, because of
either distortion limits or lack of discriminative fea-
tures for modeling. The most notable solution to this
problem is adopting syntax-based SMT models, es-
pecially methods making use of source side syntac-
tic parse trees. There are two major categories in this
line of research. One is tree-to-string model (Quirk
et al, 2005; Liu et al, 2006) which directly uses
source parse trees to derive a large set of translation
rules and associated model parameters. The other
is called syntax pre-reordering ? an approach that
re-positions source words to approximate target lan-
guage word order as much as possible based on the
features from source syntactic parse trees. This is
usually done in a preprocessing step, and then fol-
lowed by a standard phrase-based SMT system that
takes the re-ordered source sentence as input to fin-
ish the translation.
In this paper, we continue this line of work and
address the problem of word reordering based on
source syntactic parse trees for SMT. Similar to most
previous work, our approach tries to rearrange the
source tree nodes sharing a common parent to mimic
912
the word order in target language. To this end, we
propose a simple but effective ranking-based ap-
proach to word reordering. The ranking model is
automatically derived from the word aligned parallel
data, viewing the source tree nodes to be reordered
as list items to be ranked. The ranks of tree nodes are
determined by their relative positions in the target
language ? the node in the most front gets the high-
est rank, while the ending word in the target sentence
gets the lowest rank. The ranking model is trained
to directly minimize the mis-ordering of tree nodes,
which differs from the prior work based on maxi-
mum likelihood estimations of reordering patterns
(Li et al, 2007; Genzel, 2010), and does not require
any special tweaking in model training. The ranking
model can not only be used in a pre-reordering based
SMT system, but also be integrated into a phrase-
based decoder serving as additional distortion fea-
tures.
We evaluated our approach on large-scale
Japanese-English and English-Japanese machine
translation tasks, and experimental results show that
our approach can bring significant improvements to
the baseline phrase-based SMT system in both pre-
ordering and integrated decoding settings.
In the rest of the paper, we will first formally
present our ranking-based word reordering model,
then followed by detailed steps of modeling train-
ing and integration into a phrase-based SMT system.
Experimental results are shown in Section 5. Section
6 consists of more discussions on related work, and
Section 7 concludes the paper.
2 Word Reordering as Syntax Tree Node
Ranking
Given a source side parse tree Te, the task of word
reordering is to transform Te to T ?e, so that e
? can
match the word order in target language as much as
possible. In this work, we only focus on reordering
that can be obtained by permuting children of every
tree nodes in Te. We use children to denote direct de-
scendants of tree nodes for constituent trees; while
for dependency trees, children of a node include not
only all direct dependents, but also the head word
itself. Figure 1 gives a simple example showing the
word reordering between English and Japanese. By
rearranging the position of tree nodes in the English
I am trying to play music
?? ??? ?? ???? ????
PRP VBP VBG TO VB NN
NP
VP
VP
NP
S
VP
VP
S
I amtryingtoplaymusic
PRP VBPVBGTOVBNN
NP
VP
VP
NP
S
VP
VP
?? ??? ?? ???? ????
Original 
Tree
Reordered Tree
S
j0 j1 j2 j3 j4
e0 e1 e2 e3 e4 e5
j0 j1 j2 j3 j4
e0 e1 e2 e3 e4 e5
Figure 1: An English-to-Japanese sentence pair. By
permuting tree nodes in the parse tree, the source
sentence is reordered into the target language or-
der. Constituent tree is shown above the source
sentence; arrows below the source sentences show
head-dependent arcs for dependency tree; word
alignment links are lines without arrow between the
source and target sentences.
parse tree, we can obtain the same word order of
Japanese translation. It is true that tree-based re-
ordering cannot cover all word movement operations
in language translation, previous work showed that
this method is still very effective in practice (Xu et
al., 2009, Visweswariah et al, 2010).
Following this principle, the word reordering task
can be broken into sub-tasks, in which we only
need to determine the order of children nodes for
all non-leaf nodes in the source parse tree. For a
tree node t with children {c1, c2, . . . , cn}, we re-
arrange the children to target-language-like order
{cpi(i1), cpi(i2), . . . , cpi(in)}. If we treat the reordered
position pi(i) of child ci as its ?rank?, the reorder-
913
ing problem is naturally translated into a ranking
problem: to reorder, we determine a ?rank? for each
child, then the children are sorted according to their
?ranks?. As it is often impractical to directly assign
a score for each permutation due to huge number of
possible permutations, a widely used method is to
use a real valued function f to assign a value to each
node, which is called a ranking function (Herbrich
et al, 2000). If we can guarantee (f(i)? f(j)) and
(pi(i) ? pi(j)) always has the same sign, we can get
the same permutation as pi because values of f are
only used to sort the children. For example, con-
sider the node rooted at trying in the dependency
tree in Figure 1. Four children form a list {I, am, try-
ing, play} to be ranked. Assuming ranking function
f can assign values {0.94, ?1.83, ?1.50, ?1.20}
for {I, am, trying, play} respectively, we can get a
sorted list {I, play, trying, am}, which is the desired
permutation according to the target.
More formally, for a tree node t with children
{c1, c2, . . . , cn}, our ranking model assigns a rank
f(ci, t) for each child ci, then the children are sorted
according to the rank in a descending order. The
ranking function f has the following form:
f(ci, t) =
?
j
?j(ci, t) ? wj (1)
where the ?j is a feature representing the tree node t
and its child ci, and wj is the corresponding feature
weight.
3 Ranking Model Training
To learn ranking function in Equation (1), we need to
determine the feature set ? and learn weight vector
w from reorder examples. In this section, we first
describe how to extract reordering examples from
parallel corpus; then we show our features for rank-
ing function; finally, we discuss how to train the
model from the extracted examples.
3.1 Reorder Example Acquisition
For a sentence pair (e, f, a) with syntax tree Te on
the source side, we need to determine which re-
ordered tree T ?e? best represents the word order in
target sentence f . For a tree node t in Te, if its chil-
dren align to disjoint target spans, we can simply ar-
range them in the order of their corresponding target
Prob lem w ith latter procedure
??
lies
? ?? ??? ? ?
in ?
? ??
Prob lem w ith latter procedure
??
lies
? ?? ??? ? ?
in ?
? ??
( a)  gold alignment
( b )  auto alignment
Figure 2: Fragment of a sentence pair. (a) shows
gold alignment; (b) shows automatically generated
alignment which contains errors.
spans. Figure 2 shows a fragment of one sentence
pair in our training data. Consider the subtree rooted
at word ?Problem?. With the gold alignment, ?Prob-
lem? is aligned to the 5th target word, and ?with
latter procedure? are aligned to target span [1, 3],
thus we can simply put ?Problem? after ?with latter
procedure?. Recursively applying this process down
the subtree, we get ?latter procedure with Problem?
which perfectly matches the target language.
As pointed out by (Li et al, 2007), in practice,
nodes often have overlapping target spans due to er-
roneous word alignment or different syntactic struc-
tures between source and target sentences. (b) in
Figure 2 shows the automatically generated align-
ment for the sentence pair fragment. The word
?with? is incorrectly aligned to the 6th Japanese
word ?ha?; as a result, ?with latter procedure? now
has target span [1, 6], while ?Problem? aligns to
[5, 5]. Due to this overlapping, it becomes unclear
which permutation of ?Problem? and ?with latter
procedure? is a better match of the target phrase; we
need a better metric to measure word order similar-
ity between reordered source and target sentences.
We choose to find the tree T ?e? with minimal align-
ment crossing-link number (CLN) (Genzel, 2010)
to f as our golden reordered tree.1 Each crossing-
1A simple solution is to exclude all trees with overlapping
target spans from training. But in our experiment, this method
914
link (i1j1, i2j2) is a pair of alignment links crossing
each other. CLN reaches zero if f is monotonically
aligned to e?, and increases as there are more word
reordering between e? and f . For example, in Fig-
ure 1, there are 6 crossing-links in the original tree:
(e1j4, e2j3), (e1j4, e4j2), (e1j4, e5j1), (e2j3, e4j2),
(e2j3, e5j1) and (e4j2, e5j1); thus CLN for the origi-
nal tree is 6. CLN for the reordered tree is 0 as there
are no crossing-links. This metric is easy to com-
pute, and is not affected by unaligned words (Gen-
zel, 2010).
We need to find the reordered tree with minimal
CLN among all reorder candidates. As the number
of candidates is in the magnitude exponential with
respect to the degree of tree Te 2, it is not always
computationally feasible to enumerate through all
candidates. Our solution is as follows.
First, we give two definitions.
? CLN(t): the number of crossing-links
(i1j1, i2j2) whose source words e?i1 and e
?
i2
both fall under sub span of the tree node t.
? CCLN(t): the number of crossing-links
(i1j1, i2j2) whose source words e?i1 and e
?
i2 fall
under sub span of t?s two different children
nodes c1 and c2 respectively.
Apparently CLN of a tree T ? equals to
CLN(root of T ?), and CLN(t) can be recur-
sively expressed as:
CLN(t) = CCLN(t) +
?
child c of t
CLN(c)
Take the original tree in Figure 1 for example. At the
root node trying, CLN(trying) is 6 because there are
six crossing-links under its sub-span: (e1j4, e2j3),
(e1j4, e4j2), (e1j4, e5j1), (e2j3, e4j2), (e2j3, e5j1)
and (e4j2, e5j1). On the other hand, CCLN(trying)
is 5 because (e4j2, e5j1) falls under its child node
play, thus does not count towards CCLN of trying.
From the definition, we can easily see that
CCLN(t) can be determined solely by the order of
t?s direct children, and CLN(t) is only affected by
discarded too many training instances and led to degraded re-
ordering performance.
2In our experiments, there are nodes with more than 10 chil-
dren for English dependency trees.
the reorder in the subtree of t. This observation en-
ables us to divide the task of finding the reordered
tree T ?e? with minimal CLN into independently find-
ing the children permutation of each node with min-
imal CCLN. Unfortunately, the time cost for the sub-
task is stillO(n!) for a node with n children. Instead
of enumerating through all permutations, we only
search the Inversion Transduction Grammar neigh-
borhood of the initial sequence (Tromble, 2009). As
pointed out by (Tromble, 2009), the ITG neighbor-
hood is large enough for reordering task, and can be
searched through efficiently using a CKY decoder.
After finding the best reordered tree T ?e? , we can
extract one reorder example from every node with
more than one child.
3.2 Features
Features for the ranking model are extracted from
source syntax trees. For English-to-Japanese task,
we extract features from Stanford English Depen-
dency Tree (Marneffe et al, 2006), including lexi-
cons, Part-of-Speech tags, dependency labels, punc-
tuations and tree distance between head and depen-
dent. For Japanese-to-English task, we use a chunk-
based Japanese dependency tree (Kudo and Mat-
sumoto, 2002). Different from features for English,
we do not use dependency labels because they are
not available from the Japanese parser. Additionally,
Japanese function words are also included as fea-
tures because they are important grammatical clues.
The detailed feature templates are shown in Table 1.
3.3 Learning Method
There are many well studied methods available to
learn the ranking function from extracted examples.,
ListNet (?) etc. We choose to use RankingSVM
(Herbrich et al, 2000), a pair-wised ranking method,
for its simplicity and good performance.
For every reorder example t with children
{c1, c2, . . . , cn} and their desired permutation
{cpi(i1), cpi(i2), . . . , cpi(in)}, we decompose it into a
set of pair-wised training instances. For any two
children nodes ci and cj with i < j , we extract a
positive instance if pi(i) < pi(j), otherwise we ex-
tract a negative instance. The feature vector for both
positive instance and negative instance is (?ci??cj ),
where ?ci and ?cj are feature vectors for ci and cj
915
E-J
cl cl ? dst cl ? pct
cl ? dst ? pct cl ? lcl cl ? rcl
cl ? lcl ? dst cl ? rcl ? dst cl ? clex
cl ? clex cl ? clex ? dst cl ? clex ? dst
cl ? hlex cl ? hlex cl ? hlex ? dst
cl ? hlex ? dst cl ? clex ? pct cl ? clex ? pct
cl ? hlex ? pct cl ? hlex ? pct
J-E
ctf ctf ? dst ctf ? lct
ctf ? rct ctf ? lct ? dst cl ? rct ? dst
ctf ? clex ctf ? clex ctf ? clex ? dst
ctf ? clex ? dst ctf ? hf ctf ? hf
ctf ? hf ? dst ctf ? hf ? dst ctf ? hlex
ctf ? hlex ctf ? hlex ? dst ctf ? hlex ? dst
Table 1: Feature templates for ranking function. All
templates are implicitly conjuncted with the pos tag
of head node.
c: child to be ranked; h: head node
lc: left sibling of c; rc: right sibling of c
l: dependency label; t: pos tag
lex: top frequency lexicons
f : Japanese function word
dst: tree distance between c and h
pct: punctuation node between c and h
respectively. In this way, ranking function learning
is turned into a simple binary classification problem,
which can be easily solved by a two-class linear sup-
port vector machine.
4 Integration into SMT system
There are two ways to integrate the ranking reorder-
ing model into a phrase-based SMT system: the pre-
reorder method, and the decoding time constraint
method.
For pre-reorder method, ranking reorder model
is applied to reorder source sentences during both
training and decoding. Reordered sentences can go
through the normal pipeline of a phrase-based de-
coder.
The ranking reorder model can also be integrated
into a phrase based decoder. Integrated method takes
the original source sentence e as input, and ranking
model generates a reordered e? as a word order ref-
erence for the decoder. A simple penalty scheme
is utilized to penalize decoder reordering violating
ranking reorder model?s prediction e?. In this paper,
our underlying decoder is a CKY decoder follow-
ing Bracketing Transduction Grammar (Wu, 1997;
Xiong et al, 2006), thus we show how the penalty
is implemented in the BTG decoder as an example.
Similar penalty can be designed for other decoders
without much effort.
Under BTG, three rules are used to derive transla-
tions: one unary terminal rule, one straight rule and
one inverse rule:
A ? e/f
A ? [A1, A2]
A ? ?A1, A2?
We have three penalty triggers when any rules are
applied during decoding:
? Discontinuous penalty fdc: it fires for all rules
when source span of either A, A1 or A2 is
mapped to discontinuous span in e?.
? Wrong straight rule penalty fst: it fires for
straight rule when source spans of A1 and A2
are not mapped to two adjacent spans in e? in
straight order.
? Wrong inverse rule penalty fiv: it fires for in-
verse rule when source spans of A1 and A2 are
not mapped to two adjacent spans in e? in in-
verse order.
The above three penalties are added as additional
features into the log-linear model of the phrase-
based system. Essentially they are soft constraints
to encourage the decoder to choose translations with
word order similar to the prediction of ranking re-
order model.
5 Experiments
To test our ranking reorder model, we carry out ex-
periments on large scale English-To-Japanese, and
Japanese-To-English translation tasks.
5.1 Data
5.1.1 Evaluation Data
We collect 3,500 Japanese sentences and 3,500
English sentences from the web. They come from
916
a wide range of domains, such as technical docu-
ments, web forum data, travel logs etc. They are
manually translated into the other language to pro-
duce 7,000 sentence pairs, which are split into two
parts: 2,000 pairs as development set (dev) and the
other 5,000 pairs as test set (web test).
Beside that, we collect another 999 English sen-
tences from newswire domain which are translated
into Japanese to form an out-of-domain test data set
(news test).
5.1.2 Parallel Corpus
Our parallel corpus is crawled from the web,
containing news articles, technical documents, blog
entries etc. After removing duplicates, we have
about 18 million sentence pairs, which contain about
270 millions of English tokens and 320 millions of
Japanese tokens. We use Giza++ (Och and Ney,
2003) to generate the word alignment for the parallel
corpus.
5.1.3 Monolingual Corpus
Our monolingual Corpus is also crawled from the
web. After removing duplicate sentences, we have a
corpus of over 10 billion tokens for both English and
Japanese. This monolingual corpus is used to train
a 4-gram language model for English and Japanese
respectively.
5.2 Parsers
For English, we train a dependency parser as (Nivre
and Scholz, 2004) on WSJ portion of Penn Tree-
bank, which are converted to dependency trees us-
ing Stanford Parser (Marneffe et al, 2006). We con-
vert the tokens in training data to lower case, and
re-tokenize the sentences using the same tokenizer
from our MT system.
For Japanese parser, we use CABOCHA, a
chunk-based dependency parser (Kudo and Mat-
sumoto, 2002). Some heuristics are used to adapt
CABOCHA generated trees to our word segmenta-
tion.
5.3 Settings
5.3.1 Baseline System
We use a BTG phrase-based system with a Max-
Ent based lexicalized reordering model (Wu, 1997;
Xiong et al, 2006) as our baseline system for
both English-to-Japanese and Japanese-to-English
Experiment. The distortion model is trained on the
same parallel corpus as the phrase table using a
home implemented maximum entropy trainer.
In addition, a pre-reorder system using manual
rules as (Xu et al, 2009) is included for the English-
to-Japanese experiment (ManR-PR). Manual rules
are tuned by a bilingual speaker on the development
set.
5.3.2 Ranking Reordering System
Ranking reordering model is learned from the
same parallel corpus as phrase table. For efficiency
reason, we only use 25% of the corpus to train our
reordering model. LIBLINEAR (Fan et al, 2008) is
used to do the SVM optimization for RankingSVM.
We test it on both pre-reorder setting (Rank-PR)
and integrated setting (Rank-IT).
5.4 End-to-End Result
system dev web test news test
E-J
Baseline 21.45 21.12 14.18
ManR-PR 23.00 22.42 15.61
Rank-PR 22.92 22.51 15.90
Rank-IT 23.14 22.85 15.72
J-E
Baseline 25.39 24.20 14.26
Rank-PR 26.57 25.56 15.42
Rank-IT 26.72 25.87 15.27
Table 2: BLEU(%) score on dev and test data for
both E-J and J-E experiment. All settings signifi-
cantly improve over the baseline at 95% confidence
level. Baseline is the BTG phrase system system;
ManR-PR is pre-reorder with manual rule; Rank-PR
is pre-reorder with ranking reorder model; Rank-IT
is system with integrated ranking reorder model.
From Table 2, we can see our ranking reordering
model significantly improves the performance for
both English-to-Japanese and Japanese-to-English
experiments over the BTG baseline system. It also
out-performs the manual rule set on English-to-
Japanese result, but the difference is not significant.
5.5 Reordering Performance
In order to show whether the improved performance
is really due to improved reordering, we would like
to measure the reorder performance directly.
917
As we do not have access to a golden re-
ordered sentence set, we decide to use the align-
ment crossing-link numbers between aligned sen-
tence pairs as the measure for reorder performance.
We train the ranking model on 25% of our par-
allel corpus, and use the rest 75% as test data
(auto). We sample a small corpus (575 sentence
pairs) and do manual alignment (man-small). We
denote the automatic alignment for these 575 sen-
tences as (auto-small). From Table 3, we can see
setting auto auto-small man-small
None 36.3 35.9 40.1
E-J
Oracle 4.3 4.1 7.4
ManR 13.4 13.6 16.7
Rank 12.1 12.8 17.2
J-E
Oracle 6.9 7.0 9.4
Rank 15.7 15.3 20.5
Table 3: Reorder performance measured by
crossing-link number per sentence. None means the
original sentences without reordering; Oracle means
the best permutation allowed by the source parse
tree; ManR refers to manual reorder rules; Rank
means ranking reordering model.
our ranking reordering model indeed significantly
reduces the crossing-link numbers over the original
sentence pairs. On the other hand, the performance
of the ranking reorder model still fall far short of or-
acle, which is the lowest crossing-link number of all
possible permutations allowed by the parse tree. By
manual analysis, we find that the gap is due to both
errors of the ranking reorder model and errors from
word alignment and parser.
Another thing to note is that the crossing-link
number of manual alignment is higher than auto-
matic alignment. The reason is that our annotators
tend to align function words which might be left un-
aligned by automatic word aligner.
5.6 Effect of Ranking Features
Here we examine the effect of features for ranking
reorder model. We compare their influence on Rank-
ingSVM accuracy, alignment crossing-link number,
end-to-end BLEU score, and the model size. As
Table 4 shows, a major part of reduction of CLN
comes from features such as Part-of-Speech tags,
Features Acc. CLN BLEU Feat.#
E-J
tag+label 88.6 16.4 22.24 26k
+dst 91.5 13.5 22.66 55k
+pct 92.2 13.1 22.73 79k
+lex100 92.9 12.1 22.85 347k
+lex1000 94.0 11.5 22.79 2,410k
+lex2000 95.2 10.7 22.81 3,794k
J-E
tag+fw 85.0 18.6 25.43 31k
+dst 90.3 16.9 25.62 65k
+lex100 91.6 15.7 25.87 293k
+lex1000 92.4 14.8 25.91 2,156k
+lex2000 93.0 14.3 25.84 3,297k
Table 4: Effect of ranking features. Acc. is Rank-
ingSVM accuracy in percentage on the training data;
CLN is the crossing-link number per sentence on
parallel corpus with automatically generated word
alignment; BLEU is the BLEU score in percentage
on web test set on Rank-IT setting (system with in-
tegrated rank reordering model); lexn means n most
frequent lexicons in the training corpus.
dependency labels (for English), function words (for
Japanese), and the distance and punctuations be-
tween child and head. These features also corre-
spond to BLEU score improvement for End-to-End
evaluations. Lexicon features generally continue to
improve the RankingSVM accuracy and reduce CLN
on training data, but they do not bring further im-
provement for SMT systems beyond the top 100
most frequent words. Our explanation is that less
frequent lexicons tend to help local reordering only,
which is already handled by the underlying phrase-
based system.
5.7 Performance on different domains
From Table 2 we can see that pre-reorder method has
higher BLEU score on news test, while integrated
model performs better on web test set which con-
tains informal texts. By error analysis, we find that
the parser commits more errors on informal texts,
and informal texts usually have more flexible trans-
lations. Pre-reorder method makes ?hard? decision
before decoding, thus is more sensitive to parser er-
rors; on the other hand, integrated model is forced
to use a longer distortion limit which leads to more
search errors during decoding time. It is possible to
918
use system combination method to get the best of
both systems, but we leave this to future work.
6 Discussion on Related Work
There have been several studies focusing on compil-
ing hand-crafted syntactic reorder rules. Collins et
al. (2005), Wang et al (2007), Ramanathan et al
(2008), Lee et al (2010) have developed rules for
German-English, Chinese-English, English-Hindi
and English-Japanese respectively. Xu et al (2009)
designed a clever precedence reordering rule set for
translation from English to several SOV languages.
The drawback for hand-crafted rules is that they de-
pend upon expert knowledge to produce and are lim-
ited to their targeted language pairs.
Automatically learning syntactic reordering rules
have also been explored in several work. Li et
al. (2007) and Visweswariah et al (2010) learned
probability of reordering patterns from constituent
trees using either Maximum Entropy or maximum
likelihood estimation. Since reordering patterns
are matched against a tree node together with all
its direct children, data sparseness problem will
arise when tree nodes have many children (Li et
al., 2007); Visweswariah et al (2010) also men-
tioned their method yielded no improvement when
applied to dependency trees in their initial experi-
ments. Genzel (2010) dealt with the data sparseness
problem by using window heuristic, and learned re-
ordering pattern sequence from dependency trees.
Even with the window heuristic, they were unable
to evaluate all candidates due to the huge num-
ber of possible patterns. Different from the pre-
vious approaches, we treat syntax-based reordering
as a ranking problem between different source tree
nodes. Our method does not require the source
nodes to match some specific patterns, but encodes
reordering knowledge in the form of a ranking func-
tion, which naturally handles reordering between
any number of tree nodes; the ranking function is
trained by well-established rank learning method to
minimize the number of mis-ordered tree nodes in
the training data.
Tree-to-string systems (Quirk et al, 2005; Liu et
al., 2006) model syntactic reordering using minimal
or composed translation rules, which may contain
reordering involving tree nodes from multiple tree
levels. Our method can be naturally extended to deal
with such multiple level reordering. For a tree-to-
string rule with multiple tree levels, instead of rank-
ing the direct children of the root node, we rank all
leaf nodes (Most are frontier nodes (Galley et al,
2006)) in the translation rule. We need to redesign
our ranking feature templates to encode the reorder-
ing information in the source part of the translation
rules. We need to remember the source side con-
text of the rules, the model size would still be much
smaller than a full-fledged tree-to-string system be-
cause we do not need to explicitly store the target
variants for each rule.
7 Conclusion and Future Work
In this paper we present a ranking based reorder-
ing method to reorder source language to match the
word order of target language given the source side
parse tree. Reordering is formulated as a task to rank
different nodes in the source side syntax tree accord-
ing to their relative position in the target language.
The ranking model is automatically trained to min-
imize the mis-ordering of tree nodes in the training
data. Large scale experiment shows improvement on
both reordering metric and SMT performance, with
up to 1.73 point BLEU gain in our evaluation test.
In future work, we plan to extend the ranking
model to handle reordering between multiple lev-
els of source trees. We also expect to explore bet-
ter way to integrate ranking reorder model into SMT
system instead of a simple penalty scheme. Along
the research direction of preprocessing the source
language to facilitate translation, we consider to not
only change the order of the source language, but
also inject syntactic structure of the target language
into source language by adding pseudo words into
source sentences.
Acknowledgements
Nan Yang and Nenghai Yu were partially supported
by Fundamental Research Funds for the Central
Universities (No. WK2100230002), National Nat-
ural Science Foundation of China (No. 60933013),
and National Science and Technology Major Project
(No. 2010ZX03004-003).
919
References
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
ACL, pages 263-270.
Michael Collins, Philipp Koehn and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. In Journal of Machine Learning Re-
search.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Translation Models. In Proc.
ACL-Coling, pages 961-968.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proc. EMNLP, pages 263-270.
Dmitriy Genzel. 2010. Automatically Learning Source-
side Reordering Rules for Large Scale Machine Trans-
lation. In Proc. Coling, pages 376-384.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer
2000. Large Margin Rank Boundaries for Ordinal Re-
gression. In Advances in Large Margin Classifiers,
pages 115-132.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinborgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. HLT-
NAACL, pages 127-133.
Taku Kudo, Yuji Matsumoto. 2002. Japanese Depen-
dency Analysis using Cascaded Chunking. In Proc.
CoNLL, pages 63-69.
Young-Suk Lee, Bing Zhao and Xiaoqiang Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In Proc.
Coling.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li and
Ming Zhou and Yi Guan 2007. A Probabilistic Ap-
proach to Syntax-based Reordering for Statistical Ma-
chine Translation. In Proc. ACL, pages 720-727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. ACL-Coling, pages 609-616.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC 2006
Joakim Nivre and Mario Scholz 2004. Deterministic De-
pendency Parsing for English Text. In Proc. Coling.
Franz J. Och. 2002. Statistical Machine Translation:
From Single Word Models to Alignment Template.
Ph.D.Thesis, RWTH Aachen, Germany
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1): pages 19-51.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proc. ACL, pages 271-279.
A. Ramanathan, Pushpak Bhattacharyya, Jayprasad
Hegde, Ritesh M. Shah and Sasikumar M. 2008.
Simple syntactic and morphological processing can
help English-Hindi Statistical Machine Translation.
In Proc. IJCNLP.
Roy Tromble. 2009. Search and Learning for the Lin-
ear Ordering Problem with an Application to Machine
Translation. Ph.D. Thesis.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan and Nandakishore Kamb-
hatla. 2010. Syntax Based Reordering with Automat-
ically Derived Rules for Improved Statistical Machine
Translation. In Proc. Coling, pages 1119-1127.
Chao Wang, Michael Collins, Philipp Koehn. 2007. Chi-
nese syntactic reordering for statistical machine trans-
lation. In Proc. EMNLP-CoNLL.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3): pages 377-403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In Proc. ACL-Coling,
pages 521-528.
Peng Xu, Jaeho Kang, Michael Ringgaard, Franz Och.
2009. Using a Dependency Parser to Improve SMT
for Subject-Object-Verb Languages. In Proc. HLT-
NAACL, pages 376-384.
Richard Zens and Hermann Ney. 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion. In Proc. Workshop on Statistical Machine Trans-
lation, HLT-NAACL, pages 127-133.
920
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166?175,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Word Alignment Modeling with Context Dependent Deep Neural Network
Nan Yang1, Shujie Liu2, Mu Li2, Ming Zhou2, Nenghai Yu1
1University of Science and Technology of China, Hefei, China
2Microsoft Research Asia, Beijing, China
{v-nayang,shujliu,muli,mingzhou}@microsoft.com
ynh@ustc.edu.cn
Abstract
In this paper, we explore a novel bilin-
gual word alignment approach based on
DNN (Deep Neural Network), which has
been proven to be very effective in var-
ious machine learning tasks (Collobert
et al, 2011). We describe in detail
how we adapt and extend the CD-DNN-
HMM (Dahl et al, 2012) method intro-
duced in speech recognition to the HMM-
based word alignment model, in which
bilingual word embedding is discrimina-
tively learnt to capture lexical translation
information, and surrounding words are
leveraged to model context information
in bilingual sentences. While being ca-
pable to model the rich bilingual corre-
spondence, our method generates a very
compact model with much fewer parame-
ters. Experiments on a large scale English-
Chinese word alignment task show that the
proposed method outperforms the HMM
and IBM model 4 baselines by 2 points in
F-score.
1 Introduction
Recent years research communities have seen a
strong resurgent interest in modeling with deep
(multi-layer) neural networks. This trending topic,
usually referred under the name Deep Learning, is
started by ground-breaking papers such as (Hin-
ton et al, 2006), in which innovative training pro-
cedures of deep structures are proposed. Unlike
shallow learning methods, such as Support Vector
Machine, Conditional Random Fields, and Maxi-
mum Entropy, which need hand-craft features as
input, DNN can learn suitable features (represen-
tations) automatically with raw input data, given a
training objective.
DNN did not achieve expected success until
2006, when researchers discovered a proper way
to intialize and train the deep architectures, which
contains two phases: layer-wise unsupervised pre-
training and supervised fine tuning. For pre-
training, Restricted Boltzmann Machine (RBM)
(Hinton et al, 2006), auto-encoding (Bengio et al,
2007) and sparse coding (Lee et al, 2007) are pro-
posed and popularly used. The unsupervised pre-
training trains the network one layer at a time, and
helps to guide the parameters of the layer towards
better regions in parameter space (Bengio, 2009).
Followed by fine tuning in this region, DNN is
shown to be able to achieve state-of-the-art per-
formance in various area, or even better (Dahl et
al., 2012) (Kavukcuoglu et al, 2010). DNN also
achieved breakthrough results on the ImageNet
dataset for objective recognition (Krizhevsky et
al., 2012). For speech recognition, (Dahl et al,
2012) proposed context-dependent neural network
with large vocabulary, which achieved 16.0% rel-
ative error reduction.
DNN has also been applied in Natural Lan-
guage Processing (NLP) field. Most works con-
vert atomic lexical entries into a dense, low di-
mensional, real-valued representation, called word
embedding; Each dimension represents a latent as-
pect of a word, capturing its semantic and syntac-
tic properties (Bengio et al, 2006). Word embed-
ding is usually first learned from huge amount of
monolingual texts, and then fine-tuned with task-
specific objectives. (Collobert et al, 2011) and
(Socher et al, 2011) further apply Recursive Neu-
ral Networks to address the structural prediction
tasks such as tagging and parsing, and (Socher
et al, 2012) explores the compositional aspect of
word representations.
Inspired by successful previous works, we pro-
pose a new DNN-based word alignment method,
which exploits contextual and semantic similari-
ties between words. As shown in example (a) of
Figure 1, in word pair {?juda? ??mammoth?},
the Chinese word ?juda? is a common word, but
166
mammothwill be a
jiang shi yixiang juda gongcheng
job
(a)
? ? ?? ?? ??
A :farmer Yibula said
nongmin yibula shuo : ?
?
(b)
?? ??? ?
Figure 1: Two examples of word alignment
the English word ?mammoth? is not, so it is very
hard to align them correctly. If we know that
?mammoth? has the similar meaning with ?big?,
or ?huge?, it would be easier to find the corre-
sponding word in the Chinese sentence. As we
mentioned in the last paragraph, word embedding
(trained with huge monolingual texts) has the abil-
ity to map a word into a vector space, in which,
similar words are near each other.
For example (b) in Figure 1, for the word pair
{?yibula? ? ?Yibula?}, both the Chinese word
?yibula? and English word ?Yibula? are rare name
entities, but the words around them are very com-
mon, which are {?nongmin?, ?shuo?} for Chinese
side and {?farmer?, ?said?} for the English side.
The pattern of the context {?nongmin X shuo?
? ?farmer X said?} may help to align the word
pair which fill the variableX , and also, the pattern
{?yixiang X gongcheng?? ?a X job?} is helpful
to align the word pair {?juda???mammoth?} for
example (a).
Based on the above analysis, in this paper, both
the words in the source and target sides are firstly
mapped to a vector via a discriminatively trained
word embeddings, and word pairs are scored by a
multi-layer neural network which takes rich con-
texts (surrounding words on both source and target
sides) into consideration; and a HMM-like distor-
tion model is applied on top of the neural network
to characterize structural aspect of bilingual sen-
tences.
In the rest of this paper, related work about
DNN and word alignment are first reviewed in
Section 2, followed by a brief introduction of
DNN in Section 3. We then introduce the details
of leveraging DNN for word alignment, including
the details of our network structure in Section 4
and the training method in Section 5. The mer-
its of our approach are illustrated with the experi-
ments described in Section 6, and we conclude our
paper in Section 7.
2 Related Work
DNN with unsupervised pre-training was firstly
introduced by (Hinton et al, 2006) for MNIST
digit image classification problem, in which, RBM
was introduced as the layer-wise pre-trainer. The
layer-wise pre-training phase found a better local
maximum for the multi-layer network, thus led to
improved performance. (Krizhevsky et al, 2012)
proposed to apply DNN to do object recognition
task (ImageNet dataset), which brought down the
state-of-the-art error rate from 26.1% to 15.3%.
(Seide et al, 2011) and (Dahl et al, 2012) apply
Context-Dependent Deep Neural Network with
HMM (CD-DNN-HMM) to speech recognition
task, which significantly outperforms traditional
models.
Most methods using DNN in NLP start with a
word embedding phase, which maps words into
a fixed length, real valued vectors. (Bengio et
al., 2006) proposed to use multi-layer neural net-
work for language modeling task. (Collobert et al,
2011) applied DNN on several NLP tasks, such
as part-of-speech tagging, chunking, name entity
recognition, semantic labeling and syntactic pars-
ing, where they got similar or even better results
than the state-of-the-art on these tasks. (Niehues
and Waibel, 2012) shows that machine transla-
tion results can be improved by combining neural
language model with n-gram traditional language.
(Son et al, 2012) improves translation quality of
n-gram translation model by using a bilingual neu-
ral language model. (Titov et al, 2012) learns a
context-free cross-lingual word embeddings to fa-
cilitate cross-lingual information retrieval.
For the related works of word alignment, the
most popular methods are based on generative
models such as IBM Models (Brown et al, 1993)
and HMM (Vogel et al, 1996). Discriminative ap-
proaches are also proposed to use hand crafted fea-
tures to improve word alignment. Among them,
(Liu et al, 2010) proposed to use phrase and rule
pairs to model the context information in a log-
linear framework. Unlike previous discriminative
methods, in this work, we do not resort to any hand
crafted features, but use DNN to induce ?features?
from raw words.
167
3 DNN structures for NLP
The most important and prevalent features avail-
able in NLP are the words themselves. To ap-
ply DNN to NLP task, the first step is to trans-
form a discrete word into its word embedding, a
low dimensional, dense, real-valued vector (Ben-
gio et al, 2006). Word embeddings often implic-
itly encode syntactic or semantic knowledge of
the words. Assuming a finite sized vocabulary V ,
word embeddings form a (L?|V |)-dimension em-
bedding matrix WV , where L is a pre-determined
embedding length; mapping words to embed-
dings is done by simply looking up their respec-
tive columns in the embedding matrix WV . The
lookup process is called a lookup layer LT , which
is usually the first layer after the input layer in neu-
ral network.
After words have been transformed to their em-
beddings, they can be fed into subsequent classi-
cal network layers to model highly non-linear re-
lations:
zl = fl(M lzl?1 + bl) (1)
where zl is the output of lth layer, M l is a |zl| ?
|zl?1| matrix, bl is a |zl|-length vector, and fl
is an activation function. Except for the last
layer, fl must be non-linear. Common choices for
fl include sigmoid function, hyperbolic function,
?hard? hyperbolic function etc. Following (Col-
lobert et al, 2011), we choose ?hard? hyperbolic
function as our activation function in this work:
htanh(x) =
?
?
?
1 if x is greater than 1
?1 if x is less than -1
x otherwise
(2)
If probabilistic interpretation is desired, a softmax
layer (Bridle, 1990) can be used to do normaliza-
tion:
zli =
ezl?1i
|zl|?
j=1
ez
l?1
j
(3)
The above layers can only handle fixed sized in-
put and output. If input must be of variable length,
convolution layer and max layer can be used, (Col-
lobert et al, 2011) which transform variable length
input to fixed length vector for further processing.
Multi-layer neural networks are trained with
the standard back propagation algorithm (LeCun,
1985). As the networks are non-linear and the
task specific objectives usually contain many lo-
cal maximums, special care must be taken in the
optimization process to obtain good parameters.
Techniques such as layerwise pre-training(Bengio
et al, 2007) and many tricks(LeCun et al, 1998)
have been developed to train better neural net-
works. Besides that, neural network training also
involves some hyperparameters such as learning
rate, the number of hidden layers. We will address
these issues in section 4.
4 DNN for word alignment
Our DNN word alignment model extends classic
HMM word alignment model (Vogel et al, 1996).
Given a sentence pair (e, f), HMM word alignment
takes the following form:
P (a, e|f) =
|e|?
i=1
Plex(ei|fai)Pd(ai ? ai?1) (4)
where Plex is the lexical translation probability
and Pd is the jump distance distortion probability.
One straightforward way to integrate DNN
into HMM is to use neural network to compute
the emission (lexical translation) probability Plex.
Such approach requires a softmax layer in the neu-
ral network to normalize over all words in source
vocabulary. As vocabulary for natural languages
is usually very large, it is prohibitively expen-
sive to do the normalization. Hence we give up
the probabilistic interpretation and resort to a non-
probabilistic, discriminative view:
sNN (a|e, f) =
|e|?
i=1
tlex(ei, fai |e, f)td(ai, ai?1|e, f)
(5)
where tlex is a lexical translation score computed
by neural network, and td is a distortion score.
In the classic HMM word alignment model,
context is not considered in the lexical translation
probability. Although we can rewrite Plex(ei|fai)
to Plex(ei|context of fai) to model context, it in-
troduces too many additional parameters and leads
to serious over-fitting problem due to data sparse-
ness. As a matter of fact, even without any con-
texts, the lexical translation table in HMM al-
ready contains O(|Ve| ? |Vf |) parameters, where
|Ve| and Vf denote source and target vocabulary
sizes. In contrast, our model does not maintain
a separate translation score parameters for every
source-target word pair, but computes tlex through
a multi-layer network, which naturally handles
contexts on both sides without explosive growth
of number of parameters.
168
Input
Source window e Target window f
)( 323 bzM ??
)( 212 bzM ??
ii-1 i+1 j-1 j j+1
Lookup 
LT
0zLayer f1 1zLayer f2 2z
?? ??? ? farmer yibula said
)( 101 bzM ??htanh
htanh
Layer f3 ),|,( fefet jilex
Figure 2: Network structure for computing context
dependent lexical translation scores. The example
computes translation score for word pair (yibula,
yibulayin) given its surrounding context.
Figure 2 shows the neural network we used
to compute context dependent lexical transla-
tion score tlex. For word pair (ei, fj), we take
fixed length windows surrounding both ei and fj
as input: (ei? sw2 , . . . , ei+ sw2 , fj? tw2 , . . . , fj+ tw2 ),where sw, tw stand window sizes on source and
target side respectively. Words are converted to
embeddings using the lookup table LT , and the
catenation of embeddings are fed to a classic neu-
ral network with two hidden-layers, and the output
of the network is the our lexical translation score:
tlex(ei, fj |e, f)
= f3 ? f2 ? f1 ? LT (window(ei), window(fj))
(6)
f1 and f2 layers use htanh as activation functions,
while f3 is only a linear transformation with no
activation function.
For the distortion td, we could use a lexicalized
distortion model:
td(ai, ai?1|e, f) = td(ai ? ai?1|window(fai?1))
(7)
which can be computed by a neural network sim-
ilar to the one used to compute lexical transla-
tion scores. If we map jump distance (ai ? ai?1)
to B buckets, we can change the length of the
output layer to B, where each dimension in the
output stands for a different bucket of jump dis-
tances. But we found in our initial experiments
on small scale data, lexicalized distortion does not
produce better alignment over the simple jump-
distance based model. So we drop the lexicalized
distortion and reverse to the simple version:
td(ai, ai?1|e, f) = td(ai ? ai?1) (8)
Vocabulary V of our alignment model consists
of a source vocabulary Ve and a target vocabu-
lary Vf . As in (Collobert et al, 2011), in addition
to real words, each vocabulary contains a special
unknown word symbol ?unk? to handle unseen
words; two sentence boundary symbols ?s? and
?/s?, which are filled into surrounding window
when necessary; furthermore, to handle null align-
ment, we must also include a special null symbol
?null?. When fj is null word, we simply fill the
surrounding window with the identical null sym-
bols.
To decode our model, the lexical translation
scores are computed for each source-target word
pair in the sentence pair, which requires going
through the neural network (|e| ? |f|) times; af-
ter that, the forward-backward algorithm can be
used to find the viterbi path as in the classic HMM
model.
The majority of tunable parameters in our
model resides in the lookup table LT , which is
a (L ? (|Ve| + |Vf |))-dimension matrix. For a
reasonably large vocabulary, the number is much
smaller than the number of parameters in classic
HMM model, which is in the order of (|Ve|?|Vf |).
1
The ability to model context is not unique to
our model. In fact, discriminative word alignment
can model contexts by deploying arbitrary features
(Moore, 2005). Different from previous discrim-
inative word alignment, our model does not use
manually engineered features, but learn ?features?
automatically from raw words by the neural net-
work. (Berger et al, 1996) use a maximum en-
tropy model to model the bag-of-words context for
word alignment, but their model treats each word
as a distinct feature, which can not leverage the
similarity between words as our model.
5 Training
Although unsupervised training technique such as
Contrastive Estimation as in (Smith and Eisner,
2005), (Dyer et al, 2011) can be adapted to train
1In practice, the number of non-zero parameters in clas-
sic HMM model would be much smaller, as many words do
not co-occur in bilingual sentence pairs. In our experiments,
the number of non-zero parameters in classic HMM model
is about 328 millions, while the NN model only has about 4
millions.
169
our model from raw sentence pairs, they are too
computational demanding as the lexical transla-
tion probabilities must be computed from neu-
ral networks. Hence, we opt for a simpler su-
pervised approach, which learns the model from
sentence pairs with word alignment. As we do
not have a large manually word aligned corpus,
we use traditional word alignment models such as
HMM and IBM model 4 to generate word align-
ment on a large parallel corpus. We obtain bi-
directional alignment by running the usual grow-
diag-final heuristics (Koehn et al, 2003) on uni-
directional results from both directions, and use
the results as our training data. Similar approach
has been taken in speech recognition task (Dahl et
al., 2012), where training data for neural network
model is generated by forced decoding with tradi-
tional Gaussian mixture models.
Tunable parameters in neural network align-
ment model include: word embeddings in lookup
table LT , parametersW l, bl for linear transforma-
tions in the hidden layers of the neural network,
and distortion parameters sd of jump distance. We
take the following ranking loss with margin as our
training criteria:
loss(?) =
?
every (e,f)
max{0, 1? s?(a+|e, f) + s?(a?|e, f)}
(9)
where ? denotes all tunable parameters, a+ is
the gold alignment path, a? is the highest scor-
ing incorrect alignment path under ?, and s? is
model score for alignment path defined in Eq. 5
. One nuance here is that the gold alignment af-
ter grow-diag-final contains many-to-many links,
which cannot be generated by any path. Our solu-
tion is that for each source word alignment multi-
ple target, we randomly choose one link among all
candidates as the golden link.
Because our multi-layer neural network is in-
herently non-linear and is non-convex, directly
training against the above criteria is unlikely to
yield good results. Instead, we take the following
steps to train our model.
5.1 Pre-training initial word embedding with
monolingual data
Most parameters reside in the word embeddings.
To get a good initial value, the usual approach is
to pre-train the embeddings on a large monolin-
gual corpus. We replicate the work in (Collobert
et al, 2011) and train word embeddings for source
and target languages from their monolingual cor-
pus respectively. Our vocabularies Vs and Vt con-
tain the most frequent 100,000 words from each
side of the parallel corpus, and all other words are
treated as unknown words. We set word embed-
ding length to 20, window size to 5, and the length
of the only hidden layer to 40. Follow (Turian et
al., 2010), we randomly initialize all parameters
to [-0.1, 0.1], and use stochastic gradient descent
to minimize the ranking loss with a fixed learn-
ing rate 0.01. Note that embedding for null word
in either Ve and Vf cannot be trained from mono-
lingual corpus, and we simply leave them at the
initial value untouched.
Word embeddings from monolingual corpus
learn strong syntactic knowledge of each word,
which is not always desirable for word align-
ment between some language pairs like English
and Chinese. For example, many Chinese words
can act as a verb, noun and adjective without any
change, while their English counter parts are dis-
tinct words with quite different word embeddings
due to their different syntactic roles. Thus we
have to modify the word embeddings in subse-
quent steps according to bilingual data.
5.2 Training neural network based on local
criteria
Training the network against the sentence level
criteria Eq. 5 directly is not efficient. Instead, we
first ignore the distortion parameters and train neu-
ral networks for lexical translation scores against
the following local pairwise loss:
max{0, 1? t?((e, f)+|e, f) + t?((e, f)?|e, f)}
(10)
where (e, f)+ is a correct word pair, (e, f)? is a
wrong word pair in the same sentence, and t? is as
defined in Eq. 6 . This training criteria essentially
means our model suffers loss unless it gives cor-
rect word pairs a higher score than random pairs
from the same sentence pair with some margin.
We initialize the lookup table with embed-
dings obtained from monolingual training, and
randomly initialize all W l and bl in linear layers
to [-0.1, 0.1]. We minimize the loss using stochas-
tic gradient descent as follows. We randomly cy-
cle through all sentence pairs in training data; for
each correct word pair (including null alignment),
we generate a positive example, and generate two
negative examples by randomly corrupting either
170
side of the pair with another word in the sentence
pair. We set learning rate to 0.01. As there is no
clear stopping criteria, we simply run the stochas-
tic optimizer through parallel corpus for N itera-
tions. In this work, N is set to 50.
To make our model concrete, there are still
hyper-parameters to be determined: the window
size sw and tw, the length of each hidden layer
Ll. We empirically set sw and tw to 11, L1 to
120, and L2 to 10, which achieved a minimal loss
on a small held-out data among several settings we
tested.
5.3 Training distortion parameters
We fix neural network parameters obtained from
the last step, and tune the distortion parameters
sd with respect to the sentence level loss using
standard stochastic gradient descent. We use a
separate parameter for jump distance from -7 and
7, and another two parameters for longer for-
ward/backward jumps. We initialize all parame-
ters in sd to 0, set the learning rate for the stochas-
tic optimizer to 0.001. As there are only 17 param-
eters in sd, we only need to run the optimizer over
a small portion of the parallel corpus.
5.4 Tuning neural network based on sentence
level criteria
Up-to-now, parameters in the lexical translation
neural network have not been trained against the
sentence level criteria Eq. 5. We could achieve
this by re-using the same online training method
used to train distortion parameters, except that we
now fix the distortion parameters and let the loss
back-propagate through the neural networks. Sen-
tence level training does not take larger context in
modeling word translations, but only to optimize
the parameters regarding to the sentence level loss.
This tuning is quite slow, and it did not improve
alignment on an initial small scale experiment; so,
we skip this step in all subsequent experiment in
this work.
6 Experiments and Results
We conduct our experiment on Chinese-to-English
word alignment task. We use the manually aligned
Chinese-English alignment corpus (Haghighi et
al., 2009) which contains 491 sentence pairs as
test set. We adapt the segmentation on the Chinese
side to fit our word segmentation standard.
6.1 Data
Our parallel corpus contains about 26 million
unique sentence pairs in total which are mined
from web.
The monolingual corpus to pre-train word em-
beddings are also crawled from web, which
amounts to about 1.1 billion unique sentences for
English and about 300 million unique sentences
for Chinese. As pre-processing, we lowercase all
English words, and map all numbers to one spe-
cial token; and we also map all email addresses
and URLs to another special token.
6.2 Settings
We use classic HMM and IBM model 4 as our
baseline, which are generated by Giza++ (Och and
Ney, 2000). We train our proposed model from re-
sults of classic HMM and IBM model 4 separately.
Since classic HMM, IBM model 4 and our model
are all uni-directional, we use the standard grow-
diag-final to generate bi-directional results for all
models.
Models are evaluated on the manually aligned
test set using standard metric: precision, recall and
F1-score.
6.3 Alignment Result
It can be seen from Table 1, the proposed model
consistently outperforms its corresponding base-
line whether it is trained from alignment of classic
HMM or IBM model 4. It is also clear that the
setting prec. recall F-1
HMM 0.768 0.786 0.777
HMM+NN 0.810 0.790 0.798
IBM4 0.839 0.805 0.822
IBM4+NN 0.885 0.812 0.847
Table 1: Word alignment result. The first row
and third row show baseline results obtained by
classic HMM and IBM4 model. The second row
and fourth row show results of the proposed model
trained from HMM and IBM4 respectively.
results of our model also depends on the quality
of baseline results, which is used as training data
of our model. In future we would like to explore
whether our method can improve other word align-
ment models.
We also conduct experiment to see the effect
on end-to-end SMT performance. We train hier-
171
archical phrase model (Chiang, 2007) from dif-
ferent word alignments. Despite different align-
ment scores, we do not obtain significant differ-
ence in translation performance. In our C-E exper-
iment, we tuned on NIST-03, and tested on NIST-
08. Case-insensitive BLEU-4 scores on NIST-08
test are 0.305 and 0.307 for models trained from
IBM-4 and NN alignment results. The result is not
surprising considering our parallel corpus is quite
large, and similar observations have been made in
previous work as (DeNero and Macherey, 2011)
that better alignment quality does not necessarily
lead to better end-to-end result.
6.4 Result Analysis
6.4.1 Error Analysis
From Table 1 we can see higher F-1 score of our
model mainly comes from higher precision, with
recall similar to baseline. By analyzing the results,
we found out that for both baseline and our model,
a large part of missing alignment links involves
stop words like English words ?the?, ?a?, ?it? and
Chinese words ?de?. Stop words are inherently
hard to align, which often requires grammatical
judgment unavailable to our models; as they are
also extremely frequent, our model fully learns
their alignment patterns of the baseline models,
including errors. On the other hand, our model
performs better on low-frequency words, espe-
cially proper nouns. Take person names for ex-
ample. Most names are low-frequency words, on
which baseline HMM and IBM4 models show the
?garbage collector? phenomenon. In our model,
different person names have very similar word em-
beddings on both English side and Chinese side,
due to monolingual pre-training; what is more, dif-
ferent person names often appear in similar con-
texts. As our model considers both word embed-
dings and contexts, it learns that English person
names should be aligned to Chinese person names,
which corrects errors of baseline models and leads
to better precision.
6.4.2 Effect of context
To examine how context contribute to alignment
quality, we re-train our model with different win-
dow size, all from result of IBM model 4. From
Figure 3, we can see introducing context increase
the quality of the learned alignment, but the ben-
efit is diminished for window size over 5. On the
other hand, the results are quite stable even with
large window size 13, without noticeable over-
0.740.76
0.780.8
0.820.84
0.86
1 3 5 7 9 11 13
Figure 3: Effect of different window sizes on word
alignment F-score.
fitting problem. This is not surprising consider-
ing that larger window size only requires slightly
more parameters in the linear layers. Lastly, it
is worth noticing that our model with no context
(window size 1) performs much worse than set-
tings with larger window size and baseline IBM4.
Our explanation is as follows. Our model uses
the simple jump distance based distortion, which
is weaker than the more sophisticated distortions
in IBM model 4; thus without context, it does not
perform well compared to IBM model 4. With
larger window size, our model is able to produce
more accurate translation scores based on more
contexts, which leads to better alignment despite
the simpler distortions.
IBM4+NN F-1
1-hidden-layer 0.834
2-hidden-layer 0.847
3-hidden-layer 0.843
Table 3: Effect of different number of hidden lay-
ers. Two hidden layers outperform one hidden
layer, while three hidden layers do not bring fur-
ther improvement.
6.4.3 Effect of number of hidden layers
Our neural network contains two hidden layers be-
sides the lookup layer. It is natural to ask whether
adding more layers would be beneficial. To an-
swer this question, we train models with 1, 2 and
3 layers respectively, all from result of IBM model
4. For 1-hidden-layer setting, we set the hidden
layer length to 120; and for 3-hidden-layer set-
ting, we set hidden layer lengths to 120, 100, 10
respectively. As can be seen from Table 3, 2-
hidden-layer outperforms the 1-hidden-layer set-
ting, while another hidden layer does not bring
172
word good history british served labs zetian laggards
LM
bad tradition russian worked networks hongzhang underperformers
great culture japanese lived technologies yaobang transferees
strong practice dutch offered innovations keming megabanks
true style german delivered systems xingzhi mutuals
easy literature canadian produced industries ruihua non-starters
WA
nice historical uk offering lab hongzhang underperformers
great historic britain serving laboratories qichao illiterates
best developed english serve laboratory xueqin transferees
pretty record classic delivering exam fuhuan matriculants
excellent recording england worked experiments bingkun megabanks
Table 2: Nearest neighbors of several words according to their embedding distance. LM shows neighbors
of word embeddings trained by monolingual language model method; WA shows neighbors of word
embeddings trained by our word alignment model.
improvement. Due to time constraint, we have
not tuned the hyper-parameters such as length of
hidden layers in 1 and 3-hidden-layer settings, nor
have we tested settings with more hidden-layers.
It would be wise to test more settings to verify
whether more layers would help.
6.4.4 Word Embedding
Following (Collobert et al, 2011), we show some
words together with its nearest neighbors using the
Euclidean distance between their embeddings. As
we can see from Table 2, after bilingual training,
?bad? is no longer in the nearest neighborhood of
?good? as they hold opposite semantic meanings;
the nearest neighbor of ?history? is now changed
to its related adjective ?historical?. Neighbors of
proper nouns such as person names are relatively
unchanged. For example, neighbors of word
?zetian? are all Chinese names in both settings.
As Chinese language lacks morphology, the single
form and plural form of a noun in English often
correspond to the same Chinese word, thus it is
desirable that the two English words should have
similar word embeddings. While this is true for
relatively frequent nouns such as ?lab? and ?labs?,
rarer nouns still remain near their monolingual
embeddings as they are only modified a few times
during the bilingual training. As shown in last
column, neighborhood of ?laggards? still consists
of other plural forms even after bilingual training.
7 Conclusion
In this paper, we explores applying deep neu-
ral network for word alignment task. Our model
integrates a multi-layer neural network into an
HMM-like framework, where context dependent
lexical translation score is computed by neural
network, and distortion is modeled by a sim-
ple jump-distance scheme. Our model is dis-
criminatively trained on bilingual corpus, while
huge monolingual data is used to pre-train word-
embeddings. Experiments on large-scale Chinese-
to-English task show that the proposed method
produces better word alignment results, compared
with both classic HMM model and IBM model 4.
For future work, we will investigate more set-
tings of different hyper-parameters in our model.
Secondly, we want to explore the possibility of
unsupervised training of our neural word align-
ment model, without reliance of alignment result
of other models. Furthermore, our current model
use rather simple distortions; it might be helpful
to use more sophisticated model such as ITG (Wu,
1997), which can be modeled by Recursive Neural
Networks (Socher et al, 2011).
Acknowledgments
We thank anonymous reviewers for insightful
comments. We also thank Dongdong Zhang, Lei
Cui, Chunyang Wu and Zhenyan He for fruitful
discussions.
References
Yoshua Bengio, Holger Schwenk, Jean-Se?bastien
Sene?cal, Fre?deric Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. Inno-
vations in Machine Learning, pages 137?186.
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
173
Hugo Larochelle. 2007. Greedy layer-wise training
of deep networks. Advances in neural information
processing systems, 19:153.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Foundations and Trends R? in Machine Learning,
2(1):1?127.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Comput.
Linguist., 22(1):39?71, March.
JS Bridle. 1990. Neurocomputing: Algorithms, archi-
tectures and applications, chapter probabilistic inter-
pretation of feedforward classification network out-
puts, with relationships to statistical pattern recogni-
tion.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201?228.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
Audio, Speech, and Language Processing, IEEE
Transactions on, 20(1):30?42.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proc. ACL.
Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A
Smith. 2011. Unsupervised word alignment with ar-
bitrary features. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 409?419. Association for Computational Lin-
guistics.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with su-
pervised itg models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 923?931. Association for Compu-
tational Linguistics.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep be-
lief nets. Neural computation, 18(7):1527?1554.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Michae?l Mathieu, and Yann LeCun.
2010. Learning convolutional feature hierarchies for
visual recognition. Advances in Neural Information
Processing Systems, pages 1090?1098.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106?1114.
Yann LeCun, Le?on Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278?2324.
Yann LeCun. 1985. A learning scheme for asymmet-
ric threshold networks. Proceedings of Cognitiva,
85:599?604.
Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y Ng. 2007. Efficient sparse coding algo-
rithms. Advances in neural information processing
systems, 19:801.
Shujie Liu, Chi-Ho Li, and Ming Zhou. 2010. Dis-
criminative pruning for discriminative itg alignment.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL, vol-
ume 10, pages 316?324.
Y MarcAurelio Ranzato, Lan Boureau, and Yann Le-
Cun. 2007. Sparse feature learning for deep belief
networks. Advances in neural information process-
ing systems, 20:1185?1192.
Robert C Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88. Association for Computational
Linguistics.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using restricted boltzmann
machines. In Proceedings of the nineth Interna-
tional Workshop on Spoken Language Translation
(IWSLT).
Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models.
Frank Seide, Gang Li, and Dong Yu. 2011. Conversa-
tional speech transcription using context-dependent
deep neural networks. In Proc. Interspeech, pages
437?440.
174
Noah A Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
354?362. Association for Computational Linguis-
tics.
Richard Socher, Cliff C Lin, Andrew Y Ng, and
Christopher D Manning. 2011. Parsing natural
scenes and natural language with recursive neu-
ral networks. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML),
volume 2, page 7.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Le Hai Son, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: Human language
technologies, pages 39?48. Association for Compu-
tational Linguistics.
Ivan Titov, Alexandre Klementiev, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Urbana, 51:61801.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841. Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
175
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 752?760,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Punctuation Prediction with Transition-based Parsing 
Dongdong Zhang1, Shuangzhi Wu2, Nan Yang3, Mu Li1 
 
1Microsoft Research Asia, Beijing, China 
2Harbin Institute of Technology, Harbin, China 
3University of Science and Technology of China, Hefei, China 
{dozhang,v-shuawu,v-nayang,muli}@microsoft.com 
 
Abstract 
Punctuations are not available in automatic 
speech recognition outputs, which could cre-
ate barriers to many subsequent text pro-
cessing tasks. This paper proposes a novel 
method to predict punctuation symbols for the 
stream of words in transcribed speech texts. 
Our method jointly performs parsing and 
punctuation prediction by integrating a rich set 
of syntactic features when processing words 
from left to right. It can exploit a global view 
to capture long-range dependencies for punc-
tuation prediction with linear complexity. The 
experimental results on the test data sets of 
IWSLT and TDT4 show that our method can 
achieve high-level performance in punctuation 
prediction over the stream of words in tran-
scribed speech text. 
1 Introduction 
Standard automatic speech recognizers output un-
structured streams of words. They neither perform 
a proper segmentation of the output into sentences, 
nor predict punctuation symbols. The unavailable 
punctuations and sentence boundaries in tran-
scribed speech texts create barriers to many sub-
sequent processing tasks, such as summarization, 
information extraction, question answering and 
machine translation. Thus, the segmentation of 
long texts is necessary in many real applications. 
For example, in speech-to-speech translation, 
continuously transcribed speech texts need to be 
segmented before being fed into subsequent ma-
chine translation systems (Takezawa et al, 1998; 
Nakamura, 2009). This is because current ma-
chine translation (MT) systems perform the trans-
lation at the sentence level, where various models 
used in MT are trained over segmented sentences 
and many algorithms inside MT have an exponen-
tial complexity with regard to the length of inputs. 
The punctuation prediction problem has at-
tracted research interest in both the speech pro-
cessing community and the natural language pro-
cessing community. Most previous work primar-
ily exploits local features in their statistical mod-
els such as lexicons, prosodic cues and hidden 
event language model (HELM) (Liu et al, 2005; 
Matusov et al, 2006; Huang and Zweig, 2002; 
Stolcke and Shriberg, 1996). The word-level mod-
els integrating local features have narrow views 
about the input and could not achieve satisfied 
performance due to the limited context infor-
mation access (Favre et al, 2008). Naturally, 
global contexts are required to model the punctu-
ation prediction, especially for long-range de-
pendencies. For instance, in English question sen-
tences, the ending question mark is long-range de-
pendent on the initial phrases (Lu and Ng, 2010), 
such as ?could you? in Figure 1. There has been 
some work trying to incorporate syntactic features 
to broaden the view of hypotheses in the punctua-
tion prediction models (Roark et al, 2006; Favre 
et al, 2008). In their methods, the punctuation 
prediction is treated as a separated post-procedure 
of parsing, which may suffer from the problem of 
error propagation. In addition, these approaches 
are not able to incrementally process inputs and 
are not efficient for very long inputs, especially in 
the cases of long transcribed speech texts from 
presentations where the number of streaming 
words could be larger than hundreds or thousands. 
In this paper, we propose jointly performing   
punctuation prediction and transition-based de-
pendency parsing over transcribed speech text. 
When the transition-based parsing consumes the 
stream of words left to right with the shift-reduce 
decoding algorithm, punctuation symbols are pre-
dicted for each word based on the contexts of the 
parsing tree. Two models are proposed to cause 
the punctuation prediction to interact with the 
transition actions in parsing. One is to conduct 
transition actions of parsing followed by punctua-
tion predictions in a cascaded way. The other is to 
associate the conventional transition actions of 
parsing with punctuation perditions, so that pre-
dicted punctuations are directly inferred from the 
752
 
(a). The transcribed speech text without punctuations 
 
  
 
 
 
 
(b). Transition-based parsing trees and predicted punctuations over transcribed text 
 
 
(c). Two segmentations are formed when inserting the predicted punctuation symbols into the transcribed text 
Figure 1. An example of punctuation prediction. 
parsing tree. Our models have linear complexity 
and are capable of handling streams of words with 
any length. In addition, the computation of models 
use a rich set of syntactic features, which can im-
prove the complicated punctuation predictions 
from a global view, especially for the long range 
dependencies.  
Figure 1 shows an example of how parsing 
helps punctuation prediction over the transcribed 
speech text. As illustrated in Figure 1(b), two 
commas are predicted when their preceding words 
act as the adverbial modifiers (advmod) during 
parsing. The period after the word ?menu? is pre-
dicted when the parsing of an adverbial clause 
modifier (advcl) is completed. The question mark 
at the end of the input is determined when a direct 
object modifier (dobj) is identified, together with 
the long range clue that the auxiliary word occurs 
before the nominal subject (nsubj). Eventually, 
two segmentations are formed according to the 
punctuation prediction results, shown in Figure 
1(c).  
The training data used for our models is adapted 
from Treebank data by excluding all punctuations 
but keeping the punctuation contexts, so that it can 
simulate the unavailable annotated transcribed 
speech texts. In decoding, beam search is used to 
get optimal punctuation prediction results. We 
conduct experiments on both IWSLT data and 
TDT4 test data sets. The experimental results 
show that our method can achieve higher perfor-
mance than the CRF-based baseline method. 
The paper is structured as follows: Section 2 
conducts a survey of related work. The transition-
based dependency parsing is introduced in Section 
3. We explain our approach to predicting punctu-
ations for transcribed speech texts in Section 4. 
Section 5 gives the results of our experiment. The 
conclusion and future work are given in Section 6. 
2 Related Work 
Sentence boundary detection and punctuation pre-
diction have been extensively studied in the 
speech processing field and have attracted re-
search interest in the natural language processing 
field as well. Most previous work exploits local 
features for the task. Kim and Woodland (2001), 
Huang and Zweig (2002), Christensen et al 
(2001), and Liu et al (2005) integrate both pro-
sodic features (pitch, pause duration, etc.) and lex-
ical features (words, n-grams, etc.) to predict 
punctuation symbols during speech recognition, 
where Huang and Zweig (2002) uses a maximum 
entropy model, Christensen et al (2001) focus on 
finite state and multi-layer perceptron methods, 
and Liu et al (2005) uses conditional random 
fields. However, in some scenarios the prosodic 
cues are not available due to inaccessible original 
raw speech waveforms. Matusov et al (2006) in-
tegrate segmentation features into the log-linear 
model in the statistical machine translation (SMT) 
framework to improve the translation perfor-
mance when translating transcribed speech texts. 
Lu and Ng (2010) uses dynamic conditional ran-
dom fields to perform both sentence boundary and 
sentence type prediction. They achieved promis-
ing results on both English and Chinese tran-
scribed speech texts. The above work only ex-
anyway you may find your favorite if you go through the menu so could you tell me your choice 
                   anyway you may find your favorite if you go  through the menu so could you tell me your choice 
, N N N N N N N N N N . , N N N N N ? 
anyway, you may find your favorite if you go through the menu. so, could you tell me your choice? 
nsubj nsubj poss 
aux
mark pobj 
iobj 
advmod 
advcl 
nsubj dobj 
det poss aux prep 
advmod dobj 
753
ploits local features, so they were limited to cap-
turing long range dependencies for punctuation 
prediction. 
It is natural to incorporate global knowledge, 
such as syntactic information, to improve punctu-
ation prediction performance. Roark et al (2006) 
use a rich set of non-local features including par-
ser scores to re-rank full segmentations. Favre et 
al. (2008) integrate syntactic information from a 
PCFG parser into a log-linear and combine it with 
local features for sentence segmentation. The 
punctuation prediction in these works is per-
formed as a post-procedure step of parsing, where 
a parse tree needs to be built in advance. As their 
parsing over the stream of words in transcribed 
speech text is exponentially complex, their ap-
proaches are only feasible for short input pro-
cessing. Unlike these works, we incorporate punc-
tuation prediction into the parsing which process 
left to right input without length limitations. 
Numerous dependency parsing algorithms 
have been proposed in the natural language pro-
cessing community, including transition-based 
and graph-based dependency parsing. Compared 
to graph-based parsing, transition-based parsing 
can offer linear time complexity and easily lever-
age non-local features in the models (Yamada and 
Matsumoto, 2003; Nivre et al, 2006b; Zhang and 
Clark, 2008; Huang and Sagae, 2010). Starting 
with the work from (Zhang and Nivre, 2011), in 
this paper we extend transition-based dependency 
parsing from the sentence-level to the stream of 
words and integrate the parsing with punctuation 
prediction.  
Joint POS tagging and transition-based de-
pendency parsing are studied in (Hatori et al, 
2011; Bohnet and Nivre, 2012). The improve-
ments are reported with the joint model compared 
to the pipeline model for Chinese and other richly 
inflected languages, which shows that it also 
makes sense to jointly perform punctuation pre-
diction and parsing, although these two tasks of 
POS tagging and punctuation prediction are dif-
ferent in two ways: 1). The former usually works 
on a well-formed single sentence while the latter 
needs to process multiple sentences that are very 
lengthy. 2). POS tags are must-have features to 
parsing while punctuations are not. The parsing 
quality in the former is more sensitive to the per-
formance of the entire task than in the latter. 
3 Transition-based dependency parsing 
In a typical transition-based dependency parsing 
process, the shift-reduce decoding algorithm is 
applied and a queue and stack are maintained 
(Zhang and Nivre, 2011). The queue stores the 
stream of transcribed speech words, the front of 
which is indexed as the current word. The stack 
stores the unfinished words which may be linked 
with the current word or a future word in the 
queue. When words in the queue are consumed 
from left to right, a set of transition actions is ap-
plied to build a parse tree. There are four kinds of 
transition actions conducted in the parsing process 
(Zhang and Nivre, 2011), as described in Table 1.  
 
Action Description 
Shift Fetches the current word from the 
queue and pushes it to the stack 
Reduce Pops the stack 
LeftArc Adds a dependency link from the cur-
rent word to the stack top, and  pops the 
stack 
RightArc Adds a dependency link from the stack 
top to the current word, takes away the 
current word from the queue and 
pushes it to the stack 
Table 1. Action types in transition-based parsing 
The choice of each transition action during the 
parsing is scored by a linear model that can be 
trained over a rich set of non-local features ex-
tracted from the contexts of the stack, the queue 
and the set of dependency labels. As described in 
(Zhang and Nivre, 2011), the feature templates 
could be defined over the lexicons, POS-tags and 
the combinations with syntactic information. 
In parsing, beam search is performed to search 
the optimal sequence of transition actions, from 
which a parse tree is formed (Zhang and Clark, 
2008). As each word must be pushed to the stack 
once and popped off once, the number of actions 
needed to parse a sentence is always 2n, where n 
is the length of the sentence. Thus, transition-
based parsing has a linear complexity with the 
length of input and naturally it can be extended to 
process the stream of words. 
4 Our method 
4.1 Model 
In the task of punctuation prediction, we are given 
a stream of words from an automatic transcription 
of speech text, denoted by ?1
?: = ?1, ?2, ? , ?? . 
We are asked to output a sequence of punctuation 
symbols ?1
?:= ?1, ?2, ? , ??  where ??  is attached 
to ?? to form a sentence like Figure 1(c). If there 
are no ambiguities, ?1
?  is also abbreviated as ?, 
754
similarly for ?1
? as ?. We model the search of the 
best sequence of predicted punctuation symbols 
?? as: 
 
             ?? = argmaxS?(?1
?|?1
?)                     (1) 
 
We introduce the transition-based parsing tree 
? to guide the punctuation prediction in Model (2), 
where parsing trees are constructed over the tran-
scribed text while containing no punctuations. 
  
?? = argmax?? ?(?|?1
?) ? ?(?1
?|?, ?1
?)?     (2) 
 
Rather than enumerate all possible parsing trees, 
we jointly optimize the punctuation prediction 
model and the transition-based parsing model 
with the form:  
 
(??, ??) = argmax(?,?)?(?|?1
?) ?
                                                ?(?1
?|?, ?1
?)           (3) 
 
Let ?1
? be the constructed partial tree when ?1
?  
is consumed from the queue. We decompose the 
Model (3) into:  
 
(??, ??) =
argmax(?,?)? ?(?1
?|?1
??1, ?1
?) ? ?(??|?1
?, ?1
?)??=1                                              
(4) 
 
It is noted that a partial parsing tree uniquely 
corresponds to a sequence of transition actions, 
and vice versa. Suppose ?1
? corresponds to the ac-
tion sequence ?1
?  and let ?? denote the last action 
in ?1
? . As the current word ??  can only be con-
sumed from the queue by either Shift or RightArc 
according to Table 1, we have ?? ?
{?????, ????????} . Thus, we synchronize the 
punctuation prediction with the application of 
Shift and RightArc during the parsing, which is ex-
plained by Model (5).  
 
(??, ??) = argmax(?,?)? ?(?1
? , ?1
? |?1
??1, ?1
?)
?
?=1
? ?(??|?? , ?1
? , ?1
?) 
                                                                      (5) 
 
The model is further refined by reducing the 
computation scope. When a full-stop punctuation 
is determined (i.e., a segmentation is formed), we 
discard the previous contexts and restart a new 
                                                          
1 Specially, ?? is equal to 1 if there are no previous full-stop 
punctuations. 
procedure for both parsing and punctuation pre-
diction over the rest of words in the stream. In this 
way we are theoretically able to handle the unlim-
ited stream of words without needing to always 
keep the entire context history of streaming words. 
Let ?? be the position index of last full-stop punc-
tuation1 before ?, ???
?  and ???
? the partial tree and 
corresponding action sequence over the words 
???
? , Model (5) can be rewritten by: 
 
(??, ??) =
argmax(?,?)? ?(???
? , ???
? |???
??1, ???
? ) ???=1
                                ?(??|?? , ???
? , ???
? )                     (6) 
 
With different computation of Model (6), we 
induce two joint models for punctuation predic-
tion: the cascaded punctuation prediction model 
and the unified punctuation prediction model.  
4.2 Cascaded punctuation prediction model 
(CPP) 
In Model (6), the computation of two sub-models 
is independent. The first sub-model is computed 
based on the context of words and partial trees 
without any punctuation knowledge, while the 
computation of the second sub-model is condi-
tional on the context from the partially built pars-
ing tree ???
?  and the transition action. As the words 
in the stream are consumed, each computation of 
transition actions is followed by a computation of 
punctuation prediction. Thus, the two sub-models 
are computed in a cascaded way, until the optimal 
parsing tree and optimal punctuation symbols are 
generated. We call this model the cascaded punc-
tuation prediction model (CPP). 
4.3 Unified punctuation prediction model 
(UPP) 
In Model (6), if the punctuation symbols can be 
deterministically inferred from the partial tree, 
?(??|??, ???
? , ???
? ) can be omitted because it is al-
ways 1. Similar to the idea of joint POS tagging 
and parsing (Hatori et al, 2011; Bohnet and Nivre, 
2012), we propose attaching the punctuation pre-
diction onto the parsing tree by embedding ?? into 
?? . Thus, we extend the conventional transition 
actions illustrated in Table 1 to a new set of tran-
sition actions for the parsing, denoted by ???: 
 
755
??? = {???????, ??????} ? {?????(?)|? ? ?}
? {????????(?)|? ? ?} 
 
where Q is the set of punctuation symbols to be 
predicted, ? is a punctuation symbol belonging to 
Q, Shift(s) is an action that attaches s to the current 
word on the basis of original Shift action in pars-
ing, RightArc(s) attaches ? to the current word on 
the basis of original RightArc action. 
With the redefined transition action set ???, the 
computation of Model (6) is reformulated as:  
  
(??, ??) =
argmax(?,?)? ? (???
? , ???
??
?
|???
??1, ?????
??1
, ???
? )??=1        (7) 
 
Here, the computation of parsing tree and punc-
tuation prediction is unified into one model where 
the sequence of transition action outputs uniquely 
determines the punctuations attached to the words. 
We refer to it as the unified punctuation predic-
tion model (UPP). 
 
 
 
 
 
 
 
 
(a). Parsing tree and attached punctuation symbols 
 
Shift(,), Shift(N), Shift(N), LeftArc, LeftArc, LeftArc, 
Shift(N), RightArc(?), Reduce, Reduce 
 
(b). The corresponding sequence of transition actions 
Figure 2. An example of punctuation prediction 
using the UPP model, where N is a null type punc-
tuation symbol denoting no need to attach any 
punctuation to the word. 
Figure 2 illustrates an example how the UPP 
model works. Given an input ?so could you tell 
me?, the optimal sequence of transition actions in 
Figure 2(b) is calculated based on the UPP model 
to produce the parsing tree in Figure 2(a). Accord-
ing to the sequence of actions, we can determine 
the sequence of predicted punctuation symbols 
like ?,NNN?? that have been attached to the words 
shown in Figure 2(a). The final segmentation with 
the predicted punctuation insertion could be ?so, 
could you tell me??. 
4.4 Model training and decoding 
In practice, the sub-models in Model (6) and (7) 
with the form of ?(?|?) is computed with a linear 
model ?????(?, ?) as 
 
?????(?, ?) = ?(?, ?) ? ? 
 
where ?(?, ?)  is the feature vector extracted 
from the output ? and the context ?, and ? is the 
weight vector. For the features of the models, we 
incorporate the bag of words and POS tags as well 
as tree-based features shown in Table 2, which are 
the same as those defined in (Zhang and Nivre, 
2011).  
 
(a) ws; w0; w1; w2; ps; p0; p1; p2; wsps; w0p0; w1p1; 
w2p2; wspsw0p0; wspsw0; wspsp0; wsw0p0; 
psw0p0; wsw0; psp0; p0p1; psp0p1; p0p1p2; 
(b) pshpsp0; pspslp0; pspsrp0; psp0p0l; wsd; psd; w0d; 
p0d; wsw0d; psp0d; wsvl; psvl; wsvr; psvr; w0vl; 
p0vl; wsh; psh; ts; w0l; p0l; t0l; w0r; p0r; t0r; w1l; 
p1l; t1l; wsh2; psh2; tsh; wsl2; psl2; tsl2; wsr2; psr2; 
tsr2; w0l2; p0l2; t0l2; pspslpsl2; pspsrpsr2; pspshpsh2; 
p0p0lp0l2; wsTl; psTl; wsTr; psTr; w0Tl; p0Tl; 
Table 2. (a) Features of the bag of words and POS 
tags. (b). Tree-based features. w?word; p?POS 
tag; d?distance between ws and w0; v?number of 
modifiers; t?dependency label; T?set of depend-
ency labels; s, 0, 1 and 2 index the stack top and 
three front items in the queue respectively; h?head; 
l?left/leftmost; r?right/rightmost; h2?head of a 
head; l2?second leftmost; r2?second rightmost. 
The training data for both the CPP and UPP 
models need to contain parsing trees and punctu-
ation information. Due to the absence of annota-
tion over transcribed speech data, we adapt the 
Treebank data for the purpose of model training. 
To do this, we remove all types of syntactic infor-
mation related to punctuation symbols from the 
raw Treebank data, but record what punctuation 
symbols are attached to the words. We normalize 
various punctuation symbols into two types: Mid-
dle-paused punctuation (M) and Full-stop punctu-
ation (F). Plus null type (N), there are three kinds 
of punctuation symbols attached to the words. Ta-
ble 3 illustrates the normalizations of punctuation 
symbols. In the experiments, we did not further 
distinguish the type among full-stop punctuation 
because the question mark and the exclamation 
mark have very low frequency in Treebank data. 
so could you tell me 
, N N N ? 
nsubj iobj 
aux
advmod 
756
But our CPP and UPP models are both independ-
ent regarding the number of punctuation types to 
be predicted. 
 
Punctuations Normalization 
Period, question mark, 
exclamation mark 
Full-stop punctuation 
(F) 
Comma, Colon, semi-
colon 
Middle-paused punctu-
ation (M) 
Multiple Punctuations 
(e.g., !!!!?) 
Full-stop punctuation 
(F) 
Quotations, brackets, 
etc. 
Null (N) 
Table 3. Punctuation normalization in training 
data 
As the feature templates are the same for the 
model training of both CPP and UPP, the training 
instances of CPP and UPP have the same contexts 
but with different outputs. Similar to work in 
(Zhang and Clark, 2008; Zhang and Nivre, 2011), 
we train CPP and UPP by generalized perceptron 
(Collins, 2002).  
In decoding, beam search is performed to get 
the optimal sequence of transition actions in CPP 
and UPP, and the optimal punctuation symbols in 
CPP. To ensure each segment decided by a full-
stop punctuation corresponds to a single parsing 
tree, two constraints are applied in decoding for 
the pruning of deficient search paths. 
(1) Proceeding-constraint: If the partial pars-
ing result is not a single tree, the full-stop 
punctuation prediction in CPP cannot be 
performed. In UPP, if Shift(F) or 
RightArc(F) fail to result in a single parsing 
tree, they cannot be performed as well. 
(2) Succeeding-constraint: If the full-stop 
punctuation is predicted in CPP, or Shift(F) 
and RightArc(F) are performed in UPP, the 
following transition actions must be a se-
quence of Reduce actions until the stack 
becomes empty. 
5 Experiments 
5.1 Experimental setup 
Our training data of transition-based dependency 
trees are converted from phrasal structure trees in 
English Web Treebank (LDC2012T13) and the 
English portion of OntoNotes 4.0 (LDC2011T03) 
by the Stanford Conversion toolkit (Marneffe et 
al., 2006). It contains around 1.5M words in total 
and consist of various genres including weblogs, 
web texts, newsgroups, email, reviews, question-
answer sessions, newswires, broadcast news and 
broadcast conversations. To simulate the tran-
scribed speech text, all words in dependency trees 
are lowercased and punctuations are excluded be-
fore model training. In addition, every ten depend-
ency trees are concatenated sequentially to simu-
late a parsing result of a stream of words in the 
model training. 
There are two test data sets used in our experi-
ments. One is the English corpus of the IWSLT09 
evaluation campaign (Paul, 2009) that is the con-
versional speech text. The other is a subset of the 
TDT4 English data (LDC2005T16) which con-
sists of 200 hours of closed-captioned broadcast 
news.  
In the decoding, the beam size of both the tran-
sition-based parsing and punctuation prediction is 
set to 5. The part-of-speech tagger is our re-imple-
mentation of the work in (Collins, 2002).  
The evaluation metrics of our experiments are 
precision (prec.), recall (rec.) and F1-measure 
(F1). 
For the comparison, we also implement a base-
line method based on the CRF model. It incorpo-
rates the features of bag of words and POS tags 
shown in Table 2(a), which are commonly used in 
previous related work.  
5.2 Experimental results 
We test the performance of our method on both 
the correctly recognized texts and automatically 
recognized texts. The former data is used to eval-
uate the capability of punctuation prediction of 
our algorithm regardless of the noises from speech 
data, as our model training data come from formal 
text instead of transcribed speech data. The usage 
of the latter test data set aims to evaluate the ef-
fectiveness of our method in real applications 
where lots of substantial recognition errors could 
be contained. In addition, we also evaluate the 
quality of our transition-based parsing, as its per-
formance could have a big influence on the quality 
of punctuation prediction. 
5.2.1 Performance on correctly recognized 
text 
The evaluation of our method on correctly recog-
nized text uses 10% of IWSLT09 training set, 
which consists of 19,972 sentences from BTEC 
(Basic Travel Expression Corpus) and 10,061 sen-
tences from CT (Challenge Task). The average in-
put length is about 10 words and each input con-
tains 1.3 sentences on average. The evaluation re-
sults are presented in Table 4.  
757
  Measure   Middle-
Paused 
Full-stop Mixed 
Baseline 
(CRF) 
prec. 33.2% 81.5% 78.8% 
rec. 25.9% 83.8% 80.7% 
F1 29.1% 82.6% 79.8% 
 
CPP 
prec. 51% 89% 89.6% 
rec. 50.3% 93.1% 92.7% 
F1 50.6% 91% 91.1% 
 
UPP 
 
prec. 52.6% 93.2% 92% 
rec. 59.7% 91.3% 92.3% 
F1 55.9% 92.2% 92.2% 
Table 4. Punctuation prediction performance on 
correctly recognized text 
   We achieved good performance on full-stop 
punctuation compared to the baseline, which 
shows our method can efficiently process sen-
tence segmentation because each segment is de-
cided by the structure of a single parsing tree. In 
addition, the global syntactic knowledge used in 
our work help capture long range dependencies of 
punctuations. The performance of middle-paused 
punctuation prediction is fairly low between all 
methods, which shows predicting middle-paused 
punctuations is a difficult task. This is because the 
usage of middle-paused punctuations is very flex-
ible, especially in conversional data. The last col-
umn in Table 4 presents the performance of the 
pure segmentation task where the middle-paused 
and full-stop punctuations are mixed and not dis-
tinguished. The performance of our method is 
much higher than that of the baseline, which 
shows our method is good at segmentation. We 
also note that UPP yields slightly better perfor-
mance than CPP on full-stop and mixed punctua-
tion prediction, and much better performance on 
middle-paused punctuation prediction. This could 
be because the interaction of parsing and punctu-
ation prediction is closer together in UPP than in 
CPP. 
5.2.2 Performance on automatically recog-
nized text 
Table 5 shows the experimental results of punctu-
ation prediction on automatically recognized text 
from TDT4 data that is recognized using SRI?s 
English broadcast news ASR system where the 
word error rate is estimated to be 18%. As the an-
notation of middle-paused punctuations in TDT4 
is not available, we can only evaluate the perfor-
mance of full-stop punctuation prediction (i.e., de-
tecting sentence boundaries). Thus, we merge 
every three sentences into one single input before 
performing full-stop prediction. The average input 
length is about 43 words. 
 
 Measure   Full-stop 
Baseline 
(CRF) 
prec. 37.7% 
rec. 60.7% 
F1 46.5% 
 
CPP 
prec. 63% 
rec. 58.6% 
F1 60.2% 
 
UPP 
 
prec. 73.9% 
rec. 51.6% 
F1 60.7% 
Table 5. Punctuation prediction performance on 
automatically recognized text 
Generally, the performance shown in Table 5 is 
not as high as that in Table 4. This is because the 
speech recognition error from ASR systems de-
grades the capability of model prediction. Another 
reason might be that the domain and style of our 
training data mismatch those of TDT4 data. The 
baseline gets a little higher recall than our method, 
which shows the baseline method tends to make 
aggressive segmentation decisions. However, 
both precision and F1 score of our method are 
much higher than the baseline. CPP has higher re-
call than UPP, but with lower precision and F1 
score. This is in line with Table 4, which consist-
ently illustrates CPP can get higher recall on full-
stop punctuation prediction for both correctly rec-
ognized and automatically recognized texts.  
5.2.3 Performance of transition-based pars-
ing 
Performance of parsing affects the quality of 
punctuation prediction in our work. In this section, 
we separately evaluate the performance of our 
transition-based parser over various domains in-
cluding the Wall Street Journal (WSJ), weblogs, 
newsgroups, answers, email messages and re-
views. We divided annotated Treebank data into 
three data sets: 90% for model training, 5% for the 
development set and 5% for the test set. The accu-
racy of our POS-tagger achieves 96.71%. The 
beam size in the decoding of both our POS-tag-
ging and parsing is set to 5. Table 6 presents the 
results of our experiments on the measures of 
UAS and LAS, where the overall accuracy is ob-
tained from a general model which is trained over 
the combination of the training data from all do-
mains.  
758
We first evaluate the performance of our transi-
tion-based parsing over texts containing punctua-
tions (TCP). The evaluation results show that our 
transition-based parser achieves state-of-the-art 
performance levels, referring to the best depend-
ency parsing results reported in the shared task of 
SANCL 2012 workshop2, although they cannot be 
compared directly due to the different training 
data and test data sets used in the experiments. 
Secondly, we evaluate our parsing model in CPP 
over the texts without punctuations (TOP). Sur-
prisingly, the performance over TOP is better than 
that over TCP. The reason could be that we 
cleaned out data noises caused by punctuations 
when preparing TOP data. These results illustrate 
that the performance of transition-based parsing in 
our method does not degrade after being inte-
grated with punctuation prediction. As a by-prod-
uct of the punctuation prediction task, the outputs 
of parsing trees can benefit the subsequent text 
processing tasks. 
 
 Data sets UAS LAS 
 
 
Texts con-
taining punc-
tuations 
(TCP) 
 
WSJ 92.6% 90.3% 
Weblogs 90.7% 88.2% 
Answers 89.4% 85.7% 
Newsgroups 90.1% 87.6% 
Reviews 90.9% 88.4% 
Email Messages 89.6% 87.1% 
Overall 90.5% 88% 
 
 
Texts with-
out punctua-
tions (TOP) 
WSJ 92.6% 91.1% 
Weblogs 92.5% 91.1% 
Answers 95% 94% 
Newsgroups 92.6% 91.2% 
Reviews 92.6% 91.2% 
Email Messages 92.9% 91.7% 
Overall 92.6% 91.2% 
Table 6. The performance of our transition-based 
parser on written texts. UAS=unlabeled attach-
ment score; LAS=labeled attachment score 
6 Conclusion and Future Work  
In this paper, we proposed a novel method for 
punctuation prediction of transcribed speech texts. 
Our approach jointly performs parsing and punc-
tuation prediction by integrating a rich set of syn-
tactic features. It can not only yield parse trees, but 
also determine sentence boundaries and predict 
punctuation symbols from a global view of the in-
                                                          
2 https://sites.google.com/site/sancl2012/home/shared-
task/results 
puts. The proposed algorithm has linear complex-
ity in the size of input, which can efficiently pro-
cess the stream of words from a purely text pro-
cessing perspective without the dependences on 
either the ASR systems or subsequent tasks. The 
experimental results show that our approach out-
performs the CRF-based method on both the cor-
rectly recognized and automatically recognized 
texts. In addition, the performance of the parsing 
over the stream of transcribed words is state-of-
the-art, which can benefit many subsequent text 
processing tasks. 
    In future work, we will try our method on other 
languages such as Chinese and Japanese, where 
Treebank data is available. We would also like to 
test the MT performance over transcribed speech 
texts with punctuation symbols inserted based on 
our method proposed in this paper.  
References 
B. Bohnet and J. Nivre. 2012. A transition-based sys-
tem for joint part-of-speech tagging and labeled 
non-projective dependency parsing. In Proc. 
EMNLP-CoNLL 2012. 
H. Christensen, Y. Gotoh, and S. Renals. 2001. Punc-
tuation annotation using statistical prosody models. 
In Proc. of ISCA Workshop on Prosody in Speech 
Recognition and Understanding. 
M. Collins. 2002. Discriminative training methods for 
hidden Markov models: Theory and experiments 
with perceptron algorithms. In Proc. EMNLP?02, 
pages 1-8. 
B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-
Tur, and M. Ostendorf. 2008. Punctuating speech 
for information extraction. In Proc. of ICASSP?08. 
B. Favre, D. HakkaniTur, S. Petrov and D. Klein. 2008. 
Efficient sentence segmentation using syntactic fea-
tures. In Spoken Language Technologies (SLT). 
A. Gravano, M. Jansche, and M. Bacchiani. 2009. Re-
storing punctuation and capitalization in transcribed 
speech. In Proc. of ICASSP?09. 
J. Hatori, T. Matsuzaki, Y. Miyao and J. Tsujii. 2011. 
Incremental joint POS tagging and dependency 
parsing in Chinese. In Proc. Of IJCNLP?11. 
J. Huang and G. Zweig. 2002. Maximum entropy 
model for punctuation annotation from speech. In 
Proc. Of ICSLP?02. 
759
J.H. Kim and P.C. Woodland. 2001. The use of pros-
ody in a combined system for punctuation genera-
tion and speech recognition. In Proc. of Eu-
roSpeech?01. 
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2005. 
Using conditional random fields for sentence 
boundary detection in speech. In Proc. of ACL?05. 
W. Lu and H.T. Ng. 2010. Better Punctuation Predic-
tion with Dynamic Conditional Random Fields. In 
Proc. Of EMNLP?10. Pages 177-186. 
M. Marneffe, B. MacCartney, C.D. Maning. 2006. 
Generating Typed Dependency Parses from Phrase 
Structure Parses. In Proc. LREC?06. 
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic 
sentence segmentation and punctuation prediction 
for spoken language translation. In Proc. of 
IWSLT?06. 
S. Nakamura. 2009. Overcoming the language barrier 
with speech translation technology. In Science & 
Technology Trends - Quarterly Review. No. 31. 
April 2009. 
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of IWPT, pages 
149?160, Nancy, France. 
J. Nivre and M. Scholz. 2004. Deterministic depend-
ency parsing of English text. In Proc. COLING?04. 
M. Paul. 2009. Overview of the IWSLT 2009 Evalua-
tion Campaign. In Proceedings of IWSLT?09. 
B. Roark, Y. Liu, M. Harper, R. Stewart, M. Lease, M. 
Snover, I. Shafran, B. Dorr, J. Hale, A. Krasnyan-
skaya, and L. Yung. 2006. Reranking for sentence 
boundary detection in conversational speech. In 
Proc. ICASSP, 2006. 
A. Stolcke and E. Shriberg, ?Automatic linguistic seg-
mentation of conversational speech,? Proc. ICSLP, 
vol. 2, 1996. 
A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf, D. 
Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Au-
tomatic detection of sentence boundaries and disflu-
encies based on recognized words. In Proc. of 
ICSLP? 98. 
Takezawa, T. Morimoto, T. Sagisaka, Y. Campbell, N. 
Iida, H. Sugaya, F. Yokoo, A. Yamamoto, Seiichi. 
1998. A Japanese-to-English speech translation sys-
tem: ATR-MATRIX.  In Proc. ICSLP?98. 
Y. Zhang and J. Nivre. 2011. Transition-based De-
pendency Parsing with Rich Non-local Features. In 
Proc. of ACL?11, pages 188-193. 
Y. Zhang and S. Clark. A Tale of Two Parsers: inves-
tigating and combing graph-based and transition-
based dependency parsing using beam-search. 2008. 
In Proc. of EMNLP?08, pages 562-571. 
760
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110?114,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Easy-First POS Tagging and Dependency Parsing with Beam Search 
Ji Ma?   JingboZhu?  Tong Xiao?   Nan Yang? 
?Natrual Language Processing Lab., Northeastern University, Shenyang, China 
?MOE-MS Key Lab of MCC, University of Science and Technology of China, 
Hefei, China 
majineu@outlook.com 
{zhujingbo, xiaotong}@mail.neu.edu.cn 
nyang.ustc@gmail.com 
 
Abstract 
In this paper, we combine easy-first de-
pendency parsing and POS tagging algo-
rithms with beam search and structured 
perceptron. We propose a simple variant 
of ?early-update? to ensure valid update 
in the training process. The proposed so-
lution can also be applied to combine 
beam search and structured perceptron 
with other systems that exhibit spurious 
ambiguity. On CTB, we achieve 94.01% 
tagging accuracy and 86.33% unlabeled 
attachment score with a relatively small 
beam width. On PTB, we also achieve 
state-of-the-art performance. 
1 Introduction 
The easy-first dependency parsing algorithm 
(Goldberg and Elhadad, 2010) is attractive due to 
its good accuracy, fast speed and simplicity. The 
easy-first parser has been applied to many appli-
cations (Seeker et al, 2012; S?ggard and Wulff, 
2012). By processing the input tokens in an easy-
to-hard order, the algorithm could make use of 
structured information on both sides of the hard 
token thus making more indicative predictions. 
However, rich structured information also causes 
exhaustive inference intractable. As an alterna-
tive, greedy search which only explores a tiny 
fraction of the search space is adopted (Goldberg 
and Elhadad, 2010). 
 To enlarge the search space, a natural exten-
sion to greedy search is beam search. Recent 
work also shows that beam search together with 
perceptron-based global learning (Collins, 2002) 
enable the use of non-local features that are help-
ful to improve parsing performance without 
overfitting (Zhang and Nivre, 2012). Due to the-
se advantages, beam search and global learning 
has been applied to many NLP tasks (Collins and 
Roark 2004; Zhang and Clark, 2007). However, 
to the best of our knowledge, no work in the lit-
erature has ever applied the two techniques to 
easy-first dependency parsing.  
While applying beam-search is relatively 
straightforward, the main difficulty comes from 
combining easy-first dependency parsing with 
perceptron-based global learning. In particular, 
one needs to guarantee that each parameter up-
date is valid, i.e., the correct action sequence has 
lower model score than the predicted one1. The 
difficulty in ensuring validity of parameter up-
date for the easy-first algorithm is caused by its 
spurious ambiguity, i.e., the same result might be 
derived by more than one action sequences.  
For algorithms which do not exhibit spurious 
ambiguity, ?early update? (Collins and Roark 
2004) is always valid: at the k-th step when the 
single correct action sequence falls off the beam, 
                                                 
1 As shown by (Huang et al, 2012), only valid update guar-
antees the convergence of any perceptron-based training. 
Invalid update may lead to bad learning or even make the 
learning not converge at all. 
Figure 1: Example of cases without/with spurious 
ambiguity. The 3 ? 1 table denotes a beam. ?C/P? 
denotes correct/predicted action sequence. The 
numbers following C/P are model scores. 
 
110
its model score must be lower than those still in 
the beam (as illustrated in figure 1, also see the 
proof in (Huang et al, 2012)). While for easy-
first dependency parsing, there could be multiple 
action sequences that yield the gold result (C1 and 
C2 in figure 1). When all correct sequences fall 
off the beam, some may indeed have higher 
model score than those still in the beam (C2 in 
figure 1), causing invalid update. 
For the purpose of valid update, we present a 
simple solution which is based on early update. 
The basic idea is to use one of the correct action 
sequences that were pruned right at the k-th step 
(C1 in figure 1) for parameter update.  
The proposed solution is general and can also 
be applied to other algorithms that exhibit spuri-
ous ambiguity, such as easy-first POS tagging 
(Ma et al, 2012) and transition-based dependen-
cy parsing with dynamic oracle (Goldberg and 
Nivre, 2012). In this paper, we report experi-
mental results on both easy-first dependency 
parsing and POS tagging (Ma et al, 2012). We 
show that both easy-first POS tagging and de-
pendency parsing can be improved significantly 
from beam search and global learning. Specifi-
cally, on CTB we achieve 94.01% tagging accu-
racy which is the best result to date2 for a single 
tagging model. With a relatively small beam, we 
achieve 86.33% unlabeled score (assume gold 
tags), better than state-of-the-art transition-based 
parsers (Huang and Sagae, 2010; Zhang and 
Nivre, 2011). On PTB, we also achieve good 
results that are comparable to the state-of-the-art. 
2 Easy-first dependency parsing 
The easy-first dependency parsing algorithm 
(Goldberg and Elhadad, 2010) builds a depend-
ency tree by performing two types of actions 
LEFT(i) and RIGHT(i) to a list of sub-tree struc-
tures p1,?, pr. pi is initialized with the i-th word  
                                                 
2 Joint tagging-parsing models achieve higher accuracy, but 
those models are not directly comparable to ours.  
Algorithm 1: Easy-first with beam search 
Input:     sentence   of n words,  beam width s 
Output:  one best dependency tree 
     (     )        
         ( )   
    (  ) 
            // top s extensions from the beam 
1                     // initially, empty beam 
2 for    1   1 do 
3             (        ) 
4 return        ( )   // tree built by the best sequence  
 
of the input sentence. Action LEFT(i)/RIGHT(i) 
attaches pi to its left/right neighbor and then re-
moves pi from the sub-tree list. The algorithm 
proceeds until only one sub-tree left which is the 
dependency tree of the input sentence (see the 
example in figure 2). Each step, the algorithm 
chooses the highest score action to perform ac-
cording to the linear model: 
     ( )     ( ) 
Here,  is the weight vector and  is the feature 
representation. In particular,  (    ( ) 
     ( )) denotes features extracted from pi. 
The parsing algorithm is greedy which ex-
plores a tiny fraction of the search space. Once 
an incorrect action is selected, it can never yield 
the correct dependency tree. To enlarge the 
search space, we introduce the beam-search ex-
tension in the next section. 
3 Easy-first with beam search  
In this section, we introduce easy-first with beam 
search in our own notations that will be used 
throughout the rest of this paper.  
For a sentence x of n words, let   be the action 
(sub-)sequence that can be applied, in sequence, 
to x and the result sub-tree list is denoted by 
 ( )  For example, suppose x is ?I am valid? and 
y is [RIGHT(1)], then y(x) yields figure 2(b). Let 
   to be LEFT(i)/RIGHT(i) actions where    1   . 
Thus, the set of all possible one-action extension 
of   is: 
     ( )            ( )   
Here, ? ? means insert   to the end of  . Follow-
ing (Huang et al, 2012), in order to formalize 
beam search, we also use the          
    ( ) 
operation which returns the top s action sequenc-
es in   according to   ( ). Here,  denotes a 
set of action sequences,   ( ) denotes the sum of 
feature vectors of each action in    
Pseudo-code of easy-first with beam search is 
shown in algorithm 1. Beam search grows s 
(beam width) action sequences in parallel using a  
Figure 2: An example of parsing ?I am valid?. Spu-
rious ambiguity: (d) can be derived by both 
[RIGHT(1), LEFT(2)] and [LEFT(3), RIGHT(1)]. 
111
Algorithm 2: Perceptron-based training over one 
training sample (   ) 
Input:    (   ), s, parameter   
Output: new parameter    
    (       )        
     (      ( ))   
   (  ) 
 // top correct extension from the beam 
1         
2 for    1   1 do 
3     ?      (          ) 
4            (        ) 
5    if           // all correct seq. falls off the beam 
6             ( ?)   (     ) 
7         break 
8 if        ( )      // full update 
9         ( ?)   (       )  
10 return  
 
beam  , (sequences in   are sorted in terms of 
model score, i.e.,   (    )     (  1 ) ). 
At each step, the sequences in   are expanded in 
all possible ways and then   is filled up with the 
top s newly expanded sequences (line 2 ~ line 3). 
Finally, it returns the dependency tree built by 
the top action sequence in      . 
4 Training  
To learn the weight vector , we use the percep-
tron-based global learning3 (Collins, 2002) which 
updates  by rewarding the feature weights fired 
in the correct action sequence and punish those 
fired in the predicted incorrect action sequence. 
Current work (Huang et al, 2012) rigorously 
explained that only valid update ensures conver-
gence of any perceptron variants. They also justi-
fied that the popular ?early update? (Collins and  
Roark, 2004) is valid for the systems that do not 
exhibit spurious ambiguity4.  
However, for the easy-first algorithm or more 
generally, systems that exhibit spurious ambigui-
ty, even ?early update? could fail to ensure valid-
ity of update (see the example in figure 1). For 
validity of update, we propose a simple solution 
which is based on ?early update? and which can 
accommodate spurious ambiguity. The basic idea 
is to use the correct action sequence which was  
                                                 
3 Following (Zhang and Nivre, 2012), we say the training 
algorithm is global if it optimizes the score of an entire ac-
tion sequence. A local learner trains a classifier which dis-
tinguishes between single actions. 
4 As shown in (Goldberg and Nivre 2012), most transition-
based dependency parsers (Nivre et al, 2003; Huang and 
Sagae 2010;Zhang and Clark 2008) ignores spurious ambi-
guity by using a static oracle which maps a dependency tree 
to a single action sequence.  
Features of (Goldberg and Elhadad, 2010) 
for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp,  
tp-vrp, tlcp, trcp, wlcp, wlcp 
for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp,  tp-trcp, tp-tlcp-trcp 
for p, q, r in (pi-2, pi-1, pi), (pi-
1, pi+1, pi), (pi+1, pi+2 ,pi) 
tp-tq-tr, tp-tq-wr 
for p, q in (pi-1, pi) tp-tlcp-tq,   tp-trcp-tq,   ,tp-tlcp-wq,, 
 tp-trcp-wq,   tp-wq-tlcq,  tp-wq-trcq 
 
Table 1: Feature templates for English dependency 
parsing. wp denotes the head word of p, tp denotes the 
POS tag of wp. vlp/vrp denotes the number p?s of 
left/right child. lcp/rcp denotes p?s leftmost/rightmost 
child. pi denotes partial tree being considered. 
 
pruned right at the step when all correct sequence 
falls off the beam (as C1 in figure 1).  
Algorithm 2 shows the pseudo-code of the 
training procedure over one training sample 
(   ), a sentence-tree pair. Here we assume   to 
be the set of all correct action sequences/sub-
sequences. At step k, the algorithm constructs a 
correct action sequence  ? of length k by extend-
ing those in      (line 3). It also checks whether 
   no longer contains any correct sequence. If so, 
 ? together with       are used for parameter up-
date (line 5 ~ line 6). It can be easily verified that 
each update in line 6 is valid. Note that both 
?TOPC? and the operation in line 5 use   to check 
whether an action sequence y is correct or not. 
This  can  be  efficiently  implemented   (without 
explicitly enumerating  ) by checking if each 
LEFT(i)/RIGHT(i) in y are compatible with (   ): 
pi already collected all its dependents according 
to t; pi is attached to the correct neighbor sug-
gested by t.  
5 Experiments 
For English, we use PTB as our data set. We use 
the standard split for dependency parsing and the 
split used by (Ratnaparkhi, 1996) for POS tag-
ging. Penn2Malt5 is used to convert the bracket-
ed structure into dependencies. For dependency 
parsing, POS tags of the training set are generat-
ed using 10-fold jack-knifing.  
For Chinese, we use CTB 5.1 and the split 
suggested by (Duan et al, 2007) for both tagging 
and dependency parsing. We also use Penn2Malt 
and the head-finding rules of (Zhang and Clark 
2008) to convert constituency trees into depend-
encies. For dependency parsing, we assume gold 
segmentation and POS tags for the input.  
                                                 
5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 
112
Features used in English dependency parsing 
are listed in table 1. Besides the features in 
(Goldberg and Elhadad, 2010), we also include 
some trigram features and valency features 
which are useful for transition-based dependency 
parsing (Zhang and Nivre, 2011). For English 
POS tagging, we use the same features as in 
(Shen et al, 2007). For Chinese POS tagging and 
dependency parsing, we use the same features as 
(Ma et al, 2012). All of our experiments are 
conducted on a Core i7 (2.93GHz) machine, both 
the tagger and parser are implemented using C++.  
5.1 Effect of beam width 
Tagging/parsing performances with different 
beam widths on the development set are listed in 
table 2 and table 3. We can see that Chinese POS  
tagging, dependency parsing as well as English 
dependency parsing greatly benefit from beam 
search. While tagging accuracy on English only 
slightly improved. This may because that the 
accuracy of the greedy baseline tagger is already 
very high and it is hard to get further improve-
ment. Table 2 and table 3 also show that the 
speed of both tagging and dependency parsing 
drops linearly with the growth of beam width. 
5.2 Final results 
Tagging results on the test set together with some 
previous results are listed in table 4. Dependency 
parsing results on CTB and PTB are listed in ta-
ble 5 and table 6, respectively. 
On CTB, tagging accuracy of our greedy base-
line is already comparable to the state-of-the-art. 
As the beam size grows to 5, tagging accuracy 
increases to 94.01% which is 2.3% error reduc-
tion. This is also the best tagging accuracy com-
paring with previous single tagging models (For 
limited space, we do not list the performance of 
joint tagging-parsing models).  
Parsing performances on both PTB and CTB 
are significantly improved with a relatively small 
beam width (s = 8). In particular, we achieve 
86.33% uas on CTB which is 1.54% uas im-
provement over the greedy baseline parser. 
Moreover, the performance is better than the best 
transition-based parser (Zhang and Nivre, 2011) 
which adopts a much larger beam width (s = 64).  
6 Conclusion and related work 
This work directly extends (Goldberg and El-
hadad, 2010) with beam search and global learn-
ing. We show that both the easy-first POS tagger 
and dependency parser can be significantly impr- 
s PTB CTB speed  
1 97.17 93.91 1350 
3 97.20 94.15 560 
5 97.22 94.17 385 
 
Table 2: Tagging accuracy vs beam width vs. Speed is 
evaluated using the number of sentences that can be 
processed in one second 
 
s 
PTB CTB 
speed 
uas compl uas compl 
1 91.77 45.29 84.54 33.75 221 
2 92.29 46.28 85.11 34.62 124 
4 92.50 46.82 85.62 37.11 71 
8 92.74 48.12 86.00 35.87 39 
 
Table 3: Parsing accuracy vs beam width. ?uas? and 
?compl? denote unlabeled score and complete match 
rate respectively (all excluding punctuations). 
 
PTB CTB 
(Collins, 2002) 97.11 (Hatori et al, 2012) 93.82 
(Shen et al, 2007) 97.33 (Li et al, 2012) 93.88 
(Huang et al, 2012) 97.35 (Ma et al, 2012) 93.84 
this work   1 97.22 this work   1 93.87 
this work     97.28 this work     94.01? 
 
Table 4: Tagging results on the test set. ??? denotes 
statistically significant over the greedy baseline by 
McNemar?s test (      ) 
 
Systems s uas compl 
(Huang and Sagae, 2010) 8 85.20 33.72 
(Zhang and Nivre, 2011) 64 86.00 36.90 
(Li et al, 2012) ? 86.55 ? 
this work 1 84.79 32.98 
this work 8 86.33
?
 36.13 
 
Table 5: Parsing results on CTB test set. 
  
Systems s uas compl 
(Huang and Sagae, 2010) 8 92.10 ? 
(Zhang and Nivre, 2011) 64 92.90 48.50 
(Koo and Collins, 2010) ? 93.04 ? 
this work 1 91.72 44.04 
this work 8 92.47
?
 46.07 
 
Table 6: Parsing results on PTB test set.  
 
oved using beam search and global learning. 
This work can also be considered as applying 
(Huang et al, 2012) to the systems that exhibit 
spurious ambiguity. One future direction might 
be to apply the training method to transition-
based parsers with dynamic oracle (Goldberg and 
Nivre, 2012) and potentially further advance per-
formances of state-of-the-art transition-based 
parsers. 
113
Shen et al, (2007) and (Shen and Joshi, 2008) 
also proposed bi-directional sequential classifica-
tion with beam search for POS tagging and 
LTAG dependency parsing, respectively. The 
main difference is that their training method aims 
to learn a classifier which distinguishes between 
each local action while our training method aims 
to distinguish between action sequences. Our 
method can also be applied to their framework. 
Acknowledgments 
We would like to thank Yue Zhang, Yoav Gold-
berg and Zhenghua Li for discussions and sug-
gestions on earlier drift of this paper. We would 
also like to thank the three anonymous reviewers 
for their suggestions. This work was supported in 
part by the National Science Foundation of Chi-
na (61073140; 61272376), Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion (20100042110031) and the Fundamental 
Research Funds for the Central Universities 
(N100204002). 
References  
Collins, M. 2002. Discriminative training methods for 
hidden markov models: Theory and experiments 
with perceptron algorithms. In Proceedings of 
EMNLP. 
Duan, X., Zhao, J., , and Xu, B. 2007. Probabilistic 
models for action-based Chinese dependency pars-
ing. In Proceedings of ECML/ECPPKDD. 
Goldberg, Y. and Elhadad, M. 2010 An Efficient Al-
gorithm for Eash-First Non-Directional Dependen-
cy Parsing. In Proceedings of NAACL 
Huang, L. and Sagae, K. 2010. Dynamic program-
ming for linear-time incremental parsing. In Pro-
ceedings of ACL. 
Huang, L. Fayong, S. and Guo, Y. 2012. Structured 
Perceptron with Inexact Search. In Proceedings of 
NAACL. 
Koo, T. and Collins, M. 2010. Efficient third-order 
dependency parsers. In Proceedings of ACL. 
Li, Z., Zhang, M., Che, W., Liu, T. and Chen, W. 
2012. A Separately Passive-Aggressive Training 
Algorithm for Joint POS Tagging and Dependency 
Parsing. In Proceedings of COLING 
Ma, J., Xiao, T., Zhu, J. and Ren, F. 2012. Easy-First 
Chinese POS Tagging and Dependency Parsing. In 
Proceedings of COLING 
Rataparkhi, A. (1996) A Maximum Entropy Part-Of-
Speech Tagger. In Proceedings of EMNLP 
Shen, L., Satt, G. and Joshi, A. K. (2007) Guided 
Learning for Bidirectional Sequence Classification. 
In Proceedings of ACL. 
Shen, L. and  Josh, A. K. 2008. LTAG Dependency 
Parsing with Bidirectional Incremental Construc-
tion. In Proceedings of  EMNLP. 
Seeker, W., Farkas, R. and Bohnet, B. 2012 Data-
driven Dependency Parsing With Empty Heads. In 
Proceedings of COLING 
S?ggard, A. and Wulff, J. 2012. An Empirical Study 
of Non-lexical Extensions to Delexicalized Trans-
fer. In Proceedings of COLING 
Yue Zhang and Stephen Clark. 2007 Chinese Seg-
mentation Using a Word-based Perceptron Algo-
rithm. In Proceedings of ACL.  
Zhang, Y. and Clark, S. 2008. Joint word segmenta-
tion and POS tagging using a single perceptron. In 
Proceedings of ACL. 
Zhang, Y. and Nivre, J. 2011. Transition-based de-
pendency parsing with rich non-local features. In 
Proceedings of ACL. 
Zhang, Y. and Nivre, J. 2012. Analyzing the Effect of 
Global Learning and Beam-Search for Transition-
Based Dependency Parsing. In Proceedings of 
COLING. 
114
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491?1500,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
A Recursive Recurrent Neural Network
for Statistical Machine Translation
Shujie Liu
1
, Nan Yang
2
, Mu Li
1
and Ming Zhou
1
1
Microsoft Research Asia, Beijing, China
2
University of Science and Technology of China, Hefei, China
shujliu, v-nayang, muli, mingzhou@microsoft.com
Abstract
In this paper, we propose a novel recursive
recurrent neural network (R
2
NN) to mod-
el the end-to-end decoding process for s-
tatistical machine translation. R
2
NN is a
combination of recursive neural network
and recurrent neural network, and in turn
integrates their respective capabilities: (1)
new information can be used to generate
the next hidden state, like recurrent neu-
ral networks, so that language model and
translation model can be integrated natu-
rally; (2) a tree structure can be built, as
recursive neural networks, so as to gener-
ate the translation candidates in a bottom
up manner. A semi-supervised training ap-
proach is proposed to train the parameter-
s, and the phrase pair embedding is ex-
plored to model translation confidence di-
rectly. Experiments on a Chinese to En-
glish translation task show that our pro-
posed R
2
NN can outperform the state-
of-the-art baseline by about 1.5 points in
BLEU.
1 Introduction
Deep Neural Network (DNN), which essential-
ly is a multi-layer neural network, has re-gained
more and more attentions these years. With the
efficient training methods, such as (Hinton et al,
2006), DNN is widely applied to speech and im-
age processing, and has achieved breakthrough re-
sults (Kavukcuoglu et al, 2010; Krizhevsky et al,
2012; Dahl et al, 2012).
Applying DNN to natural language processing
(NLP), representation or embedding of words is
usually learnt first. Word embedding is a dense,
low dimensional, real-valued vector. Each dimen-
sion of the vector represents a latent aspect of
the word, and captures its syntactic and semantic
properties (Bengio et al, 2006). Word embedding
is usually learnt from large amount of monolin-
gual corpus at first, and then fine tuned for spe-
cial distinct tasks. Collobert et al (2011) propose
a multi-task learning framework with DNN for
various NLP tasks, including part-of-speech tag-
ging, chunking, named entity recognition, and se-
mantic role labelling. Recurrent neural networks
are leveraged to learn language model, and they
keep the history information circularly inside the
network for arbitrarily long time (Mikolov et al,
2010). Recursive neural networks, which have the
ability to generate a tree structured output, are ap-
plied to natural language parsing (Socher et al,
2011), and they are extended to recursive neural
tensor networks to explore the compositional as-
pect of semantics (Socher et al, 2013).
DNN is also introduced to Statistical Machine
Translation (SMT) to learn several components
or features of conventional framework, includ-
ing word alignment, language modelling, transla-
tion modelling and distortion modelling. Yang et
al. (2013) adapt and extend the CD-DNN-HMM
(Dahl et al, 2012) method to HMM-based word
alignment model. In their work, bilingual word
embedding is trained to capture lexical translation
information, and surrounding words are utilized to
model context information. Auli et al (2013) pro-
pose a joint language and translation model, based
on a recurrent neural network. Their model pre-
dicts a target word, with an unbounded history of
both source and target words. Liu et al (2013) pro-
pose an additive neural network for SMT decod-
ing. Word embedding is used as the input to learn
translation confidence score, which is combined
with commonly used features in the convention-
al log-linear model. For distortion modeling, Li
et al (2013) use recursive auto encoders to make
full use of the entire merging phrase pairs, going
beyond the boundary words with a maximum en-
tropy classifier (Xiong et al, 2006).
1491
Different from the work mentioned above,
which applies DNN to components of conven-
tional SMT framework, in this paper, we propose
a novel R
2
NN to model the end-to-end decod-
ing process. R
2
NN is a combination of recursive
neural network and recurrent neural network. In
R
2
NN, new information can be used to generate
the next hidden state, like recurrent neural net-
works, and a tree structure can be built, as recur-
sive neural networks. To generate the translation
candidates in a commonly used bottom-up man-
ner, recursive neural networks are naturally adopt-
ed to build the tree structure. In recursive neural
networks, all the representations of nodes are gen-
erated based on their child nodes, and it is difficult
to integrate additional global information, such as
language model and distortion model. In order to
integrate these crucial information for better trans-
lation prediction, we combine recurrent neural net-
works into the recursive neural networks, so that
we can use global information to generate the next
hidden state, and select the better translation can-
didate.
We propose a three-step semi-supervised train-
ing approach to optimizing the parameters of
R
2
NN, which includes recursive auto-encoding
for unsupervised pre-training, supervised local
training based on the derivation trees of forced de-
coding, and supervised global training using ear-
ly update strategy. So as to model the transla-
tion confidence for a translation phrase pair, we
initialize the phrase pair embedding by leveraging
the sparse features and recurrent neural network.
The sparse features are phrase pairs in translation
table, and recurrent neural network is utilized to
learn a smoothed translation score with the source
and target side information. We conduct exper-
iments on a Chinese-to-English translation task
to test our proposed methods, and we get about
1.5 BLEU points improvement, compared with a
state-of-the-art baseline system.
The rest of this paper is organized as follows:
Section 2 introduces related work on applying
DNN to SMT. Our R
2
NN framework is introduced
in detail in Section 3, followed by our three-step
semi-supervised training approach in Section 4.
Phrase pair embedding method using translation
confidence is elaborated in Section 5. We intro-
duce our conducted experiments in Section 6, and
conclude our work in Section 7.
2 Related Work
Yang et al (2013) adapt and extend CD-DNN-
HMM (Dahl et al, 2012) to word alignment.
In their work, initial word embedding is firstly
trained with a huge mono-lingual corpus, then the
word embedding is adapted and fine tuned bilin-
gually in a context-depended DNN HMM frame-
work. Word embeddings capturing lexical trans-
lation information and surrounding words model-
ing context information are leveraged to improve
the word alignment performance. Unfortunately,
the better word alignment result generated by this
model, cannot bring significant performance im-
provement on a end-to-end SMT evaluation task.
To improve the SMT performance directly, Auli
et al (2013) extend the recurrent neural network
language model, in order to use both the source
and target side information to scoring translation
candidates. In their work, not only the target word
embedding is used as the input of the network, but
also the embedding of the source word, which is
aligned to the current target word. To tackle the
large search space due to the weak independence
assumption, a lattice algorithm is proposed to re-
rank the n-best translation candidates, generated
by a given SMT decoder.
Liu et al (2013) propose an additive neural net-
work for SMT decoding. RNNLM (Mikolov et al,
2010) is firstly used to generate the source and tar-
get word embeddings, which are fed into a one-
hidden-layer neural network to get a translation
confidence score. Together with other common-
ly used features, the translation confidence score
is integrated into a conventional log-linear model.
The parameters are optimized with developmen-
t data set using mini-batch conjugate sub-gradient
method and a regularized ranking loss.
DNN is also brought into the distortion mod-
eling. Going beyond the previous work using
boundary words for distortion modeling in BTG-
based SMT decoder, Li et al (2013) propose to ap-
ply recursive auto-encoder to make full use of the
entire merged blocks. The recursive auto-encoder
is trained with reordering examples extracted from
word-aligned bilingual sentences. Given the rep-
resentations of the smaller phrase pairs, recursive
auto-encoder can generate the representation of
the parent phrase pair with a re-ordering confi-
dence score. The combination of reconstruction
error and re-ordering error is used to be the objec-
tive function for the model training.
1492
3 Our Model
In this section, we leverage DNN to model the
end-to-end SMT decoding process, using a novel
recursive recurrent neural network (R
2
NN), which
is different from the above mentioned work ap-
plying DNN to components of conventional SMT
framework. R
2
NN is a combination of recur-
sive neural network and recurrent neural network,
which not only integrates the conventional glob-
al features as input information for each combina-
tion, but also generates the representation of the
parent node for the future candidate generation.
In this section, we briefly recall the recurren-
t neural network and recursive neural network in
Section 3.1 and 3.2, and then we elaborate our
R
2
NN in detail in Section 3.3.
3.1 Recurrent Neural Network
Recurrent neural network is usually used for
sequence processing, such as language model
(Mikolov et al, 2010). Commonly used sequence
processing methods, such as Hidden Markov
Model (HMM) and n-gram language model, only
use a limited history for the prediction. In HMM,
the previous state is used as the history, and for n-
gram language model (for example n equals to
3), the history is the previous two words. Recur-
rent neural network is proposed to use unbounded
history information, and it has recurrent connec-
tions on hidden states, so that history information
can be used circularly inside the network for arbi-
trarily long time.
??
?
???1
??
??
??
Figure 1: Recurrent neural network
As shown in Figure 1, the network contains
three layers, an input layer, a hidden layer, and an
output layer. The input layer is a concatenation of
h
t?1
and x
t
, where h
t?1
is a real-valued vec-
tor, which is the history information from time 0
to t? 1. x
t
is the embedding of the input word at
time t . Word embedding x
t
is integrated with
previous history h
t?1
to generate the current hid-
den layer, which is a new history vector h
t
. Based
on h
t
, we can predict the probability of the next
word, which forms the output layer y
t
. The new
history h
t
is used for the future prediction, and
updated with new information from word embed-
ding x
t
recurrently.
3.2 Recursive Neural Network
In addition to the sequential structure above, tree
structure is also usually constructed in various
NLP tasks, such as parsing and SMT decoding.
To generate a tree structure, recursive neural net-
works are introduced for natural language parsing
(Socher et al, 2011). Similar with recurrent neural
networks, recursive neural networks can also use
unbounded history information from the sub-tree
rooted at the current node. The commonly used
binary recursive neural networks generate the rep-
resentation of the parent node, with the represen-
tations of two child nodes as the input.
?[?,?] ?[?,?]
?[?,?]
?
?[?,?]
Figure 2: Recursive neural network
As shown in Figure 2, s
[l,m]
and s
[m,n]
are
the representations of the child nodes, and they are
concatenated into one vector to be the input of the
network. s
[l,n]
is the generated representation of
the parent node. y
[l,n]
is the confidence score of
how plausible the parent node should be created.
l,m, n are the indexes of the string. For example,
for nature language parsing, s
[l,n]
is the represen-
tation of the parent node, which could be a NP or
V P node, and it is also the representation of the
whole sub-tree covering from l to n .
3.3 Recursive Recurrent Neural Network
Word embedding x
t
is integrated as new input
information in recurrent neural networks for each
prediction, but in recursive neural networks, no ad-
ditional input information is used except the two
representation vectors of the child nodes. How-
ever, some global information , which cannot be
generated by the child representations, is crucial
1493
for SMT performance, such as language model s-
core and distortion model score. So as to integrate
such global information, and also keep the ability
to generate tree structure, we combine the recur-
rent neural network and the recursive neural net-
work to be a recursive recurrent neural network
(R
2
NN).
?[?,?] ?[?,?]
?[?,?]
?[?,?] ?[?,?]
?
?[?,?]
??[?,?]
Figure 3: Recursive recurrent neural network
As shown in Figure 3, based on the recursive
network, we add three input vectors x
[l,m]
for
child node [l,m] , x
[m,n]
for child node [m,n] ,
and x
[l,n]
for parent node [l, n] . We call them
recurrent input vectors, since they are borrowed
from recurrent neural networks. The two recurrent
input vectors x
[l,m]
and x
[m,n]
are concatenat-
ed as the input of the network, with the original
child node representations s
[l,m]
and s
[m,n]
. The
recurrent input vector x
[l,n]
is concatenated with
parent node representation s
[l,n]
to compute the
confidence score y
[l,n]
.
The input, hidden and output layers are calcu-
lated as follows:
x?
[l,n]
= x
[l,m]
./ s
[l,m]
./ x
[m,n]
./ s
[m,n]
(1)
s
[l,n]
j
= f(
?
i
x?
[l,n]
i
w
ji
) (2)
y
[l,n]
=
?
j
(s
[l,n]
./ x
[l,n]
)
j
v
j
(3)
where ./ is a concatenation operator in Equation
1 and Equation 3, and f is a non-linear function,
here we use HTanh function, which is defined
as:
HTanh(x) =
?
?
?
?
?
?1, x < ?1
x, ?1 ? x ? 1
1, x > 1
(4)
Figure 4 illustrates the R
2
NN architecture for
SMT decoding. For a source sentence ?laizi faguo
he eluosi de?, we first split it into phrases ?laiz-
i?, ?faguo he eluosi? and ?de?. We then check
whether translation candidates can be found in the
translation table for each span, together with the
phrase pair embedding and recurrent input vec-
tor (global features). We call it the rule match-
ing phase. For a translation candidate of the s-
pan node [l,m] , the black dots stand for the node
representation s
[l,m]
, while the grey dots for re-
current input vector x
[l,m]
. Given s
[l,m]
and
x
[l,m]
for matched translation candidates, conven-
tional CKY decoding process is performed using
R
2
NN. R
2
NN can combine the translation pairs
of child nodes, and generate the translation can-
didates for parent nodes with their representations
and plausible scores. Only the n-best translation
candidates are kept for upper combination, accord-
ing to their plausible scores.
??
laizi
??????
faguo he eluosi
?
de
coming from France and Russia NULL
Rule Match Rule Match Rule Match
coming from France and Russia
R2NN
coming from France and Russia
R2NN
Figure 4: R
2
NN for SMT decoding
We extract phrase pairs using the conventional
method (Och and Ney, 2004). The commonly used
features, such as translation score, language mod-
el score and distortion score, are used as the recur-
rent input vector x . During decoding, recurrent
input vectors x for internal nodes are calculat-
ed accordingly. The difference between our model
and the conventional log-linear model includes:
? R
2
NN is not linear, while the conventional
model is a linear combination.
? Representations of phrase pairs are automat-
ically learnt to optimize the translation per-
formance, while features used in convention-
al model are hand-crafted.
? History information of the derivation can be
recorded in the representation of internal n-
odes, while conventional model cannot.
1494
Liu et al (2013) apply DNN to SMT decoding,
but not in a recursive manner. A feature is learn-
t via a one-hidden-layer neural network, and the
embedding of words in the phrase pairs are used
as the input vector. Our model generates the rep-
resentation of a translation pair based on its child
nodes. Li et al (2013) also generate the repre-
sentation of phrase pairs in a recursive way. In
their work, the representation is optimized to learn
a distortion model using recursive neural network,
only based on the representation of the child n-
odes. Our R
2
NN is used to model the end-to-end
translation process, with recurrent global informa-
tion added. We also explore phrase pair embed-
ding method to model translation confidence di-
rectly, which is introduced in Section 5.
In the next two sections, we will answer the fol-
lowing questions: (a) how to train the model, and
(b) how to generate the initial representations of
translation pairs.
4 Model Training
In this section, we propose a three-step training
method to train the parameters of our proposed
R
2
NN, which includes unsupervised pre-training
using recursive auto-encoding, supervised local
training on the derivation tree of forced decoding,
and supervised global training using early update
training strategy.
4.1 Unsupervised Pre-training
We adopt the Recursive Auto Encoding (RAE)
(Socher et al, 2011) for our unsupervised pre-
training. The main idea of auto encoding is to
initialize the parameters of the neural network,
by minimizing the information lost, which means,
capturing as much information as possible in the
hidden states from the input vector.
As shown in Figure 5, RAE contains two part-
s, an encoder with parameter W , and a decoder
with parameter W
?
. Given the representations of
child nodes s
1
and s
2
, the encoder generates the
representation of parent node s . With the parent
node representation s as the input vector, the de-
coder reconstructs the representation of two child
nodes s
?
1
and s
?
2
. The loss function is defined as
following so as to minimize the information lost:
L
RAE
(s
1
, s
2
) =
1
2
(
?
?
s
1
? s
?
1
?
?
2
+
?
?
s
2
? s
?
2
?
?
2
)
(5)
where ??? is the Euclidean norm.
coming from France and Russia
??
laizi
??????
faguo he eluosi
coming from France and Russia
coming from France and Russia
?1 ?2
?
?1? ?2?
?
??
Figure 5: Recursive auto encoding for unsuper-
vised pre-training
The training samples for RAE are phrase pairs
{s
1
, s
2
} in translation table, where s
1
and
s
2
can form a continuous partial sentence pair in
the training data. When RAE training is done, on-
ly the encoding model W will be fine tuned in
the future training phases.
4.2 Supervised Local Training
We use contrastive divergence method to fine tune
the parameters W and V . The loss function
is the commonly used ranking loss with a margin,
and it is defined as follows:
L
SLT
(W,V, s
[l,n]
) = max(0, 1? y
[l,n]
oracle
+ y
[l,n]
t
)
(6)
where s
[l,n]
is the source span. y
[l,n]
oracle
is
the plausible score of a oracle translation result.
y
[l,n]
t
is the plausible score for the best transla-
tion candidate given the model parameters W and
V . The loss function aims to learn a model which
assigns the good translation candidate (the oracle
candidate) higher score than the bad ones, with a
margin 1.
Translation candidates generated by forced de-
coding (Wuebker et al, 2010) are used as ora-
cle translations, which are the positive samples.
Forced decoding performs sentence pair segmen-
tation using the same translation system as decod-
ing. For each sentence pair in the training data,
SMT decoder is applied to the source side, and
any candidate which is not the partial sub-string
of the target sentence is removed from the n-best
list during decoding. From the forced decoding
result, we can get the ideal derivation tree in the
decoder?s search space, and extract positive/oracle
translation candidates.
1495
4.3 Supervised Global Training
The supervised local training uses the n-
odes/samples in the derivation tree of forced de-
coding to update the model, and the trained model
tends to over-fit to local decisions. In this subsec-
tion, a supervised global training is proposed to
tune the model according to the final translation
performance of the whole source sentence.
Actually, we can update the model from the root
of the decoding tree and perform back propaga-
tion along the tree structure. Due to the inexac-
t search nature of SMT decoding, search errors
may inevitably break theoretical properties, and
the final translation results may be not suitable
for model training. To handle this problem, we
use early update strategy for the supervised glob-
al training. Early update is testified to be useful
for SMT training with large scale features (Yu et
al., 2013). Instead of updating the model using
the final translation results, early update approach
optimizes the model, when the oracle translation
candidate is pruned from the n-best list, meaning
that, the model is updated once it performs a unre-
coverable mistake. Back propagation is performed
along the tree structure, and the phrase pair em-
beddings of the leaf nodess are updated.
The loss function for supervised global training
is defined as follows:
L
SGT
(W,V, s
[l,n]
) = ? log(
?
y
[l,n]
oracle
exp (y
[l,n]
oracle
)
?
t?nbest
exp (y
[l,n]
t
)
)
(7)
where y
[l,n]
oracle
is the model score of a oracle trans-
lation candidate for the span [l, n] . Oracle transla-
tion candidates are candidates get from forced de-
coding. If the span [l, n] is not the whole source
sentence, there may be several oracle translation
candidates, otherwise, there is only one, which is
exactly the target sentence. There are much few-
er training samples than those for supervised local
training, and it is not suitable to use ranking loss
for global training any longer. We use negative
log-likelihood to penalize all the other translation
candidates except the oracle ones, so as to leverage
all the translation candidates as training samples.
5 Phrase Pair Embedding
The next question is how to initialize the phrase
pair embedding in the translation table, so as to
generate the leaf nodes of the derivation tree.
There are more phrase pairs than mono-lingual
words, but bilingual corpus is much more difficult
to acquire, compared with monolingual corpus.
Embedding #Data #Entry #Parameter
Word 1G 500K 20 ? 500K
Word Pair 7M (500K)
2
20 ? (500K)
2
Phrase Pair 7M (500K)
4
20 ? (500K)
4
Table 1: The relationship between the size of train-
ing data and the number of model parameters. The
numbers for word embedding is calculated on En-
glish Giga-Word corpus version 3. For word pair
and phrase pair embedding, the numbers are cal-
culated on IWSLT 2009 dialog training set. The
word count of each side of phrase pairs is limited
to be 2.
Table 1 shows the relationship between the size
of training data and the number of model parame-
ters. For word embedding, the training size is 1G
bits, and we may have 500K terms. For each ter-
m, we have a vector with length 20 as parameters,
so there are 20 ? 500K parameters totally. But
for source-target word pair, we may only have 7M
bilingual corpus for training (taking IWSLT data
set as an example), and there are 20 ?(500K)
2
parameters to be tuned. For phrase pairs, the sit-
uation becomes even worse, especially when the
limitation of word count in phrase pairs is relaxed.
It is very difficult to learn the phrase pair embed-
ding brute-forcedly as word embedding is learnt
(Mikolov et al, 2010; Collobert et al, 2011), s-
ince we may not have enough training data.
A simple approach to construct phrase pair em-
bedding is to use the average of the embeddings
of the words in the phrase pair. One problem is
that, word embedding may not be able to mod-
el the translation relationship between source and
target phrases at phrase level, since some phrases
cannot be decomposed. For example, the meaning
of ?hot dog? is not the composition of the mean-
ings of the words ?hot? and ?dog?. In this section,
we split the phrase pair embedding into two parts
to model the translation confidence directly: trans-
lation confidence with sparse features and trans-
lation confidence with recurrent neural network.
We first get two translation confidence vectors sep-
arately using sparse features and recurrent neu-
ral network, and then concatenate them to be the
phrase pair embedding. We call it translation con-
fidence based phrase pair embedding (TCBPPE).
1496
5.1 Translation Confidence with Sparse
Features
Large scale feature training has drawn more at-
tentions these years (Liang et al, 2006; Yu et al,
2013). Instead of integrating the sparse features
directly into the log-linear model, we use them as
the input to learn a phrase pair embedding. For
the top 200,000 frequent translation pairs, each of
them is a feature in itself, and a special feature is
added for all the infrequent ones.
The one-hot representation vector is used as the
input, and a one-hidden-layer network generates
a confidence score. To train the neural network,
we add the confidence scores to the convention-
al log-linear model as features. Forced decoding
is utilized to get positive samples, and contrastive
divergence is used for model training. The neu-
ral network is used to reduce the space dimension
of sparse features, and the hidden layer of the net-
work is used as the phrase pair embedding. The
length of the hidden layer is empirically set to 20.
5.2 Translation Confidence with Recurrent
Neural Network
?e
????1 ?
?(??)
???1
??
???
??
Figure 6: Recurrent neural network for translation
confidence
We use recurrent neural network to generate two
smoothed translation confidence scores based on
source and target word embeddings. One is source
to target translation confidence score and the other
is target to source. These two confidence scores
are defined as:
T
S2T
(s, t) =
?
i
log p(e
i
|e
i?1
, f
a
i
, h
i
) (8)
T
T2S
(s, t) =
?
j
log p(f
j
|f
j?1
, e
a?
j
, h
j
) (9)
where, f
a
i
is the corresponding target word
aligned to e
i
, and it is similar for e
a?
j
.
p(e
i
|e
i?1
, f
a
i
, h
i
) is produced by a recurrent net-
work as shown in Figure 6. The recurrent neural
network is trained with word aligned bilingual cor-
pus, similar as (Auli et al, 2013).
6 Experiments and Results
In this section, we conduct experiments to test our
method on a Chinese-to-English translation task.
The evaluation method is the case insensitive IB-
M BLEU-4 (Papineni et al, 2002). Significant
testing is carried out using bootstrap re-sampling
method proposed by (Koehn, 2004) with a 95%
confidence level.
6.1 Data Setting and Baseline
The data is from the IWSLT 2009 dialog task.
The training data includes the BTEC and SLDB
training data. The training data contains 81k sen-
tence pairs, 655K Chinese words and 806K En-
glish words. The language model is a 5-gram lan-
guage model trained with the target sentences in
the training data. The test set is development set
9, and the development set comprises both devel-
opment set 8 and the Chinese DIALOG set.
The training data for monolingual word embed-
ding is Giga-Word corpus version 3 for both Chi-
nese and English. Chinese training corpus con-
tains 32M sentences and 1.1G words. English
training data contains 8M sentences and 247M
terms. We only train the embedding for the top
100,000 frequent words following (Collobert et
al., 2011). With the trained monolingual word em-
bedding, we follow (Yang et al, 2013) to get the
bilingual word embedding using the IWSLT bilin-
gual training data.
Our baseline decoder is an in-house implemen-
tation of Bracketing Transduction Grammar (BT-
G) (Wu, 1997) in CKY-style decoding with a lex-
ical reordering model trained with maximum en-
tropy (Xiong et al, 2006). The features of the
baseline are commonly used features as standard
BTG decoder, such as translation probabilities,
lexical weights, language model, word penalty and
distortion probabilities. All these commonly used
features are used as recurrent input vector x in
our R
2
NN.
6.2 Translation Results
As we mentioned in Section 5, constructing phrase
pair embeddings from word embeddings may be
not suitable. Here we conduct experiments to ver-
1497
ify it. We first train the source and target word em-
beddings separately using large monolingual data,
following (Collobert et al, 2011). Using monolin-
gual word embedding as the initialization, we fine
tune them to get bilingual word embedding (Yang
et al, 2013).
The word embedding based phrase pair embed-
ding (WEPPE) is defined as:
Epp
web
(s, t) =
?
i
E
wms
(s
i
) ./
?
j
E
wbs
(s
j
)
./
?
k
E
wmt
(t
k
) ./
?
l
E
wbt
(t
l
) (10)
where ./ is a concatenation operator. s and
t are the source and target phrases. E
wms
(s
i
) and
E
wmt
(t
k
) are the monolingual word embeddings,
and E
wbs
(s
i
) and E
wbt
(t
k
) are the bilingual
word embeddings. Here the length of the word
embedding is also set to 20. Therefore, the length
of the phrase pair embedding is 20? 4 = 80 .
We compare our phrase pair embedding meth-
ods and our proposed R
2
NN with baseline system,
in Table 2. We can see that, our R
2
NN models
with WEPPE and TCBPPE are both better than the
baseline system. WEPPE cannot get significan-
t improvement, while TCBPPE does, compared
with the baseline result. TCBPPE is much better
than WEPPE.
Setting Development Test
Baseline 46.81 39.29
WEPPE+R
2
NN 47.23 39.92
TCBPPE+R
2
NN 48.70 ? 40.81 ?
Table 2: Translation results of our proposed R
2
NN
Model with two phrase embedding methods, com-
pared with the baseline. Setting ?WEPPE+R
2
NN?
is the result with word embedding based phrase
pair embedding and our R
2
NN Model, and
?TCBPPE+R
2
NN? is the result of translation con-
fidence based phrase pair embedding and our
R
2
NN Model. The results with ? are significantly
better than the baseline.
Word embedding can model translation rela-
tionship at word level, but it may not be power-
ful to model the phrase pair respondents at phrasal
level, since the meaning of some phrases cannot
be decomposed into the meaning of words. And
also, translation task is difference from other NLP
tasks, that, it is more important to model the trans-
lation confidence directly (the confidence of one
target phrase as a translation of the source phrase),
and our TCBPPE is designed for such purpose.
6.3 Effects of Global Recurrent Input Vector
In order to compare R
2
NN with recursive network
for SMT decoding, we remove the recurrent input
vector in R
2
NN to test its effect, and the results
are shown in Table 3. Without the recurrent input
vectors, R
2
NN degenerates into recursive neural
network (RNN).
Setting Development Test
WEPPE+R
2
NN 47.23 40.81
WEPPE+RNN 37.62 33.29
TCBPPE+R
2
NN 48.70 40.81
TCBPPE+RNN 45.11 37.33
Table 3: Experimental results to test the effects of
recurrent input vector. WEPPE /TCBPPE+RNN
are the results removing recurrent input vectors
with WEPPE /TCBPPE.
From Table 3 we can find that, the recurren-
t input vector is essential to SMT performance.
When we remove it from R
2
NN, WEPPE based
method drops about 10 BLEU points on devel-
opment data and more than 6 BLEU points on
test data. TCBPPE based method drops about 3
BLEU points on both development and test data
sets. When we remove the recurrent input vectors,
the representations of recursive network are gener-
ated with the child nodes, and it does not integrate
global information, such as language model and
distortion model, which are crucial to the perfor-
mance of SMT.
6.4 Sparse Features and Recurrent Network
Features
To test the contributions of sparse features and re-
current network features, we first remove all the
recurrent network features to train and test our
R
2
NN model, and then remove all the sparse fea-
tures to test the contribution of recurrent network
features.
Setting Development Test
TCBPPE+R
2
NN 48.70 40.81
SF+R
2
NN 48.23 40.19
RNN+R
2
NN 47.89 40.01
Table 4: Experimental results to test the effects of
sparse features and recurrent network features.
1498
The results are shown in Table 6.4. From the
results, we can find that, sparse features are more
effective than the recurrent network features a lit-
tle bit. The sparse features can directly model the
translation correspondence, and they may be more
effective to rank the translation candidates, while
recurrent neural network features are smoothed
lexical translation confidence.
7 Conclusion and Future Work
In this paper, we propose a Recursive Recur-
rent Neural Network(R
2
NN) to combine the re-
current neural network and recursive neural net-
work. Our proposed R
2
NN cannot only inte-
grate global input information during each com-
bination, but also can generate the tree struc-
ture in a recursive way. We apply our model to
SMT decoding, and propose a three-step semi-
supervised training method. In addition, we ex-
plore phrase pair embedding method, which mod-
els translation confidence directly. We conduc-
t experiments on a Chinese-to-English translation
task, and our method outperforms a state-of-the-
art baseline about 1.5 points BLEU.
From the experiments, we find that, phrase pair
embedding is crucial to the performance of SMT.
In the future, we will explore better methods for
phrase pair embedding to model the translation e-
quivalent between source and target phrases. We
will apply our proposed R
2
NN to other tree struc-
ture learning tasks, such as natural language pars-
ing.
References
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044?
1054, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. Inno-
vations in Machine Learning, pages 137?186.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
Audio, Speech, and Language Processing, IEEE
Transactions on, 20(1):30?42.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep be-
lief nets. Neural computation, 18(7):1527?1554.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Micha?el Mathieu, and Yann LeCun.
2010. Learning convolutional feature hierarchies for
visual recognition. Advances in Neural Information
Processing Systems, pages 1090?1098.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 388?395.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106?1114.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recur-
sive autoencoders for ITG-based translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 567?
577, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 761?768. Association for Computational Lin-
guistics.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for s-
tatistical machine translation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 791?801, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of the Annual Conference of Internation-
al Speech Communication Association, pages 1045?
1048.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
1499
Richard Socher, Cliff C Lin, Andrew Y Ng, and
Christopher D Manning. 2011. Parsing natural
scenes and natural language with recursive neural
networks. In Proceedings of the 26th Internation-
al Conference on Machine Learning (ICML), vol-
ume 2, page 7.
Richard Socher, John Bauer, and Christopher D Man-
ning. 2013. Parsing with compositional vector
grammars. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistic-
s, volume 1, pages 455?465.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377?403.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 475?484. Association for Computa-
tional Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for s-
tatistical machine translation. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, volume 44, page 521.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai
Yu. 2013. Word alignment modeling with contex-
t dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1112?1123, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
1500
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555?1565,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
?
Duyu Tang
?
, Furu Wei
?
, Nan Yang
\
, Ming Zhou
?
, Ting Liu
?
, Bing Qin
?
?
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
?
Microsoft Research, Beijing, China
\
University of Science and Technology of China, Hefei, China
{dytang, tliu, qinb}@ir.hit.edu.cn
{fuwei, v-nayang, mingzhou}@microsoft.com
Abstract
We present a method that learns word em-
bedding for Twitter sentiment classifica-
tion in this paper. Most existing algorithm-
s for learning continuous word represen-
tations typically only model the syntactic
context of words but ignore the sentimen-
t of text. This is problematic for senti-
ment analysis as they usually map word-
s with similar syntactic context but oppo-
site sentiment polarity, such as good and
bad, to neighboring word vectors. We
address this issue by learning sentiment-
specific word embedding (SSWE), which
encodes sentiment information in the con-
tinuous representation of words. Specif-
ically, we develop three neural networks
to effectively incorporate the supervision
from sentiment polarity of text (e.g. sen-
tences or tweets) in their loss function-
s. To obtain large scale training corpora,
we learn the sentiment-specific word em-
bedding from massive distant-supervised
tweets collected by positive and negative
emoticons. Experiments on applying SS-
WE to a benchmark Twitter sentimen-
t classification dataset in SemEval 2013
show that (1) the SSWE feature performs
comparably with hand-crafted features in
the top-performed system; (2) the perfor-
mance is further improved by concatenat-
ing SSWE with existing feature set.
1 Introduction
Twitter sentiment classification has attracted in-
creasing research interest in recent years (Jiang et
al., 2011; Hu et al, 2013). The objective is to clas-
sify the sentiment polarity of a tweet as positive,
?
This work was done when the first and third authors
were visiting Microsoft Research Asia.
negative or neutral. The majority of existing ap-
proaches follow Pang et al (2002) and employ ma-
chine learning algorithms to build classifiers from
tweets with manually annotated sentiment polar-
ity. Under this direction, most studies focus on
designing effective features to obtain better clas-
sification performance. For example, Mohammad
et al (2013) build the top-performed system in the
Twitter sentiment classification track of SemEval
2013 (Nakov et al, 2013), using diverse sentiment
lexicons and a variety of hand-crafted features.
Feature engineering is important but labor-
intensive. It is therefore desirable to discover ex-
planatory factors from the data and make the learn-
ing algorithms less dependent on extensive fea-
ture engineering (Bengio, 2013). For the task of
sentiment classification, an effective feature learn-
ing method is to compose the representation of a
sentence (or document) from the representation-
s of the words or phrases it contains (Socher et
al., 2013b; Yessenalina and Cardie, 2011). Ac-
cordingly, it is a crucial step to learn the word
representation (or word embedding), which is a
dense, low-dimensional and real-valued vector for
a word. Although existing word embedding learn-
ing algorithms (Collobert et al, 2011; Mikolov et
al., 2013) are intuitive choices, they are not effec-
tive enough if directly used for sentiment classi-
fication. The most serious problem is that tradi-
tional methods typically model the syntactic con-
text of words but ignore the sentiment information
of text. As a result, words with opposite polari-
ty, such as good and bad, are mapped into close
vectors. It is meaningful for some tasks such as
pos-tagging (Zheng et al, 2013) as the two words
have similar usages and grammatical roles, but it
becomes a disaster for sentiment analysis as they
have the opposite sentiment polarity.
In this paper, we propose learning sentiment-
specific word embedding (SSWE) for sentiment
analysis. We encode the sentiment information in-
1555
to the continuous representation of words, so that
it is able to separate good and bad to opposite ends
of the spectrum. To this end, we extend the ex-
isting word embedding learning algorithm (Col-
lobert et al, 2011) and develop three neural net-
works to effectively incorporate the supervision
from sentiment polarity of text (e.g. sentences
or tweets) in their loss functions. We learn the
sentiment-specific word embedding from tweet-
s, leveraging massive tweets with emoticons as
distant-supervised corpora without any manual an-
notations. These automatically collected tweet-
s contain noises so they cannot be directly used
as gold training data to build sentiment classifier-
s, but they are effective enough to provide weak-
ly supervised signals for training the sentiment-
specific word embedding.
We apply SSWE as features in a supervised
learning framework for Twitter sentiment classi-
fication, and evaluate it on the benchmark dataset
in SemEval 2013. In the task of predicting posi-
tive/negative polarity of tweets, our method yields
84.89% in macro-F1 by only using SSWE as fea-
ture, which is comparable to the top-performed
system based on hand-crafted features (84.70%).
After concatenating the SSWE feature with ex-
isting feature set, we push the state-of-the-art to
86.58% in macro-F1. The quality of SSWE is al-
so directly evaluated by measuring the word sim-
ilarity in the embedding space for sentiment lexi-
cons. In the accuracy of polarity consistency be-
tween each sentiment word and its top N closest
words, SSWE outperforms existing word embed-
ding learning algorithms.
The major contributions of the work presented
in this paper are as follows.
? We develop three neural networks to learn
sentiment-specific word embedding (SSWE)
from massive distant-supervised tweets with-
out any manual annotations;
? To our knowledge, this is the first work that
exploits word embedding for Twitter senti-
ment classification. We report the results that
the SSWE feature performs comparably with
hand-crafted features in the top-performed
system in SemEval 2013;
? We release the sentiment-specific word em-
bedding learned from 10 million tweets,
which can be adopted off-the-shell in other
sentiment analysis tasks.
2 Related Work
In this section, we present a brief review of the
related work from two perspectives, Twitter senti-
ment classification and learning continuous repre-
sentations for sentiment classification.
2.1 Twitter Sentiment Classification
Twitter sentiment classification, which identifies
the sentiment polarity of short, informal tweets,
has attracted increasing research interest (Jiang et
al., 2011; Hu et al, 2013) in recent years. Gen-
erally, the methods employed in Twitter sentiment
classification follow traditional sentiment classifi-
cation approaches. The lexicon-based approaches
(Turney, 2002; Ding et al, 2008; Taboada et al,
2011; Thelwall et al, 2012) mostly use a dictio-
nary of sentiment words with their associated sen-
timent polarity, and incorporate negation and in-
tensification to compute the sentiment polarity for
each sentence (or document).
The learning based methods for Twitter sen-
timent classification follow Pang et al (2002)?s
work, which treat sentiment classification of texts
as a special case of text categorization issue. Many
studies on Twitter sentiment classification (Pak
and Paroubek, 2010; Davidov et al, 2010; Bar-
bosa and Feng, 2010; Kouloumpis et al, 2011;
Zhao et al, 2012) leverage massive noisy-labeled
tweets selected by positive and negative emoticon-
s as training set and build sentiment classifiers di-
rectly, which is called distant supervision (Go et
al., 2009). Instead of directly using the distant-
supervised data as training set, Liu et al (2012)
adopt the tweets with emoticons to smooth the lan-
guage model and Hu et al (2013) incorporate the
emotional signals into an unsupervised learning
framework for Twitter sentiment classification.
Many existing learning based methods on Twit-
ter sentiment classification focus on feature engi-
neering. The reason is that the performance of sen-
timent classifier being heavily dependent on the
choice of feature representation of tweets. The
most representative system is introduced by Mo-
hammad et al (2013), which is the state-of-the-
art system (the top-performed system in SemEval
2013 Twitter Sentiment Classification Track) by
implementing a number of hand-crafted features.
Unlike the previous studies, we focus on learning
discriminative features automatically from mas-
sive distant-supervised tweets.
1556
2.2 Learning Continuous Representations for
Sentiment Classification
Pang et al (2002) pioneer this field by using bag-
of-word representation, representing each word as
a one-hot vector. It has the same length as the size
of the vocabulary, and only one dimension is 1,
with all others being 0. Under this assumption,
many feature learning algorithms are proposed to
obtain better classification performance (Pang and
Lee, 2008; Liu, 2012; Feldman, 2013). However,
the one-hot word representation cannot sufficient-
ly capture the complex linguistic characteristics of
words.
With the revival of interest in deep learn-
ing (Bengio et al, 2013), incorporating the con-
tinuous representation of a word as features has
been proving effective in a variety of NLP tasks,
such as parsing (Socher et al, 2013a), language
modeling (Bengio et al, 2003; Mnih and Hin-
ton, 2009) and NER (Turian et al, 2010). In the
field of sentiment analysis, Bespalov et al (2011;
2012) initialize the word embedding by Laten-
t Semantic Analysis and further represent each
document as the linear weighted of ngram vec-
tors for sentiment classification. Yessenalina and
Cardie (2011) model each word as a matrix and
combine words using iterated matrix multiplica-
tion. Glorot et al (2011) explore Stacked Denois-
ing Autoencoders for domain adaptation in sen-
timent classification. Socher et al propose Re-
cursive Neural Network (RNN) (2011b), matrix-
vector RNN (2012) and Recursive Neural Tensor
Network (RNTN) (2013b) to learn the composi-
tionality of phrases of any length based on the
representation of each pair of children recursively.
Hermann et al (2013) present Combinatory Cate-
gorial Autoencoders to learn the compositionality
of sentence, which marries the Combinatory Cat-
egorial Grammar with Recursive Autoencoder.
The representation of words heavily relies on
the applications or tasks in which it is used (Lab-
utov and Lipson, 2013). This paper focuses
on learning sentiment-specific word embedding,
which is tailored for sentiment analysis. Un-
like Maas et al (2011) that follow the proba-
bilistic document model (Blei et al, 2003) and
give an sentiment predictor function to each word,
we develop neural networks and map each n-
gram to the sentiment polarity of sentence. Un-
like Socher et al (2011c) that utilize manually
labeled texts to learn the meaning of phrase (or
sentence) through compositionality, we focus on
learning the meaning of word, namely word em-
bedding, from massive distant-supervised tweets.
Unlike Labutov and Lipson (2013) that produce
task-specific embedding from an existing word
embedding, we learn sentiment-specific word em-
bedding from scratch.
3 Sentiment-Specific Word Embedding
for Twitter Sentiment Classification
In this section, we present the details of learn-
ing sentiment-specific word embedding (SSWE)
for Twitter sentiment classification. We pro-
pose incorporating the sentiment information of
sentences to learn continuous representations for
words and phrases. We extend the existing word
embedding learning algorithm (Collobert et al,
2011) and develop three neural networks to learn
SSWE. In the following sections, we introduce the
traditional method before presenting the details of
SSWE learning algorithms. We then describe the
use of SSWE in a supervised learning framework
for Twitter sentiment classification.
3.1 C&W Model
Collobert et al (2011) introduce C&W model to
learn word embedding based on the syntactic con-
texts of words. Given an ngram ?cat chills on a
mat?, C&W replaces the center word with a ran-
dom wordw
r
and derives a corrupted ngram ?cat
chills w
r
a mat?. The training objective is that the
original ngram is expected to obtain a higher lan-
guage model score than the corrupted ngram by a
margin of 1. The ranking objective function can
be optimized by a hinge loss,
loss
cw
(t, t
r
) = max(0, 1? f
cw
(t) + f
cw
(t
r
))
(1)
where t is the original ngram, t
r
is the corrupted
ngram, f
cw
(?) is a one-dimensional scalar repre-
senting the language model score of the input n-
gram. Figure 1(a) illustrates the neural architec-
ture of C&W, which consists of four layers, name-
ly lookup ? linear ? hTanh ? linear (from
bottom to top). The original and corrupted ngram-
s are treated as inputs of the feed-forward neural
network, respectively. The output f
cw
is the lan-
guage model score of the input, which is calculat-
ed as given in Equation 2, where L is the lookup
table of word embedding,w
1
, w
2
, b
1
, b
2
are the pa-
rameters of linear layers.
f
cw
(t) = w
2
(a) + b
2
(2)
1557
so cooool :D 
lookup 
linear 
hTanh 
linear 
softmax 
(a) C&W 
so cooool :D 
(b) SSWEh 
so cooool :D 
(c) SSWEu 
syntactic 
sentiment 
positive 
negative 
Figure 1: The traditional C&W model and our neural networks (SSWE
h
and SSWE
u
) for learning
sentiment-specific word embedding.
a = hTanh(w
1
L
t
+ b
1
) (3)
hTanh(x) =
?
?
?
?
?
?1 if x < ?1
x if ? 1 ? x ? 1
1 if x > 1
(4)
3.2 Sentiment-Specific Word Embedding
Following the traditional C&W model (Collobert
et al, 2011), we incorporate the sentiment infor-
mation into the neural network to learn sentiment-
specific word embedding. We develop three neural
networks with different strategies to integrate the
sentiment information of tweets.
Basic Model 1 (SSWE
h
). As an unsupervised
approach, C&W model does not explicitly capture
the sentiment information of texts. An intuitive
solution to integrate the sentiment information is
predicting the sentiment distribution of text based
on input ngram. We do not utilize the entire sen-
tence as input because the length of different sen-
tences might be variant. We therefore slide the
window of ngram across a sentence, and then pre-
dict the sentiment polarity based on each ngram
with a shared neural network. In the neural net-
work, the distributed representation of higher lay-
er are interpreted as features describing the input.
Thus, we utilize the continuous vector of top layer
to predict the sentiment distribution of text.
Assuming there are K labels, we modify the di-
mension of top layer in C&W model as K and
add a softmax layer upon the top layer. The
neural network (SSWEh) is given in Figure 1(b).
Softmax layer is suitable for this scenario be-
cause its outputs are interpreted as conditional
probabilities. Unlike C&W, SSWE
h
does not gen-
erate any corrupted ngram. Let f
g
(t), where K
denotes the number of sentiment polarity label-
s, be the gold K-dimensional multinomial distri-
bution of input t and
?
k
f
g
k
(t) = 1. For pos-
itive/negative classification, the distribution is of
the form [1,0] for positive and [0,1] for negative.
The cross-entropy error of the softmax layer is :
loss
h
(t) = ?
?
k={0,1}
f
g
k
(t) ? log(f
h
k
(t)) (5)
where f
g
(t) is the gold sentiment distribution and
f
h
(t) is the predicted sentiment distribution.
Basic Model 2 (SSWE
r
). SSWE
h
is trained by
predicting the positive ngram as [1,0] and the neg-
ative ngram as [0,1]. However, the constraint of
SSWE
h
is too strict. The distribution of [0.7,0.3]
can also be interpreted as a positive label because
the positive score is larger than the negative s-
core. Similarly, the distribution of [0.2,0.8] indi-
cates negative polarity. Based on the above obser-
vation, the hard constraints in SSWE
h
should be
relaxed. If the sentiment polarity of a tweet is pos-
itive, the predicted positive score is expected to be
larger than the predicted negative score, and the
exact reverse if the tweet has negative polarity.
We model the relaxed constraint with a rank-
ing objective function and borrow the bottom four
layers from SSWE
h
, namely lookup? linear ?
hTanh ? linear in Figure 1(b), to build the re-
laxed neural network (SSWEr). Compared with
SSWE
h
, the softmax layer is removed because
SSWE
r
does not require probabilistic interpreta-
tion. The hinge loss of SSWE
r
is modeled as de-
1558
scribed below.
loss
r
(t) = max(0, 1? ?
s
(t)f
r
0
(t)
+ ?
s
(t)f
r
1
(t) )
(6)
where f
r
0
is the predicted positive score, f
r
1
is
the predicted negative score, ?
s
(t) is an indicator
function reflecting the sentiment polarity of a sen-
tence,
?
s
(t) =
{
1 if f
g
(t) = [1, 0]
?1 if f
g
(t) = [0, 1]
(7)
Similar with SSWE
h
, SSWE
r
also does not gen-
erate the corrupted ngram.
Unified Model (SSWE
u
). The C&W model
learns word embedding by modeling syntactic
contexts of words but ignoring sentiment infor-
mation. By contrast, SSWE
h
and SSWE
r
learn
sentiment-specific word embedding by integrating
the sentiment polarity of sentences but leaving out
the syntactic contexts of words. We develop a uni-
fied model (SSWEu) in this part, which captures
the sentiment information of sentences as well as
the syntactic contexts of words. SSWE
u
is illus-
trated in Figure 1(c).
Given an original (or corrupted) ngram and
the sentiment polarity of a sentence as the in-
put, SSWE
u
predicts a two-dimensional vector for
each input ngram. The two scalars (f
u
0
, f
u
1
) s-
tand for language model score and sentiment s-
core of the input ngram, respectively. The training
objectives of SSWE
u
are that (1) the original n-
gram should obtain a higher language model score
f
u
0
(t) than the corrupted ngram f
u
0
(t
r
), and (2) the
sentiment score of original ngram f
u
1
(t) should be
more consistent with the gold polarity annotation
of sentence than corrupted ngram f
u
1
(t
r
). The loss
function of SSWE
u
is the linear combination of t-
wo hinge losses,
loss
u
(t, t
r
) = ? ? loss
cw
(t, t
r
)+
(1? ?) ? loss
us
(t, t
r
)
(8)
where loss
cw
(t, t
r
) is the syntactic loss as given
in Equation 1, loss
us
(t, t
r
) is the sentiment loss
as described in Equation 9. The hyper-parameter
? weighs the two parts.
loss
us
(t, t
r
) = max(0, 1? ?
s
(t)f
u
1
(t)
+ ?
s
(t)f
u
1
(t
r
) )
(9)
Model Training. We train sentiment-specific
word embedding from massive distant-supervised
tweets collected with positive and negative emoti-
cons
1
. We crawl tweets from April 1st, 2013 to
April 30th, 2013 with TwitterAPI. We tokenize
each tweet with TwitterNLP (Gimpel et al, 2011),
remove the @user and URLs of each tweet, and fil-
ter the tweets that are too short (< 7 words). Final-
ly, we collect 10M tweets, selected by 5M tweets
with positive emoticons and 5M tweets with nega-
tive emoticons.
We train SSWE
h
, SSWE
r
and SSWE
u
by
taking the derivative of the loss through back-
propagation with respect to the whole set of pa-
rameters (Collobert et al, 2011), and use Ada-
Grad (Duchi et al, 2011) to update the parame-
ters. We empirically set the window size as 3, the
embedding length as 50, the length of hidden lay-
er as 20 and the learning rate of AdaGrad as 0.1
for all baseline and our models. We learn embed-
ding for unigrams, bigrams and trigrams separate-
ly with same neural network and same parameter
setting. The contexts of unigram (bigram/trigram)
are the surrounding unigrams (bigrams/trigrams),
respectively.
3.3 Twitter Sentiment Classification
We apply sentiment-specific word embedding for
Twitter sentiment classification under a supervised
learning framework as in previous work (Pang et
al., 2002). Instead of hand-crafting features, we
incorporate the continuous representation of word-
s and phrases as the feature of a tweet. The senti-
ment classifier is built from tweets with manually
annotated sentiment polarity.
We explore min, average and max convolu-
tional layers (Collobert et al, 2011; Socher et
al., 2011a), which have been used as simple and
effective methods for compositionality learning
in vector-based semantics (Mitchell and Lapata,
2010), to obtain the tweet representation. The re-
sult is the concatenation of vectors derived from
different convolutional layers.
z(tw) = [z
max
(tw), z
min
(tw), z
average
(tw)]
where z(tw) is the representation of tweet tw and
z
x
(tw) is the results of the convolutional layer x ?
{min,max, average}. Each convolutional layer
1
We use the emoticons selected by Hu et al (2013). The
positive emoticons are :) : ) :-) :D =), and the negative emoti-
cons are :( : ( :-( .
1559
zx
employs the embedding of unigrams, bigrams
and trigrams separately and conducts the matrix-
vector operation of x on the sequence represented
by columns in each lookup table. The output of
z
x
is the concatenation of results obtained from
different lookup tables.
z
x
(tw) = [w
x
?L
uni
?
tw
, w
x
?L
bi
?
tw
, w
x
?L
tri
?
tw
]
where w
x
is the convolutional function of z
x
,
?L?
tw
is the concatenated column vectors of the
words in the tweet. L
uni
, L
bi
and L
tri
are the
lookup tables of the unigram, bigram and trigram
embedding, respectively.
4 Experiment
We conduct experiments to evaluate SSWE by in-
corporating it into a supervised learning frame-
work for Twitter sentiment classification. We also
directly evaluate the effectiveness of the SSWE by
measuring the word similarity in the embedding
space for sentiment lexicons.
4.1 Twitter Sentiment Classification
Experiment Setup and Datasets. We conduct
experiments on the latest Twitter sentiment clas-
sification benchmark dataset in SemEval 2013
(Nakov et al, 2013). The training and develop-
ment sets were completely in full to task partici-
pants. However, we were unable to download all
the training and development sets because some
tweets were deleted or not available due to mod-
ified authorization status. The test set is directly
provided to the participants. The distribution of
our dataset is given in Table 1. We train sentiment
classifier with LibLinear (Fan et al, 2008) on the
training set, tune parameter ?c on the dev set and
evaluate on the test set. Evaluation metric is the
Macro-F1 of positive and negative categories
2
.
Positive Negative Neutral Total
Train 2,642 994 3,436 7,072
Dev 408 219 493 1,120
Test 1,570 601 1,639 3,810
Table 1: Statistics of the SemEval 2013 Twitter
sentiment classification dataset.
2
We investigate 2-class Twitter sentiment classifica-
tion (positive/negative) instead of 3-class Twitter sentiment
classification (positive/negative/neutral) in SemEval2013.
Baseline Methods. We compare our method
with the following sentiment classification algo-
rithms:
(1) DistSuper: We use the 10 million tweets se-
lected by positive and negative emoticons as train-
ing data, and build sentiment classifier with Lib-
Linear and ngram features (Go et al, 2009).
(2) SVM: The ngram features and Support Vec-
tor Machine are widely used baseline methods to
build sentiment classifiers (Pang et al, 2002). Li-
bLinear is used to train the SVM classifier.
(3) NBSVM: NBSVM (Wang and Manning,
2012) is a state-of-the-art performer on many sen-
timent classification datasets, which trades-off be-
tween Naive Bayes and NB-enhanced SVM.
(4) RAE: Recursive Autoencoder (Socher et al,
2011c) has been proven effective in many senti-
ment analysis tasks by learning compositionality
automatically. We run RAE with randomly initial-
ized word embedding.
(5) NRC: NRC builds the top-performed system
in SemEval 2013 Twitter sentiment classification
track which incorporates diverse sentiment lexi-
cons and many manually designed features. We
re-implement this system because the codes are
not publicly available
3
. NRC-ngram refers to the
feature set of NRC leaving out ngram features.
Except for DistSuper, other baseline method-
s are conducted in a supervised manner. We do
not compare with RNTN (Socher et al, 2013b) be-
cause we cannot efficiently train the RNTN model.
The reason lies in that the tweets in our dataset do
not have accurately parsed results or fine grained
sentiment labels for phrases. Another reason is
that the RNTN model trained on movie reviews
cannot be directly applied on tweets due to the d-
ifferences between domains (Blitzer et al, 2007).
Results and Analysis. Table 2 shows the macro-
F1 of the baseline systems as well as the SSWE-
based methods on positive/negative sentimen-
t classification of tweets. Distant supervision is
relatively weak because the noisy-labeled tweet-
s are treated as the gold standard, which affects
the performance of classifier. The results of bag-
of-ngram (uni/bi/tri-gram) features are not satis-
fied because the one-hot word representation can-
not capture the latent connections between words.
NBSVM and RAE perform comparably and have
3
For 3-class sentiment classification in SemEval 2013,
our re-implementation of NRC achieved 68.3%, 0.7% low-
er than NRC (69%) due to less training data.
1560
Method Macro-F1
DistSuper + unigram 61.74
DistSuper + uni/bi/tri-gram 63.84
SVM + unigram 74.50
SVM + uni/bi/tri-gram 75.06
NBSVM 75.28
RAE 75.12
NRC (Top System in SemEval) 84.73
NRC - ngram 84.17
SSWE
u
84.98
SSWE
u
+NRC 86.58
SSWE
u
+NRC-ngram 86.48
Table 2: Macro-F1 on positive/negative classifica-
tion of tweets.
a big gap in comparison with the NRC and SSWE-
based methods. The reason is that RAE and NB-
SVM learn the representation of tweets from the
small-scale manually annotated training set, which
cannot well capture the comprehensive linguistic
phenomenons of words.
NRC implements a variety of features and
reaches 84.73% in macro-F1, verifying the impor-
tance of a better feature representation for Twit-
ter sentiment classification. We achieve 84.98%
by using only SSWE
u
as features without borrow-
ing any sentiment lexicons or hand-crafted rules.
The results indicate that SSWE
u
automatically
learns discriminative features from massive tweets
and performs comparable with the state-of-the-art
manually designed features. After concatenating
SSWE
u
with the feature set of NRC, the perfor-
mance is further improved to 86.58%. We also
compare SSWE
u
with the ngram feature by inte-
grating SSWE into NRC-ngram. The concatenated
features SSWE
u
+NRC-ngram (86.48%) outperfor-
m the original feature set of NRC (84.73%).
As a reference, we apply SSWE
u
on subjec-
tive classification of tweets, and obtain 72.17% in
macro-F1 by using only SSWE
u
as feature. Af-
ter combining SSWE
u
with the feature set of NR-
C, we improve NRC from 74.86% to 75.39% for
subjective classification.
Comparision between Different Word Embed-
ding. We compare sentiment-specific word em-
bedding (SSWE
h
, SSWE
r
, SSWE
u
) with base-
line embedding learning algorithms by only us-
ing word embedding as features for Twitter sen-
timent classification. We use the embedding of u-
nigrams, bigrams and trigrams in the experimen-
t. The embeddings of C&W (Collobert et al,
2011), word2vec
4
, WVSA (Maas et al, 2011) and
our models are trained with the same dataset and
same parameter setting. We compare with C&W
and word2vec as they have been proved effective
in many NLP tasks. The trade-off parameter of
ReEmb (Labutov and Lipson, 2013) is tuned on
the development set of SemEval 2013.
Table 3 shows the performance on the pos-
itive/negative classification of tweets
5
. ReEm-
b(C&W) and ReEmb(w2v) stand for the use
of embeddings learned from 10 million distant-
supervised tweets with C&W and word2vec, re-
spectively. Each row of Table 3 represents a word
embedding learning algorithm. Each column s-
tands for a type of embedding used to compose
features of tweets. The column uni+bi denotes the
use of unigram and bigram embedding, and the
column uni+bi+tri indicates the use of unigram,
bigram and trigram embedding.
Embedding unigram uni+bi uni+bi+tri
C&W 74.89 75.24 75.89
Word2vec 73.21 75.07 76.31
ReEmb(C&W) 75.87 ? ?
ReEmb(w2v) 75.21 ? ?
WVSA 77.04 ? ?
SSWE
h
81.33 83.16 83.37
SSWE
r
80.45 81.52 82.60
SSWE
u
83.70 84.70 84.98
Table 3: Macro-F1 on positive/negative classifica-
tion of tweets with different word embeddings.
From the first column of Table 3, we can see that
the performance of C&W and word2vec are obvi-
ously lower than sentiment-specific word embed-
dings by only using unigram embedding as fea-
tures. The reason is that C&W and word2vec do
not explicitly exploit the sentiment information of
the text, resulting in that the words with oppo-
site polarity such as good and bad are mapped
to close word vectors. When such word embed-
dings are fed as features to a Twitter sentimen-
t classifier, the discriminative ability of sentiment
words are weakened thus the classification perfor-
mance is affected. Sentiment-specific word em-
4
Available at https://code.google.com/p/word2vec/. We
utilize the Skip-gram model because it performs better than
CBOW in our experiments.
5
MVSA and ReEmb are not suitable for learning bigram
and trigram embedding because their sentiment predictor
functions only utilize the unigram embedding.
1561
beddings (SSWE
h
, SSWE
r
, SSWE
u
) effectively
distinguish words with opposite sentiment polarity
and perform best in three settings. SSWE outper-
forms MVSA by exploiting more contextual infor-
mation in the sentiment predictor function. SSWE
outperforms ReEmb by leveraging more senti-
ment information from massive distant-supervised
tweets. Among three sentiment-specific word em-
beddings, SSWE
u
captures more context informa-
tion and yields best performance. SSWE
h
and
SSWE
r
obtain comparative results.
From each row of Table 3, we can see that the
bigram and trigram embeddings consistently im-
prove the performance of Twitter sentiment classi-
fication. The underlying reason is that a phrase,
which cannot be accurately represented by uni-
gram embedding, is directly encoded into the n-
gram embedding as an idiomatic unit. A typical
case in sentiment analysis is that the composed
phrase and multiword expression may have a dif-
ferent sentiment polarity than the individual word-
s it contains, such as not [bad] and [great] deal
of (the word in the bracket has different sentiment
polarity with the ngram). A very recent study by
Mikolov et al (2013) also verified the effective-
ness of phrase embedding for analogically reason-
ing phrases.
Effect of ? in SSWE
u
We tune the hyper-
parameter ? of SSWE
u
on the development set by
using unigram embedding as features. As given
in Equation 8, ? is the weighting score of syntac-
tic loss of SSWE
u
and trades-off the syntactic and
sentiment losses. SSWE
u
is trained from 10 mil-
lion distant-supervised tweets.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
?
Ma
cro
?F1
 
 
SSWEu
Figure 2: Macro-F1 of SSWE
u
on the develop-
ment set of SemEval 2013 with different ?.
Figure 2 shows the macro-F1 of SSWE
u
on pos-
itive/negative classification of tweets with differ-
ent ? on our development set. We can see that
SSWE
u
performs better when ? is in the range
of [0.5, 0.6], which balances the syntactic context
and sentiment information. The model with ?=1
stands for C&W model, which only encodes the
syntactic contexts of words. The sharp decline at
?=1 reflects the importance of sentiment informa-
tion in learning word embedding for Twitter senti-
ment classification.
Effect of Distant-supervised Data in SSWE
u
We investigate how the size of the distant-
supervised data affects the performance of SSWE
u
feature for Twitter sentiment classification. We
vary the number of distant-supervised tweets from
1 million to 12 million, increased by 1 million.
We set the ? of SSWE
u
as 0.5, according to the
experiments shown in Figure 2. Results of posi-
tive/negative classification of tweets on our devel-
opment set are given in Figure 3.
1 2 3 4 5 6 7 8 9 10 11 12
x 106
0.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
# of distant?supervised tweets
Mac
ro?F
1
 
 
SSWEu
Figure 3: Macro-F1 of SSWE
u
with different size
of distant-supervised data on our development set.
We can see that when more distant-supervised
tweets are added, the accuracy of SSWE
u
con-
sistently improves. The underlying reason is that
when more tweets are incorporated, the word em-
bedding is better estimated as the vocabulary size
is larger and the context and sentiment informa-
tion are richer. When we have 10 million distant-
supervised tweets, the SSWE
u
feature increases
the macro-F1 of positive/negative classification of
tweets to 82.94% on our development set. When
we have more than 10 million tweets, the per-
formance remains stable as the contexts of words
have been mostly covered.
4.2 Word Similarity of Sentiment Lexicons
The quality of SSWE has been implicitly evaluat-
ed when applied in Twitter sentiment classification
in the previous subsection. We explicitly evaluate
it in this section through word similarity in the em-
1562
bedding space for sentiment lexicons. The evalua-
tion metric is the accuracy of polarity consistency
between each sentiment word and its topN closest
words in the sentiment lexicon,
Accuracy =
?
#Lex
i=1
?
N
j=1
?(w
i
, c
ij
)
#Lex?N
(10)
where #Lex is the number of words in the senti-
ment lexicon, w
i
is the i-th word in the lexicon, c
ij
is the j-th closest word tow
i
in the lexicon with co-
sine similarity, ?(w
i
, c
ij
) is an indicator function
that is equal to 1 if w
i
and c
ij
have the same sen-
timent polarity and 0 for the opposite case. The
higher accuracy refers to a better polarity consis-
tency of words in the sentiment lexicon. We set N
as 100 in our experiment.
Experiment Setup and Datasets We utilize
the widely-used sentiment lexicons, namely M-
PQA (Wilson et al, 2005) and HL (Hu and Liu,
2004), to evaluate the quality of word embedding.
For each lexicon, we remove the words that do
not appear in the lookup table of word embedding.
We only use unigram embedding in this section
because these sentiment lexicons do not contain
phrases. The distribution of the lexicons used in
this paper is listed in Table 4.
Lexicon Positive Negative Total
HL 1,331 2,647 3,978
MPQA 1,932 2,817 4,749
Joint 1,051 2,024 3,075
Table 4: Statistics of the sentiment lexicons. Join-
t stands for the words that occur in both HL and
MPQA with the same sentiment polarity.
Results. Table 5 shows our results com-
pared to other word embedding learning al-
gorithms. The accuracy of random result is
50% as positive and negative words are ran-
domly occurred in the nearest neighbors of
each word. Sentiment-specific word embed-
dings (SSWE
h
, SSWE
r
, SSWE
u
) outperform ex-
isting neural models (C&W, word2vec) by large
margins. SSWE
u
performs best in three lexicon-
s. SSWE
h
and SSWE
r
have comparable perfor-
mances. Experimental results further demonstrate
that sentiment-specific word embeddings are able
to capture the sentiment information of texts and
distinguish words with opposite sentiment polari-
ty, which are not well solved in traditional neural
Embedding HL MPQA Joint
Random 50.00 50.00 50.00
C&W 63.10 58.13 62.58
Word2vec 66.22 60.72 65.59
ReEmb(C&W) 64.81 59.76 64.09
ReEmb(w2v) 67.16 61.81 66.39
WVSA 68.14 64.07 67.12
SSWE
h
74.17 68.36 74.03
SSWE
r
73.65 68.02 73.14
SSWE
u
77.30 71.74 77.33
Table 5: Accuracy of the polarity consistency of
words in different sentiment lexicons.
models like C&W and word2vec. SSWE outper-
forms MVSA and ReEmb by exploiting more con-
text information of words and sentiment informa-
tion of sentences, respectively.
5 Conclusion
In this paper, we propose learning continuous
word representations as features for Twitter sen-
timent classification under a supervised learning
framework. We show that the word embedding
learned by traditional neural networks are not ef-
fective enough for Twitter sentiment classification.
These methods typically only model the contex-
t information of words so that they cannot dis-
tinguish words with similar context but opposite
sentiment polarity (e.g. good and bad). We learn
sentiment-specific word embedding (SSWE) by
integrating the sentiment information into the loss
functions of three neural networks. We train SS-
WE with massive distant-supervised tweets select-
ed by positive and negative emoticons. The ef-
fectiveness of SSWE has been implicitly evaluat-
ed by using it as features in sentiment classifica-
tion on the benchmark dataset in SemEval 2013,
and explicitly verified by measuring word similar-
ity in the embedding space for sentiment lexicon-
s. Our unified model combining syntactic context
of words and sentiment information of sentences
yields the best performance in both experiments.
Acknowledgments
We thank Yajuan Duan, Shujie Liu, Zhenghua Li,
Li Dong, Hong Sun and Lanjun Zhou for their
great help. This research was partly supported
by National Natural Science Foundation of China
(No.61133012, No.61273321, No.61300113).
1563
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy da-
ta. In Proceedings of International Conference on
Computational Linguistics, pages 36?44.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Analysis and Ma-
chine Intelligence.
Yoshua Bengio. 2013. Deep learning of represen-
tations: Looking forward. arXiv preprint arX-
iv:1305.0445.
Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shok-
oufandeh. 2011. Sentiment classification based on
supervised latent n-gram analysis. In Proceedings
of the Conference on Information and Knowledge
Management, pages 375?382.
Dmitriy Bespalov, Yanjun Qi, Bing Bai, and Ali Shok-
oufandeh. 2012. Sentiment classification with su-
pervised sequence embedding. In Machine Learn-
ing and Knowledge Discovery in Databases, pages
159?174. Springer.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet alocation. the Journal of ma-
chine Learning research, 3:993?1022.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Annual Meeting of the Association for
Computational Linguistics, volume 7.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of International Con-
ference on Computational Linguistics, pages 241?
249.
Xiaowen Ding, Bing Liu, and Philip S Yu. 2008. A
holistic lexicon-based approach to opinion mining.
In Proceedings of the International Conference on
Web Search and Data Mining, pages 231?240.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, pages 2121?2159.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Ronen Feldman. 2013. Techniques and application-
s for sentiment analysis. Communications of the
ACM, 56(4):82?89.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, pages 42?47.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. Proceed-
ings of International Conference on Machine Learn-
ing.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of compo-
sitional semantics. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 894?904.
Ming Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 168?177.
Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013. Unsupervised sentiment analysis with emo-
tional signals. In Proceedings of the International
World Wide Web Conference, pages 607?618.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter sen-
timent classification. The Proceeding of Annual
Meeting of the Association for Computational Lin-
guistics, 1:151?160.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In The International AAAI
Conference on Weblogs and Social Media.
Igor Labutov and Hod Lipson. 2013. Re-embedding
words. In Annual Meeting of the Association for
Computational Linguistics.
Kun-Lin Liu, Wu-Jun Li, and Minyi Guo. 2012. E-
moticon smoothed language models for twitter sen-
timent analysis. In The Association for the Advance-
ment of Artificial Intelligence.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
1564
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corra-
do, and Jeffrey Dean. 2013. Distributed representa-
tions of words and phrases and their compositionali-
ty. The Conference on Neural Information Process-
ing Systems.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Andriy Mnih and Geoffrey E Hinton. 2009. A s-
calable hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081?1088.
Saif M Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. Nrc-canada: Building the state-of-
the-art in sentiment analysis of tweets. Proceedings
of the International Workshop on Semantic Evalua-
tion.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, volume 13.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of Language Resources and Evalua-
tion Conference, volume 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. The Conference
on Neural Information Processing Systems, 24:801?
809.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the International Conference on Ma-
chine Learning, pages 129?136.
Richard Socher, J. Pennington, E.H. Huang, A.Y. Ng,
and C.D. Manning. 2011c. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 151?161.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Com-
positionality Through Recursive Matrix-Vector S-
paces. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013a. Parsing with composi-
tional vector grammars. In Annual Meeting of the
Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013b. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Conference on Empirical Methods in Nat-
ural Language Processing, pages 1631?1642.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Mike Thelwall, Kevan Buckley, and Georgios Pal-
toglou. 2012. Sentiment strength detection for the
social web. Journal of the American Society for In-
formation Science and Technology, 63(1):163?173.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. Annual Meeting of the
Association for Computational Linguistics.
Peter D Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 417?424.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguistic-
s, pages 90?94.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 347?354.
Ainur Yessenalina and Claire Cardie. 2011. Compo-
sitional matrix-space models for sentiment analysis.
In Proceedings of Conference on Empirical Methods
in Natural Language Processing, pages 172?182.
Jichang Zhao, Li Dong, Junjie Wu, and Ke Xu. 2012.
Moodlens: an emoticon-based sentiment analysis
system for chinese tweets. In Proceedings of the
18th ACM SIGKDD international conference on
Knowledge discovery and data mining.
Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013. Deep learning for chinese word segmenta-
tion and pos tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 647?657.
1565
