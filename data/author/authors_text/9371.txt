Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1025?1032,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Exploring Distributional Similarity Based Models 
for Query Spelling Correction 
 
Mu Li 
Microsoft Research Asia 
5F Sigma Center 
Zhichun Road, Haidian District 
Beijing, China, 100080 
muli@microsoft.com 
Muhua Zhu 
School of  
Information Science and Engineering 
Northeastern University 
Shenyang, Liaoning, China, 110004 
zhumh@ics.neu.edu.cn 
Yang Zhang 
School of  
Computer Science and Technology  
Tianjin University 
Tianjin, China, 300072 
yangzhang@tju.edu.cn 
Ming Zhou 
Microsoft Research Asia 
5F Sigma Center  
Zhichun Road, Haidian District 
Beijing, China, 100080 
mingzhou@microsoft.com 
 
  
Abstract 
A query speller is crucial to search en-
gine in improving web search relevance. 
This paper describes novel methods for 
use of distributional similarity estimated 
from query logs in learning improved 
query spelling correction models. The 
key to our methods is the property of dis-
tributional similarity between two terms: 
it is high between a frequently occurring 
misspelling and its correction, and low 
between two irrelevant terms only with 
similar spellings. We present two models 
that are able to take advantage of this 
property. Experimental results demon-
strate that the distributional similarity 
based models can significantly outper-
form their baseline systems in the web 
query spelling correction task.  
1 Introduction 
Investigations into query log data reveal that 
more than 10% of queries sent to search engines 
contain misspelled terms (Cucerzan and Brill, 
2004). Such statistics indicate that a good query 
speller is crucial to search engine in improving 
web search relevance, because there is little op-
portunity that a search engine can retrieve many 
relevant contents with misspelled terms.  
The problem of designing a spelling correction 
program for web search queries, however, poses 
special technical challenges and cannot be well 
solved by general purpose spelling correction 
methods. Cucerzan and Brill (2004) discussed in 
detail specialties and difficulties of a query spell 
checker, and illustrated why the existing methods 
could not work for query spelling correction. 
They also identified that no single evidence, ei-
ther a conventional spelling lexicon or term fre-
quency in the query logs, can serve as criteria for 
validate queries.  
To address these challenges, we concentrate 
on the problem of learning improved query spell-
ing correction model by integrating distributional 
similarity information automatically derived 
from query logs. The key contribution of our 
work is identifying that we can successfully use 
the evidence of distributional similarity to 
achieve better spelling correction accuracy. We 
present two methods that are able to take advan-
tage of distributional similarity information. The 
first method extends a string edit-based error 
model with confusion probabilities within a gen-
erative source channel model. The second 
method explores the effectiveness of our ap-
proach within a discriminative maximum entropy 
model framework by integrating distributional 
similarity-based features. Experimental results 
demonstrate that both methods can significantly 
outperform their baseline systems in the spelling 
correction task for web search queries. 
1025
The rest of the paper is structured as follows: 
after a brief overview of the related work in Sec-
tion 2, we discuss the motivations for our ap-
proach, and describe two methods that can make 
use of distributional similarity information in 
Section 3. Experiments and results are presented 
in Section 4. The last section contains summaries 
and outlines promising future work. 
2 Related Work 
The method for web query spelling correction 
proposed by Cucerzan and Brill (2004) is 
essentially based on a source channel model, but 
it requires iterative running to derive suggestions 
for very-difficult-to-correct spelling errors. Word 
bigram model trained from search query logs is 
used as the source model, and the error model is 
approximated by inverse weighted edit distance 
of a correction candidate from its original term. 
The weights of edit operations are interactively 
optimized based on statistics from the query logs. 
They observed that an edit distance-based error 
model only has less impact on the overall 
accuracy than the source model. The paper 
reports that un-weighted edit distance will cause 
the overall accuracy of their speller?s output to 
drop by around 2%. The work of Ahmad and 
Kondrak (2005) tried to employ an unsupervised 
approach to error model estimation. They 
designed an EM (Expectation Maximization) 
algorithm to optimize the probabilities of edit 
operations over a set of search queries from the 
query logs, by exploiting the fact that there are 
more than 10% misspelled queries scattered 
throughout the query logs. Their method is 
concerned with single character edit operations, 
and evaluation was performed on an isolated 
word spelling correction task. 
There are two lines of research in conventional 
spelling correction, which deal with non-word 
errors and real-word errors respectively. Non-
word error spelling correction is concerned with 
the task of generating and ranking a list of possi-
ble spelling corrections for each query word not 
found in a lexicon. While traditionally candidate 
ranking is based on manually tuned scores such 
as assigning weights to different edit operations 
or leveraging candidate frequencies, some statis-
tical models have been proposed for this ranking 
task in recent years. Brill and Moore (2000) pre-
sented an improved error model over the one 
proposed by Kernigham et al (1990) by allowing 
generic string-to-string edit operations, which 
helps with modeling major cognitive errors such 
as the confusion between le and al. Toutanova 
and Moore (2002) further explored this via ex-
plicit modeling of phonetic information of Eng-
lish words. Both these two methods require mis-
spelled/correct word pairs for training, and the 
latter also needs a pronunciation lexicon. Real-
word spelling correction is also referred to as 
context sensitive spelling correction, which tries 
to detect incorrect usage of valid words in certain 
contexts (Golding and Roth, 1996; Mangu and 
Brill, 1997). 
Distributional similarity between words has 
been investigated and successfully applied in 
many natural language tasks such as automatic 
semantic knowledge acquisition (Dekang Lin, 
1998) and language model smoothing (Essen and 
Steinbiss, 1992; Dagan et al, 1997). An investi-
gation on distributional similarity functions can 
be found in (Lillian Lee, 1999). 
3 Distributional Similarity-Based Mod-
els for Query Spelling Correction 
3.1 Motivation 
Most of the previous work on spelling correction 
concentrates on the problem of designing better 
error models based on properties of character 
strings. This direction ever evolves from simple 
Damerau-Levenshtein distance (Damerau, 1964; 
Levenshtein, 1966) to probabilistic models that 
estimate string edit probabilities from corpus 
(Church and Gale, 1991; Mayes et al 1991; Ris-
tad and Yianilos, 1997; Brill and Moore, 2000; 
and Ahmad and Kondrak, 2005). In the men-
tioned methods, however, the similarities be-
tween two strings are modeled on the average of 
many misspelling-correction pairs, which may 
cause many idiosyncratic spelling errors to be 
ignored. Some of those are typical word-level 
cognitive errors. For instance, given the query 
term adventura, a character string-based error 
model usually assigns similar similarities to its 
two most probable corrections adventure and 
aventura. Taking into account that adventure has 
a much higher frequency of occurring, it is most 
likely that adventure would be generated as a 
suggestion. However, our observation into the 
query logs reveals that adventura in most cases is 
actually a common misspelling of aventura. Two 
annotators were asked to judge 36 randomly 
sampled queries that contain more than one term, 
and they agreed upon that 35 of them should be 
aventura.  
To solve this problem, we consider alternative 
methods to make use of the information beyond a 
1026
term?s character strings. Distributional similarity 
provides such a dimension to view the possibility 
that one word can be replaced by another based 
on the statistics of words co-occuring with them. 
Distributional similarity has been proposed to 
perform tasks such as language model smoothing 
and word clustering, but to the best of our 
knowledge, it has not been explored in estimat-
ing similarities between misspellings and their 
corrections. In this section, we will only involve 
the consine metric for illustration purpose. 
Query logs can serve as an excellent corpus 
for distributional similarity estimation. This is 
because query logs are not only an up-to-date 
term base, but also a comprehensive spelling er-
ror repository (Cucerzan and Brill, 2004; Ahmad 
and Kondrak, 2005). Given enough size of query 
logs, some misspellings, such as adventura, will 
occur so frequently that we can obtain reliable 
statistics of their typical usage. Essential to our 
method is the observation of high distributional 
similarity between frequently occurring spelling 
errors and their corrections, but low between ir-
relevant terms. For example, we observe that 
adventura occurred more than 3,300 times in a 
set of logged queries that spanned three months, 
and its context was similar to that of aventura. 
Both of them usually appeared after words like 
peurto and lyrics, and were followed by mall, 
palace and resort. Further computation shows 
that, in the tf (term frequency) vector space based 
on surrounding words, the cosine value between 
them is approximately 0.8, which indicates these 
two terms are used in a very similar way among 
all the users trying to search aventura. The co-
sine between adventura and adventure is less 
than 0.03 and basically we can conclude that 
they are two irrelevant terms, although their 
spellings are similar. 
Distributional similarity is also helpful to ad-
dress another challenge for query spelling correc-
tion: differentiating valid OOV terms from fre-
quently occurring misspellings.  
 
 InLex  Freq Cosine 
vaccum No 18,430 
vacuum Yes 158,428 0.99 
seraphin No 1,718 
seraphim Yes 14,407 0.30 
Table 1. Statistics of two word pairs 
with similar spellings 
 
Table 1 lists detailed statistics of two word 
pairs, each of pair of words have similar spelling, 
lexicon and frequency properties. But the distri-
butional similarity between each pair of words 
provides the necessary information to make cor-
rection classification that vacuum is a spelling 
error while seraphin is a valid OOV term.  
3.2 Problem Formulation 
In this work, we view the query spelling correc-
tion task as a statistical sequence inference prob-
lem. Under the probabilistic model framework, it 
can be conceptually formulated as follows. 
Given a correction candidate set C for a query 
string q: 
}),(|{ ?<= cqEditDistcC  
in which each correction candidate c satisfies the 
constraint that the edit distance between c and q 
is less than a given threshold ?, the model is to 
find c* in C with the highest probability: 
)|(maxarg* qcPc
Cc?
=  (1) 
In practice, the correction candidate set C is 
not generated from the entire query string di-
rectly. Correction candidates are generated for 
each term of a query first, and then C is con-
structed by composing the candidates of individ-
ual terms. The edit distance threshold ? is set for 
each term proportionally to the length of the term. 
3.3 Source Channel Model 
Source channel model has been widely used for 
spelling correction (Kernigham et al, 1990; 
Mayes, Damerau et al, 1991; Brill and More, 
2000; Ahmad and Kondrak, 2005). Instead of 
directly optimize (1), source channel model tries 
to solve an equivalent problem by applying 
Bayes?s rule and dropping the constant denomi-
nator: 
)()|(maxarg* cPcqPc
Cc?
=  (2) 
In this approach, two component generative 
models are involved: source model P(c) that gen-
erates the user?s intended query c and error 
model P(q|c) that generates the real query q 
given c. These two component models can be 
independently estimated. 
In practice, for a multi-term query, the source 
model can be approximated with an n-gram sta-
tistical language model, which is estimated with 
tokenized query logs. Taking bigram model for 
example, c is a correction candidate containing n 
terms, ncccc ?21= , then P(c) can be written as 
the product of consecutive bigram probabilities: 
? ?= )|()( 1ii ccPcP  
1027
Similarly, the error model probability of a 
query is decomposed into generation probabili-
ties of individual terms which are assumed to be 
independently generated:  
?= )|()|( ii cqPcqP  
Previous proposed methods for error model 
estimation are all based on the similarity between 
the character strings of qi and ci as described in 
3.1. Here we describe a distributional similarity-
based method for this problem. Essentially there 
are different ways to estimate distributional simi-
larity between two words (Dagan et al, 1997), 
and the one we propose to use is confusion prob-
ability (Essen and Steinbiss, 1992). Formally, 
confusion probability cP  estimates the possibil-
ity that one word w1 can be replaced by another 
word w2: 
?=
w
c wPwwPwP
wwPwwP )()|(
)(
)|()|( 22112  (3) 
where w belongs to the set of words that co-
occur with both w1 and w2.  
From the spelling correction point of view, 
given w1 to be a valid word and w2 one of its 
spelling errors, )|( 12 wwPc  actually estimates 
opportunity that w1 is misspelled as w2 in query 
logs. Compared to other similarity measures such 
as cosine or Euclidean distance, confusion prob-
ability is of interest because it defines a probabil-
istic distribution rather than a generic measure. 
This property makes it more theoretically sound 
to be used as error model probability in the 
Bayesian framework of the source channel model. 
Thus it can be applied and evaluated independ-
ently. However, before using confusion probabil-
ity as our error model, we have to solve two 
problems: probability renormalization and 
smoothing.  
Unlike string edit-based error models, which 
distribute a major portion of probability over 
terms with similar spellings, confusion probabil-
ity distributes probability over the entire vocabu-
lary in the training data. This property may cause 
the problem of unfair comparison between dif-
ferent correction candidates if we directly use (3) 
as the error model probability. This is because 
the synonyms of different candidates may share 
different portion of confusion probabilities. This 
problem can be solved by re-normalizing the 
probabilities only over a term?s possible correc-
tion candidates and itself. To obtain better esti-
mation, here we also require that the frequency 
of a correction candidate should be higher than 
that of the query term, based on the observation 
that correct spellings generally occur more often 
in query logs. Formally, given a word w and its 
correction candidate set C, the confusion prob-
ability of a word w?  conditioned on w can be 
redefined as 
??
??
?
??
??
?
??
=? ? ?
Cw
Cw
wcP
wwP
wwP
Cc c
c
c
0
)|(
)|(
)|(  (4) 
where )|( wwPc ?? is the original definition of con-
fusion probability. 
In addition, we might also have the zero-
probability problem when the query term has not 
appeared or there are few context words for it in 
the query logs. In such cases there is no distribu-
tional similarity information available to any 
known terms. To solve this problem, we define 
the final error model probability as the linear 
combination of confusion probability and a string 
edit-based error model probability )|( cqPed : 
)|()1()|()|( cqPcqPcqP edc ?? ?+=  (5) 
where ? is the interpolation parameter between 0 
and 1 that can be experimentally optimized on a 
development data set.  
3.4 Maximum Entropy Model 
Theoretically we are more interested in building 
a unified probabilistic spelling correction model 
that is able to leverage all available features, 
which could include (but not limited to) tradi-
tional character string-based typographical simi-
larity, phonetic similarity and distributional simi-
larity proposed in this work. The maximum en-
tropy model (Berger et al, 1996) provides us 
with a well-founded framework for this purpose, 
which has been extensively used in natural lan 
guage processing tasks ranging from part-of-
speech tagging to machine translation.  
For our task, the maximum entropy model 
defines a posterior probabilistic distribution 
)|( qcP  over a set of feature functions fi (q, c) 
defined on an input query q and its correction 
candidate c: 
? ?
?
=
=
=
c
N
i ii
N
i ii
qcf
qcf
qcP
1
1
),(exp
),(exp
)|(
?
?
 (6) 
1028
where ?s are feature weights, which can be opti-
mized by maximizing the posterior probability  
on the training set: 
?
?
=
TDqt
qtP
),(
)|(logmaxarg* ?
?
?  
where TD denotes the set of training samples in 
the form of query-truth pairs presented to the 
training algorithm.  
We use the Generalized Iterative Scaling (GIS) 
algorithm (Darroch and Ratcliff, 1972) to learn 
the model parameter ?s of the maximum entropy 
model. GIS training requires normalization over 
all possible prediction classes as shown in the 
denominator in equation (6). Since the potential 
number of correction candidates may be huge for 
multi-term queries, it would not be practical to 
perform the normalization over the entire search 
space. Instead, we use a method to approximate 
the sum over the n-best list (a list of most prob-
able correction candidates). This is similar to 
what Och and Ney (2002) used for their maxi-
mum entropy-based statistical machine transla-
tion training.  
3.4.1 Features 
Features used in our maximum entropy model 
are classified into two categories I) baseline fea-
tures and II) features supported by distributional 
similarity evidence. Below we list the feature 
templates. 
 
Category I: 
1. Language model probability feature. This 
is the only real-valued feature with feature value 
set to the logarithm of source model probability: 
)(log),( cPcqf prob =  
2. Edit distance-based features, which are 
generated by checking whether the weighted 
Levenshtein edit distance between a query term 
and its correction is in certain range; 
All the following features, including this one, 
are binary features, and have the feature function 
of the following form: 
??
?
=
otherwise
satisfiedconstraint
cqfn 0
1
),(  
in which the feature value is set to 1 when the 
constraints described in the template are satisfied; 
otherwise the feature value is set to 0.  
3. Frequency-based features, which are gen-
erated by checking whether the frequencies of a 
query term and its correction candidate are above 
certain thresholds; 
4. Lexicon-based features, which are gener-
ated by checking whether a query term and its 
correction candidate are in a conventional spell-
ing lexicon; 
5. Phonetic similarity-based features, which 
are generated by checking whether the edit dis-
tance between the metaphones (Philips, 1990) of 
a query term and its correction candidate is be-
low certain thresholds.  
 
Category II: 
6. Distributional similarity based term fea-
tures, which are generated by checking whether a 
query term?s frequency is higher than certain 
thresholds but there are no candidates for it with 
higher frequency and high enough distributional 
similarity. This is usually an indicator that the 
query term is valid and not covered by the spell-
ing lexicon. The frequency thresholds are enu-
merated from 10,000 to 50,000 with the interval 
5,000. 
7. Distributional similarity based correction 
candidate features, which are generated by 
checking whether a correction candidate?s fre-
quency is higher than the query term or the cor-
rection candidate is in the lexicon, and at the 
same time the distributional similarity is higher 
than certain thresholds. This generally gives the 
evidence that the query term may be a common 
misspelling of the current candidate. The distri-
butional similarity thresholds are enumerated 
from 0.6 to 1 with the interval 0.1.  
4 Experimental Results 
4.1 Dataset 
We randomly sampled 7,000 queries from daily 
query logs of MSN Search and they were manu-
ally labeled by two annotators. For each query 
identified to contain spelling errors, corrections 
were given by the annotators independently. 
From the annotation results that both annotators 
agreed upon 3,061 queries were extracted, which 
were further divided into a test set containing 
1,031 queries and a training set containing 2,030 
queries. In the test set there are 171 queries iden-
tified containing spelling errors with an error rate 
of 16.6%. The numbers on the training set is 312 
and 15.3%, respectively. The average length of 
queries on training set is 2.8 terms and on test set 
it is 2.6. 
1029
In our experiments, a term bigram model is 
used as the source model. The bigram model is 
trained with query log data of MSN Search dur-
ing the period from October 2004 to June 2005. 
Correction candidates are generated from a term 
base extracted from the same set of query logs. 
For each of the experiments, the performance 
is evaluated by the following metrics: 
Accuracy: The number of correct outputs gen-
erated by the system divided by the total number 
of queries in the test set; 
Recall: The number of correct suggestions for 
misspelled queries generated by the system di-
vided by the total number of misspelled queries 
in the test set; 
Precision: The number of correct suggestions 
for misspelled queries generated by the system 
divided by the total number of suggestions made 
by the system. 
4.2 Results 
We first investigated the impact of the interpola-
tion parameter ? in equation (5) by applying the 
confusion probability-based error model on train-
ing set. For the string edit-based error model 
probability )|( cqPed , we used a heuristic score 
computed as the inverse of weighted edit dis-
tance, which is similar to the one used by Cucer-
zan and Brill (2004).  
Figure 1 shows the accuracy metric at differ-
ent settings of ?. The accuracy generally gains 
improvements before ? reaches 0.9. This shows 
that confusion probability plays a more important 
role in the combination. As a result, we empiri-
cally set ?= 0.9 in the following experiments. 
88%
89%
89%
90%
90%
91%
91%
0.05 0.15 0.25 0.35 0.45 0.55 0.65 0.75 0.85 0.95
lambda
ac
cu
ra
cy
 
Figure 1. Accuracy with different ?s 
To evaluate whether the distributional similar-
ity can contribute to performance improvements, 
we conducted the following experiments. For 
source channel model, we compared the confu-
sion probability-based error model (SC-SimCM) 
against two baseline error model settings, which 
are source model only (SC-NoCM) and the heu-
ristic string edit-based error model (SC-EdCM) 
we just described. Two maximum entropy mod-
els were trained with different feature sets. ME-
NoSim is the model trained only with baseline 
features. It serves as the baseline for ME-Full, 
which is trained with all the features described in 
3.4.1. In training ME-Full, cosine distance is 
used as the similarity measure examined by fea-
ture functions.  
In all the experiments we used the standard 
viterbi algorithm to search for the best output of 
source channel model. The n-best list for maxi-
mum entropy model training and testing is gen-
erated based on language model scores of cor-
rection candidates, which can be easily obtained 
by running the forward-viterbi backward-A* al-
gorithm. On a 3.0GHZ Pentium4 personal com-
puter, the system can process 110 queries per 
second for source channel model and 86 queries 
per second for maximum entropy model, in 
which 20 best correction candidates are used. 
 
Model Accuracy Recall Precision 
SC-NoCM 79.7% 63.3% 40.2% 
SC-EdCM 84.1% 62.7% 47.4% 
SC-SimCM 88.2% 57.4% 58.8% 
ME-NoSim 87.8% 52.0% 60.0% 
ME-Full 89.0% 60.4% 62.6% 
Table 2. Performance results for different models 
 
Table 2 details the performance scores for the 
experiments, which shows that both of the two 
distributional similarity-based models boost ac-
curacy over their baseline settings. SC-SimCM 
achieves 26.3% reduction in error rate over SC-
EdCM, which is significant to the 0.001 level 
(paired t-test). ME-Full outperforms ME-NoSim 
in all three evaluation measures, with 9.8% re-
duction in error rate and 16.2% improvement in 
recall, which is significant to the 0.01 level.  
It is interesting to note that the accuracy of 
SC-SimCM is slightly better than ME-NoSim, 
although ME-NoSim makes use of a rich set of 
features. ME-NoSim tends to keep queries with 
frequently misspelled terms unchanged (e.g. caf-
fine extractions from soda) to reduce false alarms 
(e.g. bicycle suggested for biocycle). 
We also investigated the performance of the 
models discussed above at different recall. Fig-
ure 2 and Figure 3 show the precision-recall 
curves and accuracy-recall curves of different 
models. We observed that the performance of 
SC-SimCM and ME-NoSim are very close to 
each other and ME-Full consistently yields better 
performance over the entire P-R curve. 
1030
40%
45%
50%
55%
60%
65%
70%
75%
80%
85%
35% 40% 45% 50% 55% 60%
recall
pr
ec
is
io
n
ME-Full
ME-NoSim
SC-EdCM
SC-SimCM
SC-NoCM
 
Figure 2. Precision-recall curve of different models 
 
82%
83%
84%
85%
86%
87%
88%
89%
90%
91%
35% 40% 45% 50% 55% 60%
recall
ac
cu
ra
cy
ME-Full
ME-NoSim
SC-EdCM
SC-SimCM
SC-NoCM
 
Figure 3. Accuracy-recall curve of different models 
We performed a study on the impact of train-
ing size to ensure all models are trained with 
enough data. 
40%
50%
60%
70%
80%
90%
200 400 600 800 1000 1600 2000
ME-Full Recall
ME-Full Accuracy
ME-NoSim Recall
ME-NoSim Accuracy
 
Figure 4. Accuracy of maximum entropy models 
trained with different number of samples 
 
Figure 4 shows the accuracy of the two maxi-
mum entropy models as functions of number of 
training samples. From the results we can see 
that after the number of training samples reaches 
600 there are only subtle changes in accuracy 
and recall. Therefore basically it can be con-
cluded that 2,000 samples are sufficient to train a 
maximum entropy model with the current feature 
sets. 
5 Conclusions and Future Work 
We have presented novel methods to learn better 
statistical models for the query spelling correc-
tion task by exploiting distributional similarity 
information. We explained the motivation of our 
methods with the statistical evidence distilled 
from query log data. To evaluate our proposed 
methods, two probabilistic models that can take 
advantage of such information are investigated. 
Experimental results show that both methods can 
achieve significant improvements over their 
baseline settings. 
A subject of future research is exploring more 
effective ways to utilize distributional similarity 
even beyond query logs. Currently for low-
frequency terms in query logs there are no reli-
able distribution similarity evidence available for 
them. A promising method of dealing with this in 
next steps is to explore information in the result-
ing page of a search engine, since the snippets in 
the resulting page can provide far greater de-
tailed information about terms in a query. 
References 
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. 
Proceedings of EMNLP 2005, pages 955-962. 
Adam L. Beger, Stephen A. Della Pietra, and Vincent  
J. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tion Linguistics, 22(1):39-72. 
Eric Brill and Robert C. Moore. 2000. An improved 
error model for noisy channel spelling correction.  
Proceedings of 38th annual meeting of the ACL, 
pages 286-293. 
Kenneth W. Church and William A. Gale. 1991. 
Probability scoring for spelling correction. In Sta-
tistics and Computing, volume 1, pages 93-103. 
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collec-
tive knowledge of web users. Proceedings of 
EMNLP?04, pages 293-300. 
Ido Dagan, Lillian Lee and Fernando Pereira. 1997. 
Similarity-Based Methods for Word Sense Disam-
biguation. Proceedings of the 35th annual meeting 
of ACL, pages 56-63. 
Fred Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communica-
tion of the ACM 7(3):659-664. 
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for long-linear models. Annals of Ma-
thematical Statistics, 43:1470-1480. 
Ute Essen and Volker Steinbiss. 1992. Co-occurrence 
smoothing for stochastic language modeling. Pro-
ceedings of ICASSP, volume 1, pages 161-164. 
Andrew R. Golding and Dan Roth. 1996. Applying 
winnow to context-sensitive spelling correction. 
Proceedings of ICML 1996, pages 182-190. 
Mark D. Kernighan, Kenneth W. Church and William 
A. Gale. 1990. A spelling correction program 
1031
based on a noisy channel model. Proceedings of 
COLING 1990, pages 205-210. 
Karen Kukich. 1992. Techniques for automatically 
correcting words in text. ACM Computing Surveys. 
24(4): 377-439 
Lillian Lee. 1999. Measures of distributional similar-
ity. Proceedings of the 37th annual meeting of ACL, 
pages 25-32. 
V. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet 
Physice ? Doklady 10: 707-710. 
Dekang Lin. 1998. Automatic retrieval and clustering 
of similar words. Proceedings of COLING-ACL 
1998, pages 768-774. 
Lidia Mangu and Eric Brill. 1997. Automatic rule 
acquisition for spelling correction. Proceedings of  
ICML 1997, pages 734-741. 
Eric Mayes, Fred Damerau and Robert Mercer. 1991. 
Context based spelling correction. Information 
processing and management 27(5): 517-522. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maimum entropy models for statistical 
machine translation. Proceedings of the 40th an-
nual meeting of ACL, pages 295-302. 
Lawrence Philips. 1990. Hanging on the metaphone. 
Computer Language Magazine, 7(12): 39. 
Eric S. Ristad and Peter N. Yianilos. 1997. Learning 
string edit distance. Proceedings of ICML 1997. 
pages 287-295 
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction. 
Proceedings of the 40th annual meeting of ACL, 
pages 144-151. 
 
1032
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 217?220,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Designing Special Post-processing Rules for SVM-based
Chinese Word Segmentation
Muhua Zhu, Yilin Wang, Zhenxing Wang, Huizhen Wang, Jingbo Zhu
Natural Language Processing Lab
Northeastern University
No.3-11, Wenhua Road, Shenyang, Liaoning, China, 110004
{zhumh, wangyl, wangzx, wanghz}@ics.neu.edu.cn
zhujingbo@mail.neu.edu.cn
Abstract
We participated in the Third Interna-
tional Chinese Word Segmentation Bake-
off. Specifically, we evaluated our Chi-
nese word segmenter NEUCipSeg in
the close track, on all four corpora,
namely Academis Sinica (AS), City Uni-
versity of Hong Kong (CITYU), Mi-
crosoft Research (MSRA), and Univer-
sity of Pennsylvania/University of Col-
orado (UPENN). Based on Support Vec-
tor Machines (SVMs), a basic segmenter
is designed regarding Chinese word seg-
mentation as a problem of character-based
tagging. Moreover, we proposed post-
processing rules specially taking into ac-
count the properties of results brought
out by the basic segmenter. Our system
achieved good ranks in all four corpora.
1 SVM-based Chinese Word Segmenter
We built out segmentation system following (Xue
and Shen, 2003), regarding Chinese word segmen-
tation as a problem of character-based tagging.
Instead of Maximum Entropy, we utilized Sup-
port Vector Machines as an alternate. SVMs are
a state-of-the-art learning algorithm, owing their
success mainly to the ability in control of general-
ization error upper-bound, and the smooth integra-
tion with kernel methods. See details in (Vapnik,
1995). We adopted svm-light1 as the specific
implementation of the model.
1.1 Problem Formalization
By formalizing Chinese word segmentation into
the problem of character-based tagging, we as-
1http://svmlight.joachims.org/
signed each character to one and only one of the
four classes: word-prefix, word-suffix,
word-stem and single-character. For
example, given a two-word sequence????
??, the Chinese words for ?Southeast Asia(?
??) people(?) ?, the character ???is as-
signed to the category word-prefix, indicating
the beginning of a word;???is assigned to the
category word-stem, indicating the middle po-
sition of a word; ???belongs to the category
word-suffix, meaning the ending of a Chinese
word; and last,???is assigned to the category
single-character, indicating that the single
character itself is a word.
1.2 Feature Templates
We utilized four of the five basic feature templates
suggested in (Low et al , 2005), described as
follows:
? Cn(n = ?2,?1, 0, 1, 2)
? CnCn+ 1(n = ?2,?1, 0, 1)
? Pu(C0)
? T (C?2)T (C?1)T (C0)T (C1)T (C2)
where C refers to a Chinese character. The first
two templates specify a context window with the
size of five characters, where C0 stands for the
current character: the former describes individual
characters and the latter presents bigrams within
the context window. The third template checks
if current character is a punctuation or not, and
the last one encodes characters? type, including
four types: numbers, dates, English letters and
the type representing other characters. See de-
tail description and the example in (Low et al
, 2005). We dropped template C?1C1, since,
217
in experiments, it seemed not to perform well
when incorporated by SVMs. Slightly different
from (Low et al , 2005), character set repre-
senting dates are expanded to include ????
???????????????????,
the Chinese characters for ?day?, ?month?, ?year?,
?hour?,?minute?,?second?, respectively.
2 Post-processing Rules
Segmentation results of SVM-based segmenter
have their particular properties. In respect to the
properties of segmentation results produced by the
SVM-based segmenter, we extracted solely from
training data comprehensive and effective post-
processing rules, which are grouped into two cate-
gories: The rules, termed IV rules, make ef-
forts to fix segmentation errors of character se-
quences, which appear both in training and test-
ing data; Rules seek to recall some OOV(Out
Of Vocabulary) words, termed OOV rules. In
practice, we sampled out a subset from train-
ing dataset as a development set for the analysis
of segmentation results produced by SVM-based
segmenter. Note that, in the following, we defined
Vocabulary to be the collection of words ap-
pearing in training dataset and Segmentation
Unit to be any isolated character sequence as-
sumed to be a valid word by a segmenter. A
segmentation unit can be a correctly seg-
mented word or an incorrectly segmented charac-
ter sequence.
2.1 IV Rules
The following rules are named IV rules, pur-
suing the consistence between segmentation re-
sults and training data. The intuition underlying
the rules is that since training data give somewhat
specific descriptions for most of the words in it, a
character sequence in testing data should be seg-
mented in accordance with training data as much
as possible.
Ahead of post-processing, all words in the
training data are grouped into two distinct sets:
the uniquity set, which consists of words
with unique segmentation in training data and the
ambiguity set, which includes words having
more than one distinct segmentations in training
data. For example, the character sequence???
??has two kinds of segmentations, as?? ?
??(new century) and?????(as a compo-
nent of some Named-Entity, such as the name of a
restaurant).
? For each word in the uniquity set, check
whether it is wrongly segmented into more
than one segmentation units by the SVM-
based segmenter. If true, the continuous seg-
mentation units corresponding to the word
are grouped into the united one. The in-
tuition underlying this post-processing rule
is that SVM-based segmenter prefers two-
character words or single-character words
when confronting the case that the segmenter
has low self-confidence in some character-
sequence segmentation. For example, ??
???(duplicate) was segmented as ??
???and ????(unify) was split
into ?? ??. This phenomenon is
caused by the imbalanced data distribution.
Specifically, characters belonging to category
word-stem are much less than other three
categories.
? For each segmentation unit in the result
produced by SVM-based segmenter, check
whether the unit can be segmented into more
than one IV words and, meanwhile, the words
exist in a successive form for at least once in
training data . If true, replace the segmen-
tation unit with corresponding continuously
existing words. The intuition underlying this
rule is that SVM-based segmenter tends to
combine a word with some suffix, such as
???????, two Chinese characters
representing ?person?. For example, ??
? ??(Person in registration) tends to be
grouped as a single unit.
? For any sequence in the ambiguity set, such
as ?????, check if the correct seg-
mentation can be determined by the con-
text surrounding the sequence. Without los-
ing the generality, in the following explana-
tion, we assume each sequence in the am-
biguity set has two distinct segmentations.
we collected from training data the word
preceding a sequence where each existence
of the sequence has one of its segmenta-
tions, into a collection, named preceding
word set, and, correspondingly, the fol-
lowing word into another set, which is
termed following word set. Analog-
ically, we can produce preceding word
218
set and following word set for an-
other case of segmentation. When an am-
biguous sequence appears in testing data, the
surrounding context (in fact, just one preced-
ing word and a following word) is extracted.
If the context has overlapping with either of
the pre-extracted contexts of the same se-
quence which are from training data, the seg-
mentation corresponding to one of the con-
texts is retained.
? More over, we took a look into the annotation
errors existing in training data. We assume
there unavoidably exist some annotation mis-
takes. For example, in UPENN, the sequence
????(abbreviation for China and Amer-
ica) exists, for eighty-seven times, as a whole
word and only one time, exists as?? ??.
We regarded the segmentation?? ??as
an annotation error. Generally, when the ra-
tio of two kinds of segmentations is greater
than a pre-determined threshold (the value is
set seven in our system), the sequence is re-
moved from the ambiguity set and added as
a word of unique segmentation into the uniq-
uity set.
2.2 OOV Rules
The following rules are termed OOV rules,
since they are utilized to recall some of the
wrongly segmented OOV words. A OOV word
is frequently segmented into two continuous OOV
segmentation units. For example, the OOV
word?????(Vatican) was frequently seg-
mented as ??? ??, where both ??
??and ???are OOV character sequences.
Continuous OOVs present a strong clue of po-
tential segmentation errors. A rule is designed
to merge some of continuous OOVs into a cor-
rect segmentation unit. The designed rule is ap-
plicable to all four corpora. Moreover, since dis-
tinction between different segmentation standards
frequently leads to very different segmentation of
a same OOV words in different corpora, we de-
signed rules particularly for MSRA and UPENN
respectively, to recall more OOVs.
? For two continuous OOVs, check whether
at least one of them is a single-character
word. If true, group the continuous OOVs
into a segmentation unit. The reason for
the constraint of at least one of continuous
OOVs being single-character word is that not
all continuous OOVs should be combined,
for example, ??? ???, both ??
??(Germany merchant) and????(the
company name) are OOVs, but this sequence
is a valid segmentation unit. On the other
hand, we assume appropriately that most of
the cases for character being single-character
word have been covered by training data.
That is, once a single character is a OOV seg-
mentation unit, there exists a segmentation
error with high possibility.
? MSRA has very different segmentation stan-
dard from other three corpora, mainly be-
cause it requires to group several continuous
words together into a Name Entity. For ex-
ample, the word???????(the Min-
istry of Foreign Affairs of China) appear-
ing in MSRA is generally annotated into two
words in other corpora, as????(China)
and?????(the Ministry of Foreign Af-
fairs). In our system, we first gathered all
the words from the training data whose length
are greater than six Chinese characters, filter-
ing out dates and numbers, which was cov-
ered by Finite State Automation as
a pre-processing stage. For each words col-
lected, regard the first two and three charac-
ters as NE prefix, which indicates the be-
ginning of a Name Entity. The collection of
prefixes is termed Sp(refix). Analogously, the
collection Ss(uffix) of suffixes is brought up
in the same way. Obviously not all the pre-
fixes (suffixes) are good indicators for Name
Entities. Partly inheriting from (Brill, 1995),
we applied error-driven learning to filter pre-
fixes in Sp and suffixes in Ss. Specifically,
if a prefix and a suffix are both matched in
a sequence, all the characters between them,
together with the prefix and the suffix, are
merged into a single segmentation unit. The
resulted unit is compared with corresponding
sequence in training data. If they were not ex-
actly matched, the prefix and suffix were re-
moved from collections respectively. Finally
resulted Sp and Ss are utilized to recognize
Name Entities in the initial segmentation re-
sults.
? UPENN has different segmentation standard
from other three corpora in that, for some
219
Corpus R P F ROOV RIV
AS 0.949 0.940 0.944 0.694 0.960
MSRA 0.955 0.956 0.956 0.650 0.966
UPENN 0.940 0.914 0.927 0.634 0.969
CITYU 0.965 0.971 0.968 0.719 0.981
Table 1: Our official SIGHAN bakeoff results
Locations, such as ?????(Beijing
) and Organizations, such as ???
??(the Ministry of Foreign Affairs), the
last Chinese character presents a clue that
the character with high possibility is a suf-
fix of some words. In fact, SVM-based seg-
menter sometimes mistakenly split an OOV
word into a segmentation unit followed by a
suffix. Thus, when some suffixes exist as a
single-character segmentation unit, it should
be grouped with the preceding segmentation
unit. Undoubtedly not all suffixes are appro-
priate to this rule. To gather a clean collec-
tion of suffixes, we first clustered together the
words with the same suffix, filtering accord-
ing to the number of instances in each clus-
ter. Second, the same as above, error-driven
method is utilized to retain effective suffixes.
3 Evaluation Results
We evaluated the Chinese word segmentation
system in the close track, on all four cor-
pora, namely Academis Sinica (AS), City Uni-
versity of Hong Kong (CITYU), Microsoft Re-
search (MSRA), and University of Pennsylva-
nia/University of Colorado (UPENN). The results
are depicted in Table 1, where columns R,P and
F refer to Recall, Precision, F measure
respectively, and ROOV , RIV for the recall of out-
of-vocabulary words and in-vocabulary words.
In addition to final results reported in Bake-
off, we also conducted a series of experiments to
evaluate the contributions of IV rules and OOV
rules. The experimental results are showed in
Table 2, where V1, V2, V3 represent versions
of our segmenters, which compose differently of
components. In detail, V1 represents the basic
SVM-based segmenter; V2 represents the seg-
menter which applied IV rules following SVM-
based segmentation; V3 represents the segmenter
composing of all the components, that is, includ-
ing SVM-based segmenter, IV rules and OOV
rules. Since the OOV ratio is much lower than IV
correspondence, the improvement made by OOV
rules is not so dramatic as IV rules.
Corpus V1 v2 v3
AS 0.932 0.94 0.944
MSRA 0.939 0.954 0.956
UPENN 0.914 0.923 0.927
CITYU 0.955 0.966 0.968
Table 2: Word segmentation accuracy(F Measure)
resulted from post-processing rules
4 Conclusions and future work
We added post-processing rules to SVM-based
segmenter. By doing so, we our segmentation sys-
tem achieved comparable results in the close track,
on all four corpora. But on the other hand, post-
processing rules have the problems of confliction,
which limits the number of rules. We expect to
transform rules into features of SVM-based seg-
menter, thus incorporating information carried by
rules in a more elaborate manner.
Acknowledgements
This research was supported in part by the Na-
tional Natural Science Foundation of China(No.
60473140) and by Program for New Century Ex-
cellent Talents in University(No. NCET-05-0287).
References
Nianwen Xue and Libin Shen. 2003. Chinese Word
segmentation as LMR tagging. In Proceedings of
the Second SIGHAN Workshop on Chinese Lan-
guage Processing,pages 176-179.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Berlin: Springer-Verlag.
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word
Segmentation. In Proceeding of the Fifth SIGHAN
Workshop on Chinese Language Processing, pages
161-164.
Eric.Brill. 1995. Transformation-based error-driven
learning and natural language processing:A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543-565.
220
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 143?151,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Chinese-English Organization Name Translation Based on Correla-
tive Expansion 
Feiliang Ren, Muhua Zhu,  Huizhen Wang,   Jingbo Zhu  
Natural Language Processing Lab, Northeastern University, Shenyang, China 
{renfeiliang,zhumuhua}@gmail.com 
{wanghuizhen,zhujingbo}@mail.neu.edu.cn 
Abstract 
This paper presents an approach to trans-
lating Chinese organization names into 
English based on correlative expansion. 
Firstly, some candidate translations are 
generated by using statistical translation 
method. And several correlative named 
entities for the input are retrieved from a 
correlative named entity list. Secondly, 
three kinds of expansion methods are 
used to generate some expanded queries. 
Finally, these queries are submitted to a 
search engine, and the refined translation 
results are mined and re-ranked by using 
the returned web pages. Experimental re-
sults show that this approach outperforms 
the compared system in overall transla-
tion accuracy.  
1 Introduction 
There are three main types of named entity: loca-
tion name, person name, and organization name. 
Organization name translation is a subtask of 
named entity translation. It is crucial for many 
NLP tasks, such as cross-language information 
retrieval, machine translation, question and an-
swering system. For organization name transla-
tion, there are two problems among it which are 
very difficult to handle.  
Problem I: There is no uniform rule that can 
be abided by to select proper translation methods 
for the inside words of an organization name. For 
example, a Chinese word ????, when it is used 
as a modifier for a university, it is translated to 
Northeastern for ?????/Northeastern Uni-
versity?, and is translated to Northeast for ???
????/Northeast Forestry University?, and is 
mapped to Chinese Pinyin Dongbei for ????
???/Dongbei University of Finance and Eco-
nomics?. It is difficult to decide which transla-
tion method should be chosen when we translate 
the inside words of an organization name.  
Problem II: There is no uniform rule that can 
be abided by to select proper translation order 
and proper treatment of particles Here particles 
refer to prepositions and articles) for an input 
organization name. For example, the organiza-
tion name ???????/China Construction 
Bank? and the organization name ??????
?/Agricultural Bank of China?, they are very 
similar both in surface forms and in syntax struc-
tures, but their translation orders are different, 
and their treatments of particles are also different. 
Generally, there are two strategies usually 
used for named entity translation in previous re-
search. One is alignment based approach, and the 
other is generation based approach. Alignment 
based approach (Chen et al 2003; Huang et al 
2003; Hassan and Sorensen, 2005; and so on) 
extracts named entities translation pairs from 
parallel or comparable corpus by some alignment 
technologies, and this approach is not suitable to 
solve the above two problems. Because new or-
ganization names are constantly being created, 
and alignment based method usually fails to 
cover these new organization names that don?t 
occur in the bilingual corpus.  
Traditional generation based approach (Al-
Onaizan and Knight, 2002; Jiang et al.2007; 
Yang et al 2008; and so on) usually consists of 
two parts. Firstly, it will generate some candidate 
translations for the input; then it will re-rank 
these candidate translations to assign the correct 
translations high ranks. Cheng and Zong [2008] 
proposed another generation based approach for 
organization name translation, which directly 
translates organization names according to their 
inherent structures. But their approach still can?t 
solve the above two problems. This is because 
the amount of organization names is so huge and 
many of them have their own special translation 
rules to handle the above two problems. And the 
inherent structures don?t reveal these translation 
rules. Traditional generation based approach is 
suitable for organization name translation. But in 
previous research, the final translation perform-
ance depends on the candidate translation gen-
143
eration process greatly. If this generation process 
failed, it is impossible to obtain correct result 
from the re-ranking process. In response to this, 
Huang et al [2005] proposed a novel method that 
mined key phrase translation form web by using 
topic-relevant hint words. But in their approach, 
they removed the candidate translation genera-
tion process, which will improve extra difficult 
during mining phrase. Besides, in their approach, 
the features considered to obtain topic-relevant 
words are not so comprehensive, which will af-
fect the quality of returned web pages where the 
correct translations are expected to be included. 
There is still much room for the improvement 
process of the topic-relevant words extraction.  
Inspired by the traditional generation based 
named entity translation strategy and the ap-
proach proposed by Huang et al, we propose an 
organization name translation approach that min-
ing the correct translations of input organization 
name from the web. Our aim is to solve the 
above two problems indirectly by retrieving the 
web pages that contain the correct translation of 
the input and mining the correct translation from 
them. Given an input organization name, firstly, 
some candidate translations are generated by us-
ing statistical translation method. And several 
correlative named entities for the input are re-
trieved from a correlative named entity list. Sec-
ondly, expanded queries are generated by using 
three kinds of query expansion methods. Thirdly, 
these queries are submitted to a search engine, 
and the final translation results are mined and re-
ranked by using the returned web pages.  
The rest of this paper is organized as follows, 
section 2 presents the extraction process of cor-
relative named entities, section 3 presents a detail 
description of our translation method for Chinese 
organization name, and section 4 introduces our 
parameter evaluation method, and section 5 is the 
experiments and discussions part, finally conclu-
sions and future work are given in section 6.  
2 Extraction of Correlative Named En-
tities 
The key of our approach is to find some web 
pages that contain the correct translation of the 
input. With the help of correlative named entities 
(here if two named entities are correlative, it 
means that they are usually used to describe the 
same topic), it is easier to find such web pages. 
This is because that in the web, one web page 
usually has one topic. Thus if two named entities 
are correlative, they are very likely to occur in 
pair in some web pages.  
The correlative named entity list is constructed 
in advance. During translation, the correlative 
named entities for the input organization name 
are retrieved from this list directly. To set up this 
correlative named entity list, an about 180GB-
sized collection of web pages are used. Totally 
there are about 100M web pages in this collec-
tion. Named entities are recognized from every 
web page by using a NER tool. This NER tool is 
trained by CRF model 1  with the corpus from 
SIGHAN-20082.  
2.1 Features Used 
During the extraction of correlative named enti-
ties, the following features are considered.  
Co-occurrence in a Document The more of-
ten two named entities co-occur in a document, 
the more likely they are correlative. This feature 
is denoted as 1 2( , )iCoD n n , which means the co-
occurrence of named entities 1n and 2n  in a docu-
ment iD . This feature is also the main feature 
used in Huang et al [2005].   
Co-occurrence in Documents The more often 
two named entities co-occur in different docu-
ments, the more likely they are correlative. This 
feature is denoted as 1 2( , )CoDs n n , which means 
the number of documents that both 1n  and 2n oc-
cur in. 
Distance The closer two named entities is in a 
document, the more likely they are correlative. 
This feature is denoted as 1 2( , )iDistD n n , which 
means the number of words between 1n and 2n  
in a document iD . 
Mutual Information Mutual information is a 
metric to measure the correlation degree of two 
words. The higher two named entities? mutual 
information, the more likely they are correlative. 
And the mutual information of named entities 
1n and 2n  in a document iD is computed as fol-
lowing formula. 
1 2
1 2 1 2
1 2
( , )
( , ) ( , ) log
( ) ( )i
p n n
MID n n p n n
p n p n
= ?  (1) 
Jaccard Similarity Jaccard similarity is also a 
metric to measure the correlative degree of two 
words. The higher two named entities? Jaccard 
                                                 
1 http://www.chasen.org/~taku/software/CRF++/ 
2 http://www.china-language.gov.cn/bakeoff08/ 
144
similarity, the more likely they are correlative. 
And Jaccard similarity is computed as following 
formula. 
1 2
1 2
1 2 1 2
( , )
( , )
( ) ( ) ( , )
CoDs n n
Jaccard n n
D n D n CoDs n n
= + ? (2) 
where ( )iD n  is the number of documents that 
in occurs in, and  ( , )i jCoDs n n  is the number of 
documents that both in  and jn occur in. 
TF-IDF TF-IDF is a weight computation 
method usually used in information retrieval. 
Here for a named entity in , TF-IDF is used to 
measure the importance of its correlative named 
entities. The TF-IDF value of jn in a document 
iD is computed as following formula. 
( ) log
( )i j ij j
N
TF IDF n tf
D n
? = ?               (3) 
where ijtf is the frequency of jn in docu-
ment iD , N is the number of total documents, 
and ( )jD n is the number of documents that 
jn occurs in.  
2.2 Feature Combination 
During the process of feature combination, every 
feature is normalized, and the final correlative 
degree of two named entities is the linear combi-
nation of these normalized features, and it is 
computed as following formula.   
1 2
( , ) ( , )
( , )
( , ) ( , )
k i j
i jk
i j
k i j i j
j k j
CoD n n CoDs n n
C n n
CoD n n CoDs n n
? ?= +
?
?? ?
3 4
1 ( , )( , )
1 ( , )
( , )
k i j
k i jk k
k i j
k i j j kj k
MID n nDistD n n
MID n n
DistD n n
? ?+ +
? ?
????
5 6
( )( , )
( , ) ( )
k j
i j k
i j k j
j k j
TF IDF nJaccard n n
Jaccard n n TF IDF n
? ?
?
+ + ?
?
? ??
(4) 
Finally, for every organization name in , its 
top-K correlative named entities are selected to 
construct the correlative named entity list.  
During translation, the correlative words for 
the input can be retrieved from this correlative 
list directly. If the input is not included in this list, 
the same method as in Huang et al [2005] is 
used to obtain the needed correlative words.  
3 Organization Name Translation 
Based on Correlative Expansion 
3.1 Statistical Translation Module 
The first step of our approach is to generate some 
candidate translations for every input organiza-
tion name. As shown in table 1, these candidate 
translations are used as query stems during query 
expansion. We use Moses3, a state of the art pub-
lic machine translation tool, to generate such 
candidate translations. Here Moses is trained 
with the bilingual corpus that is from the 4th 
China Workshop on Machine Translation4. Total 
there are 868,947 bilingual Chinese-English sen-
tence pairs on news domain in this bilingual cor-
pus. Moses receives an organization name as in-
put, and outputs the N-best results as the candi-
date translations of the input organization name. 
Total there are six features used in Moses: phrase 
translation probability, inverse phrase translation 
probability, lexical translation probability, in-
verse lexical translation probability, language 
model, and sentence length penalty. All the 
needed parameters are trained with MERT 
method (Och, 2003) by using a held-out devel-
opment set.  
3.2 Query Expansions 
Because the amount of available web pages is so 
huge, the query submitted to search engine must 
be well designed. Otherwise, the search engine 
will return large amount of un-related web pages. 
This will enlarge the difficulty of mining phase. 
Here three kinds of expansion methods are pro-
posed to generate some queries by combining the 
clues given by statistical translation method and 
the clues given by correlative named entities of 
the input. And these correlative named entities 
are retrieved from the correlative named entities 
list before the query expansions process. These 
three kinds of expansions are explained as fol-
lows. 
3.2.1 Monolingual Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, and jn is 
one of its correlative named entities. If jn can be 
reliably translated5, we expand is with this reli-
                                                 
3 http://www.statmt.org/moses/  
4 http://www.nlpr.ia.ac.cn/cwmt-2008  
5 A word can be reliably translated means either it has 
a unique dictionary translation or it is a Chinese 
145
able translation ( )jt n  to form a query 
? is + ( )jt n ?. This kind of expansion is called as 
monolingual expansion.  
For two named entities, if they are correlative, 
their translations are likely correlative too. So 
their translations are also likely to occur in pair 
in some web pages. Suppose a query generated 
by this expansion is ? is + ( )jt n ?, if the candidate 
translation is is the correct translation of the in-
put, there must be some returned web pages that 
contain is completely. Otherwise, it is still possi-
ble to obtain some returned web pages that con-
tain the correct translation. This is because that 
the search engine will return both the web pages 
that include the query completely and the web 
pages that include the query partly. And for a 
translation candidate is and the correct transla-
tion 'is , they are very likely to have some com-
mon words, so some of their returned web pages 
may overlap each other. Thus it can be expected 
that when we submit ? is + ( )jt n ? to search en-
gine, it will return some web pages that include 
? 'is + ( )jt n ? or include 'is .  This is very helpful 
for the mining phase. 
3.2.2 Bilingual Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, we ex-
pand is with in  to form a query ? is + in ?. This 
kind of expansion is called as bilingual expan-
sion. 
Bilingual expansion is very useful to verify 
whether a candidate translation is the correct 
translation. To give readers more information or 
they are not sure about the translation of original 
named entity, the Chinese authors usually in-
clude both the original form of a named entity 
and its translation in the mix-language web pages 
[Fei Huang et al 2005]. So the correct translation 
pair is likely to obtain more supports from the 
returned web pages than those incorrect transla-
tion pairs. Thus bilingual expansion is very use-
ful for the re-ranking phase. 
Besides, for an input organization name, if one 
of its incorrect candidate translations is  is very 
                                                                          
person name and can be translated by Pinyin map-
ping.  
similar to the correct translation 'is  in surface 
form, the correct translation is also likely to be 
contained in the returned web pages by using this 
kind of queries. The reason for this is the search 
mechanism of search engine, which has been 
explained above in monolingual expansion. 
3.2.3 Mix-language Expansion  
Given an input organization name in , suppose 
is is one of its candidate translations, and jn is 
one of its correlative named entities. We ex-
pand is with jn  to form a query ? is + jn ?. This 
kind of expansion is called as mix-language ex-
pansion.  
Mix-language expansion is a necessary com-
plement to the other two expansions. Besides, 
this mix-language expansion is more prone to 
obtain some mix-language web pages that may 
contain both the original input organization name 
and its correct translation.  
3.3 Mining 
When the expanded queries are submitted to 
search engine, the correct translation of the input 
organization name may be contained in the re-
turned web pages. Because the translation of an 
organization name must be also an organization 
name, we mine the correct translation of the in-
put among the English organization names. Here 
we use the Stanford named entity recognition 
toolkits6  to recognize all the English organiza-
tion names in the returned web pages. Then align 
these recognized organization names to the input 
by considering the following features. 
Mutual Translation Probability The transla-
tion probability measures the semantic equiva-
lence between a source organization name and its 
target candidate translation. And mutual transla-
tion probability measures this semantic equiva-
lence in two directions. For simplicity, here we 
use IBM model-1(Brown et al 1993), which 
computes two organization names? translation 
probability using the following formula. 
11
1
( | ) ( | )
J L
j lJ
lj
p f e p f e
L ==
= ??                  (6) 
where ( | )j lp f e is the lexical translation prob-
ability. Suppose the input organization name 
is in , is is one of the recognized English organi-
                                                 
6  http://nlp.stanford.edu/software/CRF-NER.shtml 
146
zation names, the mutual translation probability 
of in and is  is computed as: 
( , ) ( | ) (1 ) ( | )i i i i i imp n s p n s p s n? ?= + ?      (7) 
Golden Translation Ratio For two organiza-
tion names, their golden translation ratio is de-
fined as the percentage of words in one organiza-
tion name whose reliable transactions can be 
found in another organization name. This feature 
is used to measure the probability of one named 
entity is the translation of the other. It is com-
puted as following formula. 
( , ) ( , )
( , ) (1 )
| | | |
i j j i
i j
i j
G n s G s n
GR n s
n s
? ?= + ?   (8) 
where ( , )i jG n s is the number of golden trans-
lated words from in to js , and ( , )j iG s n  is the 
number of golden translated words from js to in .  
Co-occurrence In Web Pages For an input 
organization name in and a recognized candidate 
translation js , the more often they co-occur in 
different web pages, the more likely they are 
translations of each other. This feature is denoted 
as ( , )i jCoS n s , which means the number of web 
pages that both 1n  and js occur in. 
Input Matching Ratio This feature is defined 
as the percentage of the words in the input that 
can be found in a returned web page. For those 
mix-language web pages, this feature is used to 
measure the probability of the correct translation 
occurring in a returned web page. It is computed 
as the following formula. 
| |
( , )
| |
i k
i k
i
n s
IMR n s
n
?=                             (9) 
where ks is the k th?  returned web page. 
Correlative Named Entities Matching Ratio 
This feature is defined as the percentage of the 
words in a correlative named entity that can be 
found in a returned web page. This feature is also 
used to measure the probability of the correct 
translation occurring in a returned web page. It is 
computed as the following formula. 
| |
_ ( , )
| |
i k
i k
i
c s
CW MR c s
c
?=                   (10) 
The final confidence score of in and jt to be a 
translation pair is measured by following formula. 
As in formula 4, here every factor will be is nor-
malized during computation.   
1 2( , ) ( , ) ( , )i j i j i jC n t mp n t GR n t? ?= +  
4
3
( , )
( , )
( , )
i j
i k
ki j
j
CoSs n n
IMR n s
CoS n n K
??+ + ??
5 _ ( , )i k
i k
CW MR c s
K I
?+ ? ??           (11) 
where K is the number of returned web pages, 
I is the number of correlative named entities for 
the input organization name. 
For every input organization name, we remain 
a fixed number of mined candidate translations 
with the highest confidence scores. And add 
them to the original candidate translation set to 
form a revised candidate translation set.  
3.4 Re-ranking 
The aim of mining is to improve recall. And in 
the re-ranking phase, we hope to improve preci-
sion by assigning the correct translation a higher 
rank. The features considered here for the re-
ranking phase are listed as follows.  
Confidence Score The confidence score of 
in and jt  is not only useful for the mining phase, 
but also is useful for the re-ranking phase. The 
higher this score, the higher rank this candidate 
translation should be assigned.  
Inclusion Ratio For Bilingual Query This 
feature is defined as the percentage of the re-
turned web pages that the bilingual query is 
completely matched. It is computed as the fol-
lowing formula. 
( )
_ ( )
( )
i
i
i
h q
EHR BQ q
H q
=                           (12) 
where ( )ih q is the number of web pages that 
match the query iq completely, and ( )iH q is the 
total number of returned web pages for query iq . 
Candidate Inclusion Ratio for Monolingual 
Query and Mix-language Query This feature is 
defined as the percentage of the returned web 
pages that the candidate translation is completed 
matched. This feature for monolingual query is 
computed as formula 13, and this feature for 
mix-language query is computed as formula 14. 
( )_ ( ) ( )
i
i
i
h sECHR MlQ s H q=                (13) 
( )_ ( ) ( )
i
i
i
h sECHR MixQ s H q=              (14) 
where ( )ih s  is the number of web pages that 
match the candidate translation is completely, and 
147
( )iH q is the total number of returned web pages 
for query iq .  
Finally, the above features are combined with 
following formula.  
2
1( , ) ( , ) _ ( )i j i j i
i
R n t C n t EHR BQ q
N
??= + ?  
3 _ ( )i
i
ECHR MlQ s
M
?+ ?
4 _ ( )i
i
ECHR MixQ s
L
?+ ?              (15) 
where N is the number of candidate transla-
tions, M and L  are the number of monolingual 
queries and mix-language queries respectively. 
At last the revised candidate translation set is 
re-ranked according to this formula, and the top-
K results are outputted as the input?s translation 
results.  
4 Parameters Evaluations 
In above formula (4), formula (11) and formula 
(15), the parameters i? are interpolation feature 
weights, which reflect the importance of different 
features. We use some held-our organization 
name pairs as development set to train these pa-
rameters. For those parameters in formula (4), we 
used those considered features solely one by one, 
and evaluated their importance according to their 
corresponding inclusion ratio of correct transla-
tions when using mix-language expansion and 
the final weights are assigned according to the 
following formula. 
i
i
i
i
InclusionRate
InclusionRate
? = ?                   (16) 
Where iInclusionRate  is the inclusion rate 
when considered feature if  only. The inclusion 
rate is defined as the percentage of correct trans-
lations that are contained in the returned web 
pages as Huang et al[2005] did. 
To obtain the parameters in formula (11), we 
used those considered features solely one by one, 
and computed their corresponding precision on 
development set respectively, and final weights 
are assigned according to following formula. 
i
i
i
i
P
P
? = ?                              (17) 
Where iP  is the precision when considered 
feature if  only. And for the parameters in for-
mula (15), their assignment method is the same 
with the method used for formula (11). 
5 Experiments and Discussions 
We use a Chinese to English organization name 
translation task to evaluate our approach. The 
experiments consist of four parts. Firstly, we 
evaluate the contribution of the correlative 
named entities for obtaining the web pages that 
contain the correct translation of the input. Sec-
ondly, we evaluate the contribution of different 
query expansion methods. Thirdly, we investi-
gate to which extents our approach can solve the 
two problems mentioned in section 1. Finally, we 
evaluate how much our approach can improve 
the overall recall and precision. Note that for 
simplicity, we use 10-best outputs from Moses as 
the original candidate translations for every input. 
And the search engine used here is Live7. 
5.1 Test Set 
The test set consists of 247 Chinese organization 
names recognized from 2,000 web pages that are 
downloaded from Sina8. These test organization 
names are translated by a bilingual speaker given 
the text they appear in. And these translations are 
verified from their official government web 
pages respectively. During translation, we don?t 
use any contextual information. 
5.2 Contribution of Correlative Named En-
tities 
The contribution of correlative named entities is 
evaluated by inclusion rate, and we compare the 
inclusion rate with different amount of correla-
tive named entities and different amount of re-
turned web pages. The experimental results are 
shown in Table 1 (here we use all these three 
kinds of expanding strategies).  
# of correlative named enti-
ties used 
 
1 5 10 
1 0.17 0.39 0.47 
5 0.29 0.63 0.78 
#of web 
pages used
10    0.32 0.76 0.82 
Table 1. Comparisons of inclusion rate  
From these results we can find that our ap-
proach obtains an inclusion rate of 82% when we 
use 10 correlative named entities and 10 returned 
web pages. We notice that there are some Chi-
nese organization names whose correct English 
translations have multiple standards. For exam-
ple,  the organization name ?????is translated 
                                                 
7  http://www.live.com/ 
8  http://news.sina.com.cn/ 
148
into ?Department of Defense? when it refers to a 
department in US, but  is translated into ?Minis-
try of Defence? when it refers to a department in 
UK or in Singapore. This problem affects the 
actual inclusion rate of our approach. Another 
factor that affects the inclusion rate is the search 
engine used. There is a small difference in the 
inclusion rate when different search engines are 
used. For example, the Chinese organization 
name ?????/China CITIC Bank?, because 
the word ???? is an out-of-vocabulary word,  
the best output from Moses is ?of the bank?. 
With such candidate translation, none of our 
three expansion methods works. But when we 
used Google as search engine instead, we mined 
the correct translation. 
From these results we can conclude that by us-
ing correlative named entities, the returned web 
pages are more likely to contain the correct trans-
lations of the input organization names. 
5.3 Contribution of Three Query Expansion 
Methods 
In this section, we evaluate the contribution of 
these three query expansion methods respectively. 
To do this, we use them one by one during trans-
lation, and compare their inclusion rates respec-
tively. Experimental results are shown in Table 2. 
#of web pages 
used 
 
1 5 10
1 0.002 0.0020.004
5 0.017 0.0190.019
Monolingual 
Expansion 
Only 10 0.021 0.0370.051
1 0.112 0.1590.174
5 0.267 0.3270.472
Bilingual 
 Expansion
Only 10 0.285 0.4140.669
1 0.098 0.1380.161
5 0.231 0.3070.386
# of  
correlative 
named enti-
ties used 
Mix-language 
Expansion
Only 10 0.249 0.3980.652
Table 2. Inclusion rate of different kinds of query 
expansion methods 
From Table 2 we can see that bilingual expan-
sion and mix-language expansion play greater 
roles than monolingual expansion in obtaining 
the web pages that contain the correct transla-
tions of the inputs. This is because the condition 
of generating monolingual queries is too strict, 
which requires a reliable translation for the cor-
relative named entity. In most cases, this condi-
tion cannot be satisfied. So for many input or-
ganization names, we cannot generate any mono-
lingual queries for them at all. This is the reason 
why monolingual expansion obtains so poorer an 
inclusion rate compared with the other two ex-
pansions. To evaluate the true contribution of 
monolingual expansion method, we carry out 
another experiment. We select 10 organization 
names randomly from the test set, and translate 
all of their correlative named entities into English 
by a bilingual speaker. Then we evaluate the in-
clusion rate again on this new test set. The ex-
perimental results are shown in Table 3. 
# of correlative named enti-
ties used 
 
1 5 10 
1 0.2 0.3 0.6 
5 0.4 0.7 0.9 
#of web 
pages used
10    0.4 0.8 0.9 
Table 3. Inclusion rate for monolingual expan-
sion on new test set 
From Table 3 we can conclude that, if most of 
the correlative named entities can be reliably 
translated, the queries generated by this mono-
lingual expansion will play greater role in obtain-
ing the web pages that contain the correct trans-
lations of the inputs. 
From those results in Table 2 we can conclude 
that, these three kinds of expansions complement 
each other. Using them together can obtain 
higher inclusion rate than using anyone of them 
only. 
5.4 Efficiency on Solving Problem I and 
Problem II 
In this section, we investigate to which extents 
our approach can solve the two problems men-
tioned in section 1.We compare the wrong trans-
lation numbers caused by these two problems 
(another main kind of translation error is caused 
by the translation of out-of-vocabulary words) 
between Moses and our approach. The experi-
mental results are shown in Table 4.  
 Moses Results Our method
Problem I 44 3 
Problem II 30 0 
Table 4. Comparison of error numbers 
From Table 4 we can see that our approach is 
very effective on solving these two problems. 
Almost all of the errors caused by these two 
problems are corrected by our approach. Only 
three wrong translations are not corrected. This is 
because that there are some Chinese organization 
names whose correct English translations have 
multiple standards, such as the correct translation 
of organization name ?????depends on its 
nationality, which has been explained in section 
5.2. 
149
5.5 Our Approach vs. Other Approaches  
In this section, we compare our approach with 
other two methods: Moses and the approach pro-
posed by Huang et al [2005]. We compare their 
accuracy of Top-K results. For both our approach 
and Huang et al?s approach, we use 10 correla-
tive words for each input organization name and 
use 10 returned web pages for mining the correct 
translation result. The experimental results are 
shown in Table 5. 
 Moses  
Results 
Huang?s 
Results 
Our  
Results 
Top 1 0.09 0.44 0.53 
Top 5 0.18 0.61 0.73 
Top 10 0.31 0.68 0.79 
Table 5. Moses results vs. our results 
Moses is a state-of-the-art translation method, 
but it can hardly handle the organization name 
translation well. In addition to the errors caused 
by the above two problems mentioned in section 
1, the out-of-vocabulary problem is another ob-
stacle for Moses. For example, when translating 
the organization name ?????????
/International Tsunami Information Centre?, be-
cause the word ???? is an out-of-vocabulary 
word, Moses fails to give correct translation. But 
for those approaches that have a web mining 
process during translation, both the out-of-
vocabulary problem and the two problems men-
tioned in section 1 are less serious. This is the 
reason that Moses obtains the lowest perform-
ance compared with the other two approaches. 
Our approach is also superior to Huang?s method, 
as shown in the above table. We think this is be-
cause of the following three reasons. The first 
reason is that in our approach, we use a transla-
tion candidate generation process. Although 
these candidates are usually not so good, they 
can still provide some very useful clue informa-
tion for the web retrieval process. The second 
reason is that the features considered for correla-
tive words extraction in our approach are more 
comprehensive. Most of the time (except for the 
case that the input is not included in the correla-
tive word list) our approach is more prone to ob-
tain better correlative words for the input. The 
third reason is that our approach use more query 
expansion strategies than Huang?s approach. 
These expansion strategies may complement 
each other and improve the probability of obtain-
ing the web pages that contain the correct trans-
lations For example, both Moses and Huang?s 
approach failed to translate the organization 
name ??????????. But in our approach, 
with the candidate translation ?International In-
formation Centre? that is generated by Moses, 
our approach still can obtain the web page that 
contains the correct translation when using bilin-
gual expansion. Thus the correct translation ?In-
ternational Tsunami Information Centre? is 
mined out during the sequent mining process.  
From table 5 we also notice that the final re-
call of our approach is a little lower than the in-
clusion rate as show in table 1. This means that 
our approach doesn?t mine all the correct transla-
tions that are contained in the returned web pages. 
One of the reasons is that some of the input or-
ganization names are not clearly expressed. For 
example, an input organization name ?????
??, although its correct translation ?University 
of California, Berkeley? is contained in the re-
turned web pages, this correct translation cannot 
be mined out by our approach. But if it is ex-
pressed as ??????????????, its 
correct translation can be mined from the re-
turned web pages easily. Besides, the recognition 
errors of NER toolkits will also reduce the final 
recall of our approach.  
6 Conclusions and Future Work 
In this paper, we present a new organization 
name translation approach. It uses some correla-
tive named entities of the input and some query 
expansion strategies to help the search engine to 
retrieve those web pages that contain the correct 
translation of the input. Experimental results 
show that for most of the inputs, their correct 
translations are contained in the returned web 
pages. By mining these correct translations and 
re-ranking them, the two problems mentioned in 
section 1 are solved effectively. And recall and 
precision are also improved correspondingly.  
In the future, we will try to improve the ex-
traction perform of correlative named entities. 
We will also try to apply this approach to the 
person name translation and location name trans-
lation. 
Acknowledgments  
This work was supported by the open fund of 
National Laboratory of Pattern Recognition, In-
stitute of Automation Chinese Academy of Sci-
ence, P.R.C, and was also supported in part by 
National Science Foundation of China 
(60873091), Natural Science Foundation of 
Liaoning Province (20072032) and Shenyang 
Science and Technology Plan (1081235-1-00). 
150
References 
Chen Hsin-Hsi, Changhua Yang, and Ying Lin. 2003. 
Learning formulation and transformation rules for 
multilingual named entities. Proceedings of the 
ACL 2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition. pp1-8. 
Dekang Lin, Shaojun Zhao, Durme Benjamin Van 
Drume, Marius Pasca. Mining Parenthetical Trans-
lations from the Web by Word Alignment,  ACL08. 
pp994-1002. 
Fan Yang, Jun Zhao, Bo Zou, Kang Liu, Feifan Liu. 
2008. Chinese-English Backward Transliteration 
Assisted with Mining Monolingual Web Pages. 
ACL2008. pp541-549. 
Fei Huang, Stephan Vogel and Alex Waibel. 2003. 
Automatic Extraction of Named Entity Translin-
gual Equivalence Based on Multi-feature Cost 
Minimization. Proceedings of the 2003 Annual 
Conference of the Association for Computational 
Linguistics, Workshop on Multilingual and Mixed-
language Named Entity Recognition.   
Fei Huang, Stephan vogel and Alex Waibel. 2004. 
Improving Named Entity Translation Combining 
Phonetic and Semantic Similarities. Proceedings of 
the HLT/NAACL. pp281-288.  
Fei Huang, Ying Zhang, Stephan Vogel. 2005. Min-
ing Key Phrase Translations from Web Corpora. 
HLT-EMNLP2005, pp483-490. 
Feng, Donghui, Yajuan LV, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP 2004), pp372-379. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. ACL2003. 
pp160-167. 
Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. Learning 
Transliteration Lexicon from the Web. COL-
ING/ACL2006. pp1129-1136. 
Hany Hassan and Jeffrey Sorensen. 2005. An Inte-
grated Approach for Arabic-English Named Entity 
Translation. Proceedings of ACL Workshop on 
Computational Approaches to Semitic Languages. 
pp87-93. 
Lee, Chun-Jen and Jason S.Chang and Jyh-Shing 
Roger Jang. 2004a. Bilingual named-entity pairs 
extraction from parallel corpora. Proceedings of 
IJCNLP-04 Workshop on Named Entity Recogni-
tion for Natural Language Processing Application. 
pp9-16. 
Lee, Chun-Jen, Jason S.Chang and Thomas C. 
Chuang. 2004b. Alignment of bilingual named en-
tities in parallel corpora using statistical model. 
Lecture Notes in Artificial Intelligence. 3265:144-
153. 
Lee, Chun-Jen, Jason S.Chang, and Jyh-Shing Roger 
Jang. 2005. Extraction of transliteration pairs from 
parallel corpora using a sta Acquisition of English-
Chinese transliterated word pairs from parallel-
aligned text using a statistical transliteration model. 
Information Sciences.  
Long Jiang, Ming Zhou, Lee-Feng Chien, Cheng Niu. 
[2007]. Named Entity Translation with Web Min-
ing and Transliteration. IJCAI-2007. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases form parallel corpora. ACL-
2003. pp259-266. 
Peter F. Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2):263-311.  
Y. Al-Onaizan and K. Knight. 2002. Translating 
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meeting 
of the Association for Computational Linguistics, 
pp400-408. 
Ying Zhang and Phil Vines Using the Web for Auto-
mated Translation Extraction in Cross-Language 
Information Retrieval. SIGIR2004,pp162-169. 
Yufeng Chen, Chengqing Zong. A Structure-based 
Model for Chinese Organization Name Translation. 
ACM Transactions on Asian Language Information 
Processing, 2008, 7(1), pp1-30. 
151
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1344?1352,
Beijing, August 2010
Heterogeneous Parsing via Collaborative Decoding
Muhua Zhu Jingbo Zhu Tong Xiao
Natural Language Processing Lab.
Northeastern University
zhumuhua@gmail.com
{zhujingbo, xiaotong}@mail.neu.edu.cn
Abstract
There often exist multiple corpora for the
same natural language processing (NLP)
tasks. However, such corpora are gen-
erally used independently due to distinc-
tions in annotation standards. For the pur-
pose of full use of readily available hu-
man annotations, it is significant to simul-
taneously utilize multiple corpora of dif-
ferent annotation standards. In this pa-
per, we focus on the challenge of con-
stituent syntactic parsing with treebanks
of different annotations and propose a col-
laborative decoding (or co-decoding) ap-
proach to improve parsing accuracy by
leveraging bracket structure consensus be-
tween multiple parsing decoders trained
on individual treebanks. Experimental re-
sults show the effectiveness of the pro-
posed approach, which outperforms state-
of-the-art baselines, especially on long
sentences.
1 Introduction
Recent years have seen extensive applications of
machine learning methods to natural language
processing problems. Typically, increase in the
scale of training data boosts the performance of
machine learning methods, which in turn en-
hances the quality of learning-based NLP systems
(Banko and Brill, 2001). However, annotating
data by human is expensive in time and labor. For
this reason, human-annotated corpora are consid-
ered as the most valuable resource for NLP.
In practice, there often exist more than one cor-
pus for the same NLP tasks. For example, for
constituent syntactic parsing (Collins, 1999; Char-
niak, 2000; Petrov et al, 2006) in Chinese, in ad-
dition to the most popular treebank Chinese Tree-
bank (CTB) (Xue et al, 2002), there are also
other treebanks such as Tsinghua Chinese Tree-
bank (TCT) (Zhou, 1996). For the purpose of
full use of readily available human annotations
for the same tasks, it is significant if such cor-
pora can be used jointly. At first sight, a di-
rect combination of multiple corpora is a way to
this end. However, corpora created for the same
NLP tasks are generally built by different orga-
nizations. Thus such corpora often follow dif-
ferent annotation standards and/or even different
linguistic theories. We take CTB and TCT as
a case study. Although both CTB and TCT are
Chomskian-style treebanks, they have annotation
divergences in at least two dimensions: a) CTB
and TCT have dramatically different tag sets, in-
cluding parts-of-speech and grammar labels, and
the tags cannot be mapped one to one; b) CTB
and TCT have distinct hierarchical structures. For
example, the words ??? (Chinese) ?? (tradi-
tional) ?? (culture)? are grouped as a flat noun
phrase according to the CTB standard (right side
in Fig. 1), but in TCT, the last two words are in-
stead grouped together beforehand (left side in
Fig. 1). The differences cause such treebanks
of different annotations to be generally used in-
dependently. This paper is dedicated to solving
the problem of how to use jointly multiple dis-
parate treebanks for constituent syntactic parsing.
Hereafter, treebanks of different annotations are
1344
called heterogeneous treebanks, and correspond-
ingly, the problem of syntactic parsing with het-
erogeneous treebanks is referred to as heteroge-
neous parsing.
Previous work on heterogeneous parsing is of-
ten based on treebank transformation (or treebank
conversion) (Wang et al, 1994; Niu et al, 2009).
The basic idea is to transform annotations of one
treebank (source treebank) to fit the standard of
another treebank (target treebank). Due to diver-
gences of treebank annotations, such transforma-
tion is generally achieved in an indirect way by
selecting transformation results from the output of
a parser trained on the target treebank. A com-
mon property of all the work mentioned above is
that transformation accuracy is heavily dependent
on the performance of parsers trained on the tar-
get treebank. Sometimes transformation accuracy
is not so satisfactory that techniques like instance
pruning are needed in order to refine transforma-
tion results (Niu et al, 2009).
We claim there exists another way, interesting
but less studied for heterogeneous parsing. The
basic idea is that, although there are annotation
divergences between heterogenous treebanks, ac-
tually we can also find consensus in annotations
of bracket structures. Thus we would like to train
parsers on individual heterogeneous treebanks and
guide the parsers to gain output with consensus in
bracket structures as much as possible when they
are parsing the same sentences.
To realize this idea, we propose a generic col-
laborative decoding (or co-decoding) framework
where decoders trained on heterogeneous tree-
banks can exchange consensus information be-
tween each other during the decoding phase. The-
oretically the framework is able to incorporate a
large number of treebanks and various functions
that formalize consensus statistics.
Our contributions can be summarized: 1) we
propose a co-decoding approach to directly uti-
lizing heterogeneous treebanks; 2) we propose a
novel function to measure parsing consensus be-
tween multiple decoders. We also conduct ex-
periments on two Chinese treebanks: CTB and
TCT. The results show that our approach achieves
promising improvements over baseline systems
which make no use of consensus information.
np
nS
??
np
a
??
n
??
NP
NR
??
NN
??
NN
??
??????
(Chinese) (traditional) (culture)
Figure 1: Example tree fragments with TCT (left)
and CTB (right) annotations
2 Collaborative Decoding-based
Heterogeneous Parsing
2.1 Motivation
This section describes the motivation to use
co-decoding for heterogeneous parsing. We first
use the example in Fig. 1 to illustrate what con-
sensus information exists between heterogenous
treebanks and why such information might help
to improve parsing accuracy. This figure contains
two partial parse trees corresponding to the
words ??? (Chinese) ?? (traditional) ??
(culture)?, annotated according to the TCT (left
side) and CTB (right side) standards respectively.
Despite the distinctions in tag sets and bracket
structures, these parse trees actually have partial
agreements in bracket structures. That is, not all
bracket structures in the parse trees are different.
Specifically put, although the internal structures
of the parse trees are different, both CTB and
TCT agree to take ??? ?? ??? as a noun
phrase. Motivated by this observation, we would
like to guide parsers that are trained on CTB and
TCT respectively to verify their output interac-
tively by using consensus information implicitly
contained in these treebanks. Better performance
is expected when such information is considered.
A feasible framework to make use of consensus
information is n-best combination (Henderson
and Brill, 1999; Sagae and Lavie, 2006; Zhang et
al., 2009; Fossum and Knight, 2009). In contrast
1345
to previous work on n-best combination where
multiple parsers, say, Collins parser (Collins,
1999) and Berkeley parser (Petrov et al, 2006)
are trained on the same training data, n-best
combination for heterogeneous parsing is instead
allowed to use either a single parser or multiple
parsers which are trained on heterogeneous
treebanks. Consensus information can be incor-
porated during the combination of the output
(n-best list of full parse trees following distinct
annotation standards) of individual parsers. How-
ever, despite the success of n-best combination
methods, they suffer from the limited scope of
n-best list. Taking this into account, we prefer
to apply the co-decoding approach such that
consensus information is expected to affect the
entire procedure of searching hypothesis space.
2.2 System Overview
The idea of co-decoding is recently extensively
studied in the literature of SMT (Li et al, 2009;
Liu et al, 2009). As the name shows, co-decoding
requires multiple decoders be combined and pro-
ceed collaboratively. As with n-best combination,
there are at least two ways to build multiple de-
coders: we can either use multiple parsers trained
on the same training data (use of diversity of mod-
els), or use a single parser on different training
data (use of diversity of datasets) 1. Both ways
can build multiple decoders which are to be inte-
grated into co-decoding. For the latter case, one
method to get diverse training data is to use dif-
ferent portions of the same training set. In this
study we extend the case to an extreme situation
where heterogeneous treebanks are used to build
multiple decoders.
Fig. 2 represents a basic flow chart of heteroge-
neous parsing via co-decoding. Note that here we
discuss the case of co-decoding with only two de-
coders, but the framework is generic enough to in-
tegrate more than two decoders. For convenience
of reference, we call a decoder without incorpo-
rating consensus information as baseline decoder
1To make terminologies clear, we use parser as its regular
sense, including training models (ex. Collins model 2) and
parsing algorithms (ex. the CKY algorithm used in Collins
parser), and we use decoder to represent parsing algorithms
with specified parameter values
treebank1 treebank2
decoder1 decoder2
co-decoding
test data
Figure 2: Basic flow chart of co-decoding
and correspondingly refer to a decoder augmented
with consensus information as member decoder.
So the basic steps of co-decoding for heteroge-
neous parsing is to first build baseline decoders on
heterogeneous treebanks and then use the baseline
decoders to parse sentences with consensus infor-
mation exchanged between each other.
To complete co-decoding for heterogeneous
parsing, three key components should be consid-
ered in the system:
? Co-decoding model. A co-decoder con-
sists of multiple member decoders which are
baseline decoders augmented with consen-
sus information. Co-decoding model de-
fines how baseline decoders and consensus
information are correlated to get member de-
coders.
? Decoder coordination. Decoders in the co-
decoding model cannot proceed indepen-
dently but should have interactions between
each other in order to exchange consensus in-
formation. A decoder coordination strategy
decides on when, where, and how the inter-
actions happen.
? Consensus-based score function. Consensus-
based score functions formalize consensus
information between member decoders. Tak-
ing time complexity into consideration, con-
sensus statistics should be able to be com-
puted efficiently.
1346
In the following subsections, we first present
the generic co-decoding model and then describe
in detail how member decoders collaborate. Fi-
nally we introduce a novel consensus-based score
function which is used to quantify consensus in-
formation exchanged between member decoders.
2.3 Generic Co-decoding Model
The generic co-decoding model described here is
also used in (Li et al, 2009) for co-decoding of
machine translators. For a given sentence S, a
parsing algorithm (decoder) seeks a parse tree T ?
which is optimal in the sense that it maximizes
some score function F (T ), as shown in Eq. 1.
T ? = argmax
Ts.t.S=yield(T )
F (T ) (1)
where Ts.t.S = yield(T ) represents the set of
parse trees that yield the input sentence S. For
baseline decoders, the score function F (T ) is
generally just the inside probability P (T ) 2 of
a tree T , defined as the product of probabili-
ties of grammar rules appearing in parse tree T :?
r?R(T ) P (r). In the co-decoding framework,
F (T ) is extended so as to integrate consensus-
based score functions which measure consensus
information between member decoders, as shown
in Eq. 2.
Fm(T ) = Pm(T ) +
n?
k,k 6=m
?k(Hk(S), T ) (2)
We use dk to denote the kth decoder and use
Hk(S) to denote corresponding parsing hypoth-
esis space of decoder dk. Moreover, Pm(T ) is
referred to as baseline score given by baseline
decoders and ?k(Hk(S), T ) is consensus score
between decoders dm and dk, which is defined
as a linear combination of consensus-based score
functions, as shown in Eq. 3.
?k(Hk(S), T ) =
?
l
?k,lfk,l(Hk(S), T ) (3)
where fk,l(Hk(S), T ) represents a consensus-
based score function between T and Hk(S),
and ?k,l is the corresponding weight. Index l
2Actually, the joint probability P(S,T) of sentence S and
parse tree T is used, but we can prove that P (S, T ) = P (T ).
ranges over all consensus-based score functions
in Eq. 3. Theoretically we can define a variety
of consensus-based score functions.
For the simplest case where there are only two
member decoders and one consensus-based score
function, Eq. 2 and Eq. 3 can be combined and
simplified into the equation
Fi(T ) = Pi(T ) + ?1?if(H1?i(S), T ) (4)
where index i is set to the value of either 1 or 0.
This simplified version is used in the experiments
of this study.
2.4 Decoder Coordination
This subsection discusses the problem of decoder
coordination. Note that although Eq. 2 is defined
at sentence level, the co-decoding model actu-
ally should be applied to the parsing procedure
of any subsequence (word span) of sentence S.
So it is natural to render member decoders col-
laborate when they are processing the same word
spans. To this end, we would like to adopt best-
first CKY-style parsing algorithms as baseline de-
coders, since CKY-style decoders have the prop-
erty that they process word spans in the ascend-
ing order of span sizes. Moreover, the hypothe-
ses 3 spanning the same range of words are read-
ily stacked together in a chart cell before CKY-
style decoders move on to process other spans.
Thus, member decoders can process the same
word spans collaboratively from small ones to big
ones until they finally complete parsing the entire
sentence.
A second issue in Eq. 2 is that consensus-
based score functions are dependent on hypoth-
esis space Hk(S). Unfortunately, the whole hy-
pothesis space is not available most of the time.
To address this issue, one practical method is to
approximate Hk(S) with a n-best hypothesis list.
For best-first CKY parsing, we actually retain all
unpruned partial hypotheses over the same span
as the approximation. Hereafter, the approxima-
tion is denoted as H?k(S)
Finally, we notice in Eq. 2 that consensus score
3In the literature of syntactic parsing, especially in chart
parsing, hypotheses is often called edges. This paper will
continue to use the terminology hypothesis when no ambigu-
ity exists.
1347
?k(Hk(S), T ) and Hk(S) form a circular depen-
dency: searching for Hk(S) requires both base-
line score and consensus score; on the other hand,
calculating consensus score needs Hk(S) (its ap-
proximation in practice) to be known beforehand.
Li et al (2009) solves this dilemma with a boot-
strapping method. It starts with seedy n-best lists
generated by baseline decoders and then alter-
nates between calculating consensus scores and
updating n-best hypothesis lists. Such bootstrap-
ping method is a natural choice to break down the
circular dependency, but multi-pass re-decoding
might dramatically reduce decoding efficiency.
Actually, Li et al (2009) restricts the iteration
number to two in their experiments. In this paper,
we instead use an alternative to the bootstrapping
method. The process is described as follows.
1. In traditional best-first CKY-style parsing al-
gorithms, hypotheses over the same word
spans are grouped according to some crite-
rion of hypothesis equivalence 4. Among
equivalent hypotheses, only a single optimal
hypothesis is retained. In this paper, we in-
stead keep top k of equivalent hypotheses in
a data structure called best-first cache.
2. Use hypotheses in best-first caches to ap-
proximate Hk(S), and calculate consensus
score ?k(Hk(S), T ) between decoders.
3. Use baseline score and consensus score to lo-
cally rerank hypotheses in best-first caches.
Then remove hypotheses in caches except the
top one hypothesis.
In this study, we choose the best-first CKY-style
parsing algorithm used in Collins parser (Collins,
1999). Algorithm 1 extends this algorithm for co-
decoding. The first two steps initialize baseline
decoders and assign appropriate POS tags to sen-
tence St. Since baseline decoders are built on het-
erogeneous treebanks, POS taggers correspond-
ing to each baseline decoder are demanded, unless
gold POS tags are provided. The third step is the
core of the co-decoding algorithm. Here the com-
plete procedure invokes baseline decoders to com-
4the simplest criterion of equivalence is whether hypothe-
ses have the same grammar labels.
Algorithm 1 CKY-style Co-decoding
Argument: dk{the set of baseline decoders}
St{a sentence to be parsed}
Begin
Steps:
1. assign POS tags to sentence St
2. initialize baseline decoders dk
3. for span from 2 to sentence length do
for start from 1 to (sentence length-span+1) do
end := (start + span - 1)
for each base decoder dk do
complete(dk , start, end)
do co-decoding(start, end)
End
Subroutine:
complete(dk, start, end): base decoder dk generates
hypotheses over the span (begin.end), and fills in best-
first caches.
co-decoding(start, end): calculate consensus score
and rerank hypotheses in best-first caches. The top 1 is
chosen to be the best-first hypothesis.
plete parsing on the span [start, end] and gener-
ates H?k(s). The co-decoding procedure calculates
consensus score and locally reranks hypotheses in
best-first caches.
2.5 Consensus-based Score Function
There are at least two feasible ways to mea-
sure consensus between constituency parse trees.
By viewing parse trees from diverse perspectives,
we can either use functions on bracket structures
of parse trees, as in (Wang et al, 1994), or
use functions on head-dependent relations by first
transforming constituency trees into dependency
trees, as in (Niu et al, 2009). Although the co-
decoding model is generic enough to integrate var-
ious consensus-based score functions in a uniform
way, this paper only uses a bracket structure-based
function.
As mentioned above, the function proposed in
(Wang et al, 1994) is based on bracket struc-
tures. Unfortunately, that function is not appli-
cable in the situation of this paper. The reason is
that, the function in (Wang et al, 1994) is de-
fined to work on two parse trees, but this paper
instead needs a function on a tree T and a set of
trees (the approximation H?k(S)). To this end, we
first introduce the concept of constituent set (CS)
of a parse tree. Conceptually, CS of a parse tree is
a set of word spans corresponding to all the sub-
1348
64
1
5
2 3
[1,3],[2,3],[1,1]
[1,1]
[2,3],[2,2],[3,3]
[1,1]
[2,2]
[3,3]
Figure 3: Constituent set of a synthetic parse tree
trees of the tree, as illustrated in Fig. 3. For exam-
ple, the constituent set of the tree rooted at node
6 has three elements: [1, 1], [1, 3], and [1, 2]. For
H?k(S), the constituent set is defined as the union
of constituent sets of all elements it contains.
CS(H?k(S)) =
?
T?H?k(S)
CS(T )
In practice, we need to cut off elements in
CS(H?k(S)) in order to retain most confident
word spans.
With the concept of constituent set, a
consensus-based score function on T and H?k(S)
can be defined as follows.
f(H?k(S), T ) =
?
c?CS(T ) I(c, CS(H?k(S)))
|CS(T )| (5)
where I(c, CS(H?k(S))) is an indicator function
which returns one if c ? CS(T ) is compatible
with all the elements in CS(H?k(S)), zero oth-
erwise. Two spans, [a, b] and [i, j] are said to
be compatible if they satisfy one of the following
conditions: 1) i > b; 2) a > j; 3) a ? i ? b and
j ? b; 4) i ? a ? j and b ? j. Fig 4 uses two
example to illustrate the concept of compatibility.
3 Experiments
3.1 Data and Performance Metric
The most recent version of the CTB corpus, CTB
6.0 and the CIPS ParsEval data are used as hetero-
geneous treebanks in the experiments. Following
the split utilized in (Huang et al, 2007), we di-
vided the dataset into blocks of 10 files. For each
w1 w2 w3 w4 w1 w2 w3 w4
Figure 4: left) two spans conflict; right) two spans
are compatible
block, the first file was added to the CTB develop-
ment data, the second file was added to the CTB
testing data, and the remaining 8 files were added
to the CTB training data. For the sake of parsing
efficiency, we randomly sampled 1,000 sentences
of no more than 40 words from the CTB test set.
CTB-Partitions Train Dev Test
#Sentences 22,724 2,855 1,000
#Words 627,833 78,653 25,100
Ave-Length 30.1 30.0 20.3
TCT-Partitions Train Dev Test
#Sentences 32,771 N/A 1,000
#Words 354,767 N/A 10,400
Ave-Length 10.6 N/A 10.4
Table 1: Basic statistics on the CTB and TCT data
CIPS-ParsEval data is publicly available for the
first Chinese syntactic parsing competition, CIPS-
ParsEval 2009. Compared to CTB, sentences in
CIPS-ParsEval data are much shorter in length.
We removed sentences which have words less
than three. CIPS-ParsEval test set has 7,995 sen-
tences after sentence pruning. As with the CTB
test set, we randomly sampled 1,000 sentences
for evaluating co-decoding performance. Since
CIPS-ParsEval data is actually a portion of the
TCT corpus, for convenience of reference, we will
refer to CIPS-ParsEval data as TCT in the follow-
ing sections. Table 1 contains statistics on CTB
and TCT.
The two training sets are used individually to
build baseline decoders. With regard to the test
sets, each sentence in the test sets should have
two kinds of POS tags, according to the CTB and
TCT standards respectively. To this end, we ap-
plied a HMM-based method for POS annotation
transformation (Zhu and Zhu, 2009). During the
POS transformation, the divergences of word seg-
mentation are omitted.
For all experiments, bracketing F1 is used as
the performance metric, provided by EVALB 5.
5http://nlp.cs.nyu.edu/evalb
1349
3.2 Baseline Decoders
As already mentioned above, we apply Collins
parser in this paper. Specifically speaking, two
CKY-style baseline decoders to participate co-
decoding are built on CTB and TCT respectively
with Collins model two. For the CTB-based de-
coder, we use the CTB training data with slight
modifications: we replaced POS tags of punctua-
tions with specific punctuation symbols.
To get the TCT-based decoder, we made follow-
ing modifications. Firstly, TCT is available with
manually annotated head indices for all the con-
stituents in parse trees. For example, a grammar
label, say, np-1, means that the constituent is a
noun phrase with the second child being its head
child. In order to relax context independence as-
sumptions made in PCFG, we appended head in-
dices to grammar labels to get new labels, for ex-
ample np1. Secondly, since Collins parser is a
lexicalized parser, head rules specific to the TCT
corpus were manually created, which are used to-
gether with readily available head indices. Such
adaptation is also used in (Chen et al, 2009);
3.3 Parsing Results
We conduct experiments on both CTB and TCT
test sets. Two parameters need to be set: the cut-
off threshold for constructing constituent set of
H?k(S) and the weight ? 6 of consensus score in
Eq. 4. We tuned the parameters on the CTB de-
velopment set and finally set them to 5 and 20
respectively in the experiments. Table 2 presents
bracketing F1 scores of baseline systems and the
co-decoding approach. Here, the row of baseline
represents the performance of individual baseline
decoders, and the comparison of baseline and co-
decoding on a test set, say CTB, demonstrates
how much boosting the other side, say TCT, can
supply. For the co-decoding approach, the size
of best-first cache is set to 5 which achieves the
best result among the cache sizes we have experi-
mented.
As the results show, co-decoding achieves
promising improvements over baseline systems
on both test sets. Interestingly, we see that the
improvement on the TCT test set is larger than
6We use the same ? for both member decoders.
Test Set CTB TCT
Baseline 79.82 81.02
Co-decoding 80.33 81.77
Table 2: Baseline and Co-decoding on the CTB
and TCT test sets
that on the CTB test set. In general, a relatively
strong decoder can improve co-decoding perfor-
mance more than a relatively weak decoder does.
At the first sight, the TCT-based decoder seems to
have better performance than the CTB-based de-
coder. But if taking sentence length into consid-
eration, we can find that the TCT-based decoder
is actually relatively weak. Table 3 shows the
performance of the CTB-based decoder on short
sentences.
3.4 Analysis
Fig. 5 shows the bracketing F1 on the CTB test set
at different settings of the best-first cache size C .
F1 scores reach the peak before C increases to 6.
As a result, we set C to 5 in all our experiments.
 79
 79.5
 80
 80.5
 81
 0  1  2  3  4  5  6
br
ac
ke
tin
g 
F1
size of best-first cache
CTB
Figure 5: Bracketing F1 with varying best-first
cache size
To evaluate the effect of sentence length on co-
decoding, Table 3 presents F1 scores on portions
of the CTB test set, partitioned according to sen-
tence length. From the results we can see that
co-decoding performs better on long sentences.
One possible reason is that member decoders have
more consensus on big spans. Taking this obser-
vation into consideration, one enhancement to the
co-decoding approach is to enable co-decoding
only on long sentences. This way, parsing ef-
1350
Partitions [0,10] (10,20] (20,30] (30,40]
# Sentence 276 254 266 204
Ave-Length 6.07 15.64 25.43 35.20
Baseline 92.83 84.34 78.98 76.69
Co-decoding 92.84 84.36 79.43 77.65
Table 3: Effect of sentence length on co-decoding
performance
ficiency of co-decoding can be improved. It is
worth emphasizing that co-decoding is still help-
ful for parsers whose performance on short sen-
tences is not satisfactory, as shown in Table 2.
Another interesting analysis is to check how
many parsing results are affected by co-decoding,
compared to baseline decoders. Table 4 shows
the statistics.
Test Set # All # Improved # Decreased
CTB 1000 225 109
TCT 1000 263 92
Table 4: Statistics on sentences of test data
As the table shows, although overall accuracy is
increased, we find that on some sentences, co-
decoding instead worsens parsing accuracy. In
order to get insights on error sources, we manu-
ally analyzed 20 sentences on which co-decoding
achieves negative results. We find a large por-
tion (14 of 20) of sentences are short sentences
(of words less than 20). Actually, due to high ac-
curacy of the CTB-based decoder on short sen-
tences, co-decoding is indifferent when this de-
coder is processing short sentences. And we also
find that some errors are derived from differences
in annotation standards. Fortunately, the diver-
gence of annotations mainly exists in relatively
small spans. So one solution to the problem is to
enable co-decoding on relatively big spans. These
will be done in our future work.
4 Related Work
4.1 System Combination
In the literature of syntactic parsing, n-best com-
bination methods include parse selection, con-
stituent recombination, production recombina-
tion, and n-best reranking. Henderson and Brill
(1999) performs parse selection by maximizing
the expected precision of selected parse with re-
spect to the set of parses to be combined. Sagae
and Lavie (2006) proposes to recombine con-
stituents from the output of individual parsers.
More recently, Fossum and Knight (2009) studies
a combination method at production level. Zhang
et al (2009) reranks n-best list of one parser with
scores derived from another parser.
Compared to n-best combination, co-decoding
(Li et al, 2009; Liu et al, 2009) combines sys-
tems during decoding phase. Theoretically, sys-
tem combination during decoding phase helps de-
coders to select better approximation to hypothe-
sis space, since pruning is practically unavoidable.
To the best of our knowledge, co-decoding meth-
ods have not been applied to syntactic parsing.
4.2 Treebank Transformation
The focus of this study is heterogeneous parsing.
Previous work on this challenge is generally based
on treebank transformation. Wang et al (1994)
describes a method for transformation between
constituency treebanks. The basic idea is to train
a parser on a target treebank and generate a n-best
list for each sentence in source treebank(s). Then,
a matching metric which is a function on the num-
ber of the same word spans between two trees is
defined to select a best parse from each n-best list.
Niu et al (2009) applies a closely similar frame-
work as with (Wang et al, 1994) to transform a
dependency treebank to a constituency one.
5 Conclusions
This paper proposed a co-decoding approach to
the challenge of heterogeneous parsing. Com-
pared to previous work on this challenge, co-
decoding is able to directly utilize heterogeneous
treebanks by incorporating consensus information
between partial output of individual parsers dur-
ing the decoding phase. Experiments demonstrate
the effectiveness of the co-decoding approach, es-
pecially the effectiveness on long sentences.
Acknowledgments
This work was supported in part by the National
Science Foundation of China (60873091). We
would like to thank our anonymous reviewers for
their comments.
1351
References
Banko, Michele and Eric Brill. 2001. Scaling to
very very large corpora for natural language dis-
ambiguation. In Proc. of ACL 2001, pages 26-33.
Chen, Xiao, Changning Huang, Mu Li, and Chunyu
Kit. 2009. Better Parser Combination. Technique
Report of CIPS-ParsEval 2009.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proc. of NAACL 2000, pages
132-139.
Fossum, Victoria and Kevin Knight. 2009. Combin-
ing constituent parsers. In Proc. of NAACL 2009,
pages 253-256.
Henderson, John and Eric Brill. 1999. Exploiting di-
versity in natural language processing. In Proc. of
SIGDAT-EMNLP 1999, pages 187-194.
Huang, Zhongqiang, Mary P. Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In Proc. of EMNLP-CoNLL
2007, pages 1093-1102.
Li, Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009. Collaborative decoding: par-
tial hypothesis re-ranking using trnaslationconsen-
sus between decoders. In Proc. of ACL 2009, pages
585-592.
Liu, Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint Decoding with Multiple Translation Models.
In Proc. of ACL 2009, pages 576-584.
Niu, Zheng-Yu, Haifeng Wang, Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In
Proc. of ACL 2009, pages 46-54.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440.
Sage, Kenji and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proc. of NAACL 2006, pages
129-132.
Xue, Nianwen, Fu dong Chiou, and Martha Palmer.
2002. Building a large-scale Annotated Chinese
corpus. In Proc. of COLING 2002, pages 1-8.
Wang, Jong-Nae, Jing-Shin Chang, and Keh-Yih Su.
1994. An automatic treebank conversion algorithm
for corpus sharing. In Proc. of ACL 1994, pages
248-254.
Zhang, Hui, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proc. of EMNLP 2009, pages 1552-1560.
Zhou, Qiang. 1996. Phrase bracketing and annotating
on Chinese language corpus. (in Chinese) Ph.D.
thesis, Beijing University.
Zhu, Muhua and Jingbo Zhu. 2009. Label Corre-
spondence Learning for Part-of-Speech Annotation
Transformation. In Proc. of CIKM 2009, pages
1461-1464.
1352
Coling 2010: Poster Volume, pages 1345?1353,
Beijing, August 2010
An Empirical Study of Translation Rule Extraction with Multiple 
Parsers 
 
Tong Xiao??, Jingbo Zhu??, Hao Zhang?, Muhua Zhu?? 
 
?Natural Language Processing Lab., Northeastern University 
?Key Laboratory of Medical Image Computing, Ministry of Education 
{xiaotong,zhujingbo}@mail.neu.edu.cn 
zhanghao@ics.neu.edu.cn, zhumuhua@gmail.com 
 
Abstract 
Translation rule extraction is an impor-
tant issue in syntax-based Statistical Ma-
chine Translation (SMT). Recent studies 
show that rule coverage is one of the key 
factors affecting the success of syntax-
based systems. In this paper, we first 
present a simple and effective method to 
improve rule coverage by using multiple 
parsers in translation rule extraction, and 
then empirically investigate the effec-
tiveness of our method on Chinese-
English translation tasks. Experimental 
results show that extracting translation 
rules using multiple parsers improves a 
string-to-tree system by over 0.9 BLEU 
points on both NIST 2004 and 2005 test 
corpora. 
1 Introduction 
Recently various syntax-based models have been 
extensively investigated in Statistical Machine 
Translation (SMT), including models between 
source trees and target strings (Quirk et al, 2005; 
Liu et al, 2006; Huang et al, 2006), source 
strings and target trees (Yamada and Knight, 
2001; Galley et al, 2006; Shen et al, 2008), or 
source trees and target trees (Eisner, 2003; Ding 
and Palmer, 2005; Cowan et al, 2006; Zhang et 
al., 2008; Liu et al, 2009). In these models, au-
tomatic extraction of translation rules is an im-
portant issue, in which translation rules are typi-
cally extracted using parse trees on 
source/target-language side or both sides of the 
bilingual text. Exploiting the syntactic informa-
tion encoded in translation rules, syntax-based 
systems have shown to achieve comparable per-
formance with phrase-based systems, even out-
perform them in some cases (Marcu et al, 2006). 
Among all the factors contributing to the suc-
cess of syntax-based systems, rule coverage has 
been proved to be an important one that affects 
the translation accuracy of syntax-based systems 
(DeNeefe et al, 2007; Shen et al, 2008). How-
ever, these systems suffer from a problem that 
translation rules are extracted using only 1-best 
parse tree generated by a single parser, which 
generally results in relatively low rule coverage 
due to the limited scope in rule extraction (Mi 
and Huang, 2008). To alleviate this problem, a 
straightforward solution is to enlarge the scope 
of rule extraction, and obtain translation rules by 
using a group of diversified parse trees instead 
of a single parse tree. For example, Mi and 
Huang (2008) used k-best parses and forest to 
extract translation rules for improving the rule 
coverage in their forest-based SMT system, and 
achieved promising results. However, most pre-
vious work used the parse trees generated by 
only one parser, which still suffered somewhat 
from the relatively low diversity in the outputs 
of a single parser. 
Addressing this issue, we investigate how to 
extract diversified translation rules using multi-
ple parsers. As different parsers (or parsing 
models) can provide us with parse trees having 
relatively large diversity, we believe that it is 
beneficial to employ multiple different parsers to 
obtain diversified translation rules and thus en-
large the rule coverage. Motivated by this idea, 
we propose a simple and effective method to 
improve rule coverage by using multiple parsers 
1345
in rule extraction. Furthermore, we conduct an 
empirical study to investigate the effectiveness 
of our method on Chinese-English translation in 
a string-to-tree system. Experimental results 
show that our method improves the baseline sys-
tem by over 0.9 BLEU points on both NIST 
2004 and 2005 test corpora, even achieves a +1 
BLEU improvement when working with the k-
best extraction method. More interestingly, we 
observe that the MT performance is not very 
sensitive to the parsing performance of the pars-
ers used in rule extraction. Actually, the MT sys-
tem does not show different preferences for dif-
ferent parsers. 
2 Related Work 
In machine translation, some efforts have been 
made to improve rule coverage and advance the 
performance of syntax-based systems. For ex-
ample, Galley et al (2006) proposed the idea of 
rule composing which composes two or more 
rules with shared states to form a larger, com-
posed rule. Their experimental results showed 
that the rule composing method could signifi-
cantly improve the translation accuracy of their 
syntax-based system. Following Galley et al 
(2006)?s work, Marcu et al (2006) proposed 
SPMT models to improve the coverage of phras-
al rules, and demonstrated that the system per-
formance could be further improved by using 
their proposed models. Wang et al (2007) de-
scribed a binarization method that binarized 
parse trees to improve the rule coverage on non-
syntactic mappings. DeNeefe et al (2007) analy-
ized the phrasal coverage problem, and com-
pared the phrasal coverage as well as translation 
accuracy for various rule extraction methods 
(Galley et al, 2006; Marcu et al, 2006; Wang et 
al., 2007). 
As another research direction, some work is 
focused on enlarging the scope of rule extraction 
to improve rule coverage. For example, (Venu-
gopal et al, 2008) and (Mi and Huang, 2008) 
extracted rules from the k-best parses and forest 
generated by a single parser to alleviate the 
problem of the limited scope of 1-best parse, and 
achieved promising results. 
Our work differs from previous work in that 
we are concerned with obtaining diversified 
translation rules using multiple different parsers 
(or parsing models) instead of a single parser (or 
parsing model). It can be regarded as an en-
hancement of previous studies. As shown in the 
following parts of this paper, it works very well 
with the existing techniques, such as rule com-
posing (Galley et al, 2006), SPMT models 
(Marcu et al, 2006) and rule extraction with k-
best parses (Venugopal et al, 2008). 
3 Translation Rule Extraction 
In this work, the issue of translation rule extrac-
tion is studied in the string-to-tree model pro-
posed by Galley et al (2006).  We choose this 
model because it has been shown to be one of 
the state-of-the-art syntax-based models, and has 
been adopted in the most successful systems in 
NIST 2009 MT evaluation.  
Typically, (string-to-tree) translation rules are 
learned from the word-aligned bilingual text 
whose target-side has been parsed using a syn-
tactic parser. As the basic unit of translation, a 
translation rule consists of sequence words or 
variables in the source language, and a syntax 
tree in the target language having words (termi-
nals) and variables (non-terminals) at leaves. 
Figure 1 shows the translation rules extracted 
from a word-aligned sentence pair with a target-
side parse tree. 
 
Figure 1: Translation rules extracted from a 
string-tree pair. 
1346
 
Figure 2: Rule extraction using two different parsers (Berkeley Parser and Collins Parser). The 
shaded rectangles denote the translation rules that can be extracted from the parse tree generated by 
one parser but cannot be extracted from the parse tree generated by the other parser. 
 
To obtain basic translation rules, the (minimal) 
GHKM extraction method proposed in (Galley 
et al 2004) is utilized. The basic idea of GHKM 
extraction is to compute the set of the mini-
mally-sized translation rules that can explain the 
mappings between source-language string and 
target-language tree while respecting the align-
ment and reordering between the two languages. 
For example, from the string-tree pair shown at 
the top of Figure 1, we extract the minimal 
GHKM translation rules r1-6. In addition to 
GHKM extraction, the SPMT models (Marcu et 
al., 2006) are employed to obtain phrasal rules 
that are not covered by GHKM extraction.  For 
example, rule r8  in Figure 1 is a SPMT rule that 
is not obtained in GHKM extraction. Finally, the 
rule composing method (Galley et al, 2006) is 
used to compose two or more minimal GHKM 
or SPMT rules having shared states to form lar-
ger rules. For example, rule r7 in Figure 1 is gen-
erated by composing rules r2 and r6. 
4 Differences in Coverage between Rule 
Extractions with Different Parsers 
As described above, translation rule extraction 
relies on the outputs (parse trees) of parsers. As 
different parsers generally have large diversity 
between their outputs, rule extractions with dif-
ferent parsers generally result in very different 
sets of rules. For example, Figure 2 shows the 
rule extractions on a word-aligned sentence pair 
having two target-trees generated by Berkeley 
Parser and Collins Parser, respectively. It is ob-
served that Figure 2 (a) and (b) cover different 
sets of rule due to the different target-trees used 
in rule extraction. Particularly, well-formed rules 
ra7-a9 are extracted in Figure 2 (a), while they do 
not appear in Figure 2 (b). Also, rules rb7-b9 in 
Figure 2 (b) have the similar situation. This ob-
servation gives us an intuition that there is a 
?complementarity? between the rules extracted 
using different parsers. 
1347
We also conduct a quantitative study to inves-
tigate the impact of using different parsers 
(Berkeley Parser and Collins Parser) on rule 
coverage. Tables 1 shows the statistics of the 
rules extracted from 370K Chinese-English par-
allel sentence pairs1 using the method described 
in Section 3. In addition to the total number of 
rules extracted, the numbers of phrasal rules and 
useful rules are also reported to indicate the rule 
coverage of a rule set. Here phrasal rule refers 
to the rule whose source-side and the yield of its 
target-side contains only one phrase each, with 
optional surrounding variables. According to 
(DeNeefe et al, 2007), the number of phrasal 
rules is a good indicator of the coverage of a rule 
set. useful rule refers to the rule that can be ap-
plied when decoding the test sentences 2 . As 
shown in Table 1, the two resulting rule sets on-
ly have about 70% overlaps (Column 4), and the 
rule coverage increases by about 20% when we 
combine them together (Column 5). This finding 
confirms that the rule coverage can be improved 
by using multiple different parsers in rule extrac-
tion. 
 # of rules # of phrasal 
rules 
# of  
useful rules
Berkeley 3,538,332 2,515,243 549,783 
Collins 3,526,166 2,481,195 553,893 
Overlap 2,542,380 1,907,521 386,983 
Union 4,522,118 3,088,920 716,693 
Table 1: Comparison of rule coverage between 
different rule sets. 
5 Translation Rule Extraction with 
Multiple Parsers 
5.1 Rule Extraction Algorithm 
Motivated by the above observations, we pro-
pose a rule extraction method to improve the 
rule coverage by using multiple parsers.  
Let <f, e, a> be a tuple of <source sentence, 
target sentence, bi-directional word alignments>, 
                                                 
1 LDC2005T10, LDC2003E07, LDC2003E14 and 
LDC2005T06 
2 In this experiment, the test sentences come from 
NIST 2004 and 2005 MT evaluation sets. It should be 
noted that due to the pruning in decoding we cannot 
count the exact number of rules that can be used dur-
ing decoding. In this work, we use an alternative ? 
the number of rules matched with test sentences ? to 
estimate an upper-bound approximately. 
and {P1, ..., PN} be N syntactic parsers in target-
language. The following pseudocode formulizes 
the algorithm for extracting translation rules 
from <f, e, a> using parsers {P1, ..., PN}, where 
Pi(e) returns the parse tree generated by the i-th 
parser Pi. Function GENERATERULES() com-
putes the set of rules for <f, ti, a> by using vari-
ous rule extraction methods, such as  the method 
described in Section 3. 
Multi-Parser based Rule Extraction  
Input: <f, e, a> and P = {P1, ..., PN} 
Output: rule set R 
1 Function MULTIPAREREXTRACTOIN(<f, e, a>, P )
2     for i = 1 to N do                           <  for each parser
3        ti = Pi(e)                                      <  target-tree 
4       Ri = GENERATERULES (f, ti, a) <  rule extraction 
5       R.append(Ri) 
6     return R 
7 Function GENERATERULES ( f, ti, a ) 
8     return rules extracted from <f, ti, a> 
5.2 Learning Rule Probabilities 
In multi-parser based rule extraction, more than 
one parse trees are used, and each of them is as-
sociated with a parsing confidence (e.g. genera-
tive probability of the tree). Ideally, if the parse 
trees used in rule extraction can be accurately 
weighted, the rule probabilities will be better 
estimated according to the parse weights, for 
example, the rules extracted from a parse tree 
having a low weight should be penalized accord-
ingly in the estimation of rule probabilities. Un-
fortunately, the tree probabilities are generally 
incomparable between different parsers due to 
the different parsing models used and ways of 
implementation. Thus we cannot use the poste-
rior probability of a rule?s target-side to estimate 
the fractional count (Mi and Huang, 2008; Liu et 
al., 2009), which is used in maximum-likelihood 
estimation of rule probabilities. In this work, to 
simplify the problem, we assume that all the 
parsers have the same and maximum degrees of 
confidence on their outputs. For a rule r ex-
tracted from a string-tree pair, the count of r is 
defined to be: 
1
( , )
( )
N
i
r i
c r
N
?== ?                     (1) 
where ( , )r i? is 1 if r is extracted by using the i-
th parser, otherwise 0.  
1348
Following Mi and Huang (2008)?s work, three 
conditional rule probabilities are employed for 
experimenting with our method. 
': ( ') ( )
( )Pr( | ( ))
( )
r root r root r
c rr root r
c r=
= ?        (2) 
': ( ') ( )
( )Pr( | ( ))
( )
r lhs r lhs r
c rr lhs r
c r=
= ?             (3) 
': ( ') ( )
( )Pr( | ( ))
( )
r rhs r rhs r
c rr rhs r
c r=
= ?            (4) 
where lhs(r) and rhs(r) are the source-hand and 
target-hand sides of r respectively, and root(r) is 
the root of r?s target-tree. 
5.3 Parser Indicator Features 
For each rule, we define N indicator features (i.e. 
( , )r i? ) to indicate a rule is extracted by using 
which parsers, and add them into the translation 
model. By training the feature weights with Min-
imum Error Rate Training (MERT), the system 
can learn preferences for different parsers auto-
matically. 
6 Experiments 
The experiments are conducted on Chinese-
English translation in a state-of-the-art string-to-
tree SMT system.  
6.1 Experimental Setup 
Our bilingual data consists of 370K sentence 
pairs (9M Chinese words + 10M English words) 
which have been used in the experiment in Sec-
tion 4. GIZA++ is employed to perform the bidi-
rectional word alignment between the source and 
target sentences, and the final word alignment is 
generated using the inter-sect-diag-grow method. 
A 5-gram language model is trained on the tar-
get-side of the bilingual data and the Xinhua 
portion of English Gigaword corpus. The devel-
opment data set comes from NIST MT 2003 
evaluation set. To speed up MERT, sentences 
with more than 20 Chinese words are removed. 
The test sets are the NIST MT evaluation sets of 
2004 and 2005.  
Our baseline MT system is built based on the 
string-to-tree model proposed in (Galley et al, 
2006). In this system, both of minimal GHKM 
(Galley et al, 2004) and SPMT rules (Marcu et 
al., 2006) are extracted from the bilingual corpus, 
and the composed rules are generated by com-
posing two or three minimal GHKM and SPMT 
rules3. We use a CKY-style decoder with cube 
pruning (Huang and Chiang, 2007) and beam 
search to decode new Chinese sentences. By de-
fault, the beam size is set to 30. For integrating 
n-gram language model into decoding efficiently, 
rules containing more than two variables or 
source word sequences are binarized using the 
synchronous binarization method (Zhang et al, 
2006; Xiao et al, 2009).  
The system is evaluated in terms of the case-
insensitive NIST version BLEU (using the 
shortest reference length), and statistical signifi-
cant test is conducted using the re-sampling me-
thod proposed by Koehn (2004). 
6.2 The Parsers 
Four syntactic parsers are chosen for the ex-
periments. They are Stanford Parser4, Berkeley 
Parser 5 , Collins Parser (Dan Bikel?s reimple-
mentation of Collins Model 2) 6  and Charniak 
Parser7. The former two are state-of-the-art non-
lexicalized parsers, while the latter two are state-
of-the-art lexicalized parsers. All the parsers are 
trained on sections 02-21 of the Wall Street 
Journal (WSJ) Treebank, and tuned on section 
22. Table 2 summarizes the performance of the 
parsers. 
Parser Recall Precision F1 
Stanford 86.29% 87.21% 86.75% 
Berkeley 90.18% 90.45% 90.32% 
Collins 89.14% 88.85% 88.99% 
Charniak 89.99% 90.28% 90.13% 
Table 2: Performance of the four parsers on sec-
tion 23 of the WSJ Treebank. 
We parse the target-side of the bilingual data 
using the four parsers individually. From the 1-
best parses generated by these parsers, we obtain 
four baseline rule sets using the method de-
scribed in Section 3, as well as the rule sets usi- 
                                                 
3 Generally a higher baseline can be obtained by 
combining more (unit) rules. However, we find that 
using more composed rules does not affect the impact 
of using multiple parsers. Thus, we choose this set-
ting in order to finish all experiments in time. 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://code.google.com/p/berkeleyparser/ 
6 http://www.cis.upenn.edu/~dbikel/download.html 
7 http://www.cs.brown.edu/people/ec/#software 
1349
Rule Coverage BLEU4 (%)  Rule set 
# of rules # of  
phrasal rules
# of  
useful rules 
Dev. MT04 MT05 
Stanford (S) 3,679 K 2,581 K 573 K 39.36 36.02 36.98 
Berkeley (B) 3,538 K 2,515 K 549 K 39.32 36.05 36.98 
Collins (Co) 3,526 K 2,481 K 553 K 39.16 36.07 36.91 
B
as
el
in
e 
Charniak (Ch) 3,450 K 2,435 K 540 K 39.24 35.90 36.89 
S + B 4,567 K 3,105 K 726 K 39.87+ 36.57+ 37.47+ 
S + Co 4,734 K 3,202 K 752 K 39.94+ 36.57+ 37.52+ 
S + Ch 4,764 K 3,258 K 751 K 40.01+ 36.51 37.59+ 
B + Co 4,522 K 3,088 K 716 K 39.84+ 36.60+ 37.46+ 
B +  Ch 4,562 K 3,129 K 717 K 39.81+ 36.49 37.41 
2 
pa
rs
er
s 
Co + Ch 4,592 K 3,125 K 727 K 39.75 36.55+ 37.43+ 
S + B + Co 5,331 K 3,543 K 852 K 40.14++ 36.83++ 37.78++ 
S + B + Ch 5,380 K 3,590 K 854 K 40.05+ 36.82++ 37.70+ 
S + Co + Ch 5,551 K 3,663 K 877 K 40.35++ 36.70+ 37.70+ 3 p
ar
se
rs
 
B + Co + Ch 5,294 K 3,544 K 840 K 40.04+ 36.76+ 37.65+ 
4 S + B + Co + Ch 6,005 K 3,940 K 958 K 40.28++ 36.99++ 37.89++ 
Table 5: Evaluation results. + or ++ = significantly better than all the baseline systems (using single 
parser) at the 95% or 99% confidence level. 
 
 Stanford Berkeley Collins Charniak
Stanford 100% 76.72% 73.32% 74.89% 
Berkeley 76.72% 100% 75.69% 76.76% 
Collins 73.32% 75.69% 100% 74.84% 
Charniak 74.89% 76.76% 74.84% 100% 
Table 3: Agreement between different parsers. 
 
ng the multi-parser based rule extraction method.  
Before conducting primary experiments, we first 
investigate the differences between the 1-best 
outputs of the parsers. Table 3 shows the agree-
ment between each pair of parsers. Here the de-
gree of agreement shown in each cell is com-
puted by using one parser?s output as a good 
standard to evaluate the other parser?s output in 
terms of F1 score, and a higher agreement score 
(i.e. F1 score) means that the 1-best outputs of 
the two parsers are more similar to each other. 
We see that the agreement scores between dif-
ferent parsers are always below 80%. This result 
reflects a large diversity in parse trees generated 
by different parsers, and thus confirms our ob-
servations in Section 4. 
We also examine the ?complementarity? be-
tween the baseline rule sets generated by using 
different parsers individually. Table 4 shows the 
results, where the degree of ?complementarity? 
between two rule sets is defined as the percent-
age of the rules in one rule set that are not cov-
ered by the other rule set. It can be regarded as a 
measure of the disagreement between two rule 
sets, and a higher number indicates large ?com-
plementarity?. For example, in Row 2, Column 3 
(Table 4), ?25.09%? means that 25.09% rules in 
the first rule set (using Stanford Parser) are not 
covered by the second rule set (using Berkeley 
Parser). Table 4 shows that there is always a dis-
agreement of over 25% between different rule 
sets. These results indicate that using different 
parsers can lead to a relatively large ?comple-
mentarity? between the rule sets.  
 Stanford Berkeley Collins Charniak
Stanford 0% 25.09% 29.91% 31.43% 
Berkeley 27.98% 0% 27.90% 29.68% 
Collins 32.84% 28.15% 0% 30.89% 
Charniak 35.70% 31.43% 32.37% 0% 
Table 4: Disagreement between the rule sets ob-
tained using different parsers individually. 
6.3 Evaluation of Translations 
We then study the impact of multi-parser based 
rule extraction on translation accuracy.  Table 5 
shows the BLEU scores as well as the rule cov-
erage for various rule extraction methods. We 
see, first of all, that the rule coverage is im-
proved significantly by multi-parser based rule 
extraction. Compared to the baseline method (i.e. 
single-parser based rule extraction), the multi-
parser based rule extraction achieves over 20% 
coverage improvements when only two parsers 
are used, even yields gains of over 50 percentage 
1350
points when all the four parsers are used together. 
Also, BLEU score is improved by multi-parser 
based rule extraction. When two parsers are em-
ployed in rule extraction, there is generally a 
gain of over 0.4 BLEU points on both MT04 and 
MT05 test sets. Further improvements are 
achieved when more parsers are involved. On 
both test sets, using three parsers in rule extrac-
tion generally yields a +0.7 BLEU improvement, 
and using all the parsers together yields a +0.9 
BLEU improvement which is the biggest im-
provement achieved in this set of experiment. 
All these results show that multi-parser based 
rule extraction is an effective way to improve the 
rule coverage as well as the BLEU score of the 
syntax-based MT system. 
An interesting finding is that there seems no 
significant differences in BLEU scores between 
the baseline systems (using single parsers), 
though the parsing performance of the corre-
sponding parsers is very different from each 
other. For example, the MT performance corre-
sponding to Berkeley Parser is very similar to 
that corresponding to Stanford Parser despite a 
4-point difference in F1 score between the two 
parsers. Another example is that Charniak parser 
performs slightly worse than the other three on 
MT task, though it achieves the 2nd best parsing 
performance in all the parsers. This interesting 
finding shows that the performance of syntax-
based MT systems is not very sensitive to the 
parsing performance of the parsers used in rule 
extraction. 
6.4 Preferences for Parsers 
We also investigate the preferences for different 
parsers in our system. Table 6 shows the weights 
of the parser indicator features learned by 
MERT, as well as the number of edges gener-
ated by applying the rules corresponding to dif-
ferent parsers during decoding. Both of the met-
rics are used to evaluate the contributions of the 
parsers to MT decoding. We see that though 
Stanford Parser and Berkeley Parser are shown 
to be relatively more preferred by the decoder, 
there are actually no significant differences in 
the degrees of the contributions of different 
parsers. This result also confirms the fact ob-
served in Table 5 that the MT system does not 
have special preferences for different parsers. 
 
Indicator Weight # of edges 
(Dev.) 
# of edges 
 (MT04) 
# of edges 
(MT05) 
Stanford 0.1990 7.7 M 169.2 M 101.7 M
Berkeley 0.1982 7.7 M 166.3 M 100.2 M
Collins 0.1690 6.9 M 149.9 M   93.1 M
Charniak 0.1729 7.1 M 156.5 M   97.2 M
Table 6: Preferences for different parsers. 
Though Table 6 provides some information 
about the contributions of different parsers, it 
still does not answer how often these rules are 
really used to generate final (1-best) translation. 
Table 7 gives an answer to this question. We see 
that, following the similar trend in Table 5, dif-
ferent parsers have nearly equal contributions in 
generating final translation. 
Indicator # of rules 
used in 1-best  
(Dev.) 
# of rules 
used in 1-best  
(MT04) 
# of rules 
used in 1-best  
(MT05) 
Stanford     2,410 23,513 14,357 
Berkeley     2,455 23,878 14,670 
Collins     2,309 22,654 13,815 
Charniak     2,269 22,406 13,731 
Table 7: Numbers of rules used in generating 
final (1-best) translation. 
6.5 Rule Extraction with k-best Parses 
We also conduct experiments to compare the 
effectiveness of multi-parser based rule extrac-
tion and rule extraction with k-best parses gener-
ated by a single parser. As Berkeley parser is 
one of the best-performing parsers in previous 
experiments, we employ it to generate k-best 
parses in this set of experiment. As shown in 
Figure 3, both of the methods improve the 
BLEU scores by enlarging the set of parse trees 
used in rule extraction. Compared to k-best ex-
traction, multi-parser extraction shows consiste- 
 36.8
 37
 37.2
 37.4
 37.6
 37.8
 38
 38.2
3.5 5.0 6.5
B
LE
U
4(
%
)
# of rules (million)
1-best
4-best
10-best
20-best
30-best 50-best
2 parsers
3 parsers
4 parsers
multi-parser extraction
k-best extraction
 
Figure 3: Multi-parser based rule extraction vs. 
rule extraction with k-best parses (MT05). 
1351
ntly better BLEU scores. Using 4 different pars-
ers, it achieves an improvement of 0.6 BLEU 
points over k-best extraction where even 50-best 
parses are used. 
Finally, we extend multi-parser based rule ex-
traction to extracting rules from the k-best parses 
generated by multiple parsers. Figure 4 shows 
the results on ?S + B + Co + Ch? system. We see 
that multi-parser based rule extraction can bene-
fit from k-best parses, and yields a modest (+0.2 
BLEU points) improvement when extracting 
from 10-best parses. However, since k-best ex-
traction generally results in much slower extrac-
tion speed, it might not be a good choice to use 
k-best parses to improve our method in practice. 
 37.7
 37.8
 37.9
 38
 38.1
 38.2
 38.3
 38.4
 38.5
5.5 6.5 7.5 8.5
B
LE
U
4(
%
)
# of rules (million)
1-best
2-best
5-best
10-best
multi-parser + k-best extraction
 
Figure 4: Multi-parser based rule extraction & 
rule extraction with k-best parses (MT05). 
7 Discussion and Future Work 
In this work, all the parsers are trained using the 
same treebank. To obtain diversified parse trees 
for multi-parser based rule extraction, an alterna-
tive way is to learn parsers on treebanks anno-
tated by different organizations (e.g. Penn Tree-
bank and ICE-GB corpus). Since different tree-
banks can provide us with more diversity in 
parsing, we believe that our system can benefit a 
lot from the parsers that are learned on multiple 
different treebanks individually. But here is a 
problem that due to the different annotation 
standards used, there is generally an incompati-
bility between treebanks annotated by different 
organizations. It will result in that we cannot 
straightforwardly mix the resulting rule sets (or 
heterogeneous grammars for short) for probabil-
ity estimation as well as the use for decoding. To 
solve this problem, a simple solution might be 
that we transform the incompatible rules into a 
unified form. Alternatively, we can use hetero-
geneous decoding (or parsing) techniques (Zhu 
et al, 2010) to make use of heterogeneous 
grammars in the stage of decoding. Both topics 
are very interesting and worth studying in our 
future work.  
Besides k-best extraction, our method can also 
be applied to other rule extraction schemes, such 
as forest-based rule extraction. As (Mi and 
Huang, 2008) has shown that forest-based ex-
traction is more effective than k-best extraction 
in improving translation accuracy, it is expected 
to achieve further improvements by using multi-
parser based rule extraction and forest-based rule 
extraction together. 
8 Conclusions  
In this paper, we present a simple and effective 
method to improve rule coverage by using mul-
tiple parsers in translation rule extraction. Ex-
perimental results show that  
z Using multiple parsers in rule extraction 
achieves large improvements of rule cover-
age over the baseline method where only a 
single parser is used, as well as a +0.9 
BLEU improvement on both NIST 2004 
and 2005 test corpora. 
z The MT system can be further improved by 
using multiple parsers and k-best parses to-
gether. However, with the consideration of 
extraction speed, it might not be a good 
choice to use k-best parses to improve mul-
ti-parser based rule extraction in practice. 
z The MT performance is not influenced by 
the parsing performance of the parsers used 
in rule extraction very much. Actually, the 
MT system does not show different prefer-
ences for different parsers. 
Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and 
the Fundamental Research Funds for the Central 
Universities (N090604008). The authors would 
like to thank the anonymous reviewers and Ton-
gran Liu for their pertinent comments for im-
proving the early version of this paper, and Ru-
shan Chen for building parts of the baseline sys-
tem. 
1352
References 
Brooke Cowan, Ivona Ku?erov? and Michael Collins. 
2006. A discriminative model for tree-to-tree 
translation. In Proc. of EMNLP 2006, pages 232-
241. 
Steve DeNeefe, Kevin Knight, Wei Wang and Daniel 
Marcu. 2007. What Can Syntax-based MT Learn 
from Phrase-based MT? In Proc. of EMNLP 2007, 
pages 755-763. 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. In Proc. of ACL 2005, Ann 
Arbor, Michigan, pages 541-548. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Michel Galley, Mark Hopkins, Kevin Knight and 
Daniel Marcu. 2004. What's in a translation rule? 
In Proc. of HLT-NAACL 2004, Boston, USA, 
pages 273-280. 
Michel Galley, Jonathan Graehl, Kevin Knight, Da-
niel Marcu, Steve DeNeefe, Wei Wang and Igna-
cio Thayer. 2006. Scalable inferences and training 
of context-rich syntax translation models. In Proc. 
of COLING/ACL 2006, Sydney, Australia, pages 
961-968. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. In Proc. of ACL 2007, Prague, Czech Re-
public, pages 144-151. 
Liang Huang, Kevin Knight and Aravind Joshi. 2006. 
Statistical syntax-directed translation with ex-
tended domain of locality. In Proc. of AMTA 2006, 
pages 66-73. 
Philipp Koehn. 2004. Statistical Significance Tests 
for Machine Translation Evaluation. In Proc. of 
EMNLP 2004, Barcelona, Spain, pages 388-395. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. of COLING/ACL 2006, Syd-
ney, Australia, pages 609-616. 
Yang Liu, Yajuan L? and Qun Liu. 2009. Improving 
Tree-to-Tree Translation with Packed Forest. In 
Proc. of ACL 2009, pages 558-566. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phras-
es. In Proc. of EMNLP 2006, Sydney, Australia, 
pages 44-52. 
Haitao Mi and Liang Huang. 2008. Forest-based 
Translation Rule Extraction. In Proc. of EMNLP 
2008, pages 206-214. 
Chris Quirk, Arul Menezes and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of ACL 2005, pages 
271-279. 
Libin Shen, Jinxi Xu and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation al-
gorithm with a target dependency language model. 
In Proc. of ACL/HLT 2008, pages 577-585. 
Ashish Venugopal, Andreas Zollmann, Noah A. 
Smith and Stephan Vogel. 2008. Wider Pipelines: 
K-best Alignments and Parses in MT Training. In 
Proc. of AMTA 2008, pages 192-201. 
Wei Wang, Kevin Knight and Daniel Marcu. 2007. 
Binarizing Syntax Trees to Improve Syntax-Based 
Machine Translation Accuracy. In Proc. of 
EMNLP-CoNLL 2007, Prague, Czech Republic, 
pages 746-754. 
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and 
Ming Zhou. 2009. Better Synchronous Binariza-
tion for Machine Translation. In Proc. of EMNLP 
2009, Singapore, pages 362-370. 
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical machine translation model. In 
Proc. of ACL 2001, pages 132-139. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006, 
New York, USA, pages 256- 263. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation 
Model. In Proc. of ACL/HLT 2008, pages 559-567. 
Muhua Zhu, Jingbo Zhu and Tong Xiao. 2010. Het-
erogeneous Parsing via Collaborative Decoding. In 
Proc. of COLING 2010. 
1353
Coling 2010: Poster Volume, pages 1541?1549,
Beijing, August 2010
Automatic Treebank Conversion via Informed Decoding
Muhua Zhu
Natural Language Processing Lab.
Northeastern University
zhumuhua@gmail.com
Jingbo Zhu
Natural Language Processing Lab.
Northeastern University
zhujingbo@mail.neu.edu.cn
Abstract
In this paper, we focus on the challenge
of automatically converting a constituency
treebank (source treebank) to fit the stan-
dard of another constituency treebank (tar-
get treebank). We formalize the conver-
sion problem as an informed decoding
procedure: information from original an-
notations in a source treebank is incorpo-
rated into the decoding phase of a parser
trained on a target treebank during the
parser assigning parse trees to sentences in
the source treebank. Experiments on two
Chinese treebanks show significant im-
provements in conversion accuracy over
baseline systems, especially when training
data used for building the parser is small
in size.
1 Introduction
Recent years have seen extensive applications of
machine learning methods to natural language
processing problems. Typically, increase in the
scale of training data boosts the performance of
machine learning methods, which in turn en-
hances the quality of learning-based NLP systems
(Banko and Brill, 2001). However, annotating
data by human is time consuming and labor inten-
sive. For this reason, human-annotated corpora
are considered as the most valuable resource for
NLP.
In practice, there often exist more than one cor-
pus for the same NLP tasks. For example, for
constituent syntactic parsing (Collins, 1999; Char-
niak, 2000; Petrov et al, 2006) for Chinese, in ad-
dition to the most popular treebank Chinese Tree-
bank (CTB) (Xue et al, 2002), there are also
other treebanks such as Tsinghua Chinese Tree-
bank (TCT) (Zhou, 1996). For the purpose of
full use of readily available human annotations
for the same tasks, it is significant if such cor-
pora can be used jointly. Such attempt is es-
pecially significant for some languages that have
limited size of labeled data. At first sight, a di-
rect combination of multiple corpora is a way to
this end. However, corpora created for the same
NLP tasks are generally built by different orga-
nizations. Thus such corpora often follow dif-
ferent annotation standards and/or even different
linguistic theories. We take CTB and TCT as
a case study. Although both CTB and TCT are
Chomskian-style treebanks, they have annotation
divergences in at least two dimensions: a) CTB
and TCT have dramatically different tag sets, in-
cluding parts-of-speech and grammar labels, and
the tags cannot be mapped one to one; b) CTB and
TCT have distinct hierarchical structures. For ex-
ample, the Chinese words ??? (Chinese) ??
(traditional) ?? (culture)? are grouped as a flat
noun phrase according to the CTB standard (right
side in Fig. 1), but in TCT, the last two words are
instead grouped together beforehand (left side in
Fig. 1). The differences cause such treebanks of
different annotation standard to be generally used
independently.
In this paper, we focus on unifying multiple
constituency treebanks of distinct annotation stan-
dards through treebank conversion. The task of
treebank conversion is defined to be conversion of
annotations in one treebank (source treebank) to
1541
np
nS
??
np
a
??
n
??
NP
NR
??
NN
??
NN
??
??????
(Chinese) (traditional) (culture)
Figure 1: Example tree fragments with TCT (left)
and CTB (right) annotations
fit the standard of another treebank (target tree-
bank). To this end, we propose a language in-
dependent approach called informed decoding 1,
in which a parser trained on a target treebank au-
tomatically assigns new parse trees to sentences
in a source treebank with the aid of informa-
tion derived from annotations in the source tree-
bank. We conduct experiments on two open Chi-
nese treebanks 2: CTB and TCT. Experimental re-
sults show that our approach achieves significant
improvements over baseline systems, especially
when training data used for building the parser is
small in size.
The rest of the paper is structured as follows. In
Section 2 we describe previous work on treebank
conversion. In Section 3, we describe in detail the
informed decoding approach. Section 4 presents
experimental results which demonstrate the effec-
tiveness of our approach. Finally, Section 5 con-
cludes our work.
2 Related Work
Previous work on treebank conversion can be
grouped into two categories according to whether
grammar formalisms of treebanks are identical.
One type focuses on converting treebanks of dif-
ferent grammar formalisms. Collins et al (1999)
1The terminology decoding is referred to the parsing
phase of a parser.
2Note that although we use Chinese treebanks, our ap-
proach is language independent.
addressed constituent syntactic parsing on Czech
using a treebank converted from a Prague depen-
dency treebank, where conversion rules derived
from head-dependent pairs and heuristic rules are
applied. Xia and Palmer (2001) compared three
algorithms for conversion from dependency struc-
tures to phrase structures. The algorithms ex-
panded each node in input dependency structures
into a projection chain, and labeled the newly in-
serted node with syntactic categories. The three
algorithms differ only in heuristics adopted to
build projection chains. Xia et al (2008) auto-
matically extracted conversion rules from a tar-
get treebank and proposed strategies to handle the
case when more than one conversion rule are ap-
plicable. Instead of using conversion rules, Niu
et al (2009) proposed to convert a dependency
treebank to a constituency one by using a parser
trained on a constituency treebank to generate k-
best lists for sentences in the dependency tree-
bank. Optimal conversion results are selected
from the k-best lists. There also exists work in the
reverse direction: from a constituency treebank to
a dependency treebank (Nivre, 2006; Johansson
and Nugues, 2007).
Relatively few efforts have been put on conver-
sion between treebanks that have the same gram-
mar formalisms but follow different annotation
standards. Wang et al (1994) applied a similar
framework as in (Niu et al, 2009) to convert from
a simple constituency treebank to a more infor-
mative one. The basic idea is to apply a parser
built on a target treebank to generate k-best lists
for sentences in the source treebank. Then, a
matching metric is defined on the number of iden-
tical bracketing spans between two trees. Such a
function computes a score for each parse tree in
a k-best list and its corresponding parse tree in
the source treebank. Finally, the parse tree with
the highest score in a k-best list is selected to be
the conversion result. The difference between our
work and (Wang et al, 1994) is that, instead of us-
ing trees from the source treebank to select parse
trees from k-best lists, we propose to use such
trees to guide the decoding phase of the parser
built on the target treebank. Making use of the
source treebank in such a novel way is believed to
be the major contribution of our work.
1542
3 Treebank Conversion via Informed
Decoding
The task of treebank conversion is defined to con-
vert parse trees in a source treebank to fit the stan-
dard of a target treebank. In the informed de-
coding approach, treebank conversion proceeds in
two steps: 1) build a parser on a target treebank;
2) apply the parser to decode sentences in a source
treebank with the aid of information derived from
the source treebank. For convenience, parse trees
in a source treebank are referred to as source trees
and corresponding, trees from a target treebank
are referred to as target trees. Moreover, a parser
built on a target treebank is referred to as target
parser. In the following sections, we first describe
motivation of our work and then present details of
the informed decoding approach.
3.1 Motivation
We use the example in Fig. 2 to illustrate why
original annotations in a source treebank can help
in treebank conversion. The figure depicts three
tree fragments for the Chinese words ? (pay) ?
(already) ? (one) ? (day) ? (of) ??(salary),
among which Fig. 2(a) and Fig. 2(b) are tree frag-
ments of the CTB standard and Fig. 2(c) is a tree
fragment of the TCT standard. From the fig-
ure, we can see that these Chinese words actu-
ally have (at least) two plausible interpretations
of the meaning. In Fig. 2(a), the words mean
pay salary for one-day work while in Fig. 2(b),
the words mean spend one day on paying salary.
If Fig. 2(c) is a source tree to be converted into
the CTB standard, then Fig. 2(b) will be rejected
since it conflicts with Fig. 2(c) with respect to tree
structures. Note that structures reflect underlying
sentence meaning. On the other hand, although
Fig. 2(a) also has (minor) differences in tree struc-
tures from Fig. 2(c), it is preferred as the conver-
sion result3. From the example we can get in-
spired by the observation that original annotations
in a source treebank are informative and necessary
to converting parse trees in the source treebank.
In general, conversion like that from Fig. 2(c)
3Note that we don?t deny existence of annotation distinc-
tions between the treebanks, but we aim to make use of what
they both agree on. We assume that consensus is the major-
ity.
to Fig. 2(a) requires sentence-specific conversion
rules which are difficult to obtain in practice. In
order to make use of information provided by
original annotations in a source treebank, Wang
et al (1994) proposed a selecting-from-k-best ap-
proach where source trees are used to select one
?optimal? parse tree from each k-best list gener-
ated by a target parser. In this paper, we instead in-
corporate information of original annotations into
the parsing phase. The underlying motivation is
two-fold:
? The decoding phase of a parser is essentially
a search process. Due to the extreme mag-
nitude of searching space, pruning of search
paths is practically necessary. If reliable in-
formation is provided to guide the pruning of
search paths, more efficient parsing and bet-
ter results are expected.
? Selecting-from-k-best works on the basis of
k-best lists. Unfortunately, we often see very
few variations in k-best lists. For exam-
ple, 50-best trees present only 5 to 6 varia-
tions (Huang, 2008). The lack of diversi-
ties in k-best lists makes information from
the source treebank less effective in selecting
parse trees. By contrast, incorporating such
information into decoding makes the infor-
mation affect the whole parse forest.
3.2 Formalization of Information from
Source Treebank
In this paper, information from a source treebank
translates into two strategies which help a target
parser to prune illegal partial parse trees and to
rank legal partial parse trees higher. Following are
the two strategies:
? Pruning strategy: despite distinctions exist-
ing between annotation standards of a source
treebank and a target treebank, a source tree-
bank indeed provides treebank conversion
with indicative information on bracketing
structures and grammar labels. So when a
partial parse tree is generated, it should be
examined against the corresponding source
tree. Unless the partial parse tree does not
conflict with any constituent in the source
tree, it should be pruned out.
1543
VP
VV
?
AS
?
NP
DNP
QP
CD
?
CLP
M
?
DEC
?
NN
??
VP
DVP
VP
VV
?
AS
?
QP
CD
?
CLP
M
?
DEC
?
NP
NN
??
vp
v
?
uA
?
np
mp
m
?
qN
?
uJDE
?
n
??
(a) (b) (c)
???????
(pay) (already) (one) (day) (of) (salary)
Figure 2: tree fragments of words???????: (a) and (b) show two plausible tree fragments of
the words using the CTB standard; (c) shows a tree fragment of the TCT standard which has the same
interpretation as (a).
6
4
1
5
2 3
[1,3],[2,3],[1,1]
[1,1]
[2,3],[2,2],[3,3]
[1,1]
[2,2]
[3,3]
Figure 3: Constituent set of a synthetic parse tree
? Rescoring strategy: in practice, decoding is
often a local optimal search process. In some
cases even if a correct parse tree exits in the
parse forest, parsers may fail to rank it to the
top position. Rescoring strategy is used to
increase scores for partial parse trees which
are confidently thought to be valid.
3.2.1 Pruning Strategy
The pruning strategy used in this paper is based
on the concept of conflict which is defined in two
dimensions: structures and grammar labels. Since
a tree structure can be equivalently represented
as its span (interval of word indices) set, we can
check whether two trees conflict by checking their
spans. See Fig. 3 for an illustration of spans of a
tree. Following are criteria determining whether
two trees conflict in their structures.
? If one node in tree A is raised to be a child
of the node?s grandfather in tree B, and the
grandfather has more than two children, then
tree A and tree B conflict in structures.
? If tree A has a span [a, b] and tree B has a
span [m,k] and these two spans satisfy the
condition of either a < m ? b < k or m <
a ? k < b, then tree A and B conflict in
structures.
Fig. 4 illustrates criteria mentioned above, where
Fig. 4(a) is compatible (not conflict) with Fig. 4(b)
although they have different structures. But
Fig. 4(a) conflicts with Fig. 4(c) (according to cri-
terion 1; node 3 is raised) and (d) (according to
criterion 2).
1544
65
1 2 3
4
7
6
5
1 2
3
4
6
5
1 2
3 4
7
5
1 2
6
3 4
(a) (b) (c) (d)
Figure 4: Illustrating example of the concept of conflict: (a) and (b) are compatible (not conflict); (a)
conflicts with (c) (condition 1) and (d) (condition 2)
For the dimension of grammar labels, we manu-
ally construct a mapping between label sets (POS
tags excluded) of source and target treebanks.
Such a mapping is frequently a many-to-many
mapping. Two labels are said to be conflicting if
they are from different label sets and they cannot
be mapped.
By combining these two strategies, two parse
trees (of different standards) which yield the same
sentence are said to be conflicting if they conflict
in both structures and labels. Note that we de-
scribe pruning strategy for the case of two parse
trees. In informed decoding process, this strategy
is actually applied to every partial parse tree gen-
erated during decoding.
3.2.2 Rescoring Strategy
As mentioned above, despite that the pruning
strategy helps in improving conversion accuracy,
we are faced with the problem of how to rank
valid parse trees higher in a parse forest. To solve
the problem, we adjust the scores of those partial
parse trees that are considered to be confidently
?good?. The criteria which is used to judge ?good-
ness? of a partial parse are listed as follows:
? The partial parse tree can find in the source
tree a constituent that has the same structure
as it.
? When the first criterion is satisfied, gram-
mar categories of this partial parse should not
conflict with the grammar categories of its
counterpart.
In practice, we use a parameter ? to adjust the
score.
Pnew(e) = ? ? P (e) (1)
Here e represents any partial tree that is rescored,
and P (e) and Pnew(e) refer to original and new
scores, respectively.
3.3 Parsing Model
Theoretically all parsing models are applicable in
informed decoding, but we prefer to adopt a CKY-
style parser for two reasons: CKY style parsers
are dynamically bottom-up and always have edges
(or parsing items) belonging to the same span
stacked together in the same chart4 cell. The
property of CKY-style parsers being dynamically
bottom-up can make the pruning strategy efficient
by avoiding rechecking subtrees that have already
been checked. The property of stacking edges in
the same chart cell makes CKY-style parsers eas-
ily portable to the situaiton of informed decod-
ing. In this paper, Collins parser (Collins, 1999)
is used. Algorithm 1 presents the extended ver-
sion of the decoding algorithm used in Collins
parser. What the algorithm needs to do is to
generate edges for each span. And before edges
are allowed to enter the chart, pruning conditions
4Data structure used to store paring items that are not
pruned
1545
Algorithm 1 CKY-style decoding
Argument: a parsing decoder
a sentence to be parsed and corresponding
source tree
Begin
Steps:
1. initialization steps
2. for span from 2 to sentence length do
for start from 1 to (sentence length-span+1) do
end := (start + span - 1)
for each edge e for span [start, end] do
generate(e, start, end)
prune(e, start, end)
rescore(e, start, end)
add edge(e, start, end)
End
Subroutine:
generate: generates an edge which belongs to the
span [start, end].
prune: apply pruning strategy to check whether the
edge should be pruned.
rescore: apply rescoring strategy to weight the edge.
add edge: add the edge into chart.
should be checked in prune subroutine and rescor-
ing should be conducted in rescore subroutine
with respect to the corresponding source tree.
4 Experiments
4.1 Experimental Setup
In this paper, we conduct two groups of experi-
ments in order to evaluate 1) treebank conversion
accuracy and 2) how much newly generated data
can boost syntactic parsing accuracy. For the ex-
periments of treebank conversion, Penn Chinese
Treebank (CTB) 5.1 is used as the target treebank.
That is, the CTB standard is the one we are inter-
ested in. Following the conventional data split-
ting of CTB5.1, articles 001-270 and 400-1151
(18,100 sentences, 493,869 words) are used for
training, articles 271-300 (348 sentences, 8,008
words) are used as test data, and articles 301-325
(352 sentences, 6,821 words) are used as devel-
opment data 5. Moreover, in order to directly
evaluate conversion accuracy, we randomly sam-
pled 150 sentences from the CTB test set and have
three annotators manually label sentences of these
parse trees according to the standard of Tsinghua
Chinese Treebank (TCT). Thus each of the 150
sentences has two parse trees, following the CTB
5Development set is not used in this paper.
and TCT standard, respectively. For convenience
of reference, the set of 150 parse trees of the
CTB standard is referred to as Sample-CTB and
its counterpart which follows the TCT standard is
referred to as Sample-TCT. In such setting, the ex-
periments of treebank conversion is designed to
use the informed decoding approach to convert
Sample-TCT to the standard of CTB and conver-
sion results are evaluated with respect to Sample-
CTB. The CTB training data (or portion of it) is
used as target training data on which parsers are
trained for conversion.
For the experiments of syntactic parsing, the
TCT corpus is used as the source treebank.
The TCT corpus contains 27,268 sentences and
587,298 words, which are collected from the lit-
erature and newswire domains. In this group of
experiments, the CTB training data is again used
as target training data and the whole TCT cor-
pus is converted using the informed decoding ap-
proach. The newly-gained parse trees are used as
additional training data for syntactic parsing on
the CTB test data. One thing worth noting in the
experiments is that, using Collins parser to con-
vert the TCT corpus requires Part-of-Speech tags
of the CTB standard be assigned to sentences in
TCT ahead of conversion being conducted. To this
end, instead of using POS taggers, we use the la-
bel correspondence learning method described in
(Zhu and Zhu, 2009) in order to get high POS tag-
ging accuracy.
For all the experiments in this paper, bracketing
F1 is used as the performance metric, provided by
the EVALB program 6. ? in Eq.1 is set to 3.0 since
it provides best conversion results in our experi-
ments.
4.2 Experiments on Conversion
The setup of conversion experiments is described
above. In the experiments, we use two representa-
tive baseline systems. One, named directly pars-
ing (DP) converts Sample-TCT by directly pars-
ing using Collins parser which is trained on tar-
get training data, and the other is the method pro-
posed in (Wang et al, 1994) (hereafter referred
to as Wang94). For the latter baseline, we use
Berkeley parser (Petrov et al, 2006) instead of
6http://nlp.cs.nyu.edu/evalb
1546
Ratio 20% 40% 60% 80% 100%
DP 73.19 75.21 79.43 80.64 81.40
Wang94 75.00 76.82 78.08 81.50 82.47
This paper 82.71 83.00 83.37 84.80 84.34
Table 1: Conversion accuracy with varying size of
target training data
Collins parser. The reason is that we want to build
a strong baseline since Berkeley parser is able
to generate better k-best lists than Collins parser
does (Zhang et al, 2009). In detail, Wang94 pro-
ceeds in two steps: 1) use Berkeley parser to gen-
erate k-best lists for sentences in Sample-TCT; 2)
select a parse tree from each k-best list with re-
spect to original annotations in Sample-TCT. Here
we set k to 50. Table 1 reports F1 scores of the
baseline systems and our informed decoding ap-
proach with varying size of target training data.
The first row of the table represents fractions of
the CTB training data which are used as target
training data. For example, 40% means 7,240
parse trees (of 18,100) in the CTB training data
are used. To relieve the effect of ordering, we
randomly shuffled parse trees in the CTB training
data.
From the table, we can see that our ap-
proach performs significantly better than DP and
Wang94. In detail, when 100% CTB training data
is used as target training data, 2.95% absolute im-
provement is achieved. When the size of target
training data decreases, absolute improvements of
our approach over baseline systems are further en-
larged. More interestingly, decreasing in target
training data only results in marginal decrement
in conversion accuracy of our approach. This is of
significant importance in the situation where tar-
get treebank is small in size.
In order to evaluate the accuracy of conversion
methods on different span lengths, we compare
the results of Wang94 and informed decoding pro-
duced by using 100% CTB training data. Table 2
shows the statistics.
From the results we can see that our ap-
proach performs significantly better on long spans
and achieves marginally lower accuracy on small
ones. But notice that the informed decoding ap-
proach is implemented on the base of Collins
Span Length 2 4 6 8 10
Wang94 82.45 83.97 80.72 77.83 71.72
This paper 83.72 82.95 79.84 77.27 70.67
Span Length 12 14 16 18 20
Wang94 75.29 68.00 77.27 70.83 76.66
This paper 71.79 75.00 86.27 80.00 80.00
Table 2: Conversion accuracy on different span
lengths
Category ADJP VCD CP DNP ADVP
Wang94 79.62 57.14 65.43 84.76 91.73
This paper 88.00 66.67 71.60 88.31 93.44
Table 3: Conversion results with respect to differ-
ent grammar categories
parser and that Wang94 works on the basis of
Berkeley parser. Taking the performance gap of
Collins parser and Berkeley parser, we actually
can conclude that on small spans, our approach is
able to achieve results comparable with or even
better than Wang94. We can also infer from
the observation that our approach can outperform
Wang94 when converting parse trees which yield
long sentences.
Another line of analysis is to compare the
results of Wang94 and our approach, with re-
spect to different grammar categories. Table 3
lists five grammar categories in which our ap-
proach achieves most improvements. For cat-
egories NP and VP, absolute improvements are
1.1% and 1.4% respectively. Take into account
large amounts of instances of NP and VP, the im-
provements are also quite significant.
4.3 Experiments on Parsing
Before doing the experiments of parsing, we first
converted the whole TCT corpus using 100%
CTB training data as target training data. Us-
ing the newly-gained data only as training data
for Collins parser, we can get F1 score 75.4%
on the CTB test data. We can see that the score
is much lower than the accuracy achieved by us-
ing the CTB training data (75.4% vs. 82.04%).
Possible reasons that result in lower accuracy in-
cludes: 1) divergences in word segmentation stan-
dards between TCT and CTB; 2) divergences of
domains of TCT and CTB; 3) conversions errors
in newly-gained data. Although the newly-gained
1547
data cannot replace the CTB training data thor-
oughly, we would like to use it as additional train-
ing data besides the CTB training data. Following
experiments aim to examine effectiveness of the
newly-gained data when used as additional train-
ing data.
In the first parsing experiment, the TCT cor-
pus is converted using portions of the CTB train-
ing data. As in the conversion experiments, parse
trees in the CTB training data are randomly or-
dered before splitting of the training set. For each
portion, newly-gained data together with the por-
tion of the CTB training data are used to train a
new parser. Evaluation results on the CTB test
data are presented in Table 4.
Ratio 20% 40% 60% 80% 100%
Collins 75.74 77.65 79.43 81.22 82.04
Collins+ 78.86 79.52 80.06 81.77 82.38
Table 4: Parsing accuracy with new data added in
Here in Table 4, the first row represents ratios
of parse trees from the CTB training data. For
example, 40% means the first 40% parse trees in
the CTB training data are used. The Collins row
represents the results of only using portions of the
CTB training data, and the Collins+ row contains
the results achieved with enlarged training data.
From the results, we find that new data indeed
provides complementary information to the CTB
training data, especially when the training data is
small in size. But benefits of Collins parser gained
from additional training data level out with the in-
crement of the training data size. Actually if tech-
niques like corpus weighting (Niu et al, 2009) are
applied to weight differently training data and the
additional data, higher parsing accuracy is reason-
ably expected.
Another obversion from Table 4 is that the
parser trained on 40% CTB training data plus
additional training data achieves higher accuracy
than using 60% CTB training data. We incre-
mentally add labeled training data and automatic
training data respectively to 40% CTB training
data. The purpose of this experiment is to see the
magnitude of automatic training data which can
achieve the same effect as labeled training data
does. The results are depicted in Table 5.
# of Added Data 2k 4k 6k 8k
Labeled Data 78.51 79.52 80.01 81.37
Auto Data 78.23 79.11 79.85 79.67
Table 5: Parsing accuracy with new data added in
From the results we see that accuracy gaps be-
tween using labeled data and using automatic data
get large with the increment of added data. One
possible reason is that more noise is taken when
more data is added. This observation further veri-
fies that refining techniques like corpus weighting
are necessary for using automatically-gained data.
5 Conclusions
In this paper we proposed an approach called in-
formed decoding for the task of conversion be-
tween treebanks which have different annotation
standards. Experiments which evaluate conver-
sion accuracy directly showed that our approach
significantly outperform baseline systems. More
interestingly we found that the size of target train-
ing data have limited effect on the conversion ac-
curacy of our approach. This is extremely impor-
tant for languages which lack enough treebanks in
whose standards we are interested.
We also added newly-gained data to target
training data to check whether new data can boost
parsing results. Experiments showed additional
training data provided by treebank conversion
could boost parsing accuracy.
References
Banko, Michele and Eric Brill. 2001. Scaling to
very very large corpora for natural language dis-
ambiguation. In Proc. of ACL 2001, pages 26-33.
Charniak, Eugene. 2000. A Maximum-Entropy-
Inspired Parser. In Proc. of NAACL 2000, pages
132-139.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Collins, Michael, Lance Ramshaw, Jan Hajic, and
Christoph Tillmann. 1999. A Statistical Parser for
Czech. In Proc. of ACL 1999, pages 505-512.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proc. of NAACL 2000, pages
132-139.
1548
Huang, Liang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proc. of
ACL 2008, pages 586-594.
Johansson, Richard and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proc. of NODALIDA 2007, pages 105-
112.
Nivre, Joakim. 2006. Inductive Dependency Parsing.
In Springer, Volume 34.
Niu, Zheng-Yu, Haifeng Wang, Hua Wu. 2009. Ex-
ploiting heterogeneous treebanks for parsing. In
Proc. of ACL 2009, pages 46-54.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of COLING-
ACL 2006, pages 433-440.
Xue, Nianwen, Fu dong Chiou, and Martha Palmer.
2002. Building a large-scale Annotated Chinese
corpus. In Proc. of COLING 2002, pages 1-8.
Wang, Jong-Nae, Jing-Shin Chang, and Keh-Yih Su.
1994. An automatic treebank conversion algorithm
for corpus sharing. In Proc. of ACL 1994, pages
248-254.
Xia, Fei, Rajesh Bhatt, Owen Rambow, Martha
Palmer, and Dipti M. Sharma. 2008. Towards a
Multi-Representational Treebank. In Proc. of the
7th International Workshop on Treebanks and Lin-
guistic THeories, pages 159-170.
Zhang, Hui, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proc. of EMNLP 2009, pages 1552-1560.
Zhou, Qiang. 1996. Phrase bracketing and annotating
on Chinese language corpus. (in Chinese) Ph.D.
thesis, Beijing University.
Zhu, Muhua and Jingbo Zhu. 2009. Label Corre-
spondence Learning for Part-of-Speech Annotation
Transformation. In Proc. of CIKM 2009, pages
1461-1464.
1549
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 739?748,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Boosting-based System Combination for Machine Translation 
 
Tong Xiao, Jingbo Zhu, Muhua Zhu, Huizhen Wang 
 
Natural Language Processing Lab.  
Northeastern University, China 
{xiaotong,zhujingbo,wanghuizhen}@mail.neu.edu.cn 
zhumuhua@gmail.com 
 
 
Abstract 
In this paper, we present a simple and effective 
method to address the issue of how to generate 
diversified translation systems from a single 
Statistical Machine Translation (SMT) engine 
for system combination. Our method is based 
on the framework of boosting. First, a se-
quence of weak translation systems is gener-
ated from a baseline system in an iterative 
manner. Then, a strong translation system is 
built from the ensemble of these weak transla-
tion systems. To adapt boosting to SMT sys-
tem combination, several key components of 
the original boosting algorithms are redes-
igned in this work. We evaluate our method on 
Chinese-to-English Machine Translation (MT) 
tasks in three baseline systems, including a 
phrase-based system, a hierarchical phrase-
based system and a syntax-based system. The 
experimental results on three NIST evaluation 
test sets show that our method leads to signifi-
cant improvements in translation accuracy 
over the baseline systems. 
1 Introduction 
Recent research on Statistical Machine Transla-
tion (SMT) has achieved substantial progress. 
Many SMT frameworks have been developed, 
including phrase-based SMT (Koehn et al, 2003), 
hierarchical phrase-based SMT (Chiang, 2005), 
syntax-based SMT (Eisner, 2003; Ding and 
Palmer, 2005; Liu et al, 2006; Galley et al, 2006; 
Cowan et al, 2006), etc. With the emergence of 
various structurally different SMT systems, more 
and more studies are focused on combining mul-
tiple SMT systems for achieving higher transla-
tion accuracy rather than using a single transla-
tion system. 
The basic idea of system combination is to ex-
tract or generate a translation by voting from an 
ensemble of translation outputs. Depending on 
how the translation is combined and what voting 
strategy is adopted, several methods can be used 
for system combination, e.g. sentence-level com-
bination (Hildebrand and Vogel, 2008) simply 
selects one from original translations, while 
some more sophisticated methods, such as word-
level and phrase-level combination (Matusov et 
al., 2006; Rosti et al, 2007), can generate new 
translations differing from any of the original 
translations. 
One of the key factors in SMT system combi-
nation is the diversity in the ensemble of transla-
tion outputs (Macherey and Och, 2007). To ob-
tain diversified translation outputs, most of the 
current system combination methods require 
multiple translation engines based on different 
models. However, this requirement cannot be 
met in many cases, since we do not always have 
the access to multiple SMT engines due to the 
high cost of developing and tuning SMT systems. 
To reduce the burden of system development, it 
might be a nice way to combine a set of transla-
tion systems built from a single translation en-
gine. A key issue here is how to generate an en-
semble of diversified translation systems from a 
single translation engine in a principled way. 
Addressing this issue, we propose a boosting-
based system combination method to learn a 
combined translation system from a single SMT 
engine. In this method, a sequence of weak trans-
lation systems is generated from a baseline sys-
tem in an iterative manner. In each iteration, a 
new weak translation system is learned, focusing 
more on the sentences that are relatively poorly 
translated by the previous weak translation sys-
tem. Finally, a strong translation system is built 
from the ensemble of the weak translation sys-
tems. 
Our experiments are conducted on Chinese-to-
English translation in three state-of-the-art SMT 
systems, including a phrase-based system, a hier-
archical phrase-based system and a syntax-based 
739
Input:   a model u, a sequence of (training) samples {(f1, r1), ..., (fm, rm)} where fi is the 
i-th source sentence, and ri is the set of reference translations for fi. 
Output: a new translation system 
Initialize: D1(i) = 1 / m for all i = 1, ..., m 
For t = 1, ..., T 
1. Train a translation system u(?*t) on {(fi, ri)} using distribution Dt 
2. Calculate the error rate t? of u(?*t) on {(fi, ri)} 
3. Set 
1 1ln( )
2
t
t
t
?? ?
+=                                                         (3)
4. Update weights 
1
( )( )
t il
t
t
t
D i eD i
Z
? ?
+ =                                                    (4)
            where li is the loss on the i-th training sample, and Zt is the normalization factor. 
Output the final system:  
v(u(?*1), ..., u (?*T)) 
Figure 1: Boosting-based System Combination 
system. All the systems are evaluated on three 
NIST MT evaluation test sets. Experimental re-
sults show that our method leads to significant 
improvements in translation accuracy over the 
baseline systems. 
2 Background 
Given a source string f, the goal of SMT is to 
find a target string e* by the following equation. 
* arg max(Pr( | ))
e
e e f=                (1) 
where Pr( | )e f is the probability that e is the 
translation of the given source string f. To model 
the posterior probability Pr( | )e f , most of the 
state-of-the-art SMT systems utilize the log-
linear model proposed by Och and Ney (2002), 
as follows, 
1
' 1
exp( ( , ))
Pr( | )
exp( ( , '))
M
m m
m
M
m m
e m
h f e
e f
h f e
?
?
=
=
?= ?
?
? ?      (2) 
where {hm( f, e ) | m = 1, ..., M} is a set of fea-
tures, and ?m is the feature weight corresponding 
to the m-th feature. hm( f, e ) can be regarded as a 
function that maps every pair of source string f 
and target string e into a non-negative value, and 
?m can be viewed as the contribution of hm( f, e ) 
to the overall score Pr( | )e f . 
In this paper, u denotes a log-linear model that 
has M fixed features {h1( f ,e ), ..., hM( f ,e )}, ? = 
{?1, ..., ?M} denotes the M parameters of u, and 
u(?) denotes a SMT system based on u with pa-
rameters ?. Generally, ? is trained on a training 
data set1 to obtain an optimized weight vector ?* 
and consequently an optimized system u(?*). 
3 Boosting-based System Combination 
for Single Translation Engine  
Suppose that there are T available SMT systems 
{u1(?*1), ..., uT(?*T)}, the task of system combina-
tion is to build a new translation system 
v(u1(?*1), ..., uT(?*T)) from {u1(?*1), ..., uT(?*T)}. 
Here v(u1(?*1), ..., uT(?*T)) denotes the combina-
tion system which combines translations from the 
ensemble of the output of each ui(?*i). We call 
ui(?*i) a member system of v(u1(?*1), ..., uT(?*T)). 
As discussed in Section 1, the diversity among 
the outputs of member systems is an important 
factor to the success of system combination. To 
obtain diversified member systems, traditional 
methods concentrate more on using structurally 
different member systems, that is u1? u2 ?...? 
uT. However, this constraint condition cannot be 
satisfied when multiple translation engines are 
not available.  
In this paper, we argue that the diversified 
member systems can also be generated from a 
single engine u(?*) by adjusting the weight vector 
?* in a principled way. In this work, we assume 
that u1 = u2 =...= uT  = u. Our goal is to find a se-
ries of ?*i and build a combined system from 
{u(?*i)}. To achieve this goal, we propose a 
                                                 
1 The data set used for weight training is generally called 
development set or tuning set in the SMT field. In this paper, 
we use the term training set to emphasize the training of 
log-linear model. 
740
boosting-based system combination method (Fig-
ure 1). 
Like other boosting algorithms, such as 
AdaBoost (Freund and Schapire, 1997; Schapire, 
2001), the basic idea of this method is to use 
weak systems (member systems) to form a strong 
system (combined system) by repeatedly calling 
weak system trainer on different distributions 
over the training samples. However, since most 
of the boosting algorithms are designed for the 
classification problem that is very different from 
the translation problem in natural language proc-
essing, several key components have to be redes-
igned when boosting is adapted to SMT system 
combination. 
3.1 Training 
In this work, Minimum Error Rate Training 
(MERT) proposed by Och (2003) is used to es-
timate feature weights ? over a series of training 
samples. As in other state-of-the-art SMT sys-
tems, BLEU is selected as the accuracy measure 
to define the error function used in MERT. Since 
the weights of training samples are not taken into 
account in BLEU2, we modify the original defi-
nition of BLEU to make it sensitive to the distri-
bution Dt(i) over the training samples. The modi-
fied version of BLEU is called weighted BLEU 
(WBLEU) in this paper. 
Let E = e1 ... em be the translations produced 
by the system, R = r1 ... rm be the reference trans-
lations where ri = {ri1, ..., riN}, and Dt(i) be the 
weight of the i-th training sample (fi, ri). The 
weighted BLEU metric has the following form: 
{ }
( )
11 1
11
1/ 4
m
4 1 1
m
1
1
  WBLEU( , )
( ) min | ( ) |
exp 1 max 1,
( ) | ( ) |
( ) ( ) ( )
     (5)
( ) ( )
m
ijti j N
m
iti
N
i ijt n ni j
in t ni
E R
D i g r
D i g e
D i g e g r
D i g e
= ? ?
=
= =
= =
? ?? ?? ?? ?= ? ?? ?? ?? ?? ?? ?? ?
? ?? ?? ?? ?? ?
?
?
?? ?
I U
 
where ( )ng s  is the multi-set of all n-grams in a 
string s. In this definition, n-grams in ei and {rij} 
are weighted by Dt(i). If the i-th training sample 
has a larger weight, the corresponding n-grams 
will have more contributions to the overall score 
WBLEU( , )E R . As a result, the i-th training 
sample gains more importance in MERT. Obvi-
                                                 
2 In this paper, we use the NIST definition of BLEU where 
the effective reference length is the length of the shortest 
reference translation. 
ously the original BLEU is just a special case of 
WBLEU when all the training samples are 
equally weighted. 
As the weighted BLEU is used to measure the 
translation accuracy on the training set, the error 
rate is defined to be: 
1 WBLEU( , )t E R? = ?               (6) 
3.2 Re-weighting 
Another key point is the maintaining of the dis-
tribution Dt(i) over the training set. Initially all 
the weights of training samples are set equally. 
On each round, we increase the weights of the 
samples that are relatively poorly translated by 
the current weak system so that the MERT-based 
trainer can focus on the hard samples in next 
round. The update rule is given in Equation 4 
with two parameters t?  and li in it. 
t?  can be regarded as a measure of the im-
portance that the t-th weak system gains in boost-
ing. The definition of t?  guarantees that t?  al-
ways has a positive value3. A main effect of t?  
is to scale the weight updating (e.g. a larger t?  
means a greater update). 
li is the loss on the i-th sample. For each i, let 
{ei1, ..., ein} be the n-best translation candidates 
produced by the system. The loss function is de-
fined to be: 
*
1
1BLEU( , ) BLEU( , )ki i i ij i
j
l e e
k =
= ? ?r r  (7) 
where BLEU(eij, ri) is the smoothed sentence-level 
BLEU score (Liang et al, 2006) of the transla-
tion e with respect to the reference translations ri, 
and ei* is the oracle translation which is selected 
from {ei1, ..., ein} in terms of BLEU(eij, ri). li can 
be viewed as a measure of the average cost that 
we guess the top-k translation candidates instead 
of the oracle translation. The value of li counts 
for the magnitude of weight update, that is, a lar-
ger li means a larger weight update on Dt(i). The 
definition of the loss function here is similar to 
the one used in (Chiang et al, 2008) where only 
the top-1 translation candidate (i.e. k = 1) is 
taken into account. 
3.3 System Combination Scheme 
In the last step of our method, a strong transla-
tion system v(u(?*1), ..., u(?*T)) is built from the 
                                                 
3 Note that the definition of t?  here is different from that in 
the original AdaBoost algorithm (Freund and Schapire, 
1997; Schapire, 2001) where t?  is a negative number when 
0.5t? > . 
741
ensemble of member systems {u(?*1), ..., u(?*T)}. 
In this work, a sentence-level combination 
method is used to select the best translation from 
the pool of the n-best outputs of all the member 
systems.  
Let H(u(?*t)) (or Ht for short) be the set of the 
n-best translation candidates produced by the t-th 
member system u(?*t), and H(v) be the union set 
of all Ht (i.e. ( ) tH v H=U ). The final translation 
is generated from H(v) based on the following 
scoring function: 
*
1
( )
arg max ( ) ( , ( ))T t tt
e H v
e e e H v? ? ?=?= ? +?    (8) 
where ( )t e?  is the log-scaled model score of e in 
the t-th member system, and t?  is the corre-
sponding feature weight. It should be noted that 
ie H?  may not exist in any 'i iH ? . In this case, 
we can still calculate the model score of e in any 
other member systems, since all the member sys-
tems are based on the same model and share the 
same feature space. ( , ( ))e H v?  is a consensus-
based scoring function which has been success-
fully adopted in SMT system combination (Duan 
et al, 2009; Hildebrand and Vogel, 2008; Li et 
al., 2009). The computation of ( , ( ))e H v?  is 
based on a linear combination of a set of n-gram 
consensuses-based features.  
( , ( )) ( , ( ))n n
n
e H v h e H v? ? + += ? +?  
( , ( ))n n
n
h e H v? ? ???            (9) 
For each order of n-gram, ( , ( ))nh e H v
+ and 
( , ( ))nh e H v
?  are defined to measure the n-gram 
agreement and disagreement between e and other 
translation candidates in H(v), respectively. n? +  
and n? ? are the feature weights corresponding to 
( , ( ))nh e H v
+ and ( , ( ))nh e H v
? . As ( , ( ))nh e H v
+ and 
( , ( ))nh e H v
?  used in our work are exactly the 
same as the features used in (Duan et al, 2009) 
and similar to the features used in (Hildebrand 
and Vogel, 2008; Li et al, 2009), we do not pre-
sent the detailed description of them in this paper. 
If p orders of n-gram are used in computing 
( , ( ))e H v? , the total number of features in the 
system combination will be 2T p+ ? (T model-
score-based features defined in Equation 8 and 
2 p?  consensus-based features defined in Equa-
tion 9). Since all these features are combined 
linearly, we use MERT to optimize them for the 
combination model. 
4 Optimization 
If implemented naively, the translation speed of 
the final translation system will be very slow. 
For a given input sentence, each member system 
has to encode it individually, and the translation 
speed is inversely proportional to the number of 
member systems generated by our method. For-
tunately, with the thought of computation, there 
are a number of optimizations that can make the 
system much more efficient in practice. 
A simple solution is to run member systems in 
parallel when translating a new sentence. Since 
all the member systems share the same data re-
sources, such as language model and translation 
table, we only need to keep one copy of the re-
quired resources in memory. The translation 
speed just depends on the computing power of 
parallel computation environment, such as the 
number of CPUs. 
Furthermore, we can use joint decoding tech-
niques to save the computation of the equivalent 
translation hypotheses among member systems. 
In joint decoding of member systems, the search 
space is structured as a translation hypergraph 
where the member systems can share their trans-
lation hypotheses. If more than one member sys-
tems share the same translation hypothesis, we 
just need to compute the corresponding feature 
values only once, instead of repeating the com-
putation in individual decoders. In our experi-
ments, we find that over 60% translation hy-
potheses can be shared among member systems 
when the number of member systems is over 4. 
This result indicates that promising speed im-
provement can be achieved by using the joint 
decoding and hypothesis sharing techniques. 
Another method to speed up the system is to 
accelerate n-gram language model with n-gram 
caching techniques. In this method, a n-gram 
cache is used to store the most frequently and 
recently accessed n-grams. When a new n-gram 
is accessed during decoding, the cache is 
checked first. If the required n-gram hits the 
cache, the corresponding n-gram probability is 
returned by the cached copy rather than re-
fetching the original data in language model. As 
the translation speed of SMT system depends 
heavily on the computation of n-gram language 
model, the acceleration of n-gram language 
model generally leads to substantial speed-up of 
SMT system. In our implementation, the n-gram 
caching in general brings us over 30% speed im-
provement of the system. 
742
5 Experiments  
Our experiments are conducted on Chinese-to-
English translation in three SMT systems. 
5.1 Baseline Systems 
The first SMT system is a phrase-based system 
with two reordering models including the maxi-
mum entropy-based lexicalized reordering model 
proposed by Xiong et al (2006) and the hierar-
chical phrase reordering model proposed by Gal-
ley and Manning (2008). In this system all 
phrase pairs are limited to have source length of 
at most 3, and the reordering limit is set to 8 by 
default4. 
The second SMT system is an in-house reim-
plementation of the Hiero system which is based 
on the hierarchical phrase-based model proposed 
by Chiang (2005).  
The third SMT system is a syntax-based sys-
tem based on the string-to-tree model (Galley et 
al., 2006; Marcu et al, 2006), where both the 
minimal GHKM and SPMT rules are extracted 
from the bilingual text, and the composed rules 
are generated by combining two or three minimal 
GHKM and SPMT rules. Synchronous binariza-
tion (Zhang et al, 2006; Xiao et al, 2009) is per-
formed on each translation rule for the CKY-
style decoding. 
In this work, baseline system refers to the sys-
tem produced by the boosting-based system 
combination when the number of iterations (i.e. 
T ) is set to 1. To obtain satisfactory baseline per-
formance, we train each SMT system for 5 times 
using MERT with different initial values of fea-
ture weights to generate a group of baseline can-
didates, and then select the best-performing one 
from this group as the final baseline system (i.e. 
the starting point in the boosting process) for the 
following experiments. 
5.2 Experimental Setup 
Our bilingual data consists of 140K sentence 
pairs in the FBIS data set5. GIZA++ is employed 
to perform the bi-directional word alignment be-
tween the source and target sentences, and the 
final word alignment is generated using the inter-
sect-diag-grow method. All the word-aligned 
bilingual sentence pairs are used to extract 
phrases and rules for the baseline systems. A 5-
gram language model is trained on the target-side 
                                                 
4 Our in-house experimental results show that this system 
performs slightly better than Moses on Chinese-to-English 
translation tasks. 
5 LDC catalog number: LDC2003E14 
of the bilingual data and the Xinhua portion of 
English Gigaword corpus. Berkeley Parser is 
used to generate the English parse trees for the 
rule extraction of the syntax-based system. The 
data set used for weight training in boosting-
based system combination comes from NIST 
MT03 evaluation set. To speed up MERT, all the 
sentences with more than 20 Chinese words are 
removed. The test sets are the NIST evaluation 
sets of MT04, MT05 and MT06. The translation 
quality is evaluated in terms of case-insensitive 
NIST version BLEU metric. Statistical signifi-
cant test is conducted using the bootstrap re-
sampling method proposed by Koehn (2004). 
Beam search and cube pruning (Huang and 
Chiang, 2007) are used to prune the search space 
in all the three baseline systems. By default, both 
of the beam size and the size of n-best list are set 
to 20. 
In the settings of boosting-based system com-
bination, the maximum number of iterations is 
set to 30, and k (in Equation 7) is set to 5. The n-
gram consensuses-based features (in Equation 9) 
used in system combination ranges from unigram 
to 4-gram. 
5.3 Evaluation of Translations 
First we investigate the effectiveness of the 
boosting-based system combination on the three 
systems.  
Figures 2-5 show the BLEU curves on the de-
velopment and test sets, where the X-axis is the 
iteration number, and the Y-axis is the BLEU 
score of the system generated by the boosting-
based system combination. The points at itera-
tion 1 stand for the performance of the baseline 
systems. We see, first of all, that all the three 
systems are improved during iterations on the 
development set. This trend also holds on the test 
sets. After 5, 7 and 8 iterations, relatively stable 
improvements are achieved by the phrase-based 
system, the Hiero system and the syntax-based 
system, respectively. The BLEU scores tend to 
converge to the stable values after 20 iterations 
for all the systems. Figures 2-5 also show that the 
boosting-based system combination seems to be 
more helpful to the phrase-based system than to 
the Hiero system and the syntax-based system. 
For the phrase-based system, it yields over 0.6 
BLEU point gains just after the 3rd iteration on 
all the data sets.  
Table 1 summarizes the evaluation results, 
where the BLEU scores at iteration 5, 10, 15, 20 
and 30 are reported for the comparison. We see 
that the boosting-based system method stably ac- 
743
 33
 34
 35
 36
 37
 38
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT03 (dev.)
phrase-based
hiero
syntax-based
Figure 2: BLEU scores on the development set 
 33
 34
 35
 36
 37
 38
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT04 (test)
phrase-based
hiero
syntax-based
Figure 3: BLEU scores on the test  set of MT04 
 32
 33
 34
 35
 36
 37
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT05 (test)
phrase-based
hiero
syntax-based
Figure 4: BLEU scores on the test set of MT05 
 30
 31
 32
 33
 34
 35
 0  5  10  15  20  25  30
B
LE
U
4[
%
]
iteration number
BLEU on MT06 (test)
phrase-based
hiero
syntax-based
Figure 5: BLEU scores on the test set of MT06 
 
Phrase-based Hiero Syntax-based 
Dev. MT04 MT05 MT06 Dev. MT04 MT05 MT06 Dev. MT04 MT05 MT06 
Baseline 33.21 33.68 32.68 30.59 33.42 34.30 33.24 30.62 35.84 35.71 35.11 32.43 
Baseline+600best 33.32 33.93 32.84 30.76 33.48 34.46 33.39 30.75 35.95 35.88 35.23 32.58 
Boosting-5Iterations 33.95* 34.32* 33.33* 31.33* 33.73 34.48 33.44 30.83 36.03 35.92 35.27 33.09 
Boosting-10Iterations 34.14* 34.68* 33.42* 31.35* 33.75 34.65 33.75* 31.02 36.14 36.39* 35.47 33.15*
Boosting-15Iterations 33.99* 34.78* 33.46* 31.45* 34.03* 34.88* 33.98* 31.20* 36.36* 36.46* 35.53* 33.43*
Boosting-20Iterations 34.09* 35.11* 33.56* 31.45* 34.17* 35.00* 34.04* 31.29* 36.44* 36.79* 35.77* 33.36*
Boosting-30Iterations 34.12* 35.16* 33.76* 31.59* 34.05* 34.99* 34.05* 31.30* 36.52* 36.81* 35.71* 33.46*
Table 1: Summary of the results (BLEU4[%]) on the development and test sets. * = significantly better 
than baseline (p < 0.05). 
  
hieves significant BLEU improvements after 15 
iterations, and the highest BLEU scores are gen-
erally yielded after 20 iterations.  
Also as shown in Table 1, over 0.7 BLEU 
point gains are obtained on the phrase-based sys-
tem after 10 iterations. The largest BLEU im-
provement on the phrase-based system is over 1 
BLEU point in most cases. These results reflect 
that our method is relatively more effective for 
the phrase-based system than for the other two 
systems, and thus confirms the fact we observed 
in Figures 2-5. 
We also investigate the impact of n-best list 
size on the performance of baseline systems. For 
the comparison, we show the performance of the 
baseline systems with the n-best list size of 600 
(Baseline+600best in Table 1) which equals to 
the maximum number of translation candidates 
accessed in the final combination system (combi- 
ne 30 member systems, i.e. Boosing-30Iterations). 
744
 15
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT03 (dev.)
phrase-based
hiero
syntax-based
Figure 6: Diversity on the development set 
 10
 15
 20
 25
 30
 35
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT04 (test)
phrase-based
hiero
syntax-based
Figure 7: Diversity on the test set of MT04 
 15
 20
 25
 30
 35
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT05 (test)
phrase-based
hiero
syntax-based
Figure 8: Diversity on the test set of MT05 
 15
 20
 25
 30
 35
 40
 0  5  10  15  20  25  30
D
iv
er
si
ty
 (T
E
R
[%
])
iteration number
Diversity on MT06 (test)
phrase-based
hiero
syntax-based
Figure 9: Diversity on the test set of MT06 
 
As shown in Table 1, Baseline+600best obtains 
stable improvements over Baseline. It indicates 
that the access to larger n-best lists is helpful to 
improve the performance of baseline systems. 
However, the improvements achieved by Base-
line+600best are modest compared to the im-
provements achieved by Boosting-30Iterations. 
These results indicate that the SMT systems can 
benefit more from the diversified outputs of 
member systems rather than from larger n-best 
lists produced by a single system. 
5.4 Diversity among Member Systems 
We also study the change of diversity among the 
outputs of member systems during iterations. 
The diversity is measured in terms of the Trans-
lation Error Rate (TER) metric proposed in 
(Snover et al, 2006). A higher TER score means 
that more edit operations are performed if we 
transform one translation output into another 
translation output, and thus reflects a larger di-
versity between the two outputs. In this work, the 
TER score for a given group of member systems 
is calculated by averaging the TER scores be-
tween the outputs of each pair of member sys-
tems in this group. 
Figures 6-9 show the curves of diversity on 
the development and test sets, where the X-axis 
is the iteration number, and the Y-axis is the di-
versity. The points at iteration 1 stand for the 
diversities of baseline systems. In this work, the 
baseline?s diversity is the TER score of the group 
of baseline candidates that are generated in ad-
vance (Section 5.1). 
We see that the diversities of all the systems 
increase during iterations in most cases, though a 
few drops occur at a few points. It indicates that 
our method is very effective to generate diversi-
fied member systems. In addition, the diversities 
of baseline systems (iteration 1) are much lower 
745
than those of the systems generated by boosting 
(iterations 2-30). Together with the results shown 
in Figures 2-5, it confirms our motivation that 
the diversified translation outputs can lead to 
performance improvements over the baseline 
systems. 
Also as shown in Figures 6-9, the diversity of 
the Hiero system is much lower than that of the 
phrase-based and syntax-based systems at each 
individual setting of iteration number. This inter-
esting finding supports the observation that the 
performance of the Hiero system is relatively 
more stable than the other two systems as shown 
in Figures 2-5. The relative lack of diversity in 
the Hiero system might be due to the spurious 
ambiguity in Hiero derivations which generally 
results in very few different translations in trans-
lation outputs (Chiang, 2007). 
5.5 Evaluation of Oracle Translations 
In this set of experiments, we evaluate the oracle 
performance on the n-best lists of the baseline 
systems and the combined systems generated by 
boosting-based system combination. Our primary 
goal here is to study the impact of our method on 
the upper-bound performance.  
Table 2 shows the results, where Base-
line+600best stands for the top-600 translation 
candidates generated by the baseline systems, 
and Boosting-30iterations stands for the ensem-
ble of 30 member systems? top-20 translation 
candidates. As expected, the oracle performance 
of Boosting-30Iterations is significantly higher 
than that of Baseline+600best. This result indi-
cates that our method can provide much ?better? 
translation candidates for system combination 
than enlarging the size of n-best list naively. It 
also gives us a rational explanation for the sig-
nificant improvements achieved by our method 
as shown in Section 5.3. 
 
Data 
Set 
Method Phrase-
based 
Hiero Syntax-
based 
Baseline+600best 46.36 46.51 46.92 Dev. 
Boosting-30Iterations 47.78* 47.44* 48.70* 
Baseline+600best 43.94 44.52 46.88 MT04 
Boosting-30Iterations 45.97* 45.47* 49.40* 
Baseline+600best 42.32 42.47 45.21 MT05 
Boosting-30Iterations 44.82* 43.44* 47.02* 
Baseline+600best 39.47 39.39 40.52 MT06 
Boosting-30Iterations 41.51* 40.10* 41.88* 
Table 2: Oracle performance of various systems. 
* = significantly better than baseline (p < 0.05). 
6 Related Work 
Boosting is a machine learning (ML) method that 
has been well studied in the ML community 
(Freund, 1995; Freund and Schapire, 1997; 
Collins et al, 2002; Rudin et al, 2007), and has 
been successfully adopted in natural language 
processing (NLP) applications, such as document 
classification (Schapire and Singer, 2000) and 
named entity classification (Collins and Singer, 
1999). However, most of the previous work did 
not study the issue of how to improve a single 
SMT engine using boosting algorithms. To our 
knowledge, the only work addressing this issue is 
(Lagarda and Casacuberta, 2008) in which the 
boosting algorithm was adopted in phrase-based 
SMT. However, Lagarda and Casacuberta 
(2008)?s method calculated errors over the 
phrases that were chosen by phrase-based sys-
tems, and could not be applied to many other 
SMT systems, such as hierarchical phrase-based 
systems and syntax-based systems. Differing 
from Lagarda and Casacuberta?s work, we are 
concerned more with proposing a general 
framework which can work with most of the cur-
rent SMT models and empirically demonstrating 
its effectiveness on various SMT systems. 
There are also some other studies on building 
diverse translation systems from a single transla-
tion engine for system combination. The first 
attempt is (Macherey and Och, 2007). They em-
pirically showed that diverse translation systems 
could be generated by changing parameters at 
early-stages of the training procedure. Following 
Macherey and Och (2007)?s work, Duan et al 
(2009) proposed a feature subspace method to 
build a group of translation systems from various 
different sub-models of an existing SMT system. 
However, Duan et al (2009)?s method relied on 
the heuristics used in feature sub-space selection. 
For example, they used the remove-one-feature 
strategy and varied the order of n-gram language 
model to obtain a satisfactory group of diverse 
systems. Compared to Duan et al (2009)?s 
method, a main advantage of our method is that 
it can be applied to most of the SMT systems 
without designing any heuristics to adapt it to the 
specified systems. 
7 Discussion and Future Work 
Actually the method presented in this paper is 
doing something rather similar to Minimum 
Bayes Risk (MBR) methods. A main difference 
lies in that the consensus-based combination 
method here does not model the posterior prob-
ability of each hypothesis (i.e. all the hypotheses 
are assigned an equal posterior probability when 
we calculate the consensus-based features). 
746
Greater improvements are expected if MBR 
methods are used and consensus-based combina-
tion techniques smooth over noise in the MERT 
pipeline. 
In this work, we use a sentence-level system 
combination method to generate final transla-
tions. It is worth studying other more sophisti-
cated alternatives, such as word-level and 
phrase-level system combination, to further im-
prove the system performance. 
Another issue is how to determine an appro-
priate number of iterations for boosting-based 
system combination. It is especially important 
when our method is applied in the real-world 
applications. Our empirical study shows that the 
stable and satisfactory improvements can be 
achieved after 6-8 iterations, while the largest 
improvements can be achieved after 20 iterations. 
In our future work, we will study in-depth prin-
cipled ways to determine the appropriate number 
of iterations for boosting-based system combina-
tion. 
8 Conclusions 
We have proposed a boosting-based system com-
bination method to address the issue of building 
a strong translation system from a group of weak 
translation systems generated from a single SMT 
engine. We apply our method to three state-of-
the-art SMT systems, and conduct experiments 
on three NIST Chinese-to-English MT evalua-
tions test sets. The experimental results show that 
our method is very effective to improve the 
translation accuracy of the SMT systems. 
Acknowledgements 
This work was supported in part by the National 
Science Foundation of China (60873091) and the 
Fundamental Research Funds for the Central 
Universities (N090604008). The authors would 
like to thank the anonymous reviewers for their 
pertinent comments, Tongran Liu, Chunliang 
Zhang and Shujie Yao for their valuable sugges-
tions for improving this paper, and Tianning Li 
and Rushan Chen for developing parts of the 
baseline systems. 
References  
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. 
of ACL 2005, Ann Arbor, Michigan, pages 263-
270. 
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201-228. 
David Chiang, Yuval Marton and Philip Resnik. 2008. 
Online Large-Margin Training of Syntactic and 
Structural Translation Features. In Proc. of 
EMNLP 2008, Honolulu, pages 224-233. 
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In 
Proc. of EMNLP/VLC 1999, pages 100-110. 
Michael Collins, Robert Schapire and Yoram Singer. 
2002. Logistic Regression, AdaBoost and Bregman 
Distances. Machine Learning, 48(3): 253-285. 
Brooke Cowan, Ivona Ku?erov? and Michael Collins. 
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. of EMNLP 2006, pages 232-241. 
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency 
insertion grammars. In Proc. of ACL 2005, Ann 
Arbor, Michigan, pages 541-548. 
Nan Duan, Mu Li, Tong Xiao and Ming Zhou. 2009. 
The Feature Subspace Method for SMT System 
Combination. In Proc. of EMNLP 2009, pages 
1096-1104. 
Jason Eisner. 2003. Learning non-isomorphic tree 
mappings for machine translation. In Proc. of ACL 
2003, pages 205-208. 
Yoav Freund. 1995. Boosting a weak learning algo-
rithm by majority. Information and Computation, 
121(2): 256-285. 
Yoav Freund and Robert Schapire. 1997. A decision-
theoretic generalization of on-line learning and an 
application to boosting. Journal of Computer and 
System Sciences, 55(1):119-139. 
Michel Galley, Jonathan Graehl, Kevin Knight, 
Daniel Marcu, Steve DeNeefe, Wei Wang and 
Ignacio Thayer. 2006. Scalable inferences and 
training of context-rich syntax translation models. 
In Proc. of ACL 2006, Sydney, Australia, pages 
961-968. 
Michel Galley and Christopher D. Manning. 2008. A 
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proc. of EMNLP 2008, Hawaii, 
pages 848-856. 
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
Proc. of the 8th AMTA conference, pages 254-261. 
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language 
models. In Proc. of ACL 2007, Prague, Czech Re-
public, pages 144-151. 
747
Philipp Koehn, Franz Och and Daniel Marcu. 2003. 
Statistical Phrase-Based Translation. In Proc. of 
HLT-NAACL 2003, Edmonton, USA, pages 48-54. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of 
EMNLP 2004, Barcelona, Spain, pages 388-395. 
Antonio Lagarda and Francisco Casacuberta. 2008. 
Applying Boosting to Statistical Machine Transla-
tion. In Proc. of the 12th EAMT conference, pages 
88-96. 
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li and 
Ming Zhou. 2009. Collaborative Decoding: Partial 
Hypothesis Re-Ranking Using Translation Consen-
sus between Decoders. In Proc. of ACL-IJCNLP 
2009, Singapore, pages 585-592. 
Percy Liang, Alexandre Bouchard-C?t?, Dan Klein 
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proc. of 
COLING/ACL 2006, pages 104-111. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine 
Translation. In Proc. of ACL 2006, pages 609-616. 
Wolfgang Macherey and Franz Och. 2007. An Em-
pirical Study on Computing Consensus Transla-
tions from Multiple Machine Translation Systems. 
In Proc. of EMNLP 2007, pages 986-995. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language 
phrases. In Proc. of EMNLP 2006, Sydney, Aus-
tralia, pages 44-52. 
Evgeny Matusov, Nicola Ueffing and Hermann Ney. 
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced 
hypotheses alignment. In Proc. of EACL 2006, 
pages 33-40. 
Franz Och and Hermann Ney. 2002. Discriminative 
Training and Maximum Entropy Models for Statis-
tical Machine Translation. In Proc. of ACL 2002, 
Philadelphia, pages 295-302. 
Franz Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proc. of ACL 
2003, Japan, pages 160-167. 
Antti-Veikko Rosti, Spyros Matsoukas and Richard 
Schwartz. 2007. Improved Word-Level System 
Combination for Machine Translation. In Proc. of 
ACL 2007, pages 312-319. 
Cynthia Rudin, Robert Schapire and Ingrid Daube-
chies. 2007. Analysis of boosting algorithms using 
the smooth margin function.  The Annals of Statis-
tics, 35(6): 2723-2768. 
Robert Schapire and Yoram Singer. 2000. BoosTexter: 
A boosting-based system for text categorization. 
Machine Learning, 39(2/3):135-168. 
Robert Schapire. The boosting approach to machine 
learning: an overview. 2001. In Proc. of MSRI 
Workshop on Nonlinear Estimation and Classifica-
tion, Berkeley, CA, USA, pages 1-23. 
Matthew Snover, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proc. of the 7th AMTA confer-
ence, pages 223-231. 
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu and 
Ming Zhou. 2009. Better Synchronous Binarization 
for Machine Translation. In Proc. of EMNLP 2009, 
Singapore, pages 362-370. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proc. of ACL 
2006, Sydney, pages 521-528. 
Hao Zhang, Liang Huang, Daniel Gildea and Kevin 
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006, 
New York, USA, pages 256- 263. 
748
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 715?719,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Better Automatic Treebank Conversion Using A Feature-Based Approach
Muhua Zhu Jingbo Zhu Minghan Hu
Natural Language Processing Lab.
Northeastern University, China
zhumuhua@gmail.com
zhujingbo@mail.neu.edu.cn huminghan@ise.neu.edu.cn
Abstract
For the task of automatic treebank conversion,
this paper presents a feature-based approach
which encodes bracketing structures in a tree-
bank into features to guide the conversion of
this treebank to a different standard. Exper-
iments on two Chinese treebanks show that
our approach improves conversion accuracy
by 1.31% over a strong baseline.
1 Introduction
In the field of syntactic parsing, research efforts have
been put onto the task of automatic conversion of
a treebank (source treebank) to fit a different stan-
dard which is exhibited by another treebank (tar-
get treebank). Treebank conversion is desirable pri-
marily because source-style and target-style annota-
tions exist for non-overlapping text samples so that a
larger target-style treebank can be obtained through
such conversion. Hereafter, source and target tree-
banks are named as heterogenous treebanks due to
their different annotation standards. In this paper,
we focus on the scenario of conversion between
phrase-structure heterogeneous treebanks (Wang et
al., 1994; Zhu and Zhu, 2010).
Due to the availability of annotation in a source
treebank, it is natural to use such annotation to
guide treebank conversion. The motivating idea is
illustrated in Fig. 1 which depicts a sentence anno-
tated with standards of Tsinghua Chinese Treebank
(TCT) (Zhou, 1996) and Penn Chinese Treebank
(CTB) (Xue et al, 2002), respectively. Suppose
that the conversion is in the direction from the TCT-
style parse (left side) to the CTB-style parse (right
side). The constituents vp:[?/will??/surrender],
dj:[??/enemy?/will??/surrender], and np:[?
?/intelligence??/experts] in the TCT-style parse
strongly suggest a resulting CTB-style parse also
bracket the words as constituents. Zhu and
Zhu (2010) show the effectiveness of using brack-
eting structures in a source treebank (source-side
bracketing structures in short) as parsing constraints
during the decoding phase of a target treebank-based
parser.
However, using source-side bracketing structures
as parsing constraints is problematic in some cases.
As illustrated in the shadow part of Fig. 1, the TCT-
style parse takes ???/deems? as the right bound-
ary of a constituent while in the CTB-style parse,
???? is the left boundary of a constituent. Ac-
cording to the criteria used in Zhu and Zhu (2010),
any CTB-style constituents with ???? being the
left boundary are thought to be inconsistent with the
bracketing structure of the TCT-style parse and will
be pruned. However, if we prune such ?inconsistent?
constituents, the correct conversion result (right side
of Fig. 1) has no chance to be generated.
The problem comes from binary distinctions used
in the approach of Zhu and Zhu (2010). With bi-
nary distinctions, constituents generated by a target
treebank-based parser are judged to be either con-
sistent or inconsistent with source-side bracketing
structures. That approach prunes inconsistent con-
stituents which instead might be correct conversion
results1. In this paper, we insist on using source-
side bracketing structures as guiding information.
Meanwhile, we aim to avoid using binary distinc-
tions. To achieve such a goal, we propose to use a
feature-based approach to treebank conversion and
to encode source-side bracketing structures as a set
1To show how severe this problem might be, Section 3.1
presents statistics on inconsistence between TCT and CTB.
715
zj
dj
np
n
??
n
??
v
??
,
,
dj
n
??
vp
d
?
v
??
IP
NP
NN
??
NN
??
VP
VV
??
PU
,
IP
NP
NN
??
VP
AD
?
VV
??
?? ?? ?? , ?? ? ??
qingbao zhuanjia renwei , diren jiang touxiang
intelligence experts deem , enemy will surrender
Figure 1: An example sentence with TCT-style annotation (left) and CTB-style annotation (right).
of features. The advantage is that inconsistent con-
stituents can be scored with a function based on the
features rather than ruled out as impossible.
To test the efficacy of our approach, we conduct
experiments on conversion from TCT to CTB. The
results show that our approach achieves a 1.31% ab-
solute improvement in conversion accuracy over the
approach used in Zhu and Zhu (2010).
2 Our Approach
2.1 Generic System Architecture
To conduct treebank conversion, our approach, over-
all speaking, proceeds in the following steps.
Step 1: Build a parser (named source parser) on a
source treebank, and use it to parse sentences
in the training data of a target treebank.
Step 2: Build a parser on pairs of golden target-
style and auto-assigned (in Step 1) source-style
parses in the training data of the target tree-
bank. Such a parser is named heterogeneous
parser since it incorporates information derived
from both source and target treebanks, which
follow different annotation standards.
Step 3: In the testing phase, the heterogeneous
parser takes golden source-style parses as input
and conducts treebank conversion. This will be
explained in detail in Section 2.2.
To instantiate the generic framework described
above, we need to decide the following three factors:
(1) a parsing model for building a source parser, (2)
a parsing model for building a heterogeneous parser,
and (3) features for building a heterogeneous parser.
In principle, any off-the-shelf parsers can be used
to build a source parser, so we focus only on the
latter two factors. To build a heterogeneous parser,
we use feature-based parsing algorithms in order to
easily incorporate features that encode source-side
bracketing structures. Theoretically, any feature-
based approaches are applicable, such as Finkel et
al. (2008) and Tsuruoka et al (2009). In this pa-
per, we use the shift-reduce parsing algorithm for its
simplicity and competitive performance.
2.2 Shift-Reduce-Based Heterogeneous Parser
The heterogeneous parser used in this paper is based
on the shift-reduce parsing algorithm described in
Sagae and Lavie (2006a) and Wang et al (2006).
Shift-reduce parsing is a state transition process,
where a state is defined to be a tuple ?S,Q?. Here, S
is a stack containing partial parses, and Q is a queue
containing word-POS pairs to be processed. At each
state transition, a shift-reduce parser either shifts the
top item of Q onto S, or reduces the top one (or two)
items on S.
A shift-reduce-based heterogeneous parser pro-
ceeds similarly as the standard shift-reduce parsing
algorithm. In the training phase, each target-style
parse tree in the training data is transformed into
a binary tree (Charniak et al, 1998) and then de-
composed into a (golden) action-state sequence. A
classifier can be trained on the set of action-states,
716
where each state is represented as a feature vector.
In the testing phase, the trained classifier is used
to choose actions for state transition. Moreover,
beam search strategies can be used to expand the
search space of a shift-reduce-based heterogeneous
parser (Sagae and Lavie, 2006a). To incorporate in-
formation on source-side bracketing structures, in
both training and testing phases, feature vectors rep-
resenting states ?S,Q? are augmented with features
that bridge the current state and the corresponding
source-style parse.
2.3 Features
This section describes the feature functions used to
build a heterogeneous parser on the training data
of a target treebank. The features can be divided
into two groups. The first group of features are
derived solely from target-style parse trees so they
are referred to as target side features. This group
of features are completely identical to those used in
Sagae and Lavie (2006a).
In addition, we have features extracted jointly
from target-style and source-style parse trees. These
features are generated by consulting a source-style
parse (referred to as ts) while we decompose a
target-style parse into an action-state sequence.
Here, si denote the ith item from the top of the
stack, and qi denote the ith item from the front
end of the queue. We refer to these features as
heterogeneous features.
Constituent features Fc(si, ts)
This feature schema covers three feature functions:
Fc(s1, ts), Fc(s2, ts), and Fc(s1 ? s2, ts), which
decide whether partial parses on stack S correspond
to a constituent in the source-style parse ts. That is,
Fc(si, ts)=+ if si has a bracketing match (ignoring
grammar labels) with any constituent in ts. s1?s2
represents a concatenation of spans of s1 and s2.
Relation feature Fr(Ns(s1), Ns(s2))
We first position the lowest node Ns(si) in ts,
which dominates the span of si. Then a feature
function Fr(Ns(s1), Ns(s2)) is defined to indicate
the relationship of Ns(s1) and Ns(s2). If Ns(s1)
is identical to or a sibling of Ns(s2), we say
Fr(Ns(s1), Ns(s2)) =+.
Features Bridging Source and Target Parses
Fc(s1, ts)=?
Fc(s2, ts)=+
Fc(s1?s2, ts)=+
Fr(Ns(s1), Ns(s2))=?
Ff (RF (s1), q1)=?
Fp(RF (s1), q1)= ?v ? dj ? zj ?,?
Table 1: An example of new features. Suppose we are
considering the sentence depicted in Fig. 1.
Frontier-words feature Ff (RF (s1), q1)
A feature function which decides whether the right
frontier word of s1 and q1 are in the same base
phrase in ts. Here, a base phrase is defined to be
any phrase which dominates no other phrases.
Path feature Fp(RF (s1), q1)
Syntactic path features are widely used in the litera-
ture of semantic role labeling (Gildea and Jurafsky,
2002) to encode information of both structures and
grammar labels. We define a string-valued feature
function Fp(RF (s1), q1) which connects the right
frontier word of s1 to q1 in ts.
To better understand the above feature func-
tions, we re-examine the example depicted in
Fig. 1. Suppose that we use a shift-reduce-based
heterogeneous parser to convert the TCT-style parse
to the CTB-style parse and that stack S currently
contains two partial parses: s2:[NP (NN??) (NN
??)] and s1: (VV ??). In such a state, we can
see that spans of both s2 and s1 ?s2 correspond to
constituents in ts but that of s1 does not. Moreover,
Ns(s1) is dj and Ns(s2) is np, so Ns(s1) and
Ns(s2) are neither identical nor sisters in ts. The
values of these features are collected in Table 1.
3 Experiments
3.1 Data Preparation and Performance Metric
In the experiments, we use two heterogeneous tree-
banks: CTB 5.1 and the TCT corpus released by
the CIPS-SIGHAN-2010 syntactic parsing competi-
tion2. We actually only use the training data of these
two corpora, that is, articles 001-270 and 400-1151
(18,100 sentences, 493,869 words) of CTB 5.1 and
2http://www.cipsc.org.cn/clp2010/task2 en.htm
717
the training data (17,529 sentences, 481,061 words)
of TCT.
To evaluate conversion accuracy, we use the
same test set (named Sample-TCT) as in Zhu and
Zhu (2010), which is a set of 150 sentences with
manually assigned CTB-style and TCT-style parse
trees. In Sample-TCT, 6.19% (215/3473) CTB-
style constituents are inconsistent with respect to the
TCT standard and 8.87% (231/2602) TCT-style con-
stituents are inconsistent with respect to the CTB
standard.
For all experiments, bracketing F1 is used as the
performance metric, provided by EVALB 3.
3.2 Implementation Issues
To implement a heterogeneous parser, we first build
a Berkeley parser (Petrov et al, 2006) on the TCT
training data and then use it to assign TCT-style
parses to sentences in the CTB training data. On
the ?updated? CTB training data, we build two shift-
reduce-based heterogeneous parsers by using max-
imum entropy classification model, without/with
beam search. Hereafter, the two heterogeneous
parsers are referred to as Basic-SR and Beam-SR, re-
spectively.
In the testing phase, Basic-SR and Beam-SR con-
vert TCT-style parse trees in Sample-TCT to the
CTB standard. The conversion results are evalu-
ated against corresponding CTB-style parse trees in
Sample-TCT. Before conducting treebank conver-
sion, we apply the POS adaptation method proposed
in Jiang et al (2009) to convert TCT-style POS tags
in the input to the CTB standard. The POS conver-
sion accuracy is 96.2% on Sample-TCT.
3.3 Results
Table 2 shows the results achieved by Basic-SR and
Beam-SR with heterogeneous features being added
incrementally. Here, baseline represents the systems
which use only target side features. From the table
we can see that heterogeneous features improve con-
version accuracy significantly. Specifically, adding
the constituent (Fc) features to Basic-SR (Beam-
SR) achieves a 2.79% (3%) improvement, adding
the relation (Fr) and frontier-word (Ff ) features
yields a 0.79% (0.98%) improvement, and adding
3http://nlp.cs.nyu.edu/evalb
System Features <= 40 words Unlimited
Basic-SR baseline 83.34 80.33
+Fc 85.89 83.12
+Fr, +Ff 85.47 83.91
+Fp 86.01 84.05
Beam-SR baseline 84.40 81.27
+Fc 86.30 84.27
+Fr, + Ff 87.00 85.25
+Fp 87.27 85.38
Table 2: Adding new features to baselines improve tree-
bank conversion accuracy significantly on Sample-TCT.
the path (Fp) feature achieves a 0.14% (0.13%) im-
provement. The path feature is not so effective as
expected, although it manages to achieve improve-
ments. One possible reason lies on the data sparse-
ness problem incurred by this feature.
Since we use the same training and testing data
as in Zhu and Zhu (2010), we can compare our
approach directly with the informed decoding ap-
proach used in that work. We find that Basic-SR
achieves very close conversion results (84.05% vs.
84.07%) and Beam-SR even outperforms the in-
formed decoding approach (85.38% vs. 84.07%)
with a 1.31% absolute improvement.
4 Related Work
For phrase-structure treebank conversion, Wang et
al. (1994) suggest to use source-side bracketing
structures to select conversion results from k-best
lists. The approach is quite generic in the sense that
it can be used for conversion between treebanks of
different grammar formalisms, such as from a de-
pendency treebank to a constituency treebank (Niu
et al, 2009). However, it suffers from limited
variations in k-best lists (Huang, 2008). Zhu and
Zhu (2010) propose to incorporate bracketing struc-
tures as parsing constraints in the decoding phase of
a CKY-style parser. Their approach shows signifi-
cant improvements over Wang et al (1994). How-
ever, it suffers from binary distinctions (consistent
or inconsistent), as discussed in Section 1.
The approach in this paper is reminiscent of
co-training (Blum and Mitchell, 1998; Sagae and
Lavie, 2006b) and up-training (Petrov et al, 2010).
Moreover, it coincides with the stacking method
used for dependency parser combination (Martins
718
et al, 2008; Nivre and McDonald, 2008), the
Pred method for domain adaptation (Daume? III and
Marcu, 2006), and the method for annotation adap-
tation of word segmentation and POS tagging (Jiang
et al, 2009). As one of the most related works,
Jiang and Liu (2009) present a similar approach to
conversion between dependency treebanks. In con-
trast to Jiang and Liu (2009), the task studied in this
paper, phrase-structure treebank conversion, is rel-
atively complicated and more efforts should be put
into feature engineering.
5 Conclusion
To avoid binary distinctions used in previous ap-
proaches to automatic treebank conversion, we pro-
posed in this paper a feature-based approach. Exper-
iments on two Chinese treebanks showed that our
approach outperformed the baseline system (Zhu
and Zhu, 2010) by 1.31%.
Acknowledgments
We thank Kenji Sagae for helpful discussions on the
implementation of shift-reduce parser and the three
anonymous reviewers for comments. This work was
supported in part by the National Science Founda-
tion of China (60873091; 61073140), Specialized
Research Fund for the Doctoral Program of Higher
Education (20100042110031), the Fundamental Re-
search Funds for the Central Universities and Nat-
ural Science Foundation of Liaoning Province of
China.
References
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of COLT 1998.
Eugene Charniak, Sharon Goldwater, and Mark Johnson.
1998. Edge-Based Best-First Chart Parsing. In Pro-
ceedings of the Six Workshop on Very Large Corpora,
pages 127-133.
Hal Daume? III and Daniel Marcu. 2006. Domain Adap-
tation for Statistical Classifiers. Journal of Artifical
Intelligence Research, 26:101-166.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, Feature-Based Conditional
Random Fileds Parsing. In Proceedings of ACL 2008,
pages 959-967.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic La-
beling for Semantic Roles. Computational Linguis-
tics, 28(3):245-288.
Liang Huang. 2008. Forest Reranking: Discriminative
Parsing with Non-local Features. In Proceedings of
ACL, pages 824-831.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chinese
Word Segmentation and POS Tagging - A Case Study.
In Proceedings of ACL 2009, pages 522-530.
Wenbin Jiang and Qun Liu. 2009. Automatic Adapta-
tion of Annotation Standards for Dependency Parsing
? Using Projected Treebank As Source Corpus. In
Proceedings of IWPT 2009, pages 25-28.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stack Dependency Parsers. In
Proceedings of EMNLP 2008, pages 157-166.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Ex-
ploiting Heterogeneous Treebanks for Parsing. In Pro-
ceedings of ACL 2009, pages 46-54.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing Graph-Based and Transition-Based Dependency
Parsers. In Proceedings of ACL 2008, pages 950-958.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proceedings of ACL
2006, pages 433-440.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for Accurate Deter-
ministic Question Parsing. In Proceedings of EMNLP
2010, pages 705-713.
Kenji Sagae and Alon Lavie. 2006. A Best-First Prob-
abilistic Shift-Reduce Parser. In Proceedings of ACL-
COLING 2006, pages 691-698.
Kenji Sagae and Alon Lavie. 2006. Parser Combination
by Reparsing. In Proceedings of NAACL 2006, pages
129-132.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Anani-
adou. 2009. Fast Full Parsing by Linear-Chain Condi-
tional Random Fields. In Proceedings of EACL 2009,
pages 790-798.
Jong-Nae Wang, Jing-Shin Chang, and Keh-Yih Su.
1994. An Automatic Treebank Conversion Algorithm
for Corpus Sharing. In Proceedings of ACL 1994,
pages 248-254.
Mengqiu Wang, Kenji Sagae, and Teruk Mitamura. 2006.
A Fast, Deterministic Parser for Chinese. In Proceed-
ings of ACL-COLING 2006, pages 425-432.
Nianwen Xue, Fu dong Chiou, and Martha Palmer. 2002.
Building a Large-Scale Annotated Chinese Corpus. In
Proceedings of COLING 2002, pages 1-8.
Qiang Zhou. 1996. Phrase Bracketing and Annotation on
Chinese Language Corpus (in Chinese). Ph.D. thesis,
Peking University.
Muhua Zhu, and Jingbo Zhu. 2010. Automatic Treebank
Conversion via Informed Decoding. In Porceedings of
COLING 2010, pages 1541-1549.
719
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434?443,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Fast and Accurate Shift-Reduce Constituent Parsing
Muhua Zhu?, Yue Zhang?, Wenliang Chen?, Min Zhang? and Jingbo Zhu?
?Natural Language Processing Lab., Northeastern University, China
?Singapore University of Technology and Design, Singapore
? Soochow University, China and Institute for Infocomm Research, Singapore
zhumuhua@gmail.com yue zhang@sutd.edu.sg
chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg
zhujingbo@mail.neu.edu.cn
Abstract
Shift-reduce dependency parsers give
comparable accuracies to their chart-
based counterparts, yet the best shift-
reduce constituent parsers still lag behind
the state-of-the-art. One important reason
is the existence of unary nodes in phrase
structure trees, which leads to different
numbers of shift-reduce actions between
different outputs for the same input. This
turns out to have a large empirical impact
on the framework of global training and
beam search. We propose a simple yet
effective extension to the shift-reduce
process, which eliminates size differences
between action sequences in beam-search.
Our parser gives comparable accuracies
to the state-of-the-art chart parsers. With
linear run-time complexity, our parser is
over an order of magnitude faster than the
fastest chart parser.
1 Introduction
Transition-based parsers employ a set of shift-
reduce actions and perform parsing using a se-
quence of state transitions. The pioneering mod-
els rely on a classifier to make local decisions, and
search greedily for a transition sequence to build a
parse tree. Greedy, classifier-based parsers have
been developed for both dependency grammars
(Yamada and Matsumoto, 2003; Nivre et al, 2006)
and phrase-structure grammars (Sagae and Lavie,
2005). With linear run-time complexity, they were
commonly regarded as a faster but less accurate
alternative to graph-based chart parsers (Collins,
1997; Charniak, 2000; McDonald et al, 2005).
Various methods have been proposed to address
the disadvantages of greedy local parsing, among
which a framework of beam-search and global
discriminative training have been shown effective
for dependency parsing (Zhang and Clark, 2008;
Huang and Sagae, 2010). While beam-search
reduces error propagation compared with greedy
search, a discriminative model that is globally op-
timized for whole sequences of transition actions
can avoid local score biases (Lafferty et al, 2001).
This framework preserves the most important ad-
vantage of greedy local parsers, including linear
run-time complexity and the freedom to define ar-
bitrary features. With the use of rich non-local fea-
tures, transition-based dependency parsers achieve
state-of-the-art accuracies that are comparable to
the best-graph-based parsers (Zhang and Nivre,
2011; Bohnet and Nivre, 2012). In addition, pro-
cessing tens of sentences per second (Zhang and
Nivre, 2011), these transition-based parsers can be
a favorable choice for dependency parsing.
The above global-learning and beam-search
framework can be applied to transition-based
phrase-structure (constituent) parsing also (Zhang
and Clark, 2009), maintaining all the afore-
mentioned benefits. However, the effects were
not as significant as for transition-based depen-
dency parsing. The best reported accuracies of
transition-based constituent parsers still lag behind
the state-of-the-art (Sagae and Lavie, 2006; Zhang
and Clark, 2009). One difference between phrase-
structure parsing and dependency parsing is that
for the former, parse trees with different numbers
of unary rules require different numbers of actions
to build. Hence the scoring model needs to disam-
biguate between transitions sequences with differ-
ent sizes. For the same sentence, the largest output
can take twice as many as actions to build as the
434
smallest one. This turns out to have a significant
empirical impact on parsing with beam-search.
We propose an extension to the shift-reduce pro-
cess to address this problem, which gives signifi-
cant improvements to the parsing accuracies. Our
method is conceptually simple, requiring only one
additional transition action to eliminate size dif-
ferences between different candidate outputs. On
standard evaluations using both the Penn Tree-
bank and the Penn Chinese Treebank, our parser
gave higher accuracies than the Berkeley parser
(Petrov and Klein, 2007), a state-of-the-art chart
parser. In addition, our parser runs with over 89
sentences per second, which is 14 times faster than
the Berkeley parser, and is the fastest that we are
aware of for phrase-structure parsing. An open
source release of our parser (version 0.6) is freely
available on the Web. 1
In addition to the above contributions, we apply
a variety of semi-supervised learning techniques to
our transition-based parser. These techniques have
been shown useful to improve chart-based pars-
ing (Koo et al, 2008; Chen et al, 2012), but little
work has been done for transition-based parsers.
We therefore fill a gap in the literature by report-
ing empirical results using these methods. Experi-
mental results show that semi-supervised methods
give a further improvement of 0.9% in F-score on
the English data and 2.4% on the Chinese data.
Our Chinese results are the best that we are aware
of on the standard CTB data.
2 Baseline parser
We adopt the parser of Zhang and Clark (2009) for
our baseline, which is based on the shift-reduce
process of Sagae and Lavie (2005), and employs
global perceptron training and beam search.
2.1 Vanilla Shift-Reduce
Shift-reduce parsing is based on a left-to-right
scan of the input sentence. At each step, a tran-
sition action is applied to consume an input word
or construct a new phrase-structure. A stack
is used to maintain partially constructed phrase-
structures, while the input words are stored in a
buffer. The set of transition actions are
? SHIFT: pop the front word from the buffer,
and push it onto the stack.
1http://sourceforge.net/projects/zpar/
Axioms [?, 0, false,0]
Goal [S, n, true, C]
Inference Rules:
[S, i, false, c]
SHIFT [S|w, i + 1, false, c + cs]
[S|s1s0, i, false, c]
REDUCE-L/R-X [S|X, i, false, c + cr]
[S|s0, i, false, c]
UNARY-X [S|X, i, false, c + cu]
[S, n, false, c]
FINISH [S, n, true, c + cf ]
Figure 1: Deduction system of the baseline shift-
reduce parsing process.
? REDUCE-L/R-X: pop the top two con-
stituents off the stack, combine them into a
new constituent with label X, and push the
new constituent onto the stack.
? UNARY-X: pop the top constituent off the
stack, raise it to a new constituent with la-
bel X, and push the new constituent onto the
stack.
? FINISH: pop the root node off the stack and
ends parsing.
The deduction system for the process is shown
in Figure 1, where the item is formed as ?stack,
buffer front index, completion mark, score?, and
cs, cr , and cu represent the incremental score of
the SHIFT, REDUCE, and UNARY parsing steps,
respectively; these scores are calculated according
to the context features of the parser state item. n
is the number of words in the input.
2.2 Global Discriminative Training and
Beam-Search
For a given input sentence, the initial state has an
empty stack and a buffer that contains all the input
words. An agenda is used to keep the k best state
items at each step. At initialization, the agenda
contains only the initial state. At each step, every
state item in the agenda is popped and expanded
by applying a valid transition action, and the top
k from the newly constructed state items are put
back onto the agenda. The process repeats until
the agenda is empty, and the best completed state
item (recorded as candidate output) is taken for
435
Description Templates
unigrams s0tc, s0wc, s1tc, s1wc, s2tc
s2wc, s3tc, s3wc, q0wt, q1wt
q2wt, q3wt, s0lwc, s0rwc
s0uwc, s1lwc, s1rwc, s1uwc
bigrams s0ws1w, s0ws1c, s0cs1w, s0cs1c,
s0wq0w, s0wq0t, s0cq0w, s0cq0t,
q0wq1w, q0wq1t, q0tq1w, q0tq1t,
s1wq0w, s1wq0t, s1cq0w, s1cq0t
trigrams s0cs1cs2c, s0ws1cs2c, s0cs1wq0t
s0cs1cs2w, s0cs1cq0t, s0ws1cq0t
s0cs1wq0t, s0cs1cq0w
Table 1: A summary of baseline feature templates,
where si represents the ith item on the stack S and
qi denotes the ith item in the queue Q. w refers to
the head lexicon, t refers to the head POS, and c
refers to the constituent label.
the output.
The score of a state item is the total score of the
transition actions that have been applied to build
the item:
C(?) =
N?
i=1
?(ai) ? ~?
Here ?(ai) represents the feature vector for the ith
action ai in state item ?. It is computed by apply-
ing the feature templates in Table 1 to the context
of ?. N is the total number of actions in ?.
The model parameter ~? is trained with the aver-
aged perceptron algorithm, applied to state items
(sequence of actions) globally. We apply the early
update strategy (Collins and Roark, 2004), stop-
ping parsing for parameter updates when the gold-
standard state item falls off the agenda.
2.3 Baseline Features
Our baseline features are adopted from Zhang and
Clark (2009), and are shown in Table 1 Here si
represents the ith item on the top of the stack S
and qi denotes the ith item in the front end of the
queue Q. The symbol w denotes the lexical head
of an item; the symbol c denotes the constituent
label of an item; the symbol t is the POS of a lex-
ical head. These features are adapted from Zhang
and Clark (2009). We remove Chinese specific
features and make the baseline parser language-
independent.
3 Improved hypotheses comparison
Unlike dependency parsing, constituent parse
trees for the same sentence can have different
numbers of nodes, mainly due to the existence
of unary nodes. As a result, completed state
NP
NN
address
NNS
issues
VP
VB
address
NP
NNS
issues
Figure 2: Example parse trees of the same sen-
tence with different numbers of actions.
items for the same sentence can have different
numbers of unary actions. Take the phrase ?ad-
dress issues? for example, two possible parses
are shown in Figure 2 (a) and (b), respectively.
The first parse corresponds to the action sequence
[SHIFT, SHIFT, REDUCE-R-NP, FINISH], while
the second parse corresponds to the action se-
quence [SHIFT, SHIFT, UNARY-NP, REDUCE-L-
VP, FINISH], which consists of one more action
than the first case. In practice, variances between
state items can be much larger than the chosen ex-
ample. In the extreme case where a state item does
not contain any unary action, the number of ac-
tions is 2n, where n is the number of words in
the sentence. On the other hand, if the maximum
number of consequent unary actions is 2 (Sagae
and Lavie, 2005; Zhang and Clark, 2009), then the
maximum number of actions a state item can have
is 4n.
The significant variance in the number of ac-
tions N can have an impact on the linear sepa-
rability of state items, for which the feature vec-
tors are
?N
i=1 ? (ai). This turns out to have a sig-
nificant empirical influence on perceptron training
with early-update, where the training of the model
interacts with search (Daume III, 2006).
One way of improving the comparability of
state items is to reduce the differences in their
sizes, and we use a padding method to achieve
this. The idea is to extend the set of actions by
adding an IDLE action, so that completed state
items can be further expanded using the IDLE ac-
tion. The action does not change the state itself,
but simply adds to the number of actions in the
sequence. A feature vector is extracted for the
IDLE action according to the final state context,
in the same way as other actions. Using the IDLE
action, the transition sequence for the two parses
in Figure 2 can be [SHIFT, SHIFT, REDUCE-
NP, FINISH, IDLE] and [SHIFT, SHIFT, UNARY-
NP, REDUCE-L-VP, FINISH], respectively. Their
436
Axioms [?, 0, false, 0, 0]
Goal [S, n, true, m : 2n ? m ? 4n, C]
Inference Rules:
[S, i, false, k,c]
SHIFT [S|w, i + 1, false, k + 1, c + cs]
[S|s1s0, i, false, k, c]
REDUCE-L/R-X [S|X, i, false, k + 1, c + cr]
[S|s0, i, false, k, c]
UNARY-X [S|X, i, false, k + 1, c + cu]
[S, n, false, k, c]
FINISH [S, n, true, k + 1, c + cf ]
[S,n, true, k, c]
IDLE [S, n, true, k + 1, c + ci]
Figure 3: Deductive system of the extended tran-
sition system.
corresponding feature vectors have about the same
sizes, and are more linearly separable. Note that
there can be more than one action that are padded
to a sequence of actions, and the number of IDLE
actions depends on the size difference between the
current action sequence and the largest action se-
quence without IDLE actions.
Given this extension, the deduction system is
shown in Figure 3. We add the number of actions
k to an item. The initial item (Axioms) has k = 0,
while the goal item has 2n ? k ? 4n. Given this
process, beam-search decoding can be made sim-
pler than that of Zhang and Clark (2009). While
they used a candidate output to record the best
completed state item, and finish decoding when
the agenda contains no more items, we can sim-
ply finish decoding when all items in the agenda
are completed, and output the best state item in
the agenda. With this new transition process, we
experimented with several extended features,and
found that the templates in Table 2 are useful to
improve the accuracies further. Here sill denotes
the left child of si?s left child. Other notations can
be explained in a similar way.
4 Semi-supervised Parsing with Large
Data
This section discusses how to extract informa-
tion from unlabeled data or auto-parsed data to
further improve shift-reduce parsing accuracies.
We consider three types of information, including
s0llwc, s0lrwc, s0luwc
s0rlwc, s0rrwc, s0ruwc
s0ulwc, s0urwc, s0uuwc
s1llwc, s1lrwc, s1luwc
s1rlwc, s1rrwc, s1ruwc
Table 2: New features for the extended parser.
paradigmatic relations, dependency relations, and
structural relations. These relations are captured
by word clustering, lexical dependencies, and a
dependency language model, respectively. Based
on the information, we propose a set of novel fea-
tures specifically designed for shift-reduce con-
stituent parsing.
4.1 Paradigmatic Relations: Word
Clustering
Word clusters are regarded as lexical intermedi-
aries for dependency parsing (Koo et al, 2008)
and POS tagging (Sun and Uszkoreit, 2012). We
employ the Brown clustering algorithm (Liang,
2005) on unannotated data (word segmentation is
performed if necessary). In the initial state of clus-
tering, each word in the input corpus is regarded
as a cluster, then the algorithm repeatedly merges
pairs of clusters that cause the least decrease in
the likelihood of the input corpus. The clustering
results are a binary tree with words appearing as
leaves. Each cluster is represented as a bit-string
from the root to the tree node that represents the
cluster. We define a function CLU(w) to return the
cluster ID (a bit string) of an input word w.
4.2 Dependency Relations: Lexical
Dependencies
Lexical dependencies represent linguistic relations
between words: whether a word modifies another
word. The idea of exploiting lexical dependency
information from auto-parsed data has been ex-
plored before for dependency parsing (Chen et al,
2009) and constituent parsing (Zhu et al, 2012).
To extract lexical dependencies, we first run the
baseline parser on unlabeled data. To simplify
the extraction process, we can convert auto-parsed
constituency trees into dependency trees by using
Penn2Malt. 2 From the dependency trees, we ex-
tract bigram lexical dependencies ?w1, w2, L/R?
where the symbol L (R) means that w1 (w2) is the
head of w2 (w1). We also extract trigram lexical
2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html
437
dependencies ?w1, w2, w3, L/R?, where L means
that w1 is the head of w2 and w3, meanwhile w2
and w3 are required to be siblings.
Following the strategy of Chen et al (2009),
we assign categories to bigram and trigram items
separately according to their frequency counts.
Specifically, top-10% most frequent items are as-
signed to the category of High Frequency (HF);
otherwise if an item is among top 20%, we assign
it to the category of Middle Frequency (MF); oth-
erwise the category of Low Frequency (LF). Here-
after, we refer to the bigram and trigram lexical
dependency lists as BLD and TLD, respectively.
4.3 Structural Relations: Dependency
Language Model
The dependency language model is proposed by
Shen et al (2008) and is used as additional in-
formation for graph-based dependency parsing in
Chen et al (2012). Formally, given a depen-
dency tree y of an input sentence x, we can
denote by H(y) the set of words that have at
least one dependent. For each xh ? H(y), we
have a corresponding dependency structure Dh =
(xLk, . . . xL1, xh, xR1, . . . , xRm). The probability
P (Dh) is defined to be
P (Dh) = PL(Dh) ? PR(Dh)
where PL(Dh) can be in turn defined as:
PL(Dh) ? P (xL1|xh)
?P (xL2|xL1, xh)
? . . .
?P (xLk|xLk?1, . . . , xLk?N+1, xh)
PR(Dh) can be defined in a similar way.
We build dependency language models on auto-
parsed data. Again, we convert constituency trees
into dependency trees for the purpose of simplic-
ity. From the dependency trees, we build a bigram
and a trigram language model, which are denoted
by BLM and TLM, respectively. The following
are the templates of the records of the dependency
language models.
(1) ?xLi, xh, P (xLi|xh)?
(2) ?xRi, xh, P (xRi|xh)?
(3) ?xLi, xLi?1, xh, P (xLi|xLi?1, xh)?
(4) ?xRi, xRi?1, xh, P (xRi|xRi?1, xh)?
Here the templates (1) and (2) belong to BLM
and the templates (3) and (4) belong to TLM. To
Stat Train Dev Test Unlabeled
EN # sent 39.8k 1.7k 2.4k 3,139.1k# word 950.0k 40.1k 56.7k 76,041.4k
CH # sent 18.1k 350 348 11,810.7k# word 493.8k 8.0k 6.8k 269,057.2k
Table 4: Statistics on sentence and word numbers
of the experimental data.
use the dependency language models, we employ
a map function ?(r) to assign a category to each
record r according to its probability, as in Chen et
al. (2012). The following is the map function.
?(r) =
?
??
??
HP if P (r) ? top?10%
MP else if P (r) ? top?30%
LP otherwise
4.4 Semi-supervised Features
We design a set of features based on the infor-
mation extracted from auto-parsed data or unan-
notated data. The features are summarized in Ta-
ble 3. Here CLU returns a cluster ID for a word.
The functions BLDl/r(?), TLDl/r(?), BLMl/r(?),
and TLMl/r(?) check whether a given word com-
bination can be found in the corresponding lists.
For example, BLDl(s1w, s0w) returns a category
tag (HF, MF, or LF) if ?s1w, s0w,L? exits in the
list BLD, else it returns NONE.
5 Experiments
5.1 Set-up
Labeled English data employed in this paper were
derived from the Wall Street Journal (WSJ) corpus
of the Penn Treebank (Marcus et al, 1993). We
used sections 2-21 as labeled training data, section
24 for system development, and section 23 for fi-
nal performance evaluation. For labeled Chinese
data, we used the version 5.1 of the Penn Chinese
Treebank (CTB) (Xue et al, 2005). Articles 001-
270 and 440-1151 were used for training, articles
301-325 were used as development data, and arti-
cles 271-300 were used for evaluation.
For both English and Chinese data, we used ten-
fold jackknifing (Collins, 2000) to automatically
assign POS tags to the training data. We found that
this simple technique could achieve an improve-
ment of 0.4% on English and an improvement of
2.0% on Chinese. For English POS tagging, we
adopted SVMTool, 3 and for Chinese POS tagging
3http://www.lsi.upc.edu/?nlp/SVMTool/
438
Word Cluster Features
CLU(s1w) CLU(s0w) CLU(q0w)
CLU(s1w)s1t CLU(s0w)s0t CLU(q0w)q0w
Lexical Dependency Features
BLDl(s1w, s0w) BLDl(s1w, s0w)?s1t?s0t BLDr(s1w, s0w)
BLDr(s1w, s0w)?s1t?s0t BLDl(s1w, q0w)?s1t?q0t BLDl(s1w, q0w)
BLDr(s1w, q0w) BLDr(s1w, q0w)?s1t?q0t BLDl(s0w, q0w)
BLDl(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)
TLDl(s1w, s1rdw, s0w) TLDl(s1w, s1rdw, s0w)?s1t?s0t TLDr(s1w, s0ldw, s0w)
TLDr(s1w, s0ldw, s0w)?s1t?s0t TLDl(s0w, s0rdw, q0w)?s0t?q0t TLDl(s0w, s0rdw, q0w)
TLDr(s0w,NONE, q0w) TLDr(s0w,NONE, q0w)?s0t?q0t
Dependency Language Model Features
BLMl(s1w, s0w) BLMl(s1w, s0w)?s1t?s0t BLMr(s1w, s0w)
BLMr(s1w, s0w)?s1t?s0t BLMl(s0w, q0w) BLMl(s0w, q0w)?s0t?q0t
BLMr(s0w, q0w)?s0t?q0t BLMr(s0w, q0w) TLMl(s1w, s1rdw, s0w)
TLMl(s1w, s1rdw, s0w)?s1t?s0t TLMr(s1w, s0ldw, s0w) TLMr(s1w, s0ldw, s0w)?s1t?s0t
Table 3: Semi-supervised features designed on the base of word clusters, lexical dependencies, and
dependency language models. Here the symbol si denotes a stack item, qi denotes a queue item, w
represents a word, and t represents a POS tag.
Lan. System LR LP F1
EN
G Baseline 88.4 88.7 88.6
+padding 88.8 89.5 89.1
+features 89.0 89.7 89.3
CH
N Baseline 85.6 86.3 86.0
+padding 85.5 87.2 86.4
+features 85.5 87.6 86.5
Table 5: Experimental results on the English and
Chinese development sets with the padding tech-
nique and new supervised features added incre-
mentally.
we employed the Stanford POS tagger. 4
We took the WSJ articles from the TIPSTER
corpus (LDC93T3A) as unlabeled English data. In
addition, we removed from the unlabeled English
data the sentences that appear in the WSJ corpus
of the Penn Treebank. For unlabeled Chinese data,
we used Chinese Gigaword (LDC2003T09), on
which we conducted Chinese word segmentation
by using a CRF-based segmenter. Table 4 summa-
rizes data statistics on sentence and word numbers
of the data sets listed above.
We used EVALB to evaluate parser perfor-
mances, including labeled precision (LP), labeled
recall (LR), and bracketing F1. 5 For significance
tests, we employed the randomized permutation-
based tool provided by Daniel Bikel. 6
In both training and decoding, we set the beam
size to 16, which achieves a good tradeoff be-
tween efficiency and accuracy. The optimal iter-
ation number of perceptron learning is determined
4http://nlp.stanford.edu/software/tagger.shtml
5http://nlp.cs.nyu.edu/evalb
6http://www.cis.upenn.edu/?dbikel/software.html#comparator
Lan. Features LR LP F1
EN
G +word cluster 89.3 90.0 89.7
+lexical dependencies 89.7 90.3 90.0
+dependency LM 90.0 90.6 90.3
CH
N +word cluster 85.7 87.5 86.6
+lexical dependencies 87.2 88.6 87.9
+dependency LM 87.2 88.7 88.0
Table 6: Experimental results on the English and
Chinese development sets with different types of
semi-supervised features added incrementally to
the extended parser.
on the development sets. For word clustering, we
set the cluster number to 50 for both the English
and Chinese experiments.
5.2 Results on Development Sets
Table 5 reports the results of the extended parser
(baseline + padding + supervised features) on the
English and Chinese development sets. We inte-
grated the padding method into the baseline parser,
based on which we further incorporated the super-
vised features in Table 2. From the results we find
that the padding method improves the parser accu-
racies by 0.5% and 0.4% on English and Chinese,
respectively. Incorporating the supervised features
in Table 2 gives further improvements of 0.2% on
English and 0.1% on Chinese.
Based on the extended parser, we experimented
different types of semi-supervised features by
adding the features incrementally. The results are
shown in Table 6. By comparing the results in Ta-
ble 5 and the results in Table 6 we can see that the
semi-supervised features achieve an overall im-
provement of 1.0% on the English data and an im-
439
Type Parser LR LP F1
SI
Ratnaparkhi (1997) 86.3 87.5 86.9
Collins (1999) 88.1 88.3 88.2
Charniak (2000) 89.5 89.9 89.5
Sagae & Lavie (2005)? 86.1 86.0 86.0
Sagae & Lavie (2006)? 87.8 88.1 87.9
Baseline 90.0 89.9 89.9
Petrov & Klein (2007) 90.1 90.2 90.1
Baseline+Padding 90.2 90.7 90.4
Carreras et al (2008) 90.7 91.4 91.1
RE Charniak & Johnson (2005) 91.2 91.8 91.5Huang (2008) 92.2 91.2 91.7
SE
Zhu et al (2012)? 90.4 90.5 90.4
Baseline+Padding+Semi 91.1 91.5 91.3
Huang & Harper (2009) 91.1 91.6 91.3
Huang et al (2010)? 91.4 91.8 91.6
McClosky et al (2006) 92.1 92.5 92.3
Table 7: Comparison of our parsers and related
work on the English test set. ? Shift-reduce
parsers. ? The results of self-training with a sin-
gle latent annotation grammar.
Type Parser LR LP F1
SI
Charniak (2000)? 79.6 82.1 80.8
Bikel (2004)? 79.3 82.0 80.6
Baseline 82.1 83.1 82.6
Baseline+Padding 82.1 84.3 83.2
Petrov & Klein (2007) 81.9 84.8 83.3
RE Charniak & Johnson (2005)? 80.8 83.8 82.3
SE Zhu et al (2012) 80.6 81.9 81.2Baseline+Padding+Semi 84.4 86.8 85.6
Table 8: Comparison of our parsers and related
work on the test set of CTB5.1.? Huang (2009)
adapted the parsers to Chinese parsing on CTB5.1.
? We run the parser on CTB5.1 to get the results.
provement of 1.5% on the Chinese data.
5.3 Final Results
Here we report the final results on the English and
Chinese test sets. We compared the final results
with a large body of related work. We grouped the
parsers into three categories: single parsers (SI),
discriminative reranking parsers (RE), and semi-
supervised parsers (SE). Table 7 shows the com-
parative results on the English test set and Table 8
reports the comparison on the Chinese test set.
From the results we can see that our extended
parser (baseline + padding + supervised features)
outperforms the Berkeley parser by 0.3% on En-
glish, and is comparable with the Berkeley parser
on Chinese (?0.1% less). Here +padding means
the padding technique and the features in Table 2.
After integrating semi-supervised features, the
parsing accuracy on English is improved to 91.3%.
We note that the performance is on the same level
Parser #Sent/Second
Ratnaparkhi (1997) Unk
Collins (1999) 3.5
Charniak (2000) 5.7
Sagae & Lavie (2005)? 3.7?
Sagae & Lavie (2006)? 2.2?
Petrov & Klein (2007) 6.2
Carreras et al (2008) Unk
This Paper
Baseline 100.7
Baseline+Padding 89.5
Baseline+Padding+Semi 46.8
Table 9: Comparison of running times on the En-
glish test set, where the time for loading models
is excluded. ? The results of SVM-based shift-
reduce parsing with greedy search. ? The results of
MaxEnt-based shift-reduce parser with best-first
search. ? Times reported by authors running on
different hardware.
as the performance of self-trained parsers, except
for McClosky et al (2006), which is based on the
combination of reranking and self-training. On
Chinese, the final parsing accuracy is 85.6%. To
our knowledge, this is by far the best reported per-
formance on this data set.
The padding technique, supervised features,
and semi-supervised features achieve an overall
improvement of 1.4% over the baseline on En-
glish, which is significant on the level of p <
10?5. The overall improvement on Chinese is
3.0%, which is also significant on the level of
p < 10?5.
5.4 Comparison of Running Time
We also compared the running times of our parsers
with the related single parsers. We ran timing tests
on an Intel 2.3GHz processor with 8GB mem-
ory. The comparison is shown in Table 9. From
the table, we can see that incorporating semi-
supervised features decreases parsing speed, but
the semi-supervised parser still has the advantage
of efficiency over other parsers. Specifically, the
semi-supervised parser is 7 times faster than the
Berkeley parser. Note that Sagae & Lavie (2005)
and Sagae & Lavie (2006) are also shift-reduce
parsers, and their running times were evaluated on
different hardwares. In practice, the running times
of the shift-reduce parsers should be much shorter
than the reported times in the table.
5.5 Error Analysis
We conducted error analysis for the three sys-
tems: the baseline parser, the extended parser with
440
 86
 88
 90
 92
 94
1 2 3 4 5 6 7 8
F 
Sc
or
e
Span Length
Baseline
Extended
Semi-supervised
Figure 5: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parsers on spans of different lengths.
the padding technique, and the semi-supervised
parser, focusing on the English test set. The analy-
sis was performed in four dimensions: parsing ac-
curacies on different phrase types, on constituents
of different span lengths, on different sentence
lengths, and on sentences with different numbers
of unknown words.
5.5.1 Different Phrase Types
Table 10 shows the parsing accuracies of the base-
line, extended parser, and semi-supervised parser
on different phrase types. Here we only consider
the nine most frequent phrase types in the English
test set. In the table, the phrase types are ordered
from left to right in the descending order of their
frequencies. We also show the improvements of
the semi-supervised parser over the baseline parser
(the last row in the table). As the results show, the
extended parser achieves improvements on most
of the phrase types with two exceptions: Preposi-
tion Prase (PP) and Quantifier Phrase (QP). Semi-
supervised features further improve parsing accu-
racies over the extended parser (QP is an excep-
tion). From the last row, we can see that improve-
ments of the semi-supervised parser over the base-
line on VP, S, SBAR, ADVP, and ADJP are above
the average improvement (1.4%).
5.5.2 Different Span Lengths
Figure 5 shows a comparison of the three parsers
on spans of different lengths. Here we consider
span lengths up to 8. As the results show, both
the padding extension and semi-supervised fea-
tures are more helpful on relatively large spans:
the performance gaps between the three parsers
are enlarged with increasing span lengths.
 82
 84
 86
 88
 90
 92
 94
10 20 30 40 50 60 70
F 
Sc
or
e
Sentence Length
Baseline
Extended
Semi-supervised
Figure 6: Comparison of parsing accuracies of
the baseline, extended parser, and semi-supervised
parser on sentences of different lengths.
5.5.3 Different Sentence Lengths
Figure 6 shows a comparison of parsing accura-
cies of the three parsers on sentences of different
lengths. Each number on the horizontal axis repre-
sents the sentences whose lengths are between the
number and its previous number. For example, the
number 30 refers to the sentences whose lengths
are between 20 and 30. From the results we can
see that semi-supervised features improve parsing
accuracy on both short and long sentences. The
points at 70 are exceptions. In fact, sentences with
lengths between 60 and 70 have only 8 instances,
and the statistics on such a small number of sen-
tences are not reliable.
5.5.4 Different Numbers of Unknown Words
Figure 4 shows a comparison of parsing accura-
cies of the baseline, extended parser, and semi-
supervised parser on sentences with different num-
bers of unknown words. As the results show,
the padding method is not very helpful on sen-
tences with large numbers of unknown words,
while semi-supervised features help significantly
on this aspect. This conforms to the intuition that
semi-supervised methods reduce data sparseness
and improve the performance on unknown words.
6 Conclusion
In this paper, we addressed the problem of dif-
ferent action-sequence lengths for shift-reduce
phrase-structure parsing, and designed a set of
novel non-local features to further improve pars-
ing. The resulting supervised parser outperforms
the Berkeley parser, a state-of-the-art chart parser,
in both accuracies and speeds. In addition, we in-
corporated a set of semi-supervised features. The
441
System NP VP S PP SBAR ADVP ADJP WHNP QP
Baseline 91.9 90.1 89.8 88.1 85.7 84.6 72.1 94.8 89.3
Extended 92.1 90.7 90.2 87.9 86.6 84.5 73.6 95.5 88.6
Semi-supervised 93.2 92.0 91.5 89.3 88.2 86.8 75.1 95.7 89.1
Improvements +1.3 +1.9 +1.7 +1.2 +2.5 +2.2 +3.0 +0.9 -0.2
Table 10: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parsers
on different phrase types.
0 1 2 3 4 5 6 7
70
80
90
100
91
.9
8
89
.7
3
88
.8
7
87
.9
6
85
.9
5
83
.7
81
.4
2
82
.7
4
92
.1
7
90
.5
3
89
.5
1
87
.9
9
88
.6
6
87
.3
3
83
.8
9
80
.4
9
92
.8
8
91
.2
6
90
.4
3
89
.8
8
90
.3
5
86
.3
9 90
.6
8
90
.2
4
F-
sc
o
re
(%
)
Baseline Extended Semi-supervised
Figure 4: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parser
on sentences of different unknown words.
final parser reaches an accuracy of 91.3% on En-
glish and 85.6% on Chinese, by far the best re-
ported accuracies on the CTB data.
Acknowledgements
We thank the anonymous reviewers for their valu-
able comments. Yue Zhang and Muhua Zhu
were supported partially by SRG-ISTD-2012-038
from Singapore University of Technology and De-
sign. Muhua Zhu and Jingbo Zhu were funded
in part by the National Science Foundation of
China (61073140; 61272376), Specialized Re-
search Fund for the Doctoral Program of Higher
Education (20100042110031), and the Fundamen-
tal Research Funds for the Central Universities
(N100204002). Wenliang Chen was funded par-
tially by the National Science Foundation of China
(61203314).
References
Daniel M. Bikel. 2004. On the parameter space
of generative lexicalized statistical parsing models.
Ph.D. thesis, University of Pennsylvania.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of EMNLP, pages 12?14, Jeju Island, Ko-
rea.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of CoNLL, pages 9?16, Manchester, England.
Eugune Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180.
Eugune Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL, pages
132?139, Seattle, Washington, USA.
Wenliang Chen, Junichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving depen-
dency parsing with subtrees from auto-parsed data.
In Proceedings of EMNLP, pages 570?579, Singa-
pore.
Wenliang Chen, Min Zhang, and Haizhou Li. 2012.
Utilizing dependency language models for graph-
based dependency. In Proceedings of ACL, pages
213?222, Jeju, Republic of Korea.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL, Stroudsburg, PA, USA.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
ACL, Madrid, Spain.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael Collins. 2000. Discriminative reranking
for natural language processing. In Proceedings of
ICML, pages 175?182, Stanford, CA, USA.
Hal Daume III. 2006. Practical Structured Learn-
ing for Natural Language Processing. Ph.D. thesis,
USC.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
442
across languages. In Proceedings of EMNLP, pages
832?841, Singapore.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL, pages 1077?1086, Uppsala,
Sweden.
Zhongqiang Huang, Mary Harper, and Slav Petrov.
2010. Self-training with products of latent variable
grammars. In Proceedings of EMNLP, pages 12?22,
Massachusetts, USA.
Liang Huang. 2008. Forest reranking: discriminative
parsing with non-local features. In Proceedings of
ACL, pages 586?594, Ohio, USA.
Liang-Ya Huang. 2009. Improve Chinese parsing with
Max-Ent reranking parser. In Master Project Re-
port, Brown University.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of ICML, pages 282?289, Massachusetts, USA,
June.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts Insti-
tute of Technology.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewiz. 1993. Building a large anno-
tated corpus of English. Computational Linguistics,
19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT/NAACL, Main Conference,
pages 152?159, New York City, USA, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98, Ann Arbor, Michigan, June.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: a data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, pages
2216?2219.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL, pages 404?411, Rochester, New York,
April.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proceedings of EMNLP, Rhode Island, USA.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of IWPT, pages 125?132, Vancouver, Canada.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of HLT/NAACL,
Companion Volume: Short Papers, pages 129?132,
New York, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL, pages 577?585, Ohio, USA.
Weiwei Sun and Hans Uszkoreit. 2012. Capturing
paradigmatic and syntagmatic lexical relations: to-
wards accurate Chinese part-of-speech tagging. In
Proceedings of ACL, Jeju, Republic of Korea.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of IWPT, pages 195?206,
Nancy, France.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL/HLT, pages 888?896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the Chinese Treebank using a global dis-
criminative model. In Proceedings of IWPT, Paris,
France, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193, Portland, Ore-
gon, USA.
Muhua Zhu, Jingbo Zhu, and Huizhen Wang. 2012.
Exploiting lexical dependencies from large-scale
data for better shift-reduce constituency parsing. In
Proceedings of COLING, pages 3171?3186, Mum-
bai, India.
443
High OOV-Recall Chinese Word Segmenter
Xiaoming Xu, Muhua Zhu, Xiaoxu Fei, and Jingbo Zhu
School of
Information Science and Engineering
Northeastern University
{xuxm, zhumh, feixx}@ics.neu.edu.cn
zhujingbo@mail.neu.edu.cn
Abstract
For the competition of Chinese word seg-
mentation held in the first CIPS-SIGHNA
joint conference. We applied a subword-
based word segmenter using CRFs and ex-
tended the segmenter with OOV words
recognized by Accessor Variety. More-
over, we proposed several post-processing
rules to improve the performance. Our
system achieved promising OOV recall
among all the participants.
1 Introduction
Chinese word segmentation is deemed to be a pre-
requisite for Chinese language processing. The
competition in the first CIPS-SIGHAN joint con-
ference put the task of Chinese word segmenta-
tion in a more challengeable setting, where train-
ing and test data are obtained from different do-
mains. This setting is widely known as domain
adaptation.
For domain adaptation, either a large-scale un-
labeled target domain data or a small size of la-
beled target domain data is required to adapt a
system built on source domain data to the tar-
get domain. In this word segmentation competi-
tion, unfortunately, only a small size of unlabeled
target domain data is available. Thus we focus
on handling out-of-vocabulary (OOV) words. For
this purpose, our system is based on a combina-
tion of subword-based tagging method (Zhang et
al., 2006) and accessor variety-based new word
recognition method (Feng et al, 2004). In more
detail, we adopted and extended subword-based
method. Subword list is augmented with new-
word list recognized by accessor variety method.
Feature Template Description
a) cn(?2,?1, 0, 1, 2) unigram of characters
b) cncn+1(?2,?1, 0, 1) bigram of characters
c) cn?1cncn+1(?1, 0, 1) trigram of characters
d) Pu(C0) whether punctuation
e) T (C?1)T (C0)T (C+1) type of characters
Table 1: Basic Features for CRF-based Segmenter
We participated in the close track of the word
segmentation competition, on all the four test
datasets, in two of which our system is ranked at
the 1st position with respect to the metric of OOV
recall.
2 System Description
2.1 Subword-based Tagging with CRFs
The backbone of our system is a character-based
segmenter with the application of Conditional
Random Fields (CRFs) (Zhao and Kit, 2008). In
detail, we apply a six-tag tagging scheme, as in
(Zhao et al, 2006). That is , each Chinese char-
acter can be assigned to one of the tags in {B,
B2, B3, M , E, S }. Refer to (Zhao et al, 2006)
for detailed meaning of the tags. Table 1 shows
basic feature templates used in our system, where
feature templates a, b, d, e are also used in (Zhu et
al., 2006) for SVM-based word segmentation.
In order to extend basic CRF-based segmenter,
we first collect 2k most frequent words from train-
ing data. Hereafter, the list of such words is
referred to as subword list. Moreover, single-
character words 1, if they are not contained in
the subword list, are also added. Such proce-
1By single-character word, we refer to words that consist
solely of a Chinese character.
Feature Template Description
f) in(str, subword-list) is str in subword list
g) in(str, confident-word-list) is str in confident-word
list
Table 2: Subword Features for CRF-based Seg-
menter
dure for constructing a subword list is similar to
the one used in (Zhang et al, 2006). To en-
hance the effect of subwords, we go one step
further to build a list, named confident-word list
here and below, which contains words that are
not a portion of other words and are never seg-
mented in the training data. In the competition,
400 most frequent words in the confident-word list
are used. With subword list and confident-word
list, both training and test data are segmented
with forward maximum match method by using
the union of subword list and confident-word list.
Each segmentation unit (single-character or multi-
character unit) in the segmentation results are re-
garded as ?pseudo character? and thus can be rep-
resented with the basic features in Table 1 and
two additional features as shown in Table 2. See
the details of subword-based Chinese word seg-
mentation in (Zhang et al, 2006)
2.2 OOV Recognition with Accessor Variety
Accessor variety (AV) (Feng et al, 2004) is a sim-
ple and effective unsupervised method for extrac-
tion of new Chinese words. Given a unsegmented
text, each substring (candidate word) in the text
can be assigned a value according to the follow-
ing equation:
AV (s) = min{Lav(s), Rav(s)} (1)
where the left and right AV values, Lav(s) and
Rav(s) are defined to be the number of distinct
character types appearing on the left and right,
respectively. Candidate words are sorted in the
descending order of AV values and most highly
ranked ones can be chosen as new words. In
practical applications, heuristic filtering rules are
generally needed (Feng et al, 2004). We re-
implemented the AV method and filtering rules,
as in (Feng et al, 2004). Moreover, we filter out
candidate words that have AV values less than 3.
Unfortunately, candidate word list generated this
way still contains many noisy words (substrings
that are not words). One possible reason is that
unlabeled data (test data) used in the competition
is extremely small in size. In order to refine the
results derived from the AV method, we make use
of the training data to filter the results from two
different perspectives.
? Segment test data with the CRF-based seg-
menter described above. Then we collect
(candidate) words that are in the CRF-based
segmentation results, but not appear in the
training data. Such words are called CRF-
OOV words hereafter. We retain the intersec-
tion of CRF-OOV words and AV-based re-
sults as the set of candidate words to be pro-
cessed by the following step.
? Any candidate word in the intersection of
CRF-based and AV-based results will be fil-
tered out if they satisfy one of the following
conditions: 1) the candidate word is a part of
some word in the training data; 2) the candi-
date word is formed by connection of consec-
utive words in the training data; 3) the candi-
date word contains position words, such as
? (up), ? (down),? (left),? (right), etc.
Moreover, we take all English words in test data
as OOV words. A simple heuristic rule is defined
for the purpose of English word recognition: an
English word is a consecutive sequence of English
characters and punctuations between two English
characters (including these two characters).
We finally add all the OOV words into subword
list and confident-word list.
3 Post-Processing Rules
In the results of subword-based word segmenta-
tion with CRFs, we found some errors could be
corrected with heuristic rules. For this purpose,
we propose following post-processing rules, for
handling OOV and in-vocabulary (IV) words, re-
spectively.
3.1 OOV Rules
3.1.1 Annotation-Standard Independent
Rules
We assume the phenomena discussed in the fol-
lowing are general across all kinds of annotation
standards. Thus corresponding rules can be ap-
plied without considering annotation standards of
training data.
? A punctuation tends to be a single-character
word. If a punctation?s previous character
and next character are both Chinese charac-
ters, i.e. not punctuation, digit, or English
character, we always regard the punctuation
as a word.
? Consecutive and identical punctuations tend
to be joined together as a word. For exam-
ple, ??? represents a Chinese hyphen which
consists of three ?-?, and ?!!!? is used to
show emphasizing. Inspired by this obser-
vations, we would like to unite consecutive
and identical punctuations as a single word.
? When the character ??? appears in the train-
ing data, it is generally used as a connec-
tions symbol in a foreign person name, such
as ????? (Saint John)?. Taking this ob-
servation into consideration, we always unite
the character ??? and its previous and next
segment units into a single word. A similar
rule is designed to unite consecutive digits on
the sides of the symbol ?.?, ex. ?1.11?.
? We notice that four consecutive characters
which are in the pattern of AABB generally
form a single word in Chinese, for example
????? (dull)?. Taking this observation
into account, we always unite consecutive
characters in the AABB into a single word.
3.1.2 Templates with Generalized Digits
Words containing digits generally belong to a
open class, for example, the word ?2012? (AD
2012?? means a date. Thus CRF-based seg-
menter has difficulties in recognizing such words
since they are frequently OOV words. To attack
this challenge, we first generalize digits in the
training data. In detail, we replaced consecutive
digits with ?*?. For example, the word ?2012??
will be transformed into ?*??. Second, we col-
lect word templates which consist of three con-
secutive words on condition that at least one of
the words in a template contains the character ?*?
and that the template appears in the training data
more than 4 times. For example, we can get a
template like ?*?(month) *?(day)?(publish)?.
With such templates, we are able to correct errors,
say ?10? 17??? into ?10? 17???.
3.2 IV Rules
We notice that long words have less ambiguity
than short words in the sense of being words.
For example, characters in ????? ?full
of talents)? always form a word in the training
data, whereas ???? have two plausible split-
ting forms, as ??? (talent)? or ?? (people) ?
(only)?. In our system, we collect words that have
at least four characters and filter out words which
belong to one of following cases: 1) the word is
a part of other words; 2) the word consists solely
of punctation and/or digit. For example, ???
?? (materialism)? and ????? (120)? are
discarded, since the former is a substring of the
word ?????? (materialist)? and the latter is
a word of digits. Finally we get a list containing
about 6k words. If a character sequence in the test
data is a member in the list, it is retained as a word
in the final segmentation results.
Another group of IV rules concern character
sequences that have unique splitting in the train-
ing data. For example, ???? (women)? is al-
ways split as ??? (woman) ? (s)?. Hereafter,
we refer to such character sequences as unique-
split-sequence (USS). In our system, we are con-
cerned with UUSs which are composed of less
than 5 words. In order to apply UUSs for post-
processing, we first collect word sequence of vari-
able length (word number) from training data. In
detail, we collect word sequences of two words,
three words, and four words. Second, word se-
quences that have more than one splitting cases
in the training data are filtered out. Third, spaces
between words are removed to form USSs. For
example, the words ??? (woman) ? (s)? will
form the USS ???? ?. Finally, we search the
test data for each USS. If the searching succeeds,
the USS will be replaced with the corresponding
word sequence.
4 Evaluation Results
We evaluated our Chinese word segmenter in the
close track, in four domain: literature (Lit), com-
Domain Basic +OOV +OOV+IV
ROV RIV F ROV RIV F ROV RIV F
Lit .643 .946 .927 .652 .947 .929 .648 .952 .934
Com .839 .961 .938 .850 .961 .941 .852 .965 .947
Med .725 .938 .912 .754 .939 .917 .756 .944 .923
Fin .761 .956 .932 .854 .958 .950 .871 .961 .955
Table 3: Effectiveness of post-processing rules
puter (Com), medicine (Med) and finance (Fin).
The results are depicted in Table 4, where R,
P and F refer to Recall, Precision, F measure
respectively, and ROOV and RIV refer to recall
of OOV and IV words respectively. Since OOV
words are the obstacle for practical Chinese word
segmenters to achieve high accuracy, we have spe-
cial interest in the metric of OOV recall. We
found that our system achieved high OOV recall
2
. Actually, OOV recall of our system in the do-
mains of computer and finance are both ranked at
the 1st position among all the participants. Com-
pared with the systems ranked second in these
two domains, our system achieved OOV recall
.853 vs. .827 and .871 vs. .857 respectively.
We also examined the effectiveness of post-
processing rules, as shown in Table 3, where
Basic represents the performance achieved be-
fore post-processing, +OOV represents the results
achieved after applying OOV post-processing
rules, and +OOV+IV denotes the results achieved
after using all the post-processing rules, including
both OOV and IV rules. As the table shows, de-
signed post-processing rules can improve both IV
and OOV recall significantly.
Domain R P F ROOV RIV
Lit .931 .936 .934 .648 .952
Com .948 .945 .947 .853 .965
Med .924 .922 .923 .756 .944
Fin .953 .956 .955 .871 .961
Table 4: Performance of our system in the compe-
tition
2For the test data from the domain of literature, we actu-
ally use combination of our system and forward maximum
match, so we will omit the results on this test dataset in our
discussion.
5 Conclusions and Future Work
We proposed an approach to refine new words rec-
ognized with the accessor variety method, and in-
corporated such words into a subword-based word
segmenter. We found that such method could
achieve high OOV recall. Moreover, we designed
effective post-processing rules to further enhance
the performance of our systems. Our system fi-
nally achieved satisfactory results in the competi-
tion.
Acknowledgments
This work was supported in part by the National
Science Foundation of China (60873091).
References
Feng, Haodi, Kang Chen, Xiaotie Deng, and Weimin
zhang. 2004. Accessor Variety Criteriafor Chinese
Word Extraction. Computational Linguistics 2004,
30(1), pages 75-93.
Zhang, Ruiqiang, Genichiro Kikui, and Eiichiro
Sumita. 2006. Subword-based Tagging by Condi-
tional Random Fileds for Chinese Word Segmenta-
tion. In Proceedings of HLT-NAACL 2006, pages
193-196.
Zhao, Hai, Chang-Ning Huang, and Mu Li. 2006.
Improved Chinese Word Segmentation System with
Conditional Random Field. In Proceedings of
SIGHAN-5 2006, pages 162-165.
Zhao, Hai and Chunyu Kit. 2008. Unsupervised Seg-
mentation Helps Supervised Learning of Character
Tagging for Word Segmentation and Named Entity
Recognition. In Proceedings of SIGHAN-6 2008,
pages 106-111.
Zhu, Muhua, Yiling Wang, Zhenxing Wang, Huizhen
Wang, and Jingbo Zhu. 2006. Designing Spe-
cial Post-Processing Rules for SVM-based Chinese
Word Segmentation. In Proceedigns of SIGHAN-5
2006, pages 217-220.
