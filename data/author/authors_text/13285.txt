Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 144?152,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Comparative Study of Bayesian Models for Unsupervised Sentiment
Detection
Chenghua Lin
School of Engineering,
Computing and Mathematics
University of Exeter
Exeter, EX4 4QF, UK.
cl322@exeter.ac.uk
Yulan He
Knowledge Media Institute
The Open University
Milton Keynes
MK7 6AA, UK
Y.He@open.ac.uk
Richard Everson
School of Engineering,
Computing and Mathematics
University of Exeter
Exeter, EX4 4QF, UK.
R.E.Everson@exeter.ac.uk
Abstract
This paper presents a comparative study
of three closely related Bayesian mod-
els for unsupervised document level senti-
ment classification, namely, the latent sen-
timent model (LSM), the joint sentiment-
topic (JST) model, and the Reverse-JST
model. Extensive experiments have been
conducted on two corpora, the movie re-
view dataset and the multi-domain senti-
ment dataset. It has been found that while
all the three models achieve either bet-
ter or comparable performance on these
two corpora when compared to the exist-
ing unsupervised sentiment classification
approaches, both JST and Reverse-JST are
able to extract sentiment-oriented topics.
In addition, Reverse-JST always performs
worse than JST suggesting that the JST
model is more appropriate for joint senti-
ment topic detection.
1 Introduction
With the explosion of web 2.0, various types of
social media such as blogs, discussion forums and
peer-to-peer networks present a wealth of infor-
mation that can be very helpful in assessing the
general public?s sentiments and opinions towards
products and services. Recent surveys have re-
vealed that opinion-rich resources like online re-
views are having greater economic impact on both
consumers and companies compared to the tradi-
tional media (Pang and Lee, 2008). Driven by
the demand of gleaning insights of such great
amounts of user-generated data, work on new
methodologies for automated sentiment analysis
has bloomed splendidly.
Compared to the traditional topic-based text
classification, sentiment classification is deemed
to be more challenging as sentiment is often em-
bodied in subtle linguistic mechanisms such as
the use of sarcasm or incorporated with highly
domain-specific information. Although the task
of identifying the overall sentiment polarity of
a document has been well studied, most of the
work is highly domain dependent and favoured
in supervised learning (Pang et al, 2002; Pang
and Lee, 2004; Whitelaw et al, 2005; Kennedy
and Inkpen, 2006; McDonald et al, 2007), re-
quiring annotated corpora for every possible do-
main of interest, which is impractical for real
applications. Also, it is well-known that senti-
ment classifiers trained on one domain often fail
to produce satisfactory results when shifted to an-
other domain, since sentiment expression can be
quite different in different domains (Aue and Ga-
mon, 2005). Moreover, aside from the diversity
of genres and large-scale size of Web corpora,
user-generated contents evolve rapidly over time,
which demands much more efficient algorithms
for sentiment analysis than the current approaches
can offer. These observations have thus motivated
the problem of using unsupervised approaches for
domain-independent joint sentiment topic detec-
tion.
Some recent research efforts have been made to
adapt sentiment classifiers trained on one domain
to another domain (Aue and Gamon, 2005; Blitzer
et al, 2007; Li and Zong, 2008; Andreevskaia
and Bergler, 2008). However, the adaption perfor-
mance of these lines of work pretty much depends
on the distribution similarity between the source
and target domain, and considerable effort is still
required to obtain labelled data for training.
Intuitively, sentiment polarities are dependent
on contextual information, such as topics or do-
mains. In this regard, some recent work (Mei et
al., 2007; Titov and McDonald, 2008a) has tried to
model both sentiment and topics. However, these
two models either require postprocessing to calcu-
late the positive/negative coverage in a document
for polarity identification (Mei et al, 2007) or re-
144
quire some kind of supervised setting in which
review text should contain ratings for aspects of
interest (Titov and McDonald, 2008a). More re-
cently, Dasgupta and Ng (2009) proposed an unsu-
pervised sentiment classification algorithm by in-
tegrating user feedbacks into a spectral clustering
algorithm. Features induced for each dimension of
spectral clustering can be considered as sentiment-
oriented topics. Nevertheless, human judgement
of identifying the most important dimensions dur-
ing spectral clustering is required.
Lin and He (2009) proposed a joint sentiment-
topic (JST) model for unsupervised joint senti-
ment topic detection. They assumed that top-
ics are generated dependent on sentiment distri-
butions and then words are generated conditioned
on sentiment-topic pairs. While this is a reason-
able design choice, one may argue that the re-
verse is also true that sentiments may vary ac-
cording to topics. Thus in this paper, we studied
the reverse dependence of the JST model called
Reverse-JST, in which sentiments are generated
dependent on topic distributions in the modelling
process. We also note that, when the topic num-
ber is set to 1, both JST and reversed-JST es-
sentially become a simple latent Dirichlet aloca-
tion (LDA) model with only S (number of sen-
timent label) topics, each of which corresponds
to a sentiment label. We called it latent senti-
ment model (LSM) in this paper. Extensive ex-
periments have been conducted on the movie re-
view (MR)1 (Pang et al, 2002) and multi-domain
sentiment (MDS)2 (Blitzer et al, 2007) datasets
to compare the performance of LSM, JST and
Reverse-JST. Results show that all these three
models are able to give either better or compara-
ble performance compared to the existing unsu-
pervised sentiment classification approaches. In
addition, both JST and reverse-JST are able to ex-
tract sentiment-oriented topics. Furthermore, the
fact that reverse-JST always performs worse than
JST suggests that the JST model is more appropri-
ate for joint sentiment topic detection.
The rest of the paper is organized as follows.
Section 2 presents related work. Section 3 de-
scribes the LSM, JST and Reserver-JST models.
Experimental setup and results on the MR and
MDS datasets are discussed in Section 4 and 5 re-
1http://www.cs.cornell.edu/people/
pabo/movie-review-data
2http://www.cs.jhu.edu/
?
mdredze/
datasets/sentiment/index2.html
spectively. Finally, Section 6 concludes the paper
and outlines the future work.
2 Related Work
As opposed to the work (Pang et al, 2002; Pang
and Lee, 2004; Whitelaw et al, 2005; Kennedy
and Inkpen, 2006) that only focused on senti-
ment classification in one particular domain, re-
cent research attempts have been made to address
the problem of sentiment classification across do-
mains. Aue and Gamon (2005) explored vari-
ous strategies for customizing sentiment classifiers
to new domains, where the training is based on
a small number of labelled examples and large
amounts of unlabelled in-domain data. However,
their experiments achieved only limited success,
with most of the classification accuracy below
80%. In the same vein, some more recent work
focused on domain adaption for sentiment classi-
fiers. Blitzer et al (2007) used the structural corre-
spondence learning (SCL) algorithm with mutual
information. Li and Zong (2008) combined multi-
ple single classifiers trained on individual domains
using SVMs. However, the adaption performance
in (Blitzer et al, 2007) depends on the selection of
pivot features that used to link the source and tar-
get domains; whereas the approach of Li and Zong
(2008) heavily relies on labelled data from all the
domains to train the integrated classifier and thus
lack the flexibility to adapt the trained classifier to
domains where label information is not available.
Recent years have also seen increasing interests
in modelling both sentiment and topics simultane-
ously. The topic-sentiment mixture (TSM) model
(Mei et al, 2007) can jointly model sentiment and
topics by constructing an extra background com-
ponent and two additional sentiment subtopics on
top of the probabilistic latent semantic indexing
(pLSI) (Hofmann, 1999). However, TSM may
suffer from the problem of overfitting the data
which is known as a deficiency of pLSI, and post-
processing is also required in order to calculate
the sentiment prediction for a document. The
multi-aspect sentiment (MAS) model (Titov and
McDonald, 2008a), which is extended from the
multi-grain latent Dirichlet alocation (MG-LDA)
model (Titov and McDonald, 2008b), allows sen-
timent text aggregation for sentiment summary of
each rating aspect extracted from MG-LDA. One
drawback of MAS is that it requires that every as-
pect is rated at least in some documents, which
145
is practically infeasible. More recently, Dasgupta
and Ng (2009) proposed an unsupervised sen-
timent classification algorithm where user feed-
backs are provided on the spectral clustering pro-
cess in an interactive manner to ensure that text are
clustered along the sentiment dimension. Features
induced for each dimension of spectral cluster-
ing can be considered as sentiment-oriented top-
ics. Nevertheless, human judgement of identify-
ing the most important dimensions during spectral
clustering is required.
Among various efforts for improving senti-
ment detection accuracy, one direction is to in-
corporate prior information or subjectivity lexi-
con (i.e., words bearing positive or negative sen-
timent) into the sentiment model. Such sen-
timent lexicons can be acquired from domain-
independent sources in many different ways, from
manually built appraisal groups (Whitelaw et
al., 2005), to semi-automatically (Abbasi et al,
2008) and fully automatically (Kaji and Kitsure-
gawa, 2006) constructed lexicons. When incor-
porating lexical knowledge as prior information
into a sentiment-topic model, Andreevskaia and
Bergler (2008) integrated the lexicon-based and
corpus-based approaches for sentence-level sen-
timent annotation across different domains; Li et
al. (2009) employed lexical prior knowledge for
semi-supervised sentiment classification based on
non-negative matrix tri-factorization, where the
domain-independent prior knowledge was incor-
porated in conjunction with domain-dependent un-
labelled data and a few labelled documents. How-
ever, this approach performed worse than the JST
model on the movie review data even with 40%
labelled documents as will be shown in Section 5.
3 Latent Sentiment-Topic Models
This section describes three closely related
Bayesian models for unsupervised sentiment clas-
sification, the latent sentiment model (LSM), the
joint sentiment-topic (JST) model, and the joint
topic sentiment model by reversing the generative
process of sentiment and topics in the JST model,
called Reverse-JST.
3.1 Latent Sentiment Model (LSM)
The LSM model, as shown in Figure 1(a), can be
treated as a special case of LDA where a mixture
of only three sentiment labels are modelled, i.e.
positive, negative and neutral.
Assuming that we have a total number of S sen-
timent labels3; a corpus with a collection of D
documents is denoted by C = {d1, d2, ..., dD};
each document in the corpus is a sequence of Nd
words denoted by d = (w1, w2, ..., wNd), and
each word in the document is an item from a vo-
cabulary index with V distinct terms denoted by
{1, 2, ..., V }. The procedure of generating a word
in LSM starts by firstly choosing a distribution
over three sentiment labels for a document. Fol-
lowing that, one picks up a sentiment label from
the sentiment label distribution and finally draws a
word according to the sentiment label-word distri-
bution.
The joint probability of words and sentiment la-
bel assignment in LSM can be factored into two
terms:
P (w, l) = P (w|l)P (l|d). (1)
Letting the superscript ?t denote a quantity that
excludes data from the tth position, the conditional
posterior for lt by marginalizing out the random
variables ? and pi is
P (lt = k|w, l?t, ?,?) ?
N?twt,k + ?
N?tk + V ?
?
N?tk,d + ?k
N?td +
?
k ?k
, (2)
where Nwt,k is the number of times word wt has
associated with sentiment label k; Nk is the the
number of times words in the corpus assigned to
sentiment label k; Nk,d is the number of times
sentiment label k has been assigned to some word
tokens in document d; Nd is the total number of
words in the document collection.
Gibbs sampling is used to estimate the poste-
rior distribution of LSM, as well as the JST and
Reverse-JST models that will be discussed in the
following two sections.
3.2 Joint Sentiment-Topic Model (JST)
In contrast to LSM that only models document
sentiment, the JST model (Lin and He, 2009)
can detect sentiment and topic simultaneously, by
modelling each document with S (number of sen-
timent labels) topic-document distributions. It
should be noted that when the topic number is
set to 1, JST effectively becomes the LSM model
with only three topics corresponding to each of the
3For all the three models, i.e., LSM, JST and Reverse-
JST, we set the sentiment label number S to 3 representing
the positive, negative and neutral polarities, respectively.
146
w


l

N d D
S
(a)


w




z

l
N d D
S
S * T
(b)
w