Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 927?936,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Question Classification using Head Words and their Hypernyms
Zhiheng Huang
EECS Department
University of California
at Berkeley
CA 94720-1776, USA
zhiheng@cs.berkeley.edu
Marcus Thint
Intelligent Systems Research Center
British Telecom Group
Chief Technology Office
marcus.2.thint@bt.com
Zengchang Qin
EECS Department
University of California
at Berkeley
CA 94720-1776, USA
zqin@cs.berkeley.edu
Abstract
Question classification plays an important role
in question answering. Features are the key to
obtain an accurate question classifier. In con-
trast to Li and Roth (2002)?s approach which
makes use of very rich feature space, we pro-
pose a compact yet effective feature set. In
particular, we propose head word feature and
present two approaches to augment semantic
features of such head words using WordNet.
In addition, Lesk?s word sense disambigua-
tion (WSD) algorithm is adapted and the depth
of hypernym feature is optimized. With fur-
ther augment of other standard features such
as unigrams, our linear SVM and Maximum
Entropy (ME) models reach the accuracy of
89.2% and 89.0% respectively over a standard
benchmark dataset, which outperform the best
previously reported accuracy of 86.2%.
1 Introduction
An important step in question answering (QA) and
other dialog systems is to classify the question to
the anticipated type of the answer. For example, the
question of Who discovered x-rays should be classi-
fied into the type of human (individual). This infor-
mation would narrow down the search space to iden-
tify the correct answer string. In addition, this infor-
mation can suggest different strategies to search and
verify a candidate answer. For instance, the classifi-
cation of question What is autism to a definition type
question would trigger the search strategy specific
for definition type (e.g., using predefined templates
like: Autism is ... or Autism is defined as...). In fact,
the combination of QA and the named entity recog-
nition is a key approach in modern question answer-
ing systems (Voorhees and Dang, 2005).
The question classification is by no means trivial:
Simply using question wh-words can not achieve
satisfactory results. The difficulty lies in classify-
ing the what and which type questions. Considering
the example What is the capital of Yugoslavia, it is
of location (city) type, while What is the pH scale
is of definition type. Considering also examples (Li
and Roth, 2006) What tourist attractions are there in
Reims, What are the names of the tourist attractions
in Reims, What do most tourists visit in Reims, What
attracts tourists to Reims, and What is worth seeing
in Reims, all these reformulations are of the same
answer type of location. Different wording and syn-
tactic structures make it difficult for classification.
Many QA systems used manually constructed sets
of rules to map a question to a type, which is not effi-
cient in maintain and upgrading. With the increasing
popularity of statistical approaches, machine learn-
ing plays a more and more important role in this
task. A salient advantage of machine learning ap-
proach is that one can focus on designing insightful
features, and rely on learning process to efficiently
and effectively cope with the features. In addition, a
learned classifier is more flexible to reconstruct than
a manually constructed system because it can be
trained on a new taxonomy in a very short time. Ear-
lier question classification work includes Pinto et al
(2002) and Radev et at. (2002), in which language
model and Rappier rule learning were employed
respectively. More recently, Li and Roth (2002)
have developed a machine learning approach which
uses the SNoW learning architecture (Khardon et al,
927
1999). They have compiled the UIUC question clas-
sification dataset 1 which consists of 5500 training
and 500 test questions. The questions in this dataset
are collected from four sources: 4,500 English ques-
tions published by USC (Hovy et al, 2001), about
500 manually constructed questions for a few rare
classes, 894 TREC 8 and TREC 9 questions, and
also 500 questions from TREC 10 which serve as the
test dataset. All questions in the dataset have been
manually labeled by them according to the coarse
and fine grained categories as shown in Table 3, with
coarse classes (in bold) followed by their fine class
refinements. In addition, the table shows the dis-
tribution of the 500 test questions over such cate-
gories. Li and Roth (2002) have made use of lexical
words, part of speech tags, chunks (non-overlapping
phrases), head chunks (the first noun chunk in a
question) and named entities. They achieved 78.8%
accuracy for 50 fine grained classes. With a hand-
built dictionary of semantically related words, their
system is able to reach 84.2%.
The UIUC dataset has laid a platform for the
follow-up research. Hacioglu and Ward (2003) used
linear support vector machines with question word
bigrams and error-correcting output to obtain accu-
racy of 80.2% to 82.0%. Zhang and Lee (2003)
used linear SVMs with all possible question word
grams, and obtained accuracy of 79.2%. Later Li
and Roth (2006) used more semantic information
sources including named entities, WordNet senses,
class-specific related words, and distributional sim-
ilarity based categories in question classification
task. With all these semantic features plus the syn-
tactic ones, their model was trained on 21?500 ques-
tions and was able to achieve the best accuracy of
89.3% on a test set of 1000 questions (taken from
TREC 10 and TREC 11) for 50 fine classes. Most
recently, Krishnan et al (2005) used a short (typ-
ically one to three words) subsequence of question
tokens as features for question classification. Their
model can reach the accuracy of 86.2% using UIUC
dataset over fine grained question categories, which
is the highest reported accuracy on UIUC dataset.
In contrast to Li and Roth (2006)?s approach
which makes use of a very rich feature set, we
propose to use a compact yet effective feature set.
In particular, we propose head word feature and
1available at http://12r.cs.uiuc.edu/?cogcomp/Data/QA/QC
present two approaches to augment semantic fea-
tures of such head words using WordNet. In addi-
tion, Lesk?s word sense disambiguation (WSD) al-
gorithm is adapted and the depth of hypernym fea-
ture is optimized. With further augment of other
standard features such as unigrams, we can obtain
accuracy of 89.2% using linear SVMs, or 89.0% us-
ing ME for 50 fine classes.
2 Classifiers
In this section, we briefly present two classifiers,
support vector machines and maximum entropy
model, which will be employed in our experiments.
These two classifiers perform roughly identical in
the question classification task.
2.1 Support Vector Machines
Support vector machine (Vapnik, 1995) is a useful
technique for data classification. Given a training set
of instance-labeled pairs (xi, yi), i = 1, . . . , l where
xi ? Rn and y ? {1,?1}l , the support vector ma-
chines (SVM) require the solution of the following
optimization problem: minw,b,? 12wTw+C
?l
i=1 ?i
subject to yi(wT?(xi) + b) ? 1 ? ?i and ?i ? 0.
Here training vectors xi are mapped into a higher
(maybe infinite) dimensional space by the function
?. Then SVM finds a linear separating hyperplane
with the maximal margin in this higher dimensional
space. C > 0 is the penalty parameter of the er-
ror term. Furthermore, K(xi,xj) ? ?(xi)T?(xi) is
called the kernel function. There are four basic ker-
nels: linear, polynomial, radial basis function, and
sigmoid. In the question classification context, xi
is represented by a set of binary features, for in-
stance, the presence or absence of particular words.
yi ? {1,?1} indicates wether a question is of a
particular type or not. Due to the large number of
features in question classification, one may not need
to map data to a higher dimensional space. It has
been commonly accepted that the linear kernel of
K(xi,xj) = xiTxi is good enough for question
classification. In this paper, we adopt the LIBSVM
(Chang and Lin, 2001) implementation in our exper-
iments.
2.2 Maximum Entropy Models
Maximum entropy (ME) models (Berger et al,
1996; Manning and Klein, 2003), also known as
928
log-linear and exponential learning models, provide
a general purpose machine learning technique for
classification and prediction which has been suc-
cessfully applied to natural language processing in-
cluding part of speech tagging, named entity recog-
nition etc. Maximum entropy models can inte-
grate features from many heterogeneous informa-
tion sources for classification. Each feature corre-
sponds to a constraint on the model. In the context of
question classification, a sample feature could be the
presence of a particular word associated with a par-
ticular question type. The maximum entropy model
is the model with maximum entropy of all models
that satisfy the constraints. In this paper, we adopt
Stanford Maximum Entropy (Manning and Klein,
2003) implementation in our experiments.
3 Features
Each question is represented as a bag of features
and is feeded into classifiers in training stage. We
present five binary feature sets, namely question wh-
word, head word, WordNet semantic features for
head word, word grams, and word shape feature.
The five feature sets will be separately used by the
classifiers to determine their individual contribution.
In addition, these features are used in an incremental
fashion in our experiments.
3.1 Question wh-word
The wh-word feature is the question wh-word in
given questions. For example, the wh-word of
question What is the population of China is what.
We have adopted eight question wh-words, namely
what, which, when, where, who, how, why, and rest,
with rest being the type does not belong to any of
the previous type. For example, the question Name
a food high in zinc is a rest type question.
3.2 Head Word
Li and Roth (2002;2006) used head chunks as fea-
tures. The first noun chunk and the first verb chunk
after the question word in a sentence are defined
as head chunks in their approach. Krishnan et al
(2005) used one contiguous span of tokens which is
denoted as the informer span as features. In both
approaches, noisy information could be introduced.
For example, considering the question of What is a
group of turkeys called, both the head chunk and in-
former span of this question is group of turkeys. The
word of turkeys in the chunk (or span) contributes to
the classification of type ENTY:animal if the hyper-
nyms of WordNet are employed (as described in next
section). However, the extra word group would in-
troduce ambiguity to misclassify such question into
HUMAN:group, as all words appearing in chunk are
treated equally. To tackle this problem, we pro-
pose the feature of head word, which is one single
word specifying the object that the question seeks.
In the previous example What is a group of turkeys
called, the head word is exactly turkeys. In doing
so, no misleading word group is augmented. An-
other example is George Bush purchased a small in-
terest in which baseball team. The head chunk, in-
former span and head word are baseball team, base-
ball team and team respectively. The extra word
baseball in the head chunk and informer span may
lead the question misclassified as ENTY:sport rather
than HUM:group. In most cases, the head chunk
or informer span include head words. The head
chunk feature or informer span feature would be
beneficiary so long as the useful information plays a
stronger role than the misleading one. Nevertheless,
this is not as effective as the introduction of one head
word.
To obtain the head word feature, a syntactic parser
is required. A syntactic parser is a model that out-
puts the grammatical structure of given sentences.
There are accurate parsers available such as Cha-
niak parser (Charniak and Johnson, 2005), Stan-
ford parser (Klein and Manning, 2003) and Berkeley
parser (Petrov and Klein, 2007), among which we
use the Berkeley parser 2 to help identify the head
word. Figure 1 shows two example parse trees for
questions What year did the Titanic sink and What is
the sales tax in Minnesota respectively.
Collins rules (Collins, 1999) can be applied to
parse trees to extract the syntactic head words. For
example, the WHNP phrase (Wh-noun phrase) in
the top of Figure 1 takes its WP child as its head
word, thus assigning the word what (in the bracket)
which is associated with WP tag to the syntactic
head word of WHNP phrase. Such head word as-
signment is carried out from the bottom up and the
word did is extracted as the head word of the whole
question. Similarly, the word is is extracted as the
2available at http://nlp.cs.berkeley.edu/Main.html#parsing
929
Minnesota
VP
VB
sink
ROOT
SBARQ(did)
SQ(did)WHNP(What) .
WP NN
What year did
VBD NP(Titanic)
DT NNP
?
the Titanic
ROOT
SBARQ(is)
WHNP(What) .
WP
What
SQ(is)
VBZ
is
NP(tax)
NP(tax) PP(in)
NNS NNDT
?
IN NP
NNPsalesthe tax in 
Figure 1: Two example parse trees and their head words
assignment
syntactic head word in the bottom of Figure 1.
Collins head words finder rules have been modi-
fied to extract semantic head word (Klein and Man-
ning, 2003). To better cover the question sentences,
we further re-define the semantic head finder rules
to fit our needs. In particular, the rules to find the
semantic head word of phrases SBARQ (Clause in-
troduced by subordinating conjunction), SQ (sub-
constituent of SBARQ excluding wh-word or wh-
phrase), VP (verb phrase) and SINV (declarative
sentence with subject-aux inversion) are redefined,
with the head preference of noun or noun phrase
rather than verb or verb phrase. The new head
word assignments for the previous two examples are
shown in Figure 2.
If the head word is any of name, type or kind etc,
post fix is required to identify the real head word if
necessary. In particular, we compile a tree pattern
as shown in the left of Figure 3. If this pattern is
matched against a given parse question parse tree,
the head word is re-assigned to the head word of NP
node in the tree pattern. For example, the initial head
word extracted from parse tree of question What is
the proper name for a female walrus is name. As
such parse tree (as shown partially in the right of
Figure 3) matches the compiled tree pattern, the post
operation shall fix it to walrus, which is the head
word of the NP in the tree pattern. This post fix helps
classify the question to ENTY:animal.
Minnesota
VP
VB
sink
ROOT
SBARQ(year)
SQ(Titanic)WHNP(year) .
WP NN
What year did
VBD NP(Titanic)
DT NNP
?
the Titanic
ROOT
SBARQ(tax)
WHNP(What) .
WP
What
SQ(tax)
VBZ
is
NP(tax)
NP(tax) PP(in)
NNS NNDT
?
IN NP
NNPsalesthe tax in 
Figure 2: Two example parse trees and their revised head
words assignment
walrus
PP
IN NP
*name
type
kind
genre
group
NP
NP
PP
DT JJ NN IN NP
DT JJ NNthe nameproper for
a female
Figure 3: Post fix for the head word assignment
In addition to the question head word as described
above, we introduce a few regular expression pat-
terns to help question head word identification. Note
that these patterns depend on the question type tax-
onomy as shown in Table 3. For example, consid-
ering the questions of What is an atom and What
are invertebrates, the head word of atom and in-
vertebrates do not help classify such questions to
DESC:def. To resolve this, we create a binary fea-
ture using a string regular expression which begins
with what is/are and follows by an optional a, an, or
the and then follows by one or two words. If a ques-
tion matches this regular expression, a binary feature
(a placeholder word is used in implementation, for
instance DESC:def 1 in this case) would be inserted
to the feature set of the question. This feature, if it
is beneficial, would be picked up by the classifiers
(SVMs or MEs) in training. We list all regular ex-
pression patterns which are used in our experiments
as following:
DESC:def pattern 1 The question begins with what is/are and follows
930
by an optional a, an, or the and then follows by one or two words.
DESC:def pattern 2 The question begins with what do/does and ends
with mean.
ENTY:substance pattern The question begins with what is/are and
ends with composed of/made of/made out of.
DESC:desc pattern The question begins with what does and ends
with do.
ENTY:term The question begins with what do you call.
DESC:reason pattern 1 The question begins with what causes/cause.
DESC:reason pattern 2 The question begins with What is/are and
ends with used for.
ABBR:exp pattern The question begins with What does/do and ends
with stand for.
HUM:desc pattern The question begins with Who is/was and follows
by a word starting with a capital letter.
It is worth noting that all these patterns serve as
feature generators for given questions: the feature
becomes active if the pattern matches the ques-
tions. The algorithm to extract question head word
is shown in Algorithm 1. There is no head word
returned for when, where or why type questions, as
these hw-words are informative enough; the inclu-
sion of other words would introduce noisy informa-
tion. If the question is of type how, the word follow-
ing how is returned as head word. The patterns are
then attempted to match the question if it is of type
what or who. If there is a match, the placehold word
for such pattern (e.g., HUM:desc for HUM:desc pat-
tern) is returned as head word. If none of the above
condition is met, the candidate head word is ex-
tracted from the question parse tree using the rede-
fined head finder rules. Such extracted head word is
returned only if it has noun or noun phrase tag; oth-
erwise the first word which has noun or noun phrase
tag is returned. The last step is a back up plan in
case none of the previous procedure happens.
3.3 WordNet Semantic Feature
WordNet (Fellbaum, 1998) is a large English lexicon
in which meaningfully related words are connected
via cognitive synonyms (synsets). The WordNet is
a useful tool for word semantics analysis and has
been widely used in question classification (Krish-
nan et al, 2005; Schlaefer et al, 2007). A natural
way to use WordNet is via hypernyms: Y is a hy-
pernym of X if every X is a (kind of) Y. For exam-
ple, the question of What breed of hunting dog did
Algorithm 1 Question head word extraction
Require: Question q
Ensure: Question head word
1: if q.type == when|where|why then
2: return null
3: end if
4: if q.type == how then
5: return the word following word ?how?
6: end if
7: if q.type == what then
8: for any aforementioned regular expression r (except HUM:desc
pattern) do
9: if(q matches r)
10: return r.placehold-word
11: end for
12: end if
13: if q.type == who && q matches HUM:desc pattern then
14: return ?HUM:desc?
15: end if
16: String candidate = head word extracted from question parse tree
17: if candidate.tag starts with NN then
18: return candidate
19: end if
20: return the first word whose tag starts with NN
the Beverly Hillbillies own requires the knowledge
of animal being the hypernym of dog. In this paper,
we propose two approaches to augment WordNet se-
mantic features, with the first augmenting the hyper-
nyms of head words as extracted in previous section
directly, and the second making use of a WordNet
similarity package (Seco et al, 2004), which implic-
itly employs the structure of hypernyms.
3.3.1 Direct Use of Hypernyms
In WordNet, senses are organized into hierarchies
with hypernym relationships, which provides a nat-
ural way to augment hypernyms features from the
original head word. For example, the hierarchies for
a noun sense of domestic dog is described as: dog
? domestic animal ? animal, while another noun
sense (a dull unattractive unpleasant girl or woman)
is organized as dog ? unpleasant woman ? un-
pleasant person. In addition, a verb sense of dog is
organized as dog ? pursue ? travel. In our first ap-
proach, we attempt to directly introduce hypernyms
for the extracted head words. The augment of hyper-
nyms for given head word can introduce useful in-
formation, but can also bring noise if the head word
or the sense of head word are not correctly identi-
fied. To resolve this, three questions shall be ad-
dressed: 1) which part of speech senses should be
augmented? 2) which sense of the given word is
needed to be augmented? and 3) how many depth
931
are required to tradeoff the generality (thus more
informative) and the specificity (thus less noisy).
The first question can be answered by mapping
the Penn Treebank part of speech tag of the given
head word to its WordNet part of speech tag, which
is one of POS.NOUN, and POS.ADJECTIVE,
POS.ADVERB and POS.VERB. The second ques-
tion is actually a word sense disambiguation (WSD)
problem. The Lesk algorithm (Lesk, 1986) is a clas-
sical algorithm for WSD. It is based on the assump-
tion that words in a given context will tend to share
a common topic. A basic implementation of the The
Lesk algorithm is described as following:
1. Choosing pairs of ambiguous words within a
context
2. Checks their definitions in a dictionary
3. Choose the senses as to maximize the number
of common terms in the definitions of the cho-
sen words
In our head word sense disambiguation, the context
words are words (except the head word itself) in the
question, and the dictionary is the gloss of a sense
for a given word. Algorithm 2 shows the adapted
Lesk algorithm which is employed in our system.
Basically, for each sense of given head word, this
Algorithm 2 Head word sense disambiguation
Require: Question q and its head word h
Ensure: Disambiguated sense for h
1: int count = 0
2: int maxCount = -1
3: sense optimum = null
4: for each sense s for h do
5: count = 0
6: for each context word w in q do
7: int subMax = maximum number of common words in s
definition (gloss) and definition of any sense of w
8: count = count + sumMax
9: end for
10: if count > maxCount then
11: maxCount = count
12: optimum = s
13: end if
14: end for
15: return optimum
algorithm computes the maximum number of com-
mon words between gloss of this sense and gloss of
any sense of the context words. Among all head
word senses, the sense which results in the maxi-
mum common words is chosen as the optimal sense
to augment hypernyms later. Finally the third ques-
tion is answered via trail and error based on evaluat-
ing randomly generated 10% data from the training
dataset. Generally speaking, if the identification of
the head word is not accurate, it would brings signif-
icant noisy information. Our experiments show that
the use of depth six produces the best results over
the validation dataset. This indirectly proves that our
head word feature is very accurate: the hypernyms
introduction within six depths would otherwise pol-
lute the feature space.
3.3.2 Indirect Use of Hypernyms
In this approach, we make use of the WordNet
Similarity package (Seco et al, 2004), which im-
plicitly employs WordNet hypernyms. In particu-
lar, for a given pair of words, the WordNet similar-
ity package models the length of path traveling from
one word to the other over the WordNet network.
It then computes the semantic similarity based on
the path. For example, the similarity between car
and automobile is 1.0, while the similarity between
film and audience is 0.38. For each question, we
use the WordNet similarity package to compute the
similarity between the head word of such question
and each description word in a question categoriza-
tion. The description words for a question category
are a few words (usually one to three) which explain
the semantic meaning of such a question category
3
. For example, the descriptions words for category
ENTY:dismed are diseases and medicine. The ques-
tion category which has the highest similarity to the
head word is marked as a feature. This is equal to
a mini question classifier. For example, as the head
word walrus of question What is the proper name
for a female walrus has the highest similarity mea-
sure to animals, which is a description word of cat-
egory ENTY:animal, thus the ENTY:animal is in-
serted into the feature set of the given question.
3.4 N-Grams
An N-gram is a sub-sequence of N words from a
given question. Unigram forms the bag of words
feature, and bigram forms the pairs of words fea-
ture, and so forth. We have considered unigram, bi-
gram, and trigram features in our experiments. The
reason to use such features is to provide word sense
3available at http://12r.cs.uiuc.edu/?cogcomp/Data/QA/QC
/definition.html
932
disambiguation for questions such as How long did
Rip Van Winkle sleep, as How long (captured by
wh-word and head word features) could refer to ei-
ther NUM:dist or NUM:period. The word feature of
sleep help determine the NUM:period classification.
3.5 Word Shape
Word shape in a given question may be useful for
question classification. For instance, the question
Who is Duke Ellington has a mixed shape (begins a
with capital letter and follows by lower case letters)
for Duke, which roughly serves as a named entity
recognizer. We use five word shape features, namely
all upper case, all lower case, mixed case, all digits,
and other. The experiments show that this feature
slightly boosts the accuracy.
4 Experimental Results
We designed two experiments to test the accuracy
of our classifiers. The first experiment evaluates the
individual contribution of different feature types to
question classification accuracy. In particular, the
SVM and ME are trained from the UIUC 5500 train-
ing data using the following feature sets: 1) wh-
word + head word, 2) wh-word + head word + direct
hypernym, 3) wh-wod + head word + indirect hyper-
nym, 4) unigram, 5) bigram, 6) trigram, and 7) word
shape. We set up the tests of 1), 2) and 3) due to the
fact that wh-word and head word can be treated as a
unit, and hypernym depends on head word. In the
second experiment, feature sets are incrementally
feeded to the SVM and ME. The parameters for both
SVM and ME classifiers (e.g., the C in the SVM)
are all with the default values. In order to facilitate
the comparison with previously reported results, the
question classification performance is measured by
accuracy, i.e., the proportion of the correctly classi-
fied questions among all test questions.
4.1 Individual Feature Contribution
Table 1 shows the question classification accuracy
of SVM and ME using individual feature sets for
6 coarse and 50 fine classes. Among all feature
sets, wh-word + head word proves to be very infor-
mative for question classification. Our first Word-
Net semantic feature augment, the inclusion of di-
rect hypernym, can further boost the accuracy in the
fine classes for both SVM and ME, up to four per-
Table 1: Question classification accuracy of SVM and
ME using individual feature sets for 6 and 50 classes over
UIUC dataset
6 class 50 class
SVM ME SVM ME
wh-word + head word 92.0 92.2 81.4 82.0
wh-word + depth=1 92.0 91.8 84.6 84.8
head word + depth = 3 92.0 92.2 85.4 85.4
direct hypernym depth = 6 92.6 91.8 85.4 85.6
wh-word + head 91.8 92.0 83.2 83.6
+ indirect hypernym
unigram 88.0 86.6 80.4 78.8
bigram 85.6 86.4 73.8 75.2
trigram 68.0 57.4 39.0 44.2
word shape 18.8 18.8 10.4 10.4
cent. This phenomena conforms to Krishnan et al
(2005) that WordNet hypernym benefits mainly on
the 50 fine classes classification. Li and Roth (2006)
made use of semantic features including named en-
tities, WordNet senses, class-specific related words,
and distributional similarity based categories. Their
system managed to improve around 4 percent with
the help of those semantic features. They reported
that WordNet didn?t contribute much to the system,
while our results show that the WordNet signifi-
cantly boosts the accuracy. The reason may be that
their system expanded the hypernyms for each word
in the question, while ours only expanded the head
word. In doing so, the augmentation does not intro-
duce much noisy information. Notice that the inclu-
sion of various depth of hypernyms results in differ-
ent accuracy. The depth of six brings the highest ac-
curacy of 85.4% and 85.6% for SVM and ME under
50 classes, which is very competitive to the previ-
ously reported best accuracy of 86.2% (Krishnan et
al., 2005).
Our second proposed WordNet semantic feature,
the indirect use of hypernym, does not perform as
good as the first approach; it only contributes the
accuracy gain of 1.8 and 1.6 in the fine classes for
SVM and ME respectively. The reason may be two
fold: 1) the description words (usually one to three
words) of question categories are not representative
enough, and 2) the indirect use of hypernyms via
the WordNet similarity package is not as efficient as
direct use of hypernyms.
Among the surface words features, unigram fea-
ture perform the best with accuracy of 80.4% for
SVM under 50 classes, and 88.0% for SVM under
6 classes. It is not surprising that the word shape
933
feature only achieves small gain in question classi-
fication, as the use of five shape type does not pro-
vide enough information for question classification.
However, this feature is treated as an auxiliary one to
boost a good classifier, as we will see in the second
experiment.
4.2 Incremental Feature Contribution
Based on the individual feature contribution, we
then trained the SVMs and MEs using wh-word,
head word, direct hypernyms (with depth 6) of head
word, unigram, and word shape incrementally. Table
2 shows the question classification accuracy (bro-
ken down by question types) of SVM and ME for 6
coarse and 50 fine classes. As can be seen, the main
difficulty for question classification lies in the what
type questions. SVM and ME perform roughly iden-
tical if they use the same features. For both SVM
and ME, the baseline using the wh-head word and
head word results in 81.4% and 82.0% respectively
for 50 fine class classification (92.0% and 92.2% for
6 coarse classes). The incremental use of hypernym
feature within 6 depths boost about four percent for
both SVM and ME under 50 classes, while slight
gain or slight loss for SVM and ME for 6 coarse
classes. The further use of unigram feature leads to
another three percent gain for both SVM and ME in
50 classes. Finally, the use of word shape leads to
another 0.6% accuracy increase for both SVM and
ME in 50 classes. The best accuracy achieved for
50 classes is 89.2% for SVM and 89.0% for ME.
For 6 coarse classes, SVM and ME achieve the best
accuracy of 93.4% and 93.6% respectively.
Our best result feature space only consists of
13?697 binary features and each question has 10 to
30 active features. Compared to the over feature size
of 200?000 in Li and Roth (2002), our feature space
is much more compact, yet turned out to be more
informative as suggested by the experiments.
Note that if we replace the bigram with unigram,
SVM and ME achieve the overall accuracy of 88.4%
and 88.0% respectively for 50 fine classes, and the
use of trigram leads SVM and ME to 86.6% and
86.8% respectively. The inclusion of unigram, bi-
gram and trigram together won?t boost the accu-
racy, which reflects the fact that the bigram and tri-
gram features cannot bring more information given
that unigram, wh-word and head word features are
present. This is because the useful information
which are supposed to be captured by bigram or tri-
gram are effectively captured by wh-word and head
word features. The unigram feature thus outper-
forms bigram and trigram due to the fact that it is
less sparse. In addition, if we replace the indirect
use of hypernym with the direct use of hypernym,
the overall accuracy is 84.6% and 84.8% for SVM
and ME respectively. All these experiments conform
to the individual features contributions as shown in
Table 1.
For a better understanding of the error distribu-
tion with respect to the 50 question categories, Ta-
ble 3 shows the precision and recall for each ques-
tion type in the best result (89.2%) using SVM.
It is not surprising that some of the categories are
more difficult to predict such as ENTY:other and
ENTY:product, while others are much easier such
as HUMAN:individual, since the former are more
semantically ambiguous than the latter.
Table 3: Precision and recall for fine grained question
categories
Class # P R Class # P R
ABBR 9 desc 7 75.0 85.7
abb 1 100 100 manner 2 100 100
exp 8 88.9 100 reason 6 85.7 100
ENTITY 94 HUMAN 65
animal 16 94.1 100 group 6 71.4 83.3
body 2 100 50.0 individual 55 94.8 100
color 10 100 100 title 1 0.0 0.0
creative 0 100 100 desc 3 100 100
currency 6 100 100 LOC 81
dis.med. 2 40.0 100 city 18 100 77.8
event 2 100 50.0 country 3 100 100
food 4 100 50.0 mountain 3 100 66.7
instrument 1 100 100 other 50 83.9 94.0
lang 2 100 100 state 7 85.7 85.7
letter 0 100 100 NUM 113
other 12 45.5 41.7 code 0 100 100
plant 5 100 100 count 9 81.8 100
product 4 100 25.0 date 47 95.9 100
religion 0 100 100 distance 16 100 62.5
sport 1 100 100 money 3 100 33.3
substance 15 88.9 53.3 order 0 100 100
symbol 0 100 100 other 12 85.7 50.0
technique 1 100 100 period 8 72.7 100
term 7 100 85.7 percent 3 75.0 100
vehicle 4 100 75.0 speed 6 100 83.3
word 0 100 100 temp 5 100 60.0
DESC 138 size 0 100 100
definition 123 89.0 98.4 weight 4 100 75.0
Table 4 shows the summary of the classification
accuracy of all models which were applied to UIUC
dataset. Note (1) that SNoW accuracy without the
related word dictionary was not reported. With
the semantically related word dictionary, it achieved
91%. Note (2) that SNoW with a semantically re-
lated word dictionary achieved 84.2% but the other
algorithms did not use it. Our results are summa-
rized in the last two rows.
Our classifiers are able to classify some chal-
934
Table 2: Question classification accuracy of SVM and ME using incremental feature sets for 6 and 50 classes
6 coarse classes
Type #Quest wh+headword +headword hypernym +unigram +word shape
SVM ME SVM ME SVM ME SVM ME
what 349 88.8 89.1 89.7 88.5 89.7 90.3 90.5 91.1
which 11 90.9 90.9 100 100 100 100 100 100
when 26 100 100 100 100 100 100 100 100
where 27 100 100 100 100 100 100 100 100
who 47 100 100 100 100 100 100 100 100
how 34 100 100 100 100 100 100 100 100
why 4 100 100 100 100 100 100 100 100
rest 2 100 100 50.0 50.0 100 50.0 100 50.0
total 500 92.0 92.2 92.6 91.8 92.8 93.0 93.4 93.6
50 fine classes
Type #Quest wh+headword +headword hypernym +unigram +word shape
SVM ME SVM ME SVM ME SVM ME
what 349 77.4 77.9 82.8 82.5 85.4 85.1 86.2 86.0
which 11 81.8 90.9 81.8 90.9 90.9 100 90.9 100
when 26 100 100 100 100 100 100 100 100
where 27 92.6 92.6 92.6 92.6 92.6 92.6 92.6 92.6
who 47 100 100 100 100 100 100 100 100
how 34 76.5 76.5 76.5 79.4 97.1 91.2 97.1 91.2
why 4 100 100 100 100 100 100 100 100
rest 2 0.0 0.0 50.0 50.0 0.0 50.0 0.0 50.0
total 500 81.4 82.0 85.4 85.6 88.6 88.4 89.2 89.0
lenge questions. For instance, the question What
is the proper name for a female walrus has been
correctly classified as ENTY:animal. However, it
still has nearly ten percent error rate for 50 fine
classes. The reason is three fold: 1) there are in-
herently ambiguity in classifying a question. For
instance, the question What is mad cow disease, it
could be either of type DESC:def or ENTY:dismed;
2) there are inconsistent labeling in the training data
and test data. For instance, What is the popula-
tion of Kansas is labeled with the type NUM:other
while What is the population of Arcadia , Florida
is labeled with type NUM:count. Another exam-
ple, What county is Chicago in is labeled with type
LOC:other while What county is Phoenix , AZ in is
labeled with type LOC:city; and 3) The parser can
produce incorrect parse tree which would result in
wrong head word extraction. For instance, the head
word extracted from What is the speed humming-
birds fly is hummingbirds (the correct one should be
speed), thus leading to the incorrect classification of
ENTY:animal (rather than the correct NUM:speed).
5 Conclusion
In contrast to Li and Roth (2006)?s approach which
makes use of very rich feature space, we proposed
a compact yet effective feature set. In particular,
we proposed head word feature and presented two
Table 4: Classification accuracy of all models which were
applied to UIUC dataset
Algorithm 6 class 50 class
Li and Roth, SNoW ?(1) 78.8(2)
Hacioglu et al, SVM+ECOC ? 80.2-82
Zhang & Lee, Linear SVM 87.4 79.2
Zhang & Lee, Tree SVM 90.0 ?
Krishnan et al, SVM+CRF 93.4 86.2
Linear SVM 93.4 89.2
Maximum Entropy Model 93.6 89.0
approaches to augment semantic features of such
head words using WordNet. In addition, Lesk?s
word sense disambiguation algorithm was adapted
and the depth of hypernym feature was optimized
through cross validation, which was to introduce
useful information while not bringing too much
noise. With further augment of wh-word, unigram
feature, and word shape feature, we can obtain ac-
curacy of 89.2% using linear SVMs, or 89.0% using
ME for 50 fine classes.
6 Acknowledgments
This research was supported by British Telecom
grant CT1080028046 and BISC Program of UC
Berkeley.
935
References
Berger, A. L., V. J. D. Pietra, and S. A. D. Pietra. 1996.
A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, 22(1):39?71.
Chang, C. C and C. J. Lin. 2001. LIBSVM: a li-
brary for support vector machines. Software available
at http://www.csie.ntu.edu.tw/?cjlin/libsvm/.
Charniak, E. and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
The 43rd Annual Meeting on Association for Compu-
tational Linguistics.
Collins, M. 1999. Head-Driven Statistical Models for
Natural Language Parsing. PhD thesis, University of
Pennsylvania.
Fellbaum, C. 1998. An Electronic Lexical Database.
The MIT press.
Hacioglu, K. and W. Ward. 2003. Question Classifica-
tion with Support Vector Machines and Error Correct-
ing Codes. The Association for Computational Lin-
guistics on Human Language Technology, vol. 2, pp.
28?30.
Hovy, E., L. Gerber, U. Hermjakob, C. Y. Lin, and D.
Ravichandran. 2001. Towards Semantics-based An-
swer Pinpointing. The conference on Human language
technology research (HLT), pp. 1?7.
Khardon, R., D. Roth, and L. G. Valiant. 1999. Rela-
tional Learning for NLP using Linear Threshold Ele-
ments. The Conference on Artificial Intelligence, pp.
911?919.
Klein, D. and C. Manning. 2003. Accurate Unlexical-
ized Parsing. The Association for Computational Lin-
guistics, vol. 1, pp. 423?430.
Krishnan, V., S. Das, and S. Chakrabarti. 2005. En-
hanced Answer Type Inference from Questions using
Sequential Models. The conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing.
Lesk, Michael. 1986. Automatic Sense Disambiguation
using Machine Readable Dictionaries: how to tell a
pine cone from an ice cream cone. ACM Special Inter-
est Group for Design of Communication Proceedings
of the 5th annual international conference on Systems
documentation, pp. 24?26.
Li, X. and D. Roth. 2002. Learning Question Classifiers.
The 19th international conference on Computational
linguistics, vol. 1, pp. 1?7.
Li, X. and D. Roth. 2006. Learning Question Classifiers:
the Role of Semantic Information. Natural Language
Engineering, 12(3):229?249.
Manning, C. and D. Klein. 2003. Optimization, Maxent
Models, and Conditional Estimation without Magic.
Tutorial at HLT-NAACL 2003 and ACL 2003.
Petrov, S. and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. HLT-NAACL.
Pinto, D., M. Branstein, R. Coleman, W. B. Croft, M.
King, W. Li, and X. Wei. 2002. QuASM: A System
for Question Answering using semi-structured Data
The 2nd ACM/IEEE-CS joint conference on Digital li-
braries..
Radev, D., W. Fan, H. Qi, H. Wu, and A. Grewal. 2002.
Probabilistic question answering on the web. The 11th
international conference on World Wide Web.
Schlaefer, N., J. Ko, J. Betteridge, S. Guido, M. Pathak,
and E. Nyberg. 2007. Semantic Extensions of the
Ephyra QA System for TREC 2007. The Sixteenth
Text REtrieval Conference (TREC).
Seco, N., T. Veale, and J. Hayes. 2004. An Intrinsic
Information Content Metric for Semantic Similarity in
WordNet. Proceedings of the European Conference of
Artificial Intelligence.
Vapnik, V. N. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag New York.
Voorhees, E. M. and H. T. Dang. 2005. Overview of
the TREC 2005 Question Answering Track. The Text
Retrieval Conference (TREC2005).
Zhang D. and W. S. Lee. 2003. Question Classification
using Support Vector Machines. The ACM SIGIR con-
ference in informaion retrieval, pp. 26?32.
936
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 543?550,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Investigation of Question Classifier in Question Answering
Zhiheng Huang
EECS Department
University of California
at Berkeley
CA 94720-1776, USA
zhiheng@cs.berkeley.edu
Marcus Thint
Intelligent Systems Research Center
British Telecom Group
Chief Technology Office
marcus.2.thint@bt.com
Asli Celikyilmaz
EECS Department
University of California
at Berkeley
CA 94720-1776, USA
asli@cs.berkeley.edu
Abstract
In this paper, we investigate how an ac-
curate question classifier contributes to
a question answering system. We first
present a Maximum Entropy (ME) based
question classifier which makes use of
head word features and their WordNet hy-
pernyms. We show that our question clas-
sifier can achieve the state of the art per-
formance in the standard UIUC question
dataset. We then investigate quantitatively
the contribution of this question classifier
to a feature driven question answering sys-
tem. With our accurate question classifier
and some standard question answer fea-
tures, our question answering system per-
forms close to the state of the art using
TREC corpus.
1 Introduction
Question answering has drawn significant atten-
tion from the last decade (Prager, 2006). It at-
tempts to answer the question posed in natural
language by providing the answer phrase rather
than the whole documents. An important step in
question answering (QA) is to classify the ques-
tion to the anticipated type of the answer. For
example, the question of Who discovered x-rays
should be classified into the type of human (indi-
vidual). This information would narrow down the
search space to identify the correct answer string.
In addition, this information can suggest different
strategies to search and verify a candidate answer.
In fact, the combination of question classification
and the named entity recognition is a key approach
in modern question answering systems (Voorhees
and Dang, 2005).
The question classification is by no means triv-
ial: Simply using question wh-words can not
achieve satisfactory results. The difficulty lies
in classifying the what and which type questions.
Considering the example What is the capital of Yu-
goslavia, it is of location (city) type, while What
is the pH scale is of definition type. As with
the previous work of (Li and Roth, 2002; Li and
Roth, 2006; Krishnan et al, 2005; Moschitti et
al., 2007), we propose a feature driven statistical
question classifier (Huang et al, 2008). In partic-
ular, we propose head word feature and augment
semantic features of such head words using Word-
Net. In addition, Lesk?s word sense disambigua-
tion (WSD) algorithm is adapted and the depth of
hypernym feature is optimized. With further aug-
ment of other standard features such as unigrams,
we can obtain accuracy of 89.0% using ME model
for 50 fine classes over UIUC dataset.
In addition to building an accurate question
classifier, we investigate the contribution of this
question classifier to a feature driven question an-
swering rank model. It is worth noting that, most
of the features we used in question answering rank
model, depend on the question type information.
For instance, if a question is classified as a type of
sport, we then only care about whether there are
sport entities existing in the candidate sentences.
It is expected that a fine grained named entity rec-
ognizer (NER) should make good use of the accu-
rate question type information. However, due to
the lack of a fine grained NER tool at hand, we
employ the Stanford NER package (Finkel et al,
2005) which identifies only four types of named
entities. Even with such a coarse named entity
recognizer, the experiments show that the question
classifier plays an important role in determining
the performance of a question answering system.
The rest of the paper is organized as follow-
ing. Section 2 reviews the maximum entropy
model which are used in both question classifica-
tion and question answering ranking. Section 3
presents the features used in question classifica-
tion. Section 4 presents the question classification
543
accuracy over UIUC question dataset. Section 5
presents the question answer features. Section 6
illustrates the results based on TREC question an-
swer dataset. And Section 7 draws the conclusion.
2 Maximum Entropy Models
Maximum entropy (ME) models (Berger et al,
1996; Manning and Klein, 2003), also known as
log-linear and exponential learning models, pro-
vide a general purpose machine learning technique
for classification and prediction which has been
successfully applied to natural language process-
ing including part of speech tagging, named entity
recognition etc. Maximum entropy models can in-
tegrate features from many heterogeneous infor-
mation sources for classification. Each feature
corresponds to a constraint on the model. Given
a training set of (C,D), where C is a set of class
labels and D is a set of feature represented data
points, the maximal entropy model attempts to
maximize the log likelihood
logP (C|D,?) =
?
(c,d)?(C,D)
log
exp
?
i
?
i
f
i
(c, d)
?
c
?
exp
?
j
?
j
f
i
(c, d)
,
(1)
where f
i
(c, d) are feature indicator functions. We
use ME models for both question classification
and question answer ranking. In question answer
context, such function, for instance, could be the
presence or absence of dictionary entities (as pre-
sented in Section 5.2) associated with a particular
class type (either true or false, indicating a sen-
tence can or cannot answer the question). ?
i
are
the parameters need to be estimated which reflects
the importance of f
i
(c, d) in prediction.
3 Question Classification Features
Li and Roth (2002) have developed a machine
learning approach which uses the SNoW learning
architecture. They have compiled the UIUC ques-
tion classification dataset1 which consists of 5500
training and 500 test questions.2 All questions in
the dataset have been manually labeled according
to the coarse and fine grained categories as shown
in Table 1, with coarse classes (in bold) followed
by their fine classes.
The UIUC dataset has laid a platform for the
follow-up research including (Hacioglu and Ward,
2003; Zhang and Lee, 2003; Li and Roth, 2006;
1Available at http://12r.cs.uiuc.edu/?cogcomp/Data/QA/QC.
2Test questions are from TREC 10.
Table 1: 6 coarse and 50 fine Question types de-
fined in UIUC question dataset.
ABBR letter desc NUM
abb other manner code
exp plant reason count
ENTITY product HUMAN date
animal religion group distance
body sport individual money
color substance title order
creative symbol desc other
currency technique LOC period
dis.med. term city percent
event vehicle country speed
food word mountain temp
instrument DESC other size
lang definition state weight
Krishnan et al, 2005; Moschitti et al, 2007). In
contrast to Li and Roth (2006)?s approach which
makes use of a very rich feature set, we propose
to use a compact yet effective feature set. The fea-
tures are briefly described as following. More de-
tailed information can be found at (Huang et al,
2008).
Question wh-word The wh-word feature is the
question wh-word in given questions. For ex-
ample, the wh-word of question What is the
population of China is what.
Head Word head word is defined as one single
word specifying the object that the question
seeks. For example the head word of What
is a group of turkeys called, is turkeys. This
is different to previous work including (Li
and Roth, 2002; Krishnan et al, 2005) which
has suggested a contiguous span of words
(a group of turkeys in this example). The
single word definition effectively avoids the
noisy information brought by non-head word
of the span (group in this case). A syntac-
tic parser (Petrov and Klein, 2007) and the
Collins rules (Collins, 1999) are modified to
extract such head words.
WordNet Hypernym WordNet hypernyms are
extracted for the head word of a given ques-
tion. The classic Lesk algorithm (Lesk, 1986)
is used to compute the most probable sense
for a head word in the question context, and
then the hypernyms are extracted based on
that sense. The depth of hypernyms is set to
544
six with trial and error.3 Hypernyms features
capture the general terms of extracted head
word. For instance, the head word of ques-
tion What is the proper name for a female
walrus is extracted as walrus and its direct
hypernyms such as mammal and animal are
extracted as informative features to predict
the correct question type of ENTY:animal.
Unigram words Bag of words features. Such
features provide useful question context in-
formation.
Word shape Five word shape features, namely all
upper case, all lower case, mixed case, all
digits, and other are used to serve as a coarse
named entity recognizer.
4 Question Classification Experiments
We train a Maximum Entropy model using the
UIUC 5500 training questions and test over the
500 test questions. Tables 2 shows the accuracy of
6 coarse class and 50 fine grained class, with fea-
tures being fed incrementally. The question classi-
fication performance is measured by accuracy, i.e.,
the proportion of the correctly classified questions
among all test questions. The baseline using the
Table 2: Question classification accuracy using in-
cremental feature sets for 6 and 50 classes over
UIUC split.
6 class 50 class
wh-word 46.0 46.8
+ head word 92.2 82.0
+ hypernym 91.8 85.6
+ unigram 93.0 88.4
+ word shape 93.6 89.0
wh-head word results in 46.0% and 46.8% respec-
tively for 6 coarse and 50 fine class classification.
The incremental use of head word boosts the accu-
racy significantly to 92.2% and 82.0% for 6 and 50
classes. This reflects the informativeness of such
feature. The inclusion of hypernym feature within
6 depths boosts 3.6% for 50 classes, while result-
ing in slight loss for 6 coarse classes. The further
use of unigram feature leads to 2.8% gain in 50
classes. Finally, the use of word shape leads to
0.6% accuracy increase for 50 classes. The best
3We performed 10 cross validation experiment over train-
ing data and tried various depths of 1, 3, 6, 9 and ?, with ?
signifies that no depth constraint is imposed.
accuracies achieved are 93.6% and 89.0% for 6
and 50 classes respectively.
The individual feature contributions were dis-
cussed in greater detail in (Huang et al, 2008).
Also, The SVM (rathern than ME model) was em-
ployed using the same feature set and the results
were very close (93.4% for 6 class and 89.2% for
50 class). Table 3 shows the feature ablation ex-
periment4 which is missing in that paper. The
experiment shows that the proposed head word
and its hypernym features play an essential role
in building an accurate question classifier.
Table 3: Question classification accuracy by re-
moving one feature at a time for 6 and 50 classes
over UIUC split.
6 class 50 class
overall 93.6 89.0
- wh-word 93.6 89.0
- head word 92.8 88.2
- hypernym 90.8 84.2
- unigram 93.6 86.8
- word shape 93.0 88.4
Our best result feature space only consists of
13?697 binary features and each question has 10
to 30 active features. Compared to the over feature
size of 200?000 in Li and Roth (2002), our feature
space is much more compact, yet turned out to be
more informative as suggested by the experiments.
Table 4 shows the summary of the classification
accuracy of all question classifiers which were ap-
plied to UIUC dataset.5 Our results are summa-
rized in the last row.
In addition, we have performed the 10 cross
validation experiment over the 5500 UIUC train-
ing corpus using our best model. The result is
89.05?1.25 and 83.73?1.61 for 6 and 50 classes,6
which outperforms the best result of 86.1?1.1 for
6 classes as reported in (Moschitti et al, 2007).
5 Question Answer Features
For a pair of a question and a candidate sentence,
we extract binary features which include CoNLL
named entities presence feature (NE), dictionary
4Remove one feature at a time from the entire feature set.
5Note (1) that SNoW accuracy without the related word
dictionary was not reported. With the semantically related
word dictionary, it achieved 91%. Note (2) that SNoW with a
semantically related word dictionary achieved 84.2% but the
other algorithms did not use it.
6These results are worse than the result over UIUC split;
as the UIUC test data includes a larger percentage of easily
classified question types.
545
Table 4: Accuracy of all question classifiers which
were applied to UIUC dataset.
Algorithm 6 class 50 class
Li and Roth, SNoW ?(1) 78.8(2)
Hacioglu et al, SVM+ECOC ? 80.2-82
Zhang & Lee, Linear SVM 87.4 79.2
Zhang & Lee, Tree SVM 90.0 ?
Krishnan et al, SVM+CRF 93.4 86.2
Moschitti et al, Kernel 91.8 ?
Maximum Entropy Model 93.6 89.0
entities presence feature (DIC), numerical entities
presence feature (NUM), question specific feature
(SPE), and dependency validity feature (DEP).
5.1 CoNLL named entities presence feature
We use Stanford named entity recognizer (NER)
(Finkel et al, 2005) to identify CoNLL style NEs7
as possible answer strings in a candidate sentence
for a given type of question. In particular, if the
question is ABBR type, we tag CoNLL LOC,
ORG and MISC entities as candidate answers; If
the question is HUMAN type, we tag CoNLL PER
and ORG entities; And if the question is LOC
type, we tag CoNLL LOC and MISC entities. For
other types of questions, we assume there is no
candidate CoNLL NEs to tag. We create a binary
feature NE to indicate the presence or absence of
tagged CoNLL entities. Further more, we cre-
ate four binary features NE-PER, NE-LOC, NE-
ORG, and NE-MISC to indicate the presence of
tagged CoNLL PER, LOC, ORG and MISC enti-
ties.
5.2 Dictionary entities presence feature
As four types of CoNLL named entities are not
enough to cover 50 question types, we include the
101 dictionary files compiled in the Ephyra project
(Schlaefer et al, 2007). These dictionary files con-
tain names for specific semantic types. For exam-
ple, the actor dictionary comprises a list of actor
names such as Tom Hanks and Kevin Spacey. For
each question, if the head word of such question
(see Section 3) matches the name of a dictionary
file, then each noun phrase in a candidate sentence
is looked up to check its presence in the dictio-
nary. If so, a binary DIC feature is created. For
example, for the question What rank did Chester
7Person (PER), location (LOC), organization (ORG), and
miscellaneous (MISC).
Nimitz reach, as there is a military rank dictionary
matches the head word rank, then all the noun
phrases in a candidate sentence are looked up in
the military rank dictionary. As a result, a sen-
tence contains word Admiral will result in the DIC
feature being activated, as such word is present in
the military rank dictionary.
Note that an implementation tip is to allow the
proximity match in the dictionary look up. Con-
sider the question What film introduced Jar Jar
Binks. As there is a match between the ques-
tion head word film and the dictionary named
film, each noun phrase in the candidate sentence
is checked. However, no dictionary entities have
been found from the candidate sentence Best plays
Jar Jar Binks, a floppy-eared, two-legged creature
in ?Star Wars: Episode I ? The Phantom Men-
ace?, although there is movie entitled Star Wars
Episode I: The Phantom Menace in the dictionary.
Notice that Star Wars: Episode I ? The Phantom
Menace in the sentence and the dictionary entity
Star Wars Episode I: The Phantom Menace do not
have exactly identical spelling. The use of prox-
imity look up which allows edit distance being less
than 10% error can resolve this.
5.3 Numerical entities presence feature
There are so far no match for question types of
NUM (as shown in Table 1) including NUM:count
and NUM:date etc. These types of questions
seek the numerical answers such as the amount of
money and the duration of period. It is natural to
compile regular expression patterns to match such
entities. For example, for a NUM:money typed
question What is Rohm and Haas?s annual rev-
enue, we compile NUM:money regular expression
pattern which matches the strings of number fol-
lowed by a currency sign ($ and dollars etc). Such
pattern is able to identify 4 billion $ as a candidate
answer in the candidate sentence Rohm and Haas,
with 4 billion $ in annual sales... There are 13 pat-
terns compiled to cover all numerical types. We
create a binary feature NUM to indicate the pres-
ence of possible numerical answers in a sentence.
5.4 Specific features
Specific features are question dependent. For ex-
ample, for question When was James Dean born,
any candidate sentence matches the pattern James
Dean (number - number) is likely to answer such
question. We create a binary feature SPE to indi-
cate the presence of such match between a ques-
546
tion and a candidate sentence. We list all question
and sentence match patterns which are used in our
experiments as following:
when born feature 1 The question begins with when is/was
and follows by a person name and then follows by key
word born; The candidate sentence contains such per-
son name which follows by the pattern of (number -
number).
when born feature 2 The question begins with when is/was
and follows by a person name and then follows by key
word born; The candidate sentence contains such per-
son name, a NUM:date entity, and a key word born.
where born feature 1 The question begins with where
is/was and follows by a person name and then follows
by key word born; The candidate sentence contains
such person name, a NER LOC entity, and a key word
born.
when die feature 1 The question begins with when did and
follows by a person name and then follows by key word
die; The candidate sentence contains such person name
which follows by the pattern of (number - number).
when die feature 2 The question begins with when did and
follows by a person name and then follows by key
word die; The candidate sentence contains such person
name, a NUM:date entity, and a key word died.
how many feature The question begins with how many and
follows by a noun; The candidate sentence contains a
number and then follows by such noun.
cooccurrent Feature This feature takes two phrase argu-
ments, if the question contains the first phrase and the
candidate sentence contains the second, such feature
would be activated.
Note that the construction of specific features
require the access to aforementioned extracted
named entities. For example, the when born fea-
ture 2 pattern needs the information whether a
candidate sentence contains a NUM:date entity
and where born feature 1 pattern needs the in-
formation whether a candidate sentence contains
a NER LOC entity. Note also that the patterns of
when born feature and when die feature have
similar structure and thus can be simplified in im-
plementation. How many feature can be used
to identify the sentence Amtrak annually serves
about 21 million passengers for question How
many passengers does Amtrak serve annually. The
cooccurrent feature is the most general one. An
example of cooccurrent feature would take the
arguments of marry and husband, or marry and
wife. Such feature would be activated for ques-
tion Whom did Eileen Marie Collins marry and
candidate sentence ... were Collins? husband,
Pat Youngs, an airline pilot... It is worth noting
that the two arguments are not necessarily differ-
ent. For example, they could be both established,
which makes such feature activated for question
When was the IFC established and candidate sen-
tence IFC was established in 1956 as a member of
the World Bank Group. The reason why we use the
cooccurrence of the word established is due to its
main verb role, which may carry more information
than other words.
5.5 Dependency validity features
Like (Cui et al, 2004), we extract the dependency
path from the question word to the common word
(existing in both question and sentence), and the
path from candidate answer (such as CoNLL NE
and numerical entity) to the common word for
each pair of question and candidate sentence using
Stanford dependency parser (Klein and Manning,
2003; Marneffe et al, 2006). For example, for
question When did James Dean die and candidate
sentence In 1955, actor James Dean was killed in
a two-car collision near Cholame, Calif., we ex-
tract the pathes of When:advmod:nsubj:Dean and
1955:prep-in:nsubjpass:Dean for question and
sentence respectively, where advmod and nsubj
etc. are grammatical relations. We propose the
dependency validity feature (DEP) as following.
For all paired paths between a question and a can-
didate sentence, if at least one pair of path in which
all pairs of grammatical relations have been seen
in the training, then the DEP feature is set to be
true, false otherwise. That is, the true validity fea-
ture indicates that at least one pair of path between
the question and candidate sentence is possible to
be a true pair (ie, the candidate noun phrase in the
sentence path is the true answer).
6 Question Answer Experiments
Recall that most of the question answer features
depend on the question classifier. For instance,
the NE feature checks the presence or absence of
CoNLL style named entities subject to the clas-
sified question type. In this section, we evaluate
how the quality of question classifiers affects the
question answering performance.
6.1 Experiment setup
We use TREC99-03 factoid questions for training
and TREC04 factoid questions for testing. To fa-
cilitate the comparison to others work (Cui et al,
2004; Shen and Klakow, 2006), we first retrieve
all relevant documents which are compiled by Ken
Litkowski8 to create training and test datasets. We
8Available at http://trec.nist.gov/data/qa.html.
547
then apply key word search for each question and
retrieve the top 20 relevant sentences. We create
a feature represented data point using each pair of
question and candidate sentence and label it either
true or false depending on whether the sentence
can answer the given question or not. The labeling
is conducted by matching the gold factoid answer
pattern against the candidate sentence.
There are two extra steps performed for train-
ing set but not for test data. In order to construct
a high quality training set, we manually check the
correctness of the training data points and remove
the false positive ones which cannot support the
question although there is a match to gold answer.
In addition, in order to keep the training data well
balanced, we keep maximum four false data points
(question answer pair) for each question but no
limit over the true label data points. In doing so,
we use 1458 questions to compile 8712 training
data points and among them 1752 have true labels.
Similarly, we use 202 questions to compile 4008
test data points and among them 617 have true la-
bels.
We use the training data to train a maximum
entropy model and use such model to rank test
data set. Compared with a classification task (such
as the question classifier), the ranking process re-
quires one extra step: For data points which share
the same question, the probabilities of being pre-
dicted as true label are used to rank the data points.
In align with the previous work, performance is
evaluated using mean reciprocal rank (MRR), top
1 prediction accuracy (top1) and top 5 prediction
accuracy (top5). For the test data set, 157 among
the 202 questions have correct answers found in
retrieved sentences. This leads to the upper bound
of MRR score being 77.8%.
To evaluate how the quality of question clas-
sifiers affects the question answering, we have
created three question classifiers: QC1, QC2
and QC3. The features which are used to train
these question classifiers and their performance
are shown in Table 5. Note that QC3 is the best
question classifier we obtained in Section 4.
Table 5: Features used to train and the perfor-
mance of three question classifiers.
Name features 6 class 50 class
QC1 wh-word 46.0 46.8
QC2 wh-word+ head 92.2 82.0
QC3 All 93.6 89.0
6.2 Experiment results
The first experiment is to evaluate the individ-
ual contribution of various features derived using
three question classifiers. Table 6 shows the base-
line result and results using DIC, NE, NE-4, REG,
SPE, and DEP features. The baseline is the key
word search without the use of maximum entropy
model. As can be seen, the question classifiers
do not affect the DIC feature at all, as DIC fea-
ture does not depend on question classifiers. Bet-
ter question classifier boosts considerable gain for
NE, NE-4 and REG in their contribution to ques-
tion answering. For example, the best question
classifier QC3 outperforms the worst one (QC1)
by 1.5%, 2.0%, and 2.0% MRR scores for NE,
NE-4 and REG respectively. However, it is sur-
prising that the MRR and top5 contribution of NE
and NE-4 decreases if QC1 is replaced by QC2, al-
though the top1 score results in performance gain
slightly. This unexpected results can be partially
explained as follows. For some questions, even
QC2 produces correct predictions, the errors of
NE and NE-4 features may cause over-confident
scores for certain candidate sentences. As SPE and
DEP are not directly dependent on question clas-
sifier, their individual contribution only changes
slightly or remains the same for different ques-
tion classifiers. If the best question classifier is
used, the most important features are SPE and
REG, which can individually boost the MRR score
over 54%, while the others result in less significant
gains.
We now incrementally use various features and
the results are show in Table 6 as well. As can
be seen, the more features and the better question
classifier are used, the higher performance the ME
model has. The inclusion of REG and SPE results
in significant boost for the performance. For ex-
ample, if the best question classifier QC3 is used,
the REG results in 6.9% and 8% gain for MRR
and top1 scores respectively. This is due to a large
portion of NUM type questions in test dataset. The
SPE feature contributes significantly to the per-
formance due to its high precision in answering
birth/death time/location questions. NE and NE-4
result in reasonable gains while DEP feature con-
tributes little. However, this does not mean that
DEP is not important, as once the model reaches a
high MRR score, it becomes hard to improve.
Table 6 clearly shows that the question type
classifier plays an essential role in a high perfor-
548
Table 6: Performance of individual and incremental feature sets for three question classifiers.
Individual
Feature MRR Top1 Top5
QC1 QC2 QC3 QC1 QC2 QC3 QC1 QC2 QC3
Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4
DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4
NE 48.5 47.5 50.0 40.6 40.6 42.6 61.9 60.9 63.4
NE-4 49.5 48.5 51.5 41.6 42.1 44.6 62.4 61.9 64.4
REG 52.0 54.0 54.0 44.1 47.0 47.5 64.4 65.3 65.3
SPE 55.0 55.0 55.0 48.5 48.5 48.5 64.4 64.4 64.4
DEP 51.0 51.5 52.0 43.6 44.1 44.6 65.3 65.8 65.8
Incremental
Baseline 49.9 49.9 49.9 40.1 40.1 40.1 59.4 59.4 59.4
+DIC 49.5 49.5 49.5 42.6 42.6 42.6 60.4 60.4 60.4
+NE 50.0 48.5 51.0 43.1 42.1 44.6 62.9 61.4 64.4
+NE-4 51.5 50.0 53.0 44.1 43.6 46.0 63.4 62.9 65.8
+REG 55.0 56.9 59.9 48.0 51.0 54.0 68.3 68.8 71.8
+SPE 60.4 62.4 65.3 55.4 58.4 61.4 70.8 70.8 73.8
+DEP 61.4 62.9 66.3 55.9 58.4 62.4 71.8 71.8 73.8
mance question answer system. Assume all the
features are used, the better question classifier sig-
nificantly boosts the overall performance. For ex-
ample, the best question classifier QC3 outper-
forms the worst QC1 by 4.9%, 6.5%, and 2.0%
for MRR, top1 and top5 scores respectively. Even
compared to a good question classifier QC2, the
gain of using QC3 is still 3.4%, 4.0% and 2.0%
for MRR, top1 and top5 scores respectively. One
can imagine that if a fine grained NER is available
(rather than the current four type coarse NER), the
potential gain is much significant.
The reason that the question classifier affects
the question answering performance is straightfor-
ward. As a upstream source, the incorrect classi-
fication of question type would confuse the down-
stream answer search process. For example, for
question What is Rohm and Haas?s annual rev-
enue, our best question classifier is able to clas-
sify it into the correct type of NUM:money and
thus would put $ 4 billion as a candidate answer.
However, the inferior question classifiers misclas-
sify it into HUM:ind type and thereby could not
return a correct answer. Figure 1 shows the indi-
vidual MRR scores for the 42 questions (among
the 202 test questions) which have different pre-
dicted question types using QC3 and QC2. For al-
most all test questions, the accurate question clas-
sifier QC3 achieves higher MRR scores compared
to QC2.
Table 7 shows performance of various question
answer systems including (Tanev et al, 2004; Wu
et al, 2005; Cui et al, 2004; Shen and Klakow,
0 5 10 15 20 25 30 35 40 45
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
question id
M
R
R
 
 
QC3
QC2
Figure 1: Individual MRR scores for questions
which have different predicted question types us-
ing QC3 and QC2.
2006) and this work which were applied to the
same training and test datasets. Among all the sys-
tems, our model can achieve the best MRR score
of 66.3%, which is close to the state of the art of
67.0%. Considering the question answer features
used in this paper are quite standard, the boost is
mainly due to our accurate question classifier.
Table 7: Various system performance comparison.
System MRR Top1 Top5
Tanev et al 2004 57.0 49.0 67.0
Cui et al 2004 60.0 53.0 70.0
Shen and Klakow, 2006 67.0 62.0 74.0
This work 66.3 62.4 73.8
549
7 Conclusion
In this paper, we have presented a question clas-
sifier which makes use of a compact yet effi-
cient feature set. The question classifier outper-
forms previous question classifiers over the stan-
dard UIUC question dataset. We further investi-
gated quantitatively how the quality of question
classifier impacts the performance of question an-
swer system. The experiments showed that an ac-
curate question classifier plays an essential role
in question answering system. With our accurate
question classifier and some standard question an-
swer features, our question answering system per-
forms close to the state of the art.
Acknowledgments
We wish to thank the three anonymous review-
ers for their invaluable comments. This re-
search was supported by British Telecom grant
CT1080028046 and BISC Program of UC Berke-
ley.
References
A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra. 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. PhD thesis, University of
Pennsylvania.
H. Cui, K Li, R. Sun, T. Chua, and M. Kan. 2004. Na-
tional university of singapore at the trec-13 question
answering. In Proc. of TREC 2004, NIST.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
ACL, pages 363-370.
D. Hacioglu and W. Ward. 2003. Question classifica-
tion with support vector machines and error correct-
ing codes. In Proc. of the ACL/HLT, vol. 2, pages
28?30.
Z. Huang, M. Thint, and Z. Qin. 2008. Question clas-
sification using head words and their hypernyms. In
Proc. of the EMNLP.
D. Klein and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In Proc. of ACL 2003, vol. 1, pages
423?430.
V. Krishnan, S. Das, and S. Chakrabarti. 2005. En-
hanced answer type inference from questions using
sequential models. In Proc. of the HLT/EMNLP.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In ACM Special Inter-
est Group for Design of Communication Proceed-
ings of the 5th annual international conference on
Systems documentation, pages 24?26.
X. Li and D. Roth. 2002. Learning question classi-
fiers. In the 19th international conference on Com-
putational linguistics, vol. 1, pages 1-7.
X. Li and D. Roth. 2006. Learning question classifiers:
the role of semantic information. Natural Language
Engineering, 12(3):229?249.
C. D. Manning and D. Klein. 2003. Optimization,
maxent models, and conditional estimation with-
out magic. Tutorial at HLT-NAACL 2003 and ACL
2003.
M. D. Marneffe, B. MacCartney and C. D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC 2006.
A. Moschitti, S. Quarteroni, R. Basili and S. Manand-
har 2007. Exploiting syntactic and shallow seman-
tic kernels for question answer classification. In
Proc. of ACL 2007, pages 776-783.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of the HLT-NAACL.
J. Prager. 2006. Open-domain question-answering.
In Foundations and Trends in Information Retrieval,
vol. 1, pages 91-231, 2006.
N. Schlaefer, J. Ko, J. Betteridge, G. Sautter, M. Pathak
and E. Nyberg. 2007. Semantic extensions of the
Ephyra QA system for TREC 2007. In Proc. of the
TREC 2007.
D. Shen and D. Klakow. 2006. Exploring correlation
of dependency relation paths for answer extraction.
In Proc. of the ACL 2006.
H. Tanev, M. Kouylekov, and B. Magnini. 2004.
Combining linguistic processing and web mining for
question answering: Itc-irst at TREC-2004. In Proc.
of the TREC 2004, NIST.
E. M. Voorhees and H. T. Dang. 2005. Overview of
the TREC 2005 question answering track. In Proc.
of the TREC 2005, NIST.
M. Wu, M. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. University at Albanys ILQUA in
TREC 2005. In Proc. of the TREC 2005, NIST.
D. Zhang and W. S. Lee. 2003. Question classification
using support vector machines. In The ACM SIGIR
conference in information retrieval, pages 26?32.
550
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1232?1240,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Accurate Semantic Class Classifier for Coreference Resolution
Zhiheng Huang1, Guangping Zeng1,2, Weiqun Xu3, and Asli Celikyilmaz1
1EECS Department, University of California at Berkeley,
CA 94720, USA
{zhiheng,gpzeng,asli}@eecs.berkeley.edu
2Computer Science Department, School of Information Engineering,
University of Science and Technology Beijing, China
3ThinkIT, Institute of Acoustics, Chinese Academy of Sciences,
Beijing, 100190, China
xuweiqun@hccl.ioa.ac.cn
Abstract
There have been considerable attempts
to incorporate semantic knowledge into
coreference resolution systems: different
knowledge sources such as WordNet and
Wikipedia have been used to boost the per-
formance. In this paper, we propose new
ways to extract WordNet feature. This
feature, along with other features such as
named entity feature, can be used to build
an accurate semantic class (SC) classifier.
In addition, we analyze the SC classifica-
tion errors and propose to use relaxed SC
agreement features. The proposed accu-
rate SC classifier and the relaxation of SC
agreement features on ACE2 coreference
evaluation can boost our baseline system
by 10.4% and 9.7% using MUC score and
anaphor accuracy respectively.
1 Introduction
Coreference resolution is used to determine which
noun phrases (including pronouns, proper names,
and common nouns) refer to the same entities in
documents. Much work on coreference resolution
is based on (Soon et al, 2001), which built a de-
cision tree classifier to label pairs of mentions as
coreferent or not. Recent work aims to improve
the performance from two aspects: new models
and new features. The former cast the pair wise
mention classifications into various forms such as
the best path in a Bell tree (Luo et al, 2004), the
best graph cut (Nicolae and Nicolae, 2006), in-
teger linear programming (Denis and Baldridge,
2007) and graph partition based conditional model
(McCallum and Wellner, 2004). The latter de-
velop and investigate new linguistic features for
the problem. For instance, WordNet (Poesio et al,
2004), Wikipedia (Ponzetto and Strube, 2006), se-
mantic neighbor words (Ng, 2007a), and pattern
based features (Yang and Su, 2007) have been ex-
tensively studied.
Deeper linguistic knowledge is required to en-
able the coreference resolution to reach a higher
level of performance (Kehler et al, 2004). An im-
portant type of semantic knowledge that has been
employed in coreference resolution system is the
semantic class (SC) of an NP, which can be used
to filter out the coreference between semantically
incompatible NPs. However, the difficulty is to
accurately compute the semantic class features. In
this paper, we show that the WordNet may not be
efficiently employed in the traditional way such
as (Soon et al, 2001; Ng, 2007a; Ponzetto and
Strube, 2006) to compute the semantic class fea-
tures. We introduce new ways to use the WordNet
and the experiments show its effectiveness in de-
termining the semantic classes for noun phrases.
In addition, we analyze the classification errors of
the SC classifier and propose to use relaxed SC
agreement features. With these proposed features
and other standard syntactic features (which are
commonly employed in existing coreference sys-
tems), our coreference resolution system can ob-
tain an increase of 10.4% for MUC score and 9.7%
for anaphor accuracy from the baseline in ACE2
evaluation.
2 Related Work
WordNet (Fellbaum, 1998) as an important knowl-
edge source has been widely employed in previ-
ous coreference resolution work. For example,
Harabagiu et al (2001) have used WordNet rela-
tions such as synonym and is-a to mine the pat-
terns of WordNet paths for pairs of antecedents
and anaphors. Due to the nature of the rule based
coreference system (in contrast to machine learn-
ing based), the weights of relations may not be
accurately estimated. Vieira and Poesio (2000)
and Markert and Nissim (2005) have used Word-
Net synonym and hyponym etc. to determine if
an anaphor semantically relates to one previous
NP. Ponzetto and Strube (2006) have used Word-
Net semantic similarity and relatedness scores be-
tween antecedents and candidate anaphors. Their
1232
work is different to this work in the following: 1)
Their work involves various relations such as hy-
ponyms and meronyms while ours only makes use
of hypernyms; and 2) Their work focuses on in-
vestigating if two NPs have particular WordNet re-
lations or not, while ours focuses on using Word-
Net hypernyms for their SC classification and then
testing their SC compatibility. In doing so, we can
directly model the accuracy of semantic class clas-
sification and test its impact on coreference reso-
lution.
While the SC of a proper name is computed
fairly accurately using a named entity (NE) recog-
nizer, many coreference resolution systems sim-
ply assign to a common noun the first (i.e., most
frequent) WordNet synset as its SC (Soon et al,
2001; Markert and Nissim, 2005). This heuris-
tics, apparently, did not lead to good performance.
The best reported ACE2 coreference resolution
system (Ng, 2007a; Ng, 2007b) has proposed an
accurate SC classifier which used heterogeneous
semantic knowledge sources. WordNet is just
one of the several knowledge sources which have
been utilized. However, the WordNet based fea-
tures is not informative compared to other features
such as the semantic neighbor feature. Similarly,
Ponzetto and Strube (2006) have discovered that
the WordNet feature is no more informative than
the community-generated Wikipedia feature. In
this paper, we focus on the investigation of vari-
ous usages of WordNet for the SC classification
task. The work which is directly comparable to
ours would be (Ng, 2007a; Ng, 2007b).
Other similar work includes the mention detec-
tion (MD) task (Florian et al, 2006) and joint
probabilistic model of coreference (Daume? III and
Marcu, 2005). The MD task identifies the bound-
ary of a mention, its mention type (e.g., pronoun,
name), and its semantic type (e.g., person, orga-
nization). Unlike them, we do not perform the
boundary detection, as we make use of the noun
phrases directly from the noun phrase chunker and
NE recognizer. The joint probabilistic model mod-
els the MD and coreference simultaneously, while
our work focuses on them separately.
3 Semantic Class Classification
In this section, we describe how we compile the
training corpus and extract features using Word-
Net. We report our results on the ACE coreference
corpus due to that it has been commonly used and
it was annotated SCs of six types.1 As in (Ng,
1Person, organization, gpe, location and facility are ex-
plicitly annotated. The rest noun phrases are other type.
2007a), we first train a classifier to predict the SC
of an NP. This SC information is used later in the
coreference resolution stage. For example, the au-
dience is classified as SC of person, and it thus
should not be coreferent with the security industry,
which is usually classified as organization. This
task is by no means trivial. First, while the classi-
fication of Tom Hanks being SC of person can be
accurately achieved by an NE recognizer, the as-
sociation of audience and person requires seman-
tic language source such as WordNet. Second, the
same noun phrase can be annotated with different
SCs under different context. For example, the au-
thorities is usually annotated as person, but it is
sometimes as organization. Even worse, the same
noun phrases are sometimes annotated with one of
the five explicitly annotated classes while some-
times are not annotated at all (thus falling into the
other SC). For example, people is annotated as
person SC explicitly 20 times and is not annotated
at all 21 times in the ACE2 testset. This inconsis-
tent annotation adversely affects the performance
of an SC classifier. And this in turn would cause
errors during coreference stage. In section 4.3, we
show how to relax the strict SC agreement feature
to address this.
3.1 Training instance creation
We use ACE Phase 2 Coreference corpus to train
the SC classifier. Each noun phrase which is iden-
tified by the noun phrase chunker or NE recognizer
is used to create a training instance. Each instance
is represented by a set of lexical, syntactic and se-
mantic features, as described below. If the NP un-
der consideration is annotated as one of the five
ACE SCs in the corpus, then the classification of
the associated training instance is the ACE SC of
the NP. Otherwise, the instance is labeled as other.
ACE 2 corpus has a training set and a test set
which comprise of 422 and 97 texts respectively.
We divide the training set into a new training and a
development set: the former consists of 90% ran-
domly generated and stratified original training in-
stances and the latter consists of the rest 10% in-
stances. The test set remains the same as in ACE2
corpus. The size of each dataset and its SC dis-
tributions are shown in Table 1. Note that the
training and development datasets have exactly the
same distributions of SCs due to the stratification
procedure. That is, each class has the same pro-
portion in training and development datasets. We
tune the feature parameters against development
set and report performance on both development
set and test set.
1233
Table 1: Distributions of SCs in ACE2 corpus.
Size PER ORG GPE FAC LOC OTH
Train 55629 20.29 7.30 8.42 0.61 0.55 62.80
Dev 6181 20.29 7.30 8.42 0.61 0.55 62.80
Test 15360 20.48 7.57 6.90 0.85 0.41 63.79
3.2 Lexical features
Each instance is represented as a bag of features
and is fed into a classifier in training stage. We
present four binary lexical feature sets as follows.
Word unigrams and bigrams: An N-gram is
a sub-sequence of N words from a given noun
phrase. Unigram forms the bag of words feature,
and bigram forms the pairs of words feature, and
so forth. We have considered word unigram and
bigram features in our experiments.
First and last words: This feature extracts the
first and last words of an NP. For example, the first
word the and the last word store are extracted from
the NP the main store. This feature does not only
coarsely models the influence of the first word, for
example, a or the, but also models the head word,
since the head word usually is the last word in the
NP.
Head word: We use Collins style rules
(Collins, 1999) to extract the head words for given
NPs. These features should be most informative
if the training corpus is large enough.2 For exam-
ple, the head word company of the NP the com-
pany immediately determines its SC being organi-
zation. However, due to the sparseness of training
data, its potential importance is adversely affected.
3.3 Semantic features
NE feature is extracted from Stanford named en-
tity recognizer (NER) (Finkel et al, 2005). Three
types of named entities: person, location and or-
ganization can be recognized for a given NP. This
feature is primarily useful for SC classification of
proper nouns.
WordNet is a large English lexicon in which se-
mantically related words are connected via cogni-
tive synonyms (synsets). The WordNet is a use-
ful tool for word semantics analysis and has been
widely used in natural language processing appli-
cations. In WordNet, synsets are organized into hi-
erarchies with hypernym/hyponym relationships:
Y is a hypernym of X if every X is a (kind of) Y
(X is called a hyponym of Y in this case).
The WordNet is employed in (Ng, 2007a) as
following to create the WN CLASS feature. For
each keyword w as shown in the right column of
2It, however, is mostly useful for nominal noun phrase and
not for the pronoun and proper noun phrases.
Table 2, if the head noun of a given NP is a hy-
ponym of w in WordNet,3 then the word w be-
comes a feature for such NP. It is explained that
these keywords are correlated with the ACE SCs
and they are obtained via experimentation with
WordNet and the ACE SCs of the NPs in the ACE
training data. However, it is likely that these hand-
crafted keywords have poor coverage for general
cases. As a result, it may not make full use of
WordNet semantic knowledge. This will be shown
in our individual feature contribution experiment
in Section 3.5.
Table 2: List of keywords used in WordNet seman-
tic feature in (Ng, 2007a).
ACE SC Keywords
PER person
ORG social group
FAC establishment, construction, building,
facility, workplace
GPE country, province, government, town, city,
administration, society, island, community
LOC dry land, region, landmass, body of water
geographical area, geological formation
There are other ways of using WordNet for se-
mantic feature extraction. For example, Ponzetto
and Strube (2006) have employed WordNet sim-
ilarity measure for coreference resolution. The
difference is that they created the feature di-
rectly at the coreference resolution stage, ie, us-
ing the WordNet similarity between the antecedent
and anaphor to determine if they are coreferent,
while we focus on using this feature to classify
an NP into a particular SC. For comparison, we
implemented a WordNet similarity based feature
(WN SIM) as follows: for a given NP head word
and a key word as listed in Table 2, the WordNet
similarity package (Seco et al, 2004) models the
length of path traveling from the head word to the
key word over the WordNet network. It then com-
putes the semantic similarity based on the path.
For example, the similarity between company and
social group is 0.77, while the similarity between
company and person is 0.59. The key word which
receives the highest similarity to the head word is
marked as a feature.
The WN CLASS feature may suffer from the
coverage problem and the WN SIM feature is
heavily dependent on the definition of similarity
metric which may turn out to be inappropriate for
coreference resolution task. To make better use of
WordNet knowledge, we attempt to directly intro-
duce hypernyms for the NP head words (we denote
3Only the first synset of the NP is used.
1234
it as WN HYP feature). The most similar work
to ours is (Daume? III and Marcu, 2005), in which
two most common synsets from WordNet for all
words in an NP and their hypernyms are extracted
as features. We avoid augmenting the hypernyms
for non-head words in the NP to prevent introduc-
ing noisy information, which may potentially cor-
rupt the hypernym feature space.
Considering a WordNet hypernym structure as
shown in Fig. 1 for the word company, its first
synset (an institution created to conduct business)
has a unique id of 08058098 and can also be rep-
resented by a set of description words (company
in this case). Its third synset (the state of being
with someone) has an id of 13929588 and descrip-
tion words of company, companionship, fellow-
ship, society. Each synset can be extended by its
hypernym synsets. For example, the direct hyper-
nym of the first synset is the synset of 08053576
which can be described as institution, establish-
ment. The augmentation of hypernyms for NP
head words can introduce useful information, but
can also bring noise if the head word or the synset
of head word are not correctly identified. For an
optimal use of WordNet hypernyms, four ques-
tions shall be addressed: 1) how many depths are
required to tradeoff the generality (thus more in-
formative) and the specificity (thus less noisy)? 2)
which synset of the given word is needed to be
augmented? 3) which representation (synset id or
synset word) is better? and 4) is it helpful to en-
code the hypernym depth into the hypernym fea-
ture?4 These four questions provide the guideline
to search the optimal use of WordNet. We will de-
sign experiments in Section 3.5 to determine the
optimal configuration of WN HYP feature.
state
(08008335)
(13931145)
(13928668)
(08053576)
(07950920)
company
depth 2
depth 3
depth 4
depth 1
social group
institution,establishment
organization,organisation
(00024720)
friendship,friendlyrelationship
fellowship,society
(13929588)(08058098)
company, companionship,
company
relationship
Figure 1: WordNet hypernym hierarchy for the
word company.
4For example, we encode the synset 08053576 as
08053576-1, with the last digit 1 indicating the depth of hy-
pernym with regard to the entry word company.
3.4 Learning algorithm
Maximum entropy (ME) models (Berger et al,
1996; Manning and Klein, 2003), also known as
log-linear and exponential learning models, has
been adopted in the SC classification task. Max-
imum entropy models can integrate features from
many heterogeneous information sources for clas-
sification. Each feature corresponds to a constraint
on the model. Given a training set of (C,D),
where C is a set of class labels and D is a set
of feature represented data points, the maximum
entropy model attempts to maximize the log like-
lihood
logP (C|D,?) =
?
(c,d)?(C,D)
log
exp
?
i
?
i
f
i
(c, d)
?
c
?
exp
?
j
?
j
f
i
(c, d)
(1)
where f
i
(c, d) are feature indicator functions and
?
i
are the parameters to be estimated. We use ME
models for both SC classification and mention pair
classification.
3.5 SC classification evaluation
We design three experiments to test the accuracy
of our classifiers. The first experiment evalu-
ates the individual contribution of different fea-
ture sets to SC classification accuracy. In par-
ticular, a ME model is trained on the 55,629
training instances using the following feature sets
separately: 1) unigram, 2) bigram, 3) first-last
word, 4) head word (HW), 5) named entities
(NE), 6) HW+WN CLASS, 7) HW+WN SIM,
and 8) variants of HW+WN HYP. Note that
HW+WN CLASS is the semantic feature used in
(Ng, 2007a), HW+WN SIM is the semantic fea-
ture using WordNet similarity measure (Seco et
al., 2004), and variants of HW+WN HYP are the
work proposed in this paper. We combine head
word and the semantic features due to the fact that
WordNet features are dependent on head words
and they could be treated as units. In the second
experiment, features are fed into the ME model
incrementally until all features have been used.5
Finally, we perform the feature ablation experi-
ments. That is, we remove one feature at a time
from the entire feature set and test the accuracy
loss. The SC classification performance is mea-
sured by accuracy, i.e., the proportion of the cor-
rectly classified instances among all test instances.
Individual feature contribution Table 3 shows
the SC classification accuracy of all NPs (all)
and non-pronoun NPs (non-PN) on the develop-
ment and test datasets using individual feature
5The optimal of HW+WN HYP configuration is used.
1235
sets. Among all the lexical features, unigram fea-
Table 3: SC classification accuracy of ME using
individual feature sets for development and test
ACE2 datasets.
Feature type dev test
all non-PN all non-PN
unigram 81.3 81.6 72.4 71.9
bigram 32.5 36.4 26.3 28.4
first-last word 80.1 80.2 71.6 71.0
HW 78.2 78.0 68.3 67.1
NE 74.0 82.8 73.1 81.9
HW+WN CLASS 79.5 79.4 70.3 69.5
HW+WN SIM 81.2 81.4 73.8 73.6
HW+WN HYP (1) 82.6 83.1 74.8 74.7
HW+WN HYP (3) 82.8 83.4 75.2 75.2
HW+WN HYP (6) 83.1 83.7 75.6 75.7
HW+WN HYP (9) 83.0 83.6 75.7 75.7
HW+WN HYP (?) 83.1 83.7 75.8 75.9
HW+WN HYP (6) 82.8 83.3 75.6 75.7
word form
HW+WN HYP (6) 82.9 83.5 75.4 75.4
depth encoded
HW+WN HYP (6) 83.0 83.6 76.4 76.6
first synset
ture performs the best (81.3%) for all NPs over the
development dataset. The bigram feature performs
poorly due to the sparsity problem: NPs usually
consist of one to three words. The first-last word
feature effectively models the prefix words (such
as a and the) and the head words and thus obtains a
reasonably high accuracy of 80.1%. As mentioned
before, the head word feature may suffer from the
sparsity and it results in the accuracy of 78.2%.
We also list the accuracies for non-pronoun NP
SC classification, which are slightly different com-
pared to all NP SC classification except for bi-
gram, in which the accuracy has increased 3.9%.
Although Stanford NER performs well on
named entity recognition task, it results in ac-
curacy of 74.0% for all NP SC classification,
due to its inability to deal with pronouns such
as he and common nouns such as the govern-
ment. The removal of pronouns significantly
boosts its accuracy to 82.8%. The introduc-
tion of semantic feature HW+WN CLASS boosts
the performance to 79.5% compared to the head
word alone of 78.2%. This conforms to (Ng,
2007a) that only small gain can be achieved us-
ing WN CLASS feature. The HW+WN SIM
feature outperforms HW+WN CLASS and the
accuracy reaches 81.2%. For the variants of
HW+WN HYP, we first search the optimal depth.
This is performed by using all synsets for NP head
word, encoding the feature using synset id (rather
than synset word), and no hypernym depth is en-
coded in the features. We try various depths of
1, 3, 6, 9 and ?, with ? signifies that no depth
constraint is imposed. The optimal depth of 6 is
obtained with the accuracy of 83.1% over the de-
velopment dataset. We then fix the depth of 6 to try
using synset word as features, using synset id with
depth encoded as features, and using first synset
only. The results show that the optimum is to en-
code the features using hypernym synset id with-
out hypernym depth information and all synsets
are considered for hypernym extraction. This is
slightly different from the previous finding (Soon
et al, 2001; Lin, 1998b) that a coreference res-
olution system employing only the first WordNet
synset performs slightly better than that employ-
ing more than one synset.6 The best result reaches
the accuracy of 83.1%. Although the best seman-
tic feature only outperforms the best lexical fea-
ture by 1.8% on the development dataset, its gain
in the test dataset is more significant (3.2%, from
72.4% to 75.6%).
Incremental feature contribution Once we
use the training and development datasets to find
the optimal configuration of HW+WN HYP se-
mantic feature, we use all lexical features and the
optimal HW+WN HYP feature incrementally to
train an ME model over the combination of train-
ing and development datasets. Table 4 shows
the SC classification accuracy of all NPs (all)
and non-pronoun NPs (non-PN) on the train-
ing+development (we refer it as training hereafter)
and test datasets.
Table 4: SC classification accuracy of ME using
incremental feature sets for training and test ACE2
datasets.
Feature type train test
all non-PN all non-PN
HW 87.8 89.0 68.6 67.6
+WN HYP 87.8 89.0 75.7 75.8
+unigram 91.5 93.3 77.7 78.1
+bigram 93.1 95.2 78.7 79.2
+first-last word 93.2 95.3 78.8 79.3
+NE 93.4 95.6 83.1 84.4
Ng 2007a - 85.0 - 83.3
Note that the significant higher accuracies in
training compared to test are due to the overfit-
ting problem. The interesting evaluation thus re-
mains on the test data. As can be seen, the in-
clusion of more features results in higher perfor-
mance. This is more obvious in the test dataset
than in the training dataset. The inclusion of the
6In fact, the accuracy of the test data supports their claims.
The accuracy using the first synset compared to using all
synsets results in the accuracy increase from 75.6% to 76.4%
for all NPs over the test dataset.
1236
optimized WN HYP feature (ie, using all synsets?
hypernyms up to 6 depth and with synset id encod-
ing) results in 7.1% increase for all NP SC classifi-
cation over test data. This shows the effectiveness
of the WN HYP features to overcome the sparsity
of head word feature. The unigram, bigram and
first-last word features offer reasonable accuracy
gain, and the final inclusion of NE boosts the over-
all performance to 83.1% for all NP and 84.4% for
non pronoun NPs over test data. This result can
be directly compared to the SC classification ac-
curacy as reported in (Ng, 2007a), in which the
highest accuracy is 83.3% for non pronoun NPs.7
The large difference between the highest training
accuracies is due to that our classifier is trained di-
rectly on the ACE2 training dataset, while their SC
classifier was trained on BBN Entity Type Corpus
(Weischedel and Brunstein, 2005), which is five
times larger than the ACE2 corpus used by us. In
addition to WordNet, they have adopted multiple
knowledge sources which include BBN?s Identi-
Finder (this is equivalent to the Stanford NER in
our work), BLLIP corpus and Reuters Corpus,8
and dependency based thesaurus (Lin, 1998a). It
is remarkable that our SC classifier can achieve
even higher accuracy only using WordNet hyper-
nym and NE features. It is worth noting that the
small accuracy gain is indeed hard to achieve con-
sidering that the test data size is large (15360).
Feature ablation experiment We now perform
the feature ablation experiments to further deter-
mine the importance of individual features. We re-
move one feature at a time from the entire feature
set. Table 5 shows the SC classification accuracy
of all NPs (all) and non-pronoun NPs (non-PN) on
the training and test datasets respectively.
Table 5: SC classification accuracy of ME by re-
moving one feature at a time for training and test
ACE2 datasets.
Feature type train test
all non-PN all non-PN
overall 93.4 95.6 83.1 84.4
-HW 93.4 95.5 82.9 84.2
-WN HYP 93.4 95.5 82.6 83.8
-HW+WN HYP 93.4 95.5 82.3 83.5
-unigram 93.4 95.5 82.9 84.2
-bigram 92.5 94.5 82.7 84.0
-first-last word 93.4 95.5 82.9 84.1
-NE 93.2 95.3 78.8 79.3
Again, the significant higher accuracies in train-
ing compared to test are due to overfitting. The re-
7All NP accuracy was not reported as they excluded the
pronouns in creating their training and test data.
8They use these corpus to extract patterns to induce SC of
common nouns.
moval of NE feature results in the largest accuracy
loss of 4.3% (from 83.1% to 78.8%) for all nouns
on test data. It follows WN HYP (0.5% loss) and
the bigram (0.4%). If we treat HW+WN HYP as
one feature, the removal of it results in accuracy
loss of 0.8% for all nouns on test data. The un-
igram, first-last word and head word each results
in the loss of 0.2%. The reason that the removal
of NE results in a much significant loss is due to
the fact that the NE feature is quite different from
other features. Its strength is to distinguish SCs for
proper names, while other features are more sim-
ilar (their targets are common nouns). The pro-
posed use of HW+WN HYP can bring 0.8% gain
on top of other features, higher than other informa-
tive lexical features including unigram and first-
last word.
3.6 Error analysis
A closer look at the errors produced by our SC
classifier reveals that the second probable label is
very likely to be the actual labels if the first proba-
ble one is wrong. In fact, if we allow the classifier
to predict two most probable labels and the clas-
sification is judged to be true if the actual label is
one of the two predictions, then the classification
accuracy increases from 83.1% to 96.4%. This
is because that the same noun phrases are some-
times annotated with one of the five explicitly an-
notated classes while sometimes are not annotated
at all (thus falling into the other SC). Again for
the example of people. It is annotated as person
SC 20 times and is not annotated at all 21 times.
Given the same feature set for this instance, the
best the classifier can do is to classify it to other
semantic class. To address this annotation incon-
sistency issue, we relax the SC agreement feature
from the strict match in designing coreference res-
olution features. For example, if the first probable
SC of an NP matches the second probable SC of
another NP, we still give some partial match credit.
4 Application to Coreference Resolution
We can now incorporate the NP SC classifier into
our ME based coreference resolution system. This
section examines how our WordNet hypernym fea-
tures help improve the coreference resolution per-
formance.
4.1 Experimental setup
We use the ACE-2 (version 1.0) coreference cor-
pus. Each raw text in this corpus was prepro-
cessed automatically by a pipeline of NLP com-
ponents, including sentence boundary detection,
1237
POS-tagging and text chunking. The statistics of
corpus and mention extraction are shown in Table
6, where g-mention is the automatically extracted
mentions which contain the annotated (gold) men-
tions. The recalls of gold mentions are 95.88%
and 95.93% for training and test data respectively.
Table 6: Statistics for corpus and extracted men-
tions.
text# mention# g-mention# gold# recall(%)
train 422 61810 22990 23977 95.88
test 97 15360 5561 5797 95.93
Our coreference system uses Maximum En-
tropy model to determine whether two NPs are
coreferent. As in (Soon et al, 2001; Ponzetto and
Strube, 2006), we generate training instances as
follows: a positive instance is created for each
anaphoric NP, NP
j
, and its closest antecedent,
NP
i
; and a negative instance is created for NP
j
paired with each of the intervening NPs, NP
i+1
,
NP
i+2
, ..., NP
j?1
. Each instance is represented
by syntactic or semantic features described as fol-
lows. All training data are used to train a maxi-
mum entropy model. In the test stage,we select the
closest preceding NP that is classified as corefer-
ent with NP
j
as the antecedent of NP
j
. If no such
NP exists, no antecedent is selected for NP
j
.
Unlike other natural language processing tasks
such as information extraction which have de facto
evaluation metrics, it is an open question which
evaluation is the most suitable one. The evalu-
ation becomes more complicated when automat-
ically extracted mentions (in contrast to the gold
mentions) are used. To facilitate the comparison
with previous work, we report performance us-
ing two different scoring metrics: the commonly-
used MUC scorer (Vilain et al, 1995) and the ac-
curacy of the anaphoric references (Ponzetto and
Strube, 2006). An anaphoric reference is correctly
resolved if it and its closest antecedent are in the
same coreference chain in the resulting partition.
4.2 Baseline features
We briefly review the baseline features used in
this paper as follows. More detailed information
and implementations can be found at (Soon et al,
2001; Versley et al, 2008). For example, the
ALIAS feature takes values of true or false. The
value of true means that the antecedent and the
anaphor refer to the same entity (date, person, or-
ganization or location). The ALIAS feature de-
tection works differently depending on the named
entity type. For date, the day, month, and year
values are extracted and compared. For person,
the last words of the noun phrases are compared.
For organization names, the alias detection checks
for acronym match such as IBM and International
Business Machines Corp.
Lexical features STRING MATCH: true if
NP
i
and NP
j
have the same spelling after remov-
ing article and demonstrative pronouns, false oth-
erwise. ALIAS: true if NP
j
is the alias of NP
i
.
Grammatical features I PRONOUN: true if
NP
i
is a pronoun; J PRONOUN: true if NP
j
is pronoun; J REFL PRONOUN: true if NP
j
is
reflexive pronoun; J PERS PRONOUN: true if
NP
j
is personal pronoun; J POSS PRONOUN:
true if NP
j
is possessive pronoun; J PN: true
if NP
j
is proper noun; J DEF: true if NP
j
starts with the; J DEM: true if NP
j
starts with
this, that, these or those; J DEM NOMINAL:
true if NP
j
is a demonstrative nominal noun;
J DEM PRONOUN: true if NP
j
is a demonstra-
tive pronoun; PROPER NAME: true if both NP
i
and NP
j
are proper names; NUMBER: true if
NP
i
and NP
j
agree in number; GENDER: true
if NP
i
and NP
j
agree in gender; APPOSITIVE:
true if NP
i
and NP
j
are appositions.
Distance feature DISTANCE: how many sen-
tences NP
i
and NP
j
are apart.
Semantic feature SEMCLASS: This feature is
implemented from (Soon et al, 2001). Its possible
values are true, false, or unknown. First the fol-
lowing semantic classes are defined: female, male,
person, organization, location, date, time, money,
percent, and object. Each of these defined seman-
tic classes is then mapped to a WordNet synset.
Then the semantic class determination module de-
termines the semantic class for every NP as the
first synset of the head noun of the NP. If such
synset is a hyponym of defined semantic class,
then such semantic class is assigned to the NP.
Otherwise, unknown class is assigned. Finally, the
agreement of semantic classes of NP
i
and NP
j
is
unknown if either assigned class is unknown; true
if their assigned class are the same, false other-
wise. Notice that the WordNet use in (Ng, 2007a)
and this feature apply in the same principle except
that 1) the former is used in SC classification while
the latter is used directly for coreference resolu-
tion, and 2) they have different semantic class cat-
egories.
4.3 Proposed WordNet agreement features
For each instance which consists of NP
i
and NP
j
,
we apply our SC classifier to label them, say l
i
and
l
j
respectively. We then use these two induced la-
1238
bels to propose the SC agreement feature for NP
i
and NP
j
. In particular, SC STRICT is true if l
i
and l
j
are the same and they are not of other type,
false otherwise; SC COARSE is true if both l
i
and
l
j
are not of other type; In addition, we propose
two other SC agreement features to cope with the
SC classification errors. SC RELAX1 is true if the
first probable of NP
i
, l
i1
, is not other type and is
the same as the second probable of N
j
, l
j2
, or vice
visa. SC RELAX2 is true if the second probable
of NP
i
, l
i2
, is not other type and is the same as the
second probable label of NP
j
, l
j2
. The purpose in
using SC RELAX1 and SC RELAX2 features is
to relax the strict SC agreement feature in the hope
that partial SC match is useful for coreference res-
olution.
4.4 Coreference results
Table 7 shows the MUC score for ACE2 corpus
and its three partitions: bnews, npaper, and nwire
using baseline and the proposed semantic features.
It also shows the accuracy of resolving anaphors
for all nouns in ACE2 corpus. SC STRICT is
the configuration that uses the baseline features
with the SEMCLASS (Soon et al, 2001) replaced
by SC STRICT, and SC COARSE, SC RELAX1,
and SC RELAX2 are incrementally included into
the SC STRICT feature set.
As can be seen, the SC STRICT significantly
boosts the performance: it improves the MUC
F score and anaphor accuracy of baseline from
57.7% to 65.7% and 37.7% to 46.3% respectively.
It is remarkable that the new use of WordNet can
obtain such significant gain in both MUC score
and anaphor accuracy. The large improvement
of the precision from 58.1% to 73.3% for all
NPs shows that the SC STRICT feature can ef-
fectively filter out the semantic incompatible pairs
of antecedents and anaphors. In accordance with
our hypothesis, the relaxation of strict SC agree-
ment by including SC COARSE, SC RELAX1
and SC RELAX2 help improve the performance
further, which is reflected by both MUC score and
anaphor accuracy. For example, compared to the
baseline, the use of all proposed four SC agree-
ment features results in the maximal accuracy gain
of 9.7% (from 37.7% to 47.4%) and the use of
SC STRICT, SC COARSE, and SC RELAX1 re-
sults in the maximal MUC score gain of 10.4%
(from 57.7% to 68.1%).
Our best MUC score is 68.1% which outper-
forms the MUC score of 64.6% as reported in
(Ng, 2007a) by 3.5%, while our best accuracy
of anaphor is 47.4%, which is 4.1% less than
the accuracy of 51.5% in (Ng, 2007a). Note
that, unlike (Ng, 2007a) which performed exten-
sive experiments using different machine learn-
ing algorithms, alternative use of features (either
constraint or normal features), and heterogeneous
knowledge sources, this paper simply uses one
learning classifier (ME model) and only employs
WordNet and Stanford NER semantic sources.
The different MUC and accuracy scores reflect
the non-trivial cases of evaluating coreference sys-
tems. While we leave out the discussion of which
evaluation is more appropriate, we focus on show-
ing that the proposed SC classifier can bring sig-
nificant boost from the baseline using both MUC
and accuracy metrics.
5 Conclusion
We have showed that the traditional use of Word-
Net in coreference resolution may not effectively
exploit the WordNet semantic knowledge. We pro-
posed new ways to extract WordNet feature. This
feature, along with other features such as named
entity feature, can be used to build an accurate se-
mantic class (SC) classifier. In addition, we ana-
lyzed the classification errors of the SC classifier
and relaxed SC agreement features to cope with
part of the classification errors. The proposed ac-
curate SC classifier and the relaxation of SC agree-
ment features can boost our baseline coreference
resolution system by 10.4% and 9.7% using MUC
score and anaphor accuracy respectively.
Acknowledgments
We wish to thank Yannick Versley for his sup-
port with BART coreference resolution system and
the three anonymous reviewers for their invaluable
comments. This research was supported by British
Telecom grant CT1080028046 and BISC Program
of UC Berkeley.
References
A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
M. Collins 1999. Head-driven statistical models for
natural language parsing. PhD thesis, University of
Pennsylvania.
H. Daume? III and D. Marcu. 2005. A large-scale
exploration of effective global features for a joint
entity detection and tracking model. In Proc. of
HLT/EMNLP, pages 97-104.
1239
Table 7: MUC score and accuracy of baseline and proposed SC agreement features for ACE2 dataset.
MUC score Accuracy
All bnews npaper nwire All
System R P F R P F R P F R P F
baseline 57.4 58.1 57.7 56.6 55.4 56.0 59.3 60.4 59.9 56.2 58.6 57.3 37.7
Ng 2007a 59.5 70.6 64.6 - - - - - - - - - 51.5
SC STRICT 59.6 73.3 65.7 61.6 72.8 66.7 60.3 74.9 66.8 56.8 72.1 63.5 46.3
+ SC COARSE 59.2 76.7 66.8 61.0 76.7 67.9 59.8 77.2 67.4 56.6 76.2 64.9 45.9
+ SC RELAX1 59.8 79.0 68.1 61.3 79.8 69.3 60.9 80.3 69.3 57.2 76.7 65.5 47.2
+ SC RELAX2 60.2 77.7 67.8 61.5 78.2 68.9 61.4 78.9 69.1 57.5 75.7 65.4 47.4
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. In HLT-NAACL.
C. Fellbaum. 1998. An electronic lexical database.
The MIT press.
J. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by Gibbs sampling. In Proc. of
ACL, pages 363-370.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni.
2006. Factorizing complex models: a case study in
mention detection. In Proc. of COLING/ACL, pages
473-480.
S. M. Harabagiu, R. C. Bunescu, and S. J. Maiorano.
2001. Text and knowledge mining for coreference
resolution. In Proc. of NAACL, pages 55-62.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proc. of HLT/NAACL,
pages 289-296.
D. Lin. 1998a. Automatic retrieval and clustering
of similar words. In Proc. of COLING/ACL, pages
768-774.
D. Lin. 1998b. Using collocation statistics in informa-
tion extraction. In Proc. of MUC-7.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S.
Roukos. 2004. A mention synchronous coreference
resolution algorithm based on the Bell tree. In Proc.
of the ACL.
C. Manning and D. Klein. 2003. Optimization,
Maxent Models, and Conditional Estimation with-
out Magic. Tutorial at HLT-NAACL 2003 and ACL
2003.
K. Markert and M. Nissim. 2005. Comparing knowl-
edge sources for nominal anaphora resolution. Com-
putational Linguistics, 31(3):367-401.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proc. of the NIPS.
V. Ng. 2007a. Semantic Class Induction and Corefer-
ence Resolution. In Proc. of the ACL.
V. Ng. 2007b. Shallow Semantics for Coreference
Resolution. In Proc. of the IJCAI.
C. Nicolae and G. Nicolae 2006. BESTCUT: A Graph
Algorithm for Coreference Resolution. In Proc. of
the EMNLP.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004. Learning to resolve bridging references. In
Proc. of the ACL.
S. P. Ponzetto and M. Strube. 2006. Exploiting se-
mantic role labeling, WordNet and Wikipedia for
coreference resolution. In Proc. of the HLT/NAACL,
pages 192-199.
N. Seco, T. Veale, and J. Hayes. 2004. An Intrinsic
Information Content Metric for Semantic Similarity
in WordNet. Proc. of the European Conference of
Artificial Intelligence.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. A machine
learning approach to coreference resolution of noun
phrases. Computation Linguistics, 27(4):521-544.
Y. Versley, S. P. Ponzetto, M. Poesio, V. Eidelman, A.
Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: a modular toolkit for coreference resolution.
ACL 2008 System demo.
R. Vieira and M. Poesio. 2000. An empirically-based
system for processing definite descriptions. Compu-
tational Linguistics, 26(4):539-593.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L.
Hirschman. 1995. A model-theoretic coreference
scoreing scheme. In Proc. of MUC-6, pages 45-52.
R. Weischedel and A. Brunstein. 2005. BBN pronoun
coreference and entity type corpus. Linguistic Data
Consortium.
X. Yang and J. Su. 2007. Coreference resolution us-
ing semantic relatedness information from automat-
ically discovered pattens. In Proc. of the ACL.
1240
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 719?727,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Graph-based Semi-Supervised Learning for Question-Answering
Asli Celikyilmaz
EECS Department
University of California
at Berkeley
Berkeley, CA, 94720
asli@berkeley.edu
Marcus Thint
Intelligent Systems Research Centre
British Telecom (BT Americas)
Jacksonville, FL 32256, USA
marcus.2.thint@bt.com
Zhiheng Huang
EECS Department
University of California
at Berkeley
Berkeley, CA, 94720
zhiheng@eecs.berkeley.edu
Abstract
We present a graph-based semi-supervised
learning for the question-answering (QA)
task for ranking candidate sentences. Us-
ing textual entailment analysis, we obtain
entailment scores between a natural lan-
guage question posed by the user and the
candidate sentences returned from search
engine. The textual entailment between
two sentences is assessed via features rep-
resenting high-level attributes of the en-
tailment problem such as sentence struc-
ture matching, question-type named-entity
matching based on a question-classifier,
etc. We implement a semi-supervised
learning (SSL) approach to demonstrate
that utilization of more unlabeled data
points can improve the answer-ranking
task of QA. We create a graph for labeled
and unlabeled data using match-scores of
textual entailment features as similarity
weights between data points. We apply
a summarization method on the graph to
make the computations feasible on large
datasets. With a new representation of
graph-based SSL on QA datasets using
only a handful of features, and under lim-
ited amounts of labeled data, we show im-
provement in generalization performance
over state-of-the-art QA models.
1 Introduction
Open domain natural language question answer-
ing (QA) is a process of automatically finding an-
swers to questions searching collections of text
files. There are intensive research in this area
fostered by evaluation-based conferences, such as
the Text REtrieval Conference (TREC) (Voorhees,
2004), etc. One of the focus of these research, as
well as our work, is on factoid questions in En-
glish, whereby the answer is a short string that in-
dicates a fact, usually a named entity.
A typical QA system has a pipeline structure
starting from extraction of candidate sentences
to ranking true answers. In order to improve
QA systems? performance many research focus
on different structures such as question process-
ing (Huang et al, 2008), information retrieval
(Clarke et al, 2006), information extraction (Sag-
gion and Gaizauskas, 2006), textual entailment
(TE) (Harabagiu and Hickl, 2006) for ranking, an-
swer extraction, etc. Our QA system has a sim-
ilar pipeline structure and implements a new TE
module for information extraction phase of the QA
task. TE is a task of determining if the truth of a
text entails the truth of another text (hypothesis).
Harabagui and Hickl (2006) has shown that using
TE for filtering or ranking answers can enhance
the accuracy of current QA systems, where the an-
swer of a question must be entailed by the text that
supports the correctness of this answer.
We derive information from pair of texts, i.e.,
question as hypothesis and candidate sentence
as the text, potentially indicating containment of
true answer, and cast the inference recognition
as classification problem to determine if a ques-
tion text follows candidate text. One of the chal-
lenges we face with is that we have very lim-
ited amount of labeled data, i.e., correctly labeled
(true/false entailment) sentences. Recent research
indicates that using labeled and unlabeled data in
semi-supervised learning (SSL) environment, with
an emphasis on graph-based methods, can im-
prove the performance of information extraction
from data for tasks such as question classifica-
tion (Tri et al, 2006), web classification (Liu et
al., 2006), relation extraction (Chen et al, 2006),
passage-retrieval (Otterbacher et al, 2009), vari-
ous natural language processing tasks such as part-
of-speech tagging, and named-entity recognition
(Suzuki and Isozaki, 2008), word-sense disam-
719
biguation (Niu et al, 2005), etc.
We consider situations where there are much
more unlabeled data, XU , than labeled data, XL,
i.e., nL  nU . We construct a textual entail-
ment (TE) module by extracting features from
each paired question and answer sentence and de-
signing a classifier with a novel yet feasible graph-
based SSL method. The main contributions are:
? construction of a TE module to extract match-
ing structures between question and answer sen-
tences, i.e., q/a pairs. Our focus is on identifying
good matching features from q/a pairs, concerning
different sentence structures in section 2,
? representation of our linguistic system by a
form of a special graph that uses TE scores in de-
signing a novel affinity matrix in section 3,
? application of a graph-summarization method
to enable learning from a very large unlabeled and
rather small labeled data, which would not have
been feasible for most sophisticated learning tools
in section 4. Finally we demonstrate the results of
experiments with real datasets in section 5.
2 Feature Extraction for Entailment
Implementation of different TE models has pre-
viously shown to improve the QA task using su-
pervised learning methods (Harabagiu and Hickl,
2006). We present our recent work on the task of
QA, wherein systems aim at determining if a text
returned by a search engine contains the correct
answer to the question posed by the user. The ma-
jor categories of information extraction produced
by our QA system characterizes features for our
TE model based on analysis of q/a pairs. Here we
give brief descriptions of only the major modules
of our QA due to space limitations.
2.1 Pre-Processing for Feature Extraction
We build the following pre-processing modules
for feature extraction to be applied prior to our tex-
tual entailment analysis.
Question-Type Classifier (QC): QC is the task
of identifying the type of a given question among
a predefined set of question types. The type of
a question is used as a clue to narrow down the
search space to extract the answer. We used our
QC system presented in (Huang et al, 2008),
which classifies each question into 6-coarse cat-
egories (i.e., abbr., entity, human, location, num-
ber, description) as well as 50-fine categories (i.e.,
color, food, sport, manner, etc.) with almost
90% accuracy. For instance, for question ?How
many states are there in US??, the question-type
would be ?NUMBER? as course category, and
?Count? for the finer category, represented jointly
as NUM:Count. The QC model is trained via sup-
port vector machines (SVM) (Vapnik, 1995) con-
sidering different features such as semantic head-
word feature based on variation of Collins rules,
hypernym extraction via Lesk word disambigua-
tion (Lesk, 1988), regular expressions for wh-
word indicators, n-grams, word-shapes(capitals),
etc. Extracted question-type is used in connection
with our Named-Entity-Recognizer, to formulate
question-type matching feature, explained next.
Named-Entity Recognizer (NER): This com-
ponent identifies and classifies basic entities such
as proper names of person, organization, prod-
uct, location; time and numerical expressions such
as year, day, month; various measurements such
as weight, money, percentage; contact information
like address, web-page, phone-number, etc. This
is one of the fundamental layers of information
extraction of our QA system. The NER module
is based on a combination of user defined rules
based on Lesk word disambiguation (Lesk, 1988),
WordNet (Miller, 1995) lookups, and many user-
defined dictionary lookups, e.g. renown places,
people, job types, organization names, etc. During
the NER extraction, we also employ phrase analy-
sis based on our phrase utility extraction method
using Standford dependency parser ((Klein and
Manning, 2003)). We can categorize entities up
to 6 coarse and 50 fine categories to match them
with the NER types from QC module.
Phrase Identification(PI): Our PI module un-
dertakes basic syntactic analysis (shallow pars-
ing) and establishes simple, un-embedded linguis-
tic structures such as noun-phrases (NN), basic
prepositional phrases (PP) or verb groups (VG).
In particular PI module is based on 56 different
semantic structures identified in Standford depen-
dency parser in order to extract meaningful com-
pound words from sentences, e.g., ?They heard
high pitched cries.?. Each phrase is identified with
a head-word (cries) and modifiers (high pitched).
Questions in Affirmative Form: To derive lin-
guistic information from pair of texts (statements),
we parse the question and turn into affirmative
form by replacing the wh-word with a place-
holder and associating the question word with the
question-type from the QC module. For example:
720
?What is the capital of France?? is written in af-
firmative form as ?[X]LOC:City is the capital of
FranceLOC:Country.?. Here X is the answer text
of LOC:City NER-type, that we seek.
Sentence Semantic Component Analysis: Us-
ing shallow semantics, we decode the underlying
dependency trees that embody linguistic relation-
ships such as head-subject (H-S), head-modifier
(complement) (H-M), head-object (H-O), etc. For
instance, the sentence ?Bank of America acquired
Merrill Lynch in 2008.? is partitioned as:
? Head (H): acquired
? Subject (S): Bank of America[Human:group]
? Object (O): Merrill Lynch[Human:group]
? Modifier (M): 2008[Num:Date]
These are used as features to match components of
questions like ?Who purchased Merrill Lynch??.
Sentence Structure Analysis: In our question
analysis, we observed that 98% of affirmed ques-
tions did not contain any object and they are also
in copula (linking) sentence form that is, they
are only formed by subject and information about
the subject as: {subject + linking-verb + subject-
info.}. Thus, we investigate such affirmed ques-
tions different than the rest and call them copula
sentences and the rest as non-copula sentences. 1
For instance our system recognizes affirmed ques-
tion ? Fred Durst?s group name is [X]DESC:Def?.
as copula-sentence, which consists of subject (un-
derlined) and some information about it.
2.2 Features from Paired Sentence Analysis
We extract the TE features based on the above lex-
ical, syntactic and semantic analysis of q/a pairs
and cast the QA task as a classification problem.
Among many syntactic and semantic features we
considered, here we present only the major ones:
(1) (QTCF) Question-Type-Candidate Sen-
tence NER match feature: Takes on the value
?1? when the candidate sentence contains the fine
NER of the question-type, ?0.5? if it contains the
coarse NER or ?0? if no NER match is found.
(2) (QComp) Question component match fea-
tures: The sentence component analysis is applied
on both the affirmed question and the candidate
sentence pairs to characterize their semantic com-
ponents including subject(S), object(O), head (H)
and modifiers(M). We match each semantic com-
ponent of a question to the best matching com-
1One option would have been to leave out the non-copula
questions and build the model for only copula questions.
ponent of a candidate sentence. For example for
the given question, ?When did Nixon die??, when
the following candidate sentence, i.e., ?Richard
Nixon, 37th President of USA, passed away of
stroke on April 22, 1994.? is considered, we ex-
tract the following component match features:
? Head-Match: die?pass away
? Subject-Match: Nixon?Richard Nixon
? Object-Match: ?
? Modifier-Match: [X]?April 22, 1994
In our experiments we observed that converted
questions have at most one subject, head, object
and a few modifiers. Thus, we used one feature for
each and up to three for M-Match features. The
feature values vary based on matching type, i.e.,
exact match, containment, synonym match, etc.
For example, the S-Match feature will be ?1.0?
due to head-match of the noun-phrase.
(3) (LexSem) Lexico-Syntactic Alignment
Features: They range from the ratio of consecu-
tive word overlap between converted question (Q)
and candidate sentence (S) including
?Unigram/Bigram, selecting individual/pair of ad-
jacent tokens in Q matching with the S
?Noun and verb counts in common, separately.
?When words don?t match we attempt matching
synonyms in WordNet for most common senses.
?Verb match statistics using WordNet?s cause and
entailment relations.
As a result, each q/a pair is represented as a fea-
ture vector xi ? <d characterizing the entailment
information between them.
3 Graph Based Semi-Supervised
Learning for Entailment Ranking
We formulate semi-supervised entailment rank
scores as follows. Let each data point in
X = {x1, ..., xn}, xi ? <d represents infor-
mation about a question and candidate sentence
pair and Y = {y1, ..., yn} be their output la-
bels. The labeled part of X is represented with
XL = {x1, ..., xl} with associated labels YL =
{y1, ..., yl}
T . For ease of presentation we concen-
trate on binary classification, where yi can take
on either of {?1,+1} representing entailment or
non-entailment. X has also unlabeled part, XU =
{x1, ..., xu}, i.e., X = XL ? XU . The aim is to
predict labels for XU . There are also other testing
points, XTe, which has the same properties as X .
Each node V in graph g = (V,E) represents a
feature vector, xi ? <d of a q/a pair, characteriz-
721
ing their entailment relation information. When all
components of a hypothesis (affirmative question)
have high similarity with components of text (can-
didate sentence), then entailment score between
them would be high. Another pair of q/a sentences
with similar structures would also have high en-
tailment scores as well. So similarity between two
q/a pairs xi, xj , is represented with wij ? <n?n,
i.e., edge weights, and is measured as:
wij = 1?
d?
q=1
|xiq?xjq |
d (1)
As total entailment scores get closer, the larger
their edge weights would be. Based on our sen-
tence structure analysis in section 2, given dataset
can be further separated into two, i.e., Xcp con-
taining q/a pairs in which affirmed questions are
copula-type, and Xncp containing q/a pairs with
non-copula-type affirmed questions. Since cop-
ula and non-copula sentences have different struc-
tures, e.g., copula sentences does not usually have
objects, we used different sets of features for each
type. Thus, we modify edge weights in (1) as fol-
lows:
w?ij =
?
??????
??????
0 xi ? Xcp, xj ? Xncp
1?
dcp?
q=1
|xiq?xjq |
dcp
xi, xj ? Xcp
1?
dncp?
q=1
|xiq?xjq |
dncp
xi, xj ? Xncp
(2)
The diagonal degree matrix D is defined for graph
g by D=
?
j w?ij . In general graph-based SSL, a
function over the graph is estimated such that it
satisfies two conditions: 1) close to the observed
labels , and 2) be smooth on the whole graph by:
argminf
?
i?L
(fi ? yi)
2+?
?
i,j?L?U
w?ij(fi ? fj)
2
(3)
The second term is a regularizer to represent the
label smoothness, fTLf , where L = D?W is the
graph Laplacian. To satisfy the local and global
consistency (Zhou et al, 2004), normalized com-
binatorial Laplacian is used such that the second
term in (3) is replaced with normalized Laplacian,
L = D?1/2LD?1/2, as follows:
?
i,j?L?U
wij(
fi?
di
? fj?
dj
)2 = fTLf (4)
Setting gradient of loss function to zero, optimum
f?, where Y = {YL ? YU} , YU =
{
ynl+1 = 0
}
;
f? = (1+ ? (1? L))?1 Y (5)
Most graph-based SSLs are transductive, i.e., not
easily expendable to new test points outside L?U .
In (Delalleau et al, 2005) an induction scheme is
proposed to classify a new point xTe by
f?(xTe) =
?
i?L?U wxifi?
i?L?U wxi
(6)
Thus, we use induction, where we can, to avoid
re-construction of the graph for new test points.
4 Graph Summarization
Research on graph-based SSL algorithms point
out their effectiveness on real applications, e.g.,
(Zhu et al, 2003), (Zhou and Scho?lkopf, 2004),
(Sindhwani et al, 2007). However, there is still
a need for fast and efficient SSL methods to deal
with vast amount of data to extract useful informa-
tion. It was shown in (Delalleau et al, 2006) that
the convergence rate of the propagation algorithms
of SSL methods isO(kn2), which mainly depends
on the form of eigenvectors of the graph Laplacian
(k is the number of nearest neighbors). As the
weight matrix gets denser, meaning there will be
more data points with connected weighted edges,
the more it takes to learn the classifier function via
graph. Thus, the question is, how can one reduce
the data points so that weight matrix is sparse, and
it takes less time to learn?
Our idea of summarization is to create repre-
sentative vertices of data points that are very close
to each other in terms of edge weights. Suffice to
say that similar data points are likely to represent
denser regions in the hyper-space and are likely to
have same labels. If these points are close enough,
we can characterize the boundaries of these group
of similar data points with respect to graph and
then capture their summary information by new
representative vertices. We replace each data point
within the boundary with their representative ver-
tex, to form a summary graph.
4.1 Graph Summarization Algorithm
Let each selected dataset be denoted as Xs =
{xsi} , i = 1...m, s = 1, ..., q, where m is the
number of data points in the sample dataset and
q is the number of sample datasets drawn from
X . The labeled data points, i.e., XL, are ap-
pended to each of these selected Xs datasets,
Xs =
{
xs1, ...x
s
m?l
}
? XL. Using a separate
learner, e.g., SVM (Vapnik, 1995), we obtain pre-
dicted outputs, Y? s =
(
y?s1, ..., y?
s
m?l
)
ofXs and ap-
pend observed labels Y? s = Y? s ? YL.
722
Figure 1: Graph Summarization. (a) Actual data point with predicted class labels, (b) magnified view of
a single node (black) and its boundaries (c) calculated representative vertex, (d) summary dataset.
We define the weight W s and degree Ds ma-
trices of Xs using (1). Diagonal elements of Ds
is converted into a column vector and is sorted to
find the high degree vertices that are surrounded
with large number of close neighbors.
The algorithm starts from the highest degree
node xsi ? X
s, where initial neighbor nodes have
assumably the same labels. This is shown in Fig-
ure 1-(b) with the inner square around the mid-
dle black node, corresponding high degree node.
If its immediate k neighbors, dark blue colored
nodes, have the same label, the algorithm contin-
ues to search for the secondary k neighbors, the
light blue colored nodes, i.e., the neighbors of the
neighbors, to find out if there are any opposite la-
beled nodes around. For instance, for the corre-
sponding node (black) in Figure 1-(b) we can only
go up to two neighbors, because in the third level,
there are a few opposite labeled nodes, in red. This
indicates boundary Bsi for a corresponding node
and unique nearest neighbors of same labels.
Bsi =
{
xsi ?
{
xsj
}nm
j=1
}
(7)
In (7), nm denotes the maximum number of nodes
of aBsi and ?x
s
j , x
s
j? ? B
s
i , y
s
j = y
s
j? = yBsi , where
yBsi is the label of the selected boundary B
s
i .
We identify the edge weights wsij between each
node in the boundary Bsi via (1), thus the bound-
ary is connected. We calculate the weighted av-
erage of the vertices to obtain the representative
summary node of Bsi as shown in Figure 1-(c);
X
s
Bi =
?nm
i 6=j=1
1
2w
s
ij(x
s
i + x
s
j)
?nm
i 6=j=1w
s
ij
(8)
The boundaries of some nodes may only con-
tain themselves because their immediate neigh-
bors may have opposite class labels. Similarly
some may have only k + 1 nodes, meaning only
immediate neighbor nodes have the same labels.
For instance in Fig. 1 the boundary is drawn af-
ter the secondary neighbors are identified (dashed
outer boundary). This is an important indication
that some representative data points are better indi-
cators of class labels than the others due to the fact
that they represent a denser region of same labeled
points. We represent this information with the lo-
cal density constraints. Each new vertex is asso-
ciated with a local density constraint, 0 ? ?j ? 1,
which is equal to the total number of neighbor-
ing nodes used to construct it. We use the nor-
malized density constraints for ease of calcula-
tions. Thus, for a each sample summary dataset,
a local density constraint vector is identified as
?s = {?s1, ..., ?
s
nb}
T . The local density constraints
become crucial for inference where summarized
labeled data are used instead of overall dataset.
Algorithm 1 Graph Summary of Large Dataset
1: Given X = {x1, ..., xn} , X = XL ?XU
2: Set q ? max number of subsets
3: for s? 1, ..., q do
4: Choose a random subset with repetitions
5: Xs = {xs1, ..., x
s
m?l, xm?l+1, ..., xm}
6: Summarize Xs to obtain X
s
in (9)
7: end for
8: Obtain summary datasetX =
{
X
s}q
s=1
=
{
Xi
}p
i=1
and
local density constrains, ? = {?i}
p
i=1.
After all data points are evaluated, the sample
dataset Xs can now be represented with the sum-
mary representative vertices as
X
s
=
{
X
s
B1 , ..., X
s
Bnb
}
. (9)
and corresponding local density constraints as,
?s = {?s1, ..., ?
s
nb}
T , 0 < ?si ? 1 (10)
723
The summarization algorithm is repeated for each
random subset Xs, s = 1, ..., q of very large
dataset X = XL ? XU , see Algorithm 1. As
a result q number of summary datasets X
s
each
of which with nb labeled data points are com-
bined to form a representative sample of X , X =
{
X
s}q
s=1 reducing the number of data from n to
a much smaller number of data, p = q ? nb  n.
So the new summary of the X can be represented
with X =
{
Xi
}p
i=1. For example, an origi-
nal dataset with 1M data points can be divided
up to q = 50 random samples of m = 5000
data points each. Then using graph summariza-
tion each summarized dataset may be represented
with nb ?= 500 data points. After merging sum-
marized data, final summarized samples compile
to 500 ? 50 ?= 25K  1M data points, reduced to
1/40 of its original size. Each representative data
point in the summarized dataset X is associated
with a local density constraints, a p = q ? nb
dimensional row vector as ? = {?i}
p
i=1.
We can summarize a graph separately for dif-
ferent sentence structures, i.e., copula and non-
copula sentences. Then representative data points
from each summary dataset are merged to form fi-
nal summary dataset. The Hybrid graph summary
models in the experiments follow such approach.
4.2 Prediction of New Testing Dataset
Instead of using large dataset, we now use sum-
mary dataset with predicted labels, and local den-
sity constraints to learn the class labels of nte
number of unseen data points, i.e., testing data
points, XTe = {x1, ..., xnte}. Using graph-based
SSL method on the new representative dataset,
X ? = X ? XTe, which is comprised of sum-
marized dataset, X =
{
Xi
}p
i=1, as labeled data
points, and the testing dataset, XTe as unlabeled
data points. Since we do not know estimated lo-
cal density constraints of unlabeled data points, we
use constants to construct local density constraint
column vector for X ? dataset as follows:
?? = {1 + ?i}
p
i=1 ? [1 ... 1]
T ? <nte (11)
0 < ?i ? 1. To embed the local density con-
straints, the second term in (3) is replaced with the
constrained normalized Laplacian, Lc = ?TL?,
?
i,j?L?T
wij(
fi
?
??i ? di
?
fj
?
??j ? dj
)2 = fTLcf
(12)
If any testing vector has an edge between a labeled
vector, then with the usage of the local density
constraints, the edge weights will not not only be
affected by that labeled node, but also how dense
that node is within that part of the graph.
5 Experiments
We demonstrate the results from three sets of ex-
periments to explore how our graph representa-
tion, which encodes textual entailment informa-
tion, can be used to improve the performance of
the QA systems. We show that as we increase
the number of unlabeled data, with our graph-
summarization, it is feasible to extract information
that can improve the performance of QA models.
We performed experiments on a set of 1449
questions from TREC-99-03. Using the search en-
gine 2, we retrieved around 5 top-ranked candi-
date sentences from a large newswire corpus for
each question to compile around 7200 q/a pairs.
We manually labeled each candidate sentence as
true or false entailment depending on the contain-
ment of the true answer string and soundness of
the entailment to compile quality training set. We
also used a set of 340 QA-type sentence pairs from
RTE02-03 and 195 pairs from RTE04 by convert-
ing the hypothesis sentences into question form to
create additional set of q/a pairs. In total, we cre-
ated labeled training dataset XL of around 7600
q/a pairs . We evaluated the performance of graph-
based QA system using a set of 202 questions from
the TREC04 as testing dataset (Voorhees, 2003),
(Prager et al, 2000). We retrieved around 20 can-
didate sentences for each of the 202 test questions
and manually labeled each q/a pair as true/false en-
tailment to compile 4037 test data.
To obtain more unlabeled training data XU,
we extracted around 100,000 document headlines
from a large newswire corpus. Instead of match-
ing headline and first sentence of the document as
in (Harabagiu and Hickl, 2006), we followed a dif-
ferent approach. Using each headline as a query,
we retrieved around 20 top-ranked sentences from
search engine. For each headline, we picked the
1st and the 20th retrieved sentences. Our assump-
tion is that the first retrieved sentence may have
higher probability to entail the headline, whereas
the last one may have lower probability. Each of
these headline-candidate sentence pairs is used as
additional unlabeled q/a pair. Since each head-
2http://lucene.apache.org/java/
724
Features Model MRR Top1 Top5
Baseline ? 42.3% 32.7% 54.5%
QTCF SVM 51.9% 44.6% 63.4%
SSL 49.5% 43.1% 60.9%
LexSem SVM 48.2% 40.6% 61.4%
SSL 47.9% 40.1% 58.4%
QComp SVM 54.2% 47.5% 64.3%
SSL 51.9% 45.5% 62.4%
Table 1: MRR for different features and methods.
line represents a converted question, in order to
extract the question-type feature, we use a match-
ing NER-type between the headline and candidate
sentence to set question-type NER match feature.
We applied pre-processing and feature extrac-
tion steps of section 2 to compile labeled and un-
labeled training and labeled testing datasets. We
use the rank scores obtained from the search en-
gine as baseline of our system. We present the
performance of the models using Mean Recipro-
cal Rank (MRR), top 1 (Top1) and top 5 predic-
tion accuracies (Top5) as they are the most com-
monly used performance measures of QA systems
(Voorhees, 2004). We performed manual iterative
parameter optimization during training based on
prediction accuracy to find the best k-nearest pa-
rameter for SSL, i.e., k = {3, 5, 10, 20, 50} , and
best C =
{
10?2, .., 102
}
and ? =
{
2?2, .., 23
}
for RBF kernel SVM. Next we describe three dif-
ferent experiments and present individual results.
Graph summarization makes it feasible to exe-
cute SSL on very large unlabeled datasets, which
was otherwise impossible. This paper has no as-
sumptions on the performance of the method in
comparison to other SSL methods.
Experiment 1. Here we test individual con-
tribution of each set of features on our QA sys-
tem. We applied SVM and our graph based SSL
method with no summarization to learn models
using labeled training and testing datasets. For
SSL we used the training as labeled and testing
as unlabeled dataset in transductive way to pre-
dict the entailment scores. The results are shown
in Table 1. From section 2.2, QTCF represents
question-type NER match feature, LexSem is the
bundle of lexico-semantic features and QComp is
the matching features of subject, head, object, and
three complements. In comparison to the baseline,
QComp have a significant effect on the accuracy
of the QA system. In addition, QTCF has shown
to improve the MRR performance by about 22%.
Although the LexSem features have minimal se-
mantic properties, they can improve MRR perfor-
mance by 14%.
Experiment 2. To evaluate the performance of
graph summarization we performed two separate
experiments. In the first part, we randomly se-
lected subsets of labeled training dataset XiL ?
XL with different sample sizes, niL ={1% ? nL,
5% ? nL, 10% ? nL, 25% ? nL, 50% ? nL,
100% ? nL}, where nL represents the sample size
of XL. At each random selection, the rest of the
labeled dataset is hypothetically used as unlabeled
data to verify the performance of our SSL using
different sizes of labeled data. Table 2 reports
the MRR performance of QA system on testing
dataset using SVM and our graph-summary SSL
(gSum SSL) method using the similarity function
in (1). In the second part of the experiment, we
applied graph summarization on copula and non-
copula questions separately and merged obtained
representative points to create labeled summary
dataset. Then using similarity function in (2) we
applied SSL on labeled summary and unlabeled
testing via transduction. We call these models as
Hybrid gSum SSL. To build SVM models in the
same way, we separated the training dataset into
two based on copula and non-copula questions,
Xcp, Xncp and re-run the SVM method separately.
The testing dataset is divided into two accordingly.
Predicted models from copula sentence datasets
are applied on copula sentences of testing dataset
and vice versa for non- copula sentences. The pre-
dicted scores are combined to measure overall per-
formance of Hybrid SVM models. We repeated
the experiments five times with different random
samples and averaged the results.
Note from Table 2 that, when the number of
labeled data is small (niL < 10% ? nL), graph
based SSL, gSum SSL, has a better performance
compared to SVM. As the percentage of labeled
points in training data increase, the SVM perfor-
mance increases, however graph summary SSL is
still comparable with SVM. On the other hand,
when we build separate models for copula and
non-copula questions with different features, the
performance of the overall model significantly in-
creases in both methods. Especially in Hybrid
graph-Summary SSL, Hybrid gSum SSL, when
the number of labeled data is small (niL < 25% ?
nL) performance improvement is better than rest
725
% SVM gSum SSL Hybrid SVM Hybrid gSum SSL
#Labeled MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top5 MRR Top1 Top5
1% 45.2 33.2 65.8 56.1 44.6 72.8 51.6 40.1 70.8 59.7 47.0 75.2
5% 56.5 45.1 73.0 57.3 46.0 73.7 54.2 40.6 72.3 60.3 48.5 76.7
10% 59.3 47.5 76.7 57.9 46.5 74.2 57.7 47.0 74.2 60.4 48.5 77.2
25% 59.8 49.0 78.7 58.4 45.0 79.2 61.4 49.5 78.2 60.6 49.0 76.7
50% 60.9 48.0 80.7 58.9 45.5 79.2 62.2 51.0 79.7 61.3 50.0 77.2
100% 63.5 55.4 77.7 59.7 47.5 79.7 67.6 58.0 82.2 61.9 51.5 78.2
Table 2: The MRR (%) results of graph-summary SSL (gSum SSL) and SVM as well as Hybrid gSum
SSL and Hybrid SVM with different sizes of labeled data.
#Unlabeled MRR Top1 Top5
25K 62.1% 52.0% 76.7%
50K 62.5% 52.5% 77.2%
100K 63.3% 54.0% 77.2%
Table 3: The effect of number of unlabeled data
on MRR from Hybrid graph Summarization SSL.
of the models. As more labeled data is introduced,
Hybrid SVM models? performance increase dras-
tically, even outperforming the state-of-the art
MRR performance on TREC04 datasets presented
in (Shen and Klakow, 2006) i.e., MRR=67.0%,
Top1=62.0%, Top5=74.0%. This is due to the fact
that we establish two seperate entailment models
for copula and non-copula q/a sentence pairs that
enables extracting useful information and better
representation of the specific data.
Experiment 3. Although SSL methods are ca-
pable of exploiting information from unlabeled
data, learning becomes infeasible as the number
of data points gets very large. There are vari-
ous research on SLL to overcome the usage of
large number of unlabeled dataset challenge (De-
lalleau et al, 2006). Our graph summarization
method, Hybrid gsum SSL, has a different ap-
proach. which can summarize very large datasets
into representative data points and embed the orig-
inal spatial information of data points, namely lo-
cal density constraints, within the SSL summa-
rization schema. We demonstrate that as more la-
beled data is used, we would have a richer sum-
mary dataset with additional spatial information
that would help to improve the the performance
of the graph summary models. We gradually in-
crease the number of unlabeled data samples as
shown in Table 3 to demonstrate the effects on the
performance of testing dataset. The results show
that the number of unlabeled data has positive ef-
fect on performance of graph summarization SSL.
6 Conclusions and Discussions
In this paper, we applied a graph-based SSL al-
gorithm to improve the performance of QA task
by exploiting unlabeled entailment relations be-
tween affirmed question and candidate sentence
pairs. Our semantic and syntactic features for tex-
tual entailment analysis has individually shown to
improve the performance of the QA compared to
the baseline. We proposed a new graph repre-
sentation for SSL that can represent textual en-
tailment relations while embedding different ques-
tion structures. We demonstrated that summariza-
tion on graph-based SSL can improve the QA task
performance when more unlabeled data is used to
learn the classifier model.
There are several directions to improve our
work: (1) The results of our graph summarization
on very large unlabeled data is slightly less than
best SVM results. This is largely due to using
headlines instead of affirmed questions, wherein
headlines does not contain question-type and some
of them are not in proper sentence form. This ad-
versely effects the named entity match of question-
type and the candidate sentence named entities as
well as semantic match component feature extrac-
tion. We will investigate experiment 3 by using
real questions from different sources and construct
different test datasets. (2) We will use other dis-
tance measures to better explain entailment be-
tween q/a pairs and compare with other semi-
supervised and transductive approaches.
726
References
Jinxiu Chen, Donghong Ji, C. Lim Tan, and Zhengyu
Niu. 2006. Relation extraction using label propaga-
tion based semi-supervised learning. In Proceedings
of the ACL-2006.
Charles L.A. Clarke, Gordon V. Cormack, R. Thomas
Lynam, and Egidio L. Terra. 2006. Question an-
swering by passage selection. In In: Advances in
open domain question answering, Strzalkowski, and
Harabagiu (Eds.), pages 259?283. Springer.
Oliver Delalleau, Yoshua Bengio, and Nicolas Le
Roux. 2005. Efficient non-parametric function in-
duction in semi-supervised learning. In Proceedings
of AISTAT-2005.
Oliver Delalleau, Yoshua Bengio, and Nicolas Le
Roux. 2006. Large-scale algorithms. In In: Semi-
Supervised Learning, pages 333?341. MIT Press.
Sandra Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain ques-
tion answering. In In Proc. of ACL-2006, pages
905?912.
Zhiheng Huang, Marcus Thint, and Zengchang Qin.
2008. Question classification using headwords and
their hypernyms. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-08), pages 927?936.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the ACL-2003, pages 423?430.
Michael Lesk. 1988. They said true things, but called
them by wrong names - vocabulary problems in re-
trieval systems. In In Proc. 4th Annual Conference
of the University of Waterloo Centre for the New
OED.
Rong Liu, Jianzhong Zhou, and Ming Liu. 2006. A
graph-based semi-supervised learning algorithm for
web page classification. In Proc. Sixth Int. Conf. on
Intelligent Systems Design and Applications.
George Miller. 1995. Wordnet: A lexical database for
english. In Communications of the ACL-1995.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2005. Word sense disambiguation using labeled
propagation based semi-supervised learning. In
Proceedings of the ACL-2005.
Jahna Otterbacher, Gunes Erkan, and R. Radev
Dragomir. 2009. Biased lexrank:passage retrieval
using random walks with question-based priors. In-
formation Processing and Management, 45:42?54.
Eric W. Prager, John M.and Brown, Dragomir Radev,
and Krzysztof Czuba. 2000. One search engine or
two for question-answering. In Proc. 9th Text RE-
trieval conference.
Horacio Saggion and Robert Gaizauskas. 2006. Ex-
periments in passage selection and answer extrac-
tion for question answering. In Advances in natural
language processing, pages 291?302. Springer.
Dan Shen and Dietrich Klakow. 2006. Exploring cor-
relation of dependency relation paths for answer ex-
traction. In Proceedings of ACL-2006.
Vikas Sindhwani, Wei Chu, and S. Sathiya Keerthi.
2007. Semi-supervised gaussian process classifiers.
In Proceedings of the International Joint Conference
on Artificial Intelligence (IJCAI-07), pages 1059?
1064.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of the
ACL-2008.
Nguyen Thanh Tri, Nguyen Minh Le, and Akira Shi-
mazu. 2006. Using semi-supervised learning for
question classification. In ICCPOL, pages 31?41.
LNCS 4285.
Vilademir Vapnik. 1995. The nature of statistical
learning theory. In Springer-Verlag, New York.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proc. 12th Text RE-
trieval conference.
Ellen M. Voorhees. 2004. Overview of trec2004 ques-
tion answering track.
Dengyong Zhou and Bernhard Scho?lkopf. 2004.
Learning from labeled and unlabeled data using ran-
dom walks. In Proceedings of the 26th DAGM Sym-
posium, (Eds.) Rasmussen, C.E., H.H. Blthoff, M.A.
Giese and B. Schlkopf, pages 237?244, Berlin, Ger-
many. Springer.
Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Ja-
son Weston, and Bernhard Scho?lkopf. 2004. Learn-
ing with local and global consistency. Advances
in Neural Information Processing Systems, 16:321?
328.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahramani.
2003. Semi-supervised learning: From Gaus-
sian Fields to Gaussian processes. Technical Re-
port CMU-CS-03-175, Carnegie Mellon University,
Pittsburgh.
727
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 611?619,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Iterative Viterbi A* Algorithm forK-Best Sequential Decoding
Zhiheng Huang?, Yi Chang, Bo Long, Jean-Francois Crespo?,
Anlei Dong, Sathiya Keerthi and Su-Lin Wu
Yahoo! Labs
701 First Avenue, Sunnyvale
CA 94089, USA
{zhiheng huang,jfcrespo}@yahoo.com?
{yichang,bolong,anlei,selvarak,sulin}@yahoo-inc.com
Abstract
Sequential modeling has been widely used in
a variety of important applications including
named entity recognition and shallow pars-
ing. However, as more and more real time
large-scale tagging applications arise, decod-
ing speed has become a bottleneck for exist-
ing sequential tagging algorithms. In this pa-
per we propose 1-best A*, 1-best iterative A*,
k-best A* and k-best iterative Viterbi A* al-
gorithms for sequential decoding. We show
the efficiency of these proposed algorithms for
five NLP tagging tasks. In particular, we show
that iterative Viterbi A* decoding can be sev-
eral times or orders of magnitude faster than
the state-of-the-art algorithm for tagging tasks
with a large number of labels. This algorithm
makes real-time large-scale tagging applica-
tions with thousands of labels feasible.
1 Introduction
Sequence tagging algorithms including HMMs (Ra-
biner, 1989), CRFs (Lafferty et al, 2001), and
Collins?s perceptron (Collins, 2002) have been
widely employed in NLP applications. Sequential
decoding, which finds the best tag sequences for
given inputs, is an important part of the sequential
tagging framework. Traditionally, the Viterbi al-
gorithm (Viterbi, 1967) is used. This algorithm is
quite efficient when the label size of problem mod-
eled is low. Unfortunately, due to its O(TL2) time
complexity, where T is the input token size and L
is the label size, the Viterbi decoding can become
prohibitively slow when the label size is large (say,
larger than 200).
It is not uncommon that the problem modeled
consists of more than 200 labels. The Viterbi al-
gorithm cannot find the best sequences in tolerable
response time. To resolve this, Esposito and Radi-
cioni (2009) have proposed a Carpediem algorithm
which opens only necessary nodes in searching the
best sequence. More recently, Kaji et al (2010) pro-
posed a staggered decoding algorithm, which proves
to be very efficient on datasets with a large number
of labels.
What the aforementioned literature does not cover
is the k-best sequential decoding problem, which is
indeed frequently required in practice. For example
to pursue a high recall ratio, a named entity recogni-
tion system may have to adopt k-best sequences in
case the true entities are not recognized at the best
one. The k-best parses have been extensively stud-
ied in syntactic parsing context (Huang, 2005; Pauls
and Klein, 2009), but it is not well accommodated
in sequential decoding context. To our best knowl-
edge, the state-of-the-art k-best sequential decoding
algorithm is Viterbi A* 1. In this paper, we general-
ize the iterative process from the work of (Kaji et al,
2010) and propose a k-best sequential decoding al-
gorithm, namely iterative Viterbi A*. We show that
the proposed algorithm is several times or orders of
magnitude faster than the state-of-the-art in all tag-
ging tasks which consist of more than 200 labels.
Our contributions can be summarized as follows.
(1) We apply the A* search framework to sequential
decoding problem. We show that A* with a proper
heuristic can outperform the classic Viterbi decod-
ing. (2) We propose 1-best A*, 1-best iterative A*
decoding algorithms which are the second and third
fastest decoding algorithms among the five decod-
ing algorithms for comparison, although there is a
significant gap to the fastest 1-best decoding algo-
rithm. (3) We propose k-best A* and k-best iterative
Viterbi A* algorithms. The latter is several times or
orders of magnitude faster than the state-of-the-art
1Implemented in both CRFPP (http://crfpp.sourceforge.net/)
and LingPipe (http://alias-i.com/lingpipe/) packages.
611
k-best decoding algorithm. This algorithm makes
real-time large-scale tagging applications with thou-
sands of labels feasible.
2 Problem formulation
In this section, we formulate the sequential decod-
ing problem in the context of perceptron algorithm
(Collins, 2002) and CRFs (Lafferty et al, 2001). All
the discussions apply to HMMs as well. Formally, a
perceptron model is
f(y,x) =
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt), (1)
and a CRFs model is
p(y|x) =
1
Z(x)
exp{
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt)}, (2)
where x and y is an observation sequence and a la-
bel sequence respectively, t is the sequence position,
T is the sequence size, fk are feature functions and
K is the number of feature functions. ?k are the pa-
rameters that need to be estimated. They represent
the importance of feature functions fk in prediction.
For CRFs, Z(x) is an instance-specific normaliza-
tion function
Z(x) =
?
y
exp{
T?
t=1
K?
k=1
?kfk(yt, yt?1,xt)}. (3)
If x is given, the decoding is to find the best y which
maximizes the score of f(y,x) for perceptron or the
probability of p(y|x) for CRFs. As Z(x) is a con-
stant for any given input sequence x, the decoding
for perceptron or CRFs is identical, that is,
argmax
y
f(y,x). (4)
To simplify the discussion, we divide the features
into two groups: unigram label features and bi-
gram label features. Unigram features are of form
fk(yt,xt) which are concerned with the current la-
bel and arbitrary feature patterns from input se-
quence. Bigram features are of form fk(yt, yt?1,xt)
which are concerned with both the previous and the
current labels. We thus rewrite the decoding prob-
lem as
argmax
y
T?
t=1
(
K1?
k=1
?1kf
1
k (yt,xt)+
K2?
k=1
?2kf
2
k (yt, yt?1,xt)).
(5)
For a better understanding, one can inter-
pret the term
?K1
k=1 ?
1
kf
1
k (yt,xt) as node yt?s
score at position t, and interpret the term
?K2
k=1 ?
2
kf
2
k (yt, yt?1,xt) as edge (yt?1, yt)?s
score. So the sequential decoding problem is cast as
a max score pathfinding problem2. In the discussion
hereafter, we assume scores of nodes and edges are
pre-computed (denoted as n(yt) and e(yt?1, yt)),
and we can thus focus on the analysis of different
decoding algorithms.
3 Background
We present the existing algorithms for both 1-best
and k-best sequential decoding in this section. These
algorithms serve as basis for the proposed algo-
rithms in Section 4.
3.1 1-Best Viterbi
The Viterbi algorithm is a classic dynamic program-
ming based decoding algorithm. It has the computa-
tional complexity of O(TL2), where T is the input
sequence size and L is the label size3. Formally, the
Viterbi computes ?(yt), the best score from starting
position to label yt, as follows.
max
yt?1
(?yt?1 + e(yt?1, yt)) + n(yt), (6)
where e(yt?1, yt) is the edge score between nodes
yt?1 and yt, n(yt) is the node score for yt. Note
that the terms ?yt?1 and e(yt?1, yt) take value 0 for
t = 0 at initialization. Using the recursion defined
above, we can compute the highest score at end po-
sition T ? 1 and its corresponding sequence. The
recursive computation of ?yt is denoted as forward
pass since the computing traverses the lattice from
left to right. Conversely, the backward pass com-
putes ?yt as the follows.
max
yt+1
(?yt+1 + e(yt, yt+1) + n(yt+1)). (7)
Note that ?yT?1 = 0 at initialization. The max
score can be computed using maxy0(?0 + n(y0)).
We can use either forward or backward pass to
compute the best sequence. Table 1 summarizes
the computational complexity of all decoding algo-
rithms including Viterbi, which has the complexity
of TL2 for both best and worst cases. Note that
N/A means the decoding algorithms are not applica-
ble (for example, iterative Viterbi is not applicable
to k-best decoding). The proposed algorithms (see
Section 4) are highlighted in bold.
3.2 1-Best iterative Viterbi
Kaji et al (Kaji et al, 2010) presented an efficient
sequential decoding algorithm named staggered de-
coding. We use the name iterative Viterbi to describe
2With the constraint that the path consists of one and only
one node at each position.
3We ignore the feature size terms for simplicity.
612
this algorithm for the reason that the iterative pro-
cess plays a central role in this algorithm. Indeed,
this iterative process is generalized in this paper to
handle k-best sequential decoding (see Section 4.4).
The main idea is to start with a coarse lattice
which consists of both active labels and degenerate
labels. A label is referred to as an active label if it
is not grouped (e.g., all labels in Fig. 1 (a) and la-
bel A at each position in Fig. 1 (b)), and otherwise
as an inactive label (i.e., dotted nodes). The new la-
bel, which is made by grouping the inactive labels,
is referred to as a degenerate label (i.e., large nodes
covering the dotted ones). Fig. 1 (a) shows a lattice
which consists of active labels only and (b) shows
a lattice which consists of both active and degener-
ate ones. The score of a degenerate label is the max
score of inactive labels which are included in the de-
generate label. Similarly, the edge score between a
degenerate label z and an active label y? is the max
edge score between any inactive label y ? z and y?,
and the score of two degenerate labels z and z? is the
max edge score between any inactive label y ? z
and y? ? z?. Using the above definitions, the best
sequence derived from a degenerate lattice would be
the upper bound of the sequence derived from the
original lattice. If the best sequence does not include
any degenerate labels, it is indeed the best sequence
for the original lattice.
F
A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
A A
B
C
D
E
F
A
B
C
D
E
F
A
B
C
D
E
F
B
C
D
E
Figure 1: (a) A lattice consisting of active labels only.
(b) A lattice consisting of both active labels and degener-
ate ones. Each position has one active label (A) and one
degenerate label (consisting of B, C. D, E, and F).
The pseudo code for this algorithm is shown in
Algorithm 1. The lattice is initialized to include one
active label and one degenerate label at each position
(see Figure 1 (b)). Note that the labels are ranked
by the probabilities estimated from the training data.
The Viterbi algorithm is applied to the lattice to find
the best sequence. If the sequence consists of ac-
tive labels only, the algorithm terminates and returns
such a sequence. Otherwise, the lower bound lb4 of
the active sequence in the lattice is updated and the
lattice is expanded. The lower bound can be initial-
ized to the best sequence score using a beam search
(with beam size being 1). After either a forward or
a backward pass, the lower bound is assigned with
4The maximum score of the active sequences found so far.
the best active sequence score best(lattice)5 if the
former is less than the latter. The expansion of lat-
tice ensures that the lattice has twice active labels
as before at a given position. Figure 2 shows the
column-wise expansion step. The number of active
labels in the column is doubled only if the best se-
quence of the degenerate lattice passes through the
degenerate label of that column.
Algorithm 1 Iterative Viterbi Algorithm
1: lb = best score from beam search
2: init lattice
3: for i=0;;i++ do
4: if i %2 == 0 then
5: y = forward()
6: else
7: y = backward()
8: end if
9: if y consists of active labels only then
10: return y
11: end if
12: if lb < best(lattice) then
13: lb = best(lattice)
14: end if
15: expand lattice
16: end for
Algorithm 2 Forward
1: for i=0; i < T; i++ do
2: Compute ?(yi) and ?(yi) according to Equations (6) and (7)
3: if ?(yi) + ?(yi) < lb then
4: prune yi from the current lattice
5: end if
6: end for
7: Node b = argmaxyT?1 ?(yT?1)
8: return sequence back tracked by b
(c)
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
B
C
D
E
F
A A A A
(a) (b)
Figure 2: Column-wise lattice expansion: (a) The best
sequence of the initial degenerate lattice, which does not
pass through the degenerate label in the first column. (b)
Column-wise expansion is performed and the best se-
quence is searched again. Notice that the active label in
the first column is not expanded. (c) The final result.
Algorithm 2 shows the forward pass in which the
node pruning is performed. That is, for any node,
if the best score of sequence which passes such a
node is less than the lower bound lb, such a node
is removed from the lattice. This removal is safe
as such a node does not have a chance to form an
optimal sequence. It is worth noting that, if a node
is removed, it can no longer be added into the lattice.
5We do not update the lower bound lb if we cannot find an
active sequence.
613
This property ensures the efficiency of the iterative
Viterbi algorithm. The backward pass is similar to
the forward one and it is thus omitted.
The alternative calls of forward and backward
passes (in Algorithm 1) ensure the alternative updat-
ing/lowering of node forward and backward scores,
which makes the node pruning in either forward pass
(see Algorithm 2) or backward pass more efficient.
The lower bound lb is updated once in each iteration
of the main loop in Algorithm 1. While the forward
and backwards scores of nodes gradually decrease
and the lower bound lb increases, more and more
nodes are pruned.
The iterative Viterbi algorithm has computational
complexity of T and TL2 for best and worst cases
respectively. This can be proved as follows (Kaji et
al., 2010). At the m-th iteration in Algorithm 1, it-
erative Viterbi decoding requires order of T4m time
because there are 2m active labels (plus one degen-
erate label). Therefore, it has
?m
i=0 T4
i time com-
plexity if it terminates at the m-th iteration. In the
best case in which m = 0, the time complexity is T .
In the worst case in which m = dlog2 Le ? 1 (d.e is
the ceiling function which maps a real number to the
smallest following integer), the time complexity is
order of TL2 because
?dlog2 Le?1
i=0 T4
i < 4/3TL2.
3.3 1-Best Carpediem
Esposito and Radicioni (2009) have proposed a
novel 1-best6 sequential decoding algorithm, Car-
pediem, which attempts to open only necessary
nodes in searching the best sequence in a given lat-
tice. Carpediem has the complexity of TL logL and
TL2 for the best and worst cases respectively. We
skip the description of this algorithm due to space
limitations. Carpediem is used as a baseline in our
experiments for decoding speed comparison.
3.4 K-Best Viterbi
In order to produce k-best sequences, it is not
enough to store 1-best label per node, as the k-
best sequences may include suboptimal labels. The
k-best sequential decoding gives up this 1-best
label memorization in the dynamic programming
paradigm. It stores up to k-best labels which are nec-
essary to form k-best sequences. The k-best Viterbi
algorithm thus has the computational complexity of
KTL2 for both best and worst cases.
Once we store the k-best labels per node in a lat-
tice, the k-best Viterbi algorithm calls either the for-
ward or the backward passes just in the same way as
the 1-best Viterbi decoding does. We can compute
6They did not provide k-best solutions.
the k highest score at the end position T ? 1 and the
corresponding k-best sequences.
3.5 K-Best Viterbi A*
To our best knowledge the most efficient k-best se-
quence algorithm is the Viterbi A* algorithm as
shown in Algorithm 3. The algorithm consists of one
forward pass and an A* backward pass. The forward
pass computes and stores the Viterbi forward scores,
which are the best scores from the start to the cur-
rent nodes. In addition, each node stores a backlink
which points to its predecessor.
The major part of Algorithm 3 describes the back-
ward A* pass. Before describing the algorithm, we
note that each node in the agenda represents a se-
quence. So the operations on nodes (push or pop)
correspond to the operations on sequences. Initially,
the L nodes at position T ? 1 are pushed to an
agenda. Each of the L nodes ni, i = 0, . . . , L ? 1,
represents a sequence. That is, node ni represents
the best sequence from the start to itself. The best of
the L sequences is the globally best sequence. How-
ever, the i-th best, i = 2, . . . , k, of the L sequence
may not be the globally i-th best sequence. The pri-
ority of each node is set as the score of the sequence
which is derived by such a node. The algorithm then
goes to a loop of k. In each loop, the best node is
popped off from the agenda and is stored in a set r.
The algorithm adds alternative candidate nodes (or
sequences) to the agenda via a double nested loop.
The idea is that, when an optimal node (or sequence)
is popped off, we have to push to the agenda all
nodes (sequences) which are slightly worse than the
just popped one. The interpretation of slightly worse
is to replace one edge from the popped node (se-
quence). The slightly worse sequences can be found
by the exact heuristic derived from the first Viterbi
forward pass.
Figure 3 shows an example of the push operations
for a lattice of T = 4, Y = 4. Suppose an optimal
node 2:B (in red, standing for node B at position 2,
representing the sequence of 0:A 1:D 2:B 3:C) is
popped off, new nodes of 1:A, 1:B, 1:C and 0:B,
0:C and 0:D are pushed to the agenda according to
the double nested for loop in Algorithm 3. Each
of the pushed nodes represents a sequence, for ex-
ample, node 1:B represents a sequence which con-
sists of three parts: Viterb sequence from start to
1:B (0:C 1:B), 2:B and forward link of 2:B (3:C
in this case). All of these pushed nodes (sequences)
are served as candidates for the next agenda pop op-
eration.
The algorithm terminates the loop once it has op-
timal k nodes. The k-best sequences can be de-
rived by the k optimal nodes. This algorithm has
614
TB
C
D
B
C
D
B
C
D
B
C
D
A A A A
31 20
Figure 3: Alternative nodes push after popping an opti-
mal node.
computation complexity of TL2 + TL for both best
and worst cases, with the first term accounting for
Viterbi forward pass and the second term account-
ing for A* backward process. The bottleneck is thus
at the Viterbi forward pass.
Algorithm 3K-Best Viterbi A* algorithm
1: forward()
2: push L best nodes to agenda q
3: c = 0
4: r = {}
5: while c < K do
6: Node n = q.pop()
7: r = r ? n
8: for i = n.t? 1; i ? 0; i?? do
9: for j = 0; j < L; j + + do
10: if j! = n.backlink.y then
11: create new node s at position i and label j
12: s.forwardlink = n
13: q.push(s)
14: end if
15: end for
16: n = n.backlink
17: end for
18: c+ +
19: end while
20: return K best sequences derived by r
4 Proposed Algorithms
In this section, we propose A* based sequen-
tial decoding algorithms that can efficiently handle
datasets with a large number of labels. In particular,
we first propose the A* and the iterative A* decod-
ing algorithm for 1-best sequential decoding. We
then extend the 1-best A* algorithm to a k-best A*
decoding algorithm. We finally apply the iterative
process to the Viterbi A* algorithm, resulting in the
iterative Viterbi A* decoding algorithm.
4.1 1-Best A*
A*(Hart et al, 1968; Russell and Norvig, 1995), as
a classic search algorithm, has been successfully ap-
plied in syntactic parsing (Klein and Manning, 2003;
Pauls and Klein, 2009). The general idea of A* is to
consider labels yt which are likely to result in the
best sequence using a score f as follows.
f(y) = g(y) + h(y), (8)
where g(y) is the score from start to the current node
and h(y) is a heuristic which estimates the score
from the current node to the target. A* uses an
agenda (based on the f score) to decide which nodes
are to be processed next. If the heuristic satisfies the
condition h(yt?1) ? e(yt?1, yt) + h(yt), then h is
called monotone or admissible. In such a case, A* is
guaranteed to find the best sequence. We start with
the naive (but admissible) heuristic as follows
h(yt) =
T?1?
i=t+1
(maxn(yi) + max e(yi?1, yi)). (9)
That is, the heuristic of node yt to the end is the sum
of max edge scores between any two positions and
max node scores per position. Similar to (Pauls and
Klein, 2009) we explore the heuristic in different
coarse levels. We apply the Viterbi backward pass
to different degenerate lattices and use the Viterbi
backward scores as different heuristics. Different
degenerate lattices are generated from different it-
erations of Algorithm 1: The m-th iteration corre-
sponds to a lattice of (2m+1)?T nodes. A largerm
indicates a more accurate heuristic, which results in
a more efficient A* search (fewer nodes being pro-
cessed). However, this efficiency comes with the
price that such an accurate heuristic requires more
computation time in the Viterbi backward pass. In
our experiments, we try the naive heuristic and the
following values of m: 0, 3, 6 and 9.
In the best case, A* expands one node per posi-
tion, and each expansion results in the push of all
nodes at next position to the agenda. The search is
similar to the beam search with beam size being 1.
The complexity is thus TL. In the worst case, A*
expands every node per position, and each expan-
sion results in the push of all nodes at next position
to the agenda. The complexity thus becomes TL2.
4.2 1-Best Iterative A*
The iterative process as described in the iterative
Viterbi decoding can be used to boost A* algorithm,
resulting in the iterative A* algorithm. For simplic-
ity, we only make use of the naive heuristic in Equa-
tion (9) in the iterative A* algorithm. We initialize
the lattice with one active label and one degenerate
label at each position (see Figure 1 (b)). We then run
A* algorithm on the degenerate lattice and get the
best sequence. If the sequence is active we return
it. Otherwise we expand the lattice in each iteration
until we find the best active sequence. Similar to
iterative Viterbi algorithm, iterative A* has the com-
plexity of T and TL2 for the best and worst cases
respectively.
4.3 K-Best A*
The extension from 1-best A* to k-best A* is again
due to the memorization of k-best labels per node.
615
Table 1: Best case and worst case computational complexity of various decoding algorithms.
1-best decoding K-best decoding
best case worst case best case worst case
beam TL TL KTL KTL
Viterbi TL2 TL2 KTL2 KTL2
iterative Viterbi T TL2 N/A N/A
Carpediem TL logL TL2 N/A N/A
A* TL TL2 KTL KTL2
iterative A* T TL2 N/A N/A
Viterbi A* N/A N/A TL2 +KTL TL2 +KTL
iterative Viterbi A* N/A N/A T +KT TL2 +KTL
We use either the naive heuristic (Equation (9)) or
different coarse level heuristics by setting m to be 0,
3, 6 or 9 (see Section 4.1). The first k nodes which
are popped off the agenda can be used to back track
the k-best sequences. The k-best A* algorithm has
the computational complexity of KTL and KTL2
for best and worst cases respectively.
4.4 K-Best Iterative Viterbi A*
We now present the k-best iterative Viterbi A* algo-
rithm (see Algorithm 4) which applies the iterative
process to k-best Viterbi A* algorithm. The major
difference between 1-best iterative Viterbi A* algo-
rithm (Algorithm 1) and this algorithm is that the
latter calls the k-best Vitebi A* (Algorithm 3) after
the best sequence is found. If the k-best sequences
are all active, we terminate the algorithm and return
the k-best sequences. If we cannot find either the
best active sequence or the k-best active sequences,
we expand the lattice to continue the search in the
next iteration.
As in the iterative Viterbi algorithm (see Section
3.2), nodes are pruned at each position in forward
or backward passes. Efficient pruning contributes
significantly to speeding up decoding. Therefore, to
have a tighter (higher) lower bound lb is important.
We initialize the lower bound lb with the k-th best
score from beam search (with beam size being k) at
line 1. Note that the beam search is performed on the
original lattice which consists of L active labels per
position. The beam search time is negligible com-
pared to the total decoding time. At line 16, we up-
date lb as follows. We enumerate the best active se-
quences backtracked by the nodes at position T ? 1.
If the current lb is less than the k-th active sequence
score, we update the lbwith the k-th active sequence
score (we do not update lb if there are less than k ac-
tive sequences). At line 19, we use the sequences
returned from Viterbi A* algorithm to update the lb
in the same manner. To enable this update, we re-
quest the Viterbi A* algorithm to return k?, k? > k,
sequences (line 10). A larger number of k? results
in a higher chance to find the k-th active sequence,
which in turn offers a tighter (higher) lb, but it comes
with the expense of additional time (the backward
A* process takes O(TL) time to return one more
sequence). In experiments, we found the lb updates
on line 1 and line 16 are essential for fast decoding.
The updating of lb using Viterbi A* sequences (line
19) can boost the decoding speed further. We exper-
imented with different k? values (k? = nk, where n
is an integer) and selected k? = 2k which results in
the largest decoding speed boost.
Algorithm 4K-Best iterative Viterbi A* algorithm
1: lb = k-th best (original lattice)
2: init lattice
3: for i = 0; ; i+ + do
4: if i%2 == 0 then
5: y = forward()
6: else
7: y = backward()
8: end if
9: if y consists of active labels only then
10: ys= k-best Viterbi A* (Algorithm 3)
11: if ys consists of active sequences only then
12: return ys
13: end if
14: end if
15: if lb < k-th best(lattice) then
16: lb = k-th best(lattice)
17: end if
18: if lb < k-th best(ys) then
19: lb = k-th best(ys)
20: end if
21: expand lattice
22: end for
5 Experiments
We compare aforementioned 1-best and k-best se-
quential decoding algorithms using five datasets in
this section.
5.1 Experimental setting
We apply 1-best and k-best sequential decoding al-
gorithms to five NLP tagging tasks: Penn TreeBank
(PTB) POS tagging, CoNLL2000 joint POS tag-
ging and chunking, CoNLL 2003 joint POS tagging,
chunking and named entity tagging, HPSG supertag-
ging (Matsuzaki et al, 2007) and a search query
named entity recognition (NER) dataset. We used
616
sections 02-21 of PTB for training and section 23
for testing in POS task. As in (Kaji et al, 2010),
we combine the POS tags and chunk tags to form
joint tags for CoNLL 2000 dataset, e.g., NN|B-NP.
Similarly we combine the POS tags, chunk tags, and
named entity tags to form joint tags for CoNLL 2003
dataset, e.g., PRP$|I-NP|O. Note that by such tag
joining, we are able to offer different tag decodings
(for example, chunking and named entity tagging)
simultaneously. This indeed is one of the effective
approaches for joint tag decoding problems. The
search query NER dataset is an in-house annotated
dataset which assigns semantic labels, such as prod-
uct, business tags to web search queries.
Table 2 shows the training and test sets size (sen-
tence #), the average token length of test dataset and
the label size for the five datasets. POS and su-
pertag datasets assign tags to tokens while CoNLL
2000 , CoNLL 2003 and search query datasets as-
sign tags to phrases. We use the standard BIO en-
coding for CoNLL 2000, CoNLL 2003 and search
query datasets.
Table 2: Training and test datasets size, average token
length of test set and label size for five datasets.
training # test # token length label size
POS 39831 2415 23 45
CoNLL2000 8936 2012 23 319
CoNLL2003 14987 3684 12 443
Supertag 37806 2291 22 2602
search query 79569 6867 3 323
Due to the long CRF training time (days to weeks
even for stochastic gradient descent training) for
these large label size datasets, we choose the percep-
tron algorithm for training. The models are averaged
over 10 iterations (Collins, 2002). The training time
takes minutes to hours for all datasets. We note that
the selection of training algorithm does not affect
the decoding process: the decoding is identical for
both CRF and perceptron training algorithms. We
use the common features which are adopted in previ-
ous studies, for example (Sha and Periera, 2003). In
particular, we use the unigrams of the current and its
neighboring words, word bigrams, prefixes and suf-
fixes of the current word, capitalization, all-number,
punctuation, and tag bigrams for POS, CoNLL2000
and CoNLL 2003 datasets. For supertag dataset,
we use the same features for the word inputs, and
the unigrams and bigrams for gold POS inputs. For
search query dataset, we use the same features plus
gazetteer based features.
5.2 Results
We report the token accuracy for all datasets to facil-
itate comparison to previous work. They are 97.00,
94.70, 95.80, 90.60 and 88.60 for POS, CoNLL
2000, CoNLL 2003, supertag, and search query re-
spectively. We note that all decoding algorithms as
listed in Section 3 and Section 4 are exact. That is,
they produce exactly the same accuracy. The accu-
racy we get for the first four tasks is comparable to
the state-of-the-art. We do not have a baseline to
compare with for the last dataset as it is not pub-
licly available7. Higher accuracy may be achieved if
more task specific features are introduced on top of
the standard features. As this paper is more con-
cerned with the decoding speed, the feature engi-
neering is beyond the scope of this paper.
Table 3 shows how many iterations in average
are required for iterative Viterbi and iterative Viterbi
A* algorithms. Although the max iteration size is
bounded to dlog2 Le for each position (for exam-
ple, 9 for CoNLL 2003 dataset), the total iteration
number for the whole lattice may be greater than
dlog2 Le as different positions may not expand at
the same time. Despite the large number of itera-
tions used in iterative based algorithms (especially
iterative Viterbi A* algorithm), the algorithms are
still very efficient (see below).
Table 3: Iteration numbers of iterative Viterbi and itera-
tive Viterbi A* algorithms for five datasets.
POS CoNLL2000 CoNLL2003 Supertag search query
iter Viter 6.32 8.76 9.18 10.63 6.71
iter Viter A* 14.42 16.40 15.41 18.62 9.48
Table 4 and 5 show the decoding speed (sen-
tences per second) of 1-best and 5-best decoding al-
gorithms respectively. The proposed decoding algo-
rithms and the largest decoding speeds across differ-
ent decoding algorithms (other than beam) are high-
lighted in bold. We exclude the time for feature ex-
traction in computing the speed. The beam search
decoding is also shown as a baseline. We note that
beam decoding is the only approximate decoding al-
gorithm in this table. All other decoding algorithms
produce exactly the same accuracy, which is usually
much better than the accuracy of beam decoding.
For 1-best decoding, iterative Viterbi always out-
performs other ones. A* with a proper heuristic de-
noted as A* (best), that is, the best A* using naive
heuristic or the values of m being 0, 3, 6 or 9 (see
Section 4.1), can be the second best choice (ex-
cept for the POS task), although the gap between
iterative Viterbi and A* is significant. For exam-
ple, for CoNLL 2003 dataset, the former can de-
code 2239 sentences per second while the latter only
decodes 225 sentences per second. The iterative
process successfully boosts the decoding speed of
iterative Viterbi compared to Viterbi, but it slows
down the decoding speed of iterative A* compared
7The lower accuracy is due to the dynamic nature of queries:
many of test query tokens are unseen in the training set.
617
to A*(best). This is because in the Viterbi case,
the iterative process has a node pruning procedure,
while it does not have such pruning in A*(best)
algorithm. Take CoNLL 2003 data as an exam-
ple, the removal of the pruning slows down the 1-
best iterative Viterbi decoding from 2239 to 604
sentences/second. Carpediem algorithm performs
poorly in four out of five tasks. This can be ex-
plained as follows. The Carpediem implicitly as-
sumes that the node scores are the dominant factors
to determine the best sequence. However, this as-
sumption does not hold as the edge scores play an
important role.
For 5-best decoding, k-best Viterbi decoding is
very slow. A* with a proper heuristic is still slow.
For example, it only reaches 11 sentences per second
for CoNLL 2003 dataset. The classic Viterbi A* can
usually obtain a decent decoding speed, for example,
40 sentences per second for CoNLL 2003 dataset.
The only exception is supertag dataset, on which the
Viterbi A* decodes 0.1 sentence per second while
the A* decodes 3. This indicates the scalability is-
sue of Viterbi A* algorithm for datasets with more
than one thousand labels. The proposed iterative
Viterbi A* is clearly the winner. It speeds up the
Viterbi A* to factors of 4, 7, 360, and 3 for CoNLL
2000, CoNLL 2003, supertag and query search data
respectively. The decoding speed of iterative Viterbi
A* can even be comparable to that of beam search.
Figure 4 shows k-best decoding algorithms de-
coding speed with respect to different k values for
CoNLL 2003 data . The Viterbi A* and iterative
Viterbi A* algorithms are significantly faster than
the Viterbi and A*(best) algorithms. Although the
iterative Viterbi A* significantly outperforms the
Viterbi A* for k < 30, the speed of the former con-
verges to the latter when k becomes 90 or larger.
This is expected as the k-best sequences span over
the whole lattice: the earlier iteration in iterative
Viterbi A* algorithm cannot provide the k-best se-
quences using the degenerate lattice. The over-
head of multiple iterations slows down the decoding
speed compared to the Viterbi A* algorithm.
l l l l l l l l l l10 20 30 40 50 60 70 80 90 100020
406080
100120140
160180200
k
sentenc
es/secon
d l ViterbiA*(best)Viterbi A*iterative Viterbi A*
Figure 4: Decoding speed of k-best decoding algorithms
for various k for CoNLL 2003 dataset.
6 Related work
The Viterbi algorithm is the only exact algorithm
widely adopted in the NLP applications. Esposito
and Radicioni (2009) proposed an algorithm which
opens necessary nodes in a lattice in searching the
best sequence. The staggered decoding (Kaji et al,
2010) forms the basis for our work on iterative based
decoding algorithms. Apart from the exact decod-
ing, approximate decoding algorithms such as beam
search are also related to our work. Tsuruoka and
Tsujii (2005) proposed easiest-first deterministic de-
coding. Siddiqi and Moore (2005) presented the pa-
rameter tying approach for fast inference in HMMs.
A similar idea was applied to CRFs as well (Cohn,
2006; Jeong, 2009). We note that the exact algo-
rithm always guarantees the optimality which can-
not be attained in approximate algorithms.
In terms of k-best parsing, Huang and Chiang
(2005) proposed an efficient algorithm which is sim-
ilar to the k-best Viterbi A* algorithm presented in
this paper. Pauls and Klein (2009) proposed an algo-
rithm which replaces the Viterbi forward pass with
an A* search. Their algorithm optimizes the Viterbi
pass, while the proposed iterative Viterbi A* algo-
rithm optimizes both Viterbi and A* passes.
This paper is also related to the coarse to fine
PCFG parsing (Charniak et al, 2006) as the degen-
erate labels can be treated as coarse levels. How-
ever, the difference is that the coarse-to-fine parsing
is an approximate decoding while ours is exact one.
In terms of different coarse levels of heuristic used
in A* decoding, this paper is related to the work of
hierarchical A* framework (Raphael, 2001; Felzen-
szwalb et al, 2007). In terms of iterative process,
this paper is close to (Burkett et al, 2011) as both
exploit the search-and-expand approach.
7 Conclusions
We have presented and evaluated the A* and itera-
tive A* algorithms for 1-best sequential decoding in
this paper. In addition, we proposed A* and iterative
Viterbi A* algorithm for k-best sequential decoding.
K-best Iterative A* algorithm can be several times
or orders of magnitude faster than the state-of-the-
art k-best decoding algorithm. It makes real-time
large-scale tagging applications with thousands of
labels feasible.
Acknowledgments
We wish to thank Yusuke Miyao and Nobuhiro Kaji
for providing us the HPSG Treebank data. We are
grateful for the invaluable comments offered by the
anonymous reviewers.
618
Table 4: Decoding speed (sentences per second) of 1-best decoding algorithms for five datasets.
POS CoNLL2000 CoNLL2003 supertag query search
beam 7252 1381 1650 395 7571
Viterbi 2779 51 41 0.19 443
iterative Viterbi 5833 972 2239 213 6805
Carpediem 2638 14 20 0.15 243
A* (best) 802 131 225 8 880
iterative A* 1112 84 109 3 501
Table 5: Decoding speed (sentences per second) of 5-best decoding algorithms for five datasets.
POS CoNLL2000 CoNLL2003 supertag query search
beam 2760 461 592 75 4354
Viterbi 19 0.41 0.25 0.12 3.83
A* (best) 205 4 11 3 92
Viterbi A* 1266 47 40 0.1 357
iterative Viterbi A* 788 200 295 36 1025
References
D. Burkett, D. Hall, and D. Klein. 2011. Optimal graph
search with iterated graph cuts. Proceedings of AAAI.
E. Charniak, M. Johnson, M. Elsner, J. Austerweil, D.
Ellis, I. Haxton, C. Hill, R. Shrivaths, J. Moore, M.
Pozar, and T. Vu. 2006. Multi-level coarse-to-fine
PCFG parsing. Proceedings of NAACL.
T. Cohn. 2006. Efficient inference in large conditional
random fields. Proceedings of ECML.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. Proceedings of EMNLP.
R. Esposito and D. P. Radicioni. 2009. Carpediem:
Optimizing the Viterbi Algorithm and Applications to
Supervised Sequential Learning. Journal of Machine
Learning Research.
P. Felzenszwalb and D. McAllester. 2007. The general-
ized A* architecture. Journal of Artificial Intelligence
Research.
P. E. Hart, N. J. Nilsson, and B. Raphael. 1968. A For-
mal Basis for the Heuristic Determination of Minimum
Cost Paths. IEEE Transactions on Systems Science
and Cybernetics.
L. Huang and D. Chiang. 2005. Better k-best parsing.
Proceedings of the International Workshops on Parsing
Technologies (IWPT).
M. Jeong, C. Y. Lin, and G. G. Lee. 2009. Efficient infer-
ence of CRFs for large-scale natural language data.
Proceedings of ACL-IJCNLP Short Papers.
N. Kaji, Y. Fujiwara, N. Yoshinaga, and M. Kitsuregawa.
2010. Efficient Staggered Decoding for Sequence La-
beling. Proceedings of ACL.
D. Klein and C. Manning. 2003. A* parsing: Fast exact
Viterbi parse selection. Proceedings of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. Proceedings of
ICML.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2007. Efficient
HPSG parsing with supertagging and CFG-filtering.
Proceedings of IJCAI.
A. Pauls and D. Klein. 2009. K-Best A* Parsing. Pro-
ceedings of ACL.
L. R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of The IEEE.
C. Raphael. 2001. Coarse-to-fine dynamic program-
ming. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence.
S. Russell and P. Norvig. 1995. Artificial Intelligence: A
Modern Approach.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. Proceedings of HLT-NAACL.
S. M. Siddiqi and A. Moore. 2005. Fast inference and
learning in large-state-space HMMs. Proceedings of
ICML.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional in-
ference with the easiest-first strategy for tagging se-
quence data. Proceedings of HLT/EMNLP.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymptotically optimum decoding algo-
rithm. IEEE Transactions on Information Theory.
619
