Named Entity Discovery Using Comparable News Articles
Yusuke SHINYAMA and Satoshi SEKINE
Computer Science Department
New York University
715, Broadway, 7th Floor
New York, NY, 10003
yusuke@cs.nyu.edu, sekine@cs.nyu.edu
Abstract
In this paper we describe a way to discover
Named Entities by using the distribution of
words in news articles. Named Entity recog-
nition is an important task for today?s natural
language applications, but it still suffers from
data sparseness. We used an observation that a
Named Entity is likely to appear synchronously
in several news articles, whereas a common
noun is less likely. Exploiting this characteris-
tic, we successfully obtained rare Named Enti-
ties with 90% accuracy just by comparing time
series distributions of a word in two newspa-
pers. Although the achieved recall is not suf-
ficient yet, we believe that this method can be
used to strengthen the lexical knowledge of a
Named Entity tagger.
1 Introduction
Recently, Named Entity (NE) recognition has been
getting more attention as a basic building block for
practical natural language applications. A Named
Entity tagger identifies proper expressions such as
names, locations and dates in sentences. We are
trying to extend this to an Extended Named Entity
tagger, which additionally identifies some common
nouns such as disease names or products. We be-
lieve that identifying these names is useful for many
applications such as information extraction or ques-
tion answering (Sekine et al, 2002).
Normally a Named Entity tagger uses lexical or
contextual knowledge to spot names which appear
in documents. One of the major problem of this task
is its data sparseness. Names appear very frequently
in regularly updated documents such as news arti-
cles or web pages. They are, however, much more
varied than common nouns, and changing contin-
uously. Since it is hard to construct a set of pre-
defined names by hand, usually some corpus based
approaches are used for building such taggers.
However, as Zipf?s law indicates, most of the
names which occupy a large portion of vocabulary
are rarely used. So it is hard for Named Entity
tagger developers to keep up with a contemporary
set of words, even though a large number of docu-
ments are provided for learning. There still might
be a ?wild? noun which doesn?t appear in the cor-
pora. Several attempts have been made to tackle
this problem by using unsupervised learning tech-
niques, which make vast amount of corpora avail-
able to use. (Strzalkowski and Wang, 1996) and
(Collins and Singer, 1999) tried to obtain either lex-
ical or contextual knowledge from a seed given by
hand. They trained the two different kind of knowl-
edge alternately at each iteration of training. (Yan-
garber et al, 2002) tried to discover names with a
similar method. However, these methods still suffer
in the situation where the number of occurrences of
a certain name is rather small.
2 Synchronicity of Names
In this paper we propose another method to
strengthen the lexical knowledge for Named Entity
tagging by using synchronicity of names in com-
parable documents. One can view a ?comparable?
document as an alternative expression of the same
content. Now, two document sets where each doc-
ument of one set is associated with one in the other
set, is called a ?comparable? corpus. A compara-
ble corpus is less restricted than a parallel corpus
and usually more available. Several different news-
papers published on the same day report lots of the
same events, therefore contain a number of compa-
rable documents. One can also take another view of
a comparable corpus, which is a set of paraphrased
documents. By exploiting this feature, one can ex-
tract paraphrastic expressions automatically from
parallel corpora (Barzilay and McKeown, 2001) or
comparable corpora (Shinyama and Sekine, 2003).
Named Entities in comparable documents have
one notable characteristic: they tend to be preserved
across comparable documents because it is gener-
ally difficult to paraphrase names. We think that
it is also hard to paraphrase product names or dis-
ease names, so they will also be preserved. There-
fore, if one Named Entity appears in one document,
it should also appear in the comparable document.
 0
 5
 10
 15
 20
 25
 30
 0  50  100  150  200  250  300  350  400
Fre
que
ncy
Date
LATWPNYTREUTE
The occurrence of the word ?yigal?
 0
 20
 40
 60
 80
 100
 120
 140
 0  50  100  150  200  250  300  350  400
Fre
que
ncy
Date
LATWPNYTREUTE
The occurrence of the word ?killed?
Figure 1: The occurrence of two words in 1995
Consequently, if one has two sets of documents
which are associated with each other, the distribu-
tion of a certain name in one document set should
look similar to the distribution of the name in the
other document set.
We tried to use this characteristic of Named En-
tities to discover rare names from comparable news
articles. We particularly focused on the time series
distribution of a certain word in two newspapers.
We hypothesized that if a Named Entity is used
in two newspapers, it should appear in both news-
papers synchronously, whereas other words don?t.
Since news articles are divided day by day, it is easy
to obtain the time series distribution of words ap-
pearing in each newspaper.
Figure 2 shows the time series distribution of the
two words ?yigal? and ?killed?, which appeared in
several newspapers in 1995. The word ?yigal? (the
name of the man who killed Israeli Prime Minister
Yitzhak Rabin on Nov. 7, 1995) has a clear spike.
There were a total of 363 documents which included
the word that year and its occurrence is synchronous
between the two newspapers. In contrast, the word
?killed?, which appeared in 21591 documents, is
spread over all the year and has no clear character-
istic.
3 Experiment
To verify our hypothesis, we conducted an experi-
ment to measure the correlation between the occur-
rence of Named Entity and its similarity of time se-
ries distribution between two newspapers.
First, we picked a rare word, then obtained its
document frequency which is the number of articles
which contain the word. Since newspaper articles
are provided separately day by day, we sampled the
document frequency for each day. These numbers
form, for one year for example, a 365-element in-
teger vector per newspaper. The actual number of
news articles is oscillating weekly, however, we nor-
malized this by dividing the number of articles con-
taining the word by the total number of all articles
on that day. At the end we get a vector of fractions
which range from 0.0 to 1.0.
Next we compared these vectors and calculated
the similarity of their time series distributions across
different news sources. Our basic strategy was to
use the cosine similarity of two vectors as the likeli-
hood of the word?s being a Named Entity. However,
several issues arose in trying to apply this directly.
Firstly, it is not always true that the same event is re-
ported on the same day. An actual newspaper some-
times has a one or two-day time lag depending on
the news. To alleviate this effect, we applied a sim-
ple smoothing to each vector. Secondly, we needed
to focus on the salient use of each word, otherwise a
common noun which constantly appears almost ev-
ery day has an undesirable high similarity between
newspapers. To avoid this, we tried to intensify the
effect of a spike by comparing the deviation of the
frequency instead of the frequency itself. This way
we can degrade the similarity of a word which has a
?flat? distribution.
In this section we first explain a single-word ex-
periment which detects Named Entities that consist
of one word. Next we explain a multi-word exper-
iment which detects Named Entities that consist of
exactly two words.
3.1 Single-word Experiment
In a single-word experiment, we used two one-
year newspapers, Los Angeles Times and Reuters in
1995. First we picked a rare word which appeared
in either newspaper less than 100 times throughout
the year. We only used a simple tokenizer and con-
verted all words into lower case. A part of speech
tagger was not used. Then we obtained the docu-
ment frequency vector for the word. For each word
w which appeared in newspaper A, we got the doc-
ument frequency at date t:
fA(w, t) = dfA(w, t)/NA(t)
where dfA(w, t) is the number of documents which
contain the word w at date t in newspaper A. The
normalization constant NA(t) is the number of all
articles at date t. However comparing this value
between two newspapers directly cannot capture a
time lag. So now we apply smoothing by the fol-
lowing formula to get an improved version of fA:
f ?A(w, t) =
?
?W?i?W
r|i|fA(w, t+ i)
Here we give each occurrence of a word a ?stretch?
which sustains for W days. This way we can cap-
ture two occurrences which appear on slightly dif-
ferent days. In this experiment, we used W = 2
and r = 0.3, which sums up the numbers in a 5-day
window. It gives each occurrence a 5-day stretch
which is exponentially decreasing.
Then we make another modification to f ?A by
computing the deviation of f ?A to intensify a spike:
f ??A(w, t) =
f ?A(w, t)? f? ?A
?
where f? ?A and ? is the average and the standard de-
viation of f ?A(w):
f? ?A =
?
t f ?A(w, t)
T
? =
??
t (f ?A(w, t)? f? ?A)2
T
T is the number of days used in the experiment, e.g.
T = 365 for one year. Now we have a time series
vector FA(w) for word w in newspaper A:
FA(w) = {f ??A(w, 1), f ??A(w, 2), ..., f ??A(w, T )}
Similarly, we calculated another time series
FB(w) for newspaper B. Finally we computed
sim(w), the cosine similarity of two distributions
of the word w with the following formula:
sim(w) = FA(w) ? FB(w)|FA(w)||FB(w)|
Since this is the cosine of the angle formed by
the two vectors, the obtained similarity ranges from
?1.0 to 1.0. We used sim(w) as the Named Entity
score of the word and ranked these words by this
score. Then we took the highly ranked words as
Named Entities.
3.2 Multi-word Experiment
We also tried a similar experiment for compound
words. To avoid chunking errors, we picked all
consecutive two-word pairs which appeared in both
newspapers, without using any part of speech tagger
or chunker. Word pairs which include a pre-defined
stop word such as ?the? or ?with? were eliminated.
As with the single-word experiment, we measured
the similarity between the time series distributions
for a word pair in two newspapers. One different
point is that we compared three newspapers 1 rather
than two, to gain more accuracy. Now the ranking
score sim(w) given to a word pair is calculated as
follows:
sim(w) = simAB(w)? simBC(w)? simAC(w)
where simXY (w) is the similarity of the distribu-
tions between two newspapers X and Y , which can
be computed with the formula used in the single-
word experiment. To avoid incorrectly multiply-
ing two negative similarities, a negative similarity
is treated as zero.
4 Evaluation and Discussion
To evaluate the performance, we ranked 966 sin-
gle words and 810 consecutive word pairs which are
randomly selected. We measured how many Named
Entities are included in the highly ranked words.
We manually classified as names the words in the
following categories used in IREX (Sekine and Isa-
hara, 2000): PERSON, ORGANIZATION, LOCA-
TION, and PRODUCT. In both experiments, we re-
garded a name which can stand itself as a correct
Named Entity, even if it doesn?t stretch to the entire
noun phrase.
4.1 Single-word Experiment
Table 1 shows an excerpt of the ranking result. For
each word, the type of the word, the document fre-
quency and the similarity (score) sim(w) is listed.
Obvious typos are classified as ?typo?. One can ob-
serve that a word which is highly ranked is more
likely a Named Entity than lower ones. To show this
correlation clearly, we plot the score of the words
and the likelihood of being a Named Entity in Fig-
ure 2. Since the actual number of the words is
discrete, we computed the likelihood by counting
Named Entities in a 50-word window around that
score.
Table 3 shows the number of obtained Named En-
tities. By taking highly ranked words (sim(w) ?
1For the multi-word experiment, we used Los Angeles
Times, Reuters, and New York Times.
Word Type Freq. Score
sykesville LOCATION 4 1.000
khamad PERSON 4 1.000
zhitarenko PERSON 6 1.000
sirica PERSON 9 1.000
energiyas PRODUCT 4 1.000
hulya PERSON 5 1.000
salvis PERSON 5 0.960
geagea PERSON 27 0.956
bogdanor PERSON 6 0.944
gomilevsky PERSON 6 0.939
kulcsar PERSON 15 0.926
carseats noun 17 0.912
wilsons PERSON 32 0.897
yeud ORGANIZATION 10 0.893
yigal PERSON 490 0.878
bushey PERSON 10 0.874
pardew PERSON 17 0.857
yids PERSON 5 0.844
bordon PERSON 113 0.822
... ... ... ...
katyushas PRODUCT 56 0.516
solzhenitsyn PERSON 81 0.490
scheuer PERSON 9 0.478
morgue noun 340 0.456
mudslides noun 151 0.420
rump noun 642 0.417
grandstands noun 42 0.407
overslept verb 51 0.401
lehrmann PERSON 13 0.391
... ... ... ...
willowby PERSON 3 0.000
unknowable adj 48 0.000
taubensee PERSON 22 0.000
similary (typo) 3 0.000
recommitment noun 12 0.000
perorations noun 3 0.000
orenk PERSON 2 0.000
malarkey PERSON 34 0.000
gherardo PERSON 5 0.000
dcis ORGANIZATION 3 0.000
... ... ... ...
merritt PERSON 149 -0.054
echelon noun 97 -0.058
plugging verb 265 -0.058
normalcy noun 170 -0.063
lovell PERSON 238 -0.066
provisionally adv 74 -0.068
sails noun 364 -0.075
rekindled verb 292 -0.081
sublime adj 182 -0.090
afflicts verb 168 -0.116
stan PERSON 994 -0.132
Table 1: Ranking Result (Single-word)
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8
Lik
elih
ood
Score
Figure 2: Relationship of the score and the likeli-
hood of being a Named Entity (Single-word). The
horizontal axis shows the score of a word. The ver-
tical axis shows the likelihood of being a NE. One
can see that the likelihood of NE increases as the
score of a word goes up. However there is a huge
peak near the score zero.
0.6), we can discover rare Named Entities with 90%
accuracy. However, one can notice that there is a
huge peak near the score sim(w) = 0. This means
that many Named Entities still remain in the lower
score. Most such Named Entities only appeared in
one newspaper. Named Entities given a score less
than zero were likely to refer to a completely differ-
ent entity. For example, the word ?Stan? can be used
as a person name but was given a negative score, be-
cause this was used as a first name of more than 10
different people in several overlapping periods.
Also, we took a look at highly ranked words
which are not Named Entities as shown in Table 2.
The words ?carseats?, ?tiremaker?, or ?neurotripic?
happened to appear in a small number of articles.
Each of these articles and its comparable counter-
parts report the same event, but both of them use
the same word probably because there was no other
succinct expression to paraphrase these rare words.
This way these three words made a high spike. The
word ?officeholders? was misrecognized due to the
Word Type Freq. Score
carseats noun 17 0.9121
tiremaker noun 21 0.8766
officeholders noun 101 0.8053
neurotrophic adj 11 0.7850
mishandle verb 12 0.7369
Table 2: Errors (Single-word)
Words NEs
All words 966 462 (48%)
sim(w) ? 0.6 102 92 (90%)
sim(w) ? 0 511 255 (50%)
Table 3: Obtained NEs (Single-word)
Word pairs NEs
All word pairs 810 60 (7%)
sim(w) ? 0.05 27 11 (41%)
sim(w) ? 0 658 30 (5%)
Table 4: Obtained NEs (Multi-word)
repetition of articles. This word appeared a lot of
times and some of them made the spike very sharp,
but it turned out that the document frequency was
undesirably inflated by the identical articles. The
word ?mishandle? was used in a quote by a per-
son in both articles, which also makes a undesirable
spike.
4.2 Multi-word Experiment
In the multi-word experiment, the accuracy of the
obtained Named Entities was lower than in the
single-word experiment as shown in Table 4, al-
though correlation was still found between the score
and the likelihood. This is partly because there were
far fewer Named Entities in the test data. Also,
many word pairs included in the test data incorrectly
capture a noun phrase boundary, which may con-
tain an incomplete Named Entity. We think that this
problem can be solved by using a chunk of words
instead of two consecutive words. Another notable
example in the multi-word ranking is a quoted word
pair from the same speech. Since a news article
sometime quotes a person?s speech literally, such
word pairs are likely to appear at the same time in
both newspapers. However, since multi-word ex-
pressions are much more varied than single-word
ones, the overall frequency of multi-word expres-
sions is lower, which makes such coincidence easily
stand out. We think that this kind of problem can be
alleviated to some degree by eliminating completely
identical sentences from comparable articles.
The obtained ranking of word pairs are listed in
Table 5. The relationship between the score of word
pairs and the likelihood of being Named Entities is
plotted in Figure 3.
5 Conclusion and Future Work
In this paper we described a novel way to discover
Named Entities by using the time series distribution
Word Type Freq. Score
thai nation ORG. 82 0.425
united network ORG. 31 0.290
government open - 87 0.237
club royale ORG. 32 0.142
columnist pat - 81 0.111
muslim minister - 28 0.079
main antenna - 22 0.073
great escape PRODUCT 32 0.059
american black - 38 0.051
patrick swayze PERSON 112 0.038
finds unacceptable - 19 0.034
mayor ron PERSON 49 0.032
babi yar LOCATION 34 0.028
bet secret - 97 0.018
u.s. passport - 58 0.017
thursday proposed - 60 0.014
atlantic command ORG. 30 0.013
prosecutors asked - 73 0.011
unmistakable message - 25 0.010
fallen hero - 12 0.008
american electronics ORG. 65 0.007
primary goal - 138 0.007
beach boys ORG. 119 0.006
amnon rubinstein PERSON 31 0.005
annual winter - 43 0.004
television interviewer - 123 0.003
outside simpson - 76 0.003
electronics firm - 39 0.002
sanctions lifted - 83 0.001
netherlands antilles LOCATION 29 0.001
make tough - 60 0.000
permanent exhibit - 17 0.000
Table 5: Ranking Result (Multi-word)
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.02  0.04  0.06  0.08  0.1
Lik
elih
ood
Score
Figure 3: Relationship of the score and the likeli-
hood of being a Named Entity (Multi-word). The
horizontal axis shows the score of a word. The ver-
tical axis shows the likelihood of being a NE.
of names. Since Named Entities in comparable doc-
uments tend to appear synchronously, one can find a
Named Entity by looking for a word whose chrono-
logical distribution is similar among several compa-
rable documents. We conducted an experiment with
several newspapers because news articles are gener-
ally sorted chronologically, and they are abundant in
comparable documents. We confirmed that there is
some correlation between the similarity of the time
series distribution of a word and the likelihood of
being a Named Entity.
We think that the number of obtained Named En-
tities in our experiment was still not enough. So
we expect that better performance in actual Named
Entity tagging can be achieved by combining this
feature with other contextual or lexical knowledge,
mainly used in existing Named Entity taggers.
6 Acknowledgments
This research was supported in part by the De-
fense Advanced Research Projects Agency as part
of the Translingual Information Detection, Extrac-
tion and Summarization (TIDES) program, un-
der Grant N66001-001-1-8917 from the Space and
Naval Warfare Systems Center, San Diego, and by
the National Science Foundation under Grant ITS-
00325657. This paper does not necessarily reflect
the position of the U.S. Government.
References
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of ACL/EACL 2001.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
Proceedings of EMNLP 1999.
Satoshi Sekine and Hitoshi Isahara. 2000. IREX:
IR and IE evaluation-based project in Japanese.
In Proceedings of LREC 2000.
Satoshi Sekine, Kiyoshi Sudo, and Chikashi No-
bata. 2002. Extended named entity hierarchy. In
Proceedings of LREC 2002.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In
Proceedings of International Workshop on Para-
phrasing 2003.
Tomek Strzalkowski and Jin Wang. 1996. A self-
learning universal concept spotter. In Proceed-
ings of COLING 1996.
Roman Yangarber, Winston Lin, and Ralph Grish-
man. 2002. Unsupervised learning of general-
ized names. In Proceedings of COLING 2002.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 304?311,
New York, June 2006. c?2006 Association for Computational Linguistics
Preemptive Information Extraction using Unrestricted Relation Discovery
Yusuke Shinyama Satoshi Sekine
New York University
715, Broadway, 7th Floor
New York, NY, 10003
{yusuke,sekine}@cs.nyu.edu
Abstract
We are trying to extend the boundary of
Information Extraction (IE) systems. Ex-
isting IE systems require a lot of time and
human effort to tune for a new scenario.
Preemptive Information Extraction is an
attempt to automatically create all feasible
IE systems in advance without human in-
tervention. We propose a technique called
Unrestricted Relation Discovery that dis-
covers all possible relations from texts and
presents them as tables. We present a pre-
liminary system that obtains reasonably
good results.
1 Background
Every day, a large number of news articles are cre-
ated and reported, many of which are unique. But
certain types of events, such as hurricanes or mur-
ders, are reported again and again throughout a year.
The goal of Information Extraction, or IE, is to re-
trieve a certain type of news event from past articles
and present the events as a table whose columns are
filled with a name of a person or company, accord-
ing to its role in the event. However, existing IE
techniques require a lot of human labor. First, you
have to specify the type of information you want and
collect articles that include this information. Then,
you have to analyze the articles and manually craft
a set of patterns to capture these events. Most exist-
ing IE research focuses on reducing this burden by
helping people create such patterns. But each time
you want to extract a different kind of information,
you need to repeat the whole process: specify arti-
cles and adjust its patterns, either manually or semi-
automatically. There is a bit of a dangerous pitfall
here. First, it is hard to estimate how good the sys-
tem can be after months of work. Furthermore, you
might not know if the task is even doable in the first
place. Knowing what kind of information is easily
obtained in advance would help reduce this risk.
An IE task can be defined as finding a relation
among several entities involved in a certain type of
event. For example, in the MUC-6 management
succession scenario, one seeks a relation between
COMPANY, PERSON and POST involved with hir-
ing/firing events. For each row of an extracted ta-
ble, you can always read it as ?COMPANY hired
(or fired) PERSON for POST.? The relation between
these entities is retained throughout the table. There
are many existing works on obtaining extraction pat-
terns for pre-defined relations (Riloff, 1996; Yangar-
ber et al, 2000; Agichtein and Gravano, 2000; Sudo
et al, 2003).
Unrestricted Relation Discovery is a technique to
automatically discover such relations that repeatedly
appear in a corpus and present them as a table, with
absolutely no human intervention. Unlike most ex-
isting IE research, a user does not specify the type
of articles or information wanted. Instead, a system
tries to find all the kinds of relations that are reported
multiple times and can be reported in tabular form.
This technique will open up the possibility of try-
ing new IE scenarios. Furthermore, the system itself
can be used as an IE system, since an obtained re-
lation is already presented as a table. If this system
works to a certain extent, tuning an IE system be-
comes a search problem: all the tables are already
built ?preemptively.? A user only needs to search
for a relevant table.
304
Article dump be-hit
2005-09-23 Katrina New Orleans
2005-10-02 Longwang Taiwan
2005-11-20 Gamma Florida
Keywords: storm, evacuate, coast, rain, hurricane
Table 1: Sample discovered relation.
We implemented a preliminary system for this
technique and obtained reasonably good perfor-
mance. Table 1 is a sample relation that was ex-
tracted as a table by our system. The columns of the
table show article dates, names of hurricanes and the
places they affected respectively. The headers of the
table and its keywords were also extracted automat-
ically.
2 Basic Idea
In Unrestricted Relation Discovery, the discovery
process (i.e. creating new tables) can be formulated
as a clustering task. The key idea is to cluster a set
of articles that contain entities bearing a similar rela-
tion to each other in such a way that we can construct
a table where the entities that play the same role are
placed in the same column.
Suppose that there are two articles A and B,
and both report hurricane-related news. Article A
contains two entities ?Katrina? and ?New Orleans?,
and article B contains ?Longwang? and ?Taiwan?.
These entities are recognized by a Named Entity
(NE) tagger. We want to discover a relation among
them. First, we introduce a notion called ?basic
pattern? to form a relation. A basic pattern is a
part of the text that is syntactically connected to
an entity. Some examples are ?X is hit? or ?Y?s
residents?. Figure 1 shows several basic patterns
connected to the entities ?Katrina? and ?New Or-
leans? in article A. Similarly, we obtain the basic
patterns for article B. Now, in Figure 2, both enti-
ties ?Katrina? and ?Longwang? have the basic pat-
tern ?headed? in common. In this case, we connect
these two entities to each other. Furthermore, there
is also a common basic pattern ?was-hit? shared by
?New Orleans? and ?Taiwan?. Now, we found two
sets of entities that can be placed in correspondence
at the same time. What does this mean? We can infer
that both entity sets (?Katrina?-?New Orleans? and
?Longwang?-?Taiwan?) represent a certain relation
that has something in common: a hurricane name
Katrina New Orleans
headed
threatened
is-category-5
...
was-hit
has-been-evacuated
-residents
...
Basic patternsfor entity "Katrina" Basic patternsfor entity "New Orleans"article A
1. 2.
. . . . . . . . .
Figure 1: Obtaining basic patterns.
Katrina New Orleans
headed
threatened
is-category-5
...
was-hit
has-been-evacuated
-residents
...article A
1. 2.
. . . . . . . . .
Longwang Taiwan
hit
headed
swirling
...
?s-coast
was-pounded
was-hit
...article B
1. 2.
. . . . . . . . .
Common Pattern"stroke" Common Pattern"was-hit"
article A Katrina New Orleans
article B Longwang Taiwan
ObtainedTable
Figure 2: Finding a similar relation from two articles.
and the place it affected. By finding multiple par-
allel correspondences between two articles, we can
estimate the similarity of their relations.
Generally, in a clustering task, one groups items
by finding similar pairs. After finding a pair of arti-
cles that have a similar relation, we can bring them
into the same cluster. In this case, we cluster articles
by using their basic patterns as features. However,
each basic pattern is still connected to its entity so
that we can extract the name from it. We can con-
sider a basic pattern to represent something like the
?role? of its entity. In this example, the entities that
had ?headed? as a basic pattern are hurricanes, and
the entities that had ?was-hit? as a basic pattern are
the places it affected. By using basic patterns, we
can align the entities into the corresponding column
that represents a certain role in the relation. From
this example, we create a two-by-two table, where
each column represents the roles of the entities, and
each row represents a different article, as shown in
the bottom of Figure 2.
We can extend this table by finding another article
305
in the same manner. In this way, we gradually extend
a table while retaining a relation among its columns.
In this example, the obtained table is just what an IE
system (whose task is to find a hurricane name and
the affected place) would create.
However, these articles might also include other
things, which could represent different relations. For
example, the governments might call for help or
some casualties might have been reported. To ob-
tain such relations, we need to choose different en-
tities from the articles. Several existing works have
tried to extract a certain type of relation by manu-
ally choosing different pairs of entities (Brin, 1998;
Ravichandran and Hovy, 2002). Hasegawa et al
(2004) tried to extract multiple relations by choos-
ing entity types. We assume that we can find such
relations by trying all possible combinations from
a set of entities we have chosen in advance; some
combinations might represent a hurricane and gov-
ernment relation, and others might represent a place
and its casualties. To ensure that an article can have
several different relations, we let each article belong
to several different clusters.
In a real-world situation, only using basic patterns
sometimes gives undesired results. For example,
?(President) Bush flew to Texas? and ?(Hurricane)
Katrina flew to New Orleans? both have a basic pat-
tern ?flew to? in common, so ?Bush? and ?Kat-
rina? would be put into the same column. But we
want to separate them in different tables. To allevi-
ate this problem, we put an additional restriction on
clustering. We use a bag-of-words approach to dis-
criminate two articles: if the word-based similarity
between two articles is too small, we do not bring
them together into the same cluster (i.e. table). We
exclude names from the similarity calculation at this
stage because we want to link articles about the same
type of event, not the same instance. In addition, we
use the frequency of each basic pattern to compute
the similarity of relations, since basic patterns like
?say? or ?have? appear in almost every article and it
is dangerous to rely on such expressions.
Increasing Basic Patterns
In the above explanation, we have assumed that we
can obtain enough basic patterns from an article.
However, the actual number of basic patterns that
one can find from a single article is usually not
enough, because the number of sentences is rather
small in comparison to the variation of expressions.
So having two articles that have multiple basic pat-
terns in common is very unlikely. We extend the
number of articles for obtaining basic patterns by
using a cluster of comparable articles that report the
same event instead of a single article. We call this
cluster of articles a ?basic cluster.? Using basic clus-
ters instead of single articles also helps to increase
the redundancy of data. We can give more confi-
dence to repeated basic patterns.
Note that the notion of ?basic cluster? is different
from the clusters used for creating tables explained
above. In the following sections, a cluster for creat-
ing a table is called a ?metacluster,? because this is
a cluster of basic clusters. A basic cluster consists
of a set of articles that report the same event which
happens at a certain time, and a metacluster consists
of a set of events that contain the same relation over
a certain period.
We try to increase the number of articles in a basic
cluster by looking at multiple news sources simulta-
neously. We use a clustering algorithm that uses a
vector-space-model to obtain basic clusters. Then
we apply cross-document coreference resolution to
connect entities of different articles within a basic
cluster. This way, we can increase the number of ba-
sic patterns connected to each entity. Also, it allows
us to give a weight to entities. We calculate their
weights using the number of occurrences within a
cluster and their position within an article. These
entities are used to obtain basic patterns later.
We also use a parser and tree normalizer to gen-
erate basic patterns. The format of basic patterns
is crucial to performance. We think a basic pat-
tern should be somewhat specific, since each pat-
tern should capture an entity with some relevant con-
text. But at the same time a basic pattern should
be general enough to reduce data sparseness. We
choose a predicate-argument structure as a natural
solution for this problem. Compared to traditional
constituent trees, a predicate-argument structure is
a higher-level representation of sentences that has
gained wide acceptance from the natural language
community recently. In this paper we used a logical
feature structure called GLARF proposed by Mey-
ers et al (2001a). A GLARF converter takes a syn-
tactic tree as an input and augments it with several
306
Katrina
hit
coast
SBJ OBJ
Louisiana
T-POS
?s
SUFFIX
Figure 3: GLARF structure of the sentence ?Katrina
hit Louisiana?s coast.?
features. Figure 3 shows a sample GLARF structure
obtained from the sentence ?Katrina hit Louisiana?s
coast.? We used GLARF for two reasons: first,
unlike traditional constituent parsers, GLARF has
an ability to regularize several linguistic phenom-
ena such as participial constructions and coordina-
tion. This allows us to handle this syntactic variety
in a uniform way. Second, an output structure can
be easily converted into a directed graph that rep-
resents the relationship between each word, without
losing significant information from the original sen-
tence. Compared to an ordinary constituent tree, it is
easier to extract syntactic relationships. In the next
section, we discuss how we used this structure to
generate basic patterns.
3 Implementation
The overall process to generate basic patterns and
discover relations from unannotated news articles is
shown in Figure 4. Theoretically this could be a
straight pipeline, but due to the nature of the im-
plementation we process some stages separately and
combine them in the later stage. In the following
subsection, we explain each component.
3.1 Web Crawling and Basic Clustering
First of all, we need a lot of news articles from mul-
tiple news sources. We created a simple web crawler
that extract the main texts from web pages. We ob-
served that the crawler can correctly take the main
texts from about 90% of the pages from each news
site. We ran the crawler every day on several news
sites. Then we applied a simple clustering algorithm
to the obtained articles in order to find a set of arti-
Web
Crawling
Basic
Clustering
Coreference
Resolution
Parsing
GLARFing
Basic Pattern
Generation
Metaclustering
Newspapers...
... BasicClusters
Basic Patterns
Metaclusters
(Tables)
Figure 4: System overview.
cles that talk about exactly the same news and form
a basic cluster.
We eliminate stop words and stem all the other
words, then compute the similarity between two ar-
ticles by using a bag-of-words approach. In news
articles, a sentence that appears in the beginning of
an article is usually more important than the others.
So we preserved the word order to take into account
the location of each sentence. First we computed a
word vector from each article:
Vw(A) = IDF(w)
?
i?POS(w,A)
exp(? iavgwords)
where Vw(A) is a vector element of word w in article
A, IDF (w) is the inverse document frequency of
word w, and POS(w,A) is a list of w?s positions
in the article. avgwords is the average number of
words for all articles. Then we calculated the cosine
value of each pair of vectors:
Sim(A1, A2) = cos(V (A1) ? V (A2))
We computed the similarity of all possible pairs of
articles from the same day, and selected the pairs
307
whose similarity exceeded a certain threshold (0.65
in this experiment) to form a basic cluster.
3.2 Parsing and GLARFing
After getting a set of basic clusters, we pass them
to an existing statistical parser (Charniak, 2000) and
rule-based tree normalizer to obtain a GLARF struc-
ture for each sentence in every article. The current
implementation of a GLARF converter gives about
75% F-score using parser output. For the details of
GLARF representation and its conversion, see Mey-
ers et al (2001b).
3.3 NE Tagging and Coreference Resolution
In parallel with parsing and GLARFing, we also ap-
ply NE tagging and coreference resolution for each
article in a basic cluster. We used an HMM-based
NE tagger whose performance is about 85% in F-
score. This NE tagger produces ACE-type Named
Entities 1: PERSON, ORGANIZATION, GPE, LO-
CATION and FACILITY 2. After applying single-
document coreference resolution for each article, we
connect the entities among different articles in the
same basic cluster to obtain cross-document coref-
erence entities with simple string matching.
3.4 Basic Pattern Generation
After getting a GLARF structure for each sentence
and a set of documents whose entities are tagged
and connected to each other, we merge the two out-
puts and create a big network of GLARF structures
whose nodes are interconnected across different sen-
tences/articles. Now we can generate basic patterns
for each entity. First, we compute the weight for
each cross-document entity E in a certain basic clus-
ter as follows:
WE =
?
e?E
mentions(e) ? exp(?C ? firstsent(e))
where e ? E is an entity within one article and
mentions(e) and firstsent(e) are the number of
mentions of entity e in a document and the position
1The ACE task description can be found at
http://www.itl.nist.gov/iad/894.01/tests/ace/ and the ACE
guidelines at http://www.ldc.upenn.edu/Projects/ACE/
2The hurricane names used in the examples were recognized
as PERSON.
Katrina
hit
coast
SBJ OBJ
Louisiana
T-POS
?s
SUFFIX
GPE+T-POS:coast
PER+SBJ:hit
PER+SBJ:hit-OBJ:coast
Figure 5: Basic patterns obtained from the sentence
?Katrina hit Louisiana?s coast.?
of the sentence where entity e first appeared, respec-
tively. C is a constant value which was 0.5 in this ex-
periment. To reduce combinatorial complexity, we
took only the five most highly weighted entities from
each basic cluster to generate basic patterns. We ob-
served these five entities can cover major relations
that are reported in a basic cluster.
Next, we obtain basic patterns from the GLARF
structures. We used only the first ten sentences
in each article for getting basic patterns, as most
important facts are usually written in the first few
sentences of a news article. Figure 5 shows all
the basic patterns obtained from the sentence ?Ka-
trina hit Louisiana?s coast.? The shaded nodes
?Katrina? and ?Louisiana? are entities from which
each basic pattern originates. We take a path
of GLARF nodes from each entity node until it
reaches any predicative node: noun, verb, or ad-
jective in this case. Since the nodes ?hit? and
?coast? can be predicates in this example, we ob-
tain three unique paths ?Louisiana+T-POS:coast
(Louisiana?s coast)?, ?Katrina+SBJ:hit (Katrina
hit something)?, and ?Katrina+SBJ:hit-OBJ:coast
(Katrina hit some coast)?.
To increase the specificity of patterns, we generate
extra basic patterns by adding a node that is imme-
diately connected to a predicative node. (From this
example, we generate two basic patterns: ?hit? and
?hit-coast? from the ?Katrina? node.)
Notice that in a GLARF structure, the type
of each argument such as subject or object is
preserved in an edge even if we extract a sin-
gle path of a graph. Now, we replace both
entities ?Katrina? and ?Louisiana? with variables
308
based on their NE tags and obtain parameter-
ized patterns: ?GPE+T-POS:coast (Louisiana?s
coast)?, ?PER+SBJ:hit (Katrina hit something)?,
and ?PER+SBJ:hit-OBJ:coast (Katrina hit some
coast)?.
After taking all the basic patterns from every basic
cluster, we compute the Inverse Cluster Frequency
(ICF) of each unique basic pattern. ICF is similar
to the Inverse Document Frequency (IDF) of words,
which is used to calculate the weight of each basic
pattern for metaclustering.
3.5 Metaclustering
Finally, we can perform metaclustering to obtain ta-
bles. We compute the similarity between each basic
cluster pair, as seen in Figure 6. XA and XB are
the set of cross-document entities from basic clusters
cA and cB , respectively. We examine all possible
mappings of relations (parallel mappings of multi-
ple entities) from both basic clusters, and find all the
mappings M whose similarity score exceeds a cer-
tain threshold. wordsim(cA, cB) is the bag-of-words
similarity of two clusters. As a weighting function
we used ICF:
weight(p) = ? log(clusters that include pall clusters )
We then sort the similarities of all possible pairs
of basic clusters, and try to build a metacluster by
taking the most strongly connected pair first. Note
that in this process we may assign one basic clus-
ter to several different metaclusters. When a link is
found between two basic clusters that were already
assigned to a metacluster, we try to put them into
all the existing metaclusters it belongs to. However,
we allow a basic cluster to be added only if it can
fill all the columns in that table. In other words, the
first two basic clusters (i.e. an initial two-row table)
determines its columns and therefore define the re-
lation of that table.
4 Experiment and Evaluation
We used twelve newspapers published mainly in the
U.S. We collected their articles over two months
(from Sep. 21, 2005 - Nov. 27, 2005). We obtained
643,767 basic patterns and 7,990 unique types. Then
we applied metaclustering to these basic clusters
Source articles 28,009
Basic clusters 5,543
Basic patterns (token) 643,767
Basic patterns (type) 7,990
Metaclusters 302
Metaclusters (rows ? 3) 101
Table 2: Articles and obtained metaclusters.
and obtained 302 metaclusters (tables). We then re-
moved duplicated rows and took only the tables that
had 3 or more rows. Finally we had 101 tables. The
total number the of articles and clusters we used are
shown in Table 2.
4.1 Evaluation Method
We evaluated the obtained tables as follows. For
each row in a table, we added a summary of the
source articles that were used to extract the rela-
tion. Then for each table, an evaluator looks into
every row and its source article, and tries to come
up with a sentence that explains the relation among
its columns. The description should be as specific as
possible. If at least half of the rows can fit the ex-
planation, the table is considered ?consistent.? For
each consistent table, the evaluator wrote down the
sentence using variable names ($1, $2, ...) to refer
to its columns. Finally, we counted the number of
consistent tables. We also counted how many rows
in each table can fit the explanation.
4.2 Results
We evaluated 48 randomly chosen tables. Among
these tables, we found that 36 tables were consis-
tent. We also counted the total number of rows that
fit each description, shown in Table 3. Table 4 shows
the descriptions of the selected tables. The largest
consistent table was about hurricanes (Table 5). Al-
though we cannot exactly measure the recall of each
table, we tried to estimate the recall by comparing
this hurricane table to a manually created one (Table
6). We found 6 out of 9 hurricanes 3. It is worth
noting that most of these hurricane names were au-
tomatically disambiguated although our NE tagger
didn?t distinguish a hurricane name from a person
3Hurricane Katrina and Longwang shown in the previous
examples are not included in this table. They appeared before
this period.
309
for each cluster pair (cA, cB) {
XA = cA.entities
XB = cB .entities
for each entity mapping M = [(xA1, xB1), ..., (xAn, xBn)] ? (2|XA| ? 2|XB |) {
for each entity pair (xAi, xBi) {
Pi = xAi.patterns ? xBi.patterns
pairscorei =
?
p?Pi weight(p)
}
mapscore =
?
pairscorei
if T1 < |M | and T2 < mapscore and T3 < wordsim(cA.words, cB .words) {
link cA and cB with mapping M .
}
}
}
Figure 6: Computing similarity of basic clusters.
Tables:
Consistent tables 36 (75%)
Inconsistent tables 12
Total 48
Rows:
Rows that fit the description 118 (73%)
Rows not fitted 43
Total 161
Table 3: Evaluation results.
Description Rows
Storm $1(PER) probably affected $2(GPE). 8/16
Nominee $2(PER) must be confirmed by $1(ORG). 4/7
$1(PER) urges $2(GPE) to make changes. 4/6
$1(GPE) launched an attack in $2(GPE). 3/5
$1(PER) ran against $2(PER) in an election. 4/5
$2(PER) visited $1(GPE) on a diplomatic mission. 2/4
$2(PER) beat $1(PER) in golf. 4/4
$2(GPE) soldier(s) were killed in $1(GPE). 3/3
$2(PER) ran for governor of $1(GPE). 2/3
Boxer $1(PER) fought boxer $2(PER). 3/3
Table 4: Description of obtained tables and the num-
ber of fitted/total rows.
name. The second largest table (about nominations
of officials) is shown in Table 7.
We reviewed 10 incorrect rows from various ta-
bles and found 4 of them were due to coreference er-
rors and one error was due to a parse error. The other
4 errors were due to multiple basic patterns distant
from each other that happened to refer to a different
event reported in the same cluster. The causes of the
one remaining error was obscure. Most inconsistent
tables were a mixture of multiple relations and some
of their rows still looked consistent.
We have a couple of open questions. First, the
overall recall of our system might be lower than ex-
isting IE systems, as we are relying on a cluster of
comparable articles rather than a single document to
discover an event. We might be able to improve this
in the future by adjusting the basic clustering algo-
rithm or weighting schema of basic patterns. Sec-
ondly, some combinations of basic patterns looked
inherently vague. For example, we used the two ba-
sic patterns ?pitched? and ??s-series? in the fol-
lowing sentence (the patterns are underlined):
Ervin Santana pitched 5 1-3 gutsy innings in his post-
season debut for the Angels, Adam Kennedy hit a go-
ahead triple that sent Yankees outfielders crashing to the
ground, and Los Angeles beat New York 5-3 Monday
night in the decisive Game 5 of their AL playoff series.
It is not clear whether this set of patterns can yield
any meaningful relation. We are not sure how much
this sort of table can affect overall IE performance.
5 Conclusion
In this paper we proposed Preemptive Information
Extraction as a new direction of IE research. As
its key technique, we presented Unrestricted Rela-
tion Discovery that tries to find parallel correspon-
dences between multiple entities in a document, and
perform clustering using basic patterns as features.
To increase the number of basic patterns, we used
a cluster of comparable articles instead of a single
document. We presented the implementation of our
preliminary system and its outputs. We obtained
dozens of usable tables.
310
Article 1:dump 2:coast
2005-09-21 (1) Rita Texas
2005-09-23 Rita New Orleans
2005-09-25 Bush Texas
2005-09-26 Damrey Hainan
2005-09-27 (2) Damrey Vietnam
2005-10-01 Rita Louisiana
2005-10-02 Otis Mexico
2005-10-04 Longwang China
2005-10-05 Stan Mexico
2005-10-06 Tammy Florida
2005-10-07 Tammy Georgia
2005-10-19 (3) Wilma Florida
2005-10-25 Wilma Cuba
2005-10-25 Wilma Massachusetts
2005-10-28 Beta Nicaragua
2005-11-20 Gamma Florida
1. More than 2,000 National Guard troops were put on
active-duty alert to assist as Rita slammed into the string
of islands and headed west, perhaps toward Texas. ...
2. Typhoon Damrey smashed into Vietnam on Tuesday af-
ter killing nine people in China, ...
3. Oil markets have been watching Wilma?s progress ner-
vously, ... but the threat to energy interests appears to
have eased as forecasters predict the storm will turn to-
ward Florida. ...
Table 5: Hurricane table (?Storm $1(PER) probably
affected $2(GPE).?) and the actual expressions we
used for extraction.
Hurricane Date (Affected Place) Articles
Philippe Sep 17-20 (?) 6
* Rita Sep 17-26 (Louisiana, Texas, etc.) 566
* Stan Oct 1-5 (Mexico, Nicaragua, etc.) 83
* Tammy Oct 5-? (Georgia, Alabama) 18
Vince Oct 8-11 (Portugal, Spain) 12
* Wilma Oct 15-25 (Cuba, Honduras, etc.) 368
Alpha Oct 22-24 (Haiti, Dominican Rep.) 80
* Beta Oct 26-31 (Nicaragua, Honduras) 55
* Gamma Nov 13-20 (Belize, etc.) 36
Table 6: Hurricanes in North America between mid-
Sep. and Nov. (from Wikipedia). Rows with a
star (*) were actually extracted. The number of the
source articles that contained a mention of the hurri-
cane is shown in the right column.
Article 1:confirm 2:be-confirmed
2005-09-21 Senate Roberts
2005-10-03 Supreme Court Miers
2005-10-20 Senate Bush
2005-10-26 Senate Sauerbrey
2005-10-31 Senate Mr. Alito
2005-11-04 Senate Alito
2005-11-17 Fed Bernanke
Table 7: Nomination table (?Nominee $2(PER)
must be confirmed by $1(ORG).?)
Acknowledgements
This research was supported by the National Science
Foundation under Grant IIS-00325657. This paper
does not necessarily reflect the position of the U.S.
Government. We would like to thank Prof. Ralph
Grishman who provided useful suggestions and dis-
cussions.
References
Eugene Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting Relations from Large Plaintext Collections. In
Proceedings of the 5th ACM International Conference
on Digital Libraries (DL-00).
Sergey Brin. 1998. Extracting Patterns and Relations
from the World Wide Web. In WebDB Workshop at
EDBT ?98.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-2000.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of the Annual
Meeting of Association of Computational Linguistics
(ACL-04).
Adam Meyers, Ralph Grishman, Michiko Kosaka, and
Shubin Zhao. 2001a. Covering Treebanks with
GLARF. In ACL/EACL Workshop on Sharing Tools
and Resources for Research and Education.
Adam Meyers, Michiko Kosaka, Satoshi Sekine, Ralph
Grishman, and Shubin Zhao. 2001b. Parsing and
GLARFing. In Proceedings of RANLP-2001, Tzigov
Chark, Bulgaria.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Ellen Riloff. 1996. Automatically Generating Extrac-
tion Patterns from Untagged Text. In Proceedings of
the 13th National Conference on Artificial Intelligence
(AAAI-96).
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An Improved Extraction Pattern Representa-
tion Model for Automatic IE Pattern Acquisition. In
Proceedings of the Annual Meeting of Association of
Computational Linguistics (ACL-03).
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Unsupervised Discovery
of Scenario-Level Patterns for Information Extraction.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING-00).
311
Paraphrase Acquisition for Information Extraction
Yusuke Shinyama
Department of Computer Science
New York University
715, Broadway, 7th Floor, NY, 10003
yusuke@cs.nyu.edu
Satoshi Sekine
Department of Computer Science
New York University
715, Broadway, 7th Floor, NY, 10003
sekine@cs.nyu.edu
Abstract
We are trying to find paraphrases from
Japanese news articles which can be used
for Information Extraction. We focused
on the fact that a single event can be re-
ported in more than one article in differ-
ent ways. However, certain kinds of noun
phrases such as names, dates and numbers
behave as ?anchors? which are unlikely to
change across articles. Our key idea is to
identify these anchors among comparable
articles and extract portions of expressions
which share the anchors. This way we
can extract expressions which convey the
same information. Obtained paraphrases
are generalized as templates and stored for
future use.
In this paper, first we describe our ba-
sic idea of paraphrase acquisition. Our
method is divided into roughly four steps,
each of which is explained in turn. Then
we illustrate several issues which we en-
counter in real texts. To solve these prob-
lems, we introduce two techniques: coref-
erence resolution and structural restriction
of possible portions of expressions. Fi-
nally we discuss the experimental results
and conclusions.
1 Introduction
We are trying to obtain paraphrases which can be
used for Information Extraction (IE) systems. IE
systems scan articles and retrieve specific informa-
tion which is required for a certain domain defined
in advance. Currently, many IE tasks are performed
by pattern matching. For example, if the system re-
ceives a sentence ?Two more people have died in
Hong Kong from SARS,? and the system has a pat-
tern ?NUMBER people die in LOCATION? in its in-
ventory, then the system can apply the pattern to
the sentence and fill the slots, and obtain informa-
tion such as ?NUMBER = two more, LOCATION =
Hong Kong?. In most IE systems, the performance
of the system is dependent on these well-designed
patterns.
In natural language sentences, a single event can
be expressed in many different ways. So we need
to prepare patterns for various kinds of expressions
used in articles. We are interested in clustering
IE patterns which capture the same information.
For example, a pattern such as ?LOCATION reports
NUMBER deaths? can be used for the same purpose
as the previous one, since this pattern could also cap-
ture the casualties occurring in a certain location.
Prior work to relate two IE patterns was reported by
(Shinyama et al, 2002). However, in this attempt
only limited forms of expressions could be obtained.
Furthermore, the obtained paraphrases were limited
to existing IE patterns only. We are interested in col-
lecting various kinds of clues, including similar IE
patterns themselves, to connect two patterns. In this
paper, we tried to obtain more varied paraphrases.
Although our current method is intended for use in
Information Extraction, we think the same approach
can be applied to obtain paraphrases for other pur-
poses, such as machine translation or text summa-
rization.
There have been several attempts to obtain para-
phrases. (Barzilay and McKeown, 2001) applied
text alignment to parallel translations of a single
text and used a part-of-speech tagger to obtain para-
phrases. (Lin and Pantel, 2001) used mutual infor-
mation of word distribution to calculate the simi-
larity of expressions. (Pang et al, 2003) also used
text alignment and obtained a finite state automaton
which generates paraphrases. (Ravichandran and
Hovy, 2002) used pairs of questions and answers
to obtain varied patterns which give the same an-
swer. Our approach is different from these works in
that we used comparable news articles as a source of
paraphrases and used Named Entity tagging and de-
pendency analysis to extract corresponding expres-
sions.
2 Overall Procedure of Paraphrase
Acquisition
Our main goal is to obtain pattern clusters for IE,
which consist of sets of equivalent patterns captur-
ing the same information. So we tried to discover
paraphrases contained in Japanese news articles for
a specific domain. Our basic idea is to search news
articles from the same day. We focused on the fact
that various newspapers describe a single event in
different ways. So if we can discover an event
which is reported in more then one newspaper, we
can hope these articles can be used as the source of
paraphrases. For example, the following articles ap-
peared in ?Health? sections in different newspapers
on Apr. 11:
1. ?The government has announced that two more
people have died in Hong Kong after contract-
ing the SARS virus and 61 new cases of the
illness have been detected.? (Reuters, Apr. 11)
2. ?Hong Kong reported two more deaths and 61
fresh cases of SARS Friday as governments
across the world took tough steps to stop the
killer virus at their borders.? (Channel News
Asia, Apr. 11)
In these articles, we can find several correspond-
ing parts, such as ?NUMBER people have died in
LOCATION? and ?LOCATION reported NUMBER
deaths?. Although their syntactic structures are dif-
ferent, they still convey the same single fact. Here
it is worth noting that even if a different expression
is used, some noun phrases such as ?Hong Kong?
or ?two more? are preserved across the two arti-
cles. We found that these words shared by the two
sentences provide firm anchors for two different ex-
pressions. In particular, Named Entities (NEs) such
as names, locations, dates or numbers can be the
firmest anchors since they are indispensable to re-
port an event and difficult to paraphrase.
We tried to obtain paraphrases by using this prop-
erty. First we collect a set of comparable articles
which reports the same event, and pull appropriate
portions out of the sentences which share the same
anchors. If we carefully choose appropriate portions
of the sentences, the extracted expressions will con-
vey the same information; i.e. they are paraphrases.
After corresponding portions are obtained, we gen-
eralize the expressions to templates of paraphrases
which can be used in future.
Our method is divided into four steps:
1. Find comparable sentences which report the
same event from different newspapers.
2. Identify anchors in the comparable sentences.
3. Extract corresponding portions from the sen-
tences.
4. Generalize the obtained expressions to para-
phrase templates.
Figure 1 shows the overall procedure. In the re-
mainder of this section, we describe each step in
turn.
2.1 Find Comparable Sentences
To find comparable articles and sentences, we used
methods developed for Topic Detection and Track-
ing (Wayne, 1998). The actual process is divided
into two parts: article level matching and sentence
level matching. Currently we assume that a pair
of paraphrases can be found in a single sentence
of each article and corresponding expressions don?t
range across two or more sentences. Article level
matching is first required to narrow the search space
and reduce erroneous matching of anchors.
Article
A
Article
B
SARS
1
SARS
2
Find comparable
articles and 
sentences
Identify
anchors
Extract
corrsponding
portions
(Articles on
the same day)
anchors paraphrases
"NUMBER
people die
in LOCATION"
"LOCTION
reports
NUMBER
deaths"
Generalize
expressions
Figure 1: The overall procedure
Before applying this technique, we first prepro-
cessed the articles by stripping off the strings which
are not considered as sentences. Then we used a
part-of-speech tagger to obtain segmented words. In
the actual matching process we used a method de-
scribed in (Papka et al, 1999) to find a set of com-
parable articles. Then we use a simple vector space
model for sentence matching.
2.2 Identify Anchors
Before extracting paraphrases, we find anchors in
comparable sentences. We used Extended Named
Entity tagging to identify anchors. A Named Entity
tagger identifies proper expressions such as names,
locations and dates in sentences. In addition to these
expressions, an Extended Named Entity tagger iden-
tifies some common nouns such as disease names or
numbers, that are also unlikely to change (Sekine
et al, 2002). For each corresponding pair of sen-
tences, we apply the tagger and identify the same
noun phrases which appear in both sentences as an-
chors.
2.3 Extract Corresponding Sentence Portions
Now we identify appropriate boundaries of expres-
sions which share the anchors identified in the pre-
vious stage. To avoid extracting non-grammatical
expressions, we operate on syntactically structured
text rather than sequences of words. Dependency
analysis is suitable for this purpose, since using de-
pendency trees we can reconstruct grammatically
correct expressions from a spanning subtree whose
root is a predicate. Dependency analysis also allows
us to extract expressions which are subtrees but do
not correspond to a single contiguous sequence of
words.
We applied a dependency analyzer to a pair of
corresponding sentences and obtained tree structures
for each sentence. Each node of the tree is either a
predicate such as a verb or an adjective, or an argu-
ment such as a noun or a pronoun. Each predicate
can take one or more arguments. We generated all
possible combinations of subtrees from each depen-
dency tree, and compared the anchors which are in-
cluded in both subtrees. After a pair of correspond-
ing subtrees which share the anchors is found, the
subtree pair can be recognized as paraphrases. In ac-
tual experiments, we put some restrictions on these
subtrees, which will be discussed later. This way
we can obtain grammatically well-formed portions
of sentences (Figure 2).
2.4 Generalize Expressions
After corresponding portions are obtained, we gen-
eralize the expressions to form usable templates of
paraphrases. Actually this is already done by Ex-
tended Named Entity tagging. An Extended Named
Entity tagger classifies proper expressions into sev-
eral categories. This is similar to a part-of-speech
tagger as it classifies words into several part-of-
speech categories. For example, ?Hong Kong? is
tagged as a location name, and ?two more? as a
number. So an expression such as ?two more peo-
ple die in Hong Kong? is finally converted into the
form ?NUMBER people die in LOCATION? where
NUMBER and LOCATION are slots to fill in. This
way we obtain expressions which can be used as IE
patterns.
the government
has announced
have died
in Hong Kong
two more people
subject
object
in
subject
Hong Kong
reported
two more deaths
subject
object
predicates predicates
paraphrases
LOCATION is included.
NUMBER is included.
anchors
Figure 2: Extracting portions of sentences
3 Handling Problems in Real Texts
In the previous section we described our method for
obtaining paraphrases in principle. However there
are several issues in actual texts which pose difficul-
ties for our method.
The first one is in finding anchors which refer to
the same entity. In actual articles, names are some-
time referred to in a slightly different form. For ex-
ample, ?President Bush? can also be referred to as
?Mr. Bush?. Additionally, sometime it is referred
to by a pronoun, such as ?he?. Since our method
relies on the fact that those anchors are preserved
across articles, anchors which appear in these var-
ied forms may reduce the actual number of obtained
paraphrases.
To handle this problem, we extended the notion
of anchors to include not just Extended Named En-
tities, but also pronouns and common nouns such
as ?the president?. We used a simple corefer-
ence resolver after Extended Named Entity tag-
ging. Currently this is done by simply assigning
the most recent antecedent to pronouns and finding
a longest common subsequence (LCS) between two
noun groups. Since it is possible to form a com-
pound noun such as ?President-Bush? in Japanese,
we computed LCS for each character in the two
noun groups. We used the following condition to
decide whether two noun groups s1 and s2 are coref-
erential:
? if 2 ? min(|s1|, |s2|) ? |LCS(s1, s2)|, then
s1 and s2 are considered coreferential.
Here |s| denotes the length of noun group s and
LCS(s1, s2) is the LCS of two noun groups s1 and
s2.
The second problem is to extract appropriate por-
tions as paraphrase expressions. Since we use a tree
structure to represent the expressions, finding com-
mon subtrees may take an exponential number of
steps. For example, if a dependency tree in one
article has one single predicate which has n argu-
ments, the number of possible subtrees which can
be obtained from the tree is 2n. So the matching
process between arbitrary combinations of subtrees
may grow exponentially with the length of the sen-
tences. Even worse, it can generate many combina-
tions of sentence portions which don?t make sense as
paraphrases. For example, from the expression ?two
more people have died in Hong Kong? and ?Hong
Kong reported two more deaths?, we could extract
expressions ?in Hong Kong? and ?Hong Kong re-
ported?. Although both of them share one anchor,
this is not a correct paraphrase. To avoid this sort of
error, we need to put some additional restrictions on
the expressions.
(Shinyama et al, 2002) used the frequency of ex-
pressions to filter these incorrect pairs of expres-
sions. First the system obtained a set of IE patterns
from corpora (Sudo and Sekine, 2001), and then cal-
culated the score for each candidate paraphrase by
counting how many times that expression appears as
an IE pattern in the whole corpus. However, with
this method, obtainable expressions are limited to
existing IE patterns only. Since we wanted to ob-
tain a broader range of expressions not limited to
IE patterns themselves, we tried to use other restric-
tions which can be acquired independently of the IE
system.
We partly solve this problem by calculating the
plausibility of each tree structure. In Japanese sen-
tences, the case of each argument which modifies
a predicate is represented by a case marker (post-
position or joshi) which follows a noun phrase, just
like prepositions in English but in the opposite order.
These arguments include subjects and objects that
are elucidated syntactically in English sentences.
We collected frequent cases occurring with a spe-
cific predicate in advance. We applied this restric-
tion when generating subtrees from a dependency
tree by calculating a score for each predicate as fol-
lows:
Let an instance of predicate p have cases C =
{c1, c2, ..., cn} and a function Np(I) be the number
of instances of p in the corpus whose cases are I =
{c1, c2, ..., cm}. We compute the score Sp(C) of the
instance:
Sp(C) =
?
I?C Np(I)
the number of instances of p in the corpus .
Using this metric, a predicate which doesn?t have
cases that it should usually have is given a lower
score. A subtree which includes a predicate whose
score is less than a certain threshold is filtered out.
This way we can filter out expressions such as
?Hong Kong reported? in Japanese since it would
lack an object case which normally the verb ?re-
port? should have. Moreover, this greatly reduces
the number of possible combinations of subtrees.
4 Experiments
We used Japanese news articles for this experi-
ment. First we collected articles for a specific do-
main from two different newspapers (Mainichi and
Nikkei). Then we used a Japanese part-of-speech
tagger (Kurohashi and Nagao, 1998) and Extended
Named Entity tagger to process documents, and put
them into a Topic Detection and Tracking system.
In this experiment, we used a modified version of a
Japanese Extended Named Entity tagger (Uchimoto
et al, 2000). This tagger tags person names, orga-
nization names, locations, dates, times and numbers.
Article pairs:
Obtained Correct
System 195 156
(80%)
Sentence pairs:
(from top 20 article pairs)
Obtained Correct
Manual 93 93
W/o coref. 55 41 (75%)
W coref. 75 52 (69%)
Paraphrase pairs:
Obtained Correct
W/o coref. or restriction 106 25 (24%)
W/o coref., w restriction 32 18 (56%)
W coref. and restriction 37 23 (62%)
Manual (in 5 hours) (100) (100)
Table 1: Results in the murder cases domain
Sample 1:
? PERSON1 killed PERSON2.
? PERSON1 let PERSON2 die from loss of blood.
Sample 2:
? PERSON1 shadowed PERSON2.
? PERSON1 kept his eyes on PERSON2.
Figure 3: Sample correct paraphrases obtained
(translated from Japanese)
Sample 3:
? PERSON1 fled to LOCATION.
? PERSON1 fled and lay in ambush to LOCATION.
Sample 4:
? PERSON1 cohabited with PERSON2.
? PERSON1 murdered in the room for cohabitation
with PERSON2.
Figure 4: Sample incorrect paraphrases obtained
(translated from Japanese)
Next we applied a simple vector space method to ob-
tain pairs of sentences which report the same event.
After that, we used a simple coreference resolver to
identify anchors. Finally we used a dependency an-
alyzer (Kurohashi, 1998) to extract portions of sen-
tences which share at least one anchor.
In this experiment, we used a set of articles which
reports murder cases. The results are shown in Ta-
ble 1. First, with Topic Detection and Tracking,
there were 156 correct pairs of articles out of 193
pairs obtained. To simplify the evaluation process,
we actually obtained paraphrases from the top 20
pairs of articles which had the highest similarities.
Obtained paraphrases were reviewed manually. We
used the following criteria for judging the correct-
ness of paraphrases:
1. They has to be describing the same event.
2. They should capture the same information if we
use them in an actual IE application.
We tried several conditions to extract paraphrases.
First we tried to extract paraphrases using neither
coreference resolution nor case restriction. Then we
applied only the case restriction with the threshold
0.3 < Sp(C), and observed the precision went up
from 24% to 56%. Furthermore, we added a sim-
ple coreference resolution and the precision rose to
62%. We got 23 correct paraphrases. We found
that several interesting paraphrases are obtained.
Some examples are shown in Figure 3 (correct para-
phrases) and Figure 4 (incorrect paraphrases).
It is hard to say how many paraphrases can be ul-
timately obtained from these articles. However, it is
worth noting that after spending about 5 hours for
this corpus we obtained 100 paraphrases manually.
5 Discussion
Some paraphrases were incorrectly obtained. There
were two major causes. The first one was depen-
dency analysis errors. Since our method recognizes
boundaries of expressions using dependency trees, if
some predicates in a tree take extra arguments, this
may result in including extraneous portions of the
sentence in the paraphrase. For example, the predi-
cate ?lay in ambush? in Sample 3 should have taken
a different noun as its subject. If so, the predicate
doesn?t share the anchors any more and could be
eliminated.
The second cause was the lack of recognizing
contexts. In Sample 4, we observed that even if two
expressions share multiple anchors, an obtained pair
can be still incorrect. We hope that this kind of error
can be reduced by considering the contexts around
expressions more extensively.
6 Future Work
We hope to apply our approach further to ob-
tain more varied paraphrases. After a certain
number of paraphrases are obtained, we can use
the obtained paraphrases as anchors to obtain
additional paraphrases. For example, if we know
?A dismantle B? and ?A destroy B? are para-
phrases, we could apply them to ?U.N. reported
Iraq dismantling more missiles? and ?U.N. official
says Iraq destroyed more Al-Samoud 2 missiles?,
and obtain another pair of paraphrases ?X reports Y?
and ?X says Y?.
This approach can be extended in the other direc-
tion. Some entities can be referred to by completely
different names in certain situations, such as ?North
Korea? and ?Pyongyang?. We are also planning to
identify these varied external forms of a single entity
by applying previously obtained paraphrases. For
example, if we know ?A restarted B? and ?A reac-
tivated B? as paraphrases, we could apply them to
?North Korea restarted its nuclear facility? and ?Py-
ongyang has reactivated the atomic facility?. This
way we know ?North Korea? and ?Pyongyang? can
refer to the same entity in a certain context.
In addition, we are planning to give some credi-
bility score to anchors for improving accuracy. We
found that some anchors are less reliable than oth-
ers even if they are considered as proper expres-
sions. For example, in most U.S. newspapers the
word ?U.S.? is used in much wider contexts than
word such as ?Thailand? although both of them are
country names. So we want to give less credit to
these widely used names.
We noticed that there are several issues in general-
izing paraphrases. Currently we simply label every
Named Entity as a slot. However expressions such
as ?the governor of LOCATION? can take only a cer-
tain kind of locations. Also some paraphrases might
require a narrower context than others and are not
truly interchangeable. For example, ?PERSON was
sworn? can be replaced with ?PERSON took office?,
but not vice versa.
7 Conclusions
In this paper, we described a method to obtain para-
phrases automatically from corpora. Our key notion
is to use comparable articles which report the same
event on the same day. Some noun phrases, espe-
cially Extended Named Entities such as names, lo-
cations and numbers, are preserved across articles
even if the event is reported using different expres-
sions. We used these noun phrases as anchors and
extracted portions which share these anchors. Then
we generalized the obtained expressions as usable
paraphrases.
We adopted dependency trees as a format for ex-
pressions which preserve syntactic constraints when
extracting paraphrases. We generate possible sub-
trees from dependency trees and find pairs which
share the anchors. However, simply generating all
subtrees ends up obtaining many inappropriate por-
tions of sentences. We tackled this problem by cal-
culating a score which tells us how plausible ex-
tracted candidates are. We confirmed that it con-
tributed to the overall accuracy. This metric was
also useful to trimming the search space for match-
ing subtrees. We used a simple coreference resolver
to handle some additional anchors such as pronouns.
Acknowledgments
This research is supported by the Defense Advanced
Research Projects Agency as part of the Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES) program, under Grant N66001-
001-1-8917 from the Space and Naval Warfare Sys-
tems Center San Diego, and by the National Science
Foundation under Grant IIS-0081962. This paper
does not necessarily reflect the position or the pol-
icy of the U.S. Government.
References
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting Paraphrases from a Parallel Corpus. In Pro-
ceedings of the ACL/EACL.
Sadao Kurohashi and Makoto Nagao, 1998. Japanese
Morphological Analysis System JUMAN. Kyoto Uni-
versity, version 3.61 edition.
Sadao Kurohashi, 1998. Kurohashi-Nagao parser. Ky-
oto University, version 2.0 b6 edition.
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343?360.
Bo Pang, Kevin Knight, and Danial Marcu. 2003.
Syntax-based Alignment of Multiple Translations: Ex-
tracting Paraphrases and Generating New Sentences.
In NAACL-HLT.
Ron Papka, James Allen, and Victor Lavrenko. 1999.
UMASS Approaches to Detection and Tracking at
TDT2. In DARPA: Broadcast News Workshop.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL).
Satoshi Sekine, Kiyoshi sudo, and Chikashi Nobata.
2002. Extended Named Entity Hierarchy. In Proceed-
ings of the LREC.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic Paraphrase Ac-
quisition from News Articles. In Proceedings of the
Second International Conference on Human Language
Technology Research.
Kiyoshi Sudo and Satoshi Sekine. 2001. Automatic Pat-
tern Acquisition for Japanese Information Extraction.
In Proceedings of the HLT.
Kiyotaka Uchimoto, Masaki Murata, Qing Ma, Hiromi
Ozaku, and Hitoshi Isahara. 2000. Named Entity Ex-
traction Based on A Maximum Entropy Model and
Transformation Rules. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 326?335.
Charles L. Wayne. 1998. Topic Detection & Tracking: A
Case Study in Corpus Creation & Evaluation Method-
ologies. In Proceedings of the LREC.
Proceedings of the Linguistic Annotation Workshop, pages 184?190,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Shared Corpora Working Group Report
Adam Meyers
New York
University
New York, NY
meyers
at cs.nyu.edu
Nancy Ide
Vassar College
Poughkeepsie, NY
ide at cs.vassar.edu
Ludovic Denoyer
University of Paris
Paris, France
ludovic.denoyer
at lip6.fr
Yusuke Shinyama
New York
University
New York, NY
yusuke
at cs.nyu.edu
Abstract
We seek to identify a limited amount of rep-
resentative corpora, suitable for annotation
by the computational linguistics annotation
community. Our hope is that a wide vari-
ety of annotation will be undertaken on the
same corpora, which would facilitate: (1)
the comparison of annotation schemes; (2)
the merging of information represented by
various annotation schemes; (3) the emer-
gence of NLP systems that use informa-
tion in multiple annotation schemes; and (4)
the adoption of various types of best prac-
tice in corpus annotation. Such best prac-
tices would include: (a) clearer demarca-
tion of phenomena being annotated; (b) the
use of particular test corpora to determine
whether a particular annotation task can fea-
sibly achieve good agreement scores; (c)
The use of underlying models for represent-
ing annotation content that facilitate merg-
ing, comparison, and analysis; and (d) To
the extent possible, the use of common an-
notation categories or a mapping among cat-
egories for the same phenomenon used by
different annotation groups.
This study will focus on the problem of
identifying such corpora as well as the suit-
ability of two candidate corpora: the Open
portion of the American National Corpus
(Ide and Macleod, 2001; Ide and Suder-
man, 2004) and the ?Controversial? portions
of the WikipediaXML corpus (Denoyer and
Gallinari, 2006).
1 Introduction
This working group seeks to identify a limited
amount of representative corpora, suitable for an-
notation by the computational linguistics annotation
community. Our hope is that a wide variety of anno-
tation will be undertaken on the same corpora, which
would facilitate:
1. The comparison of annotation schemes
2. The merging of information represented by var-
ious annotation schemes
3. The emergence of NLP systems that use infor-
mation in multiple annotation schemes; and
4. The adoption of various types of best practice
in corpus annotation, including:
(a) Clearer demarcation of the phenomena be-
ing annotated. Thus if predicate argu-
ment structure annotation adequately han-
dles relative pronouns, a new project that
is annotating coreference is less likely to
include relative pronouns in their annota-
tion; and
(b) The use of particular test corpora to de-
termine whether a particular annotation
task can feasibly achieve good agreement
scores.
(c) The use of underlying models for repre-
senting annotation content that facilitate
merging, comparison, and analysis.
184
(d) To the extent possible, the use of common
annotation categories or a mapping among
categories for the same phenomenon used
by different annotation groups.
In selecting shared corpora, we believe that the
following issues must be taken into consideration:
1. The diversity of genres, lexical items and lin-
guistic phenomena ? this will ensure that the
corpora will be useful to many different types
of annotation efforts. Furthermore, systems us-
ing these corpora and annotation as data will
be capable of handling larger and more varied
corpora.
2. The availability of the same or similar corpora
in a wide variety of languages;
3. The availability of corpora in a standard format
that can be easily processed ? there should be
mechanisms in place to maintain the availabil-
ity of corpora in this format in the future;
4. The ease in which the corpora can be obtained
by anyone who wants to process or annotate
them ? corpora with free licenses or that are in
the public domain are preferred
5. The degree with which the corpora is represen-
tative of text to be processed ? this criterion can
be met if the corpora is diverse (1 above) and/or
if more corpora of the same kind is available for
processing.
We have selected the following corpora for con-
sideration:1
1. The OANC: the Open sections of the ANC cor-
pus. These are the sections of the American
National Corpus subject to the opened license,
allowing them to be freely distributed. The full
Open ANC (Version 2.0) contains about 14.5
megawords of American English and covers a
variety of genres as indicated by the full path-
names taken from the ANC distribution (where
a final 1 or 2 indicates which DVD the directory
originates from):
1These corpora can be downloaded from:
http://nlp.cs.nyu.edu/wiki/corpuswg/SharedCorpora
? spoken/telephone/switchboard
? written 1/fiction/eggan
? written 1/journal/slate
? written 1/letters/icic
? written 2/non-fiction/OUP
? written 2/technical/biomed
? written 2/travel guides/berlitz1
? written 2/travel guides/berlitz2
? written 1/journal/verbatim
? spoken/face-to-face/charlotte
? written 2/technical/911report
? written 2/technical/plos
? written 2/technical/government
2. The Controversial-Wikipedia-Corpus, a section
of the Wikipedia XML corpus. WikipediaXML
is a corpus derived from Wikipedia, convert-
ing Wikipedia into an XML corpus suitable
for NLP processing. This corpus was selected
from:
? Those articles cited as controversial
according to the November 28, 2006
version of the following Wikipedia page:
http://en.wikipedia.org/wiki/Wikipedia:
List of controversial issues
? The talk pages corresponding to these ar-
ticles where Wikipedia users and the com-
munity debate aspects of articles. These
debates may be about content or editorial
considerations.
? Articles in Japanese that are linked to
the English pages (and the associated talk
pages) are also part of our corpus.
2 American National Corpus
The American National Corpus (ANC) project (Ide
and Macleod, 2001; Ide and Suderman, 2004) has
released over 20 million words of spoken and writ-
ten American English, available from the Linguis-
tic Data Consortium. The ANC 2nd release con-
sists of fiction, non-fiction, newspapers, technical
reports, magazine and journal articles, a substan-
tial amount of spoken data, data from blogs and
other unedited web sources, travel guides, techni-
cal manuals, and other genres. All texts are an-
notated for sentence boundaries; token boundaries,
185
lemma, and part of speech produced by two differ-
ent taggers ; and noun and verb chunks. A sub-
corpus of 10 million words reflecting the genre dis-
tribution of the full ANC is currently being hand-
validated for word and sentence boundaries, POS,
and noun and verb chunks. For a complete descrip-
tion of the ANC 2nd release and its contents, see
http://AmericanNationalCorpus.org.
Approximately 65 percent of the ANC data is dis-
tributed under an open license, which allows use and
re-distribution of the data without restriction. The
remainder of the corpus is distributed under a re-
stricted license that disallows re-distribution or use
of the data for commercial purposes for five years
after its release date, unless the user is a member of
the ANC Consortium. After five years, the data in
the restricted portions of the corpus are covered by
the open license.
ANC annotations are distributed as stand-off doc-
uments representing a set of graphs over the primary
data, thus allowing for layering of annotations and
inclusion of multiple annotations of the same type.
Because most existing tools for corpus access and
manipulation do not handle stand-off annotations,
we have developed an easy-to-use tool and user in-
terface to merge the user?s choice of stand-off anno-
tations with the primary data to form a single docu-
ment in any of several XML and non-XML formats,
which is distributed with the corpus. The ANC ar-
chitecture and format is described fully in (Ide and
Suderman, 2006).
2.1 The ULA Subcorpus
The Unified Linguistic Annotation (ULA) project
has selected a 40,000 word subcorpus of the Open
ANC for annotation with several different annota-
tion schemes including: the Penn Treebank, Prop-
Bank, NomBank, the Penn Discourse Treebank,
TimeML and Opinion Annotation.2 This initial sub-
corpus can be broken down as follows:
? Spoken Language
? charlotte: 5K words
? switchboard: 5K words
? letters: 10K words
2Other corpora being annotated by the ULA project include
sections of the Brown corpus and LDC parallel corpora.
? Slate (Journal): 5K words
? Travel guides: 5K words
? 911report: 5K words
? OUP books (Kaufman): 5K words
As the ULA project progresses, the participants
intend to expand the corpora annotated to include a
larger subsection of the OANC. They believe that the
diversity of this corpus make it a reasonable testbed
for tuning annotation schemes for diverse modali-
ties. The Travel guides and some of the slate arti-
cles have already been annotated by the FrameNet
project. Thus the inclusion of these documents fur-
thered the goal of producing a multiply annotated
corpus by one additional project.
It is the recommendation of this working group
that: (1) other groups annotate these same subcor-
pora; and (2) other groups choose additional corpora
from the OANC to annotate and publicly announce
which subsections they choose. We would be happy
to put all such subsections on our website for down-
load. The basic idea is to build up a consensus of
what should be mutually annotated, in part, based
on what groups choose to annotate and to try to get
annotation projects to gravitate toward multiply an-
notated, freely available corpora.
3 The WikipediaXML Corpus
3.1 Why Wikipedia?
The Wikipedia corpus consists of articles in a wide
range of topics written in different genres and
mainly (a) main pages are encyclopedia style arti-
cles; and (b) talk pages are discussions about main
pages they are linked to. The topics of these discus-
sions range from editing contents to disagreements
about content. Although Wikipedia texts are mostly
limited to these two genres, we believe that it is well
suited as training data for natural language process-
ing because:
1. they are lexically diverse (e.g., providing a lot
of lexical information for statistical systems);
2. the textual information is well structured
3. Wikipedia is a large and growing corpus
186
4. the articles are multilingual (cf. section 3.4)
5. and the corpus has various other properties that
many researchers feel would be interesting to
exploit.
To date research in Computational Linguistics us-
ing Wikipedia includes: Automatic derivation of
taxonomy information (Strube and Ponzetto, 2006;
Suchanek et al, 2007; Zesch and Gurevych, 2007;
Ponzetto, 2007); automatic recognition of pairs of
similar sentences in two languages (Adafre and de
Rijke, 2006); corpus mining (Ru?diger Gleim and
Alexander Mehler and Matthias Dehmer, 2007),
Named Entity Recognition (Toral and noz, 2007;
Bunescu and Pasc?a, 2007) and relation extraction
(Nguyen et al, 2007). In addition several shared
tasks have been set up using Wikipedia as the tar-
get corpus including question answering (cf. (D.
Ahn and V. Jijkoun and G. Mishne and K. Mu?ller
and M. de Rijke and S. Schlobach, 2004) and
http://ilps.science.uva.nl/WiQA/); and information
retrieval (Fuhr et al, 2006). Some other interest-
ing properties of Wikipedia that have yet to be ex-
plored to our knowledge include: (1) Most main ar-
ticles have talk pages which discuss them ? perhaps
this relation can be exploited by systems which try
to detect discussions about topics, e.g., searches for
discussions about current events topics; (2) There
are various meta tags, many of which are not in-
cluded in the WikipediaXML (see below), but nev-
ertheless are retrievable from the original HTML
files. Some of these may be useful for various ap-
plications. For example, the levels of disputabil-
ity of the content of the main articles is annotated
(cf. http://en.wikipedia.org/wiki/Wikipedia: Tem-
plate messages/Disputes ).
3.2 Why WikipediaXML?
WikipediaXML (Denoyer and Gallinari, 2006) is an
XML version of Wikipedia data, originally designed
for Information Retrieval tasks such as INEX (Fuhr
et al, 2006) and the XML Document Mining Chal-
lenge (Denoyer and P. Gallinari, 2006). Wikipedi-
aXML has become a standard machine readable
form for Wikipedia, suitable for most Computa-
tional Linguistics purposes. It makes it easy to
identify and read in the text portions of the doc-
ument, removing or altering html and wiki code
that is difficult to process in a standard way. The
WikipediaXML standard has (so far) been used to
process Wikipedia documents written in English,
German, French, Dutch, Spanish, Chinese, Arabic
and Japanese.
3.3 The Controversial Wikipedia Corpus
The English Wikipedia corpus is quite large (about
800K articles and growing). Frozen versions of
the corpus are periodically available for download.
We selected a 5 million word subcorpus which
we believed would be good for a wide variety
of annotation schemes. In particular, we chose
articles listed as being controversial (in the En-
glish speaking world) according to the November
28, 2006 version of the following Wikipedia
page: http://en.wikipedia.org/wiki/Wikipedia:
List of controversial issues. We believed that
controversial articles would be more likely than
randomly selected articles to: (1) include interesting
discourse phenomena and emotive language; and
(2) have interesting ?talk? pages (indeed, some of
Wikipedia pages have no associated talk pages).
3.4 The Multi-linguality of Wikipedia
One of the main good points of Wikipedia is the fact
that it is a very large multilingual resource. This
provides several advantages over single-language
corpora, perhaps the clearest such advantage being
the availability of same-genre/same-format text for
many languages. Although, Wikipedia in languages
other than English do not approach 800K articles in
size, there are currently at least 14 languages with
over 100K entries.
It should be clear however, that it is definitely not
a parallel corpus. Although pages are sometimes
translated in their entirety, this is the exception, not
the rule. Pages can be partially translated or summa-
rized into the target language. Individually written
pages can be linked after they are created if it is be-
lieved that they are about the same topic. Also, ini-
tially parallel pages can be edited in both languages,
causing them to diverge. We therefore decided to
do a small small pilot study to attempt to charac-
terize the degree of similarity between English arti-
cles in Wikipedia and articles written in other lan-
guages that have been linked. There are 476 En-
glish Wikipedia articles in the Controversial corpus
187
Classification Frequency
Totally Different 2
Same General Topic 3
Overlapping Topics 11
Same Topics 33
Parallel 1
and 384 associated ?talk? pages. There are approxi-
mately 10,000 articles of various languages that are
linked to the English articles. We asked some En-
glish/Japanese bilingual speakers to evaluate the de-
gree of similarity of as many of the the 305 Japanese
articles that were linked to English controversial ar-
ticles. As of this date, 50 articles were evaluated
with the results summarized as table 3.4.3 These
preliminary results suggest the following:
? Languate-linked Wikipedia would usually be
classified as ?comparable? corpora as 34 (68%)
of the articles were classified as covering the
same topics or being parallel.
? It may be possible to extract a parallel corpus
for a given pair of languages from Wikipedia.
If the above sample is representative, approxi-
mately 2% of the articles are parallel. (While
the existance of one parallel article does not
provide statistically significant evidence that
2% of Wikipedia is parallel, the article?s ex-
istance is still significant.) Furthermore, addi-
tional parallel sentences may be extracted from
some of the other comparable articles using
techniques along the lines of (Adafre and de Ri-
jke, 2006).
Obviously, a more detailed study would be neces-
sary to gain a more complete understanding of how
language-linked articles are related in Wikipedia.4
Such a study would include characterizations of all
linked articles for several languages. This study
could lead to some practical applications, e.g., (1)
the creation of parallel subcorpora for a number of
languages; (2) the selection of an English monolin-
gual subcorpus consisting of articles, each of which
3According to www.wikipedia.org there are currently over
350K Japanese articles.
4Long Wikipedia articles may be split into multiple articles.
This can result in N to 1, or even N to N, matches between
language-linked articles if a topic is split in one language, but
not in another.
is parallel to some article in some other language;
etc.; (3) A compilation of parallel sentences ex-
tracted from comparable articles. While parallel
subcorpora are of maximal utility, finding parallel
sentences could still be extremely useful. (Adafre
and de Rijke, 2006) reports one attempt to automat-
ically select parallel Dutch/English sentences from
language-linked Wikipedia articles with an accuracy
of approximately 45%. Even if higher accuracy can-
not be achieved, this still suggests that it is possible
to create a parallel corpus (of isolated sentences) us-
ing a combination of automatic and manual means.
A human translator would have to go through pro-
posed parallel sentences and eliminate about one
half of them, but would not have to do any man-
ual translation. Selection of corpora for annotation
purposes depends on a number of factors including:
the type of annotation (e.g., a corpus of isolated sen-
tences would not be appropriate for discourse anno-
tation); and possibly an application the annotation
is tuned for (e.g., Machine Translation, Information
Extraction, etc.)
It should be noted that the corpus was chosen for
the controversialness of its articles in the English-
speaking community. It should, however, not be ex-
pected that the same articles will be controversial
in other languages. More generally, the language-
linked Wikipedia articles may have different cultural
contexts depending on the language they are written
in. This is an additional feature that we could test
in a wider study. Furthermore, English pages are
somewhat special because they?re considered as the
common platform and expected to be neutral to any
country. But other lanauages somewhat reflects the
view of each country where the language is spoken.
Indeed, some EN articles are labeled as USA-centric
(cf. http://en.wikipedia.org/wiki/Category:USA-
centric).
Finally, our choice of a corpus based on contro-
versy may have not been the most efficient choice
if our goal had been specifically to find parallel cor-
pora. Just as choosing corpora of articles that are
controversial (in the English-speaking world) may
have helped finding articles interesting to annotate
it is possible that some other choice, e.g., techni-
cal articles, may have helped select articles likely
188
to be translated in full5 Thus further study may be
required to choose the right Wikipedia balance for a
set of priorities agreed upon by the annotation com-
munity.
4 Legal Issues
The American National Corpus has taken great pains
to establish that the open subset of the corpus is
freely usable by the community. The open license6
makes it clear that these corpora can be used for any
reason and are freely distributable.
In contrast, some aspects of the licensing agree-
ment of corpora derived from Wikipedia are unclear.
Wikipedia is governed by the GNU Free Document
License which includes a provision that ?derived
works? are subject to this license as well. While
most academic researchers would be uneffected by
this provision, the effect of this provision is unclear
with respect to commercial products.
Under one view, a machine translation system that
uses a statistical model trained on Wikipedia corpora
is not derived from these corpora. However, on an-
other view it is derived. We contacted Wikipedia
staff by letter asking for clarification on this issue
and received the following response from Michelle
Kinney on behalf of Wikipedia information team:
Wikipedia does not offer legal advice,
and therefore cannot help you decide how
the GNU Free Documentation License
(GFDL) or any other free license applies
to your particular situation. Please con-
tact a local bar association, law society or
similar association of jurists in your legal
jurisdiction to obtain a referral to a com-
petent legal professional.
You may also wish to review the full text
of the GFDL yourself:
http://en.wikipedia.org/wiki/Wikipedia:
Text of the GNU Free Documentation License
5Informally, we observe that linked Japanese/English pairs
of articles about abstract topics (e.g., Adultery, Agnosticsism,
Antisemitism, Capitalism, Censorship, Catholicism) are less
likely to contain parallel sentences than articles about specific
events or people (e.g., Adolf Hitler, Barbara Streisand, The Los
Angeles Riots, etc.)
6http://projects.ldc.upenn.edu/ANC/ANC SecondRelease
EndUserLicense Open.htm
While some candidate corpora are completely in
the public domain, e.g., political speeches and very
old documents, many candidate corpora are under
the GFDL or similar ?copyleft? licenses. These in-
clude other licenses by the GNU organization and
several Creative Commons licenses. It is simply un-
clear how copyleft licenses should be applied to cor-
pora used as data in computational linguistics and
we believe that this is an important legal question
for the Computational Linguistics community. In
addition to Wikipedia, this issue effects a wide vari-
ety of corpora (e.g., other wiki corpora, some of the
corpora being developed by the American National
Corpus, etc.).
However, getting such legal opinions is expensive
and has to be done carefully. Hypothetically, sup-
pose NYU?s legal department wrote an opinion let-
ter stating that products that were not corpora them-
selves were not to be considered derived works for
purposes of some list of copyleft licensing agree-
ments. Furthermore, let?s suppose that several anno-
tation projects relied on this opinion and produced
millions of dollars worth of annotation for one such
corpus. Large corporations still might not use these
corpora unless their own legal departments agreed
with NYU?s opinion. For the annotation community,
this could mean that certain annotation would only
be used by academics and not by industry, and most
annotation researchers would not be happy with this
outcome. It therefore may be worth some effort
on the part of whole NLP community to seek some
clear determinations on this issue.
5 Concluding Remarks
The working group selected two freely distributable
corpora for purposes of annotation. Our goal was to
choose texts for annotation by multiple annotation
research groups and describe the process and the pit-
falls involved in selecting those texts. We, further-
more, aimed to establish a protocol for sharing texts,
so that the same texts are annotated with multiple
annotation schemes. This protocol cannot be setup
carte blanche by this group of researchers. Rather,
we believe that our report in combination with the
discussion at the upcoming meeting of the Lingus-
tic Annotation Workshop will provide the jumpstart
necessary for such a protocol to be put in place.
189
References
Sisay Fissaha Adafre and Maarten de Rijke. 2006. Find-ing Similar Sentences across Multiple Languages in
Wikipedia. In EACL 2006 Workshop: Wikis and blogsand other dynamic text source, Trento, Italy.
Razvan Bunescu and Marius Pasc?a. 2007. Using En-
cyclopedic Knowledge for Named Entity Disambigua-tion. In Proc. of NAACL/HLT 2007.
D. Ahn and V. Jijkoun and G. Mishne and K. Mu?ller and
M. de Rijke and S. Schlobach. 2004. Using Wikipediaat the TREC QA Track. In Proc. TREC 2004.
Ludovic Denoyer and Patrick Gallinari. 2006. The
Wikipedia XML Corpus. SIGIR Forum.
L. Denoyer and A. Vercoustre P. Gallinari. 2006. Reporton the XML Mining Track at INEX 2005 and INEX
2006 : Categorization and Clustering of XML Docu-ments. In Advances in XML Information Retrieval andEvaluation: Fifthth Workshop of the INitiative for theEvaluation of XML Retrieval (INEX?06).
N. Fuhr, M. Lalmas, and S. Malik. 2006. Advances inXML Information Retrieval and Evaluation. In 5th In-ternational Workshop of the Initiative for the Evalua-tion of XML Retrieval, INEX 2006.
N. Ide and C. Macleod. 2001. The american national
corpus: A standardized resource of american english.In Proceedings of Corpus Linguistics 2001, Lancaster,UK.
N. Ide and K. Suderman. 2004. The american nationalcorpus first release. In Proceedings of LREC 2004,pages 1681?1684, Lisbon, Portugal.
N. Ide and K. Suderman. 2006. Integrating linguistic re-sources: The american national corpus model. In Pro-ceedings of the 6th International Conference on Lan-guage Resources and Evaluation, Genoa, Italy.
D. P.T. Nguyen, Y. Matsuo, and M. Ishizuka. 2007. Sub-tree Mining for Relation Extraction from Wikipedia.
In Proc. of NAACL/HLT 2007.
Simone Paolo Ponzetto. 2007. Creating a KnowledgeBase From a Collaboratively Generated Encyclopedia.
In Proc. of NAACL/HLT 2007.
Ru?diger Gleim and Alexander Mehler and MatthiasDehmer. 2007. Web Corpus Mining by instance of
Wikipedia. In Proc. 2nd Web as Corpus Workshop atEACL 2006.
M. Strube and S. P. Ponzetto. 2006. WikiRelate! Com-
puting semantic relatedness using Wikipedia. In Proc.of AAAI-06, pages 1419?1424.
F. M. Suchanek, G. Kasneci, and G.Weikum. 2007.
YAGO: A core of semantic knowledge. In Proc. ofWWW-07.
Antonio Toral and Rafael Mu noz. 2007. A proposal toautomatically build and maintain gazetteers for Named
Entity Recognition by using Wikipedia. In Proc. ofNAACL/HLT 2007.
Torsten Zesch and Iryna Gurevych. 2007. Analysis ofthe Wikipedia Category Graph for NLP Applications.
In Proc of NAACL-HLT 2007 Workshop: TextGraphs-2.
190
