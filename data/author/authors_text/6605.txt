Coling 2008: Companion volume ? Posters and Demonstrations, pages 139?142
Manchester, August 2008
Sentence Compression as a Step in Summarization or an Alternative Path
in Text Shortening
Mehdi Yousfi-Monod
University of Montpellier 2, CNRS
LIRMM, 161 rue Ada
34392 Montpellier Cedex 5
yousfi@lirmm.fr
Violaine Prince
University of Montpellier 2, CNRS
LIRMM, 161 rue Ada
34392 Montpellier Cedex 5
prince@lirmm.fr
Abstract
The originality of this work leads in tack-
ling text compression using an unsuper-
vised method, based on a deep linguistic
analysis, and without resorting on a learn-
ing corpus. This work presents a system
for dependent tree pruning, while preserv-
ing the syntactic coherence and the main
informational contents, and led to an op-
erational software, named COLIN. Exper-
iment results show that our compressions
get honorable satisfaction levels, with a
mean compression ratio of 38 %.
1 Introduction
Automatic summarization has become a crucial
task for natural language processing (NLP) since
information retrieval has been addressing it as one
of the most usual user requirements in its panel
of products. Most traditional approaches are con-
sidering the sentence as a minimal unit in the
summarization process. Some more recent works
get into the sentence in order to reduce the num-
ber of words by discarding incidental informa-
tion. Some of these approaches rely on statistical
models (Knight and Marcu, 2002; Lin and Hovy,
2002; Hovy et al, 2005), while some other works
use rule-based linguistically-motivated heuristics
(McKeown et al, 2002; Dorr et al, 2003; Gagnon
and Sylva, 2005) to improve the determination of
the importance of textual segments. Considering
a deeper linguistic analysis could considerably im-
prove the quality of reduced sentences, we decided
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
to develop a sentence compression approach ex-
clusively focused on linguistic heuristics. Our first
work (Yousfi-Monod and Prince, 2005), slightly
anterior to (Gagnon and Sylva, 2005), showed in-
teresting results, and led to a deeper and more com-
plete work fully detailed in (Yousfi-Monod, 2007).
This paper sums up our approach.
2 Theoretical framework
The main hypothesis of this work leans on the ob-
servation that incident sentence constituents are
often not as important as principal constituents.
For instance, let us consider the temporal adver-
bial in the following sentence: ?Taiwan elected
on Saturday its first president?. While the subject
?Taiwan? and the verb ?elected? are principal con-
stituents of the sentence, ?on Saturday? is incident
and can be removed without causing neither an
agrammaticality nor a weighty content lost. Two
aspects of the constituent significance: Grammati-
cality and content, are dealt with in this section.
2.1 Grammaticality preservation thanks to
the syntactic function
The principal/incident constituent principle can be
found in constituent or dependency grammar rep-
resentations. The approach embedded in COLIN
is based on such grammars, while adapting them
to the relevant proprieties for sentence compres-
sion. As we aim at preserving sentences gram-
maticality, our first goal is to get a syntactic tree
based on the grammatical importance, where for
each node, a daughter node is an incident con-
stituent which may be removed under certain con-
ditions. We opted for the X-bar theory (Chomsky,
1970), which represents a sentence through a tree
of constituents, composed by heads and governed
constituents (also dependents). While a head is
139
grammatically mandatory, its dependents can often
be removed, depending on some of their linguistic
properties and/or those of their heads. Our goal is
first to have a syntactic structure modeling based
on constituents grammatical importance. Syntactic
writing rules of the X-bar theory are focusing on
sentence construction by placing specifiers, com-
plements and adjuncts in the subtree of their con-
stituent. While adjuncts are systematically remov-
able, we have had to adopt a case-by-case study
for specifiers and complements. For instance, in a
noun phrase (NP), the article, if present, is a speci-
fier, and it cannot be removed, while in an adjective
phrase, the specifier is typically an adverb, which
is removable. The removability of a complement
depends on the subcategorisation properties of its
head. On a clause scale, the dependents are not
well defined in the X-bar theory and may include
the subject and verbal groups, as, respectively, the
specifier and the complement of the clause. Thus,
the specifier (subject) cannot be removed. Our
study has then consisted in a categorization of the
X-bar?s functional entities according to their re-
moval property. We have decided (i) to consider
mandatory specifiers as complements required by
their head and (ii) to bring together optional spec-
ifiers and adjuncts in a different category: Modi-
fiers
1
.
We have defined two classes of functions: Comple-
ments (X-bar complements and mandatory speci-
fiers) and Modifiers (X-bar adjuncts and optional
specifiers). This syntactic function classification
allows us to clearly define which sentence objects
can be candidates for removal.
Nevertheless, the syntactic function information,
although crucial, is not sufficient. One has to use
other linguistic properties in order to refine the as-
sessment of the constituent importance.
2.2 Important content preservation thanks to
linguistic proprieties
Subcategorisation. For noun and clause heads,
some of our complements have been identified
as systematically mandatory in order to preserve
the sentence coherence (subject, verbal group, ar-
ticles. . . ). Other heads (verb, adjective, adverb,
preposition and pronoun) may admit optional or
mandatory complements, depending on either the
lexical head category or a particular head instance
1
We have chosen the term ?modifier? as its definitions in
the literature fit quite well our needs.
(a lexical entry). Indeed, prepositions are sys-
tematically requiring a complement
2
, while other
heads must be considered on a case-by-case basis.
Once we get the subcategorisation information for
a head, we are able to determine whether its com-
plement(s) can be removed without causing an in-
coherence.
Other linguistic proprieties. We identified sev-
eral other linguistic clues that may help assessing
the importance of dependents. We do not detail our
analysis here for space reasons, refer to (Yousfi-
Monod, 2007) for the full description. These clues
include lexical functions, fixed expressions, type
of the article (definite or indefinite), parenthetical
phrases, detached noun modifiers, the dependent
constituent position in the sentence, negation and
interrogation.
3 COLIN?s compressor: System
architecture and implementation
3.1 Architecture
We assume we have a raw text as an input, which
may be the output of an extract summarizer, and
we have to produce a compressed version of it,
by reducing as many sentences as we can, without
deleting a single one.
Syntactic analysis. This step consists in using
a syntactic analyzer to produce, from the source
text, dependent trees according to our syntactic
model (heads, complements, modifiers). In order
to handle the important content assessment, the
parser uses linguistic resources including subcat-
egorisation information, lexical functions, and the
other linguistic properties (section 2.2), and then
enriches the trees with this information.
Pruning and linearization. The trees will be
then pruned according to a set of compression rules
defined from our theoretical analysis. Several set
of rules can be defined according to (i) the desired
importance conservation, (ii) the desired compres-
sion rate, (iii) the confidence in syntactic analy-
sis results, (iv) the trust in the identified linguistic
clues, (v) the textual genre of the source text. In
order to get effective rules, we have first defined
a relatively reliable kernel of rules. Then we have
decided to define and test, during our evaluation
2
Accordingly to the X-bar structure as well as ours: The
preposition is the head of the prepositional syntagm.
140
described in the next section, several rules configu-
rations, taking into account each of the five points,
in order to find the most effective ones. Rules tag
each tree node (complements and modifiers) which
will be removed, then trees are pruned and lin-
earized to get back new sentences, compressed.
3.2 Implementation
The first step in our implementation was to se-
lect a parser satisfying our syntactic requirements
as much as possible. SYGFRAN (described in
(Yousfi-Monod, 2007)), is the one that has been
chosen as: (i) It produces constituent trees very
close to our model, (ii) it has a good syntactic cov-
erage, (iii) it has a very good parsing complexity
(O(n.log(n)), with n the size of the data in words),
and (iv) its author and developer, Jacques Chauch?e,
works in our research team at LIRMM
3
, which
considerably eases the adaptation of the syntac-
tic model to ours. SYGFRAN consists in a set
of grammar networks, each of them containing
several set of transformational rules. COLIN and
SYGFRAN?s rules are implemented with the parser
SYGMART, a tree transducers system (Chauch?e,
1984). COLIN?s rules are split into several gram-
mars including (i) a basic anaphora resolution, (ii)
a tagging of candidate nodes
4
, (iii) a pruning of
tagged constituents and a linearization of leaves.
4 Evaluation, experimentation and
results
This section sums up the validation process used
for our approach. Our evaluation protocol is man-
ual intrinsic, focuses on facilitating the evalua-
tor? task and is inspired from (Knight and Marcu,
2002)?s one. For space reasons, we do not detail
the protocol here, a full description of the proto-
col as well as the experimentation is available in
(Yousfi-Monod, 2007).
Setting up. As our approach deeply relies on
syntactic properties, which are not always properly
detected by current parsers, we decided to manu-
ally improve the syntactic analysis of our evalua-
tion corpus. Otherwise, the evaluation would have
more reflected the users? satisfaction about parsing
than their opinion about the quality of our impor-
tance criteria. In order to assess the genre influ-
3
http://www.lirmm.fr
4
We tag trees before pruning them as COLIN can work in
a semi-automatic mode (not presented here) where a user can
modify the tagging.
ence, we selected three genres: Journalistic, narra-
tive and scientific. We composed 5 texts per gen-
res, each of them contained about 5 paragraphs,
16 sentences and 380 words, thus a total of 240
sentences. We decided to test the importance of
different clause modifiers, i.e. adverbial phrases,
according to their type. We considered the follow-
ing types: Temporal, locative and other ones. So,
while keeping the core rules for each rules config-
uration, we tested the removal of (i) all adverbials,
(ii) temporal adverbials, (iii) locative adverbials,
(iv) only other adverbials (keeping temporal and
place ones).
We got 25 active users participating to the evalua-
tion, who were mainly composed of PhD students
and PhDs, working in NLP or computational lin-
guistic domains, and being fluent in French. Some
of them did a set of manual compressions, used to
compare the quality with COLIN compressions in
the scoring stage of the evaluation. 59 text com-
pressions were done, corresponding to about 3,9
compressions per text. In the scoring stage, judges
gave about 5,2 notations per compressed paragraph
for manual and automatic compressions.
Results. Tables 1 and 2 present the results of
respectively obtained average compression rates
5
and paragraph scorings
6
, per genre. For COLIN?s
evaluation, we only display the rules configuration
which has obtained the best results for the com-
pression rate relatively to the paragraph scoring,
i.e. the rules configuration (iv).
Jour. Narr. Scien. Mean
Manual 36 % 17 % 23 % 25 %
COLIN 38 % 35 % 41 % 38 %
Table 1: Average compression rates.
Jour. Jour. Scien. Mean
Manual 4,03 3,67 3,41 3,7
COLIN 3,7 3 3 3,23
Table 2: Average paragraph scorings.
The compression rate proposed by COLIN is
quite better than the manual one for a quality scor-
ing just below the latter. COLIN is obviously
5
A higher percentage means a shorter compressed text.
6
Scores are between 1 and 5, a value of 1 means a com-
pletely unsatisfying compression, while a value of 5 means a
very good compression for the judge.
141
far better in compression time, with about 5 sec-
onds per document versus between 200 and 300
seconds for the manual compressions. COLIN?s
compression?quality?time ratio is therefore really
better than the manual compressions. Each genre
obtained a good compression rate as well as a cor-
rect quality scoring, particularly for the journalis-
tic one. Note that our results could have been im-
proved if they weren?t sensibly degraded because
of an imperfect parsing, despite some focused im-
provements we did on it.
A performance comparison with similar ap-
proaches was an issue for our approach for at least
two reasons: (i) As our parser is exclusively for
French, we had to do comparisons with French
tongue systems only. The system presented in
(Gagnon and Sylva, 2005) is the only that matches
this constraint. (ii) Our evaluation protocol drasti-
cally differs from traditional ones in several points:
1. Having a single human judge who compresses
sentences produces compressions which are too
much subjective to the latter, that?s why each of
our texts were compressed about 4 times by dif-
ferent humans. Evaluating compressions rise the
same issue of subjectivity, so each of our compres-
sions were evaluated about 5 times. 2. We con-
sider assessing the quality of separated compressed
sentences is harder and less relevant for evaluators
than assessing full paragraphs as we did. 3. Text
genre has an actual influence on NLP approaches,
thus we took into account this factor in our eval-
uation, as described above, while the above cited
system extracted random sentences in a single cor-
pus. For all these reasons, we haven?t been able to
perform a comparison with the above cited system
yet.
5 Conclusion
In this paper we have addressed the task of sen-
tence compression based on a deep linguistic anal-
ysis. The system we developed, called COLIN,
theoretically relies on a constituents and depen-
dencies sentence tree pruning, removing those
branches which could be cut without jeopardiz-
ing the sentence construction, or tempering too
strongly with the sentence meaning. A careful
study of syntactic properties, lexical functions,
verbs arguments has led us to design several differ-
ent configurations in which the sentence compres-
sion quality could degrade if compression goes too
far. The appreciation of a compression quality has
been here demonstrated as a user satisfaction pro-
tocol. If COLIN has been able to shorten texts by
an average 38%, humans were not able to remove
more than 25%. At the same time, the satisfaction
mean score is 3.23 over 5, whereas the same users
attribute to human compressors a satisfaction mean
score of 3.7, really not so much more.
References
Chauch?e, Jacques. 1984. Un outil multidimensionnel
de l?analyse du discours. In Coling?84, pages 11?15.
Chomsky, Noam. 1970. Remarks on nominalization.
In R. Jacobs and P. Rosenbaum (eds.) Reading in En-
glish Transformational Grammar, pages 184?221,
Waltham: Ginn.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In In R. Radev & S. Teufel
(Eds.), Proceedings of the HLT-NAACL 2003 Work-
shop on Text Summarization. Omnipress, pages 1?8.
Gagnon, Michel and Lyne Da Sylva. 2005. Text
summarization by sentence extraction and syntac-
tic pruning. In Computational Linguistics in the
North East (CliNE?05), Universit?e du Qu?ebec en
Outaouais, Gatineau, 26 August.
Hovy, Eduard H., Chin-Yew Lin, and Liang Zhou.
2005. A be-based multi-document summarizer with
sentence compression. In the Multilingual Summa-
rization Evaluation Workshop at the ACL 2005 con-
ference.
Knight, Kevin and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: a probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence archive, 139(1):91?107, July.
Lin, Chin-Yew and Eduard H. Hovy. 2002. Auto-
mated multi-document summarization in neats. In
the DARPA Human Language Technology Confer-
ence, pages 50?53.
McKeown, K., D. Evans, A. Nenkova, R. Barzi-
lay, V. Hatzivassiloglou, B. Schiffman, S. Blair-
Goldensohn, J. Klavans, and S. Sigelman. 2002.
The columbia multi-document summarizer for duc
2002.
Yousfi-Monod, Mehdi and Violaine Prince. 2005. Au-
tomatic summarization based on sentence morpho-
syntactic structure: narrative sentences compres-
sion. In the 2nd International Workshop on Natu-
ral Language Understanding and Cognitive Science
(NLUCS 2005), pages 161?167, Miami/USA, May.
Yousfi-Monod, Mehdi. 2007. Compression automa-
tique ou semi-automatique de textes par ?elagage des
constituants effac?ables : une approche interactive et
ind?ependante des corpus. Ph.D. thesis, University of
Montpellier II, Montpellier, November.
142
Antonymy and Conceptual Vectors
Didier Schwab, Mathieu Lafourcade and Violaine Prince
LIRMM
Laboratoire d?informatique, de Robotique
et de Microe?lectronique de Montpellier
MONTPELLIER - FRANCE.
{schwab,lafourca,prince}@lirmm.fr
http://www.lirmm.fr/ ?{schwab, lafourca, prince}
Abstract
For meaning representations in NLP, we focus
our attention on thematic aspects and concep-
tual vectors. The learning strategy of concep-
tual vectors relies on a morphosyntaxic analy-
sis of human usage dictionary definitions linked
to vector propagation. This analysis currently
doesn?t take into account negation phenomena.
This work aims at studying the antonymy as-
pects of negation, in the larger goal of its inte-
gration into the thematic analysis. We present a
model based on the idea of symmetry compat-
ible with conceptual vectors. Then, we define
antonymy functions which allows the construc-
tion of an antonymous vector and the enumer-
ation of its potentially antinomic lexical items.
Finally, we introduce a measure which evaluates
how a given word is an acceptable antonym for
a term.
1 Introduction
Research in meaning representation in NLP is
an important problem still addressed through
several approaches. The NLP team at LIRMM
currently works on thematic and lexical disam-
biguation text analysis (Laf01). Therefore we
built a system, with automated learning capa-
bilities, based on conceptual vectors for mean-
ing representation. Vectors are supposed to en-
code ?ideas? associated to words or to expres-
sions. The conceptual vectors learning system
automatically defines or revises its vectors ac-
cording to the following procedure. It takes, as
an input, definitions in natural language con-
tained in electronic dictionaries for human us-
age. These definitions are then fed to a morpho-
syntactic parser that provides tagging and anal-
ysis trees. Trees are then used as an input
to a procedure that computes vectors using
tree geometry and syntactic functions. Thus,
a kernel of manually indexed terms is necessary
for bootstrapping the analysis. The transver-
sal relationships1, such as synonymy (LP01),
antonymy and hyperonymy, that are more or
less explicitly mentioned in definitions can be
used as a way to globally increase the coher-
ence of vectors. In this paper, we describe a
vectorial function of antonymy. This can help
to improve the learning system by dealing with
negation and antonym tags, as they are often
present in definition texts. The antonymy func-
tion can also help to find an opposite thema to
be used in all generative text applications: op-
posite ideas research, paraphrase (by negation
of the antonym), summary, etc.
2 Conceptual Vectors
We represent thematic aspects of textual seg-
ments (documents, paragraph, syntagms, etc)
by conceptual vectors. Vectors have been used
in information retrieval for long (SM83) and
for meaning representation by the LSI model
(DDL+90) from latent semantic analysis (LSA)
studies in psycholinguistics. In computational
linguistics, (Cha90) proposes a formalism for
the projection of the linguistic notion of se-
mantic field in a vectorial space, from which
our model is inspired. From a set of elemen-
tary concepts, it is possible to build vectors
(conceptual vectors) and to associate them to
lexical items2. The hypothesis3 that considers
a set of concepts as a generator to language
has been long described in (Rog52). Polysemic
words combine different vectors corresponding
1well known as lexical functions (MCP95)
2Lexical items are words or expressions which consti-
tute lexical entries. For instance, ?car ? or ?white ant ? are
lexical items. In the following we will (some what) use
sometimes word or term to speak about a lexical item.
3that we call thesaurus hypothesis.
to different meanings. This vector approach
is based on known mathematical properties, it
is thus possible to undertake well founded for-
mal manipulations attached to reasonable lin-
guistic interpretations. Concepts are defined
from a thesaurus (in our prototype applied to
French, we have chosen (Lar92) where 873 con-
cepts are identified). To be consistent with the
thesaurus hypothesis, we consider that this set
constitutes a generator family for the words and
their meanings. This familly is probably not
free (no proper vectorial base) and as such, any
word would project its meaning on it according
to the following principle. Let be C a finite set
of n concepts, a conceptual vector V is a linear
combinaison of elements ci of C. For a meaning
A, a vector V (A) is the description (in exten-
sion) of activations of all concepts of C. For ex-
ample, the different meanings of ?door ? could be
projected on the following concepts (the CON-
CEPT [intensity] are ordered by decreasing val-
ues): V(?door ?) = (OPENING[0.8], BARRIER[0.7],
LIMIT [0.65], PROXIMITY [0.6], EXTERIOR[0.4], IN-
TERIOR[0.39], . . .
In practice, the larger C is, the finer the mean-
ing descriptions are. In return, the computing
is less easy: for dense vectors4, the enumera-
tion of activated concepts is long and difficult
to evaluate. We prefer to select the themati-
cally closest terms, i.e., the neighbourhood. For
instance, the closest terms ordered by increas-
ing distance to ?door ? are: V(?door ?)=?portal ?,
?portiere?, ?opening?, ?gate?, ?barrier ?,. . .
2.1 Angular Distance
Let us define Sim(A,B) as one of the similar-
ity measures between two vectors A et B, of-
ten used in information retrieval (Mor99). We
can express this function as: Sim(A,B) =
cos(A?, B) = A?B?A???B? with ??? as the scalar
product. We suppose here that vector com-
ponents are positive or null. Then, we define
an angular distance DA between two vectors A
and B as DA(A,B) = arccos(Sim(A,B)). In-
tuitively, this function constitutes an evaluation
of the thematic proximity and measures the an-
gle between the two vectors. We would gener-
ally consider that, for a distance DA(A,B) ? pi4
4Dense vectors are those which have very few null
coordinates. In practice, by construction, all vectors are
dense.
(45 degrees) A and B are thematically close and
share many concepts. For DA(A,B) ? pi4 , the
thematic proximity between A and B would be
considered as loose. Around pi2 , they have no
relation. DA is a real distance function. It ver-
ifies the properties of reflexivity, symmetry and
triangular inequality. We have, for example,
the following angles(values are in radian and de-
grees).
DA(V(?tit ?), V(?tit ?))=0 (0)
DA(V(?tit ?), V(?bird ?))=0.55 (31)
DA(V(?tit ?), V(?sparrow ?))=0.35 (20)
DA(V(?tit ?), V(?train ?))=1.28 (73)
DA(V(?tit ?), V(?insect ?))=0.57 (32)
The first one has a straightforward interpreta-
tion, as a ?tit ? cannot be closer to anything else
than itself. The second and the third are not
very surprising since a ?tit ? is a kind of ?sparrow ?
which is a kind of ?bird ?. A ?tit ? has not much
in common with a ?train?, which explains a large
angle between them. One can wonder why there
is 32 degrees angle between ?tit ? and ?insect ?,
which makes them rather close. If we scruti-
nise the definition of ?tit ? from which its vector
is computed (Insectivourous passerine bird with
colorful feather.) perhaps the interpretation of
these values seems clearer. In effect, the the-
matic is by no way an ontological distance.
2.2 Conceptual Vectors Construction.
The conceptual vector construction is based on
definitions from different sources (dictionaries,
synonym lists, manual indexations, etc). Defini-
tions are parsed and the corresponding concep-
tual vector is computed. This analysis method
shapes, from existing conceptual vectors and
definitions, new vectors. It requires a bootstrap
with a kernel composed of pre-computed vec-
tors. This reduced set of initial vectors is man-
ually indexed for the most frequent or difficult
terms. It constitutes a relevant lexical items
basis on which the learning can start and rely.
One way to build an coherent learning system
is to take care of the semantic relations between
items. Then, after some fine and cyclic compu-
tation, we obtain a relevant conceptual vector
basis. At the moment of writing this article,
our system counts more than 71000 items for
French and more than 288000 vectors, in which
2000 items are concerned by antonymy. These
items are either defined through negative sen-
tences, or because antonyms are directly in the
dictionnary. Example of a negative definition:
?non-existence?: property of what does not exist.
Example of a definition stating antonym: ?love?:
antonyms: ?disgust ?, ?aversion?.
3 Definition and Characterisation of
Antonymy
We propose a definition of antonymy compat-
ible with the vectorial model used. Two lexi-
cal items are in antonymy relation if there is
a symmetry between their semantic components
relatively to an axis. For us, antonym construc-
tion depends on the type of the medium that
supports symmetry. For a term, either we can
have several kinds of antonyms if several possi-
bilities for symmetry exist, or we cannot have
an obvious one if a medium for symmetry is not
to be found. We can distinguish different sorts
of media: (i) a property that shows scalar val-
ues (hot and cold which are symmetrical values
of temperature), (ii) the true-false relevance or
application of a property (e.g. existence/non-
existence) (iii) cultural symmetry or opposition
(e.g. sun/moon).From the point of view of lex-
ical functions, if we compare synonymy and
antonymy, we can say that synonymy is the
research of resemblance with the test of sub-
stitution (x is synonym of y if x may replace
y), antonymy is the research of the symmetry,
that comes down to investigating the existence
and nature of the symmetry medium. We have
identified three types of symmetry by relying
on (Lyo77), (Pal76) and (Mue97). Each sym-
metry type characterises one particular type of
antonymy. In this paper, for the sake of clarity
and precision, we expose only the complemen-
tary antonymy. The same method is used for
the other types of antonymy, only the list of
antonymous concepts are different.
3.1 Complementary Antonymy
The complementary antonyms are couples like
event/unevent, presence/absence.
he is present ? he is not absent
he is absent ? he is not present
he is not absent ? he is present
he is not present ? he is absent
In logical terms, we would have:
?x P (x)? ?Q(x) ?x ?P (x)? Q(x)
?x Q(x)? ?P (x) ?x ?Q(x)? P (x)
This corresponds to the exclusive disjunction
relation. In this frame, the assertion of one
of the terms implies the negation of the other.
Complementary antonymy presents two kinds
of symmetry, (i) a value symmetry in a boolean
system, as in the examples above and (ii) a sym-
metry about the application of a property (black
is the absence of color, so it is ?opposed? to all
other colors or color combinaisons).
4 Antonymy Functions
4.1 Principles and Definitions.
The aim of our work is to create a function
that would improve the learning system by sim-
ulating antonymy. In the following, we will be
mainly interested in antonym generation, which
gives a good satisfaction clue for these functions.
We present a function which, for a given lex-
ical item, gives the n closest antonyms as the
neighbourhood function V provides the n clos-
est items of a vector. In order to know which
particular meaning of the word we want to op-
pose, we have to assess by what context mean-
ing has to be constrained. However, context is
not always sufficient to give a symmetry axis
for antonymy. Let us consider the item ?father ?.
In the ?family? context, it can be opposite to
?mother ? or to ?children? being therefore ambigu-
ous because ?mother ? and ?children? are by no way
similar items. It should be useful, when context
cannot be used as a symmetry axis, to refine
the context with a conceptual vector which is
considered as the referent. In our example, we
should take as referent ?filiation?, and thus the
antonym would be ?children? or the specialised
similar terms (e.g. ?sons? , ?daughters?) ?marriage?
or ?masculine? and thus the antonym would be
?mother ?.
The function AntiLexS returns the n closest
antonyms of the word A in the context defined
by C and in reference to the word R.
AntiLexS(A,C,R, n)
AntiLexR(A,C, n) = AntiLexS(A,C,C, n)
AntiLexB(A,R, n) = AntiLexS(A,R,R, n)
AntiLexA(A, n) = AntiLexS(A,A,A, n)
The partial function AntiLexR has been de-
fined to take care of the fact that in most cases,
context is enough to determine a symmetry axis.
AntiLexB is defined to determine a symmetry
axis rather than a context. In practice, we have
AntiLexB = AntiLexR. The last function is
the absolute antonymy function. For polysemic
words, its usage is delicate because only one
word defines at the same time three things: the
word we oppose, the context and the referent.
This increases the probability to get unsatis-
factory results. However, given usage habits,
we should admit that, practically, this function
will be the most used. It?s sequence process is
presented in picture 1. We note Anti(A,C) the
ITEMS
ANTONYMOUS
CONCEPTUAL VECTOR
CALCULATION
IDENTIFICATION
OF THE CLOSEST 
ITEMS
neighbourhood
CONCEPTUAL VECTORS
strong contextualisation
CALCULATION
X, C, R
X1, X2, ..., Xn
ITEMS
VAnti
VECTOR
ANTONYMOUS
OF THEanti
Vcx, Vcr
VECTORS
CORRESPONDING
OF THE
Figure 1: run of the functions AntiLex
antonymy function at the vector level. Here,
A is the vector we want to oppose and C the
context vector.
Items without antonyms: it is the case
of material objects like car, bottle, boat, etc.
The question that raises is about the continu-
ity the antonymy functions in the vector space.
When symmetry is at stake, then fixed points
or plans are always present. We consider the
case of these objects, and in general, non op-
posable terms, as belonging to the fixed space
of the symmetry. This allows to redirect the
question of antonymy to the opposable proper-
ties of the concerned object. For instance, if we
want to compute the antonym of a ?motorcycle?,
which is a ROAD TRANSPORT, its opposable prop-
erties being NOISY and FAST, we consider its cat-
egory (i.e. ROAD TRANSPORT) as a fixed point,
and we will look for a road transport (SILEN-
CIOUS and SLOW ), something like a ?bicycle? or
an ?electric car ?. With this method, thanks to
the fixed points of symmetry, opposed ?ideas?
or antonyms, not obvious to the reader, could
be discovered.
4.2 Antonym vectors of concept lists
Anti functions are context-dependent and can-
not be free of concepts organisation. They
need to identify for every concept and for ev-
ery kind of antonymy, a vector considered as
the opposite. We had to build a list of triples
?concept, context, vector?. This list is called
antonym vectors of concept list (AVC).
4.2.1 AVC construction.
The Antonym Vectors of Concepts list is manu-
ally built only for the conceptual vectors of the
generating set. For any concept we can have the
antonym vectors such as:
AntiC(EXISTENCE, V ) = V (NON-EXISTENCE)
AntiC(NON-EXISTENCE, V ) = V (EXISTENCE)
AntiC(AGITATION, V ) = V (INERTIA)? V (REST)
AntiC(PLAY, V ) = V (PLAY)
?V
AntiC(ORDER, V (order) ? V (disorder)) =
V (DISORDER)
AntiC(ORDER, V (classification) ? V (order)) =
V (CLASSIFICATION)
As items, concepts can have, according to
the context, a different opposite vector even
if they are not polysemic. For instance, DE-
STRUCTION can have for antonyms PRESERVA-
TION, CONSTRUCTION, REPARATION or PROTEC-
TION. So, we have defined for each concept, one
conceptual vector which allows the selection of
the best antonym according to the situation.
For example, the concept EXISTENCE has the
vector NON-EXISTENCE for antonym for any con-
text. The concept DISORDER has the vector of
ORDER for antonym in a context constituted by
the vectors of ORDER ?DISORDER5 and has CLAS-
SIFICATION in a context constituted by CLASSI-
FICATION and ORDER.
The function AntiC(Ci, Vcontext) returns for
a given concept Ci and the context defined by
Vcontext , the complementary antonym vector in
the list.
4.3 Construction of the antonym
vector: the Anti Function
4.3.1 Definitions
We define the relative antonymy function
AntiR(A,C) which returns the opposite vec-
tor of A in the context C and the absolute
antonymy function AntiA(A) = AntiR(A,A).
The usage of AntiA is delicate because the lexi-
cal item is considered as being its own context.
We will see in 4.4.1 that this may cause real
problems because of sense selection. We should
stress now on the construction of the antonym
vector from two conceptual vectors: Vitem, for
5? is the normalised sum V = A?B | vi = xi+yi?V ?
the item we want to oppose and the other, Vc,
for the context (referent).
4.3.2 Construction of the Antonym
Vector
The method is to focus on the salient notions in
Vitem and Vc. If these notions can be opposed
then the antonym should have the inverse ideas
in the same proportion. That leads us to define
this function as follows:
AntiR(Vitem, Vc) =
?N
i=1 Pi ?AntiC(Ci, Vc)
with Pi = V 1+CV (Vitem)itemi ?max(Vitemi , Vci)
We crafted the definition of the weight P after
several experiments. We noticed that the func-
tion couldn?t be symmetric (we cannot reason-
ably have AntiR(V(?hot ?),V(?temperature?)) =
AntiR(V(?temperature?),V(?hot ?))). That is why
we introduce this power, to stress more on the
ideas present in the vector we want to oppose.
We note also that the more conceptual6 the vec-
tor is, the more important this power should be.
That is why the power is the variation coeffi-
cient7 which is a good clue for ?conceptuality?.
To finish, we introduce this function max be-
cause an idea presents in the item, even if this
idea is not present in the referent, has to be op-
posed in the antonym. For example, if we want
the antonym of ?cold ? in the ?temperature? con-
text, the weight of ?cold ? has to be important
even if it is not present in ?temperature?.
4.4 Lexical Items and Vectors:
Problem and Solutions
The goal of the functions AntiLex is to return
antonym of a lexical item. They are defined
with the Anti function. So, we have to use tools
which allow the passage between lexical items
and vectors. This transition is difficult because
of polysemy, i.e. how to choose the right relation
between an item and a vector. In other words,
how to choose the good meaning of the word.
4.4.1 Transition lexical items ?
Conceptual Vectors
As said before, antonymy is relative to a con-
text. In some cases, this context cannot be suf-
ficient to select a symmetry axis for antonymy.
6In this paragraph, conceptual means: closeness of a
vector to a concept
7The variation coefficient is SD(V )?(V ) with SD as the
standart deviation and ? as the arithmetic mean.
To catch the searched meaning of the item and,
if it is different from the context, to catch the
selection of the meaning of the referent, we use
the strong contextualisation method. It com-
putes, for a given item, a vector. In this vector,
some meanings are favoured against others ac-
cording to the context. Like this, the context
vector is also contextualised.
This contextualisation shows the problem
caused by the absolute antonymy function
Anti?R . In this case, the method will compute
the vector of the word item in the context item.
This is not a problem if item has only one defini-
tion because, in this case, the strong contextu-
alisation has no effect. Otherwise, the returned
conceptual vector will stress on the main idea it
contains which one is not necessary the appro-
priate one.
4.4.2 Transition Conceptual Vectors ?
Lexical Items
This transition is easier. We just have to com-
pute the neighbourhood of the antonym vector
Vant to obtain the items which are in thematic
antonymy with Vitem. With this method, we
have, for instance:
V(AnticR(death, ?death ? & ?life?))=(LIFE 0.4)
(?killer ? 0.449) (?murderer ? 0.467) (?blood sucker ?
0.471) (?strige? 0.471) (?to die? 0.484) (?to live? 0.486)
V(AnticR(life, ?death ? & ?life?))=(?death ? 0.336)
(DEATH 0.357) (?murdered ? 0.367) (?killer ? 0.377)
(C3:AGE OF LIFE 0.481) (?tyrannicide? 0.516) (?to kill ?
0.579) (?dead ? 0.582)
V(AntiCcA(LIFE))=(DEATH 0.034) (?death ? 0.427)
(C3:AGE OF LIFE 0.551) (?killer ? 0.568) (?mudered ?
0.588) (?tyrannicide? 0.699) (C2:HUMAN 0.737) (?to
kill ? 0.748) (?dead ? 0.77)
It is not important to contextualise the con-
cept LIFE because we can consider that, for ev-
ery context, the opposite vector is the same.
In complementary antonymy, the closest item
is DEATH. This result looks satisfactory. We can
see that the distance between the antonymy vec-
tor and DEATH is not null. It is because our
method is not and cannot be an exact method.
The goal of our function is to build the best
(closest) antonymy vector it is possible to have.
The construction of the generative vectors is the
second explanation. Generative vectors are in-
terdependent. Their construction is based on an
ontology. To take care of this fact, we don?t have
boolean vectors, with which, we would have ex-
actly the same vector. The more polysemic the
term is, the farthest the closest item is, as we
can see it in the first two examples.
We cannot consider, even if the potential of
antonymy measure is correct, the closest lexical
item from Vanti as the antonym. We have to
consider morphological features. Simply speak-
ing, if the antonym of a verb is wanted, the re-
sult would be better if a verb is caught.
4.5 Antonymy Evaluation Measure
Besides computing an antonym vector, it seems
relevant to assess wether two lexical items can
be antonyms. To give an answer to this ques-
tion, we have created a measure of antonymy
evaluation. Let A and B be two vectors.
The question is precisely to know if they can
reasonably be antonyms in the context of C.
The antonymy measure MantiEval is the an-
gle between the sum of A and B and the sum
of AnticR(A,C) and AnticR(B,C). Thus, we
have:
MantiEval = DA(A?B,AntiR(A,C)?AntiR(B,C))
A+B
A
B
Anti(A,C)
Anti(B,C)
Anti(A,C)+Anti(B,C)
Figure 2: 2D geometric representation of the antonymy
evaluation measure MantiEval
The antonymy measure is a pseudo-distance.
It verifies the properties of reflexivity, symme-
try and triangular inequality only for the subset
of items which doesn?t accept antonyms. In this
case, notwithstanding the noise level, the mea-
sure is equal to the angular distance. In the
general case, it doesn?t verify reflexivity. The
conceptual vector components are positive and
we have the property: Distanti ? [0, pi2 ]. The
smaller the measure, the more ?antonyms? the
two lexical items are. However, it would be a
mistake to consider that two synonyms would be
at a distance of about pi2 . Two lexical items atpi
2 have not much in common8. We would rather
see here the illustration that two antonyms
share some ideas, specifically those which are
not opposable or those which are opposable with
a strong activation. Only specific activated con-
cepts would participate in the opposition. A
distance of pi2 between two items should rather
be interpreted as these two items do not share
much idea, a kind of anti-synonymy. This re-
sult confirms the fact that antonymy is not the
exact inverse of synonymy but looks more like a
?negative synonymy? where items remains quite
related. To sum up, the antonym of w is not
a word that doesn?t share ideas with w, but a
word that opposes some features of w.
4.5.1 Examples
In the following examples, the context has been
ommited for clarity sake. In these cases, the
context is the sum of the vectors of the two
items.
MantiEval(EXISTENCE,NON-EXISTENCE) = 0.03
MantiEvalC(?existence?, ?non-existence?) = 0.44
MantiEvalC(EXISTENCE, CAR) = 1.45
MantiEvalC(?existence?, ?car ?) = 1.06
MantiEvalC(CAR, CAR) = 0.006
MantiEvalC(?car ?, ?car ?) = 0.407
The above examples confirm what pre-
sented. Concepts EXISTENCE and NON-
EXISTENCE are very strong antonyms in comple-
mentary antonymy. The effects of the polysemy
may explain that the lexical items ?existence? and
?non-existence? are less antonyms than their re-
lated concepts. In complementary antonymy,
CAR is its own antonym. The antonymy mea-
sure between CAR and EXISTENCE is an exam-
ple of our previous remark about vectors shar-
ing few ideas and that around pi/2 this mea-
sure is close to the angular distance (we have
DA(existence, car) = 1.464.). We could con-
sider of using this function to look in a concep-
tual lexicon for the best antonyms. However,
the computation cost (around a minute on a P4
at 1.3 GHz) would be prohibitive.
8This case is mostly theorical, as there is no language
where two lexical items are without any possible relation.
5 Action on learning and method
evaluation
The function is now used in the learning process.
We can use the evaluation measure to show the
increase of coherence between terms:
MantiEvalC new old
?existence?, ?non-existence? 0.33 0.44
?existence?, ?car ? 1.1 1.06
?car ?, ?car ? 0.3 0, 407
There is no change in concepts because they are
not learned. In the opposite, the antonymy eval-
uation measure is better on items. The exemple
shows that ?existence? and ?non-existence? have
been largely modified. Now, the two items are
stronger antonyms than before and the vector
basis is more coherent. Of course, we can test
these results on the 71000 lexical items which
have been modified more or less directly by the
antonymy function. We have run the test on
about 10% of the concerned items and found an
improvement of the angular distance through
MantiEvalC ranking to 0.1 radian.
6 Conclusion
This paper has presented a model of antonymy
using the formalism of conceptual vectors. Our
aim was to be able: (1) to spot antonymy if
it was not given in definition and thus provide
an antonym as a result, (2) to use antonyms
(discovered or given) to control or to ensure the
coherence of an item vector, build by learning,
which could be corrupted. In NLP, antonymy is
a pivotal aspect, its major applications are the-
matic analysis of texts, construction of large lex-
ical databases and word sense disambiguation.
We grounded our research on a computable lin-
guisitic theory being tractable with vectors for
computational sake. This preliminary work on
antonymy has also been conducted under the
spotlight of symmetry, and allowed us to express
antonymy in terms of conceptual vectors. These
functions allow, from a vector and some contex-
tual information, to compute an antonym vec-
tor. Some extensions have also been proposed so
that these functions may be defined and usable
from lexical items. A measure has been identi-
fied to assess the level of antonymy between two
items. The antonym vector construction is nec-
essary for the selection of opposed lexical items
in text generation. It also determines opposite
ideas in some negation cases in analysis.
Many improvements are still possible, the
first of them being revision of the VAC lists.
These lists have been manually constructed by
a reduced group of persons and should widely be
validated and expanded especially by linguists.
We are currently working on possible improve-
ments of results through learning on a corpora.
References
Jacques Chauche?. De?termination se?mantique
en analyse structurelle : une expe?rience base?e
sur une de?finition de distance. TAL Informa-
tion, 1990.
Scott C. Deerwester, Susan T. Dumais,
Thomas K. Landauer, George W. Furnas, and
Richard A. Harshman. Indexing by latent se-
mantic analysis. Journal of the American So-
ciety of Information Science, 41(6):391?407,
1990.
Mathieu Lafourcade. Lexical sorting and lexical
transfer by conceptual vectors. In Proceeding
of the First International Workshop on Mul-
tiMedia Annotation, Tokyo, January 2001.
Larousse. The?saurus Larousse - des ide?es aux
mots, des mots aux ide?es. Larousse, 1992.
Mathieu Lafourcade and Violaine Prince. Syn-
onymies et vecteurs conceptuels. In actes de
TALN?2001, Tours, France, July 2001.
John Lyons. Semantics. Cambridge University
Press, 1977.
Igor Mel?c?uk, Andre? Clas, and Alain Polgue`re.
Introduction a` la lexicologie explicative et
combinatoire. Duculot, 1995.
Emmanuel Morin. Extraction de liens
se?mantiques entre termes a` partir de
corpus techniques. PhD thesis, Universite? de
Nantes, 1999.
Victoria Lynn Muehleisen. Antonymy and se-
mantic range in english. PhD thesis, North-
western university, 1997.
F.R. Palmer. Semantics : a new introduction.
Cambridge University Press, 1976.
P. Roget. Roget?s Thesaurus of English Words
and Phrases. Longman, London, 1852.
Gerard Salton and Michael McGill. Introduc-
tion to Modern Information Retrieval. Mc-
GrawHill, 1983.
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 31?34,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Just Title It! (by an Online Application)
Ce?dric Lopez, Violaine Prince, and Mathieu Roche
LIRMM, CNRS, University of Montpellier 2
161, rue Ada
Montpellier, France
{lopez,prince,mroche}@lirmm.fr
Abstract
This paper deals with an application of au-
tomatic titling. The aim of such application
is to attribute a title for a given text. So,
our application relies on three very differ-
ent automatic titling methods. The first one
extracts relevant noun phrases for their use
as a heading, the second one automatically
constructs headings by selecting words ap-
pearing in the text, and, finally, the third
one uses nominalization in order to propose
informative and catchy titles. Experiments
based on 1048 titles have shown that our
methods provide relevant titles.
1 Introduction
The important amount of textual documents is
in perpetual growth and requires robust applica-
tions. Automatic titling is an essential task for
several applications: Automatic titling of e-mails
without subjects, text generation, summarization,
and so forth. Furthermore, a system able to ti-
tle HTML documents and so, to respect one of
the W3C standards about Web site accessibility,
is quite useful. The titling process goal is to pro-
vide a relevant representation of a document con-
tent. It might use metaphors, humor, or emphasis,
thus separating a titling task from a summariza-
tion process, proving the importance of the rhetor-
ical status in both tasks.
This paper presents an original application con-
sisting in titling all kinds of texts. For that pur-
pose, our application offers three main meth-
ods. The first one (called POSTIT) extracts noun
phrases to be used as headings, the second one
(called CATIT) automatically builds titles by se-
lecting words appearing in the text, and, finally,
the third one (called NOMIT) uses nominalization
in order to propose relevant titles. Morphologic
and semantic treatments are applied to obtain ti-
tles close to real titles. In particular, titles have to
respect two characteristics: Relevance and catch-
iness.
2 Text Titling Application
The application presented in this paper was de-
veloped with PHP, and it is available on the
Web1. It is based on several methods using Nat-
ural Language Processing (NLP) and Information
Retrieval (IR) techniques. So, the input is a text
and the output is a set of titles based on different
kinds of strategies.
A single automatic titling method is not suffi-
cient to title texts. Actually, it cannot respect di-
versity, noticed in real titles, which vary accord-
ing to the writer?s personal interests or/and his/her
writing style. With the aim of getting closer to this
variety, the user can choose the more relevant title
according to his personal criteria among a list of
titles automatically proposed by our system.
A few other applications have focused on ti-
tling: One of the most typical, (Banko, 2000),
consists in generating coherent summaries that
are shorter than a single sentence. These sum-
maries are called ?headlines?. The main diffi-
culty is to adjust the threshold (i.e, the headline
length), in order to obtain syntactically correct
titles. Whereas our methods create titles which
are intrinsically correct, both syntactically and se-
mantically.
In this section, we present the POSTIT, CATIT,
and NOMIT methods. These three methods run
1https://www2.lirmm.fr/?lopez/Titrage_
general/
31
in parallel, without interaction with each other.
Three very different titles are thus determined for
every text. For each of them, an example of the
produced title is given on the following sample
text: ?In her speech, Mrs Merkel has promised
concrete steps towards a fiscal union - in effect
close integration of the tax-and-spend polices of
individual eurozone countries, with Brussels im-
posing penalties on members that break the rules.
[...]?. Even if examples are in English, the ap-
plication is actually in French (but easily repro-
ducible in English). The POS tagging was per-
formed by Sygfran (Chauche?, 1984).
2.1 POSTIT
(Jin, 2001) implemented a set of title generation
methods and evaluated them: The statistical ap-
proach based on the TF-IDF obtains the best re-
sults. In the same way, the POSTIT (Titling using
Position and Statistical Information) method uses
statistical information. Related works have shown
that verbs are not as widely spread as nouns,
named entities, and adjectives (Lopez, 2011a).
Moreover, it was noticed that elements appearing
in the title are often present in the body of the text
(Zajic et al 2002). (Zhou and Hovy, 2003) sup-
ports this idea and shows that the covering rate of
those words present in titles, is very high in the
first sentences of a text. So, the main idea is to
extract noun phrases from the text and to select
the more relevant for its use as title. The POSTIT
approach is composed of the following steps:
1. Candidate Sentence Determination. We as-
sume that any text contains only a few rel-
evant sentences for titling. The goal of this
step consists in recognizing them. Statistical
analysis shows that, very often, terms useful
for titling are located in the first sentences of
the text.
2. Extracting Candidate Noun Phrases for Ti-
tling. This step uses syntactical filters re-
lying on the statistical studies previously
led. For that purpose, texts are tagged with
Sygfran. Our syntactical patterns allowing
noun phrase extraction are also inspired from
(Daille, 1996).
3. Selecting a Title. Last, candidate noun
phrases (t) are ranked according to a score
based on the use of TF-IDF and information
about noun phrase position (NPPOS) (see
Lopez, 2011a). With ? = 0.5, this method
obtains good results (see Formula 1).
NPscore(t) = ??NPPOS(t)
+ (1? ?)?NPTF?IDF (t) (1)
Example of title with POSTIT: Concrete steps
towards a fiscal union.
On one hand, this method proposes titles which
are syntactically correct. But on the other hand,
provided titles can not be considered as original.
Next method, called CATIT, enables to generate
more ?original? titles.
2.2 CATIT
CATIT (Automatic Construction of Titles) is an
automatic process that constructs short titles. Ti-
tles have to show coherence with both the text and
the Web, as well as with their dynamic context
(Lopez, 2011b). This process is based on a global
approach consisting in three main stages:
1. Generation of Candidates Titles. The pur-
pose is to extract relevant nouns (using TF-
IDF criterion) and adjectives (using TF cri-
terion) from the text. Potential relevant cou-
ples (candidate titles) are built respecting the
?Noun Adjective? and/or ?Adjective Noun?
syntactical patterns.
2. Coherence of Candidate Titles. Among the
list of candidate titles, which ones are gram-
matically and semantically consistent ? The
produced titles are supposed to be consis-
tent with the text through the use of TF-
IDF. To reinforce coherence, we set up a
distance coefficient between a noun and an
adjective which constitutes a new coherence
criterion in candidate titles. Besides, the fre-
quency of appearance of candidate titles on
the Web (with Dice measure) is used in order
to measure the dependence between the noun
and the adjective composing a candidate ti-
tle. This method thus automatically favors
well-formed candidates.
3. Dynamic Contextualization of Candidate Ti-
tles. To determine the most relevant candi-
date title, the text context is compared with
the context in which these candidates are met
32
on the Web. They are both modeled as vec-
tors, according to Salton?s vector model.
Example of title with CATIT: Fiscal penalties.
The automatic generation of titles is a complex
task because titles have to be coherent, grammat-
ically correct, informative, and catchy. These cri-
teria are a brake in the generation of longer ti-
tles (being studied). That is why we suggest a
new approach consisting in reformulating rele-
vant phrases in order to determine informative and
catchy ?long? titles.
2.3 NOMIT
Based on statistical analysis, NOMIT (Nominal-
ization for Titling) provides original titles relying
on several rules to transform a verbal phrase in a
noun phrase.
1. Extracting Candidates. First step consists in
extracting segments of phrases which con-
tain a past participle (in French). For exam-
ple: In her speech, Mrs Merkel has promised
?concrete steps towards a fiscal union? -
in effect close integration of the tax-and-
spend polices of individual eurozone coun-
tries, with Brussels imposing penalties on
members that break the rules.
2. Linguistic Treatment. The linguistic treat-
ment of the segments retained in the previous
step consists of two steps aiming at nominal-
izing the ?auxiliary + past participle? form
(very frequent in French). First step consists
in associating a noun for each past participle.
Second step uses transforming rules in order
to obtain nominalized segments. For exam-
ple: has promised? promise.
3. Selecting a Title. Selection of the most rel-
evant title relies on a Web validation. The
interest of this validation is double. On one
hand, the objective is to validate the connec-
tion between the nominalized past partici-
ple and the complement. On the other hand,
the interest is to eliminate incorrect semantic
constituents or not popular ones (e.g., ?an-
nunciation of the winners ?), to prefer those
which are more popular on Web (e.g. , ?an-
nouncement of the winners?).
Figure 1: Screenshot of Automatic Titling Evaluation
Example of title with NOMIT: Mrs Merkel:
Promise of a concrete step towards a fiscal union.
This method enables to obtain even more orig-
inal titles than the previous one (i.e. CATIT).
A positive aspect is that new transforming rules
can be easily added in order to respect morpho-
syntactical patterns of real titles.
3 Evaluations
3.1 Protocol Description
An online evaluation has been set up, accessi-
ble to all people (cf. Figure 1)2. The benefit of
such evaluation is to compare different automatic
methods according to several judgements. So, for
each text proposed to the human user, several ti-
tles are presented, each one resulting from one of
the automatic titling methods presented in this pa-
per (POSTIT, CATIT, and NOMIT). Furthermore,
random titles stemming from CATIT and POSTIT
methods are evaluated (CATIT-R, and POSTIT-
R), i.e., candidate titles built by our methods but
not selected because of their bad score. The idea
is to measure the efficiency of our ranking func-
tions.
This evaluation is run on French articles stem-
ming from the daily newspaper ?Le Monde?. We
retained the first article published every day for
the year 1994, up to a total of 200 journalistic ar-
ticles. 190 people have participated to the online
experiment, evaluating a total of 1048 titles. On
average, every person has evaluated 41 titles. Ev-
ery title has been evaluated by several people (be-
tween 2 and 10). The total number of obtained
evaluations is 7764.
2URL: http://www2.lirmm.fr/?lopez/
Titrage_general/evaluation_web2/
33
3.2 Results
Results of this evaluation indicate that the most
adapted titling method for articles is NOMIT. This
one enables to title 82.7% of texts in a relevant
way (cf. Table 1). However, NOMIT does not de-
termine titles for all the texts (in this evaluation,
NOMIT determined a title for 58 texts). Indeed,
if no past participle is present in the text, there is
no title returned with this method. It is thus essen-
tial to consider the other methods which assure a
title for every text. POSTIT enables to title 70%
of texts in a relevant way. It is interesting to note
that both gathered methods POSTIT and NOMIT
provide at least one relevant title for 74 % of texts
(cf. Table 2). Finally, even if CATIT obtains a
weak score, this method provides a relevant title
where POSTIT and NOMIT are silent. So, these
three gathered methods propose at least one rele-
vant title for 81% of journalistic articles.
Concerning catchiness, the three methods seem
equivalent, proposing catchy titles for approxi-
mately 50% of texts. The three gathered methods
propose at least one catchy title for 78% of texts.
Real titles (RT) obtain close score (80.5%).
% POSTIT POSTIT-R CATIT CATIT-R NOMIT RT
Very relevant (VR) 39.1 16.4 15.7 10.3 60.3 71.4
Relevant (R) 30.9 22.3 21.3 14.5 22.4 16.4
(VR) and (R) 70.0 38.7 37.0 24.8 82.7 87.8
Not relevant 30.0 61.4 63.0 75.2 17.2 12.3
Catchy 49.1 30.9 47.2 32.2 53.4 80.5
Not catchy 50.9 69.1 52.8 67.8 46.6 19.5
Table 1: Average scores of our application.
% POSTIT & NOMIT POSTIT & CATIT NOMIT & CATIT POSTIT, CATIT, & NOMIT
(VR) 47 46 28 54
(R) or (VR) 74 78 49 81
Catchy 57 73 55 78
Table 2: Results of gathered methods.
Also, let us note that our ranking functions
are relevant since CATIT-R and POSTIT-R obtain
weak results compared with CATIT and POSTIT.
4 Conclusions
In this paper, we have compared the efficiency of
three methods using various techniques. POSTIT
uses noun phrases extracted from the text, CATIT
consists in constructing short titles, and NOMIT
uses nominalization. We proposed three different
methods to approach the real context. Two per-
sons can propose different titles for the same text,
depending on personal criteria and on its own in-
terests. That is why automatic titling is a complex
task as much as evaluation of catchiness which
remains subjective. Evaluation shows that our ap-
plication provides relevant titles for 81% of texts
and catchy titles for 78 % of texts. These re-
sults are very encouraging because real titles ob-
tain close results.
A future work will consist in taking into ac-
count a context defined by the user. For exam-
ple, the generated titles could depend on a polit-
ical context if the user chooses to select a given
thread. Furthermore, an ?extended? context, au-
tomatically determined from the user?s choice,
could enhance or refine user?s desiderata.
A next work will consist in adapting this appli-
cation for English.
References
Michele Banko, Vibhu O. Mittal, and Michael J Wit-
brock. 1996. Headline generation based on statisti-
cal translation. COLING?96. p. 318?325.
Jacques Chauche?. 1984. Un outil multidimensionnel
de l?analyse du discours. COLING?84. p. 11-15.
Be?atrice Daille. 1996. Study and implementation
of combined techniques for automatic extraction of
terminology. The Balancing Act: Combining Sym-
bolic and Statistical Approaches to language. p. 29-
36.
Rong Jin, and Alexander G. Hauptmann. 1996. Au-
tomatic title generation for spoken broadcast news.
Proceedings of the first international conference on
Human language technology research. p. 1?3.
Ce?dric Lopez, Violaine Prince, and Mathieu Roche.
2011. Automatic titling of Articles Using Position
and Statistical Information. RANLP?11. p. 727-
732.
Ce?dric Lopez, Violaine Prince, and Mathieu Roche.
2011. Automatic Generation of Short Titles.
LTC?11. p. 461-465.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information Processing and Management 24. p.
513-523.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. International Confer-
ence on New Methods in Language Processing. p.
44-49.
Franck Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations
for bilingual lexicons: A statistical approach. Com-
putational linguistics, 22(1). p. 1-38.
David Zajic, Bonnie Door, and Rich Schwarz. 2002.
Automatic headline generation for newspaper sto-
ries. ACL 2002. Philadelphia.
Liang Zhou and Eduard Hovy. 2002. Headline sum-
marization at ISI. DUC 2003. Edmonton, Alberta,
Canada.
34
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 274?283,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
NOMIT: Automatic Titling by Nominalizing
C?dric Lopez, Violaine Prince, and Mathieu Roche
LIRMM, CNRS, Univ. Montpellier 2
161, rue Ada
Montpellier, France
{lopez,prince,mroche}@lirmm.fr
Abstract
The important mass of textual documents is
in perpetual growth and requires strong ap-
plications to automatically process informa-
tion. Automatic titling is an essential task for
several applications: ?No Subject? e-mails ti-
tling, text generation, summarization, and so
forth. This study presents an original ap-
proach consisting in titling journalistic articles
by nominalizing. In particular, morphological
and semantic processing are employed to ob-
tain a nominalized form which has to respect
titles characteristics (in particular, relevance
and catchiness). The evaluation of the ap-
proach, described in the paper, indicates that
titles stemming from this method are informa-
tive and/or catchy.
1 Introduction
A title establishes a link between a reader and a
text. It has two main functions. First of all, a ti-
tle can be informative (it conveys relevant informa-
tion about the text content and aim), and second, it
can be catchy or incentive (Herrero Cecilia, 2007).
A heading is said to be catchy when it succeeds in
capturing the reader?s attention on an aspect of the
announced event, in a ingenious, metaphoric, enig-
matic, or shocking way. From a syntactic point of
view, a title can be a word, a phrase, an expression,
a sentence, that designates a paper or one of its parts,
by giving its subject.
Titles are used within applications such as auto-
matic generation of contents, or summarization. So,
it is interesting to automate the process that produces
relevant titles by extracting them from texts, and
supplying other applications with such data, while
avoiding any human intervention: Direct applica-
tions (as automatic titling of "no object" e-mails) are
thus possible.
The point is that several titles can be relevant for a
same text: This constitutes the main difficulty of au-
tomatic titling. Some writers prefer informative ti-
tles, whereas others prefer catchy ones. Others jug-
gle with both criteria according to the context and
the type of the publication. So, evaluation of au-
tomatic titling is a complex step requiring a human
intervention. Indeed, how can titles relevance be es-
timated ? How an automatic title can be compared
to a human-written ("real") title, knowing that both
can have a very different morphosyntactic structure?
Automatic titling is a full process, possessing its
own functions. It has to be sharply differentiated
from summarization and indexation tasks. Its pur-
pose is to propose title(s) that have to be short, infor-
mative and/or catchy, and keep a coherent syntactic
structure. NLP1 methods will be exploited in order
to abide by language morphosyntactic and semantic
constraints in titling.
In this paper, we describe an approach of auto-
matic titling relying on nominalization, i.e. rules
transforming a verb phrase into a noun phrase (e.g.
"the president left" is nominalized into " President?s
Departure"). This study raises two crucial questions:
(1) Determining sentences and phrases containing
relevant information (2) Nominalizing a chosen item
and using it as a title. Example: From the fol-
lowing pair of sentences "The disappointing perfor-
1Natural Language Processing
274
mance, on Sunday October 9th, of S?gol?ne Royal,
amazed the French citizens. For months, they de-
fended their candidate on the Web.", containing the
relevant information about an article in the French
press in 2007, the idea is to built the following title:
"S?gol?ne Royal: Surprise of the French citizens".
In fact, other titles could apply such as "S?gol?ne
Royal?s Disappointing Performance" or "Surprising
the French Citizens", but notice that both are less in-
formative, since they drop a part of the information.
This article is organized as such: The follow-
ing section briefly positions automatic titling in its
research environment and describes previous work
(section 2). The next one describes NOMIT, our ap-
proach of automatic titling by nominalization, which
consists in three successive steps: Extracting candi-
date headings from the document (section 3.1), pro-
cessing them linguistically (section 3.2), and last,
selecting one among the produced headings, which
will play the role of the system heading suggestion
(section 3.3). Finally, the results of NOMIT evalua-
tion are presented and discussed (section 4).
2 Previous Work
Automatic titling of textual documents is a subject
often confused with summarization and indexation
tasks. While a summary has to give an outline of the
text contents, the title has to indicate the subject of
the text without revealing all the contents. The pro-
cess of summarization can use titles, as in (Blais et
al., 2007) and (Amini et al, 2005), thus demonstrat-
ing their importance. Automatic summarization pro-
vides a set of relevant sentences extracted from the
text: The total number of sentences is diminished,
but sentences are not shortened by themselves. Ul-
timately reducing the number to one does not pro-
vide a title, since the latter is very rarely a sentence,
but needs to be grammatically consistent. It is also
necessary to differentiate automatic titling from text
compression: Text compression might shorten sen-
tences but keep the original number of sentences
(Yousfi-Monod and Prince, 2008). Mixing both ap-
proaches appears as a very costly process to under-
take, more adapted to a summarization task, when
titling might be obtained by less expansive tech-
niques.
Titling must also be differentiated from indexa-
tion because titles do not always contain the text
key-words: Headings can present a partial or total
reformulation of the text, not relevant for an index,
which role is to facilitate the user?s search and re-
trieval. Once again, the construction of an index can
use titles appearing in the document. So, if deter-
mining relevant titles is a successful task, the quality
of indexation will largely be improved.
An automatic titling approach, named POSTIT,
extracts relevant noun phrases to be used as titles
(Lopez et al, 2011b). One of its benefits is that long
titles, syntactically correct, can be proposed. The
main inconvenience is that it cannot provide orig-
inal titles, using a funny form for example, unless
this one already appears in the text (which can be
rather scarce, even in newspapers articles). In the
same environment, a variant of this approach, called
CATIT, constructing short titles, has been developed
by the same authors (Lopez et al, 2011a). It tries to
built titles which are relevant to the texts. It evalu-
ates their quality by browsing the Web (popular and
recognized expressions), as well as including those
titles dynamic context. Applied to a corpus of jour-
nalistic articles, CATIT was able to provide head-
ings both informative and catchy. However, syntac-
tical patterns used for titles building were short (two
terms) and experience showed that longer titles were
often preferred.
Another approach, presented by (Banko et al,
2000), consists in generating coherent summaries
that are shorter than a single sentence. These sum-
maries are called "headlines". The main difficulty is
to adjust the threshold (i.e., the length of the head-
line), in order to obtain syntactically correct titles.
This is the main difference with our method NOMIT,
which ensures that its produced titles are always syn-
tactically correct.
If a system were to produce informative, catchy,
and variable-sized (in number of words) titles, the
nominalization of constituents seems to be an inter-
esting approach. Nominalization is a process trans-
forming an adjective or a verb into a noun or noun
phrase. In a nominalized constituent, the time of the
event is not in touch with the time of the speech of
the event (for example, "President?s departure" does
not infer that the president already left, contrary to
"The president left"). In some languages such as
German and French, nominalization answers an ac-
275
tivity of conceptualization and conciseness. In a ti-
tle, it allows to focus, according to the context of
the author, on the dimension of the event consid-
ered the most relevant. (Moirand, 1975) already no-
ticed that in French journalistic articles, numerous
titles appear with a nominalized form. This obser-
vation was recently confirmed by (Herrero Cecilia,
2007). It is thus interesting to study automatic ti-
tling by nominalization of constituents when dealing
with languages where it is often used. In English, the
method stays the same, but the pattern changes: En-
glish headings patterns incline towards progressive
present (e.g. "Tempest looming"), an infinitive form
with a past participle (e.g. "Conference to be held"),
and always with a deletion of articles. This paper fo-
cuses mostly on French because of its available data,
but a shift in languages and patterns is contemplated
in a further step.
3 NOMIT: Titling by Nominalizing
Since nominalization converts a sentence into a noun
or a noun phrase, it can always be described by a
transformation. Some transformations are easy-to-
do, in particular, transforming verb participles into
names or adjectives (such as defined by (Dubois and
Dubois-Charlier, 1970)). For example, "arriv?(e)"
(arrived is a French verbal participle which is equal
to its nominalized shape "arriv?e" (arrival). Others
are more complex, for example the past participle
"parti" (gone) which nominalized form is "d?part"
(departure). For these last ones, the use of a lexicon
is necessary.
The nominalization process embedded in NOMIT
develops three successive stages. The first one con-
cerns the extraction of candidates according to a
classical process in NLP: Data preparation, mor-
phosyntactic labeling, selection of the data to be
studied. The second phase consists in performing
a linguistic process, including morphosyntactic and
semantic aspects. Finally, the third phase focuses on
selecting a relevant title. Figure 1 presents the global
process, detailed in the following sub-sections.
We chose to focus our study on journalistic ar-
ticles stemming from Le Monde (year 1994), a fa-
mous French daily paper, since their electronic form
is available for scientific investigation. Note that the
method presented in this paper is applicable to all
Figure 1: Global process of NOMIT
types of texts (articles, news, blogs, and so forth).
3.1 Extracting Candidates
This first phase consists in extracting the candidates
(cf. section 3.2), which will be considered as poten-
tial titles after a linguistic treatment. It consists, in
turn, of four steps. The first step determines the ar-
ticle relevant data (i.e. fragments or reformulations
representing at best the main information emanating
from the text).
The described approach relies on the assumption
that good candidate phrases can be found in the first
two sentences of the article. Actually the best cov-
ering rate of the words of real titles is obtained with
these first sentences (see (Baxendale, 1958), (Vinet,
1993), (Jacques and Rebeyrolle, 2004), and (Lopez
et al, 2011b) regarding the POSTIT approach), jus-
tifying this choice. So, here, the selection of relevant
sentences (cf. Fig. 1, step 1.a) is limited to extract-
ing the first two sentences of the text.
Step 1.b (cf. Fig. 1) consists in labeling these
two sentences via SYGFRAN (Chauch? and Prince,
276
2007), a morphosyntactic parser that tags words.
Thus, the presence of a "auxiliary + past partici-
ple" form syntactic pattern is tested2 (for example,
"a augment?" meaning has increased). If such a pat-
tern is recognized in the sentence, then it is retained
and goes into the following stages. Otherwise, the
sentence is ignored. Then, sentences are pruned ac-
cording to two heuristics.
(Knight and Marcu, 2002) have studied sentence
compression by using a noisy-channel model which
consists in making the following hypothesis: The
sentence to be compressed was formerly short and
the author has extended it with additional informa-
tion (noise). Sentence compression, could, at a first
glance, appear as a possible clue, however, our ap-
proach does not aim at reducing at most the treated
sentence. Indeed, elements which can be pruned to
obtain a good summary do not always need to be
pruned to obtain a good title. So, the NOMIT sen-
tence pruning step (cf. Fig. 1, step 1.c) does not only
preserve the governors3. Here, the text is pruned
according to three heuristics, inspired from (Yousfi-
Monod and Prince, 2008), focusing on the function
and position of constituents in the syntactic tree:
1. Elimination of dates (for example "The disap-
pointing performance, on Sunday, October 9th,
of S?gol?ne Royal" becomes "The disappoint-
ing performance of S?gol?ne Royal "),
2. Elimination of phrases directly juxtaposed to a
past participle (for example "He chose, while
he was still hesitating, to help him" becomes
"He chose to help him"),
3. Elimination of the relative pronoun and the
proposition introduced by it ("Its presence,
which was not moreover wished, was noticed"
becomes "Its presence was noticed ").
These three heuristics are crucial to obtain a co-
herent title. In this step, grammaticality4 and conci-
sion5 must be respected.
2the pattern features are tuned to French, but the same struc-
ture globally applies to English too.
3governors of constituents considered as indispensable to
the grammatical and semantic coherence of the sentence
4The sentence must be well formed and must obey the lan-
guage grammar.
5a pruned sentence has to contain the relevant information
of the original sentence.
Finally, both sentences are segmented accord-
ing to punctuation (points, commas, colons, brack-
ets, interrogation marks, exclamation marks, and so
forth6) and only segments containing a "auxiliary +
past participle" pattern are preserved (cf. Fig. 1,
step 1.d). Also, segments containing pronouns are
not retained in the following steps to avoid problems
related to referents 7.
In the following example, each step is indicated
by a reference sending back to the global process
presented in Figure 1:
Original text:
? Yet they truly believed in it. The disappointing
performance, on Sunday, October 9th, of S?-
gol?ne Royal, amazed the French citizens. For
months, they defended their candidate on the
Web.
Treatments:
? (1.a) Yet they truly believed in it. The disap-
pointing performance, on Sunday, October 9th,
of S?gol?ne Royal, amazed the French citizens.
? (1.b) The disappointing performance, on Sun-
day, October 9th, of S?gol?ne Royal, amazed
the French citizens.
? (1.c) The disappointing performance of S?-
gol?ne Royal, amazed the French citizens.
? (1.d) amazed the French citizens8.
The following step enables to determine a relevant
title from the result obtained at step 1.d.
3.2 Linguistic Treatment
The linguistic treatment of segments, present in
those sentences retained in the previous section, is
constituted by two stages aiming at nominalizing the
6Points marking an abbreviation are not obviously taken into
account in this step.
7For example, the title "Disappointment of her partisans"
would not be very informative because of the presence of "her"
(unknown referent).
8We shall see in the section 3.2.2 how, in some cases, it is
possible to take into account the subject, i.e. S?gol?ne Royal in
this example.
277
"auxiliary + past participle" pattern. Here, the verbal
basis is transformed into an action noun.
The first step consists in obtaining the infinitive
of the verb to be nominalized from the past partici-
ple. Then, from the infinitive, possible nominalized
forms are returned. Even if several linguistic stud-
ies propose classifications by families of suffixes, it
is complex to process them automatically. The use
of a lexicon is a good solution allowing to ensure a
correct nominalized form.
3.2.1 Semantic Treatment
From past participle towards infinitive verb.
In step 1.b, segments of sentences containing the
"auxiliary + past participle" syntactic pattern were
extracted. For every past participle extracted, the
endings of conjugation are eliminated, and only
radicals are preserved (for example, "mang?es"
(eaten) becomes "mang" (eat) (cf. Fig. 1, step
2.a). Afterwards, every radical is associated with its
infinitive verb using a lexicon9 built for that purpose
from the data established by the parser SYGFRAN
(cf. Fig. 1, step 2.b).
From infinitive verb towards the verb action.
JeuxDeMots10 is a French serious game enabling
the construction of a lexical network via a recre-
ational activity proposed on the Web. The prototype
was created in 2008 (Lafourcade and Zampa, 2007).
Today, more than 238,000 terms and more than
1,200,000 relations constitute the network. This
popular, evolutionary, and good quality network,
possesses a satisfactory knowledge coverage. All in
all, more than 40 types of relations were recorded
in the network. One of them interests us more par-
ticularly: The relation called "verb action". This
"action" is very interesting for obtaining a nominal-
ized form, in particular for verbs having their struc-
ture modified during their nominalization (addition
of suffix or prefix in particular). For example, we
obtain "d?part" (departure) from the infinitive "par-
tir" (to leave)(cf. Fig. 1, step 2.c).
Let us note that several action names can exist for
the same verb. For example, "annonce" (announce-
ment) and "annonciation" (annunciation) are two ac-
tions of the verb "annoncer" (to announce). At this
9this lexicon contains 5,897 entries.
10http://www.jeuxdemots.org
stage, all action names are preserved and will be
considered in the next phase, consisting in nominal-
izing the candidates determined in the step before.
3.2.2 Morphosyntactic Treatment
The morphosyntactic processing aims at estab-
lishing rules that automatically transform a con-
stituent into its nominalized form. The purpose is
not to establish an exhaustive list of transformation
rules but to assure a correct transformation.
To transpose the agents of a verb into a nominal-
ized constituent, the French language makes a pro-
ficient use of prepositions. So when nominalizing
"auxiliary + past participle" in order to connect it
with its complement, the preposition "de" ("of") is
mandatory11. In English, although "X of Y" is an
accepted pattern, the genitive form "Y(?s) X" would
be preferred. If the complement does not exist, the
subject takes its place.
? Rule 1: Subject + Aux + PP + Complement =>
Verb action + (de) + Complement
? Original sentence: Il a annonc? les gag-
nants (He announced the winners)
? Radicalisation (2.a): Annonc
? Infinitive (2.b): Annoncer
? Actions associated to the infinitive (2.c):
Annonce ; annonciation
? Nominalization (2.d): Annonce des gag-
nants (Announcement of the winners or
Winners? announcement ) ; annonciation
des gagnants (Annunciation of the winners
or Winners? annunciation)
? Rule 2: Subject + Aux + PP => Action of the
verb + (de) + Subject
? Original sentence: Le pr?sident a d?mis-
sionn? (The president resigned)
? Radicalisation (2.a): D?mission
? Infinitive (2.b): D?missionner
? Actions associated to the infinitive (2.c):
D?mission (Resignation)
? Nominalization (2.d): D?mission du
pr?sident (Resignation of the president or
President?s resignation)
11The preposition can be contracted if needed ("de le" = "du",
"de les" = "des", and so forth.)
278
In section 3.1, relative subordinate pronoun and
subordinate clauses are eliminated because the in-
formation they convey is too secondary to be empha-
sized in a title. For example, "My cousin, who lives
in Paris, moved" becomes "My cousin moved". So,
according to the second rule, the nominalized form
will be "Moving of my cousin" and not "Moving of
my cousin who lives in Paris".
The third rule leads to titles with a very popular
form in French newspapers. It is about contextual-
izing the information via the use of a proper noun.
So, if in the treated constituent a single proper noun
appears (easily locatable by the presence of a capital
letter), the common noun can be put in connection
with the nominalized past participle (without con-
cluding that this common noun is an agent of the
nominalized verb). This new rule produces titles
with the following form: "Proper noun: verb action
+ Prep + Complement". For example, "S?gol?ne re-
turned to Strasburg" becomes "S?gol?ne: Strasburg
comeback".
? Rule 3: Subject + Aux + PP => Proper Noun:
Verb action + (de) + Complement (if it exists
only one proper noun in the subject)
? Original sentence: Bon nombre de par-
ticuliers se sont pr?cipit?s (rushed)aux
guichets des banques pour souscrire ? des
PEL (Several individuals rushed to bank
counters and subscribed to home-buying
savings plans)
? Radicalisation (2.a): Pr?cipit
? Infinitive (2.b): Pr?cipiter
? Action associated to the infinitive (2.c):
Pr?cipitation
? Nominalization (2.d): PEL : pr?cipitation
aux guichets des banques (Home Buying
Saving plans: Rush at Banks Counters)
Section 3.2.1, pointed that several nominalized
forms were possible for the same verb. So, the phase
of linguistic treatment enables to determine a list of
possible noun forms for every constituent. For ex-
ample, if in step 1 we had "The restaurant Gazza,
situated in a business area, announced a new price",
rule 1 would transform this sentence into two can-
didates: "Gazza: New price announcement" and
"Gazza: New price annunciation" (queer indeed!).
The following phase consists in selecting the most
relevant candidate.
3.3 Selecting a Title
The selection of the most relevant title relies on a
Web validation (cf. Fig. 1, stage 3). A segment that
frequently appears on the Web tends to be seen as:
(1) popular, (2) structurally sound. Thus, the fre-
quency of appearance of n-grams on the Web (via
the Google search engine) appears as a good indica-
tor of the n-gram popularity/soundness (Keller and
Lapata, 2003) . In our case, a n-gram is a segment of
the nominalized constituent, constituted by the nom-
inalized past participle (NPP) and by the preposition
followed by the short complement (i.e. reduced to
the common noun).
The benefit of this validation is double. On one
hand, it backs up the connection between the NPP
and the complement (or subject according to the rule
of used transformation). On the other hand, it helps
eliminating semantically incorrect or unpopular con-
stituents (for example, "Winners? annunciation") to
prefer those which are more popular on the Web (for
example, "Winners? announcement") 12.
3.4 Discussion
Our automatic titling approach (NOMIT) proposes
titles for journalistic articles containing a "auxiliary
+ past participle" form in at least one of its first two
sentences. The rationale for such a method is not
only conciseness, but also presentation: How to gen-
erate a heading inciting the reader to go further on.
Of course, transformation rules such as those pre-
sented here, can be numerous and various, and de-
pend on language, genre, and purpose. The basic
purpose of this work is to provide a sort of a "proof
of concept", in which relevant titles might be auto-
matically shaped.
12We do not here claim to select the most coherent con-
stituents regarding the text. Since the main hypothesis underly-
ing this study is that the first two sentences of the article contain
the necessary and sufficient information to determine a relevant
title, we consider implicitly obtaining nominalized constituents,
that are relevant to the text
279
4 Evaluation
Evaluation of titles is a difficult and boring task.
That is why we set up an online evaluation to share
the amount of work. A call for participation was
submitted in the French community of researchers
(informatics, linguistics). Even if we do not know
the information relative to every annotator (national-
ity, age, etc.), we think that a great majority of these
annotators have a rather good level in French, to
judge titles (this is confirmed by the well-writing of
the collected definitions for "relevance" and "catch-
iness").
NOMIT has been evaluated according to two pro-
tocols. The first one consisted in a quantitative
evaluation, stemming from an on-line user evalua-
tion13. 103 people have participated to this evalua-
tion. The second was an evaluation performed by 3
judges. This last one enables to compute the agree-
ment inter-judges on the various criteria of the eval-
uation process. In both cases, the French daily paper
Le Monde (1994) is used, thus avoiding any con-
nection to the subjectivity of recent news personal
analysis.
4.1 Quantitative Evaluation
4.1.1 Protocol Description
As previously seen, titles proposed by automatic
methods cannot be automatically evaluated. So, an
on-line evaluation was set up, opened to every per-
son. The interest of such an evaluation is to compare
the various methods of automatic titling (cf. section
2) according to several judgments. So, for every text
proposed to the human judges, four titles were pre-
sented, each resulting from different methods of ti-
tling:
? NOMIT: Automatic Titling by Nominalizing.
? POSTIT: Based on the extraction of noun
phrases to propose them as titles.
? CATIT: Based on the construction of short ti-
tles.
? Real Title (RT).
13http://www.lirmm.fr/~lopez/Titrage_
general/evaluation_web2/
For every title, the user had to attribute one of the
following labels: "relevant", "rather relevant", "irrel-
evant", "neutral". Also, the user had to estimate the
catchiness, by choosing one of the following labels:
"catchy", "not catchy", "neutral". Before beginning
the evaluation, the user is asked about his/her own
definition of a relevant title and of a catchy title
(all in all, 314 definitions were collected). Globally,
there is a popular consensus saying that a title is rel-
evant if it is syntactically correct while reflecting the
essential idea conveyed in the document. However,
definitions of catchiness were less consensual. Here
are some collected definitions:
1. A title is catchy if the words association is syn-
tactically correct but semantically "surprising".
However, a catchy title has to be close to the
contents of the text.
2. A catchy title is a title which tempts the reader
into going through the article.
3. A title which holds attention, a title which we
remember, a funny title for example.
4. A title which is going to catch my attention be-
cause it corresponds to my expectations or my
centers of personal interests.
5. A catchy title is a short and precise title.
The titled texts were distributed to the judges in a
random way. Every title was estimated by a number
of persons between 2 and 10. All in all, 103 persons
participated in the evaluation of NOMIT.
Let p1 be the number of titles considered relevant,
p2 the number of titles considered rather relevant,
and let p3 be the number of titles considered irrel-
evant. Within the framework of this evaluation, it
is considered that a title is relevant if p1 ? p3, and
rather relevant if p2 ? p3.
A title is considered "catchy" if at least two judges
considered it catchy.
4.1.2 Results
In spite of the weak number of titles estimated in
this first evaluation, the significant number of judges
helped obtaining representative results. In our ex-
periments, 53 titles generated by the NOMIT ap-
proach were evaluated representing a total of 360
280
evaluations. These results were compared with the
200 titles generated with POSTIT, 200 with CATIT,
and 200 RT (653 titles and 8354 evaluations). Re-
sults (cf. Table 1) show that 83% of the titles pro-
posed by NOMIT were seen as relevant or rather
relevant, against 70% for the titles stemming from
the POSTIT approach, and 37% for the titles stem-
ming from CATIT. Besides, NOMIT determines ti-
tles appreciably more catchy than both POSTIT
and CATIT. Concerning the real titles (RT), 87.8%
were judged relevant and 80.5% were catchy, mean-
ing that humans still perform better than automated
techniques, but only slightly for the relevance crite-
rion, and anyway, are not judged as perfect (refer-
ence is far from absolute!).
en % Relevant Weak relevant Irrelevant Catchy Not catchy
POSTIT 39.1 30.9 30 49.1 50.9
CATIT 15.7 21.3 63 47.2 52.8
NOMIT 60.3 22.4 17.2 53.4 46.6
RT 71.4 16.4 12.3 80.5 19.5
Table 1: Evaluation Results for POSTIT, CATIT,
NOMIT, and RT (Real Titles).
4.2 Agreement Inter-judges
4.2.1 Protocol Description
This evaluation is similar to the previous one
(same Web interface). The main difference is that
we retained the first 100 articles appeared in Le
Monde 1994 which enables our approach to return
a title. Three judges estimated the real title as well
as the NOMIT title for each of the texts, that is, a
total of 600 evaluations.
4.2.2 Results
Kappa coefficient (noted K) is a measure defined
by (Cohen, 1960) calculating the agreement between
several annotators. It is based on the rate of ob-
served concordances (Po) and on the rate of ran-
dom concordances (Pe). Here the Kappa coeffi-
cient estimates the agreement inter-judges about the
relevance and of catchiness of NOMIT titles (cf. Ta-
bles 2 - 4). Considering the results and according to
(Landis and Koch, 1977), judges seem to obtain an
average concordance for the relevance of NOMIT ti-
tles. This can be justified by the fact that there is a
consensus between the three judges about the defini-
tion of what is a relevant title (cf. Table 3). Approxi-
mately 71% of the titles were considered relevant by
three judges (cf. Table 2).
On the other hand, the three judges obtain a bad
concordance regarding catchiness; a catchy title for
the one, could not be catchy for the other one. This
is perfectly coherent with the definitions given by
the three judges:
1. A title is catchy if the association of the words
is syntactically correct but semantically "sur-
prising".
2. A catchy title is a title which drives you to read
the article.
3. A catchy title is a title which holds attention of
the reader and tempts him/her to read the con-
cerned text .
So, people have judged catchiness according to
syntax, the relation between semantics of the title
and semantic of the text, or have evaluated catchi-
ness according to personal interests. The notion of
catchiness is based on these three criteria. So, we
could not expect a strong agreement between the as-
sessors concerning the catchy character of a title (cf.
Table 3).
in % Relevant Irrelevant Neutral Total
Relevant 70.7 10.3 0.7 81.7
Irrelevant 6.0 10.3 0.7 17.0
Neutral 1.0 0.3 0.0 1.3
Total 77.7 21.0 0.7 100.0
Table 2: Contingency Matrix for NOMIT (relevance).
in % Catchy Not Catchy Neutral Total
Catchy 13.3 7.7 0.0 21.0
Not catchy 34.7 41.0 1.3 77.0
Neutral 0.7 1.3 0.0 2.0
Total 48.7 50.0 1.3 100.0
Table 3: Contingency Matrix for NOMIT (catchiness).
As a rough guide, short journalistic articles14 ob-
tain better results than long articles (93% are rele-
vant in that case and 69% are catchy). It thus seems
14We consider that an article is short when its number of
words is less than 100.
281
K avg. Po avg. Pe avg.
Relevance 0.42 0.81 0.67
Catchiness 0.10 0.54 0.49
Average 0.28 0.68 0.58
Table 4: Kappa average for relevance and catchiness of
titles obtained with NOMIT.
that our approach of automatic titling by nominaliza-
tion is more adapted to short texts. We are extremely
prudent concerning this interpretation because it is
based on only 29 articles.
5 Conclusion
Automatic titling is a complex task because titles
must be at once informative, catchy, and syntacti-
cally correct. Based on linguistic and semantic treat-
ments, our approach determines titles among which
approximately 80% were evaluated as relevant and
more than 60% were qualified as catchy. Experiment
and results discussion have pointed at the following
liability: The value of Kappa, the inter-judges agree-
ment coefficient, is very difficult to evaluate, mostly
when catchiness is at stake. The main cause is that it
depends on personal interests. It is thus necessary to
ask the following question: Do we have to consider
that a title is definitely catchy when at least one per-
son judges it so? Otherwise, how many people at
least? This is still an open question and needs to be
further investigated.
Also, some interesting extensions could be en-
visaged: The approach presented in this paper uses
three rules of transformation based on the presence
of an auxiliary followed by a past participle. The ad-
dition of new rules would enable a syntactic enrich-
ment of the titles. So, it might be profitable to set up
rules taking into account the presence of syntactical
patterns (others than "auxiliary + past participle") to
allow more texts to be titled by NOMIT.
Taking the punctuation of the end of sentences
into account might also be a promising track. For
example, "did it use an electric detonator?" would
become "Use of an electric detonator?". It is an in-
teresting point because the presence of a punctuation
at the end of a title (in particular the exclamation or
the interrogation) constitutes a catchy criterion.
Last, NOMIT is a method (easily reproducible in
other languages, English in particular) that stepped
out of preceding attempts in automatic headings
generation (POSTIT, CATIT). Exploring syntac-
tic patterns, as it does, means that increasing the
amount of linguistic information in the process
might lead to a reliable heading method. One of
the perspectives can be to track the optimum point
between the richness of involved information and
processes, and the cost of the method. The in-
cremental methodology followed from POSTIT to
NOMIT tends to enhance the belief that parameters
(i.e. length, shape, relevance, etc...) for an auto-
matic heading procedure have to be studied and well
defined, thus leading to a customized titling process.
References
Massih R. Amini, Nicolas Usunier, and Patrick Gallinari.
2005. Automatic text summarization based on word-
clusters and ranking algorithms. Advances in Informa-
tion Retrieval, pages 142?156.
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 318?325. Association for Computational
Linguistics.
Phyllis B. Baxendale. 1958. Man-made index for tech-
nical literature - an experiment. IBM Journal of Re-
search and Development., pages 354?361.
Antoine Blais, Iana Atanassova, Jean-Pierre Descl?s,
Mimi Zhang, and Leila Zighem. 2007. Discourse au-
tomatic annotation of texts: an application to summa-
rization. In Proceedings of the Twentieth International
Florida Artificial Intelligence Research Society Con-
ference, May, pages 7?9.
Jacques Chauch? and Violaine Prince, Vp. 2007. Clas-
sifying texts through natural language parsing and
semantic filtering. In 3rd International Language
and Technology Conference, pages 012?020, Poznan,
Pologne, October.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and psychological measure-
ment, 20(1):37?46.
Jean Dubois and Fran?oise Dubois-Charlier. 1970. El?-
ments de linguistique fran?aise: syntaxe. Larousse.
Juan Herrero Cecilia. 2007. Syntaxe, s?mantique
et pragmatique des titres des nouvelles de la presse
fran?aise construits en forme de phrase nominale ou
averbale: aspects cognitifs et communicatifs. In Lit-
t?rature, langages et arts: rencontres et cr?ation,
page 97. Servicio de Publicaciones.
282
Marie-Paule Jacques and Josette Rebeyrolle. 2004.
Titres et structuration des documents. In Actes In-
ternational Symposium: Discourse and Document.,
pages 125?152.
Franck Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional linguistics, 29(3):459?484.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Mathieu Lafourcade and Virginie Zampa. 2007. Making
people play for lexical acquisition. In SNLP 2007, 7th
Symposium on Natural Language Processing. Pattaya.
J. Richard Landis and Garry G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159.
C?dric Lopez, Violaine Prince, and Mathieu Roche.
2011a. Automatic generation of short titles. In 5th
Language and Technology Conference, LTC?11, pages
461?465.
C?dric Lopez, Violaine Prince, and Mathieu Roche.
2011b. Automatic titling of articles using position
and statistical information. In RANLP?11: Recent Ad-
vances in Natural Language Processing, pages 727?
732, Hissar, Bulgarie, September.
Sophie Moirand. 1975. Le r?le anaphorique de la nom-
inalisation dans la presse ?crite. Langue fran?aise,
28(1):60?78.
Marie-Th?r?se Vinet. 1993. L?aspect et la copule vide
dans la grammaire des titres. Persee, 100:83?101.
Mehdi Yousfi-Monod and Violaine Prince. 2008. Sen-
tence compression as a step in summarization or an
alternative path in text shortening. In Coling?08: In-
ternational Conference on Computational Linguistics,
Manchester, UK., pages 139?142.
283
