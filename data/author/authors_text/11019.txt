Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 825?832
Manchester, August 2008
From Words to Senses: A Case Study of Subjectivity Recognition
Fangzhong Su
School of Computing
University of Leeds, UK
fzsu@comp.leeds.ac.uk
Katja Markert
School of Computing
University of Leeds, UK
markert@comp.leeds.ac.uk
Abstract
We determine the subjectivity of word
senses. To avoid costly annotation, we
evaluate how useful existing resources es-
tablished in opinion mining are for this
task. We show that results achieved with
existing resources that are not tailored to-
wards word sense subjectivity classifica-
tion can rival results achieved with super-
vision on a manually annotated training
set. However, results with different re-
sources vary substantially and are depen-
dent on the different definitions of subjec-
tivity used in the establishment of the re-
sources.
1 Introduction
In recent years, subjectivity analysis and opinion
mining have attracted considerable attention in the
NLP community. Unlike traditional information
extraction and document classification tasks which
usually focus on extracting facts or categorizing
documents into topics (e.g., ?sports?, ?politics?,
?medicine?), subjectivity analysis focuses on de-
termining whether a language unit (such as a word,
sentence or document) expresses a private state,
opinion or attitude and, if so, what polarity is ex-
pressed, i.e. a positive or negative attitude.
Inspired by Esuli and Sebastiani (2006) and
Wiebe and Mihalcea (2006), we explore the auto-
matic detection of the subjectivity of word senses,
in contrast to the more frequently explored task
of determining the subjectivity of words (see Sec-
tion 2). This is motivated by many words being
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
subjectivity-ambiguous, i.e. having both subjec-
tive and objective senses, such as the word positive
with its two example senses given below.
1
(1) positive, electropositive?having a positive electric
charge;?protons are positive? (objective)
(2) plus, positive?involving advantage or good; ?a plus
(or positive) factor? (subjective)
Subjectivity labels for senses add an additional
layer of annotation to electronic lexica and allow to
group many fine-grained senses into higher-level
classes based on subjectivity/objectivity. This can
increase the lexica?s usability. As an example,
Wiebe and Mihalcea (2006) prove that subjectiv-
ity information for WordNet senses can improve
word sense disambiguation tasks for subjectivity-
ambiguous words (such as positive). In addition,
Andreevskaia and Bergler (2006) show that the
performance of automatic annotation of subjectiv-
ity at the word level can be hurt by the presence of
subjectivity-ambiguous words in the training sets
they use. Moreover, the prevalence of different
word senses in different domains also means that
a subjective or an objective sense of a word might
be dominant in different domains; thus, in a sci-
ence text positive is likely not to have a subjective
reading. The annotation of words as subjective and
objective or positive and negative independent of
sense or domain does not capture such distinctions.
In this paper, we validate whether word sense
subjectivity labeling can be achieved with existing
resources for subjectivity analysis at the word and
sentence level without creating a dedicated, man-
ually annotated training set of WordNet senses la-
beled for subjectivity.
2
We show that such an ap-
1
All examples in this paper are from WordNet 2.0.
2
We use a subset of WordNet senses that are manually an-
notated for subjectivity as test set (see Section 3).
825
proach ? even using a simple rule-based unsu-
pervised algorithm ? can compete with a stan-
dard supervised approach and also compares well
to prior research on word sense subjectivity label-
ing. However, success depends to a large degree
on the definition of subjectivity used in the estab-
lishment of the prior resources.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Sec-
tion 3 introduces our human annotation scheme
for word sense subjectivity and also shows that
subjectivity-ambiguous words are frequent. Sec-
tion 4 describes our proposed classification algo-
rithms in detail. Section 5 presents the experimen-
tal results and evaluation, followed by conclusions
and future work in Section 6.
2 Related Work
There has been extensive research in opinion min-
ing at the document level, for example on product
and movie reviews (Pang et al, 2002; Pang and
Lee, 2004; Dave et al, 2003; Popescu and Etzioni,
2005). Several other approaches focus on the
subjectivity classification of sentences (Kim and
Hovy, 2005; Kudo and Matsumoto, 2004; Riloff
and Wiebe, 2003). They often build on the pres-
ence of subjective words in the sentence to be clas-
sified.
Closer to our work is the large body of work
on the automatic, context-independent classifica-
tion of words according to their polarity, i.e as pos-
itive or negative (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003; Kim and Hovy,
2004; Takamura et al, 2005). They use either
co-occurrence patterns in corpora or dictionary-
based methods. Many papers assume that sub-
jectivity recognition, i.e. separating subjective
from objective words, has already been achieved
prior to polarity recognition and test against word
lists containing subjective words only (Hatzivas-
siloglou and McKeown, 1997; Takamura et al,
2005). However, Kim and Hovy (2004) and An-
dreevskaia and Bergler (2006) also address the
classification into subjective/objective words and
show this to be a potentially harder task than po-
larity classification with lower human agreement
and automatic performance.
There are only two prior approaches address-
ing word sense subjectivity or polarity classifi-
cation. Esuli and Sebastiani (2006) determine
the polarity of word senses in WordNet, distin-
guishing among positive, negative and objective.
They expand a small, manually determined seed
set of strongly positive/negative WordNet senses
by following WordNet relations and use the result-
ing larger training set for supervised classification.
The resulting labeled WordNet gives three scores
for each sense, representing the positive, negative
and objective score respectively. However, there
is no evaluation as to the accuracy of their ap-
proach. They then extend their work (Esuli and
Sebastiani, 2007) by applying the Page Rank algo-
rithm to ranking the WordNet senses in terms of
how strongly a sense possesses a given semantic
property (e.g., positive or negative).
Wiebe and Mihalcea (2006) label word senses
in WordNet as subjective or objective. They use a
method relying on distributional similarity as well
as an independent, large manually annotated opin-
ion corpus (MPQA) (Wiebe et al, 2005) for deter-
mining subjectivity. One of the disadvantages of
their algorithm is that it is restricted to senses that
have distributionally similar words in the MPQA
corpus, excluding 23.2% of their test data from au-
tomatic classification.
3 Human Annotation of Word Sense
Subjectivity and Polarity
In contrast to other researchers (Hatzivassiloglou
and McKeown, 1997; Takamura et al, 2005), we
do not see polarity as a category that is depen-
dent on prior subjectivity assignment and therefore
applicable to subjective senses only. We follow
Wiebe and Mihalcea (2006) in that we see subjec-
tive expressions as private states ?that are not open
to objective observation or verification?. This in-
cludes direct references to emotions, beliefs and
judgements (such as anger, criticise) as well as ex-
pressions that let a private state be inferred, for
example by referring to a doctor as a quack. In
contrast, polarity refers to positive or negative as-
sociations of a word or sense. Whereas there is a
dependency in that most subjective senses have a
relatively clear polarity, polarity can be attached to
objective words/senses as well. For example, tu-
berculosis is not subjective ? it does not describe
a private state, is objectively verifiable and would
not cause a sentence containing it to carry an opin-
ion, but it does carry negative associations for the
vast majority of people.
We therefore annotate subjectivity of word
senses similarly to Wiebe and Mihalcea (2006),
826
distinguishing between subjective (S), objective
(O) or both (B). Both is used if a literal and
metaphoric sense of a word are collapsed into one
WordNet synset or if a WordNet synset contains
both opinionated and objective expressions (such
as bastard and illegitimate child in Ex. 3 below).
We expand their annotation scheme by also an-
notating polarity, using the labels positive (P), neg-
ative (N) and varied (V). The latter is used when a
sense?s polarity varies strongly with the context,
such as Example 8 below, where we would ex-
pect uncompromising to be a judgement but this
judgement will be positive or negative depending
on what a person is uncompromising about. To
avoid prevalence of personalised associations, an-
notators were told to only annotate polarity for
subjective senses, as well as objective senses that
carry a strong association likely to be shared by
most people at least in Western culture (such as the
negative polarity for words referring to diseases
and crime). Other objective senses would receive
the label O:NoPol.
Therefore, we have 7 sub categories in total:
O:NoPol, O:P, O:N, S:P, S:N, S:V, and B. The
notation before and after the colon represents the
subjectivity and polarity label respectively. We list
some annotated examples below.
(3) bastard, by-blow, love child, illegitimate child, illegiti-
mate, whoreson? the illegitimate offspring of unmar-
ried parents (B)
(4) atrophy?undergo atrophy; ?Muscles that are not used
will atrophy? (O:N)
(5) guard, safety, safety device?a device designed to pre-
vent injury (O:P)
(6) nasty, awful?offensive or even (of persons) mali-
cious;?in a nasty mood?;?a nasty accident?; ?a nasty
shock? (S:N)
(7) happy?enjoying or showing or marked by joy or plea-
sure or good fortune; ?a happy smile?;?spent many
happy days on the beach?; ?a happy marriage? (S:P)
(8) uncompromising, inflexible?not making concessions;
?took an uncompromising stance in the peace talks?
(S:V)
As far as we are aware, this is the first annotation
scheme for both subjectivity and polarity of word
senses. We believe both are relevant for opinion
extraction: subjectivity for finding and analysing
directly expressed opinions, and polarity for ei-
ther classifying these further or extracting objec-
tive words that, however, serve to ?colour? a text
or present bias rather than explicitly stated opin-
ions. Su and Markert (2008) describe the annota-
tion scheme and agreement study in full.
3.1 Agreement Study
We used the Micro-WNOp corpus containing 1105
WordNet synsets to test our annotation scheme.
3
The Micro-WNOp corpus is representative of the
part-of-speech distribution in WordNet.
Two annotators (both near-native English speak-
ers) independently annotated 606 synsets of the
Micro-WNOp corpus for subjectivity and polarity.
One annotator is the second author of this paper
whereas the other is not a linguist. The overall
agreement using all 7 categories is 84.6%, with a
kappa of 0.77, showing high reliability for a dif-
ficult pragmatic task. This at first seems at odds
with the notion of sentiment as a fuzzy category as
expressed in (Andreevskaia and Bergler, 2006) but
we believe is due to three factors:
? The annotation of senses instead of words
splits most subjectivity-ambiguous words
into several senses, removing one source of
annotation difficulty.
? The annotation of senses in a dictionary pro-
vided the annotators with sense descriptions
in form of Wordnet glosses as well as re-
lated senses, providing more information than
a pure word annotation task.
? The split of subjectivity and polarity annota-
tion made the task clearer and the annotation
of only very strong connotations for objective
word senses ?de-individualized? the task.
As in this paper we are only interested in subjec-
tivity recognition, we collapse S:V, S:P, and S:N
into a single label S and O:NoPol, O:P, and O:N
into a single label O. Label B remains unchanged.
For this three-way annotation overall percentage
agreement is 90.1%, with a kappa of 0.79.
3.2 Gold Standard
After cases with disagreement were negotiated be-
tween the two annotators, a gold standard annota-
tion was agreed upon. Our test set consists of this
agreed set as well as the remainder of the Micro-
WNOp corpus annotated by one of the annotators
alone after agreement was established. This set is
available for research purposes at http://www.
comp.leeds.ac.uk/markert/data.
3
The corpus has originally been annotated by the
providers (Esuli and Sebastiani, 2007) with scores for posi-
tive, negative and objective/no polarity, thus a mixture of sub-
jectivity and polarity annotation. We re-annotated the corpus
with our annotation scheme.
827
How many words are subjectivity-ambiguous?
As the number of senses increases with word fre-
quency, we expect rare words to be less likely
to be subjectivity-ambiguous than frequent words.
The Micro-WNOp corpus contains relatively fre-
quent words so we will get an overestimation of
subjective-ambiguous word types from this cor-
pus, though not necessarily of word tokens. It in-
cludes 298 different words with all their synsets
in WordNet 2.0. Of all words, 97 (32.5%) are
subjectivity-ambiguous, a substantial number.
4 Algorithms
In this section, we present experiments using five
different resources as training sets or clue sets for
this task. The first is the Micro-WNOp corpus with
our own dedicated word sense subjectivity anno-
tation which is used in a standard supervised ap-
proach as training and test set via 10-fold cross-
validation. This technique presupposes a man-
ual annotation effort tailored directly to our task
to provide training data. As it is costly to create
such training sets, we investigate whether exist-
ing resources such as two different subjective sen-
tence lists (Section 4.2) and two different subjec-
tive word lists (Section 4.3) can be adapted to pro-
vide training data or clue sets although they do not
provide any information about word senses. All
resources are used to create training data for su-
pervised approaches; the subjective word lists are
also used in a simple rule-based unsupervised ap-
proach.
All algorithms were tested on the Micro-WNOp
corpus by comparing to the human gold stan-
dard annotation. However, we excluded all senses
with the label both from Micro-WNOp for test-
ing the automatic algorithms, resulting in a final
1061 senses, with 703 objective and 358 subjec-
tive senses. We also compare all algorithms to a
baseline of always assigning the most frequent cat-
egory (objective) to each sense, which results in an
overall accuracy of 66.3%.
4.1 Standard Supervised Approach: 10-fold
Cross-validation (CV) on Micro-WNOp
We use 10-fold cross validation for training and
testing on the annotated synsets in the Micro-
WNOp corpus. We applied a Naive Bayes clas-
sifier,
4
using the following three types of features:
4
We also experimented with KNN, Maximum Entropy,
Rocchio and SVM algorithms and overall Naive Bayes per-
Lexical Features: These are unigrams in the
glosses. We use a bag-of-words approach and filter
out stop words.
As glosses are usually quite short, using a bag-
of-word feature representation will result in high-
dimensional and sparse feature vectors, which of-
ten deteriorate classification performance. In order
to address this problem to some degree, we also ex-
plored other features which are available as train-
ing and test instances are WordNet synsets.
Part-of-Speech (POS) Features: each sense
gets its POS as a feature (adjective, noun, verb or
adverb).
Relation Features: WordNet relations are good
indicators for determining subjectivity as many
of them are subjectivity-preserving. For exam-
ple, if sense A is subjective, then its antonym
sense B is likely to be subjective. We employ
8 relations here?antonym, similar-to, derived-
from, attribute, also-see, direct-hyponym, direct-
hypernym, and extended-antonym. Each relation
R leads to 2 features that describe for a sense A
how many links of that type it has to synsets in the
subjective or the objective training set respectively.
Finally, we represent the feature weights
through a TF*IDF measure.
Considering the size of WordNet (115,424
synsets inWordNet 2.0), the labeled Micro-WNOp
corpus is small. Therefore, the question arises
whether it is possible to adapt other data sources
that provide subjectivity information to our task.
4.2 Sentence Collections: Movie and MPQA
It is reasonable to cast word sense subjectivity
classification as a sentence classification task, with
the glosses that WordNet provides for each sense
as the sentences to be classified. Then we can in
theory feed any collection of annotated subjective
and objective sentences as training data into our
classifier while the annotated Micro-WNOp cor-
pus is used as test data. We experimented with two
different available data sets to test this assumption.
Movie-domain Subjectivity Data Set (Movie):
Pang and Lee (2004) used a collection of labeled
subjective and objective sentences in their work
on review classification.
5
The data set contains
5000 subjective sentences, extracted from movie
reviews collected from the Rotten Tomatoes web
formed best.
5
Available at http://www.cs.cornell.edu/
People/pabo/movie-review-data/
828
site.
6
The 5000 objective sentences were col-
lected from movie plot summaries from the In-
ternet Movie Database (IMDB). The assumption
is that all the snippets from the Rotten Tomatoes
pages are subjective (as they come from a review
site), while all the sentences from IMDB are ob-
jective (as they focus on movie plot descriptions).
The MPQA Corpus contains news articles
manually annotated at the phrase level for opin-
ions, their polarity and their strength. The cor-
pus (Version 1.2) contains 11,112 sentences. We
convert it into a corpus of subjective and objective
sentences following exactly the approach in (Riloff
et al, 2003; Riloff and Wiebe, 2003) and obtain
6127 subjective and 4985 objective sentences re-
spectively. Basically any sentence that contains at
least one strong subjective annotation at the phrase
level is seen as a subjective sentence.
We again use a Naive Bayes algorithm with lex-
ical unigram features. Note that part-of-speech
and relation features are not applicable here as the
training set consists of corpus sentences, notWord-
Net synsets.
4.3 Word Lists: General Inquirer and
Subjectivity List
Several word lists annotated for subjectivity or po-
larity such as the General Inquirer (GI)
7
or the sub-
jectivity clues list (SL) collated by Janyce Wiebe
and her colleagues
8
are available.
The General Inquirer (GI) was developed by
Philip Stone and colleagues in the 1960s. It con-
centrates on word polarity. Here we make the
simple assumption that both positive and negative
words in the GI list are subjective clues whereas
all other words are objective.
The Subjectivity Lexicon (SL) centers on
subjectivity so that it is ideally suited for our
task. It provides fine-grained information for each
clue, such as part-of-speech, subjectivity strength
(strong/weak), and prior polarity (positive, nega-
tive, or neutral). For example, object(verb) is a
subjective clue whereas object(noun) is objective.
Regarding strength, the adjective evil is marked as
strong subjective whereas the adjective exposed is
marked as a weak subjective clue.
Both lexica do not include any information
about word senses and therefore cannot be used
directly for subjectivity assignment at the sense
6
http://www.rottentomatoes.com/
7
http://www.wjh.harvard.edu/
?
inquirer/
8
http://www.cs.pitt.edu/mpqa/
level. For example, at least one sense of any
subjectivity-ambiguous word will be labeled incor-
rectly if we just adopt a word-based label. In addi-
tion, these lists are far from complete: compared to
the over 100,000 synsets in WordNet, GI contains
11,788 words marked for polarity (1915 positive,
2291 negative and 7582 no-polarity words) and the
SL list contains about 8,000 subjective words.
Still, it is a reasonable assumption that any gloss
that contains several subjective words indicates a
subjective sense overall. This intuition is strength-
ened by the characteristics of glosses. They nor-
mally are short and concise without a complex syn-
tactic structure, thus the occurrence of subjective
words in such a short string is likely to indicate
a subjective sense overall. This contrasts, for ex-
ample, with sentences in newspapers where one
clause might express an opinion, whereas other
parts of the sentence are objective.
Therefore, for the rule-based unsupervised
algorithm we lemmatized and POS-tagged the
glosses in the Micro-WNOp test set. Then we
compute a subjectivity score S for each synset by
summing up the weight values of all subjectivity
clues in its gloss. If S is equal or higher than an
agreed threshold T, then the synset is classified as
subjective, otherwise as objective. For the GI lexi-
con, all subjectivity clues have the same weight 1,
whereas for the SL list we assign a weight value
2 to strongly subjective clues and 1 to weakly
subjective clues. We experimented with several
thresholds T and report here the results for the best
thresholds, which were 2 for SL and 4 for the GI
word list. The corresponding methods are called
Rule-SL and Rule-GI.
This approach does not allow us to easily inte-
grate relational WordNet features. It might also
suffer from the incompleteness of the lexica and
the fact that it has to make decisions for bor-
derline cases (at the value of the threshold set).
We therefore explored instead to generate larger,
more reliable training data consisting of Word-
Net synsets from the word lists. To achieve this,
we assign a subjectivity score S as above to all
WordNet synsets (excluding synsets in the test set).
If S is higher or equal to a threshold T
1
it is added
to the subjective training set, if it is lower or equal
to T
2
it is added to the objective training set. This
allows us to choose quite clear thresholds so that
borderline cases with a score between T
1
and T
2
are not in the training set. It also allows to use part-
829
of-speech and relational features as the training set
then consists of WordNet synsets. In this way,
we can automatically generate (potentially noisy)
training data of WordNet senses marked for sub-
jectivity without annotating any WordNet senses
manually for subjectivity.
We experimented with several different thresh-
old sets but we found that they actually have a min-
imal impact on the final results. We report here the
best results for a threshold T
1
of 4 and T
2
of 2 for
the SL lexicon and of 3 and 1 respectively for the
GI word list.
5 Experiments and Evaluation
We measure the classification performance with
overall accuracy as well as precision, recall and
balanced F-score for both categories (objective and
subjective). All results are summarised in Table 1.
Results are compared to the baseline of majority
classification using a McNemar test at the signifi-
cance level of 5%.
5.1 Experimental Results
Table 1 shows that SL
?
performs best among all
the methodologies. All CV, Rule-SL and SL meth-
ods significantly beat the baseline. In addition, if
we compare the results of methods with and with-
out additional parts-of-speech and WordNet rela-
tion features, we see a small but consistent im-
provement when we use additional features. It is
also worthwhile to expand the rule-based unsuper-
vised method into a method for generating train-
ing data and use additional features as SL
?
signifi-
cantly outperforms Rule-SL.
5.2 Discussion
Word Lists. Surprisingly, using SL greatly outper-
forms GI, regardless of whether we use the super-
vised or unsupervised method or whether we use
lexical features only or the other features as well.
9
There are several reasons for this. First, the GI
lexicon is annotated for polarity, not subjectivity.
More specifically, words that we see as objective
but with a strong positive or negative association
(such as words for crimes) and words that we see
as subjective are annotated with the same polar-
ity label in the GI lexicon. Therefore, the GI def-
inition of subjectivity does not match ours. Also,
9
This pattern is repeated for all threshold combinations,
which are not reported here.
the GI lexicon does not operate with a clearly ex-
pressed polarity definition, leading to conflicting
annotations and casting doubt on its widespread
use in the opinion mining community as a gold
standard (Turney and Littman, 2003; Takamura et
al., 2005; Andreevskaia and Bergler, 2006). For
example, amelioration is seen as non-polar in GI
but improvement is annotated with positive polar-
ity. Second, in contrast to SL, GI does not consider
different parts-of-speech of a word and subjectiv-
ity strength (strong/weak subjectivity). Third, GI
contains many fewer subjective clues than SL.
Sentence Data. When using the Movie dataset
and MPQA corpus as training data, the results
are not satisfactory. We first checked the purity
of these two datasets to see whether they are too
noisy. For this purpose, we used a naive Bayes
algorithm with unigram features and conducted a
10-fold cross validation experiment on recognizing
subjective/objective sentences within the Movie
dataset and MPQA independently. Interestingly,
the accuracy for the Movie dataset and MPQA cor-
pus achieved 91% and 76% respectively. Consid-
ering that they are balanced datasets with a most
frequent category baseline of about 50%, this ac-
curacy is high, especially for the Movie dataset.
However, again the subjectivity definition in the
Movie corpus does not seem to match ours. Re-
call that we see a word sense or a sentence as sub-
jective if it expresses a private state (i.e., emotion,
opinion, sentiment, etc.), and objective otherwise.
Inspecting the movie data set, we found that in-
deed the sentences included in its subjective set
would mostly be seen as subjective in our sense
as well as they contain opinions about the movie
such as it desperately wants to be a wacky , screw-
ball comedy , but the most screwy thing here is how
so many talented people were convinced to waste
their time. It is also true that the sentences (plot
descriptions) in its ?objective? data set relatively
rarely contain opinions about the movie. How-
ever, they still contain other opinionated content
like opinions and emotions of the characters in the
movie such as the obsession of a character with
John Lennon in the beatles fan is a drama about
Albert, a psychotic prisoner who is a devoted fan
of John Lennon and the beatles. Since the data
set?s definition of subjective sentences is closer
to ours than the one for objective sentences, we
conducted a one-class learning approach (Li and
Liu, 2003) using Movie?s subjective sentences as
830
Table 1: Results
Method Subjective Objective Accuracy
Precision Recall F-score Precision Recall F-score
Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3%
CV 65.2% 52.8% 58.3% 78.1% 85.6% 81.7% 74.6%?
CV
?
69.5% 55.3% 61.6% 79.4% 87.6% 83.3% 76.7%?
Movie 43.8% 60.1% 50.6% 74.9% 60.7% 67.1% 60.5%
MPQA 44.5% 78.5% 56.8% 82.1% 50.1% 62.2% 59.7%
GI 50.4% 39.4% 44.2% 72.2% 80.2% 76.0% 66.4%
GI
?
54.5% 33.5% 41.5% 71.7% 85.8% 78.1% 68.1%
SL 64.3% 62.8% 63.6% 81.3% 82.2% 81.8% 75.7%?
SL
?
66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9%?
Rule-GI 38.5% 5.6% 9.8% 66.5% 95.4% 78.4% 65.1%
Rule-SL 59.7% 70.4% 64.6% 83.4% 75.8% 79.4% 74.0%?
1
CV, GI and SL correspond to methods using lexical features only.
2
CV
?
, GI
?
and SL
?
correspond to methods using a feature combination of lexical,
part-of-speech, and WordNet relations.
3
? indicates results significantly better than the baseline.
the only training data. The algorithm
10
combines
Expectation Maximization and Naive Bayes algo-
rithms, and we used randomly extracted 50,000
unlabeled synsets in WordNet as the necessary un-
labeled data. This approach achieves an accuracy
of 69.4% on Micro-WNOp, which is significantly
better than the baseline.
The subjectivity definition in the MPQA corpus
is quite close to ours. However, our mapping from
its phrase annotation to sentence annotation might
be too coarse-grained as many sentences in the cor-
pus span several clauses containing both opinions
and factual description. We assume that this is pos-
sibly also the reason why its purity is lower than
in the Movie dataset. We therefore experimented
again with a one-class learning approach using just
the subjective phrases in MPQA as training data.
The accuracy does improve to 67.6% but is still
not significantly higher than the baseline.
5.3 Comparison to Prior Approaches
Esuli and Sebastiani (2006) make their labeled
WordNet SentiWordNet 1.0 publically available.
11
Recall that they actually use polarity classification:
however, as there is a dependency between po-
larity and subjectivity classification for subjective
senses, we map their polarity scores to our subjec-
tivity labels as follows. If the sum of positive and
10
Available at http://www.cs.uic.edu/?liub/LPU/.
11
Available at http://sentiwordnet.isti.cnr.
it/
negative scores of a sense in SentiWordNet is more
than or equal to 0.5, then it is subjective and other-
wise objective.
12
Using this mapping, it achieves
an accuracy of 75.3% on the Micro-WNOp cor-
pus, compared to our gold standard. Therefore our
methods CV
?
and SL
?
perform slightly better than
theirs, although the improvement is not significant.
The task definition in Wiebe and Mihal-
cea (2006) is much more similar to ours but they
use different annotated test data, which is not pub-
lically available, so an exact comparison is not pos-
sible. Both data sets, however, seem to include rel-
atively frequent words. One disadvantage of their
method is that it is not applicable to all WordNet
senses as it is dependent on distributionally sim-
ilar words being available in the MPQA. Thus,
23% of their test data is excluded from evaluation,
whereas our methods can be used on any WordNet
sense. They measure precision and recall for sub-
jective senses in a precision/recall curve: Precision
is about 48/9% at a recall of 60% for subjective
senses whereas our best SL
?
method has a preci-
sion of 66% at about the same recall. Although
this suggests better performance of our method, it
is not possible to draw final conclusions from this
comparison due to the data set differences.
12
We experimented with slightly different mappings but
this mapping gave SentiWordNet the best possible result.
There is a relatively large number of cases with a 0.5/0.5 split
in SentiWordNet, making it hard to decide between subjective
and objective senses.
831
6 Conclusion and Future Work
We proposed different ways of extracting training
data and clue sets for word sense subjectivity label-
ing from existing opinion mining resources. The
effectiveness of the resulting algorithms depends
greatly on the generated training data, more specif-
ically on the different definitions of subjectivity
used in resource creation. However, we were able
to show that at least one of these methods (based
on the SL word list) resulted in a classifier that per-
formed on a par with a supervised classifier that
used dedicated training data developed for this task
(CV). Thus, it is possible to avoid any manual an-
notation for the subjectivity classification of word
senses.
Our future work will explore new methodolo-
gies in feature representation by importing more
background information (e.g., syntactic informa-
tion). Furthermore, our current method of integrat-
ing the rich relation information in WordNet (us-
ing them as standard features) does not use joint
classification of several senses. Instead, we think
it will be more promising to use the relations to
construct graphs for semi-supervised graph-based
learning of word sense subjectivity. In addition, we
will also explore whether the derived sense labels
improve applications such as sentence classifica-
tion and clustering WordNet senses.
References
Andreevskaia, Alina and Sabine Bergler. 2006. Min-
ing WordNet for Fuzzy Sentiment: Sentiment Tag
Extraction from WordNet Glosses. Proceedings of
EACL?06.
Dave, Kushal, Steve Lawrence, and David Pennock.
2003. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
Proceedings of WWW?03.
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource for
Opinion Mining. Proceedings of LREC?06.
Esuli, Andrea and Fabrizio Sebastiani. 2007. PageR-
anking WordNet Synsets: An application to Opinion
Mining. Proceedings of ACL?07.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. Proceedings of ACL?97.
Kim, Soo-Min and Eduard Hovy. 2004. Determining
the Sentiment of Opinions. Proceedings of COL-
ING?04.
Kim, Soo-Min and Eduard Hovy. 2005. Automatic
Detection of Opinion Bearing Words and Sentences.
Proceedings of ICJNLP?05.
Kudo, Taku and Yuji Matsumoto. 2004. A Boosting
Algorithm for Classification of Semi-structured Text.
Proceedings of EMNLP?04.
Li, Xiaoli and Bing Liu. 2003. Learning to classify
text using positive and unlabeled data. Proceedings
of IJCAI?03.
Pang, Bo and Lillian Lee. 2004. A Sentiment Edu-
cation: Sentiment Analysis Using Subjectivity sum-
marization Based on Minimum Cuts. Proceedings of
ACL?04.
Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification us-
ing Machine Learning Techniques. Proceedings of
EMNLP?02.
Popescu, Ana-Maria and Oren Etzioni. 2003. Ex-
tracting Product Fatures and Opinions from Reviews
Proceedings of EMNLP?05.
Riloff, Ellen, JanyceWiebe, and TheresaWilson. 2003.
Learning Subjective Nouns using Extraction Pattern
Bootstrapping. Proceedings of CoNLL?03
Riloff, Ellen and Janyce Wiebe. 2003. Learning Ex-
traction Patterns for Subjective Expressions. Pro-
ceedings of EMNLP?03.
Su, Fangzhong and Katja Markert. 2008. Elicit-
ing Subjectivity and Polarity Judgements on Word
Senses. Proceedings of Coling?08 workshop of Hu-
man Judgements in Computational Linguistics.
Takamura, Hiroya, Takashi Inui, and Manabu Oku-
mura. 2005. Extracting Semantic Orientations of
Words using Spin Model. Proceedings of ACL?05.
Turney, Peter and Michael Littman. 2003. Measuring
Praise and Criticism: Inference of Semantic Orien-
tation from Association. ACM Transaction on Infor-
mation Systems.
Wiebe, Janyce and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resources and
Evaluation.
832
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Subjectivity Recognition on Word Senses via Semi-supervised Mincuts
Fangzhong Su
School of Computing
University of Leeds
fzsu@comp.leeds.ac.uk
Katja Markert
School of Computing
University of Leeds
markert@comp.leeds.ac.uk
Abstract
We supplement WordNet entries with infor-
mation on the subjectivity of its word senses.
Supervised classifiers that operate on word
sense definitions in the same way that text
classifiers operate on web or newspaper texts
need large amounts of training data. The re-
sulting data sparseness problem is aggravated
by the fact that dictionary definitions are very
short. We propose a semi-supervised mini-
mum cut framework that makes use of both
WordNet definitions and its relation structure.
The experimental results show that it outper-
forms supervised minimum cut as well as stan-
dard supervised, non-graph classification, re-
ducing the error rate by 40%. In addition, the
semi-supervised approach achieves the same
results as the supervised framework with less
than 20% of the training data.
1 Introduction
There is considerable academic and commercial in-
terest in processing subjective content in text, where
subjective content refers to any expression of a pri-
vate state such as an opinion or belief (Wiebe et
al., 2005). Important strands of work include the
identification of subjective content and the determi-
nation of its polarity, i.e. whether a favourable or
unfavourable opinion is expressed.
Automatic identification of subjective content of-
ten relies on word indicators, such as unigrams
(Pang et al, 2002) or predetermined sentiment lex-
ica (Wilson et al, 2005). Thus, the word positive
in the sentence ?This deal is a positive development
for our company.? gives a strong indication that
the sentence contains a favourable opinion. How-
ever, such word-based indicators can be misleading
for two reasons. First, contextual indicators such as
irony and negation can reverse subjectivity or po-
larity indications (Polanyi and Zaenen, 2004). Sec-
ond, different word senses of a single word can ac-
tually be of different subjectivity or polarity. A typ-
ical subjectivity-ambiguous word, i.e. a word that
has at least one subjective and at least one objec-
tive sense, is positive, as shown by the two example
senses given below.1
(1) positive, electropositive?having a positive electric
charge;?protons are positive? (objective)
(2) plus, positive?involving advantage or good; ?a
plus (or positive) factor? (subjective)
We concentrate on this latter problem by automat-
ically creating lists of subjective senses, instead
of subjective words, via adding subjectivity labels
for senses to electronic lexica, using the exam-
ple of WordNet. This is important as the prob-
lem of subjectivity-ambiguity is frequent: We (Su
and Markert, 2008) find that over 30% of words
in our dataset are subjectivity-ambiguous. Informa-
tion on subjectivity of senses can also improve other
tasks such as word sense disambiguation (Wiebe
and Mihalcea, 2006). Moreover, Andreevskaia and
Bergler (2006) show that the performance of auto-
matic annotation of subjectivity at the word level can
be hurt by the presence of subjectivity-ambiguous
words in the training sets they use.
1All examples in this paper are from WordNet 2.0.
1
We propose a semi-supervised approach based on
minimum cut in a lexical relation graph to assign
subjectivity (subjective/objective) labels to word
senses.2 Our algorithm outperforms supervised min-
imum cuts and standard supervised, non-graph clas-
sification algorithms (like SVM), reducing the error
rate by up to 40%. In addition, the semi-supervised
approach achieves the same results as the supervised
framework with less than 20% of the training data.
Our approach also outperforms prior approaches to
the subjectivity recognition of word senses and per-
forms well across two different data sets.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Section 3
describes our proposed semi-supervised minimum
cut framework in detail. Section 4 presents the ex-
perimental results and evaluation, followed by con-
clusions and future work in Section 5.
2 Related Work
There has been a large and diverse body of research
in opinion mining, with most research at the text
(Pang et al, 2002; Pang and Lee, 2004; Popescu and
Etzioni, 2005; Ounis et al, 2006), sentence (Kim
and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff
et al, 2003; Yu and Hatzivassiloglou, 2003) or word
(Hatzivassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kim and Hovy, 2004; Takamura et
al., 2005; Andreevskaia and Bergler, 2006; Kaji and
Kitsuregawa, 2007) level. An up-to-date overview is
given in Pang and Lee (2008).
Graph-based algorithms for classification into
subjective/objective or positive/negative language
units have been mostly used at the sentence and
document level (Pang and Lee, 2004; Agarwal and
Bhattacharyya, 2005; Thomas et al, 2006), instead
of aiming at dictionary annotation as we do. We
also cannot use prior graph construction methods
for the document level (such as physical proxim-
ity of sentences, used in Pang and Lee (2004)) at
the word sense level. At the word level Taka-
mura et al (2005) use a semi-supervised spin model
for word polarity determination, where the graph
2It can be argued that subjectivity labels are maybe rather
more graded than the clear-cut binary distinction we assign.
However, in Su and Markert (2008a) as well as Wiebe and Mi-
halcea (2006) we find that human can assign the binary distinc-
tion to word senses with a high level of reliability.
is constructed using a variety of information such
as gloss co-occurrences and WordNet links. Apart
from using a different graph-based model from ours,
they assume that subjectivity recognition has already
been achieved prior to polarity recognition and test
against word lists containing subjective words only.
However, Kim and Hovy (2004) and Andreevskaia
and Bergler (2006) show that subjectivity recogni-
tion might be the harder problem with lower human
agreement and automatic performance. In addition,
we deal with classification at the word sense level,
treating also subjectivity-ambiguous words, which
goes beyond the work in Takamura et al (2005).
Word Sense Level: There are three prior ap-
proaches addressing word sense subjectivity or po-
larity classification. Esuli and Sebastiani (2006) de-
termine the polarity (positive/negative/objective) of
word senses in WordNet. However, there is no eval-
uation as to the accuracy of their approach. They
then extend their work (Esuli and Sebastiani, 2007)
by applying the Page Rank algorithm to rank the
WordNet senses in terms of how strongly a sense
possesses a given semantic property (e.g., positive
or negative). Apart from us tackling subjectivity
instead of polarity, their Page Rank graph is also
constructed focusing on WordNet glosses (linking
glosses containing the same words), whereas we
concentrate on the use of WordNet relations.
Both Wiebe and Mihalcea (2006) and our prior
work (Su and Markert, 2008) present an annota-
tion scheme for word sense subjectivity and algo-
rithms for automatic classification. Wiebe and Mi-
halcea (2006) use an algorithm relying on distribu-
tional similarity and an independent, large manually
annotated opinion corpus (MPQA) (Wiebe et al,
2005). One of the disadvantages of their algorithm is
that it is restricted to senses that have distributionally
similar words in the MPQA corpus, excluding 23%
of their test data from automatic classification. Su
and Markert (2008) present supervised classifiers,
which rely mostly on WordNet glosses and do not
effectively exploit WordNet?s relation structure.
3 Semi-Supervised Mincuts
3.1 Minimum Cuts: The Main Idea
Binary classification with minimum cuts (Mincuts)
in graphs is based on the idea that similar items
2
should be grouped in the same cut. All items in the
training/test data are seen as vertices in a graph with
undirected weighted edges between them specifying
how strong the similarity/association between two
vertices is. We use minimum s-t cuts: the graph con-
tains two particular vertices s (source, corresponds
to subjective) and t (sink, corresponds to objective)
and each vertex u is connected to s and t via a
weighted edge that can express how likely u is to
be classified as s or t in isolation.
Binary classification of the vertices is equivalent
to splitting the graph into two disconnected subsets
of all vertices, S and T with s ? S and t ? T .
This corresponds to removing a set of edges from
the graph. As similar items should be in the same
part of the split, the best split is one which removes
edges with low weights. In other words, a minimum
cut problem is to find a partition of the graph which
minimizes the following formula, where w(u, v) ex-
presses the weight of an edge between two vertices.
W (S, T ) = ?
u?S,v?T
w(u, v)
Globally optimal minimum cuts can be found in
polynomial time and near-linear running time in
practice, using the maximum flow algorithm (Pang
and Lee, 2004; Cormen et al, 2002).
3.2 Why might Semi-supervised Minimum
Cuts Work?
We propose semi-supervised mincuts for subjectiv-
ity recognition on senses for several reasons.
First, our problem satisfies two major conditions
necessary for using minimum cuts. It is a bi-
nary classification problem (subjective vs. objective
senses) as is needed to divide the graph into two
components. Our dataset alo lends itself naturally
to s-t Mincuts as we have two different views on the
data. Thus, the edges of a vertex (=sense) to the
source/sink can be seen as the probability of a sense
being subjective or objective without taking similar-
ity to other senses into account, for example via con-
sidering only the sense gloss. In contrast, the edges
between two senses can incorporate the WordNet re-
lation hierarchy, which is a good source of similar-
ity for our problem as many WordNet relations are
subjectivity-preserving, i.e. if two senses are con-
nected via such a relation they are likely to be both
subjective or both objective.3 An example here is
the antonym relation, where two antonyms such as
good?morally admirable and evil, wicked?morally
bad or wrong are both subjective.
Second, Mincuts can be easily expanded into
a semi-supervised framework (Blum and Chawla,
2001). This is essential as the existing labeled
datasets for our problem are small. In addition,
glosses are short, leading to sparse high dimensional
vectors in standard feature representations. Also,
WordNet connections between different parts of the
WordNet hierarchy can also be sparse, leading to
relatively isolated senses in a graph in a supervised
framework. Semi-supervised Mincuts allow us to
import unlabeled data that can serve as bridges to
isolated components. More importantly, as the unla-
beled data can be chosen to be related to the labeled
and test data, they might help pull test data to the
right cuts (categories).
3.3 Formulation of Semi-supervised Mincuts
The formulation of our semi-supervised Mincut for
sense subjectivity classification involves the follow-
ing steps, which we later describe in more detail.
1. We define two vertices s (source) and t (sink),
which correspond to the ?subjective? and ?ob-
jective? category, respectively. Following the
definition in Blum and Chawla (2001), we call
the vertices s and t classification vertices, and
all other vertices (labeled, test, and unlabeled
data) example vertices. Each example vertex
corresponds to one WordNet sense and is con-
nected to both s and t via a weighted edge. The
latter guarantees that the graph is connected.
2. For the test and unlabeled examples, we see
the edges to the classification vertices as the
probability of them being subjective/objective
disregarding other example vertices. We use a
supervised classifier to set these edge weights.
For the labeled training examples, they are con-
nected by edges with a high constant weight to
the classification vertices that they belong to.
3. WordNet relations are used to construct the
edges between two example vertices. Such
3See Kamps et al (2004) for an early indication of such
properties for some WordNet relations.
3
edges can exist between any pair of example
vertices, for example between two unlabeled
examples.
4. After graph construction we then employ a
maximum-flow algorithm to find the minimum
s-t cuts of the graph. The cut in which the
source vertex s lies is classified as ?subjective?,
and the cut in which the sink vertex t lies is ?ob-
jective?.
We now describe the above steps in more detail.
Selection of unlabeled data: Random selection
of unlabeled data might hurt the performance of
Mincuts, as they might not be related to any sense in
our training/test data (denoted by A). Thus a basic
principle is that the selected unlabeled senses should
be related to the training/test data by WordNet rela-
tions. We therefore simply scan each sense inA, and
collect all senses related to it via one of the WordNet
relations in Table 1. All such senses that are not in
A are collected in the unlabeled data set.
Weighting of edges to the classification ver-
tices: The edge weight to s and t represents how
likely it is that an example vertex is initially put in
the cut in which s (subjective) or t (objective) lies.
For unlabeled and test vertices, we use a supervised
classifier (SVM4) with the labeled data as training
data to assign the edge weights. The SVM is also
used as a baseline and its features are described in
Section 4.3. As we do not wish the Mincut to re-
verse labels of the labeled training data, we assign a
high constant weight of 5 to the edge between a la-
beled vertex and its corresponding classification ver-
tex, and a low weight of 0.01 to the edge to the other
classification vertex.
Assigning weights to WordNet relations: We
connect two vertices that are linked by one of
the ten WordNet relations in Table 1 via an edge.
Not all WordNet relations we use are subjectivity-
preserving to the same degree: for example, hy-
ponyms (such as simpleton) of objective senses
(such as person) do not have to be objective. How-
ever, we aim for high graph connectivity and we
can assign different weights to different relations
4We employ LIBSVM, available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm/. Linear kernel and prob-
ability estimates are used in this work.
to reflect the degree to which they are subjectivity-
preserving. Therefore, we experiment with two
methods of weight assignment. Method 1 (NoSL)
assigns the same constant weight of 1.0 to all Word-
Net relations.
Method 2 (SL) reflects different degrees of pre-
serving subjectivity. To do this, we adapt an un-
supervised method of generating a large noisy set
of subjective and objective senses from our previ-
ous work (Su and Markert, 2008). This method
uses a list of subjective words (SL)5 to classify each
WordNet sense with at least two subjective words
in its gloss as subjective and all other senses as ob-
jective. We then count how often two senses re-
lated via a given relation have the same or a dif-
ferent subjectivity label. The weight is computed
by #same/(#same+#different). Results are listed in
Table 1.
Table 1: Relation weights (Method 2)
Method #Same #Different Weight
Antonym 2,808 309 0.90
Similar-to 6,887 1,614 0.81
Derived-from 4,630 947 0.83
Direct-Hypernym 71,915 8,600 0.89
Direct-Hyponym 71,915 8,600 0.89
Attribute 350 109 0.76
Also-see 1,037 337 0.75
Extended-Antonym 6,917 1,651 0.81
Domain 4,387 892 0.83
Domain-member 4,387 892 0.83
Example graph: An example graph is shown in
Figure 1. The three example vertices correspond
to the senses religious?extremely scrupulous and
conscientious, scrupulous?having scruples; aris-
ing from a sense of right and wrong; principled;
and flicker, spark, glint?a momentary flash of light
respectively. The vertex ?scrupulous? is unlabeled
data derived from the vertex ?religious?(a test item)
by the relation ?similar-to?.
4 Experiments and Evaluation
4.1 Datasets
We conduct the experiments on two different gold
standard datasets. One is the Micro-WNOp corpus,
5Available at http://www.cs.pitt.edu/mpqa
4
scrupulous
religious
subjective objective
flicker
0.24 0.76
0.83 0.17
0.16 0.84
0.81similar-to
Figure 1: Graph of Word Senses
which is representative of the part-of-speech distri-
bution in WordNet 6. It includes 298 words with
703 objective and 358 subjective WordNet senses.
The second one is the dataset created by Wiebe
and Mihalcea (2006).7 It only contains noun and
verb senses, and includes 60 words with 236 ob-
jective and 92 subjective WordNet senses. As the
Micro-WNOp set is larger and also contains adjec-
tive and adverb senses, we describe our results in
more detail on that corpus in the Section 4.3 and
4.4. In Section 4.5, we shortly discuss results on
Wiebe&Mihalcea?s dataset.
4.2 Baseline and Evaluation
We compare to a baseline that assigns the most
frequent category objective to all senses, which
achieves an accuracy of 66.3% and 72.0% onMicro-
WNOp and Wiebe&Mihalcea?s dataset respectively.
We use the McNemar test at the significance level of
5% for significance statements. All evaluations are
carried out by 10-fold cross-validation.
4.3 Standard Supervised Learning
We use an SVM classifier to compare our proposed
semi-supervised Mincut approach to a reasonable
6Available at http://www.comp.leeds.ac.uk/
markert/data. This dataset was first used with a different
annotation scheme in Esuli and Sebastiani (2007) and we also
used it in Su and Markert (2008).
7Available at http://www.cs.pitt.edu/?wiebe/
pubs/papers/goldstandard.total.acl06.
baseline.8 Three different feature types are used.
Lexical Features (L): a bag-of-words representa-
tion of the sense glosses with stop word filtering.
Relation Features (R): First, we use two features
for each of the ten WordNet relations in Table 1, de-
scribing how many relations of that type the sense
has to senses in the subjective or objective part of the
training set, respectively. This provides a non-graph
summary of subjectivity-preserving links. Second,
we manually collected a small set (denoted by
SubjSet) of seven subjective verb and noun senses
which are close to the root in WordNet?s hypernym
tree. A typical example element of SubjSet is psy-
chological feature ?a feature of the mental life of a
living organism, which indicates subjectivity for its
hyponyms such as hope ? the general feeling that
some desire will be fulfilled. A binary feature de-
scribes whether a noun/verb sense is a hyponym of
an element of SubjSet.
Monosemous Feature (M): for each sense, we
scan if a monosemous word is part of its synset. If
so, we further check if the monosemous word is col-
lected in the subjective word list (SL). The intuition
is that if a monosemous word is subjective, obvi-
ously its (single) sense is subjective. For example,
the sense uncompromising, inflexible?not making
concessions is subjective, as ?uncompromising? is
a monosemous word and also in SL.
We experiment with different combinations of
features and the results are listed in Table 2, prefixed
by ?SVM?. All combinations perform significantly
better than the more frequent category baseline and
similarly to the supervised Naive Bayes classifier
(see S&M in Table 2) we used in Su and Mark-
ert (2008). However, improvements by adding more
features remain small.
In addition, we compare to a supervised classifier
(see Lesk in Table 2) that just assigns each sense
the subjectivity label of its most similar sense in
the training data, using Lesk?s similarity measure
from Pedersen?s WordNet similarity package9. We
use Lesk as it is one of the few measures applicable
across all parts-of-speech.
8This SVM is also used to provide the edge weights to the
classification vertices in the Mincut approach.
9Available at http://www.d.umn.edu/?tpederse/
similarity.html.
5
Table 2: Results of SVM and Mincuts with different settings of feature
Method Subjective Objective Accuracy
Precision Recall F-score Precision Recall F-score
Baseline N/A 0 N/A 66.3% 100% 79.7% 66.3%
S&M 66.2% 64.5% 65.3% 82.2% 83.2% 82.7% 76.9%
Lesk 65.6% 50.3% 56.9% 77.5% 86.6% 81.8% 74.4%
SVM-L 69.6% 37.7% 48.9% 74.3% 91.6% 82.0% 73.4%
L-SL 82.0% 43.3% 56.7% 76.7% 95.2% 85.0% 77.7%
L-NoSL 80.8% 43.6% 56.6% 76.7% 94.7% 84.8% 77.5%
SVM-LM 68.9% 42.2% 52.3% 75.4% 90.3% 82.2% 74.1%
LM-SL 83.2% 44.4% 57.9% 77.1% 95.4% 85.3% 78.2%
LM-NoSL 83.6% 44.1% 57.8% 77.1% 95.6% 85.3% 78.2%
SVM-LR 68.4% 45.3% 54.5% 76.2% 89.3% 82.3% 74.5%
LR-SL 82.7% 65.4% 73.0% 84.1% 93.0% 88.3% 83.7%
LR-NoSL 82.4% 65.4% 72.9% 84.0% 92.9% 88.2% 83.6%
SVM-LRM 69.8% 47.2% 56.3% 76.9% 89.6% 82.8% 75.3%
LRM-SL 85.5% 65.6% 74.2% 84.4% 94.3% 89.1% 84.6%
LRM-NoSL 84.6% 65.9% 74.1% 84.4% 93.9% 88.9% 84.4%
1 L, R and M correspond to the lexical, relation and monosemous features respectively.
2 SVM-L corresponds to using lexical features only for the SVM classifier. Likewise, SVM-
LRM corresponds to using a combination for lexical, relation, and monosemous features
for the SVM classifier.
3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier,
and subjective list (SL) to infer the weight of WordNet relations. Likewise, LM-NoSL
corresponds to the Mincut algorithm that uses lexical and monosemous features for the
SVM, and predefined constants for WordNet relations (without subjective list).
4.4 Semi-supervised Graph Mincuts
Using our formulation in Section 3.3, we import
3,220 senses linked by the ten WordNet relations to
any senses in Micro-WNOp as unlabeled data. We
construct edge weights to classification vertices us-
ing the SVM discussed above and use WordNet re-
lations for links between example vertices, weighted
by either constants (NoSL) or via the method illus-
trated in Table 1 (SL). The results are also summa-
rized in Table 2. Semi-supervised Mincuts always
significantly outperform the corresponding SVM
classifiers, regardless of whether the subjectivity list
is used for setting edge weights. We can also see
that we achieve good results without using any other
knowledge sources (setting LR-NoSL).
The example in Figure 1 explains why semi-
supervised Mincuts outperforms the supervised ap-
proach. The vertex ?religious? is initially assigned
the subjective/objective probabilities 0.24/0.76 by
the SVM classifier, leading to a wrong classification.
However, in our graph-based Mincut framework, the
vertex ?religious? might link to other vertices (for
example, it links to the vertex ?scrupulous? in the
unlabeled data by the relation ?similar-to?). The
mincut algorithm will put vertices ?religious? and
?scrupulous? in the same cut (subjective category) as
this results in the least cost 0.93 (ignoring the cost of
assigning the unrelated sense of ?flicker?). In other
words, the edges between the vertices are likely to
correct some initially wrong classification and pull
the vertices into the right cuts.
In the following we will analyze the best mini-
mum cut algorithm LRM-SL in more detail. We
measure its accuracy for each part-of-speech in the
Micro-WNOp dataset. The number of noun, adjec-
tive, adverb and verb senses in Micro-WNOp is 484,
265, 31 and 281, respectively. The result is listed
in Table 3. The significantly better performance of
semi-supervised mincuts holds across all parts-of-
speech but the small set of adverbs, where there is
no significant difference between the baseline, SVM
and the Mincut algorithm.
6
Table 3: Accuracy for Different Part-Of-Speech
Method Noun Adjective Adverb Verb
Baseline 76.9% 61.1% 77.4% 72.6%
SVM 81.4% 63.4% 83.9% 75.1%
Mincut 88.6% 78.9% 77.4% 84.0%
We will now investigate how LRM-SL performs
with different sizes of labeled and unlabeled data.
All learning curves are generated via averaging 10
learning curves from 10-fold cross-validation.
Performance with different sizes of labeled data:
we randomly generate subsets of labeled data A1,
A2... An, and guarantee that A1 ? A2... ? An.
Results for the best SVM (LRM) and the best min-
imum cut (LRM-SL) are listed in Table 4, and the
corresponding learning curve is shown in Figure 2.
As can be seen, the semi-supervised Mincuts is
consistently better than SVM. Moreover, the semi-
supervised Mincut with only 200 labeled data items
performs even better than SVM with 954 training
items (78.9% vs 75.3%), showing that our semi-
supervised framework allows for a training data re-
duction of more than 80%.
Table 4: Accuracy with different sizes of labeled data
# labeled data SVM Mincuts
100 69.1% 72.2%
200 72.6% 78.9%
400 74.4% 82.7%
600 75.5% 83.7%
800 76.0% 84.1%
900 75.6% 84.8%
954 (all) 75.3% 84.6%
Performance with different sizes of unlabeled
data: We propose two different settings.
Option1: Use a subset of the ten relations to
generate the unlabeled data (and edges between
example vertices). For example, we first use
{antonym, similar-to} only to obtain a unlabeled
dataset U1, then use a larger subset of the relations
like {antonym, similar-to, direct-hyponym, direct-
hypernym} to generate another unlabeled dataset
U2, and so forth. Obviously, Ui is a subset of Ui+1.
Option2: Use all the ten relations to generate the
unlabeled data U . We then randomly select subsets
of U , such as subset U1, U2 and U3, and guarantee
that U1 ? U2 ? U3 ? . . . U .
 68
 71
 74
 77
 80
 83
 86
 89
 100  200  300  400  500  600  700  800  900 1000
A
c
c
u
r
a
c
y
(
%
)
Size of Labeled Data
Mincuts
SVM
Figure 2: Learning curve with different sizes of labeled
data
The results are listed in Table 5 and Table 6 re-
spectively. The corresponding learning curves are
shown in Figure 3. We see that performance im-
proves with the increase of unlabeled data. In addi-
tion, the curves seem to converge when the size of
unlabeled data is larger than 3,000. From the results
in Tabel 5 one can also see that hyponymy is the re-
lation accounting for the largest increase.
Table 6: Accuracy with different sizes of unlabeled data
(random selection)
# unlabeled data Accuracy
0 75.9%
200 76.5%
500 78.6%
1000 80.2%
2000 82.8%
3000 84.0%
3220 84.6%
Furthermore, these results also show that a super-
vised mincut without unlabeled data performs only
on a par with other supervised classifiers (75.9%).
The reason is that if we exclude the unlabeled data,
there are only 67 WordNet relations/edges between
senses in the small Micro-WNOp dataset. In con-
trast, the use of unlabeled data adds more edges
(4,586) to the graph, which strongly affects the
graph cut partition (see also Figure 1).
4.5 Comparison to Prior Approaches
In our previous work (Su and Markert, 2008), we re-
port 76.9% as the best accuracy on the same Micro-
7
Table 5: Accuracy with different sizes of unlabeled data from WordNet relation
Relation # unlabeled data Accuracy
{?} 0 75.3%
{similar-to} 418 79.1%
{similar-to, antonym} 514 79.5%
{similar-to, antonym, direct-hypernym, direct-
hyponym}
2,721 84.4%
{similar-to, antonym, direct-hypernym, direct-
hyponym, also-see, extended-antonym}
3,004 84.4%
{similar-to, antonym, direct-hypernym, direct-
hyponym, also-see, extended-antonym, derived-from,
attribute, domain, domain-member}
3,220 84.6%
 75
 77
 79
 81
 83
 85
 87
 89
 0  500  1000  1500  2000  2500  3000  3500
A
c
c
u
r
a
c
y
(
%
)
Size of Unlabeled Data
Option1
Option2
Figure 3: Learning curve with different sizes of unlabeled
data
WNOp dataset used in the previous sections, using a
supervised Naive Bayes (S&M in Tabel 2). Our best
result from Mincuts is significantly better at 84.6%
(see LRM-SL in Table 2).
For comparison to Wiebe and Mihalcea (2006),
we use their dataset for testing, henceforth called
Wiebe (see Section 4.1 for a description). Wiebe
and Mihalcea (2006) report their results in precision
and recall curves for subjective senses, such as a pre-
cision of about 55% at a recall of 50% for subjective
senses. Their F-score for subjective senses seems to
remain relatively static at 0.52 throughout their pre-
cision/recall curve.
We run our best Mincut LRM-SL algorithm with
two different settings on Wiebe. Using Micro-
WNOp as training set and Wiebe as test set, we
achieve an accuracy of 83.2%, which is similar to the
results on the Micro-WNOp dataset. At the recall of
50% we achieve a precision of 83.6% (in compari-
son to their precision of 55% at the same recall). Our
F-score is 0.63 (vs. 0.52).
To check whether the high performance is just due
to our larger training set, we also conduct 10-fold
cross-validation on Wiebe. The accuracy achieved
is 81.1% and the F-score 0.56 (vs. 0.52), suggesting
that our algorithm performs better. Our algorithm
can be used on all WordNet senses whereas theirs is
restricted to senses that have distributionally similar
words in the MPQA corpus (see Section 2). How-
ever, they use an unsupervised algorithm i.e. they
do not need labeled word senses, although they do
need a large, manually annotated opinion corpus.
5 Conclusion and Future Work
We propose a semi-supervised minimum cut algo-
rithm for subjectivity recognition on word senses.
The experimental results show that our proposed ap-
proach is significantly better than a standard super-
vised classification framework as well as a super-
vised Mincut. Overall, we achieve a 40% reduction
in error rates (from an error rate of about 25% to an
error rate of 15%). To achieve the results of standard
supervised approaches with our model, we need less
than 20% of their training data. In addition, we com-
pare our algorithm to previous state-of-the-art ap-
proaches, showing that our model performs better
on the same datasets.
Future work will explore other graph construc-
tion methods, such as the use of morphological re-
lations as well as thesaurus and distributional sim-
ilarity measures. We will also explore other semi-
supervised algorithms.
8
References
Alekh Agarwal and Pushpak Bhattacharyya. 2005. Sen-
timent Analysis: A new Approach for Effective Use
of Linguistic Knowledge and Exploiting Similarities
in a Set of Documents to be Classified. Proceedings of
ICON?05.
Alina Andreevskaia and Sabine Bergler. 2006. Mining
WordNet for Fuzzy Sentiment: Sentiment Tag Extrac-
tion from WordNet Glosses. Proceedings of EACL?06.
Avrim Blum and Shuchi Chawla. 2001. Learning from
Labeled and Unlabeled Data using Graph Mincuts.
Proceedings of ICML?01.
Thomas Cormen, Charles Leiserson, Ronald Rivest and
Clifford Stein. 2002. Introduction to Algorithms.
Second Edition, the MIT Press.
Kushal Dave, Steve Lawrence, and David Pennock.
2003. Mining the Peanut Gallery: Opinion Extrac-
tion and Semantic Classification of Product Reviews.
Proceedings of WWW?03.
Andrea Esuli and Fabrizio Sebastiani. 2006. SentiWord-
Net: A Publicly Available Lexical Resource for Opin-
ion Mining. Proceedings of LREC?06.
Andrea Esuli and Fabrizio Sebastiani. 2007. PageRank-
ing WordNet Synsets: An application to Opinion Min-
ing. Proceedings of ACL?07.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Adjec-
tives. Proceedings of ACL?97.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing Lexicon for Sentiment Analysis from Massive
Collection of HTML Documents. Proceedings of
EMNLP?07.
Japp Kamps, Maarten Marx, Robert Mokken, and
Maarten de Rijke. 2004. Using WordNet to Measure
Semantic Orientation of Adjectives. Proceedings of
LREC?04.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
Sentiment of Opinions. Proceedings of COLING?04.
Soo-Min Kim and Eduard Hovy. 2005. Automatic De-
tection of Opinion BearingWords and Sentences. Pro-
ceedings of ICJNLP?05.
Taku Kudo and Yuji Matsumoto. 2004. A Boosting
Algorithm for Classification of Semi-structured Text.
Proceedings of EMNLP?04.
Iadh Ounis, Maarten de Rijke, Craig Macdonald, Gilad
Mishne and Ian Soboroff. 2006. Overview of the
TREC-2006 Blog Track. Proceedings of TREC?06.
Bo Pang and Lillian Lee. 2004. A Sentiment Education:
Sentiment Analysis Using Subjectivity summarization
Based on Minimum Cuts. Proceedings of ACL?04.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval 2(1-2).
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification us-
ing Machine Learning Techniques. Proceedings of
EMNLP?02.
Livia Polanyi and Annie Zaenen. 2004. Contextual Va-
lence Shifters. Proceedings of the AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text: The-
ories and Applications.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing Product Features and Opinions from Reviews Pro-
ceedings of EMNLP?05.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning Subjective Nouns using Extraction Pattern
Bootstrapping. Proceedings of CoNLL?03
Fangzhong Su and Katja Markert. 2008. From Words
to Senses: A Case Study in Subjectivity Recognition.
Proceedings of COLING?08.
Fangzhong Su and Katja Markert. 2008a. Elicit-
ing Subjectivity and Polarity Judgements on Word
Senses. Proceedings of COLING?08 workshop of Hu-
man Judgements in Computational Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting Semantic Orientations of Words us-
ing Spin Model. Proceedings of ACL?05.
Matt Thomas, Bo Pang and Lilian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. Proceedings of
EMNLP?06.
Peter Turney. 2002. Thumbs up or Thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. Proceedings of ACL?02.
Peter Turney and Michael Littman. 2003. Measuring
Praise and Criticism: Inference of Semantic Orienta-
tion from Association. ACM Transaction on Informa-
tion Systems.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
Answering Opinion Questions: Separating Facts from
Opinions and Identifying the Polarity of Opinion Sen-
tences. Proceedings of EMNLP?03.
Janyce Wiebe and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating Expressions of Opinions and Emotions in
Language. Language Resources and Evaluation.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity in
Phrase-Level Sentiment Analysis. Proceedings of
HLT/EMNLP?05.
9
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 357?360,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Word Sense Subjectivity for Cross-lingual Lexical Substitution
Fangzhong Su
School of Computing
University of Leeds, UK
scsfs@leeds.ac.uk
Katja Markert
School of Computing
University of Leeds, UK
scskm@leeds.ac.uk
Abstract
We explore the relation between word sense
subjectivity and cross-lingual lexical substitu-
tion, following the intuition that good substi-
tutions will transfer a word?s (contextual) sen-
timent from the source language into the target
language. Experiments on English-Chinese
lexical substitution show that taking a word?s
subjectivity into account can indeed improve
performance. We also show that just using
word sense subjectivity can perform as well
as integrating fully-fledged fine-grained word
sense disambiguation for words which have
both subjective and objective senses.
1 Introduction
Cross-lingual lexical substitution has been proposed
as a Task at SemEval-2010.1 Given a target word
and its context in a source language (like English),
the goal is to provide correct translations for that
word in a target language (like Chinese). The trans-
lations must fit the given context.
In this paper, we explore the relation between the
sentiment of the used word in the source language
and translation choice in the target language, focus-
ing on English as the source and Chinese as the tar-
get language. Our work is motivated by the intuition
that most good word translations will be sentiment-
invariant, i.e. if a source word is used in a subjec-
tive (opinion-carrying) sense it will be often trans-
lated with a subjective sense in the target language
whereas if it used in an objective sense, it will be
1http://lit.csci.unt.edu/index.php/
Semeval_2010
translated with an objective sense. As an exam-
ple, consider the two words positive and collaborate
with example senses from WordNet 2.0 below.
(1) positive?greater than zero; ?positive numbers?
(objective)
(2) plus, positive?involving advantage or good; ?a
plus (or positive) factor? (subjective)
(3) collaborate, join forces, cooperate?work together
on a common enterprise of project; ?We joined
forces with another research group?(objective)
(4) collaborate?cooperate as a traitor; (subjective)
In most cases, if the word positive is used in
the sense ?greater than zero? (objective) in an
English context, the corresponding Chinese trans-
lation is ? ff?; if ?involving advantage or
good?(subjective) is used, its Chinese translations
are ??4ff,?ff?. Similarly, for the word collab-
orate, the sense ?work together on a common en-
terprise of project? (objective) corresponds to ??
?,?? in Chinese translation, and ?cooperate as
a traitor? (subjective) corresponds to ?(, H
?r?. Therefore, subjectivity information should
be effective for improving lexical translation for
what we previously (Su and Markert, 2008) termed
subjectivity-ambiguous words, i.e. words with both
subjective and objective senses such as positive and
collaborate above.
We therefore incorporate subjectivity word sense
disambiguation (SWSD) as defined in Akkaya et
al. (2009) into lexical substitution. SWSD is a
binary classification task that decides in context
whether a word occurs with one of its subjective or
one of its objective senses. In contrast to standard
357
multi-class Word Sense Disambiguation (WSD), it
uses a coarse-grained sense inventory that allows to
achieve higher accuracy than WSD and therefore in-
troduces less noise when embedded in another task
such as word translation. For example, the accuracy
reported in Akkaya et al (2009) for SWSD is over
20% higher than for standard WSD. Coarse-grained
senses are also easier to annotate, so getting train-
ing data for learning is less arduous. On the mi-
nus side, SWSD can only be useful for subjectivity-
ambiguous words. However, we showed (Su and
Markert, 2008) that subjectivity-ambiguity is fre-
quent (around 30% of common words).
2 Related Work
McCarthy and Navigli (2007) organized a monolin-
gual English lexical substitution task in Semeval-
2007, i.e finding English substitutions for an English
target word. Mihalcea et al organize an English-
Spanish lexical substitution task in SemEval-2010.
Approaches to lexical substitution in the past com-
petitions did not use sentiment features.
Independent of these lexical substitution tasks, the
connection between word senses and word transla-
tion has been explored in Chan et al (2007) and
Carpuat and Wu (2007), who predict the probabil-
ities of a target word being translated as an item in
a ?sense inventory?, where the sense inventory is a
list of possible translations. They then incorporate
these probabilities into machine translation. How-
ever, they do not consider sentiment explicitly.
Subjectivity at the word sense level has been
discussed by (Wiebe and Mihalcea, 2006; Su and
Markert, 2008; Akkaya et al, 2009). Wiebe and
Mihalcea (2006) and Su and Markert (2008) both
show that this is a well-defined concept via human
annotation as well as automatic recognition. Akkaya
et al (2009) show that subjectivity word sense dis-
ambiguation (SWSD) can boost the performance of
a sentiment analysis system. None of these paper
considers the impact of word sense subjectivity on
cross-lingual lexical substitution.
3 Methodology
3.1 Task and Dataset
We constructed an English-Chinese lexical substi-
tution gold standard by translating the English tar-
get words in the SENSEVAL 2 and SENSEVAL 3
lexical sample training and test sets into Chinese.
We choose the SENSEVAL datasets as they are rel-
atively domain-independent and also because we
can use them for our SWSD/WSD subtasks as well.
The translation is carried out by two native Chinese
speakers with a good command of English. First,
candidate Chinese translations (denoted by T) of the
English target words are provided from the on-line
English-Chinese dictionary iciba2, which is com-
posed of more than 150 different English-Chinese
dictionaries. To reduce annotation bias, the order
of the Senseval sentences is randomized. The an-
notators then independently assign the most fitting
Chinese translation(s) (from T) for the English tar-
get words in the given Senseval sentences. For the
agreement study, different Chinese translations (for
example, ?%? and ??? of the word author-
ity) that are actually synonyms are merged. The
observed agreement between the two annotators is
86.7%. Finally, the two annotators discuss the dis-
agreed examples together, leading to a gold stan-
dard.
Since we evaluate how word sense subjectivity
affects cross-lingual lexical substitution, we lim-
ited our study to the SENSEVAL words that are
subjectivity-ambiguous. Therefore, following the
annotation schemes in (Su and Markert, 2008;
Wiebe and Mihalcea, 2006), all senses of all target
words in SENSEVAL 2&3 are annotated by a near-
native English speaker as subjective orobjective.
This annotator was not involved in the English to
Chinese translation. We also discard subjectivity-
ambiguous words if its subjective or objective senses
do not appear in both training and test set. In total we
collect 28 subjectivity-ambiguous words. Their En-
glish example sentences and translations yield 2890
training sentence pairs and 1444 test sentence pairs.
3.2 Algorithms
For the English-Chinese lexical substitution task, we
first develop a basic system (called B) to assign Chi-
nese translations to the target English words in con-
text. This system uses only standard contextual fea-
tures from the English sentences (see Section 3.3).
We then add word sense subjectivity information to
2http://www.iciba.com
358
the basic system (see Section 3.4). We also compare
including word sense subjectivity to the inclusion of
full fine-grained sense information (Section 3.5).
All systems are supervised classifiers trained on
the SENSEVAL training data and evaluated on the
SENSEVAL test data for each of the 28 words. We
employ an SVM classifier from the libsvm pack-
age3 with a linear kernel.
3.3 Common Features
In the basic system B, we adopt features which are
commonly used in WSD or lexical translation.
Surrounding Words: Lemmatized bag of words
with stop word filtering.
Part-of-Speech (POS): The POS of the neigh-
bouring words of the target word. We extract POS
tag of the 3 words to the right and left together with
position information.
Collocation: The neighbouring words of the tar-
get word. We extract 4 lemmatized words to the
right and left, together with position information.
Syntactic Relations: We employ the MaltParser4
for dependency parsing and extract 4 features: the
head word of the target word, POS of the head word,
the dependency relation between head word and tar-
get word, and the relative position (left or right) of
the head word to the target word.
3.4 Subjectivity Features
We add a feature that incorporates whether the origi-
nal English word is used subjectively or objectively.
For an upper bound, we use the SENSEVAL gold
standard sense annotation (gold-subj), mapped onto
binary subjective/objective labels. For a more re-
alistic assessment, we use SWSD to derive the sub-
jectivity sense label automatically (auto-subj) using
standard supervised binary SVMs and the features in
Section 3.3 on the SENSEVAL data.
3.5 Sense Features
We compare using subjectivity information to using
full fine-grained word sense information, incorpo-
rating a feature that specifies the exact word sense
of the target word to be translated. This setting
3http://www.csie.ntu.edu.tw/?cjlin/
libsvm
4http://w3.msi.vxu.se/?nivre/research/
MaltParser.html
also compares the SENSEVAL gold standard (gold-
senses) and automatically predicted sense informa-
tion (auto-senses), the latter via supervised multi-
class learning on the SENSEVAL dataset.
4 Experiments and Evaluation
For the English-Chinese lexical substitution task, we
evaluate 6 different methods: Baseline (assign the
most frequent translation to all examples), B (use
common features), B+gold subj (incorporate gold
standard word sense subjectivity), B+gold sense (in-
corporate gold standard sense), B+auto subj (incor-
porate automatically predicted word sense subjectiv-
ity), and B+auto sense (incorporate automatically
predicted fine-grained senses). We measure lexical
substitution accuracy on the SENSEVAL test data by
comparing to the human gold standard annotation
(see Section 3.1). Results are listed in Table 1.
Results. Table 1 shows that our standard lexical
substitution system B improves strongly (near 11%
average accuracy gain) over the most frequent trans-
lation baseline. Incorporating sense subjectivity as
in B+gold subj leads to a further strong improve-
ment, confirming our hypothesis that word sense
subjectivity can improve lexical substitution. Incor-
porating fine-grained senses B+gold senses yields
only a slightly higher gain, showing that a coarse-
grained subjective/objective classification might be
sufficient for subjectivity-ambiguous words for aid-
ing translation. In addition, the small gain using
fine-grained senses might disappear in practice as
automatic WSD is a more challenging task than
SWSD: in our experiment, B+auto sense performs
worse than B+auto subj. The current improve-
ment of B+auto subj over B is significant (McNe-
mar test at the 5% level). The difference between
the actual performance of word sense subjectivity
and its potential as exemplified in B+gold subj is,
obviously, caused by imperfect performance of the
SWSD component, mostly due to a distributional
bias in the SENSEVAL training data, with few ex-
amples for rarer senses of the target words.
For some words (such as authority and stress),
the additional sense subjectivity feature does not im-
prove lexical substitution, even when gold standard
labels are used. There are two main reasons for this.
First, one candidate Chinese translation might cover
359
Table 1: Accuracy of lexical substitution with different
different feature settings
Word Subjectivity
of Senses
Baseline Basic
(B)
B+gold
subj
B+gold
senses
B+auto
subj
B+auto
senses
authority 3-S 4-O 50.5% 70.3% 70.3% 84.6% 70.3% 79.1%
blind 2-S 1-O 87.0% 88.9% 94.4% 94.4% 88.9% 88.9%
cool 3-S 3-O 46.0% 46.0% 68.0% 68.0% 58.0% 48.0%
dyke 1-S 1-O 89.3% 89.3% 92.9% 92.9% 89.3% 89.3%
fatigue 1-S 2-O 1-B 80.0% 80.0% 82.5% 85.0% 82.5% 82.5%
fine 5-S 4-O 78.5% 78.5% 90.8% 80.0% 80.0% 78.5%
nature 1-S 3-O 1-B 53.3% 62.2% 73.3% 71.1% 64.4% 62.2%
oblique 1-S 1-O 65.5% 75.9% 86.2% 89.7% 79.3% 79.3%
sense 3-S 2-O 47.5% 67.5% 77.5% 77.5% 75.0% 72.5%
simple 2-S 2-O 1-B 71.2% 71.2% 75.8% 74.2% 72.7% 71.2%
stress 3-S 2-O 92.1% 92.1% 92.1% 92.1% 92.1% 92.1%
collaborate 1-S 1-O 90.0% 90.0% 93.3% 93.3% 93.3% 90.0%
drive 3-S 5-O 1-B 51.4% 78.4% 89.2% 86.5% 83.8% 78.4%
play 4-S 13-O 1-B 23.3% 40.0% 48.3% 56.7% 41.7% 43.3%
see 7-S 11-O 30.9% 36.8% 58.8% 61.8% 42.6% 38.2%
strike 3-S 10-O 1-B 20.5% 27.3% 43.2% 45.5% 29.5% 38.6%
treat 2-S 4-O 36.4% 61.4% 65.9% 81.8% 56.8% 65.9%
wander 1-S 2-O 1-B 79.2% 81.3% 83.3% 83.3% 81.3% 81.3%
work 2-S 9-O 2-B 56.8% 56.8% 75.0% 75.0% 63.6% 61.4%
appear 1-S 2-O 42.7% 63.4% 80.2% 90.8% 65.6% 66.4%
express 2-S 2-O 81.5% 81.5% 90.7% 88.9% 83.3% 81.5%
hot 3-S 4-O 1-B 85.0% 85.0% 85.0% 85.0% 85.0% 85.0%
image 3-S 4-O 56.7% 83.6% 94.0% 92.5% 85.1% 79.1%
interest 2-S 4-O 1-B 38.7% 73.1% 84.9% 88.2% 74.2% 71.0%
judgment 4-S 3-O 46.9% 65.6% 78.1% 75.0% 68.8% 62.5%
miss 3-S 5-O 50.0% 63.3% 70.0% 66.7% 63.3% 60.0%
solid 4-S 10-O 40.0% 40.0% 44.0% 48.0% 44.0% 44.0%
watch 3-S 4-O 86.3% 86.3% 90.2% 88.2% 86.3% 86.3%
AVERAGE 57.4% 68.5% 77.9% 80.2% 70.7% 70.1%
both subjective and objective uses of the word. For
example, both the objective sense (?physics force
that produces strain on a physical body?) and sub-
jective senses (?difficulty that causes worry or emo-
tional emotional tension? and ? a state of mental
or emotional strain or suspense? ) of stress are of-
ten translated as ???? in Chinese. Second, in
some cases, subjectivity word sense disambiguation
is too coarse-grained and finer-grained WSD is ac-
tually necessary. For example, the subjective usages
of authority in SENSEVAL examples are often trans-
lated as ?;[, %?, ?g&? or ??&? (called
List-S), and objective usages are often translated
as ??, ??,???,??, ?? or ??, 1O?
(called List-O). In this case, word sense subjectivity
might help to distinguish List-S from List-O, but
not among the candidate translations within a single
list.
5 Discussion
We tackle cross-lingual lexical substitution as a su-
pervised task, using sets of manual translations for a
target word as training data even for baseline system
B. However, we do not necessarily need dedicated
human translated data as we could also use existing
parallel texts in which the target word occurs. There-
fore, we think that a supervised approach to lexical
substitution is feasible. However, we do need addi-
tional monolingual sense-tagged data in the source
language for incorporating our word sense subjec-
tivity features.5 Although a disadvantage, more and
more sense-tagged data does become available (such
as OntoNotes). We also only need tagging at a
coarse-grained sense level, which is much easier to
create than fine-grained data.
6 Conclusion and Future Work
We investigate the relation between word sense sub-
jectivity and cross-lingual lexical substitution. The
experimental results show that incorporating word
sense subjectivity into a standard supervised classi-
fication model yields a significantly better perfor-
mance for an English-Chinese lexical substitution
task. We also compare the effect of sense subjec-
tivity to the effect of fine-grained sense informa-
tion on lexical substitution. The differences be-
tween the two methods turn out to be small, mak-
ing a case for the ?easier?, coarse-grained SWSD
over WSD for subjectivity-ambiguous words. Fu-
ture work will widen the study by (i) looking at a
wider range of words and languages, (ii) improv-
ing automatic SWSD results for better application
and (iii) integrating unsupervised subjectivity fea-
tures into cross-lingual lexical substitution.
References
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea. 2009.
Subjectivity Word Sense Disambiguation. Proceed-
ings of EMNLP?09.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. Proceedings of EMNLP?07.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word Sense Disambiguation Improves Statistical Ma-
chine Translation. Proceedings of ACL?07.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 Task 10: English Lexical Substitution Task. Pro-
ceedings of SemEval-2007.
Fangzhong Su and Katja Markert. 2008. From Words
to Senses: A Case Study in Subjectivity Recognition.
Proceedings of COLING?08.
Janyce Wiebe and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
5In our case, this is the same data as the data the lexical
substitution algorithms are trained on, but this is not mandatory.
360
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 91?96,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
ACCURAT Toolkit for Multi-Level Alignment and  
Information Extraction from Comparable Corpora 
 
M?rcis Pinnis1, Radu Ion2, Dan ?tef?nescu2, Fangzhong Su3, 
Inguna Skadi?a1, Andrejs Vasi?jevs1, Bogdan Babych3 
 1Tilde, Vien?bas gatve 75a, Riga, Latvia 
{marcis.pinnis,inguna.skadina,andrejs}@tilde.lv 
 
2Research Institute for Artificial Intelligence, Romanian Academy 
{radu,danstef}@racai.ro 
 
3Centre for Translation Studies, University of Leeds 
{f.su,b.babych}@leeds.ac.uk 
 
 
Abstract 
The lack of parallel corpora and linguistic 
resources for many languages and domains is 
one of the major obstacles for the further 
advancement of automated translation. A 
possible solution is to exploit comparable 
corpora (non-parallel bi- or multi-lingual text 
resources) which are much more widely 
available than parallel translation data. Our 
presented toolkit deals with parallel content 
extraction from comparable corpora. It consists 
of tools bundled in two workflows: (1) 
alignment of comparable documents and 
extraction of parallel sentences and (2) 
extraction and bilingual mapping of terms and 
named entities. The toolkit pairs similar 
bilingual comparable documents and extracts 
parallel sentences and bilingual terminological 
and named entity dictionaries from comparable 
corpora. This demonstration focuses on the 
English, Latvian, Lithuanian, and Romanian 
languages. 
Introduction 
In recent decades, data-driven approaches have 
significantly advanced the development of 
machine translation (MT). However, lack of 
sufficient bilingual linguistic resources for many 
languages and domains is still one of the major 
obstacles for further advancement of automated 
translation. At the same time, comparable corpora, 
i.e., non-parallel bi- or multilingual text resources 
such as daily news articles and large knowledge 
bases like Wikipedia, are much more widely 
available than parallel translation data.  
While methods for the use of parallel corpora in 
machine translation are well studied (Koehn, 
2010), similar techniques for comparable corpora 
have not been thoroughly worked out. Only the 
latest research has shown that language pairs and 
domains with little parallel data can benefit from 
the exploitation of comparable corpora (Munteanu 
and Marcu, 2005; Lu et al, 2010; Smith et al, 
2010; Abdul-Rauf and Schwenk, 2009 and 2011). 
In this paper we present the ACCURAT 
toolkit1 - a collection of tools that are capable of  
analysing comparable corpora and extracting 
parallel data which can be used to improve the 
performance of statistical and rule/example-based 
MT systems. 
Although the toolkit may be used for parallel 
data acquisition for open (broad) domain systems, 
it will be most beneficial for under-resourced 
languages or specific domains which are not 
covered by available parallel resources. 
The ACCURAT toolkit produces: 
? comparable document pairs with 
comparability scores, allowing to estimate 
the overall comparability of corpora; 
? parallel sentences which can be used as 
additional parallel data sources for 
statistical translation model learning; 
                                                          
1 http://www.accurat-project.eu/ 
91
? terminology dictionaries ? this type of 
data is expected to improve domain-
dependent translation; 
? named entity dictionaries. 
The demonstration showcases two general use 
case scenarios defined in the toolkit: ?parallel data 
mining from comparable corpora? and ?named 
entity/terminology extraction and mapping from 
comparable corpora?. 
The next section provides a general overview of 
workflows followed by descriptions of methods 
and tools integrated in the workflows. 
1 Overview of the Workflows 
The toolkit?s tools are integrated within two 
workflows (visualised in Figure 1). 
 
  
Figure 1. Workflows of the ACCURAT toolkit. 
 
The workflow for parallel data mining from 
comparable corpora aligns comparable corpora in 
the document level (section 2.1). This step is 
crucial as the further steps are computationally 
intensive. To minimise search space, documents 
are aligned with possible candidates that are likely 
to contain parallel data. Then parallel sentence 
pairs are extracted from the aligned comparable 
corpora (section 2.2). 
The workflow for named entity (NE) and 
terminology extraction and mapping from 
comparable corpora extracts data in a dictionary-
like format. Providing a list of document pairs, the 
workflow tags NEs or terms in all documents using 
language specific taggers (named entity 
recognisers (NER) or term extractors) and 
performs multi-lingual NE (section 2.3) or term 
mapping (section 2.4), thereby producing bilingual 
NE or term dictionaries. The workflow also 
accepts pre-processed documents, thus skipping 
the tagging process. 
Since all tools use command line interfaces, task 
automation and workflow specification can be 
done with simple console/terminal scripts. All 
tools can be run on the Windows operating system 
(some are also platform independent). 
2 Tools and Methods 
This section provides an overview of the main 
tools and methods in the toolkit. A full list of tools 
is described in ACCURAT D2.6. (2011). 
2.1 Comparability Metrics 
We define comparability by how useful a pair of 
documents is for parallel data extraction. The 
higher the comparability score, the more likely two 
documents contain more overlapping parallel data. 
The methods are developed to perform lightweight 
comparability estimation that minimises search 
space of relatively large corpora (e.g., 10,000 
documents in each language). There are two 
comparability metric tools in the toolkit: a 
translation based and a dictionary based metric. 
The Translation based metric (Su and Babych, 
2012a) uses MT APIs for document translation 
into English. Then four independent similarity 
feature functions are applied to a document pair: 
? Lexical feature ? both documents are pre-
processed (tokenised, lemmatised, and 
stop-words are filtered) and then 
vectorised. The lexical overlap score is 
calculated as a cosine similarity function 
over the vectors of two documents. 
? Structural feature ? the difference of 
sentence counts and content word counts 
(equally interpolated). 
? Keyword feature ? the cosine similarity 
of top 20 keywords. 
? NE feature ? the cosine similarity of NEs 
(extracted using Stanford NER). 
These similarity measures are linearly combined in 
a final comparability score. This is implemented by 
a simple weighted average strategy, in which each 
92
type of feature is associated with a weight 
indicating its relative confidence or importance. 
The comparability scores are normalised on a scale 
of 0 to 1, where a higher comparability score 
indicates a higher comparability level. 
The reliability of the proposed metric has been 
evaluated on a gold standard of comparable 
corpora for 11 language pairs (Skadi?a et al, 
2010). The gold standard consists of news articles, 
legal documents, knowledge-base articles, user 
manuals, and medical documents. Document pairs 
in the gold standard were rated by human judges as 
being parallel, strongly comparable, or weakly 
comparable. The evaluation results suggest that the 
comparability scores reliably reflect comparability 
levels. In addition, there is a strong correlation 
between human defined comparability levels and 
the confidence scores derived from the 
comparability metric, as the Pearson R correlation 
scores vary between 0.966 and 0.999, depending 
on the language pair.  
The Dictionary based metric (Su and Babych, 
2012b) is a lightweight approach, which uses 
bilingual dictionaries to lexically map documents 
from one language to another. The dictionaries are 
automatically generated via word alignment using 
GIZA++ (Och and Ney, 2000) on parallel corpora. 
For each word in the source language, the top two 
translation candidates (based on the word 
alignment probability in GIZA++) are retrieved as 
possible translations into the target language. This 
metric provides a much faster lexical translation 
process, although word-for-word lexical mapping 
produces less reliable translations than MT based 
translations. Moreover, the lower quality of text 
translation in the dictionary based metric does not 
necessarily degrade its performance in predicting 
comparability levels of comparable document 
pairs. The evaluation on the gold standard shows a 
strong correlation (between 0.883 and 0.999) 
between human defined comparability levels and 
the confidence scores of the metric. 
2.2 Parallel Sentence Extractor from 
Comparable Corpora 
Phrase-based statistical translation models are 
among the most successful translation models that 
currently exist (Callison-Burch et al, 2010). 
Usually, phrases are extracted from parallel 
corpora by means of symmetrical word alignment 
and/or by phrase generation (Koehn et al, 2003). 
Our toolkit exploits comparable corpora in order to 
find and extract comparable sentences for SMT 
training using a tool named LEXACC (?tef?nescu 
et al, 2012). 
LEXACC requires aligned document pairs (also 
m to n alignments) for sentence extraction. It also 
allows extraction from comparable corpora as a 
whole; however, precision may decrease due to 
larger search space. 
LEXACC scores sentence pairs according to five 
lexical overlap and structural matching feature 
functions. These functions are combined using 
linear interpolation with weights trained for each 
language pair and direction using logistic 
regression. The feature functions are: 
? a lexical (translation) overlap score for 
content words (nouns, verbs, adjectives, 
and adverbs) using GIZA++ (Gao and 
Vogel, 2008) format dictionaries; 
? a lexical (translation) overlap score for 
functional words (all except content 
words) constrained by the content word 
alignment from the previous feature; 
? the alignment obliqueness score, a measure 
that quantifies the degree to which the 
relative positions of source and target 
aligned words differ; 
? a score indicating whether strong content 
word translations are found at the 
beginning and the end of each sentence in 
the given pair; 
? a punctuation score which indicates 
whether the sentences have identical 
sentence ending punctuation. 
For different language pairs, the relevance of 
the individual feature functions differ. For 
instance, the locality feature is more important for 
the English-Romanian pair than for the English-
Greek pair. Therefore, the weights are trained on 
parallel corpora (in our case - 10,000 pairs). 
LEXACC does not score every sentence pair in 
the Cartesian product between source and target 
document sentences. It reduces the search space 
using two filtering steps (?tef?nescu et al, 2012). 
The first step makes use of the Cross-Language 
Information Retrieval framework and uses a search 
engine to find sentences in the target corpus that 
are the most probable translations of a given 
sentence. In the second step (which is optional), 
93
the resulting candidates are further filtered, and 
those that do not meet minimum requirements are 
eliminated.  
To work for a certain language pair, LEXACC 
needs additional resources: (i) a GIZA++-like 
translation dictionary, (ii) lists of stop-words in 
both languages, and (iii) lists of word suffixes in 
both languages (used for stemming). 
The performance of LEXACC, regarding 
precision and recall, can be controlled by a 
threshold applied to the overall interpolated 
parallelism score. The tool has been evaluated on 
news article comparable corpora. Table 1 shows 
results achieved by LEXACC with different 
parallelism thresholds on automatically crawled 
English-Latvian corpora, consisting of 41,914 
unique English sentences and 10,058 unique 
Latvian sentences. 
 
Threshold Aligned pairs Precision 
Useful 
pairs 
0.25 1036 39.19% 406 
0.3 813 48.22% 392 
0.4 553 63.47% 351 
0.5 395 76.96% 304 
0.6 272 84.19% 229 
0.7 151 88.74% 134 
0.8 27 88.89% 24 
0.9 0 - 0 
 
Table 1. English-Latvian parallel sentence extraction 
results on a comparable news corpus. 
 
Threshold Aligned pairs Precision Useful pairs
0.2 2324 10.32% 240 
0.3 1105 28.50% 315 
0.4 722 53.46% 386 
0.5 532 89.28% 475 
0.6 389 100% 389 
0.7 532 100% 532 
0.8 386 100% 386 
0.9 20 100% 20 
 
Table 2. English-Romanian parallel sentence extraction 
results on a comparable news corpus. 
Table 2 shows results for English-Romanian on 
corpora consisting of 310,740 unique English and 
81,433 unique Romanian sentences. 
Useful pairs denote the total number of parallel 
and strongly comparable sentence pairs (at least 
80% of the source sentence is a translation in the 
target sentence). The corpora size is given only as 
an indicative figure, as the amount of extracted 
parallel data greatly depends on the comparability 
of the corpora. 
2.3 Named Entity Extraction and Mapping 
The second workflow of the toolkit allows NE and 
terminology extraction and mapping. Starting with 
named entity recognition, the toolkit features the 
first NER systems for Latvian and Lithuanian 
(Pinnis, 2012). It also contains NER systems for 
English (through an OpenNLP NER2 wrapper) and 
Romanian (NERA). In order to map named entities, 
documents have to be tagged with NER systems 
that support MUC-7 format NE SGML tags.  
The toolkit contains the mapping tool NERA2. 
The mapper requires comparable corpora aligned 
in the document level as input. NERA2 compares 
each NE from the source language to each NE 
from the target language using cognate based 
methods. It also uses a GIZA++ format statistical 
dictionary to map NEs containing common nouns 
that are frequent in location names. This approach 
allows frequent NE mapping if the cognate based 
method fails, therefore, allowing increasing the 
recall of the mapper. Precision and recall can be 
tuned with a confidence score threshold. 
2.4 Terminology Mapping 
During recent years, automatic bilingual term 
mapping in comparable corpora has received 
greater attention in light of the scarcity of parallel 
data for under-resourced languages. Several 
methods have been applied to this task, e.g., 
contextual analysis (Rapp, 1995; Fung and 
McKeown, 1997) and compositional analysis 
(Daille and Morin, 2008). Symbolic, statistical, and 
hybrid techniques have been implemented for 
bilingual lexicon extraction (Morin and 
Prochasson, 2011). 
Our terminology mapper is designed to map 
terms extracted from comparable or parallel 
                                                          
2 Open NLP - http://incubator.apache.org/opennlp/. 
94
documents. The method is language independent 
and can be applied if a translation equivalents table 
exists for a language pair. As input, the application 
requires term-tagged bilingual corpora aligned in 
the document level. 
The toolkit includes term-tagging tools for 
English, Latvian, Lithuanian, and Romanian, but 
can be easily extended for other languages if a 
POS-tagger, a phrase pattern list, a stop-word list, 
and an inverse document frequency list (calculated 
on balanced corpora) are available. 
The aligner maps terms based on two criteria 
(Pinnis et al, 2012; ?tef?nescu, 2012): (i) a 
GIZA++-like translation equivalents table and (ii) 
string similarity in terms of Levenshtein distance 
between term candidates.  For evaluation, Eurovoc 
(Steinberger et al, 2002) was used. Tables 4 and 5 
show the performance figures of the mapper for 
English-Romanian and English-Latvian. 
 
Threshold P R F-measure
0.3 0.562 0.194 0.288 
0.4 0.759 0.295 0.425 
0.5 0.904 0.357 0.511 
0.6 0.964 0.298 0.456 
0.7 0.986 0.216 0.359 
0.8 0.996 0.151 0.263 
0.9 0.995 0.084 0.154 
 
Table 3. Term mapping performance for English-
Romanian. 
 
Threshold P R F-measure 
0.3 0.636 0.210 0.316 
0.4 0.833 0.285 0.425 
0.5 0.947 0.306 0.463 
0.6 0.981 0.235 0.379 
0.7 0.996 0.160 0.275 
0.8 0.996 0.099 0.181 
0.9 0.997 0.057 0.107 
 
Table 4. Term mapping performance for English-
Latvian. 
3 Conclusions and Related Information 
This demonstration paper describes the 
ACCURAT toolkit containing tools for multi-level 
alignment and information extraction from 
comparable corpora. These tools are integrated in 
predefined workflows that are ready for immediate 
use. The workflows provide functionality for the 
extraction of parallel sentences, bilingual NE 
dictionaries, and bilingual term dictionaries from 
comparable corpora. 
The methods, including comparability metrics, 
parallel sentence extraction and named entity/term 
mapping, are language independent. However, they 
may require language dependent resources, for 
instance, POS-taggers, Giza++ translation 
dictionaries, NERs, term taggers, etc.3 
 The ACCURAT toolkit is released under the 
Apache 2.0 licence and is freely available for 
download after completing a registration form4.  
Acknowledgements 
The research within the project ACCURAT 
leading to these results has received funding from 
the European Union Seventh Framework 
Programme (FP7/2007-2013), grant agreement no 
248347. 
References  
Sadaf Abdul-Rauf and Holger Schwenk. On the use of 
comparable corpora to improve SMT performance. 
EACL 2009: Proceedings of the 12th conference of 
the European Chapter of the Association for 
Computational Linguistics, Athens, Greece, 16-23. 
Sadaf Abdul-Rauf and Holger Schwenk. 2011. Parallel 
sentence generation from comparable corpora for 
improved SMT. Machine Translation, 25(4): 341-
375. 
ACCURAT D2.6 2011. Toolkit for multi-level 
alignment and information extraction from 
comparable corpora (http://www.accurat-project.eu). 
Dan Gusfield. 1997. Algorithms on strings, trees and 
sequences. Cambridge University Press. 
Chris Callison-Burch, Philipp Koehn, Christof Monz, 
Kay Peterson, Mark Przybocki and Omar Zaidan. 
2010. Findings of the 2010 Joint Workshop on 
Statistical Machine Translation and Metrics for 
Machine Translation. Proceedings of the Joint Fifth 
Workshop on Statistical Machine Translation and 
MetricsMATR, 17-53. 
B?atrice Daille and Emmanuel Morin. 2008. Effective 
compositional model for lexical alignment. 
Proceedings of the 3rd International Joint Conference 
                                                          
3 Full requirements are defined in the documentation of each 
tool (ACCURAT D2.6, 2011). 
4 http://www.accurat-project.eu/index.php?p=toolkit 
95
on Natural Language Processing, Hyderabad, India, 
95-102. 
Franz Josef Och and Hermann Ney. 2000. Improved 
statistical alignment models. Proceedings of the 38th 
Annual Meeting of the Association for 
Computational Linguistics, 440-447. 
Pascale Fung and Kathleen Mckeown. 1997. Finding 
terminology translations from non-parallel corpora. 
Proceedings of the 5th Annual Workshop on Very 
Large Corpora, 192-202. 
Qin Gao and Stephan Vogel. 2008. Parallel 
implementations of a word alignment tool. 
Proceedings of ACL-08 HLT: Software Engineering, 
Testing, and Quality Assurance for Natural Language 
Processing, June 20, 2008. The Ohio State 
University, Columbus, Ohio, USA, 49-57. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. 
Proceedings of the Human Language Technology and 
North American Association for Computational 
Linguistics Conference (HLT/NAACL), May 27-
June 1, Edmonton, Canada. 
Philip Koehn. 2010. Statistical machine translation, 
Cambridge University Press. 
Bin Lu, Tao Jiang, Kapo Chow and Benjamin K. Tsou. 
2010. Building a large English-Chinese parallel 
corpus from comparable patents and its experimental 
application to SMT. Proceedings of the 3rd workshop 
on building and using comparable corpora: from 
parallel to non-parallel corpora, Valletta, Malta, 42-
48. 
Drago? ?tefan Munteanu and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
nonparallel corpora. ACL-44: Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics, 
Morristown, NJ, USA, 81-88. 
Emmanuel Morin and Emmanuel Prochasson. 2011. 
Bilingual lexicon extraction from comparable 
corpora enhanced with parallel corpora. ACL HLT 
2011, 27-34. 
M?rcis Pinnis. 2012. Latvian and Lithuanian named 
entity recognition with TildeNER. Proceedings of the 
8th international conference on Language Resources 
and Evaluation (LREC 2012), Istanbul, Turkey. 
M?rcis Pinnis, Nikola Ljube?i?, Dan ?tef?nescu, Inguna 
Skadi?a, Marko Tadi?, Tatiana Gornostay. 2012. 
Term extraction, tagging, and mapping tools for 
under-resourced languages. Proceedings of the 10th 
Conference on Terminology and Knowledge 
Engineering (TKE 2012), June 20-21, Madrid, Spain. 
Reinhard Rapp. 1995. Identifying word translations in 
non-parallel texts. Proceedings of the 33rd annual 
meeting on Association for Computational 
Linguistics, 320-322.  
Jason R. Smith, Chris Quirk, and Kristina Toutanova. 
2010.  Extracting parallel sentences from comparable 
corpora using document level alignment. Proceedings 
of NAACL 2010, Los Angeles, USA. 
Dan ?tef?nescu. 2012. Mining for term translations in 
comparable corpora. Proceedings of the 5th 
Workshop on Building and Using Comparable 
Corpora (BUCC 2012) to be held at the 8th edition of 
Language Resources and Evaluation Conference 
(LREC 2012), Istanbul, Turkey, May 23-25, 2012. 
Ralf Steinberger, Bruno Pouliquen and Johan Hagman. 
2002. Cross-lingual document similarity calculation 
using the multilingual thesaurus Eurovoc. 
Proceedings of the 3rd International Conference on 
Computational Linguistics and Intelligent Text 
Processing (CICLing '02), Springer-Verlag London, 
UK, ISBN:3-540-43219-1. 
Inguna Skadi?a, Ahmet Aker, Voula Giouli, Dan Tufis, 
Rob Gaizauskas, Madara Mieri?a and Nikos 
Mastropavlos. 2010. Collection of comparable 
corpora for under-resourced languages. In 
Proceedings of the Fourth International Conference 
Baltic HLT 2010, IOS Press, Frontiers in Artificial 
Intelligence and Applications, Vol. 219, pp. 161-168. 
Fangzhong Su and Bogdan Babych. 2012a. 
Development and application of a cross-language 
document comparability metric. Proceedings of the 
8th international conference on Language Resources 
and Evaluation (LREC 2012), Istanbul, Turkey.  
Fangzhong Su and Bogdan Babych.  2012b. Measuring 
comparability of documents in non-parallel corpora 
for efficient extraction of (semi-) parallel translation 
equivalents. Proceedings of  EACL'12 joint 
workshop on Exploiting Synergies between 
Information Retrieval and Machine Translation 
(ESIRMT) and Hybrid Approaches to Machine 
Translation (HyTra), Avignon, France.  
Dan ?tef?nescu, Radu Ion and Sabine Hunsicker. 2012. 
Hybrid parallel sentence mining from comparable 
corpora. Proceedings of the 16th Conference of the 
European Association for Machine Translation 
(EAMT 2012), Trento, Italy.  
96
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 42?50
Manchester, August 2008
Eliciting Subjectivity and Polarity Judgements on Word Senses
Fangzhong Su
School of Computing
University of Leeds
fzsu@comp.leeds.ac.uk
Katja Markert
School of Computing
University of Leeds
markert@comp.leeds.ac.uk
Abstract
There has been extensive work on elicit-
ing human judgements on the sentiment
of words and the resulting annotated word
lists have frequently been used for opin-
ion mining applications in Natural Lan-
guage Processing (NLP). However, this
word-based approach does not take differ-
ent senses of a word into account, which
might differ in whether and what kind
of sentiment they evoke. In this paper,
we therefore introduce a human annotation
scheme for judging both the subjectivity
and polarity of word senses. We show that
the scheme is overall reliable, making this
a well-defined task for automatic process-
ing. We also discuss three issues that sur-
faced during annotation: the role of anno-
tation bias, hierarchical annotation (or un-
derspecification) and bias in the sense in-
ventory used.
1 Introduction
Work in psychology, linguistics and computational
linguistics has explored the affective connotations
of words via eliciting human judgements (see
Section 2 for an in-depth review). Two impor-
tant parameters in determining affective meaning
that have emerged are subjectivity and polarity.
Subjectivity identification focuses on determining
whether a language unit (such as a word, sentence
or document) is subjective, i.e. whether it ex-
presses a private state, opinion or attitude, or is
factual. Polarity identification focuses on whether
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
a language unit has a positive or negative connota-
tion.
Word lists that result from such studies would,
for example tag good or positive as a positive
word, bad as negative and table as neither. Such
word lists have frequently been used in natural lan-
guage processing applications, such as the auto-
matic identification of a review as favourable or
unfavourable (Das and Chen, 2001). However,
the word-based annotation conducted so far is at
least partially unreliable. Thus Andreevskaia and
Bergler (2006) find only a 78.7% agreement on
subjectivity/polarity tags between two widely used
word lists. One problem they identify is that word-
based annotation does not take different senses
of a word into account. Thus, many words are
subjectivity-ambiguous or polarity-ambiguous, i.e.
have both subjective and objective or both posi-
tive and negative senses, such as the words posi-
tive and catch with corresponding example senses
given below.
1
(1) positive, electropositive?having a positive electric
charge;?protons are positive? (objective)
(2) plus, positive?involving advantage or good; ?a plus (or
positive) factor? (subjective)
(3) catch?a hidden drawback; ?it sounds good but what?s
the catch?? (negative)
(4) catch, match?a person regarded as a good matrimonial
prospect (positive)
Inspired by Andreeivskaia and Bergler (2006)
and Wiebe and Mihalcea (2006), we therefore ex-
plore the subjectivity and polarity annotation of
word senses instead of words. We hypothesize
that annotation at the sense level might eliminate
one possible source of disagreement for subjectiv-
ity/polarity annotation and will therefore hopefully
lead to higher agreement than at the word level.
1
All examples in this paper are from WordNet 2.0.
42
An additional advantage for practical purposes is
that subjectivity labels for senses add an additional
layer of annotation to electronic lexica and can
therefore increase their usability. As an example,
Wiebe and Mihalcea (2006) prove that subjectiv-
ity information for WordNet senses can improve
word sense disambiguation tasks for subjectivity-
ambiguous words (such as positive). In addition,
Andreevskaia and Bergler (2006) show that the
performance of automatic annotation of subjectiv-
ity at the word level can be hurt by the presence of
subjectivity-ambiguous words in the training sets
they use. A potential disadvantage for annotation
at the sense level is that it is dependent on a lexical
resource for sense distinctions and that an annota-
tion scheme might have to take idiosyncracies of
specific resources into account or, ideally, abstract
away from them.
In this paper, we investigate the reliability
of manual subjectivity labeling of word senses.
Specifically, we mark up subjectivity/attitude (sub-
jective, objective, and both) of word senses as well
as polarity/connotation (positive, negative and no
polarity). To the best of our knowledge, this is
the first annotation scheme for judging both sub-
jectivity and polarity of word senses. We test its
reliability on the WordNet sense inventory. Over-
all, the experimental results show high agreement,
confirming our hypothesis that agreement at sense
level might be higher than at the word level. The
annotated sense inventory will be made publically
available to other researchers at http://www.
comp.leeds.ac.uk/markert/data.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous related work.
Section 3 describes our human annotation scheme
for word sense subjectivity and polarity in detail.
Section 4 presents the experimental results and
evaluation. We also discuss the problems of bias in
the annotation scheme, the impact of hierarchical
organization or underspecification on agreement as
well as problems with bias in WordNet sense de-
scriptions. Section 5 compares our annotation to
the annotation of a different scheme, followed by
conclusions and future work in Section 6.
2 Related Work
Osgood et al (1957) proposed semantic differ-
ential to measure the connotative meaning of
concepts. They conducted a factor analysis of
large collections of semantic differential scales and
pointed out three referring attitudes that people use
to evaluate words and phrases?evaluation (good-
bad), potency (strong-weak), and activity (active-
passive). Also, they showed that these three di-
mensions of affective meaning are cross-cultural
universals from a study on dozens of cultures (Os-
good et al, 1975). This work has spawned a con-
siderable amount of linguistic and psychological
work in affect analysis on the word level. In psy-
chology both the Affective Norms for English
Words (ANEW) project as well as the Magel-
lan project focus on collecting human judgements
on affective meanings of words, roughly follow-
ing Osgood?s scheme. In the ANEW project they
collected numerical ratings of pleasure (equivalent
to our term polarity), arousal, and dominance for
1000 English terms (Bradley and Lang, 2006) and
in Magellan they collected cross-cultural affective
meanings (including polarity) in a wide variety of
countries such as the USA, China, Japan, and Ger-
many (Heise, 2001). Both projects concentrate on
collecting a large number of ratings on a large va-
riety of words: there is no principled evaluation of
agreement.
The more linguistically oriented projects of the
General Inquirer (GI) lexicon
2
and the Ap-
praisal framework
3
also provide word lists anno-
tated for affective meanings but judgements seem
to be currently provided by one researcher only.
Especially the General Enquirer which contains
11788 words marked for polarity (1915 positive,
2291 negative and 7582 no-polarity words) seems
to use a relatively ad hoc definition of polarity.
Thus, for example amelioration is marked as no-
polarity whereas improvement is marked as posi-
tive.
The projects mentioned above center on subjec-
tivity analysis on words and therefore are not good
at dealing with subjectivity or polarity-ambiguous
words as explained in the Introduction. Work that
like us concentrates on word senses includes ap-
proaches where the subjectivity labels are automat-
ically assigned such as WordNet-Affect (Strap-
parava and Valitutti, 2004), which is a subset of
WordNet senses with semi-automatically assigned
affective labels (such as emotion, mood or be-
haviour). In a first step, they manually collect
an affective word list and a list of synsets which
contain at least one word in this word list. Fine-
2
Available at http://www.wjh.harvard.edu/ inquirer/
3
Available at http://www.grammatics.com/appraisal/
43
grained affect labels are assigned to these synsets
by the resource developers. Then they automati-
cally expand the lists by employing WordNet re-
lations which they consider to reliably preserve
the involved labels (such as similar-to, antonym,
derived-from, pertains-to, and attribute). Our work
differs from theirs in three respects. First, they
focus on their semi-automatic procedure, whereas
we are interested in human judgements. Second,
they use a finer-grained set of affect labels. Third,
they do not provide agreement results for their an-
notation. Similarly, SentiWordNet
4
is a resource
with automatically determined polarity of word
senses in WordNet (Esuli and Sebastiani, 2006),
produced via bootstrapping from a small manually
determined seed set. Each synset has three scores
assigned, representing the positive, negative and
neutral score respectively. No human annotation
study is conducted.
There are only two human annotation studies
on subjectivity of word senses as far as we are
aware. Firstly, theMicro-WNOp corpus is a list of
about 1000 WordNet synsets annotated by Cerini
et al (2007) for polarity. The raters manually as-
signed a triplet of numerical scores to each sense
which represent the strength of positivity, negativ-
ity, and neutrality respectively. Their work dif-
fers from us in two main aspects. First, they fo-
cus on polarity instead of subjectivity annotation
(see Section 3 for a discussion of the two con-
cepts). Second, they do not use absolute categories
but give a rating between 0 and 1 to each synset?
thus a synset could have a non-zero rating on both
negativity and positivity. They also do not report
on agreement results. Secondly, Wiebe and Mi-
halcea (2006) mark up WordNet senses as subjec-
tive, objective or both with good agreement. How-
ever, we expand their annotation scheme with po-
larity annotation. In addition, we hope to annotate
a larger set of word senses.
3 Human Judgements on Word Sense
Subjectivity and Polarity
We follow Wiebe and Mihalcea (2006) in that
we see subjective expressions as private states
?that are not open to objective observation or ver-
ification? and in that annotators distinguish be-
tween subjective (S), objective (O) and both sub-
jective/objective (B) senses.
4
Available at http://sentiwordnet.isti.cnr.it/
Polarity refers to positive or negative connota-
tions associated with a word or sense. In contrast
to other researchers (Hatzivassiloglou and McKe-
own, 1997; Takamura et al, 2005), we do not see
polarity as a category that is dependent on prior
subjectivity assignment and therefore applicable to
subjective senses only. Whereas there is a depen-
dency in that most subjective senses have a rel-
atively clear polarity, polarity can be attached to
objective words/senses as well. For example, tu-
berculosis is not subjective ? it does not describe
a private state, is objectively verifiable and would
not cause a sentence containing it to carry an opin-
ion, but it does carry negative associations for the
vast majority of people. We allow for the polarity
categories positive (P), negative (N), varying (V)
or no-polarity (NoPol).
Overall we combine these annotations into 7
categories?S:N, S:P, S:V, B, O:N, O:P, and
O:NoPol, which are explained in detail in the sub-
sequent sections. Figure 1 gives an overview of the
hierarchies over all categories.
As can be seen in Figure 1, our annotation
scheme allows for hierarchical annotation, i.e. it is
possible to only annotate for subjectivity or polar-
ity. This can be necessary to achieve higher agree-
ment by merging categories or to concentrate in
specific applications on only one aspect.
3.1 Subjectivity
3.1.1 Subjective Senses
Subjective senses include several categories,
which can be expressed by nouns, verbs, adjec-
tives or adverbs. Firstly, we include emotions.
Secondly, we include judgements, assessments and
evaluations of behaviour as well as aesthetic as-
sessments of individuals, natural objects and arte-
facts. Thirdly, mental states such as doubts, beliefs
and speculations are also covered by our definition.
This grouping follows relatively closely the def-
inition of attitudinal positioning in the Appraisal
scheme (which has, however, only been used on
words, not on word senses before).
These types of subjectivity can be expressed via
direct references to an emotion or mental state (see
Example 5 or 8 below) as well as by expressive
subjective elements (Wiebe and Mihalcea, 2006).
Expressive subjective elements contain judgemen-
tal references to objects or events. Thus, pontifi-
cate in Example 6 below is a reference to a speech
event that always judges it negatively; beautiful as
44
word sense
subjective(S) both(B) objective(O)
negative positive varying/context-depedent(S:V) strong negativeconnotation(O:N) no strongconnotation(O:NoPol) strong positiveconnotation(O:P)(S:N) (S:P)
Figure 1: Overview of the hierarchies over all categories
in Example 7 below is a positive judgement.
(5) angry?feeling or showing anger; ?angry at the
weather; ?angry customers; an angry silence? (emotion)
(6) pontificate?talk in a dogmatic and pompous manner;
?The new professor always pontificates? (assessment of
behaviour)
(7) beautiful?aesthetically pleasing (aesthetic assess-
ment)
(8) doubt, uncertainty, incertitude, dubiety, doubtfulness,
dubiousness?the state of being unsure of something
(mental state)
3.1.2 Objective Senses
Objective senses refer to persons, objects, ac-
tions, events or states without an inherent emotion
or judgement or an expression of a mental state.
Examples are references to individuals via named
entities (see Example 9) or non-judgemental refer-
ences to artefacts, persons, animals, plants, states
or events (see Example 10 and 11). If a sentence
contains an opinion, it is not normally due to the
presence of this word sense and the sense often
expresses objectively verifiable states or events.
Thus, Example 12 is objective as we can verify
whether there is a war going on. In addition, a sen-
tence containing this sense of war does not neces-
sarily express an opinion.
(9) Einstein, Albert Einstein ? physicist born in Germany
who formulated the special theory of relativity and the
general theory of relativity; Einstein also proposed that
light consists of discrete quantized bundles of energy
(later called photons) (1879-1955) (named entity)
(10) lawyer, attorney ? a professional person authorized to
practice law; conducts lawsuits or gives legal advice
(non-judgemental reference to person)
(11) alarm clock, alarm ? a clock that wakes sleeper at preset
time (non-judgemental reference to object)
(12) war, warfare ? the waging of armed conflict against an
enemy; ?thousands of people were killed in the war?
(non-judgemental reference to event)
3.1.3 Both
In rare cases, a sense can be both subjective and
objective (denoted by B). The following are the
two most frequent cases. First, a WordNet sense
might conflate a private state meaning and an ob-
jective meaning of a word in the gloss description.
Thus, in Example 13 we have the objective literal
use of the word tarnish mentioned such as tarnish
the silver, which does not express a private state.
However, it also includes a metaphorical use of
tarnish as in tarnish a reputation, which implicitly
expresses a negative attitude.
(13) tarnish, stain, maculate, sully, defile?make dirty or
spotty, as by exposure to air; also used metaphorically;
?The silver was tarnished by the long exposure to the
air?; ?Her reputation was sullied after the affair with a
married man?
The second case includes the inclusion of near-
synonyms (Edmonds, 1999) which differs on sen-
timent in the same synset list. Thus in Example
14, the term alcoholic is objective as it is not nec-
essarily judgemental, whereas the other words in
the synset such as soaker or souse are normally in-
sults and therefore subjective.
(14) alcoholic, alky, dipsomaniac, boozer, lush, soaker,
souse?a person who drinks alcohol to excess habitu-
ally
3.2 Polarity
3.2.1 Polarity of Subjective Senses
The polarity of a subjective sense can be positive
(Category S:P), negative (S:N), or varying, depen-
dent on context or individual preference (S:V). The
definitions of these three categories are as follows.
? S:P is assigned to private states that express
a positive attitude, emotion or judgement (see
Example 7).
? S:N is assigned to private states that express a
negative attitude, emotion or judgement (see
Example 5, 6 and 8).
? S:V is used for senses where the polarity is
varying by context or user. For example, it is
45
likely that you give an opinion about some-
body if you call him aloof; however, only
context can determine whether this is positive
or negative (see Example 15).
(15) aloof, distant, upstage?remote in manner; ?stood apart
with aloof dignity?; ?a distant smile?; ?he was upstage
with strangers? (S:V)
3.2.2 Polarity of Objective Senses
There are many senses that are objective but
have strong negative or positive connotations. For
example, war describes in many texts an objec-
tive state (?He fought in the last war?) but still
has strong negative connotations. In many (but not
all) cases the negative or positive associations are
mentioned in the WordNet gloss. Therefore, we
can determine three polarity categories for objec-
tive senses:
? O:NoPol Objective with no strong, generally
shared connotations (see Example 9, 10, 11
and 16).
? O:P Objective senses with strong positive
connotations. These refer to senses that do
not describe or express a mental state, emo-
tion or judgement but whose presence in a
text would give it a strong feel-good flavour
(see Example 17).
? O:N Objective senses with strong negative
connotations. These are senses that do not
describe or express an emotion or judgement
but whose presence in a text would give it a
negative flavour (see Example 12). Another
example is (18): you can verify objectively
whether a liquor was diluted, but it is nor-
mally associated negatively.
(16) above?appearing earlier in the same text; ?flaws in the
above interpretation? (O:NoPol)
(17) remedy, curative, cure ? a medicine or therapy that
cures disease or relieve pain (O:P)
(18) adulterate, stretch, dilute, debase?corrupt, debase, or
make impure by adding a foreign or inferior substance;
often by replacing valuable ingredients with inferior
ones; ?adulterate liquor? (O:N)
We only allow positive and negative annotations
for objective senses if we expect strong connota-
tions that are shared among most people (in West-
ern culture). Thus, for example war, diseases and
crimes can relatively safely be predicted to have
shared negative connotations. In contrast, a sense
like the one of alarm clock in Example 11 might
have negative connotations for late risers but it
would be annotated as O:NoPol in our scheme. We
are interested in strong shared connotations as the
presence of such ?loaded? terms can partially in-
dicate bias in a text. In addition, such objective
senses are likely to give rise to figurative subjec-
tive senses (see Example 18).
4 Experiments and Evaluation
This section describes the experimental setup for
our annotation experiments, presents reliability re-
sults and discusses the benefits of the use of a hier-
archical annotation scheme as well as the problems
of bias in the annotation scheme, annotator prefer-
ences and bias in the sense inventory.
4.1 Dataset and Annotation Procedure
The dataset used in our annotation scheme is the
Micro-WNOp corpus
5
, which contains all senses
of 298 words in WordNet 2.0. We used it as it is
representative of WordNet with respect to its part-
of-speech distribution and includes synsets of rel-
atively frequent words, including a wide variety of
subjective senses. It contains 1105 synsets in total,
divided into three groups common (110 synset),
group1 (496 synsets) and group2 (499 synsets).
We used common as the training set for the anno-
tators and tested annotation reliability on group1.
Annotation was performed by two annotators.
Both are fluent English speakers; one is a compu-
tational linguist whereas the other is not in linguis-
tics. All annotation was carried out independently
and without discussion during the annotation pro-
cess. The annotators were furnished with guide-
line annotations with examples for each category.
Annotators saw the full synset, including all syn-
onyms, glosses and examples.
4.2 Agreement Study
Training. The two annotators first annotated the
common group for training. Observed agreement
on the training data is 83.6%, with a kappa (Co-
hen, 1960) of 0.76. Although this looks overall
quite good, several categories are hard to identify,
for example B and S:V, as can be seen in the con-
fusion matrix below (Table 1) with Annotator 1 in
columns and Annotator 2 in the rows.
Testing. Problem cases were discussed between
the annotators and a larger study on group 1 as test
5
Available at http://www.unipv.it/wnop/micrownop.tgz
46
Table 1: Confusion matrix for the training data
B S:N S:P S:V O:NoPol O:N O:P total
B 1 0 0 0 2 0 0 3
S:N 0 13 0 0 0 2 0 15
S:P 0 0 8 1 1 0 0 10
S:V 1 1 0 13 6 0 0 21
O:NoPol 1 0 0 0 50 0 0 51
O:N 0 0 0 0 2 4 0 6
O:P 0 0 1 0 0 0 3 4
total 3 14 9 14 61 6 3 110
data was carried out. Table 2 shows the confusion
matrix for all 7 categories.
Table 2: Confusion matrix on the test set
B S:N S:P S:V O:NoPol O:N O:P total
B 7 2 0 2 0 0 0 11
S:N 0 41 1 0 0 0 0 42
S:P 0 0 65 4 0 0 2 71
S:V 0 0 7 17 3 0 0 27
O:NoPol 9 1 2 6 253 5 8 284
O:N 0 14 0 2 0 25 0 41
O:P 1 0 5 0 1 0 13 20
total 17 58 80 31 257 30 23 496
The observed agreement is 84.9% and the kappa
is 0.77. This is good agreement for a relatively
subjective task. However, there is no improve-
ment over agreement in training although an ad-
ditional clarification phase of the training material
took place between training and testing.
We also computed single category kappa in or-
der to estimate which categories proved the most
difficult. Single category-kappa concentrates on
one target category and conflates all other cate-
gories into one non-target category and measures
agreement between the two resulting categories.
The results showed that S:N (0.80), S:P (0.84)
and O:NoPol (0.86) were highly reliable with less
convincing results for B (0.49), S:V (0.56), O:N
(0.68), and O:P (0.59). B is easily missed dur-
ing annotation (see Example 19), S:V is easily con-
fused with several other categories (Example 20),
whereas O:N is easily confused with O:NoPol and
S:N (Example 21); andO:P is easily confused with
O:NoPol and S:P (Example 22).
(19) antic, joke, prank, trick, caper, put-on?a ludicrous
or grotesque act done for fun and amusement (B vs
O:NoPol)
(20) humble?marked by meekness or modesty; not arro-
gant or prideful; ?a humble apology? (S:V vs S:P)
(21) hot?recently stolen or smuggled; ?hot merchandise?;
?a hot car? (O:N vs O:NoPol)
(22) profit, gain?the advantageous quality of being benefi-
cial (S:P vs O:P)
Our annotation scheme also needs testing on an
even larger data set as a few categories such as B
and O:P occur relatively rarely.
4.3 The Effect of Hierarchical Annotation
As mentioned above, our annotation scheme al-
lows us to consider the subjectivity or polarity dis-
tinction individually, leaving the full categoriza-
tion underspecified.
Subjectivity Distinction Only. For subjectivity
distinctions we collapse S:V, S:P and S:N into a
single label S (subjective) and O:NoPol, O:N and
O:P into a single label O (objective). B remains
unchanged. The resulting confusion matrix on the
test set is in Table 3.
Table 3: Confusion matrix for Subjectivity
B S O total
B 7 4 0 11
S 0 135 5 140
O 10 30 305 345
total 17 169 310 496
Observed agreement is 90.1% and kappa is 0.79.
Single category kappa is 0.49 for B, 0.82 for S and
0.80 for O. As B is a very rare category (less than
5% of items), this is overall an acceptable level
of distinction with excellent reliability for the two
main categories.
Polarity Distinction Only. We collapse O:N
and S:N into a single category N (negative) and
O:P and S:P into P (positive), leaving the other
categories intact. This results in 5 categories B,
S:V/V, NoPol, N and P. The resulting confusion
matrix is in Table 4.
Table 4: Confusion matrix for Polarity
B N P V NoPol total
B 7 2 0 2 0 11
N 0 80 1 2 0 83
P 1 0 85 4 1 91
V 0 0 7 17 3 27
NoPol 9 6 10 6 253 284
total 17 88 103 31 257 496
Observed agreement is 89.1% and kappa is 0.83.
Single category kappa is as follows: B (0.49), N
(0.92), P (0.85), V (0.56), and NoPol (0.86). This
means all categories but B and V (together about
10% of items) are reliably identifiable.
Overall we show that both polarity and sub-
jectivity identification of word senses can be re-
liably annotated and are well-defined tasks for
automatic classification. Specifically the per-
47
centage agreement of about 90% for word sense
polarity/subjectivity identification is substantially
higher than the one of 78% reported in An-
dreeivskaia and Bergler (2006). Agreement for
polarity-only is significantly higher than for the
full annotation scheme, showing the value of hi-
erarchical annotation. We believe hierarchical an-
notation is also appropriate for this task, as sub-
jectivity and polarity are linked but still separate
concepts. Thus, a researcher might want to mainly
focus on explicitly expressed opinions as exempli-
fied by subjectivity, whereas another can also focus
on opinion bias in a text as expressed by loaded
words of positive or negative polarity.
4.4 Bias in Annotation Performance, Sense
Inventory and Annotation Guidelines
Why do annotators assign different labels to some
senses? Three main aspects are responsible for
non-spurious disagreement.
Firstly, individual perspective or bias played a
role. For example, Annotator 2 was more inclined
to give positive or negative polarity labels than An-
notator 1 as can be seen in Table 4, where Anno-
tator 2 assigned 103 positive and 88 negative la-
bels,whereas Annotator 1 assigned only 91 posi-
tive and 83 negative labels.
Secondly, the WordNet sense inventory con-
flates near-synonyms which just differ in sentiment
properties (see Section 3.1.3 and Example 14). Al-
though the labels B and S:V were specifically cre-
ated in the annotation scheme to address this prob-
lem, these cases still proved confusing to annota-
tors and do not readily lead to consistent annota-
tion.
Thirdly, WordNet sometimes includes a conno-
tation bias either in its glosses or in its hierarchical
organization. Here we use the word connotation
bias for the inclusion of connotations that seem
highly controversial. Thus, in Example 23, the
WordNet gloss for Iran evokes negative connota-
tions by mentioning allegations of terrorism.
6
In
Example 24 skinhead is a hyponym of bully, giv-
ing strong negative connotations for all skinheads.
Although the annotation scheme explicitly encour-
ages annotators to disregard especially such con-
troversial connotations as in Example 23 such ex-
amples can still confuse annotators and show that
word sense annotation is to a certain degree depen-
6
Note that this was part of WordNet 2.0 and has been re-
moved in WordNet 2.1.
dent on the sense inventory used.
(23) Iran, Islamic Republic of Iran, Persia?a theocratic is-
lamic republic in the Middle East in western Asia; Iran
was the core of the ancient empire that was known
as Persia until 1935; rich in oil; involved in state-
sponsored terrorism
(24) skinhead ?? bully, tough, hooligan, ruffian, rough-
neck, rowdy, yob, yobo, yobbo
Some of our good reliability performance might
be due to one particular instance of bias in the an-
notation guidelines. We strongly advised annota-
tors to only annotate positive or negative polarity
for objective senses when strong, shared connota-
tions are expected,
7
thereby ?de-individualising?
the task of polarity annotation. This introduces
a bias towards the category NoPol for objective
senses. We also did not allow varying polarity for
objective senses, instructing annotators that such
polarity would be unclear and should be annotated
as NoPol as not being a strong shared connotation.
It can of course be questioned whether the intro-
duction of such a bias is good or not. It helps
agreement but might reduce the usefulness of the
annotation as individual connotations are not an-
notated for objective senses. However, to consider
more individual connotations needs an annotation
effort with a much larger number of annotators to
arrive at a profile of polarity connotations over a
larger population. We leave this for future work.
Our current framework is comprehensive for sub-
jectivity as well as polarity for subjective senses.
4.5 Gold Standard
After discussion between the two annotators, a
gold standard annotation was agreed upon. Our
data set consists of this agreed set as well as the re-
mainder of the Micro-WNOp corpus (group2) an-
notated by one of the annotators alone after agree-
ment was established.
How many words are subjectivity-ambiguous or
polarity-ambiguous, i.e. how much information
do we gain by annotating senses over annotating
words? As the number of senses increases with
word frequency, we expect rare words to be less
likely to be subjectivity-ambiguous than frequent
words. The Micro-WNOp corpus contains rela-
tively frequent words so we will get an overesti-
mation of subjectivity-ambiguous word types from
this corpus, though not necessarily of word tokens.
Of all 298 words, 97 (32.5%) are subjectivity-
ambiguous, a substantial number. Fewer words are
7
See Section 3.2.2 for justification.
48
polarity-ambiguous: only 10 words have at least
one positive and one negatively annotated sense
with a further 44 words having at least one sub-
jective sense with varying polarity (S:V). This sug-
gests that subjective and objective uses of the same
word are more frequent than reverses in emotional
orientation.
5 Comparison to Original Polarity
Annotation (Cerini et al)
We can compare the reliability of our own annota-
tion scheme with the original (polarity) annotation
in the Micro-WNOp corpus. Cerini et al (2007)
do not present agreement figures but as their cor-
pus is publically available we can easily compute
reliability. Recall that each synset has a triplet of
numerical scores between 0 and 1 each: positiv-
ity, negativity and neutrality, which is not explic-
itly annotated but derived as 1 ? (positivity +
negativity). Subjectivity in our sense (existence
of a private state) is not annotated.
The ratings of three annotators are available for
Group 1 and of two annotators for Group 2. We
measured the Pearson correlation coefficient be-
tween each annotator pair for both groups for both
negativity and positivity scoring. As correlation
can be high without necessarily high agreement
on absolute values, we also computed a variant of
kappa useful for numerical ratings, namely alpha
(Artstein and Poesio, 2005), which gives weight
to degrees of disagreement. Thus, a disagreement
between two scores would be weighted as the ab-
solute value of score1 ? score2. The results are
listed in Table 5.
Table 5: Reliability of original annotation on
Micro-WNOp
dataset raters score type correlation alpha
Group 1 1 and 2 negative 83.7 64.9
Group 1 1 and 3 negative 86.4 71.8
Group 1 2 and 3 negative 82.5 56.9
Group 1 1 and 2 positive 80.5 60.9
Group 1 1 and 3 positive 87.8 74.9
Group 1 2 and 3 positive 78.2 57.5
Group 2 1 and 2 negative 95.9 90.7
Group 2 1 and 2 positive 92.2 84.9
Correlation between the annotators is high.
However, Rater 2 (in Group1) still behaves differ-
ently from the other two raters, giving consistently
higher or lower scores overall, leading to low al-
pha. Thus, we can conclude that Group 2 is much
more reliably annotated than Group 1 and that es-
pecially Rater 2 in Group 1 is an outlier in this
(small) set of raters. This also shows that work
with several annotators is valuable and should be
conducted for our scheme as well.
6 Conclusion and Future Work
We elicit human judgements on the subjectivity
and polarity of word senses. To the best of our
knowledge, this is the first such annotation scheme
for both categories. We detail the definitions for
each category and measure the reliability of the an-
notation. The experimental results show that when
using all 7 categories, only 3 categories (S:N, S:P,
and O:NoPol) are reliable while the reliability of
the other 4 categories is not high. We also show
that this is improved by the virtue of hierarchical
annotation and that the general tasks of subjectivity
and polarity annotation on word senses are there-
fore well-defined. Moreover, we also discuss the
effect of different kinds of bias on our approach.
In future we will refine the guidelines for the
more difficult categories, including more detailed
advice on how to deal with sense inventory bias.
We will also perform larger-scale annotation exer-
cises with more annotators as the latter is necessary
to deal with more individualised polarity connota-
tions. In addition, we will use the data to test learn-
ing methods for the automatic detection of subjec-
tivity and polarity properties of word senses.
References
Andreevskaia, Alina and Sabine Bergler. 2006. Min-
ing WordNet for Fuzzy Sentiment: Sentiment Tag
Extraction from WordNet Glosses. Proceedings of
EACL?06.
Artstein, Ron and Massimo Poesio. 2005.
Kappa
3
=alpha(or beta). Technical Report CSM-
437, University of Essex.
Bradley, Margaret and Peter Lang. 1999. Affective
Norms for EnglishWords (ANEW): Stimuli, Instruc-
tion Manual and Affective Ratings Technical report
C-1, the Center for Research in Psychophysiology,
University of Florida. .
Cerini, Sabrina, Valentina Compagnoni, Alice Demon-
tis, Maicol Formentelli, and Caterina Gandini. 2007.
Micro-WNOp: A Gold Standard for the Evaluation
of Automatically Compiled Lexical Resources for
Opinion Mining. Language resources and linguis-
tic theory: Typology, second language acquisition,
English linguistics.
49
Cohen, Jacob. 1960. A Coefficient of Agreement
for Nominal Scales. Educational and Psychological
Measurement, Vol.20, No.1.
Das, Sanjiv and Mike Chen. 2001. Yahoo! for Ama-
zon: Extracting Market Sentiment from Stock Mes-
sage Boards. Proceedings of APFA?01.
Edmonds, Philip. 1999. Semantic Representations
Of Near-Synonyms For Automatic Lexical Choice.
PhD thesis, University of Toronto.
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource for
Opinion Mining. Proceedings of LREC?06.
Hatzivassiloglou, Vasileios and Kathleen McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. Proceedings of ACL?97.
Heise, David. 2001. Project Magellan: Collecting
Cross-culture Affective Meanings via the Internet.
Electronic Journal of Sociology.
Osgood, Charles, William May, and Murray Miron.
1975. Cross-cultural Universals of Affective Mean-
ing. University of Illinois Press.
Osgood, Charles, George Suci, and Percy Tannenbaum.
1957. The Measurment of Meaning. University of
Illinois Press.
Strapparava, Carlo and Alessandro Valitutti. 2004.
WordNet-Affect: an Affective Extension of Word-
Net. Proceedings of LREC?04.
Takamura, Hiroya, Takashi Inui, and Manabu Oku-
mura. 2005. Extracting Semantic Orientations of
Words using Spin Model. Proceedings of ACL?05.
Wiebe, Janyce and Rada Micalcea. 2006. Word Sense
and Subjectivity. Proceedings of ACL?06.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating Expressions of Opinions and
Emotions in Language. Language Resources and
Evaluation.
50
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 10?19,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Measuring Comparability of Documents in Non-Parallel Corpora for
Efficient Extraction of (Semi-)Parallel Translation Equivalents
Fangzhong Su
Centre for Translation Studies
University Of Leeds
LS2 9JT, Leeds, UK
smlfs@leeds.ac.uk
Bogdan Babych
Centre for Translation Studies
University Of Leeds
LS2 9JT, Leeds, UK
b.babych@leeds.ac.uk
Abstract
In this paper we present and evaluate three
approaches to measure comparability of
documents in non-parallel corpora. We de-
velop a task-oriented definition of compa-
rability, based on the performance of auto-
matic extraction of translation equivalents
from the documents aligned by the pro-
posed metrics, which formalises intuitive
definitions of comparability for machine
translation research. We demonstrate ap-
plication of our metrics for the task of
automatic extraction of parallel and semi-
parallel translation equivalents and discuss
how these resources can be used in the
frameworks of statistical and rule-based
machine translation.
1 Introduction
Parallel corpora have been extensively exploited
in different ways in machine translation (MT)
? both in Statistical (SMT) and more recently,
in Rule-Based (RBMT) architectures: in SMT
aligned parallel resources are used for building
translation phrase tables and calculating transla-
tion probabilities; and in RBMT, they are used
for automatically building bilingual dictionaries
of translation equivalents and automatically deriv-
ing bilingual mappings for frequent structural pat-
terns. However, large parallel resources are not
always available, especially for under-resourced
languages or narrow domains. Therefore, in re-
cent years, the use of cross-lingual comparable
corpora has attracted considerable attention in
the MT community (Sharoff et al, 2006; Fung
and Cheung, 2004a; Munteanu and Marcu, 2005;
Babych et al, 2008).
Most of the applications of comparable cor-
pora focus on discovering translation equivalents
to support machine translation, such as bilingual
lexicon extraction (Rapp, 1995; Rapp, 1999;
Morin et al, 2007; Yu and Tsujii, 2009; Li and
Gaussier, 2010; Prachasson and Fung, 2011), par-
allel phrase extraction (Munteanu and Marcu,
2006), and parallel sentence extraction (Fung and
Cheung, 2004b; Munteanu and Marcu, 2005;
Munteanu et al, 2004; Smith et al, 2010).
Comparability between documents is often un-
derstood as belonging to the same subject domain,
genre or text type, so this definition relies on these
vague linguistic concepts. The problem with this
definition then is that it cannot be exactly bench-
marked, since it becomes hard to relate automated
measures of comparability to such inexact and un-
measurable linguistic concepts. Research on com-
parable corpora needs not only good measures for
comparability, but also a clearer, technologically-
grounded and quantifiable definition of compara-
bility in the first place.
In this paper we relate comparability to use-
fulness of comparable texts for MT. In particu-
lar, we propose a performance-based definition of
comparability, as the possibility to extract parallel
or quasi-parallel translation equivalents ? words,
phrases and sentences which are translations of
each other. This definition directly relates compa-
rability to texts? potential to improve the quality
of MT by adding extracted phrases to phrase ta-
bles, training corpus or dictionaries. It also can be
quantified as the rate of successful extraction of
translation equivalents by automated tools, such
as proposed in Munteanu and Marcu (2006).
Still, successful detection of translation equiv-
alents from comparable corpora very much de-
10
pends on the quality of these corpora, specifically
on the degree of their textual equivalence and suc-
cessful alignment on various text units. There-
fore, the goal of this work is to provide compa-
rability metrics which can reliably identify cross-
lingual comparable documents from raw corpora
crawled from the Web, and characterize the de-
gree of their similarity, which enriches compara-
ble corpora with the document alignment infor-
mation, filters out documents that are not useful
and eventually leads to extraction of good-quality
translation equivalents from the corpora.
To achieve this goal, we need to define a
scale to assess comparability qualitatively, met-
rics to measure comparability quantitatively, and
the sources to get comparable corpora from. In
this work, we directly characterize comparability
by how useful comparable corpora are for the task
of detecting translation equivalents in them, and
ultimately to machine translation. We focus on
document-level comparability, and use three cat-
egories for qualitative definition of comparability
levels, defined in terms of granularity for possible
alignment:
? Parallel: Traditional parallel texts that are
translations of each other or approximate
translations with minor variations, which can
be aligned on the sentence level.
? Strongly-comparable: Texts that talk about
the same event or subject, but in different
languages. For example, international news
about oil spill in the Gulf of Mexico, or
linked articles in Wikipedia about the same
topic. These documents can be aligned on
the document level on the basis of their ori-
gin.
? Weakly-comparable: Texts in the same sub-
ject domain which describe different events.
For example, customer reviews about hotel
and restaurant in London. These documents
do not have an independent alignment across
languages, but sets of texts can be aligned
on the basis of belonging to the same subject
domain or sub-domain.
In this paper, we present three different ap-
proaches to measure the comparability of cross-
lingual (especially under-resourced languages)
comparable documents: a lexical mapping based
approach, a keyword based approach, and a ma-
chine translation based approach. The experimen-
tal results show that all of them can effectively
predict the comparability levels of the compared
document pairs. We then further investigate the
applicability of the proposed metrics by measur-
ing their impact on the task of parallel phrase ex-
traction from comparable corpora. It turns out
that, higher comparability level predicted by the
metrics consistently lead to more number of paral-
lel phrase extracted from comparable documents.
Thus, the metrics can help select more compara-
ble document pairs to improve the performance of
parallel phrase extraction.
The remainder of this paper is organized as fol-
lows. Section 2 discusses previous work. Section
3 introduces our comparability metrics. Section
4 presents the experimental results and evaluation.
Section 5 describes the application of the metrics.
Section 6 discusses the pros and cons of the pro-
posed metrics, followed by conclusions and future
work in Section 7.
2 Related Work
The term ?comparability?, which is the key con-
cept in this work, applies to the level of corpora,
documents and sub-document units. However, so
far there is no widely accepted definition of com-
parability. For example, there is no agreement on
the degree of similarity that documents in com-
parable corpora should have or on the criteria for
measuring comparability. Also, most of the work
that performs translation equivalent extraction in
comparable corpora usually assumes that the cor-
pora they use are reliably comparable and focuses
on the design of efficient extraction algorithms.
Therefore, there has been very little literature dis-
cussing the characteristics of comparable corpora
(Maia, 2003). In this section, we introduce some
representative work which tackles comparability
metrics.
Some studies (Sharoff, 2007; Maia, 2003;
McEnery and Xiao, 2007) analyse comparability
by assessing corpus composition, such as struc-
tural criteria (e.g., format and size), and linguistic
criteria (e.g., topic, domain, and genre). Kilgarriff
and Rose (1998) measure similarity and homo-
geneity between monolingual corpora. They gen-
erate word frequency list from each corpus and
then apply ?2 statistic on the most frequent n (e.g.,
500) words of the compared corpora.
11
The work which deals with comparability
measures in cross-lingual comparable corpora is
closer to our work. Saralegi et al (2008) measure
the degree of comparability of comparable cor-
pora (English and Basque) according to the dis-
tribution of topics and publication dates of docu-
ments. They compute content similarity for all the
document pairs between two corpora. These sim-
ilarity scores are then input as parameters for the
EMD (Earth Mover?s Distance) distance measure,
which is employed to calculate the global com-
patibility of the corpora. Munteanu and Marcu
(2005; 2006) select more comparable document
pairs in a cross-lingual information retrieval based
manner by using a toolkit called Lemur1. The
retrieved document pairs then serve as input for
the tasks of parallel sentence and sub-sentence ex-
traction. Smith et al (2010) treat Wikipedia as
a comparable corpus and use ?interwiki? links to
identify aligned comparable document pairs for
the task of parallel sentence extraction. Li and
Gaussier (2010) propose a comparability met-
ric which can be applied at both document level
and corpus level and use it as a measure to se-
lect more comparable texts from other external
sources into the original corpora for bilingual lex-
icon extraction. The metric measures the propor-
tion of words in the source language corpus trans-
lated in the target language corpus by looking up
a bilingual dictionary. They evaluate the met-
ric on the rich-resourced English-French language
pair, thus good dictionary resources are available.
However, this is not the case for under-resourced
languages in which reliable language resources
such as machine-readable bilingual dictionaries
with broad word coverage or word lemmatizers
might be not publicly available.
3 Comparability Metrics
To measure the comparability degree of document
pairs in different languages, we need to translate
the texts or map lexical items from the source lan-
guage into the target languages so that we can
compare them within the same language. Usually
this can be done by using bilingual dictionaries
(Rapp, 1999; Li and Gaussier, 2010; Prachasson
and Fung, 2011) or existing machine translation
tools. Based on this process, in this section we
present three different approaches to measure the
1Available at http://www.lemurproject.org/
comparability of comparable documents.
3.1 Lexical mapping based metric
It is straightforward that we expect a bilingual dic-
tionary can be used for lexical mapping between a
language pair. However, unlike the language pairs
in which both languages are rich-resourced (e.g.,
English-French, or English-Spanish) and dictio-
nary resources are relatively easy to obtain, it is
likely that bilingual dictionaries with good word
coverage are not publicly available for under-
resourced languages (e.g., English-Slovenian, or
English-Lithuanian). In order to address this
problem, we automatically construct dictionaries
by using word alignment on large-scale parallel
corpora (e.g., Europarl and JRC-Acquis2).
Specifically, GIZA++ toolkit (Och and Ney,
2000) with default setting is used for word align-
ment on the JRC-Acquis parallel corpora (Stein-
berger et al, 2006). The aligned word pairs to-
gether with the alignment probabilities are then
converted into dictionary entries. For example,
in Estonian-English language pair, the alignment
example ?kompanii company 0.625? in the word
alignment table means the Estonian word ?kom-
panii? can be translated as (or aligned with) the
English candidate word ?company? with a prob-
ability of 0.625. In the dictionary, the transla-
tion candidates are ranked by translation proba-
bility in descending order. Note that the dictio-
nary collects inflectional form of words, but not
only base form of words. This is because the dic-
tionary is directly generated from the word align-
ment results and no further word lemmatization is
applied.
Using the resulting dictionary, we then per-
form lexical mapping in a word-for-word map-
ping strategy. We scan each word in the source
language texts to check if it occurs in the dic-
tionary entries. If so, the first translation candi-
date are recorded as the corresponding mapping
word. If there are more than one translation can-
didate, the second candidate will also be kept as
the mapping result if its translation probability is
higher than 0.33. For non-English and English
2The JRC-Acquis covers 22 European languages and
provides large-scale parallel corpora for all the 231 language
pairs.
3From the manual inspection on the word alignment re-
sults, we find that if the alignment probability is higher than
0.3, it is more reliable.
12
language pair, the non-English texts are mapped
into English. If both languages are non-English
(e.g., Greek-Romanian), we use English as a pivot
langauge and map both the source and target
language texts into English4. Due to the lack
of reliable linguistic resources in non-English
languages, mapping texts from non-English lan-
guage into English can avoid language process-
ing in non-English texts and allows us to make
use of the rich resources in English for further
text processing, such as stop-word filtering and
word lemmatization5. Finally, cosine similarity
measure is applied to compute the comparability
strength of the compared document pairs.
3.2 Keyword based metric
The lexical mapping based metric takes all the
words in the text into account for comparability
measure, but if we only retain a small number of
representative words (keywords) and discard all
the other less informative words in each docu-
ment, can we judge the comparability of a doc-
ument pair by comparing these words? Our in-
tuition is that, if two document share more key-
words, they should be more comparable. To
validate this, we then perform keyword extrac-
tion by using a simple TFIDF based approach,
which has been shown effective for keyword or
keyphrase extraction from the texts (Frank et al,
1999; Hulth, 2003; Liu et al, 2009).
More specifically, the keyword based metric
can be described as below. First, similar to the
lexical mapping based metric, bilingual dictionar-
ies are used to map non-English texts into En-
glish. Thus, only the English resources are ap-
plied for stop-word filtering and word lemmatiza-
tion, which are useful text preprocessing steps for
keyword extraction. We then use TFIDF to mea-
sure the weight of words in the document and rank
the words by their TFIDF weights in descending
order. The top n (e.g., 30) words are extracted
as keywords to represent the document. Finally,
the comparability of each document pair is deter-
mined by applying cosine similarity to their key-
4Generally in JRC-Acquis, the size of parallel corpora
for most of non-English langauge pairs is much smaller than
that of language pairs which contain English. Therefore, the
resulting bilingual dictionaries which contain English have
better word coverage as they have many more dictionary en-
tries.
5We use WordNet (Fellbaum, 1998) for word lemmatiza-
tion.
word lists.
3.3 Machine translation based metrics
Bilingual dictionary is used for word-for-word
translation in the lexical mapping based metric
and words which do not occur in the dictionary
will be omitted. Thus, the mapping result is like
a list of isolated words and information such as
word order, syntactic structure and named entities
can not be preserved. Therefore, in order to im-
prove the text translation quality, we turn to the
state-of-the-art SMT systems.
In practice, we use Microsoft translation API6
to translate texts in under-resourced languages
(e.g, Lithuanian and Slovenian) into English and
then explore several features for comparability
metric design, which are listed as below.
? Lexical feature: Lemmatized bag-of-word
representation of each document after stop-
word filtering. Lexical similarity (denoted
by WL) of each document pair is then ob-
tained by applying cosine measure to the lex-
ical feature.
? Structure feature: We approximate it by
the number of content words (adjectives, ad-
verbs, nouns, verbs and proper nouns) and
the number of sentences in each document,
denoted by CD and SD respectively. The in-
tuition is that, if two documents are highly
comparable, their number of content words
and their document length should be similar.
The structure similarity (denoted by WS) of
two documentsD1 andD2 is defined as bel-
low.
WS = 0.5 ? (CD1/CD2)+ 0.5 ? (SD1/SD2)
suppose that CD1<=CD2, and SD1<=SD2.
? Keyword feature: Top-20 words (ranked by
TFIDF weight) of each document. keyword
similarity (denoted by WK) of two docu-
ments is also measured by cosine.
? Named entity feature: Named entities of
each document. If more named entities co-
occur in two documents, they are very likely
to talk about the same event or subject and
6Available at http://code.google.com/p/microsoft-
translator-java-api/
13
thus should be more comparable. We use
Stanford named entity recognizer7 to extract
named entities from the texts (Finkel et al,
2005). Again, cosine is then applied to mea-
sure the similarity of named entities (denoted
by WN ) between a document pair.
We then combine these four different types of
score in an ensemble manner. Specifically, a
weighted average strategy is applied: each indi-
vidual score is associated with a constant weight,
indicating the relative confidence (importance) of
the corresponding type of score. The overall com-
parability score (denoted by SC) of a document
pair is thus computed as below:
SC = ? ?WL + ? ?WS + ? ?WK + ? ?WN
where ?, ?, ?, and ? ? [0, 1], and ?+?+?+? =
1. SC should be a value between 0 and 1, and
larger SC value indicates higher comparability
level.
4 Experiment and Evaluation
4.1 Data source
To investigate the reliability of the proposed
comparability metrics, we perform experiments
for 6 language pairs which contain under-
resoured languages: German-English (DE-EN),
Estonian-English (ET-EN), Lithuanian-English
(LT-EN), Latvian-English (LV-EN), Slovenian-
English (SL-EN) and Greek-Romanian (EL-RO).
A comparable corpus is collected for each lan-
guage pair. Based on the definition of compa-
rability levels (see Section 1), human annota-
tors fluent in both languages then manually anno-
tated the comparability degree (parallel, strongly-
comparable, and weakly-comparable) at the doc-
ument level. Hence, these bilingual comparable
corpora are used as gold standard for experiments.
The data distribution for each language pair, i.e.,
number of document pairs in each comparability
level, is given in Table 1.
4.2 Experimental results
We adopt a simple method for evaluation. For
each language pair, we compute the average
scores for all the document pairs in the same com-
parability level, and compare them to the gold
7Available at http://nlp.stanford.edu/software/CRF-
NER.shtml
Language
pair
#document
pair
parallel strongly-
comparable
weakly-
comparable
DE-EN 1286 531 715 40
ET-EN 1648 182 987 479
LT-EN 1177 347 509 321
LV-EN 1252 184 558 510
SL-EN 1795 532 302 961
EL-RO 485 38 365 82
Table 1: Data distribution of gold standard corpora
standard comparability labels. In addition, in or-
der to better reveal the relation between the scores
obtained from the proposed metrics and compara-
bility levels, we also measure the Pearson correla-
tion between them8. For the keyword based met-
ric, top 30 keywords are extracted from each text
for experiment. For the machine translation based
metric, we empirically set ? = 0.5, ? = ? = 0.2,
and ? = 0.1. This is based on the assumption
that, lexical feature can best characterize the com-
parability given the good translation quality pro-
vided by the powerful MT system, while keyword
and named entity features are also better indica-
tors of comparability than the simple document
length information.
The results for the lexical mapping based met-
ric, the keyword based metric and the machine
translation based metric are listed in Table 2, 3,
and 4, respectively.
Language
pair
parallel strongly-
comparable
weakly-
comparable
correlation
DE-EN 0.545 0.476 0.182 0.941
ET-EN 0.553 0.381 0.228 0.999
LT-EN 0.545 0.461 0.225 0.964
LV-EN 0.625 0.494 0.179 0.973
SL-EN 0.535 0.456 0.314 0.987
EL-RO 0.342 0.131 0.090 0.932
Table 2: Average comparability scores for lexical map-
ping based metric
Overall, from the average scores for each
comparability level presented in Table 2, 3,
and 4, we can see that, the scores obtained
from the three comparability metrics can reli-
8For correlation measure, we use numerical calibration
to different comparability degrees: ?Parallel?, ?strongly-
comparable? and ?weakly-comparable? are converted as 3,
2, and 1, respectively. The correlation is then computed
between the numerical comparability levels and the cor-
responding average comparability scores automatically de-
rived from the metrics.
14
Language
pair
parallel strongly-
comparable
weakly-
comparable
correlation
DE-EN 0.526 0.486 0.084 0.941
ET-EN 0.502 0.345 0.184 0.990
LT-EN 0.485 0.420 0.202 0.954
LV-EN 0.590 0.448 0.124 0.975
SL-EN 0.551 0.505 0.292 0.937
EL-RO 0.210 0.110 0.031 0.997
Table 3: Average comparability scores for keyword
based metric
Language
pair
parallel strongly-
comparable
weakly-
comparable
correlation
DE-EN 0.912 0.622 0.326 0.999
ET-EN 0.765 0.547 0.310 0.999
LT-EN 0.755 0.613 0.308 0.984
LV-EN 0.770 0.627 0.236 0.966
SL-EN 0.779 0.582 0.373 0.988
EL-RO 0.863 0.446 0.214 0.988
Table 4: Average comparability scores for machine
translation based metric
ably reflect the comparability levels across dif-
ferent language pairs, as the average scores
for higher comparable levels are always sig-
nificantly larger than those of lower compara-
ble levels, namely SC(parallel)>SC(strongly-
comparable)>SC(weakly-comparable). In addi-
tion, in all the three metrics, the Pearson correla-
tion scores are very high (over 0.93) across dif-
ferent language pairs, which indicate that there
is strong correlation between the comparability
scores obtained from the metrics and the corre-
sponding comparability level.
Moreover, from the comparison of Table 2, 3,
and 4, we also have several other findings. Firstly,
the performance of keyword based metric (see
Table 3) is comparable to the lexical mapping
based metric (see Table 2) as their comparability
scores for the corresponding comparability levels
are similar. This means it is reasonable to deter-
mine the comparability level by only comparing a
small number of keywords of the texts. Secondly,
the scores obtained from the machine translation
based metric (see Table 4) are significantly higher
than those in both the lexical mapping based met-
ric and the keyword based metric. Clearly, this
is due to the advantages of using the state-of-the-
art MT system. In comparison to the approach
of using dictionary for word-for-word mapping,
it can provide much better text translation which
allows detecting more proportion of lexical over-
lapping and mining more useful features in the
translated texts. Thirdly, in the lexical mapping
based metric and keyword based metric, we can
also see that, although the average scores for EL-
RO (both under-resourced languages) conform to
the comparability levels, they are much lower than
those of the other 5 language pairs. The reason
is that, the size of the parallel corpora in JRC-
Acquis for these 5 language pairs are significantly
larger (over 1 million parallel sentences) than that
of EL-EN, RO-EN9, and EL-RO, thus the result-
ing dictionaries of these 5 language pairs also con-
tain many more dictionary entries.
5 Application
The experiments in Section 4 confirm the reli-
ability of the proposed metrics. The compara-
bility metrics are thus useful for collecting high-
quality comparable corpora, as they can help filter
out weakly comparable or non-comparable doc-
ument pairs from the raw crawled corpora. But
are they also useful for other NLP tasks, such as
translation equivalent detection from comparable
corpora? In this section, we further measure the
impact of the metrics on parallel phrase extraction
(PPE) from comparable corpora. Our intuition is
that, if document pairs are assigned higher com-
parability scores by the metrics, they should be
more comparable and thus more parallel phrases
can be extracted from them.
The algorithm of parallel phrase extraction,
which develops the approached presented in
Munteanu and Marcu (2006), uses lexical over-
lap and structural matching measures (Ion, 2012).
Taking a list of bilingual comparable document
pairs as input, the extraction algorithm involves
the following steps.
1. Split the source and target language docu-
ments into phrases.
2. Compute the degree of parallelism for each
candidate pair of phrases by using the bilin-
gual dictionary generated from GIZA++
(base dictionary), and retain all the phrase
pairs with a score larger than a predefined
parallelism threshold.
9Remember that in our experiment, English is used as the
pivot language for non-English langauge pairs.
15
3. Apply GIZA++ to the retained phrase pairs
to detect new dictionary entries and add them
to the base dictionary.
4. Repeat Step 2 and 3 for several times (empir-
ically set at 5) by using the augmented dic-
tionary, and output the detected phrase pairs.
Phrases which are extracted by this algorithm
are frequently not exact translation equivalents.
Below we give some English-German examples
of extracted equivalents with their corresponding
alignment scores:
1. But a successful mission ? seiner u?beraus
erfolgreichen Mission abgebremst ?
0.815501989333333
2. Former President Jimmy Carter ? Der
ehemalige US-Pra?sident Jimmy Carter ?
0.69708324976825
3. on the Korean Peninsula ? auf der koreanis-
chen Halbinsel ? 0.8677432145
4. across the Muslim world ? mit der muslim-
ischen Welt ermo?glichen ? 0.893330864
5. to join the United Nations ? der Weg
in die Vereinten Nationen offensteht ?
0.397418711927629
Even though some of the extracted phrases are
not exact translation equivalents, they may still
be useful resources both for SMT and RBMT if
these phrases are passed through an extra pre-
processing stage, of if the engines are modified
specifically to work with semi-parallel translation
equivalents extracted from comparable texts. We
address this issue in the discussion section (see
Section 6).
For evaluation, we measure how the metrics af-
fect the performance of parallel phrase extraction
algorithm on 5 language pairs (DE-EN, ET-EN,
LT-EN, LV-EN, and SL-EN). A large raw compa-
rable corpus for each language pair was crawled
from the Web, and the metrics were then applied
to assign comparability scores to all the docu-
ment pairs in each corpus. For each language pair,
we set three different intervals based on the com-
parability score (SC) and randomly select 500
document pairs in each interval for evaluation.
For the MT based metric, the three intervals are
(1) 0.1<=SC<0.3, (2) 0.3<=SC<0.5, and (3)
SC>=0.5. For the lexical mapping based metric
and keyword based metric, since their scores are
lower than those of the MT based metric for each
comparability level, we set three lower intervals at
(1) 0.1<=SC<0.2, (2) 0.2<=SC<0.4, and (3)
SC>=0.4. The experiment focuses on counting
the number of extracted parallel phrases with par-
allelism score>=0.410, and computes the average
number of extracted phrases per 100000 words
(the sum of words in the source and target lan-
guage documents) for each interval. In addition,
the Pearson correlation measure is also applied to
measure the correlation between the interval11 of
comparability scores and the number of extracted
parallel phrases. The results which summarize the
impact of the three metrics to the performance of
parallel phrase extraction are listed in Table 5, 6,
and 7, respectively.
Language
pair
0.1<=
SC<0.2
0.2<=
SC<0.4
SC>=0.4 correlation
DE-EN 728 1434 2510 0.993
ET-EN 313 631 1166 0.989
LT-EN 258 419 894 0.962
LV-EN 470 859 1900 0.967
SL-EN 393 946 2220 0.975
Table 5: Impact of the lexical mapping based metric to
parallel phrase extraction
Language
pair
0.1<=
SC<0.2
0.2<=
SC<0.4
SC>=0.4 correlation
DE-EN 1007 1340 2151 0.972
ET-EN 438 650 1050 0.984
LT-EN 306 442 765 0.973
LV-EN 600 966 1722 0.980
SL-EN 715 1026 1854 0.967
Table 6: Impact of the keyword based metric to parallel
phrase extraction
From Table 5, 6, and 7, we can see that
for all the 5 language pairs, based on the aver-
age number of extracted aligned phrases, clearly
we have interval (3)>(2)>(1). In other words, in
any of the three metrics, a higher comparability
level always leads to significantly more number
10A manual evaluation of a small set of extracted data
shows that parallel phrases with parallelism score>=0.4 are
more reliable.
11For the purpose of correlation measure, the three inter-
vals are numerically calibrated as ?1?, ?2?, and ?3?, respec-
tively.
16
Language
pair
0.1<=
SC<0.3
0.3<=
SC<0.5
SC>=0.5 correlation
DE-EN 861 1547 2552 0.996
ET-EN 448 883 1251 0.999
LT-EN 293 483 1070 0.959
LV-EN 589 1072 2037 0.982
SL-EN 560 1151 2421 0.979
Table 7: Impact of the machine translation based met-
ric to parallel phrase extraction
of aligned phrases extracted from the comparable
documents. Moreover, although the lexical map-
ping based metric and the keyword based metric
produce lower comparability scores than the MT
based metric (see Section 4), they have similar
impact to the task of parallel phrase extraction.
This means, the comparability score itself does
not matter much, as long as the metrics are re-
liable and proper thresholds are set for different
metrics.
In all the three metrics, the Pearson correla-
tion scores are very close to 1 for all the language
pairs, which indicate that the intervals of compa-
rability scores obtained from the metrics are in
line with the performance of equivalent extrac-
tion algorithm. Therefore, in order to extract more
parallel phrases (or other translation equivalents)
from comparable corpora, we can try to improve
the corpus comparability by applying the compa-
rability metrics beforehand to add highly compa-
rable document pairs in the corpora.
6 Discussion
We have presented three different approaches to
measure comparability at the document level. In
this section, we will analyze the advantages and
limitations of the proposed metrics, and the feasi-
bility of using semi-parallel equivalents in MT.
6.1 Pros and cons of the metrics
Using bilingual dictionary for lexical mapping is
simple and fast. However, as it adopts the word-
for-word mapping strategy and out-of-vocabulary
(OOV) words are omitted, the linguistic structure
of the original texts is badly hurt after mapping.
Thus, apart from lexical information, it is diffi-
cult to explore more useful features for the com-
parability metrics. The TFIDF based keyword ex-
traction approach allows us to select more repre-
sentative words and prune a large amount of less
informative words from the texts. The keywords
are usually relevant to subject and domain terms,
which is quite useful in judging the comparabil-
ity of two documents. Both the lexical mapping
based approach and the keyword based approach
use dictionary for lexical translation, thus rely on
the availability and completeness of the dictionary
resources or large scale parallel corpora.
For the machine translation based metric, it
provides much better text translation than the
dictionary-based approach so that the comparabil-
ity of two document can be better revealed from
the richer lexical information and other useful
features, such as named entities. However, the
text translation process is expensive, as it depends
on the availability of the powerful MT systems12
and takes much longer than the simple dictionary
based translation.
In addition, we use a translation strategy of
translating texts from under-resourced (or less-
resourced) languages into rich-resourced lan-
guage. In case that both languages are under-
resourced languages, English is used as the pivot
langauge for translation. This can compensate the
shortage of the linguistic resources in the under-
resourced languages and take advantages of vari-
ous resources in the rich-resourced languages.
6.2 Using semi-parallel equivalents in MT
systems
We note that modern SMT and RBMT sys-
tems take maximal advantage of strictly parallel
phrases, but they still do not use full potential
of the semi-parallel translation equivalents, of the
type that is illustrated in the application section
(see Section 5). Such resources, even though they
are not exact equivalents contain useful informa-
tion which is not used by the systems.
In particular, the modern decoders do not work
with under-specified phrases in phrase tables, and
do not work with factored semantic features. For
example, the phrase:
But a successful mission ? seiner u?beraus er-
folgreichen Mission abgebremst
The English side contains the word but, which
pre-supposes contrast, and on the Greman side
words u?beraus erfolgreichen (?generally success-
ful?) and abgebremst (?slowed down?) ? which
taken together exemplify a contrast, since they
12Alternatively, we can also train MT systems for text
translation by using the available SMT toolkits (e.g., Moses)
on large scale parallel corpora.
17
have different semantic prosodies. In this example
the semantic feature of contrast can be extracted
and reused in other contexts. However, this would
require the development of a new generation of
decoders or rule-based systems which can suc-
cessfully identify and reuse such subtle semantic
features.
7 Conclusion and Future work
The success of extracting good-quality translation
equivalents from comparable corpora to improve
machine translation performance highly depends
on ?how comparable? the used corpora are. In this
paper, we propose three different comparability
measures at the document level. The experiments
show that all the three approaches can effectively
determine the comparability levels of comparable
document pairs. We also further investigate the
impact of the metrics on the task of parallel phrase
extraction from comparable corpora. It turns out
that higher comparability scores always lead to
significantly more parallel phrases extracted from
comparable documents. Since better quality of
comparable corpora should have better applica-
bility, our metrics can be applied to select highly
comparable document pairs for the tasks of trans-
lation equivalent extraction.
In the future work, we will conduct more com-
prehensive evaluation of the metrics by capturing
its impact to the performance of machine transla-
tion systems with extended phrase tables derived
from comparable corpora.
Acknowledgments
We thank Radu Ion at RACAI for providing us
the toolkit of parallel phrase extraction, and the
three anonymous reviewers for valuable com-
ments. This work is supported by the EU funded
ACCURAT project (FP7-ICT-2009-4-248347) at
the Centre for Translation Studies, University of
Leeds.
References
Bogdan Babych, Serge Sharoff and Anthony Hartley.
2008. Generalising Lexical Translation Strategies
for MT Using Comparable Corpora. Proceedings
of LREC 2008, Marrakech, Morocco.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. Proceedings of
COLING 2002, Taipei, Taiwan.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA,
USA.
Jenny Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. Proceedings of ACL 2005, University of
Michigan, Ann Arbor, USA.
Eibe Frank, Gordon Paynter and Ian Witten. 1999.
Domain-specific keyphrase extraction. Proceedings
of IJCAI 1999, Stockholm, Sweden.
Pascale Fung and Percy Cheung. 2004a. Mining very
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and EM. Proceedings
of EMNLP 2004, Barcelona, Spain.
Pascale Fung and Percy Cheung. 2004b. Multi-level
bootstrapping for extracting parallel sentences from
a quasicomparable corpus. Proceedings of COL-
ING 2004, Geneva, Switzerland.
Anette Hulth. 2003. Improved Automatic Keyword
Extraction Given More Linguistic Knowledge. Pro-
ceedings of EMNLP 2003, Sapporo, Japan.
Radu Ion. 2012. PEXACC: A Parallel Data Mining
Algorithm from Comparable Corpora. Proceedings
of LREC 2012, Istanbul, Turkey.
Adam Kilgarriff and Tony Rose. 1998. Measures for
corpus similarity and homogeneity. Proceedings of
EMNLP 1998, Granada, Spain.
Bo Li and Eric Gaussier. 2010. Improving cor-
pus comparability for bilingual lexicon extraction
from comparable corpora. Proceedings of COL-
ING 2010, Beijing, China.
Feifan Liu, Deana Pennell, Fei Liu and Yang Liu.
2009. Unsupervised Approaches for Automatic
Keyword Extraction Using Meeting Transcripts.
Proceedings of NAACL 2009, Boulder, Colorado,
USA.
Belinda Maia. 2003. What are comparable corpora?
Proceedings of the Corpus Linguistics workshop on
Multilingual Corpora: Linguistic requirements and
technical perspectives, 2003, Lancaster, U.K.
Anthony McEnery and Zhonghua Xiao. 2007. Par-
allel and comparable corpora? In Incorporating
Corpora: Translation and the Linguist. Translating
Europe. Multilingual Matters, Clevedon, UK.
Emmanuel Morin, Beatrice Daille, Korchi Takeuchi
and Kyo Kageura. 2007. Bilingual terminology
mining ? using brain, not brawn comparable cor-
pora. Proceedings of ACL 2007, Prague, Czech Re-
public.
Dragos Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. Proceedings of ACL 2006, Syn-
dey, Australia.
Dragos Munteanu and Daniel Marcu. 2005. Improv-
ing machine translation performance by exploiting
non-parallel corpora. Computational Linguistics,
31(4): 477-504.
18
Dragos Munteanu, Alexander Fraser and Daniel
Marcu. 2004. Improved machine translation
performance via parallel sentence extraction from
comparable corpora. Proceedings of HLT-NAACL
2004, Boston, USA.
Franz Och and Hermann Ney. 2000. Improved Statis-
tical Alignment Models. Proceedings of ACL 2000,
Hongkong, China.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
Word Translation Extraction from Aligned Compa-
rable Documents. Proceedings of ACL-HLT 2011,
Portland, USA.
Reinhard Rapp. 1995. Identifying Word Translation
in Non-Parallel Texts. Proceedings of ACL 1995,
Cambridge, Massachusetts, USA.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. Proceedings of ACL 1999, College
Park, Maryland, USA.
Xabier Saralegi, Inaki Vicente and Antton Gurrutxaga.
2008. Automatic Extraction of Bilingual Terms
from Comparable Corpora in a Popular Science
Domain. Proceedings of the Workshop on Compa-
rable Corpora, LREC 2008, Marrakech, Morocco.
Serge Sharoff. 2007. Classifying Web corpora into
domain and genre using automatic feature identifi-
cation. Proceedings of 3rd Web as Corpus Work-
shop, Louvain-la-Neuve, Belgium.
Serge Sharoff, Bogdan Babych and Anthony Hartley.
2006. Using Comparable Corpora to Solve Prob-
lems Difficult for Human Translators. Proceedings
of ACL 2006, Syndey, Australia.
Jason Smith, Chris Quirk and Kristina Toutanova.
2010. Extracting Parallel Sentences from Compa-
rable Corpora using Document Level Alignment.
Proceedings of NAACL 2010, Los Angeles, USA.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat and Dan Tufis. 2006. The JRC-
Acquis: A multilingual aligned parallel corpus
with 20+ languages. Proceedings of LREC 2006,
Genoa, Italy.
Kun Yu and Junichi Tsujii. 2009. Extracting bilingual
dictionary from comparable corpora with depen-
dency heterogeneity. Proceedings of HLT-NAACL
2009, Boulder, Colorado, USA.
19
