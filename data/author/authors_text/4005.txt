Recognizing Expressions of Commonsense Psychology in English Text
Andrew Gordon, Abe Kazemzadeh, Anish Nair and Milena Petrova
University of Southern California
Los Angeles, CA 90089 USA
gordon@ict.usc.edu, {kazemzad, anair, petrova}@usc.edu
Abstract
Many applications of natural language
processing technologies involve analyzing
texts that concern the psychological states
and processes of people, including their
beliefs, goals, predictions, explanations,
and plans. In this paper, we describe our
efforts to create a robust, large-scale lexi-
cal-semantic resource for the recognition
and classification of expressions of com-
monsense psychology in English Text.
We achieve high levels of precision and
recall by hand-authoring sets of local
grammars for commonsense psychology
concepts, and show that this approach can
achieve classification performance greater
than that obtained by using machine
learning techniques. We demonstrate the
utility of this resource for large-scale cor-
pus analysis by identifying references to
adversarial and competitive goals in po-
litical speeches throughout U.S. history.
1 Commonsense Psychology in Language
Across all text genres it is common to find words
and phrases that refer to the mental states of people
(their beliefs, goals, plans, emotions, etc.) and their
mental processes (remembering, imagining, priori-
tizing, problem solving). These mental states and
processes are among the broad range of concepts
that people reason about every day as part of their
commonsense understanding of human psychol-
ogy. Commonsense psychology has been studied
in many fields, sometimes using the terms Folk
psychology or Theory of Mind, as both a set of be-
liefs that people have about the mind and as a set
of everyday reasoning abilities.
Within the field of computational linguistics,
the study of commonsense psychology has not re-
ceived special attention, and is generally viewed as
just one of the many conceptual areas that must be
addressed in building large-scale lexical-semantic
resources for language processing. Although there
have been a number of projects that have included
concepts of commonsense psychology as part of a
larger lexical-semantic resource, e.g. the Berkeley
FrameNet Project (Baker et al, 1998), none have
attempted to achieve a high degree of breadth or
depth over the sorts of expressions that people use
to refer to mental states and processes.
The lack of a large-scale resource for the analy-
sis of language for commonsense psychological
concepts is seen as a barrier to the development of
a range of potential computer applications that in-
volve text analysis, including the following:
?  Natural language interfaces to mixed-initiative
planning systems (Ferguson & Allen, 1993;
Traum, 1993) require the ability to map ex-
pressions of users? beliefs, goals, and plans
(among other commonsense psychology con-
cepts) onto formalizations that can be ma-
nipulated by automated planning algorithms.
?  Automated question answering systems
(Voorhees & Buckland, 2002) require the abil-
ity to tag and index text corpora with the rele-
vant commonsense psychology concepts in
order to handle questions concerning the be-
liefs, expectations, and intentions of people.
? Research efforts within the field of psychology
that employ automated corpus analysis tech-
niques to investigate developmental and men-
tal illness impacts on language production, e.g.
Reboul & Sabatier?s (2001) study of the dis-
course of schizophrenic patients, require the
ability to identify all references to certain psy-
chological concepts in order to draw statistical
comparisons.
In order to enable future applications, we un-
dertook a new effort to meet this need for a lin-
guistic resource. This paper describes our efforts in
building a large-scale lexical-semantic resource for
automated processing of natural language text
about mental states and processes. Our aim was to
build a system that would analyze natural language
text and recognize, with high precision and recall,
every expression therein related to commonsense
psychology, even in the face of an extremely broad
range of surface forms. Each recognized expres-
sion would be tagged with an appropriate concept
from a broad set of those that participate in our
commonsense psychological theories.
Section 2 demonstrates the utility of a lexical-
semantic resource of commonsense psychology in
automated corpus analysis through a study of the
changes in mental state expressions over the course
of over 200 years of U.S. Presidential State-of-the-
Union Addresses. Section 3 of this paper describes
the methodology that we followed to create this
resource, which involved the hand authoring of
local grammars on a large scale. Section 4 de-
scribes a set of evaluations to determine the per-
formance levels that these local grammars could
achieve and to compare these levels to those of
machine learning approaches. Section 5 concludes
this paper with a discussion of the relative merits
of this approach to the creation of lexical-semantic
resources as compared to other approaches.
2 Applications to corpus analysis
One of the primary applications of a lexical-
semantic resource for commonsense psychology is
toward the automated analysis of large text cor-
pora. The research value of identifying common-
sense psychology expressions has been
demonstrated in work on children?s language use,
where researchers have manually annotated large
text corpora consisting of parent/child discourse
transcripts (Barsch & Wellman, 1995) and chil-
dren?s storybooks (Dyer et al, 2000). While these
previous studies have yielded interesting results,
they required enormous amounts of human effort
to manually annotate texts. In this section we aim
to show how a lexical-semantic resource for com-
monsense psychology can be used to automate this
annotation task, with an example not from the do-
main of children?s language acquisition, but rather
political discourse.
We conducted a study to determine how politi-
cal speeches have been tailored over the course of
U.S. history throughout changing climates of mili-
tary action. Specifically, we wondered if politi-
cians were more likely to talk about goals having
to do with conflict, competition, and aggression
during wartime than in peacetime. In order to
automatically recognize references to goals of this
sort in text, we used a set of local grammars
authored using the methodology described in Sec-
tion 3 of this paper. The corpus we selected to ap-
ply these concept recognizers was the U.S. State of
the Union Addresses from 1790 to 2003. The rea-
sons for choosing this particular text corpus were
its uniform distribution over time and its easy
availability in electronic form from Project Guten-
berg (www.gutenberg. net). Our set of local gram-
mars identified 4290 references to these goals in
this text corpus, the vast majority of them begin
references to goals of an adversarial nature (rather
than competitive). Examples of the references that
were identified include the following:
? They sought to use the rights and privileges
they had obtained in the United Nations, to
frustrate its purposes [adversarial-goal] and
cut down its powers as an effective agent of
world progress. (Truman, 1953)
? The nearer we come to vanquishing [adver-
sarial-goal] our enemies the more we inevita-
bly become conscious of differences among
the victors. (Roosevelt, 1945)
? Men have vied [competitive-goal] with each
other to do their part and do it well. (Wilson,
1918)
? I will submit to Congress comprehensive leg-
islation to strengthen our hand in combating
[adversarial-goal] terrorists. (Clinton, 1995)
Figure 1 summarizes the results of applying our
local grammars for adversarial and competitive
goals to the U.S. State of the Union Addresses. For
each year, the value that is plotted represents the
number of references to these concepts that were
identified per 100 words in the address. The inter-
esting result of this analysis is that references to
adversarial and competitive goals in this corpus
increase in frequency in a pattern that directly cor-
responds to the major military conflicts that the
U.S. has participated in throughout its history.
Each numbered peak in Figure 1 corresponds to
a period in which the U.S. was involved in a mili-
tary conflict. These are: 1) 1813, War of 1812, US
and Britain; 2) 1847, Mexican American War; 3)
1864, Civil War; 4) 1898, Spanish American War;
5) 1917, World War I; 6) 1943, World War II; 7)
1952, Korean War; 8) 1966, Vietnam War; 9)
1991, Gulf War; 10) 2002, War on Terrorism.
The wide applicability of a lexical-semantic re-
source for commonsense psychology will require
that the identified concepts are well defined and
are of broad enough scope to be relevant to a wide
range of tasks. Additionally, such a resource must
achieve high levels of accuracy in identifying these
concepts in natural language text. The remainder of
this paper describes our efforts in authoring and
evaluating such a resource.
3 Authoring recognition rules
The first challenge in building any lexical-semantic
resource is to identify the concepts that are to be
recognized in text and used as tags for indexing or
markup. For expressions of commonsense psy-
chology, these concepts must describe the broad
scope of people?s mental states and processes. An
ontology of commonsense psychology with a high
degree of both breadth and depth is described by
Gordon (2002). In this work, 635 commonsense
psychology concepts were identified through an
analysis of the representational requirements of a
corpus of 372 planning strategies collected from 10
real-world planning domains. These concepts were
grouped into 30 conceptual areas, corresponding to
various reasoning functions, and full formal mod-
els of each of these conceptual areas are being
authored to support automated inference about
commonsense psychology (Gordon & Hobbs,
2003). We adopted this conceptual framework in
our current project because of the broad scope of
the concepts in this ontology and its potential for
future integration into computational reasoning
systems.
The full list of the 30 concept areas identified is
as follows: 1) Managing knowledge, 2) Similarity
comparison, 3) Memory retrieval, 4) Emotions, 5)
Explanations, 6) World envisionment, 7) Execu-
tion envisionment, 8) Causes of failure, 9) Man-
aging expectations, 10) Other agent reasoning, 11)
Threat detection, 12) Goals, 13) Goal themes, 14)
Goal management, 15) Plans, 16) Plan elements,
17) Planning modalities, 18) Planning goals, 19)
Plan construction, 20) Plan adaptation, 21) Design,
22) Decisions, 23) Scheduling, 24) Monitoring, 25)
Execution modalities, 26) Execution control, 27)
Repetitive execution, 28) Plan following, 29) Ob-
servation of execution, and 30) Body interaction.
Our aim for this lexical-semantic resource was
to develop a system that could automatically iden-
tify every expression of commonsense psychology
in English text, and assign to them a tag corre-
sponding to one of the 635 concepts in this ontol-
ogy. For example, the following passage (from
William Makepeace Thackeray?s 1848 novel,
Vanity Fair) illustrates the format of the output of
this system, where references to commonsense
psychology concepts are underlined and followed
by a tag indicating their specific concept type de-
limited by square brackets:
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
1
7
9
0
1
7
9
8
1
8
0
6
1
8
1
4
1
8
2
2
1
8
3
0
1
8
3
8
1
8
4
6
1
8
5
4
1
8
6
2
1
8
7
0
1
8
7
8
1
8
8
6
1
8
9
8
1
9
0
6
1
9
1
4
1
9
2
2
1
9
3
0
1
9
3
9
1
9
4
7
1
9
5
5
1
9
6
3
1
9
7
1
1
9
7
9
1
9
8
7
1
9
9
7
Year
F
e
a
t
u
r
e
s
 
p
e
r
 
1
0
0
 
w
o
r
d
s
1
2
7
6
5
4
3
8
9 10
Figure 1.  Adversarial and competitive goals in the U.S. State of the Union Addresses from 1790-2003
Perhaps [partially-justified-proposition] she
had mentioned the fact [proposition] already to
Rebecca, but that young lady did not appear to
[partially-justified-proposition] have remem-
bered it [memory-retrieval]; indeed, vowed and
protested that she expected [add-expectation] to
see a number of Amelia's nephews and nieces.
She was quite disappointed [disappointment-
emotion] that Mr. Sedley was not married; she
was sure [justified-proposition] Amelia had said
he was, and she doted so on [liking-emotion] lit-
tle children.
The approach that we took was to author (by
hand) a set of local grammars that could be used to
identify each concept. For this task we utilized the
Intex Corpus Processor software developed by the
Laboratoire d'Automatique Documentaire et Lin-
guistique (LADL) of the University of Paris 7 (Sil-
berztein, 1999). This software allowed us to author
a set of local grammars using a graphical user in-
terface, producing lexical/syntactic structures that
can be compiled into finite-state transducers. To
simplify the authoring of these local grammars,
Intex includes a large-coverage English dictionary
compiled by Blandine Courtois, allowing us to
specify them at a level that generalized over noun
and verb forms. For example, there are a variety of
ways of expressing in English the concept of reaf-
firming a belief that is already held, as exemplified
in the following sentences:
1) The finding was confirmed by the new
data. 2) She told the truth, corroborating his
story. 3) He reaffirms his love for her. 4) We
need to verify the claim. 5) Make sure it is true.
Although the verbs in these sentences differ in
tense, the dictionaries in Intex allowed us to recog-
nize each using the following simple description:
(<confirm> by | <corroborate> | <reaffirm> |
<verify> | <make> sure)
While constructing local grammars for each of
the concepts in the original ontology of common-
sense psychology, we identified several conceptual
distinctions that were made in language that were
not expressed in the specific concepts that Gordon
had identified. For example, the original ontology
included only three concepts in the conceptual area
of memory retrieval (the sparsest of the 30 areas),
namely memory, memory cue, and memory re-
trieval. English expressions such as ?to forget? and
?repressed memory? could not be easily mapped
directly to one of these three concepts, which
prompted us to elaborate the original sets of con-
cepts to accommodate these and other distinctions
made in language. In the case of the conceptual
area of memory retrieval, a total of twelve unique
concepts were necessary to achieve coverage over
the distinctions evident in English.
These local grammars were authored one con-
ceptual area at a time. At the time of the writing of
this paper, our group had completed 6 of the origi-
nal 30 commonsense psychology conceptual areas.
The remainder of this paper focuses on the first 4
of the 6 areas that were completed, which were
evaluated to determine the recall and precision per-
formance of our hand-authored rules. These four
areas are Managing knowledge, Memory, Expla-
nations, and Similarity judgments. Figure 2 pre-
sents each of these four areas with a single
fabricated example of an English expression for
each of the final set of concepts. Local grammars
for the two additional conceptual areas, Goals (20
concepts) and Goal management (17 concepts),
were authored using the same approach as the oth-
ers, but were not completed in time to be included
in our performance evaluation.
After authoring these local grammars using the
Intex Corpus Processor, finite-state transducers
were compiled for each commonsense psychology
concept in each of the different conceptual areas.
To simplify the application of these transducers to
text corpora and to aid in their evaluation, trans-
ducers for individual concepts were combined into
a single finite state machine (one for each concep-
tual area). By examining the number of states and
transitions in the compiled finite state graphs, some
indication of their relative size can be given for the
four conceptual areas that we evaluated: Managing
knowledge (348 states / 932 transitions), Memory
(203 / 725), Explanations (208 / 530), and Similar-
ity judgments (121 / 500).
4 Performance evaluation
In order to evaluate the utility of our set of hand-
authored local grammars, we conducted a study of
their precision and recall performance. In order to
calculate the performance levels, it was first neces-
sary to create a test corpus that contained refer-
ences to the sorts of commonsense psychological
concepts that our rules were designed to recognize.
To accomplish this, we administered a survey to
1. Managing knowledge (37 concepts)
He?s got a logical mind (managing-knowledge-ability). She?s very gullible (bias-toward-belief). He?s skepti-
cal by nature (bias-toward-disbelief). It is the truth (true). That is completely false (false). We need to know
whether it is true or false (truth-value). His claim was bizarre (proposition). I believe what you are saying (be-
lief). I didn?t know about that (unknown). I used to think like you do (revealed-incorrect-belief). The assumption
was widespread (assumption). There is no reason to think that (unjustified-proposition). There is some evidence
you are right (partially-justified-proposition). The fact is well established (justified-proposition). As a rule, stu-
dents are generally bright (inference). The conclusion could not be otherwise (consequence). What was the rea-
son for your suspicion (justification)? That isn?t a good reason (poor-justification). Your argument is circular
(circular-justification). One of these things must be false (contradiction). His wisdom is vast (knowledge). He
knew all about history (knowledge-domain). I know something about plumbing (partial-knowledge-domain).
He?s got a lot of real-world experience (world-knowledge). He understands the theory behind it (world-model-
knowledge). That is just common sense (shared-knowledge). I?m willing to believe that (add-belief). I stopped
believing it after a while (remove-belief). I assumed you were coming (add-assumption). You can?t make that
assumption here (remove-assumption). Let?s see what follows from that (check-inferences). Disregard the con-
sequences of the assumption (ignore-inference). I tried not to think about it (suppress-inferences). I concluded
that one of them must be wrong (realize-contradiction). I realized he must have been there (realize). I can?t think
straight (knowledge-management-failure). It just confirms what I knew all along (reaffirm-belief).
2. Memory (12 concepts)
He has a good memory (memory-ability). It was one of his fondest memories (memory-item). He blocked out
the memory of the tempestuous relationship (repressed-memory-item). He memorized the words of the song
(memory-storage). She remembered the last time it rained (memory-retrieval). I forgot my locker combination
(memory-retrieval-failure). He repressed the memories of his abusive father (memory-repression). The widow
was reminded of her late husband (reminding). He kept the ticket stub as a memento (memory-cue). He intended
to call his brother on his birthday (schedule-plan).  He remembered to set the alarm before he fell asleep (sched-
uled-plan-retrieval). I forgot to take out the trash (scheduled-plan-retrieval-failure).
3. Explanations (20 concepts)
He?s good at coming up with explanations (explanation-ability). The cause was clear (cause). Nobody knew
how it had happened (mystery). There were still some holes in his account (explanation-criteria). It gave us the
explanation we were looking for (explanation). It was a plausible explanation (candidate-explanation). It was
the best explanation I could think of (best-candidate-explanation). There were many contributing factors (fac-
tor). I came up with an explanation (explain). Let?s figure out why it was so (attempt-to-explain). He came up
with a reasonable explanation (generate-candidate-explanation). We need to consider all of the possible expla-
nations (assess-candidate-explanations). That is the explanation he went with (adopt-explanation). We failed to
come up with an explanation (explanation-failure). I can?t think of anything that could have caused it (explana-
tion-generation-failure). None of these explanations account for the facts (explanation-satisfaction-failure).
Your account must be wrong (unsatisfying-explanation). I prefer non-religious explanations (explanation-
preference). You should always look for scientific explanations (add-explanation-preference). We?re not going
to look at all possible explanations (remove-explanation-preference).
4. Similarity judgments (13 concepts)
She?s good at picking out things that are different (similarity-comparison-ability). Look at the similarities
between the two (make-comparison). He saw that they were the same at an abstract level (draw-analogy). She
could see the pattern unfolding (find-pattern). It depends on what basis you use for comparison (comparison-
metric). They have that in common (same-characteristic). They differ in that regard (different-characteristic). If
a tree were a person, its leaves would correspond to fingers (analogical-mapping). The pattern in the rug was
intricate (pattern). They are very much alike (similar). It is completely different (dissimilar). It was an analogous
example (analogous).
Figure 2. Example sentences referring to 92 concepts in 4 areas of commonsense psychology
collect novel sentences that could be used for this
purpose.
This survey was administered over the course
of one day to anonymous adult volunteers who
stopped by a table that we had set up on our uni-
versity?s campus. We instructed the survey taker to
author 3 sentences that included words or phrases
related to a given concept, and 3 sentences that
they felt did not contain any such references. Each
survey taker was asked to generate these 6 sen-
tences for each of the 4 concept areas that we were
evaluating, described on the survey in the follow-
ing manner:
? Managing knowledge: Anything about the
knowledge, assumptions, or beliefs that people
have in their mind
? Memory: When people remember things, for-
get things, or are reminded of things
? Explanations: When people come up with pos-
sible explanations for unknown causes
? Similarity judgments: When people find simi-
larities or differences in things
A total of 99 people volunteered to take our
survey, resulting in a corpus of 297 positive and
297 negative sentences for each conceptual area,
with a few exceptions due to incomplete surveys.
Using this survey data, we calculated the preci-
sion and recall performance of our hand-authored
local grammars. Every sentence that had at least
one concept detected for the corresponding concept
area was treated as a ?hit?. Table 1 presents the
precision and recall performance for each concept
area. The results show that the precision of our
system is very high, with marginal recall perform-
ance.
The low recall scores raised a concern over the
quality of our test data. In reviewing the sentences
that were collected, it was apparent that some sur-
vey participants were not able to complete the task
as we had specified. To improve the validity of the
test data, we enlisted six volunteers (native English
speakers not members of our development team) to
judge whether or not each sentence in the corpus
was produced according to the instructions. The
corpus of sentences was divided evenly among
these six raters, and each sentence that the rater
judged as not satisfying the instructions was fil-
tered from the data set. In addition, each rater also
judged half of the sentences given to a different
rater in order to compute the degree of inter-rater
agreement for this filtering task. After filtering
sentences from the corpus, a second preci-
sion/recall evaluation was performed. Table 2 pre-
sents the results of our hand-authored local
grammars on the filtered data set, and lists the in-
ter-rater agreement for each conceptual area among
our six raters. The results show that the system
achieves a high level of precision, and the recall
performance is much better than earlier indicated.
The performance of our hand-authored local
grammars was then compared to the performance
that could be obtained using more traditional ma-
chine-learning approaches. In these comparisons,
the recognition of commonsense psychology con-
cepts was treated as a classification problem,
where the task was to distinguish between positive
Concept area Correct Hits
(a)
Wrong hits
(b)
Total positive
sentences (c)
Total negative
sentences
Precision
(a/(a+b))
Recall
(a/c)
Managing knowledge 205 16 297 297 92.76% 69.02%
Memory 240 4 297 297 98.36% 80.80%
Explanations 126 7 296 296 94.73% 42.56%
Similarity judgments 178 18 296 297 90.81% 60.13%
749 45 1186 1187 94.33% 63.15%
Table 1. Precision and recall results on the unfiltered data set
Concept area Inter-rater
agreement (K)
Correct
Hits (a)
Wrong
hits (b)
Total positive
sentences (c)
Total negative
 sentences
Precision
(a/(a+b))
Recall
(a/c)
Managing knowledge 0.5636 141 12 168 259 92.15% 83.92%
Memory 0.8069 209 0 221 290 100% 94.57%
Explanations 0.7138 83 5 120 290 94.21% 69.16%
Similarity judgments 0.6551 136 12 189 284 91.89% 71.95%
0.6805 569 29 698 1123 95.15% 81.51%
Table 2. Precision and recall results on the filtered data set, with inter-rater agreement on filtering
and negative sentences for any given concept area.
Sentences in the filtered data sets were used as
training instances, and feature vectors for each
sentence were composed of word-level unigram
and bi-gram features, using no stop-lists and by
ignoring punctuation and case. By using a toolkit
of machine learning algorithms (Witten & Frank,
1999), we were able to compare the performance
of a wide range of different techniques, including
Na?ve Bayes, C4.5 rule induction, and Support
Vector Machines, through stratified cross-
validation (10-fold) of the training data. The high-
est performance levels were achieved using a se-
quential minimal optimization algorithm for
training a support vector classifier using polyno-
mial kernels (Platt, 1998). These performance re-
sults are presented in Table 3. The percentage
correctness of classification (Pa) of our hand-
authored local grammars (column A) was higher
than could be attained using this machine-learning
approach (column B) in three out of the four con-
cept areas.
We then conducted an additional study to de-
termine if the two approaches (hand-authored local
grammars and machine learning) could be com-
plimentary. The concepts that are recognized by
our hand-authored rules could be conceived as ad-
ditional bimodal features for use in machine
learning algorithms. We constructed an additional
set of support vector machine classifiers trained on
the filtered data set that included these additional
concept-level features in the feature vector of each
instance along side the existing unigram and bi-
gram features. Performance of these enhanced
classifiers, also obtained through stratified cross-
validation (10-fold), are also reported in Table 3 as
well (column C). The results show that these en-
hanced classifiers perform at a level that is the
greater of that of each independent approach.
5 Discussion
The most significant challenge facing developers
of large-scale lexical-semantic resources is coming
to some agreement on the way that natural lan-
guage can be mapped onto specific concepts. This
challenge is particularly evident in consideration of
our survey data and subsequent filtering. The
abilities that people have in producing and recog-
nizing sentences containing related words or
phrases differed significantly across concept areas.
While raters could agree on what constitutes a
sentence containing an expression about memory
(Kappa=.8069), the agreement on expressions of
managing knowledge is much lower than we
would hope for (Kappa=.5636). We would expect
much greater inter-rater agreement if we had
trained our six raters for the filtering task, that is,
described exactly which concepts we were looking
for and gave them examples of how these concepts
can be realized in English text. However, this ap-
proach would have invalidated our performance
results on the filtered data set, as the task of the
raters would be biased toward identifying exam-
ples that our system would likely perform well on
rather than identifying references to concepts of
commonsense psychology.
Our inter-rater agreement concern is indicative
of a larger problem in the construction of large-
scale lexical-semantic resources. The deeper we
delve into the meaning of natural language, the less
we are likely to find strong agreement among un-
trained people concerning the particular concepts
that are expressed in any given text. Even with
lexical-semantic resources about commonsense
knowledge (e.g. commonsense psychology), finer
distinctions in meaning will require the efforts of
trained knowledge engineers to successfully map
between language and concepts. While this will
certainly create a problem for future preci-
A. Hand authored local
grammars
B. SVM with word level
features
C. SVM with word and
concept features
Concept area Pa K Pa K Pa K
Managing knowledge 90.86% 0.8148 86.0789% 0.6974 89.5592% 0.7757
Memory 97.65% 0.8973 93.5922% 0.8678 97.4757% 0.9483
Explanations 89.75% 0.7027 85.9564% 0.6212 89.3462% 0.7186
Similarity judgments 86.25% 0.7706 92.4528% 0.8409 92.0335% 0.8309
Table 3. Percent agreement (Pa) and Kappa statistics (K) for classification using hand-authored local
grammars (A), SVMs with word features (B), and SVMs with word and concept features (C)
sion/recall performance evaluations, the concern is
even more serious for other methodologies that
rely on large amounts of hand-tagged text data to
create the recognition rules in the first place. We
expect that this problem will become more evident
as projects using algorithms to induce local gram-
mars from manually-tagged corpora, such as the
Berkeley FrameNet efforts (Baker et al, 1998),
broaden and deepen their encodings in conceptual
areas that are more abstract (e.g. commonsense
psychology).
The approach that we have taken in our re-
search does not offer a solution to the growing
problem of evaluating lexical-semantic resources.
However, by hand-authoring local grammars for
specific concepts rather than inducing them from
tagged text, we have demonstrated a successful
methodology for creating lexical-semantic re-
sources with a high degree of conceptual breadth
and depth. By employing linguistic and knowledge
engineering skills in a combined manner we have
been able to make strong ontological commitments
about the meaning of an important portion of the
English language. We have demonstrated that the
precision and recall performance of this approach
is high, achieving classification performance
greater than that of standard machine-learning
techniques. Furthermore, we have shown that
hand-authored local grammars can be used to
identify concepts that can be easily combined with
word-level features (e.g. unigrams, bi-grams) for
integration into statistical natural language proc-
essing systems. Our early exploration of the appli-
cation of this work for corpus analysis (U.S. State
of the Union Addresses) has produced interesting
results, and we expect that the continued develop-
ment of this resource will be important to the suc-
cess of future corpus analysis and human-computer
interaction projects.
Acknowledgments
This paper was developed in part with funds from
the U.S. Army Research Institute for the Behav-
ioral and Social Sciences under ARO contract
number DAAD 19-99-D-0046. Any opinions,
findings and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of the Depart-
ment of the Army.
References
Baker, C., Fillmore, C., & Lowe, J. (1998) The Ber-
keley FrameNet project. in Proceedings of the
COLING-ACL, Montreal, Canada.
Bartsch, K. & Wellman, H. (1995) Children talk about
the mind. New York: Oxford University Press.
Dyer, J., Shatz, M., & Wellman, H. (2000) Young chil-
dren?s storybooks as a source of mental state infor-
mation. Cognitive Development 15:17-37.
Ferguson, G. & Allen, J. (1993) Cooperative Plan Rea-
soning for Dialogue Systems, in AAAI-93 Fall Sym-
posium on Human-Computer Collaboration:
Reconciling Theory, Synthesizing Practice, AAAI
Technical Report FS-93-05. Menlo Park, CA: AAAI
Press.
Gordon, A. (2002) The Theory of Mind in Strategy
Representations. 24th Annual Meeting of the Cogni-
tive Science Society. Mahwah, NJ: Lawrence Erl-
baum Associates.
Gordon, A. & Hobbs (2003) Coverage and competency
in formal theories: A commonsense theory of mem-
ory. AAAI Spring Symposium on Formal Theories of
Commonsense knowledge, March 24-26, Stanford.
Platt, J. (1998). Fast Training of Support Vector Ma-
chines using Sequential Minimal Optimization. In B.
Sch?lkopf, C. Burges, and A. Smola (eds.) Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA: MIT Press.
Reboul A., Sabatier P., No?l-Jorand M-C. (2001) Le
discours des schizophr?nes: une ?tude de cas. Revue
fran?aise de Psychiatrie et de Psychologie M?dicale,
49, pp 6-11.
Silberztein, M. (1999) Text Indexing with INTEX.
Computers and the Humanities 33(3).
Traum, D. (1993) Mental state in the TRAINS-92 dia-
logue manager. In Working Notes of the AAAI Spring
Symposium on Reasoning about Mental States: For-
mal Theories and Applications, pages 143-149, 1993.
Menlo Park, CA: AAAI Press.
Voorhees, E. & Buckland, L. (2002) The Eleventh Text
REtrieval Conference (TREC 2002). Washington,
DC: Department of Commerce, National Institute of
Standards and Technology.
Witten, I. & Frank, E. (1999) Data Mining: Practical
Machine Learning Tools and Techniques with Java
Implementations. Morgan Kaufman.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 115?120,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A System for Real-time Twitter Sentiment Analysis of 2012 U.S. Presidential Election Cycle 
  Hao Wang*, Dogan Can**, Abe Kazemzadeh**,  Fran?ois Bar* and Shrikanth Narayanan** Annenberg Innovation Laboratory (AIL)* Signal Analysis and Interpretation Laboratory (SAIL)** University of Southern California, Los Angeles, CA {haowang@, dogancan@, kazemzad@, fbar@, shri@sipi}.usc.edu       Abstract This paper describes a system for real-time analysis of public sentiment toward presidential candidates in the 2012 U.S. election as expressed on Twitter, a micro-blogging service. Twitter has become a central site where people express their opinions and views on political parties and candidates. Emerging events or news are often followed almost instantly by a burst in Twitter volume, providing a unique opportunity to gauge the relation between expressed public sentiment and electoral events. In addition, sentiment analysis can help explore how these events affect public opinion. While traditional content analysis takes days or weeks to complete, the system demonstrated here analyzes sentiment in the entire Twitter traffic about the election, delivering results instantly and continuously. It offers the public, the media, politicians and scholars a new and timely perspective on the dynamics of the electoral process and public opinion. 
1 Introduction Social media platforms have become an important site for political conversations throughout the world. In the year leading up to the November 2012 presidential election in the United States, we 
have developed a tool for real-time analysis of sentiment expressed through Twitter, a micro-blogging service, toward the incumbent President, Barack Obama, and the nine republican challengers - four of whom remain in the running as of this writing. With this analysis, we seek to explore whether Twitter provides insights into the unfolding of the campaigns and indications of shifts in public opinion. Twitter allows users to post tweets, messages of up to 140 characters, on its social network. Twitter usage is growing rapidly. The company reports over 100 million active users worldwide, together sending over 250 million tweets each day (Twitter, 2012). It was actively used by 13% of on-line American adults as of May 2011, up from 8% a year prior (Pew Research Center, 2011). More than two thirds of U.S. congress members have created a Twitter account and many are actively using Twitter to reach their constituents (Lassen & Brown, 2010; TweetCongress, 2012). Since October 12, 2012, we have gathered over 36 million tweets about the 2012 U.S. presidential candidates, a quarter million per day on average.  During one of the key political events, the Dec 15, 2011 primary debate in Iowa, we collected more than half a million relevant tweets in just a few hours. This kind of ?big data? vastly outpaces the capacity of traditional content analysis approaches, calling for novel computational approaches.  Most work to date has focused on post-facto analysis of tweets, with results coming days or even months after the collection time. However, 
115
because tweets are short and easy to send, they lend themselves to quick and dynamic expression of instant reactions to current events. We expect automated real-time sentiment analysis of this user-generated data can provide fast indications of changes in opinion, showing for example how an audience reacts to particular candidate?s statements during a political debate. The system we present here, along with the dashboards displaying analysis results with drill-down ability, is precisely aimed at generating real-time insights as events unfold. Beyond the sheer scale of the task and the need to keep up with a rapid flow of tweets, we had to address two additional issues. First, the vernacular used on Twitter differs significantly from common language and we have trained our sentiment model on its idiosyncrasies. Second, tweets in general, and political tweets in particular, tend to be quite sarcastic, presenting significant challenges for computer models (Gonz?lez-Ib??ez et al, 2011). We will present our approaches to these issues in a separate publication. Here, we focus on presenting the overall system and the visualization dashboards we have built. In section 2, we begin with a review of related work; we then turn in section 3 to a description of our system?s architecture and its components (input, preprocessing, sentiment model, result aggregation, and visualization); in sections 4 and 5 we evaluate our early experience with this system and discuss next steps. 2 Related Work  In the last decade, interest in mining sentiment and opinions in text has grown rapidly, due in part to the large increase of the availability of documents and messages expressing personal opinions (Pang & Lee, 2008). In particular, sentiment in Twitter data has been used for prediction or measurement in a variety of domains, such as stock market, politics and social movements (Bollen et al, 2011; 
Choy et al, 2011; Tumasjan et al, 2010; Zeitzoff, 2011). For example, Tumasjan (2010) found tweet volume about the political parties to be a good predictor for the outcome of the 2009 German election, while Choy et al (2011) failed to predict with Twitter sentiment the ranking of the four candidates in Singapore?s 2011 presidential election. Past studies of political sentiment on social networks have been either post-hoc and/or carried out on small and static samples. To address these issues, we built a unique infrastructure and sentiment model to analyze in real-time public sentiment on Twitter toward the 2012 U.S. presidential candidates. Our effort to gauge political sentiment is based on bringing together social science scholarship with advanced computational methodology: our approach combines real-time data processing and statistical sentiment modeling informed by, and contributing to, an understanding of the cultural and political practices at work through the use of Twitter. 3  The System For accuracy and speed, we built our real-time data processing infrastructure on the IBM?s InfoSphere Streams platform (IBM, 2012), which enables us to write our own analysis and visualization modules and assemble them into a real-time processing pipeline. Streams applications are highly scalable so we can adjust our system to handle higher volume of data by adding more servers and by distributing processing tasks. Twitter traffic often balloons during big events (e.g. televised debates or primary election days) and stays low between events, making high scalability strongly desirable. Figure 1 shows our system?s architecture and its modules. Next, we introduce our data source and each individual module. 
Figure 1. The system architecture for real-time processing Twitter data  
Preprocessinge.g.,Tokenization Match Tweetto Candidate
Real-time Twitter data
Throttle
SentimentModel Aggregate byCandidate Visualization
OnlineHumanAnnotation
Recordeddata
116
3.1 Input/Data Source We chose the micro-blogging service Twitter as our data source because it is a major source of online political commentary and discussion in the U.S. People comment on and discuss politics by posting messages and ?re-tweeting? others? messages. It played a significant role in political events worldwide, such as the Arab Spring Movement and the Moldovian protests in 2009. In response to events, Twitter volume goes up sharply and significantly. For example, during a republican debate, we receive several hundred thousand to a million tweets in just a few hours for all the candidates combined. Twitter?s public API provides only 1% or less of its entire traffic (the ?firehose?), without control over the sampling procedure, which is likely insufficient for accurate analysis of public sentiment. Instead, we collect all relevant tweets in real-time from the entire Twitter traffic via Gnip Power Track, a commercial Twitter data provider. To cope with this challenge during the later stages of the campaign, when larger Twitter traffic is expected, our system can handle huge traffic bursts over short time periods by distributing the processing to more servers, even though most of the times its processing load is minimal. Since our application targets the political domain (specifically the current Presidential election cycle), we manually construct rules that are simple logical keyword combinations to retrieve relevant tweets ? those about candidates and events (including common typos in candidate names). For example, our rules for Mitt Romney include Romney, @MittRomney, @PlanetRomney, @MittNews, @believeinromney, #romney, #mitt, #mittromney, and #mitt2012. Our system is tracking the tweets for nine Republican candidates (some of whom have suspended their campaign) and Barack Obama using about 200 rules in total. 3.2 Preprocessing The text of tweets differs from the text in articles, books, or even spoken language. It includes many 
idiosyncratic uses, such as emoticons, URLs, RT for re-tweet, @ for user mentions, # for hashtags, and repetitions. It is necessary to preprocess and normalize the text. As standard in NLP practices, the text is tokenized for later processing. We use certain rules to handle the special cases in tweets. We compared several Twitter-specific tokenizers, such as TweetMotif (O'Connor et al, 2010) and found Christopher Potts? basic Twitter tokenizer best suited as our base. In summary, our tokenizer correctly handles URLs, common emoticons, phone numbers, HTML tags, twitter mentions and hashtags, numbers with fractions and decimals, repetition of symbols and Unicode characters (see Figure 2 for an example). 3.3 Sentiment Model The design of the sentiment model used in our system was based on the assumption that the opinions expressed would be highly subjective and contextualized.  Therefore, for generating data for model training and testing, we used a crowd-sourcing approach to do sentiment annotation on in-domain political data. To create a baseline sentiment model, we used Amazon Mechanical Turk (AMT) to get as varied a population of annotators as possible. We designed an interface that allowed annotators to perform the annotations outside of AMT so that they could participate anonymously. The Turkers were asked their age, gender, and to describe their political orientation.  Then they were shown a series of tweets and asked to annotate the tweets' sentiment (positive, negative, neutral, or unsure), whether the tweet was sarcastic or humorous, the sentiment on a scale from positive to negative, and the tweet author's political orientation on a slider scale from conservative to liberal.  Our sentiment model is based on the sentiment label and the sarcasm and humor labels. Our training data consists of nearly 17000 tweets (16% positive, 56% negative, 18% neutral, 10% unsure), including nearly 2000 that were multiply annotated 
Tweet WAAAAAH!!! RT @politico: Romney: Santorum's 'dirty tricks' could steal Michigan: http://t.co/qEns1Pmi #MIprimary #tcot #teaparty #GOP Tokens WAAAAAH !!! RT @politico : Romney : Santorum's ' dirty tricks ' could steal Michigan : http://politi.co/wYUz7m #MIprimary #tcot #teaparty #GOP Figure 2. The output tokens of a sample tweet from our tokenizer 
117
to calculate inter-annotator agreement. About 800 Turkers contributed to our annotation. The statistical classifier we use for sentiment analysis is a na?ve Bayes model on unigram features. Our features are calculated from tokenization of the tweets that attempts to preserve punctuation that may signify sentiment (e.g., emoticons and exclamation points) as well as twitter specific phenomena (e.g., extracting intact URLs). Based on the data we collected our classifier performs at 59% accuracy on the four category classification of negative, positive, neutral, or unsure. These results exceed the baseline of classifying all the data as negative, the most prevalent sentiment category (56%). The choice of our model was not strictly motivated by global accuracy, but took into account class-wise performance so that the model performed well on each sentiment category. 3.4 Aggregation Because our system receives tweets continuously and uses multiple rules to track each candidate?s tweets, our display must aggregate sentiment and tweet volume within each time period for each candidate. For volume, the system outputs the number of tweets every minute for each candidate. For sentiment, the system outputs the number of positive, negative, neutral and unsure tweets in a sliding five-minute window. 3.5 Display and Visualization  We designed an Ajax-based HTML dashboard 
(Figure 3) to display volume and sentiment by candidate as well as trending words and system statistics. The dashboard pulls updated data from a web server and refreshes its display every 30 seconds. In Figure 3, the top-left bar graph shows the number of positive and negative tweets about each candidate (right and left bars, respectively) in the last five minutes as an indicator of sentiment towards the candidates. We chose to display both positive and negative sentiment, instead of the difference between these two, because events typically trigger sharp variations in both positive and negative tweet volume. The top-right chart displays the number of tweets for each candidate every minute in the last two hours. We chose this time window because a live-broadcast primary debate usually lasts about two hours. The bottom-left shows system statistics, including the total number of tweets, the number of seconds since system start and the average data rate. The bottom-right table shows trending words of the last five minutes, computed using TF-IDF measure as follows: tweets about all candidates in a minute are treated as a single ?document?; trending words are the tokens from the current minute with the highest TF-IDF weights when using the last two hours as a corpus (i.e., 120 ?documents?). Qualitative examination suggests that the simple TF-IDF metric effectively identifies the most prominent words when an event occurs. The dashboard gives a synthetic overview of volume and sentiment for the candidates, but it is often desirable to view selected tweets and their sentiments. The dashboard includes another page 
Figure 3. Dashboard for volume, sentiment and trending words 
118
(Figure 4) that displays the most positive, negative and frequent tweets, as well as some random neutral tweets. It also shows the total volume over time and a tag cloud of the most frequent words in the last five minutes across all candidates. Another crucial feature of this page is that clicking on one of the tweets brings up an annotation interface, so the user can provide his/her own assessment of the sentiment expressed in the tweet. The next section describes the annotation interface. 3.6 Annotation Interface The online annotation interface shown in Figure 5 lets dashboard (Figure 4) users provide their own judgment of a tweet. The tweet?s text is displayed at the top, and users can rate the sentiment toward the candidate mentioned in the tweet as positive, negative or neutral or mark it as unsure. There are also two options to specify whether a tweet is sarcastic and/or funny. This interface is a simplified version of the one we used to collect annotations from Amazon Mechanical Turk so that annotation can be performed quickly on a single tweet.  The online interface is designed to be used while watching a campaign event and can be displayed on a tablet or smart phone. The feedback from users allows annotation of recent data as well as the ability to correct misclassifications. As a future step, we plan to 
establish an online feedback loop between users and the sentiment model, so users? judgment serves to train the model actively and iteratively. 4 System Evaluation In Section 3.3, we described our preliminary sentiment model that automatically classifies tweets into four categories: positive, negative, neutral or unsure. It copes well with the negative bias in political tweets. In addition to evaluating 
Figure 5. Dashboard for most positive, negative and frequent tweets 
Figure 4. Online sentiment annotation interface 
119
the model using annotated data, we have also begun conducting correlational analysis of aggregated sentiment with political events and news, as well as indicators such as poll and election results. We are exploring whether variations in twitter sentiment and tweet volume are predictive or reflective of real-world events and news. While this quantitative analysis is part of ongoing work, we present below some quantitative and qualitative expert observations indicative of promising research directions. One finding is that tweet volume is largely driven by campaign events. Of the 50 top hourly intervals between Oct 12, 2011 and Feb 29, 2012, ranked by tweet volume, all but two correspond either to President Obama?s State of the Union address, televised primary debates or moments when caucus or primary election results were released. Out of the 100 top hourly intervals, all but 18 correspond to such events. The 2012 State of the Union address on Jan 24 is another good example. It caused the biggest volume we have seen in a single day since last October, 1.37 million tweets in total for that day. Both positive and negative tweets for President Obama increased three to four times comparing to an average day. During the Republican Primary debate on Jan 19, 2012 in Charleston, NC one of the Republican candidates, Newt Gingrich, was asked about his ex-wife at the beginning of the debate. Within minutes, our dashboard showed his negative sentiment increase rapidly ? it became three times more negative in just two minutes. This illustrates how tweet volume and sentiment are extremely responsive to emerging events in the real world (Vergeer et al, 2011). These examples confirm our assessment that it is especially relevant to offer a system that can provide real-time analysis during key moments in the election cycle. As the election continues and culminates with the presidential vote this November, we hope that our system will provide rich insights into the evolution of public sentiment toward the contenders. 5 Conclusion We presented a system for real-time Twitter sentiment analysis of the ongoing 2012 U.S. presidential election. We use the Twitter ?firehose? and expert-curated rules and keywords to get a full 
and accurate picture of the online political landscape. Our real-time data processing infrastructure and statistical sentiment model evaluates public sentiment changes in response to emerging political events and news as they unfold. The architecture and method are generic, and can be easily adopted and extended to other domains (for instance, we used the system for gauging sentiments about films and actors surrounding Oscar nomination and selection). References  Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood predicts the stock market. Journal of Computational Science, 2(1), 1-8. doi: 10.1016/j.jocs.2010.12.007 Choy, M., Cheong, L. F. M., Ma, N. L., & Koo, P. S. (2011). A sentiment analysis of Singapore Presidential Election 2011 using Twitter data with census correction. Gonz?lez-Ib??ez, R., Muresan, S., & Wacholder, N. (2011). Identifying Sarcasm in Twitter: A Closer Look. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. IBM. (2012). InfoSphere Streams, from http://www-01.ibm.com/software/data/infosphere/streams/ Lassen, D. S., & Brown, A. R. (2010). Twitter: The Electoral Connection? Social Science Computer Review. O'Connor, B., Krieger, M., & Ahn, D. (2010). TweetMotif: Exploratory Search and Topic Summarization for Twitter. In Proceedings of the the Fourth International AAAI Conference on Weblogs and Social Media, Washington, DC. Pang, B., & Lee, L. (2008). Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2), 1-135. doi: 10.1561/1500000011 Pew Research Center. (2011). 13% of online adults use Twitter. Retrieved from http://www.pewinternet.org/ ~/media//Files/Reports/2011/Twitter%20Update%202011.pdf Tumasjan, A., Sprenger, T. O., Sandner, P. G., & Welpe, I. M. (2010). Predicting Elections with Twitter: What 140 Characters Reveal about Political Sentiment. TweetCongress. (2012). Congress Members on Twitter  Retrieved Mar 18, 2012, from http://tweetcongress.org/members/ Twitter. (2012). What is Twitter  Retrieved Mar 18, 2012, from https://business.twitter.com/en/basics/what-is-twitter/ Vergeer, M., Hermans, L., & Sams, S. (2011). Is the voter only a tweet away? Micro blogging during the 2009 European Parliament election campaign in the Netherlands. First Monday [Online], 16(8).  Zeitzoff, T. (2011). Using Social Media to Measure Conflict Dynamics. Journal of Conflict Resolution, 55(6), 938-969. doi: 10.1177/0022002711408014 
120


References
 Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood 
predicts the stock market.  Journal of Computational 
Science, 2(1), 1-8. doi: 10.1016/j.jocs.2010.12.007
Choy, M., Cheong, L. F. M., Ma, N. L., & Koo, P. S. 
(2011). A sentiment analysis of Singapore Presidential 
Election 2011 using Twitter data with census correction.
González-Ibáñez, R., Muresan, S., & Wacholder, N. 
(2011). Identifying Sarcasm in Twitter: A Closer Look.
In Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics.
IBM. (2012). InfoSphere Streams, from http://www-
01.ibm.com/software/data/infosphere/streams/
Lassen, D. S., & Brown, A. R. (2010). Twitter: The 
Electoral Connection? Social Science Computer Review.
O\'Connor, B., Krieger, M., & Ahn, D. (2010). TweetMotif: 
Exploratory Search and Topic Summarization for 
Twitter. In Proceedings of the the Fourth International 
AAAI Conference on Weblogs and Social Media, 
Washington, DC.
Pang, B., & Lee, L. (2008). Opinion Mining and Sentiment 
Analysis.  Foundations and Trends in Information 
Retrieval, 2(1-2), 1-135. doi: 10.1561/1500000011
Pew Research Center. (2011). 13% of online adults use 
Twitter. Retrieved from http://www.pewinternet.org/
~/media//Files/Reports/2011/Twitter%20Update%2020
11.pdf
Tumasjan, A., Sprenger, T. O., Sandner, P. G., & Welpe, I. 
M. (2010). Predicting Elections with Twitter: What 140 
Characters Reveal about Political Sentiment.
TweetCongress. (2012). Congress Members on Twitter  
Retrieved Mar 18, 2012, from 
http://tweetcongress.org/members/
Twitter. (2012). What is Twitter  Retrieved Mar 18, 2012, 
from https://business.twitter.com/en/basics/what-istwitter/
Vergeer, M., Hermans, L., & Sams, S. (2011). Is the voter 
only a tweet away? Micro blogging during the 2009 
European Parliament election campaign in the 
Netherlands. First Monday [Online], 16(8). 
Zeitzoff, T. (2011). Using Social Media to Measure 
Conflict Dynamics.  Journal of Conflict Resolution, 
55(6), 938-969. doi: 10.1177/0022002711408014Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 438?442, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SAIL: A hybrid approach to sentiment analysis
Nikolaos Malandrakis1, Abe Kazemzadeh2, Alexandros Potamianos3, Shrikanth Narayanan1
1 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2 Annenberg Innovation Laboratory (AIL), USC, Los Angeles, CA 90089, USA
3Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, kazemzad@usc.edu, potam@telecom.tuc.gr, shri@sipi.usc.edu
Abstract
This paper describes our submission for Se-
mEval2013 Task 2: Sentiment Analysis in
Twitter. For the limited data condition we use
a lexicon-based model. The model uses an af-
fective lexicon automatically generated from a
very large corpus of raw web data. Statistics
are calculated over the word and bigram af-
fective ratings and used as features of a Naive
Bayes tree model. For the unconstrained data
scenario we combine the lexicon-based model
with a classifier built on maximum entropy
language models and trained on a large exter-
nal dataset. The two models are fused at the
posterior level to produce a final output. The
approach proved successful, reaching rank-
ings of 9th and 4th in the twitter sentiment
analysis constrained and unconstrained sce-
nario respectively, despite using only lexical
features.
1 Introduction
The analysis of the emotional content of text, is
relevant to numerous natural language processing
(NLP), web and multi-modal dialogue applications.
To that end there has been a significant scientific
effort towards tasks like product review analysis
(Wiebe and Mihalcea, 2006; Hu and Liu, 2004),
speech emotion extraction (Lee and Narayanan,
2005; Lee et al, 2002; Ang et al, 2002) and pure
text word (Esuli and Sebastiani, 2006; Strappar-
ava and Valitutti, 2004) and sentence (Turney and
Littman, 2002; Turney and Littman, 2003) level
emotion extraction.
The rise of social media in recent years has seen
a shift in research focus towards them, particularly
twitter. The large volume of text data available is
particularly useful, since it allows the use of com-
plex machine learning methods. Also important is
the interest on the part of companies that are actively
looking for ways to mine social media for opinions
and attitudes towards them and their products. Sim-
ilarly, in journalism there is interest in sentiment
analysis for a way to process and report on the public
opinion about current events (Petulla, 2013).
Analyzing emotion expressed in twitter borrows
from other tasks related to affective analysis, but
also presents unique challenges. One common is-
sue is the breadth of content available in twitter: a
more limited domain would make the task easier,
however there are no such bounds. There is also a
significant difference in the form of language used
in tweets. The tone is informal and typographical
and grammatical errors are very common, making
even simple tasks, like Part-of-Speech tagging much
harder. Features like hashtags and emoticons can
also be helpful (Davidov et al, 2010).
This paper describes our submissions for Se-
mEval 2013 task 2, subtask B, which deals pri-
marily with sentiment analysis in twitter. For the
constrained condition (using only the organizer-
provided twitter sentences) we implemented a sys-
tem based on the use of an affective lexicon and part-
of-speech tag information, which has been shown
relevant to the task (Pak and Paroubek, 2010).
For the unconstrained condition (including external
sources of twitter sentences) we combine the con-
strained model with a maximum entropy language
438
model trained on external data.
2 Experimental procedure
We use two separate models, one for the constrained
condition and a combination for the unconstrained
condition. Following are short descriptions.
2.1 Lexicon-based model
The method used for the constrained condition is
based on an affective lexicon containing out-of-
context affective ratings for all terms contained in
each sentence. We use an automated algorithm of
affective lexicon expansion based on the one pre-
sented in (Malandrakis et al, 2011), which in turn
is an expansion of (Turney and Littman, 2002).
We assume that the continuous (in [?1, 1]) va-
lence and arousal ratings of any term can be repre-
sented as a linear combination of its semantic simi-
larities to a set of seed words and the affective rat-
ings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (1)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
and wj . For the purposes of this work, the seman-
tic similarity metric is the cosine similarity between
context vectors computed over a corpus of 116 mil-
lion web snippets collected by posing one query for
every word in the Aspell spellchecker?s vocabulary
to the Yahoo! search engine and collecting up to 500
of the top results.
Given a starting, manually annotated, lexicon we
can select part of it to serve as seed words and then
use 1 to create a system of linear equations where
the only unknowns are the weights ai. The system
is solved using Least Squares Estimation. That pro-
vides us with an equation that can generate affective
ratings for every term (not limited to words), as long
as we can estimate the semantic similarity between
it and the seed words.
Seed word selection is performed by a simple
heuristic (though validated through experiments):
we want seed words to have extreme affective rat-
ings (maximum absolute value) and we want the set
to be as closed to balanced as possible (sum of seed
ratings equal to zero).
Given these term ratings, the next step is combin-
ing them through statistics. To do that we use sim-
ple statistics (mean, min, max) and group by part
of speech tags. The results are statistics like ?max-
imum valence among adjectives?, ?mean arousal
among proper nouns? and ?number of verbs and
nouns?. The dimensions used are: valence, absolute
valence and arousal. The grouping factors are the 39
Penn treebank pos tags plus higher order tags (adjec-
tives, verbs, nouns, adverbs and combinations of 2,3
and 4 of them). The statistics extracted are: mean,
min, max, most extreme, sum, number, percentage
of sentence coverage. In the case of bigram terms no
part-of-speech filtering/grouping is applied. These
statistics form the feature vectors.
Finally we perform feature selection on the mas-
sive set of candidates and use them to train a model.
The model selected is a Naive Bayes tree, a tree with
Naive Bayes classifiers on each leaf. The motivation
comes by considering this a two stage problem: sub-
jectivity detection and polarity classification, mak-
ing a hierarchical model a natural choice. NB trees
proved superior to other types of trees during our
testing, presumably due to the smoothing of obser-
vation distributions.
2.2 N-gram language model
The method used for the unconstrained condition
is based on a combination of the automatically ex-
panded affective lexicon described in the previ-
ous section together with a bigram language model
based on the work of (Wang et al, 2012), which
uses a large set of twitter data from the U.S. 2012
Presidential election. As a part of the unconstrained
system, we were able to leverage external annotated
data apart from those provided by the SEMEVAL
2013 sentiment task dataset. Of the 315 million
tweets we collected about the election, we anno-
tated a subset of 40 thousand tweets using Ama-
zon Mechanical Turk. The annotation labels that
we used were ?positive?, ?negative?, ?neutral?, and
?unsure?, and additionally raters could mark tweets
for sarcasm and humor. We excluded tweets marked
as ?unsure? as well as tweets that had disagree-
439
ment in labels if they were annotated by more than
one annotator. To extract the bigram features, we
used a twitter-specific tokenizer (Potts, 2011), which
marked uniform resource locators (URLs), emoti-
cons, and repeated characters, and which lowercased
words that began with capital letters followed by
lowercase letters (but left words in all capitals). The
bigram features were computed as presence or ab-
sense in the tweet rather than counts due to the small
number of words in tweets. The machine learning
model used to classify the tweets was the Megam
maximum entropy classifier (Daume? III, 2004) in
the Natural Language Toolkit (NLTK) (Bird et al,
2009).
2.3 Fusion
The submitted system for the unconstrained condi-
tion leverages both the lexicon-based and bigram
language models. Due to the very different nature
of the models we opt to not fuse them at the feature
level, using a late fusion scheme instead. Both par-
tial models are probabilistic, therefore we can use
their per-class posterior probabilities as features of
a fusion model. The fusion model is a linear kernel
SVM using six features, the three posteriors from
each partial model, and trained on held out data.
3 Results
Following are results from our method, evaluated
on the testing sets (of sms and twitter posts) of
SemEval2013 task 2. We evaluate in terms of 3-
class classification, polarity classification (positive
vs. negative) and subjectivity detection (neutral vs.
other). Results shown in terms of per category f-
measure.
3.1 Constrained
The preprocessing required for the lexicon-based
model is just part-of-speech tagging using Treetag-
ger (Schmid, 1994). The lexicon expansion method
is used to generate valence and arousal ratings for
all words and ngrams in all datasets and the part of
speech tags are used as grouping criteria to gener-
ate statistics. Finally, feature selection is performed
using a correlation criterion (Hall, 1999) and the re-
sulting feature set is used to train a Naive Bayes
tree model. The feature selection and model train-
Table 1: F-measure results for the lexicon-based model,
using different machine learning methods, evaluated on
the 3-class twitter testing data.
model
per-class F-measure
neg neu pos
Nbayes 0.494 0.652 0.614
SVM 0.369 0.677 0.583
CART 0.430 0.676 0.593
NBTree 0.561 0.662 0.643
Table 2: F-measure results for the constrained condition,
evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.561 0.662 0.643
pos vs neg 0.679 0.858
neu vs other 0.685 0.699
sms
3-class 0.506 0.709 0.531
pos vs neg 0.688 0.755
neu vs other 0.730 0.628
ing/classification was conducted using Weka (Wit-
ten and Frank, 2000).
The final model uses a total of 72 features, which
can not be listed here due to space constraints. The
vast majority of these features are necessary to de-
tect the neutral category: positive-negative separa-
tion can be achieved with under 30 features.
One aspect of the model we felt worth investigat-
ing, was the type of model to be used. Using a multi-
stage model, performing subjectivity detection be-
fore positive-negative classification, has been shown
to provide an improvement, however single models
have also been used extensively. We compared some
popular models: Naive Bayes, linear kernel SVM,
CART-trained tree and Naive Bayes tree, all using
the same features, on the twitter part of the SemEval
testing data. The results are shown in Table 1. The
two Naive Bayes-based models proved significantly
better, with NBTree being clearly the best model for
these features.
Results from the submitted constrained model are
shown in Table 2. Looking at the twitter data re-
sults and comparing the positive-negative vs the
440
3-class results, it appears the main weakness of
this model is subjectivity detection, mostly on the
neutral-negative side. It is not entirely clear to us
whether that is an artifact of the model (the nega-
tive class has the lowest prior probability, thus may
suffer compared to neutral) or of the more complex
forms of negativity (sarcasm, irony) which we do not
directly address. There is a definite drop in perfor-
mance when using the same twitter-trained model on
sms data, which we would not expect, given that the
features used are not twitter-specific. We believe this
gap is caused by lower part-of-speech tagger perfor-
mance: visual inspection reveals the output on twit-
ter data is fairly bad.
Overall this model ranked 9th out of 35 in the
twitter set and 11th out of 28 in the sms set, among
all constrained submissions.
3.2 Unconstrained
Table 3: F-measure results for the maximum entropy
model with bigram features, evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.403 0.661 0.623
pos vs neg 0.586 0.804
neu vs other 0.661 0.704
sms
3-class 0.390 0.587 0.542
pos vs neg 0.710 0.648
neu vs other 0.587 0.641
Table 4: F-measure results for the unconstrained condi-
tion, evaluated on the testing data.
set classes
per-class F-measure
neg neu pos/other
twitter
3-class 0.565 0.679 0.655
pos vs neg 0.672 0.881
neu vs other 0.667 0.732
sms
3-class 0.502 0.723 0.538
pos vs neg 0.625 0.772
neu vs other 0.710 0.637
In order to create the submitted unconstrained
model we train an SVM model using the lexicon-
based and bigram language model posterior proba-
bilities as features. This fusion model is trained on
held-out data (the development set of the SemEval
data). The results of classification using the bigram
language model alone are shown in Table 3 and the
results from the final fused model are shown in Ta-
ble 4. Looking at relative per-class performance, the
results follow a form most similar to the constrained
model, though there are gains in all cases. These
gains are less significant when evaluated on the sms
data, resulting in a fair drop in ranks: the bigram lan-
guage model (expectedly) suffers more when mov-
ing to a different domain, since it uses words as
features rather than the more abstract affective rat-
ings used by the lexicon-based model. Also, because
the external data used to train the bigram language
model was from discussions of politics on Twitter,
the subject matter also varied in terms of prior senti-
ment distribution in that the negative class was pre-
dominant in politics, which resulted in high recall
but low precision for the negative class.
This model ranked 4th out of 16 in the twitter set
and 7th out of 17 in the sms set, among all uncon-
strained submissions.
4 Conclusions
We presented a system of twitter sentiment analy-
sis combining two approaches: a hierarchical model
based on an affective lexicon and a language model-
ing approach, fused at the posterior level. The hier-
archical lexicon-based model proved very successful
despite using only n-gram affective ratings and part-
of-speech information. The language model was
not as good individually, but provided a noticeable
improvement to the lexicon-based model. Overall
the models achieved good performance, ranking 9th
of 35 and 4th of 16 in the constrained and uncon-
strained twitter experiments respectively, despite us-
ing only lexical information.
Future work will focus on incorporating im-
proved tokenization (including part-of-speech tag-
ging), making better use of twitter-specific features
like emoticons and hashtags, and performing affec-
tive lexicon generation on twitter data.
441
References
J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP, pages 2037?2040.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly Media.
H. Daume? III. 2004. Notes on cg and lm-bfgs op-
timization of logistic regression. Paper available at
http://pub. hal3. name# daume04cg-bfgs, implementa-
tion available at http://hal3. name/megam.
D. Davidov, O. Tsur, and A. Rappoport. 2010. Enhanced
sentiment learning using twitter hashtags and smileys.
In Proc. COLING, pages 241?249.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A pub-
licly available lexical resource for opinion mining. In
Proc. LREC, pages 417?422.
M. A. Hall. 1999. Correlation-based feature selection
for machine learning. Ph.D. thesis, The University of
Waikato.
M. Hu and B. Liu. 2004. Mining and summarizing cus-
tomer reviews. In Proc. SIGKDD, KDD ?04, pages
168?177. ACM.
C. M. Lee and S. Narayanan. 2005. Toward detecting
emotions in spoken dialogs. IEEE Transactions on
Speech and Audio Processing, 13(2):293?303.
C. M. Lee, S. Narayanan, and R. Pieraccini. 2002. Com-
bining acoustic and language information for emotion
recognition. In Proc. ICSLP, pages 873?876.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
A. Pak and P. Paroubek. 2010. Twitter as a corpus
for sentiment analysis and opinion mining. In Proc.
LREC, pages 1320?1326.
S. Petulla. 2013. Feelings, nothing more than feelings:
The measured rise of sentiment analysis in journalism.
Neiman Journalism Lab, January.
C. Potts. 2011. Sentiment symposium tutorial: Tokeniz-
ing. Technical report, Stanford Linguistics.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proc. International Confer-
ence on New Methods in Language Processing, vol-
ume 12, pages 44?49.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an affective extension of WordNet. In Proc. LREC,
volume 4, pages 1083?1086.
P. Turney and M. L. Littman. 2002. Unsupervised
Learning of Semantic Orientation from a Hundred-
Billion-Word Corpus. Technical report ERC-1094
(NRC 44929). National Research Council of Canada.
P. Turney and M. L. Littman. 2003. Measuring praise
and criticism: Inference of semantic orientation from
association. ACM Transactions on Information Sys-
tems, 21:315?346.
H. Wang, D. Can, A. Kazemzadeh, F. Bar, and
S. Narayanan. 2012. A system for real-time twitter
sentiment analysis of 2012 u.s. presidential election
cycle. In Proc. ACL, pages 115?120.
J. Wiebe and R. Mihalcea. 2006. Word sense and subjec-
tivity. In Proc. COLING/ACL, pages 1065?1072.
Ian H.Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann.
442
